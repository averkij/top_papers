{
    "paper_title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "authors": [
        "Marvin Schmitt",
        "Anne Schwerk",
        "Sebastian Lempert"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task."
        },
        {
            "title": "Start",
            "content": ", Anne Schwerk , Sebastian Lempert IU International University of Applied Sciences, Juri-Gagarin-Ring 152, Erfurt, 99084, Thuringia, Germany 6 2 0 2 3 ] . [ 1 2 0 3 8 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLMs architecture and the semantic complexity of the task. Keywords: sentiment analysis, irony detection, large language models (LLMs), prompt engineering 1. Introduction Large language models (LLMs) like GPT-4 have demonstrated remarkable capabilities across natural language processing (NLP) tasks, including sentiment analysis (W. Zhang et al., 2024). However, effectively harnessing these models often depends on how they are prompted to produce good answer of high quality. Prompt engineering the craft of designing input prompts to guide the models outputs has emerged as key factor in unlocking LLMs full potential (Chen et al., 2023; Marvin et al., 2024; Schulhoff et al., 2024). Recent advances propose sophisticated prompting techniques (e.g. providing few-shot examples or instructing the model to reason step-by-step) to improve performance beyond naive zero-shot usage. This study focuses on analyzing how such advanced prompt engineering strategies impact LLM performance on sentiment analysis tasks, specifically evaluating two cutting-edge models (OpenAIs GPT-4o-mini and Googles gemini-1.5-flash) across spectrum of sentiment analysis challenges. We compare baseline prompt (simple zero-shot instruction) against advanced prompting approaches including few-shot prompting, chain-of-thought (CoT) reasoning, and self-consistency and quantitatively assess their impact using standard classification metrics like accuracy and F1-score. By systematically varying the prompting strategy, we aim to determine to what extent Corresponding author, E-mail address: sebastian.lempert@iu.org E-mail address: marvin.schmitt@iu.org E-mail address: anne.schwerk@iu.org prompt engineering can optimize sentiment classification outcomes. To thoroughly evaluate these effects, we consider multiple representative datasets covering both straightforward and nuanced sentiment analysis scenarios. The experimental tasks span: Binary sentiment classification: Stanford Sentiment Treebank (SST-2) movie reviews (English), labeled positive vs. negative Multilingual three-class sentiment: SB-10k corpus of 10,000 German tweets labeled positive, neutral, or negative Aspect-based sentiment analysis (ABSA): SemEval-2014 ABSA challenge dataset (customer reviews in domains like laptops and restaurants) annotated with sentiment for specific aspects in each text. Irony detection: SemEval-2018 Task 3 dataset of 3,000 English tweets, each manually annotated as ironic or not ironic. By evaluating prompt techniques on this diverse set of sentiment analysis problems from basic polarity classification to aspect-level opinion mining and recognizing irony we ensure comprehensive analysis of prompt engineerings impact. The inclusion of both English and non-English content (e.g. German tweets in SB-10k) further tests each models adaptability to multilingual prompts. Through this broad experimental scope, our work investigates whether carefully engineered prompts can consistently boost LLM accuracy across varying sentiment tasks, and how the effectiveness of these techniques may differ by task type and by model. 1.1. Motivation Sentiment analysis is long-standing challenge in NLP, and certain sub-tasks remain difficult for traditional approaches, as for example n-grams (Jim et al., 2024). For instance, detecting aspect-based sentiments within text (identifying sentiment toward specific attributes in review) and recognizing subtle linguistic nuances like irony or sarcasm are complex problems that often require contextual reasoning beyond surface-level cues (Jim et al., 2024). Contemporary LLMs bring new capabilities to these problems they possess broad world knowledge and the ability to perform reasoning in context yet leveraging these capabilities is not trivial. While fine-tuned models have historically dominated sentiment analysis, the rise of powerful generative LLMs suggests we might obtain strong performance without task-specific training if we can prompt the model effectively (Zhou et al., 2024). Prompt engineering offers flexible, data-efficient way to adapt single LLM to multiple sentiment tasks by simply varying its input instructions, potentially obviating the need for collecting large labeled datasets or training separate models for each sentiment domain. Despite the explosion of LLM applications, there is notable research gap on how advanced prompting techniques specifically affect sentiment analysis performance (Sun et al., 2024). Prior works have extensively benchmarked different LLMs on general NLP tasks, but few have isolated the contribution of prompt design on sentiment tasks. This gap is significant: sentiment analysis often involves subjective and context-dependent judgments that might benefit from prompts which encourage the model to think (e.g. via CoT) or see more examples (fewshot). We are motivated to address whether and how specialized prompts can reliably enhance LLM accuracy in sentiment analysis, as opposed to using generic prompt. Understanding this would both advance the theory of prompt-based learning and inform practical deployment of LLMs for sentiment tasks. From an applied perspective, even modest gains in sentiment classification accuracy can be impactful. For example, in customer service applications, accurately detecting customers sentiment and any sarcastic tone in their message is crucial for generating appropriate, empathetic responses. Prompt strategies that significantly improve irony detection or aspect understanding could be directly leveraged to build more sensitive sentiment analysis systems in marketing, finance, or social media monitoring. In summary, the motivation of this work is to explore prompt engineering as means to bridge the gap between generalpurpose LLMs and the specialized demands of sentiment analysis. By experimentally quantifying improvements due to prompts, we seek to contribute insights on maximizing LLM performance in affective computing contexts without additional model fine-tuning. This has important theoretical implications for how language models interpret prompts, and practical implications for rapidly adapting AI to real-world sentiment understanding problems. 1.2. Research question Building on the above motivations, our study is guided by the following research question: RQ: To what extent can advanced prompt engineering techniques (such as few-shot examples, CoT prompting, and selfconsistency) improve the accuracy and F1-score of LLMs on sentiment analysis tasks compared to baseline zero-shot prompt? We ask this in the context of both simple sentiment classification and more nuanced sentiment detection (e.g., irony). While this study primarily focuses on performance gains, it also considers additional aspects of prompt engineering. Specifically, we investigate variations in these effects across different LLM architectures (e.g., OpenAI GPT-4o-mini vs. Google gemini-flash-1.5). Furthermore, we explore the efficacy of specific prompting strategies for distinct sentiment analysis tasks (e.g., reasoning-based prompts (CoT) for sarcasm detection) and assess whether techniques such as self-consistency enhance the stability and robustness of model predictions. 2. Related work 2.1. LLMs in sentiment analysis The advent of generative LLMs has prompted researchers to evaluate their capabilities on sentiment tasks traditionally handled by smaller, fine-tuned models. Zhou et al. (2024) conducted comprehensive evaluation of multiple LLMs on aspect-based sentiment analysis (ABSA) across 13 datasets. They found that with appropriate prompting (in-context demonstrations), large models can achieve stateof-the-art performance on ABSA, in some cases rivaling or surpassing fine-tuned smaller models without any task-specific training. This underscores the promise of prompt-based learning for sentiment tasks. In contrast, Filip et al. (2024) indicate with regard to ABSA that lightly fine-tuned medium-sized models can outperform incontext prompting on fine-grained sentiment tasks. In the financial domain, W. Luo and Gong (2024) showed that adapting pre-trained LLM through light supervised finetuning drastically improved financial news sentiment classification, outperforming prior state-of-the-art algorithms. Notably, even relatively small 7B-parameter LLaMA 2 model, when properly adapted, exceeded the accuracy of specialized models on detecting positive/negative sentiment in stock news. These studies illustrate that LLMs possess strong capabilities for sentiment analysis which can be unlocked either by careful prompting or minimal fine-tuning. Several works have specifically explored targeted or finegrained sentiment tasks with LLMs, highlighting both their strengths and the importance of prompt design. Juro≈° et al. (2024) investigated targeted sentiment analysis of news headlines (sentiment toward particular entity in the headline) using GPT models. They reported that LLMs 2 outperformed traditional fine-tuned encoder-based models on datasets with more descriptive annotations, thanks to LLMs broad knowledge and reasoning. Moreover, they found that the level of prompt prescriptiveness markedly affected performance: providing more elaborate instructions or few-shot examples improved F1 scores and calibration of the models confidence, though there was an optimal level beyond which returns diminished. This result points to the delicate balance in prompt engineering overly simple prompts underutilize the model, while overly complex prompts can introduce confusion or noise. Stigall et al. (2024) provide related perspective, comparing the performance of various LLMs on emotion and sentiment classification tasks. Their work introduced finetuned small model EmoBERTTiny specialized for emotion detection, which actually outperformed some larger pre-trained LLMs on sentiment/emotion benchmarks. This finding suggests that without proper prompting or adaptation, generic LLM may still lag behind dedicated model, reinforcing the need for effective prompt engineering (or fine-tuning) to get the best results from LLMs. 2.2. Prompt engineering and affective computing With LLMs becoming prevalent, researchers have started examining how prompt formulation influences their performance in affective computing tasks (sentiment, emotion, toxicity, sarcasm). Amin and Schuller (2024) present focused study on the prompt sensitivity of ChatGPT (GPT-3.5 and GPT-4) for sentiment analysis, toxicity detection, and sarcasm detection. By systematically varying prompt wording, structure, and generation parameters, they demonstrated that the models performance can vary significantly under different prompts, even when the core task remains the same. Certain phrasings or instructions yielded substantially better accuracy, whereas others led to declines, highlighting that prompt design is an essential lever for optimizing LLM behavior. They also explored how decoding settings (like temperature in generation) affect outcomes, noting trade-off between creativity and consistency that practitioners must manage. Yao et al. (2024) propose SarcasmCue, framework with four prompting methods (chain of contradiction, graph of cues, bagging of cues, tensor of cues) to help LLMs detect sarcasm by considering both sequential and non-sequential cues. Their experiments show: (1) advanced models like GPT-4 and Claude 3.5 perform best with CoC and GoC prompts; (2) the ToC method greatly boosts smaller LLM performance; and (3) their framework consistently beats the previous state-of-the-art across four datasets, proving its effectiveness and robustness. Y. Zhang et al. (2024) introduce SarcasmBench, benchmark for evaluating LLMs on sarcasm detection, testing eleven leading LLMs and eight pre-trained language models across six datasets using various prompting methods. Their key findings are: (1) current LLMs perform worse than supervised PLMs for sarcasm detection, indicating more work is needed; (2) GPT-4 outperforms other LLMs by significant margin; and (3) fewshot input/output prompting outperforms zero-shot input/output prompting and few-shot CoT. The importance of prompt design is further echoed by surveys of LLM applications in sentiment analysis. Yang et al. (2024) provide survey on text-centric multimodal sentiment analysis in the LLM era, noting that while models like ChatGPT open new possibilities, it remains unclear how to best adapt them to complex multimodal sentiment tasks, and that careful prompt and instruction tuning is likely required. Krugmann and Hartmann (2024) examined sentiment analysis in the age of generative AI and found that even in zero-shot settings, LLMs like GPT-4 can match or exceed the accuracy of domain-specific sentiment classifiers. They emphasize, however, that the characteristics of the input text and the prompt heavily influence performance: for example, longer and more content-rich texts allowed the LLMs to shine, whereas very short or informally written inputs saw the LLMs struggle somewhat. Notably, Krugmann and Hartmann (2024) also reported that LLMs (especially an instruction-tuned model like LLaMA 2) can provide human-like explanations for their sentiment decisions, which is valuable byproduct of techniques like CoT prompting that elicit the models reasoning. 2.3. Summary of related work and positioning of this work In summary, existing work establishes that LLMs are powerful tools for sentiment analysis across domains (from customer reviews to news and finance) when used appropriately. Proper prompt engineering appears repeatedly as decisive factor: it can determine whether an LLM merely competes with older models or decisively surpasses them (Krugmann & Hartmann, 2024). Our research builds on this foundation by providing focused, experimental look at advanced prompting techniques (few-shot, CoT, self-consistency) across multiple sentiment tasks. This bridges the gap between broad evaluations of LLMs on sentiment tasks and targeted analysis of how to prompt them best for optimal performance. 3. Background 3.1. Prompting Before providing the methodological details, we provide brief background on the relevant areas of prompting, prompt engineering, and sentiment analysis. First, prompt serves as the interface between users and LLMs. It is an instruction, possibly accompanied by specific contextual information, that is given to model to elicit particular response. Prompts can vary widely, ranging from simple, everyday expressions to complex, nested instructions (Liu et al., 2021). Marvin et al. (2024) further elucidate these fundamental principles by breaking down prompt into components such as instruction, input data, context, and an output directive. Specifying the desired output format, such as CSV or Markdown, can help structure the LLM-generated output. Style instructions are 3 an additional variant of formatting directives, adjusting the outputs style without altering its structure. Finally, supplementary information can also be part of prompt to contextualize the output. When instructing to draft an email, details like name and position might be added as system instructions to create an appropriate signature (Schulhoff et al., 2024). 3.2. Prompt engineering Second, the concept of prompt engineering pertains to the design and conceptualization of prompts. Adhering to various rules and nuances in prompt formulation or handling can significantly enhance output performance and quality (White et al., 2023). Additionally, LLMs can be \"programmed\" using prompt engineering. For example, prompt can be structured to prompt the model to ask the user multiple questions until sufficient information is gathered to generate specific output, such as piece of code (White et al., 2023). In addition to the advanced prompt engineering techniques examined in this research, it is essential to acknowledge certain foundational principles that, while not classified as prompt engineering approaches within the scope of this study, still play critical role in effective prompt design. These include the importance of clear and precise prompt formulation, ensuring that the model receives unambiguous instructions tailored to the specific task (Chen et al., 2023). Furthermore, role prompting, in which the model is explicitly assigned role (e.g., \"you are sentiment analysis expert\"), can help guide the models responses by providing contextual framing (Chen et al., 2023). While these principles are not considered advanced techniques in this work, they serve as fundamental best practices in prompt creation and contribute to establishing strong baseline for subsequent optimizations. 3.3. Sentiment analysis Third, since this study centers on the field of sentiment analysis, it is essential to highlight its broad range of applications. sentiment analysis is employed across various domains, including the analysis of customer feedback, market monitoring, the optimization of customer service, and the evaluation of public opinion in political and social contexts. Owing to its versatility, sentiment analysis serves as key enabler of data-driven decision-making and facilitates more personalized and contextsensitive responses to individual needs (Sweta, 2024). Sweta (2024) highlights different areas within sentiment including binary classification (positive/negative), analysis, multi-class classification (including neutral), and aspect-based analyses that focus on specific features or attributes of an object. Further specialized areas encompass irony detection, temporal analysis of opinion shifts, domain-specific adaptations, and fine-grained analyses that capture the strength or intensity of opinions. In this study, we focus on selected areas of sentiment analysis, as they represent various dimensions of text analysis and enable nuanced assessment of the performance of LLMs. Specifically, the inclusion of classical sentiment classification (SC), ABSA, and multi-faceted analysis of subjective texts (MAST) permits an in-depth exploration of both the versatility and limitations of LLMs and individual prompting approaches in the context of sentiment analysis. While SC is concerned with categorizing texts into predefined sentiment categories, ABSA allows more detailed examination of specific aspects mentioned within the text. MAST, on the other hand, encompasses more complex tasks such as irony detection, which require deeper understanding of linguistic nuances and contextual interpretation (Wang et al., 2024; W. Zhang et al., 2024). 4. Experimental design 4.1. Prompt engineering approaches The selection of shot prompting, CoT, and self-consistency in this study is grounded in their demonstrated effectiveness for complex, context-dependent tasks such as sentiment analysis. As Sahoo et al. (2024) emphasize, prompt engineering strategies must be adapted to the specific domain, with sentiment analysis particularly requiring reasoning over subtle and nuanced language. These approaches have consistently shown substantial performance gains in the literature (Q. Luo et al., 2023; Wei et al., 2022; W. Zhang et al., 2024), offering structured reasoning processes that help reduce inconsistencies and infer implicit meanings. Their domain alignment and empirical success make them well-justified focus for our investigation. Table 1 provides an overview of the prompting strategies employed in this study, summarizing the respective advantages and limitations of each approach. The complete set of prompts used throughout the study is presented in Appendix A. 4.2. Hypotheses and experimental setup As outlined in the central research question (Chapter 1.2), this study examines the extent to which advanced PE techniques can enhance the performance of LLMs on SA tasks. The analysis spans both standard sentiment classification and more nuanced tasks such as irony detection. Accordingly, the experimental design is structured around three hypotheses, each addressing specific aspect of the research question: H1: The application of various prompt engineering techniques increases the accuracy and F1-score of LLMs in sentiment classification compared to baseline prompts. H2: Precisely designed prompt engineering techniques enhance the ability of LLMs to detect aspect-based sentiments within text compared to generic prompts, as measured by metrics such as accuracy and weighted F1-score. H3: Well-structured prompts utilizing advanced techniques significantly improve accuracy and F1-score in the detection of ironic tweets compared to simpler prompts. The experimental design follows the approach of W. Zhang et al. (2024), as their methodology provides comprehensive and nuanced evaluation of the performance of LLMs across various sentiment analysis tasks. Their study incorporates both straightforward classification tasks and more complex analyses, such as ABSA and irony detection."
        },
        {
            "title": "Advantages",
            "content": "zero-shot Efficient, no examples needed One-/few-shot"
        },
        {
            "title": "CoT",
            "content": "zero-shot-CoT self-consistency Improves performance with examples; adapts to tasks Enhances structured thinking; improves accuracy in complex tasks Combines zero-shot with CoT; reduces example dependency Increases response reliability; enhances multipath tasks Limitations Limited accuracy for complex tasks; struggles with subtle contexts Risk of overfitting; depends on example quality; potential bias Inefficient for simple tasks; requires large LLMs, increasing cost Needs large LLMs; sensitive to question phrasing; possible inefficiencies Needs multiple runs (higher resource use); risk of inconsistent outputs Table 1: Overview of prompting approaches Building on this foundation, the experimental procedure adopted in this study follows structured, multi-step process designed to systematically investigate different promptengineering techniques and their impact on LLM performance in the three specified areas of sentiment analysis. The impact of these techniques is assessed by comparing their performance against baseline prompt, with accuracy and F1-score serving as key evaluation metrics. For the classification process (the LLM invocation), each text instance is submitted to the selected LLMs (GPT-4o-mini and gemini-1.5-flash) via API requests. To balance computational efficiency and statistical validity, subset of 1,000 randomly sampled entries per dataset is used. To assess whether performance differences between prompting strategies (e.g., baseline vs. one-shot) are statistically significant, bootstrap resampling procedure was applied. In this process, 1,000 bootstrap samples were drawn with replacement from the evaluation set, and the weighted F1-score was computed for each model within each sample. The resulting distribution of F1-score differences was then used to construct 95 % confidence interval. If this interval does not include zero, the observed performance difference is considered statistically significant. Appendix provides detailed account of the results obtained through the bootstrap resampling procedure."
        },
        {
            "title": "The experiment also considers",
            "content": "language consistency: prompts are formulated in the same language as the dataset to prevent misinterpretations caused by language mismatches. The structured design progresses through the three analysis tasks sentiment classification, ABSA, and irony detection evaluating results for each prompt type across both models. 4.2.1. Models For this experiment, the current model variants as of January 2025 (GPT-4o-mini from OpenAI and gemini-1.5-flash from Google) were deliberately selected, as they optimally meet the requirements for both performance and cost efficiency. The choice of these specific models is based on strategic consideration: both variants offer powerful language understanding while being cost-optimized, enabling resource-efficient execution of the experiment. This is particularly relevant in the context of scientific study, where balance between precise results and feasible cost structures is essential. GPT-4o-mini is more compact version of GPT-4, specifically designed for use cases requiring high performance without incurring the costs associated with larger variants (OpenAI, 2024a, 2024b, 2025a). It thus provides solid foundation for testing the effectiveness of prompt engineering techniques in sentiment analysis. At the same time, gemini-1.5-flash, the most balanced model in the current Gemini series (as of January 2025), allows for comprehensive evaluation under an alternative architectural approach (Google, 2025a). Both models provide dedicated Python libraries that seamlessly integrate into the Jupyter environment (Google, 2025b; OpenAI, 2025b). These libraries facilitate interaction with the models by enabling structured and efficient use of their respective APIs. Once installed and configured, queries can be executed directly from the code, ensuring flexible and reproducible interaction with the language models (Caelen & Blete, 2024). From technical perspective, both models utilize structured prompts, distinguishing between system prompts (which define the models role and behavior) and user prompts (which contain the specific classification request). The experimental setup follows this schema, with system instructions tailored to each subtask. To ensure consistency, the temperature parameter is reduced from the default value of 1 to 0.2, minimizing variability in model responses. 4.2.2. Datasets For each area of the sentiment analysis examined in this study, distinct dataset is utilized. In the case of sentiment classification, two different datasets are employed, covering both binary classification and multi-class sentiment classification. Additionally, German-language dataset is included to evaluate the models linguistic versatility. For the remaining two areas, ABSA and irony detection, one dataset per task is applied. An overview of the datasets is provided in Table 2. 4.2.3. Evaluation To establish reference point, the experiment begins with baseline prompt using simple sentiment classification prompt (e.g., Classify the following statement as positive, neutral, or"
        },
        {
            "title": "Usage",
            "content": "SB10k The corpus SB10k contains 9783 German tweets. Each tweet has sentiment annotations on tweet level by 3 human annotators, using sentiment classes positive, negative, neutral, mixed, and unknown. For the experiment, all tweets classified as mixed or unknown were removed."
        },
        {
            "title": "Sentiment\nclassification",
            "content": "SST-2 / SST binary SemEval 2014-ABSA Challenge Dataset SemEval2018-Irony Dataset The Stanford Sentiment Treebank (SST) is corpus with fully labeled parse trees that were parsed from 11,855 single sentences that were extracted from movie reviews. Each phrase has fine-grained sentiment annotations by 3 human annotators, using sentiment classes very negative, negative, neutral, positive, and very positive. The subset containing binary classifications using sentiment classes positive and negative (neutral sentences were discarded) is referred to as SST-2 or SST binary. This dataset was introduced as part of the SemEval-2014 Task 4, which focused on ABSA. The dataset consists of customer reviews from domains such as restaurants and laptops. Each review is annotated to identify both the relevant aspects (e.g. price, food or battery life) and the associated sentiments (positive, negative or neutral). This dataset is designed to advance research in distinguishing ironic expressions, which can significantly affect sentiment analysis and natural language understanding. This dataset includes 3,000 English-language tweets that were labeled manually by three independent annotators. The corresponding labels of the tweets are binary categorized and indicate whether tweet contains irony or not. Table 2: Datasets Sources Cieliebak et al. (2017), Geislinger (2024), and Papers with Code (2024a) Jiang (2020), Papers with Code (2024b), and Socher et al. (2013)"
        },
        {
            "title": "ABSA",
            "content": "Pontiki et al. (2014)"
        },
        {
            "title": "Irony\ndetection",
            "content": "Van Hee et al. (2018) negative). This baseline serves as benchmark for subsequent advanced prompting techniques, such as few-shot learning, CoT, and self-consistency. These advanced approaches are designed to improve the models ability to handle complex tasks like irony detection and ABSA. Performance is assessed using classification metrics, including accuracy, precision, recall, and F1-score, with particular attention to class-specific performance (e.g., positive vs. negative sentiment). Following result documentation, detailed analysis is conducted to identify systematic misclassifications and assess potential weaknesses in different prompting techniques. This provides deeper insights into model limitations and opportunities for further optimization. This rigorous documentation minimizes methodological biases and supports the validation and extension of findings. 5. Results Based on this factual presentation, the following analysis aims to uncover the specific limitations of each approach and to identify recurring patterns in misclassifications. The analysis investigates whether certain sentiment categories or linguistic nuances systematically pose greater classification challenges, thereby identifying key areas for potential optimization in future work. An overview of the key results is presented in Table 3. comprehensive and objective summary of all results is provided in Appendix C. Furthermore, to support interpretation and enhance understanding, selected confusion matrices are provided in Appendix and are referenced in the subsequent analysis. 5.1. Shot prompting Building upon the baseline prompt, more advanced prompts were designed and iteratively refined using the prompt engineering techniques outlined in Chapter 3. The effectiveness of each prompting strategy is then systematically evaluated using standard classification metrics, including accuracy, precision, recall, and F1-score. These metrics not only provide an overall performance measure but also allow for direct comparisons between the different prompt types. Furthermore, to enable more granular analysis, the metrics are computed separately for individual sentiment classes (e.g., positive, negative), thereby offering deeper insights into the models ability to distinguish between varying sentiment expressions. In the binary classification setting, the selection of the example in the one-shot approach appears to have only moderate impact on overall performance, suggesting that the specific choice of the exemplar does not significantly affect classification outcomes. In the context of binary classification, one might expect that performance improvements would primarily occur within the class represented by the selected example. However, an examination of the confusion matrix for irony detection under the gemini-1.5-flash model (see Figure D.1) reveals that, despite the example being drawn from the ironic class, the model also demonstrates improved recognition of the non-ironic (negative) class."
        },
        {
            "title": "GPT\nGemini",
            "content": "Sentiment classification (H1) SB10k SST2 few-shot 0.72 (+14 %) few-shot 0.93 (+2 %) few-shot 0.61 (+15 %) CoT / self-consistency 0.83 (+2,5 %) CoT 0.95 (+12 %) ABSA (H2) SemEval-2014 few-shot 0.85 (+2,4 %) Irony detection (H3) SemEval-2018 few-shot 0.76 (+4 %) CoT 0.6 (+46 %) Table 3: Overview of the best results (weighted F1) compared to the baseline approach Furthermore, in multi-class classification tasks, such as those in the SB10k dataset or ABSA, the selection of the example has more pronounced effect on model performance. During the baseline evaluation, it was already observed that both models struggled to classify neutral sentiments. LLMs tend to assign certain polarity to generated responses, making neutral sentiment classification particularly challenging. To counteract this tendency, the one-shot prompt for these datasets included an example from the neutral class. closer examination of the results reveals that the inclusion of this single neutral exemplar encouraged the models to classify instances as neutral more frequently, thereby mitigating the initial bias toward polarized sentiment predictions. Using this approach under gemini-1.5-flash, the recall for the neutral class in the SB10k dataset increased from 0.37 to 0.51, demonstrating statistically significant improvement in the models ability to accurately identify neutral sentiments (see Figure D.2). In the few-shot approach employed in this experiment, two examples per class were provided for each specific subtask. However, additional examples were included for the neutral class to encourage the selection of neutral sentiments. As shown in Table 3, this strategy led to the best performance across most subtasks compared to all other prompting approaches. In particular, for the SB10k dataset, GPT-4o-mini exhibited significant improvements of approximately 10 percentage points in both accuracy and weighted F1-score compared to the baseline approach. Notably, this improved performance was not solely driven by better classification of the neutral class, as observed in the one-shot approach. Instead, the examples had clear impact on all three sentiment classes, resulting in enhanced F1-scores across the board. However, the positive effect of multiple examples appears to be more pronounced for GPT than for gemini-1.5-flash. While both models benefited from few-shot prompting compared to one-shot prompting, GPT-4o-mini exhibited substantially stronger performance gain. For example, in the SB10k dataset, GPTs F1-score increased from 0.68 to 0.72, whereas gemini-1.5-flash showed only marginal improvement of 0.01 percentage points, effectively making its performance indistinguishable from the one-shot approach. Regarding statistical significance, the majority of performance improvements observed under the few-shot approach are statistically significant. Notable exceptions include the ABSA task when using gemini1.5-flash and the irony detection task when using GPT-4o-mini, where the observed improvements did not reach statistical significance. 5.2. Chain-of-thought Based on the current state of research, the CoT approach was initially anticipated to achieve the highest performance. However, under GPT-4o-mini, the results indicate that both accuracy and F1-score slightly decline starting from the CoT approach compared to the baseline, with the observed differences failing to reach statistical significance. An advantage of the CoT approach in this context is that it allows greater transparency in the decision-making process of the model, as its reasoning steps can be explicitly traced. The following example highlights misclassification, thereby revealing possible shortcoming in the CoT-based reasoning process (GPT-4o-mini, irony detection): Tweet: Jusuf Nurkic #Bulls RT @JonathanTjarks: He clearly has lot of talent. Kind of amazing that Denver got him and Gary Harris for McDermott. (Gold label: no irony) CoT Explanation: [Step(explanation=The text discusses trade in which Denver acquired Jusuf Nurkic and Gary Harris in exchange for McDermott. The phrase kind of amazing could imply that the speaker finds the trade to be surprisingly beneficial for Denver, suggesting that they got lot of talent in return for McDermott. This could be seen as ironic if the speaker believes that Denver got much better deal than expected, highlighting disparity between the perceived value of the players involved in the trade., output=Yes)] This example illustrates that while the CoT approach produces plausible line of reasoning that justifies the presence of irony, it ultimately leads to an incorrect conclusion. In contrast, the baseline approach correctly classified the text as no irony. Under gemini-1.5-flash, however, it can be concluded that the CoT approach demonstrates the best results in comparison. In the SST2 dataset, this approach achieves very high accuracy and F1-score of 0.95, meaning that nearly all sentiments in the dataset are correctly classified. In addition to this dataset, clear performance improvements are also observed in irony detection under gemini-1.5-flash, with statistically significant gains. This is particularly noteworthy in this subdomain, as gemini1.5-flash has generally exhibited poor performance in irony detection prior to this approach. Upon closer examination of the recall within individual classes or by analyzing the confusion matrix (see Figure D.3), it becomes apparent that the performance for the negative class, i.e., no irony, plays critical role. While in the zero-shot approach (baseline), almost all texts are classified as ironic, the CoT approach results in stronger classification of no irony. This is reflected in low recall for the negative class of 0.06 in baseline, while the recall can be improved to 0.38 in the CoT approach. 7 The zero-shot-CoT approach frequently produces results in sentiment classification that fall below those of the baseline approach when evaluated using weighted F1 or Accuracy. 5.3. Self-consistency In contrast to previous discussed results, the self-consistency approach forms an exception. For GPT-4o-mini and the SST2 dataset, it is the only approach that exhibits decreased performance compared to the baseline approach. Figure D.4 comparatively juxtaposes the classification matrix from the best approach (few-shot) with the worst approach (self-consistency) in GPT-4o-mini. In this context, it is noteworthy that the proportion of falsenegative classifications in the model is particularly pronounced. This results in low precision in the negative class. It is also interesting to observe that the model appears confident in these misclassifications. The self-consistency approach, therefore, frequently provides an incorrect outcome for the same classification task, despite the majority decision (statistical mode). An interesting aspect of the self-consistency approach is the interaction between the mentioned parameter, which refers to the iteration frequency for query, and the general \"temperature\" parameter of LLMs (see Chapter 4.2.1). In the course of the experiment, lower parameter setting of 0.2 was chosen to naturally produce more consistent model results. Additionally, the iteration frequency was set to 3. Furthermore, if higher consistency can be achieved through the prompting approach, it is advisable to increase the temperature. This can enhance the models response variability, which positively influences the underlying CoT structure as the reasoning steps become more variable, potentially leading the model to different and better results. However, this idea is accompanied by the greatest disadvantage of the self-consistency approach. Due to the n-fold iteration per query, costs increase both in monetary and temporal terms. 6. Synthesis of findings and answering the research question This chapter consolidates the central findings of the study and synthesizes them in light of the overarching research question: To what extent can advanced prompt engineering techniques enhance the performance of LLMs on diverse sentiment analysis tasks, including binary and multi-class sentiment classification, ABSA, and irony detection? Guided by three hypotheses (H1H3), the study systematically examined the performance of two state-of-the-art LLMsOpenAIs GPT4o-mini and Googles gemini-1.5-flashacross variety of prompting strategies and evaluation metrics. The results present nuanced but coherent narrative about the capabilities and limitations of prompt-based approaches in affective computing. 6.1. H1: Prompt engineering boosts LLM accuracy in sentiment classification H1 is strongly supported by the experimental evidence. Across both binary (SST-2) and ternary (SB10k) classification tasks, most advanced prompting techniques led to significant improvements over the zero-shot baseline. Few-shot prompting, in particular, emerged as the most reliable strategy, substantially improving both accuracy and weighted F1-score, especially under GPT-4o-mini. This effect was especially pronounced for the neutral sentiment class in the SB10k dataset, where recall under gemini-1.5-flash improved from 0.37 to 0.51 after introducing class-balanced examples in one-shot and fewshot settings. The findings highlight that exemplar-rich prompting not only increases overall performance but also helps mitigate well-documented biases in LLMs toward polar sentiments. Importantly, while GPT-4o-mini showed the greatest relative improvement across prompting types, gemini-1.5-flashs performance gains were more constrained, indicating difference in how model architectures respond to prompt conditioning. 6.2. H2: Targeted prompts enhance aspect-based sentiment detection in LLMs H2 received only partial empirical support. Although prompting techniques generally improved results in ABSA tasks derived from the SemEval 2014 dataset, the magnitude of these gains was consistently lower compared to standard sentiment classification. This suggests that ABSA, as more semantically complex and fine-grained task, requires more than simple prompt design to resolve sentiment toward specific aspects within contextually rich text. Notably, zero-shot-CoT and selfconsistency approaches underperformed or failed to yield statistically significant improvements in this domain. These findings point to the limitations of universal prompting strategies and the need for more specialized configurations when dealing with multi-level sentiment structures. The moderate gains observed from few-shot approaches suggest that even small-scale example-based conditioning can help LLMs better handle aspectual cues, but further precision is likely required for consistent success. 6.3. H3: Advanced prompt techniques sharpen LLM detection of ironic tweets H3 produced mixed results, with clear divergences across model architectures and prompting techniques. Under GPT-4omini, while few-shot prompting provided modest gains, both CoT and self-consistency approaches led to performance degradation. CoT-generated reasoning chains, although coherent in surface structure, frequently resulted in misclassifications, particularly when applied to ambiguous or sarcastic inputs. The reasoning steps appeared plausible yet systematically incorrecthighlighting fundamental limitation in equating reasoning transparency with classification correctness. In stark contrast, gemini-1.5-flash exhibited substantial improvements in irony detection when CoT prompting was applied, achieving remarkable 46% increase in weighted F1-score compared to the baseline. This gain was especially evident in recall performance for the no irony class, which had previously suffered under baseline and zero-shot settings. These findings suggest that the capacity to benefit from structured reasoning is tightly coupled to the models internal architecture and its ability to handle subtle pragmatic cues. 8 6.4. Synthesis of findings set of cross-cutting insights emerges from the synthesis of these results. First, few-shot prompting consistently outperforms other approaches in terms of robustness, transferability, and ease of integration across tasks and models. Its effectiveness appears to lie in its ability to provide contextual anchors that guide the models internal representations during inference. Given that few-shot examples function as implicit task conditioning, these results suggest that LLMs rely heavily on exemplar-driven representation shaping, which supports recent work showing that few-shot learning operates as form of incontext gradient descent inside transformer layers (Huang et al., 2025). Second, reasoning-based approaches like CoT and selfconsistency offer mixed utility, often depending on the alignment between prompt structure and model behavior. While they have the potential to elicit richer outputs, they may also reinforce erroneous logic paths when initial assumptions are flawed. This was particularly evident in the self-consistency results under GPT-4o-mini, where multiple sampling iterations with majority voting consistently converged on confidently incorrect predictions, especially in the SST2 dataset. This highlights that reasoning chains alone do not guarantee improved classification performance; instead, their utility depends on whether the model possesses sufficiently calibrated internal heuristics to translate step-by-step reasoning into correct decision boundaries. Lastly, zero-shot-CoT approaches underperformed across nearly all tasks, despite their conceptual appeal. The systematic errors observed across zero-shot-CoT prompts further indicate that unguided reasoning amplifies model hallucination tendencies, illustrating that LLMs require externally provided semantic anchors rather than unconstrained analytical autonomy. The absence of grounding exemplars led to interpretive errors, particularly in nuanced domains like irony detection and ABSA, indicating that reasoning in vacuum often lacks the necessary semantic context for accurate classification. Taken together, the findings underscore that while advanced prompt engineering techniques offer substantial potential to improve LLM performance in sentiment analysis, their success is neither universal nor guaranteed. The effectiveness of given strategy is highly contingent upon the interplay between prompt structure, task complexity, and model architecture. In particular, reasoning transparencywhile intuitively desirabledoes not always correlate with reasoning correctness, distinction that becomes critical in sensitive applications such as irony detection, where the surface plausibility of response may mask systematic misclassification. In conclusion, the study contributes both theoretical and practical insights into how LLMs can be strategically guided through prompt engineering to perform better on affective computing tasks. The evidence presented advocates for modelaware and task-specific approach to prompt design, in which strategies like few-shot prompting serve as reliable foundation, while more complex techniques like CoT and selfconsistency require cautious implementation and empirical validation. As LLMs continue to be deployed in real-world applicationsranging from customer sentiment monitoring to educational platformsunderstanding these dynamics is crucial 9 for ensuring both the efficacy and reliability of AI-driven sentiment analysis systems. Taken together, these insights suggest that prompt engineering should be conceptualized not as uniform technique but as context-dependent optimization problem shaped by semantic ambiguity, model scale, and architectural reasoning depth. 7. Conclusion and future work 7.1. Key insights This study provides systematic empirical evidence that advanced prompt engineering techniques can significantly enhance the performance of LLMs across diverse sentiment analysis tasks. While few-shot prompting consistently yielded robust improvements in accuracy and F1-scoreparticularly by reducing sentiment polarity bias and improving class-level balancereasoning-based strategies such as CoT and selfconsistency exhibited notable limitations. Their effectiveness was found to be highly model-dependent, with GPT-4o-mini often producing plausible but incorrect inferences, while gemini1.5-flash benefited substantially from CoT in complex tasks such as irony detection. The partial confirmation of improvements in ABSA further underscores the need for task-specific prompt calibration. Across all evaluations, the divergence in performance between GPT-4o-mini and gemini-1.5-flash reaffirms that prompting strategies must be aligned with both model architecture and task complexity. This architectural sensitivity underscores that prompting strategies cannot be transferred across models without validation, as each system exhibits distinct inductive biases and differing capacities for pragmatic inference. By uncovering the mechanisms through which LLMs interpret prompts, structure reasoning, and respond to contextual exemplars, this study contributes foundational insights to the design of more reliable, transparent, and context-aware affective computing systems. As LLMs become increasingly embedded in high-stakes domains, principled prompt design will remain essential for guiding their behavior and ensuring trustworthy deployment. 7.2. Limitations This study provides empirical evidence on the efficacy of prompt engineering techniques in enhancing LLM performance on sentiment analysis and irony detection tasks. However, several limitations must be noted. First, the findings are modelspecific, based solely on GPT-4o-mini and gemini-1.5-flash, limiting generalizability across architectures. Additionally, because both models are proprietary systems with opaque training corpora, it remains unclear which linguistic patterns or domain distributions may have biased their responsesan uncertainty that limits deeper causal interpretation of the results. Second, dataset sizes were downsampled to 1,000 instances per task for efficiency, which may reduce statistical power and sensitivity to rare classes. Third, prompt strategies were manually designed without adaptive tuning, potentially constraining their effectiveness. systematic prompt ablation study could have revealed which elements of the prompt contribute most to performance gains, providing more fine-grained understanding of promptmodel interactions. Furthermore, decoding parameters (e.g., temperature, sample size) were fixed, precluding exploration of parameterprompt interactions. The study also lacks linguistic error analysis and does not control for multiple hypothesis testing, which may overstate significance. Furthermore, the absence of detailed linguistic error analysis limits insight into which syntactic or semantic constructions systematically trigger misclassification, particularly in irony detection where pragmatic subtlety is crucial. Together, these factors highlight the need for broader model evaluation, automatic prompt optimization, and deeper analysis of underlying linguistic mechanisms. 7.3. Directions for further research Recent advancements in LLMs have demonstrated significant potential for range of NLP tasks, including sentiment analysis. The flexibility of prompt-based learning enables models to be rapidly adapted to new tasks without extensive finetuning, making prompt engineering critical aspect of leveraging LLM capabilities. However, the effectiveness of different prompting strategies remains incompletely understood, particularly in the context of nuanced sentiment analysis scenarios. Despite promising initial results, several open questions remain regarding the alignment between specific prompting techniques and task requirements, the generalizability of findings across model architectures, and the impact of prompt design on output reliability. Addressing these questions can inform best practices for deploying LLMs in real-world sentiment analysis applications and guide future methodological and theoretical advancements in the field. Accordingly, several key research avenues emerge: Technique efficacy across sentiment analysis tasks: Future research could systematically evaluate whether specific prompting strategies exhibit differential efficacy across various sentiment analysis tasks. For instance, it would be valuable to investigate if few-shot prompting achieves the highest performance improvements for multi-class sentiment classification, whereas reasoning-based prompts (e.g., Chain-of-Thought prompting) yield greater benefits for more nuanced tasks such as sarcasm or irony detection. Cross-model comparisons: An important avenue for future work is to examine how the effectiveness of prompting techniques varies across different LLM architectures. Specifically, research could explore whether optimal prompting strategies differ between different LLMs, potentially due to distinctions in their pre-training datasets, architectures, or reasoning capabilities. more in-depth analysis that includes models of different sizes and architectures could provide valuable insights into the extent to which the effectiveness of prompt engineering techniques is model-dependent. Another promising direction is the exploration of retrieval-augmented prompting, where models receive contextual evidence before classification, potentially reducing hallucinations and improving robustness in nuanced sentiment tasks. Impact on robustness and consistency: Further investigation is needed regarding the extent to which prompt engineering influences the robustness and consistency of model outputs. This includes assessing whether aggregation-based methodssuch as self-consistency, which combines outputs from multiple reasoning trajectorieslead to more stable sentiment predictions (i.e., reduced prediction variability and error rates) relative to single-pass prompting approaches. In summary, systematically addressing these research avenues will contribute to deeper understanding of prompt engineering for sentiment analysis and help unlock the full potential of LLMs in this domain."
        },
        {
            "title": "CRediT authorship contribution statement",
            "content": "Marvin Schmitt: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Writing - Original Draft, Writing - Review & Editing, Visualization. Anne Schwerk: Conceptualization, Writing - Original Draft, Writing - Review & Editing, Supervision. Sebastian Lempert: Conceptualization, Writing - Original Draft, Writing - Review & Editing, Supervision. Declaration of generative AI and AI-assisted technologies in the manuscript preparation process During the preparation of this work, the authors utilized DeepL and GPT-4 to enhance the quality of the English language, since none of the authors are native English speakers. Furthermore, the Stanford Agentic Reviewer was used to obtain feedback on the clarity, methodology, and overall presentation of this work. The tool was used to identify potential areas for improvement and to refine the manuscript prior to submission. All revisions were carefully reviewed and approved by the authors to ensure accuracy and scholarly integrity. After using these tools, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article. No generative AI tools were used to write substantive sections of the manuscript itself."
        },
        {
            "title": "Declaration of competing interest",
            "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."
        },
        {
            "title": "Code",
            "content": "is available in this GitHub-Repository: https://github.com/Marvin2108/ESCID-LLM-APET"
        },
        {
            "title": "Data availability",
            "content": "Data will be made available on request."
        },
        {
            "title": "This work was supported by the IU International University",
            "content": "of Applied Sciences. 11 Appendix A. All Prompts A.1. One shot prompting"
        },
        {
            "title": "Task",
            "content": "SC"
        },
        {
            "title": "Prompt",
            "content": "System: You classify sentiments of text. Here is an example: Input: Classify the following text into one of these two sentiments [negative, positive]: Text: enriched by an imaginatively mixed cast of antic spirits. Output: 1. User: Classify the following text into one of these two sentiments [negative, positive]. Text: {text} System: You classify the sentiment of specific aspect within text. Input: Classify the sentiment for the following aspect in the following text into one of these three sentiments [negative, positive, neutral]. Aspect: cord, Text: charge it at night and skip taking the cord with me because of the good battery life.. Output: 2. User: Classify the sentiment for the following aspect in the following text into one of these three sentiments [negative, positive, neutral]. Aspect: {aspect} Text: {text} Irony-detection System: You determine whether text contains ironic or sarcastic elements. Input: Does the following text contain ironic or sarcastic elements? Text: \"@BaniHillal feel bad for the American people, now we should invade their country and kill their children so we can save them\". Output: 1. User: Does the following text contain ironic or sarcastic elements? Text: {text} Table A.1: One shot prompting"
        },
        {
            "title": "Prompt",
            "content": "A.2. Few shot prompting"
        },
        {
            "title": "Task",
            "content": "SC"
        },
        {
            "title": "ABSA",
            "content": "System: You classify sentiments of text. Here are some examples: [...4 Exemplars (2 per class)...] User: Classify the following text into one of these two sentiments [negative, positive]. Text: {text} System: You classify the sentiment of specific aspect within text. Here are some examples: [...2 negative, 2 positive, 3 neutral...] User: Classify the sentiment for the following aspect in the following text into one of these three sentiments [negative, positive, neutral]. Aspect: aspect Text: {text} Irony-detection System: You determine whether text contains ironic or sarcastic elements. Here are some examples: [...4 Exemplars (2 per class)...] User: Does the following text contain ironic or sarcastic elements? Text: {text} Table A.2: Few shot prompting 12 A.3. CoT prompting"
        },
        {
            "title": "Task",
            "content": "SC"
        },
        {
            "title": "Prompt",
            "content": "System: You classify the sentiment of text. Example: [...1 Exemplar with CoT for neutral class...] Output: neutral. User: Classify the following text in one of these three sentiments [negative, positive, neutral]: Text: {text} System: You classify the sentiment of specific aspect within text. Example: [...1 Exemplar with CoT for neutral class...] User: Classify the sentiment for the following aspect in the following text into one of these three sentiments [negative, positive, neutral]. Aspect: aspect Text: {text} Irony-detection System: You determine whether text contains ironic or sarcastic elements. Example: [...1 Exemplar with CoT for positive class...] User: Does the following text contain ironic or sarcastic elements? Text: {text} Table A.3: CoT prompting"
        },
        {
            "title": "Prompt",
            "content": "A.4. Zero-shot CoT prompting"
        },
        {
            "title": "Task",
            "content": "SC"
        },
        {
            "title": "ABSA",
            "content": "System: You classify sentiments of text. Analyze the text step-by-step to determine whether it expresses positive or negative sentiment. Explain each step in detail before providing your final answer. User: Classify the following text into one of these two sentiments [negative, positive]. Text: {text} System: You classify the sentiment of specific aspect within text. Reason step-by-step by breaking down the analysis for the specified aspect. Explain each step in detail before providing your final answer. User: Classify the sentiment for the following aspect in the following text into one of these three sentiments [negative, positive, neutral]. Aspect: aspect Text: {text} Irony-detection System: You determine whether text contains ironic or sarcastic elements. Analyze the text carefully and explain your reasoning step-by-step before providing the final answer. User: Does the following text contain ironic or sarcastic elements? Text: {text} Table A.4: Zero-shot CoT prompting A.5. Self consistency prompting"
        },
        {
            "title": "Task",
            "content": "SC Application of the CoT prompt with multiple sampling rounds followed by majority voting scheme."
        },
        {
            "title": "ABSA",
            "content": "Application of the CoT prompt with multiple sampling rounds followed by majority voting scheme. Irony-detection Application of the CoT prompt with multiple sampling rounds followed by majority voting scheme. Table A.5: Self consistency prompting 13 Appendix B. Statistical testing (Bootstrap) This section presents the results of the statistical significance analysis based on bootstrap resampling. Instead of relying on p-values, statistical significance is assessed via 95 % bootstrap confidence interval of the F1-score difference. If the interval does not contain zero, the difference is considered statistically significant. SC (SST2) SC (SB10k) LLM GPT Gemini GPT Gemini PE-Approach One-Shot Few-Shot CoT 95% confidence interval of the F1 difference [-0.0288, -0.0004] [0.0673, 0.1195] [-0.0197, 0.0134] Zero-Shot CoT [-0.0055, 0.0233] Significance base PE No difference No difference Self-Consistency [-0.1053, -0.0597] base 95% confidence interval of the F1 difference [0.0389, 0.0751] [0.0549, 0.0931] [0.0397, 0.0801] [0.0002, 0.0462] [0.0278, 0.0698] Significance PE PE PE PE PE 95% confidence interval of the F1 difference [0.0288, 0.0684] [0.0673, 0.1195] Significance PE PE 95% confidence interval of the F1 difference [0.0540, 0.1004] [0.0549, 0.1005] [-0.0267, 0.0239] No difference [-0.0082, 0.0483] [-0.1895, -0.121] base [-0.0403, 0.0145] Significance PE PE No difference No difference [-0.0114, 0.0373] No difference [0.0201, 0.0736] PE Table B.1: Bootstrap sentiment classification (base = baseline approach delivers significantly better results, PE = prompt approach delivers significantly better results, No difference = no statistically significant difference was found between the results) ABSA (SemEval-2014) Irony-detection (SemEval-2018) LLM GPT Gemini GPT Gemini PE-Approach One-Shot Few-Shot CoT 95% confidence interval of the F1 difference [0.0003, 0.0267] [0.0053, 0.0364] Significance PE PE 95% confidence interval of the F1 difference [-0.0143, 0.0169] [-0.0043, 0.0335] [-0.0220, 0.0111] No difference [-0.0055, 0.0352] Significance No difference No difference No difference 95% confidence interval of the F1 difference [-0.0331, 0.0082] [-0.0024, 0.0404] [-0.1091, -0.0531] Zero-Shot CoT [-0.0460, -0.0051] base [-0.0422, -0.0025] base [-0.1504, -0.0868] Self-Consistency [-0.0254, 0.0066] No difference [-0.0039, 0.0373] No difference [-0.2014, -0.1265] Significance No difference No difference base base base 95% confidence interval of the F1 difference [0.1318, 0.1931] [0.1248, 0.1877] [0.1578, 0.2282] [0.0155, 0.0599] [0.0317, 0.0799] Significance PE PE PE PE PE Table B.2: Bootstrap ABSA & Irony (base = baseline approach delivers significantly better results, PE = prompt approach delivers significantly better results, No difference = no statistically significant difference was found between the results) Appendix C. All results C.1. Results sentiment-classification PE Approach Metric Negative Positive Total Negative Positive Total Negative Positive Neutral Total Negative Positive Neutral Total SST2 SB10k GPT GEMINI GPT GEMINI"
        },
        {
            "title": "BASE",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "One Shot",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "Few Shot",
            "content": "F1 (macro) Weighted Avg F1 Accuracy Precision Recall CoT F1 (macro) Weighted Avg F1 Accuracy Precision Recall ZeroShot-CoT F1 (macro) Weighted Avg F1 Accuracy Precision Recall Self-Consistency F1 (macro) Weighted Avg F1 Accuracy 0,85 0,97 0, 0,84 0,99 0,91 0,88 0,98 0, 0,87 0,99 0,93 0,87 0,97 0, 0,76 0,99 0,86 0,75 0,99 0, 0,81 0,99 0,89 0,84 0,99 0, 0,94 0,97 0,95 0,81 0,96 0, 0,82 0,97 0,89 0,97 0,86 0, 0,99 0,84 0,91 0,98 0,89 0, 0,99 0,87 0,92 0,97 0,88 0, 0,99 0,73 0,84 0,91 0,92 0, 0,91 0,91 0,92 0,92 0,91 0, 0,91 0,93 0,94 0,93 0,93 0, 0,93 0,93 0,93 0,92 0,92 0, 0,93 0,92 0,92 0,92 0,88 0, 0,85 0,85 0,85 0,41 0,87 0, 0,45 0,88 0,6 0,52 0,82 0, 0,42 0,87 0,56 0,37 0,91 0, 0,42 0,89 0,57 0,99 0,72 0, 0,99 0,81 0,89 0,99 0,84 0, 0,97 0,94 0,96 0,96 0,8 0, 0,97 0,82 0,89 0,87 0,86 0, 0,85 0,85 0,90 0,90 0,89 0, 0,89 0,92 0,92 0,91 0,91 0, 0,96 0,96 0,96 0,95 0,95 0, 0,88 0,88 0,87 0,87 0,90 0, 0,89 0,89 0,89 0,58 0,72 0, 0,62 0,7 0,66 0,71 0,6 0, 0,56 0,78 0,65 0,46 0,78 0, 0,57 0,76 0,65 0,33 0,94 0, 0,36 0,88 0,52 0,39 0,89 0, 0,38 0,91 0,53 0,32 0,93 0, 0,39 0,93 0,55 0,84 0,52 0, 0,86 0,6 0,7 0,81 0,74 0, 0,88 0,49 0,63 0,83 0,28 0, 0,57 0,65 0,65 0,61 0,70 0, 0,63 0,62 0,64 0,73 0,65 0, 0,67 0,68 0,72 0,69 0,72 0, 0,62 0,71 0,61 0,63 0,62 0, 0,66 0,51 0,47 0,5 0,52 0, 0,62 0,64 0,63 0,55 0,65 0, 0,61 0,62 0,62 0,58 0,66 0, 0,51 0,79 0,62 0,52 0,6 0, 0,53 0,79 0,63 0,83 0,37 0, 0,81 0,51 0,62 0,82 0,5 0, 0,9 0,36 0,52 0,8 0,36 0, 0,9 0,41 0,56 0,57 0,65 0, 0,53 0,52 0,59 0,67 0,59 0, 0,59 0,60 0,68 0,59 0,61 0, 0,60 0,69 0,56 0,54 0,55 0, 0,63 0,51 0,51 0,51 0,61 0, 0,58 0,58 0,58 Table C.1: All results sentiment-classification 15 C.2. Results aspect-based sentiment analysis"
        },
        {
            "title": "Positive Neutral Total",
            "content": "SemEval-"
        },
        {
            "title": "BASE",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "One Shot",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "Few Shot",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "CoT",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "Recall",
            "content": "ZeroShot-CoT F1 (macro) Weighted Avg F"
        },
        {
            "title": "Recall",
            "content": "Self-Consistency F1 (macro) Weighted Avg F"
        },
        {
            "title": "Accuracy",
            "content": "0,84 0,91 0,87 0,84 0,9 0, 0,88 0,88 0,88 0,83 0,92 0, 0,81 0,93 0,87 0,83 0,91 0, 0,87 0,92 0,89 0,88 0,92 0, 0,89 0,91 0,9 0,9 0,9 0, 0,85 0,94 0,9 0,88 0,89 0, 0,79 0,95 0,86 0,79 0,94 0, 0,83 0,94 0,88 0,87 0,88 0, 0,83 0,92 0,87 0,87 0,89 0, 0,7 0,46 0,56 0,72 0,53 0, 0,66 0,63 0,65 0,63 0,49 0, 0,69 0,29 0,4 0,62 0,48 0, 0,80 0,76 0,77 0,83 0,84 0, 0,78 0,79 0,84 0,85 0,81 0, 0,81 0,85 0,85 0,79 0,77 0, 0,83 0,83 0,78 0,72 0,72 0, 0,82 0,78 0,76 0,76 0,82 0, Table C.2: All results ABSA 16 0,88 0,92 0,9 0, 0,91 0,89 0,87 0,93 0,9 0, 0,9 0,9 0,83 0,92 0,87 0, 0,9 0,89 0,75 0,34 0,46 0, 0,37 0,49 0,76 0,41 0,53 0, 0,56 0,57 0,59 0,29 0,39 0, 0,55 0,57 0,81 0,63 0,68 0, 0,83 0,80 0,74 0,75 0,81 0, 0,82 0,76 0,77 0,82 0,84 0, 0,78 0,78 0,83 0,83 0,75 0, 0,71 0,79 0,81 0,78 0,78 0, 0,83 0,83 C.3. Results irony-detection"
        },
        {
            "title": "Positive Total",
            "content": "SemEval-"
        },
        {
            "title": "BASE",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "One Shot",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "Few Shot",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "CoT",
            "content": "F1 (macro) Weighted Avg F"
        },
        {
            "title": "Recall",
            "content": "ZeroShot-CoT F1 (macro) Weighted Avg F"
        },
        {
            "title": "Recall",
            "content": "Self-Consistency F1 (macro) Weighted Avg F"
        },
        {
            "title": "Accuracy",
            "content": "0,76 0,67 0,71 0,78 0,62 0, 0,76 0,73 0,74 0,77 0,47 0, 0,77 0,38 0,51 0,72 0,31 0, 0,84 0,06 0,12 0,81 0,28 0, 0,78 0,28 0,41 0,75 0,37 0, 0,79 0,11 0,19 0,77 0,13 0, 0,72 0,8 0,75 0,7 0,83 0, 0,75 0,78 0,77 0,63 0,87 0, 0,6 0,89 0,72 0,56 0,88 0, 0,74 0,74 0,73 0,73 0,74 0, 0,73 0,73 0,72 0,73 0,76 0, 0,76 0,76 0,76 0,70 0,67 0, 0,66 0,67 0,69 0,64 0,62 0, 0,64 0,64 0,60 0,56 0,56 0, Table C.3: All results irony-detection 17 0,52 0,99 0,69 0, 0,94 0,71 0,57 0,93 0,71 0, 0,88 0,71 0,53 0,97 0,69 0, 0,96 0,69 0,68 0,53 0,41 0, 0,54 0,70 0,61 0,57 0,57 0, 0,68 0,61 0,56 0,56 0,61 0, 0,63 0,60 0,60 0,63 0,66 0, 0,44 0,44 0,55 0,66 0,55 0, 0,44 0,55 Appendix D. Confusion matrices D.1. One-shot prompting (a) Baseline prompting (b) One-shot prompting Figure D.1: Confusion matrix baseline vs. one-shot irony detection (gemini-flash1.5). In Figure D.1, the confusion matrix for the irony detection task under the gemini-1.5-flash model using one-shot prompting is presented. The results indicate that even when the positive class (i.e., irony) is used as the single example, the models performance in detecting the negative class (i.e., no irony) improves. (a) Baseline prompting (b) One-shot prompting Figure D.2: Confusion matrix baseline vs. one-shot SB10k (gemini-flash-1.5). In Figure D.2, it can be observed that the performance for the neutral class improves significantly when single example of that class is provided using one-shot prompting. 18 D.2. CoT prompting (a) Baseline prompting (b) CoT prompting Figure D.3: Confusion matrix baseline vs. CoT irony detection (gemini-flash-1.5) Figure D.3 shows that the recall for the negative class (no irony) is extremely low (0.06) in the zero-shot baseline, as most texts are falsely classified as ironic. In contrast, the CoT approach improves the recall for this class to 0.38, indicating better distinction between ironic and non-ironic content. D.3. Self-consistency prompting (a) Few-shot prompting (b) Self-consistency prompting Figure D.4: Confusion matrix few-shot vs. self-consistency SST2 (GPT-4o-mini). Figure D.4 compares the confusion matrices of the best-performing (few-shot) and worst-performing (self-consistency) GPT-4omini approaches. The self-consistency method shows high number of false negatives, leading to low precision for the negative class. Interestingly, the model consistently misclassifies with apparent confidence, despite majority voting."
        },
        {
            "title": "References",
            "content": "Amin, M. M., & Schuller, B. W. (2024). On Prompt Sensitivity of ChatGPT in Affective Computing [arXiv preprint arXiv:2403.14006v1]. https://doi.org/10.48550/arXiv. 2403.14006 Caelen, O., & Blete, M.-A. (2024). Developing apps with GPT4 and ChatGPT: Build Intelligent Chatbots, Content Generators, and More (Second edition). OReilly Media, Inc. Chen, B., Zhang, Z., Langren√©, N., & Zhu, S. (2023). Unleashing the potential of prompt engineering in Large Language Models: comprehensive review [arXiv preprint arXiv:2310.14735v5]. https : / / doi . org / 10 . 48550/arXiv.2310.14735 Cieliebak, M., Deriu, J. M., Egger, D., & Uzdilli, F. (2017). Twitter Corpus and Benchmark Resources for German Sentiment Analysis. Proceedings of the 5th International Workshop on Natural Language Processing for Social Media, 4551. https:// doi.org/ 10.18653/ v1 / W17-1106 Filip, T., Pavl√≠Àácek, M., & Sos√≠k, P. (2024). Fine-tuning multilingual language models in Twitter/X sentiment analysis: study on Eastern-European V4 languages [arXiv preprint arXiv:2408.02044v1]. https : / / doi . org / 10 . 48550/arXiv.2408.02044 Geislinger, R. (2024). Hugging Face: Alienmaster/SB10k (Dataset) [Hugging Face]. Retrieved February 10, 2025, from https : / / huggingface . co / datasets / Alienmaster/SB10k Google. (2025a). Gemini models: Gemini 1.5 Flash. Retrieved April 9, 2025, from https : / / ai . google . dev / gemini - api/docs/models%5C#gemini-1.5-flash Google. (2025b, May). Google-genai: GenAI Python SDK. Retrieved May 21, 2025, from https : / / github . com / googleapis/python-genai Huang, J., Wang, Z., & Lee, J. D. (2025). Transformers learn to implement multi-step gradient descent with chain of thought [arXiv preprint arXiv:2502.21212v1]. https : //doi.org/10.48550/arXiv.2502.21212 Jiang, Y. (2020). GitHub: SST-2-sentiment-analysis (Dataset). Retrieved February 10, 2025, from https://github.com/ YJiangcm/SST-2-sentiment-analysis Jim, J. R., Talukder, M. A. R., Malakar, P., Kabir, M. M., Nur, K., & Mridha, M. (2024). Recent advancements and challenges of NLP-based sentiment analysis: stateof-the-art review. Natural Language Processing Journal, 6, 100059. https://doi.org/10.1016/j.nlp.2024. Juro≈°, J., Majer, L., & Snajder, J. (2024). LLMs for Targeted Sentiment in News Headlines: Exploring the Descriptive-Prescriptive Dilemma. Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, 329 343. https://doi.org/10.18653/v1/2024.wassa-1.27 Krugmann, J. O., & Hartmann, J. (2024). Sentiment Analysis in the Age of Generative AI. Customer Needs and So20 lutions, 11(1), 3. https://doi.org/10.1007/s40547-02400143-4 Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2021). Pre-train, Prompt, and Predict: Systematic Survey of Prompting Methods in Natural Language Processing [arXiv preprint arXiv:2107.13586v1]. https://doi.org/10.48550/arXiv. 2107.13586 Luo, Q., Zeng, W., Chen, M., Peng, G., Yuan, X., & Yin, Q. (2023). Self-Attention and Transformers: Driving the Evolution of Large Language Models. 2023 IEEE 6th International Conference on Electronic Information and Communication Technology (ICEICT), 401 405. https : / / doi . org / 10 . 1109 / ICEICT57916 . 2023 . Luo, W., & Gong, D. (2024). Pre-trained Large Language Models for Financial Sentiment Analysis [arXiv preprint arXiv:2401.05215v1]. https://doi.org/10.48550/arXiv. 2401.05215 Marvin, G., Hellen, N., Jjingo, D., & Nakatumba-Nabende, J. (2024). Prompt Engineering in Large Language Models. In I. J. Jacob, S. Piramuthu, & P. Falkowski-Gilski (Eds.), Data Intelligence and Cognitive Informatics (pp. 387402). Springer Nature Singapore. https://doi. org/10.1007/978-981-99-7962-2_30 OpenAI. (2024a). GPT-4o System Card. Retrieved March 27, 2025, from https://openai.com/index/gpt-4o-systemcard/ OpenAI. (2024b, July). GPT-4o mini: Advancing cost-efficient intelligence. Retrieved May 21, 2025, from https : / / openai . com / index / gpt - 4o - mini - advancing - cost - efficient-intelligence/ OpenAI. (2025a). GPT-4o mini. Retrieved April 9, 2025, from https://platform.openai.com OpenAI. (2025b, May). Openai: The official Python library for the openai API. Retrieved May 21, 2025, from https: //github.com/openai/openai-python Papers with Code. (2024a). SB10k Dataset. Retrieved February 10, 2025, from https://paperswithcode.com/dataset/ sb10k Papers with Code. (2024b). SST-2 Dataset. Retrieved February 10, 2025, from https://paperswithcode.com/dataset/ sst-2 Pontiki, M., Galanis, D., Pavlopoulos, J., Papageorgiou, H., Androutsopoulos, I., & Manandhar, S. (2014). SemEval2014 Task 4: Aspect Based Sentiment Analysis. Proceedings of the 8th International Workshop on Semantic Evaluation, 2735. https : / / doi . org / 10 . 3115 / v1 / S14-2004 Sahoo, P., Singh, A. K., Saha, S., Jain, V., Mondal, S., & Chadha, A. (2024). Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications [arxiv preptint arXiv:2402.07927v1]. https://doi.org/10.48550/arXiv. 2402. Schulhoff, S., Ilie, M., Balepur, N., Kahadze, K., Liu, A., Si, C., Li, Y., Gupta, A., Han, H., Schulhoff, S., Dulepet, P. S., Zhang, W., Deng, Y., Liu, B., Pan, S., & Bing, L. (2024). Sentiment Analysis in the Era of Large Language Models: Reality Check. Findings of the Association for Computational Linguistics: NAACL 2024, 38813906. https://doi.org/10.18653/v1/2024.findings-naacl.246 Zhang, Y., Zou, C., Lian, Z., Tiwari, P., & Qin, J. (2024). SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding [arXiv preprint arXiv:2408.11319v2]. https://doi.org/10.48550/arXiv. 2408.11319 Zhou, C., Song, D., Tian, Y., Wu, Z., Wang, H., Zhang, X., Yang, J., Yang, Z., & Zhang, S. (2024, December). Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment Analysis [arXiv preprint arXiv:2412.02279v1]. https://doi.org/10.48550/arXiv. 2412.02279 Vidyadhara, S., Ki, D., Agrawal, S., Pham, C., Kroiz, G., Li, F., Tao, H., Srivastava, A., . . . Resnik, P. (2024). The Prompt Report: Systematic Survey of Prompting Techniques [arXiv preprint arXiv:2406.06608v5]. https://doi.org/10.48550/arXiv.2406.06608 Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., & Potts, C. (2013). Recursive Deep Models for Semantic Compositionality Over Sentiment Treebank. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 1631 1642. https://doi.org/10.18653/v1/D13-1170 Stigall, W., Al Hafiz Khan, M. A., Attota, D., Nweke, F., & Pei, Y. (2024). Large Language Models Performance Comparison of Emotion and Sentiment Classification. Proceedings of the 2024 ACM Southeast Conference, 6068. https://doi.org/10.1145/3603287.3651183 Sun, X., Zhang, K., Liu, Q., Bao, M., & Chen, Y. (2024). Harnessing domain insights: prompt knowledge tuning method for aspect-based sentiment analysis. Knowledge-Based Systems, 298, 111975. https://doi. org/10.1016/j.knosys.2024. Sweta, S. (2024). Sentiment Analysis and its Application in Educational Data Mining. Springer Nature Singapore. https://doi.org/10.1007/978-981-97-2474-1 Van Hee, C., Lefever, E., & Hoste, V. (2018). SemEval-2018 Task 3: Irony Detection in English Tweets. Proceedings of The 12th International Workshop on Semantic Evaluation, 3950. https://doi.org/10.18653/v1/S181005 Wang, L., Bi, W., Zhao, S., Ma, Y., Lv, L., Meng, C., Fu, J., & Lv, H. (2024). Investigating the Impact of Prompt Engineering on the Performance of Large Language Models for Standardizing Obstetric Diagnosis Text: Comparative Study. JMIR Formative Research, 8, e53216. https://doi.org/10.2196/53216 Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., & Zhou, D. (2022). Chain-ofthought prompting elicits reasoning in large language models. Proceedings of the 36th International Conference on Neural Information Processing Systems, 2482424837. White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., & Schmidt, D. C. (2023). Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT [arXiv preprint arXiv:2302.11382v1]. https://doi.org/10.48550/arXiv. 2302.11382 Yang, H., Zhao, Y., Wu, Y., Wang, S., Zheng, T., Zhang, H., Ma, Z., Che, W., & Qin, B. (2024). Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: Survey [arXiv preprint arXiv:2406.08068v2]. https://doi.org/10.48550/arXiv.2406.08068 Yao, B., Zhang, Y., Li, Q., & Qin, J. (2024). Is Sarcasm Detection Step-by-Step Reasoning Process in Large Language Models? [arXiv preprint arXiv:2407.12725v2]. https://doi.org/10.48550/arXiv.2407."
        }
    ],
    "affiliations": [
        "IU International University of Applied Sciences"
    ]
}