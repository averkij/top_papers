{
    "paper_title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench",
    "authors": [
        "Fenfen Lin",
        "Yesheng Liu",
        "Haiyu Xu",
        "Chen Yue",
        "Zheqi He",
        "Mingxuan Zhao",
        "Miguel Hu Chen",
        "Jiakang Liu",
        "JG Yao",
        "Xi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reading measurement instruments is effortless for humans and requires relatively little domain expertise, yet it remains surprisingly challenging for current vision-language models (VLMs) as we find in preliminary evaluation. In this work, we introduce MeasureBench, a benchmark on visual measurement reading covering both real-world and synthesized images of various types of measurements, along with an extensible pipeline for data synthesis. Our pipeline procedurally generates a specified type of gauge with controllable visual appearance, enabling scalable variation in key details such as pointers, scales, fonts, lighting, and clutter. Evaluation on popular proprietary and open-weight VLMs shows that even the strongest frontier VLMs struggle measurement reading in general. A consistent failure mode is indicator localization: models can read digits or labels but misidentify the key positions of pointers or alignments, leading to big numeric errors despite plausible textual reasoning. We have also conducted preliminary experiments with reinforcement learning over synthetic data, and find encouraging results on in-domain synthetic subset but less promising for real-world images. Our analysis highlights a fundamental limitation of current VLMs in fine-grained spatial grounding. We hope this resource can help future advances on visually grounded numeracy and precise spatial perception of VLMs, bridging the gap between recognizing numbers and measuring the world."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 6 8 6 2 . 0 1 5 2 : r Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench BAAI FlagEval Team TL;DR: Fine-grained visual understanding tasks such as visual measurement reading have been surprisingly challenging for frontier general-purpose vision-language models. We introduce MeasureBench, benchmark with diverse images of measuring instruments collected from both real-world images and new data synthesis pipeline."
        },
        {
            "title": "Abstract",
            "content": "Reading measurement instruments is eﬀortless for humans and requires relatively little domain expertise, yet it remains surprisingly challenging for current vision-language models (VLMs) as we ﬁnd in preliminary evaluation. In this work, we introduce MeasureBench, benchmark on visual measurement reading covering both real-world and synthesized images of various types of measurements, along with an extensible pipeline for data synthesis. Our pipeline procedurally generates speciﬁed type of gauge with controllable visual appearance, enabling scalable variation in key details such as pointers, scales, fonts, lighting, and clutter. Evaluation on popular proprietary and open-weight VLMs shows that even the strongest frontier VLMs struggle measurement reading in general. consistent failure mode is indicator localization: models can read digits or labels but misidentify the key positions of pointers or alignments, leading to big numeric errors despite plausible textual reasoning. We have also conducted preliminary experiments with reinforcement learning over synthetic data, and ﬁnd encouraging results on in-domain synthetic subset but less promising for real-world images. Our analysis highlights fundamental limitation of current VLMs in ﬁne-grained spatial grounding. We hope this resource can help future advances on visually grounded numeracy and precise spatial perception of VLMs, bridging the gap between recognizing numbers and measuring the world."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in vision-language models (VLMs) have demonstrated impressive capabilities in tackling complex reasoning tasks that combine textual and visual information. Models or systems such as GPT5 (OpenAI, 2025a) and Gemini 2.5 Pro (Gemini Team, 2025) achieve human-expert level performance on college-level problems in MMMU (Yue et al., 2024) and MMMU-Pro (Yue et al., 2025). Even on Humanitys Last Exam (HLE; Phan et al., 2025), benchmark at the frontier of human knowledge, state-of-the-art models achieve accuracies exceeding 25%, substantially surpassing the human average. Beyond academic-style evaluations, VLMs have also been applied to real-world scenarios such as embodied intelligence and autonomous driving, where success relies more heavily on precise visual perception than on complex logical reasoning. That said, state-of-the-art visionlanguage models (VLMs) still struggle with ﬁne-grained perceptione.g., low-level visual cues, precise geometry, and subtle changeseven when their high-level reasoning appears strong. Existing ﬁne-grained evaluations are well represented by text reading and chart reasoning (Singh et al., 2019; Masry et al., 2022; Tang et al., 2025), or by similarly artiﬁcial low-level vision tests such as BlindTest (Rahmanzadehgervi et al., 2024) and SalBench (Dahou et al., 2025). However, they rarely require mapping physical scales to numeric values. Visual instrument reading tasks usually require ﬁne-grained visual perception, light quantitative reasoning, and basic arithmetic. Examples include reading pressure gauges in industrial settings, and thermometers or even as simple as clocks in daily life. Accurate interpretation of these instruments is crucial for safety, eﬃciency, and decision-making across domains for vision language models or future embodied AI systems. While few exist studies have covered very speciﬁc types of reading such as clocks (Saxena et al., 2025; Yang et al., 2022), rulers (Matuzevičius, 2023; Pan et al., 2025), industrial gauges (Izquierdo-Domenech et al., 2025; Valente et al., 2025), and household meters (Van et al., 2025), they do not span the diversity of instruments or reading designs. Full list of authors attached in the end. Project page: https://flageval-baai.github.io/MeasureBenchPage/ 1 Figure 1: Sampled MeasureBench examples real-world set, these four reading designs are commomly used in various measuring instruments. To ﬁll this gap, we introduce MeasureBench, benchmark for evaluating VLMs on measuring instrument reading across 26 instrument types and four types of readout designs. Each image is paired with reading question. MeasureBench comprises 2,442 imagequestion pairs: 1,272 diverse real-world images collected and human-annotated, and 1,170 synthetic images generated with randomized readings for 39 instruments. Our data synthesis pipeline has two complementary backends: (i) 2D programmatic renderer for diverse layouts with full control over fonts and geometry; and (ii) 3D Blender renderer for photorealistic scenes with realistic lighting, materials, reﬂections, and occlusions. The pipeline is fully automated and readily scalable in both breadth (instrument types) and depth (variations). This pipeline can be used to generate additional data for training or evaluation. We evaluate number of modern VLMs on MeasureBench and report these key ﬁndings: Persisting diﬃculty. Current VLMs still struggle with instrument reading, with the best model achieving only 30.3% accuracy on the real-world set and 26.1% on the synthetic set. Object recognition and text reading seems easy, but inferring numbers is hard. Models exhibit strong image understanding and text recognitione.g., reading unitsreaching over 90% accuracy on unit identiﬁcation. Yet they falter on mapping scales to numeric values. Systematic ﬁne-grained errors. Models often know how to read but miss details: They misinterpret pointer positions, confuse adjacent ticks, and mismatch values to scale markings, leading to near-miss but incorrect answers. With our data synthesis pipeline that produces accurately annotated readings, we have also conducted preliminary experiments of reinforcement learning using synthetic data. Results are encouraging in that the synthetic subset of MeasureBench can get signiﬁcantly improved, but not as promising on real-world images. In summary, this work makes the following contributions: We present MeasureBench, comprehensive benchmark targeting ﬁne-grained instrument reading across 26 instrument types and 2,442 imagequestion pairs. We provide controllable 2D/3D synthesis pipeline that produces precise labels for sketch or photorealistic images with randomized readings for 39 instruments. We deliver standardized evaluation of 17 contemporary VLMs and an analysis of their failure modes, highlighting concrete gaps in low-level perception and precise geometric reasoning that are not bridged by language priors. 2 Statistics Total Questions Real-World Images * Dial/Linear/Dig./Comp. * Instrument Types Synthetic Images * Dial/Linear/Dig./Comp. * Instrument Types * Instrument Appearances Number 2442 1272 (52%) 711/361/96/104 26 1170 (48%) 750/300/60/60 16 39 Table 1: Key statistics of MeasureBench. Figure 2: Distribution of reading designs and instrument types. Based on our synthetic data pipeline, we conduct preliminary experiments of reinforcement learning, leading to some promising results for more data curation but also implying potential need for future eﬀorts on more crafted design of visual representation for better generalization. Together, these results position MeasureBench as focused test for the ﬁne-grained visual competencies. We hope our results and this new benchmark could help future modeling eﬀorts towards more ﬁne-grained visual representation that would enable future VLMs to reason from detailed visual clues, then naturally generalize on instrument reading and other visual tasks that requires crucial visual capabilities such as geometric alignment and spatial understanding."
        },
        {
            "title": "2 MeasureBench",
            "content": "2.1 Overview of MeasureBench We introduce MeasureBench, comprehensive benchmark for evaluating the ability to read values from measuring instruments. MeasureBench comprises two main components: (i) diverse dataset of instrument images with standardized annotations, and (ii) data synthesis framework for generating additional training and evaluation data. By visual appearance, we categorize instruments into four readout designs (see also Figure 1 for examples from the real-world images in MeasureBench): Dial: Analog instruments with one or more pointers (e.g., ammeters and pressure gauges which typically have single pointer, whereas clocks often have two or three). Digital: Devices with electronic or mechanical digital readouts (e.g., pulse oximeters and electromechanical electricity meters). Linear: Instruments with linear scales and no pointers (e.g., rulers with single scale, and vernier calipers with main and vernier scale). Composite: Instruments combining multiple readout designs, such as dial calipers and complex water meters. As shown in Table 1, MeasureBench contains 2,442 questions: 1,272 real-world images and 1,170 synthetic images. The real-world subset spans 26 instrument types, while the synthetic subset covers 16 types with 39 distinct appearances. Figure 2 summarizes the distribution of reading designs and instrument types in the real-world set. To better explore the capability of VLMs in ﬁne-grained instrument reading, we place greater emphasis on dial and linear instruments because digital devices primarily test OCR capabilities, and composite instruments are comparatively rare in practice. Figure 3: An hybrid measuring instrument synthesis framework. Figure 4: Examples of synthetic images. 2.2 Evaluation Metrics Measurement error is natural when reading from any instrument that does not explicitly display deterministic digital value on the screen. Therefore, we determine the correctness of the ﬁnal reading via interval matching instead of strict value, along with the correctness of unit prediction. Answer extraction. To get the reading from natural language output, we extract the ﬁnal answer after common markers (e.g., Answer:) or inside boxed{...}. Our evaluation script will speciﬁcally parse: (i) numeric integers, decimals, scientiﬁc notation, and fractions (a/bﬂoat). If multiple scalars appear, use the rightmost. (ii) time the ﬁrst hh:mm[:ss] pattern, converted to seconds. Preserve adjacent tokens for unit matching. 1 Interval match. Each sample in our benchmark includes one or more candidates, where each candidate is closed numeric interval and an optional set of acceptable unit substrings to indicate correct unit in model response. prediction is value-correct if its parsed number lies within candidates interval, and unit-correct if any of that candidates units appears (case-insensitive). Fully-correct requires value-correct and, when speciﬁed, unit-correct for the same candidate. If multiple candidates exist, score against the one that maximizes correctness (prefer fully-correct; otherwise prefer value-correct; break ties by smaller relative error, then by narrower interval). 2.3 Real-World Subset Curation We assembled real-world subset of images from three sources: (i) Google Image Search using instrumentspeciﬁc keywords, restricted to images under permissive licenses for usage, (ii) photos contributed by team members under private authorization, and (iii) images purchased from third-party vendor. We removed low-quality images (e.g., blurry, low-resolution, or occluded) and annotated the remaining images using standardized schema. For each image, we recorded the instrument type, readout design, candidate units, and the valid interval of reading values; any value within this interval is considered correct. We recruited 10 qualiﬁed annotators and assigned tasks aligned with their professional backgrounds. Each image was independently labeled by one annotator and veriﬁed by another; disagreements were adjudicated by third annotator. Another independent round of review was conducted to verify the correctness of annotation, including the numerical intervals and the unit. 2.4 Data Synthesis Framework We build data synthesis framework that scales to many instrument types at low cost. Each measuring instrument is implemented as generator with uniﬁed interface and registered in global registry. Within each generator we randomize the number and type of scales, units, scale ranges, pointer angles, materials, lighting, backgrounds, and camera poses. Given list of target instruments, the framework queries the registry and returns rendered images with standardized labels for evaluation and training. 1Unicode characters are normalized for equivalence matching. Model Real-world subset Synthetic subset Overall Value Unit Overall Value Unit Gemini-2.5-Pro Qwen3-VL-235b-a22b GPT-5-Mini Gemini-2.5-Flash GPT-5 Qwen2.5-VL-7B Qwen2.5-VL-72B Claude-Opus-4.1 InternVL3.5-38B Claude-Sonnet-4 Qwen2.5-VL-32B LLaMA-4-maverick LLaMA-4-scout Mistral-medium-3.1 InternVL3.5-8B Mistral-small-3.2-24b Grok-4 30.3 23.7 22.1 20.3 19.8 15.5 15.2 14.4 12.9 12.6 12.4 12.2 11.0 10.6 9.7 8.5 7. 30.9 24.1 22.5 21.2 19.9 15.9 15.5 14.9 13.6 13.2 12.6 12.9 11.4 11.3 10.9 9.8 7.7 95.7 95.2 94.8 92.6 95.3 92.6 92.4 94.6 89.8 90.0 94.4 90.6 90.4 92.9 84.0 80.7 80.4 26.1 19.0 17.8 18.0 16.8 11.0 11.5 13.1 12.6 10.9 10.7 12.1 9.0 8.5 7.7 6.4 6.2 26.8 19.7 18.6 19.0 17.5 11.7 11.8 14.1 15.4 11.5 10.9 13.2 10.2 8.8 8.4 8.0 6.4 92.3 93.4 92.4 90.9 93.4 87.5 93.0 91.7 78.5 91.5 95.3 89.0 85.2 91.0 84.6 79.9 70.6 Table 2: Model performance on real and synthetic images, showing overall accuracy alongside separate accuracies for the numerical value and the unit. All values are percentages (%). Every generator produces (i) rendered image and (ii) standardized label schema covering the reading value, unit, design (dial/linear/composite/digital). This uniform contract enables plug-and-play additions and consistent evaluation across instrument families. We provide two complementary rendering paths: 2D programmatic rendering. prompt template speciﬁes appearance, reading layout, scale rules, units, and an optional reference image. We restrict the code to oﬄine-safe libraries (e.g., pillow, numpy, matplotlib). LLMs (e.g., GPT, Gemini) draft the code; we run automatic tests and perform light edits before registering the generator. This path is fast and inexpensive, ideal for large-scale ablations. 3D physically based rendering. We adapt Blender2 assets, augment scenes with contextual objects, automate pointer manipulation, and render with calibrated cameras and realistic backgrounds. This process produces semantically consistent images and reduces the sim-to-real gap. We implement 39 distinct appearances spanning 17 instrument types. For benchmarking purpose, we independently generate 30 images per appearance, totaling 1,170 synthetic images. As illustrated in Figure 4 , our dataset varies along four axes multi-style (2D vs. photorealistic 3D), multi-scale (ranges/units and dual scales), multi-orientation (rotations/tilts and imaging perturbations), and multi-class (dial, linear, composite, digital)providing broad coverage for robust reading models."
        },
        {
            "title": "3 Evaluation Results",
            "content": "We present systematic evaluation of various visionlanguage models (VLMs) on MeasureBench: 8 proprietary and 9 open-weight. The evaluated model families include GPT (OpenAI, 2025a), Claude (Anthropic, 2025), Gemini (Gemini Team, 2025), Mistral (Mistral AI, 2025), Grok (xAI, 2025), Qwen-VL (Bai et al., 2025), InternVL3 (Zhu et al., 2025), and LLaMA-4 (Meta AI, 2025). We report the results and analyze performance across models. 3.1 Main Results Table 2 reports results on MeasureBench for 17 VLMs. The best model, Gemini 2.5 Pro, reaches only 30.3% overall accuracy on real images and 26.1% on synthetic images, showing that reading measuring instruments remains challenging ﬁne-grained vision task for current VLMs. From the overall results, we make the following observations. 2https://www.blender.org/ 5 Model Real-world Subset Synthetic Subset Dial Digital Linear Composite Dial Digital Linear Composite Gemini-2.5-Pro Qwen3-VL-235b-a22b GPT-5-Mini Gemini-2.5-Flash GPT-5 Qwen2.5-VL-7B Qwen2.5-VL-72B Claude-Opus-4.1 InternVL3.5-38B Claude-Sonnet-4 Qwen2.5-VL-32B LLaMA-4-maverick LLaMA-4-scout Mistral-medium-3.1 InternVL3.5-8B Mistral-small-3.2-24b Grok-4 31.5 23.9 20.8 20.5 18.3 14.5 13.1 14.8 12.1 15.0 10.0 12.1 8.2 6.9 10.4 7.9 6.5 80.2 68.7 70.8 65.6 66.7 48.4 53.7 38.5 51.6 20.8 50.5 44.8 54.2 57.3 30.5 32.3 24.0 21.9 16.0 16.9 13.0 15.2 13.0 13.3 11.1 7.7 9.1 10.2 7.2 8.0 8.3 5.5 5.8 7.5 4.0 4.5 3.0 1.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0. 18.1 13.8 12.0 11.9 9.7 6.1 6.4 6.1 6.3 4.8 5.9 6.1 5.3 3.7 3.5 3.2 3.3 70.0 63.0 56.7 75.0 48.3 33.3 40.0 45.0 41.7 26.7 26.7 50.0 20.0 23.3 26.7 5.0 25.0 39.3 26.8 27.7 25.3 31.3 21.0 20.7 26.7 25.3 25.0 21.7 21.7 17.7 19.0 16.0 16.0 10.3 15.0 2.0 1.7 1.7 1.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.8 Table 3: Performance comparison of state-of-the-art Vision Language Models on the MeasureBench benchmark. The table details the accuracy(%) of each model across the four instrument reading types (Dial, Digital, Linear, and Composite) on both real-world and synthetic image sets. Reading the value is the bottleneck; reading the unit is almost solved. Across models, unit recognition is consistently above 90% on both real and synthetic sets, while value accuracy is much lower (e.g., Gemini 2.5 Pro: 95.7% unit vs. 30.9% value on real images). This suggests OCR and image understanding capabilities of current VLMs are already suﬃcient for unit prediction, whereas estimating the numerical valuewhich often requires precise localization of pointers, ticks, and scalesdemands ﬁne-grained visual understanding that current systems lack. Diﬀerent readout designs are not equally challenging for VLMs. Table 3 decomposes accuracy by instrument type. Digital displays are much easier (e.g., up to 80.2% on real images for Gemini 2.5 Pro), reﬂecting reliance on OCR. Dial and linear instruments remain challenging (typically 10 32%), as they require needle localization or reading tick marks under clutter, highlights, and distortion. Composite instruments are by far the hardest: they require combining multiple readout designs, reading each component correctly, and performing the corresponding numerical calculationsdemands that push well beyond the capabilities of current VLMs. Larger models may not always be better for ﬁne-grained reading. In Table 3, we found it strange that GPT-5Mini outperform GPT-5, and Qwen2.5-VL-7B outperforms Qwen2.5-VL-32B and Qwen2.5-VL-72B. Through case studies, we observed that while portion of GPT-5-Minis correct answers were attributable to successful guessing, GPT-5 genuinely erred in image recognition on diﬀerent subset of problems. Within the Qwen family, overall performance does not monotonically improve with larger LLMs when the visual tower remains unchanged, Qwen2.5-VL-7B, Qwen2.5-VL-32B and Qwen2.5-VL-72B use ViT with the same number of parameters. This implies that larger language backbones do not translate into better ﬁnegrained perception without commensurate upgrades to the visual encoder, input resolution, or tokenization of high-frequency details. Real vs. synthetic. The gap between real and synthetic is modest but consistent: most models lose few points on Overall (e.g., Gemini-2.5-Pro 30.3 to 26.1, GPT-5 19.8 to 16.8), showing that synthetic scenes remain genuine challenge rather than an easy shortcut. That drop is driven mainly by Value accuracy, while Unit is comparatively stable, suggesting numeric extraction is the primary failure mode. Yet the sim-to-real gap is small enough to preserve model ordering, indicating synthetic is challenging yet highly transferable, and can be useful proxy for real-world performance. Category-wise performance varies widely. Figure 5 shows instrument-category accuracy across models on real images. Performance varies substantially by instrument. Categories with high proportion of digital readouts (e.g., electricity meters) tend to achieve higher accuracy. In contrast, categories dominated by multineedle dials (e.g., clocks, water meters) are challenging for all models. Dials with single needle and sparse Figure 5: Accuracy heatmap by instrument category across models on the real-world subset. tick marks are generally easier to read, and linear gauges (e.g., rulers, measuring cylinders) are easier than dials overall. 3.2 Thinking vs. No-Thinking has demands thinking ﬁne-grained been Inference-time widely adopted to improve large language models (LLMs) on complex textbased reasoning. We ask whether this also holds for VLMs on MeasureBench, visual which perception coupled with numerical reasoning. We compare couple of hybrid reasoning models under no-thinking settingreasoning tokens set to 0against thinking setting with maximum of 10, 240 reasoning tokens. The study InternVL3.5-8B, covers ﬁve models: InternVL3.5-38B, Qwen3-VL-235BA22B, Claude 4.1 Opus, and Gemini 2.5 Flash. As shown in Figure 6, enabling thinking yields very little improvement, sometimes even degrades performance. While thinking often boosts text-only reasoning, it does not appear to Figure 6: Performance and eﬃciency analysis of various large vision-language models. The accuracy against the average token count is ploted to show the performance-cost trade-oﬀ. 7 help VLMs attend to the most relevant image regions or to enhance ﬁne-grained visual perception on MeasureBench. This conforms to what we have found in our recent evaluation report on the utility of test-time thinking for visual problems (FlagEval Team, 2025). Figure 6 further relates accuracy to the average number of reasoning tokens consumed per sample. Although thinking increases token usage, the increment is modesttypically few hundred up to roughly 12 103 tokensyet accuracy gains remain limited. Instrument reading primarily requires precise visual decoding rather than extended chain-of-thought; accordingly, increasing reasoning tokens is not an eﬀective way to improve performance on this task. 3.3 Case Studies Figure 7: Case study of VLM instrument reading. Text in green marks statements consistent with the image; yellow marks contradictions. Figure 7 shows two typical examples from our benchmark: measuring cylinder and an ammeter. In each panel, text in green highlights denote statements consistent with the image, whereas text in yellow denotes claims that are contradicted by the visual evidence. What VLMs get right. Models generally know the task. They identify the instrument, locate the indicator (meniscus/needle), infer the unit and major tick spacing, and try to interpolate to ﬁnal value. This shows mission awareness and an active search for the pointer. Where they fail. Most errors arise from small perceptual mistakes that dominate the numeric outcome: (i) Pointer localization: one minor tick left/right changes the reading (e.g., 4.4 vs. 4.5 A). (ii) Indicator interpretation: wrong minor-tick count or reading the wrong edge of the meniscus. Right answer, wrong reasons. We observe frequent error cancellation. In the cylinder example (Gemini-2.5pro), an incorrect subdivision story coincidentally oﬀsets later mistake, yielding the correct number. Such cases inﬂate accuracy if only the ﬁnal answer is scored."
        },
        {
            "title": "4 Training with Synthetic Data",
            "content": "Our data synthesis pipeline, which provides accurate measurement readings, naturally raises the question of whether task-speciﬁc post-training can further improve performance on this task. To investigate this, we synthetically generated 100 samples for each of the 39 instruments in our framework, yielding 3,900 imagequestion pairs that we used for model training. The task format especially suits reinforcement learning via assigning positive reward on correct reading results. In this work, we adapt the GRPO algorithm (Shao et al., 2024) to ﬁne-tune Qwen2.5-VL-7B with reinforcement learning. Training details are listed in appendix A.4. We consider two reward variants: (i) rule-based reward aligned with the evaluation method and (ii) softmargin reward that assigns partial credit to predictions close to the target interval. Evaluation-aligned reward. To stay consistent with the scoring used in our evaluation, we ﬁrst adopt discrete, rule-based reward: = 0.9 fully_correct + 0.1 format_correct where fully_correct, format_correct {0, 1}, with fully_correct=1 iﬀ both the value and unit are correct, and format_correct=1 iﬀ the output matches <think>.*</think>.*Final Answer.*. Soft-margin reward. For numeric answers given as an interval [l, r], we additionally allow partial credit for predictions near the interval. Let ˆy be the prediction and = (cid:40) 0, min(ˆy l, ˆy r), otherwise, ˆy [l, r], be the distance to the closest boundary. We set margin and deﬁne linearly decaying partial credit: = (cid:40) > l, l, 0.05 l, otherwise, partial = 0.5 max 0, 1 (cid:18) (cid:19) , + ε (1) (2) (3) where ε is small constant. Finally, we combine it with the evaluation-aligned reward by taking the better value term: Rsoft = 0.9 max(fully_correct, partial) + 0.1 format_correct. This keeps rewards consistent with evaluation when the answer is exact, while giving informative feedback to near-miss predictions. (4) We evaluated the eﬀect of reinforce learning result using only new batch of synthetic data without using any image in MeasureBench, with results shown in Table 4. Reinforcement learning leads to signiﬁcant performance boost on the in-domain synthetic image test setwhere the overall accuracy increased by more than threefold, from 11.0% to 35.2%. Moreover, the model exhibited enhanced generalization to out-ofdistribution (OOD) real-world images, with accuracy rising notably from 15.5% to 20.1%. However, our experiments indicate that the carefully designed soft-margin variant brings no substantial additional gains over the evaluation-aligned reward. Model/Dataset Overall Value Unit Qwen2.5-VL-7B (Real-world) Qwen2.5-VL-7B+GRPO (Real-world) Qwen2.5-VL-7B+GRPO-soft (Real-world) Qwen2.5-VL-7B (Synthetic) Qwen2.5-VL-7B+GRPO (Synthetic) Qwen2.5-VL-7B+GRPO-soft (Synthetic) 15.5 20.1 (+29.7%) 20.0 (+29.0%) 11.0 35.2 (+219.1%) 35.3 (+220.9%) 15.9 20.8 (+30.8%) 20.8 (+30.8%) 11.7 35.6 (+204.3%) 35.6 (+204.3%) 92.6 92.4 (-0.2%) 91.9 (-0.7%) 87.5 96.7 (+10.5%) 98.0 (+12.0%) Table 4: Results of Qwen2.5-VL-7B with GRPO on real-world and synthetic subsets. 9 In general, these results show potential from more data curation for VLM training, but also leaving question on whether we should instead pursuit better model architectures and visual encoding schemes that would make future VLM genuinely reasoning from detailed visual cues and generalizing over unseen types of instruments."
        },
        {
            "title": "5 Related Work",
            "content": "VLMs and Benchmarks VisionLanguage Models (VLMs) have made rapid progress in recent years. Early systems such as LLaVA (Liu et al., 2023) and InstructBLIP (Dai et al., 2023) pioneered vision instruction tuning, while families like Qwen-VL (Bai et al., 2023), InternVL (Chen et al., 2024), and GPT-4o (OpenAI, 2024) demonstrated strong general multimodal understanding. More recently, models augmented with reinforcement learning and veriﬁable rewards (e.g., OpenAI o3 (OpenAI, 2025b), Gemini 2.5 Pro (Gemini Team, 2025), Claude Opus 4 (Anthropic, 2025), Qwen3-VL (Bai et al., 2025)) exhibit improved stepwise reasoning and planning. To assess these capabilities, broad suite of benchmarks has emerged. General-purpose evaluations (e.g., MMBench, MM-Vet, Seed-Bench) target holistic multimodal competence; knowledge-intensive suites (e.g., MMMU (Yue et al., 2024), MMMU-Pro (Yue et al., 2025), ScienceQA (Lu et al., 2022)) emphasize academic problem solving; math-centric sets (e.g., MathVision, MathVerse) probe visual mathematical reasoning; and perception-focused tests (e.g., CV-Bench (Tong et al., 2024), BLIND (Rahmanzadehgervi et al., 2024)) stress ﬁne-grained visual understanding. More specialized studies on ﬁne-grained reading report persistent weaknesses: SalBench highlights diﬃculties with low-level perceptual cues, while BlindTest (Rahmanzadehgervi et al., 2024), SRBench (Stogiannidis et al., 2025), and VisOnlyQA (Kamoi et al., 2025) expose brittle shape, geometry, and spatial reasoning. Despite this progress, relatively less attention has been paid to instrument reading, which requires precise localized visual perception coupled with light numerical computation (e.g., inferring tick intervals, decimal placement, and unit normalization). Measuring Instruments Reading Reading measuring instruments is challenging because it integrates ﬁnegrained visual perception, text reading, and visuospatial reasoning. Numerous computer vision methods target speciﬁc families of tools such as rulers (Pan et al., 2025), clocks (Yang et al., 2022; Saxena et al., 2025), water meters (Van et al., 2025), pressure gauges (Reitsma et al., 2024), and other analog dials (Howells et al., 2021; Salomon et al., 2022; Shu et al., 2023; Leon-Alcazar et al., 2024). Typical pipelines combine detection/segmentation of scales and pointers, geometric rectiﬁcation, and OCR or tick-interval estimation to map visuals to numeric values and units. However, these approaches are narrowly tailored and generalize poorly across device types, design variations, viewpoints, glare/occlusion, and unit ambiguity. (See also relevant trials in Appendix A.7) More recently, VLMs have been applied to instrument reading: GPT-4o (OpenAI, 2024) reports preliminary ability on industrial gauges, and CAD2DMD-SET (Valente et al., 2025) evaluates several VLMs on digital measurement devices. Yet current evaluations remain fragmented: they cover limited device diversity, emphasize categorical correctness over calibrated numeric error, and seldom assess unit normalization, tolerance bands, or robustness stressors."
        },
        {
            "title": "6 Conclusions and Discussion",
            "content": "We introduced MeasureBench, comprehensive benchmark with both real-world and synthetic subsets for evaluating visionlanguage models (VLMs) on instrument reading. Our analyses reveal persistent limitation of current VLMs: diﬃculty with ﬁne-grained visual cues and precise visualnumeric correspondences, leading to errors in value estimation and unit normalization. The proposed synthetic data generation pipeline serves both as source of controlled benchmarks and as an eﬀective means of training data augmentation. We also explored reinforcement learning with GRPO. Preliminary results suggest that even small amounts of targeted synthetic data can yield measurable gains that transfer to real-world settings, but only to moderate extent. We hope this work could help future VLM development with more comprehensive training data curation or better visual representation modeling to enable stronger capabilities in ﬁne-grained understanding, geometric alignment, and spatial reasoning."
        },
        {
            "title": "Authors",
            "content": "Fenfen Lin, Yesheng Liu, Haiyu Xu, Chen Yue, Zheqi He, Mingxuan Zhao, Miguel Hu Chen, Jiakang Liu, JG Yao, Xi Yang"
        },
        {
            "title": "References",
            "content": "Anthropic. Claude opus 4 & claude sonnet 4 system card. claude-4-system-card, 2025. https://www.anthropic.com/ Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24185 tasks. 24198, 2024. Yasser Dahou, Ngoc Dung Huynh, Phuc H. Le-Khac, Wamiq Reyaz Para, Ankit Singh, and Sanath Narayan. Salbench: Vision-language models cant see the obvious. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. URL https://salbench.github.io. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Instructblip: Towards general-purpose vision-language models with instruction tuning. and Steven Hoi. Advances in neural information processing systems, 36:4925049267, 2023. FlagEval Team. FlagEval ﬁndings report: preliminary evaluation of large reasoning models on automatically veriﬁable textual and visual questions, 2025. URL https://arxiv.org/abs/2509.17177. Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. URL https://arxiv.org/ abs/2507.06261. Ben Howells, James Charles, and Roberto Cipolla. Real-time analogue gauge transcription on mobile phone. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 23692377, 2021. Juan Izquierdo-Domenech, Jordi Linares-Pellicer, Carlos Aliaga-Torro, and Isabel Ferri-Molla. Towards robust industrial control interpretation through comparative analysis of visionlanguage models. Machines, 13(9): 759, 2025. Ryo Kamoi, Yusen Zhang, Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, and Rui Zhang. VisonlyQA: Large vision language models still struggle with visual perception of geometric information. In Second Conference on Language Modeling, 2025. URL https://openreview.net/forum?id=PYHwlyu2fa. Juan Leon-Alcazar, Yazeed Alnumay, Cheng Zheng, Hassane Trigui, Sahejad Patel, and Bernard Ghanem. Learning to read analog gauges from synthetic data. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 86168625, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. Project lead; equally contributed to this work. correspondance to: zqhe at baai.ac.cn 11 Ahmed Masry, Do Long, Jia Qing Tan, Shaﬁq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL https://aclanthology.org/2022.findings-acl.177. Dalius Matuzevičius. Rulers2023: an annotated dataset of synthetic and real images for ruler detection using deep learning. Electronics, 12(24):4924, 2023. Meta AI. Llama 4 models. https://www.llama.com/models/llama-4/, 2025. Oﬃcial Llama 4 model overview. Mistral AI. Mistral medium 3. https://mistral.ai/news/mistral-medium-3, 2025. Oﬃcial announcement; Medium 3 series. OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, May 2024. Accessed: 2025-09-25. OpenAI. Introducing gpt-5, August 2025a. URL https://openai.com/index/introducing-gpt-5/. OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/introducing-o3-and-o4-mini/, April 2025b. Accessed: 2025-09-25. Yimu Pan, Manas Mehta, Gwen Sincerbeaux, Jeﬀery Goldstein, Alison Gernand, and James Wang. Reading ruler in the wild. arXiv preprint arXiv:2507.07077, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, and Others. Humanitys last exam, 2025. URL https://arxiv.org/abs/2501.14249. Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. arXiv preprint arXiv:2407.06581, 2024. Maurits Reitsma, Julian Keller, Kenneth Blomqvist, and Roland Siegwart. Under pressure: learning-based analog gauge reading in the wild. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 1420. IEEE, 2024. Gabriel Salomon, Rayson Laroca, and David Menotti. Image-based automatic dial meter reading in unconstrained scenarios. Measurement, 204:112025, 2022. Rohit Saxena, Aryo Pradipta Gema, and Pasquale Minervini. Lost in time: Clock and calendar understanding challenges in multimodal llms. arXiv preprint arXiv:2502.05092, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yan Shu, Shaohui Liu, Honglei Xu, and Feng Jiang. Read pointer meters based on human-like alignment and recognition algorithm. In CCF National Conference of Computer Applications, pp. 162178. Springer, 2023. Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. In Proceedings of the IEEE Conference on Computer Vision and Pattern Towards vqa models that can read. Recognition, pp. 83178326, 2019. Ilias Stogiannidis, Steven McDonagh, and Sotirios Tsaftaris. Mind the gap: Benchmarking spatial reasoning in vision-language models. arXiv preprint arXiv:2503.19707, 2025. Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, and Greg Durrett. Chartmuseum: Testing visual reasoning capabilities of large vision-language models, 2025. URL https://arxiv.org/abs/2505.13444. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. 12 João Valente, Atabak Dehban, and Rodrigo Ventura. Cad2dmd-set: Synthetic generation tool of digital arXiv preprint measurement device cad model datasets for ﬁne-tuning large vision-language models. arXiv:2508.21732, 2025. Bay Nguyen Van, Anh Nguyen, Kiet Tran Trung, Thien Ho Huong, Ha Duong Thi Hong, Hau Nguyen Trung, and Vinh Truong Hoang. Water meter reading based on text recognition techniques and deep learning. IEEE Access, 2025. xAI. Grok 4 model announcement. https://x.ai/news/grok-4, 2025. Charig Yang, Weidi Xie, and Andrew Zisserman. Its about time: Analog clock reading in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 25082517, 2022. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of ACL, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Reading of Complex Measuring Instruments Complex measuring instruments, including those with composite readout designs or multiple dials and pointers.\" These instruments are very challenging for current VLMs. Figure 8 shows an example of an multi-dial electricity meter, which has 5 dials and each dial has pointer. The result shows that most VLMs correctly detect the number of dials and can name the leftmost and rightmost ones, but they consistently read the dials left-to-right. The correct protocol is right-to-left, since when pointer is near tick on an earlier dial, the subsequent dial disambiguates whether to round down or advance. Moreover, the per-dial pointer estimates produced by current models deviate substantially from the true indications, yielding large cumulative reading errors. A.2 Example of guessing correctly As shown in Figure 9, GPT-5 incorrectly estimated the ammeter reading as 26 A, while GPT-5-Mini correctly identiﬁed it as 20 A. However, the reasoning process reveals that GPT-5-Mini is not fully conﬁdent in its answer and includes an element of guesswork. A.3 Numerical output distributions Beyond simple accuracy, we analyze the statistical distribution of numerical outputs. Our ﬁndings indicate that model-generated numbers exhibit strong priors, which contrasts with the smooth distribution of the ground truth data. We list two key examples of this behavior: rounded numbers and the 10:10\" priors. Spiky distributions at integers: An analysis of the numerical output distributions (Figure 10) reveals key diﬀerence between model answers and the ground truth. While the ground truth values are distributed smoothly and uniformly, model-generated answers are heavily concentrated around \"round\" numbers. This creates \"spiky\" distribution with prominent peaks at integers, multiples of 10, and range endpoints like 0 and 1. We also compare distributions before and after RFT. Before RFT, Qwen2.5-VL-7B shows pronounced spikes at round values (e.g., 10, 20). After RFT, these peaks are reduced and the distribution becomes smoother, though some round-number bias remains. 13 Figure 8: Model results on an electricity meter The 10:10\" Priors: In the advertisements and product listings, clocks and watches are tended to be set to 10:10. This speciﬁc time is chosen for aesthetic reasons. This bias is inherited from training data, so as shown in Table 5, some models exhibit strong predisposition for this answer. A.4 Training details We employ reinforcement ﬁnetuning on the synthesis datasets. Following Deepseek-R1, we employ GRPO with format reward function to optimize the model to output the thinking process within <think>...</think>. Training is performed on 8H100 GPUs for 15 epochs with global batch size of 128 and learning rate of 1 106, and rollout number of 8. A.5 Examples of Synthetic Measuring Instruments Here we provide additional examples of synthesized images of measuring instruments generated by our framework. Each generator in our framework is expected to render an image of an instrument with similar appearance along with random readings. In Figure 13, 2D images are rendered by oﬄine-only libraries like Pillow, NumPy and Matplotlib, 3D images are rendered by Blender. A.6 3D Model Acquisition and Preparation with Blender To construct large collection of measurement-related 3D assets, we use Blender (v4.2) in combination with publicly available online repositories. The procedure was as follows. A.6.1 Asset Retrieval We integrate the BlenderKit plugin into Blender to access free 3D assets, including models, HDRs, and materials. For categories underrepresented in BlenderKit (e.g., cylinder, hygrometer), we also retrieved models 14 Figure 9: Example where GPT-5 answered incorrectly but GPT-5-Mini guessed correctly Model Real-world Synthetic Qwen2.5-VL-72B-Instruct GPT-5-Mini Claude-Sonnet-4 Qwen2.5-VL-7B-Instruct Qwen2.5-VL-32B-Instruct GPT-5 mistral-small-3.2-24b-instruct InternVL3.5-38B-thinking Mistral-medium-3.1 Claude-Opus-4.1 InternVL3.5-8B-thinking Claude-Opus-4.1-thinking Qwen3-VL-235b-a22b-instruct InternVL3.5-38B Gemini-2.5-Pro Gemini-2.5-Flash InternVL3.5-8B Grok-4 Gemini-2.5-Flash-thinking LLaMA-4-maverick qwen3-vl-235b-a22b-thinking LLaMA-4-scout 70.34% 29.66% 26.27% 24.58% 21.19% 20.34% 20.34% 19.49% 16.95% 13.56% 12.71% 12.71% 12.07% 11.86% 11.86% 4.24% 3.39% 3.39% 0.85% 0.85% 0.85% 0.00% 51.48% 7.78% 16.30% 16.67% 9.26% 6.30% 15.56% 10.74% 4.07% 9.63% 7.04% 10.37% 12.45% 10.00% 3.33% 1.11% 6.30% 4.81% 1.48% 1.11% 1.16% 1.85% Table 5: Proportion of \"10:10\" responses on clock images in MeasureBench. from Sketchfab. The queries included watches, clocks, scales and rulers, thermometers, covering both pointerbased and linear-scale instruments. 15 Figure 10: comparison of the numerical output distributions for model-generated answers (blue) and ground truth values (orange). The histograms show results for two models across two common numerical ranges, [0100] and [0-1]. Figure 11: Numerical output distributions of[0-100] Figure 12: Numerical output distributions of [0-1] A.6.2 Model Normalization 1. Pointer-based models (e.g., clocks, scales): In many assets, the pointer was not initially aligned with the zero position. We manually rotated the pointer to zero and reset its transformations (rotation along the x, y, axes set to 0). 2. Linear-scale models (e.g., thermometers): For these, we determined the minimum-maximum mapping on the scale and adjusted the geometry proportionally so that the linear transformations of pointer correctly represented measurement values. Figure 13: Additional examples of synthetic measuring instruments generated by our pipeline. A.6.3 Contextual Scene Augmentation Some downloaded models only represented the measurement instrument itself, which led to unrealistic renderings when the pointer indicated nonzero value. To improve semantic consistency, we augmented scenes with additional objects: Scales: To avoid showing dial reading 1 kg with an empty plate, we placed an additional object (e.g., fruit model, such as dragon fruit) on the weighing surface. Rulers: Since rulers measure relative length, we included reference object (a pen model). The pen was rescaled and positioned alongside the ruler, allowing queries such as \"How long is the pen?\" to be grounded in the rendered image. These contextual additions ensured that pointer readings were visually consistent with the surrounding scene, enhancing dataset realism, and reducing ambiguity for vision-language evaluation. A.6.4 Pointer Rotation Control Pointer manipulation was automated with Blenders Python API. Clocks: For clocks, rotation angles were computed directly from the target hour, minute, and (optionally) second values: 1 second_angle = math . radians ( target_second * 6) 2 minute_angle = math . radians ( target_minute * 6 + target_second * 0.1) 3 hour_angle = math . radians (( target_hour % 17 The axis of rotation varied across diﬀerent models (i.e. whether Oxy, Oxz, or Oyz). For example, clocks hour hand can be controlled with: hour_hand.rotation_euler = (0, 0, -hour_angle). However, depending on the model, the rotation angle might be applied to the ﬁrst or second component of the Euler tuple rather than to the third. Other dials (e.g., hygrometers): For these, the degree of pointer rotation depends on the speciﬁc model geometry. We ﬁrst check for the maximum rotation angle (max.rot.deg) that corresponded to the maximum scale value, and set pointer positions linearly: 1 max_rot = math . radians ( max_rot_deg ) 2 rot_z = min_rot + ( humidity - min_humidity ) / ( max_humidity - min_humidity ) 3 * ( max_rot - min_rot ) This approach is generalized to other instruments with linear or semi-linear dial mappings. If the geometry of the model used nonstandard orientation, we rotated the entire object to align it with the desired axis. A.6.5 Camera Alignment Since the dial panels of many models were not centered at the origin, we applied oﬀsets to position the camera such that it directly faces the dial. Camera distance and angle were tuned empirically to maximize legibility of the dial face and pointer. For small-scale instruments, shorter distances and narrower angle ranges provided clearer renderings, whereas larger instruments beneﬁted from wider perspectives. A.6.6 Lighting and Environment HDRs To ensure consistent illumination across renderings, we used two strategies depending on the dataset requirements: HDR environment maps: For most models, we initialized scenes with background environment maps (.exr ﬁles), either using Blenders built-in HDRIs or downloading additional ones via BlenderKit. These provided realistic lighting and surface reﬂections. HDRs were ﬁrst manually conﬁgured and later automated using Python. Direct light sources: For cases where clean background was preferred, we disabled HDRs and instead added light objects from diﬀerent positions (e.g., point lights or area lights). This clearly illuminated the dial while leaving the background neutral. A.6.7 Rendering Execution Scripts were executed either directly within Blenders Scripting panel or externally via Python (importing the bpy module) in an IDE such as Visual Studio Code. This ﬂexibility enabled large-scale automated rendering of models across diﬀerent instrument categories. Figure 14 provides illustrative examples of the augmentation strategies described above. A.7 Trials from Earlier Computer Vision Systems Some may wonder how traditional domain-speciﬁc computer vision systems might behave. We ﬁnd the source code and pretrained models from two gauge reading systems (Shu et al., 2023; Reitsma et al., 2024) still available, thus we have also tried relevant subset (dial meters) of our benchmark data. The results are mostly disappointing, showing very little generalization across detection, pointer localization, or text reading on our benchmark data which may diﬀer lot from their training images. Table 6 indicates that the generalization ability for pointer value detection is inferior to that of generalpurpose VLMs. The end-to-end neural network (Shu et al., 2023) might have overﬁtted to the original training data distribution, resulting in failures to detect pointers or scale marks on most out-of-distribution datasets. Meanwhile, the accuracy of OCR-based unit recognition (Reitsma et al., 2024) is signiﬁcantly lower than that of VLMs. 18 Figure 14: Examples of augmentation strategies applied during 3D model acquisition and preparation with Blender (v4.2). subset Reitsma et al. Shu et al.2023 Overall Value Unit Overall Value Unit Overall Value Unit Gemini-2.5-Pro Real-world Synthetic 6.8 15 10.2 15 17.8 17. N/A N/A 0.0 0.0 N/A N/A 22 20 22 20 94.9 97. Table 6: Performance (accuracy %) on earlier computer vision system versus Gemini 2.5 Pro on relevant subset of MeasureBench benchmark. (N/A indicates failed to detect for all examples we tried.)"
        }
    ],
    "affiliations": [
        "BAAI FlagEval Team"
    ]
}