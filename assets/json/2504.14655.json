{
    "paper_title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs",
    "authors": [
        "Yunhui Xia",
        "Wei Shen",
        "Yan Wang",
        "Jason Klein Liu",
        "Huifeng Sun",
        "Siyue Wu",
        "Jian Hu",
        "Xiaolong Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models, addressing two key challenges in LLM research: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode Python problems with rich metadata, broad coverage, 100+ test cases per problem, and temporal splits (pre/post July 2024), our dataset enables contamination-free evaluation and efficient supervised fine-tuning (SFT). Experiments show reasoning models significantly outperform non-reasoning counterparts, while SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts. The dataset and evaluation framework are available on Hugging Face and Github."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 5 5 6 4 1 . 4 0 5 2 : r LeetCodeDataset: Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs Yunhui Xia newfacade@163.com Wei Shen shenwei0917@126.com Yan Wang wangyanps4@126.com Jason Klein Liu jasonkleinlove@gmail.com Huifeng Sun shelon_2008@126.com Siyue Wu wusy104@gmail.com Jian Hu janhu9527@gmail.com Xiaolong Xu xlxu@ieee.org"
        },
        {
            "title": "Abstract",
            "content": "We introduce LeetCodeDataset, high-quality benchmark for evaluating and training code-generation models, addressing two key challenges in LLM research: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode1 Python problems with rich metadata, broad coverage, 100+ test cases per problem, and temporal splits (pre/post July 2024), our dataset enables contamination-free evaluation and efficient supervised finetuning (SFT). Experiments show reasoning models significantly outperform nonreasoning counterparts, while SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts. The dataset and evaluation framework are available on Hugging Face2 and Github3."
        },
        {
            "title": "Introduction",
            "content": "Code generation is critical in research and applications of large language models (LLMs). With the emergence of advanced reasoning models like OpenAI o1 (OpenAI, 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025a), two key challenges are highlighted. The first challenge is the lack of coding benchmarks that accurately assess LLMs reasoning abilities. LiveCodeBench (Jain et al., 2024), commonly used benchmark, addresses this by sourcing problems from platforms like LeetCode and AtCoder and using live updates to avoid data contamination. However, it has limitations: it covers few problems per platform and lacks detailed tags for algorithms and data structures, making in-depth analysis difficult. The second challenge is the absence of self-contained testbed for training LLMs to master competition-level coding through methods such as supervised fine-tuning (SFT) (Zhou et al., 2024), direct preference optimization (DPO) (Rafailov et al., 2023), and reinforcement learning (RL), Corresponding author 1https://leetcode.com/ 2https://huggingface.co/datasets/newfacade/LeetCodeDataset 3https://github.com/newfacade/LeetCodeDataset 1 which are widely used for aligning model behavior with desired coding performance (Shen & Zhang, 2024; Shen et al., 2025; Hu, 2025; Liu et al., 2025). While datasets such as APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022), and TACO (Li et al., 2023) provide competition problems split into training and test sets, they lack live updates and easy tools to support RL training workflows. Recently released Open-R1 CodeForces-CoTs (Penedo et al., 2025) dataset, generated by DeepSeek-R1, fails to filter solutions for correctness, limiting its reliability for rigorous skill evaluation. To address these challenges, we introduce LeetCodeDataset, which fully leverages high-quality resources from LeetCode. LeetCode is popular online platform for coding practice and technical interview preparation. It offers over 3,000 algorithm and data structure problems at varying difficulty levels. The platform supports multiple languages (Python, Java, C++, etc.), providing real-time code testing with execution feedback. Developers use LeetCode to improve their problemsolving skills, prepare for tech company interviews, and join global programming competitions. We meticulously curated LeetCode dataset covering over 90% of Python problems on the platform. Each problem is annotated with rich metadataincluding difficulty levels, release dates, and topic tagsand paired with 100+ test cases of varying complexity to minimize false positives. The dataset also includes an evaluation toolkit for fast and reliable assessment. To ensure temporal validity, we adopted strict time-based split: problems released after July 1, 2024, form the test set for benchmarking, while those released earlier constitute the training set. Using this dataset, we evaluated popular modelsincluding proprietary and open-source modelsand reasoning and non-reasoning architectures. Our evaluation shows that reasoning models outperform non-reasoning ones in competitive programming tasks, with Claude 3.7 Sonnet (Anthropic, 2024) performing best in its category. Additionally, we conducted supervised fine-tuning (SFT) training on the LeetCode training set. Despite using only 2.6K samples, the resulting model achieves performance comparable to counterparts trained on 110K code examples, demonstrating the exceptional training efficiency of the LeetCodeDataset."
        },
        {
            "title": "2 LeetCodeDataset",
            "content": "2.1 Data Collection As of the end of March 2025, the LeetCode platform hosted approximately 3,505 programming problems, among which 3,115 supported Python submissions. Our data collection process begins with this Python problem set, and we describe our process below. Metadata Acquisition: LeetCode provides GraphQL API4 for accessing problem metadata and platform-hosted information. The following metadata fields were systematically collected for each problem: slug (URL identifier and primary key), question_id (unique sequential number), difficulty (Easy/Medium/Hard), problem_description (full text, with examples and constraints, see Figure 1), starter_code (language template code), and topic_tags (problem tags such as Array, Dynamic Programming). Canonical Solution Verification: We retrieved reference solutions from various open-source GitHub repositories56, and then verified the correctness of these solutions on the LeetCode platform, establishing ground truth solutions with 100% acceptance rate. 4https://github.com/fspv/python-leetcode 5https://github.com/doocs/leetcode 6https://github.com/walkccc/LeetCode Figure 1: An example of LeetCode problem. Entry Point Identification: The entry point refers to the function targeted for testing. In Figure 1, this is missingNum. Most starter codes contain single function that is automatically identified as the entry point through text pattern matching. Specialized validation logic is necessary for problems requiring multiple functions (standard in design/simulation scenarios). However, such judgment codes are unavailable and challenging to develop. Therefore, our implementation focuses exclusively on single-function starter code scenarios. Input Generation: To generate inputs for the entry point as part of test case development, we use one-shot prompting (Figure 4) with the LLM. However, this method often produces overly simplistic inputs. To address this, we further prompt the LLM (Figure 5) to generate more complex inputs. By applying both approaches multiple times, we construct an average of over 100 inputs per problem, including many complex cases, significantly reducing the risk of false positives. Test Case Generation: Now we have all the necessary information to generate test cases: specifically, we compute the Canonical Solutions entry point output using the previously generated inputs. To enable this, we developed sandboxed execution environment for safe code evaluation, inserted the necessary imports before the canonical solution as part of the prompt, and handled special data structures such as binary trees (see Figure 7) and linked lists (see Figure 6) separately. After these steps, we successfully generated outputs for 2,869 problems, identifying the remaining cases as edge scenarios requiring additional investigation. Our pipeline ensures high dataset quality and comprehensive coverage, covering over 90% of all Python problems available on the platform. LeetCodeDataset for SFT: We designed LeetCodeDataset to serve dual purposes of model training and performance evaluation. The dataset employs temporal split strategy: problems published after predefined cutoff date (e.g., 2024-07-01) form our evaluation set, while earlier problems are allocated for supervised fine-tuning. The query of LeetCodeDataset is consistent with Live3 CodeBenchs construction (Jain et al., 2024). For response generation, we intentionally avoided canonical solutions (often containing minimal comments or reasoning), which makes them suboptimal for instructional tuning. The detailed analysis can be found in section 4. We employed Qwen2.5-Coder-32B-Instruct (Hui et al., 2024), highly sample-efficient and capable model, to implement multi-stage generation process: High-temperature sampling (T = 1.0) produces diverse solution candidates. Automated test case verification filters functionally correct responses. For persistently failing problems, ground truth code snippets are integrated as contextual hints to improve the likelihood of correctness. Finally, we developed the LeetCodeDataset, which features broad coverage, reliable benchmarking, evaluation/training splits based on release dates, and verified model-generated (query, response) pairs for SFT. The dataset can also support RL training by leveraging test cases as verifiers, making it self-contained testbed for LLM development in code generation. 2.2 Dataset Overview Now lets examine the constructed LeetCodeDataset. LeetCode problems can be categorized along multiple dimensionswe highlight three key ones below: difficulty, release date, and topic tags. Difficulty Levels: As shown in Table 1, LeetCode problems are categorized by difficulty into three levels: Easy: Focuses on validating basic syntax and foundational data structure applications, typically solvable with straightforward logic. Medium: Requires familiarity with classical algorithms (e.g., dynamic programming, greedy) and the ability to design efficient strategies. Hard: Involves complex algorithmic combinations, mathematical insights, or specialized optimizations. Difficulty Release Year Type Count Proportion (%) Period Count Proportion (%) Easy Medium Hard 686 1498 686 23.91 52.21 23.88 Before 2020 20202022 2023 1077 1009 783 37.54 35.17 27.29 Table 1: Distribution of difficulty and release year on the LeetCodeDataset. Release Date: The release dates of LeetCode problems also offer valuable insights such as contamination-free evaluation of LLMs. Since LeetCodes weekly contest release dates and question IDs are publicly available, we use them as anchors to estimate each problems release date. As shown in Table 1, the yearly release distribution indicates approximately 350 new problems added annually in recent years. We argue that using problems from the past 612 months for benchmarking strikes an effective balance between bias and variance. Topic Tags: The LeetCode platform labels each problem with algorithm and data structure tags (e.g., Array, Binary Search), allowing multiple tags per problem. As shown in Figure 2, we examine how problems are distributed across these categories. This tagging system can help learners focus on specific skills. We believe this will provide insights to LLMs as well. Figure 2: Topic frequency distribution."
        },
        {
            "title": "3 Holistic Evaluation",
            "content": "We evaluate six models on the LeetCodeDataset test set, comprising 256 programming problems that were newly released after July 1, 2024. The evaluated models include two proprietary systems, GPT4o (OpenAI et al., 2024) and Claude 3.7 Sonnet (Anthropic, 2024); and four open-source models, DeepSeek-V3 (DeepSeek-AI et al., 2025b), DeepSeek-R1 (DeepSeek-AI et al., 2025a), Qwen2.5Max (Team, 2024), and QwQ-Plus (Team, 2025b). All experiments employ identical generation parameters with temperature=0.2 and top_p=0.95 to ensure fair comparisons. Following LiveCodeBenchs temporal evaluation methodology, we analyze monthly accuracy change relative to problem release months as shown in Figure 3, and summarize model pass rates across difficulty levels in Table 2. This approach identifies potential data contamination by detecting declines in post-release accuracy, which would indicate overfitting to pre-release training data. Our findings reveal three key insights: Superior Performance of Reasoning Models: The evaluation highlights DeepSeek-R1 (pass@1 rate = 65.23%) and QwQ-Plus (pass@1 rate = 56.25%) as top performers, demonstrating the substantial advantage of long-CoT reasoning models in solving complex competition-level coding problems. Baseline Comparison: Claude-3.7-Sonnet, operating without extended thinking, achieves superior performance within its model category. The two models, GPT-4o and DeepSeek5 V3, achieved the same overall score. GPT-4o performs slightly better on easy problems, while DeepSeek-V3 performs slightly better on hard problems. Contamination Analysis: The minimal temporal overlap between GPT-4o-0806s release date (August 2024) and our test problem release window (post-July 2024) strongly suggests authentic model capability measurements. We see similar curves among GPT-4o-0806, DeepSeek-V3, and Qwen2.5-Max; we believe the monthly accuracy fluctuations are mainly due to changes in problem difficulty. Figure 3: Monthly pass rates of various models on the LeetCodeDataset. Model Easy (%) Medium (%) Hard (%) Overall (%) GPT-4o-0806 Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-Max QwQ-Plus 81.48 87.04 77.78 94.44 74.07 92. 32.76 54.31 31.90 68.97 25.00 62.93 10.47 23.26 13.95 41.86 10.47 24.42 35.55 50.78 35.55 65.23 30.47 56.25 Table 2: Model pass rates by difficulty level on the LeetCodeDataset. We also analyze model pass rates across different topic tags, as depicted in Table 3. By comparing these results, we identify each models strengths and weaknesses, which provides insights for future improvements. Our key findings include: The reasoning model DeepSeek-R1 shows strong performance across all topic tags, with pass rates mostly ranging from 60% to 70% and minimal variation. In contrast, nonreasoning models like GPT-4o exhibit significant fluctuations, such as dropping to 7.7% in Binary Search tasks but reaching 63.2% in Simulation tasks. We observe significant performance differences between reasoning and non-reasoning models in Dynamic Programming, Binary Search, and Tree-related tasks. This pattern demonstrates the need for additional reasoning capabilities in these domains. GPT-4o DeepSeek -V3 Qwen2.5 -Max Claude-3.7 -Sonnet DeepSeek -R1 QwQ -Plus Array String Dynamic Programming Hash Table Math Greedy Sorting Prefix Sum Binary Search Sliding Window Enumeration Matrix Simulation Depth-First Search Bit Manipulation Combinatorics Counting Graph Heap (Priority Queue) Number Theory Breadth-First Search Tree Two Pointers Segment Tree All 32.1 37.3 10.5 39.5 38.2 12.5 20.0 17.9 7.7 52.2 27.3 19.0 63.2 31.6 33.3 12.5 20.0 40.0 40.0 38.5 41.7 27.3 20.0 30.0 35.5 34.5 38.8 15.8 37.5 40.0 15.6 20.0 14.3 23.1 47.8 31.8 33.3 57.9 21.1 44.4 18.8 26.7 33.3 53.3 30.8 33.3 18.2 30.0 30.0 35.5 28.0 35.8 8.8 35.7 32.7 12.5 6.7 14.3 11.5 43.5 9.1 19.0 42.1 26.3 27.8 12.5 26.7 46.7 33.3 30.8 50.0 9.1 30.0 30.0 30. 51.2 49.3 31.6 50.0 56.4 21.9 36.7 35.7 30.8 69.6 45.5 52.4 63.2 31.6 50.0 37.5 46.7 53.3 66.7 38.5 58.3 9.1 40.0 70.0 50.8 67.9 68.7 70.2 66.1 69.1 62.5 66.7 71.4 73.1 56.5 63.6 76.2 63.2 57.9 50.0 93.8 53.3 66.7 66.7 69.2 58.3 72.7 80.0 80.0 65.2 55.4 50.7 40.4 50.0 58.2 28.1 53.3 35.7 30.8 52.2 50.0 61.9 84.2 57.9 66.7 25.0 46.7 66.7 66.7 53.8 75.0 54.5 40.0 30.0 56.2 Table 3: Pass rates of models across topic tags."
        },
        {
            "title": "4 Efficient Training",
            "content": "4.1 Experiment Setup We conducted SFT using Qwen2.5-Coder-7B (Hui et al., 2024) as our base model. The model was trained for 3 epochs with an initial learning rate of 1e-5, employing warmup ratio of 0.1 and cosine learning rate scheduling. All experiments utilized consistent hyperparameters, including batch size of 32 across different datasets. 7 4.2 Results To evaluate the training efficiency of LeetCodeDataset, we conducted comparative experiments with five widely-used coding datasets (Wei et al., 2024; Luo et al., 2023; Penedo et al., 2025; Team, 2025a) ranging from 9.5K to 111.1K samples - all substantially larger than our LeetCodeDataset training set. Under identical experimental configurations above, we trained models on each dataset and evaluated them across four benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), LiveCodeBench (Jain et al., 2024), alongside our newly developed LeetCodeDataset evaluation set. As demonstrated in Table 4, we summarize our key findings: Superior Model-Generated Training Data. The SFT-trained model using model-generated responses from the pre-2024-07 LeetCodeDataset significantly outperformed the version trained on human-written responses (79.9% vs. 55.5% on HumanEval; 77.5% vs. 53.4% on MBPP), despite both response types being verified as correct. The result highlights the quality advantage of model-generated training data for code generation tasks. High Data Efficiency. Training with only 2.6K model-generated LeetCode samples achieved superior performance on HumanEval (79.9%) and MBPP (77.5%), surpassing models trained on much larger datasets (9.5K111.1K rows). The finding demonstrates exceptional data efficiency for domain-specific code generation. Limitations on Hard Benchmarks. Despite being in-distribution for LeetCodeDataset (post-2024-07), the 2.6K-trained model underperformed on hard benchmarks. It suggests that small-scale SFT primarily develops basic programming skills. Training Data Rows Human Eval MBPP LiveCode Bench 24-0825-02 LeetCode Dataset 24-0725-03 Magicoder Evol-Instruct-110K Magicoder OSS-Instruct-75K Open-R1 CodeForces-CoT OpenThoughts 114k LeetCodeDataset Pre 2024-07 human LeetCodeDataset Pre 2024-07 model"
        },
        {
            "title": "5 Related Work",
            "content": "111.1K 75.1K 9.5K 19.9K 2.6K 2.6K 77.4 73.8 79.9 77.4 55.5 79. 74.1 76.5 74.1 75.7 53.4 77. 15.1 15.1 15.8 16.9 14.0 15. Table 4: Model SFT-training results. 13.7 12.9 13.3 16.4 10. 12.5 Code Generation Benchmarks. Numerous benchmarks have been developed to evaluate the code generation capabilities of LLMs. For foundational Python programming, widely used benchmarks include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). EvalPlus (Liu et al., 2023) 8 offers more rigorous variant. Multiple-E (Cassano et al., 2022) further extends these two popular benchmarks by translating them into 18 other programming languages. As LLM capabilities advance, many of these benchmarks are becoming too easy to assess modern models adequately. few specialized benchmarks focus on competitive programming challenges. APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022), and TACO (Li et al., 2023) source problems from platforms like Codeforces and AtCoder. LiveCodeBench (Jain et al., 2024) provides holistic and contamination-free evaluations by dynamically updating coding challenges from platforms like LeetCode and AtCoder. CODEELO (Quan et al., 2025) tries to align with the CodeForces platform by submitting directly to the platform and developing an Elo rating calculation system. Fine-tuning Dataset of Code. Synthetic data is one primary source of LLM SFT data. CodeAlpaca (Chaudhary, 2023) employs few-shot prompting and teacher models to synthesize data for codespecific fine-tuning. Magicoder (Wei et al., 2024) leverages open-source code snippets to generate high-quality instructional data for coding tasks. In competitive programming benchmarks like APPS and CodeTest, training splits are provided for SFT, utilizing competition-level problems to enhance model problem-solving capabilities. For advanced reasoning, pen-R1 CodeForcesCoTs (Penedo et al., 2025) includes 10K CodeForces problems with up to five reasoning traces generated by DeepSeek R1. In contrast, OpenThoughts (Team, 2025a) is synthetic dataset with 114K high-quality examples spanning math, science, code, and puzzles."
        },
        {
            "title": "6 Limitations",
            "content": "While our LeetCode dataset effectively benchmarks and fine-tunes code models, it has three key limitations: False Positive Risks: Though we designed diverse inputs and test cases to reduce incorrect solutions passing, our dataset lacks extremely complex input patterns and suffers from an imbalanced test case distribution. These limitations present residual risks of false positives (e.g., solutions passing tests despite logic errors). Complexity Analysis Gap: Determining time/space complexity for problems requires LeetCodestyle test cases tailored to each algorithms behavior. The limitation exceeds our current scope as it demands manual problem-specific validation. Coverage Gaps: We havent included certain problem types, particularly problems with multiple solution entry points."
        },
        {
            "title": "7 Conclusion",
            "content": "We present LeetCodeDataset, rigorously curated resource that addresses key challenges in codegeneration research for large language models. By aggregating 2,869 Python LeetCode problemseach annotated with rich metadata (difficulty, tags, release dates) and augmented with 100+ diverse test casesour dataset enables reliable, contamination-free model evaluation and highly efficient training. Its temporal split (with post-July 2024 problems as the test set) ensures clean benchmarking and supports longitudinal studies. This dataset comprehensively covers algorithms and data structures, facilitating robust overall evaluation and fine-grained skill analysis. With an integrated evaluation toolkit, LeetCodeDataset streamlines assessment and comparison across models. Notably, we show that models trained on just 2.6K curated samples from LeetCodeDataset can match the performance of those trained on 110K examples from previous benchmarks, demonstrating strong data efficiency. We expect LeetCodeDataset to become foundational resource for developing, training, and evaluating advanced code-generation models."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/claude/sonnet. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021. URL https://arxiv.org/abs/2108.07732. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. Multipl-e: scalable and extensible approach to benchmarking neural code generation, 2022. URL https://arxiv.org/abs/2208.08227. Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https: //github.com/sahil280114/codealpaca, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish, et al. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. URL https://arxiv.org/abs/2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, et al. Deepseek-v3 technical report, 2025b. URL https://arxiv.org/abs/2412.19437. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/2403.07974. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022. 10 Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023. URL https://arxiv.org/abs/2305.01210. Yuliang Liu, Junjie Lu, Zhaoling Chen, Chaofeng Qu, Jason Klein Liu, Chonghan Liu, Zefan Cai, Yunhui Xia, Li Zhao, Jiang Bian, et al. Adaptivestep: Automatically dividing reasoning step through model confidence. arXiv preprint arXiv:2502.13943, 2025. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2023. URL https://arxiv.org/abs/2306.08568. OpenAI. Introducing openai o1-preview, September 2024. URL https://openai.com/index/ introducing-openai-o1-preview/. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, et al. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces cots. https://huggingface.co/datasets/open-r1/codeforces-cots, 2025. Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, and Junyang Lin. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings, 2025. URL https://arxiv.org/abs/2501.01257. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Wei Shen and Chuheng Zhang. Policy filtration in rlhf to fine-tune llm for code generation. arXiv preprint arXiv:2409.06957, 2024. Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan. Exploring data scaling trends and effects in reinforcement learning from human feedback. arXiv preprint arXiv:2503.22230, 2025. OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025a. Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https://qwenlm.github.io/blog/qwq-32b/. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with oss-instruct, 2024. URL https://arxiv.org/abs/2312.02120. Jing Zhou, Chenglin Jiang, Wei Shen, Xiao Zhou, and Xiaonan He. Leveraging web-crawled data for high-quality fine-tuning. arXiv preprint arXiv:2408.08003, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Prompts During input generation for entry points, we sample three tree, three linked list, and four other problem types, extracting input specifications from their descriptions to define entry points. These 10 selected problems serve as one-shot examples in the Input-Generation-Prompt, with domainspecific constraints: tree problems use only tree examples; linked list problems draw from linked list cases; others follow the same principle, ensuring generated inputs align with each problem types structural requirements. Input-Generation-Prompt You are an expert Python programmer. You will be given question (including problem specification and starter code). Your task is to generate inputs that are consistent with the problem specification and starter code. An example will be provided for illustration. **** Example **** #### Question: {example problem description and starter code} #### Some valid inputs of the starter code (json format): ```json {example problem inputs} ``` **** Now Your Task **** #### Question: {problem description and starter code} #### Some valid inputs of the starter code (json format): Figure 4: Prompt structure for input generation. Complex-Input-Generation-Prompt You are an expert Python programmer. You will be given question (including problem specification and starter code) along with few sample inputs. Your task is to generate additional inputs that are consistent with the question and the provided sample inputs. #### Question: {problem description and starter code} #### Sample inputs (using json format): ```json {sample inputs} ``` #### Generate some additional ing json format): inputs that are more complex than the sample inputs (usFigure 5: Prompt structure for complex input generation. 12 A.1.1 Handle Data Structures To ensure robust evaluation, we prepend essential imports (e.g., from typing import List) to all code completions. Special handling is required for binary tree and linked list data structures, which involve additional utility functions for serialization/deserialization. Below are the supplementary imports and helper functions used to manage these structures: 1 from typing import Optional 2 from collections import deque 3 4 5 class ListNode : 6 def __init__ ( self , val =0 , next = None ): 7 9 self . val = val self . next = next 10 11 def list_node ( values : list ) -> Optional [ ListNode ]: 12 if not values : return None head = ListNode ( values [0]) = head for val in values [1:]: node = ListNode ( val ) p. next = node = node return head 13 14 15 17 18 19 20 21 22 23 def linked_list_to_list ( head : Optional [ ListNode ]) -> list : result = [] current = head while current : result . append ( current . val ) current = current . next return result 25 26 28 29 30 31 32 def is_same_list ( p1 : Optional [ ListNode ], p2 : Optional [ ListNode ]) -> bool : 33 if p1 is None and p2 is None : 35 36 37 return True if not p1 or not p2 : return False return p1 . val == p2 . val and is_same_list ( p1 . next , p2 . next ) Figure 6: Additional imports related to linked list2. 13 1 from typing import Optional 2 from collections import deque 3 4 5 class TreeNode : 6 def __init__ ( self , val =0 , left = None , right = None ): 7 9 10 self . val = val self . left = left self . right = right 11 12 def tree_node ( values : list ) -> Optional [ TreeNode ]: 13 if not values : return None root = TreeNode ( values [0]) = 1 queue = deque () queue . append ( root ) while queue : node = queue . popleft () if < len ( values ) and values [i] is not None : node . left = TreeNode ( values [ ]) queue . append ( node . left ) += 1 if < len ( values ) and values [i] is not None : node . right = TreeNode ( values [i ]) queue . append ( node . right ) += return root 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 def tree_node_to_list ( root : Optional [ TreeNode ]) -> list : 33 if not root : return [] result = [] queue = deque () queue . append ( root ) while queue : node = queue . popleft () if node : result . append ( node . val ) queue . append ( node . left ) queue . append ( node . right ) else : result . append ( None ) while result and result [ -1] is None : result . pop () return result 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 def is_same_tree (p: Optional [ TreeNode ], q: Optional [ TreeNode ]) -> bool : 56 if not and not q: return True elif not or not q: return False elif p. val != q. val : return False 14 58 59 60 61 62 else : return is_same_tree (p. left , q. left ) and is_same_tree (p . right , . right ) Figure 7: Additional imports related to binary tree."
        }
    ],
    "affiliations": []
}