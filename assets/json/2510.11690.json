{
    "paper_title": "Diffusion Transformers with Representation Autoencoders",
    "authors": [
        "Boyang Zheng",
        "Nanye Ma",
        "Shengbang Tong",
        "Saining Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 0 9 6 1 1 . 0 1 5 2 : r a"
        },
        {
            "title": "DIFFUSION TRANSFORMERS\nWITH REPRESENTATION AUTOENCODERS",
            "content": "Boyang Zheng Nanye Ma Shengbang Tong Saining Xie New York University"
        },
        {
            "title": "ABSTRACT",
            "content": "Latent generative modeling, where pretrained autoencoder maps pixels into latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, lowdimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using DiT variant equipped with lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256 256 (no guidance) and 1.13 at both 256 256 and 512 512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training. Project page: rae-dit.github.io Figure 1: Representation Autoencoder (RAE) uses frozen pretrained representations as the encoder with lightweight decoder to reconstruct input images without compression. RAE enables faster convergence and higher-quality samples in latent diffusion training compared to VAE-based models."
        },
        {
            "title": "INTRODUCTION",
            "content": "The evolution of generative modeling has been driven by continual redefinition of where and how models learn to represent data. Early pixel-space models sought to directly capture image statistics, but the emergence of latent diffusion (Vahdat et al., 2021; Rombach et al., 2022) reframed generation as process operating within learned, compact representation space. By diffusing in this space rather than in raw pixels, models such as Latent Diffusion Models (LDM) (Rombach et al., 2022) and Diffusion Transformers (DiT) (Peebles & Xie, 2023; Ma et al., 2024) achieve higher visual fidelity and efficiency, powering the most capable image and video generators of today. Despite progress in diffusion backbones, the autoencoder defining the latent space remains largely unchanged. The widely used SD-VAE (Rombach et al., 2022) still relies on heavy channel-wise"
        },
        {
            "title": "Preprint",
            "content": "compression and reconstruction-only objective, producing low-capacity latents that capture local appearance but lack global semantic structure crucial for generalization and generative performance of diffusion models (Song et al., 2025). In addition, SD-VAE, built on legacy convolutional design, remains computationally inefficient (see Fig. 2). Meanwhile, visual representation learning has undergone rapid transformation. Self-supervised and multimodal encoders such as DINO (Oquab et al., 2023), MAE (He et al., 2021), JEPA (Assran et al., 2023) and CLIP / SigLIP (Radford et al., 2021; Tschannen et al., 2025) learn semantically structured visual features that generalize across tasks and scales and provide natural basis for visual understanding. However, latent diffusion remains largely isolated from this progress, continuing to diffuse in reconstruction-trained VAE spaces rather than semantically meaningful representational ones. Recent work attempts to improve latent quality indirectly through REPA-style (Yu et al., 2025; Yao et al., 2025; Leng et al., 2025) alignment with external encoders, but these methods introduce extra training stages, auxiliary losses, and tuning complexity. This separation stems from long-standing assumptions about the incompatibility between semantic and generative objectives. It is widely believed that encoders trained to capture semantics are unsuited for faithful reconstruction, since they focus on high-level information and can only reconstruct an image with high-level semantic similarities. (Yu et al., 2024b) In addition, diffusion models are believed to perform poorly in high-dimensional latent spaces (Skorokhodov et al., 2025; Yao et al., 2025; Esser et al., 2024; Liu et al., 2024), leading practitioners to favor low-dimensional VAE latents over the typically much higher-dimensional representations of semantic encoders. In this work, we show that both assumptions might be wrong. We demonstrate that frozen representation encoders, even those explicitly optimized for semantics over reconstruction, can be repurposed into powerful autoencoders for generation, yielding reconstructions superior to SD-VAE without architectural complexity or auxiliary losses. Furthermore, we find that diffusion transformer training can be stable and efficient in these higher-dimensional latent spaces. With the right architectural adjustments, higher-dimensional representations are not liability but an advantage, offering richer structure, faster convergence, and better generation quality. Notably, higher-dimensional latents introduce effectively no extra compute or memory costs since the token count is fixed (determined by the patch size) and the channels are projected to the DiT hidden dimension in the first layer. We formalize this insight through Representation Autoencoders (RAEs), new class of autoencoders that replace the VAE with pretrained representation encoder (e.g., DINO) paired with trained decoder. RAEs produce latent spaces that are semantically rich, structurally coherent, and diffusionfriendly, linking semantic and generative modeling through shared latent representation. While feasible in principle, adapting diffusion transformers to these high-dimensional semantic latents requires careful design. Original DiTs, designed for compact SD-VAEs, struggle with the increased dimensionality due to: (1) Transformer design: DiTs cannot fit even single image unless their width exceeds the token dimension, implying width must scale with latent dimensionality; (2) Noise scheduling: resolution-based schedule shifts (Chen, 2023; Hoogeboom et al., 2023; Esser et al., 2024), derived from pixeland VAE-based inputs, neglect token dimensionality, motivating dimensionality-dependent shift; (3) Decoder robustness: unlike VAEs trained on continuous latent distributions (Kingma & Welling, 2014), RAE decoders learn from discretely supported latents but must reconstruct samples from diffusion model that follow continuous distribution, which we address by noise-augmented decoder training. Finally, we introduce new DiT variant, DiTDH, inspired by DDT (Wang et al., 2025c) but motivated by different design perspective. It augments the standard DiT architecture with lightweight, shallow yet wide head, enabling the diffusion model to scale in width without incurring quadratic computational costs. Empirically, this design further enhances diffusion transformer training in high-dimensional RAE spaces. Empirically, RAEs demonstrate strong visual generation performance. On ImageNet, our RAEbased DiTDH achieves FIDs of 1.51 at 256256 without any guidance, and 1.13 at both 256256 and 512512 with AutoGuidance (Karras et al., 2025), showing the effectiveness of RAEs as an alternative to conventional VAEs in diffusion transformer training. More broadly, these results reframes autoencoding from compression mechanism into representation foundation, one that enables diffusion transformers to train more efficiently and generate more effectively."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Comparison of SD-VAE and RAE (DINOv2-B). The VAE relies on convolutional backbones with aggressive downand up-sampling, while the RAE uses ViT architecture without compression. SD-VAE is also more computationally expensive, requiring about 6 and 3 more GFLOPs than RAE for the encoder and decoder, respectively. GFlops are evaluated on one 256 256 image."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Here, we discuss previous work on the line of representation learning and reconstruction/generation. We present more detailed related work discussion in Section A. Representation for Reconstruction. Recent work explores enhancing VAEs with semantic representations: VA-VAE (Yao et al., 2025) aligns VAE latents with pretrained representation encoder, while MAETok (Chen et al., 2025a), DC-AE 1.5 (Chen et al., 2025d), and l-DEtok (Yang et al., 2025) incorporate MAEor DAE (Vincent et al., 2008)-inspired objectives into VAE training. Such alignment greatly improves reconstruction and generation performance of VAEs, yet its reliance on heavily compressed, low-dimensional latents still limits both reconstruction fidelity and representation quality. In contrast, we reconstruct directly from representation encoders features without compression. We show that, with simple ViT decoder on top of frozen representation encoders features, it achieves reconstruction quality comparable to or better than SD-VAE (Rombach et al., 2022), while preserving substantially stronger representations. Representation for Generation. Recent work also explores using semantic representations to improve generative modeling. REPA (Yu et al., 2025) accelerates DiT convergence by aligning its middle block with representation encoders features. DDT (Wang et al., 2025c) further improves convergence by decoupling DiT into an encoderdecoder and applying REPA loss to the encoder output. REG (Wu et al., 2025) introduces learnable token into the DiT sequence and explicitly aligns it with representation encoders representation. ReDi (Kouzelis et al., 2025b) generates both VAE latents and PCA components of DINOv2 features within diffusion model. In contrast, we train diffusion models directly on representation encoders and achieve faster convergence. We also recommend (Dieleman, 2025) for broader perspective on this topic."
        },
        {
            "title": "3 HIGH FIDELITY RECONSTRUCTION FROM FROZEN ENCODERS",
            "content": "In this section, we challenge the common assumption that pretrained representation encoders, such as DINOv2 (Oquab et al., 2023) and SigLIP2 (Tschannen et al., 2025), are unsuitable for the reconstruction task because they emphasize high-level semantics while downplaying low-level details (Tang et al., 2025; Yu et al., 2024b). We show that, with properly trained decoder, frozen representation encoders can in fact serve as strong encoders for the diffusion latent space. Our Representation Autoencoders (RAE) pair frozen, pretrained representation encoders with ViTbased decoder, yielding reconstructions on par with or even better than SD-VAE. More importantly, RAEs alleviate the fundamental limitations of VAEs (Kingma & Welling, 2014), whose heavily compressed latent space (e.g. SD-VAE maps 2562 images to 322 4 (Esser et al., 2021; Rombach et al., 2022) latents) restricts reconstruction fidelity and more importantly, representation quality. Our training recipe for the RAE decoder is as follows. Given an input R3HW and the frozen representation encoder with patch size pe and hidden size d, we obtain = HW/p2 tokens with channel d. ViT decoder with patch size pd maps them back to pixels with shape 3 pd ; By default we use pd = pe, so the reconstruction matches the input resolution. For pe all experiments on 256 256 images, the encoder produces 256 tokens, matching the token count of most prior DiT-based models trained with SD-VAE latents (Peebles & Xie, 2023; Yu et al., 2025; Ma et al., 2024). The decoder is trained with combination of L1, LPIPS (Zhang et al., 2018), pd pe"
        },
        {
            "title": "Preprint",
            "content": "Model rFID DINOv2-B 0.49 0.53 SigLIP2-B 0.16 MAE-B SD-VAE 0.62 Decoder rFID GFLOPs Encoder rFID Model Top-1 Acc. ViT-B ViT-L ViT-XL 0.58 0.50 0.49 22.2 78.1 106.7 SD-VAE 0.62 310.4 0.52 DINOv2-S DINOv2-B 0.49 DINOv2-L 0. DINOv2-B SigLIP2-B MAE-B SD-VAE 84.5 79.1 68.0 8.0 (a) Encoder choice. All encoders outperform SD-VAE. (b) Larger decoders improve rFID while remaining much more efficient than VAEs. (c) Encoder scaling. rFID is stable across RAE sizes. (d) Representation quality. RAEs have much higher linear probing accuracy than VAEs. Table 1: RAEs consistently outperform SD-VAE in reconstruction (rFID) and representation quality (linear probing accuracy) on ImageNet-1K, while being more efficient. If not specified, we use ViTXL as the decoder and DINOv2-B as the encoder for RAE. Default settings in this paper are in gray. and adversarial losses (Goodfellow et al., 2014), following common practice in VAEs: = E(x), ˆx = D(z) Lrec(x) = ωL LPIPS(ˆx, x)+ L1(ˆx, x) + ωGλ GAN(ˆx, x), We provide implementation details about the decoder architecture, hyperparameters, coefficients, and GAN training details in Appendix C. We select three representative encoders from different pretraining paradigms: DINOv2-B (Oquab et al., 2023) (pe=14, d=768), self-supervised self-distillation model; SigLIP2-B (Tschannen et al., 2025) (pe=16, d=768), language-supervised model; and MAE-B (He et al., 2021) (pe=16, d=768), masked autoencoder. For DINOv2, we also study different model sizes S,B,L (d=384,768,1024). Unless otherwise specified, we use an ViT-XL decoder for all RAEs. We use FID score (Heusel et al., 2017) computed on the reconstructed ImageNet (Russakovsky et al., 2015) validation set as our main metric for reconstruction quality, denoted as rFID. Reconstruction, scaling, and representation. As shown in Table 1a, RAEs with frozen encoders achieve consistently better reconstruction quality (rFID) than SD-VAE. For instance, RAE with MAE-B/16 reaches an rFID of 0.16, clearly outperforming SD-VAE and challenging the assumption that representation encoders cannot recover pixel-level detail. We next study the scaling behavior of both encoders and decoders. As shown in Table 1c, reconstruction quality remains stable across DINOv2-S, B, and L, indicating that even small representation encoders models preserve sufficient low-level detail for decoding. On the decoder side (Table 1b), increasing capacity consistently improves rFID: from 0.58 with ViT-B to 0.49 with ViT-XL. Importantly, ViT-B already outperforms SD-VAE while being 14 more efficient in GFLOPs, and ViT-XL further improves quality at only one-third of SD-VAEs cost. We also evaluate representation quality via linear probing on ImageNet-1K in Table 1d. Because RAEs use frozen pretrained encoders, they directly inherit the representation of the underlying representation encoders. Since RAEs use frozen pretrained encoders, they retain the strong representations of the underlying representation encoders. In contrast, SD-VAE achieves only 8% accuracy."
        },
        {
            "title": "4 TAMING DIFFUSION TRANSFORMERS FOR RAE",
            "content": "With RAE demonstrating good reconstruction quality, we now proceed to investigate the diffusability (Skorokhodov et al., 2025) of its latent space; that is, how easily its latent distribution can be modeled by diffusion model, and how good the generation performance can be. Before turning to generation, we fix the encoder to study generation capabilities. Table 1a shows that MAE, SigLIP2, and DINOv2 all achieve lower reconstruction rFID than SD-VAE, with MAE the best among them. However, reconstruction alone does not determine generation quality. Empirically, DINOv2 produces the strongest generation results, so unless otherwise noted, we adopt DINOv2 as our default encoder and defer the full comparison to Section G.1. Following standard practice, we adopt the flow matching objective (Lipman et al., 2023; Liu et al., 2023) with linear interpolation xt = (1 t)x + tε, where p(x) and ε (0, I), and train the model to predict the velocity v(xt, t) (see Section J). We use LightningDiT (Yao et al., 2025), variant of DiT (Peebles & Xie, 2023), as our model backbone. We adopt patch size of 1, which results in sequence length of 256 for all RAEs on 256 256 images, matching the token"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Overfitting to single sample. Left: increasing model width lead to lower loss and better sample quality; Right: changing model depth has marginal effect on overfitting results. length used by VAE-based DiTs (Peebles & Xie, 2023; Yu et al., 2025; Yao et al., 2025). Since the computational cost of DiT depends primarily on the sequence length, using DiT on RAE therefore effectively incurs no additional overhead compared to its VAE-based counterparts. We evaluate our models using FID computed on 50K samples generated with 50 steps with the Euler sampler (denoted as gFID), and all quantitative results are trained for 80 training epochs on ImageNet at 256 256 unless otherwise specified. More training details are included in Section D. DiT does not work out of the box. To our surprise, the standard diffusion recipe fails with RAE (see Table 2). Training directly on RAE latents causes small backbone such as DiT-S to completely fail, while larger backbone like DiT-XL significantly underperforms its counterpart with the SD-VAE latents. RAE SD-VAE DiT-S DiT-XL 215.76 23.08 51.74 7.13 To investigate this observation, we raise several hypotheses detailed below, which we will discuss in the following sections: Table 2: Standard DiT struggles to model RAEs latent distribution. Suboptimal design for diffusion transformers. When modeling high-dimensional RAE tokens, the optimal design choices for diffusion transformers can diverge from those of the standard DiT, which was originally tailored for low-dimensional VAE tokens. Suboptimal noise scheduling. Prior noise scheduling and loss re-weighting tricks are derived for image-based or VAE-based input, and it remains unclear if they transfer well to high-dimension semantic tokens. Diffusion generates noisy latents. VAE decoders are trained to reconstruct images from noisy latents, making them more tolerant to small noises in diffusion outputs. In contrast, RAE decoders are trained on only clean latents and may therefore struggle to generalize. 4.1 SCALING DIT WIDTH TO MATCH TOKEN DIMENSIONALITY To better understand the training dynamics of diffusion transformers with RAE latents, we first construct simplified experiment. Rather than training on the entire ImageNet, we randomly select single image, encode it by RAE, and test whether the diffusion model can reconstruct it. Table 2 shows that although RAE underperforms SD-VAE, DiT performance improves with increased capacity. To dissect this effect, we vary model width while fixing depth. Using DiT-S, we increase the hidden dimension from 384 to 784. As shown in Figure 3, sample quality is poor when the model width < token dimension = 768, but improves sharply and reproduces the input almost perfectly once n. Training losses exhibit the same trend, converging only when n. One might suspect that this improvement still arises from the larger model capacity. To disentangle this effect, we fix the width at = 384 and vary the depth of the SiT-S model. As shown in Figure 3, even when doubling the depth from 12 to 24, the generated images remain artifact-heavy, and the training losses shown in Figure 3 fail to converge to similar level of = 768."
        },
        {
            "title": "Preprint",
            "content": "Together, the results indicate that for generation in RAEs latent space to succeed, the diffusion models width must match or exceed the RAEs token dimension. At first glance, this appears to contradict the common belief that data manifolds have low intrinsic dimensionality (Pope et al., 2021), allowing generative models such as GANs to operate effectively within that manifold without scaling to the full data dimension (Sauer et al., 2022). We argue that this contradiction arises from the formulation of diffusion models (Section J): injecting Gaussian noise directly to the data (e.g., in the construction of xt) throughout training effectively extends the data distributions support to the entire space, thereby diffusing the data manifold into full-rank one, requiring model capacity that scales with the full data dimensionality. In the following, we provide theoretical justification for this conjecture: Theorem 1. Assuming p(x) Rn, ε (0, In), [0, 1]. Let xt = (1 t)x + tε, consider the function family Gd = {g(xt, t) = Bf (Axt, t) : Rdn, Rnd, : [0, 1] Rd Rd} (1) where < n, refers to stack of standard DiT blocks whose width is smaller than the token dimension from the representation encoder, and A, denote the input and output linear projections, respectively. Then for any Gd, L(g, θ) = (cid:90) 1 0 Exp(x),εN (0,In) (cid:2)g(xt, t) (ε x)2(cid:3)dt (cid:88) λi i=d+1 (2) where λi are the eigenvalues of the covariance matrix of the random variable = ε x. Notably, when n, Gd contains the unique minimizer to L(g, θ). Proof. See Section B.1. In our toy setting where p(x) = δ(x x0), we have (x, In) and λi = 1 for all i. Thus by (cid:80)n Theorem 1, the lower bound of the average loss becomes L(g, θ) 1 . As shown in Figure 3, this theoretical bound is consistent with our empirical results. i=d+1 1 = nd We further extend our investigation to more practical setting by examining three models of varying width, {DiT-S, DiT-B, DiT-L}. Each model is overfit on single image encoded by {DINOv2-S, DINOv2-B, DINOv2-L}, respectively, corresponding to different token dimensions. As shown in Table 3, convergence occurs only when the model width is at least as large as the token dimension (e.g., DiT-B with DINOv2-B), while the loss fails to converge otherwise (e.g., DiT-S with DINOv2-B). DiT-S DiT-B DiT-L DINOv2-S 3.6e2 1.0e3 9.7e4 DINOv2-B 5.2e1 2.4e2 1.3e3 DINOv2-L 6.5e1 2.7e1 2.2e2 Table 3: Overfitting losses. Compared between different combinations of model width and token dimension. Suboptimal design for diffusion transformers. We now fix the width of DiT to be at least as large as the RAE token dimension. For RAE with the DINOv2-B encoder, we pair it with DiT-XL in our following experiments. 4.2 DIMENSION-DEPENDENT NOISE SCHEDULE SHIFT Many prior works (Teng et al., 2023; Chen, 2023; Hoogeboom et al., 2023; Esser et al., 2024) have observed that, for inputs RCHW , increasing the spatial resolution (H ) reduces information corruption at the same noise level, impairing diffusion training. These findings, however, are based mainly on pixelor VAE-encoded inputs with few channels (e.g., 16). In practice, the Gaussian noise is applied to both spatial and channel dimensions; as the number of channels increases, the effective resolution per token also grows, reducing information corruption further. We therefore argue that proposed resolution-dependent strategies in these prior works should be generalized to the effective data dimension, defined as the number of tokens times their dimensionality."
        },
        {
            "title": "Preprint",
            "content": "αtn 1+(α1)tn We adopt the shifting strategy of Esser et al. (2024): for schedule tn [0, 1] and input dimensions n, m, the shifted timestep is where α = (cid:112)m/n is dimensiondefined as tm = dependent scaling factor. We follow (Esser et al., 2024) in using = 4096 as the base dimension and set to the effective data dimension of RAE. As shown in Table 4, this yields significant performance gains, showing its importance for training diffusion models in the high-dimensional RAE latent space. gFID 23.08 4.81 w/o shift w/ shift Table 4: Impact of schedule shift. Suboptimal noise scheduling. We now default the noise schedule to be dependent on the effective data dimension for all our following experiments."
        },
        {
            "title": "4.3 NOISE-AUGMENTED DECODING",
            "content": "Unlike VAEs, where latent tokens are encoded as continuous distribution (µ, σ2I) (Kingma & Welling, 2014), the RAE decoder is trained to reconstruct images from the discrete distribution p(z) = (cid:80) δ(x zi), where {zi} denotes the training set processed by the RAE encoder E. At inference time, however, the diffusion model may generate latents that are noisy or deviate slightly from the training distribution due to imperfect training and sampling Abuduweili et al. (2024). This could introduce notable out-of-distribution challenge for D, which degrades sampling quality. To mitigate this issue, inspired by prior works on Normalizing Flows (Dinh et al., 2017; Ho et al., 2019; Zhai et al., 2025), we augment the RAE decoder training with an additive noise (0, σ2I). Concretely, rather than decoding directly from the clean latent distribution p(z), we train on smoothed distribution pn(z) = (cid:82) p(z n)N (0, σ2I)(n)dn to enhance the decoders generalization to the denser output space of diffusion models. We further introduce stochasticity into σ by sampling it from (0, τ 2), which helps regularize training and improve robustness. gFID rFID p(z) pn(z) 4.81 4.28 0.49 0. Table 5: Impact of pn(z). We analyze how pn(z) affects reconstruction and generation. As shown in Table 5, it improves gFID but slightly worsens rFID. This trade-off is expected: adding noise smooths the latent distribution and, therefore, helps reduce OOD issues for the decoder, but also removes fine details, lowering reconstruction quality. We conduct more ablation experiments on τ and different encoders in Section G.2. Diffusion generates noisy latents. We now adopt the noise-augmented decoding for all our following experiments. We combine all of the above techniques to train DiT-XL model on RAE latents. Our improved diffusion recipe achieves gFID of 4.28 (Figure 4) after only 80 epochs and 2.39 after 720 epochs in RAEs latent space. With same model size, this not only surpasses prior diffusion baselines (e.g., SiT-XL (Ma et al., 2024)) trained on VAE latents (achieving 47 training speedup), but also outperforms the convergence speed of recent methods based on representation alignment (e.g., REPA-XL (Yu et al., 2025)), achieving 16 training speedup. In the following sections, we investigate ways to make RAE generation more efficient and effective, pushing it toward state-of-the-art performance. Figure 4: DiT w/ RAE reaches much faster convergence and better FID than SiT or REPA."
        },
        {
            "title": "Preprint",
            "content": "(a) DiTDH scales much better than DiT with RAE latents. (b) DiTDH with RAE converges faster than VAE-based methods. (c) DiTDH with RAE reaches better FID than VAE-based methods at all model scales. Bubble area indicates the flops of the model. Figure 6: Scalability of DiTDH. With RAE latents, DiTDH scales more efficiently in both training compute and model size than RAE-based DiT and VAE-based methods."
        },
        {
            "title": "IMPROVING THE MODEL SCALABILITY WITH WIDE DIFFUSION HEAD",
            "content": "As discussed in Section 4, within the standard DiT framework, handling higher-dimensional RAE latents requires scaling up the width of the entire backbone, which quickly becomes computationally expensive. To overcome this limitation, we draw inspiration from DDT (Wang et al., 2025c) and introduce the DDT heada shallow yet wide transformer module dedicated to denoising. By attaching this head to standard DiT, we effectively increase model width without incurring quadratic growth in FLOPs. We refer to this augmented architecture as DiTDH throughout the remainder of the paper. We also conduct experiment of the design choice of DDT head in Section G.3 Wide DDT head. Formally, DiTDH model consists of base DiT and an additional wide, shallow transformer head H. Given noisy input xt, timestep t, and an optional class label y, the combined model predicts the velocity vt as Figure 5: The Wide DDT Head. zt = (xt t, y), vt = H(xt zt, t), DiTDH converges faster than DiT. We train series of DiTDH models with varying backbone sizes (DiTDH-S, B, L, and XL) on RAE latents. We use 2-layer, 2048-dim DDT head for all DiTDH models. Performance is compared against the standard DiT-XL baseline. As shown in Figure 6a, DiTDH is substantially more FLOP-efficient than DiT. For example, DiTDH-B requires only 40% of the training FLOPs yet outperforms DiT-XL by large margin; when scaled to DiTDH-XL under comparable training budget, DiTDH achieves an FID of 2.16nearly half that of DiT-XL. DINOv Model 3.50 DiT-XL DiTDH-XL 2.42 4.28 2.16 6.09 2.73 Table 6: DiTDH outperforms DiT across RAE encoder sizes. DiTDH maintains its advantage across RAE scales. We compare DiTDH-XL and DiT-XL on three RAE encodersDINOv2S, DINOv2-B, and DINOv2-L. As shown in Table 6, DiTDH consistently outperforms DiT, and the advantage grows with encoder size. For example, with DINOv2-L, DiTDH improves FID from 6.09 to 2.73. We attribute this robustness to the DDT head. Larger encoders produce higher-dimensional latents, which amplify the width bottleneck of DiT. DiTDH addresses this by satisfying the width requirement discussed in Section 4 while keeping features compact. It also filters out noisy information that becomes more prevalent in high-dimensional RAE latents."
        },
        {
            "title": "Preprint",
            "content": "Method Epochs #Params Generation@256 w/o guidance Generation@256 w/ guidance gFID IS Prec. Rec. gFID IS Prec. Rec. Autoregressive VAR (Tian et al., 2024) MAR (Li et al., 2024b) xAR (Ren et al., 2025) Pixel Diffusion ADM (Dhariwal & Nichol, 2021) RIN (Jabri et al., 2023) PixelFlow (Chen et al., 2025e) PixNerd (Wang et al., 2025b) SiD2 (Hoogeboom et al., 2025) Latent Diffusion with VAE DiT (Peebles & Xie, 2023) MaskDiT (Zheng et al.) SiT (Ma et al., 2024) MDTv2 (Gao et al., 2023) VA-VAE (Yao et al., 2025) REPA (Yu et al., 2025) DDT (Wang et al., 2025c) REPA-E (Leng et al., 2025) Latent Diffusion with RAE (Ours) DiT-XL (DINOv2-S) DiTDH-XL (DINOv2-B) 350 800 800 400 480 320 160 1280 1400 1600 1400 1080 80 800 80 800 80 80 800 800 20 80 800 2.0B 943M 1.1B 554M 410M 677M 700M - 675M 675M 675M 675M 675M 675M 675M 675M 676M 839M 1.92 2.35 - 323.1 227.8 - 10.94 3.42 - - - 9.62 5.69 8.61 - 4.29 2.17 7.90 5. 6.62 6.27 3.46 1.70 1.87 3.71 2.16 1.51 101.0 182.0 - - - 121.5 177.9 131.7 - - 205.6 122.6 158.3 135.2 154.7 159.8 217.3 209.7 198.7 214.8 242. 0.82 0.79 - 0.69 - - - - 0.67 0.74 0.68 - - 0.77 0.70 0.70 0.69 0. 0.77 0.77 0.80 0.86 0.82 0.79 0.59 0.62 - 0.63 - - - - 0.67 0.60 0.67 - - 0.65 0.65 0.68 0.67 0.69 0.63 0.66 0.63 0.50 0.59 0. 1.73 1.55 1.24 3.94 - 1.98 2.15 1.38 2.27 2.28 2.06 1.58 - 1.35 - 1.29 1.52 1. 1.67 1.15 1.41 1.13 350.2 303.7 301.6 215.8 - 282.1 297.0 - 278.2 276.6 270.3 314. - 295.3 - 306.3 263.7 310.6 266.3 304.0 309.4 262. 0.82 0.81 0.83 0.83 - 0.81 0.79 - 0.83 0.80 0.82 0.79 - 0.79 - 0.79 0.78 0. 0.80 0.79 0.80 0.78 0.60 0.62 0.64 0.53 - 0.60 0.59 - 0.57 0.61 0.59 0. - 0.65 - 0.64 0.63 0.65 0.63 0.66 0.63 0. Table 8: Class-conditional performance on ImageNet 256256. RAE reaches an FID of 1.51 without guidance, outperforming all prior methods by large margin. It also achieves an FID of 1.13 with AutoGuidance (Karras et al., 2025). We identified an inconsistency in the FID evaluation protocol in prior literature and re-ran the sampling process for several baselines. This resulted in higher baseline numbers than those originally reported. Further details are discussed in Section 5.1. 5.1 STATE-OF-THE-ART DIFFUSION TRANSFORMERS Convergence. We compare the convergence behavior of DiTDH-XL with previous state-of-theart diffusion models (Peebles & Xie, 2023; Ma et al., 2024; Yu et al., 2025; Gao et al., 2023; Yao et al., 2025) in terms of FID without guidance. In Figure 6b, we show the convergence curve of DiTDH-XL with training epochs/GFLOPs, while baseline models are plotted at their reported final performance. DiTDH-XL already surpasses REPAXL, MDTv2-XL, and SiT-XL around 5 1010 GFLOPs, and by 5 1011 GFLOPs it achieves the best FID overall, requiring over 40 less compute. Method Generation@512 gFID IS Prec. Rec. BigGAN-deep (Brock et al., 2019) 8.43 177.9 0.88 StyleGAN-XL (Sauer et al., 2022) 2.41 267.8 0.77 0.29 0.52 VAR (Tian et al., 2024) MAGVIT-v2 (Yu et al., 2024a) XAR (Ren et al., 2025) 2.63 303.2 1.91 324.3 1.70 281.5 - - - - - - ADM SiD2 3.85 221.7 0.84 - 1.50 - 0.53 - DiT SiT DiffiT (Hatamizadeh et al., 2024) REPA DDT EDM2 (Karras et al., 2024) DiTDH-XL (DINOv2-B) 3.04 240.8 0.84 2.62 252.2 0.84 2.67 252.1 0.83 2.08 274.6 0.83 1.28 305.1 0.80 - 1. 0.54 0.57 0.55 0.58 0.63 - Scaling. We compare DiTDH with recent methods of different model at different scales. As shown in Figure 6c, increasing the size of DiTDH consistently improves the FID scores. The smallest model, DiTDH-S, reaches competitive FID of 6.07, already outperforming the much larger REPA-XL. When scaling from DiTDH-S to DiTDHB, the FID improves significantly from 6.07 to 3.38, surpassing all prior works of similar or even Table 7: Class-conditional performance on ImageNet 512512 with guidance. DiTDH with 400epoch training achieves an strong FID score of 1.13. 1.13 259.6 0.80 0.63 -"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Qualitative samples from our model trained at 512 512 resolution with AutoGuidance. The RAE-based DiT demonstrates strong diversity, fine-grained detail, and high visual quality. larger scale. The performance continues to improve with DiTDH-XL, setting new state-of-the-art result of 2.16 at 80 training epochs. Performance. Finally, we provide quantitative comparison between DiTDH-XL, our most performant model, with recent state-of-the-art diffusion models on ImageNet 256 256 and 512 512 in Table 8 and Table 7. Our method outperforms all prior diffusion models by large margin, setting new state-of-the-art FID scores of 1.51 without guidance and 1.13 with guidance at 256 256. On 512 512, with 400-epoch training, DiTDH-XL further achieves an FID of 1.13 with guidance, surpassing the previous best performance achieved by EDM-2 (1.25). Following the quantitative results, we present qualitative samples from our best models in Figure 7. These visualizations exhibit both high semantic diversity and fine-grained details comparable to ground-truth ImageNet samples, consistent with the achieved state-of-the-art FID. We provide additional visualization samples in Section and unconditional generation results in Section L. Remarks on FID evaluation. To construct the 50,000 samples used for conditional FID evaluation, we note that previous works such as DDT (Wang et al., 2025c), VAR (Tian et al., 2024), MAR (Li et al., 2024b), and xAR (Ren et al., 2025) typically evaluate using exactly 50 images per class, while others employ uniform random sampling across the 1,000 class labels. Although uniform random sampling asymptotically approaches the balanced sampling case, we surprisingly observed that class-balanced sampling consistently achieves around 0.1 lower FID scores. To ensure fair comparison, we re-evaluate several recent methods with accessible checkpoints, such as SiT (Peebles & Xie, 2023), REPA (Yu et al., 2025), REPA-E (Leng et al., 2025) using class-balanced sampling and update their reported scores accordingly. more detailed comparison is provided in Section E."
        },
        {
            "title": "6 DISCUSSIONS",
            "content": "6.1 HOW CAN RAE EXTEND TO HIGH-RESOLUTION SYNTHESIS EFFICIENTLY? central challenge in generating high-resolution images is that resolution scales with the number of tokens: doubling image size in each dimension requires roughly four times as many tokens. To address this, we let the decoder handle resolution scaling by allowing its patch size patch size pd"
        },
        {
            "title": "Preprint",
            "content": "to differ from the encoder patch size pe. When pd = pe, the output matches the input resolution; setting pd = 2pe produces 2 upsampled image, reconstructing 512 512 image from the same tokens used at 256 256. Since the decoder is decoupled from both the encoder and the diffusion process, we can reuse diffusion models trained at 256 256 resolution, simply swapping in an upsampling decoder to produce 512 512 outputs without retraining. As shown in Table 9, this approach slightly increases rFID but achieves competitive gFID, while being 4 more efficient than quadrupling the number of tokens."
        },
        {
            "title": "6.2 DOES DITDH WORK WITHOUT RAE?",
            "content": "Method #Tokens gFID rFID Direct Upsample 1024 1.13 1.61 0.53 0.97 Table 9: Comparison on ImageNet 512 512. Decoder upsampling achieves competitive FID compared to direct 512-resolution training. Both models are trained for 400 epochs. In this work, we propose and study RAE and DiTDH. In Section 4, we showed that RAE with DiT already brings substantial benefits, even in the absence of DiTDH. Here, we turn the question around: can DiTDH still provide improvements, without the latent space of RAE? VAE DINOv2-B DiT-XL 7.13 DiTDH-XL 11. 4.28 2.16 Table 10: Performance on VAE. DiTDH yields worse FID than DiT, despite using extra compute for the wide DDT head. To investigate, we train both DiT-XL and DiTDH-XL on SD-VAE latents with patch size of 2, alongside DINOv2-B for comparison, for 80 epochs, and report unguided FID. As shown in Table 10, DiTDH-XL performs even worse than DiT-XL on SD-VAE, despite the additional computation introduced by the diffusion head. This indicates that the DDT head provides little benefit in lowdimensional latent spaces, and its primary strength arises in high-dimensional diffusion tasks introduced by RAE. 6.3 THE ROLE OF STRUCTURED REPRESENTATION IN HIGH-DIMENSIONAL DIFFUSION? DiTDH achieves strong performance when paired with the high-dimensional latent space of RAE. This raises key question: is the structured representation of RAE essential, or would DiTDH work equally well on unstructured high-dimensional inputs such as raw pixels? To evaluate this, we train DiT-XL and DiTDH-XL directly on raw pixels. For 256 256 images with patch size of 16, the resulting DiT input token dimensionality is 16 16 3 = 768, matching that of the DINOv2-B latents. We report unguided FID after 80 epochs. As shown in Table 11, DiTDH outperforms DiT on pixels, but both models perform far worse than their counterparts trained on RAE latents. These results demonstrate that high dimensionality alone is not sufficient: the structured representation provided by RAE is crucial for achieving strong performance gains. Table 11: Comparison on pixel diffusion. Pixel Diffusion has much worse FID than diffusion on DINOv2-B. 51.09 DiT-XL DiTDH-XL 30.56 Pixel DINOv2-B 4.28 2."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we challenge the belief that pretrained representation encoders are too highdimensional and too semantic for reconstruction or generation. We show that frozen representation encoder, paired with lightweight trained decoder, forms an effective Representation Autoencoder (RAE). On this latent space, we train Diffusion Transformers in stable and efficient way with three added components: (1) match DiT width to the encoder token dimensionality; (2) apply dimension-dependent shift to the noise schedule; and (3) add decoder noise augmentation so the decoder handles diffusion outputs. We also introduce DiTDH, shallow-but-wide diffusion transformer head that increases width without quadratic compute. Empirically, RAEs enable strong visual generation: on ImageNet, our RAE-based DiTDH-XL achieves 1.51 FID at 256 256 (without guidance) and 1.13 at both 256 256 and 512 512 (with guidance). We believe RAE latents serve as strong candidate for training diffusion transformers in the future."
        },
        {
            "title": "8 ACKNOWLEGMENTS",
            "content": "The authors would like to thank Sihyun Yu, Jaskirat Singh, Shusheng Yang, Xichen Pan, Chenyu Li, Bingda Tang, Fred Lu, David Fan, Amir Bar, Shangyuan Tong for insightful discussions and feedback on the manuscript. This work was mainly supported by the Google TPU Research Cloud (TRC) program and the Open Path AI Foundation. ST is supported by Meta AI Mentorship Program. SX also acknowledges support from the MSIT IITP grant (RS-2024-00457882) and the NSF award IIS-2443404."
        },
        {
            "title": "REFERENCES",
            "content": "Abulikemu Abuduweili, Chenyang Yuan, Changliu Liu, and Frank Permenter. Enhancing sample generation of diffusion models using noise level correction. TMLR, 2024. 7 Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 20, 27, 28 Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In CVPR, 2023. 2 Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In ICLR, 2019. 9 Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficientvqgan: Towards high-resolution image generation with efficient vision transformers. In ICCV, 2023. 19 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 22 Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In ICML, 2025a. 3 Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, 2025b. 19 Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. In ICLR, 2025c. Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, and Han Cai. Dc-ae 1.5: Accelerating diffusion model convergence with structured latent space, 2025d. 3 Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020a. 19 Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025e. Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972, 2023. 2, 6 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020b. 19 Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In ICLR, 2025. 22 Jeffrey De Fauw, Sander Dieleman, and Karen Simonyan. Hierarchical autoregressive image models with auxiliary decoders. arXiv preprint arXiv:1903.04933, 2019. 19 Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. 9, 24, 27, 28 Sander Dieleman. Generative modelling in latent space, 2025. URL https://sander.ai/ 2025/04/15/latents.html."
        },
        {
            "title": "Preprint",
            "content": "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In ICLR, 2017. 7 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 19, 22 Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 3, 19, Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, 6, 7, 27 David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, et al. Scaling language-free visual representation learning. In ICCV, 2025a. 19 Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. In ICLR, 2025b. 19 Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 9 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 4, 22 Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 19 Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from scaling visual tokenizers for reconstruction and generation. In ICML, 2025. 19 Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. DiffiT: Diffusion vision transformers for image generation. In ECCV, 2024. 9 Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arxiv e-prints, art. In CVPR, 2019. 19 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2021. 2, 4, 19, 22 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 4, 28 Geoffrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786), 2006. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. 27 Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flowbased generative models with variational dequantization and architecture design. In ICML, 2019. 7 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 19, 27 Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. 2,"
        },
        {
            "title": "Preprint",
            "content": "Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. In CVPR, 2025. 9 Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In ICML, 2023. 9 Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In NeurIPS, 2022. 27 Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In CVPR, 2024. 9 Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. In NeurIPS, 2025. 2, 9, 27 Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2, 3, 7, 19 Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Eq-vae: Equivariance regularized latent space for improved generative image modeling. In ICML, 2025a. 19 Theodoros Kouzelis, Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos KoIn NeurIPS, modakis. Boosting generative image modeling via joint image-feature synthesis. 2025b. 3 Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. precision and recall metric for assessing generative models. In NeurIPS, 2019. 28 Improved Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In NeurIPS, 2024. 27 Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. In ICCV, 2025. 2, 9, 10, 25 Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. In NeurIPS, 2024a. 28 Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024b. 9, 10, 19, 24, 25 Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. 4, 27 Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models, 2024. 2 Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 4, 27 Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. 1, 3, 7, 9, 19, 24, 25, 27, 28 Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In ICLR, 2024. 19 Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021."
        },
        {
            "title": "Preprint",
            "content": "Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017. 19 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. TMLR, 2023. 2, 3, 4 Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries, 2025. 19 Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, 2018. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 1, 3, 4, 5, 9, 10, 19, 24 Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. In ICLR, 2021. 6 Kai Qiu, Xiang Li, Hao Chen, Jason Kuen, Xiaohao Xu, Jiuxiang Gu, Yinyi Luo, Bhiksha Raj, Zhe Lin, and Marios Savvides. Image tokenizer needs post-training, 2025. URL https://arxiv. org/abs/2509.12474. 19 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, Vivek Ramanujan, Kushal Tirumala, Armen Aghajanyan, Luke Zettlemoyer, and Ali Farhadi. When worse is better: Navigating the compression-generation tradeoff in visual tokenization. In NeurIPS, 2025. 19 Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 19 Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In NeurIPS, 2019. 19 Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond nexttoken: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. 9, 10, 24, 25 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 3, 19 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211252, 2015. 4, 23, 24 Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NeurIPS, 2016. 28 Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In SIGGRAPH, 2022. 6, Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In ICML, 2023. 22, 23 Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin. Improving the diffusability of autoencoders. In ICML, 2025. 2, 4 Kiwhan Song, Jaeyeon Kim, Sitan Chen, Yilun Du, Sham Kakade, and Vincent Sitzmann. Selective underfitting in diffusion models. arXiv preprint arXiv:2510.01378, 2025."
        },
        {
            "title": "Preprint",
            "content": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 23 Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024. 19 Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception architecture for computer vision. In CVPR, 2016. 28 Hao Tang, Chenwei Xie, Xiaoyi Bao, Tingyu Weng, Pandeng Li, Yun Zheng, and Liwei Wang. Unilip: Adapting clip for unified multimodal understanding, generation and editing, 2025. 3, 19 Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. In ICLR, Relay diffusion: Unifying diffusion process across resolutions for image synthesis. 2023. 6 Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. 9, 10, 24, 25 Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. In ICCV, 2025. 19 Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. 2, 3, 4, 19 Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In NeurIPS, 2021. 1 Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. 3, 19 Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, and Abhinav Shrivastava. Larp: Tokenizing videos with learned autoregressive generative prior. In ICLR, 2025a. 19 Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025b. 9 Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer, 2025c. 2, 3, 8, 9, 10, 24, 25 Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, and Xiaohua Zhai. Scaling pre-training to one hundred billion data for vision language models. arXiv preprint arXiv:2502.07617, 2025d. 19 Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, Ming-Ming Cheng, and Xiang Li. Representation entanglement for generation:training diffusion transformers is much easier than you think, 2025. 3 Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, and Yue Wang. Latent denoising makes good visual tokenizers, 2025. 3, 19 Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. 2, 3, 4, 5, 9, 23, 24 Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. TMLR, 2022."
        },
        {
            "title": "Preprint",
            "content": "Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusion tokenizer is key to visual generation. In ICLR, 2024a. 9 Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. In NeurIPS, 2024b. 2, 3, 19 Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. 2, 3, 5, 7, 9, 10, 19, 24, 25 Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. In ICML, 2025. 7 Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 3, 22 Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. In NeurIPS, 2020. 23 Yue Zhao, Yuanjun Xiong, and Philipp Krahenbuhl. Image and video tokenization with binary spherical quantization. In ICLR, 2025. 19 Anlin Zheng, Xin Wen, Xuanyang Zhang, Chuofan Ma, Tiancai Wang, Gang Yu, Xiangyu Zhang, and Xiaojuan Qi. Vision foundation models as effective visual tokenizers for autoregressive image generation, 2025. 19 Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. In NeurIPS, 2022. 19 Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. TMLR. Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021. 19 ibot: Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. 19 Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, and Lidong Bing. Stabilize the latent space for image autoregressive modeling: unified perspective. In NeurIPS, 2024."
        },
        {
            "title": "A EXTENDED RELATED WORK",
            "content": "Representation encoder as autoencoder. Recent studies have investigated leveraging semantic representations for reconstruction, particularly in MLLMs where diffusion decoders are conditioned on semantic tokens (Sun et al., 2024; Chen et al., 2025b; Pan et al., 2025; Tong et al., 2025). While this improves visual quality, the reliance on large pretrained diffusion decoders makes reconstructions less faithful to the input, limiting their effectiveness as true autoencoders. Very recently, UniLIP (Tang et al., 2025) employs one-step convolutional decoder on top of InternViT (Zhu et al., 2025), achieving reconstruction quality surpassing SD-VAE. However, UniLIP relies on additional large-scale fine-tuning of pretrained ViTs, arguing that frozen pretrained representation encoders lacks sufficient visual detail. In contrast, we show this is not the case: frozen representation encoders achieves comparable reconstruction performance while enabling much faster convergence in diffusion training. Another line of related work also try to utilize representation encoders directly as tokenizers. VFMTok (Zheng et al., 2025) and DiGIT (Zhu et al., 2024) applies vector-quantization directly to pretrained representation encoders like Dino or SigLIP. These approaches transform representation encoders into an effective tokenizer for AR models, but still suffer from the information capacity bottleneck brought by quantization. Compressed image tokenizers. Autoencoders have long been used to compress images into low-dimensional representations for reconstruction (Hinton & Salakhutdinov, 2006; Vincent et al., 2008). VAEs (Kingma & Welling, 2014) extend this paradigm by mapping inputs to Gaussian distributions, while VQ-VAEs (Oord et al., 2017; Razavi et al., 2019) introduce discrete latent codes. VQGAN (Esser et al., 2021) adds adversarial objectives, and ViT-VQGAN (Esser et al., 2021; Cao et al., 2023) modernizes the architecture with Vision Transformers (ViTs) (Dosovitskiy et al., 2021). Other advances include multi-stage quantization (Lee et al., 2022; Zheng et al., 2022), lookup-free schemes (Mentzer et al., 2024; Zhao et al., 2025), token-efficient designs such as TiTok and DCAE (Yu et al., 2024b; Chen et al., 2025c), and structure-preserving approaches like EQVAE (Kouzelis et al., 2025a). (Hansen-Estruch et al., 2025) further explores the scaling behavior of VAEs. LARP, CRT, REPA-E (Wang et al., 2025a; Ramanujan et al., 2025; Yu et al., 2025) tried to improve VAE with generative priors via back-propagation. In contrast, we dispense with aggressive compression and instead adopt pretrained representation encoders as encoder. This avoids the encoder collapsing into shallow features optimized only for reconstruction loss, while providing strong pretrained representations that serve as robust latent space. Generative models. Modern image generation is dominated by two paradigms: autoregressive (AR) models and diffusion models. AR models (De Fauw et al., 2019; Ramesh et al., 2022; Yu et al., 2022; Parmar et al., 2018; Chen et al., 2020a) generate images sequentially, token by token, and benefit from powerful language-model architectures but often suffer from slow sampling. Diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021; Rombach et al., 2022; Ma et al., 2024; Peebles & Xie, 2023) instead learn to iteratively denoise noisy signals, offering superior sample quality and scalability, though at the cost of many sampling steps. In this work, we build on diffusion models but adapt them to high-dimensional latent spaces provided by pretrained representation encoders. We find representation encoders provide faster convergence and improved scaling behavior. Robust decoders for generation. Recent works suggest that incorporating masking or latent denoising losses can improve tokenizer training. l-DeTok (Yang et al., 2025) shows that combining both losses yields strong VAEs for second-stage MAR (Li et al., 2024b; Fan et al., 2025b) generation, while RobusTok (Qiu et al., 2025) demonstrates that training with perturbed tokens makes decoders more robust. Notably, Yang et al. (2025) report that denoising loss only increase performance when encoder and decoder are jointly trained, whereas we observe substantial gains in generation quality with frozen encoders. Visual representation learning. Visual representations primarily fall into two broad families: self-supervised encoders that learn invariances from augmented views or masked prediction (He et al., 2019; Chen et al., 2020b; Grill et al., 2020; Zhou et al., 2021; He et al., 2021), and languagesupervised encoders trained from imagetext pairs (Radford et al., 2021; Zhai et al., 2023; Chen et al., 2024; Tschannen et al., 2025). Recent work (Fan et al., 2025a; Tschannen et al., 2025; Wang"
        },
        {
            "title": "Preprint",
            "content": "et al., 2025d) scales both lines on billion-scale corpora, yielding robust, semantically rich features. In this work, we leverage these pretrained encoders directly: we freeze the encoder, and train lightweight decoder to form Representation Autoencoder (RAE). We show that such RAEs are effective for both reconstruction and generation."
        },
        {
            "title": "B PROOFS",
            "content": "B.1 PROOF OF LOWER BOUND FOR TRAINING LOSS Theorem 1. Assuming p(x) Rn, ε (0, In), [0, 1]. Let xt = (1 t)x + tε, consider the function family Gd = {g(xt, t) = Bf (Axt, t) : Rdn, Rnd, : [0, 1] Rd Rd} (1) where < n, refers to stack of standard DiT blocks whose width is smaller than the token dimension from the representation encoder, and A, denote the input and output linear projections, respectively. Then for any Gd, L(g, θ) = (cid:90) 1 0 Exp(x),εN (0,In) (cid:2)g(xt, t) (ε x)2(cid:3)dt (cid:88) λi i=d+1 (2) where λi are the eigenvalues of the covariance matrix of the random variable = ε x. Notably, when n, Gd contains the unique minimizer to L(g, θ). Proof. By Albergo et al. (2023), the distribution ρt of xt satisfies ρ0 = p(x), ρ1 = (0, In), and tρ + (vρ) = 0 where is the optimal velocity predictor defined as v(xt, t) = E[ε xxt]. Also, by Theorem 2.7 in Albergo et al. (2023), there exists 0((C 1(Rn))n; [0, 1])1 that uniquely minimizes the L(f, θ) and perfectly approximates v. By our training setting, its reasonable to assume that p(x) and ε (0, In) are independent. Then the distribution of the objective = ε py(y) satisfies py(y) = (cid:82) Rn (0, In)(y + x)p(x)dx. Clearly, py has full support on Rn and is strictly positive, indicating has nonzero probability anywhere in Rn. Similarly, for xt = (1 t)x + tε, given any t, pxt(w) = (cid:82) 1t )dx also has full support on Rn and is strictly positive, indicating 1 (1t) p( Rn (0, t2In)(wx) xt has non-zero probability anywhere in Rn as well. Recall that for any function : Y, Im(f ) = {f (x) : }. Then for linear transformation (x) = with Rdn, Im(f ) = {M : Rn}; we denote this as Im(M ). Now, for any Gd, Im(g) = {Bf (Ax) : Rn} {By : Rd} = Im(B). Since rank(B) d, dim Im(g) rank(B) < n, therefore Im(g) Im(B) Rn. Now, given Gd and the deterministic pair (xt, y, t) (Rn, Rn, [0, 1]), by Projection Theorem, g(xt, t) y2 ug y2 (3) where ug Im(g) is the unique minimizer and ug is orthogonal to Im(g). Since 2 0, we can take expectation on both sides inf gGd Exp(x),εN (0,In) (cid:2)g(xt, t) y2(cid:3) inf gGd E(cid:2)ug y2(cid:3) inf uS;dim Sd E(cid:2)u y2(cid:3) inf S;dim Sd E(cid:2)y2 PSy2(cid:3) (4) 1family of functions : [0, 1] Rn Rn that is continuous in for all (x, t) [0, 1] Rn, and (, t) is continuously differentiable function from Rn to Rn."
        },
        {
            "title": "Preprint",
            "content": "where PS denote the projection matrix from Rn onto S. Without loss of generality, we assume E[x] = 02, then Eq 4 can be expanded as Exp(x),εN (0,In) (cid:2)g(xt, t) y2(cid:3) inf gGd (cid:88) i= E[y2 ] sup S;dim Sd (cid:88) i=1 E[(PSy)2 ] = Tr(Cov(y)) sup S;dim Sd Tr(Cov(PSy)) (cid:88) i= λi (cid:88) i=1 λi = (cid:88) λi i=d+1 (5) where Eq 5 is obtained via Ky-Fan Maximum Principle. When n, supS 0((C 1(Rn))n; [0, 1]). E[PSy2] = E[y2], leading to trivial lower bound in Eq 5, and Gd = B.2 PROOF OF LOWER BOUND FOR INFERENCE LOSS Theorem 2. Consider the same setup as Theorem 1. Let x1 be the initial random variables in the sampling process, and x0 = ODE(g, x1, 1 0) 0 = ODE(f , x1, 1 0) where ODE(f, x, s) refers to any ODE solver that integrates from time to using as the initial condition. We further assume for any (x, y, t) (Rn, Rn, [0, 1]), there exists constant > 0 such that then (x, t) (y, t) Lx x 0 x0 1 eL (cid:88) λi i=d+1 Proof. We first define forward ODE that integrates from 0 1 t := xt, and dx = g(x , t)dt dx = (x , t)dt t"
        },
        {
            "title": "Then",
            "content": "d dt x , t) g(x = (x , t) (x , t) (x , t) g(x Lx , t) (x , t) (6) (7) where denotes the approximation error to for Gd. Applying Gronwalls Lemma, we have eL t (cid:12) 1 (cid:12) (cid:12) (cid:12) 0 = 0 x0 = x 1 (cid:90) eLsds 0 (1 eL) (cid:88) λi i=d+1 (8) where by Theorem 1, (cid:80)n i=d+1 λi. 2Non-centered with E[x] = µ will additionally introduce µ2 PSµ2 to the lower bound; we ignore this term since most data processing pipelines will center the data."
        },
        {
            "title": "C RAE IMPLEMENTATION",
            "content": "C.1 ENCODER NORMALIZATION For any given frozen representation encoders, we discard any [CLS] or [REG] token produced by the encoder, and keep the all of patch tokens. We then apply layer normalization to each token independently, to ensure each token has zero mean and unit variance across channels. We note that all representation encoders we use adopt the standard ViT architecture (Dosovitskiy et al., 2021), which have already applied layer normalization after the last transformer block. Therefore, we only need to cancel the affine parameters of the layer normalization in representation encoders. This does not affect the representation quality of representation encoders, as it is linear transformation. Practical Notes. Specifically, we use DINOv2 with Registers (Darcet et al., 2025). Since DINOv2 only provides variants with pe = 14, we interpolate the input images to 224 224 but set pd = 16, ensuring the model still produces 256 tokens while reconstructing 256 256 images. C.2 DECODER TRAINING DETAILS Datasets. We primarily use ImageNet-1K for training all decoders. Most experiments are conducted at resolution of 256 256. For 512-resolution synthesis without decoder upsampling, we train decoders directly on 512 512 images. Decoder Architecture. The decoder takes the token embeddings produced by the frozen encoder takes the token embedding reconstructs them back into the pixel space using the same patch size as the encoder. As result, it can generate images with the same spatial spatial resolution as the encoders inputs input. Following He et al. (2021), we prepend learnable [CLS] token decoders input sequence and discard it after decoding. Discriminator Architecture. We include the majority of our decoder training details in Table 12. We follow most of the design choices in StyleGAN-T (Sauer et al., 2023), except for using frozen Dino-S/8 (Caron et al., 2021) instead of Dino-S/16 as the discriminator. We found using Dino-S/8 stabilizes training and avoid the decoder to generate adversarial patches. We also remove the virtual batch norm in Sauer et al. (2023) and use the standard batch norm instead. All input is interpolated to 224 224 resolution before feeding into the discriminator. Table 12: Training configuration for decoder and discriminator. Component Decoder Discriminator optimizer max learning rate min learning rate learning rate schedule optimizer betas weight decay batch size warmup loss Model LPIPS start epoch disc. start epoch adv. loss start epoch Training epochs Adam 2 104 2 105 cosine decay (0.5, 0.9) 0.0 512 1 epoch Adam 2 104 2 105 cosine decay (0.5, 0.9) 0.0 512 1 epoch ℓ1 + LPIPS + GAN adv. ViT-(B, L, XL) 0 8 16 Dino-S/8 (frozen) 6 10 Losses. For training the decoder, we use mixture of L1, LPIPS (Zhang et al., 2018) and adversarial loss (Goodfellow et al., 2014): = E(x), ˆx = D(z) Lrec(x) = ωL LPIPS(ˆx, x)+ L1(ˆx, x) + ωGλ GAN(ˆx, x),"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Reconstruction examples. From left to right: input image, RAE (DINOv2-B), RAE (SigLIP2-B), RAE (MAE-B), SD-VAE. Zoom in for details. where E, are the encoder and decoder of RAE. We set ωL = 1. and ωG = 0.75. We use the same losses as in StyleGAN-T (Sauer et al., 2023) for discriminator, and GAN loss as in Esser et al. (2021). We also adopt the adaptive weight λ for GAN loss proposed in Esser et al. (2021) to balance the scales of reconstruction and adversarial losses. λ is defined as: λ = ˆxLrec ˆx GAN(ˆx, x) + ϵ , Augmentations. For data augmentation, we first resize the input image to 384 384 and then randomly crop to 256 256. We also apply differentiable augmentations with default hyperparameters in Zhao et al. (2020) before discriminator. C.3 VISUALIZATIONS We present visualizations of reconstructions from different RAEs. As shown in Figure 8, all RAEs achieve satisfactory reconstruction fidelity."
        },
        {
            "title": "D DIFFUSION MODEL IMPLEMENTATION",
            "content": "Datasets. We primarily use ImageNet-1K (Russakovsky et al., 2015) for diffusion training. Most experiments are conducted at resolution of 256256. For 512-resolution synthesis without decoder upsampling, we train diffusion models directly on 512 512 images. Model Config Dim Num-Heads Depth Models. By default, we use LightningDiT (Yao et al., 2025) as the backbone of our diffusion model. We use continuous time formulation of flow matching and restrict the timestep input to real values in [0, 1]. Following prior work (Song et al., 2021), we replace the timestep embedding with Gaussian Fourier embedding layer. We also add Absolute Positional Embeddings (APE) to the input tokens in addition to RoPE, though we do not observe significant performance difference with or without APE. For DiTDH, we generally follow the same architecture as DiT, and does not reapply APE for the DDT head input. We use linear layer to map the DiTDH encoder output to the DiTDH decoder Table 13: Model configurations for different sizes. XL XXL T 384 768 1024 1152 1280 1536 2048 2688 12 12 24 28 32 32 40 40 6 12 16 16 16 16 16"
        },
        {
            "title": "Preprint",
            "content": "dimension when the dimension of DiT and DDT head mismatches. We provide detailed model configuration in Table 13. Compute. For all models based on RAE, we use patch size of 1. For baseline experiments on VAE and pixel inputs, we use patch sizes of 2 and 16, respectively. Across all 256 256 experiments, the diffusion model processes token sequence of length 256. Consequently, the computational cost of the DiT backbone remains identical across RAE, VAE, and pixel settings, with differences arising only from the patchification step. Since patchification accounts for less than 1% of the total GFLOPs, training DiT on different autoencoders introduces only negligible compute overhead. Optimization. For DiT, we strictly follow the optimization strategy in LightningDiT (Yao et al., 2025), using AdamW with constant learning rate of 2.0 104, batch size of 1024 and an EMA weight of 0.9999. We do not observe instability or abnormal training dynamics with this recipe on DiT. For DiTDH, we find using the recipe in (Yao et al., 2025) leads to loss spikes at later epochs and slow EMA model convergence at early epochs. We instead use linear decay from 2.0 104 to 2.0 105 with constant warmup of 40 epochs. To encourage the convergence of EMA model, we change the EMA weight from 0.9999 to 0.9995. Additionally, we use gradient clipping of 1.0 for DiTDH. Other optimization hyperparameters are the same as DiT. All models are trained for 80 epochs unless otherwise specified. We only report EMA model performance. Sampling. We use standard ODE sampling with Euler sampler and 50 steps by default. We find the performance generally converges above 50 steps. We use the same sampling hyperparameters for both DiT and DiTDH. Computation. We use PyTorch/XLA on TPU for all training and inference on RAE. For evaluation, we use single v6e-8 to generate 50K samples. For the 800 epoch DiTDH-XL (DINOv2-B) result, we conduct the generation on machine with 4 A100s and evaluate FID on CPU due to lack of TensorFlow GPU support. We use an internal JAX codebase for training baseline models on VAEs."
        },
        {
            "title": "E SAMPLING DETAILS FOR FID EVALUATION",
            "content": "For the FID-50K evaluation on RAE, we followed the protocol used in (Tian et al., 2024; Li et al., 2024b; Ren et al., 2025; Wang et al., 2025c), sampling 50 images from each class for total of 50,000 images. The reference statistics were taken from the ADM pre-computed statistics (Dhariwal & Nichol, 2021) over the full ImageNet dataset. Another commonly adopted sampling strategy is to uniformly sample class labels 50K times and generate images accordingly (Peebles & Xie, 2023; Ma et al., 2024; Yu et al., 2025). It is worth noting that this strategy is not equivalent to the per-class sampling schemealthough random sampling asymptotically converges to the per-class version, the resulting FID scores differ slightly in practice. To ensure fair comparison, we re-evaluate previous state-of-the-art methods that did not use classbalanced sampling with class-balanced sampling. We also evaluate our method with random sampling. As shown in Table 14, all methods FID improves consistently with class-balanced sampling. We also note that the original ImageNet training set is inherently unbalanced, with class sizes ranging from approximately 732 to 1,300 samples (Russakovsky et al., 2015). Therefore, the common assumption behind both balanced and random samplingnamely, that the label distribution is uniformis not entirely accurate. However, since 895 classes contain exactly 1,300 samples, the dataset exhibits high degree of near-equivalence among most classes. This partial balance may partly explain why balanced sampling consistently yields better results, as it more closely approximates the true label distribution of the training set. As the FID values approach the lower ranges, these subtle details begin to have greater impact. We therefore want to raise awareness within the community. Finally, while the FID metric remains useful indicator of generative quality, its absolute value becomes less meaningful as generation fidelity continues to improve."
        },
        {
            "title": "F THEORY EXPERIMENT SETUP",
            "content": "In this section we list the setup of experiments in Section 4 for overfitting images."
        },
        {
            "title": "Preprint",
            "content": "Method Epochs Generation@256 w/o guidance Generation@256 w/ guidance Random Balanced Random Balanced gFID IS gFID IS gFID IS gFID IS Autoregressive VAR (Tian et al., 2024) MAR (Li et al., 2024b) xAR-H (Ren et al., 2025) Latent Diffusion with VAE SiT (Ma et al., 2024) REPA (Yu et al., 2025) DDT (Wang et al., 2025c) REPA-E (Leng et al., 2025) 350 800 800 1400 800 400 800 - - - - - - 8.61 5.90 - 1. 131.7 157.8 - 217.3 1.92 2.35 - 8.54 5.78 6.27 1.70 323.1 227.8 - 132.0 158.3 154.7 217.3 - - - - - - 2.06 1.42 1.40 1.26 270.3 305.7 303.6 314.9 1.73 1.55 1.24 1.95 1.29 1.26 1.15 350.2 303.7 301. 259.5 306.3 310.6 304.0 Latent Diffusion with RAE (Ours) DiTDH-XL (DINOv2-B) 800 1.60 242. 1.51 242.9 1.28 262.9 1.13 262. Table 14: Performance of different methods using different sampling strategies. The officially reported numbers are marked in gray. Models. By default, we use DiT with depth 12, width 768 and attention head of 4. The depth varies in {384.512, 640, 768, 896} and width varies in {4, 12, 16, 24} in Figure 3. Other configurations are the same as Section D. Targets. We use three images for overfitting experiments, and all numbers reported are the average on three independent run on each images. We resize all targets to 256 256 and not use any data augmentation . Optimizations & Sampling. For single target image, the batch size only influences the timestep. We therefore use relatively small batch size of 32 and constant learning rate of 2 104, optimized with AdamW (β = (0.9, 0.95)). The model is trained for 1200 steps without EMA. For sampling, We use standard ODE sampling with Euler sampler and 25 steps by default."
        },
        {
            "title": "G ADDITIONAL ABLATION STUDIES",
            "content": "G.1 GENERATION PERFORMANCE ACROSS ENCODERS As shown in Table 15a, DINOv2-B achieves the best overall performance. MAE performs substantially worse in generation, despite yielding much lower rFID. This shows that low rFID does not necessarily imply good image tokenizer. Therefore, we use DINOv2-B as the default encoder for our image generation experiments. (a) gFID and rFID of different encoders w/ and w/o noisy-robust decoding. gFID and rFID of different DINOv2 (b) sizes w/ and w/o noisy-robust decoding. (c) Scaling τ for DINOv2-B. gFID Model DINOv2-B 4.81 / 4.28 6.69 / 4.93 SigLIP2-B 16.14 / 8.38 MAE-B rFID 0.49 / 0.57 0.53 / 0.82 0.16 / 0.28 Model gFID 3.83 / 3.50 4.81 / 4.28 6.77 / 6.09 rFID 0.52 / 0.64 0.49 / 0.57 0.52 / 0. τ 0.0 0.5 0.8 1.0 gFID rFID 4.81 0.49 4.39 0.54 4.28 0.57 4.20 0.60 Table 15: Ablations on noise-augmented decoder training. Despite minor drop in rFID, the noiseaugmented training strategy can greatly improve the gFID across different encoders and model sizes. Default setups are marked in gray."
        },
        {
            "title": "Depth Width",
            "content": "GFLops FID 6 4 2 1152 (XL) 2048 (G) 2048 (G) 25.65 53.14 26.78 2.36 2.31 2.16 22-1536 2-2048 2-2688 Dino-S Dino-B Dino-L 2.66 2.49 N/A 2.47 2.24 2. 2.42 2.16 2.73 2.43 2.22 2.64 Table 16: DDT head should be wide and shallow. wide, shallow head yields lower FID than deeper (4-layer G) or narrower (6-layer XL) ones . Table 17: Unguided gFID of different RAE and DDT head. Larger RAE benefits more from wider DDT head. d-w: DDT head with layers and width w. Default setups are marked in gray. G.2 DESIGN CHOICES FOR NOISE-AUGMENTED DECODING We first analyze how noise-robust decoding affects reconstruction and generation. Table 15c shows that larger τ improves generative FID (gFID) consistently, but slightly worsens reconstruction FID (rFID). This supports our intuition: noise encourages the decoder to learn smoother mappings that generalize better to imperfect latents, improving generation quality, but reducing exact reconstruction accuracy. To test the robustness of this trade-off, we evaluate different encoders (Table 15a) with σ = 0.8. Across all encoders, noisy training improves gFID while mildly harming rFID. The effect is strongest for weaker encoders such as MAE-B, where gFID improves from 16.14 to 8.38. Finally, Table 15b shows that the benefit holds across encoder sizes, suggesting that robust decoder training is broadly applicable. Together, these results highlight general principle: decoders should not only reconstruct clean latents, but also handle their noisy neighborhoods. This simple change enables RAEs to serve as stronger backbones for diffusion models. G.3 DESIGN CHOICES FOR THE DDT HEAD We now investigate design variants of the DDT head to identify those that serve its role more effectively. Two factors turn out to be crucial: (a) the head needs to be wide and shallow, and (b) its benefit depends on the size of the underlying RAE encoder. Width and Depth. We first vary the architecture of the DDT head, sweeping both width and depth while keeping the total parameter count approximately fixed. As shown in Table 16, 2-layer, 2048dim (G) head outperforms 6-layer, 1152-dim (XL) head by large margin, despite having similar GFlops. Moreover, 4-layer, 2048-dim head does not improve over the 2-layer version, even though it has double the GFlops. This suggests that wide and shallow head is more effective for denoising. Dependence on Encoder Size. Next, we analyze how the effect of the DDT head scales with the size of the RAE encoder. We fix the DiT backbone as DiT-XL and vary the DDT head width from 768 (B) to 1536 (H), 2048 (G), and 2688 (T). We train DiTDH models on top of three RAEs: DINOv2-S, DINOv2-B, and DINOv2-L. As shown in Table 17, the optimal DDT head width increases as the encoder scales. When using DINOv2-S and DINOv2-B, the performance converges at DDT head width of 2048 (G), while 2688 (T) head still brings performance gains on DINOv2-L. This suggests that the larger RAE encoders benefit more from wider DDT head. By default, we use 2-layer, 2048-dim DDT head for all DiTDH models."
        },
        {
            "title": "H ADDITIONAL SCALING RESULTS",
            "content": "We examine how model scale affects training loss in Figure 9. Increasing the models computational capacity leads DiTDH to converge faster and reach lower final loss."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Training loss of DiTDH on DINOv2-B. We use an EMA weight of 0.9 to smooth the loss."
        },
        {
            "title": "I GUIDANCE",
            "content": "We primarily adopt AutoGuidance (Karras et al., 2025) as our guidance method, as it is easier to tune than CFG with interval (Ho & Salimans, 2022; Kynkaanniemi et al., 2024) and consistently delivers better performance. CFG with interval is used only for the DiT-XL + DINOv2-S with guidance result reported in Table 8. AutoGuidance. We adopt AutoGuidance (Karras et al., 2025) as our primary guidance method. The idea is to use weaker diffusion model to guide stronger one, analogous to the principle of Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). Empirically, we observe that weaker base models and earlier training checkpoints consistently yield stronger guidance effects. In practice, we use the smallest variant, DiTDH-S, as the guiding model, typically adopting an early checkpoint. Unless otherwise specified, AutoGuidance results are obtained using guidance scale of 1.5 and 20-epoch checkpoint of DiTDH-S. For the best result of 1.13 FID (DiTDH-XL on DINOv2-B, 256 256), we sweep the guidance scale and use guidance scale of 1.42 with 14-epoch DiTDH-S checkpoint. Notably, training this base model requires only about 0.05% of the compute used to train the guided model (DiTDH-XL for 800 epochs). Classifier-Free Guidance. We also evaluate CFG (Ho & Salimans, 2022) on RAE. Interestingly, CFG without interval does not improve FID; in fact, applying it from the first diffusion step increases FID. With Guidance Interval (Kynkaanniemi et al., 2024), CFG can achieve competitive FID after careful grid search over scale and interval. However, on our final model (DiTDH-XL with DINOv2B), the best CFG result remains inferior to AutoGuidance. Considering both performance and tuning overhead, we adopt AutoGuidance as our default guidance method. DESCRIPTIONS FOR FLOW-BASED MODELS Diffusion Models (Ho et al., 2020; Dhariwal & Nichol, 2021; Karras et al., 2022) and more generally flow-based models (Albergo et al., 2023; Lipman et al., 2023; Liu et al., 2023) are family of generative models that learn to reverse reference noising process. One of the most commonly used noising process is the linear interpolation between i.i.d Gaussian noise and clean data (Esser et al., 2024; Ma et al., 2024): xt = (1 t)x + tε where p(x), ε (0, In), [0, 1], and we denote xts distribution as ρt(x) with ρ0 = p(x) and ρ1 = (0, I). Generation then starts at = 1 with pure noise, and simulates some differential equation to progressively denoise the sample to clean one. Specifically for flow-based models, the differential equations (an ordinary differential equation (ODE) or stochastic differential equation (SDE)) are formulated through an underlying velocity v(xt, t) and score function s(xt, t) ODE SDE dxt = v(xt, t)dt dxt = v(xt, t)dt wts(xt, t)dt + wtd wt 1"
        },
        {
            "title": "Preprint",
            "content": "where wt is any scalar-valued continuous function (Ma et al., 2024), and wt is the reverse-time Wiener process. The velocity v(xt, t) is represented as conditional expectation v(xt, t) = E[ xtxt] = E[ε xxt] and can be approximated with model vθ by minimizing the following training objective Lvelocity(θ) = (cid:90) 1 (cid:20) vθ(xt, t) (ε x) (cid:21) dt Ex,ε 0 The score function s(xt, t) is also represented as conditional expectation s(xt, t) = 1 E[εxt] Notably, is equivalent to up to constant factor (Albergo et al., 2023), so its enough to estimate only one of the two vectors."
        },
        {
            "title": "K EVALUATION DETAILS",
            "content": "K.1 EVALUATION We strictly follow the setup and use the same reference batches of ADM (Dhariwal & Nichol, 2021) for evaluation, following their official implementation.3 We use TPUs for generating 50k samples and use one single NVIDIA A100 80GB GPU for evaluation. In what follows, we explain the main concept of metrics that we used for the evaluation. FID (Heusel et al., 2017) evaluates the distance between the feature distributions of real and generated images. It relies on the Inception-v3 network (Szegedy et al., 2016) and assumes both distributions follow multivariate Gaussians. IS (Salimans et al., 2016) also uses Inception-v3, but evaluates logits directly. It measures the KL divergence between the marginal label distribution and the conditional label distribution after softmax normalization. Precision and recall (Kynkaanniemi et al., 2019) follow their standard definitions: precision reflects the fraction of generated images that appear realistic, while recall reflects the portion of the training data manifold covered by generated samples."
        },
        {
            "title": "L UNCONDITIONAL GENERATION",
            "content": "We are also interested in how RAEs perform in unconditional generation. To evaluate this, we train DiTDH-XL on RAE latents without labels. Following RCG (Li et al., 2024a), we set labels to null during training and use the same null label at generation time. While classifier-free guidance (CFG) does not apply in this setting, AutoGuidance remains applicable. We therefore train DiTDH-XL for 200 epochs with AG (detailed in Section I). As shown in Table 18, our model achieves substantially better performance than DiT-XL trained on VAE latents. Compared to RCG, method specifically designed for unconditional generation, our approach attains competitive performance while being much simpler and more straightforward, without the need for two-stage generation. Method DiT-XL + VAE DiTDH-XL + DINOv2-B (w/ AG) RCG + DiT-XL gFID IS 30.68 4.96 4.89 32.73 123.12 143.2 Table 18: Comparison of unconditional generation on ImageNet 256 256."
        },
        {
            "title": "M VISUAL RESULTS",
            "content": "We show uncurated 512 512 samples sampled from our most performant model: DiTDH-XL on DINOv2-B with autoguidance scale = 1.5. 3https://github.com/openai/guided-diffusion/tree/main/evaluations"
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Uncurated 512 512 DiTDH-XL samples. AutoGudance Scale = 1.5 Class label = golden retriever (207) Figure 11: Uncurated 512 512 DiTDH-XL samples. AutoGudance Scale = 1.5 Class label = husky (250)"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Uncurated 512 512 DiTDH-XL samples. AutoGudance Scale = 1.5 Class label = cliff (972) Figure 13: Uncurated 512 512 DiTDH-XL samples. AutoGudance Scale = 1.5 Class label = macaw (88)"
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Uncurated 512 512 DiTDH-XL samples. AutoGudance Scale = 1.5 Class label = arctic fox (279) Figure 15: Uncurated 512 512 DiTDH-XL samples. AutoGudance Scale = 1.5 Class label = balloon (417)"
        }
    ],
    "affiliations": [
        "New York University"
    ]
}