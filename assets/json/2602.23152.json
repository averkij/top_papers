{
    "paper_title": "The Trinity of Consistency as a Defining Principle for General World Models",
    "authors": [
        "Jingxuan Wei",
        "Siyuan Li",
        "Yuhang Xu",
        "Zheng Sun",
        "Junjie Jiang",
        "Hexuan Jin",
        "Caijun Jia",
        "Honghao He",
        "Xinglong Xu",
        "Xi bai",
        "Chang Yu",
        "Yumou Liu",
        "Junnan Zhu",
        "Xuanhe Zhou",
        "Jintao Chen",
        "Xiaobin Hu",
        "Shancheng Pang",
        "Bihui Yu",
        "Ran He",
        "Zhen Lei",
        "Stan Z. Li",
        "Conghui He",
        "Shuicheng Yan",
        "Cheng Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress."
        },
        {
            "title": "Start",
            "content": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks principled theoretical framework that defines the essential properties requisite for General World Model. In this paper, we propose that World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under unified evaluation protocol. Our work establishes principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress. Code Leaderboard Dataset 6 2 0 2 6 ] . [ 1 2 5 1 3 2 . 2 0 6 2 : r Figure 1: The Trinity of Consistency in world models: Modal Consistency (Semantics), Spatial Consistency (Geometry), and Temporal Consistency (Causality)."
        },
        {
            "title": "1 Introduction",
            "content": ""
        },
        {
            "title": "2 Foundational Exploration of Consistencies",
            "content": ". . . . . . . ."
        },
        {
            "title": "2.3 Spatial Consistency .",
            "content": "5 5 2.1 The Anatomy of General World Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Modal Consistency . 7 2.2.1 Theoretical Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.2 Discrete Sequences vs. Continuous Manifolds . . . . . . . . . . . . . . . . . . . . 8 2.2.3 Architectural Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.4 Intent Alignment via RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.5 Cognitive Loop via Test-time Compute . . . . . . . . . . . . . . . . . . . . . . . . 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3.1 Geometric Decomposition of Consistency . . . . . . . . . . . . . . . . . . . . . . . 14 2.3.2 Theoretical Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2D Proxy Manifold & Domain Mismatch . . . . . . . . . . . . . . . . . . . . . . . 16 2.3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Implicit Continuous Fields 2.3.4 2.3.5 Explicit Lagrangian Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.3.6 Generative Statistical Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.4.1 From Frequency Stability to Physical Compliance . . . . . . . . . . . . . . . . . . 23 2.4.2 Latent Temporal Inflation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.4.3 Discrete Autoregressive Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.4.4 Unified Spatiotemporal Modeling via DiT . . . . . . . . . . . . . . . . . . . . . . . 26 2.4.5 Logical Consistency and Causal Reasoning . . . . . . . . . . . . . . . . . . . . . . 27 2.5 Outlook of the Consistencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.4 Temporal Consistency .",
            "content": ". ."
        },
        {
            "title": "3 Initial Integration of Multiple Consistencies",
            "content": "3.2 28 3.1 The Rise of Large Multimodal Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.1.1 LLM as Core Cognitive Base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.1.2 Cognitive Evolution as Multimodal Integration of Modal and Spatial Consistency . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.2.1 Pixel Space Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.2.2 View Space Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.2.3 Volume Space Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.2.4 Reinforcement Learning for Modal-Spatial Alignment . . . . . . . . . . . . . . . 37 Integration of Modal and Temporal Consistency . . . . . . . . . . . . . . . . . . . . . . . 38 3.3.1 End-to-End Scalable Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.3.2 Explicit Structured Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.3.3 Unified Comprehension and Generation Symbiosis Architecture . . . . . . . . . 46 3.3.4 Reinforcement Learning for Modal-Temporal Alignment . . . . . . . . . . . . . . 47 Integration of Spatial and Temporal Consistency . . . . . . . . . . . . . . . . . . . . . . . 49 3.4.1 Implicit Spatiotemporal Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.4.2 Explicit Geometric Anchoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3.3 3."
        },
        {
            "title": "4 Challenges, Benchmarks, and Outlook",
            "content": "61 4.1 Core Challenges from Preliminary Fusion to True Unification . . . . . . . . . . . . . . . 61 4.2 Constructing Comprehensive Evaluation Benchmarks . . . . . . . . . . . . . . . . . . . . 62 4.2.1 Modal Consistency: From Symbol Mapping to Knowledge Synergy . . . . . . . . 62 4.2.2 Spatial Consistency: From Visual Similarity to Topological & Physical Verification 63 4.2.3 Temporal Consistency: From Inter-frame Smoothness to Logical Causal Evolution 63 4.2.4 Limitations of Existing Benchmarks & Design Rationale of Our Benchmark . . . 64 4.3 Ultimate Outlook: General World Simulator . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 CoW-Bench",
            "content": ". . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.2 Evaluation metrics\n5.3 Comparison with Existing Benchmarks\n5.4 Main Results .\n.\n.\n5.5",
            "content": "66 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 5.1.1 Dataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 5.1.2 Dataset Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 . Single-Axis Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 5.5.1 Modal Consistency Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 5.5.2 Temporal Consistency Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 Spatial Consistency Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 5.5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 5.6.1 ModalSpace Consistency Results: Semantic-to-Geometry Binding . . . . . . . . 78 5.6.2 ModalTime Consistency Results: Executing Temporal Program . . . . . . . . . 79 5.6.3 Time-Space Consistency Results: Navigation Exposes the Missing World State . 80 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 Sample Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 5.7.1 Single Consistency Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 5.7.2 Compound Consistency Tasks"
        },
        {
            "title": "5.6 Cross-Axis Consistency .",
            "content": "5.7 . . . . . . . ."
        },
        {
            "title": "7 Contributions",
            "content": "84"
        },
        {
            "title": "1 Introduction",
            "content": "The pursuit of Artificial General Intelligence (AGI) is fundamentally anchored in the aspiration to endow machines with profound understanding of the physical reality. truly intelligent agent must evolve from passive observer [1] into proactive simulator [2, 3], possessing an internal world model capable of learning objective physical laws, reasoning about counterfactual scenarios [4], and predicting future states from current actions [5]. Recent years have witnessed an explosion in generative capability, driven by the data-driven Scaling Laws. Video generation models, represented by Sora [2] and Gen-3 [6], have demonstrated an astonishing ability to approximate complex dynamics, creating high-fidelity visual sequences that often are indistinguishable from reality. Simultaneously, the rise of Unified Multimodal Models (UMMs) [7, 8] has offered promising architectural paradigm for integrating diverse sensory inputs into shared semantic manifold [9]. However, critical gap remains: existing models, despite their visual plausibility, often behave as naive physicists. They frequently suffer from structural hallucinations, temporal inconsistencies, and violations of causalitysymptoms of system that mimics pixel statistics rather than internalizing physical principles. The field lacks principled theoretical framework to define the essential properties requisite for General World Model. To bridge the chasm between visual generation and physical simulation, we propose that robust World Model must be grounded in the Trinity of Consistency. We argue that valid internal simulator must satisfy three orthogonal yet synergistic constraints: Modal Consistency (The Semantic Interface): The ability to align heterogeneous information (text, image, tactile) into unified semantic space, serving as the cognitive interface for instruction and feedback. Spatial Consistency (The Geometric Basis): The capacity to construct 3D-aware representation that respects geometry, occlusion, and object permanence, ensuring the static plausibility of the simulated world. Temporal Consistency (The Causal Engine): The adherence to physical laws and causal logic over time, ensuring that dynamic evolution follows predictable and logically sound trajectory. Through this tripartite lens, we systematically review the evolution of generative models from specialized modules to unified world simulators. We trace the trajectory from loosely coupled specialized modules toward end-to-end unified architectures. We argue that dissolving the barriers between these dimensions is the necessary substrate for the emergence of world simulation capabilities, ensuring that modality, space, and time do not operate in isolation but synergize to model coherent reality. This paper is organized to mirror the evolutionary path from specialized modules to unified world simulators. First (2), we deconstruct the independent development of Modal, Spatial, and Temporal consistencies, analyzing their respective theoretical foundations. Second (3), we investigate the paradigm shift enabled by UMMs, detailing how the deep integration of these dimensions facilitates the emergence of physical simulation capabilities. Third (4), we identify the remaining gaps between current probabilistic generators and true physical simulators, setting the stage for rigorous evaluation. The notation used is summarized in Table 1. Finally, theoretical frameworks require rigorous verification. We introduce CoW-Bench (Consistency of World-models Benchmark), unified evaluation suite centered on multi-frame reasoning and constraint satisfaction. Unlike previous benchmarks, CoW-Bench rigorously tests the models ability to maintain the Trinity of Consistency under complex, open-ended scenarios, forcing it to prove it understands the world, not just how to paint it."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 2: Performance Comparison of Mainstream Models across Different Tasks. The score has been linearly rescaled from the original range of [0, 10] to percentage scale of [0, 100]."
        },
        {
            "title": "2.1 The Anatomy of General World Models",
            "content": "As discussed in Section 1 (1), the construction of world models relies on the organic integration of modal consistency (serving as the information interface), spatial consistency (serving as the geometric cornerstone), and temporal consistency (serving as the dynamic engine). In the evolution of specialized models, these consistencies have not developed in isolation but have rather interpenetrated one another: the unified representation space derived from modality alignment provides semantic priors for the reconstruction of spatial geometry, while the 3D manifold of spatial consistency establishes physical constraints for temporal evolution. This section deconstructs that evolutionary history. We trace how specialized models first conquered these challenges in isolation: modality alignment matured through high-dimensional manifold mapping, spatial consistency was solved via the transition from 2D proxies to explicit 3D primitives, and temporal consistency evolved from simple frame interpolation to causal dynamics modeling. Here, we systematically analyze the theoretical foundations and mechanism shifts of each dimension, establishing the necessary prerequisites that eventually enabled the emergence of the unified world simulators discussed in later sections."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 1: Notation and Descriptions Symbol Description Symbol Description S, st, at π τ xobs , World Model State & Action Space State & Action Instance Policy Trajectory Dynamics Function Latent World State Multimodal Observation Latent Vector Encoder / Decoder VQ Codebook Diffusion Coefficient Velocity Field Noise Predictor Token Sequence Projection Weight Wproj I(X; Z) Mutual Information ϵθ vt g(t) αt, σt Ft Graph DKL SNR Parameters Wiener Process KL Divergence Loss Function Causal Graph STFT (Fourier Transform) Pt Π Mgeo Gk σ Ff und low Mepi T(t) Φ 3D Position Camera Pose at Intrinsic Matrix Projection Operator Keyframe Set Geometric Manifold 3D Gaussian Primitive Volume Density View-dependent Radiance Fundamental Matrix Optical Flow Epipolar Mask Continuous Trajectory Spatiotemporal Field Physical Property Field Force Vector Ψ DΦ/Dt Material Derivative Divergence Mdyn Phys const Dynamic Manifold Implicit Gradient Guidance Scale Physics Score Constraint Deviation"
        },
        {
            "title": "2.2 Modal Consistency",
            "content": "The core challenge in constructing general world models lies in the semantic alignment of heterogeneous modalities. Unlike the homogeneity of unimodal generation, multimodal consistency is essentially problem of solving high-dimensional heterogeneous manifold alignment, as illustrated in Figure 3. The model must transcend entropy disparity and topological mismatch to construct unified representation space that is physically complete and logically self-consistent. To this end, we introduce two fundamental theoretical assumptions, the Platonic Representation Hypothesis and the Hypersphere Geometry Hypothesis, and use these as basis to expound on the cognitive architectural evolution from direct feed-forward mapping to iterative reasoning and planning. To systematically deconstruct this alignment process, this section will first elucidate the origins of the modality gap from the perspective of geometric topology (2.2.1); subsequently, it will analyze two mainstream generative manifold mechanismsnamely, discrete autoregression and continuous flow matching (2.2.2); it will then explore the orthogonal decoupled architecture evolved to minimize gradient conflicts (2.2.3); and finally, it will introduce feedback-based intent alignment and the cognitive inference loop moving towards test-time compute (2.2.5)."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 3: Unified Representation Goal. Modal consistency aims to project heterogeneous inputs (Text, Image, Video, Audio) into unified, physically-aligned latent space."
        },
        {
            "title": "2.2.1 Theoretical Foundations",
            "content": "Platonic Cave & Projected Manifolds The theoretical foundation of multimodal learning can be traced back to the platonic representation hypothesis [9]. This hypothesis formally defines the existence of an objective latent physical state space, Zworld, in the real world, where images and text are projections of this high-dimensional entity onto different low-dimensional subspaces. The essence of modal consistency is solving joint inverse projection problem: reconstructing the shared latent variable via observed shadows {ximg, xtxt}. However, this is typical ill-posed problemthe visual projection Pimg retains vast amount of high-frequency physical entropy, whereas the textual projection Ptxt highly abstracts discrete symbolic logic. This Entropy Asymmetry constitutes the primary obstacle to direct alignment. Hypersphere Hypothesis & Modal Gap To mathematically align these two heterogeneous spaces, mainstream paradigms (such as CLIP) introduce the Hypersphere Hypothesis [43], which forces feature vectors to be uniformly distributed on unit hypersphere Sd1. However, this strong assumption ignores the pervasive modal gap in multimodal representations [44]. On one hand, empirical studies by Liang et al. pointed out the cone effect, as shown in Figure 5: joint optimization causes visual and textual embeddings to collapse into two narrow and separated conical regions, destroying the isotropy of the feature space. On the other hand, from the perspective of manifold learning, this gap reveals deeper topological mismatch: visual data is typically distributed on continuous, dense low-dimensional manifold, while linguistic data presents sparse, discrete clustering structure. This fundamental difference in intrinsic dimensionality and data density leads to manifold nonisomorphism, rendering the achievement of perfect isometric alignment between the two spaces, while maintaining their respective semantic structures, an ill-posed problem. Evolution of Computational Paradigms: From Amortized Inference to Test-time Compute Facing the inherent representation errors caused by the aforementioned geometric topological mismatch, simple parameter internalization strategies face theoretical bottlenecks, prompting the modeling of modal consistency to undergo transition between two major computational paradigms. This"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "y t n a f i o l M o fi n Geometric Isolation & Connector Paradigm Early Fusion & Unified Optimization Orthogonal Decoupling (Native MM-DiT) Intent Alignment & Cognitive Loop Dual-Tower Contrastive Connector-Based Alignment Discrete Unified Interface Asymmetric Projection Weight Decoupling Flow Matching Dynamics VLM-as-a-Judge (Critic) Process Supervision (RL) CLIP [10], ALIGN [11], SigLIP [12], MetaCLIP [13], etc. Flamingo [14], BLIP/BLIP-2 [15, 16], Qwen-VL [17], LLaVA-NeXT [18], etc. Chameleon [19], Unified-IO 2 [20], Show-o [21], CM3leon [22], etc. LLaVA [23], CogVLM [24], InternVL [25], MiniGPT-4 [26], etc. Stable Diffusion 3.5 [27], Emu3 [28], PixArt-α [29], Lumina-Next [30], etc. Rectified Flow [31], InstaFlow [32], Flux.1 [33], SiT [34], etc. MetaMorph [35], SRUM [36], ImageReward [37], VLMScore [38], etc. SPO [39], VisualPRM [40], PhyGDPO [41], AR-GRPO [42], etc. Figure 4: Evolution of Modal Consistency: From Geometric Isolation to Cognitive Alignment profoundly reflects the trade-off between train-time compute and test-time compute [45]. Early direct feed-forward mapping corresponds to Dual-Tower architectures [10] and single-step generative models, the core of which is identifying physical rules into neural network weights through large-scale training, i.e., Amortized Inference [46]. This paradigm requires only one forward pass during inference (NFE = 1). Although highly efficient, it is limited by in-distribution statistical correlations and essentially can only interpolate within established conical regions, making it difficult to handle unseen counterfactual combinations [47]. In contrast, the current trend is shifting towards iterative reasoning & planning, corresponding to iterative reasoning architectures. This paradigm acknowledges the limitations of single-pass mapping in bridging the modality gap and instead introduces explicit state space search during the inference phase. By constructing Tree of Thoughts [48] in the latent space or executing gradient-guided dynamic planning, the model utilizes additional reasoning compute to instantly correct physical drift. This marks shift in consistency modeling from static pattern matching to dynamic manifold planning."
        },
        {
            "title": "2.2.2 Discrete Sequences vs. Continuous Manifolds",
            "content": "To computationally realize the Joint Inverse Projection process in the above theory, academia has explored two distinct mathematical paths to model the target conditional probability density P(ximgxtxt). This choice determines the physical nature of the latent space manifold: Is it treated as Discrete Symbolic Sequence or Continuous Euclidean Vector Field? We compare the mathematical forms and dynamic characteristics of these two paradigms in Table 2. Table 2: Mechanism Comparison: Discrete AR vs. Continuous Flow Matching. The formulations highlight the trade-off between optimization objectives and error propagation dynamics. Paradigm Discrete AR Flow Matching Objective (The Soul) LAR = (cid:2) log P(sts<t)(cid:3) LFM = (cid:104) vθ(xt) (x1 x0)2(cid:105) Error Exp. Linear Topology Discrete Euclidean"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 5: The Modal Gap Challenge. (Left) Ideal hypersphere alignment assumes uniform distribution. (Right) In reality, entropy disparity causes visual embeddings to collapse into narrow cone, leading to topological mismatch with discrete text tokens. Discrete Autoregressive (AR) The core of this paradigm lies in the Token-centric philosophy, attempting to transform visual generation into sequence prediction problem through unified discrete symbol interface [49, 50]. Its generation process involves strictly coupled stages: first quantizing continuous images into discrete symbols via VQ-GAN, followed by maximizing the sequence log-likelihood using the causal attention mask of Transformer. Exponential Drift & Codebook Collapse. Although the AR paradigm achieves interface unification, it suffers from two endogenous defects when viewed from dynamic perspective [51]. First is the curse of dimensionality. The discretization process is governed by the Dirichlet process; as the codebook dimension increases, the effective utilization rate decays exponentially, leading to the loss of highfrequency textures [52, 53]. Second is error accumulation dynamics. The essence of autoregressive generation is the recursive application of operators. Assuming the local Lipschitz constant of the operator is > 1, the cumulative drift of the initial quantization error ϵ0 after steps is δT LTϵ0. This exponential error amplification explains why AR models often exhibit structural collapse at the tail end when generating long sequences [54]. Continuous Flow Matching (FM) To circumvent quantization errors, the new generation of paradigms (such as Stable Diffusion 3 [27], Emu3 [28]) returns to the continuous latent space. Unlike traditional diffusion models based on the SDE denoising perspective, Flow Matching (FM) [55] adopts an ODE perspective, constructing deterministic transport path connecting noise and data. Velocity Field Regression & Rectified Path. The core idea of continuous FM is to directly fit the velocity field of the probability flow. During training, the intermediate state xt is defined as linear interpolation between data and noise, corresponding to an ideal straight trajectory with target velocity field constantly being vt = x1 x0. The neural network directly regresses this velocity vector via Mean Squared Error loss. Rectified Flow [56] demonstrates that this Reflow operation rectifies the transport"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "trajectory, corresponding to Lipschitz constant 1. This implies that error accumulation transforms into linear growth δT ϵstep, allowing FM to generate high-fidelity samples in very few steps while perfectly preserving the continuous semantic manifold of the latent space."
        },
        {
            "title": "2.2.3 Architectural Evolution",
            "content": "Establishing the generation mechanism only solves the mathematical expression of the target manifold. How to inject heterogeneous modal information into this manifold depends on the conditioning mechanism of the model. The evolution of multimodal architectures exhibits non-linear characteristics, essentially seeking the optimal parameter space topology to minimize gradient conflict and information loss between modalities. This process has undergone three-stage evolution from geometric isolation to early fusion, and finally converging to orthogonal decoupling, as shown in Figure 6. Figure 6: Evolution of Multimodal Fusion Paradigms. Transitioning from geometric isolation (DualTower) to unstable Early Fusion (Adapter), and finally to the orthogonally decoupled Native unified multimodal model (MM-DiT) in large-scale unified architectures. (1) Early Evolution: Establishment of Dual-Tower Architectures and Connector Paradigms. Early exploration of multimodal alignment presented two clear technological evolution paths. First was the Dual-Tower Architecture, represented by CLIP [10] and ALIGN [57]. This paradigm utilized contrastive learning to project heterogeneous modalities onto shared hypersphere. Although excellent in retrieval tasks, the separate processing of images and text by independent encoders resulted in natural asymmetry in geometric topology, lacking deep, fine-grained interaction. To address this limitation, the Connector-based Paradigm, represented by Flamingo [14] and BLIP/BLIP2 [15, 16], emerged. These methods froze the pre-trained visual encoder and innovatively introduced learnable bridge modules (such as Perceiver Resampler or Q-Former) to align visual features with the semantic space of LLMs. This design of Frozen Visual Backbone & Lightweight Connector not only reduced training costs but also established standard architectural template for subsequent LMMs. (2) Early Fusion and the Challenge of Unified Optimization. To further break the geometric isolation between modalities, academia began exploring more radical Early Fusion strategies. Representative"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "works such as Unified-IO [58] attempted to handle various heterogeneous tasks within unified sequence-to-sequence framework, promoting the development of general interfaces. However, this fully unified paradigm exposes deep Optimization Instability. Particularly when introducing discretization strategies (such as Chameleon [59]), despite achieving interface unification, different modalities exhibited significant differences in training dynamics. Empirical evidence shows that the gradient variance of visual tokens is significantly higher than that of text, making it difficult for the model to converge to an optimal solution during joint training. Furthermore, continuous asymmetric paradigms, such as LLaVA [23], interface with large language models through projection layer. However, the linear projection layer Wproj essentially acts as low-rank compressor (as shown in Figure 7). During optimization, the model is encouraged to preserve semantic information that is relevant for textual reasoning, while suppressing high-frequency components that are essential for image synthesis. As result, the mutual information between the input image and the projected representation is substantially reduced. This explains why LLaVA excels in understanding tasks but fails to restore texture details in generation tasks. Figure 7: Information Asymmetry in LLaVA. The linear projection layer Wproj acts as low-rank compressor, prioritizing semantic alignment with the LLM while discarding high-frequency visual textures needed for controllable visual generation. (3) The Mainstream Paradigm of Orthogonal Decoupling. Addressing the aforementioned gradient conflict, works represented by Stable Diffusion 3.5 [27] and Emu3 [28] established the current MM-DiT architecture. The core lies in the weight decoupling strategymaintaining independent weight sets Wtxt, Wimg for text and images, exchanging data only during attention operations, as shown in Figure 8. From the perspective of optimization dynamics, this design forces the Hessian matrix of the joint loss function to exhibit an approximate block-diagonal structure: Htotal (cid:20)Htxt 0 (cid:21)"
        },
        {
            "title": "0\nHimg",
            "content": ", s.t. 2L WtxtWimg 0, (1) where Htotal denotes the joint Hessian matrix, and Wtxt/img represents the modality-specific parameters. This structure effectively isolates modality-specific curvature, causing gradient updates for different modalities to tend towards orthogonality in the parameter space. Empirical data indicates that this"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 8: MM-DiT Architecture. By maintaining independent weight sets for both text and image modalities and interacting only via joint Attention, MM-DiT achieves orthogonal gradient updates, effectively resolving the modality conflict. mechanism significantly reduces the gradient conflict rate from over 50% in AR paradigms to approximately 30% [60]. This was validated in Stable Diffusion 3.5 Large: thanks to modality decoupling, the model demonstrates instruction following capabilities and physical fidelity significantly superior to asymmetric architectures such as LLaVA on tasks requiring complex typography rendering and long-text comprehension."
        },
        {
            "title": "2.2.4 Intent Alignment via RL",
            "content": "After achieving orthogonal decoupling with the MM-DiT architecture, the focus of consistency modeling shifts from physical representation fitting to high-level semantic alignment. Although traditional maximum likelihood estimation (MLE) captures pixel statistical correlations, it often falls into semantic drift due to lack of explicit supervision when dealing with ill-posed joint inverse projection problems [9]. To this end, academia has introduced reinforcement learning with human feedback (RLHF) [61], reframing alignment as reward-guided search on the hypersphere manifold [43]. Process Supervision & Physical Constraints The architectural evolution based on preference finetuning began with efficient DiT baselines, exemplified by PixArt-α [29]. Owing to their relatively low training cost, these architectures enable practical end-to-end alignment under preference supervision. Addressing the sparsity of trajectory feedback in traditional DPO (Direct Preference Optimization), SPO [39] and VisualPRM [40] introduced stepwise evaluation mechanisms, performing fine-grained supervision on every inference step in the denoising path. Meanwhile, to address non-physical phenomena such as gravity violation, PhyGDPO [41] introduced physics-aware VLM feedback, where the core loss function is implemented by penalizing physical violation term PhysScore: (cid:34) (cid:32) LPhy-DPO = log σ β log πθ(vw) πre (vw) β log πθ(vl) πre (vl) (cid:33)(cid:35) + αPhysScore , (2)"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "where β is the KL divergence penalty coefficient that controls the deviation from the reference policy πref, vw and vl denote the winning and losing video samples respectively, and PhysScore measures the difference in physical compliance scores. Perception-Generation Synergistic Loop To further break through the upper limits of static datasets, academia has established an interactive optimization paradigm centered on VLM-as-a-Judge. This paradigm utilizes the strong semantic perception capabilities of Multimodal Large Models as Critic to construct Generate-Evaluate-Refine closed-loop system. Representative works such as MetaMorph [35] achieved unified alignment of understanding and generation through instruction tuning; while SRUM [36] further proposed unified multimodal self-correction mechanism. SRUM guides the iterative fine-tuning of the diffusion model by backpropagating discriminant gradients to the generator or by utilizing fine-grained deedback captions generated by the VLM. This reciprocal improvement between perception and generation not only resolves attribute omission issues under complex prompts but also enables T2I models to continuously approach the semantic understanding upper bound of VLMs through bootstrapping in the absence of external human annotation. Factorized Optimization for AR Models Unlike the denoising optimization of Diffusion models, AR models face the dual challenges of discrete space non-differentiability and temporal error accumulation. Addressing this, AR-GRPO [42] and ReasonGen-R1 [62] in 2025 proposed factorized optimization strategy for sequence generation: LARRL = Eπ[R(x)] (cid:123)(cid:122) (cid:125) Alignment Gain (cid:124) β DKL(ππre ) (cid:125) (cid:123)(cid:122) Temporal Smoothing (cid:124) , (3) where R(x) is the reward function derived from CLIP or VQA feedback, and β serves as the regularization coefficient for the KL divergence term DKL. This paradigm explicitly decomposes the loss function into alignment gain and temporal smoothing term. The alignment term utilizes CLIP/VQA rewards to guide token selection to conform to semantic intent, while the KL divergence constraint forces the policy to remain within the pre-trained language manifold, preventing the model from suffering Language Collapse due to over-optimization of rewards. Empirical evidence shows that this strategy effectively suppresses token repetition and garbled text in long sequence generation."
        },
        {
            "title": "2.2.5 Cognitive Loop via Test-time Compute",
            "content": "Although reinforcement learning has achieved preliminary alignment of human intent, modal consistency remains limited by the platonic statistical boundary [9]. Existing generative models are essentially pattern-matching interpolators that fit the training distribution solely through amortized inference [47]. When faced with counterfactual tasks that require multi-step chain deduction, this one-pass mapping mechanism lacks real-time verification and is prone to logical hallucinations [63]. To correct logical drift in long-range generation, consistency modeling is shifting towards the test-time compute [45] paradigm. This paradigm acknowledges the limitations of single-shot inverse projection and instead introduces explicit state space search during the inference phase. In this closed loop, the generation process is redefined as an optimal path search problem on the spatiotemporal manifold M. Recent paradigms like UniGen [64] and EvoSearch [65] have introduced multi-step reasoning architectures, combining monte carlo tree search (MCTS) [66] with verifier mechanisms [67], to achieve inference-time scaling during generation. Addressing the high-dimensional nature of visual tasks, VisualPRM [40] utilizes process reward model to perform fine-grained verification on logical nodes of the denoising trajectory, thereby mathematically enhancing the logical consistency of generated results. Furthermore, by integrating an explicit causal planning layer [68], the model is enabled to utilize additional reasoning compute to detect and correct deviations in physical trajectories."
        },
        {
            "title": "2.3 Spatial Consistency",
            "content": "Figure 9: Spatial Consistency via Multi-View Constraints. The model ensures that the generated subject (Doge) maintains geometric coherence across Front, Side, and Top-down views, preventing structural distortion and the Janus problem. The modal consistency discussed in the previous section successfully constructed unified semantic mapping for heterogeneous data. However, for constructing an executable Internal Simulator, having only semantic alignment is incomplete. As developmental psychology research points out, cognition of the world is built upon the foundations of Object Permanence [69] and 3D Exclusivity [70]. Such semantic representations, lacking geometric entities, cannot support an agents navigation and interaction within three-dimensional space [71]. The core mission of spatial consistency is to ground these semantic latent variables onto three-dimensional geometric manifold Mgeo that conforms to physical laws. This is essentially solving typical Ill-posed Inverse Problem [72], as shown in Figure 9: specifically, how to recover high-dimensional state space satisfying multi-view geometric constraints (such as epipolar equivariance) from dimensionality-reduced, sparse 2D observations, while avoiding structural artifacts like the Janus Problem. To construct unified theoretical framework, we formalize this process as solving set of coupled differential equation inverse problems on spatiotemporal manifold. This section will elucidate how models establish the static geometric basis of the world model by introducing physical priors and generative diffusion priors, following an evolutionary path from 2D proxy manifolds to 3D implicit fields, and finally converging to Explicit Lagrangian Primitives."
        },
        {
            "title": "2.3.1 Geometric Decomposition of Consistency",
            "content": "To mathematically characterize spatial consistency, we decompose this abstract concept into two complementary and hierarchically progressive topological constraints: the former governs the microscopic continuity of the physical surface, while the latter guarantees the macroscopic uniqueness and coherence of the object structure. Micro-level: Local Neighborhood Topological Consistency. This constraint focuses on the Intrinsic Continuity of the manifold M, which corresponds mathematically to the Lipschitz Condition. That is, for any two adjacent points on the manifold, the difference in their physical attributes (such as color, density) should be strictly constrained linearly by their Euclidean distance. In 3D reconstruction and generation tasks, this constraint is typically implemented explicitly through geometric regularization"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "y t n a p n u E t n 4 / 3 2D Proxy Manifolds (Manifold Hypothesis) Implicit Continuous Fields (NeRF/SDF) Deep Recurrent & PDE Physics-Informed Priors Continuous Integration Surface & Eikonal Constraint Explicit Lagrangian Primitives (3DGS) Rasterization & Splatting 4D Dynamics & Physics Generative Statistical Priors (World Model) Score Distillation (SDS/VSD) Large Reconstruction Models ConvLSTM [73], PredRNN [74], PhyDNet [75], SVG [76], etc. PINN [77], DeLaN [78], HNN [79], Latent ODEs [80], etc. NeRF [81], Mip-NeRF [82], Zip-NeRF [83], Instant-NGP [84], etc. NeuS [85], VolSDF [86], IGR [87], MonoSDF [88], etc. 3DGS [89], Scaffold-GS [90], 2DGS [91], Mip-Splatting [92], etc. PhysGaussian [93], 4D-GS [94], Deformable-GS [95], SpacetimeGS [96], etc. DreamFusion [97], ProlificDreamer [98], MVDream [99], ImageReward [37], etc. LGM (G-Objaverse) [100], Objaverse-XL [101], SV3D [102], Dust3R [103], See3D [104], etc. Figure 10: Evolution of Spatial Consistency Paradigms: From 2D Proxy to Generative Primitives. terms. For example, IGR (Implicit Geometric Regularization) [87] utilizes the Eikonal equation to constrain the norm of gradients, while RegNeRF [105] introduces smoothness loss to suppress nonphysical high-frequency noise generated under sparse views, ensuring the generated object possesses smooth and physically reasonable surface. Macro-level: Global Geometric Consistency. Local smoothness alone is insufficient; the model must also satisfy Epipolar Equivariance in multi-view geometry [72]. That is, when observing the same object from different viewpoints va, vb, its projected coordinates should satisfy strict algebraic constraints Fabxa = 0. In generative models, violating this constraint is the root cause of the Janus Problem [97], where different viewpoints produce incompatible object geometries. To address this, SyncDreamer [106] constructs an explicit 3D cost volume to enforce alignment, while MVDream [99] utilizes multi-view self-attention mechanism to internalize hard geometric constraints into attention weights, directly locking the global topological uniqueness of the generated object. The above decomposition clarifies the geometric objectives of spatial consistency. However, how to systematically solve these topological constraints within the parameter space of neural network requires establishing unified differential equation perspective."
        },
        {
            "title": "2.3.2 Theoretical Formulation",
            "content": "To construct theoretical framework, we formalize the spatial consistency in 3D visual generation as solving set of coupled Inverse Differential Problems on the spatiotemporal manifold R3 R+. From this perspective, the construction of the full state field Φ(x, t) follows three core physical laws, which respectively define the worlds presentation mode, generation rules, and motion laws. Physical Rendering: The RTE. Both explicit and implicit 3D representations can be physically viewed as discretized solutions to the Radiative Transfer Equation (RTE) [107]. For ray r(s) = + sd, the variation of its radiance along the path follows: L(x, d) (cid:123)(cid:122) (cid:125) (cid:124) Transport = σ(x)L(x, d) (cid:125) (cid:123)(cid:122) Absorption (cid:124) + σ(x)c(x, d) (cid:123)(cid:122) (cid:125) Emission (cid:124) , (4)"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "where σ(x) represents the Volume Density at position x, and c(x, d) denotes the view-dependent Color Emission. The difference in discretization constitutes the divergence in technical routes: NeRF (Implicit Fields) employs volume rendering integration, approximating the solution by dense Riemann summation of Eq. (4) along the ray; while 3DGS (Explicit Primitives) discretizes the continuous field into set of Lagrangian Gaussian basis functions, transforming the integral into efficient analytical rasterization. The former ensures continuity, while the latter achieves real-time performance. Generative Evolution: The SDE. In the generative prior paradigm, spatial consistency originates from the probability distribution of the pre-trained model. We model the process of recovering from Gaussian white noise zT to the data manifold z0 as Stochastic Differential Equation (SDE) [108]: dΦt = (Φt, t)dt + g(t)dw, (5) where () is the deterministic drift term governing semantic evolution, g(t) denotes the diffusion coefficient, and represents the standard Wiener process. Modern generative models aim to learn the reverse process of the above SDE (score matching). When the diffusion term g(t) = 0, the SDE degenerates into deterministic Ordinary Differential Equation (ODE), i.e., Flow Matching. This provides theoretical basis for understanding how generative models recover smooth and topologically consistent geometric structures from disordered noise. Motion Law: Lagrangian Transport. To ensure topological consistency of the spatial structure along the time axis, the motion of material points must follow Lagrangian Flow: dx dt = v(x, t), s.t. DΦ Dt = 0 (Material Derivative), (6) where represents the velocity field driving the particle motion, and DΦ Dt denotes the material derivative. This constraint implies that feature Φ remains conserved as it moves with the fluid (the material derivative is 0). This directly corresponds to the particle tracking mechanism in the explicit primitive paradigm and serves as the mathematical bridge connecting static geometry and dynamic video. The history of spatial consistency evolution is essentially process where academia shifted from solving the static RTE (NeRF) to inversely solving the generative SDE (Diffusion), and finally integrating Lagrangian dynamic constraints. This iterative process of moving from attempting to fit dynamics on 2D projected manifolds to implicit continuous field integration, and then returning to explicit Lagrangian primitives, is illustrated in Figure 11. 2.3.3 2D Proxy Manifold & Domain Mismatch Before explicit 3D representations established their mainstream status, the primary path to addressing spatiotemporal consistency was video prediction based on the Manifold Hypothesis. This paradigm avoided expensive SE(3) spatial modeling and instead attempted to reduce the high-dimensional physical state field Φs evolutionary dynamics operator F3D : SE(3) R3 R3 into parameterized mapping Fθ : RHW RHW on the 2D image manifold Mimg. Although this proxy manifold strategy offered computational complexity advantages, it introduced fundamental Domain Mismatch. Dynamics Fitting Lacking SE(3) Equivariance. Early works like ConvLSTM [73] and PredRNN [74, 109], while mitigating long-sequence gradient decay through improved recurrent units (e.g., Gradient Highway Unit, GHU), relied on convolution operations that only possess Translation Equivariance and lack the ability to perceive the 3D rotation group SO(3). As stated in [110, 111, 112, 113], attempting to simulate 3D rigid body rotation through non-linear transformations of 2D pixel grid is essentially approximating high-dimensional topology on low-dimensional manifold. This misalignment of inductive bias leads to the models inability to decouple extrinsic camera motion from"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 11: Evolution of Spatial Consistency Paradigms. We trace the trajectory from early 2D Proxy Manifolds, to Implicit Continuous Fields like NeRF, moving towards Explicit Lagrangian Primitives like 3DGS, and finally integrating Generative Diffusion Priors. intrinsic object deformation, inevitably causing non-physical Non-rigid Distortion or texture stretching in generated videos during large viewpoint transformations. Early Attempts and Limitations of Physics-aware Modeling. To alleviate the blurriness caused by pure statistical fitting and enhance the robustness of temporal extrapolation, academia attempted to endow black-box models with physical interpretability, the core idea being to inject physical conservation laws into the neural networks parameter space. pioneer in this direction is Physics-Informed Neural Networks (PINN) [77], which adds the residuals of Partial Differential Equations (PDEs) as regularization terms to the loss function, forcing the network output to conform to physical constraints like fluid mechanics or wave equations. Subsequently, Deep Lagrangian Networks (DeLaN) [78] and Hamiltonian Neural Networks (HNN) [79] further introduced energy conservation priors, explicitly modeling the systems total energy (Hamiltonian) using Euler-Lagrange equations, thereby achieving precise trajectory prediction for complex dynamic systems in continuous time. In the field of video prediction, PhyDNet [75] drew on these ideas by explicitly disentangling the hidden state into physical dynamics branch Hphy and residual texture branch Hres. Unlike the soft constraints of PINNs, PhyDNet directly restricts convolution kernel weights via Moment Matching, making them approximate PDE finite difference operators on discrete grid: t ck kH xk = Filter Weights Moment Finite Difference Stencils, (7) where denotes the disentangled hidden state, is the spatial coordinate, and ck represents the partial differential coefficients. Furthermore, addressing the limitations of discrete time sampling, Latent ODEs [80] proposed by Rubanova et al. utilize continuous time ODE Solver to model hidden state evolution, effectively"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "handling temporal consistency issues under non-uniform sampling. Although these methods and variational inference models like SVG [76] made progress in short-term prediction, modeling based on 2D manifolds implies spatial continuity assumption. Once depth mutations caused by Occlusion occur, the optical flow field becomes non-differentiable, and PDE constraints immediately fail. This defect of being unable to model object permanence indicates theoretical limitation in solving strict 3D consistency on 2D proxy manifold."
        },
        {
            "title": "2.3.4 Implicit Continuous Fields",
            "content": "Addressing the theoretical limitations of 2D proxy manifolds in 3D consistency, academia turned to defining state fields directly in 3D Euclidean space. The establishment of this paradigm was built upon Mesh-based differentiable rendering works like SoftRas [114] and DIB-R [115], which verified the feasibility of calculating gradients I/V through smooth rasterization process. NeRF [81] further discarded discrete geometry, using MLPs to parameterize the scene as continuous coordinate mapping function FΘ : (x, d) (c, σ), and connecting the 3D field with 2D observations through differentiable Volume Rendering Integral. (1) Representation Efficiency & Frequency Fidelity. The evolution of Neural Radiance Fields is essentially process of seeking balance between parameter efficiency and signal fidelity. The challenges in this field have deepened from initial inference acceleration (introducing discrete representations) to maintaining frequency domain anti-aliasing characteristics in discrete space. (i) The Shift to Hybrid Representations. To break the efficiency bottleneck of pure MLP architectures, NVIDIAs Instant-NGP [84] introduced Multiresolution Hash Grids, using spatial hashing to map continuous coordinates to learnable feature table; while in the generative domain, EG3D [116] proposed Tri-plane representation, establishing the mainstream paradigm for 3D GANs. These methods (including TensoRF [117]) significantly improved training efficiency and geometric generation capabilities by introducing explicit spatial inductive biases. (ii) Aliasing & Signal Processing Correction. However, the aforementioned discretized representations (as well as point-wise sampling in original NeRF) introduced severe aliasing in high-frequency regions. Mip-NeRF [82] corrected this defect from signal processing perspective, pointing out that discrete sampling ignoring the sampling volume violates the Nyquist sampling theorem. By introducing Cone Tracing and Integrated Positional Encoding (IPE), Mip-NeRF calculated the feature expectation within Gaussian volume, revealing the essence of anti-aliasing in its mathematical form: γ(µ, Σ) = xN (µ,Σ)[γ(x)] sin(µ) exp (cid:18) 1 2 diag(Σ) (cid:19) , (8) where µ and Σ denote the mean vector and covariance matrix of the conical frustum, and represents the element-wise product. This formula reveals profound physical mechanism: the exponential decay term exp(Σ) essentially acts as an Adaptive Low-pass Filter. When the sampling cone radius increases (i.e., variance Σ increases, corresponding to distant views or low-resolution regions), high-frequency features are exponentially suppressed. To transfer this excellent anti-aliasing property to efficient grid representations, Zip-NeRF [118] further combined Multisampling with feature smoothing techniques, resolving the scale uncertainty inherent in hash grids. This series of evolutions is mathematically equivalent to the Uncertainty Principle in Fourier transforms: the wider the spatial localization (Σ is large), the narrower the frequency bandwidth, thereby mechanistically eliminating moire patterns and high-frequency artifacts, achieving unification of efficiency and fidelity."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "(2) Level Set Ambiguity & Eikonal Manifold Constraints. NeRFs density field σ suffers from physical ambiguity. When extracting surfaces, the artificially set threshold τ leads to Level Set Ambiguity. To obtain precise geometric surfaces, NeuS [85] and VolSDF [86] converted the representation from density field to Signed Distance Field (SDF). By introducing an unbiased Logistic transformation ϕs( (x)) and imposing an Eikonal regularization term: Lgeo = Ex[( (x)2 1)2], (9) where (x) is the signed distance function, and the gradient norm constraint 2 = 1 ensures physical validity. This constraint forces the gradient norm of the implicit field to be constant at 1, ensuring the zero-level set = {x (x) = 0} converges to smooth, closed manifold surface that satisfies physical constraints. Viewing from the perspective of manifold optimization, implicit continuous fields essentially trade Inference Latency for Geometric Completeness [85]. Due to the continuous differentiability of SDF, this paradigm constitutes an ideal basis for high-fidelity inverse rendering. It is not only suitable for reconstructing closed Watertight Manifolds to realize static asset digitization [86, 119], but also effectively avoids geometric holes common in explicit methods through Eikonal regularization-induced smoothing priors under sparse views [87]. However, its mathematical properties also define theoretical upper bound: the high sampling cost of volume integration O(Nsamples) makes it difficult to support high-frame-rate real-time interaction [81], and the smoothing assumption of continuous fields faces expressive bottlenecks when modeling dynamic scenes with drastic topological fractures [120, 89]."
        },
        {
            "title": "2.3.5 Explicit Lagrangian Primitives",
            "content": "Although implicit continuous fields established theoretical completeness for multi-view consistency, their sampling mechanism relying on volume integration constitutes computational bottleneck for real-time simulation. The 3D Gaussian Splatting (3DGS) proposal [89]CC2 marks the return of the representation form of the state field ΦIQ3 from an implicit field to explicit particles (as shown in Figure 12CR1(c)). This paradigm discretizes the scene into set of anisotropic Gaussian primitives Φ = {Gi(µ, Σ, α, SH)}M i=1 and reconstructs the projection operator as Rasterization. (1) Mechanisms of Static Representation. Unlike the ray marching of NeRF [81], 3DGS [89] utilizes the GPU sorting pipeline for acceleration, containing three key characteristics: (i) Rasterization Pipeline. The algorithm involves two key steps: first is Frustum Culling and Projection, projecting 3D Gaussian into 2D screen space covariance matrix Σ2D = JWΣ3DW JT; second is tiled radix sort, which is the computational bottleneck with complexity O(N k). By leveraging tile-based parallel rendering strategy, the method restricts computation to overlapping Gaussians and requires only α-blending on during rasterization, avoiding invalid sampling of empty space. (ii) Integral Duality. NeRF adopts Backward Pull, prone to gradient masking (C/σf ar 0). In contrast, 3DGS adopts Forward Push; explicit sparsity allows error gradients µ to bypass the MLP and backpropagate directly and sparsely to geometric parameters. This explicit gradient flow is the mathematical foundation for the efficient convergence of 3DGS. (iii) Adaptive Density Control. This approach can be viewed as variant of AMR (Adaptive Mesh Refinement). The core idea is: if the gradient is too large and variance is small (L > τ, Σ < ϵ), it is judged as underfitting and the Gaussian is cloned; if the gradient is large and variance is large, it is judged as overfitting and the Gaussian is split. Through this mechanism, the method dynamically adjusts the density of Lagrangian particles in response to the underlying optimization landscape. (2) Evolution towards 4D Dynamics. Addressing 4D spatiotemporal modeling, the explicit primitive paradigm has developed three main evolutionary paths based on how the time dimension is handled:"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 12: Key Mechanisms for Advanced Spacetime Modeling. taxonomy of core techniques underpinning modern models: (a) Full Spacetime Attention enables dense long-range dependencies; (b) Causal Masking ensures temporal causality; (c) 3D Gaussian Splatting offers explicit, differentiable 3D structure; (d) Object-Centric Slots decompose complex scenes into distinct entities. (i) Lagrangian Particle Tracking. As in PhysGaussian [93], it assumes Gaussian primitives possess material point properties, solving the equation of motion µ(t) = µ0 + (cid:82) v(τ)dτ by introducing continuum mechanics equations (ρ = σ + g). By embedding physical constraints into the optimization process, the method enables joint learning of visual appearance and physical behavior. (ii) Eulerian Tensor Decomposition. As in 4D-GS [121], the 4D spatiotemporal field is modeled as high-dimensional tensor , using CP or Tucker decomposition to reduce dimensionality: (x, y, z, t) r=1 ur(x) vr(y) wr(z) hr(t), (10) where denotes the outer product, is the tensor rank, and ur, vr, wr, hr represent the factor vectors along each dimension. This form optimizes storage complexity from O(N4) to O(N2), effectively supporting dynamic changes in topological structure. (iii) Canonical Deformation. As in Deformable-GS [95], it adopts static base with transient offsets formulation, predicting coordinate offsets µ via MLP, leveraging the spectral bias of MLPs to effectively capture high-frequency motion fields. The explicit primitive paradigm shows significant advantages in balancing high frame rate rendering and high-resolution reconstruction. However, its discrete nature introduces topological adaptability limitations, making it difficult to naturally handle fractures and fusions in fluid dynamics like implicit fields [122], indicating the need to introduce higher-order generative dynamics models."
        },
        {
            "title": "2.3.6 Generative Statistical Priors",
            "content": "In open-world generation tasks, observation conditions are extremely sparse, causing the problem to degenerate into an ill-posed one. In this phase, works utilize video diffusion models as implicit world"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "model priors, establishing an algorithm-data synergistic framework. (1) Algorithmic & Geometric Constraints. To elevate 2D priors to 3D consistency, academia has reconstructed optimization objectives and architectural designs: (i) Score Distillation Sampling (SDS) & Variational Correction. Unlike photometric loss, SDS [97] obtains gradients by calculating the score function of pre-trained diffusion model. Addressing the over-smoothing problem of SDS, VSD (Variational Score Distillation) [123] introduces variational distribution, minimizing the KL divergence between the generated distribution and the prior distribution, thereby recovering high-frequency texture details. (ii) Multi-View Geometric Attention. Pure 2D priors are difficult to guarantee multi-head consistency. Works like MVDream [99] modify the U-Net architecture, upgrading spatial self-attention to 3D correspondence attention. This design forces the model to perform feature alignment via camera parameters (R, T) when generating different views, achieving soft geometric consistency. (2) Scaled Data Foundation. To break the 3D data bottleneck, academia has adopted SyntheticReal-Generative hybrid construction strategy for large-scale dataset construction: (i) Aggregation. Objaverse-XL [101] integrated tens of millions of 3D assets collected from the internet, fundamentally alleviating the scarcity of large-scale 3D data. G-Objaverse [124] provided high-quality RGB-D-Normal triplets through physical rendering pipeline, becoming the standard source for training Large General Reconstruction Models (LGM). (ii) Real-world Perception. MVImgNet [125] and Co3D-v2 [126] provide millions of object-centric video sequences captured in real-world environments. While dense geometric ground truth is largely unavailable, these datasets play crucial role in reducing the domain discrepancy between synthetic and real data, particularly in appearance and texture distributions. (iii) Inverse Generative Engine. See3D [104] advances an automated data generation paradigm by coupling generative video models with geometric reconstruction. Specifically, large-scale pseudo-3D videos are synthesized using video diffusion models such as SV3D [127], followed by geometric inference via Dust3R [103] and rapid reconstruction through LGM, constructing closed-loop data production engine to achieve exponential asset expansion. The development of spatial consistency modeling exhibits clear iterative trajectory. Early methods relied on 2D proxy fitting, which gradually evolved into 3D implicit representations to improve geometric coherence. Subsequently, explicit formulations such as 3D Gaussian Splatting reintroduced computational efficiency and rendering scalability. Current trends indicate convergence toward hybrid architectures that combine explicit geometric primitives with implicit diffusion-based priors, leveraging the complementary strengths of both representations [124, 127]. Looking ahead, the research focus in this field is shifting from pure visual reconstruction to deep physical interaction modeling. On one hand, Neuro-symbolic Grounding will become the key to connecting semantic space and geometric space. Future models aim to establish differentiable mappings between LLM symbolic logic and numerical parameters, as shown in works like Eureka [128], to realize an endogenous understanding of object materials and force mechanisms, thus transcending pixel statistics-based imitation. On the other hand, the scope of spatial consistency is expanding to Action-Consistency. As World Models evolve towards interactive environments [129], Reinforcement Learning (RL) will be introduced into the generative loop, ensuring that the scene follows physical causality when responding to actions π(atst). To support this capability, the architectural level is expected to break the Cascaded Generation pipeline and shift towards End-to-End Native 4D Streaming, i.e., performing real-time streaming inference directly with compressed 4D Tokens [130]."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Zero-Shot & Anchoring Latent Temporal Inflation (2D Priors) Temporal Adaptation Frequency & Noise Correction Discrete Autoregressive Sequence Modeling Tokenization & Scaling Long-Sequence Optimization Native Spatiotemporal Continuous DiT Global 3D Attention Efficient Linearization Logical Consistency & Causal Reasoning Visual Chain-of-Thought Physics & Audio Causality e s l p f i o n s & o n Text2Video-Zero [131], FateZero [132], ControlNet [133], Tune-A-Video [134], etc. AnimateDiff [135], VideoCrafter1 [136], ModelScope [137], Gen-1 [138], etc. GFN [139], AFNO [140], FastInit [141], FreeNoise [142], etc. VideoPoet [143], W.A.L.T [144], MagViT-v2 [145], Cosmos [146], etc. VAR (Next-Scale) [147], FramePack [148], Show-o (Unified) [21], Diffusion Forcing [149], etc. Sora [2], HunyuanVideo [150], Lumiere [151], Veo/Veo 3 [152], etc. Video-TTT [130], Pyramid Flow [153], TeaCache [154], Movie Gen [155], etc. Visual CoT [156], UV-CoT [157], Mini-O3 [158], VChain [68], etc. Video-CoT [159], Think Sound [160], Physics-IQ [161], VCD [162], etc. Figure 13: Evolution of Temporal Consistency Paradigms: From Latent Inflation and Discrete Sequence Modeling to Native Spatiotemporal DiT and Causal World Simulators."
        },
        {
            "title": "2.4 Temporal Consistency",
            "content": "Through the modeling of spatial consistency (2.3), we have successfully constructed geometrically complete static world. However, the core value of World Model lies not in archiving the state of moment, but in rehearsing future trajectories. If spatial consistency is regarded as the Static Geometric Basis of the world model [163], temporal consistency constitutes the key element establishing its physical evolutionary Temporal Dynamics [1]. Mathematically, this process is equivalent to solving Multi-objective Optimization Problem constrained by both physical constraints Lphy and causal logic Lcausal within high-dimensional manifold space [164], as illustrated in Figure 14. Figure 14: Temporal Consistency and Identity Preservation. An illustration of the temporal attention mechanism ensuring identical subject features across consecutive frames (t0 tn). The generation process is governed by two key constraints: Physical Constraints (Lphy) enforce smoothness in motion trajectories to prevent flickering artifacts, while Causal Constraints (Lcausal) ensure logical progression of events (e.g., object permanence) throughout the timeline."
        },
        {
            "title": "2.4.1 From Frequency Stability to Physical Compliance",
            "content": "To objectively measure the evolutionary trajectory of temporal consistency technologies, evaluation metrics must transcend traditional perceptual dimensions. For long time, academia relied on FVD (Frechet Video Distance) [165] to assess video quality, but empirical studies indicate that FVD primarily characterizes the similarity of spatial feature distributions and has limitations in detecting temporal high-frequency Flickering and non-physical deformations. It must be pointed out that frequency stability in temporal consistency does not exist in isolation; it must be built upon the semantic foundation of modality alignment (2.2) and the topological constraints of spatial geometry (2.3). For instance, frontier models like Veo 3 [152] effectively suppress high-frequency artifacts and achieve physically compliant causal reasoning precisely by integrating MM-DiT (modal consistency) and 3DGS (spatial consistency). To fill this gap, Video Consistency Distance (VCD) [166] was designed as Reward-based Fine-tuning Objective. As shown in Figure 15, VCD measures the feature difference between the generated video ˆV and natural video in the temporal frequency spectrum: LVCD( ˆV) = Et (cid:104) Ft(ϕ( ˆvt)) Ft(ϕ( ˆvt1)) High-Pass (cid:105) , (11) where ϕ() denotes the feature extractor (e.g., CLIP Image Encoder [10]), and Ft represents the ShortTime Fourier Transform (STFT) along the time axis. The physical meaning of this formula is that motion features in the real world should possess continuity in the frequency domain, whereas temporal inconsistencies in generative models (such as texture flickering) will manifest as significant energy fluctuations in the high-frequency band. Figure 15: Video Consistency Analysis: Spatial vs. Frequency View. Traditional distribution-based metrics such as FVD primarily assess spatial perceptual quality and smooth motion in feature space, often overlooking high-frequency temporal flickering. In contrast, VCD explicitly models temporal consistency by analyzing the Fourier spectrum of feature embeddings, enabling the detection of subtle high-frequency noise and flicker artifacts invisible to spatial statistics. From Perception to Physical Reasoning Traditional evaluations focus on visual quality, while new standards have expanded to physical causal dimensions. As shown in Table 3, firstly, addressing temporal jitter, the Generative Prior Paradigm (World Model Priors) significantly reduces high-frequency"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 3: Empirical Evolution of Cross-Generational Models. Data synthesized from VBench (Temporal), Physics-IQ (Physics), and Veo 3 Technical Report (Reasoning) benchmarks. Generation Paradigm Rep. Model Temporal Consistency Physics Compliance Causal Reasoning Freq. Fidelity (VCD) (VBench Norm.) (Physics-IQ) (Task Success Rate) (Reward Penalty) Temporal Inflation Discrete AR Native DiT World Model Priors High (> 1.2) Medium ( 0.9) Low ( 0.6) Minimal (< 0.3) *Note: World Model scores are extrapolated based on relative improvements reported in [152] compared to Native DiT baselines. Refers to success rates on complex physical interaction tasks (e.g., object manipulation) as demonstrated in [167]. N/A (< 10%) Low ( 25%) Medium ( 45%) High (> 70%) AnimateDiff VideoPoet HunyuanVideo Google Veo 3 0.68 0.79 0.88 0.95 0.42 0.55 0.78 0.86 artifacts (VCD < 0.3) by introducing frequency domain reward fine-tuning. Secondly, to assess adherence to physical laws, Physics-IQ [161] is used to quantify model compliance in rigid body dynamics and fluid simulation. Finally, causal reasoning has become core evaluation dimension for models like Veo 3 [152]. Veo 3 demonstrates emergent capabilities in zero-shot physical interaction tasks (such as predicting domino toppling), with task success rate exceeding 70%, marking the evolution of video generation technology from pure visual simulation to dynamic systems capable of logical deduction."
        },
        {
            "title": "2.4.2 Latent Temporal Inflation",
            "content": "In the early stages when large-scale 4D data was not yet widespread, academia dedicated efforts to lowering the training threshold for video generation. Works represented by Tune-A-Video [134] and AnimateDiff [135] established the Temporal Inflation paradigm of Spatial Freeze, Temporal Insertion. Independence Assumption & ELBO Relaxation. The core strategy of this paradigm is to extend pre-trained 2D Text-to-Image (T2I) models into video generators, specifically by freezing the spatial convolution layers of the 2D U-Net and inserting learnable 1D temporal attention modules only between layers. Viewing from probabilistic graph perspective, this is essentially simplifying the joint distribution of video generation p(x1:T) into first-order Markov chain. Theoretical derivation shows that this relaxation of the Evidence Lower Bound (ELBO) ignores high-order dependencies of p(xtx<t1), leading to significant increase in the KL divergence term over long sequences. In practical applications (such as VideoCrafter1 [136]), this mathematical relaxation manifests as significant Semantic Drift: as the number of generated frames increases (T > 16), the identity features of the initial frame are gradually diluted by independent noise injection. Spatial Anchoring & Zero-shot Injection. To suppress semantic drift, early works explored trainingfree consistency enhancement paths. Text2Video-Zero [131] and FateZero [132] adopted Zero-shot Attention Injection mechanism, forcing subsequent frames to reuse the Key/Value feature matrices of the first frame. Meanwhile, inspired by ControlNet [133], some works introduced explicit geometric conditions (such as Depth/Pose) as spatial anchors. Empirical data shows that although these methods perform well in static backgrounds, when object motion amplitude exceeds 20% of the screen width, forced feature injection leads to obvious Smearing Artifacts, revealing the limitations of the inflation paradigm in handling complex dynamics. Frequency Filtering & Dynamic Correction. Besides temporal drift, existing temporal inflation models typically face the problem of Frequency Blindness. Since the temporal attention mechanism operates independently in the (B HW) dimension, it often exhibits lack of inductive bias when capturing high-frequency texture changes. Fourier spectral analysis reveals that generated videos exhibit significant energy loss in the high-frequency band (> 15Hz), visually manifesting as nonphysical texture flickering. Addressing the capture of long-range dependencies and high-frequency information, frequency domain learning offers novel perspective. Global Filter Networks (GFN) [139]"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "proposed using 2D Discrete Fourier Transform (2D DFT) instead of self-attention mechanisms, achieving long-range spatiotemporal interaction capture with O(N log N) complexity by performing global filtering operations in the frequency domain. Building on this, Adaptive Fourier Neural Operators (AFNO) [140] further optimized inter-channel information aggregation, proving that frequency domain Token Mixers can effectively overcome spatial blindness and precisely retain high-frequency details. Furthermore, addressing noise interference in sequence modeling, BERT4Rec [168] and Denoising SASRec [169] introduced uncertainty quantification mechanisms, achieving dynamic suppression of irrelevant perturbations by zeroing out gradients of high-noise samples during backpropagation (gradient pruning). In the video generation domain, FastInit [141] drew on these denoising ideas, proposing learning-based noise initialization strategy. This method discards traditional independent Gaussian sampling and instead trains lightweight inversion network to directly predict the optimal initial noise for the current frame based on spatiotemporal features of preceding frames, significantly enhancing generation coherence while suppressing latent space temporal high-frequency jitter. The Theoretical Boundary of Inflation. Although methods like FastInit [141] alleviate frequency domain flickering, the temporal inflation paradigm is perpetually limited by its 2D topological anchor. Since the core spatial convolution layers are frozen, the model is essentially performing minute elastic deformations on static images rather than generating true temporal dynamics. Empirical research [170] indicates that when facing large viewpoint transformations (such as an object rotating 180 degrees) or the emergence of new content, this class of models often produces severe texture stretching. This over-reliance on pre-trained 2D priors condemns it to the role of transitional solution. To capture true physical world dynamics, academia has turned to exploring native video architectures trained from scratch, which is the driving force behind the development of the discrete autoregressive paradigm."
        },
        {
            "title": "2.4.3 Discrete Autoregressive Modeling",
            "content": "To break the theoretical bottleneck of long-sequence modeling, VideoPoet [143], CogVideo [171], and W.A.L.T [144] drew on the scaling law of LLMs, establishing the two-stage autoregressive generation paradigm. By expanding the context window, this paradigm reconstructs video generation as longrange causal prediction of discrete Tokens. Causal 3D Tokenizer & Data Compression. The cornerstone of the discrete autoregressive paradigm is an efficient 3D VQ-VAE. Unlike image Tokenizers, video compression must strictly adhere to temporal causality. MagViT-v2 [145] innovatively introduced asymmetric Temporal Padding and Causal 3D Convolution, strictly limiting the receptive field of convolution kernels to the current frame and preceding moments, ensuring that future information does not leak during the compression process. Addressing reconstruction blurriness in low-motion scenes, VTokenizer-Plus [172] further introduced Object-Centric representation, significantly improving texture fidelity of static backgrounds by separating foreground and background codebooks. Memory Decay in Long Sequences. With the release of models like NVIDIA Cosmos [146], the AR paradigm has regained attention due to its superior data scaling capabilities. However, Error Accumulation remains the core challenge of this paradigm. According to sequence modeling theory [54], the distribution shift between Teacher Forcing during training and autoregressive generation during inference (Exposure Bias) causes minute inter-frame prediction errors to amplify exponentially with time step t. To suppress this sequence variance, VAR [173] proposed the Next-Scale Prediction mechanism, reconstructing the autoregressive process from pixel scanning to coarse-to-fine scale recursion, mathematically reducing inference steps from linear O(N) to logarithmic O(log N). Furthermore, FramePack [148] introduced frame context packing mechanism and bidirectional anti-drift sampling, combined with the PFP (Pretraining Frame Preservation) [174] objective, significantly improving reconstruction fidelity under long time sequences."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Return to Continuous Latent Space. Despite continuous architectural optimization, the nondifferentiability of the discretization operation zq = arg min ze ek constitutes an inherent optimization difficulty for this paradigm. Training typically relies on the Straight-Through Estimator (STE) [175] for approximation, but in high-dimensional video space (D > 4096), the gradient variance caused by STE (σ2 > 103) easily triggers codebook collapse [49]. This discretization gap limits the precision of AR models in generating minute textures and sub-pixel motion. Precisely this limitation has driven the technical focus to shift towards Continuous Latent Space, utilizing Diffusion Transformers to directly model continuous probability density on the manifold. Hybrid Transition: Fusing AR and Diffusion. Between pure AR and DiT, academia has explored fusion paths of the two, aiming to combine the long-range causality of AR with the high-fidelity decoding capability of Diffusion. First, at the inference level, Diffusion Forcing [149] proposed non-rigid sequence modeling scheme, modeling each time step as an independent diffusion process, supporting rollback and branch exploration during inference, breaking the traditional AR restriction of no return. Second, at the architectural level, Show-o [176] proposed the Unified Omni-Model paradigm. This method is not simple stacking of modules, but achieves isomorphic modeling of discrete tokens (for semantic understanding) and continuous tokens (for visual generation) within single set of weights. Through mixed masking mechanism, Show-o achieves bidirectional interoperability of understanding and generation in physical weights."
        },
        {
            "title": "2.4.4 Unified Spatiotemporal Modeling via DiT",
            "content": "Compared to the spatiotemporal fragmentation caused by the temporal inflation paradigm and the quantization loss brought by the discrete AR paradigm, the new generation of paradigms represented by Sora [2] and HunyuanVideo [150] established the current benchmark for temporal consistency in video generation by thoroughly returning to continuous latent space and adopting the Diffusion Transformer (DiT) architecture. This evolutionary path from Spatiotemporal Decoupling to Full Spatiotemporal Isomorphism is shown in Figure 16. Native Spatiotemporal Architecture. Native 3D DiT treats video as sequence of 3D Patches = W, its core advantage being the capture of non-local physical interactions through global receptive field. (i) Full Sequence Joint Attention. By introducing 3D-RoPE to calculate joint attention Attn = Softmax(QKT/ + M)V, the model can calculate joint attention across the full sequence. Empirical studies (such as Physics-IQ [161]) indicate that decomposition architectures which sever spatiotemporal connections are mathematically difficult to approximate the convective terms and long-range correlations in Navier-Stokes equations. Only the global spatiotemporal receptive field provided by full attention mechanisms can capture such non-local physical interactions. (ii) Manifold Diffeomorphism. On this basis, the generation process based on Flow Matching [55] corresponds mathematically to diffeomorphism on the manifold, enabling the model to smoothly recover minute texture details from Gaussian noise, eliminating the edge flickering caused by discretization. Computational Evolution: Linearization & Inference Acceleration. Although DiT established the image quality benchmark, the quadratic complexity of Transformers (O(N2)) causes VRAM usage to become physical obstacle in moving from short clips to long videos. (i) Linearization & Caching. On the architecture side, Video-TTT [130] introduced the Test-Time Training paradigm, compressing historical context into neural network weights, achieving memory retention for long videos while maintaining O(N) linear complexity. Complementary to this, Pyramid Flow [153] utilized the spatiotemporal redundancy of video, proposing pyramid flow matching mechanism, reducing the computational cost of high quality video generation by 5-10 times through hierarchical"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 16: Evolution of Video Generation Paradigms. The technical path advances from Temporal Inflation (prone to drift) and Discrete AR (quantization loss) to the current Native DiT. This paradigm achieves full spatiotemporal isomorphism, serving as the foundation for world models. decoupling strategy. On the inference side, TeaCache [154] exploited the extremely high similarity of feature outputs in adjacent time steps in diffusion models (Pearson correlation coefficient > 0.98), introducing training-free dynamic caching mechanism to achieve 2-3 times end-to-end acceleration with zero image quality loss. Convergence & Divergence in Industry. The industry has not simply piled up parameters but has demonstrated three distinct evolutionary routes: (i) Standardization vs. Heterogeneity. Works represented by Meta Movie Gen [177] established the standardized paradigm of DiT + Flow Matching, where the proposed temporally causal 3D VAE solved the temporal slice flickering problem in long videos. In contrast, Google DeepMind persisted with the Space-Time U-Net architecture in Lumiere and Veo [151, 152], avoiding the temporal inconsistency caused by cascaded super-resolution through the full spatiotemporal attention mechanism, defining the upper limit of high-fidelity simulation quality. (ii) Ecosystem & Controllability. Application-layer models like Runway Gen-3 [6] and ByteDance PixelDance [178] focus on fine-grained interaction, achieving complex instruction following through multimodal director modes and trajectory-level control. Meanwhile, open-source foundations like CogVideoX [179] and HunyuanVideo [150] lowered the fine-tuning threshold, directly promoting the development of the video fine-tuning ecosystem in the HuggingFace community."
        },
        {
            "title": "2.4.5 Logical Consistency and Causal Reasoning",
            "content": "Although DiT-based generative models have solved visual continuity, they still face challenges when dealing with long-range physical logic (such as causal irreversibility). To bridge this gap, academia is shifting from pure fitting paradigm to cognitive reasoning paradigm, mainly manifested in the exploration of two complementary directions: imagetext interleaving reasoning in multimodal perception models and temporal chain reasoning in generative video models."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Think-with-Image in Multimodal Perception. As the cognitive front-end of world models, LMMs are attempting to enhance logical capabilities by introducing the visual modality as Intermediate Reasoning Steps, rather than relying solely on text CoT. Works represented by Mini-O3 [158] and VisCoT [156] assist logical jumps by generating or retrieving images during the inference process. RECAP [180] further formalized this flow, proposing recursive Retrieve-Generate-Verify loop, utilizing visual information to compensate for texts deficiencies in spatial relation reasoning. UV-CoT [157] explored image-text thought alignment under unsupervised conditions. Although these works mainly focus on the perception and understanding side, their image-assisted thinking mechanism provides valuable architectural insights for generative models tasked with complex spatiotemporal logic. Chain-of-Frame & Temporal Causality. On the generation side, the core of temporal consistency has ascended from visual fluency to event causality. The model must understand the sequence of occurrence of physical events, not just pixel interpolation. Video-CoT [159] and Video Espresso [142] introduce the Chain-of-Frame paradigm, which decomposes video generation into keyframe planning and intermediate frame synthesis. In contrast to pixel-level autoregressive approaches, this framework explicitly deduces future key states in the latent space, forcing the model to determine causal nodes first, then generate the visual process. Think Sound [160] further extended this causality to the auditory modality, constraining the physical evolution of video via audio cues. By aligning the underlying causal graph structures across modalities, this approach enforces logical self-consistency throughout the full spatiotemporal span, mitigating the logical degradation that commonly emerges in long videos."
        },
        {
            "title": "2.5 Outlook of the Consistencies",
            "content": "Through the evolution of specialized models, three distinct computational engines have effectively emerged. Modal Consistency has addressed semantic translation across modalities; Spatial Consistency has progressed from coarse 2D approximations to explicit 3D primitives; and Temporal Consistency has advanced from simple frame interpolation toward causal world simulation. Yet treating these capabilities as independent optimization objectives introduces fundamental bottleneck. collection of highly specialized modules, regardless of individual sophistication, cannot constitute coherent world simulator in the absence of shared cognitive substrate. The central challenge therefore shifts from refining isolated components to achieving architectural unification. The future of world models lie in reaching equilibrium in which semantic understanding, geometric structure, and causal reasoning co-emerge within single parameter space. This requirement motivates the paradigm shift examined next: the emergence of the UMMs."
        },
        {
            "title": "3.1 The Rise of Large Multimodal Models",
            "content": "In previous chapters, Modal, Spatial, and Temporal Consistency were treated as independent technical dimensions. However, the construction of general world model ultimately hinges not on the isolated advancement of these capabilities, but on their coherent integration into unified cognitive system. Addressing this challenge requires moving beyond modular solutions toward architectures that can jointly reason across modalities, space, and time. The rise of Large Multimodal Models (LMMs), represented by LLaVA [23] and GPT-4V [181], marks decisive paradigm shift from single-task specialists toward general cognitive entities."
        },
        {
            "title": "3.1.1 LLM as a Core Cognitive Base",
            "content": "The core design philosophy of modern LMMs [182, 7, 183] is to treat the pre-trained LLM [184, 185] as universal reasoning engine [186, 187]. Its essence lies in mapping heterogeneous modality data into"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "the LLMs Word Embedding Space [16, 188]. This process is not simple dimension transformation but is achieved through specific translator mechanisms (e.g., visual connectors or adapters) [14, 23, 189] to realize semantic alignment and conversion across modalities [10]. (1) Modal Tokenization & Representation Bridging. In the specific implementation path, the model first utilizes Visual Encoder (such as CLIP-ViT [10] or SigLIP [190]) to extract high-dimensional feature maps RHWC. To enable the LLM to process these non-text signals, LLaVA [23] and its subsequent improvements [18, 188] employ an MLP or Linear Projection Layer Wϕ to directly project image patch features into set of Visual Tokens = {v1, v2, . . . , vn} (where vi Rd) that are dimensionally aligned with the text tokens. These tokens are then concatenated with text embeddings as soft prompts to form hybrid input sequence: (cid:104) Xinput = (1) text, . . . , (m) text, v1, . . . , vn (cid:105) , (12) where Xinput represents the aligned multimodal sequence, etext denotes the text embeddings, and Rd is the visual token. From this perspective, the physical significance of alignment is to enable the LLMs self-attention mechanism to compute the association entropy between visual tokens in the same manner as it processes text tokens. (2) From Rigid Projection to Perceiver Bottleneck. To address the issue of sequence length redundancy potentially caused by direct projection, BLIP-2 [16] and Flamingo [14]as representative architectures of Q-Former and Perceiver Resampler methodsutilize fixed number of Learned Queries as intermediaries to filter out redundant information from massive Pixel Features. This mechanism is mathematically equivalent to form of semantic pooling: it forces the model to compress thousands of Spatial Patches into dozens of tokens with highly abstract semantics. This not only resolves the problem of computational overhead but also theoretically satisfies the Information Bottleneck hypothesis [191]; by constraining the capacity of I(Z; Xvis), the model is forced to retain only those features conducive to Language Reasoning during the alignment process. Furthermore, experiments from DeepSeek-VL [4] and InternVL [25] demonstrate that this alignment process can induce the formation of cross-modal physical manifold within the LLM during the pre-alignment stage, allowing the model to maintain fundamental logical consistency even in unseen scenarios."
        },
        {
            "title": "3.1.2 Cognitive Evolution as a Multimodal",
            "content": "The emergence of LMMs transcends the traditional end-to-end mapping paradigm [192, 193], endowing systems with resource scheduling and logic coordination capabilities akin to multimodal operating system. Within this architecture, the LLM no longer functions merely as feature processor but serves as the Kernel [194, 195], responsible for managing complex instruction flows and invoking heterogeneous Specialized Modules on demand [196, 59, 197]. (1) Hierarchical Task Planning & Programmatic Instruction. To address semantic drift in longhorizon tasks, LMMs demonstrate capability for recursive decomposition, breaking down high-level ambiguous instructions into atomic sub-tasks. Distinct from earlier static mapping, VisProg [198] and ViperGPT [197] proposed the visual programmatic reasoning paradigm, which parses visual queries into executable python code flows, achieving logical self-consistency by combining low-level visual operators. The essence of this mechanismtransforming physical instructions into logical programsis the utilization of the LLMs in-context learning to project open-domain problems onto constrained operator space. Furthermore, PaLM-E [199] and Voyager [200] have demonstrated that by incorporating real-time feedback from multimodal perception, LLMs can perform hierarchical search within latent action space, maintaining long-term consistency in dynamic environments."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 17: Tool-use & Closed-loop Verification. Diagram illustrating the ReAct paradigm, where an AI agent cyclically calls external detection and correction tools to refine generation outputs. (2) Tool-use & Closed-loop Verification. To rectify physical hallucination during the generation process, LMMs have evolved closed-loop refinement mechanism based on test-time compute. Frameworks represented by Visual ChatGPT [195] and HuggingGPT [201] utilize the ReAct (Reasoning and Acting) paradigm [202], as illustrated in Figure 17. This allows the model to actively suspend the generation path to invoke external expert models (e.g., calling detector to verify spatial relations or diffusion model to redraw irrational textures). Architectures like Chameleon [59] and Auto-GPT [203] further introduce feedback evaluation stage: by calculating the mutual information or geometric constraint deviation ϕ between the generated intermediate state and the original instruction, the model can execute gradient-guided iterative refinement. 3."
        },
        {
            "title": "Integration of Modal and Spatial Consistency",
            "content": "The fusion of modality and spatial consistency constitutes core bridge toward physical world simulators [204, 205]. This profound cross-domain synergy aims to resolve the persistent issue of rich semantics but collapsed geometry in traditional generative models, with its core utility manifesting in two dimensions. In terms of semantic-spatial alignment, it empowers models with the capability for precise responses to complex spatial instructions (such as occlusion, surrounding, and perspective stacking) [206], achieving qualitative leap in controllability from text describes texture to language defines layout [207], as shown in Figure 18. In terms of geometric-physical grounding, it forces generated content to adhere to the geometric laws of the objective world, effectively eliminating structural non-rigid deformation and spatial misalignment hallucinations under multi-view conditions [208]. This integration ensures that AI is no longer confined to the statistical fitting of 2D pixels but possesses the capacity to infer spatiotemporal dynamics within 3D manifold [209, 151]. In current research, the deep integration of modality and spatial consistency presents four parallel technical paths, as illustrated in Figure 19, exploring unique paradigms of implicit emergence, explicit synergy, structured isomorphism, and reinforcement learning. Pixel space manipulation focuses on leveraging the scale effects of large-scale multimodal corpora to internalize geometric transformations"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 18: Modal + Spatial Consistency: Language Controls Precise Spatial Relations. The model combines modality and spatial consistency, using language instructions to precisely control the spatial relations between the subject (Doge) and objects (e.g., Sit ON the box, Hide BEHIND the cushion). as implicit semantic mappings, achieving intuitive instruction as space control within universal generation [210, 211, 212]. In parallel, view space mapping introduces camera poses and depth maps as explicit geometric conditions, allowing semantic and geometric flows to co-exist and synergize on 2D plane through cross-attention mechanisms, effectively balancing generative flexibility with perspective accuracy [208, 213, 214]. Meanwhile, volume space representation adopts the world coordinate system as its foundation, anchoring semantic features directly to neural volume fields or 3D Gaussian primitives, making spatial consistency an intrinsic physical attribute of the representation [215, 216]. Finally, reinforcement learning addresses the bag-of-words deficiencies in compositional instructions by introducing the System-2 Reasoning paradigm. Through region-temporal decoupling and inferenceside scaling mechanisms, it elevates the generation process from pure statistical sampling to planned solution equipped with logical verification [217, 218, 219]. These four paradigms are not linear replacements but are complementary and symbiotic; they collectively expand the boundaries of semantics-space fusion in world models from the four dimensions of data-driven generalization, conditional control flexibility, physical modeling precision, and logical reasoning robustness."
        },
        {
            "title": "3.2.1 Pixel Space Manipulation",
            "content": "The core philosophy of this paradigm lies in anchoring on data distribution, explicitly trading geometric priors for scale [97, 220]. Unlike traditional graphics that rely on expensive, hard-coded geometric priors [81, 84], pixel space manipulation advocates for constructing joint distribution pθ(ximg, c) of image-text and spatiotemporal data [221] based on pre-trained 2D generative bases (such as Latent Diffusion or Autoregressive Transformers) [222, 223]. Mathematically, this is equivalent to assuming that the massive volume of 2D projection data {ximg,i}N i=1 is sufficient to cover the topological structure of the high-dimensional 3D manifold Mworld [208]. In this context, Modal Consistency is manifested as the semantic alignment of conditional probability p(ximgcsem) [10, 133], while Spatial Consistency spontaneously emerges as an outcome of optimizing the joint distribution when reconstruction error is minimized [224, 135]. (1) Instruction-Driven Image Editing. To address the common issue of geometric collapse in textbased editing (e.g. non-physical distortion of the background when instructing dog to sit), instruction-"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "y t n a f i o c s o i S Pixel Space Manipulation Instruction-Driven Image Editing EditWorld [210], Step-1X-Edit [225], SGEdit [211], etc. General Image Generation DreamLLM [226], UniReal [227], MENTOR [228], MIO [212], etc. View Space Mapping Pose-Aligned Coupled Training Conditional 3D Generation Zero-1-to-3 [208], MoAI [213], Scaling Transformer [229], Wonder3D [214], SyncDreamer [106], etc. DreamFusion [97], MVDream [99], RealFusion [230], Hunyuan3D-Omni [231], LangScene-X [232], GSFixer [233], NeRF-HuGS [234], etc. Volume Space Representation Multimodal Alignment ULIP-2 [235], OpenShape [215], ShapeLLM-Omni [236], Genesis [237], Viewset Diffusion [238], etc. 3D Understanding or Editing LERF [239], Instruct-NeRF2NeRF [240], CORE-3D [241], CLIP-NeRF [242], Lift3D [243], SKED [244], ICE-G [245], Lang3D-XL [216], etc. DDPO [217], AlignProp [246], R-DPO [247], TTPO [218], SPO [248], Layout-CoT [219], etc. Reinforcement Learning Figure 19: Evolution of Modal Consistency and Spatial Consistency. driven image editing has established hybrid paradigm integrating gradient decoupling & update and attention injection as gating. This paradigm aims to resolve the intrinsic contradiction between semantic reconstruction and structural preservation by constructing orthogonal control paths, achieving heavy semantic bridge [249], light weight structural constraint architecture as illustrated in Figure 20. Gradient Decoupling & Update. To effectively decouple and protect the original spatial layout Sorig while injecting new semantics c, mainstream paradigms (such as ControlNet [133] or IP-Adapter [250]) employ structured decoupling architecture. By freezing the pre-trained base (e.g., SDXL) and only fine-tuning the side-network or decoupled cross-attention, the model constructs gradient update path on the parameter manifold that is orthogonal to the base: Lprior θL = θbase (cid:123)(cid:122) (cid:125) 0 (Frozen) (cid:124) + θadapter (cid:123)(cid:122) Semantics Ledit (cid:125) (cid:124) , (13) where θbase represents the frozen parameters of the base model, and θadapter denotes the trainable parameters of the side-network. This ensures that the physical common sense (e.g., lighting, occlusion) internalized within the base remains undisturbed. Attention Injection as Gating. During the inference phase, Prompt-to-Prompt [249] and MasaCtrl [251] reveal strong correlation between cross-attention maps and spatial layouts. To maintain spatial consistency, the model injects the attention map Msrc attn of the original image into the editing steps as geometric hard-gating mechanism: Attnedit(Q, K, ) α Softmax (cid:19) (cid:18) QKT + (1 α) Msrc attn, (14) where Msrc attn denotes the attention map preserved from the source image to guide spatial layout, and α is the injection strength coefficient. Combined with the MLLM Semantic Hub mechanism proposed by Step-1X Edit [225], this method successfully achieves semantic change with topological conservation. Subsequent work such as EditWorld [210] further introduced post-edit closed-loop, utilizing SAM masks for second-order geometric verification to resolve pixel artifacts at object edges. (2) General Image Generation. General image generation is undergoing paradigm reconstruction from external plug-in alignment to native full-duplex modeling, aiming to directly capture the"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 20: Instruction-Driven Image Editing. The diagram illustrating structure-preserving image editing, where Doge is repositioned via text instruction. It depicts the mechanisms of gradient decoupling (frozen base) and attention injection (geometric gating) to maintain spatial consistency. spatiotemporal dynamics distribution of the physical world through end-to-end joint training. The paradigm shift in this field is characterized as: From external alignment (CLIP-based) to End-to-End Interleaved Modeling. This transition no longer relies on frozen feature extractors but instead constructs generative foundation where modality and space are tightly coupled through joint modeling [226], video stream supervision [252], and lightweight connections [228]. (i) Joint Modeling Breaking Information Bottleneck. Traditional two-stage models (such as DALL-E 2) are limited by the modality isolation of the CLIP encoder, which results in the loss of spatial relations during feature compression. new generation of models, such as DreamLLM [226] and Emu [253], abandons this design in favor of directly performing joint modeling on raw image-text sequences using autoregressive or diffusion approaches: Ljoint = log (cid:0)ximg,t ximg,<t, xtxt(cid:1) log t (cid:16) xtxt ximg, xtxt <j (cid:17) , (15) where Ljoint denotes the unified training objective, ximg,t represents the image tokens at step t, and xtxt corresponds to the text tokens. This full-duplex information flow enables the model to capture pixel-level spatial constraints implicit in descriptions such as cat on table. (ii) Video as World Simulator. Transcending simple geometric perspective transformations, empirical research on Sora [2] reveals the profound value of video data: it provides endogenous supervision signals regarding physical plausibility. Unlike static images, temporal dependencies in video streams force the model to learn object permanence [252, 254]for instance, inferring that an occluded object has not disappeared but continues to move along its trajectory. This self-supervision compels the model to construct dynamics model within the latent space that conforms to physical conservation laws (e.g., gravity, collision, fluid dynamics) [163], thereby elevating the generative model from mere pixel statistical fitting to predictive simulation of physical world evolution [204]. (iii) Lightweight Connection Layer. To balance computational efficiency with multimodal alignment, the perceiver resampler in Flamingo [14] and the MLP connection layer design in Mentor [228] demonstrate how visual features can be projected onto the LLMs semantic manifold using minimal parameters. This proves that as long as the base is sufficiently powerful, simple linear mappings can maintain complex space-semantics correspondence."
        },
        {
            "title": "3.2.2 View Space Mapping",
            "content": "Figure 21: Pose-Aligned View Synthesis. Diagram illustrating pose-conditioned generation using decoupled backbone and epipolar attention. The target view features split display of RGB texture and purple-blue Normal Map, representing cross-domain mutual supervision for geometric accuracy. Pose-Aligned Coupled Training. The core philosophy of this paradigm lies in abandoning purely data-driven black-box assumptions and injecting 3D geometric information as structured condition variables τ = {Pt, D} (where Pt SE(3) is the camera pose and is the depth prior) into pre-trained diffusion model [133, 208], as illustrated in Figure 21. Its mathematical essence is the construction of conditional denoising distribution constrained by geometry: Lview = Ez,t,c,τ,ϵ (cid:104) ϵ ϵθ(zt, t, c, τ)2 2 (cid:105) + λRconsist, (16) where zt represents the noisy latent at timestep t, ϵθ is the noise prediction network conditioned on geometry τ, and Rconsist denotes the regularization term for multi-view consistency. The successful implementation of this paradigm relies on the following three synergistic mechanisms: (i) Backbone Decoupling & Injection. To circumvent catastrophic forgetting while preserving the semantic generation capability of the pre-trained model, the academic community has established design of frozen backbone and bypass control. Represented by Zero-1-to-3 [208] and ControlNet [133], this approach achieves selective gradient flow by locking the backbone network Flocked and introducing trainable copy Fcopy: hout = Flocked(hin) + (Fcopy(hin, τ)). This zero convolution strategy ensures that the model generates photo-realistic textures while precisely executing geometric instructions. (ii) Structured Sparse Attention. To address the janus problem in multi-view generation, models introduce structured sparse attention. MVDream [99] and SyncDreamer [106] innovatively transform the epipolar geometry constraints in 3D space into an attention mask: Attn(Qi, Kj, Vj) exp (cid:32) QiKT (cid:33) + Mepi(i, j) , (17) where Qi and Kj denote features from view and view j, respectively, and Mepi represents the geometric bias derived from epipolar constraints. This mechanism forces tokens from different views"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "to interact only with their geometrically corresponding epipolar line regions, thereby converting geometric hard constraints into soft inductive bias within the attention mechanism. (iii) Cross-Domain Attention Regularization. To further enhance geometric accuracy, Wonder3D [214] and MoAI [213] achieve mutual supervision between texture semantics and geometric structure by generating RGB and normal maps in parallel and introducing cross-domain attention injection frgb fgeo. Coupled with 3D consistent noise initialization strategy (initializing noise based on the camera projection matrix), this paradigm successfully breaks the i.i.d. assumption from the initial state, achieving transition from simple image generation to geometrically controllable generation."
        },
        {
            "title": "3.2.3 Volume Space Representation",
            "content": "Unlike the previous two paradigms that simulate 3D on 2D plane, volume space representation chooses to directly confront the three-dimensional essence of objects [97, 230]. The core philosophy of this direction is to utilize 3D Native Representations (NeRF, 3D Gaussian Splatting) as the primary layer of architectural abstraction. This makes spatial consistency an intrinsic property of the representation, while modal consistency is transformed into synergistic optimization problem between cross-modal queries and differentiable rendering. (1) Conditional 3D Generation: From 2D Distillation to Video Manifold Constraints. Conditional 3D generation aims to overcome the bottleneck of 3D data scarcity by restructuring pre-trained generative models as frozen cognitive engines, establishing technical trajectory that evolves from 2D semantic distillation toward video manifold constraints. Due to the extreme scarcity of high-quality 3D-text data pairs (which are 23 orders of magnitude fewer than 2D data), this direction no longer seeks to train 3D generators from scratch. Instead, it focuses on discovering and transferring the spatial intelligence inherent in pre-trained 2D or video models [97, 104]. (i) Gradient Flow from 2D Priors. DreamFusion [97] and RealFusion [230] established the foundational formula for this field: Score Distillation Sampling (SDS). Its core principle is not to optimize pixel error, but to optimize parameterized 3D field θ (such as NeRF or 3DGS) such that the image rendered from any viewpoint, ximg = g(θ, Pt), resides in the low-energy regions of 2D diffusion model: θLSDS = Et,ϵ (cid:20) wguidance (ϵθ(zt, t) ϵ) (cid:21) , ximg θ (18) where wguidance is the weighting factor, ϵθ is the predicted noise from the frozen diffusion model, and ximg represents the Jacobian of the differentiable renderer. This formula indicates that the semantic θ residual computed by the 2D model is backpropagated through the Jacobian matrix differentiable renderer to directly sculpt the 3D geometry. ximg θ of the (ii) Video Manifold as Dynamic 3D Prior. To address the janus problem caused by 2D priors, recent research has shifted toward leveraging the physical consistency inherent in Video Diffusion Models (VDMs). The core hypothesis is that Temporal Correlation = Spatial Consistency. See3D [104] and V3D [255] propose utilizing video generative models as multi-view generators. By fine-tuning the VDM, the time axis is implicitly reconstructed as camera trajectory Pt (e.g., an orbital viewpoint): p(ximg, novel ximg, ref) pvideo(ximg,t+1 ximg,t, motion cond), where pvideo denotes the transition probability learned by the video model, and motion cond represents the camera trajectory condition. Under this paradigm, SV3D [127] utilizes the temporal attention layer of the video model as soft epipolar constraint, forcing the generation of multi-view sequence with geometric continuity. Subsequently, SDS is used to distill this dynamic video prior into static 3D assets, fundamentally resolving viewpoint conflicts. (19)"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "(iii) Prior-Constraint Two-Stage Loop. Given the ill-posedness of single-view generation, Magic123 [256] and One-2-3-45 [257] established the paradigm of coarse generation fine optimization. Current trends involve using video models [231] to rapidly generate multi-views as an initial guess, followed by geometry refinement using SDS in combination with lightweight solver [258]. This strategy of video initialization and physics fine-tuning preserves semantic richness while utilizing video priors to rectify the topological plausibility of the 3D structure. Figure 22: Multimodal Alignment. The diagram illustrating the convergence of Text, Image, and 3D Point Cloud into central Unified Embedding. It depicts contrastive alignment for inputs and generative tokenization for 3D data integration. (2) Multimodal Alignment. Multimodal alignment aims to construct universal representations that span geometry and semantics. By establishing dual-track mechanism of discriminative metric alignment and generative interaction fusion, it breaks the long-standing representation silo dilemma of 3D data. To process 3D data as CLIP processes images, this direction focuses on building Unified Embedding Space Zuni. The technical philosophy is to transform spatial consistency into structured constraints during network forward propagation, as shown in Figure 22. Contrastive Metric Learning. ULIP-2 [235] and OpenShape [215] employ large-scale triplet contrastive learning. By mining hard negatives and utilizing the InfoNCE loss, the feature distribution of the 3D encoder (PointNet++ or Transformer) is forced to align with CLIPs text/image space: Lalign = log exp (z3D ztxt/τ) z3D zj txt/τ exp (cid:16) (cid:17) , (20) where z3D and ztxt represent the feature embeddings of the 3D shape and text, respectively, and τ is the temperature parameter. Genesis [237] further extends this to 4D spatiotemporal alignment"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "by introducing cross-view attention in voxel space to fuse video and LiDAR modalities, achieving alignment across space-time dimensions. Generative Integration. Unlike the holistic alignment of contrastive learning, ShapeLLM-Omni [236] and ViewSetDiffusion [238] introduce 3D VQ-VAE to discretize continuous geometry into token sequences. This enables the LLM to directly read and generate 3D geometry, achieving generative interaction between modalities rather than simple retrieval matching. (3) 3D Understanding & Editing: Semantic Lifting. 3D understanding and editing aim to endow 3D geometric entities with the dual capabilities of semantic perception and linguistic manipulation. The core paradigm involves injecting the cognitive priors of 2D vision foundation models into 3D space via Semantic Lifting, constructing mapping F(x, y, z) (cid:55) RDclip. Semantic Field Construction. LERF [239] and Lang3D-XL [216] propose training semantic head in parallel with the color head of NeRF. This module learns CLIP feature fields through multi-scale supervision, enabling every coordinate point p(x, y, z) in space to respond to natural language queries (e.g., Find the crack on the chair). SKED [244] and CoRe-3D [241] introduce hierarchical semantic fields, embedding instance-part-material hierarchies into the representation to solve fine-grained semantic localization problems. Language-Driven Topology Editing. For editing tasks, CLIP-NeRF [242] utilizes decoupled latent mapping to achieve near-instant modifications of shape and appearance. InstructNeRF2NeRF [259] employs an iterative dataset update strategy: it first modifies the rendering view images using InstructPix2Pix and then uses the modified images as Pseudo-GT to back-update the NeRF. Lift3D [243] and ICE-G [245] introduce canonical space constraints to ensure that topological structures do not collapse even during significant geometric deformations, such as instructing cat to stand up."
        },
        {
            "title": "3.2.4 Reinforcement Learning for Modal-Spatial Alignment",
            "content": "Despite the explicit geometric conditions provided by architectures such as ControlNet [133], LMMs still frequently exhibit severe modal-spatial misalignment when processing compositional instructions (e.g., attribute binding: Red cat on blue car) [206]. To address this bag-of-words model deficiency, the academic community is undergoing paradigm shift from black-box optimization toward System-2 Reasoning [207]. This paradigm evolution can be summarized into three stages: Discriminator-Guided Explicit Anchoring. Early efforts focused on utilizing off-the-shelf visual discriminators as an external reward function to forcibly establish the correspondence between text prompts and bounding boxes. Black-box Discrete Optimization. DDPO [217] models diffusion denoising as Markov Decision Process (MDP). For spatial instructions, it introduces an open-vocabulary detector (such as GroundingDINO [260]) to compute an IoU reward. This represents loosely-coupled fusion; while it enhances object recall, the sparsity of the reward signal makes it difficult to resolve complex attribute binding. White-box Gradient Backpropagation. AlignProp [246] proposes fine-tuning the discriminator into differentiable reward model. This establishes an end-to-end gradient path pixelsLalign, allowing spatial errors to back-propagate directly to the denoising network, thereby achieving pixel-level precision in refinement. Region-Temporal Decoupling. To prevent global rewards from confusing semantics with spatial information, subsequent work shifted toward fine-grained control."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "R-DPO [247] proposed sub-manifold optimization under spatial masks. Unlike traditional DPO, it decomposes the image ximg and text into several local pairs (xk sub), ensuring that specific modal attributes (e.g., Red) only back-propagate to specific spatial regions (e.g., within the coordinates of the Cat): crop, ck LR-DPO = (cid:34) (cid:32) (xw,xl)Bk log σ β log πθ(xk πref(xk img, ck) img, ck) β log πθ(xk πref(xk img, ck) ck) img, (cid:33)(cid:35) , (21) where Bk represents the local preference dataset for region k, xw and xl denote the winning and losing image crops respectively, and σ is the sigmoid function. Concurrently, SPO [39] leverages the frequency characteristics of diffusion models by adopting time-division multiplexing strategy: focusing on spatial IoU optimization during the early stages of denoising (t [T, T/2]) and switching to semantic optimization in the later stages (t [T/2, 0]) to avoid gradient conflicts. Furthermore, DRaFT [261] utilizes VLMs to generate natural language critiques regarding spatial errors and maps them to dense reward map RHW. This marks the transition of RL alignment from discrete boxes to continuous pixel fields, enabling generative models to comprehend extremely subtle spatial-modal instructions such as the left leg is distorted. TTT & Visual CoT. Following the success of DeepSeek-R1 and OpenAI o1 in demonstrating the efficacy of inference-side scaling, recent research has begun introducing RL into the inference-time stage of generation, achieving strong logical fusion between modality and space. Test-Time Preference Optimization (TTPO). To address the insufficient perceptual quality of pre-trained models under specific distributions, TTPO [218] proposes an on-the-fly optimization mechanism. This method avoids heavy re-training by using lightweight reward model (such as an image quality score) to iteratively update the latent variable during the inference stage. While this work primarily validates its effectiveness in image restoration tasks, this test-time fine-tuning paradigm provides general compute-for-quality path for resolving highly counter-intuitive generative tasks. Visual Chain-of-Thought (Visual CoT). To resolve the logic breaks inherent in one-step generation, LayoutCoT [219] borrow the reasoning paradigm of LLMs. This approach decomposes the generation process into an explicit chain: Planning Alignment Generation. The model first generates discrete layout plan in low-dimensional space and employs RL to perform logical verification on this plan. Only chains-of-thought that pass verification are decoded into pixels. This mechanism essentially moves System-2 logic verification to the front end, fundamentally eliminating hallucinations such as interpenetration or spatial misalignment. 3."
        },
        {
            "title": "Integration of Modal and Temporal Consistency",
            "content": "The deep integration of modality and temporal consistency marks the formal transition of Generative AI from the Frozen Moment of static images toward the Continuous Deduction of the dynamic world as illustrated in Figure 23 [2]. The core utility of this dimension lies in constructing Probabilistic Simulation of Spatiotemporal Causality: at the Semantic Level, it ensures that video content strictly adheres to the definitions of text or image instructions (e.g., Blooming, Running), thereby eliminating cross-modal semantic drift [262, 151]; at the Dynamics Level, it endows the model with an endogenous understanding of Object Permanence and Physical Conservation Laws, ensuring that the generated frame sequence is no longer random stacking of discrete pixels, but rather Continuous Manifold consistent with logical evolution [221, 263]. This fusion fundamentally resolves chronic issues in traditional video generation, such as motion flickering, temporal logic chaos, and long video collapse. Based on this objective, current exploration paths present Four Progressive Technical Paradigms as shown in Figure 24: End-to-End Scalable Modeling follows the data philosophy of Brute Force"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "with Data, relying on Diffusion Models and Autoregressive Architectures to validate the Scaling Law, aiming to learn general physical simulator directly from massive data [209, 264, 265]; Explicit Structured Control targets the controllability requirements of industrial applications by introducing motion vectors, trajectory heatmaps, and orthogonal decoupling mechanisms to explicitly inject human intent into the generation process, addressing the ambiguity issues of end-to-end models [266, 178, 267]; meanwhile, the Unified Comprehension and Generation Symbiosis Architecture attempts to break the barriers between perception and generation through shared representation and bi-directional adaptation, constructing general agent with closed loop of perceiving and acting [268, 269]; finally, Reinforcement Learning Driven Alignment addresses the non-convexity of SFT (Supervised FineTuning) in optimizing modal semantic and temporal dynamics by constructing Multi-dimensional Reward Manifold. By integrating DPO and Self-Refinement mechanisms, this paradigm achieves joint optimization of alignment targets, driving the model to surpass binary games and converge to the Pareto Frontier of spatiotemporal trade-offs [270, 271]. These four paradigms collectively build complete architecture for modality and temporal intelligence from the dimensions of General Foundation, Controllable Interface, Cognitive Top-level, and Value Optimization. Figure 23: Modal + Temporal Consistency: Language Controls Time Evolution. The model integrates modality and temporal consistency, using language instructions to control the time evolution process (e.g., the cherry blossom tree behind the dog blooms and scatters across T1-Winter, T2-Spring, T3-Late Spring), ensuring coherent changes over time."
        },
        {
            "title": "3.3.1 End-to-End Scalable Modeling",
            "content": "End-to-End Scalable Modeling represents paradigm shift in the field of video generation from Divide and Conquer toward Unified Field. Its core objective is to validate the efficacy of the Scaling Law on high-dimensional spatiotemporal manifoldsspecifically, by synergistically expanding both model and data scales to directly fit the joint distribution p(ximgc) from multi-modal inputs to video outputs. Unlike earlier cascaded pipelines that relied heavily on hand-crafted interpolation and super-resolution modules, this paradigm is dedicated to constructing general physical simulator, driving the industrialization of models from Sora [2] to Wan 2.1 [273]. (1) Diffusion Model. As the core engine of end-to-end video generation, the Diffusion Model has completely restructured the technical path from Image Animation to Native World Simulation by"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Diffusion Model Sora [272], Imagen Video [221], Wan 2.1 [273], Vidu [274], CogVideo [171], Make-A-Video [275], UniVG [276], Pika [277], Video Diffusion Models [209], Hunyuan Video [278], SVD [263], Kling [279], Runway Gen [138], Tar [280], etc. End-to-End Scalable Modeling Autoregressive Model VideoPoet [143], RFLAV [281], UniForm [282], Ovi [264], VILA-U [283], NoVA [284], etc. e s C d o t v n i C o T Autoregressive-Diffusion Model NFD [285], ACDC [286], HybridVLA [265], DiCoDe [287], ARLON [288], RFLAV [281], AR-Diffusion [289], CausVid [290], VLOGGER [291], etc. Motion-Geometry Explicit Encoding VAST 1.0 [266], CMVC [292], VideoComposer [262], Control-A-Video [293], Diverse and Aligned [294], Gen-1 [138], TM2D [295], etc. Explicit Structured Control Start-End Frame Anchoring and Interpolation Make Pixels Dance [178], Videogen [296], Show-1 [297], InteractivateVideo [298], KeyVID [299], etc. Unified Comprehension and Generation Symbiosis Architecture Reinforcement Learning Multi-Condition Decoupling Architecture MagicEdit [300], Swap Attention [301], MoonShot [302], CCEdit [303], TATS [304], FancyVideo [267], etc. Shared Representation Bidirectional Synergy HERMES [305], Gaia-1 [205], UniVid [306], Phenaki [268], Unified Discrete Diffusion [307], etc. Pre-training Driven Synergistic Adaptation Omni-Video [269], HunyuanCustom [308], merv [309], HermesFlow [310], VIMI [311], etc. VideoDPO [270], T2V-Turbo [312], Video-STaR [313], VPO [271], VideoScore [314], etc. Figure 24: Evolution of Modal Consistency and Temporal Consistency. validating the Scaling Law within the Latent Space Z. To support this complex goal of physical consistency, the evolution of modern architecture is no longer confined to simple denoising iterations; instead, it has systematically reshaped four core pillars: from the ODE unification of generation theory [31], the causal decoupling of compressed representations [179], and the native three-dimensionalization of attention modeling [135], to the progressive cascading of generation strategies [280]. Together, these form the underlying foundation for spatiotemporal intelligence. (i) Theoretical Unification via Flow Matching. Although early works followed the DDPM paradigm based on SDEs, state-of-the-art (SOTA) models such as Sora [2] and Wan 2.1 [273] have generally shifted toward the Flow Matching (FM) framework to enhance sampling efficiency and temporal coherence. Rather than predicting Gaussian noise ϵ, FM formalizes the generation process as constructing deterministic Ordinary Differential Equation (ODE) trajectory between the noise distribution π0 and the data distribution π1. The core optimization objective transforms into regressing the velocity field vt on the optimal transport path: LFM(θ) = Et,z0,z (cid:104) vθ (t, (1 t)z0 + tz1) (z1 z0)2(cid:105) , (22) where vθ denotes the velocity field predicted by the network parameters θ, and z0, z1 represent samples from the prior noise and data distributions respectively. As demonstrated by Rectified Flow [31], this paradigm forces the latent variable to evolve along linear trajectory, significantly reducing transport curvature. This allows the model to generate dynamic textures with physical conservation in very few steps, resolving the structural collapse issues inherent in DDPMs during long-term sampling. (ii) Causal Spatiotemporal Compression. To circumvent the computational bottlenecks of high-dimensional video data, the primary challenge in architecture design lies in constructing compact latent space that satisfies causality. MagViT-v2 [315] and CogVideoX [179] identified the risk of future information leakage in traditional 3D convolutions. Consequently, modern encoders generally introduce Causal 3D VAEs [263, 151], utilizing asymmetric temporal padding and causal convolution kernels to ensure that the generation of latent code zt depends only on historical frames ximg,t. This design not only"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "mathematically guarantees the unidirectionality of temporal logic but also provides the architectural foundation for streaming inference. Furthermore, by employing heterogeneous downsampling strategies (e.g., 4, 8, 8), models achieve decoupled compression of high-frequency motion information and low-frequency semantic features [223, 316]. (iii) Native 3D Attention Modeling. Regarding dynamics modeling in latent space, the academic community has undergone profound correction of inductive bias. Early works like AnimateDiff [135] utilized spatial-temporal factorized attention, which reduced computational costs but severed spatiotemporal coupling, making it difficult to simulate complex fluid dynamics. HunyuanVideo [278] and OpenSora [317] have since established the dominance of the Native 3D DiT, calculating joint self-attention across the entire spatiotemporal sequence using 3D-RoPE. Although this introduces quadratic complexity of O((THW)2), the integration of sequence parallelism techniques such as Ring Attention [318] enables the model to capture long-range spatiotemporal dependencies, thereby allowing the emergence of coherent motion consistent with physical laws. (iv) Progressive Alignment & Cascading. To address error accumulation in long video generation, models employ coarse-to-fine condition control strategy. Visual Dialect, proposed by Tar [280], achieves native alignment of semantics by mapping text to visually compatible tokens. For generating long-range videos (> 10s), Kling [279] and Vidu [274] utilize spatiotemporal cascade strategies. The model first generates semantic skeleton at low frame rate, which then serves as condition cctx for Temporal Super-Resolution Model. This cascade architecture essentially decomposes the high-dimensional joint distribution p(ximg) [319] into product of multiple conditional probabilities, effectively mitigating VRAM pressure and logic drift in single models during long sequence generation [320]. (2) Autoregressive Model (AR). The Autoregressive Model draws on the Scaling Law of LLMs [321, 322], with its core philosophy being Everything is Token. This paradigm discards the denoising prior of diffusion models and reformulates video generation as causal sequence prediction problem within discrete latent space [222, 323]. Its mathematical essence is the maximization of the loglikelihood of the joint probability distribution, forcing the model to learn the temporal causality of the physical world through the unidirectional chain rule. To support this vision of unified sequence modeling, the technical evolution of this paradigm is unfolding across four key dimensions: the fidelity of discrete encoding, the topology of multi-modal interaction, the temporal robustness of hybrid generation, and the generalization boundaries of multi-task reasoning. (i) The Discretization Bottleneck & Causal 3D Codebook. The upper bound of an AR model depends on the compression quality of the tokenizer. Early VQGANs suffered from severe codebook collapse and highfrequency flickering. VideoPoet [143] and MagViT-v2 [315] achieved breakthroughs by introducing Lookup-Free Quantization (LFQ) and Causal 3D Convolution. The former reduces quantization variance through direct projection, while the latter ensures that the compression process does not violate physical causality via asymmetric padding. VILA-U [283] further proposed Unified Vision Tower, which forces the alignment of visual tokens and text embeddings during the pre-training phase, fundamentally resolving the semantic gap between heterogeneous modalities in discrete space. (ii) Omni-Modal Interaction Topology. During the sequence modeling phase, the core of architectural design lies in handling the interaction granularity of multi-modal tokens. Sequence Concatenation: UniForm [282] adopts an aggressive early fusion strategy, concatenating video, audio, and text tokens into single long sequence. While using shared-weights Transformers to capture cross-modal dependencies maximizes knowledge transfer between modalities, it faces an O(N2) explosion in attention computation. Dual-Stream Gated Modulation: To reduce computational overhead, RFLAV [281] and Ovi [264] employ late fusion. Ovi designs symmetric dual-backbone architecture, aligning the sampling rates of different modalities through RoPE frequency scaling; RFLAV introduces temporal averaging modulation in the AdaLN layer of the Transformer, achieving soft alignment of audio-video features without significant increase in parameter count."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "(iii) Long-Horizon Dynamics & Hybrid Paradigms. Pure discrete AR models often face collapse due to error accumulation when generating long videos. To correct this deficiency, researchers have begun exploring hybrid paths of discrete planning + continuous correction. Non-Quantized AR: NoVA [284] challenges the assumption that data must be discretized, proposing continuous AR prediction in continuous space. It decomposes video into frame-wise temporal steps and set-wise spatial steps, predicting continuous features via diffusion decoder to circumvent information loss from quantization. Rolling Flow Matching: RFLAV [281] innovatively introduces sliding window mechanism. After the AR predicts coarse tokens, flow matching is used for local refinement. Through rolling strategy of removing the first frame and adding the noised last frame, it theoretically achieves physically consistent generation of infinite duration, solving the inherent malady of AR models being logical but lacking details. (iv) Unified Multi-Task Reasoning. The ultimate advantage of the AR architecture lies in its zero-shot generalization. As demonstrated by VideoPoet, by introducing specific task tokens (e.g., optical - flow, depth), single model can perform video generation, style transfer, and even audio-visual QA tasks without fine-tuning. This omnivore characteristic proves the unique potential of the autoregressive paradigm in building universal world simulator. Figure 25: Autoregressive-Diffusion Model. It depicts an AR Planner constructing causal temporal skeleton (blue wireframes), which then passes through Diffusion Refiner (warm mist) for detail injection, resulting in high-quality long-duration video output. (3) Autoregressive-Diffusion Hybrid Model. The core synergy mechanism of the AutoregressiveDiffusion Hybrid Model, as essentially illustrated in Figure 25, is the injection of the temporal causal constraints of AR into the iterative denoising manifold of the Diffusion Model. The universal mathematical representation of this hybrid generation is no longer simple probability superposition, but rather the construction of joint probability density of causal logic and high-quality generation: pθ(ximg,1:T, z1:T c) = t= pAR(zt z<t, ximg,<t, c) (cid:124) (cid:125) (cid:123)(cid:122) Causal Temporal Dynamics pDiff(ximg,t zt, ximg,<t, c) (cid:125) (cid:123)(cid:122) (cid:124) Conditionally Denoised Rendering , (23) where ximg,1:T denotes the generated multi-modal sequence (Video/Audio), zt represents the noisy latent or intermediate features, and pAR and pDiff correspond to the low-dimensional causal modeling and the high-fidelity conditional denoising distribution, respectively. This mechanism aims to combine the long-range planning advantage of AR with the detail generation capability of diffusion, overcoming the inherent defects of single models. Based on this joint modeling approach, the technical evolution of this paradigm is unfolding along two orthogonal paths: temporal fusion optimization and crossparadigm modal synergy, aiming to simultaneously solve the bottleneck of dynamical consistency in long-sequence generation and the challenge of alignment between heterogeneous modalities. (i) Temporal Fusion Optimization. The core lies in balancing strict causal dependency with generation flexibility, breaking the efficiency bottleneck of long video generation through differentiated"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "architecture design. Real-time Streaming Dynamics. To address generation speed and VRAM limits, several works have restructured the inference paradigm. AR-Diffusion [289] proposed traininginference unified diffusion corruption mechanism, establishing temporal baseline by enforcing non-decreasing frame time step constraint (t1 t2 ... tF), which, combined with dynamic scheduler, enables error-free variable-length generation. CausVid [290] converts bidirectional diffusion into an AR architecture through distribution matching distillation; combined with KV cache and sliding window mechanisms, it balances the real-time performance of streaming generation with infinite length extension capabilities. Furthermore, NFD [285] utilizes block-wise causal attention and speculative sampling to achieve real-time generation at 30+ FPS for the first time at scale of 300M+ parameters. RFLAV [281] innovatively introduces rolling flow matching and lightweight temporal modulation module, achieving precise alignment generation of infinite-length audio-video while significantly reducing computational overhead. Long-term Coherence Guidance. To address logic drift in long-term sequences, synergistic guidance strategies have become key. ARLON [288] employs coarse-grained anchoring - fine-grained refinement strategy, using an AR model to generate coarse features containing long-range semantics to guide Diffusion Transformer (DiT) in detail refinement, while utilizing VQ-VAE unified representation space to resist noise interference. ACDC [286] proposes zero-shot synergy framework that, without modifying the architecture, allows the AR model to act as global context planner and the diffusion model as local corrector, utilizing the external memory module of an LLM to effectively alleviate error accumulation in long sequence prediction. (ii) Cross-Paradigm Modal Synergy. This focuses on the precision of modal alignment and the tightness of integrated architecture, aiming for deep coupling of heterogeneous signals. Diffusion-Augmented Representation. DiCoDe [287] challenges traditional discretization methods with diffusion cascaded tokenization scheme. It first encodes video into continuous latent features and then uses diffusion process to compress them into high-fidelity discrete tokens. This approach achieves thousand-fold compression while preserving visual details and strengthens text-video semantic alignment through cross-attention mechanisms, providing high-quality vocabulary for long video generation. Endto-End Architectural Fusion. HybridVLA [265] demonstrates the potential of paradigm fusion in the field of Embodied AI. It seamlessly integrates diffusion generation and AR prediction within single LLM framework, projecting continuous action vectors generated by diffusion into the LLMs word embedding space. By introducing special tokens to separate the two paradigms and adaptively fusing prediction results based on AR confidence, the model achieves an end-to-end logical loop across visual, language, and action modalities, significantly strengthening the coherence of the agents perception-reasoning-execution link."
        },
        {
            "title": "3.3.2 Explicit Structured Control",
            "content": "Although end-to-end models have achieved breakthroughs in image quality, their text-as-all interaction mode exhibits significant ambiguity in industrial applications. Explicit structured control aims to resolve the challenge of controllability. Its core concept involves projecting the high-dimensional dynamics manifold Mdyn onto low-dimensional interpretable control manifold (e.g., depth, optical flow, skeleton). This paradigm reformulates video generation as constrained optimization problem: Eximg,cstruct,cmot (cid:2)log pθ (cid:0)ximg Etxt(ctxt), Estr(cstruct), Emot(cmot)(cid:1)(cid:3) , (24) max θ where Estr and Emot denote the encoders processing explicit conditions for spatial structure and temporal motion, respectively. (1) Motion-Geometry Explicit Encoding. This school of thought primarily inherits and extends the principles of 2D ControlNet, aiming to eliminate generated geometric hallucinations by injecting explicit physical priors. Facing spatiotemporal degrees of freedom that far exceed those of static images, this paradigm is dedicated to constructing set of hard-constrained physical interfaces. It"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "has achieved breakthrough progress in two key dimensionsresidual-based spatiotemporal feature injection and multimodal narrative structure orchestration. (i) Residual-based Feature Injection. The core challenge lies in injecting strong geometric constraints without compromising pre-trained generation priors. Spatiotemporal ControlNet Adaptation. VideoComposer [262] proposed an explicit encoding strategy using Motion Vectors (MVs), utilizing MV signals in the compressed domain as low-rank approximation of temporal conditions. This addresses control difficulties in complex motion scenarios, such as the coupling of camera translation and object deformation. ControlVideo [293] explored training-free path by introducing cross-frame geometric masks in the self-attention layer to force multiple frames to share the same ControlNet features, thereby achieving temporal consistency in structure. Trajectory-aware Latent Navigation. For fine-grained control of object movement paths, DragNUWA [324] and MotionCtrl [325] introduced the joint encoding of trajectory heatmaps and camera poses Pt. Unlike simple optical flow low injection, they explicitly map user-drawn 2D trajectories to manifold evolution directions in 3D latent space by flow : zt zt+1. (ii) Multimodal Storyboarding. To handle long-range narratives, VAST [266] introduced storyboard mechanism, decoupling text descriptions into dual-stream constraints of Layout + Pose. Its innovation lies in constructing bi-directional autoencoder that maps discrete control signals to continuous sequence latent vectors, providing rigid skeleton for cross-frame generation and effectively suppressing object identity drift in long sequences. (2) Start-End Frame Anchoring and Interpolation. This paradigm transforms video generation from extrapolation into mathematically more stable interpolation problem, specifically solving for Brownian Bridge with ximg,start and ximg,end as boundary conditions. Under this mathematical framework, technical evolution focuses on constructing smooth, high-fidelity spatiotemporal transition manifolds and exploring multimodal interactive control within constrained spaces, forming two core pillars: boundary-condition-driven path planning and dynamic instruction injection. (i) Boundary-Conditioned Path Planning. Temporal Generative Inpainting. SEINE [326] and MorphStudio [327] treat two input images as masks {0, 1}THW, performing denoising only on the intermediate frames during the diffusion process. To ensure transition smoothness, they introduced interpolated attention, allowing the query vectors of intermediate frames to simultaneously query the keys/values of the start and end frames, thus achieving smooth blending of physical states in the feature space. Cascaded Super-Resolution Architecture. To address the blurriness caused by interpolation, Show-1 [297] proposed cascaded strategy of coarse-to-fine anchoring. Stage 1 utilizes pixel-level model to generate low-frequency motion skeleton, while Stage 2 employs latent diffusion for highfrequency texture inpainting. This design skillfully leverages the structural sensitivity of pixel space and the texture generation capability of latent space. (ii) Dynamic Instruction Injection. For complex interactive generation, InteractiveVideo [298] refines control signals into quadruple (Image, Content, Action, Trajectory) and injects them at specific time steps via gated cross-attention. KeyVID [299] focuses on audio-driven scenarios, utilizing ImageBind to extract audio peaks as implicit keyframes, achieving automated anchoring of audio-visual sync. (3) Multi-Condition Decoupling Architecture. To resolve feature entanglement between multimodal signals (e.g., changing characters action causing background texture changes), recent architectures favor the orthogonal decoupling design illustrated in Figure 26. Within this framework, technical evolution is proceeding along two key axes: the separation of appearance-motion features and the complementary interaction of spatiotemporal dimensions, aiming to simultaneously solve identity drift under high dynamics and the imbalance between spatial structure and temporal manifolds in long-sequence generation. (i) Appearance-Motion Two-Stream. This is currently the mainstream paradigm for digital human animation [328, 329]. Facing the inherent conflict between maintaining identity (appearance) and driving"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 26: Multi-Condition Decoupling Architecture. The diagram illustrating the Two-Stream Architecture for digital human animation. It depicts the orthogonal decoupling of Reference Appearance (static portrait) and Motion Skeleton (stick figure), which are fused via Spatial Attention Zipper to generate spatiotemporally consistent video loop. complex actions (motion), this paradigm advocates for abandoning single-stream processing in favor of two-stream decoupling mechanism at the architectural level. This involves extracting static texture features and dynamic pose features separately, then fusing them through specific modules orthogonally. This includes: Explicit Spatial Decoupling. Targeting the issue where single-stream networks lose appearance features over time, Animate Anyone [330] and MagicAnimate [331] introduced an independent ReferenceNet as the appearance stream. This branch does not participate in the denoising process but specifically extracts high-fidelity features from the reference image, which are then injected layer-by-layer via spatial attention into the Main UNet (motion stream) responsible for action generation. Formally, this achieves an explicit decomposition of the generated features zgen: zgen = Fmotion(zt, cpose) (cid:125) (cid:123)(cid:122) Motion Stream (cid:124) +λ Fapp(Iref) (cid:125) (cid:124) (cid:123)(cid:122) Appearance Stream , (25) where zgen denotes the synthesized feature map, cpose represents the pose control signal, Iref is the reference source image processed by the appearance encoder Fapp, and λ is the fusion coefficient. This dual-tower design forces the separation of texture encoding and motion inference, ensuring consistency of details during large-scale dynamic movements [332, 333]. Implicit Attention Disentanglement. Moving beyond physical dual-network structures, Moonshot [302] and CCEdit [303] explore logical dualstreams within single network. They argue that traditional cross-attention tends to confuse structural signals (pose/shape) with content signals (texture/identity). Consequently, these works propose decoupled attention mechanism that splits key/value mappings into independent structure branches and appearance branches. Through orthogonal gradient backpropagation, the model is forced to ensure that changes in the motion stream do not interfere with the feature distribution of the appearance stream. This mechanism achieves zero-interference between appearance and motion at the micro-level, resolving the chronic issue of identity drift caused by action changes [334]. (ii) Spatiotemporal Complementary Loop. TATS [304] and Swap Attention [301] explore the decoupling of spatiotemporal dimensions. Swap Attention utilizes role-swapping mechanism within 3D windows to construct loop where space guides time, and time feeds back to space. This design mathematically forces the model to maintain texture consistency along the spatial axis and optical flow low coherence along the temporal axis, effectively solving the infinite loop or motion freezing phenomena common in autoregressive generation."
        },
        {
            "title": "3.3.3 Unified Comprehension and Generation Symbiosis Architecture",
            "content": "Traditional computer vision research treats Understanding (Discriminative) and Generation (Generative) as opposing binary tasks: the former models the conditional probability p(yximg), while the latter models p(ximgy) [223, 335]. However, the Unified Comprehension and Generation Symbiosis Architecture seeks to construct unified probability model p(ximg, y), aiming to dismantle the barriers between perception and simulation [59, 307, 336]. The core assumption of this paradigm, as illustrated in Figure 27, is that: perfect generator should implicitly contain perfect discriminator. Figure 27: Unified Comprehension and Generation Symbiosis Architecture. It features central LLM converting multimodal inputs into unified cloud of discrete tokens, enabling seamless Any-to-Any transformation (e.g., video to text and vice versa). (1) Shared Representation Bidirectional Synergy. This direction aims to map heterogeneous signals into the same manifold space by constructing an Omni-modal Isomorphic Representation, thereby achieving Any-to-Any conversion within single set of model parameters. Specifically, to break the chasm between perception and generation, this paradigm establishes the dominance of discrete tokens as universal interaction primitives and explores the unique value of geometric representations as physical anchors in embodied scenarios. This has resulted in two primary technical tracks based on symbolic unification and geometric symbiosis. (i) Token-based World Modeling. Inspired by the success of LLMs, discretized tokens have become the general currency for unifying understanding and generation. Fully Discretized Autoregression. Gaia-1 [205] and Phenaki [268] proposed video encoding schemes based on C-ViViT, which unify the encoding of driving videos, control signals, and text descriptions into discrete token sequence z1:L. The training objective of the model is unified into standard Next-Token Prediction: Luni = i= log pθ(zi z<i, TaskToken), (26) where z1:L represents the unified discrete token sequence combining visual and textual information,"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "and TaskToken serves as the prompt indicator to switch between understanding and generation modes. This paradigm allows the model to switch functions via simple Task Prompting: inputting video tokens to predict text tokens constitutes understanding, while the reverse constitutes generation. Unified Discrete Diffusion. Unified Discrete Diffusion [307] and Show-O [176] challenge the notion that autoregression is the only solution. They designed Unified Transition Matrix that allows image and text tokens to undergo bidirectional denoising within the same diffusion process. Show-O further utilizes Hybrid Attention Mechanism that applies Causal Mask to the text portion and Full Mask to the visual portion, achieving the seamless coexistence of understanding and generation within single Transformer weights. (ii) Domain-Specific Geometric Symbiosis. In the field of Embodied AI, HERMES [310] proposed using BEV (Birds-Eye-View) features as shared hub. It utilizes World Queries mechanism to compress 2D images from multi-view cameras into 3D BEV features. This not only supports downstream path planning (understanding) but also enables the generation of future prediction videos (generation) via decoder for BEV features, proving that 3D geometric constraints serve as strong bridge connecting perception and simulation. (2) Pre-training Driven Synergistic Adaptation. Unlike training unified multimodal model from scratch [337, 59], this paradigm advocates Shoulders of Giants strategy: using frozen MLLM (such as GPT-4V or LLaVA) as the cognitive hub (Brain), connected via lightweight adapters to visual generation decoder (Eyes/Hands). The goal is to transfer the general reasoning capability of LLMs to video generation tasks at low cost [338, 339]. Under this architecture, the core technical challenge shifts to constructing high-bandwidth interface connecting the cognitive space and the generative space, aiming to precisely map high-level reasoning to low-level generative conditions through an LLM-centric projection mechanism. (i) LLM-Centric Projection. The core challenge lies in achieving zero-loss interface between the semantic space of the LLM and the pixel space of video generation. Input-Output Bidirectional Adaptation. Omni-Video [269] and NExT-GPT [340] established general bridging framework for Any-to-Any conversion. On the input side, linear projections or Q-Formers are used to align visual signals with the LLM embedding space; on the output side, the model triggers Vision Head by predicting special [IMG] token, projecting the hidden states of the LLM into the conditional input cdi for diffusion model. This achieves an explicit translation from textual thinking to visual signals. Mixture of Encoders. MERV [309] notes that single visual encoder struggles to balance semantic understanding with texture details. It introduces learnable cross-attention mechanism that connects multiple frozen encoders in parallel, such as CLIP (strong semantics), DINOv2 (strong structure), and VideoMAE (strong action). Through dynamic weighting via the LLMs attention mechanism, the model can automatically select the optimal visual feature source when processing complex instructions, achieving gathering of strengths for visual perception."
        },
        {
            "title": "3.3.4 Reinforcement Learning for Modal-Temporal Alignment",
            "content": "The introduction of Reinforcement Learning (RL) techniques aims to address the non-convexity issues encountered by traditional Supervised Fine-Tuning (SFT) when handling modal semantics and temporal dynamics [341, 342]. SFT tends to average distributions, often leading generation results into binary dilemma of being either high semantic fidelity but static or high dynamic but collapsed. The RL paradigm, by constructing Multi-dimensional Reward Manifold [37, 343], transforms discrete modal alignment objectives and continuous temporal evolution objectives into joint optimization problem, guiding the model to converge toward the Pareto Frontier of the semantics-temporal trade-off. Driven by this objective, and to precisely characterize and optimize this complex manifold, technical evolution is unfolding across three dimensions: preference-based joint alignment, self-refinement-based iterative evolution, and the logical restructuring of universal"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "reward models. These efforts aim to comprehensively enhance the models ability to synergistically control heterogeneous modalities and dynamic sequences. (1) Preference-based Joint Alignment. This path utilizes DPO [344] and its variants to encode the implicit dependency between semantic understanding and temporal evolution into preference rankings, forcing the model to learn temporal dynamics consistent with physical laws while maintaining textual/image semantic precision. (i) Dynamic Preference & Static Penalty. VideoDPO [270] was the first to point out that directly applying image-level DPO leads to motion collapse (i.e., the model sacrifices temporal dynamics to cater to semantic scores). It constructs preference dataset encompassing the trade-off between semantic alignment vs. motion magnitude. Through KL divergence constraints, it mathematically pushes the probability density toward high-dynamic and high-fidelity regions, achieving joint calibration of modal instructions and temporal motion. (ii) Mixed Reward Distillation. T2V-Turbo [312] proposes multi-path signal fusion strategy. Rather than relying solely on single preference model, it integrates reward signals from HPSv2 (measuring modal aesthetics) and InternVideo2 (measuring temporal consistency). Through reward-weighted regression, it distills evaluation metrics for modal aesthetics and temporal fluency into consistency-model-based student network, rapidly approaching the joint optimal distribution of semantics and dynamics. (2) Iterative Alignment via Self-Refinement. This direction draws on the self-play concept from LLMs, constructing feedback loop that allows the model to find the optimal balance point between modal instructions and temporal evolution within generation-evaluation-correction cycle. (i) Semantic-Dynamic Hierarchical Reward. Hierarchical optimization frameworks [271] design hierarchical reward mechanism to specifically address the disconnection between first-frame semantics and subsequent-frame actions. It applies an image quality reward (modal level) to the first frame and coherence penalty based on motion vectors (temporal level) to subsequent frame sequences. This introduces temporal gradient backpropagation into PPO [345] updates, enabling the model to foresee the dynamical consequences on the time axis while generating first-frame semantics. (ii) Instruction-Following Self-Evolution. Video-STaR [313] proposes self-evolution framework based on rejection sampling. It utilizes an MLLM (such as GPT-4V) as discriminator to select high-quality samples that are both instruction-following accurate (modal) and action-natural and smooth (temporal) to fine-tune the generator. This mechanism filters out noise data that is either text-image matched but temporally collapsed or temporally smooth but semantically lost, significantly enhancing the models capability to understand complex spatiotemporal instructions. (3) Universal Reward Modeling. The upper bound of RL depends on whether the reward model (RM) can accurately decouple and measure the contributions of modality and time. Research focus in 20242025 has shifted toward constructing universal RMs capable of simultaneously understanding semantic logic and physical causality. (i) Decomposition-Fusion Evaluation System. VPO [271] proposes explicitly decomposing the reward function into semantic alignment (Video-LLM) and temporal smoothness (optical flow low). By performing weighted optimization of these two orthogonal objectives along the diffusion denoising trajectory, the model learns to eliminate inter-frame flickering using low constraints without compromising text semantics, achieving deep fusion of modal content and temporal continuity. (ii) From Noun Alignment to Causal Logic. VideoScore [314] challenges the traditional CLIP-Score [346] by constructing universal automatic evaluation metric based on Video-LMM. It captures not only static pixel-level quality but also deep temporal causal logic (e.g., if an instruction requires cup shattering, the shattering action must occur after the fall, not before). Using VideoScore as direct optimization target for RL allows the model to move beyond simple noun-based modal alignment and truly master the causal consistency of temporal logic and modal semantics."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "(4) Embodied Action Alignment via VLA-RL. When alignment extends to the Vision-LanguageAction (VLA) domain, VLA models face the more rigorous challenge of functional temporal alignment. In this context, temporal evolution is no longer merely the coherence of visual frames but physical intervention sequence a1:T driven by linguistic instructions [347]. Traditional VLA training primarily relies on Supervised Fine-Tuning (SFT) based on human demonstrations. However, from statistical perspective, SFT is essentially re-weighting of the known data distribution [348, 349]. Its objective function minθ log Pθ(as, Ddemo) compels the model to fit the average behavior of demonstration data, causing the effective search space to be confined near the local optima of human experts. Once an Out-of-Distribution (OOD) shift occurs in the environmental state, the model often succumbs to cascading errors induced by covariate shift due to lack of exploration capability. To overcome this theoretical bottleneck, works such as TwinRL-VLA [350] and RL-VLA3 [348] have pioneered paradigm shift from passive imitation to active exploration. Their core mechanism involves transforming the optimization objective from minimizing imitation loss to maximizing long-term cumulative reward J(θ) = Eτπθ [t γtr(st, at)]. (i) Digital Twin Verification Mechanism. Unlike implicit reward models, TwinRL introduces Digital Twin as an explicit physical verifier. The system utilizes 3D Gaussian Splatting (3DGS) to reconstruct high-fidelity scenes [351] and executes the policy-generated action sequences a1:T in parallel within physics engine. This mechanism provides deterministic physical feedback as sparse reward signal, compelling the model to not only align with the semantic intent of linguistic instructions temporally but also satisfy the feasibility constraints of physical interaction [352, 353]. (ii) Exploration Boundary Expansion. By conducting large-scale trial and error in zero-cost simulation environment, the RL agent can reach long-tail state spaces not covered in human demonstration data, such as extreme physical contacts or rare object poses [354]. Theoretically, this mechanism expands the effective support set of the policy, enabling the VLA model to evolve from interpolation capabilities on finite samples to extrapolation capabilities in unknown environments. 3."
        },
        {
            "title": "Integration of Spatial and Temporal Consistency",
            "content": "The fusion of spatial and temporal consistency marks the ultimate leap of generative models from Frame-wise Painting toward World Construction [2, 163]. As illustrated in Figure 28, the core utility of this dimension lies in establishing Dynamic Object Permanence: specifically, during spatiotemporal evolution, an object must not only maintain the rigidity of its geometric form but also follow motion trajectory consistent with physical laws. Its intrinsic properties must not drift even during occlusions or drastic viewpoint changes [142, 355]. This fusion elevates time passage from mere pixel changes to the topological evolution of 3D manifold, constituting the physical cornerstone of the 4D generation technology stack [275, 121]. Under this vision, technical evolution presents four-stage evolutionary lineage from representation construction to value alignment, as shown in Figure 29: Implicit Spatiotemporal Learning adopts destructuring strategy, mapping 2D video priors to the probability distributions of 4D fields via score distillation, trading statistical flexibility for generative generalization [356, 357]; Explicit Geometric Anchoring introduces point clouds and camera trajectories as rigid skeleton, parameterizing the time axis as SE(3) transformations to achieve precise control with geometry as constraint [358, 359, 360]; Unified Spatiotemporal Representation utilizes 4D Gaussian primitives or hybrid tensor fields to establish continuous mathematical fields with native support for deformation and lighting, whichcoupled with the global association of dense trajectory fieldsrealizes an isomorphic representation of spatiotemporal dimensions [361, 362, 363]; and finally, Reinforcement Learning Alignment aims to overcome the exposure bias of SFT by constructing composite reward function that integrates explicit physical costs. This forces the model to solve the Pareto optimization of spatial fidelity and temporal coherence, achieving paradigm shift from probability fitting to physical value alignment [364, 365]. Together,"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "these four stages define the evolutionary path toward physical realism for current 4D world models. Figure 28: Temporal + Spatial Consistency: Dynamic Object Permanence across Occlusion. The model combines temporal and spatial consistency to realize dynamic object permanence during occlusion: the subject (Doge) retains consistent features (e.g., sunglasses, bone) via latent memory when occluded, and re-emerges with unchanged attributes, ensuring the continuity of the object."
        },
        {
            "title": "3.4.1 Implicit Spatiotemporal Learning",
            "content": "Beyond explicit geometric representations (such as 3DGS), Implicit Spatiotemporal Learning represents an alternative minimalist paradigm of destructuring. In particular, the direction of Video Prior Distillation fundamentally refutes the necessity of full-parameter fine-tuning based on large-scale 3D data, instead pioneering training-free path based on posterior modulation. The theoretical foundation of this paradigm is built upon Score Distillation Sampling (SDS) [97] and Score Jacobian Chaining (SJC) [401], reframing 4D scene generation as an intersection problem between two orthogonal probability manifolds: the geometric manifold Mgeo and the dynamics manifold Mdyn. Video Prior Distillation. The core logic lies in utilizing the Tweedie formula [402, 403] to model the generated denoising step ϵθ as linear combination of two heterogeneous gradient fields. This forces the latent variable zt to converge toward the overlapping high-density region of two prior distributions during the inverse diffusion process [404]: zt log p(zt c) ωs(t) zt log pMVD(zt cview) (cid:125) (cid:124) (cid:123)(cid:122) Geometric Constraint +ωt(t) zt log pVDM(zt cmotion) (cid:125) (cid:123)(cid:122) Dynamic Guidance (cid:124) , (27) where zt denotes the latent variable at timestep t, ωs(t) and ωt(t) are time-dependent weighting coefficients, and pMVD and pVDM represent the probability densities of the Multi-View and Video Diffusion priors, respectively. This process is essentially Maximum Posteriori estimation in high-dimensional space that satisfies P(x) PMVD(x) PVDM(x) [405]. However, facing gradient conflicts and distribution mismatches triggered by the direct superposition of heterogeneous priors, the academic community has evolved systematic solutions across four dimensions: scanning generation & trajectory mapping, variance reduction & SDE solvers, frequency decoupling & progressive modulation, and deep manifold alignment. (i) Scanning Generation & Trajectory Mapping. To materialize the aforementioned probability framework, VIVID-1-to-3 [356] pioneered the isomorphism of the Novel View Synthesis (NVS) task into camera"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Implicit Spatiotemporal Constraints Video Prior Distillation NVS-Solver [357], ViVid-1-to-3 [356], Vivid-ZOO [366], Diffusion² [367], etc. Point Cloud Conditioning GEN3C [368], RealCam-I2V [369], ViewCrafter [370], EPiC [371], etc. Explicit Geometric Anchoring Geometric Embedding Injection cameractrl [372], World-consistent Video Diffusion [359], OmniView [373], PostCam [374], viewdiff [375], VD3D [376], motionctrl [325],etc. Trajectory Parametric Control TC4D [377], Diffusion as Shader [360], 3DTrajMaster [378], SV3D [379], etc. Hybrid Volumetric Representation Unified Spatiotemporal Representation Explicit Structured Representation 4Diffusion [380], Point-DynRF [381], Consistent4D [382], 4Dynamic [361], DyBluRF [383], DynIBaR [384], 4D-fy [385], TiNeuVox [386], MoBluRF [387], SV4D [388], K-Planes [389], etc. 4DGS [121], STAG4D [390], DreamGaussian4D [391], Align Your Gaussians [392], H3D-DGS [393], CAT4D [362], Diffusion4D [394], etc. e s l a o t v n i C r e a Trajectory-Centric Foundation Models Trace Anything [395], OmniMotion [396], VGGSfM [363], SpatialTracker [397], MASt3R [398], etc. Reinforcement Learning T2V-Turbo-v2 [364], VistaDPO [399], DR-Tune [400], InstructVideo [365], etc. Figure 29: Evolution of Spatial Consistency and Temporal Consistency. moving along trajectory video generation problem. This method utilizes video diffusion model (such as ZeroScope or SVD) as the dynamics engine. By explicitly mapping changes in camera extrinsics Pt SE(3) to video timestamps t, it forces the VDM to interpret geometric parallax as optical flow motion. To suppress geometric distortion in single-frame generation, VIVID-1-to-3 introduced an Epipolar Attention Bias, utilizing multi-view diffusion model (such as Zero-1-to-3 [208]) to anchor the geometric structure at keyframes. This dual-diffusion synergy strategy effectively leverages the powerful inter-frame smoothing prior of the VDM to suppress flickering artifacts common in independent view synthesis [406, 136]. (ii) Variance Reduction & SDE Solver. Although score composition provides unified framework, direct superposition of heterogeneous priors in high-dimensional latent space often leads to severe gradient conflicts. NVS-Solver [357] approaches this from the numerical solution of Stochastic Differential Equations (SDEs), noting that simple score addition violates the It ˆo integral conditions of the diffusion process, causing deviation in the drift term of the sampling trajectory. To address this, NVS-Solver introduced high-order approximations based on Taylor expansion and variance-reducing sampling strategy. By explicitly correcting the variance inflation caused by heterogeneous gradients within the SDE solver, the method mathematically ensures that the generation trajectory can smoothly traverse the boundary between the two manifolds. Empirical results show reduction in stochastic jittering during sampling by approximately 40%, significantly enhancing the sharpness and spatiotemporal consistency of the generation results [407, 408, 108]. (iii) Frequency Decoupling & Progressive Modulation. Dynamics analysis of the generation process reveals that diffusion models follow spectral bias of first global structure (low-frequency), then texture details (high-frequency) [409, 410]. Based on this observation, VividZoo [366] proposed time-variant modulation. This mechanism replaces fixed weight allocation with dynamic annealing schedule: in the early denoising stages (high noise t), MVD is assigned higher gradient weight ωs > ωt to leverage its strong geometric prior for establishing the main topology of the object and preventing distortion. In the later denoising stages (low noise t), the weights are reversed (ωt > ωs) to utilize the temporal smoothing characteristics of the VDM to eliminate high-frequency flickering. This design, which aligns with the laws of generative spectral evolution, effectively resolves structural distortion"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "and texture blurring issues caused by prior competition [123, 411]. (iv) Deep Manifold Alignment. Most aforementioned methods remain at the level of score mixing on the output side (pixel/noise space), ignoring the semantic gap in the models internal representations. Diffusion2 [412] proposed deep fusion architecture to resolve the distribution mismatch between VDM and MVD latent spaces. Rather than simply mixing noise, this method inserts learnable 3D2D cross-attention adapters between the U-Nets of the two diffusion models. By minimizing the Sliced Wasserstein Distance at the feature level, the model forces zMVD and zVDM to share the same representation manifold in intermediate layers. This design enables the model to perceive the feature distribution of the other, fundamentally eliminating the ghosting phenomenon caused by domain gaps and achieving true feature-level synergy [413, 330, 414]."
        },
        {
            "title": "3.4.2 Explicit Geometric Anchoring",
            "content": "If implicit learning is soft fitting of spatiotemporal statistical laws, then explicit geometric anchoring represents radical attempt to restructure video generation from probability prediction to 3D rendering. This paradigm rejects treating space and time as entangled latent variables within deep networks; instead, by introducing explicit 3D point clouds [358, 369] and camera trajectories [372, 374], it parameterizes time as continuous SE(3) pose sequence and fixes space as static geometric structure. Its core philosophy is that spatiotemporal consistency should not be achieved by network memory of historical frames, but rather naturally derived from the rigidity of the underlying geometric proxy [325]. (1) Point Cloud Conditioning This direction models video generation as problem of neural rendering with geometric proxies. Its mathematical essence lies in constructing static world model and projecting it into visual feature flow via camera parameters Pt. This process is not mere image processing but rigid transformation strictly following the pinhole camera model: ct = Π(W, Pt), (28) where ct denotes the projected visual features at time t, and Π represents the perspective projection function mapping the static world under camera pose Pt. To implement this physical rigidity within probabilistic diffusion models, current research has explored two dimensions: representation construction and inference control. Infrastructure & Representation. To overcome the memory bottlenecks of pure generative models, Gen-3C [358] and RealCamI2V [369] established metric scale spaces based on Structure from Motion (SfM). Gen-3C utilizes back-projection from monocular depth estimation to construct 3D cache C, transforming the evolution of the time dimension into camera roaming within static point cloud. RealCamI2V further introduces scale alignment loss to enforce consistency between generated local geometry and global SfM point clouds in Euclidean space, thereby resolving the scale drift common in long sequence generation (minutes-level) [415, 416]. Endogenous Consistency & Inference-Driven (System 2 Generation). Unlike the one-pass inference of end-toend modes, this school emphasizes explicit computation during the inference stage. ViewCrafter [370] employs dense stereo matching to reconstruct high-precision point clouds, using the rendering result ˆxrender as hard visual anchor for the video LDM. This design shifts the source of consistency from the black-box statistics of network weights to the white-box geometry of the input side. EPIC [371] proposes dynamic masking strategy: by calculating the occlusion map mocc of the point cloud projection, it applies lightweight ControlNet constraints only to the visible regions while allowing generative freedom in unseen regions. This explicit XYZ UV geometric projection effectively avoids texture misalignment caused by depth errors [417]. (2) Geometric Embedding Injection While point cloud conditioning is explicit rendering, geometric embedding injection is its implicit mapping within the Transformer latent space. As shown in Figure 30,"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 30: Geometric Embedding Injection. It features Doge on perspective grid with trajectory, converting physical rays into geometric tokens to enforce epipolar constraints across frames. this approach aims to encode 3D spatial coordinate information into geometric tokens [373] isomorphic to visual tokens, which are injected directly into the self-attention mechanism to establish cross-frame shared world coordinate system [325]. To achieve this deep 3D-2D alignment, the community has focused on architectural designs across coordinate representation and dynamic association: Tokenization of World Coordinates. VD3D [376] and ViewDiff [375] introduced Pl ucker coordinate encoding, mapping each camera ray = (o, d) to high-dimensional embedding vector egeo. By injecting these into Attention(Q, Kimg + Kgeo, Vimg + Vgeo), the model no longer simply predicts the statistical distribution of pixels but learns the correspondence between pixels and 3D spatial positions (p). This mechanism essentially injects an epipolar inductive bias into the attention matrix, allowing the query at frame to accurately attend to the key at frame corresponding to the same physical coordinates, thereby realizing the notion that time naturally emerges from space [81, 418]. Spatiotemporal Interface & Self-Association Mechanism. To convert static anchoring into dynamic coherence, PostCam [374] and CameraCtrl [372] designed trajectory parameterization modules that inject camera pose sequences (P1:T) into the temporal Transformer blocks. OmniView [373] and MotionCtrl [325] further proposed geometric similarity gating, utilizing implicit 3D correspondence maps to construct self-associations between cross-frame points. Under this mechanism, object motion is no longer hallucinated texture flow by the network but physical motion guided by the spatial reasoning of geometric tokens, marking leap from data fitting to physical constraints [419, 413]. (3) Trajectory Parametric Control For dynamic scenes, the trajectory parametric control direction explicitly models 3D motion as differentiable function T(t), achieving physical-level decoupling of object motion laws [377]. Research efforts focus on motion representation mechanisms and optimization constraints: Motion Elevation & Identity. This paradigm elevates discrete pixel displacement to continuous EulerianLagrangian flow. TC4D [377] employs global-local decomposition strategy, decomposing scene"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "motion into superposition of rigid camera motion Pt and local object deformation field Ψ(p, t). DiffusionShader [360] assigns 3D identity ID in the world coordinate system to each pixel, simplifying complex dynamic prediction into an ID matching problem along the time axis, which fundamentally eliminates texture flickering. Explicit Physical Constraints. 3DTrajMaster [378] treats trajectories as entities subject to physical laws, explicitly adding acceleration regularization to the loss function: Lsmooth = pt+1 2pt + pt12, (29) where pt denotes the position vector at time step t, and the expression minimizes the second-order difference (approximation of acceleration), effectively suppressing high-frequency jitter to ensure smooth motion. Coupled with timestep annealing strategy, the model fits the low-frequency trajectory skeleton during the early stages of denoising and fills in high-frequency deformation in the late stages, effectively preventing error accumulation. SV3D [127] further adopts pipeline of first generate consistent observation, then optimize unified representation, allowing generative models to maintain physical plausibility while unleashing creativity by dynamically adjusting trajectory control strength during inference [127, 420]."
        },
        {
            "title": "3.4.3 Unified Spatiotemporal Representation",
            "content": "The Unified Spatiotemporal Representation paradigm elevates video generation from pixel interpolation to spatiotemporal manifold reconstruction by constructing 4D physical representation space [421, 121, 422]. (1) Hybrid Volumetric Representation: Low-Rank Tensor Decomposition & Hybrid Fields. Hybrid Volumetric Representation aims to resolve the inherent contradiction between high-dimensional spatiotemporal modeling and the capture of high-frequency dynamics, addressing the curse of dimensionality. This paradigm abandons expensive dense 4D voxel grids in favor of compact factorization strategies, which decouple complex 4D fields into tensor products of low-dimensional subspaces while embedding explicit physical motion constraints. This approach enables models to maintain the continuity of neural representations while achieving the efficient query capabilities of grid-based methods. To this end, current research has advanced architectural innovation across three progressive levels: hybrid volumetric representation [423], spatiotemporal factorization [421], and dynamics coupling & trajectory integration [384]. (i) Hybrid Volumetric Representation. Through the synergistic design of explicit structure encoding and implicit neural decoding, researchers seek the Pareto optimality between grid query efficiency and neural network compactness [423, 424]. To address the limitations of pure implicit NeRFs in capturing high-frequency dynamics [81], this direction introduces low-rank tensor decomposition theory, decomposing the 4D spatiotemporal field into tensor products of multiple low-dimensional subspaces as illustrated in Eq. 10 [421]. (ii) Spatiotemporal Factorization. K-Planes [425] and HexPlane [421] proposed decomposition strategies based on six planes, transforming feature queries in 4D space into feature interpolation and Hadamard products across six 2D planes. This design not only reduces the representations space complexity from O(N4) to O(N2) but, more importantly, introduces critical inductive bias: spatial planes (e.g., xy-plane) enforce 3D consistency of visual appearance, while spatiotemporal planes (e.g., xtplane) explicitly constrain the continuous evolution trajectory of pixels over time. Building on this, Tensor4D [426] introduced hierarchical tensor decomposition, utilizing multi-scale feature grids to capture the full spectrum of informationfrom coarse actions to fine texturesthereby resolving artifact issues in fast-motion scenes [386]."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "(iii) Dynamics Coupling & Trajectory Integration. Since static decomposition struggles with complex topological changes, dynamic constraints must be introduced. DynIBaR [384] innovatively integrated the time dimension into the volumetric rendering equation through trajectory-based rendering. Instead of sampling at fixed points along ray, this method warps sampling points to their corresponding positions in neighboring frames based on velocity field vt: C(r) = Tiαic (cid:18) pi + (cid:90) t vτ(pi)dτ, (cid:19) , (30) where Ti is the accumulated transmittance, αi represents the opacity at the i-th sample point, and vτ denotes the instantaneous velocity field. This design internalizes temporal consistency as an integral term in the rendering equation, achieving physical-level aggregation of cross-frame information. SV4D [388] employs 3D skeleton to lock the spatial structure and constructs multi-frame multi-view attention within the latent space. By guiding dense 4D generation via sparse 3D keypoints, it effectively mitigates geometric collapse during long sequence generation [385, 427]. (2) Explicit Structured Representation Explicit Structured Representation is primarily based on 3D Gaussian Splatting, marking paradigm shift from an Eulerian perspective to Lagrangian perspective [428, 429]. Its core logic involves modeling the scene as set of discrete primitives with specific attributes (position p/µ, covariance Σ, spherical harmonics coefficients SH, and opacity α), enabling real-time rendering through differentiable rasterization [391]. Current research establishes technical framework across three dimensions: canonical-deformation decomposition, multi-source priors & physical guidance, and topological constraints & geometric driving. (i) Canonical-Deformation Decomposition. To handle non-rigid motion, mainstream methods adopt the Canonical Space and Deformation Field modeling approach. 4D Gaussian Splatting [121] and Deformable 3DGS [95] define static canonical space to store geometric topology and utilize an MLP-based deformation field (p, t) conditioned on time to predict the displacement and rotation of each Gaussian sphere at specific moments: µt = µ0 + µ(µ0, t), Σt = (Σ 0, r(µ0, t)) , (31) where µ0 and Σ 0 represent the mean and covariance of the k-th Gaussian Gk in the canonical space, µ is the predicted position offset, and denotes the rotation update. H3D-DGS [393] further splits the deformation field into an observable rigid part and an unobservable completion part, introducing hard-coded priors to restrict degrees of freedom and prevent overfitting to high-frequency noise. DreamGaussian4D [391] combines HexPlane decomposition to parameterize Gaussian deformation, significantly reducing the VRAM usage for 4D optimization [122]. (ii) Multi-source Priors & Physical Guidance. To hallucinate plausible 4D structures from 2D video, this paradigm relies on powerful generative priors. STAG4D [390] proposes injecting first-frame time-anchor during the Score Distillation Sampling (SDS) optimization process, forcing the generation of subsequent frames to strictly follow the geometric standards established in the first frame. Ling et al. (2024) [392] employs compositional score distillation, utilizing text-to-image, text-to-video, and 3D-aware diffusion models simultaneously to provide multi-source gradient supervision, thereby achieving cross-modal physical constraints. Diffusion4D [394] introduces revolutionary explicit 4D diffusion model that performs denoising directly within the voxelized Gaussian parameter space, allowing results to be back-projected into an explicit 4D field, fundamentally ensuring the endogenous consistency of spatiotemporal logic [430]. (iii) Topological Constraints & Geometric Driving. For complex action control, simple MLP-based deformations often struggle to maintain the topological structure of articulated objects like the human body. CT4D [431] introduces Gaussian clustering mechanism to automatically discover rigid parts within scene and assign pseudo-skeleton weights, enabling skeleton-like motion driven by video"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "diffusion signals. Cat4D [362] proposes manifold distillation, mapping the feature manifolds of pretrained video generation models (e.g., SVD) into 4D Gaussian space. This ensures the generation process is not merely blind parameter fitting but is implicitly constrained by physical laws such as volume conservation and motion continuity, marking leap from statistical correlation to physical interpretability. Figure 31: Trajectory-Centric Foundation Models. This illustrates the transformation of 2D video input into dense trajectory fields (with occlusion handling) and their subsequent lifting into 4D canonical space for volumetric reconstruction. (3) Trajectory-Centric Foundation Models Distinguished from the trajectory parametric control discussed in 3.4.2, where trajectories serve as parameterized constraints, the paradigm explored in this section (Figure 31) reframes them as the core data representation connecting 2D visual flow to 4D physical space. With the maturation of hybrid voxel and explicit Gaussian architectures, academic focus is shifting from per-scene optimization toward generalizable 4D inference. To overcome the bottleneck of native 4D data scarcity, recent works [422, 396, 397, 398, 432] propose using dense trajectory fields as universal intermediate representation, aiming to build unified world models with physical consistency through automated video-as-trajectory transformation. Research follows two main directions: trajectory lifting strategies and end-to-end generalization. (i) Trajectory Field: Universal Bridge Between 2D Pixels and 4D Physics. Traditional 4D modeling often relies on expensive multi-view capture or sparse offline COLMAP calculations, making it difficult to leverage massive in-the-wild internet videos. New-generation methods advocate for treating video not as set of individual frame images, but as collection of continuous 3D trajectory flows τ evolving over time. Lifting 2D Tracks to 3D. Works such as Trace Anything [422] and SpatialTracker [397] redefine the input signals for 4D reconstruction. By utilizing long-range, occlusion-robust 2D point trajectories extracted by foundation models like CoTracker3 [432] or TAPIR, and applying monocular depth estimation with decoupled rigid/non-rigid optimization, they explicitly lift the 2D pixel flow u(t) into 3D spatial trajectories T(t) / X(t). The revolutionary nature of this approach is that it allows arbitrary monocular video to be converted into 4D pseudo-ground truth with physical attributes, providing infinite data fuel for training general world models. Volumetric Motion Representation. OmniMotion [396] further proposes quasi-3D global motion representation. Unlike traditional optical flow low that only captures relationships between adjacent frames, OmniMotion constructs continuous bijective mapping that projects all pixels in video into canonical 3D space. This means the model can track visible points and predict physically plausible full lifecycle trajectories for occluded objects, breaking the limitations of visibility breaks in traditional 4D modeling. (ii) Foundation Models for Generalizable 4D. Building on unified data formats, 4D foundation models with zero-shot generalization capabilities are emerging, eliminating the need for test-time optimization"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "for each video. End-to-End Dynamic Geometric Matching. MASt3R [398] unifies video generation and 3D reconstruction within single Transformer architecture. By learning dense correspondences and 3D geometric transformations between image pairs, it can directly output 3D point clouds and camera motion for dynamic scenes without requiring camera parameters. This marks shift from optimizationbased pipelines to learning-based end-to-end models. Globally Consistent Structure Recovery. To address cumulative errors in long videos, VGGSfM [363] proposes fully differentiable global SfM framework. By using extracted dense trajectories as constraints, it solves for camera poses Pt and scene geometry in an end-to-end manner within deep learning framework. This ensures that the world model maintains 3D structural consistency even when processing hour-long videos, overcoming the failure modes of traditional methods under dynamic object interference."
        },
        {
            "title": "3.4.4 Reinforcement Learning for Spatial-Temporal Alignment",
            "content": "Traditional Supervised Fine-Tuning is limited by the teacher forcing mode [433], which often struggles to correct exposure bias in long sequence generation [54]. This frequently leads to spatial structure collapse or temporal causality break in the late stages of inference [434]. To address this challenge, recent frontier works have introduced Reinforcement Learning as an adhesive for spatiotemporal fusion [217]. Unlike LLMs that focus solely on text response quality, the core of RL in video generation lies in constructing composite reward function R. This forces the model to solve the Pareto optimization problem between Image Fidelity (Spatial) and Motion Coherence (Temporal) within the latent space [365]. Current research primarily explores two dimensions: Global Mixed-Reward Feedback, which emphasizes macro-statistical balance, and Explicit Physical Costs, which focuses on micro-physical repair. Global Mixed-Reward Feedback & Dynamic Alignment. Addressing the defects where SFT struggles to balance quality and motion, this paradigm establishes spatiotemporal value balance at the macro level by introducing decoupled reward models. (i) Multi-dimensional Value Distillation. T2VTurbo-v2 [364] designs spatially and temporally decoupled mixed reward mechanism. It utilizes HPSv2 to score single-frame aesthetics and InternVideo2 to score dynamic coherence, unifying these two mutually constrained objectives through consistency distillation. This effectively mitigates the motion averaging phenomenon caused by traditional MSE loss. (ii) Deep Reward Tuning. DR-Tune [400] further notes that simple reward weighting can lead to excessive gradient variance. It proposes Deep Reward Tuning strategy that dynamically adjusts the weights of spatiotemporal gradients along the denoising path of the diffusion model. This ensures that the model enhances temporal consistency without sacrificing the spatial fidelity of individual frames. Hierarchical Structural Decoupling & Explicit Costs. Distinct from the black box optimization of global feedback, this school of thought advocates decomposing spatiotemporal consistency into differentiable explicit physical costs for micro-repair at the pixel and structural levels. (i) 3D Hierarchical Alignment. VistaDPO [399] proposes fine-grained hierarchical alignment framework. It decomposes the optimization objective into three orthogonal dimensions: instance-level (semantic fidelity), temporal-level (motion manifold Mdyn), and perceptual-level (spatial structure). This design allows the model to surgically repair time-skipping on the temporal axis without compromising the integrity of spatial textures. (ii) Flow Consistency Constraint. InstructVideo [365] provides specific mathematical implementation means. It moves away from general preference models in favor of reward-weighted fine-tuning, explicitly introducing flickering penalty and optical flow consistency low as cost functions. This method translates physical laws into direct gradient signals, forcing pixel flow to remain smooth during temporal evolution and resolving the high-frequency jittering problem common in long video generation."
        },
        {
            "title": "3.5 Preliminary Emergence of World Models",
            "content": "With the maturation of unified multimodal model architectures and breakthroughs in multimodal pre-training technology, the World Model has moved beyond the stage of independent modeling for single dimensions. It has gradually begun to manifest the prototype of the Modal-Spatial-Temporal Trinity of Consistency Synergistic Emergence, as illustrated in Figure 32. The core characteristic of this stage is that the model is no longer mere pixel generator but has evolved into deductive internal simulator. Specifically, modal consistency provides multi-source interaction interfaces, spatial consistency constructs the static geometric skeleton, and temporal consistency injects the causal evolution engine. This synergistic mechanism has been quantitatively verified through benchmarks and has demonstrated deep understanding of the physical world. Figure 32: The Trinity of Consistency. It depicts Robot Doge performing embodied tasks (stacking blocks) guided by counterfactual reasoning (visualized as prediction bubble)."
        },
        {
            "title": "3.5.1 From Benchmark Establishment to Diverse Evolution",
            "content": "During this evolutionary process, Sora [272] and Open-Sora [317] represent dual milestones for the closed-source commercial-grade and open-source academic communities, respectively. Together, they have established the mainstream paradigm of Spacetime Patchification and DiT Architecture, while other models serve as lateral verifications of rich technical landscape. Sora: Paradigm Establishment of World Simulator. Sora [272] is undoubtedly masterpiece of trinity synergy. It does not rely on explicit 3D inductive biases but instead leverages large-scale spacetime patch training to convincingly validate the capability emergence triggered by the Scaling Law in video generation [321, 435, 316]. By compressing video into spacetime patches within the latent space, the model handles high-dimensional visual data in manner analogous to language tokens, achieving deep interoperability between spacetime and modality [436]. This mechanism not only breaks the modal consistency bottleneck in long video generation but also, driven by massive data, facilitates the spontaneous emergence of an implicit understanding of the physical world. Even without explicit geometric constraints, Sora maintains perspective constancy of the spatial structure during complex camera movements and exhibits physical interactions (e.g., collisions, occlusions) that"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "adhere to temporal causality. This signifies that generative models have begun to possess the deductive characteristics of world simulator [1, 163]. Open-Sora: Technology Democratization & Architecture Verification. As pioneer of the opensource community, Open-Sora [317] successfully replicated and verified the core logic of the Video DiT, providing transparent and efficient testbed for the academic exploration of the three consistencies. Its core innovation lies in the adoption of the Spatial-Temporal Diffusion Transformer [319] architecture, which utilizes cleverly designed alternating computation mechanism for spatial and temporal attention. This decoupling and synergistic design significantly reduces computational complexity while effectively balancing spatial fidelity within single frames and temporal coherence across frames. Supplemented by cascade training strategy and high-efficiency Video VAE encoders/decoders [223, 437], Open-Sora further confirms the stability of minute-level long sequence generation. This demonstrates that the efficient synergy of the trinity is not solely dependent on compute stacking; rather, rational architectural design is equally essential to achieving consistency with the physical world. Transition from Passive Observation to Active Interaction. If Sora established the emergence of physical common sense based on large-scale observations, then interactive world models represented by Genie 1/2/3 [438], LingBot-World [5], and GameNGen [439] mark fundamental leap of the trinity consistency from passive movie projection to active interactive simulation. The core breakthrough of these models lies in the explicit introduction of the action operator at into the spatiotemporal generation logic, transforming probabilistic modeling from P(x uture xpast) to controlled state transitions P(st+1 st, at) [163]. Specifically, Genie-3 [438] utilizes an unsupervisedLatent Action Model to decouple discrete action tokens from massive unlabeled videos, using them as conditional inputs for the spatiotemporal Transformer to ensure temporal causality under specific instructions (e.g., the causal feedback of character jumping after specific key is pressed). LingBot-World [5] further constructs unified cognition-action manifold, coupling high-level semantic instructions with low-level physical attributes such as collision detection and force feedback within the same latent space. This architecture not only maintains macro modal consistency but also preserves spatial fidelity under complex boundary conditions at 60 FPS generation rate through the introduction of spatiotemporal consistency regularization. The emergence of these programmable worlds proves that the synergy of the trinity can evolve into differentiable, predictable, and interactive World API, providing embodied agents with near-realistic mental sandbox simulation environment [438, 129]. Synergistic Corroboration of Diverse Technical Paths. Beyond the aforementioned models, other systems have enriched the technical map of world models from various dimensions, collectively corroborating the inevitable trend of three-consistency fusion: (i) 3D Causality & Explicit Modeling. CogVideoX [179] and Wan2.1 [273] both emphasize the role of 3D VAEs. CogVideoX strengthens inter-frame dependency through 3D RoPE, while Wan2.1s Causal 3D VAE enforces the unidirectional flow of the time dimension within the latent space, significantly enhancing the physical plausibility of dynamic evolution. (ii) High Fidelity & Fine-grained Control. Gen-3 [440] and HailuoAI [441] focus on industrial-grade consistency performance. Gen-3 demonstrates cinematic-grade lighting maintenance and complex physical interaction simulations, while HailuoAI utilizes dedicated engine optimizations to resolve structural collapse issues in complex dynamics scenarios. (iii) Architecture Exploration & Multimodal Alignment. HunyuanVideo [150], VideoCrafter [136], and LTX-Video [442] have conducted deep explorations into multimodal embedding spaces and attention mechanisms. These efforts further strengthen the semantic alignment between textual instructions and visual content, providing solid foundation for instruction-driven world simulation."
        },
        {
            "title": "3.5.2 Combat Loop of Three Consistencies",
            "content": "While generative models imagine the world, Embodied AI physically intervenes in the world under the guidance of the trinity. In this context, consistency is no longer merely visual sensory indicator but the cornerstone of agent decision safety and task success. Beyond RT-2 [349] and GAIA-1 [205], the academic and industrial sectors have evolved from simple vision-language mapping to deep closed-loop paradigm based on physical simulation and latent space planning. Interactive World Simulators: The Convergence of Physics, Logic, and 3D Fidelity. Building general simulator with interactive physical dynamics is prerequisite for embodied agents to engage in low-cost trial and error. new generation of world models is evolving from single video prediction toward full-dimensional Digital Twins. The Google Genie series (1-3) [438] and Matrix-Game 2.0 [443] first addressed the action-logic consistency problem: Genie achieved unsupervised action space discretization through its Latent Action Model, while Matrix-Game 2.0 introduced multi-agent game-theoretic logic, allowing simulated environments to handle complex social interactions and causal arbitration. At the spatial construction level, Hunyuan 3D World Model 1.0 [444] and NVIDIA Cosmos [146] have filled the gap in high-fidelity physical attributes. Hunyuan 3D replaces traditional 2D textures with generated explicit 3D assets to ensure geometric consistency during multi-view exploration by the agent; Cosmos embeds rigid/fluid dynamics equations into Transformer masks to achieve industrial-grade physical simulation. Building upon this, TwinRL-VLA [350] further validates the practical utility of Digital Twins: by introducing the Exploration Space Expansion strategy, it enables agents to perform large-scale parallel Online RL within the digital twin environment, effectively addressing the challenges of cold starts and constrained data distribution inherent in real-world training. Simultaneously, to address the high-frequency texture noise often produced by generative models, V-JEPA [445, 3] and DreamerV3 [446] adhere to the non-generative prediction paradigm. They model state transitions in an abstract representation spacePred(Enc(xt), z) Enc(xt+1)providing agents with denoised, efficient planning space focused on essential laws [1]. Unified Cognition-Action Manifolds: From Manipulation to Navigation. In real-world physical environment deployments, the core challenge lies in aligning high-dimensional semantic cognition with low-dimensional action execution on unified manifold. This paradigm has evolved from early simple instruction mapping to large-scale, all-modal closed-loop control. In the manipulation domain, WorldVLA [447] and LingBot-World [5] represent the SOTA evolutionary directions. WorldVLA demonstrates, through massive data scaling, that world models can serve as universal action compilers, directly translating vague linguistic intent into precise joint control flows. LingBot-World further proposes the Cognition-Action Unified Manifold, utilizing an asymmetric dual-stream architecture to couple semantic instructions with tactile/force feedback signals. Combined with the intermediate geometric generation capabilities of 3D-VLA [448], this explicitly resolves spatial ambiguity and physical constraints during manipulation. In the navigation domain, UniAD [449] and DriveVLM [450] extend this logic to autonomous driving. UniAD breaks the barriers between perception and planning by constructing full-stack unified feature flow, while DriveVLM leverages LLMs to demonstrate human-like counterfactual reasoning. This is essentially analogous to the logic of Matrix-Game 2.0: conducting causal simulations within the world model to achieve an evolution from reactive obstacle avoidance to proactive game-theoretic robust decision-making. Spatio-Temporal Constraints Based on Physical Causality In the physical world of embodied AI, spatio-temporal alignment must transcend mere visual plausibility and satisfy strict physical causality. Purely generative video models are often plagued by physical hallucinations, such as object interpenetration or levitation, while Digital Twins are emerging as the ultimate spatio-temporal anchor to address this issue. Studies represented by TwinRL-VLA [350] and RoboGen [354] propose solution based on explicit modeling: leveraging the state evolution of physics engines to replace the"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "pixel prediction of neural networks. Explicit State Reconstruction. This class of methods constructs Twin World isomorphic to the real world. In this space, spatio-temporal evolution is no longer sampled from probability distribution P(xt+1xt) but adheres to rigid body dynamics equations st+1 = physics(st, at) [93]. This imposes inviolable hard constraints on the spatio-temporal manifold; any generated spatio-temporal trajectory that violates physical laws is directly truncated or penalized during the simulation stage. Consistency Assurance in Sim-to-Real Transfer. Empirical studies demonstrate that this physics-engine-based spatio-temporal alignment possesses exceptionally strong transfer robustness. SimplerEnv [451] and ManiSkill2 [452] prove that policies trained in simulated spatio-temporal spaces that have undergone strict physical verification can be transferred to the real world with minimal adaptation cost (Sim-to-Real Gap). This mechanistically proves that spatiotemporal alignment achieved through simulation-based automatic search is more generalizable than that achieved solely through visual imitation, as it captures the underlying causal dynamic structure rather than merely pixel-level superficial correlations [453]. In summary, the development of the World Model is at key inflection point toward the synergistic emergence of the modality-spatial-temporal trinity of consistency [1]. From the implicit learning of physical laws in generative models like Sora to the causal deduction in latent space by embodied agents like 3D-VLA and DreamerV3, this series of theoretical verifications and implementations reveals core trend: the next stage of AGI lies in constructing General World Simulator capable of internalizing physical laws and possessing counterfactual reasoning capabilities [454, 455, 456]. This trinity synergy not only resolves the spatiotemporal hallucination issues in video generation [457] but also, by endowing models with deep understanding of the physical world, bridges the last mile from digital generation to physical interaction. It lays solid architectural foundation for AGI to establish unified cognition of the objective world [458]."
        },
        {
            "title": "4.1 Core Challenges from Preliminary Fusion to True Unification",
            "content": "Although the trinity of consistency across modality, spatial, and temporal dimensions has begun to show signs of synergy within the frameworks of MM-DiT and LMMs, the ultimate vision of constructing world model still faces significant theoretical gap [1]. This challenge transcends simple optimization of generation quality; its essence lies in the fundamental lack of completeness in physical ontology and robustness in causal epistemology in current models [459]. (i) Primary gap lies in the lack of differentiability of physical Authenticity. Existing diffusion models and autoregressive architectures still hold pixel-level or token-level likelihood maximization as their highest goal [223, 222]. This leads generation results into the trap of visual plausibilitywhere rigid bodies hover without support, fluid momentum is not conserved, and elastic coefficients drift with gestures [460]. The model merely learns the statistical textures of physical phenomena rather than the underlying vector mechanics. Future challenges lie in how to embed Hamiltonians, conservation laws, or differential equations into the loss function as soft constraints or even differentiable operators, forcing the network to move from painting the skin to painting the bones [77]. (ii) The butterfly effect brittleness of long-term causal chains remains unsolved. Current spatiotemporal attention mechanisms can only maintain short-range memory for tens of seconds [221]. Once entering the hour-day scale, object identity consistency and event logic suffer an avalanche of failure due to error accumulation [434]. The solution may lie in introducing hierarchical implicit dynamics: the macro level maintains abstract causality via symbolic narratives or scene graphs [461], the meso level compresses event nodes with sparse 4D representations, and the micro level utilizes high-dimensional attention to complete texture details, achieving multi-clock mechanism of slow variable fidelity and fast variable sampling [455, 462]."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Evaluation of Benchmark Modality Spatial Temporal ROVER [473], UniSandbox [474], WISE [475], etc. PhysDreamer [476], VR-Bench [477], VBench [478], PhysBench [479], etc. TiViBench [480], MME-COF [481], WEAVE [482], Thinking with Video [483], V-ReasonBench [484], GGBench [485], TempViz [486], etc. Figure 33: Evaluation of benchmarks. (iii) The paradigm shift of controllability and interactivity is imperative. Upgrading prompts to APIs means users are no longer passive describers but active World Editors [463, 464]. Users should be able to insert forces Ff orce at arbitrary spatiotemporal coordinates, modify materials, reset boundary conditions, and obtain real-time feedback that adheres to physical laws [465]. This implies that generative networks must embed neural surrogate models, allowing gradients to penetrate the complete chain of user action at state evolution (st+1 . . . ) sensory observation ximg, transforming blind box generation into draggable, scriptable, and programmable online simulation [438, 129]. (iv) Lastly, expanding the horizon to agentic evolution and digital ecosystems. The final form of world models should not stop at being physical sandbox but should become the Matrix that accommodates the evolution and gaming of autonomous agents [466, 200]. First, the introduction of multi-agent gaming requires the model to upgrade from modeling physical causality to modeling social causality [467]. In complex non-zero-sum games, the world model must be able to simulate the intentionality and strategic behaviors of multiple agents, deducing the Nash Equilibrium dynamics under the interaction of different policies πi, rather than remaining limited to single-agent physical feedback [468, 469]. Second, the rise of GUI agents requires world models to possess cross-domain generalization capabilitiesextending from simulating the 3D physical world to simulating 2D digital environments (the digital world) [470]. The model needs to understand the functional semantics of screen layouts and the state transition logic P(Sscreen , aui) of API calls, thereby supporting agents in achieving an end-to-end closed loop from perception to action within the virtual world of operating systems. This marks the evolution of the world model from pure physical simulator into General World OS encompassing both physical and digital attributes [471, 472]. Sscreen t+"
        },
        {
            "title": "4.2 Constructing Comprehensive Evaluation Benchmarks",
            "content": "As the world model leaps from short video generation toward becoming physical simulator [272], distribution statistical metrics represented by FID and FVD have become inadequate for capturing deep-level logical fractures [487, 165, 488]. Continuing to rely on such perceptual metrics would cause model optimization to stall in local optima that are visually realistic but causally distorted. To drive the domain toward deducible and verifiable direction, the community has introduced series of evaluation benchmarks targeting the core requirements of the Trinity, as shown in Figure 33 [478, 473], aiming to establish complete verification loop from symbolic logic to physical simulation."
        },
        {
            "title": "4.2.1 Modal Consistency: From Symbol Mapping to Knowledge Synergy",
            "content": "Traditional modal consistency evaluation relies primarily on CLIP scores for shallow semantic cooccurrence calculations. The current evolutionary direction has shifted toward knowledge internalization and cross-modal reasoning."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Knowledge-driven Alignment. WISE [475] introduces structured prompt library covering natural sciences, utilizing WiScore to quantify models ability to internalize world knowledge into visual representations; by constructing counterfactual negative samples, it fills the evaluation gap between symbol and perception. ROVER [473] verifies the closed-loop coherence of the bidirectional generation chain (Text Pixel) through reciprocal reasoning. Execution Gap between Understanding and Generation. UniSandbox [474] reveals an asymmetric phenomenon where understanding is correct but generation is wrong. This benchmark quantifies the execution gap of models in complex attribute transfer and mathematical visualization, proving that the introduction of an explicit CoT) is key mechanism for bridging this gap."
        },
        {
            "title": "4.2.2 Spatial Consistency: From Visual Similarity to Topological & Physical Verification",
            "content": "Evaluation in the spatial dimension has shifted from perceptual visual scoring to rigorous 3D topological structure and physical repulsiveness verification. We categorize related work into two levels: semantic logic verification and physical dynamics verification. Topological Logic & Interactive Reasoning. VR-Bench [478] focuses on complex spatial relation reasoning, particularly occlusion, perspective, and path planning tasks. Its research reveals significant modality dependency, where existing models perform much worse on pure visual spatial reasoning than on text-assisted reasoning. VBench [478] further proposed decoupled evaluation standards, using VLM-as-a-judge to refine spatial consistency into object constancy and spatial relations. Crucially, by calculating the graph edit distance between the generated scene graph and the prompt scene graph, it precisely quantifies the logical accuracy of spatial layouts. Physical Simulation & Penetration Detection. To compensate for the lack of dynamic constraints in pure visual evaluation, PhysBench [479] and PhysDreamer [164] introduce physics engines as groundtruth referees. They reconstruct pseudo-3D point clouds through depth estimation and calculate the minimum Euclidean distance min pi pj2 between objects as penalty term. This method strictly detects spatial penetration and floating artifacts, establishing spatial evaluation standard for rigid bodies under Newtonian mechanics constraints."
        },
        {
            "title": "4.2.3 Temporal Consistency: From Inter-frame Smoothness to Logical Causal Evolution",
            "content": "A profound paradigm shift has occurred in the evaluation of temporal consistency: moving from focus on the visual continuity between video frames to the logical chronology underlying the generation process. We categorize this into visual physical chronology and symbolic logical chronology. (1) Static Temporal Semantics (Time-as-Attribute). The physical foundation of temporal consistency lies in the models state awareness of entities as they evolve over time (e.g., seasons, aging, historical eras). Addressing previous limitations that focused solely on video dynamics, TempViz [486] proposed static evaluation paradigm for temporal knowledge. By constructing dataset containing 7.9k prompts, this work quantifies the ability of text-to-image models to understand time-variant attributes. The study reveals that even state-of-the-art models exhibit significant knowledge gaps when generating contextually relevant images (e.g., distinguishing between spring landscape and winter landscape) and proves that automated metrics like CLIP fail to capture such temporal nuances. (2) Visual Physical Chronology (Thinking-in-Video). TiViBench [480] introduced the Think-in-Video concept, forcing models to demonstrate the problem-solving process of physical tasks (e.g., fluid motion, maze navigation) by generating video. The core objective is to verify whether the intermediate state trajectory τ = {s1, . . . , sT} adheres to Markov dynamics. V-ReasonBench [484] further introduced the"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "optical flow operator low to monitor motion mutations, effectively avoiding visual hallucinations from VLM referees. (3) Symbolic Logical Chronology & Process Verifiability. While GGBench [485] is oriented toward geometric reasoning, its core mechanism utilizes GeoGebra as an executable environment to verify the step-by-step construction sequence S1 S2 Sn of multimodal reasoning. Geometric construction is essentially the construction of causal chain along the time axis. GGBench not only checks the correctness of the final image but also verifies temporal dependency in the construction steps through code execution (e.g., points and must be defined before line segment AB can be constructed). This evaluation of logical time reveals whether world model possesses reasoning robustness isomorphic to physical time when handling long-range dependency tasks. (4) Long-range Defects & Constancy Failure. Regarding the catastrophic forgetting common in long sequence generation, MME-COF [481] and WEAVE [482] systematically expose physical hallucinations in SOTA models (e.g., rigid body collisions violating the law of reflection) and failures in object permanence. Notably, Thinking with Video [483] points out that the powerful temporal reasoning of models often relies on text priors from LLMs rather than native visual causal discovery capabilities."
        },
        {
            "title": "4.2.4 Limitations of Existing Benchmarks & Design Rationale of Our Benchmark",
            "content": "Although existing evaluation systems are effective in verifying single-point capabilities, they exhibit significant structural defects in their evaluation paradigms when judged by the standards of general world simulator. These defects lead to severe disconnect between evaluation results and the models actual physical capabilities, manifesting at four pragmatic levels: (1) Soft Ceiling of Metrics & Judge Hallucination. Current mainstream benchmarks (e.g., TiViBench [480], V-ReasonBench [484]) rely excessively on MLLMs such as GPT-4o or Gemini as referees. This model evaluating model approach possesses an intrinsic defect: VLMs themselves have extremely low perception precision for fine-grained physical attributes (e.g., friction coefficients, fluid viscosity) [489], often resulting in misjudgments due to visual masking logicwhere generated video is awarded high score as long as the frames are smooth, even if it violates Newtons Third Law. Although recent work has attempted to introduce self-reflection/critic models [490] or design complex fine-grained rubrics to reduce variance, these patch-like corrections do not address the core contradiction: the lack of hard verification based on simulation engine ground truth [460]. Pure visual referees will never distinguish between physical simulation and visual deception, causing evaluation to remain at the level of surface semantics without reaching physical essence. (2) In-Distribution Memory Masks OOD Generalization Shortcomings. Existing datasets [478, 481] are largely collected from real-world videos or standard game recordings, which often causes large models to fall into the trap of rote memorization of training data [491]. This fitting effect fails completely in OOD scenarios, manifesting as: causal chain ruptures in ultra-long temporal sequences (e.g., an object disappearing after being occluded for one minute) [492]; attribute confusion in multi-object complex interactions (e.g., color swapping after three objects collide) [493]; and deduction failure in counter-intuitive physical environments (e.g., negative gravity or non-Euclidean geometric space). As the isolation experiments in UniSandbox [474] revealed, when common visual backgrounds are stripped away and models are forced to make physical predictions under unfamiliar combinations, their performance drops significantly. This proves that current high scores often stem from overfitting specific distributions rather than truly learning transferable world laws. (3) Error Accumulation in Long-range Generation & Lack of Process Verification. The vast majority of benchmarks only test short sequence (10s) generation, masking the state drift issues of world"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "models in long-range simulation [494]. The deep technical crux of this problem is that existing generation architectures (whether autoregressive or diffusion models) inherently lack online process verifiers and physical constraint correction modules [495]. Unlike traditional physics engines that solve equations frame-by-frame, generative models rely primarily on probabilistic sampling. Minor physical errors (e.g., collision penetration, slight non-conservation of momentum) can undergo exponential amplification (the butterfly effect) as the timestep advances in the absence of differential equation hard constraints, eventually leading to the logical collapse of the entire world [496, 77]. Existing benchmarks lack deep probe into this generation process verifiability and cannot quantify models ability to counter entropy increase in long sequences. (4) Lack of Causal Probes for Active Intervention. Existing evaluations operate in static spectator mode, only requiring the model to predict what happens next. True world cognition must undergo the test of an intervener mode, namely counterfactual reasoning [454]. For example, If the support is removed at this moment, how will the object trajectory τ change? [497]. Current benchmarks lack an evaluation interface supporting such parameterized interventions, making it impossible to verify whether model has constructed structured causal graph or is merely performing pixel-level probabilistic completion. Faced with the quadruple dilemma of evaluation subjectivity, scenario greenhouse, temporal myopia, and interaction static, constructing next-generation evaluation benchmark characterized by hardcore physical standards, dynamic long-range evolution, and support for causal intervention has become top priority. To systematically decouple and evaluate the three core consistencies of world modelsmodality, spatial, and temporal consistencyand their pairwise fusion relationships, subsequent work in this paper introduces CoW-Bench. Unlike previous datasets that relied on static images or vague semantic scoring (e.g., CLIP score) [498], CoW-Bench organizes evaluation around six task categories derived from the three consistencies and their intersections, comprising 18 sub-tasks in total. Each sub-task is paired with five carefully designed human checklists, yielding comprehensive, task-driven protocol with fine-grained criteria to pinpoint complementary failure modes and enable more precise, interpretable quantification."
        },
        {
            "title": "4.3 Ultimate Outlook: General World Simulator",
            "content": "As the aforementioned challenges are sequentially overcome, the World Model will shed its guise as content generation tool and undergo dimensional ascent to become General World Simulator [272, 1]a digital universe capable of instantiating arbitrary physical laws and narrative rules on demand. For scientific exploration, it serves as virtual laboratory for verifying complex hypotheses; for Embodied AI, it acts as an inexhaustible safe training ground and real-time online brain vestibulewhere robots can perform extreme trial and error at millisecond-level and transfer distilled policies π to reality via zero-shot transfer [499, 446, 128]. Furthermore, when world models can self-consistently simulate the multiple entanglements of physics, society, and emotion [466, 200], we will possess, for the first time, an ultimate testbed capable of mirroring all externalities of human intelligence. In that realm, constructing world models and understanding the essence of intelligence will merge into one: the world provides constraints while intelligence generates hypotheses, and the two endlessly negotiate, converge, and evolve within differentiable spacetime [500]."
        },
        {
            "title": "5.1.1 Dataset Construction",
            "content": "Consistency-Centered Task Blueprinting We construct the overall task framework of CoW-Bench around the three core consistencies of world modelsmodal, spatial, and temporal consistencyand their pairwise integration. Each task category is further decomposed into three sub-tasks designed to characterize distinct yet complementary failure modes within the same consistency dimension (see Table 4). To ensure that evaluation signals are interpretable and attributable, we introduce SingleConsistency Variable Control Protocol during the task design phase: for each sub-task, only variables directly related to the target consistency are permitted to vary, while other potential confounding factors (e.g., number of entities, background complexity, camera movement, motion magnitude, and occlusion conditions) are explicitly constrained. This design avoids coupling interference between different consistency factors, allowing model behavior to be stably attributed to the target capability. Table 4: Taxonomy of Tasks in CoW-Bench. The benchmark covers three foundational dimensions: (Modal Consistency), (Spatial Consistency), and (Temporal Consistency). Crucially, it probes their deep synergies required for world simulation: MS, MT, and ST. Each family is further decomposed into three specific sub-tasks to isolate distinct failure modes. Task Sub-tasks I. Basic / Atomic II. Structured / Dynamic III. Complex / Constraint Single-Consistency Dimensions Style/Material transfer Planar layout Worldline persistence Cross-Consistency Synergies MS MT ST Semantic planar binding Long-horizon anchoring Planar maze trajectory Fine-grained control Hierarchical occlusion Rule-guided evolution Multi-constraint composition Multi-view 3D structure Ordered stage transitions Semantic hierarchy control Attribute dynamics alignment Occlusion dynamics Semantic 3D view consistency Triggered event compliance 3D loop navigation coherence Reasoning-Driven Seed Construction Once the task blueprint is finalized, we first construct set of seed instances to anchor the logical core of each task. This phase employs models with deep reasoning capabilities, whose objective is not merely to generate samples matching description, but to accurately internalize target consistency constraints and design challenging instances capable of authentically triggering corresponding failure modes. Each seed instance adopts unified structured representation, including text prompt (inputText), an initial state description (inputImageDesc), and specifications for the expected image and video outputs (outputImageExpect, outputVideoExpect). This structured design binds conditions, initial states, and target results into verifiable units, providing stable reference for subsequent controlled expansion."
        },
        {
            "title": "5.1.2 Dataset Analysis",
            "content": "To demonstrate that CoW-Bench serves as rigorous and non-trivial benchmark for evaluating World Models, we conduct comprehensive analysis of its statistical distribution, fine-grained complexity, and semantic diversity. All statistics reported are based on the audited data presented in Table 5. Statistics and Hierarchical Ontology. CoW-Bench comprises 1,485 meticulously constructed samples, organized into two-level hierarchy: Modal Level (Single vs. Cross) and Task Level (spanning Modal,"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 34: Hierarchical Taxonomy of CoW-Bench. The inner ring represents the main consistency dimensions (Modal, Space, Time), while the outer ring details the 18 fine-grained sub-tasks. The uniform sector sizes visually confirm the rigorously balanced distribution of the dataset. Spatial, Temporal dimensions and their intersections). Unlike prior benchmarks that often exhibit long-tail distributions leading to evaluation bias, CoW-Bench maintains strict distributional balance. As shown in Table 5, each of the 18 fine-grained sub-tasks contains between 69 and 91 samples (with the specific inclusion of 50 hard Maze cases). This uniformity ensures fair and unbiased assessment across all capability dimensions, preventing models from achieving inflated scores by overfitting to simple or frequent task types. Fine-grained Complexity Analysis. core design principle of CoW-Bench is the coverage of comprehensive difficulty gradient. We argue for the non-triviality of the tasks from three complementary dimensions. (1) Instruction Span & Semantic Depth. The dataset exhibits significant variance in instruction complexity. Ranging from atomic tasks like Modal-Subj-Attr (avg. 7.1 words) to compositional tasks like Modal-Multi-Require (avg. 74.8 words), this vast span (7.174.8 words) challenges the robustness of World Models in language understanding, requiring them to handle both explicit short commands and long-context, multi-constraint instructions. (2) Visual & Cognitive Load. We quantify visual complexity using the average element count per sample. Quantitative analysis reveals that cross-modal tasks generally impose higher cognitive load (e.g., 3D-Reconstruct involves 2.1 complex elements on average, significantly higher than the 1.6 in single-modal tasks). This confirms that cross-modal tasks effectively probe the models retention capabilities in visually dense and structurally complex scenes. (3) Dynamic Evolution Complexity. Beyond static elements, the Action Complexity metric highlights the temporal richness of the benchmark. Tasks such as Time-State exhibit extremely high"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 5: Comprehensive Statistics of CoW-Bench. Mode Task Sub-Task Modal Single Space Time MS Cross MT TS Subj-Attr Minor-Ctrl Multi-Req 2D-Layed 2D-Rel 3D Consist Slow-Evol State 2D-Layed 2D-Rel 3D-POV Event-Rsp Prop-Cons Prop-Var 3D-Recon Maze-2D Cam-Mask 91 87 81 89 91 91 88 81 85 69 76 80 86 91 80 50 91 Scene Diff Complexity Metrics (Avg.) Prmpt ImgR Obj Obj Mix Mix Obj Room Obj Obj Mix Room Obj Out Mix Obj Mix Out Flat Mix Easy Med Hard Med Easy Hard Med Easy Hard Med Med Hard Med Med Hard Hard Hard Hard 7.1 37.4 74.8 38.2 42.7 47.1 16.4 3.3 8.0 46.1 48.7 65.4 44.7 47.2 51.6 35.4 12.5 10. 13.3 37.4 13.6 24.7 17.1 35.0 33.8 33.3 31.4 35.5 33.6 33.5 22.7 28.4 26.3 63.4 15.0 27. Act 22.0 37.1 64.7 32.8 52.8 57.4 38.6 40.1 77.7 27.1 44.6 38.5 36.4 37.7 50. 43.6 35.5 57.2 Elem 2.1 1.6 1.6 2.4 1.7 2.2 2.1 2.9 2.0 2.1 1.6 2. 2.7 1.8 2.3 2.1 3.0 2.2 action complexity (avg. 77.7 words), indicating that the generated videos contain intricate dynamic evolutions rather than simple static scene translations. It is worth noting that the aforementioned data are not merely automated outputs but have undergone rigorous multi-source auditing process (see Section 5.1.1). Through human-machine collaborative verification, we corrected metric biases and confirmed semantic alignment, establishing CoW-Bench as reliable and reproducible gold standard in the community."
        },
        {
            "title": "5.2 Evaluation metrics",
            "content": "Consistency Capability Evaluation. CoW-Bench evaluates the consistency capabilities of world models by formalizing the task as constraint satisfaction problem: given text conditions, reference images, or initial states, the generated output must satisfy the constraints implied or explicitly stated in these conditions while remaining stable across temporal and spatial dimensions. Unlike holistic similarity or perceptual quality metrics such as FID/IS, critical failures in world models often manifest not as lack of realism, but as the violation or implicit relaxation of constraints. Typical scenarios include: reverting rare materials to common ones, diffusing local edits into global drifts, reinitializing the same worldline frame-by-frame in temporal sequence, reversing foreground-background relations during occlusion, or redrawing different worlds under multi-view conditions. Since these failures may appear plausible to the eye, the core evaluation signal must be whether the constraints are actually honored. Atomic Decomposition. To obtain attributable, diagnostic, and reusable evaluation signals, we employ atomic decomposition: abstracting recurring failure modes across tasks into set of observable atomic checks, and defining the evaluation metrics for each task family as combination of several atomic checks. This design achieves two core objectives: (1) Diagnosability: each atomic check corresponds to specific failure mechanism (e.g., identity drift, attribute rebinding, boundary leakage, worldline drift, or occlusion contradiction), allowing the scoring results to pinpoint the source of the problem; (2) Modular Reuse: the same atom maintains the same semantics across different task families, ensuring that cross-task comparisons are conducted within unified measurement coordinate system. It is"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 6: Metric families and their five sub-metrics in CoW-Bench. Abbreviations: Id+Attr = identity and attribute consistency; Min-change = minimal change; Inter-state = intermediate-state validity; Persp/Scale = perspective and scale consistency; Occ-update = occlusion-update plausibility; Geo-self = geometric self-consistency; Excl. = mutual exclusivity; Env-stab = environment stability. Family Focus Sub1 Sub2 Sub Sub-metrics M1 M2 M3 T1 T2 T3 S1 S2 S3 MS1 MS2 MS3 MT1 MT2 MT ST1 ST2 ST3 Subj-Attr Local-Edit Multi-Const Worldline Slow-Evol Stage-Order Sem-Planar Occ/Contain MV-3D Sem-Planar Sem-Hier Sem-MV Long-Horizon Attr-Dyn Trigger-Event Maze-2D Occ-Motion 3D-Loop Id+Attr Target Complete Subj-cons Subj-lock Order Dir Occl. Struct Ent-match Pos-rel Anchor Init-anchor Target(E,A) Pre-hold Start/Goal Occ-move Struct Backoff Min-change Attr-corr Attr-stab Trend Identif. Count Boundary Surface Act-align Neg-rel View-stab Long-stab Follow Trigger Traj-cont Parallax Rel Dominance Leakage Rel-corr Env-stab Time-scale Timing Rule Visible Persp/Scale NT-stab Excl. Lateral Cross-scene Smooth Post-comp Legal Rigid View-smooth Sub4 Clarity Clarity Omission Visual Inter-state Process Boundary Rel-stab Occ-update Attr-bind Vis+Layer Scene Attr-bind Rate State-stab Correct Natural Physical Sub5 Excl. No-extra No-extra Evol-cont Rule Worldline Layout Layer Geo-self Global Id-stab Marker No-unexp Env-stab Env-stab Struct-stab Env-stab Entity-stab important to emphasize that CoW-Bench contains 18 metric families (M1ST3), while the atomic library contains 16 atomic checks (A1A16). Metric Families and Sub-metrics Regarding what is specifically measured for each task family, we first provide the names of their five corresponding sub-metrics (see Table 6). These sub-metrics provide human-readable descriptions at the task-family level and are semantically aligned one-to-one with the subsequent atomic library: sub-metrics capture the focus within task, while atomic checks provide cross-task consistent criteria, thereby achieving balance between readability and rigor. Atomic Library: Unified Criteria Shared Across Tasks. Table 7 presents the library of atomic checks. Each atomic check employs an operational definition to ensure that evaluation does not rely on aesthetic preferences but on verifiable phenomena; furthermore, the sharing of the atomic library across different task families provides consistent semantic foundation for cross-task comparisons. Compositional Definition: Constructing Metric Families via the Atomic Library. Building upon the atomic library, we define each metric family compositionally as structured aggregation of invoked atomic checks. Table 8 presents the invocation matrix of the metric families for A1A16. This matrix makes modular reuse explicitly visible at the structural level: the same atom assumes the same measurement semantics across different task families, thereby avoiding the redundant definition of approximate metrics for each task family and ensuring that evaluation results can be compared and attributed along shared measurement dimensions. Scoring Scale (02). For each sample, we provide an ordinal score of 02 for each evaluation dimension invoked by its corresponding metric family: 0 indicates clear violation or failure; 1 indicates partial fulfillment but with ambiguity, deviation, or unclear evidence; 2 indicates clear, stable, and undisputed fulfillment. This discrete scale is consistent with constraint satisfaction interpretation, reducing subjective noise introduced by continuous scoring while maintaining diagnostic resolution for failure modes. In evaluation and result aggregation, sample-level 02 scores are first used to form average"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 7: Atomic library of CoW-Bench. Each atomic check defines reusable, observable evaluation criterion with concise operational one-sentence definition for systematic consistency assessment. ID A1 A2 A3 A5 A6 A7 A8 A9 A10 A11 A12 Atomic check Identity lock Attribute binding Constraint non-relaxation Evidence clarity Mutual exclusivity Locality of change Non-target invariance No spurious additions Set completeness Relation correctness Multi-constraint coverage Worldline stability Temporal continuity A14 Stage structure A15 Occlusion & layering 3D geometric coherence Operational definition The intended target entity remains unchanged; no identity swap, duplication, or replacement occurs. Key attributes remain bound to the same entity; no attribute migration occurs. Specified constraints are not weakened or substituted with more common but non-equivalent variants. Evidence supporting each constraint judgment is clear and unambiguous. Mutually incompatible properties do not co-occur on the same target. Changes are confined to the designated region or attribute without boundary spillover. Non-target entities or regions remain stable except for explicitly permitted changes. No extra entities, objects, or parts appear beyond the instruction. Required entities form complete set with correct cardinalities. Specified relations or actions are satisfied without role swapping. Multiple constraints are jointly satisfied without selective omission. The output depicts single consistent world rather than frame-wise reinitialization or scene drift. Permitted changes evolve smoothly without abrupt jumps or oscillatory backtracking. When discrete stages are specified, they are identifiable and appear in the correct order without spurious steps. Depth ordering and occlusion are correct and non-contradictory; visible boundaries update plausibly. Multi-view outputs remain explainable as projections of single 3D scene with consistent perspective and occlusion. scores for each sub-metric within task; subsequently, the average scores of various sub-tasks under the same metric family are aggregated with equal weighting to obtain the final score. Evaluation Protocol: 22 Grid Temporal Sampling. For video tasks, we uniformly sample 4 frames from the entire sequence in chronological order. For image tasks, we generate four key images in chronological order. These four frames or images are arranged in 22 grid (left-to-right, topto-bottom). Evaluators must analyze the sequence frame-by-frame without skipping and provide justifications and 02 scores for each item based on five-question chain aligned with the metric family. This protocol explicitly exposes critical evidence of temporal consistency (such as continuity, intermediate states, stage structure, and worldline stability) as verifiable phenomena, thereby reducing bias caused by selective observation. The evaluation prompt template is shown below. Consistency-Oriented Evaluation Prompt Role: You are an expert in evaluating the quality of AI-generated results. Input: 22 grid of four frames sampled uniformly in temporal order (left-to-right, top-to-bottom). Task: Evaluate the sequence using five-question chain aligned with the metric family. Scoring: Each question is scored from 0 to 2 with justification. Rules: Analyze frames sequentially without skipping; judgments must be based only on the sampled frames. Output: For each question, output exactly: QuestionX, Score, Rationale."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 8: Compositional definition of metric families via the atomic library. checkmark indicates that the metric family invokes the corresponding atomic check. A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 A11 A12 A13 A14 A15 A"
        },
        {
            "title": "Metric family",
            "content": "Attribute Fidelity (M1) Local Edit Precision (M2) Multi-constraint Satisfaction (M3) Worldline Persistence (T1) Evolutionary Dynamics (T2) Ordered Stage Transitions (T3) Planar Layout Correctness (S1) Hierarchical Occlusion (S2) Multi-view 3D Coherence (S3) Semantic Role Binding (MS1) Semantic Hierarchy Compliance (MS2) Semantic Multi-view Stability (MS3) Long-horizon Anchoring (MT1) Attribute Dynamics Alignment (MT2) Triggered Event Compliance (MT3) Planar Maze Trajectory (ST1) Occlusion Dynamics Under Motion (ST2) 3D Loop Navigation Coherence (ST3)"
        },
        {
            "title": "5.3 Comparison with Existing Benchmarks",
            "content": "Existing multimodal evaluation systems are primarily constructed around the understanding capabilities of MLLMs, forming standardized paradigms represented by UniBench [512] and MANBench [513]. However, significant dimensional gap remains in the evaluation of generative world models. We define the essential differences between CoW-Bench and existing work across three key dimensions: Discriminative Perception vs. Generative Simulation. UniBench addresses the fragmentation of multimodal evaluation by integrating over 50 existing datasets to comprehensively assess the discriminative capabilities of models in visual perception, attribute reasoning, and spatial relationship understanding [512]. This evaluation paradigm implicitly assumes that the model functions as passive observer, tasked with deconstructing static images or videos provided as input. In contrast, CoW-Bench targets the generative simulation capabilities of world models, treating them as active simulators. Rather than emphasizing whether model demonstrates exceptional question-answering proficiency, as in MANBench [513], our focus lies in assessing whether model can actively preserve physical constraints and causal coherence throughout dynamic world evolution. In this sense, CoWBench fills critical evaluation gap between assessing models ability to perceive and reason about the world and its capacity to consistently construct and simulate it over time. Evaluation Signals: QA Accuracy vs. Dynamic Constraint Satisfaction. The core contribution of MANBench lies in establishing human performance reference frame, where evaluation signals are derived from the VQA (Video Question Answering) accuracy measured against static ground truth [513]. However, this discrete binary judgment (correct versus incorrect) is insufficient for capturing continuous, non-binary physical failures that frequently arise in generative settings. CoW-Bench instead formulates evaluation as multi-factor constraint satisfaction problem. As UniBench identifies hallucination as major bottleneck for MLLMs [512], in generative scenarios such hallucinations typically manifest as breakdowns in spatio-temporal consistency, such as objects disappearing after"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 9: Main results on CoW-Bench over 18 sub-tasks (higher is better); MEAN averages all sub-tasks. Abbrev: SUAT=Subj-Attr, LCED=Local-Edit, MCON=Multi-Const; WLIN=Worldline, SLEV=SlowEvol, STOR=Stage-Order; SEPL=Sem-Planar, OCCO=Occ/Contain; MV3D=MV-3D; TREV=TriggerEvent; LOHO=Long-Horizon, ATDY=Attr-Dyn; SEMV=Sem-MV; 3DLO=3D-Loop; OCMO=OccMotion; MAZE=Maze-2D. AVG is rescaled from the original [0, 10] to percentage scale of [0, 100]. Model SUAT LCED MCON WLIN SLEV STOR SEPL OCCO MV3D TREV LOHO ATDY SEPL OCCO SEMV 3DLO OCMO MAZE AVG Modal Temporal Spatial Modal-Temporal Modal-Spatial Temporal-Spatial Closed-Source Video-Generation Models Sora [2] Kling [279] 5.16 4.11 8.35 8.19 8.38 5.63 9.32 9.10 5.80 5.17 6.22 5. 8.41 8.72 6.19 7.88 8.51 9.32 6.96 7.08 8.12 8.71 5.25 6. 8.64 8.20 5.97 6.79 9.49 9.44 8.40 8.08 9.25 9.30 4.17 5. 73.66 73.96 7.37 GPT-image-1 [501] 5.73 Seedream-4-0 [502] 6.69 Seedream-4-5 [502] Nano Banana [503] 7.19 Nano Banana Pro [503] 7.39 GPT-image-1.5 [504] 7.75 Allegro [505] HunyuanVideo [506] LTX-Video [442] CogVideoX [179] Easy Animate [507] Wan2.2-I2V-14B [273] SkyReels-V2 [508] Qwen-Image [509] BAGEL [8] UniVideo [510] Emu3.5 [511] 1.97 2.91 3.59 3.75 3.78 3.32 3.16 0.72 5.01 4.07 6. 8.96 7.57 7.78 8.47 8.81 8.99 5.79 6.89 6.78 5.90 7.11 7.61 7.45 2.21 5.86 7.27 8.76 7.96 6.73 6.91 5.63 6.98 8.34 4.41 2.41 4.54 5.82 5.10 6.57 5.29 7.73 5.53 4.14 8. 9.14 8.63 8.77 8.87 8.88 9.23 7.03 8.62 8.53 8.29 8.70 8.54 8.89 2.10 6.27 8.58 8.77 7.75 6.25 7.46 7.86 8.39 8.65 1.91 3.06 3.67 4.13 4.33 4.00 4.03 0.41 5.01 3.87 5. 5.68 6.27 6.76 6.71 7.48 7.14 3.33 2.94 3.20 3.72 3.59 3.80 3.74 1.64 3.96 3.08 4.72 Closed-Source Image-Generation Models 7.43 6.84 7.21 7.84 8.48 8.53 9.26 8.82 9.06 9.34 9.61 9. 6.95 6.70 7.17 7.33 7.84 8.13 9.28 8.63 9.11 9.66 9.56 9.74 7.22 6.77 6.77 6.67 7.36 8.05 9.00 8.12 8.04 8.68 9.51 9.45 Open-Source Video-Generation Models 5.75 5.67 6.17 5.55 6.36 6.10 5. 7.89 6.04 6.49 5.70 7.81 6.27 7.93 4.72 4.01 4.76 5.15 4.94 5.33 5.39 7.67 9.52 7.34 8.66 7.85 8.37 8.87 4.80 3.66 4.95 5.29 5.29 5.24 5.66 4.22 5.83 5.48 6.44 6.01 6.69 7.70 Open-Source Image-Generation Models 1.70 6.43 6.15 8.62 1.35 7.68 6.81 8.77 1.72 4.45 4.29 5.70 0.84 8.60 8.72 9.42 1.61 4.91 5.69 6.10 1.69 7.08 6.26 8. 8.09 6.09 6.27 7.64 8.10 8.32 6.82 6.52 6.83 6.61 7.35 7.37 7.92 1.89 5.33 6.84 8.81 6.83 7.36 7.27 7.95 9.17 8.69 5.30 4.78 5.13 5.01 5.58 6.17 6.75 0.61 5.22 5.09 8. 9.79 9.50 9.59 9.20 9.10 9.79 7.19 9.64 8.77 8.93 7.95 9.51 9.07 1.71 8.89 9.08 9.76 8.46 8.13 8.23 8.13 8.86 8.54 6.87 6.97 6.73 6.37 6.78 7.11 8.18 2.96 5.51 7.34 8. 8.22 6.78 7.51 8.76 8.65 8.20 7.27 6.79 8.67 8.23 8.71 6.84 8.18 0.77 6.00 7.49 9.22 7.24 3.48 2.28 5.16 4.46 7.26 1.86 2.08 1.24 2.04 2.98 2.46 3.66 0.32 5.08 3.16 5. 80.35 71.33 73.82 78.38 82.57 85.62 52.67 54.63 57.15 58.66 61.23 61.83 65.37 17.77 59.34 59.96 77.76 occlusion. To address this limitation, we adopt fine-grained atomic checks that explicitly quantify models robustness with respect to modal, spatial, and temporal constraints in long-horizon generation, rather than relying solely on semantic alignment. Complexity Sources: Cognitive Depth vs. Spatiotemporal Entanglement. MANBench primarily evaluates the high-order cognitive abilities of models, where task difficulty is largely attributed to the depth of logical reasoning and the breadth of knowledge invocation required to exceed human-level performance [513]. In contrast, the difficulty of CoW-Bench arises from the intrinsic entanglement of spatiotemporal dynamics. Empirical results demonstrate that even models with strong cognitive reasoning capabilities, such as GPT-4V, exhibit pronounced failures on cross-consistency tasks, particularly those involving modaltemporal coupling (e.g., T). These findings indicate that the core challenge for world models does not lie in abstract problem-solving capacity, but rather in maintaining coherent dynamic inference under multiple interacting physical constraints."
        },
        {
            "title": "5.4 Main Results",
            "content": "Table 9 reports the task-level scores of CoW-Bench, covering 18 sub-tasks that span modal, temporal, spatial, and cross-consistency regimes. The overall ranking highlights clear trend: closed-source image generation models dominate the average score, while open-source video generators remain substantially behind on most consistency-sensitive tasks. In particular, GPT-image-1.5 achieves the best overall performance, followed by Nano Banana Pro and GPT-image-1. This gap suggests that todays strongest unified multimodal priors already encode rich static world regularities, yet still face systematic failure modes when consistency constraints require long-horizon, multi-factor enforcement."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "(1) Temporal control is the bottleneck rather than coherence. Across multiple families, T-WL (worldline persistence) is consistently high even for several video models (e.g., Sora reaches 9.32), indicating that generating visually continuous footage is no longer the hardest part. However, temporal tasks that demand rule-grounded evolution or structured state progression show more uneven landscape (e.g., T-Rule and T-Stage-Order vary sharply across models). This separation supports key CoWBench thesis: world models require constraint satisfaction over time, not merely smoothness. model can look temporally plausible while still violating causal constraints. (2) Spatial consistency is strong in single-view 3D, but cross-view anchoring still breaks. Most top models score highly on S-3D (single-scene 3D plausibility), with several exceeding 9.0 (e.g., Nano Banana Pro reaches 9.61). Yet cross-consistency tasks reveal tighter bottleneck: while MS-3D (text-to3D viewpoint control) remains high for leading models (often 9), TS-Maze-2D and some time-space settings remain much lower. This pattern suggests that local geometric plausibility is easier than maintaining globally anchored spatial structure under motion and decision-like trajectories. (3) Fusion tasks reveal the real world-model gap: persistent semantics under dynamics. The strongest separation between state-of-the-art models and the rest occurs in cross-consistency families (MT, MS, TS). For instance, leading models obtain near-ceiling performance on MT-PropKeep (property persistence under temporal evolution), yet degrade on MT-PropChange (attribute change alignment) and especially on TS-Maze-2D (navigation-style structure preservation). Notably, some high-avg models still exhibit pronounced weaknesses on TS-Maze-2D (e.g., Nano Banana Pro reports 4.46), indicating that global world-state maintenance and trajectory-level constraint enforcement remain unsolved even when per-frame fidelity is excellent. This is precisely the regime where UMMs must evolve from perceptual generators into genuine internal simulators. (4) Open-source models expose failure modes aligned with CoW-Benchs motivation. Open-source video generators typically underperform on modal grounding (M-Subj-Attr) and cross-consistency tasks, consistent with qualitative observations that they either (i) relax rare constraints into common defaults, or (ii) preserve motion while drifting in identity/attributes. Meanwhile, open-source image models show large variance: Emu3.5 is competitive in many columns but still drops on time-centric and time-space settings, reinforcing that CoW-Bench targets the gap between single-shot plausibility and multi-step consistency. Takeaway. UMMs perform well on static or single-view settings, where local plausibility is sufficient. However, when task requires maintaining stable world while changes unfold over time and space, performance drops sharply. These cross-consistency scenarios are the clearest indicator of whether model truly behaves as world model rather than frame generator."
        },
        {
            "title": "5.5.1 Modal Consistency Results",
            "content": "Table 10 reports modality-consistency performance across three metric families: subject attribute fidelity (M1), local edit precision (M2), and multi-constraint satisfaction (M3). Overall, the table reinforces the core motivation of CoW-Bench: even when generations look plausible, models frequently weaken, mis-bind, or silently reinterpret the specified conditions, which is exactly the failure mode that world-model interface cannot afford. (1) Identity-and-attribute binding is the hardest modal interface primitive. Across nearly all model groups, Id+Attr remains noticeably lower than other M1 dimensions. Even top closed-source image"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 10: Modal-consistency results on CoW-Bench (02 scale; higher is better), grouped into three metric families: M1 SubjectAttribute Fidelity (IDAT, BKOF, DOMN, CLAR, EXCL), M2 Local Edit Precision (TARG, MNCH, LEAK, CLAR, NEXA), and M3 Multi-constraint Satisfaction (CMPL, ATCO, RLCO, OMIS, NEXA). Refer to Table 9 for abbreviations. BKOF measures whether rare constraints are replaced by common defaults, while NEXA penalizes spurious additions beyond the instruction. Model IDAT BKOF DOMN CLAR EXCL TARG MNCH LEAK CLAR NEXA CMPL ATCO RLCO OMIS NEXA Subj-Attr (SUAT) Local-Edit (LCED) Multi-Const (MCON) Closed-Source Video-Generation Models Sora [2] Kling [279] 0.40 0.33 1.48 1.32 0.91 0.58 1.15 0. 1.22 0.93 1.12 1.23 1.96 1.82 1.82 1.74 1.45 1.49 2.00 1. 1.96 1.36 1.67 1.14 1.32 0.74 1.59 0.86 1.84 1.53 0.96 GPT-image-1 [501] GPT-image-1.5 [504] 1.19 0.94 Seedream-4-0 [502] Seedream-4-5 [502] 1.10 1.17 Nano Banana [503] Nano Banana Pro [503] 1. Allegro [505] Easy Animate [507] CogVideoX [179] Wan2.2-I2V-14B [273] SkyReels-V2 [508] HunyuanVideo [506] LTX-Video [442] BAGEL [8] UniVideo [510] Emu3.5 [511] Qwen-Image [509] 0.10 0.13 0.09 0.11 0.14 0.01 0.15 0.73 0.27 0.81 0.00 1.71 1.76 1.44 1.53 1.59 1.68 0.60 1.23 1.29 1.10 1.00 1.23 1. 1.13 1.09 1.37 0.18 1.69 1.77 1.35 1.52 1.62 1.72 0.24 0.55 0.53 0.40 0.38 0.31 0.49 1.16 0.77 1.25 0.04 Closed-Source Image-Generation Models 1.75 1.68 1.10 1.38 1.59 1. 0.56 0.97 0.86 0.90 0.88 0.60 0.82 1.36 1.20 1.69 0.48 1.40 1.50 1.25 1.43 1.36 1.55 1.98 1.94 1.77 1.84 1.79 1.80 1.91 1.92 1.70 1.78 1.77 1.81 1.73 1.73 1.54 1.54 1.67 1. 1.95 1.90 1.31 1.20 1.89 1.95 Open-Source Video-Generation Models 0.70 0.71 0.70 0.83 0.77 0.30 0.62 1.31 1.76 1.53 1.87 1.80 1.97 1.80 1.28 1.67 1.26 1.77 1.63 1.64 1.48 0.83 1.11 0.77 1.32 1.34 0.98 0. 1.67 1.86 1.64 1.82 1.91 2.00 1.91 Open-Source Image-Generation Models 1.01 0.79 1.41 0.10 1.30 1.90 1.90 0.40 1.14 1.57 1.86 0.29 0.79 1.07 1.70 0. 1.62 1.94 1.89 1.33 1.89 1.89 1.78 1.82 1.38 1.63 1.19 1.51 1.69 1.68 1.20 0.28 1.38 1.53 0.79 1.87 1.84 1.26 1.35 0.89 1.16 1.22 1.22 0.47 0.90 0.98 0.81 0.76 0.76 0. 0.63 0.74 1.03 0.02 1.45 1.51 1.44 1.41 1.04 1.42 0.82 0.95 1.22 1.38 1.05 0.23 0.96 1.01 0.65 1.79 1.66 1.34 1.49 1.28 1.24 0.89 1.21 0.59 0.68 0.65 1.11 0.67 0.09 0. 0.75 0.35 1.68 1.49 1.47 1.66 1.44 1.45 1.09 1.32 0.70 0.87 0.95 1.32 0.93 0.12 0.69 0.85 0.89 1.74 1.60 1.80 1.79 0.78 0.99 1.23 1.40 1.11 1.09 1.31 1.08 1.44 1.69 0. 1.39 1.46 1.53 1.14 models stay far from saturation on Id+Attr (e.g., GPT-image-1.5: 1.19; Nano Banana Pro: 1.25), while many video generators collapse to near-zero (e.g., HunyuanVideo: 0.01). This pattern indicates that the dominant bottleneck is not producing visually consistent output, but keeping the intended entity and its key attributes locked together when the prompt contains multiple constraints. The semantic channel from language to perceptual state still suffers from unstable variable binding. (2) Constraint backoff is widespread and often looks reasonable. The Backoff column reveals systematic tendency to replace unusual or strict constraints with more common defaults. Closed-source image models reduce this behavior (typically 1.61.8), but the effect remains non-trivial; several open-source models show substantially weaker resistance to backoff. This is precisely the failure mode CoW-Bench targets: model can generate realistic image while quietly relaxing the instruction, which similarity-based metric would not penalize. (3) Local editing separates preserve the background from hit the target. For M2, many models score high on Min-change and Leakage, suggesting that they often keep non-target regions stable and avoid global corruption. However, Target can be much lowermost clearly for some open-source video models (e.g., HunyuanVideo: Target=0.30 while Min-change=1.97). This gap indicates common failure mode: models preserve the scene but fail to localize the intended edit, producing changes that are visually mild yet semantically incorrect."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "(4) Multi-constraint fulfillment stresses completeness and role binding, not just more text. M3 exposes different bottleneck. Strong models such as Emu3.5 remain consistently high across Complete/Attr-corr/Rel-corr (1.87/1.79/1.68), while some systems show uneven profiles: for instance, Qwen-Image achieves relatively high Complete and attribute/relationship scores yet performs extremely poorly on M1 identity binding. This mismatch suggests that satisfying multiple listed constraints is not sufficient if the model cannot maintain stable referent for those constraints. Meanwhile, several models exhibit reduced No-extra under M3 (e.g., the Seedream variants), indicating that under compositional pressure they may introduce spurious entitiesan error that is particularly harmful for downstream planning and verification. Takeaway. For the SUAT task, the model often exhibits ambiguity when uniformly aligning constraints across different modalities. It fails to accurately extract corresponding constraints from text and images to guide generation according to requirements. Instead, it blends the extracted constraints from both modalities into single, confused constraint that guides generation, resulting in chaotic outcomes."
        },
        {
            "title": "5.5.2 Temporal Consistency Results",
            "content": "Table 11 reports temporal-consistency performance over three metric families: T1 Worldline Persistence, T2 Rule-guided Slow Evolution, and T3 Ordered Stage Transitions. Two consistent patterns emerge that align with CoW-Benchs central thesis: temporal plausibility is not equivalent to temporal constraint satisfaction, and the hardest failures arise when models must enforce structured dynamics rather than merely maintain visual continuity. Worldline persistence is comparatively strong, even for many video generators. Most closed-source video models score near the upper range on T1 (e.g., Sora: high Env-stab and Visual), and several open-source video models also achieve solid T1 profiles (e.g., SkyReels-V2 and Wan2.2-I2V). This suggests that maintaining stable scene layout and avoiding frame-wise reinitialization is becoming largely solved capability for high-capacity generators. In contrast, T2 exposes sharp The main bottleneck is rule-following evolution, not continuity. drop on Trend, Time-scale, and Inter-state for many video models (often below 0.6), even when Subj-lock is high. This gap indicates common failure mode: models keep the same subject and background, yet fail to realize monotonic, correctly paced process with identifiable intermediate states. Notably, strong closed-source image models show markedly higher T2 scores (e.g., GPT-image-1.5 maintains high values across Trend/Time-scale/Inter-state), suggesting that stronger instructionfollowing priors help when the temporal constraint is expressed semantically and must be respected throughout the sequence. Takeaway. We found that in the STOR task, image generation models generally grasp the overall progression over time. However, transitions between different states exhibit distinct discontinuities rather than smooth evolution, with instances of reversed sequences occurring between states, making it difficult to maintain consistent processes and content. Conversely, in the SLEV task, image generation models demonstrate higher degree of understanding and adherence to world rules. Video generation models, on the other hand, exhibit probabilistic compliance with rules, yielding divergent outcomes for identical or similar scenario tasks. Stage-ordering remains fragile under explicit multi-step structure. For T3, the weakest columns concentrate on Order and Identif., especially for open-source video models (often near 0.3 or lower). Even when Worldline at the end of T3 stays high, low Order/Identif. implies that the sequence"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 11: Temporal-consistency results on CoW-Bench (02 scale; higher is better). We report three metric families: T1 Worldline Persistence (SBJC, ATST, ENST, VISU, EVCT), T2 Rule-guided Slow Evolution (SBJL, TREN, TSCL, INTS, RULE), and T3 Ordered Stage Transitions (ORDR, IDEN, TIME, PROC, WLIN). Refer to Table 9 for abbreviations. INTS evaluates the visibility of plausible intermediate states, while TSCL measures whether the evolution speed matches the prompt-specified process. Model Worldline (WLIN) Slow-Evol (SLEV) Stage-Order (STOR) SBJC ATST ENST VISU EVCT SBJL TREN TSCL INTS RULE ORDR IDEN TIME PROC WLIN Closed-Source Video-Generation Models Sora [2] Kling [279] 1.87 1.85 1.84 1.82 1.94 1.93 1.91 1. 1.76 1.64 1.95 1.98 0.96 0.80 0.99 0.73 0.91 0.70 0.99 0. 0.93 0.94 0.91 0.81 1.12 0.76 1.42 1.13 1.84 1.89 1.75 GPT-image-1 [501] 1.77 GPT-image-1.5 [504] 1.70 Seedream-4-0 [502] Seedream-4-5 [502] 1.71 Nano Banana [503] 1.71 Nano Banana Pro [503] 1. Allegro [505] Easy Animate [507] CogVideoX [179] Wan2.2-I2V-14B [273] SkyReels-V2 [508] HunyuanVideo [506] LTX-Video [442] BAGEL [8] UniVideo [510] Emu3.5 [511] Qwen-Image [509] 1.38 1.69 1.63 1.76 1.79 1.87 1.76 1.31 1.82 1.66 0.22 1.84 1.86 1.66 1.76 1.72 1.73 1.37 1.69 1.56 1.70 1.74 1.77 1. 1.13 1.74 1.72 0.22 1.93 1.91 1.85 1.89 1.84 1.83 1.57 1.90 1.90 1.86 1.84 1.86 1.85 1.57 1.90 1.91 0.58 1.91 1.93 1.84 1.78 1.84 1.89 1.45 1.82 1.67 1.78 1.90 1.86 1. 1.20 1.79 1.85 0.70 Closed-Source Image-Generation Models 1.71 1.76 1.59 1.63 1.76 1.69 1.98 1.98 1.75 1.85 1.88 1.90 1.49 1.66 1.14 1.49 1.54 1.65 1.43 1.69 1.14 1.40 1.51 1. 1.49 1.70 1.05 1.40 1.53 1.64 1.38 1.62 1.17 1.32 1.41 1.54 0.79 1.26 1.14 1.29 1.20 1.40 Open-Source Video-Generation Models 1.26 1.60 1.53 1.44 1.62 1.26 1.49 0.90 1.98 1.85 1.88 1.86 2.00 1. 0.27 0.60 0.59 0.56 0.57 0.22 0.53 0.23 0.54 0.49 0.43 0.48 0.21 0.49 0.19 0.49 0.46 0.38 0.48 0.11 0.40 0.32 0.72 0.74 0.75 0.64 0.52 0.60 0.64 0.50 0.69 0.71 0.61 0.50 0.52 Open-Source Image-Generation Models 1.06 1.33 1.63 0.38 1.49 1.99 1.95 0.28 1.01 0.48 0.84 0.07 0.81 0.41 0.74 0.04 0.91 0.32 0.78 0.00 0.79 0.67 1.00 0. 0.69 0.31 0.54 0.26 0.86 1.18 1.13 1.20 1.23 1.37 0.29 0.33 0.30 0.42 0.35 0.08 0.18 0.59 0.19 0.59 0.08 1.06 1.35 1.05 1.12 1.12 1.41 0.36 0.26 0.27 0.26 0.26 0.06 0. 0.67 0.17 0.73 0.02 1.19 1.45 1.24 1.32 1.35 1.48 0.74 0.70 0.63 0.65 0.71 0.48 0.55 0.96 0.60 1.13 0.44 1.79 1.90 1.71 1.83 1.80 1.83 1.30 1.80 1.83 1.76 1.81 1.82 1. 1.05 1.81 1.73 0.84 may remain in one world but fails to realize the intended discrete stage structure reliably. This finding motivates CoW-Benchs decomposition: model can be temporally stable while still violating high-level temporal logic."
        },
        {
            "title": "5.5.3 Spatial Consistency Results",
            "content": "Table 12 reports spatial-consistency across S1 Sem-Planar, S2 Occlusion/Containment, and S3 Multiview 3D coherence. The results echo CoW-Benchs central view: spatial world modeling is not only about producing plausible geometry in single frame, but about maintaining structural constraints that remain verifiable under interactions such as occlusion, containment, and viewpoint change. (1) Planar layout is the entry-level test, yet directional grounding remains fragile. Most models score relatively high on Layout (often 1.8), indicating that producing globally coherent 2D composition is increasingly reliable. In contrast, Dir is consistently the lowest sub-metric across model families (e.g., Sora: 0.64; several open-source video models 0.5; Qwen-Image: 0.02). This gap suggests that models can maintain visually stable arrangement while still failing to execute explicit directional constraints (left/right/inside/outside) with high fidelity. For world model, directional grounding is"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Table 12: Spatial-consistency results on CoW-Bench (02 scale; higher is better). We report three metric families: S1 Sem-Planar (DIRC, COUNT, RULE, BNDY, LAYT), S2 Occlusion/Containment (OCCL, BNDY, VISB, RSTB, LAYR), and S3 Multi-view 3D coherence (STRC, SURF, PSCL, OUPD, GEOS). VISB evaluates whether visible regions agree with the implied occlusion relation, while OUPD measures whether occlusion boundaries update plausibly under viewpoint change. Model Sem-Planar (SEPL) Occ-Cont (OCCO) MV-3D DIRC COUNT RULE BNDY LAYT OCCL BNDY VISB RSTB LAYR STRC SURF PSCL OUPD GEOS Closed-Source Video-Generation Models Sora [2] Kling [279] 0.64 1.10 1.22 1.52 1.00 1.53 1.47 1. 1.88 1.91 1.51 1.67 1.49 1.62 1.74 1.70 1.76 1.78 1.91 1. 1.77 1.93 1.72 1.85 1.56 1.80 1.67 1.86 1.79 1.88 0.81 GPT-image-1 [501] 1.36 GPT-image-1.5 [504] Seedream-4-0 [502] 0.96 1.13 Seedream-4-5 [502] Nano Banana [503] 1.23 Nano Banana Pro [503] 1. Allegro [505] Easy Animate [507] CogVideoX [179] Wan2.2-I2V-14B [273] SkyReels-V2 [508] HunyuanVideo [506] LTX-Video [442] BAGEL [8] UniVideo [510] Emu3.5 [511] Qwen-Image [509] 0.69 0.44 0.48 0.29 0.37 0.15 0.64 0.56 0.36 1.41 0.02 1.58 1.70 1.31 1.40 1.59 1.56 1.14 1.41 1.10 1.41 1.27 1.34 1. 1.52 1.38 1.63 0.15 1.45 1.62 1.28 1.37 1.41 1.66 0.91 1.12 0.95 1.03 0.98 0.92 1.14 1.10 1.10 1.81 0.08 1.66 1.87 1.51 1.66 1.71 1.79 1.26 1.53 1.18 1.48 1.38 1.38 1. 1.47 1.42 1.81 0.15 Closed-Source Image-Generation Models 1.92 1.54 1.98 1.59 1.79 1.07 1.66 1.22 1.89 1.35 1.93 1.52 1.37 1.46 0.91 1.00 1.36 1.46 1.55 1.57 1.05 1.06 1.43 1.58 1.75 1.83 1.30 1.31 1.69 1. 1.89 1.87 1.77 1.69 1.82 1.84 Open-Source Video-Generation Models 1.75 1.86 1.84 1.89 1.80 1.88 1.74 1.29 1.34 1.30 1.44 1.40 1.15 1.30 1.18 1.26 1.16 1.16 1.43 0.81 1.08 1.26 1.37 1.13 1.34 1.49 1.18 1. 1.39 1.47 1.26 1.54 1.66 1.44 1.37 1.70 1.91 1.76 1.89 1.94 1.94 1.79 Open-Source Image-Generation Models 1.78 1.89 1.96 1.30 1.17 1.29 1.78 0.24 0.98 1.18 1.64 0. 0.85 1.34 1.67 0.15 0.90 1.28 1.81 0.21 1.43 1.75 1.91 1.12 1.91 1.92 1.90 1.87 1.96 1.94 1.58 1.68 1.42 1.58 1.67 1.75 1.57 1.68 1.74 1.82 0. 1.85 1.89 1.78 1.78 1.90 1.94 1.65 1.52 1.23 1.25 1.67 1.15 1.33 1.58 1.38 1.78 0.26 1.77 1.81 1.59 1.77 1.75 1.92 1.48 1.54 1.08 1.09 1.46 0.89 1.21 1.42 1.12 1.72 0. 1.86 1.85 1.77 1.82 1.87 1.90 1.59 1.56 1.01 1.20 1.62 0.98 1.13 1.49 1.23 1.72 0.25 1.88 1.85 1.78 1.82 1.87 1.91 1.59 1.51 0.96 1.15 1.51 1.27 1.25 1.51 1.34 1.73 0. core interface requirement because it turns language into testable spatial relations. (2) Occlusion/containment is largely learned, but visible-part evidence is the weak link. Closedsource models are strong on Layer and Rel-stab (typically 1.81.95), implying that they often preserve consistent depth ordering without obvious contradictions. However, Visible shows wider spread, especially for open-source image models (e.g., BAGEL: 0.85; Qwen-Image: 0.15). This pattern indicates that models may capture coarse layering intent while failing on the operational evidencewhether the actually visible portions match the implied occlusion boundary. This is exactly the kind of looks plausible but violates checkable constraint failure that CoW-Bench is designed to reveal. (3) Multi-view 3D coherence separates geometry plausibility from world-state invariance. Top closed-source image models achieve near-ceiling scores on Struct and Persp/Scale (e.g., Nano Banana Pro: 1.94/1.92; GPT-image-1.5: 1.92/1.81), indicating strong single-object 3D plausibility. Yet several open-source video models drop substantially on Occ-update and Geo-self (e.g., CogVideoX: 1.01/0.96), suggesting that they struggle to update occlusions and maintain self-consistent 3D explanation across views. This supports key world-model implication: generating plausible view is easier than maintaining persistent 3D scene hypothesis that survives viewpoint change."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Takeaway. In MV-3D, image generation models tend to mistakenly apply mirror symmetry to scenes to simulate different viewpoints rather than accurately capturing distinct perspectives within the same scene. Additionally, when switching viewpoints, the detailed attributes of the same object across different angles often fail to remain consistent. When tackling OccCont tasks, models frequently generate unrealistic changes that defy common sense, such as objects penetrating one another, merging into each other, or exhibiting unnatural movements to maintain hierarchical relationships."
        },
        {
            "title": "5.6.1 Modal–Space Consistency Results: Semantic-to-Geometry Binding",
            "content": "Figure 35 visualizes ModalSpace consistency, where models must map language constraints (entities, attributes, relations) into executable spatial roles and keep them verifiable under layout and viewpoint variation. The figure supports CoW-Benchs central goal: distinguishing looks plausible from constraint-faithful semantic grounding in space. Figure 35: ModalSpace consistency on CoW-Bench, shown as heatmap over sub-metrics (rows) and models (columns), with scores on the 02 scale (higher is better). Rows group into Sem-Planar (Ent-match, Act-align, NT-stab, Attr-bind, Global), Sem-Hier (Pos-rel, Neg-rel, Excl., Vis+Layer, Id-stab), and Sem-MV (Anchor, View-stab, Lateral, Scene, Marker). Semantic role binding is the dominant bottleneck. Across many models, the most consistent performance drops appear on Act-align and Pos-rel. This indicates frequent failures to (i) bind an instructed action/relation to the correct entity and (ii) realize constructive positive spatial relation precisely. Importantly, these failures can coexist with strong scores on geometry-leaning cues (e.g., Id-stab, Scene), producing characteristic plausible-but-misbound outcome: the scene is coherent, yet the constraint is attached to the wrong object or only weakly reflected. Avoiding violations is often easier than constructing exact relations. For broad set of mid-tier models, Neg-rel and Excl. are noticeably stronger than Pos-rel. This asymmetry suggests that models more reliably avoid forbidden configurations than they enforce an exact required placement. For world-model use, this gap matters because planning and verification rely on constructive satisfaction (placing the right entity in the right role), not only on the absence of obvious violations."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Multi-view semantic stability is strong for top models but still reveals tail-risk failures. The Sem-MV block (Anchor, View-stab, Lateral, Scene, Marker) is generally high for leading closed-source image models and competitive systems, indicating that stable reference frames and identity markers under viewpoint change are increasingly attainable. However, the heatmap also shows that weaker models can fail catastrophically on these anchors, which makes multi-view stability sensitive probe of whether model maintains an invariant scene state rather than redrawing new world per view. Takeaway. When handling 3D perspective issues, the model also tends to generate mirrorsymmetrical results. Additionally, during perspective transitions, non-primary content may retain its original viewpoint, leading to partial object perspective shifts."
        },
        {
            "title": "5.6.2 Modal–Time Consistency Results: Executing a Temporal Program",
            "content": "Figure 36 visualizes ModalTime consistency, where models must (i) keep language-specified anchors stable over long horizons, (ii) execute attribute dynamics specified by the prompt, and (iii) respond to discrete trigger events without breaking the worldline. The heatmap supports CoW-Benchs central goal: separating visually plausible temporal outputs from constraint-faithful temporal execution. Figure 36: ModalTime consistency on CoW-Bench, shown as heatmap over sub-metrics (rows) and models (columns), with scores on the 02 scale (higher is better). Rows cover Long-Horizon anchoring (Init-anchor, Long-stab, Cross-scene, Attr-bind, No-unexp), Attr-Dyn alignment (Target(E,A), Follow, Smooth, Rate, Env-stab), and Trigger-Event compliance (Pre-hold, Trigger, Post-comp, State-stab, Env-stab). Anchoring is generally strong; the main variance concentrates in the weakest systems. The LongHorizon block is consistently high for leading closed-source image models and remains competitive for many video generators, indicating that persistent identity/attribute anchoring is often attainable once the anchor is observable. The most salient failures appear as isolated low-score columns (e.g., very low Init-anchor/Long-stab in the weakest model), which then correlate with downstream temporalcontrol breakdown. Dynamics attribute is the primary bottleneck, dominated by instruction-following and rate control. Within Attr-Dyn, Env-stab stays near the upper range for most models, while Follow and Rate remain substantially lowerespecially for video generators. This pattern indicates common failure mode:"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "models keep the scene stable but do not execute the instructed evolution reliably (direction/schedule/pacing), yielding sequences that look smooth yet violate semantic temporal commitments. Triggered events expose timing and post-event persistence failures. In Trigger-Event, Pre-hold is often relatively strong, but Trigger and Post-comp degrade noticeably for many video models, revealing two coupled issues: the event is not made salient at the correct time, and the post-trigger state does not persist. These errors are particularly damaging for planning-style use, where discrete events serve as causal checkpoints. Takeaway. video generation models are more prone to introducing state conditions not specified in text constraints, leading to chaotic variations. In contrast, image generation models demonstrate higher compliance with textual instruction constraints."
        },
        {
            "title": "5.6.3 Time-Space Consistency Results: Navigation Exposes the Missing World State",
            "content": "Time-Space consistency evaluates whether model maintains an invariant spatial structure while executing temporally extended motion. Figure 37 visualizes performance across ST1ST3 and highlights central message consistent with CoW-Benchs motivation: models can achieve strong local motion plausibility and even stable environments, yet fail when the task requires persistent, goal-directed world state. Figure 37: Time-Space consistency on CoW-Bench, shown as heatmap over sub-metrics (rows) and models (columns), with scores on the 02 scale (higher is better). Rows cover three metric families: ST1 Maze-2D (Start/Goal, Traj-cont, Legal, Correct, Struct-stab), ST2 Occlusion Dynamics under Motion (Occ-move, Parallax, Rigid, Natural, Env-stab), and ST3 3D Loop Navigation (Struct, Rel, View-smooth, Physical, Entity-stab). (1) Maze-2D remains the sharpest discriminator. In ST1, many video generators score non-trivially on Legal and sometimes on Struct-stab, but still fall to near zero on Start/Goal and Correct (e.g., Sora and Kling show low Correct despite moderate Legal). This pattern indicates that the core failure is not producing plausible maze-like motion, but maintaining single, identifiable trajectory that starts from the correct anchor and reaches the correct goal without implicit resets or shortcutting. By contrast, stronger image-centric models obtain substantially higher ST1 correctness, suggesting that explicit goal-conditioned state tracking is still the limiting factor for video-style generation."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "(2) Occlusion-under-motion is comparatively mature, with remaining errors concentrated on depthlayer updates. For ST2, many models achieve high Rigid, Natural, and Env-stab, which suggests that layered motion and global temporal stability are increasingly handled well. The remaining spread concentrates on Occ-move and Parallax, implying that the hardest cases involve consistent depth ordering and visibility updates under motion rather than overall smoothness. (3) 3D loop navigation stresses viewpoint continuity and relational stability. In ST3, leading models maintain strong Struct and Entity-stab, but weaker systems drop on View-smooth and Rel. This is consistent with viewpoint reset failure mode: geometry can look plausible frame-by-frame, yet the sequence cannot be explained as single 3D scene traversed along continuous camera path. CoW-Bench therefore treats loop navigation as probe of whether models preserve state under transformation, beyond single-view realism. Takeaway. video generation models often yield results with greater freedom and more pronounced sense of temporal progression, whereas image generation models tend to depict changes over time in more disjointed manner, with less scope for creative expression."
        },
        {
            "title": "5.7 Sample Analysis",
            "content": "To deeply analyze the models inference mechanism under multi-dimensional constraints, we have constructed new set of evaluation criteria in CoW-Bench based on the six core consistency challenges defined previously. Moving beyond traditional evaluations that focus solely on visual quality, this benchmark utilizes frame-by-frame Physical State Ground Truth to precisely quantify the fundamental differences between Generator and World Simulator from the perspectives of dataset construction and model capability boundaries."
        },
        {
            "title": "5.7.1 Single Consistency Tasks",
            "content": "The design philosophy of single consistency tasks is to isolate complex interference and use the purity of simulated data to establish the baseline for models foundational reasoning. The specific effects of each sub-task are illustrated in Figures 3840. Modal Consistency Tasks. This task examines whether model can clearly distinguish between constraints from different modalities and avoid information blending. In Subject Attribute Fidelity, models demonstrate strong feature decoupling capabilities, successfully extracting the texture and material of butterfly from reference image and mapping it onto the geometric structure of fish. The resulting creature possesses distinct scale and luster features without incorporating the butterflys wing morphology. For finer-grained Local Edit Precision, models exhibit precise pixel control in clockediting task, modifying only the hour and minute hands according to instructions while the background wall and clock frame remain strictly locked. Furthermore, under the complex instructions of Multiconstraint Satisfaction, models accurately capture the clothing, actions, and positional attributes of multiple characters without incorrect role assignment or attribute leakage, proving their precision in parsing long-text constraints. Spatial Consistency Tasks. This task evaluates whether the scenes established by the model are geometrically self-consistent rather than merely appearing plausible in 2D images. In Sem-Planar, models correctly understand relative positions in non-occluded scenarios, with two cats moving to the left and right respectively without confusing the directional semantics. For more complex Occlusion/Containment, as drawer slowly closes, the model correctly renders the process of internal books gradually moving into darkness; the books follow physical laws of occlusion rather than disappearing abruptly, reflecting an understanding of the container concept. In tests of MV-3D, as"
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 38: Example diagrams of Modal sub-tasks. the viewpoint slowly shifts, the desk lamp naturally disappears at the edge of the field of view while the bed layout gradually reveals itself. Throughout this process, the relative positions of objects in the room remain unchanged and the lighting environment stays stable. Temporal Consistency Tasks. We elevate temporal consistency from visual smoothness to ruleIn Worldline governed evolution, examining whether models follow the worlds implicit laws. Persistence, an electric fan maintains the physical integrity of its blades during long-term rotation, with no blade breakage or sudden material mutations. Rule-guided Slow Evolution further demonstrates the models understanding of physical entropy: in simulated one-hour duration, candle gradually shortens according to combustion laws rather than staying the same in violation of common sense. In the house-collapse task of Ordered Stage Transitions, the model clearly displays continuous state from structural integrity to ruins. The collapse sequence follows gravitational logic, and the ruins remain static after falling, avoiding non-causal jittering such as collapsing and then recovering."
        },
        {
            "title": "5.7.2 Compound Consistency Tasks",
            "content": "Compound consistency tasks simulate the complexity of the real world, examining the models tradeoff and reasoning capabilities when multi-dimensional constraints restrict (or even conflict with) each other. The specific effects of each sub-task are illustrated in Figures 4143. Modal-Spatial Consistency Tasks evaluate the models ability to transform semantic information into executable spatial constraints, achieving semanticspatial coupling. The model must not only understand what an object is but also precisely execute geometric instructions regarding where the object is, ensuring accurate grounding of semantic referents in the spatial dimension. As shown in Figure 41, in Sem-Planar, models successfully identify specific vehicle with blue roof within complex traffic flow and control only that vehicle to move right, achieving precise semantic-spatial binding. In the Sem-Hier task, models accurately generate scene where apples are in the bowl and pears are outside, strictly adhering to the spatial semantics of containment and exclusion. However, Sem-MV exposes current weakness: during viewpoint transitions, while the perspective change of signpost remains reasonable, the occlusion relationship of books and pens relative to the signpost undergoes erroneous drift (moving from behind to the side), indicating that the models ability to maintain micro-spatial semantics under dynamic viewpoints still requires improvement. Modal-Temporal Consistency Tasks evaluate the models fidelity to instructions during long-sequence generation. The core is to examine whether the model treats the prompt as high-priority temporal constitution, implementing semantic elements and logical constraints throughout the entire video to resist semantic drift and forgetting over time. As shown in Figure 42, in Long Horizon tasks, the body color, patterns, and relative positions of vehicle remain highly stable during long-distance movement, with no blurring or texture alterations. Attribute Dynamic further tests temporal programming capabilities, where models successfully control spheres color to change in complex sequence of red orange green blue red with clear steps and no color bleeding. In Trigger Event, models demonstrate acute capture of causal logic: phone screen stays black before button is pressed and lights up instantly only after the action is triggered, aligning exactly to the events trigger point. Spatial-Temporal Consistency Tasks evaluate whether models possess the prototype of built-in physical engine under weak modality constraints. It focuses on whether the model can maintain the self-consistency of spatial topology and motion parallax during dynamic evolution, rather than relying solely on pixel-level smooth interpolation. As shown in Figure 43, in Maze-2D, although models maintain the static structure of the maze walls, the subject ultimately fails to correctly plan path to the goal, suggesting limitations in spatial reasoning. In contrast, Occlusion Dynamics under Motion perfectly reproduces motion parallax: near trees move at high speed to create motion blur, while the background moves slowly and the vehicle remains relatively stationary, achieving dual spatio-temporal self-consistency. Finally, 3D Loop Navigation achieves closed-loop roaming from bedroom to city and back, with smooth structural continuity and no spatial collapse, demonstrating potential reference frame stability over long-term roaming. To comprehensively verify these mechanisms, we used CoW-Bench to test mainstream world models including Sora [2], Kling [279], GPT-Image-1.5 [504], Seedream-4-5 [502], Nano Banana Pro [503], Wan2.2-I2V-14B [273] , SkyReels-V2 [508], HunyuanVideo [506], BAGEL [8], and Emu3.5 [511]. We display selected results: single consistency comparisons are shown in Figure 44 (Modal), Figure 45 (Spatial), and Figure 46 (Temporal); compound consistency results are shown in Figure 47 (ModalSpatial), Figure 48 (Modal-Temporal), and Figure 49 (Spatial-Temporal). Takeaway. CoW-Bench establishes new standard for evaluating world models. While video models outperform image models in instruction following, they fundamentally lack physical evolutionary logic. Heavily relying on pixel-based interpolation rather than genuine reasoning, these models exhibit non-physical distortions and topological collapse."
        },
        {
            "title": "6 Conclusion",
            "content": "This survey has re-examined the trajectory of generative AI through the lens of the Trinity of Consistency, establishing general framework for what constitutes World Model. By deconstructing the capability space into Modality, Spatial, and Temporal dimensions, we argue that true physical understanding does not emerge from single-axis performance but from the robustness of cross-dimensional interactions. Our analysis highlights that the most critical failures in current systems are not visual artifacts, but ruptures in consistency: the inability to bind semantic instructions to geometric roles (ModalSpace), the failure to maintain identity under long-horizon evolution (Modal-Time), and the loss of environmental permanence during navigation (Time-Space). Figure 50: Evolutionary spectrum of World Model paradigms based on interactive action spaces. This figure illustrates the fields trajectory from the early Vector-as-Action paradigm (left: e.g., JEPA, relying on uninterpretable latent space predictions), through the intermediate Key-as-Action paradigm (center: e.g., Genie series, constrained by predefined discrete control spaces), and ultimately advancing towards the Prompt-as-Action paradigm. In the latter, semantic compiler translates natural language intents into universal spatiotemporal dynamic simulations. To rigorously diagnose these cross-dimensional ruptures, we introduced CoW-Bench, comprehensive benchmark that unifies the evaluation of mainstream video generation models and UMMs under shared protocol. CoW-Bench employs carefully designed multi-frame evaluation protocol derived from human expert reasoning. By analyzing temporally sampled grids against fine-grained atomic checklists, we operationalize consistency as strict constraint-satisfaction problem. This rigorous approach exposes pervasive constraint-backoff phenomenon, where models generate plausible-looking textures while silently violating logical commitments, thus providing the necessary diagnostic resolution to distinguish between visual mimicry and genuine physical simulation. Crucially, our findings indicate that constraint backoff is not merely consequence of insufficient training data or scale, but structural artifact of how current models represent interaction. When the action space is either uninterpretable or rigidly predefined, models lack the expressive capacity to ground semantic commitments in physical dynamics. Under such constraints, consistency violations become not accidental errors but almost inevitable outcomes. Addressing this limitation therefore demands paradigm shift in how interaction itself is formalized within world models. To systematically characterize this transition, we organize the evolution of world model paradigms according to the expressiveness of their interactive action spaces (Figure 50). As illustrated on the left side of the figure, early explorations such as JEPA [1] operate at the Vector-as-Action level. While enabling latent-space prediction, their interaction mechanisms remain opaque and lack semantic interpretability. The middle section presents the Key-as-Action paradigm, exemplified by the Genie series [438, 514, 515]. Although introducing limited interactivity, these models remain confined to narrow, discrete, and predefined action spaces."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "The right side of the figure illustrates forward-looking paradigm: Prompt-as-Action paradigm in which UMMs with modality consistency and video generation models with spatialtemporal consistency are unified. Equipped with an internal semantic compiler, such models can interpret highdimensional natural-language prompts and translate them into universal spatiotemporal simulations that adhere to the Trinity of Consistency. Recent systems such as PixVerse-R1 [516] offer an early glimpse of this direction, demonstrating real-time world modeling that responds instantly to user input and unifies multiple modalities within an autoregressive architecture. By moving beyond predefined action abstractions, this paradigm begins to bridge the gap between human semantic intent and the underlying dynamics of the physical world. The central conviction of this survey is therefore simple yet uncompromising: consistency is not an optional attribute of world modelit is its criterion of existence. system that produces visually compelling pixels but fails to maintain cross-dimensional consistency, regardless of scale, remains fundamentally texture synthesizer rather than simulator of the world. The Trinity of Consistency thus delineates more than an analytical framework; it marks boundarya paradigmatic divide between generating images that resemble the world and constructing models that understand it."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 39: Example diagrams of Spatial sub-tasks."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 40: Example diagrams of Temporal sub-tasks."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 41: Example diagrams of modal and spatial sub-tasks."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 42: Example diagrams of modal and temporal sub-tasks."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 43: Example diagrams of spatial and temporal sub-tasks."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 44: Comparison with different models for modal consistency task."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 45: Comparison with different models for spatial consistency task."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 46: Comparison with different models for temporal consistency task."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 47: Comparison with different models for modal and spatial consistency task."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 48: Comparison with different models for modal and temporal consistency task."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "Figure 49: Comparison with different models for spatial and temporal consistency task."
        },
        {
            "title": "Leading Authors",
            "content": "Jingxuan Wei2, Siyuan Li3, Cheng Tan"
        },
        {
            "title": "Core Contributors",
            "content": "Yuhang Xu2, Zheng Sun2, Junjie Jiang2, Hexuan Jin2, Caijun Jia2, Honghao He2, Xinglong Xu2, Xi Bai"
        },
        {
            "title": "Other Contributors",
            "content": "Chang Yu3, Yumou Liu5, Junnan Zhu2, Xuanhe Zhou5, Jintao Chen6, Xiaobin Hu4, Shancheng Pang7, Bihui Yu2, Ran He2, Zhen Lei2, Stan Z. Li3,"
        },
        {
            "title": "Corresponding Authors",
            "content": "Conghui He1, Shuicheng Yan4, Cheng Tan"
        },
        {
            "title": "Affiliation",
            "content": "1Shanghai Artificial Intelligence Laboratory 2University of Chinese Academy of Sciences 3Westlake University 4National University of Singapore 5Shanghai Jiaotong University 6Zhejiang University 7China University of Petroleum (East China)"
        },
        {
            "title": "References",
            "content": "[1] Y. LeCun, path towards autonomous machine intelligence, OpenReview, 2022. [2] T. Brooks, B. Peebles, C. Holmes et al., Video generation models as world simulators, OpenAI Blog, vol. 1, no. 8, p. 1, 2024. [3] A. Bardes, Q. Garrido, J. Ponce et al., Revisiting feature prediction for learning visual representations from video, TMLR, 2025. [4] H. Lu, W. Liu, B. Zhang et al., Deepseek-vl: towards real-world vision-language understanding, arxiv, 2024. [5] R. Team, Z. Gao, Q. Wang et al., Advancing open-source world models, arxiv, 2026. [6] Runway Research, Gen-3 alpha: new frontier for video generation, https://runwayml.com/research/ introducing-gen-3-alpha, 2024, runway Technical Report. [7] G. Team, R. Anil, S. Borgeaud et al., Gemini: family of highly capable multimodal models, arxiv, 2023. [8] C. Deng, D. Zhu, K. Li et al., Emerging properties in unified multimodal pretraining, arxiv, 2025. [9] M. Huh, B. Cheung, T. Wang et al., The platonic representation hypothesis, arxiv, 2024. [10] A. Radford, J. W. Kim, C. Hallacy et al., Learning transferable visual models from natural language supervision, in ICML. PMLR, 2021, pp. 87488763. [11] C. Jia, Y. Yang, Y. Xia et al., Scaling up visual and vision-language representation learning with noisy text supervision, in ICML, 2021. [12] X. Zhai, B. Mustafa, A. Kolesnikov et al., Sigmoid loss for language image pre-training, in ICCV, 2023. [13] H. Xu, S. Xie, X. E. Tan et al., Demystifying clip data, arxiv, 2023. [14] J.-B. Alayrac, J. Donahue, P. Luc et al., Flamingo: visual language model for few-shot learning, NeurIPS, 2022. [15] J. Li, D. Li, C. Xiong et al., Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in ICML. PMLR, 2022, pp. 12 88812 900. [16] J. Li, D. Li, S. Savarese et al., Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in ICML, 2023, pp. 19 73019 742. [17] J. Bai, S. Bai, S. Yang et al., Qwen-vl: frontier large vision-language model with versatile abilities, arxiv, 2023. [18] F. Li, R. Zhang, H. Zhang et al., Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024. [19] C. Team, Chameleon: Mixed-modal early-fusion foundation models, arxiv, 2024. [20] J. Lu, C. Clark, S. Lee et al., Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action, in CVPR, 2024, pp. 26 43926 455. [21] J. Xie, W. Mao, Z. Bai et al., Show-o: One single transformer to unify multimodal understanding and generation, arxiv, 2024. [22] L. Yu, B. Shi, R. Pasunuru et al., Scaling autoregressive multi-modal models: Pretraining and instruction tuning (2023), arxiv, 2023. [23] H. Liu, C. Li, Q. Wu et al., Visual instruction tuning, in NeurIPS, 2023. [24] W. Wang, Q. Lv, W. Yu et al., Cogvlm: Visual expert for pretrained language models, NeurIPS, 2024. [25] Z. Chen, J. Wu, W. Wang et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in CVPR, 2024."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[26] D. Zhu, J. Chen, X. Shen et al., Minigpt-4: Enhancing vision-language understanding with advanced large language models, arxiv, 2023. [27] P. Esser, S. Kulal, A. Blattmann et al., Scaling rectified flow transformers for high-resolution image synthesis, in ICML, 2024. [28] X. Wang, X. Zhang, Z. Luo et al., Emu3: Next-token prediction is all you need, arxiv, 2024. [29] J. Chen, J. Yu, C. Ge et al., Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis, ICLR, 2024. [30] P. Gao, L. Zhuo, D. Liu et al., Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers, arxiv, 2024. [31] X. Liu, C. Gong et al., Flow straight and fast: Learning to generate and transfer data with rectified flow, in ICLR, 2023. [32] X. Liu, X. Zhang, J. Ma et al., Instaflow: One step is enough for high-quality diffusion-based text-to-image generation, in ICLR, 2023. [33] Black Forest Labs, Flux.1: Unleashing the power of flow matching, https://blackforestlabs.ai, 2024. [34] N. Ma, M. Goldstein, M. S. Albergo et al., Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers, in ECCV, 2024. [35] S. Tong, D. Fan, J. Li et al., Metamorph: Multimodal understanding and generation via instruction tuning, in ICCV, 2025, pp. 17 00117 012. [36] W. Jin, Y. Niu, J. Liao et al., Srum: Fine-grained self-rewarding for unified multimodal models, arxiv, 2025. [37] J. Xu, X. Liu, Y. Wu et al., Imagereward: Learning and evaluating human preferences for text-to-image generation, in NeurIPS, 2023. [38] Z. Lin, D. Pathak, B. Li et al., Evaluating text-to-visual generation with image-to-text generation, in ECCV. Springer, 2024, pp. 366384. [39] Z. Liang, Y. Yuan, S. Gu et al., Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization, in CVPR, 2025. [40] W. Wang, Z. Gao, L. Chen et al., Visualprm: An effective process reward model for multimodal reasoning, arxiv, 2025. [41] Y. Cai, K. Li, M. Jia et al., Phygdpo: Physics-aware groupwise direct preference optimization for physically consistent text-to-video generation, arxiv, 2026. [42] S. Yuan, Y. Liu, Y. Yue et al., Ar-grpo: Training autoregressive image generation models via reinforcement learning, arxiv, 2025. [43] T. Wang and P. Isola, Understanding contrastive representation learning through alignment and uniformity on the hypersphere, in ICML, 2020, pp. 99299939. [44] V. W. Liang, Y. Zhang, Y. Kwon et al., Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning, NeurIPS, vol. 35, pp. 17 61217 625, 2022. [45] C. Snell, J. Lee, K. Xu et al., Scaling llm test-time compute optimally can be more effective than scaling model parameters, arxiv, 2024. [46] S. J. Gershman and N. D. Goodman, Amortized inference in probabilistic reasoning, in ACCSS, 2014. [47] S. C. Lowe, System 2 reasoning capabilities are nigh, arxiv, 2024. [48] S. Yao, D. Yu, J. Zhao et al., Tree of thoughts: Deliberate problem solving with large language models, in NeurIPS, vol. 36, 2023, pp. 11 80911 822. [49] A. Van Den Oord, O. Vinyals et al., Neural discrete representation learning, NeurIPS, 2017."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[50] A. Ramesh, M. Pavlov, G. Goh et al., Zero-shot text-to-image generation, in ICML. PMLR, 2021, pp. 88218831. [51] M. Huh, B. Cheung, P. Agrawal et al., Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks, in ICML, 2023. [52] T. Xiong, J. H. Liew, Z. Huang et al., Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation, in ICCV, 2025. [53] S. Li, L. Zhang, Z. Wang et al., Mergevq: unified framework for visual generation and representation with disentangled token merging and quantization, in CVPR, 2025. [54] S. Bengio, O. Vinyals, N. Jaitly et al., Scheduled sampling for sequence prediction with recurrent neural networks, in NeurIPS, 2015. [55] Y. Lipman, R. T. Chen, H. Ben-Hamu et al., Flow matching for generative modeling, arxiv, 2022. [56] X. Liu, C. Gong, and Q. Liu, Flow straight and fast: Learning to generate and transfer data with rectified flow, arxiv, 2022. [57] C. Jia, Y. Yang, Y. Xia et al., Scaling up visual and vision-language representation learning with noisy text supervision, in ICML. PMLR, 2021, pp. 49044916. [58] L. Jiasen, C. Christopher, Z. Rowan et al., Unified-io: unified model for vision, language, and multi-modal tasks, arxiv, 2022. [59] C. Team, Chameleon: Mixed-modal early-fusion foundation models, arxiv, 2024. [60] C. Ma and Y. Zhang, Theoretical bounds of modality alignment in world models, in ICML, 2024. [61] K. Lee, H. Liu, M. Ryu et al., Aligning text-to-image models using human feedback, NeurIPS, 2023. [62] Y. Zhang, Y. Li, Y. Yang et al., Reasongen-r1: Cot for autoregressive image generation models through sft and rl, arxiv, 2025. [63] Y. Ji, J. Li, Y. Xiang et al., survey of test-time compute: From intuitive inference to deliberate reasoning, arxiv, 2025. [64] R. Tian, M. Gao, M. Xu et al., Unigen: Enhanced training & test-time strategies for unified multimodal understanding and generation, in arxiv, 2025. [65] H. He, J. Liang, X. Wang et al., Scaling image and video generation via test-time evolutionary search, arxiv, 2025. [66] D. Silver, J. Schrittwieser, K. Simonyan et al., Mastering the game of go without human knowledge, Nature, 2017. [67] K. Cobbe, V. Kosaraju, M. Bavarian et al., Training verifiers to solve math word problems, arxiv, 2021. [68] Z. Huang, N. Yu, G. Chen et al., Vchain: Chain-of-visual-thought for reasoning in video generation, arxiv, 2025. [69] R. Baillargeon, Object permanence in 3½and 4½-month-old infants, Dev. Psychol., vol. 23, no. 5, pp. 655664, 1987. [70] E. S. Spelke, Principles of object perception, Cog. Sci., vol. 14, no. 1, pp. 2956, 1990. [71] P. Anderson, Q. Wu, D. Teney et al., On evaluation of embodied navigation agents, arxiv, 2018. [72] R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision, 2nd ed. Cambridge University Press, 2003. [73] X. Shi, Z. Chen, H. Wang et al., Convolutional lstm network: machine learning approach for precipitation nowcasting, NeurIPS, 2015. [74] Y. Wang, M. Long, J. Wang et al., Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms, NeurIPS, 2017."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[75] V. L. Guen and N. Thome, Disentangling physical dynamics from unknown factors for unsupervised video prediction, in CVPR, 2020. [76] E. Denton and R. Fergus, Stochastic video generation with learned prior, in ICML, 2018. [77] M. Raissi, P. Perdikaris, and G. Karniadakis, Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, J. Comput. Phys., vol. 378, pp. 686707, 2019. [78] M. Lutter, C. Ritter, and J. Peters, Deep lagrangian networks: Using physics as model prior for deep learning, in ICLR, 2019. [79] S. Greydanus, M. Dzamba, and J. Yosinski, Hamiltonian neural networks, NeurIPS, vol. 32, 2019. [80] Y. Rubanova, R. T. Chen, and D. K. Duvenaud, Latent ordinary differential equations for irregularlysampled time series, NeurIPS, vol. 32, 2019. [81] B. Mildenhall, P. P. Srinivasan, M. Tancik et al., Nerf: Representing scenes as neural radiance fields for view synthesis, in ECCV, 2020. [82] J. T. Barron, B. Mildenhall, M. Tancik et al., Mip-nerf: multiscale representation for anti-aliasing neural radiance fields, in ICCV, 2021. [83] J. T. Barron, B. Mildenhall, D. Verbin et al., Zip-nerf: Anti-aliased grid-based neural radiance fields, in ICCV, 2023, pp. 19 69719 705. [84] T. Muller, A. Evans, C. Schied et al., Instant neural graphics primitives with multiresolution hash encoding, ACM TOG, 2022. [85] P. Wang, L. Liu, Y. Liu et al., Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction, arxiv, 2021. [86] L. Yariv, J. Gu, Y. Kasten et al., Volume rendering of neural implicit surfaces, in NeurIPS, 2021. [87] A. Gropp, L. Yariv, N. Haim et al., Implicit geometric regularization for learning shapes, in ICML, 2020, pp. 37893799. [88] Z. Yu, S. Peng, M. Niemeyer et al., Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction, NeurIPS, vol. 35, pp. 25 01825 032, 2022. [89] B. Kerbl, G. Kopanas, T. Leimkuhler et al., 3d gaussian splatting for real-time radiance field rendering. TOG, 2023. [90] T. Lu, M. Yu, L. Xu et al., Scaffold-gs: Structured 3d gaussian splatting for view-adaptive rendering, in CVPR, 2024. [91] B. Huang, Z. Yu, A. Chen et al., 2d gaussian splatting for geometrically accurate radiance fields, in SIGGRAPH, 2024. [92] Z. Yu, A. Chen, B. Huang et al., Mip-splatting: Alias-free 3d gaussian splatting, in CVPR, 2024. [93] T. Xie, Z. Zong, Y. Qiu et al., Physgaussian: Physics-integrated 3d gaussians for generative dynamics, in CVPR, 2024, pp. 43894398. [94] G. Wu, T. Yi, J. Fang et al., 4d gaussian splatting for real-time dynamic scene rendering, in CVPR, 2024. [95] Z. Yang, X. Gao, W. Zhou et al., Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction, in CVPR, 2024. [96] Z. Li, Z. Chen, Z. Li et al., Spacetime gaussian feature splatting for real-time dynamic view synthesis, in CVPR, 2024. [97] B. Poole, A. Jain, J. T. Barron et al., Dreamfusion: Text-to-3d using 2d diffusion, arxiv, 2022. [98] Z. Wang, C. Lu, Y. Wang et al., Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation, in NeurIPS, 2023."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[99] Y. Shi, P. Wang, J. Ye et al., Mvdream: Multi-view diffusion for 3d generation, arxiv, 2023. [100] J. Tang, Z. Chen, X. Chen et al., Lgm: Large multi-view gaussian model for high-resolution 3d content creation, ECCV, 2024. [101] M. Deitke, D. Schwenk, J. Salvador et al., Objaverse: universe of annotated 3d objects, in CVPR, 2023, pp. 13 14213 153. [102] V. Voleti, C.-H. Yao, M. Boss et al., Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion, in ECCV, 2024. [103] S. Wang, V. Leroy, Y. Cabon et al., Dust3r: Geometric 3d vision made easy, in CVPR, 2024, pp. 20 69720 709. [104] B. Ma, H. Gao, H. Deng et al., You see it, you got it: Learning 3d creation on pose-free videos at scale, in CVPR, 2025. [105] N. Michael, T. B. Jonathan, M. Ben et al., Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs, in CVPR, 2022. [106] Y. Liu, C. Lin, Z. Zeng et al., Syncdreamer: Generating multiview-consistent images from single-view image, arxiv, 2023. [107] J. T. Kajiya, The rendering equation, ACM SIGGRAPH, 1986. [108] S. Yang, S.-D. Jascha, P. K. Diederik et al., Score-based generative modeling through stochastic differential equations, in ICLR, 2021. [109] Y. Wang, Z. Gao, M. Long et al., Predrnn++: Towards resolution of the deep-in-time dilemma in spatiotemporal predictive learning, in ICML, 2018. [110] Z. Gao, C. Tan, L. Wu et al., Simvp: Simpler yet better video prediction, in CVPR, 2022. [111] C. Tan, J. Wang, Z. Gao et al., Ustep: Spatio-temporal predictive learning under unified view, IEEE T-PAMI, 2025. [112] C. Tan, Z. Gao, L. Wu et al., Temporal attention unit: Towards efficient spatiotemporal predictive learning, in CVPR, 2023, pp. 18 77018 782. [113] J. Wei, C. Tan, Z. Gao et al., Interpretable and generalizable spatiotemporal predictive learning with disentangled consistency, in ECML/PKDD. Springer, 2024, pp. 320. [114] S. Liu, T. Li, W. Chen et al., Soft rasterizer: differentiable renderer for image-based 3d reasoning, in ICCV, 2019. [115] W. Chen, H. Ling, J. Gao et al., Learning to predict 3d objects with an interpolation-based differentiable renderer, NeurIPS, 2019. [116] E. R. Chan, C. Z. Lin, M. A. Chan et al., Efficient geometry-aware 3d generative adversarial networks, in CVPR, 2022. [117] A. Chen, Z. Xu, A. Geiger et al., Tensorf: Tensorial radiance fields, in ECCV, 2022. [118] J. T. Barron, B. Mildenhall, D. Verbin et al., Zip-nerf: Anti-aliased grid-based neural radiance fields, in ICCV, 2023. [119] L. Yariv, Y. Kasten, D. Moran et al., Multiview neural surface reconstruction by disentangling geometry and appearance, in NeurIPS, vol. 33, 2020, pp. 24922502. [120] K. Park, U. Sinha, P. Hedman et al., Hypernerf: higher-dimensional representation for topologically varying neural radiance fields, ACM TOG, 2021. [121] G. Wu, T. Yi, J. Fang et al., 4d gaussian splatting for real-time dynamic scene rendering, in CVPR, 2024, pp. 20 31020 320. [122] J. Luiten, G. Kopanas, B. Leibe et al., Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis, in 3DV, 2024."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[123] Z. Wang, C. Lu, Y. Wang et al., Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation, PR, vol. 36, pp. 84068441, 2023. [124] J. Tang, Z. Chen, X. Chen et al., Lgm: Large multi-view gaussian model for high-resolution 3d content creation, in ECCV. Springer, 2024, pp. 118. [125] X. Yu, M. Xu, Y. Zhang et al., Mvimgnet: large-scale dataset of multi-view images, in CVPR, 2023, pp. 91509161. [126] J. Reizenstein, R. Shapovalov, P. Henzler et al., Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction, in ICCV, 2021, pp. 10 90110 911. [127] V. Voleti, C.-H. Yao, M. Boss et al., Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion, in ECCV. Springer, 2024, pp. 439457. [128] Y. J. Ma, W. Liang, G. Wang et al., Eureka: Human-level reward design via coding large language models, in ICLR, 2024. [129] W. Menapace, S. Lathuili`ere, S. Tulyakov et al., Playable environments: Video generation in space and time, in CVPR. IEEE, 2022, pp. 35843594. [130] K. Dalal, D. Koceja, G. Hussein et al., One-minute video generation with test-time training, CVPR, 2025. [131] L. Khachatryan, A. Movsisyan, V. Tadevosyan et al., Text2video-zero: Text-to-image diffusion models are zero-shot video generators, in ICCV. IEEE, 2023, pp. 15 95415 964. [132] C. Qi, X. Cun, Y. Zhang et al., Fatezero: Fusing attentions for zero-shot text-based video editing, in ICCV, 2023, pp. 15 93215 942. [133] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in ICCV, 2023. [134] J. Z. Wu, Y. Ge, X. Wang et al., Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation, in ICCV, 2023, pp. 76237633. [135] Y. Guo, C. Yang, A. Rao et al., Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, arxiv, 2023. [136] H. Chen, M. Xia, Y. He et al., Videocrafter1: Open diffusion models for high-quality video generation, arxiv, 2023. [137] J. Wang, H. Yuan, D. Chen et al., Modelscope text-to-video technical report, arxiv, 2023. [138] P. Esser, J. Chiu, P. Atighehchian et al., Structure and content-guided video synthesis with diffusion models, in ICCV, 2023, pp. 73467356. [139] Y. Rao, W. Zhao, Z. Zhu et al., Global filter networks for image classification, NeurIPS, vol. 34, pp. 980993, 2021. [140] J. Guibas, M. Mardani, Z. Li et al., Adaptive fourier neural operators: Efficient token mixers for transformers, arxiv, 2021. [141] C. Bai, Y. Li, Z. Zhao et al., Fastinit: Fast noise initialization for temporally consistent video generation, arxiv, 2025. [142] H. Qiu, M. Xia, Y. Zhang et al., Freenoise: Tuning-free longer video diffusion via noise rescheduling, in ICLR, 2024. [143] D. Kondratyuk, L. Yu, X. Gu et al., Videopoet: large language model for zero-shot video generation, arxiv, 2023. [144] A. Gupta, L. Yu, K. Sohn et al., Photorealistic video generation with diffusion models, in ECCV. Springer, 2024, pp. 393411. [145] L. Yu, J. Lezama, N. B. Gundavarapu et al., Language model beats diffusiontokenizer is key to visual generation, arxiv, 2023."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[146] NVIDIA, N. Agarwal, A. Ali et al., Cosmos world foundation model platform for physical ai, arxiv, 2025. [147] K. Tian, Y. Jiang, Z. Yuan et al., Visual autoregressive modeling: Scalable image generation via next-scale prediction, in NeurIPS, vol. 37, 2024, pp. 84 83984 865. [148] L. Zhang, S. Cai, M. Li et al., Frame context packing and drift prevention in next-frame-prediction video diffusion models, in NeurIPS, 2025. [149] B. Chen, D. M. Mons o, Y. Du et al., Diffusion forcing: Next-token prediction meets full-sequence diffusion, in NeurIPS, 2024. [150] W. Kong, Q. Tian, Z. Zhang et al., Hunyuanvideo: systematic framework for large video generative models, arxiv, 2024. [151] O. Bar-Tal, H. Chefer, O. Tov et al., Lumiere: space-time diffusion model for video generation, arxiv, 2024. [152] Google DeepMind, Veo: High-fidelity video generation with compressed latent representation, https: //deepmind.google/technologies/veo/, 2025, technical Report. [153] Y. Jin, Z. Sun, N. Li et al., Pyramidal flow matching for efficient video generative modeling, arxiv, 2024. [154] F. Liu, S. Zhang, X. Wang et al., Timestep embedding tells: Its time to cache for video diffusion model, CVPR, pp. 73537363, 2025. [155] A. Polyak, A. Zohar, A. Brown et al., Movie gen: cast of media foundation models, arxiv, 2024. [156] H. Shao, S. Qian, H. Xiao et al., Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning, NeurIPS, vol. 37, pp. 86128642, 2024. [157] X. Wang and D. Zhou, Chain-of-thought reasoning without prompting, NeurIPS, vol. 37, pp. 66 38366 409, 2024. [158] X. Lai, J. Li, W. Li et al., Mini-o3: Scaling up reasoning patterns and interaction turns for visual search, arxiv, 2025. [159] S. Wang, J. Jin, X. Wang et al., Video-thinker: Sparking thinking with videos via reinforcement learning, arxiv, 2025. [160] H. Liu, K. Luo, J. Wang et al., Thinksound: Chain-of-thought reasoning in multimodal large language models for audio generation and editing, arxiv, 2025. [161] S. Motamed, L. Culp, K. Swersky et al., Do generative video models understand physical principles? arxiv, 2025. [162] T. Aoshima, Y. Shinohara, and B. Park, Video consistency distance: Enhancing temporal consistency for image-to-video generation via reward-based fine-tuning, arxiv, 2025. [163] D. Ha and J. Schmidhuber, World models, in NeurIPS, 2018. [164] T. Zhang, H.-X. Yu, R. Wu et al., Physdreamer: Physics-based interaction with 3d objects via video generation, in ECCV. Springer, 2024, pp. 388406. [165] T. Unterthiner, S. van Steenkiste, K. Kurach et al., Towards accurate generative models of video: new metric & challenges, arxiv, 2018. [166] T. Aoshima, Y. Shinohara, and B. Park, Video consistency distance: Enhancing temporal consistency for image-to-video generation via reward-based fine-tuning, arxiv, 2025. [167] T. Wiedemer, Y. Li, P. Vicol et al., Video models are zero-shot learners and reasoners, arxiv, 2025. [168] F. Sun, J. Liu, J. Wu et al., Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer, in ACM CIKM, 2019, pp. 14411450. [169] H. Chen, Y. Lin, M. Pan et al., Denoising self-attentive sequential recommendation, in ACM RecSys, 2022, pp. 92101."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[170] J. Xing, M. Xia, Y. Zhang et al., Dynamicrafter: Animating open-domain images with video diffusion priors, in ECCV, 2024. [171] W. Hong, M. Ding, W. Zheng et al., Cogvideo: Large-scale pretraining for text-to-video generation via transformers, arxiv, 2022. [172] A. K. Akan and Y. Yemez, Compositional video synthesis by temporal object-centric learning, arxiv, 2025. [173] K. Tian, Y. Jiang, Z. Yuan et al., Visual autoregressive modeling: Scalable image generation via next-scale prediction, NeurIPS, vol. 37, pp. 84 83984 865, 2024. [174] L. Zhang, S. Cai, M. Li et al., Pretraining frame preservation in autoregressive video memory compression, arxiv, 2025. [175] Y. Bengio, N. Leonard, and A. Courville, Estimating or propagating gradients through stochastic neurons for conditional computation, arxiv, 2013. [176] J. Xie, W. Mao, Z. Bai et al., Show-o: One single transformer to unify multimodal understanding and generation, arxiv, 2024. [177] A. Polyak, A. Zohar, A. Brown et al., Movie gen: cast of media foundation models, arxiv, 2024. [178] Y. Zeng, G. Wei, J. Zheng et al., Make pixels dance: High-dynamic video generation, arxiv, 2023. [179] Z. Yang, J. Teng, W. Zheng et al., Cogvideox: Text-to-video diffusion models with an expert transformer, arxiv, 2024. [180] Z. Zhang, A. Zhang, M. Li et al., Multimodal chain-of-thought reasoning in language models, arxiv, 2023. [181] OpenAI, Gpt-4v(ision) system card, https://cdn.openai.com/papers/GPTV System Card.pdf, sep 2023, technical report. [182] , Gpt-4 technical report, arxiv, 2023. [183] S. Yin, C. Fu, S. Zhao et al., survey on multimodal large language models, NSR, vol. 11, no. 12, p. nwae403, 2024. [184] T. Brown, B. Mann, N. Ryder et al., Language models are few-shot learners, NeurIPS, vol. 33, pp. 18771901, 2020. [185] H. Touvron, T. Lavril, G. Izacard et al., Llama: Open and efficient foundation language models, arxiv, 2023. [186] J. Wei, X. Wang, D. Schuurmans et al., Chain-of-thought prompting elicits reasoning in large language models, NeurIPS, vol. 35, pp. 24 82424 837, 2022. [187] T. Kojima, S. S. Gu, M. Reidsma et al., Large language models are zero-shot reasoners, in NeurIPS, vol. 35, 2022, pp. 22 19922 213. [188] D. Zhu, J. Chen, X. Shen et al., Minigpt-4: Enhancing vision-language understanding with advanced large language models, arxiv, 2023. [189] P. Gao, J. Zhang, R. Liu et al., Llama-adapter: Efficient fine-tuning of language models with zero-init attention, arxiv, 2023. [190] X. Zhai, B. Mustafa, A. Kolesnikov et al., Sigmoid loss for language image pre-training, in ICCV, 2023, pp. 11 97511 986. [191] N. Tishby and N. Zaslavsky, Deep learning and the information bottleneck principle, ITW, 2015. [192] W. Kim, B. Son, and I. Kim, Vilt: Vision-and-language transformer without convolution or region supervision, in ICML, 2021, pp. 55835594. [193] J. Wang, Z. Yang, X. Hu et al., Git: generative image-to-text transformer for vision and language, TMLR, 2022."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[194] Z. Yang, Z. Gan, J. Wang et al., Mm-react: Prompting chatgpt for multimodal reasoning and action, arxiv, 2023. [195] C. Wu, S. Yin, W. Qi et al., Visual chatgpt: Talking, drawing and editing with visual foundation models, arxiv, 2023. [196] T. Schick, J. Dwivedi-Yu, R. Dessi et al., Toolformer: Language models can teach themselves to use tools, in NeurIPS, 2023. [197] D. Surıs, S. Menon, and C. Vondrick, Vipergpt: Visual inference via python execution for reasoning, in ICCV, 2023. [198] G. Tanmay and K. Aniruddha, Visual programming: Compositional visual reasoning without training, in CVPR, 2023. [199] D. Driess, F. Xia, M. S. Sajjadi et al., Palm-e: An embodied multimodal language model, arxiv, 2023. [200] G. Wang, Y. Xie, Y. Jiang et al., Voyager: An open-ended embodied agent with large language models, in arxiv, 2024. [201] Y. Shen, K. Song, X. Tan et al., Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, in NeurIPS, 2023. [202] S. Yao, J. Zhao, D. Yu et al., React: Synergizing reasoning and acting in language models, in ICLR, 2023. [203] T. B. Richards, Auto-gpt: An autonomous gpt-4 experiment, https://github.com/Significant-Gravitas/ Auto-GPT, 2023. [204] Z. Yang, S. Wang, M. Ma et al., Unisim: neural closed-loop sensor simulator, in CVPR, 2023, pp. 13891399. [205] A. Hu, L. Russell, H. Yeo et al., Gaia-1: generative world model for autonomous driving, arxiv, 2023. [206] Y. Li, H. Liu, Q. Wu et al., Gligen: Open-set grounded text-to-image generation, in CVPR, 2023, pp. 22 51122 521. [207] L. Lian, B. Shi, A. Yala et al., Llm-grounded video diffusion models, in ICLR, 2024. [208] R. Liu, R. Wu, B. Van Hoorick et al., Zero-1-to-3: Zero-shot one image to 3d object, in ICCV, 2023. [209] J. Ho, T. Salimans, A. Gritsenko et al., Video diffusion models, in NeurIPS, 2022. [210] B. Zeng, L. Yang, J. Liu et al., Editworld: Simulating world dynamics for instruction-following image editing, in ACM MM, 2025, pp. 12 67412 681. [211] Z. Zhang, D. Chen, and J. Liao, Sgedit: Bridging llm with text2image generative model for scene graphbased image editing, arxiv, 2024. [212] Z. M. Wang, K. Zhu, C. Xu et al., Mio: foundation model on multimodal tokens, in EMNLP, 2025, pp. 50775099. [213] M.-S. Kwak, J. Kim, S. Yun et al., Aligned novel view image and geometry synthesis via cross-modal attention instillation, arxiv, 2025. [214] X. Long, Y.-C. Guo, C. Lin et al., Wonder3d: Single image to 3d using cross-domain diffusion, in CVPR, 2024, pp. 99709980. [215] M. Liu, R. Shi, K. Kuang et al., Openshape: Scaling up 3d shape representation towards open-world understanding, PR, vol. 36, pp. 44 86044 879, 2023. [216] S. Krakovsky, G. Fiebelman, S. Benaim et al., Lang3d-xl: Language embedded 3d gaussians for large-scale scenes, in ACM SIGGRAPH, 2025, pp. 111. [217] K. Black, M. Janner, Y. Du et al., Training diffusion models with reinforcement learning, in arxiv, 2023. [218] B. Li, X. Li, J. Xu et al., Test-time preference optimization for image restoration, arxiv, 2025."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[219] H. Shi, J. Su, H. Ning et al., Layoutcot: Unleashing the deep reasoning potential of large language models for layout generation, arxiv, 2025. [220] C.-H. Lin, J. Gao, L. Tang et al., Magic3d: High-resolution text-to-3d content creation, in CVPR, 2023, pp. 300309. [221] J. Ho, W. Chan, C. Saharia et al., Imagen video: High definition video generation with diffusion models, arxiv, 2022. [222] P. Esser, R. Rombach, and B. Ommer, Taming transformers for high-resolution image synthesis, in CVPR, 2021. [223] R. Rombach, A. Blattmann, D. Lorenz et al., High-resolution image synthesis with latent diffusion models, in CVPR, 2022, pp. 10 68410 695. [224] A. Blattmann, R. Rombach, H. Ling et al., Align your latents: High-resolution video synthesis with latent diffusion models, in CVPR, 2023, pp. 22 56322 575. [225] S. Liu, Y. Han, P. Xing et al., Step1x-edit: practical framework for general image editing, arxiv, 2025. [226] R. Dong, C. Han, Y. Peng et al., Dreamllm: Synergistic multimodal comprehension and creation, arxiv, 2023. [227] X. Chen, Z. Zhang, H. Zhang et al., Unireal: Universal image generation and editing via learning real-world dynamics, in PR, 2025, pp. 12 50112 511. [228] H. Zhao, Z. Cai, S. Si et al., Mentor: Efficient multimodal-conditioned tuning for autoregressive vision generation models, arxiv, 2025. [229] N. G. Nair, S. Kaza, X. Luo et al., Scaling transformer-based novel view synthesis with models token disentanglement and synthetic data, in ICCV, 2025, pp. 28 56728 576. [230] L. Melas-Kyriazi, I. Laina, C. Rupprecht et al., Realfusion: 360deg reconstruction of any object from single image, in CVPR, 2023, pp. 84468455. [231] T. Hunyuan3D, B. Zhang, C. Guo et al., Hunyuan3d-omni: unified framework for controllable generation of 3d assets, arxiv, 2025. [232] F. Liu, H. Li, J. Chi et al., Langscene-x: Reconstruct generalizable 3d language-embedded scenes with trimap video diffusion, arxiv, 2025. [233] X. Yin, Q. Zhang, J. Chang et al., Gsfixer: Improving 3d gaussian splatting with reference-guided video diffusion priors, arxiv, 2025. [234] J. Chen, Y. Qin, L. Liu et al., Nerf-hugs: Improved neural radiance fields in non-static scenes using heuristics-guided segmentation, in CVPR, 2024, pp. 19 43619 446. [235] L. Xue, N. Yu, S. Zhang et al., Ulip-2: Towards scalable multimodal pre-training for 3d understanding, in CVPR, 2024, pp. 27 09127 101. [236] J. Ye, Z. Wang, R. Zhao et al., Shapellm-omni: native multimodal llm for 3d generation and understanding, arxiv, 2025. [237] X. Guo, Z. Wu, K. Xiong et al., Genesis: Multimodal driving scene generation with spatio-temporal and cross-modal consistency, arxiv, 2025. [238] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, Viewset diffusion:(0-) image-conditioned 3d generative models from 2d data, in ICCV, 2023, pp. 88638873. [239] J. Kerr, C. M. Kim, K. Goldberg et al., Lerf: Language embedded radiance fields, in ICCV, 2023, pp. 19 72919 739. [240] A. Haque, M. Tancik, A. A. Efros et al., Instruct-nerf2nerf: Editing 3d scenes with instructions, in ICCV, 2023, pp. 19 74019 750."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[241] M. A. Mirzaei, P. Amoie, A. Ekhterachian et al., Core-3d: Context-aware open-vocabulary retrieval by embeddings in 3d, arxiv, 2025. [242] C. Wang, M. Chai, M. He et al., Clip-nerf: Text-and-image driven manipulation of neural radiance fields, in CVPR, 2022, pp. 38353844. [243] P. Wang, Z. Fan, Z. Wang et al., Lift3d: Zero-shot lifting of any 2d vision model to 3d, in CVPR, 2024, pp. 21 36721 377. [244] A. Mikaeili, O. Perel, M. Safaee et al., Sked: Sketch-guided text-based 3d editing, in ICCV, 2023, pp. 14 60714 619. [245] V. Jaganathan, H. H. Huang, M. Z. Irshad et al., Ice-g: Image conditional editing of 3d gaussian splats, arxiv, 2024. [246] M. Prabhudesai, A. Goyal, D. Pathak et al., Aligning text-to-image diffusion models with reward backpropagation, arxiv, 2023. [247] V. Gallego, Refined direct preference optimization with synthetic data for behavioral alignment of llms, in ICML. Springer, 2024, pp. 92105. [248] Z. Liang, Y. Yuan, S. Gu et al., Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization, arxiv, 2025. [249] A. Hertz, R. Mokady, J. Tenenbaum et al., Prompt-to-prompt image editing with cross attention control, in ICLR, 2023. [250] H. Ye, J. Zhang, S. Liu et al., Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models, arxiv, 2023. [251] M. Cao, X. Wang, Z. Qi et al., Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing, in ICCV, 2023. [252] T. Kipf, E. van der Pol, and M. Welling, Contrastive learning of structured world models, in ICLR, 2020. [253] Q. Sun, Q. Yu, Y. Cui et al., Emu: Generative pretraining in multimodality, in ICLR, 2024. [254] F. Locatello, D. Weissenborn, T. Unterthiner et al., Object-centric learning with slot attention, in NeurIPS, 2020. [255] Z. Chen, Y. Wang, F. Wang et al., V3d: Video diffusion models are effective 3d generators, arxiv, 2024. [256] G. Qian, J. Mai, A. Hamdi et al., Magic123: One image to high-quality 3d generation using both 2d and 3d diffusion priors, in arxiv, 2023. [257] M. Liu, C. Xu, H. Jin et al., One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization, PR, vol. 36, pp. 22 22622 246, 2023. [258] T. Shen, J. Gao, K. Yin et al., Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis, in NeurIPS, 2021. [259] A. Haque, M. Tancik, A. A. Efros et al., Instruct-nerf2nerf: Editing 3d scenes with instructions, in ICCV, 2023. [260] S. Liu, Z. Zeng, T. Ren et al., Grounding dino: Marrying dino with grounded pre-training for open-set object detection, arxiv, 2023. [261] K. Clark, P. Vicol, K. Swersky et al., Directly fine-tuning diffusion models on differentiable rewards, arxiv, 2023. [262] X. Wang, H. Yuan, S. Zhang et al., Videocomposer: Compositional video synthesis with motion controllability, in NeurIPS, vol. 36, 2023, pp. 75947611. [263] A. Blattmann, T. Dockhorn, S. Kulal et al., Stable video diffusion: Scaling latent video diffusion models to large datasets, arxiv, 2023."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[264] C. Low, W. Wang, and C. Katyal, Ovi: Twin backbone cross-modal fusion for audio-video generation, arxiv, 2025. [265] J. Liu, H. Chen, P. An et al., Hybridvla: Collaborative diffusion and autoregression in unified visionlanguage-action model, arxiv, 2025. [266] C. Zhang, Y. Liang, X. Qiu et al., Vast 1.0: unified framework for controllable and consistent video generation, arxiv, 2024. [267] J. Feng, A. Ma, J. Wang et al., Fancyvideo: Towards dynamic and consistent video generation via cross-frame textual guidance, arxiv, 2024. [268] R. Villegas, M. Babaeizadeh, P.-J. Kindermans et al., Phenaki: Variable length video generation from open domain textual description, in ICLR, 2023. [269] Z. Tan, H. Yang, L. Qin et al., Omni-video: Democratizing unified video understanding and generation, arxiv, 2025. [270] R. Liu, H. Wu, Z. Zheng et al., Videodpo: Omni-preference alignment for video diffusion generation, in CVPR, 2025, pp. 80098019. [271] J. Cheng, R. Lyu, X. Gu et al., Vpo: Aligning text-to-video generation models with prompt optimization, arxiv, 2025. [272] Y. Liu, K. Zhang, Y. Li et al., Sora: review on background, technology, limitations, and opportunities of large vision models, arxiv, 2024. [273] T. Wan, A. Wang, B. Ai et al., Wan: Open and advanced large-scale video generative models, arxiv, 2025. [274] F. Bao, C. Xiang, G. Yue et al., Vidu: highly consistent, dynamic and skilled text-to -video generator with diffusion models, arxiv, 2024. [275] U. Singer, A. Polyak, T. Hayes et al., Make-a-video: Text-to-video generation without text-video data, arxiv, 2022. [276] L. Ruan, L. Tian, C. Huang et al., Univg: Towards unified-modal video generation, in IEEE ICME. IEEE, 2025, pp. 16. [277] Pika Labs, Pika: An idea-to-video platform, https://pika.art, 2023, accessed: 2026-02-09. [278] W. Kong, Q. Tian, Z. Zhang et al., Hunyuanvideo: systematic framework for large video generative models, arxiv, 2024. [279] K. Team, J. Chen, Y. Ci et al., Kling-omni technical report, arxiv, 2025. [280] J. Han, H. Chen, Y. Zhao et al., Vision as dialect: Unifying visual understanding and generation via text-aligned representations, arxiv, 2025. [281] A. Ergasti, G. G. Tarollo, F. Botti et al., R-flav: Rolling flow matching for infinite audio video generation, arxiv, 2025. [282] L. Zhao, L. Feng, D. Ge et al., Uniform: unified multi-task diffusion transformer for audio-video generation, arxiv, 2025. [283] Y. Wu, Z. Zhang, J. Chen et al., Vila-u: unified foundation model integrating visual understanding and generation, arxiv, 2024. [284] H. Deng, T. Pan, H. Diao et al., Autoregressive video generation without vector quantization, arxiv, 2024. [285] X. Cheng, T. He, J. Xu et al., Playing with transformer at 30+ fps via next-frame diffusion, arxiv, 2025. [286] H. Chung, D. Lee, and J. C. Ye, Acdc: Autoregressive coherent multimodal generation using diffusion correction, arxiv, 2024. [287] Y. Li, Y. Ge, Y. Ge et al., Dicode: Diffusion-compressed deep tokens for autoregressive video generation with language models, arxiv, 2024."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[288] Z. Li, H. Shujie, L. Shujie et al., Arlon: Boosting diffusion transformers with autoregressive models for long video generation, in ICLR, 2025. [289] M. Sun, W. Wang, G. Li et al., Ar-diffusion: Asynchronous video generation with auto-regressive diffusion, in CVPR, 2025, pp. 73647373. [290] T. Yin, Q. Zhang, R. Zhang et al., From slow bidirectional to fast autoregressive video diffusion models, in CVPR, 2025, pp. 22 96322 974. [291] E. Corona, A. Zanfir, E. G. Bazavan et al., Vlogger: Multimodal diffusion for embodied avatar synthesis, in CVPR, 2025. [292] P. Zhang, J. Li, M. Wang et al., When video coding meets multimodal large language models: unified paradigm for video coding, arxiv, 2024. [293] W. Chen, Y. Ji, J. Wu et al., Control-a-video: Controllable text-to-video diffusion models with motion prior and reward feedback learning, arxiv, 2023. [294] G. Yariv, I. Gat, S. Benaim et al., Diverse and aligned audio-to-video generation via text-to-video model adaptation, in AAAI, vol. 38, no. 7, 2024, pp. 66396647. [295] K. Gong, D. Lian, H. Chang et al., Tm2d: Bimodality driven 3d dance generation via music-text integration, in ICCV, 2023, pp. 99429952. [296] X. Li, W. Chu, Y. Wu et al., Videogen: reference-guided latent diffusion approach for high definition text-to-video generation, arxiv, 2023. [297] D. J. Zhang, J. Z. Wu, J.-W. Liu et al., Show-1: Marrying pixel and latent diffusion models for text-to-video generation, IJCV, vol. 133, no. 4, pp. 18791893, 2025. [298] Y. Zhang, Y. Kang, Z. Zhang et al., Interactivevideo: User-centric controllable video generation with synergistic multimodal instructions, arxiv, 2024. [299] X. Wang, J. Liu, Z. Wang et al., Keyvid: Keyframe-aware video diffusion for audio-synchronized visual animation, arxiv, 2025. [300] J. H. Liew, H. Yan, J. Zhang et al., Magicedit: High-fidelity and temporally coherent video editing, arxiv, 2023. [301] W. Wang, H. Yang, Z. Tuo et al., Swap attention in spatiotemporal diffusions for text-to-video generation, IJCV, pp. 119, 2025. [302] D. J. Zhang, D. Li, H. Le et al., Moonshot: Towards controllable video generation and editing with multimodal conditions, arxiv, 2024. [303] R. Feng, W. Weng, Y. Wang et al., Ccedit: Creative and controllable video editing via diffusion models, in CVPR, 2024, pp. 67126722. [304] S. Ge, T. Hayes, H. Yang et al., Long video generation with time-agnostic vqgan and time-sensitive transformer, in ECCV, 2022, pp. 102118. [305] X. Zhou, D. Liang, S. Tu et al., Hermes: unified self-driving world model for simultaneous 3d scene understanding and generation, arxiv, 2025. [306] L. Chen, Y. Gu, and Q. Mao, Univid: Unifying vision tasks with pre-trained video generation models, arxiv, 2025. [307] M. Hu, C. Zheng, H. Zheng et al., Unified discrete diffusion for simultaneous vision-language generation, arxiv, 2022. [308] T. Hu, Z. Yu, Z. Zhou et al., Hunyuancustom: multimodal-driven architecture for customized video generation, arxiv, 2025. [309] J. Chung, T. Zhu, M. G. Saez-Diez et al., Unifying specialized visual encoders for video language models, arxiv, 2025."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[310] L. Yang, X. Zhang, Y. Tian et al., Hermesflow: Seamlessly closing the gap in multimodal understanding and generation, arxiv, 2025. [311] Y. Fang, W. Menapace, A. Siarohin et al., Vimi: Grounding video generation through multi-modal instruction, in EMNLP, 2024, pp. 44444456. [312] J. Li, W. Feng, T.-J. Fu et al., T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback, in NeurIPS, vol. 37, 2024, pp. 75 69275 726. [313] O. Zohar, X. Wang, Y. Bitton et al., Video-star: Self-training enables video instruction tuning with any supervision, in arxiv, 2024. [314] X. He, D. Jiang, G. Zhang et al., Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation, in EMNLP, 2024, pp. 21052123. [315] L. Yu, J. Lezama, N. B. Gundavarapu et al., Language model beats diffusiontokenizer is key to visual generation, arxiv, 2023. [316] W. Peebles and S. Xie, Scalable diffusion models with transformers, in ICCV, 2023, pp. 41954205. [317] Z. Zheng, X. Peng, T. Yang et al., Open-sora: Democratizing efficient video production for all, arxiv, 2024. [318] H. Liu, M. Zaharia, and P. Abbeel, Ring attention with blockwise transformers for near-infinite context, arxiv, 2023. [319] X. Ma, Y. Wang, X. Chen et al., Latte: Latent diffusion transformer for video generation, arxiv, 2024. [320] J. Z. Wu, Y. Ge, X. Wang et al., Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation, in ICCV, 2023, pp. 76237633. [321] J. Kaplan, S. McCandlish, T. Henighan et al., Scaling laws for neural language models, arxiv, 2020. [322] J. Hoffmann, S. Borgeaud, A. Mensch et al., Training compute-optimal large language models, in NeurIPS, vol. 35, 2022, pp. 30 01630 030. [323] A. Van Den Oord, O. Vinyals et al., Neural discrete representation learning, in NeurIPS, 2017. [324] S. Yin, C. Wu, J. Liang et al., Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory, in arxiv, 2024. [325] Z. Wang, Z. Yuan, X. Wang et al., Motionctrl: unified and flexible motion controller for video generation, in ACM SIGGRAPH, 2024, pp. 111. [326] X. Chen, Y. Wang, L. Zhang et al., Seine: Short-to-long video diffusion model for generative transition and prediction, in ICLR, 2024. [327] Morph Studio Team, Morph studio: Ai video generation platform, https://www.morphstudio.com/, 2024. [328] A. Siarohin, S. Lathuili`ere, S. Tulyakov et al., First order motion model for image animation, in NeurIPS, vol. 32, 2019. [329] J. Zhao and H. Zhang, Thin-plate spline motion model for image animation, in CVPR, 2022, pp. 36573666. [330] L. Hu, X. Gao, P. Zhang et al., Animate anyone: Consistent and controllable image-to-video synthesis for character animation, in CVPR, 2024. [331] Z. Xu, J. Zhang, J. H. Liew et al., Magicanimate: Temporally consistent human image animation using diffusion model, in CVPR, 2024. [332] S. Zhu, J. L. Chen, Z. Dai et al., Champ: Controllable and consistent human image animation with 3d parametric guidance, arxiv, 2024. [333] M. Zhang, Z. Cai, L. Pan et al., Motiondiffuse: Text-driven human motion generation with diffusion model, IEEE T-PAMI, 2022."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[334] D. Epstein, A. Jabri, B. Poole et al., Diffusion self-guidance for controllable image generation, in NeurIPS, 2023. [335] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, in NeurIPS, vol. 33, 2020, pp. 68406851. [336] C. Wu, Y. Xia, S. Gao et al., Janus: Decoupling visual encoding for unified multimodal understanding and generation, CVPR, 2025. [337] X. Dai, J. Hou, C.-Y. Ma et al., Emu: Enhancing image generation models using photogenic needles in haystack, arxiv, 2023. [338] H. Zhang, X. Li, and L. Bing, Video-llama: An instruction-tuned audio-visual language model for video understanding, in EMNLP, 2023, pp. 543553. [339] K. Li, Y. He, Y. Wang et al., Videochat: Chat-centric video understanding, arxiv, 2023. [340] S. Wu, H. Fei, L. Qu et al., Next-gpt: Any-to-any multimodal llm, in ICML, 2024. [341] P. F. Christiano, J. Leike, T. Brown et al., Deep reinforcement learning from human preferences, in NeurIPS, 2017. [342] L. Ouyang, J. Wu, X. Jiang et al., Training language models to follow instructions with human feedback, in NeurIPS, 2022. [343] X. Wu, K. Sun, F. Zhu et al., Human preference score: Better aligning text-to-image models with human preference, in ICCV, 2023, pp. 20962105. [344] R. Rafailov, A. Sharma, E. Mitchell et al., Direct preference optimization: Your language model is secretly reward model, in NeurIPS, vol. 36, 2023, pp. 53 70253 741. [345] J. Schulman, F. Wolski, P. Dhariwal et al., Proximal policy optimization algorithms, arxiv, 2017. [346] A. Radford, J. W. Kim, C. Hallacy et al., Learning transferable visual models from natural language supervision, in ICML, 2021. [347] M. J. Kim, K. Pertsch, S. Karamcheti et al., Openvla: An open-source vision-language-action model, arxiv, 2024. [348] Z. Guan, H. Sun, Y. Guo et al., Rl-vla 3: Reinforcement learning vla accelerating via full asynchronism, arxiv, 2026. [349] B. Zitkovich, T. Yu, S. Xu et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in CoRL, 2023. [350] Q. Xu, J. Liu, R. Zhou et al., Twinrl-vla: Digital twin-driven reinforcement learning for real-world robotic manipulation, arxiv, 2026. [351] G. Lu, S. Zhang, Z. Wang et al., Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation, in ECCV, 2024. [352] Y. J. Ma, W. Liang, H.-J. Wang et al., Dreureka: Language model guided sim-to-real transfer, 2024, arxiv, 2024. [353] B. Zhang, Y. Zhang, J. Ji et al., Safevla: Towards safety alignment of vision-language-action model via constrained learning, arxiv, 2025. [354] Y. Wang, Z. Xian, F. Chen et al., Robogen: Towards unleashing infinite data for automated robot learning via generative simulation, arxiv, 2023. [355] S. Liu, Y. Zhang, W. Li et al., Video-p2p: Video editing with cross-attention control, in CVPR, 2024, pp. 85998608. [356] J.-g. Kwak, E. Dong, Y. Jin et al., Vivid-1-to-3: Novel view synthesis with video diffusion models, in CVPR, 2024, pp. 67756785."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[357] M. You, Z. Zhu, H. Liu et al., Nvs-solver: Video diffusion model as zero-shot novel view synthesizer, arxiv, 2024. [358] X. Ren, T. Shen, J. Huang et al., Gen3c: 3d-informed world-consistent video generation with precise camera control, in PR, 2025, pp. 61216132. [359] Q. Zhang, S. Zhai, M. A. B. Martin et al., World-consistent video diffusion with explicit 3d modeling, in PR, 2025, pp. 21 68521 695. [360] Z. Gu, R. Yan, J. Lu et al., Diffusion as shader: 3d-aware video diffusion for versatile video generation control, in PR, 2025, pp. 112. [361] Y.-J. Yuan, L. Kobbelt, J. Liu et al., 4dynamic: Text-to-4d generation with hybrid priors, arxiv, 2024. [362] R. Wu, R. Gao, B. Poole et al., Cat4d: Create anything in 4d with multi-view video diffusion models, in CVPR, 2025, pp. 26 05726 068. [363] J. Wang, N. Karaev, C. Rupprecht et al., Vggsfm: Visual geometry grounded deep structure from motion, in CVPR, 2024. [364] J. Li, Q. Long, J. Zheng et al., T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design, arxiv, 2024. [365] H. Yuan, S. Zhang, X. Wang et al., Instructvideo: Instructing video diffusion models with human feedback, in CVPR, 2024, pp. 64636474. [366] B. Li, C. Zheng, W. Zhu et al., Vivid-zoo: Multi-view video generation with diffusion model, PR, vol. 37, pp. 62 18962 222, 2024. [367] Z. Yang, Z. Pan, C. Gu et al., Diffusion 2: Dynamic 3d content generation via score composition of video and multi-view diffusion models, arxiv, 2024. [368] X. Ren, T. Shen, J. Huang et al., Gen3c: 3d-informed world-consistent video generation with precise camera control, in CVPR, 2025. [369] T. Li, G. Zheng, R. Jiang et al., Realcam-i2v: Real-world image-to-video generation with interactive complex camera control, in ICCV, 2025, pp. 28 78528 796. [370] W. Yu, J. Xing, L. Yuan et al., Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis, arxiv, 2024. [371] Z. Wang, J. Cho, J. Li et al., Epic: Efficient video camera control learning with precise anchor-video guidance, arxiv, 2025. [372] H. He, Y. Xu, Y. Guo et al., Cameractrl: Enabling camera control for text-to-video generation, arxiv, 2024. [373] X. Fan, S. Girish, V. Ramanujan et al., Omniview: An all-seeing diffusion model for 3d and 4d view synthesis, arxiv, 2025. [374] Y. Chen, Z. Ye, Z. Fang et al., Postcam: Camera-controllable novel-view video generation with query-shared cross-attention, arxiv, 2025. [375] L. Hollein, A. Bozic, N. Muller et al., Viewdiff: 3d-consistent image generation with text-to-image models, in CVPR, 2024, pp. 50435052. [376] S. Bahmani, I. Skorokhodov, A. Siarohin et al., Vd3d: Taming large video diffusion transformers for 3d camera control, arxiv, 2024. [377] S. Bahmani, X. Liu, W. Yifan et al., Tc4d: Trajectory-conditioned text-to-4d generation, in ECCV. Springer, 2024, pp. 5372. [378] X. Fu, X. Liu, X. Wang et al., 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation, arxiv, 2024. [379] V. Voleti, C.-H. Yao, M. Boss et al., Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion, in ECCV, 2024."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[380] H. Zhang, X. Chen, Y. Wang et al., 4diffusion: Multi-view video diffusion model for 4d generation, PR, vol. 37, pp. 15 27215 295, 2024. [381] G. Wu, T. Yi, J. Fang et al., 4d gaussian splatting for real-time dynamic scene rendering, in CVPR, 2024, pp. 20 31020 320. [382] Y. Jiang, L. Zhang, J. Gao et al., Consistent4d: Consistent 360 dynamic object generation from monocular video, arxiv, 2023. [383] H. Sun, X. Li, L. Shen et al., Dyblurf: Dynamic neural radiance fields from blurry monocular video, in CVPR, 2024, pp. 75177527. [384] Z. Li, Q. Wang, F. Cole et al., Dynibar: Neural dynamic image-based rendering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 42734284. [385] S. Bahmani, I. Skorokhodov, V. Rong et al., 4d-fy: Text-to-4d generation using hybrid score distillation sampling, in CVPR, 2024, pp. 79968006. [386] J. Fang, T. Yi, X. Wang et al., Fast dynamic radiance fields with time-aware neural voxels, in ACM SIGGRAPH, 2022, pp. 19. [387] M.-Q. V. Bui, J. Park, J. Oh et al., Moblurf: Motion deblurring neural radiance fields for blurry monocular video, IEEE T-PAMI, 2025. [388] Y. Xie, C.-H. Yao, V. Voleti et al., Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency, arxiv, 2024. [389] S. Fridovich-Keil, G. Meanti, F. R. Warburg et al., K-planes: Explicit radiance fields in space, time, and appearance, in CVPR, 2023. [390] Y. Zeng, Y. Jiang, S. Zhu et al., Stag4d: Spatial-temporal anchored generative 4d gaussians, in ECCV. Springer, 2024, pp. 163179. [391] J. Ren, L. Pan, J. Tang et al., Dreamgaussian4d: Generative 4d gaussian splatting, arxiv, 2023. [392] H. Ling, S. W. Kim, A. Torralba et al., Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models, in CVPR, 2024, pp. 85768588. [393] B. He, Y. Chen, G. Lu et al., H3d-dgs: Exploring heterogeneous 3d motion representation for deformable 3d gaussian splatting, in NeurIPS, 2025. [394] H. Liang, Y. Yin, D. Xu et al., Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models, arxiv, 2024. [395] X. Liu, Y. Xiao, D. Y. Chen et al., Trace anything: Representing any video in 4d via trajectory fields, arxiv, 2025. [396] Q. Wang, Y.-Y. Chang, R. Cai et al., Tracking everything everywhere all at once, in ICCV, 2023. [397] Y. Xiao, Q. Wang, S. Zhang et al., Spatialtracker: Tracking any 2d pixels in 3d space, in CVPR, 2024. [398] V. Leroy, Y. Cabon, and J. Revaud, Grounding image matching in 3d with mast3r, in ECCV, 2024. [399] H. Huang, H. Chen, S. Wu et al., Vistadpo: Video hierarchical spatial-temporal direct preference optimization for large video models, arxiv, 2025. [400] N. Zhou, J. Chen, and D. Huang, Dr-tune: Improving fine-tuning of pretrained visual models by distribution regularization with semantic calibration, in ICCV, 2023, pp. 15471556. [401] H. Wang, X. Du, J. Li et al., Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation, in CVPR, 2023. [402] B. Efron, Tweedies formula and selection bias, JASA, vol. 106, no. 496, pp. 16021614, 2011. [403] D. Kim, Y. Kim, S. J. Kwon et al., Refining generative process with discriminator guidance in score-based diffusion models, in ICML. ML Research Press, 2023, pp. 16 56716 598."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[404] Y. Du, C. Durkan, R. Strudel et al., Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc, in ICML. PMLR, 2023, pp. 84898510. [405] N. Liu, S. Li, Y. Du et al., Compositional visual generation with composable diffusion models, in ECCV, 2022. [406] Y. Wei, S. Zhang, Z. Qing et al., Dreamvideo: Composing your dream videos with customized subject and motion, in CVPR, 2024. [407] C. Lu, Y. Zhou, F. Bao et al., Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps, in NeurIPS, 2022. [408] W. Zhao, L. Bai, Y. Rao et al., Unipc: unified predictor-corrector framework for fast sampling of diffusion models, in NeurIPS, 2023. [409] J. Zhu and P. Zhuang, Hifa: High-fidelity text-to-3d generation with advanced diffusion guidance, in arxiv, 2024. [410] S. Yang, Y. Zhou, Z. Liu et al., Freeu: Free-lunch for diffusion models, in CVPR, 2024. [411] W. Li, R. Chen, X. Chen et al., Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d, in arxiv, 2024. [412] Z. Yang, Z. Pan, C. Gu et al., Diffusion 2: Dynamic 3d content generation via score composition of video and multi-view diffusion models, arxiv, 2024. [413] Y. Zhang, Y. Wei, D. Jiang et al., Controlvideo: Training-free controllable text-to-video generation, in arxiv, 2023. [414] C. Mou, X. Wang, L. Xie et al., T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, in AAAI, 2024. [415] J. L. Schonberger and J.-M. Frahm, Structure-from-motion revisited, in CVPR, 2016. [416] Z. Teed and J. Deng, Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras, in NeurIPS, 2021. [417] A. Yu, V. Ye, M. Tancik et al., pixelnerf: Neural radiance fields from one or few images, in CVPR, 2021, pp. 45784587. [418] M. Suhail, C. E. a. L. S. Perez, and A. Makadia, Generalizable patch-based neural rendering, in ECCV, 2022. [419] H. Ni, C. Shi, K. Li et al., Conditional image-to-video generation with latent flow diffusion models, in CVPR, 2023. [420] J. Zeng, L. Qiu, F. Li et al., Make-it-3d: High-fidelity 3d creation from single image with diffusion prior, in ICCV, 2023. [421] A. Cao and J. Johnson, Hexplane: fast representation for dynamic scenes, in CVPR, 2023. [422] X. Liu, Y. Xiao, D. Y. Chen et al., Trace anything: Representing any video in 4d via trajectory fields, arxiv, 2025. [423] A. Yu, S. Fridovich-Keil, M. Tancik et al., Plenoxels: Radiance fields without neural networks, in CVPR, 2022, pp. 55015510. [424] S. Singh, S. Abu-El-Haija, N. Johnston et al., End-to-end learning of compressible features, in NeurIPS, vol. 35, 2022, pp. 11 75011 762. [425] S. Fridovich-Keil, G. Meanti, F. R. Warburg et al., K-planes: Explicit radiance fields in space, time, and appearance, in CVPR, 2023. [426] R. Shao, Z. Zheng, H. Tu et al., Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering, in CVPR, 2023."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[427] Z. Wu, C. Yu, Y. Jiang et al., Sc4d: Sparse-controlled video-to-4d generation and motion transfer, in ECCV, 2024. [428] M. Zwicker, H. Pfister, J. Van Baar et al., Surface splatting, in ACM SIGGRAPH, 2001, pp. 371378. [429] C. Lassner and M. Zollhofer, Pulsar: Efficient sphere-based neural rendering, in CVPR, 2021, pp. 1440 1449. [430] Y.-H. Huang, Y.-T. Sun, Z. Yang et al., Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes, in CVPR, 2024. [431] C. Chen, S. Huang, X. Chen et al., Ct4d: Consistent text-to-4d generation with animatable meshes, arxiv, 2024. [432] N. Karaev, Y. Makarov, J. Wang et al., Cotracker3: Simpler and better point tracking by pseudo-labelling real videos, in ICCV, 2025. [433] R. Villegas, J. Yang, S. Hong et al., Decomposing motion and content for natural video sequence prediction, in ICLR, 2017. [434] R. Villegas, J. Yang, Y. Zou et al., Learning to generate long-term future via hierarchical prediction, in ICML, 2017. [435] J. Wei, Y. Tay, R. Bommasani et al., Emergent abilities of large language models, TMLR, 2022. [436] A. Ramesh, P. Dhariwal, A. Nichol et al., Hierarchical text-conditional image generation with clip latents, arxiv, vol. 1, no. 2, p. 3, 2022. [437] J. Gu, S. Wang, H. Zhao et al., Reuse and diffuse: Iterative denoising for text-to-video generation, arxiv, 2023. [438] J. Bruce, M. D. Dennis, A. Edwards et al., Genie: Generative interactive environments, in ICML. PMLR, 2024, pp. 46034623. [439] D. Valevski, Y. Leviathan, M. Arar et al., Diffusion models are real-time game engines, in ICLR, 2024. [440] Runway, Introducing gen-3 alpha: new frontier for video generation, https://runwayml.com/research/ introducing-gen-3-alpha, 2024, accessed: 2025-02-24. [441] HailuoAI, Hailuo, https://hailuoai.video/, 2024, accessed: 2025-02-24. [442] Y. HaCohen, N. Chiprut, B. Brazowski et al., Ltx-video: Realtime video latent diffusion, arxiv, 2024. [443] X. He, C. Peng, Z. Liu et al., Matrix-game 2.0: An open-source real-time and streaming interactive world model, arxiv, 2025. [444] H. Team, Z. Wang, Y. Liu et al., Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels, arxiv, 2025. [445] M. Assran, Q. Duval, I. Misra et al., Self-supervised learning from images with joint-embedding predictive architecture, in CVPR, 2023, pp. 15 61915 629. [446] D. Hafner, J. Pasukonis, J. Ba et al., Mastering diverse domains through world models, in arxiv, 2024. [447] J. Cen, C. Yu, H. Yuan et al., Worldvla: Towards autoregressive action world model, arxiv, 2025. [448] H. Zhen, X. Qiu, P. Chen et al., 3d-vla: 3d vision-language-action generative world model, in arxiv, 2024. [449] Y. Hu, J. Yang, L. Chen et al., Planning-oriented autonomous driving, in CVPR, 2023. [450] X. Tian, J. Gu, B. Li et al., Drivevlm: The convergence of autonomous driving and large vision-language models, in ECCV, 2024. [451] X. Li, K. Hsu, J. Gu et al., Evaluating real-world robot manipulation policies in simulation, arxiv, 2024. [452] J. Gu, F. Xiang, X. Li et al., Maniskill2: unified benchmark for generalizable manipulation skills, arxiv, 2023."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[453] L. Wang, Y. Ling, Z. Yuan et al., Gensim: Generating robotic simulation tasks via large language models, arxiv, 2023. [454] J. Pearl, Causality. Cambridge university press, 2009. [455] Y. Bengio, From system 1 deep learning to system 2 deep learning, in NeurIPS, 2019. [456] R. Firoozi, J. Tucker, S. Tian et al., Foundation models in robotics: Applications, challenges, and the future, arxiv, 2023. [457] Z. Ji, N. Lee, R. Frieske et al., Survey of hallucination in natural language generation, ACM Comput. Surv., vol. 55, pp. 138, 2023. [458] B. Goertzel, Artificial general intelligence: Concept, state of the art, and future prospects, J. Artif. Gen. Intell., vol. 5, no. 1, pp. 148, 2014. [459] G. Marcus, The next decade in ai: Four steps towards robust artificial intelligence, arxiv, 2020. [460] D. Bear, E. Wang, D. Mrowca et al., Physion: Evaluating physical prediction from vision in humans and machines, in NeurIPS, 2021. [461] J. Mao, C. Gan, P. Kohli et al., The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision, in ICLR, 2019. [462] V. Saxena, J. Ba, and D. Hafner, Clockwork variational autoencoders, NeurIPS, vol. 34, pp. 29 24629 257, 2021. [463] X. Pan, A. Tewari, T. Leimkuhler et al., Drag your gan: Interactive point-based manipulation on the generative image manifold, in ACM SIGGRAPH. ACM, 2023, pp. 111. [464] T. Brooks, A. Holynski, and A. A. Efros, Instructpix2pix: Learning to follow image editing instructions, in CVPR, 2023, pp. 18 39218 402. [465] Y. Hu, L. Anderson, T.-M. Li et al., Difftaichi: Differentiable programming for physical simulation, in ICLR, 2020. [466] J. S. Park, J. OBrien, C. J. Cai et al., Generative agents: Interactive simulacra of human behavior, in ACM UIST, 2023, pp. 122. [467] J. Leibo, V. Zambaldi, M. Lanctot et al., Multi-agent reinforcement learning in sequential social dilemmas, in AAMAS, vol. 16. ACM, 2017, pp. 464473. [468] Y. Shoham and K. Leyton-Brown, Multiagent systems, Cambridge Books, 2009. [469] D. Silver, T. Hubert, J. Schrittwieser et al., general reinforcement learning algorithm that masters chess, shogi, and go through self-play, Science, vol. 362, no. 6419, pp. 11401144, 2018. [470] W. Hong, W. Wang, Q. Lv et al., Cogagent: visual language model for gui agents, in CVPR, 2024, pp. 14 28114 290. [471] Z. Xi, W. Chen, X. Guo et al., The rise and potential of large language model based agents: survey, Sci. China Inf. Sci., vol. 68, no. 2, p. 121101, 2025. [472] L. Wang, C. Ma, X. Feng et al., survey on large language model based autonomous agents, Front. Comput. Sci., vol. 18, no. 6, p. 186345, 2024. [473] Y. Liang, W. Chow, F. Li et al., Rover: Benchmarking reciprocal cross-modal reasoning for omnimodal generation, arxiv, 2025. [474] Y. Niu, W. Jin, J. Liao et al., Does understanding inform generation in unified multimodal models? from analysis to path forward, arxiv, 2025. [475] Y. Niu, M. Ning, M. Zheng et al., Wise: world knowledge-informed semantic evaluation for text-to-image generation, arxiv, 2025. [476] T. Zhang, H.-X. Yu, R. Wu et al., Physdreamer: Physics-based interaction with 3d objects via video generation, in ECCV, 2024."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[477] C. Yang, H. Wan, Y. Peng et al., Reasoning via video: The first evaluation of video models reasoning abilities through maze-solving tasks, arxiv, 2025. [478] Z. Huang, Y. He, J. Yu et al., Vbench: Comprehensive benchmark suite for video generative models, in CVPR, 2024. [479] F. Meng, J. Liao, X. Tan et al., Towards world simulator: Crafting physical commonsense-based benchmark for video generation, arxiv, 2024. [480] H. H. Chen, D. Lan, W.-J. Shu et al., Tivibench: Benchmarking think-in-video reasoning for video generative models, arxiv, 2025. [481] Z. Guo, X. Chen, R. Zhang et al., Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark, arxiv, 2025. [482] W. Chow, J. Pan, Y. Liang et al., Weave: Unleashing and benchmarking the in-context interleaved comprehension and generation, arxiv, 2025. [483] J. Tong, Y. Mou, H. Li et al., Thinking with video: Video generation as promising multimodal reasoning paradigm, arxiv, 2025. [484] Y. Luo, X. Zhao, B. Lin et al., V-reasonbench: Toward unified reasoning benchmark suite for video generation models, arxiv, 2025. [485] J. Wei, C. Jia, X. Bai et al., Ggbench: geometric generative reasoning benchmark for unified multimodal models, arxiv, 2025. [486] C. Holtermann, N. Krebs, and A. Lauscher, Tempviz: On the evaluation of temporal knowledge in text-to-image models, arxiv, 2026. [487] M. Heusel, H. Ramsauer, T. Unterthiner et al., Gans trained by two time-scale update rule converge to local nash equilibrium, NeurIPS, vol. 30, 2017. [488] A. Borji, Pros and cons of gan evaluation measures, CVIU, vol. 179, pp. 4165, 2019. [489] S. Tong, Z. Liu, Y. Zhai et al., Eyes wide shut? exploring the visual shortcomings of multimodal llms, in CVPR, 2024. [490] Z. Shao, P. Wang, Q. Zhu et al., Deepseekmath-v2: Towards self-verifiable mathematical reasoning, arxiv, 2025. [491] N. Carlini, J. Hayes, M. Nasr et al., Extracting training data from diffusion models, in USENIX SEC, 2023, pp. 52535270. [492] L. S. Piloto, A. Weinstein, P. Battaglia et al., Intuitive physics learning in deep-learning model inspired by developmental psychology, Nat. Hum. Behav, vol. 6, no. 9, pp. 12571267, 2022. [493] K. Yi, C. Gan, Y. Li et al., Clevrer: Collision events for video representation and reasoning, in ICLR, 2021. [494] V. Voleti, A. Jolicoeur-Martineau, and C. Pal, Mcvd-masked conditional video diffusion for prediction, generation, and interpolation, NeurIPS, vol. 35, pp. 23 37123 385, 2022. [495] H. Lightman, V. Kosaraju, Y. Burda et al., Lets verify step by step, in ICLR, 2023. [496] G. E. Karniadakis, I. G. Kevrekidis, L. Lu et al., Physics-informed machine learning, Nat. Rev. Phys., vol. 3, no. 6, pp. 422440, 2021. [497] O. Ahmed, F. Trauble, A. Goyal et al., Causalworld: robotic manipulation benchmark for causal structure and transfer learning, in ICLR, 2021. [498] J. Hessel, A. Holtzman, M. Forbes et al., Clipscore: reference-free evaluation metric for image captioning, in EMNLP, 2021, pp. 75147528. [499] J. Tobin, R. Fong, A. Ray et al., Domain randomization for transferring deep neural networks from simulation to the real world, in IEEE/RSJ IROS. IEEE, 2017, pp. 2330. [500] D. Silver, S. Singh, D. Precup et al., Reward is enough, AI, vol. 299, p. 103535, 2021."
        },
        {
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "content": "[501] OpenAI, GPT-Image-1: Image Generation API, https://openai.com/index/image-generation-api/, 2025, accessed: 2025-01. [502] T. Seedream, Y. Chen, Y. Gao et al., Seedream 4.0: Toward next-generation multimodal image generation, arxiv, 2025. [503] G. Comanici, E. Bieber, M. Schaekermann et al., Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, arxiv, 2025. [504] OpenAI, GPT-Image-1.5: New ChatGPT Image Generation, https://openai.com/zh-Hans-CN/index/ new-chatgpt-images-is-here, 2025, accessed: 2025-01. [505] R. Mroczkowski, P. Rybak, A. Wr oblewska et al., Herbert: Efficiently pretrained transformer-based language model for polish, in BSNLP, 2021, pp. 110. [506] B. Wu, C. Zou, C. Li et al., Hunyuanvideo 1.5 technical report, arxiv, 2025. [507] J. Xu, X. Zou, K. Huang et al., Easyanimate: high-performance long video generation method based on transformer architecture, arxiv, 2024. [508] D. Li, Z. Fei, T. Li et al., Skyreels-v3 technique report, arxiv, 2026. [509] C. Wu, J. Li, J. Zhou et al., Qwen-image technical report, arxiv, 2025. [510] C. Wei, Q. Liu, Z. Ye et al., Univideo: Unified understanding, generation, and editing for videos, arxiv, 2025. [511] Y. Cui, H. Chen, H. Deng et al., Emu3. 5: Native multimodal models are world learners, arxiv, 2025. [512] H. Al-Tahan, Q. Garrido, R. Balestriero et al., Unibench: Visual reasoning requires rethinking visionlanguage beyond scaling, Advances in Neural Information Processing Systems, vol. 37, pp. 82 41182 437, 2024. [513] H. Zhou, Q. Xu, Y. Dong et al., Manbench: Is your multimodal model smarter than human? arxiv, 2025. [514] J. Parker-Holder, P. Ball, J. Bruce et al., Genie 2: large-scale foundation world model, https://deepmind. google/discover/blog/genie-2-a-large-scale-foundation-world-model, 2024, accessed: 2026-02. [515] Google DeepMind, Genie 3: real-time interactive world model, 2025, technical Report. [516] PixVerse Research, Pixverse-r1: Next-generation real-time world model, https://pixverse.ai/en/blog/ pixverse-r1-next-generation-real-time-world-model, 2026, technical Report on real-time multimodal world model for interactive video generation."
        }
    ],
    "affiliations": []
}