{
    "paper_title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer",
    "authors": [
        "Weixian Lei",
        "Jiacong Wang",
        "Haochen Wang",
        "Xiangtai Li",
        "Jun Hao Liew",
        "Jiashi Feng",
        "Zilong Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 2 6 4 0 1 . 4 0 5 2 : r The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with Single Transformer Weixian Lei* Jiacong Wang* Haochen Wang* Jun Hao Liew Jiashi Feng Zilong Huang Equal contribution, Project Lead Bytedance Seed"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces SAIL, single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within singular architecture. Unlike existing modular MLLMs, which rely on pre-trained vision transformer (ViT), SAIL eliminates the need for separate vision encoder, presenting more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mixattention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAILs propertiesincluding scalability, cross-modal information flow patterns, and visual representation capabilitieswith those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAILs scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available1. 1. Introduction intelligence has driven the The pursuit of multimodal development of Multimodal Large Language Models (MLLMs) [49, 61, 68], which typically adopt moda pre-trained vision encoder (e.g., CLIPular design: ViT [15, 62]) extracts image features, Large Language Model (LLM) [2, 16, 38, 5860] processes text, and lightweight projector aligns the two modalities. This framework achieves strong performance through multi-stage pretraining, supervised fine-tuning (SFT), and post-training on multimodal datasets [3, 18, 49, 76, 77, 91]. While effective, 1https://github.com/bytedance/SAIL Figure 1. (A) Data scaling curve for Modular Multimodal Large Language Model (MLLM) and SAIL, our Single Transformerbased MLLM. As pretraining data increases, the single transformer SAIL shows sharper performance gain, demonstrating its superior data scalability. (B) Comparison to existing Single Transformer-based MLLMs: our SAIL pushes the performance boundaries on both vision tasks and vision-language tasks. this modular MLLM paradigm inherently fragments multimodal processing, reinforces reliance on pretrained visual encoders, which may limit deployment flexibility and scalability [11, 21, 54]. promising alternative is to eliminate the visual encoder entirely and process raw image patches and text tokens within single Transformer. This unified architecture removes modality-specific modules, enabling parameter sharing and end-to-end learning of vision-language interactions. Previous works [11, 21, 54] have primarily explored the architecture design, training data, and methods of Single Transformer-based MLLMs. However, little exploration has been given to their fundamental properties, such as scalability, cross-modal information flow patterns, and visual representation capabilities. deeper understanding of these properties is crucial for unlocking the full potential of Single Transformer-based MLLMs. In this work, we present an experimental analysis of the fundamental properties of Single Transformer-based MLLMs and compare them to modular MLLMs (e.g. 1 LLaVA [49]). Addtionally, in the absence of pre-trained visual encoder, Single Transformers have to learn visual representations from scratch. Thus, an intriguing question arises: can trained Single Transformer emerge as strong vision encoder? We conduct series of experiments to train and study our Single trAnsformer model for vIsion and Language (SAIL). While we do not propose novel architecture designs, we introduce necessary modifications to enable the model to process different modalities in unified architecture. In its micro architecture design, we address the different spatial characteristics of 2D images and 1D text data by employing mixed attention mechanism: bidirectional attention for image patches and causal attention for text tokens, combined with multimodal rotary position embedding. Through model and data scaling, SAIL achieves performance on vision-language benchmarks comparable to modular MLLMs, while also functioning as highperforming vision backbone, as shown in Figure 1. More concretely, our empirical analysis uncovers following striking advantages of Single Transformer architectures: (i) Superior Data Scaling: In controlled experiments, SAIL exhibits steeper performance gains as pretraining data scales. While LLaVA-style modular MLLMs initially perform well, our models performance becomes very close to theirs when pretrained on 512M samples, as shown in Figure 1(A). This suggests that unified architectures can effectively leverage large-scale data and potentially match the performance of modular MLLMs. (ii) Vision Centric Information Flow Pattern: Through analysis of attention distributions, we observe that Single Transformers assign significantly higher attention scores to image tokens during token prediction compared to modular MLLMs. This indicates that the information flow in Single Transformer MLLMs is more direct, with visual tokens influencing prediction tokens more prominently, highlighting vision-centric approach to decision-making. (iii) Vision Encoder Functioning: Our experiments further demonstrate that the pretrained Single Transformer inherently serves as powerful vision encoder. Comprehensive evaluations on vision-centric tasks, such as image classification and semantic segmentation, show that the model learns rich visual representations during multimodal pretraining. These representations enhance its capacity for both semantic-level comprehension (e.g., object categorization) and pixel-level understanding (e.g., fine-grained segmentation masks), bridging high-level abstraction and lowlevel visual reasoning within unified architecture. our findings In summary, that Single Transformer-based MLLMs hold great promise in surpassing modular MLLMs in terms of leveraging large-scale data, forming direct vision-centric information pathways, and functioning as effective vision encoders. We hope indicate our empirical findings inspire further research to refine and enhance Single Transformer architecture, ultimately driving advancements in multimodal intelligence from new perspective. 2. Related Work 2.1. Paradigms in Vision-Language Model Design Modular MLLMs with Visual Encoders. The prevailing approach in MLLM design employs modular architectures [3, 49, 61, 68, 77] that rely on pretrained vision encoders (e.g., CLIP-ViT [15, 62], InternViT [14]) to process visual inputs. The visual features extracted from these frozen encoders are then aligned with LLM input spaces via linear [7, 49, 75, 91] or cross-attention layers [1, 33]. While this module design enables effective transfer of pretrained visual-language knowledge, it also introduces several limitations. First, incorporating separate ViT encoder significantly slows down both training and inference, increasing deployment complexity and requiring costly infrastructureespecially when compared to single transformer unified model. Second, common strategies for integrating visual features, such as direct mapping into LLM inputs [7, 49, 91] or sharing them across LLM layers [1, 33], often struggle to reconcile the inherent differences between images and text representations. Finally, as model scale, balancing the interactions between the the encoder, LLM, and alignment layers becomes increasingly challenging [11, 54]. Thus, in this work, we explore single transformer-based MLLM architecture that eliminates the ViT encoder and alignment components to overcome these challenges. Single Transformer-based MLLMs Without Visual Encoders. Emerging research explores end-to-end architectures that process raw image patches and text tokens through single Transformer, bypassing visual encoders entirely. These monolithic designs fall into two categories: continuous tokenization and discrete tokenization. Continuous tokenization, exemplified by Fuyu-8B [5] and SOLO [11], directly maps patches to LLM embeddings via linear projections, enabling flexible resolution handling but requiring massive pretraining data. Discrete tokenization, adopted by Chameleon [67] and Emu3 [79], employs VQ-VAE tokenizers to compress images into discrete tokens, trading pixel-level fidelity for generation capabilities. While later efforts such as EVE [21] and MonoInternVL [54] demonstrate the feasibility of encoder-free training, critical gaps remain: (1) Existing methods rely on extra designs and auxiliary loss [21], complicating training pipelines; (2) The scaling laws and fundamental properties of purely end-to-end trained models remain poorly understood; (3) Vision-language interaction in shared parameter spaces lacks systematic analysismost prior MLLMs 2 default to causal attention for processing image-text sequences. In this work, we reveal that enabling bidirectional attention between image patches significantly enhances visual representation learning, addressing key limitation in current designs. More importantly, our study bridges these gaps by establishing foundational principles for training scalable, self-contained single-transformer MLLMs. 2.2. Vision Representation Learning Learning effective vision representations is core challenge in computer vision research, with extensive works [4, 19, 20, 26, 31, 80] dedicated to this problem. With the proliferation of large-scale web-sourced image-text datasets [6, 27, 63], recent methods leverage this data to train deep visual representations via three primary paradigms: Text as Classification Labels. Early methods used textual descriptions as weak supervision by extracting categorical labels from captions. For example, frameworks like Tag2Text [34] and RAM [89] used ViTs [23] to predict noun-based pseudo-labels from datasets like CC12M [6]. CatLIP [57] scaled labels to millions using object-centric supervision, and SuperClass [35] directly used tokenized text tokens as classification categories. Image-Text Contrastive Learning Contrastive pretraining, as exemplified by CLIP [15, 62] and ALIGN [37], aligns global image-text embeddings within shared latent space. Subsequent works [8, 14, 42, 45, 78, 87] focused on enhancing CLIPs performance and improving training efficiency. Text as Autoregressive Targets. Caption generation as pretext task is another approach for visual representation learning. SimVLM [80] trains encoder-decoder architectures to autoregressively predict captions, while CapPa [71] trains vision encoders through sequence prediction. These methods often retain modular designs or auxiliary components like contrastive losses [83]. Our work aligns with this category but removes architectural fragmentation by jointly modeling image patches and text tokens in single Transformer. We find that the pre-trained Single Transformer learns transferable vision representations, enabling it to handle downstream multimodal understanding tasks and function as vision encoder without modifications. 3. SAIL: Training Single Transformer for Vision and Language 3.1. Model Architecture SAIL is built upon unified Transformer architecture (Figure 2(A)) that processes multimodal inputs through streamlined, modality-specific preprocessing. For text, raw input is tokenized using the language models tokenizer and then transformed into embeddings via the textual embedding module. For images, we partition the input into fixedsize patches and project them into continuous embeddings via linear projection. Additionally, we maintain list of special tokens explicitly designed for visual modality encoding: <vision> and </vision> tokens mark the beginning and end of an image patch span, respectively. In multimodal scenarios, such as image-text pairs, these embeddings are concatenated into single sequence and fed into the Transformer, enabling joint cross-modal interactions through unified self-attention layers. This design eliminates the need for modality-specific encoders, which efficiently processess heterogeneous data within singletransformer framework. Bidirectional attention within image patches. While existing multimodal large language models (MLLMs) [41, 48, 49, 91] predominantly adopt causal attention for autoregressive sequence modeling, our experiments reveal that enabling full bidirectional attention among tokens from the same image significantly enhances visual representation learning and boosts downstream vision-language task performance. Note that previous Single Transformer works [11, 21, 54, 79] have only utilized causal attention, without exploring the potential of mixed attention mechanisms. As illustrated in Figure 2(B), for SAIL we implement mixed attention scheme: (1) For text tokens, we preserve causal attention to maintain autoregressive generation capabilities, allowing each token to attend only to its predecessors. (2) For image tokens, we activate full bidirectional attention within each image patch group, empowering every visual token to interact with all others in the same image. This design captures holistic spatial relationships and contextual dependencies among visual elements, addressing the under-explored potential of attention mechanisms in cross-modal alignment. The improved interaction paradigm not only refines vision-language feature fusion but also provides stronger inductive biases for complex reasoning tasks. Multimodal Rotary Position Embeddings. Following [77], we implement Multimodal RoPE (M-RoPE) in SAIL to harmonize positional modeling for multimodal inputs. The method decomposes positional encoding into two axes: height, and width. For text tokens, all axes share uniform position IDs (aligned with 1D-RoPE), whereas for images, height/width IDs adaptively map to token coordinates, as is shown in Fig 2(C). Notably, position indexing is sequentially initialized across modalities (e.g., starting from images before extending to subsequent text), preserving inter-modal consistency. M-RoPE not only improves positional sensitivity but also constrains absolute position magnitudes for visual tokens, facilitating robust generalization to extended sequences in inference. 3 Figure 2. Model architecture and micro-designs for SAIL. (A) Model Architecture: SAIL is unified transformer that processes both images and texts without extra module designs. (B) Mixed Attention Mechanism: we adopt bidirectional attention for image patches from the same image and causal attention for text tokens. Examples for multimodal sequence and text sequence are provided. Colored squares represent allow to attend and white squares indicate prevent from attending. (C) Multimodal RoPE: an illustration of the multimodal rotary position embedding for SAIL, with examples for multimodal sequence and text sequence. 3.2. Pretraining apply two-stage We curriculum to progressively strengthen the visual perception of SAIL while preserving its inherent language capabilities. Stage 1: Accelerated Visual Knowledge Acquisition. In this stage, we pretrain SAIL on large-scale image-text pairs to rapidly bootstrap its visual understanding. To maximize data throughput, we uniformly resize all images to lower resolution (e.g., 224 224), reducing multimodal sequence lengths and enabling the model to process more samples within fixed training time. To prevent catastrophic forgetting of linguistic knowledge, we interleave pure text corpora with multimodal data during training. This hybrid approach ensures efficient exposure to visual patterns while maintaining robust language proficiency. Stage 2: Enhancing Any-Resolution Image Understanding. Real-world applications require robustness to images of varying resolutions and aspect ratios, such as documents, charts, or infographics. Following prior works [11, 21], we extend pretraining with an any-resolution strategy: images retain their native resolutions during processing, and positional embeddings adapt dynamically to arbitrary spatial dimensions. This stage further refines SAIL ability to model fine-grained visual details (e.g., tabular structures, text-rich graphics) while continuing to incorporate text-only data for language capability preservation. Pretraining Objective. Throughout both stages, we optimize the standard language modeling loss only on text tokens. Image patches and special visual tokens are excluded from loss computation. 3.3. Supervised Fine-tuning During the Supervised Fine-tuning (SFT) stage, we train SAIL on publicly available, multi-source instruction datasets to enhance its understanding of complex linguistic instructions and diverse dialogue patterns critical for realworld deployment. This phase fine-tunes the entire network architecture, focusing on aligning the models responses with human intent through exposure to varied instructional formats and multimodal interactions. Table 1 shows the details of training datasets for pretraining and supervised fine-tuning (SFT) across all stages. During Stage 1 pretraining, we utilized mixed multimodal and pure text datasets including Recap-DataComp-1B [43] and SlimPajama [66], with images at resolution of 224x224, totaling 512M image-text pairs. In Stage 2, the pretraining datasets include Capfusion [84], self-curated OCR data from LAION COCO [63], InfinityMM Stage 2 subset [29], and SlimPajama, utilizing the any resolution (AnyRes) strategy, with combined total of 86M image-text pairs along with text data. The SFT stage employed the InfinityMM Stage 3 subset, processed at any resolution, containing 6M image-text pairs. 4 Stage Dataset Img.Res Num Pretraining S1 Pretraining SFT Recap-DataComp-1B [43] SlimPajama [66] Capfusion [84] OCR from LAION COCO [63] InifinityMM Stage 2 subset [29] SlimPajama [66] InifinityMM Stage3 [29] 224x224 AnyRes AnyRes 512M - 60M 7M 19M - 6M Table 1. Details of training datasets used across all stages. Img.Res refers to the image resolution settings applied during each training stage. All datasets listed are publicly available. Note that these settings represent the default configuration for standard SAIL training, while separate settings are used for scaling experiments and ablation studies. 4. Experiment 4.1. Experimental Settings Evaluation Benchmarks. For evaluation of vision and language tasks, we evaluate SAIL and existing MLLMs on broad range of multimodal benchmarks. Specifically, MLLM benchmarks encompass MMBenchEN [50], SEEDBench-IMG [40], MMVet [85], MME [46], HallusionBench [30], MathVistaMINI [53], and OCRBench [51]. Visual question answering benchmarks include TextVQA [65], ScienceQA-IMG [52], AI2D [39], MMStar [9], RealWorldQA [81]. For evaluation of vision representation learning, we conduct experiments on ImageNet-1K [20] for image classification, ADE20K for semantic segmentation [90], and ARO [86] for attribute, relation, and ordering. Implementation Details. For pretraining, we initialize SAIL from the Mistral-7B-v0.1 base LLM and set the patch size to 14. We modify Megatron [64] to support SAILs multimodal input. Pretraining uses 128 NVIDIA A100 80G GPUs with 2-way tensor parallelism and 64-way data parallelism. The learning rate is set at 5e-5 and decays cosinely to minimum of 5e-6. For training efficiency, we concatenate sequences from different data samples into one long sequence of 32,768 tokens, adjusting the attention mask to ensure that tokens from different samples do not attend to each other. We use round-robin approach to interleave image-text packed sequences and pure text packed sequences, configuring the global batch to contain approximately 16K image-text pairs. For Supervised Fine-Tuning (SFT), the global batch size is set to 512. Training is performed for one epoch with maximum learning rate of 1e-5, following linear warm-up phase and then transitioning to cosine decay schedule. For vision, we load the checkpoint after Stage 1 pretraining and keep it frozen for downstream evaluations, including (1) image classification on ImageNet-1K [20], (2) semantic segmentation on ADE20K [90], and (3) attribute, relation and, ordering on the ARO benchmark [86]. Specifically, (1) for image classification, we utilize an attentionbased classifier [25] with 90 epochs of linear probing, where detailed configurations are mostly obtained from common practices [32, 73, 74]. Images are resized to 224224 and the global batch size is 8,192 across 8 A100 (80G) GPUs. (2) For semantic segmentation, we adopt ViT-Adapter [12] with UperNet [82] as the segmentation decoder. The implementation is based on MMSegmentation [17] with 80k training iterations. The input resolution is 512512 and the global batch size is 16 across 8 A100 (80G) GPUs. (3) For attribute, relation and, ordering, we regard the negative of the caption loss over each image-text pair as the similarity metric for retrieval. 4.2. Experimental Results 4.2.1. Results on Vision Language Tasks As shown in Table 2, we compare SAIL against existing MLLMs across 13 vision-language benchmarks. SAIL consistently outperforms other Single Transformer-based models like Fuyu [5], EVE [21], SOLO [11], MonoInternVL [54], and EVE2 [22] across diverse visionlanguage tasks. This demonstrates that SAIL can achieve significant performance gains and push the boundaries of Single Transformer-based MLLMs without needing extra component designs or auxiliary training losses. Moreover, when compared to methods employing discrete vision tokens (e.g., Chameleon and Emu3), SAIL demonstrates superior performance. These results validate that scaling up single-transformer pretraining effectively enhances crossmodal alignment between images and text. Compared to the state-of-the-art modular MLLM LLaVA-OneVision [41], SAIL achieves comparable performance on some benchmarks, such as MMStar, SEEDBench-IMG, and RealWorldQA. While the performance of Single Transformerbased MLLMs still lags behind modular MLLMs in certain areas, we hypothesize that scaling the pretraining data volume or incorporating higher-quality instruction-tuning data will bridge the remaining performance gap. 4.2.2. Results on Vision Representation Learning In this section, we compare the quality of learned visual representations of our SAIL with other Single Transformerbased alternatives, including EVE [21], EVE2 [22], and SOLO [11]. Classification and Segmentation. As demonstrated in Table 3, our method, SAIL, achieves Top-1 accuracy of 84.95% and Top-5 accuracy of 97.59% on the validation set of ImageNet-1K [20], significantly outperforming stateof-the-art alternatives [11, 21, 22]. In the segmentation task, SAIL also demonstrates superior performance with an mIoU of 55.30%, an mAcc of 67.24%, and an aAcc of 84.87%, illustrated in Table 3. These results indicate that SAIL is effective in both classification and segmentation tasks, of5 Method #Param #Data #Vtoken General VQA Hallucination Math&knowledge OCR VQA MMS* MMBen SEEDI MMV MME RWQA POPE Hallu SQAI MathV TQA AI2D OCRB Modular MLLMs: InternVL-1.5 [13] QwenVL-Chat [3] LLaVA-1.5 [47] LLaVA-1.6 [48] Cambrian-1 [69] LLaVA-OneVision [41] 2.2B 7B 7B 7B 8B 7B 3328 / 7.2B / 50M 256 0.4B+ / 665K 576 0.4B+ / 760K 2880 10B+ / 7M 576 10B+ / 3.2M 7290 Single Transformer-based MLLMs: Fuyu [5] Chameleon [67] EVE [21] SOLO [11] Mono-InternVL [54] Emu3 [79] EVE2 [22] SAIL 8B 7B 7B 8B 3B 8B 7B 7B / 1.4B+ / 1.8M 1024 33M / 1.8M 2304 43.7M / 2M 1024 6400 1.3B / 7M 16K / 92M / 7.3M 2500 600M / 6M 46.7 34.5 33.1 37.6 50.7 60.9 34.4 31.1 35.8 46.6 53.1 70.9 60.6 64.3 67.4 75.9 81.7 10.7 31.1 52.3 67.7 65.5 58.5 66.3 70.1 69.8 58.2 64.3 64.7 74.7 74.8 59.3 30.6 64.6 64.4 67.4 68.2 71.4 72. 39.3 1902 1848 30.5 1859 43.9 1842 58.8 1998 21.4 8.3 170 25.7 1628 30.4 1260 40.1 1875 37.2 45.0 1709 46.3 57.9 49.3 54.8 57.8 64.2 65.5 43.7 39 44.7 57.4 62.4 63.9 88.3 85.9 86.4 84 19.4 85.0 78.6 85.2 87.6 85.8 37.3 36.8 27.6 27.6 30.6 29.8 17.1 40.4 45.7 31.7 54.2 84.9 68.2 66.8 70.2 80.4 96.6 56.8 47.2 64.9 73.3 93.6 89.2 96.2 93.3 41.3 35.3 25.5 32.5 48.1 56.1 30.2 22.5 32.9 45.7 31.3 57.0 70.5 69.8 61.5 45.9 46.1 54.8 64.9 66.6 71.7 73.0 81. 46.8 4.8 46.0 56.8 61.0 25.0 61.4 72.6 68.6 64.7 70.0 71.1 74.8 77.1 76.7 654 488 318 532 366 7.0 398 126 767 687 702 783 Table 2. Comparison with existing vision-language models on various vision-language benchmarks, including MMS*: MMStar [9]; MMBen: MMBench-EN [50]; SEEDI: SEEDBench-Img [40]; MMV: MMVet [85]; MME [46]; POPE [44]; Hallu: HallusionBench [30]; SQAI: ScienceQA-Img [52]; TVQA: TextVQA [65]; MathV: MathVistaMINI [53]; AI2D [39]; RWQA: RealWorldQA [81]; OCRB: OCRBench [51]. Note that #A-Param denotes the number of activated parameters; #Data represents the pre-training / fine-tuning data volume; #Vtoken indicates the maximum image patch tokens. For MME, we report the sum of perception and cognition scores. The top two results are highlighted in bold and underline, respectively. All results are derived from those reported in other papers and the official reproduction results from the OpenCompass leaderboard [24]. Our results are obtained by VLMEvalKit [24]. Method Classification Segmentation Top-1 Top-5 mIoU mAcc aAcc EVE [21] EVE2 [22] SOLO [11] SAIL 42.03 44.86 59.10 84.95 65.77 69.41 80.89 97.59 27.12 40.85 35.11 55.30 35.89 53.53 44.81 67. 72.91 79.31 76.02 84.87 Table 3. Comparison on image classification and semantic segmentation with other encoder-free approaches. Our SAIL outperforms other alternatives by large margin. Method Relation Attribute Order COCO Flickr30K OpenCLIP-H [15] OpenCLIP-G [15] CLIP-B/32 [62] CLIP-L/14 [62] InternViT [14] NegCLIP [86] CapPa [72] SAIL 49.9 49.9 59.2 61.2 59.6 81.0 86.7 100.0 64.6 65.6 62.9 61.7 66.0 71.0 85.7 99.5 32.6 33.0 48.1 46.8 73.4 86.0 98.8 100.0 40.4 38.3 57.9 56.8 76.3 91.0 99.2 100.0 Method #Data #Param ImageNet-1K ADE20K OpenCLIP-H [15] OpenCLIP-G [15] ViT-22B [19] InternViT [14] SAIL 2B 2B 3B 6B 0.5B 0.6B 1.8B 22B 6B 7B 84.4 86.2 89.5 88.2 85.0 39.3 55.3 58.7 55. Table 4. Comparison on image classification and semantic segmentation with other vision backbones. indicates training with head tuning using UperNet [82], while others are based on ViTAdapter [12]. SAIL, with significantly less training data, achieves competitive performance. fering substantial improvements over existing methods. In Table 4, even when comparing with other state-of-the-art vision backbones, our SAIL manages to achieve remarkable competitive performance with significantly less training data, demonstrating the scaling property of SAIL. Attribute, Relation, and Ordering. To systematically evaluate the ability of SAIL to understand different types of Table 5. Comparison on attribute, relation, and ordering (ARO) with other vision backbones. SAIL almost encodes compositional relationships between objects and attributes perfectly. relationships, attributes, and order information, we conduct experiments on the ARO benchmark [86]. As demonstrated in Table 5, SAIL encodes compositional relationships between objects and attributes almost perfectly, significantly surpassing other state-of-the-art vision backbones. For additional vision-related tasks, please refer to PixelSAIL [88] for SAILs downstream capabilities in pixelgrounded understanding. 4.3. Properties of Single Transformer 4.3.1. Scaling Properties. Model Scaling: We selected models of different sizes: SAIL-0.5B, SAIL-3B, and SAIL-7B (SAIL by default) for our experiments. Each model underwent Stage 1 pretraining on mixed multimodal and pure text dataset, encountering 512M image-text pairs. Subsequently, they were fine6 Figure 3. Model scaling of SAIL. Left: As the model size increases, the training language modeling loss gradually decreases. Right: As the model size increases, performance on downstream VLM tasks progressively improves. tuned on the LLaVA-mix-665K dataset using the any resolution (anyres) strategy. We evaluated the models based on their performance on vision and language benchmarks after supervised fine-tuning. The normalized performance of SAIL against model size is plotted in Figure 3. As the model size scales up, we observe corresponding enhancement in performance. Additionally, as shown on the left side of Figure 3, the training language modeling loss decreases with increasing model size. This reduction in training loss indicates that larger models have greater capacity to learn multimodal alignments effectively, enabling them to capture complex relationships between vision and language more accurately. The improved learning capacity directly translates to better performance on downstream VLM tasks, showcasing the benefits of scaling up the Single Transformer architecture. Data Scaling: we compared SAIL with its modular MLLM counterpart. For the modular MLLM, we used SigLIPSO [87] as the vision encoder, and the language model shared the same architecture and initialization parameters as SAIL. Both models were pre-trained using Pretraining stage-1 setting, with SAIL encountering 32M, 128M, and 512M image-text pairs during training, followed by finetuning on the LLaVA-mix-665K dataset. All parameters of both models are trainable. Both models employ an identical number of input tokens for images and text. The normalized performance of both models is plotted in Figure 1(A). The results show that in the low-data regime (32M), SAILs performance lags behind the modular MLLM, likely due to SigLIPs prior training on 40B samples. However, as the data scales, SAIL exhibits steeper performance curve, indicating more promising data scaling properties. At 512M image-text pairs, SAIL achieves performance comparable to the modular MLLM in our evaluation subset. This demonstrates the single transformers superior data scalability, even without pretrained vision encoder. Quantitative results on evaluated benchmark tasks of model scaling and data scaling are tabulated in the appendix. Figure 4. Image Attention Score Allocation: The figure shows the proportion of image attention scores across different transformer layers for Single Transformer-based MLLM and modular MLLM when predicting tokens. Single Transformer-based MLLM generally allocates higher attention weights to image tokens compared to modular MLLM. 4.3.2. Information Flow Pattern Different attention pattern compared to modular MLLM: since our comparative experiments show that the Single Transformer model exhibits more promising data scaling properties, we conducted an analysis of the trained SAIL model and its modular MLLM counterpart. Specifically, we followed the methodology from FastV [10] to analyze the attention score distribution for each predicted token given an image and user query. This analysis focuses on how much attention is allocated to image tokens during token prediction. We selected 1000 samples from various datasets including VQAv2, GQA, TextVQA, DocVQA, MME, SEEDBench-IMG, MMBench, and some self-curated dialog examples. For each model prediction, we computed the average attention scores assigned to previous image tokens by the output token. We conducted comparative experiment between Single Transformer-based MLLMs and modular MLLMs. The Single Transformer-based MLLMs included SAIL, SOLO [11], and EVE [21], while the modular MLLMs included Qwen2-VL [77], LLaVA-OneVision [41], and LLaVA1.5 [47]. The results are depicted in Figure 4. Single Transformerbased MLLMs allocate between 60% and 80% of attention scores to image tokens across all layers when predicting tokens. In contrast, modular MLLMs such as Qwen2-VL and LLaVA-OneVision allocate only 10% to 30% of attention scores to image tokens across different layers. For LLaVA1.5, which does not update the ViT parameters during supervised fine-tuning (SFT), the image attention score is relatively high in the first two transformer layers but declines sharply in subsequent layers. Method MMBench [50] MME [46] Physical Relation Celebrity Relation Position Posters Celebrity Modular MLLM 30.4 52.2 SAIL 50.5 88.9 98.3 160.0 134.0 108.2 100.3 75. Table 6. Performance Comparison of SAIL and Modular MLLM in MMBench and MME Tasks: the strengths of SAIL in spatial reasoning tasks (MMBench Physical Relation and MME Position split) and its weaknesses in world knowledge tasks (MMBench Celebrity Relation and MME Celebrity and Posters splits). Exp. Setting Default #1 No Img full attn #2 No pure text in PT VQAv2 GQA SQA TQA SEED-I 59.1 57.8 56.3 46.9 45.2 42.1 59.6 20.1 58.7 16.2 48.6 18. 35.1 33.8 32.4 Table 7. Ablation Study on Basic Factors for SAIL: This table presents the impact of different ablation settings on the performance of SAIL across VQAv2 [28], GQA [36], SQA [52], TQA [65], and SEED-I [40]. The default setting includes image full attention and the inclusion of pure text data in pretraining. Ablation #1 removes image full attention, and ablation #2 excludes pure text in pretraining. From this experiment, we can conclude that Single Transformer-based MLLMs tend to allocate significant portion of attention to previous image tokens during prediction. In contrast, modular MLLMs allocate smaller portion of their attention directly to image tokens, indicating less image-centric approach in their prediction mechanism."
        },
        {
            "title": "These findings indicate that",
            "content": "the Single Transformer model places more emphasis on grounding its predictions in the visual information. As the model undergoes data scaling, it allocates more effective computation to image tokens, thereby enhancing its capability as vision-centric model. In summary, the attention pattern analysis underscores the Single Transformers ability to robustly integrate visual context, enabling it to scale efficiently and potentially outperform modular MLLMs in vision-language tasks. 4.3.3. Task-Specific Performance Analysis We dissect SAIL strengths and limitations through targeted case studies: Strengths: Spatial Reasoning. SAIL excels at tasks requiring precise spatial location. As shown in Table 6, under the setting of our data scaling experiment, it outperforms the modular counterpart by 61.7 points on the MME Position split and 21.8% on MMBench Physical Relation questions. The unified architecture likely enables tighter coupling between visual geometry and linguistic descriptions. Weaknesses: World Knowledge. Conversely, SAIL falls short in tasks that demand extensive world knowledge. As shown in Table 6 SAIL underperforms in the MME celebrity and art splits compared to the modular MLLM. This underperformance can be attributed to SAILs lack of diverse domain-specific data during pretraining, gap that was not sufficiently addressed during supervised fine-tuning. Modular MLLMs, with their pretrained vision encoders like CLIP [15, 62] or SigLIP [87], have broader knowledge base and therefore handle such tasks more effectively. We hypothesize that scaling up SAIL pretraining data diversity could help bridge this gap, enhancing its performance on knowledge-intensive tasks. 4.4. Empirical Observations on Basic Factors To guide scalable training of single-transformer MLLMs, we conduct ablation studies on two critical design choices using SAIL-0.5B pretrained on 128M image-text pairs and fine-tuned on LLaVA-mix-665K. Performance is evaluated through zero-shot image classification after pretraining [71] and vision-language benchmarks after SFT. Bidirectional Attention for Image Patches with Multimodal Position Encoding. We compare two approaches for integrating image patches into the transformer: (1) Causal attention with 1D positional encoding, using <vision sep> token to demarcate image rows. (2) Full bidirectional attention for image patches paired with multimodal rotary position embeddings (RoPE), which jointly encode spatial coordinates (e.g., 2D grid positions) and text token positions. As shown in Table 7, configuration of using bidirectional attention with multimodal RoPE significantly improves performance on vision-language tasks, with particularly notable gain of 3.1% on TextVQA. This suggests that enabling cross-patch interactions during pretraining enhances visual representation learning and tightens crossmodal alignment. Interleaving Pure Text Data During Pretraining. We analyze the impact of mixing SlimPajama text data with image-text pairs during pretraining. The results, as presented in Table 7 #2, reveal that mixing in pure text data consistently improves performance across vision and language benchmarks. This finding underscores the importance of preserving language capabilities in the LLM when training Single Transformer models, as maintaining strong language skills is crucial for building multimodal model capable of complex reasoning. Currently, incorporating text data in training is one of the effective methods to maintain the language abilities of the model. In conclusion, our ablation studies identify key design choices for training SAIL effectively. Using bi-directional attention with multimodal rotary position embeddings enhances visual perception, while incorporating pure text data preserves essential language capabilities for robust multimodal performance. 8 5. Conclusion In this work, we conducted an extensive analysis of Single Transformer-based MLLMs compared to modular MLLMs. Our investigation explored the unique properties of Single Transformers, including scalability, cross-modal information flow patterns, and visual representation capabilities. series of experiments on our trained SAIL model demonstrated that this unified architecture achieves performance on vision-language benchmarks comparable to modular MLLMs while also functioning effectively as vision backbone. Our findings highlight several advantages of Single Transformer architectures, such as superior data scalability, vision-centric information flow, and inherent capabilities as powerful vision encoder. We hope our empirical findings will inspire further research to refine and enhance Single Transformer architectures, advancing the field of multimodal intelligence."
        },
        {
            "title": "Appendix",
            "content": "In the appendix, we provide additional experimental details and results."
        },
        {
            "title": "Additional Experimental Details",
            "content": "Training Configurations. In this section, we provide the corresponding setups for our experiment series in the main paper, including the default setting, the data scaling series, the model scaling series, and ablation experiment settings. The detailed configurations are shown in Table 8. Evaluation Configurations. In the main paper, we measure the model performance on several benchmarks: VQAv2 [28], GQA [36], ScienceQA-IMG [52], TextVQA [65], POPE [44], MME [46], MMBench [50], and SEEDBench-IMG [40]. We normalized the performance to full score of 100 and averaged the performance across these benchmarks to plot the curves shown in Figure 1(A) and Figure 3. The detailed experimental results are shown in Table 9."
        },
        {
            "title": "Additional Experimental Results",
            "content": "A comparison of SAIL and LLaVA1.5. In this section, we conduct an experiment to compare SAIL with LLaVA1.5 [47]. In this experiment, our SAIL is trained on 512M image-text pairs in Pretraining Stage 1, followed by fine-tuning on the LLaVA-mix-665K dataset. To fairly compare the performance of the two models, we do not use the anyres strategy during SFT. Instead, we adopt the same image processing approach as LLaVA1.5, ensuring that the aspect ratio and number of image tokens are consistent across both models. The experimental results are presented in Table 10. Despite our model being trained on only 512M image-text pairs, which is significantly smaller than the CLIP pretraining data used in the LLaVA1.5 model, the results show that our model achieves comparable performance to LLaVA1.5 across various benchmarks. Remarkably, our model even outperforms LLaVA1.5 on specific benchmarks such as DocVQA and ChartVQA. These findings highlight the strong potential of Single Transformer models in terms of data scaling. Specifically, they suggest that even with relatively smaller pretraining dataset, Single Transformer models can perform on par with, or even exceed, more extensively trained modular MLLMs like LLaVA1.5 when similar preprocessing and controlled variables are applied. Compare SAIL and LLaVA on MMVP. We compare SAIL and LLaVA1.5 [47] on MMVP [70] to dissect the behavior of the two models. The results are shown in Figure 5. From examples (A) and (B), we observe that SAIL performs better in perceiving minor regions and objects. Examples (C) and (D) illustrate that SAIL can more accurately distinguish the states of objects. Additional Experiments on Information Flow Pattern Analysis. In the main paper, we analyzed the distribution patterns of image attention scores for different Single Transformer-based MLLMs and modular MLLMs. The results showed that Single Transformer-based MLLMs allocate more attention weights to image tokens. However, this could be due to different models processing varying numbers of image tokens, where more image tokens lead to higher aggregated attention scores. To analyze this in more controlled manner, we designed an additional experiment. Using the data scaling setup at 512M, we pretrained SAIL and its modular MLLM counterpart. After pretraining, we fine-tuned both models using the LLaVA-mix-665K dataset, fixing the resolution size to 224x224 during SFT, instead of using any resolution. The results, shown in Figure 6, reveal that SAIL allocates higher attention scores to image tokens across all transformer layers compared to the modular MLLM, particularly in medium layers (+43.5% in layer 14) and deep layers (+41.2% in layer 31). From this, we can conclude that Single Transformerbased MLLMs tend to allocate significant portion of attention to previous image tokens during prediction. In contrast, modular MLLMs allocate smaller portion of their attention directly to image tokens, indicating less imagecentric approach in their prediction mechanism. Attention Map Visualization. In the main paper, we found that Single Transformer-based MLLMs allocate large portion of attention weights to image tokens during inference, indicating more vision-centric model. Here, we visualize the attention distribution of SAIL across different regions of the image when predicting tokens. The results in Figure 7 illustrate the attention maps for 9 Exp Model LLM Stage 1 Data LR SFT Data LR SAIL, point 32M Mistral-7B-v0.1 Standard Stage 1 Data (32M image-text pairs) (5e-5, 5e-6) Figure 1(A) Figure 1(A) SAIL, point 128M Mistral-7B-v0.1 Standard Stage 1 Data (128M image-text pairs) (5e-5, 5e-6) Figure 1(A), Table 6 SAIL, point512M Mistral-7B-v0.1 Standard Stage 1 Data (512M image-text pairs) (5e-5, 5e-6) Figure 1(B), Table 2 SAIL SAIL Table 3, 4, 5 SAIL-0.5B Figure 3, Table 7 SAIL-3B Figure 3 SAIL-7B Figure 3 LLaVA-mix-665K (1e-5,0) LLaVA-mix-665K (1e-5,0) LLaVA-mix-665K (1e-5,0) Mistral-7B-v0.1 Standard Stage 1 Data (512M image-text pairs) (5e-5, 5e-6) Standard Stage 2 Data (1e-5, 5e-6) Standard SFT Data (1e-5, 0) Mistral-7B-v0.1 Standard Stage 1 Data (512M image-text pairs) (5e-5, 5e-6) Qwen2.5-0.5B Standard Stage 1 Data (128M image-text pairs) (5e-4, 5e-6) Qwen2.5-3B Standard Stage 1 Data (128M image-text pairs) (1e-4, 5e-6) Mistral-7B-v0.1 Standard Stage 1 Data (128M image-text pairs) (5e-5, 5e-6) LLaVA-mix-665K (1e-5, 0) LLaVA-mix-665K (1e-5, 0) LLaVA-mix-665K (1e-5,0) - - - - - - - - - - Stage 2 Data - - - LR - - - Table 8. Experimental Configurations for Various Setups. The table lists the models used, the specific LLM variants, the datasets, and learning rates (LR) applied during each training stage (Pretraining Stage 1, Pretraining Stage 2, and SFT). Standard Stage 1 Data, Standard Stage 2 Data and Standard SFT Data are listed in Table 1. Specific points and tables/figures referred to in the text are also indicated. Model VQAv2 GQA SciQA-IMG TextVQA POPE MME MMBench SEEDBench-IMG Norm.avg Figure 1, modular MLLM, 32M 76.96 58.7 Figure 1, modular MLLM, 128M 78.47 59.78 Figure 1, modular MLLM, 512M 80.06 62.38 Figure 1, SAIL, 32M 70.51 57.95 76.36 60.93 Figure 1, SAIL, 128M Figure 1, SAIL, 512M 78.51 62.06 53.2 67.3 Figure 3, SAIL-3B 46.9 59.1 Figure 3, SAIL-0.5B 68.48 70.05 70.34 63.32 62.61 67.48 63.8 59.6 58.68 59.82 57.85 31.67 56.86 63.94 30.9 20.1 88.17 1599 86.78 1638 83.14 1379 81.77 1421 85.5 1458 86.04 1530 66.9 820.8 59.8 761.45 69.44 68.57 70.82 48.22 53.94 56.71 44.6 38. 70.31 68.11 69.83 61.51 66.60 68.83 55.4 35.1 61.41 61.52 61.86 51.93 57.91 60.51 47.80 39.92 Table 9. Detailed experimental results in the main paper. Method Pretrain SFT VQAv2 GQA SciQA-IMG TextVQA POPE MMBench SEEDBench DocVQA ChartQA AI2D MMStar avg LLaVA-1.5-336px [47] 12.8B+558K 665K 78.5 665K 77.8 SAIL 512M 62.0 61.6 66.8 68.0 58.2 56.4 85.9 86. 64.3 61.3 66.1 69.8 28.1 29.3 18.2 21.5 54.8 58.7 32.4 37. 58.3 59.1 Table 10. Comparison of SAIL and LLaVA1.5. We evaluate the models on VQAv2 [28], GQA [36], ScienceQA [52], TextVQA [65], POPE [44], MMBench [50], SEEDBench [40], DocVQA [56], ChartQA [55], AI2D [39] and MMStar [9]. specific tokens to the image portion when SAIL generates predictions for multimodal queries. The visualizations show that in the early transformer layers, the predicted tokens primarily focus on the salient regions of the image. As the model progresses to deeper layers, the attention shifts to areas more relevant to the predicted tokens. This behavior demonstrates that SAIL has the potential to function as grounding model, effectively correlating text tokens with their corresponding image regions. In other words, during inference, the model incrementally concentrates attention weights on relevant regions, aiding in decision-making. This progressive focusing of attention signifies the models capability to ground text tokens in the corresponding visual context, enhancing its performance in vision-language tasks. Visual Understanding Demonstration. We investigate several vision perception and reasoning capabilities of our SAIL. These include its ability to understand rich OCR information  (Table 11)  , interpret real-world scenes  (Table 12)  , comprehend scientific charts  (Table 13)  , and analyze poster contents  (Table 14)  . 10 Figure 5. Comparison of SAIL and LLaVA1.5 on MMVP examples. SAIL demonstrates better performance in perceiving minor regions and objects, as well as more accurately distinguishing object states. Figure 6. Image attention score allocation for SAIL and its modular MLLM counterpart. We compared the attention score allocation distribution for shallow layers, medium layers, and deep layers between these two models. The Single Transformer-based MLLM model significantly allocates higher proportion of attention score to image tokens during prediction than the modular MLLM. 11 Figure 7. Visualization of SAILs attention distribution across image regions during token prediction. In early transformer layers, attention primarily focuses on the salient regions of the image. As the model progresses to deeper layers, attention shifts to areas more relevant to the predicted tokens. 12 Example 1: Understanding OCR Information in Webpage. User Please output all the text information. SAIL Cornell University arXiv cs arXiv:2304.08485 Computer Science Computer Vision and Pattern Recognition Visual Instruction Tuning HaoTian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLM for general-purpose visual and language visual and language this http URL URL Early Experiments: LLMs demonstrate impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLAVA and GPT-4 achieves new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available. Comments: NeurIPS 2023 Oral; project page: this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) Cite as: arXiv:2304.08485 [cs.CV] (or for this version) https://doi.org/10.48550/arXiv.2304.08485 Submission history From: Haotian Liu [view email] [v1] Mon, 17 Apr 2023 17:59:25 UTC (4,360 KB) [v2] Mon, 11 Dec 2023 17:46:14 UTC (4,985 KB) Table 11. SAIL demonstrates strong capability to extract information in OCR-rich scenarios. 13 Example 2: Understanding Real-World Scene. User Please describe the image in detail. SAIL There are several notable elements in the image: 1. The highway is multi-lane, with multiple lanes visible. 2. The traffic is dense, with vehicles in various lanes. 3. The weather seems to be overcast, as indicated by the grayish tones of the image. 4. There are several traffic signs and signs visible, including green highway sign indicating 1/2 mile to the right, and green sign with the number 16 and the word MILE on the left side of the image. 5. The background includes hill or mountain, with trees and greenery visible. 6. There are also several power lines and electrical pylons visible along the sides of the highway. The images focus is on the traffic and the surrounding environment. The vehicles are primarily in the center and right lanes of the highway, with some vehicles in the left lanes as well. The highway is flanked by lush greenery, including trees and grassy areas, suggesting that this is rural or possibly suburban area. The traffic appears to be moving at moderate pace. The weather conditions suggest that this is daytime scene, possibly during the morning or late afternoon. The image is taken from perspective that suggests the viewer is in vehicle on the highway, observing the traffic ahead. In summary, the image captures typical highway scene with moderate traffic, surrounded by mix of natural and built environments, under an overcast sky. The road is in good condition, and the vehicles are traveling smoothly in both directions. Table 12. SAIL demonstrates strong capability about understanding the real-world scene. Example 3: Understanding the Scientific Charts. User Please describe the image in detail. SAIL This image is collage of various objects, foods, and animals, all of which are predominantly blue or have blue elements. The image is divided into sections labeled with letters from to O. Here is detailed description of each section: 1. - Pumpkin: blue pumpkin is depicted, which is common symbol associated with autumn and Halloween. 2. - Fish: blue fish is shown, which could be representation of common aquatic animal. 3. - M&M: blue M&M candy is present.4. - Milk and Soap: glass of milk, bar of soap, and blue sponge.5. - Bread and Jelly Beans: Blue wheat, blue loaf of bread, and blue jelly beans.6. - Castor Oil: bottle of castor oil.7. - Cheerios and Snacks: box of Cheerios, glass of dark blue drink, blue popsicle, blue donut, and blue muffin. 8. - Vegetables: collection of vegetables including blue cucumber, green leafy vegetables, and bunch of blue carrots.9. - Apple and Stones: green apple, blue candy, and blue stones.10. - Syrup and Candy: Sugar cubes, bottle of blue syrup, and blue candy bar.11. - Coconut and Aloe: blue coconut, bottle of blue liquid, and blue aloe vera plant.12. - Pineapple and Juice: blue pineapple, blue banana, and glass of blue juice.13. - Candy Bar: blue candy bar.14. - Blueberries: Blueberries.15. - Oval Object: blue oval-shaped object. Table 13. SAIL demonstrates strong capability about understanding the scientific charts. Example 4: Understanding Poster Content. User Please describe the image in detail. SAIL At the top of the image, there is decorative border with pattern of hanging golden ornaments, which adds festive and celebratory feel to the overall design. The background is plain white, which helps the colorful elements stand out. In the center of the image, the text Happy Sankranti is prominently displayed in large, elegant, and stylized font. The text is in warm orange color, which is traditionally associated with the festival. Below the text, there is depiction of woman dressed in traditional Indian attire, likely saree, in shades of blue and green. She is engaged in the act of preparing traditional Sankranti sweet, which is common practice during the festival. The sweet is shaped like pyramid and is decorated with intricate patterns, reflecting the artistic and cultural heritage of the region. To the left of the woman, there is three-tiered pot, known as patala, which is traditional Indian cooking vessel. The pot is adorned with colorful design and is placed on colorful circular mat, which is also traditional element in Indian households. On the right side of the woman, there is small hut with thatched roof, which is common architectural style in rural Indian villages. The hut is depicted in simple and rustic manner, emphasizing the rural and traditional aspect of the scene. Overall, the image captures the essence of the Sankranti festival, highlighting the cultural and religious aspects of the celebration. The use of traditional clothing, the preparation of traditional sweets, and the depiction of rural village scene all contribute to sense of cultural authenticity and celebration. Table 14. SAIL demonstrates strong capability about understanding the poster content."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024. 1 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 2, 6 [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. [5] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasrlar. Introducing our multimodal models, 2023. 2, 5, 6 [6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 3 [7] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. 2 [8] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision In Proceedings of the models in the vision-language era. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3 [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 5, 6, 10 [10] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. [11] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. single transformer for scalable vision-language modeling. Transactions on Machine Learning Research, 2024. 1, 2, 3, 4, 5, 6, 7 [12] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. 5, 6 [13] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 6 [14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2, 3, 6 [15] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scalIn Proing laws for contrastive language-image learning. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28182829, 2023. 1, 2, 3, 6, 8 [16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. [17] MMSegmentation Contributors. MMSegmentation: and https : / / github . com / open - segmentation toolbox semantic Openmmlab benchmark. mmlab/mmsegmentation, 2020. [18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. 1 [19] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512. PMLR, 2023. 3, 6 [20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 3, 5 [21] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. In Advances in Neural Information Processing Systems, pages 5254552567. Curran Associates, Inc., 2024. 1, 2, 3, 4, 5, 6, 7 [22] Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, and Xinlong Wang. Evev2: Improved baselines for encoder-free visionlanguage models. arXiv preprint arXiv:2502.06788, 2025. 5, 6 [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 3 [24] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 6 [25] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua Susskind, and Armand Joulin. Scalable pretraining of large autoregressive image models. arXiv preprint arXiv:2401.08541, 2024. 5 [26] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1935819369, 2023. 3 [27] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. 3 [28] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 8, 9, 10 [29] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. 4, [30] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual ilIn Proceedings of lusion in large vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. 5, 6 [31] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 3 [32] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 5 [33] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023. 2 [34] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei Zhang. Tag2text: Guiding vision-language model via image tagging. arXiv preprint arXiv:2303.05657, 2023. 3 [35] Zilong Huang, Qinghao Ye, Bingyi Kang, Jiashi Feng, and Haoqi Fan. Classification done right for vision-language pretraining. In NeurIPS, 2024. [36] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 8, 9, 10 [37] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. 3 [38] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume LamarXiv preprint ple, Lucile Saulnier, et al. Mistral 7b. arXiv:2310.06825, 2023. 1 [39] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. 5, 6, 10 [40] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 5, 6, 8, 9, 10 [41] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3, 5, 6, [42] Xianhang Li, Zeyu Wang, and Cihang Xie. An inverse scaling law for clip training. In NeurIPS, 2023. 3 [43] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024. 4, 5 [44] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 6, 9, 10 [45] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training In Proceedings of the IEEE/CVF conference via masking. on computer vision and pattern recognition, pages 23390 23400, 2023. [46] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large In Proceedings of the 3rd International language models. Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. 5, 6, 8, 9 [47] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 6, 7, 9, 10 18 [48] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 3, 6 [49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2, 3 [50] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 5, 6, 8, 9, 10 [51] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. 5, [52] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 5, 6, 8, 9, 10 [53] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 5, 6 [54] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jiawen Liu, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202, 2024. 1, 2, 3, 5, 6 [55] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 10 [56] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 10 [57] Sachin Mehta, Maxwell Horton, Fartash Faghri, Mohammad Hossein Sekhavat, Mahyar Najibi, Mehrdad Farajtabar, Oncel Tuzel, and Mohammad Rastegari. Catlip: Cliplevel visual recognition accuracy with 2.7 faster prearXiv preprint training on web-scale image-text data. arXiv:2404.15653, 2024. [58] Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. Accessed: 2024-04-18. 1 [59] OpenAI. Introducing chatgpt. OpenAI Blog, 2021. [60] OpenAI. Gpt-4 technical report, 2023. 1 [61] OpenAI. Gpt-4v(ision) system card, 2023. 1, 2 [62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1, 2, 3, 6, 8 [63] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 3, 4, 5 [64] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatronlm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. 5 [65] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 5, 6, 8, 9, 10 [66] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. https://cerebras.ai/blog/slimpajamaa627btokencleaned- anddeduplicatedversion-of-redpajama, 2023. 4, [67] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2, 6 [68] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 2 [69] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. 6 [70] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 9 [71] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners are scalable vision learners too. Advances in Neural Information Processing Systems, 36:4683046855, 2023. 3, 8 [72] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners are scalable vision learners too. Advances in Neural Information Processing Systems, 36:4683046855, 2023. 6 [73] Haochen Wang, Junsong Fan, Yuxi Wang, Kaiyou Song, Tiancai Wang, Xiangyu Zhang, and Zhaoxiang Zhang. Bootstrap masked visual modeling via hard patches mining. arXiv preprint arXiv:2312.13714, 2023. 5 [74] Haochen Wang, Kaiyou Song, Junsong Fan, Yuxi Wang, Jin Xie, and Zhaoxiang Zhang. Hard patches mining for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037510385, 2023. 19 Feng. Pixel-sail: Single transformer for pixel-grounded understanding. arXiv, 2025. 6 [89] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17241732, 2024. 3 [90] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. 5 [91] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 2, 3 [75] Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Ge Zheng, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. In International Conference on Learning Representations, 2025. [76] Jiacong Wang, Bohong Wu, Haiyong Jiang, Zhou Xun, Xin Xiao, Haoyuan Guo, and Jun Xiao. World to code: Multimodal data generation via self-instructed compositional capIn Proceedings of the 2024 Confertioning and filtering. ence on Empirical Methods in Natural Language Processing, pages 46084623, 2024. 1 [77] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 1, 2, 3, 7 [78] Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, and Xinlong Wang. Diffusion feedback helps clip see better. arXiv preprint arXiv:2407.20171, 2024. 3 [79] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2, 3, 6 [80] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. SimVLM: Simple visual language model pretraining with weak supervision. In International Conference on Learning Representations, 2022. 3 [81] X.ai. Grok-1.5 vision preview. https://x.ai/blog/ grok-1.5v, 2024. 5, [82] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European conference on computer vision (ECCV), pages 418434, 2018. 5, 6 [83] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. Transactions on Machine Learning Research, 2022. 3 [84] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1402214032, 2024. 4, 5 [85] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 5, 6 [86] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936, 2022. 5, 6 [87] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 3, 7, 8 [88] Tao Zhang, Xiangtai Li, Zilong Huang, Yanwei Li, Weixian Lei, Xueqing Deng, Shihao Chen, Shunping Ji, and Jiashi"
        }
    ],
    "affiliations": [
        "Bytedance Seed"
    ]
}