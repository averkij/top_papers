{
    "paper_title": "Video-T1: Test-Time Scaling for Video Generation",
    "authors": [
        "Fangfu Liu",
        "Hanyang Wang",
        "Yimo Cai",
        "Kaiyan Zhang",
        "Xiaohang Zhan",
        "Yueqi Duan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: https://liuff19.github.io/Video-T1"
        },
        {
            "title": "Start",
            "content": "Video-T1: Test-Time Scaling for Video Generation Fangfu Liu1*, Hanyang Wang1*, Yimo Cai1, Kaiyan Zhang1, Xiaohang Zhan2, Yueqi Duan1 1Tsinghua University, 2Tencent 5 2 0 2 4 2 ] . [ 1 2 4 9 8 1 . 3 0 5 2 : r Figure 1. Video-T1: We present the generative effects and performance improvements of video generation under Test-Time Scaling (TTS) settings. The videos generated with TTS are of higher quality and more consistent with the prompt than those generated without TTS."
        },
        {
            "title": "Abstract",
            "content": "With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given challenging text prompt. In this work, we reinterpret the test-time scal- *Equal contribution. The corresponding author. ing of video generation as searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy testtime computation costs, we further design more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on textconditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project Page: https://liuff19.github.io/Video-T1. 1. Introduction The field of generative modeling has witnessed remarkable progress in recent years [1, 39, 42, 60], with applications spanning from image and text generation to more complex tasks, such as video synthesis. Among these, video generation [22, 23] stands out due to its potential to revolutionize digital content creation, enabling the automatic production of high-quality videos from simple textual descriptions [62]. This capability has profound implications for various industries [22, 23, 30] (e.g., entertainment, education, and advertisements). The pivotal factor of the exponential growth in video generation lies in the scaling-up capability by training with an expanding volume of data, more computational sources, and larger model sizes [22, 38]. This scaling behavior during the training process, commonly referred to as Scaling Laws [12, 19, 38, 41], plays crucial guiding role in the advancement of generative models with progressively higher capabilities. Despite these advancements, generating high-quality videos remains challenging due to the need for maintaining temporal coherence and capturing complex dynamics across frames [62]. While scaling video generation methods in the training process [22, 32] has yielded significant improvements, it is inherently limited by high costs and resource demands, making it challenging to scale further. Recently, researchers in LLMs have expanded the study of scaling to the test-time [29] (e.g., DeepSeek-R1 [8] and OpenAI o1 [15]) and demonstrated that Test-time Scaling (TTS) can significantly improve the performance of LLMs with more contextually appropriate responses by allocating additional computation at inference time [8, 15, 48, 54]. In this paper, we propose to investigate Test-Time Scaling (TTS) for video generation. Specifically, we aim to answer the question: If video generation model is permitted to use the larger amount of inference-time computation, how much can it improve the generation quality for challenging text prompts? We seek to explore the potential of TTS to enhance video generation without the need for expensive retraining or model enlargement. To understand the benefits of scaling up test-time computation in video diffusion, we propose general framework for TTS video generation, called Video-T1, which reinterprets the TTS of video generation as searching problem within the space of possible video trajectories originating from Gaussian noise. The key insight is to scale the search space at test time with increased computation so that we can find broader range of potential solutions to generate higher-quality and textaligned videos. In our search framework, we introduce testtime verifiers to assess the quality of intermediate results and heuristic algorithms to navigate the search space efInitially, we conduct straightforward random ficiently. linear search strategy by sampling noise candidates in parallel and selecting the one that scores the highest per test-time verifier. However, recognizing the computational intensity of this approach, particularly when denoising all video frames simultaneously, we introduce more efficient framework called Tree-of-Frames (ToF). ToF operates in an autoregressive manner under tree structure, which leverages the feedback from verifiers and adaptively expands and prunes branches of video frames to balance computational cost and generation quality. Through extensive experiment on text-conditioned video generation benchmark, our findings reveal that increasing test-time compute leads to substantial improvements in the quality and human-preference alignment of samples generated by video generation models. Longer term, this offers significant promise on how to leverage inference-time computation to achieve superior results in computer vision. (See qualitative results gallery in Figure 1 and quantitative results in Figure 2). Our contributions are summarized as follows: We propose fundamental framework Video-T1 for testtime scaling for video generation, which reinterprets this process as search problem to sample better video trajectories. We show that scaling the search space of video generation can boost video performance across different dimensions of the benchmark. We carefully build the search space in test-time scaling by test-time verifiers to provide feedback and heuristic algorithms (i.e., straightforward random linear search and ToF search for more efficient test-time scaling) to guide the search process. Extensive experiments demonstrate that scaling the search space of video generation can boost the performance of various video generation models across different dimensions of the benchmark, and our proposed ToF search can significantly reduce scaling cost when achieving high-quality results. 2. Related Work Test-Time Scaling in LLMs. Recent advancements have demonstrated the effectiveness of test-time scaling (TTS) methods such as chain-of-thought prompting [34, 55], outcome reward models, and process reward models [25, 52, 64] in enhancing the reasoning capabilities of large language models (LLMs) during inference stages. Notable examples include implementations in OpenAI o1 [15] and DeepSeek-R1 [8]. These methods promote the generation of intermediate reasoning steps, resulting in more precise responses. These researches suggests that reallocating computational resources from pre-training [18] to testtime can enhance performance more efficiently [29, 44]. Moreover, strategies like self-consistency [5, 53], best-ofN [36, 46], Monte Carlo Tree Search [56, 67], and Rewardguided Search [6, 20] employ diverse generation techniques and sophisticated aggregation methods, often facilitated by process reward models. These approaches help in produc2 Figure 2. Results of Test-Time Scaling for Video Generation. As the number of samples in the search space increases by scaling testtime computation (TTS), the models performance exhibits consistent improvement (In the bar chart, light colors correspond to the results without TTS, while dark colors represent the improvement after TTS.). ing diverse and integrated outputs. Additionally, DeepSeek R1 [8] utilizes outcome-based reinforcement learning techniques, like group relative policy optimization [43], to enhance the reasoning capabilities of pre-trained models. The combination of parallel and sequential generation techniques in these models represents nuanced approach to generating contextually appropriate outputs, thereby establishing new operational standards for LLMs in complex problem-solving scenarios. Test-Time Scaling in Computer Vision. In both the visual understanding and visual generation fields, researchers have investigated various test-time scaling methods to further push the performance boundaries. With the success of test-time scaling methods in LLMs, several recent vision language models (VLMs) [49, 57] utilized step-by-step reasoning capability enhanced by test-time scaling methods and surpassed larger models in visual question-answering tasks. Recent investigations on image diffusion models have demonstrated that image diffusion models generation quality could be further enhanced with test-time scaling methods [9]. With verifiers providing judgments and algorithms selecting better candidates, image diffusion models consistently improve their performance across generation tasks by scaling up inference time [33]. Video Generation. Efficient and high-quality video generation has attracted increasing attention due to its wide applications in areas [2628, 47]. With the success of diffusion models [11, 40] in text-to-image generation, several studies have extended them to text-to-video (T2V) tasks, achieving promising results. One line of work [2, 4, 13, 31, 60] improves video quality by scaling up diffusion transformer (DiT) [37] pre-training, leading to high visual fidelity and smoother motion. These models have reached nearproduction-level performance but require extensive computational resources, especially for long videos [16]. Another line of work [3, 7, 16, 17, 21, 59] combines diffusion models with autoregressive mechanisms to better handle long and complex videos. For example, NOVA [7] generates videos by predicting frames sequentially over time while sampling tokens in random spatial order, unifying various generation tasks into single framework. Pyramid-Flow [16] redefines the generation process as multi-scale trajectory over compressed representations, using spatial and temporal pyramids to reduce training costs while maintaining quality. The autoregressive approaches show strong potential to generate longer, coherent, and high-quality videos with improved efficiency, making them promising direction for future research. 3. Method 3.1. How to Scale Video Generation at Test Time In the realm of LLMs, researchers have explored the benefits of scaling up test-time computation to boost model performance. Several key factors have been identified that shape the effectiveness of test-time scaling strategies in LLMs, such as the choice of policy models, process reward models (PRMs), and varying levels of problem difficulty [29, 45]. Similarly, Test-Time Scaling (TTS) in video generation hinges on key components like different video generation models, multimodal evaluation models, and the complexity of prompts across diverse benchmark dimensions. However, unlike LLMs, video generation poses specific challenges. First, videos inherently exhibit strong temporal continuity, meaning that while they consist of discrete frames, ensuring smooth transitions between frames is es3 Figure 3. Pipeline of Tes-Time Scaling for Video Generation. Top: Random Linear Search for TTS video generation is to randomly sample Gaussian noises, prompt the video generator to generate sequential of video clips through step-by-step denoising in linear manner, and select the highest score form the test verifiers. Bottom: Tree of Frames (ToF) Search for TTS video generation is to divide the video generation process into three stages: (a) the first stage performs image-level alignment that influences the later frames; (b) the second stage is to apply dynamic prompt in test verifiers to focus on motion stability, physical plausibility to provide feedback that guides heuristic searching process; (c) the last stage assesses the overall quality of the video and select the video with highest alignment with text prompts. sential for perceptually coherent results. Second, state-ofthe-art video generation models are primarily based on diffusion models, which employ multi-step denoising process that complicates the direct scaling of computational resources. These factors introduce additional complexities: test-time scaling in video generation must simultaneously address both spatial (frame-level) quality and temporal consistency while also considering the heavy iterative diffusion denoising process. To address these challenges, we propose to reinterpret video TTS as path-search problem to sample better trajectories from pure Gaussian noise space to the target video distribution. The key insight is to scale the search space at test time with increased computation so that we can explore broader range of potential solutions to generate higherquality and text-aligned videos. Taking closer look at this scheme, video can be represented as sequence of discrete frames. Considering the temporal nature of the frame sequence, it can be modeled as chain-like architecture, where the video generation resembles the growth of degenerate tree (i.e., tree where each non-leaf node has exactly one child rooted in the Gaussian noise space of the video domain). In this way, we formalize the generation of high-quality video as searching problem: starting from an initial root node, we seek path through steps that reaches leaf node, maximizing the quality along the generated sequence. To build such search space, we define several key components: Video Generator G: Video generation models, which generate videos from given text prompts by the multi-step denoising process. Formally, we define: : RHW CT , (1) where represents the input text condition, and the output is generated video with frames. Test Verifiers V: Multimodal evaluation models that assess the quality of generated videos and assign final score to provide feedback in the generation process. This can be expressed as: : RHW CT R, (2) where the function takes both the generated video and the input condition to produce scalar quality score. Heuristic Search Algorithms : The optimization methods that leverage feedback from the verifier to guide the search trajectory, ultimately finding better video sequences. We define this as: : (RHW C)N RHW CT , (3) 4 where (RHW C)N represents the set of initial noise samples (i.e., root nodes in the search forest), and RHW CT denotes the final selected video sequence (i.e., path from root node to leaf node at depth ). 3.2. Random Linear Search straightforward approach for TTS video generation is to randomly sample Gaussian noises, prompt to generate complete video sequences by performing the full denoising process for each sample, and perform the Best-of-N selection to obtain the one with the highest score from the test verifiers V. We refer to this method as random linear search (the top of Figure 3), as it performs step-by-step denoising in linear manner along the noise dimension. In this search algorithm, each noise sample deterministically corresponds to determined video output, and the only scaling factor for test-time scaling is the number of noise samples , leading to computational cost that increases linearly with the number of samples. From more structural perspective, random linear search can be interpreted as forest consisting of degenerate trees, where each tree represents an independent sequence of denoising steps. The search task then reduces to selecting better length-T path among them. The total number of nodes in the forest is , leading to generation time complexity of O(T ). Since each video evaluation requires constant-time assessment of its quality, the evaluation cost per sample is O(1), resulting in an overall quadratic time and space complexity O(T ). While random linear search provides simple baseline, its linear structure introduces two inherent limitations: 1) Simplicity of linear structure. Although the final path selects single branch, the tight bounds of this approach require exhaustive traversal of the entire space, lacking efficient optimization mechanisms. 2) Isolation of independent structure. Without any feedback or interaction mechanisms between trees, it introduces additional randomness, making it slower for test-time scaling. 3.3. Tree-of-Frames Search Random linear search is essentially adopted by Best-of-N strategy that scales test-time computation through increasing the number of initial noise samples . However, this approach requires fixed time complexity of O(T ) as analyzed above, which becomes increasingly inefficient as either scales up or the video length grows, making it impractical for long video generation or high-quality sampling at larger scales. To address this limitation and achieve better balance between video quality and test-time computational efficiency, we propose Tree-of-Frames (ToF) Search (Algorithm 2), which leverages the sequential generation capability of autoregressive models (unlike diffusion models that denoise the entire video sequence simulAlgorithm 1 Random Linear Search Require: Number of noise samples , video frame length , verifier V, video generator and decoder D, Gaussian noise distribution Ensure: Video ˆv with the highest verifier score 4: 5: 6: 1: Initialize empty set {} 2: for = 1 to do 3: Sample initial noise z(i) Initialize x(i) 0 z(i) for = 0 to 1 do G(x(i) x(i) t1, t) end for Decode video v(i) D(x(i) ) Compute score s(i) V(v(i)) Add (v(i), s(i)) to 7: 8: 9: 10: 11: end for 12: Final verify ˆv arg max (v,s)C 13: return ˆv taneously), introducing inference-time reasoning along the temporal dimension. This approach enables more flexible and scalable video generation process, structured into three distinct stages: given text prompt as input, (a) the first stage is to generate the initial frame with text-alignment on various dimensions (e.g., spatial relation, appearance style, color), which strongly impacts later frames due the continuity of video frames; (b) the second stage focuses on generating intermediate frames which should consider the key factors like subject consistency, motion stability, even physics plausibility to guarantee smooth video flow; (c) the final stage is dedicated to assessing the overall video quality and alignment with text prompts. According to the goal of three stages, we meticulously design three key techniques in ToF search algorithm: image-level alignment, hierarchical prompting, and heuristic pruning. Image-level alignment. Different from LLMs, video generation involves both spatial and temporal dimensions. Along the spatial axis, video frames are generated through step-wise denoising employed in diffusion models. Inspired by the Chain-of-Thought (CoT) reasoning mechanism in image generation [9], we introduce progressive evaluation strategy at the frame level to dynamically scale computation during the denoising process. Specifically, during the denoising of each frame, potential test verifier evaluates whether the partially denoised image has reached sufficient clarity for reliable assessment. Early stage frames often remain too blurry for meaningful evaluation, which could mislead the scoring of frame quality. Once the frame reaches visually informative state, the model further assesses its potential to evolve into high-quality final image. By performing early rejection of low-potential candi5 dates and allocating compute toward promising trajectories, image-level scaling ensures more efficient use of resources during inference. Hierarchical prompting. From spatial perspective, each video frame is generated as an independent image. However, different frames play distinct roles in shaping the videos narrative and temporal coherence. With the analysis above, we design hierarchical prompting strategy in three different stages: (a) for the first frame, we extract the key prompts related to core semantics (e.g., color, object count, relative positions) to prompt the verifiers to provide feedback that determines the consistency and correctness of subsequent frames; (b) for intermediate frames, we apply dynamic prompt in test verifiers to focus on action description and motion continuity based on the context established by the first frame; (c) lastly, we prompt test verifiers to assess the overall quality of the final text-video alignment while mitigating the risk of accumulating temporal artifacts from excessive motion. To maintain smooth transitions across these distinct stages, we introduce adaptive branching by injecting additional initial noise samples when switching between stages, thereby improving temporal coherence and diversity. Heuristic pruning. Throughout the generation process, we model the video as the dynamic growth of forest, where trees represent possible generation paths and are expanded and pruned over time. Similar to random linear search, we start by generating initial frames, corresponding to the roots of trees. Each time step [0, 1] corresponds to layer in the tree, with each frame acting as node. At each time step, every surviving parent node kt1 dynamically branches into bt candidate continuations. All kt1 bt nodes are evaluated using heuristic reward score by test verifiers V, after which only the top kt nodes are retained for further growth. The heuristic score balances local frame quality with global consistency to prioritize the most promising paths. By iteratively applying adaptive branching and heuristic pruning, ToF search efficiently explores the search space while maintaining manageable compute costs. See Algorthm 2 for more details. Complexity analysis. The time complexity of growing one level of the tree is: O(kt1bt + bt log(kt1bt)). (4) Here, generating kt1bt nodes takes O(kt1bt) time, and heap sorting for pruning costs O(bt log(kt1bt)). By iteratively applying dynamic branching and heuristic pruning, the deepest leaf nodes in the forest correspond to the final frames of the video, with the path to those nodes representing the optimal video sequence. The overall time complexity of this process is: O(k0 + 1 (cid:88) t=1 kt1bt + bt log(kt1bt)). (5) In practice, we set k0 = and branching limit bi = 2. In the worst-case scenario, assuming bi = = 2 for all i, the resulting time complexity is: O(N + + 2T log(N )) = O(T ). (6) This complexity is consistent with the random linear search. In our practical experiments, we perform branching operations only at specific prompt stages to ensure diverse and stable transition between stages. Consequently, bt remains 1 for most timesteps, and Eq. 5 can simplify to O(N + ). Compared to the quadratic complexity of random linear search, our proposed ToF significantly reduces computational costs while maintaining high sample diversity. The logarithmic dependency on ensures efficient scaling. Additionally, by dynamically adjusting the branching factor, we achieve better trade-off between exploration in early timesteps and convergence in later stages. For detailed complexity analysis, please refer to supplementary materials. 3.4. Multi-Verifiers Beyond test-time scaling in policy models, previous research [24, 35, 65] has demonstrated that applying testtime scaling to generative verfier models can significantly enhance performance. This improvement can be achieved through methods such as majority voting with single verifier model [35] or by ensembling multiple verifiers [24]. To further boost the performance of test-time scaling in video generation, we employ mixture of different verifiers to mitigate biases and select the best videos from the candidates: ˆi = arg max 0<i<n = arg max 0<i<n (cid:16) H(f (i)) (cid:17) (cid:32) 1 (cid:88) vM (cid:33) (7) cvRankv(f (i)) , where is the set of test verifiers, Rankv indicates the score ranking assigned by verifier to the ith candidate video (i), cv denotes the weight associated with verifier v, is the total number of sampled candidates, and ˆi is the index of the candidate with the highest score. This approach ensures the robustness of test-time scaling and yields better performance gains. 4. Experiment 4.1. Experiment Setup Video Generation Models. We evaluate our TTS strategy (i.e., random linear search and ToF search) using six Algorithm 2 Tree-of-Frames (ToF) Search Require: Initial number of roots , maximum tree depth , branching factors {bt}T t=0, heuristic score by test verifier V, video generator with image-level scaling, noise distribution Ensure: Video path ˆv with the highest heuristic score t=1, pruning sizes {kt}T 0 }) into 1: Initialize empty priority queue 2: for = 1 to do 3: Sample initial noise z(i) Inital root frame (i) 0 z(i), 0 Enqueue (f (i) 0 , score = 0, path = {f (i) 4: 5: 6: end for 7: for = 1 to do 8: 9: 10: 11: 12: Initialize empty list {} for = 0 to kt1 do Dequeue node (f, s, p) from for = 1 to bt do 13: 14: 15: 16: 17: Generate continuation fm G(f, t) Compute heuristic reward hm H(fm, t) Add (fm, + hm, {fm}) to end for end for Heap sort by total score in descending order Clear for = 1 to kt do Enqueue the n-th top node from into 18: 19: 20: 21: 22: end for 23: Final verify ( ˆf , ˆs, ˆv) arg max end for (f,s,v)C 24: return ˆv popular open-sourced pre-trained video generation models, including three diffusion-based video models (OpenSorav1.2 [66], CogVideoX-2B, and CogVideoX-5B [60]) and three autoregressive models (NOVA [7], Pyramid-Flow (SD3), and Pyramid-Flow (FLUX) [16]). These models span parameter range from 0.6B to 5B. Test Verifiers. To obtain reasonable feedback and provide the heuristic score in different stages, we leverage three multi-modal reward models specific to video generation (i.e., VisionReward [58], VideoScore [10], and VideoLLaMA3 [63]) to assess generated video quality under two search algorithms. VisionReward [58] is designed to capture human preferences across multiple dimensions (dividing the evaluation into 29 weighted questions), while VideoScore [10] is initialized from LMM and trained on dataset containing human-provided multi-aspect scores to automatically assess video quality. VideoLLaMA3 [63] is multimodal foundation model that exhibits state-of-the-art image and video understanding. For comparison, we use 7 metrics VBench [14] as ground-truth verifier to demonstrate the upper bound achievable by the three test verifiers. Details of Search Algorithms. We conduct experiments on two search algorithms assessed on VBench [14]. For the random linear search, experiments are conducted on 6 video generation models using various verifiers, where the initial sample noise level is incremented from 1 to 30 for each trial. In the case of ToF search, the method is applied to 3 autoregressive models using the best-performing multiple verifier where the initial sample noise is varied from 1 to 7. Metrics. To quantify the performance of text-to-video generation, we use VBench [14], which is comprehensive benchmark incorporating 16 fine-grained dimensions that evaluate both motion quality and semantic alignment. For the computational cost, we extend the metric of the number of function evaluations (NFE) [33, 50, 61] from image generation to video generation by defining NFE as the product of the total number of denoising steps executed during the generation process and the temporal length of latent embeddings, which takes the temporal dimension of the video into inference computational costs. 4.2. Analysis of Experimental Results TTS consistently yields stable performance gains across different video generation models. We conduct series of random linear search experiments across multiple video generation models using different verifiers. In these experiments, the final video outputs were evaluated with the VBench [14] total scorea composite metric aggregating 16 distinct dimensions of quality (e.g., motion smoothness, semantic alignment, aesthetic quality). Figure 4 demonstrates that as the inference computational budget increases, all video generation models exhibit improved performance across different verifiers, eventually approaching convergence limit once certain threshold is reached. This finding indicates that the TTS strategy can effectively guide the search process during test time and significantly enhance generation quality. Moreover, when comparing different verifiers applied to the same video model, we observe varying growth rates and extents in their performance curves. This divergence suggests that each verifier emphasizes different evaluation aspects. Multiple verifiers can further boost the curve of TTS. Beyond the test-time scaling in video generation models, we ensemble the multiple verifiers in Figure 4 that can further boost the performance of test-time scaling in video generation. Such mixture of different verifiers can also mitigate biases and select the best video from the candidates. Advanced foundation models offer significant potential for improvement with TTS. Additionally, comparative analysis across video models in Figure 4 and Table 2 reveals that lightweight models (e.g., NOVA) exhibit only marginal performance improvements with increased inference effort, Figure 4. Performance of random linear search on different video models and verifiers. The top row displays results for autoregressive models, while the bottom row shows diffusion-based models. The initial points of the curves represent the random video sample results without TTS. The models are arranged in order of increasing parameter count from left to right; different colored curves represent the performance trends under various verifiers, and the gray dashed line corresponds to the baseline established by VBench, which serves as ground-truth verifier. Figure 5. Comparison between random linear search and ToF search. The red curve represents random linear search. The blue curve represents ToF search, with the dashed line being the predicted curve from geometric series decay approximation. Curve fitting reveals that similar subsequent trends tend to converge to an upper limit. whereas larger models (e.g., CogVideoX-5B) benefit from substantially wider search space and thus achieve more significant enhancements. This observation underscores the potential of larger models to leverage the TTS strategy more effectively, thereby yielding higher-quality video generation under increased computational budgets. Table 1. Inference-time scaling cost comparison on GFLOPs. Methods Linear Search ToF Search Pyramid-Flow(FLUX) Pyramid-Flow(SD3) NOVA 5.22 107 3.66 107 4.02 106 1.62 107 1.13 107 1.41 106 ToF Search is more efficient and superior to the random linear search. We implement the ToF search in three autoregressive models and conduct comparison experiment with the random linear search and ToF search in Figure 5. We observe that the ToF search achieves comparable performance at much lower computational cost, highlighting its high efficiency. To minimize the significant differences in computational costs among models of different sizes, We also show quantitative results of GFLOPs in Table 1. Moreover, larger and better models show higher efficiency, as evidenced by the faster rising speed of the curve. Performance across most dimensions can be greatly improved with TTS. The complexity of prompts across diverse benchmark dimensions is key component in video TTS. We conduct experiments to quantitatively evaluate the performance improvement of different models using TTS methods across various dimensions (See Figure 6 and Table 2). As ToF and random linear search can achieve 8 Figure 6. Qualitative TTS performance improvement ratio on different complexities of prompts across different video generation models across diverse benchmark dimensions of Vbench. Table 2. Quantitative Performance Comparison on VBench across different video generation models. Total Score Quality Score Semantic Score Object Class Scene Multiple Objects Diffusion-based Models CogVideoX-5B + TTS CogVideoX-2B + TTS OpenSora-v1.2 + TTS Autoregressive Models Pyramid-Flow (SD3) + TTS Pyramid-Flow (FLUX) NOVA + TTS + TTS 81.61 84.42+3.44% 80.91 83.89+3.68% 79.76 81.65+2.37% 81.72 85.31+4.39% 81.61 86.51+5.86% 78.56 79.80+1.58% 82.75 84.32+1.90% 82.18 85.27+3.76% 81.35 81.90+0.68% 84.74 86.84+2.48% 84.11 87.50+3.26% 83.79 84.99+1.43% 77.04 84.83+10.1% 75.83 78.39+3.38% 73.39 80.63+9.87% 69.62 79.21+13.8% 71.61 82.56+18.6% 57.63 59.03+2.43% 85.23 99.38+16.6% 83.37 88.89+6.62% 82.22 98.29+19.5% 86.67 90.31+4.20% 93.49 99.69+3.38% 91.36 93.91+2.79% 53.20 63.07+18.6% 51.14 52.94+3.52% 42.44 48.82+15.0% 43.20 58.39+35.2% 47.65 56.07+17.7% 45.22 47.69+5.46% 62.11 84.47+36.0% 62.63 63.79+1.85% 63.34 66.99+5.76% 50.71 78.00+53.8% 61.08 88.93+45.6% 67.87 69.89+2.98% similar convergence score during test-time scaling, we choose the better score for (+TTS). We find that for common prompt sets (e.g., Scene, Object) and easily assessable categories (e.g., Imaging Quality), TTS methods achieve significant improvements across different models. few dimensions heavily rely on the capabilities of foundation models, making improvements challenging for TTS. However, for some hard-to-evaluate latent properties (e.g., Motion Smoothness, Temporal Flickering), the improvement is less pronounced. This is likely because Motion Smoothness requires precise control of motion trajectories across frames, which is challenging for current video generation models to achieve. Temporal Flickering, on the other hand, involves maintaining consistent appearance and intensity over time, which is difficult to precisely assess, especially when dealing with complex scenes and dynamic objects. (See Figure 6 and Table 2) 5. Conclusion In conclusion, this study presents novel framework for test-time scaling in video generation, redefining it as search problem for optimal video trajectories. We build the search space in TTS by test-time verifiers and to provide feedback and employ heuristic algorithms like random linear search and the more efficient ToF search algorithm. Extensive experiments demonstrate that scaling the search space can boost the video performance across various video generation models, and our proposed ToF search can significantly reduce scaling cost when achieving highquality video outputs. This framework opens new avenues for research into efficient test-time optimization strategies in video generation. 9 Acknowledgments: This work was supported in part by the National Natural Science Foundation of China under Grant 62206147, and in part by 2024 Tencent AI Lab Rhino-Bird Focused Research Program."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [3] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2025. 3 [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, pages 73107320, 2024. 3 [5] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal selfarXiv consistency for large language model generation. preprint arXiv:2311.17311, 2023. 2 [6] Haikang Deng and Colin Raffel. Reward-augmented decoding: Efficient controlled text generation with unidirectional reward model. arXiv preprint arXiv:2310.09520, 2023. 2 [7] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 3, [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3, 13 [9] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. 3, 5, 13 [10] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21052123, 2024. 7 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 3 [12] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 2 [13] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for and Jie Tang. text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 7 [15] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 2 [16] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 3, 7, 14 [17] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. Video-lavit: unified video-language pre-training with decoupled visual-motional tokenization. In Proceedings of the 41st International Conference on Machine Learning, pages 2218522209, 2024. 3 [18] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 2 [19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [20] Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. Args: Alignment as reward-guided search. arXiv preprint arXiv:2402.01694, 2024. 2 [21] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In Proceedings of the 41st International Conference on Machine Learning, pages 2510525124, 2024. 3 [22] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 10 [23] Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, and Lei Bai. survey on long video generation: Challenges, methods, and prospects. arXiv preprint arXiv:2403.16407, 2024. [24] Shalev Lifshitz, Sheila A. McIlraith, and Yilun Du. Multiagent verification: Scaling test-time compute with multiple verifiers, 2025. 6 [25] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 2 [26] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. 3 [27] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. [28] Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, and Yueqi Duan. Physics3d: Learning physical properties of 3d gaussians via video diffusion. arXiv preprint arXiv:2406.04338, 2024. 3 [29] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. 2, 3 [30] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85998608, 2024. [31] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. 3 [32] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. 2 [33] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. Inference-time scaling for diffusion models beyond scaling denoising steps, 2025. 3, 7 [34] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. 2 [35] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models, 2024. 6 Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. 2 [37] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 3 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 3 Maddison, [41] Yangjun Ruan, Chris and Tatsunori Hashimoto. Observational scaling laws and the predictability of langauge model performance. Advances in Neural Information Processing Systems, 37:1584115892, 2025. 2 [42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [43] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 3 [44] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more arXiv preprint effective than scaling model parameters. arXiv:2408.03314, 2024. [45] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more arXiv preprint effective than scaling model parameters. arXiv:2408.03314, 2024. 3 [46] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. 2 [47] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion, 2024. 3 [48] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 2 [36] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu [49] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed 11 [62] Ailing Zeng, Yuhang Yang, Weidong Chen, and Wei Liu. The dawn of video generation: Preliminary explorations with sora-like models. arXiv preprint arXiv:2410.05227, 2024. 2 [63] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 7 [64] Kaiyan Zhang, Jiayuan Zhang, Haoxin Li, Xuekai Zhu, Ermo Hua, Xingtai Lv, Ning Ding, Biqing Qi, and Bowen Zhou. Openprm: Building open-domain process-based reward models with preference trees. In The Thirteenth International Conference on Learning Representations, 2025. 2 [65] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction, 2025. 6 [66] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 7 [67] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023. 2 Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, and Salman Khan. Llamav-o1: Rethinking step-bystep visual reasoning in llms, 2025. [50] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 7 [51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 13 [52] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. 2 [53] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 2 [54] Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585, 2025. 2 [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2 [56] Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. 2 [57] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason stepby-step, 2025. [58] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. 7 [59] Yilun Xu, Ziming Liu, Max Tegmark, and Tommi Jaakkola. Poisson flow generative models. Advances in Neural Information Processing Systems, 35:1678216795, 2022. 3 [60] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 7, 14 [61] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 7 12 A. More Implementation Details A.1. More Discussion of Image-level Alignment In our generation process, inspired by recent work [9], each frame is generated with image-level TTS. Specifically, we employ two-stage evaluation mechanism. First, potential assessment reward model examines whether the partially generated frame exhibits sufficient visual clarity for meaningful evaluation and assigns binary label. If the output is deemed unclear (no), the model skips further processing for that frame. Otherwise (yes), the frame advances to secondary evaluation stage where its potential to yield high-quality final image is assessed. Again, binary decision is made: if the frame is unlikely to lead to an optimal outcome, the generation path is truncated immediately; if it passes, the synthesis continues to produce the final image. A.2. More Discussion of Hierarchical Prompting In our approach to hierarchical prompting, we employ DeepSeek-R1-8B [8], large language model distilled from the LLaMA-8B [51] architecture. DeepSeek-R1-8B is used to decompose given input prompt into three distinct hierarchical prompts, each tailored to represent specific stage of the video sequence: Static scene description: The first prompt provides detailed depiction of the static scene intended for the initial frame, establishing the starting visual context of the sequence. Action/motion directions: The second prompt outlines the actions or motion directions that guide the dynamic progression across the intermediate frames, ensuring coherent evolution of the scene. Expected ending state: The third prompt delineates the expected ending state for the concluding frame, defining the desired outcome of the video sequence. DeepSeek-R1-8B processes the input prompt and generates these three hierarchical prompts, which are then returned as an ordered list of length three. At each stage of the video sequenceinitial, intermediate, and finalthe test verifier evaluates the generated output with the corresponding hierarchical prompt. This stage-specific assessment ensures that the video content aligns with the intended descriptions, maintaining both accuracy and coherence throughout the sequence. Figure 9 gives an example of hierarchical prompting generation and test-time verification. A.3. Detailed Complexity Analysis In our method, the video generation process is modeled as the dynamic growth of forest, where the trees are branched and pruned over time. Specifically, similar to the linear search, we begin by generating initial frames, representing the roots of trees in the forest. Each time step [0, 1] corresponds to level in the tree, and each frame is treated as tree node. We consider the process in which each of the trees grows by adding one level of nodes. At each time step, the kt1 surviving parent nodes dynamically branch into bt possible continuations. We then evaluate all kt1 bt nodes using heuristic reward function H, followed by pruning to retain only the top kt branches. The time complexity of growing one level of the tree is: O(kt1bt + bt log(kt1bt)). (8) Here, generating kt1bt nodes takes O(kt1bt) time, the evaluation cost per node is O(1), and the total evaluation time is O(kt1bt). Heap sorting for pruning costs O(bt log(kt1bt)). By iteratively applying dynamic branching and heuristic pruning, the deepest leaf nodes in the forest correspond to the final frames of the video, with the path to those nodes representing the optimal video sequence. The overall time complexity of this process is: O(k0 + 1 (cid:88) t=1 kt1bt + bt log(kt1bt)). (9) In practice, we set branching limit for dynamic branching, i.e., bt b. In the heuristic pruning step, we use the heuristic reward function to prune branches that fall below the average value. On average, each pruning step i=1 bi retains kt kt1bt 2t branches before kt 2t drops to 1. Therefore, we have: 2 = k0 k0bt (cid:81)t O(k0 + 1 (cid:88) t= kt1bt + bt log(kt1bt)) = O(k0 + 1 (cid:88) k0 t=1 (cid:81)t i=1 bi 2t + bt log( k0 (cid:81)t i=1 bi 2t ) (10) In practice, we set k0 = and = 2. In the worst-case scenario, assuming bi = = 2 for all i, the resulting time complexity is: O(N + + 2T log(N )) = O(T ). (11) This complexity is consistent with that of the linear search. However, in our actual experiments, we perform branching operations only at specific prompt stages to ensure diverse and stable transition between stages. Consequently, bt remains 1 for most timesteps, leading to the following update rule: kt = (cid:26) kt1bt 2 1, , 0 < log(k0) > log(k0). (12) Thus, Eq. 9 can simplify to: 13 Figure 7. Using TTS, the small model (Pyramid-Flow) achieves scores that are close to, or even exceed, those of the 13B large model (HunyuanVideo) in many dimensions. The gray dashed horizontal line in the figures indicates HunyuanVideos score in that dimension. O(k0 + 1 (cid:88) t= kt1bt + bt log(kt1bt)) = O(k0 + 1 (cid:88) log(k0) (cid:88) k0 bt + t=log(k0)+1 t=1 (cid:81)t i=1 bi 2t + bt log( (cid:81)t i=1 bi 2t )) = O(k0 + log(k0) + k0 + log2(k0) log 2 2 log2(k0)) = O(N + ). (13) Compared to the quadratic complexity of linear search, our approach converge at geometric rate, ultimately achieving linear complexity. It significantly reduces computational costs while maintaining high sample diversity. The logarithmic dependency on ensures efficient scaling, making our method more suitable for high-dimensional video generation. Additionally, by dynamically adjusting the branching factor, we achieve better trade-off between exploration in early timesteps and convergence in later stages. B.2. More Qualitative Results We provide additional visual results for Pyramid-Flow [16] and CogVideoX-5B [60] in Figures 13-18. Each example displays different video outputs as NFE increases with the same prompt input. The results show that video quality and text alignment improve with more TTS samples. B.3. Comparison with Large Models Figure 7 compares the outputs of the small model PyramidFlow (2B) with TTS and the large model HunyuanVideo (13B) single. As scaling increases, the small models performance approaches or even surpasses that of the large model in many dimensions. B.4. Failure Cases Figure 8 shows failure cases in Pyramid-Flow experiments where TTS does not significantly improve video quality. Increasing inference computation fails to generate reasonable details (e.g., hand movements), indicating that model performance limitations constrain TTS method improvements. B. More Experiments B.1. More Quantitative Results We performed multiple random linear search experiments across various video generation models with different verifiers. Figures 10-12 present more quantitative results on VBench across different dimensions. These indicate that TTS consistently delivers stable performance improvements across dimensions. Moreover, the evaluation accuracy of different verifiers varies across dimensions, justifying our use of multiple verifiers. Figure 8. Failure cases on prompt person is clapping. 14 Figure 9. Verifications during TTS process. 15 Figure 10. TTS performance on Appearance Style across diverse verifiers. Figure 11. TTS performance on Aesthetic Quality across diverse verifiers. Figure 12. TTS performance on Color across diverse verifiers. 16 Figure 13. More visual results during TTS process on Pyramid-Flow. From left to right, each row of frames are extracted from video sequence. From top to bottom, each row represents the output video results of TTS with an increasing number of samples. 17 Figure 14. More visual results during TTS process on Pyramid-Flow. The TTS method can effectively alleviate common issues in video generation, such as those related to human motion and complex movements. 18 Figure 15. More visual results during TTS process on CogVideoX-5B. 19 Figure 16. More visual results during TTS process on CogVideoX-5B. 20 Figure 17. More visual results during TTS process on CogVideoX-2B. The TTS method can help to enhance multi-object spatial perception. 21 Figure 18. More visual results during TTS process on NOVA. The TTS method improves the alignment between the spatial relationships of objects in videos and the corresponding text prompt."
        }
    ],
    "affiliations": [
        "Tencent",
        "Tsinghua University"
    ]
}