{
    "paper_title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers",
    "authors": [
        "Wei Pang",
        "Kevin Qinghong Lin",
        "Xiangru Jian",
        "Xi He",
        "Philip Torr"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 7 9 4 1 2 . 5 0 5 2 : r Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers 1Wei Pang, 2Kevin Qinghong Lin (cid:0), 1Xiangru Jian, 1Xi He(cid:0), 3Philip Torr 1 University of Waterloo 2 National University of Singapore 3 University of Oxford Project Page: https://paper2poster.github.io"
        },
        {
            "title": "Abstract",
            "content": "Academic poster generation is crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i) Visual Qualitysemantic alignment with human posters, (ii) Textual Coherencelanguage fluency, (iii) Holistic Assessmentsix fine-grained aesthetic and informational criteria scored by VLM-as-judge, and notably (iv) PaperQuizthe posters ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, top-down, visualin-the-loop multi-agent pipeline: the (a) Parser distills the paper into structured asset library; the (b) Planner aligns textvisual pairs into binary-tree layout that preserves reading order and spatial balance; and the (c) PainterCommenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputsthough visually appealing at first glanceoften exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g., based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms 22-page paper into finalized yet editable .pptx poster all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster."
        },
        {
            "title": "Introduction",
            "content": "Academic posters play pivotal role in scientific communication, enabling rapid dissemination of key findings at conferences where attendees have only minutes to grasp core insights from the full papers. Despite significant progress in automated slide generation with systems such as PPTAgent [37] and D2S [29] pioneering text-to-slide pipelines poster creation [33, 30, 3] remains an underexplored and substantially more challenging task. Unlike slide decks, which distribute content across multiple, single-message slides, academic posters must condense an entire paper into single, visually coherent page. This requires (i) handling much longer multi-modal context [24], (ii) tightly interleaving text and graphics to convey complex ideas at glance [33, 3], and (iii) respecting stringent spatial constraints to avoid text overflow or layout collapse [10, 30]. These factors make VLMor LLM-only approaches insufficient: without explicit visual feedback like humans, it is difficult to reason about spatial layouts, maintain logical flow within confined canvas, ensuring legibility and aesthetic. To systematically evaluate poster generation, we propose the Paper2Poster Benchmark, the first benchmark and metric suite for this novel task. Our benchmark comprises recent conference papers Equal contribution. (cid:0) Corresponding to: xihe@uwaterloo.ca, kevin.qh.lin@gmail.com Preprint. Figure 1: Overview of this work. We address two core challenges in scientific poster generation: Left: How to create poster from paperwe propose PosterAgent (Sec. 4), framework that transforms long-context scientific papers (20K+ tokens) into structured visual posters; and Right: How to evaluate poster qualitywe introduce the Paper2Poster benchmark (Sec. 3), which enables systematic comparison between agent-generated and author-designed posters. paired with author-designed posters, along with human-and-model evaluation protocol that measures (i) Visual Quality how well the generated poster aligns visually with the human-designed version. (ii) Textual Coherence the clarity and fluency of the posters language. (iii) Holistic Assessment the overall aesthetic and informational quality, rated across six fine-grained dimensions by VLM as Judge. Notably, (iv) PaperQuiz motivated by the posters role as bridge between authors and readers, this metric evaluates how effectively the poster alone conveys core paper content by simulating diverse reader comprehension using VLMs to answer questions derived from the paper. To tackle multimodal context compression in Paper2Poster, we introduce PosterAgent, multiagent framework that first globally organizes document content and then performs panel-level refinementswhile weaving visual feedback into every stage. Starting with the Parser, we ingest the full paper PDF and transform it into an asset library of section-level text summaries and extracted figures and tables. Next, the Planner semantically matches each synopsis to its corresponding visual asset and generates binary-tree layout, allocating panels by estimated content length while preserving reading order and spatial balance. Finally, the PainterCommenter loop refines each panel: the Painter distills section-figure pairs into concise bullet points and renders draft panels via python-pptx code, and the Commentera VLM with zoom-in reference promptsprovides targeted feedback to correct text overflow and spatial alignment. This top-down, visual-in-the-loop design produces concise, coherent posters without manual tuning. Using Paper2Poster, we comprehensively evaluate human-designed (oracle) posters, state-of-theart generative models (e.g., GPT-4o), and multi-agent solutions, revealing several key insights: (i) GPT-4os outputs, though visually appealing at first glance, suffer from noisy or incoherent text, yielding high perplexity and poor PaperQuiz performance; (ii) VLM-based judging shows the primary aesthetic bottleneck is Engagement rather than informational content, since human posters convey meaning predominantly through visual semantics; (iii) PaperQuiz proves reliable metricVLM reader scores correlate closely with human evaluations, and more capable VLMs achieve higher scores on well-designed posters; and (iv) our Paper2Poster pipeline, built on fully open-source toolbox (e.g., Qwen-2.5-VL-7B), surpasses existing GPT-4obased multi-agent approaches on nearly all metrics while consuming 87% fewer tokens. Our findings illuminate pathways for the next generation of models and agent systems aimed at fully automated poster generation."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Visual Design Automation Recent advances in multi-modal learning have driven significant progress in automating visual design tasks. These tasks commonly fall into two broad categories: (i) Text-rich Image Generation. Tasks such as poster generation [3, 17, 11, 33] have greatly benefited from diffusion-based approaches [11, 10, 31], which enable the synthesis of detailed visuals conditioned on natural language descriptions. However, ensuring the quality and fidelity of embedded textual content via an end-to-end pixel generative model remains major challenge, as generated text at the pixel level appears blurry and hard to read. (ii) Complex Visual Layouts. Tasks like website designing [7, 27, 16, 23] or slide generation [37, 2, 8, 18, 26, 29] involve intricate visual structures and require integrating diverse components. To handle such complexity, mainstream approaches [37, 5] often employ agentic workflows that rely heavily on code generation and tool usage to assemble complete visual outputs. In contrast, our Paper2Poster addresses more demanding yet highly practical setting: scientific visual design based on academic papers. This involves long-context, interleaved multi-modal, inputs and outputs, posing substantial challenges in both effectiveness and computational efficiency. 2 2.2 Vision-Language Agents Recent progress has revealed the promising potential of LLMs beyond pure language understanding. Techniques such as ReAct [36, 35] have demonstrated that LLMs can serve as autonomous agents, capable of solving complex tasks through step-by-step reasoning and dynamic interaction via coding [32, 34], API function calling [25, 15], or UI interface interaction [13, 22, 19]. Despite these advances, general-purpose agents still struggle with professional tasks [12] as they require serious, accurate interaction and domain-specific knowledge. One closely related application is slide automation [5, 37], where agents translate brief textual queries into executable Python code (e.g., via python-pptx) to render presentation slides. However, our Paper2Poster setting is significantly more challenging: instead of text prompt, we take full-length academic papers as inputs and generate compact, wellstructured posters as output. This novel task requires careful design of both evaluation metrics and an effective, practical automation workflow."
        },
        {
            "title": "3 Paper2Poster Benchmark",
            "content": "3.1 Task Definition Given scientific paper composed of interleaved text, figures, and tables, the goal is to automatically generate single-page academic poster that faithfully conveys the papers core content in visually coherent and spatially efficient format. This task presents several unique challenges: a. Long-Context Long-Horizon Task: Scientific papers span multiple pages and thousands of words. Summarizing key insights while preserving coherence demands hierarchical understanding and selective abstraction. The complexity further necessitates long-horizon reasoning and multiple iterative interactions, making the task especially challenging. b. Interleaved Multimodal Inputs: Papers integrate numerous figures, tables, and charts, each semantically linked to the surrounding text. Successful poster generation demands the ability to extract, interpret, and align these multimodal elements in contextually appropriate manner. c. Layout-Aware Multimodal Outputs: Unlike tasks focused solely on text (e.g., blog) or vision, poster generation requires producing interleaved textimage outputs within constrained spatial layout. This necessitates joint reasoning over language, visual content, and layout to prevent overflow, imbalance, and logical misalignment. 3.2 Data Curation Data Source. We focus exclusively on AI papers for three key reasons: (1) they are relatively recent and undergo rigorous peer review, ensuring high scientific quality; (2) they offer diverse content across subfieldssuch as image-rich computer vision, text-centric NLP, and theory papers with numerous equationsproviding broad range of input modalities. To support this, we adopt the POSTERSUM dataset [24], which contains large collection of paperposter pairs from recent AI conferences including ICML, NeurIPS, and ICLR (20222024). We specifically use the test split to reduce the risk of overlap with training data. Diverse Sampling. Based on the initial candidate set, we apply two filtering criteria to curate highquality data: (1) Length Control: We deliberately include longer papers, including supplementary material, selecting PDFs that exceed 15 pages and extend up to 50 pages. (2) Latest Version: We manually retrieve the most recent PDF version for each paper to ensure the dataset reflects final camera-ready submissions. From the filtered set, we construct the final Paper2Poster dataset consisting of 100 paperposter pairs, stratified by publication year to ensure temporal balance: 33 pairs from 2022, 33 from 2023, and 34 from 2024. To further enhance diversity, we also stratify by source venueselecting 35 papers from NeurIPS, 37 from ICML, and 28 from ICLR, ensuring broad coverage across these leading conferences. Data Statistics. Overall, Paper2Poster comprises 100 paper-poster pairs spanning 280 distinct topics across domains such as Computer Vision (19%), Natural Language Processing (17%), and Reinforcement Learning (10%), ensuring comprehensive coverage across subfields. As illustrated in Fig. 2 (a-b), the input papers contain an average of 12155.7 words across 22.6 pages, amounting to approximately 20370.3 tokens, with an average of 22.59 figures per paper. In Fig. 2 (c-d), the corresponding author-designed posters include an average of 774.1 words (1416.2 tokens) and 8.7 figures. This reflects textual compression ratio of approximately 14.4 and figure reduction ratio of about 2.6 from paper to poster. 3 (a) Word cloud of topics (b) # of tokens (c) # of figures Figure 2: Data Statistics of Paper2Poster. (a) Word cloud illustrating the diversity of research topics. (b) Textual Token statistics and Figure count statistics for input papers vs. posters provided by authors. Overall, these statistics highlight that Paper2Poster is multimodal context compression task, requiring effective abstraction of both textual and visual content. 3.3 Evaluation Metrics To systematically measure the quality of generated posters, we establish comprehensive evaluation framework that covers four essential dimensions as shown in Fig. 3 (left): (i) visual quality, (ii) textual coherence, (iii) quality assessment via VLM (i.e.,VLM-as-judge), and notably our proposed (iv) PaperQuiz which measures how effectively the poster conveys the papers core knowledge. Figure 3: Left: Overview of the evaluation framework in Paper2Poster. Middle: We automatically generate multiple-choice questions from each paper using an LLM (o3), forming the our PaperQuiz evaluation. Right: In PaperQuiz, we simulate multiple reader by allowing VLMsrepresenting different expertise levels (e.g., student, professor)to read each generated poster and answer the quiz. The poster that achieves the highest average score is considered the most effective in conveying the papers content. (i) Visual Quality. The visual presentation of poster directly impacts reader comprehension and engagement. To evaluate visual quality from both global and local perspectives, we employ two metrics: (1) We measure Visual Similarity between the generated and the author-designed posters as ground-truth using CLIP image embeddings. This approach is favored over traditional distribution-based metrics (such as FID used in prior works [5, 37]), as it assesses instance-level semantic consistency. (2) We measure Figure Relevance by computing the average CLIP similarity between figures and their corresponding text sections in the original paper. This metric ensures figures are contextually appropriate and effectively integrated, assigning zero relevance to posters lacking visual content. For both metrics, we employ AltCLIP [4] due to its robustness in handling longer sequences alignment. Detailed definition of both metrics can be found in Appendix F.1. (ii) Textual Coherence. Clear and fluent text is essential for poster readability and comprehension. We therefore quantify textual coherence by computing the standard Perplexity (PPL) of the entire poster text under Llama-2-7b-hf. Lower PPL indicates more predictable, coherent language. detailed definition is provided in Appendix F.2. (iii) Holistic Assessment (VLM-as-Judge). To evaluate overall poster effectiveness in fine-grained dimension, we prompt VLM (e.g., GPT-4o) as an automated judge by outputting score (15). For each poster image, the model assigns 6 criterion-level scores: 3 under Aesthetic Score{Element Quality, Layout Balance, Engagement}, and 3 under Information Score{Clarity, Content Completeness, Logical Flow}. This direct, image-centric evaluation preserves fidelity to both visual design and content, while also capturing informativeness. It provides fine-grained feedback to guide future poster design. Full prompt templates and scoring protocols are detailed in Appendix F.3. 4 Figure 4: Illustration of the PosterAgent pipeline. Given an input paper, PosterAgent generates structured academic poster through three modules: 1. Parser: Extracts key textual and visual assets using combination of tools and LLM-based summarization, resulting in structured asset library. 2. Planner: Matches assets and arranges them into coherent layouts, iteratively generating panels with zoom-in operation. 3. PainterCommenter: The Painter generates panel-level bullet-content along with executable code, and renders the visual output, while the Commentera VLM with in-context referenceprovides feedback to ensure layout coherence and prevent content overflow. (iv) PaperQuiz. Given the posters central role in communicating the content of its source paperserving as bridge between authors and readerswe design an evaluation protocol that simulates this communication scenario. As shown in Fig. 3 (middle), each paper PDF is first submitted to o3 as examiner to generate 100 multiple-choice questions per paper: 50 verbatim questions (directly answerable from the text, spanning 13 content aspects) and 50 interpretive questions (targeting high-level comprehension across 10 conceptual dimensions). Next, as illustrated in Fig. 3 (right), we present each poster image to six VLMs (both openand closed-source), simulating range of reader standards from casual to expert. These models then answer the quiz based solely on the poster content. By comparing their quiz scores across different poster variants, we identify which poster best conveys the original paper content. Given that poster is visual medium rather than plain text like note, we further adjust the raw Quiz scores sr [0, 100] by incorporating length-based penalty, resulting in penalized score sa [0, 200]: (cid:16) sa = sr 1 + 1 max(1, L/W ) (cid:17) , where denotes the total text length of the poster, and is the median text length of human-designed (ground-truth) posters. Further details on metric design, question curation, evaluation workflow, and scoring procedures can be found in Appendix F.4."
        },
        {
            "title": "4 PosterAgent\nOverview. Identifying the challenges posed by the Paper2Poster, we formulate it as a problem of\nmultimodal context compression, and introduce PosterAgent, a multi-agent pipeline that adopts\na “Top-down” design philosophy: it first globally restructures the entire document into concise,\ncoherent sections, followed by local refinements for fine-grained, panel-level control. As shown in\nFig. 4. The pipeline consists of three key components: 1. Parser: Extracts key textual and visual\ncontent by tools and LLM-based summarization to build an asset library. 2. Planner: Aligns assets\nand arranges them into coherent layouts, generating panels iteratively with a zoom-in mechanism.\n3. Painter–Commenter: The Painter produces panel-level bullet points and executable code for\nrendering, while a VLM as Commenter—ensures layout coherence and avoids overflow.",
            "content": "4.1 Parser: global organization Given paper, the first step is to globally organize the information into structured format to support subsequent processing. This is handled by the Parser, which performs coarse-grained compression 5 by ingesting the raw PDF and producing an asset library across two modalities: (1) Text assets that capture the document hierarchy like human first glance focus on section headingeach key is section heading and the associated value paragraph-level synopsis; (2) Visual assets built in parallel, where figure or table captions serve as keys and the extracted image files are stored as values. We leverage MARKER[21] and DOCLING[14] to convert each page into Markdown, which is then processed by an LLM to generate structured, JSON-like outline. This transformation compresses the raw text into compact asset library that preserves essential semantics while significantly reducing size, enabling more efficient downstream iteration and layout generation. 4.2 Planner: local organization With the visual and text assets collected by the Parser, the next step is to select the relevant content and begin constructing the poster. Rather than generating the entire poster in one shot, we emphasize the importance of layout configuration and adopt an iterative, section-by-section completion processmirroring how humans typically start with template and sequentially fill in each section. Asset matching. This step aims to associate visual assets with corresponding textual contentfor example, matching teaser image to the introduction paragraph. We employ an LLM to semantically align each visual asset with its most relevant section from the asset library, resulting in set of (section, figure) pairs. Layout generation. An essential step is determining the panel-level layout, which requires precise absolute coordinates while accounting for the relative informativeness of each section. We found that directly predicting numerical coordinates using an LLM was unstable. Therefore, we adopt the binary-tree layout strategy [30], which reliably translates hierarchical constraints into panel bounding boxes by estimating content length (e.g.,, word number, figure size), maintaining reading order, and preserving aspect ratioensuring each poster section corresponds to well-defined panel. Panel iteration. Once the paper layout is configured, the next stage is to populate each panel with content. To ensure precise control, the Planner iterates over each sections synopsis and condenses it into concise, hierarchically structured bullet pointscreating compact format well-suited for poster panels. Inspired by how humans design postersinitially filling in content and iteratively refining it based on visual feedbackwe introduce the Painter-Commenter loop (Sec. 4.3), which mimics this process while maintaining visual clarity and appeal. After all panels undergo this process, the finalized poster is produced. 4.3 PainterCommenter: local refinement For each panel, the Painter converts its asset pair i.e., (section, figure) into executable code instructions and invokes the runtime environment to render draft panel image. Particularly, the Painter comprises two modules: (i) an LLM that ingests the section synopsis and distills it into concise set of bullet points, and (ii) deterministic code generator that leverages the python-pptx library together with predefined helper functions to generate presentation code, which is subsequently executed and rendered into an image of the current panel. However, in practice, single pass rarely produces flawless panel. To address this, we pair the Painter with Commentera VLM that evaluates the quality of the rendered panel image. While VLMs are promising, they often hallucinate in visual design tasks, leading to unreliable judgments. To mitigate this, we employ Zoom-in strategy that focuses attention on the panel region. Additionally, we enhance the Commenter with an in-context reference prompt containing two examples: one with severe overflow and one with an ideal layout. Guided by these references, the Commenter provides targeted visual feedbacksuch as overflow, too blank, or good to gowhich informs the Painters next revision. This loop continues until the Commenter signals success or maximum number of iterations is reached, ensuring each panel is accurate, readable, and visually well-balanced."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Baselines and Settings We evaluate four categories of baselines: (i) Oracle methods, which serve as upper boundsPaper (the original PDF with maximum informativeness) for content fidelity, and GT Poster (the author-designed poster from Paper2Poster) as the best possible presentation in terms of human understanding and layout quality; (ii) End-to-end methods, where GPT-4o directly generates posters either through text-based rendering4o-HTML (Markdown-to-HTML)or image genera6 Vis. quality & Txt. coherence VLM-as-Judge Model Vis. Sim. PPL Fig. Rel. Aesthetic score Information score Element Layout Engage. Avg. Clarity Content Logic Avg. Overall Oracle methods Paper GT Poster End-to-end methods 4o-HTML 4o-Image Multi-Agent methods OWL-4o PPTAgent-4o PosterAgent variants PosterAgent-4o PosterAgent-Qwen 0.53 1.00 0.52 0.76 0. 0.50 0.75 0.75 4.60 11.26 9. 77.13 11.46 6.20 8.31 8.81 0. 0.21 0.21 0.16 0. 0.24 4.05 4.07 3.53 2.93 2. 2.49 3.95 3.93 3.89 3.90 3. 3.02 3.62 3.05 3.86 3.67 2. 2.70 3.58 4.00 3.56 4.09 4.68 3.96 3.98 4. 3.89 3.98 2.72 2.75 3.36 3.94 2.90 1.05 3. 2.04 3.47 3.68 2.22 1.77 2.56 2.45 2.98 3. 2.66 2.05 2.89 1.26 3.36 3.39 1.38 1.56 3. 3.77 3.52 2.33 3.19 2.11 2. 2.89 3.58 4.03 3.50 3.95 3.96 3.85 3.60 3. 3.68 3.83 3.72 3.66 Table 1: Detailed evaluation of Paper2Poster across four categories of baselines, including Visual Quality & Text Coherence and VLM-as-Judge for fine-grained assessments. Oracle methods together (Paper or author-designed poster) serve as upper bounds in theory and strong baselines empirically. tion4o-Image (poster graphics produced via GPT-4os web interface); (iii) Multi-agent workflows, which decompose the task using specialized toolkitsOWL[6], general-purpose PDF-toHTML converter, and PPTAgent[37], Python-pptx-based slide generator, where candidate posters are selected via manual inspection; (iv) PosterAgent, our proposed approachPosterAgent-4o uses GPT-4o for both internal LLM and VLM commenter, while PosterAgent-Qwen is purely open-source solution, employs Qwen-2.5-7B for text generation and Qwen-2.5-VL-7B for commenter. 5.2 Main Results Visual Quality & Text Coherence. In the left part of Tab. 1, we evaluate visual quality Interestingly, while 4o-Image achieves the highest visual similarity, and textual coherence. it also records the worst perplexity, suggesting that although the generated posters may appear visually appealing at first glance, they often contain noisy or incoherent text. As expected, the original paper performs best in terms of textual coherence. Notably, the authordesigned poster (GT) still shows relatively high PPL, indicating that authors often prioritize visual appeal and reader engagement by conveying information through visual rather than textual means. Our PosterAgent achieves the highest figure relevance compared to PPTAgent, primarily due to our visual-semantic-aware asset library construction and asset matching. It also ranks second in visual similarity, closely following the human-designed poster. Above results highlight that each metric captures only specific aspect of quality and has its limitations. Therefore, we turn to the VLM-as-Judge and PaperQuiz next. VLM as Judge Metrics. In the right part of Tab. 1, we conduct comprehensive evaluation using suite of metrics. We find that both the Paper and GT Poster achieve the highest aesthetic and information scores. In contrast, 4o-Image performs poorly in terms of information, aligning with findings from previous PPL studies. Overall, PosterAgent-4o achieves an average score of 3.72, reaching level comparable to that of human-designed posters. Variants of PosterAgent that use GPT-4o as the visual commenter outperform those using Qwen2.5-VL-7B, highlighting the superior visual perception capabilities of 4o, particularly in panel refinement tasks such as preventing text overflow. scores Figure 5: PaperQuizs Avg. across different Reader VLMs (x-axis) for each poster type (legend lines). Refer to Append. Tab. 3 for full model names. PPTAgent frequently fails to replace placeholder content or fill templates properly, leading to meaningless text or large blank areas, and thus receives low scores in both aesthetics and informativeness. 7 Model Verbatim Interpretive open-source closed-source V-Avg open-source closed-source I-Avg Overall V-Avg I-Avg Overall Raw Accuracy Density-Augmented Score Oracle methods Paper GT Poster End-to-end methods 4o-HTML 4o-Image Multi-Agent methods OWL-4o PPTAgent-4o PosterAgent variants PosterAgent-4o PosterAgent-Qwen 51.45 51.75 52.45 48. 47.87 39.63 52.95 51.81 82.95 58. 48.00 30.89 31.96 11.99 49.17 48. 67.20 54.93 48.48 49.19 50.23 39. 50.78 50.19 39.92 25.81 49.94 36. 51.06 50.30 52.29 52.57 81.61 77. 75.14 70.67 74.38 37.15 78.42 76. 65.05 63.37 66.12 59.15 72.69 70. 71.52 103.56 120.00 111.78 62.96 60.43 56. 50.18 95.72 120.55 108.13 79.86 120. 100.36 62.16 36.68 51.04 31.25 78. 122.91 100.80 51.62 73.37 62.49 65. 64.62 58.21 57.46 101.87 130.39 116.13 100.35 128. 114.65 Table 2: PaperQuiz Evaluation on Paper2Poster based on 6 different Readers, including opensource and closed-source VLMs. Both Raw Accuracy and Density-Augmented Score are included for Verbatim and Interpretive settings. Oracle methods together (Paper or author-designed poster) serve as upper bounds empirically. Despite not generating images, 4o-HTML yields the highest aesthetic score among baselines, owing to its clean and structured layout. Overall, we found that the primary bottleneck in existing poster generation lies in Engagement, where all variants score below 3. In contrast, most variants achieve good Information scores, likely due to the robust long-context handling capabilities of GPT-4o. All PosterAgent variantseven those using Qwen2.5-7Bsurpass baselines in information quality, demonstrating the effectiveness of our content planning and generation framework in mitigating limitations of less capable LLMs. Although PPTAgent is also powered by GPT-4o, its rigid template-filling mechanism often fails to properly populate content, leading to poor performance. PaperQuiz. As shown in Tab. 2, we draw several key observations: (i) Verbatim questions are (ii) generally more challenging than those assessing broader understanding and interpretation. Without textual brevity penalties, Paper achieves the highest overall score. When the penalty is applied, the GT Poster performs best. This highlights both the comprehensiveness of the full paper and the value of concise, well-designed posters. It also reinforces how the PaperQuiz setup reflects poster generation as process of effective context compression, where careful condensation rather than sheer content volume is rewarded. (iii) GPT-4o supplies strong base ability. Its 4o-HTML variant outperforms OWL-4o, and even its purely visual 4o-Image generation surpasses PPTAgent-4o. Our proposed PosterAgent variants consistently achieve the best scores. (iv) Across all methods, performance on open-source reader models is consistently lower than on closed-source ones. This suggests that stronger perceptual ability correlates with better poster comprehension. (v) Notably, both 4o-HTML and OWL-4o, despite leveraging GPT-4o and generating lengthy, figure-free, blog-style outputs, are outperformed in raw accuracy by our PosterAgent-Qwen variant, even though they are exempt from brevity penalties. This result further affirms that PaperQuiz evaluates more than content volume; presentation quality matters. Our PosterAgent-Qwen surpasses more resource-intensive baselines despite relying on the relatively weaker Qwen-2.5-VL-7B, due to two key design choices: (a) structured, multi-step compression process that enables even weaker LMs to distill information with minimal loss; and (b) layout that presents information clearly and with logical reading order, making it easy for VLM-based readers to locate and interpret key points, similar to how clear visual structure supports efficient comprehension for human poster readers. PaperQuiz readers comparison. In Fig. 5, we compare the PaperQuiz scores of different readers on four baseline posters. On GT and PosterAgents posters, we observe that as model reasoning capabilities improve, their ability to interpret structured content also increases, leading to higher QA accuracy. In contrast, this trend is not evident for 4o-Image and Paper, suggesting that more capable models benefit more from poster layouts and condensed information than from information-dense papers, thereby improving their comprehension and response quality. Figure 8: Illustration of poster variants for the paper generated by different methods, including (a) Author designed, (b) Our PosterAgent, multi-agent methods (c) OWL [6] and (d) PPTAgent [37], pixel generative method (e) 4o-Image and website generative method (f) 4o-HTML. We provide the PaperQuizs augmented score for each method. Human evaluation. To assess our method with human judgment, we recruited PhD student to complete the PaperQuiz on 5 randomly selected papers from the Paper2Poster dataset, covering 4 baselines, 2 ground-truth variants, and 2 PosterAgent variants, following the setup in Section 5.1. Details of the human evaluation protocol are provided in Appendix G. Figure 6 demonstrates the average PaperQuiz scores across different types of posters (x-axis) for each reader (colored lines). PaperQuiz scores across different posters exhibit good consistency across both human and VLMs evaluations. This alignment supports the use of reader models as effective proxies to simulate human judgment. 5.3 Qualitative Analysis Figure 6: PaperQuizs Avg scores across different types of posters (x-axis) for readers (colored lines) on human evaluation subset. In Figure 8, we present quantitative comparison across different poster baselines for paper [20]. GPT-4os pixelbased generation produces visually acceptable layouts at first glance, but closer inspection (zoom-in region) reveals impaired text rendering, leading to poor readability of fine-grained details. 4o-HTML and OWL generate blog-like, text-dense posters that suffer from low visual readability. PPTAgent struggles with layout control, often resulting in missing panels. In contrast, our PosterAgent generates structurally coherent and readable posters, achieving the highest scores while using significantly fewer words than (c) and (f). However, there is still room for improvements compared to human-designed versions. Figure 7: Average token consumptions for different methods. Details are provided in Appendix E.1. 5.4 Efficiency Analysis Figure 7 presents the average token cost per poster across different methods. Our PosterAgent achieves great token efficiency, using only 101.1K (4o-based) and 47.6K (Qwen-based) tokensreducing cost by 60%87% compared to OWL-4o [6]. This translates to just $0.55 for 4o and $0.0045 for Qwen per poster, highlighting its effectiveness, (see Append. E.2 for further details)."
        },
        {
            "title": "6 Conclusions",
            "content": "We present new benchmark, Paper2Poster, for poster generation from academic papers, and we highlight the challenges and limitations of current generative models or agents in handling long-context, layout-sensitive tasks. Our proposed solution, the PosterAgent framework, leverages structured parsing, hierarchical planning, and visual feedback to significantly enhance generation quality. PosterAgent not only narrows the performance gap with human-designed posters but also establishes new efficiency standard, offering practical and scalable approach to scientific communication."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [2] Sambaran Bandyopadhyay, Himanshu Maheshwari, Anandhavelu Natarajan, and Apoorv Saxena. Enhancing presentation slide generation by LLMs with multi-staged end-to-end approach. In Saad Mahamood, Nguyen Le Minh, and Daphne Ippolito, editors, Proceedings of the 17th International Natural Language Generation Conference, pages 222229, Tokyo, Japan, September 2024. Association for Computational Linguistics. [3] Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu, Ying-Cong Chen, Lei Zhu, and Xinchao Wang. Posta: go-to framework for customized artistic poster generation. arXiv preprint arXiv:2503.14908, 2025. [4] Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, and Ledell Wu. Altclip: Altering the language encoder in clip for extended language capabilities. arXiv preprint arXiv:2211.06679, 2022. [5] Jiaxin Ge, Zora Zhiruo Wang, Xuhui Zhou, Yi-Hao Peng, Sanjay Subramanian, Qinyue Tan, Maarten Sap, Alane Suhr, Daniel Fried, Graham Neubig, and Trevor Darrell. Autopresent: Designing structured visuals from scratch. arXiv preprint arXiv:2501.00912, 2025. [6] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. GitHub repository, 2025. [7] Thisaranie Kaluarachchi and Manjusri Wickramasinghe. Webdraw: machine learning-driven tool for automatic website prototyping. Science of Computer Programming, 233:103056, 2024. [8] Keshav Kumar and Ravindranath Chowdary. Slidespawn: An automatic slides generation system for research publications. arXiv preprint arXiv:2411.17719, 2024. [9] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [10] Fengheng Li, An Liu, Wei Feng, Honghe Zhu, Yaoyu Li, Zheng Zhang, Jingjing Lv, Xin Zhu, Junjie Shen, Zhangang Lin, and Jingping Shao. Relation-aware diffusion model for controllable poster layout generation. arXiv preprint arXiv:2306.09086, 2024. [11] Zhaochen Li, Fengheng Li, Wei Feng, Honghe Zhu, Yaoyu Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Zhangang Lin, Jingping Shao, and Zhenglu Yang. Planning and rendering: Towards product poster generation with diffusion models. arXiv preprint arXiv:2312.08822, 2024. [12] Kevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, and Mike Zheng Shou. Videogui: benchmark for gui automation from instructional videos. arXiv preprint arXiv:2406.10227, 2024. 10 [13] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. [14] Nikolaos Livathinos, Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, and Peter W. J. Staar. Docling: An efficient open-source toolkit for ai-driven document conversion. arXiv preprint arXiv:2501.17887, 2025. [15] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic framework with extensible tools for complex reasoning. arXiv preprint arXiv:2502.11271, 2025. [16] Yuwen Lu, Ziang Tong, Qinyi Zhao, Chengzhi Zhang, and Toby Jia-Jun Li. Ui layout generation with llms guided by ui grammar. arXiv preprint arXiv:2310.15455, 2023. [17] Jian Ma, Yonglin Deng, Chen Chen, Nanyang Du, Haonan Lu, and Zhenyu Yang. Glyphdraw2: Automatic generation of complex glyph posters with diffusion models and large language models. arXiv preprint arXiv:2407.02252, 2025. [18] Ishani Mondal, Shwetha S, Anandhavelu Natarajan, Aparna Garimella, Sambaran Bandyopadhyay, and Jordan Boyd-Graber. Presentations by the humans and for the humans: Harnessing LLMs for generating persona-aware slides from documents. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26642684, St. Julians, Malta, March 2024. Association for Computational Linguistics. [19] Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A. Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M. Tamer Özsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, Perouz Taslakian, Spandana Gella, and Sai Rajeswar. Ui-vision: desktop-centric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661, 2025. [20] Seungeun Oh, Jihong Park, Sihun Baek, Hyelin Nam, Praneeth Vepakomma, Ramesh Raskar, Mehdi Bennis, and Seong-Lyun Kim. Differentially private cutmix for split learning with vision transformer. arXiv preprint arXiv:2210.15986, 2022. [21] Vik Paruchuri. marker: Convert pdf to markdown + json quickly with high accuracy. https: //github.com/VikParuchuri/marker, 2025. Accessed: 2025-05-13. [22] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [23] Juan A. Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte Suresh, François Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, Pierre-Andre Noel, Mats Leon Richter, Saverio Vadacchino, Shubham Agarwal, Sanket Biswas, Sara Shanian, Ying Zhang, Sathwik Tejaswi Madhusudhan, Joao Monteiro, Krishnamurthy Dj Dvijotham, Torsten Scholak, Nicolas Chapados, Sepideh Kharaghani, Sean Hughes, M. Özsu, Siva Reddy, Marco Pedersoli, Yoshua Bengio, Christopher Pal, Issam H. Laradji, Spandana Gella, Perouz Taslakian, David Vazquez, and Sai Rajeswar. Bigdocs: An open dataset for training multimodal models on document and code tasks. In The Thirteenth International Conference on Learning Representations, 2025. [24] Rohit Saxena, Pasquale Minervini, and Frank Keller. Postersum: multimodal benchmark for scientific poster summarization. arXiv preprint arXiv:2502.17540, 2025. [25] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, and et al. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. [26] Athar Sefid, Prasenjit Mitra, and Lee Giles. Slidegen: an abstractive section-based slide generator for scholarly documents. In Proceedings of the 21st ACM Symposium on Document Engineering, DocEng 21, New York, NY, USA, 2021. Association for Computing Machinery. 11 [27] Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2Code: Benchmarking multimodal code generation for automated front-end engineering. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 39563974, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. [28] Stability AI. Stable image ultra. https://platform.stability.ai/docs/ getting-started/stable-image, 2024. Accessed: 2025-05-16. [29] Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy X. R. Wang. D2S: Document-to-slide generation via query-based text summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14051418, Online, June 2021. Association for Computational Linguistics. [30] Yu ting Qiang, Yanwei Fu, Xiao Yu, Yanwen Guo, Zhi-Hua Zhou, and Leonid Sigal. Learning to generate posters of scientific papers by probabilistic graphical models. arXiv preprint arXiv:1702.06228, 2017. [31] Alex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie Li, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, et al. Textatlas5m: large-scale dataset for dense text image generation. arXiv preprint arXiv:2502.07870, 2025. [32] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [33] Sheng Xu and Xiaojun Wan. Posterbot: system for generating posters of scientific papers with neural models. Proceedings of the AAAI Conference on Artificial Intelligence, 36(11):13233 13235, Jun. 2022. [34] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [35] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. [36] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. [37] Hao Zheng, Xinyan Guan, Hao Kong, Jia Zheng, Weixiang Zhou, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. Pptagent: Generating and evaluating presentations beyond text-to-slides. arXiv preprint arXiv:2501.03936, 2025."
        },
        {
            "title": "Contents",
            "content": "A Limitations and Future Work Example Visualization Ablation Study Abbreviations More Analysis E.1 Efficiency Analysis . E.2 Cost Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Impact of Backbone Choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Definition of Evaluation Metrics F.1 Visual Quality Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Textual Coherence Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Holistic Quality Assessment via VLMs (VLM-as-Judge) . . . . . . . . . . . . . . F.4 PaperQuiz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Human Evaluation Protocol Error Analysis H.1 Text Integrity Issues . . H.2 Visual / Layout Flaws . H.3 Missing Visuals . H.4 Overflow Issues . . . Prompt Templates I.1 Baseline Prompts . I.2 Parser Prompts . . Planner Prompts . . . . . . . . . . . . . . Failure by Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Illustration of In-context reference for Commenter 14 15 22 22 22 22 23 23 23 24 33 33 33 33 33 37 37 38 40"
        },
        {
            "title": "A Limitations and Future Work",
            "content": "We spot limitation in the current design: the sequential execution of panel refinements constitutes the primary efficiency bottleneck. Each panels generaterevise cycle is structurally independent and could be parallelized, yet our implementation processes them serially to preserve modularity and output quality. As result, end-to-end poster creation takes approximately 4.5 minutes per documentacceptable for isolated use but restrictive for large-scale or interactive workflows. Introducing panel-level parallelism is clear avenue for future work, with the potential to dramatically reduce runtime and improve scalability in batch generation and real-time editing contexts. Future works. (i) well-considered poster should integrate external knowledge beyond paper such as community feedbacksuch as OpenReview comments and social media reactionsand leverage external assets like institutional icons and conference logos; and (ii) an improved workflow would involve humanAI collaboration, where the agent produces an initial draft, solicits user feedback, and iteratively refines its output to meet requirements. We leave these explorations in future."
        },
        {
            "title": "B Example Visualization",
            "content": "We present representative examples from our Paper2Poster dataset, which comprises 100 pairs of full-length research papers and their corresponding author-designed posters. For each selected paper, we show (a) the original poster created by the authorsdesigned to convey the papers abstract, methodology, results, and key visuals in single coherent layoutand (b) the poster automatically generated by our PosterAgent framework, demonstrating its ability to extract, summarize, and arrange multimodal content into visually balanced single-page design. These examples span range of subfields (reinforcement learning, anomaly detection, neuroscience) and illustrate how PosterAgent handles diverse layouts, content compression ratios, and figure-to-text integration. (a) Author-designed poster. (b) PosterAgent-generated poster. Figure 9: Posters for Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning. (a) Author-designed poster. (b) PosterAgent-generated poster. Figure 10: Posters for MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images. 14 (a) Author-designed poster. (b) PosterAgent-generated poster. Figure 11: Posters for Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (a) Author-designed poster. (b) PosterAgent-generated poster. Figure 12: Posters for Conformal Semantic Keypoint Detection with Statistical Guarantees. (a) Author-designed poster. (b) PosterAgent-generated poster. Figure 13: Posters for Neural Tangent Kernels for Axis-Aligned Tree Ensembles."
        },
        {
            "title": "C Ablation Study",
            "content": "We conduct ablation studies to evaluate three key design choices in PosterAgent: (1) the binary-tree layout strategy for layout planning; (2) the inclusion of commenter module as visual critic; and (3) the use of in-context examples to enhance the visual perception capabilities of the commenter. We define the following variants: Direct: replacing the binary-tree layout with direct layout generation by an LLM; Tree: using the binary-tree layout strategy but removing the commenter module; 15 (a) Author-designed poster. (b) PosterAgent-generated poster. Figure 14: Posters for Sparse Parameterization for Epitomic Dataset Distillation. (a) Author-designed poster. (b) PosterAgent-generated poster. Figure 15: Posters for Truly Scale-Equivariant Deep Nets with Fourier Layers. (a) Author-designed poster. (b) PosterAgent-generated poster. Figure 16: Posters for Identifying the Context Shift between Test Benchmarks and Production Data. 16 Tree + Commenter: including the commenter module but without in-context examples; Tree + Commenter + IC: the full system, with both the commenter and in-context examples. All ablation variants are implemented using PosterAgent-4o, keeping all other components unchanged to isolate the effect of each factor. We visualize and compare results across five randomly selected papers from Paper2Poster, as shown in Figures 17 to 21. When prompting the LLM to directly generate poster layouts (Direct), the results are often structurally compromised (e.g., Figures 17a19a), or resemble blog-style layouts that lack visual hierarchy and appeal (Figures 20a,21a). Fine-grained layout components, such as text boxes and figures, are especially challenging to synthesize in this setting: for instance, Figures17a20a exhibit missing text boxes that leave noticeable blank areas, and Figure 20a fails to preserve the correct aspect ratio of figures. The Tree variant, which omits the commenter module, leads to severe layout defects across all test cases (Figures 17b21b), primarily manifesting as text overflowwhere content spills outside its designated textbox or section panelresulting in overlaps with other text or visual elements. Using Tree + Commenter, which includes the commenter but without in-context examples, yields improved results compared to the variant without the commenter, but still exhibits noticeable issues. As shown in Figures 17c,18c,20c, and 21c, some degree of text overflow remains. Furthermore, Figures 19c and 20c highlight substantial unused white space that the commenter fails to flag in the absence of in-context guidance. Finally, the full Tree+Commenter+IC system achieves the best results, as detailed throughout the main paper and demonstrated in Fig. 17d,18d,19d,20d,21d. (a) Direct. (b) Tree. (c) Tree + Commenter. (d) Tree + Commenter + IC. Figure 17: Ablation study on Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval. Text overflow areas are highlighted with red bounding boxes. 17 (a) Direct. (b) Tree. (c) Tree + Commenter. (d) Tree + Commenter + IC. Figure 18: Ablation study on Visual Correspondence Hallucination. Text overflow areas are highlighted with red bounding boxes. 18 (a) Direct. (b) Tree. (c) Tree + Commenter. (d) Tree + Commenter + IC. Figure 19: Ablation study on DARTFormer: Finding The Best Type Of Attention. Text overflow areas are highlighted with red bounding boxes, large blank regions are highlighted with purple bounding boxes. 19 (a) Direct. (b) Tree. (c) Tree + Commenter. (d) Tree + Commenter + IC. Figure 20: Ablation study on CW-ERM: Improving Autonomous Driving Planning with Closed-loop Weighted Empirical Risk Minimization. Text overflow areas are highlighted with red bounding boxes, and large blank regions are highlighted with purple bounding boxes. 20 (a) Direct. (b) Tree. (c) Tree + Commenter. (d) Tree + Commenter + IC. Figure 21: Ablation study on DeepJoint: Robust Survival Modelling Under Clinical Presence Shift. Text overflow areas are highlighted with red bounding boxes."
        },
        {
            "title": "D Abbreviations",
            "content": "We provide reference for the abbreviations of models used in this paper in Tab. 3. Abbreviation Full Name llava-ov-7b phi4 gemini-2.0 llama4-17b 4o-mini LLaVA-OneVision-Qwen2-7b-ov-hf [9] Phi-4-multimodal-instruct [1] Gemini-2.0-Flash Llama-4-Scout-17B-16E-Instruct GPT-4o-mini Table 3: List of abbreviations and their full names."
        },
        {
            "title": "E More Analysis",
            "content": "E.1 Efficiency Analysis In Tab. 4, we evaluate the efficiency of PosterAgent against both direct generation and multi-agent baselines. While 4o-Image achieves the highest efficiency by avoiding multi-turn reasoning, it lacks layout-awareness. PosterAgent-Qwen-2.5-7B strikes strong balance, significantly reducing token usage and runtime (47.6K, 192.0s) compared to PPTAgent (255.7K, 230.7s), while maintaining output quality. This highlights the challenge, as well as the efficiency issue of Paper2Poster. E.2 Cost Analysis Token consumptions are depicted in Figure 7 and Table 4. Using GPT-4o as the backbone for both the LLM and VLM components, the average cost of generating single paper with PosterAgent-4o is approximately: 98.1 1000 1, 000, 000 based on OpenAIs GPT-4o API pricing as of May 22, 2025. 3 1000 1, 000, 5 + 20 = 0.55 USD, Using Qwen-2.5-7B as the backbone for LLM and Qwen-2.5-VL-7B as VLM, the average cost of generating single paper with PosterAgent-4o is approximately: 29.22 1000 1, 000, 000 0.04 + 3.56 1000 1, 000, 0.1 + 14.78 1000 1, 000, 000 0.2 = 0.0045 USD, based on OpenRouters API pricing as of May 26, 2025. E.3 Impact of Backbone Choices Table 5 compares four PosterAgent variants obtained by crossing two language models (LMs)GPT-4o and Qwen-2.5-7Bwith the same two models used as visionlanguage backbones (VLMs). Overall robustness. All configurations perform similarly. The PaperQuiz metric spans only 114.09 (Qwen4o) to 118.25 (4oQwen), spread approximately 4, indicating that PosterAgent is largely insensitive to the specific backbone combination. Open-source competitiveness. The fully open-source stack (QwenQwen) achieves PaperQuiz score of 114.65, trailing the best closed-source variant by merely 3.6. Strong performance is therefore attainable without proprietary APIs. Stable multimodal quality. Visual similarity and figure relevance vary by less than 0.01 across variants, underscoring the stability of our multimodal generation pipeline. LLM vs. VLM trade-off. Holding the LLM fixed, substituting Qwen for the VLM consistently improves PaperQuiz (4o-Qwen: +2.1 over 4o-4o; Qwen-Qwen: +0.56 over Qwen-4o). We attribute this to GPT-4o acting as stricter layout critic, trimming overflow text and modestly reducing information volume. Conversely, the stricter VLM raises aesthetic scores, yielding higher VLMas-judge ratings (4o-4o: 3.72 vs. 4o-Qwen: 3.58). The 4o-4o configuration thus offers the best balance between informativeness and visual appeal. 22 Model in_t (K) out_t (K) in_v (K) out_v (K) total_t (K) total_v (K) Input Tokens (K) Output Tokens (K) Total Tokens (K) Time (s) Cost ($) End-to-end methods 4o-HTML Multi-Agent methods OWL-4o PPTAgent-4o PosterAgent variants PosterAgent-4o PosterAgent-Qwen 18.53 2.15 356.48 202.46 4.62 33. 0 0 0 0 18.98 0. 28.85 29.22 2.95 3.56 69.25 14. 0.05 0.02 20.67 361.00 235.88 31. 32.78 0 0 19.85 69.30 14. 18.53 356.48 221.43 98.10 43.97 2. 4.62 34.29 3.00 3.58 20.67 62. 0.14 361.10 255.73 101.10 47.55 124. 230.70 1.87 1.79 281.55 124.29 0. 0.0045 Table 4: Efficiency Analysis in terms of text and vision tokens, and computation times. Prices of GPT-4o are based on OpenAIs GPT-4o API pricing as of May 22, 2025 ($5 / MTok for input and $20 / MTok for output). Prices of Qwen-2.5-7B ($0.04 / MTok input and $0.1 / MTok for output) and Qwen-2.5-VL-7B ($0.2 / MTok for both) are based on the ones offered by OpenRouter on May 26, 2025. Best scores in each column are bolded and second best are underlined. LLM VLM GPT-4o GPT-4o GPT-4o Qwen-2.5-7B Qwen-2.5-7B GPT-4o Qwen-2.5-7B Qwen-2.5-7B Vis. quality & Txt. coherence VLM-as-Judge Density-augmented Score Visual Similarity PPL Figure Relevance Aesthetic Information Overall V-Avg I-Avg Overall 0.75 0.75 0.76 0.75 8. 9.25 9.12 8.81 0.24 0.24 0. 0.24 3.58 3.33 3.57 3.50 3. 3.82 3.82 3.83 3.72 3.58 3. 3.66 101.87 130.39 116.13 105.61 130. 118.25 100.09 128.09 114.09 100.35 128. 114.65 Table 5: Ablation studies of our PosterAgent variants. Best scores in each column are bolded and second best are underlined."
        },
        {
            "title": "F Detailed Definition of Evaluation Metrics",
            "content": "We elaborate on the details of all four types of evaluation metrics applied in this study in this section. F.1 Visual Quality Metrics Two metrics fall into this type, namely Visual Similarity and Figure Relevance. Visual Similarity is computed as the cosine similarity between the CLIP image embeddings of the generated poster ˆP and the ground-truth poster . Concretely, letting denote the CLIP image encoder, we set zI (X) = CLIPimage(X) sVS = cosine_similarity(cid:0)zI ( ˆP ), zI (P )(cid:1) [1, 1]. (1) By operating at the instance level rather than comparing distributional statistics (e.g., FID [? ]), this measure directly captures semantic alignment and overall content fidelity between individual poster images. Figure Relevance assesses whether each figure in the generated poster is contextually appropriate. For set of figure crops {fi}N from the original paper, we compute image and text embeddings i=1 extracted from ˆP and their corresponding section text {ti}N i= We then define zI (fi) = CLIPimage(fi), zT (ti) = CLIPtext(ti). sFR = (cid:88) i= 1 0, cosine_similarity(cid:0)zI (fi), zT (ti)(cid:1), > 0, = 0. F.2 Textual Coherence Metrics We quantify textual coherence by computing the standard perplexity (PPL) of the poster text under the Llama-2-7b-hf language model. Specifically, let the poster be tokenized into sequence w1:n. The model assigns each token conditional probability p(wi w<i). We then define perplexity as 23 PPL = exp (cid:16) 1 (cid:88) i=1 log p(wi w<i) (cid:17) . Lower values of PPL correspond to more predictable and then more coherent text. We employ full-sequence PPL for its simplicity and direct interpretability in capturing overall textual fluency. F.3 Holistic Quality Assessment via VLMs (VLM-as-Judge) Each poster is scored on six criteria by visionlanguage model. For each criterion we supply dedicated prompt in tcolorbox using the prompt_func style; the model returns: {\"reason\": \"<justification>\", \"score\": <15>} Element Quality. This criterion evaluates the visual clarity, resolution, and stylistic consistency of individual graphic elements (figures, charts, icons). ı Prompt: Element Quality Judge System Prompt: You are an extremely discerning visual-element judge. Scrutinize every figure, chart, and image for any visual or stylistic issue. Always look for even subtle flaws: low contrast, imperfect resolutions, slightly inconsistent styles, crowded or mislabeled legends, etc. Be wary of awarding high scores unless the visuals truly meet the strictest standards. Instructions: Five-Point Scale 1 Point: Graphics are blurry, pixelated, or illegible. Color choices severely hinder interpretation. Visuals may significantly detract from comprehension. 2 Points: At least one graphic is clear, while others suffer from poor resolution or style. Legends or labels are missing or too small to read comfortably. Color schemes create some confusion or difficulty. 3 Points: Most graphics are legible and relevant, but have notable issues with consistency, sizing, or clarity. Some mismatches in style or color usage detract from cohesion. Minor but noticeable labeling/legend shortcomings. 4 Points: High-quality graphics with generally consistent styling. Clear legends and color schemes aid interpretation. Any remaining flaws are slight and do not significantly hinder understanding. 5 Points: Rarely awarded; strictly reserved for publication-grade visuals. Crisp resolution with no instances of blurriness. Harmonious color palette, impeccable labeling, and an exceptionally consistent style. Example Output: {\"reason\": \"...\", \"score\": int} 24 Think step by step and be conservative with your rating. Layout Balance. This criterion assesses the overall arrangement, alignment, and spacing of text and graphics to ensure coherent and readable poster structure. ı Prompt: Layout Balance Judge System Prompt: You are an uncompromising poster-layout judge. Critique the overall arrangement of all visual components (text blocks, headings, figures, white-space, alignment) that affect readability. Always scan for subtle alignment issues, uneven spacing, or any layout feature that might disrupt reader comprehension. Resist giving high scores unless the layout is exceptionally polished. Instructions: Five-Point Scale 1 Point: Highly disorganized layout; elements overlap, making text or graphics illegible. Margins are violated or reading path is nearly impossible to follow. Severely hinders comprehension. 2 Points: Some semblance of structure (columns/rows) but marred by inconsistent alignment or overcrowded sections. White-space distribution may be haphazard or insufficient. Reading flow is interrupted, though one can still piece it together. 3 Points: Recognizable structure with mostly consistent alignment and spacing. Some minor layout distractions remain (e.g., slightly cramped text, uneven spacing, small alignment slips). Generally readable but not particularly polished. 4 Points: Well-organized grid or arrangement; logical reading path that mostly flows. Appropriate font sizes, spacing, and alignment; only subtle layout imperfections. White-space usage clean and deliberate; nearly professional. 5 Points: Very rarely granted; must be pristine, professional-grade layout. Seamless alignment, balanced spacing, and expertly guided reading path. Flawless design synergy that maximizes readability and comprehension. Example Output: {\"reason\": \"...\", \"score\": int} Think step by step and be tough on small alignment/spacing issues. Engagement. This criterion judges how effectively the posters design elementscolor, typography, and compositioncapture and sustain viewer attention. 25 ı Prompt: Engagement Judge System Prompt: You are an uncompromising poster-aesthetics judge focusing on engagement. Be extremely critical of color harmony, typography, visual balance, and the posters ability to grab and hold attention. Always look for subtle issuescolor clashes, overly busy or dull designs, inappropriate font choices, awkward spacing, or anything that might reduce engagement. Reserve high scores for truly exemplary work. Instructions: Five-Point Scale"
        },
        {
            "title": "1 Point:",
            "content": "Visually off-putting; clashing colors or crowded design repel viewers. Typography choice is jarring or illegible at glance. Overall fails to engage or entice."
        },
        {
            "title": "2 Points:",
            "content": "Some visually appealing elements exist but are overshadowed by dull or inconsistent design moments. Font sizes or styles reduce accessibility or attractiveness. Limited capacity to draw an audiences focus. 3 Points: Shows generally pleasing color scheme and typography, though lacking wow factor. Balance and visual flow are acceptable but reveal minor weaknesses (e.g., slightly crowded or sparse areas). Engagement is average; neither strong nor particularly weak. 4 Points: Eye-catching design using mostly harmonious colors and effective typography. Good use of negative space; the layout guides the viewers eye effectively. Only minor flaws or bland spots prevent it from being top-tier. 5 Points: Rarely awardedreserved for truly striking, magazine-cover-caliber visuals. Flawless color palette and typography; everything works together seamlessly. Immediately captivating design that retains audience interest without any noticeable weakness. Example Output: {\"reason\": \"...\", \"score\": int} Think step by step and be very conservative when scoring. Clarity. This criterion evaluates sentence-level readability, grammar, and phrasing to ensure the text is polished and error-free. ı Prompt: Clarity Judge System Prompt: You are an uncompromising micro-text judge. Critically evaluate sentence-level clarity, grammar, phrasing, and intra-section coherence. Look for even subtle grammatical slips, confusing jargon, or clumsy phrasing. Be slow to award top marks unless the text is impeccably polished. Instructions: Five-Point Scale"
        },
        {
            "title": "1 Point:",
            "content": "Rampant grammatical or spelling errors; sentences may be unreadable. Overly technical jargon without explanations; fragments or run-ons predominate. Overall, text quality severely impedes understanding."
        },
        {
            "title": "2 Points:",
            "content": "Meaning is generally discernible, but multiple grammar or syntax problems appear in each section. Awkward or unclear phrasing disrupts the flow of reading. Only partial clarity is achieved."
        },
        {
            "title": "3 Points:",
            "content": "Overall readable text with few noticeable grammar or wording missteps. Occasional awkward phrasing or redundancies appear, but readers can follow without major confusion. Average clarity. 4 Points: Well-written, mostly free of grammatical or spelling errors. Terminology is used properly; text flows smoothly within paragraphs. Minor slip-ups can be present but do not disrupt understanding. 5 Points: Exceptional text quality, error-free, and elegantly phrased. Complex ideas conveyed with clear, concise language. Granted only if absolutely no grammatical, spelling, or stylistic flaws are detected. Example Output: {\"reason\": \"...\", \"score\": int} Think step by step. Content Completeness. This criterion measures whether all key sections are included and richly detailed, reflecting comprehensive coverage of the papers main contributions. ı Prompt: Content Completeness Judge System Prompt: You are an uncompromising content-depth judge. Assess whether the poster includes all essential sections and whether each section presents sufficient detail. Look for any missing or under-developed segments; do not hesitate to penalize for insufficient depth. Award the highest scores only if the poster expertly covers every necessary aspect. Instructions: Five-Point Scale 1 Point: Critical sections (e.g., objectives or results) are completely missing or trivial. Data grossly insufficient to comprehend the study or conclusions. Very poor depth that fails to convey essential information. 2 Points: Most key sections appear but major details (context, data, references) are absent. Lack of elaboration on methods or results leaves big gaps. Overall content too shallow to properly inform."
        },
        {
            "title": "3 Points:",
            "content": "All standard sections included with fundamental information. Some omissions or scant detail in certain areas (e.g., results or methodology). Only moderate depth; the reader must fill many gaps themselves."
        },
        {
            "title": "4 Points:",
            "content": "All essential sections present, each treated with adequate-to-strong detail. Robust description of objectives, methods, results, and references. Only minor improvements needed."
        },
        {
            "title": "5 Points:",
            "content": "Very rarely granted; everything must be comprehensive and thorough. Exhaustive detail on methodology, results (with statistics), interpretation, references, and future work. Leaves readers with minimal unanswered questions. Example Output: {\"reason\": \"...\", \"score\": int} Think step by step. Logical Flow. This criterion examines the coherence and progression of ideas across poster sections, ensuring seamless narrative from introduction to conclusion. ı Prompt: Logical Flow Judge System Prompt: You are an uncompromising macro-logic judge. Examine how well the posters major sections (Introduction, Methods, Results, Conclusions, etc.) connect to form coherent narrative. Pay attention to continuity, how logically each section flows from the previous, and whether there are any abrupt gaps. Only award the highest marks if the storyline is perfectly seamless. Instructions: Five-Point Scale 1 Point: Sections are disjointed; little to no logical connection between them. Key transitions or the central rationale is missing, creating confusion. 2 Points: General sequence recognizable but important logical steps are weak or missing. Readers must infer key links. 3 Points: Mostly coherent narrative with minor gaps. Transitions exist but some logical steps are lightly justified. 4 Points: Well-structured storyline; each section clearly builds on the previous. Transitions are stated, rationale is mostly strong. 5 Points: Extremely rare; flawless logical flow from introduction to conclusion. Seamless transitions; no inferential leaps. 28 Example Output: {\"reason\": \"...\", \"score\": int} Think step by step and penalize any noticeable logical gap or awkward transition. For each poster, we record all six criterion scores and compute two aggregated metrics: Aesthetic Score = Information Score = Element Quality + Layout Balance + Engagement 3 Clarity + Content Completeness + Logical Flow 3 . , F.4 PaperQuiz QA Dataset Curation. Each paper PDF is converted to markdown via our PDF parser. We then prompt o3 to generate 100 multiple-choice questions per paper, where we have 50 verbatim and 50 interpretive questions as follows: Verbatim questions (50): directly answerable from the paper text, covering 13 orthogonal content aspects (e.g., objectives, methodology, key results). Interpretive questions (50): requires high-level comprehension beyond verbatim text, spanning 10 conceptual dimensions (e.g., motivation, contribution synthesis, implication analysis). The exact prompts that are applied to generate the questions are given below, for verbatim and interpretive questions, respectively. ı Prompt: Generate Verbatim QA System Prompt: You are Question-Generation agent for academic posters. Your task is to read the supplied Markdown text (document_markdown) and produce exactly 50 multiple-choice QA items whose answers can be located verbatim or nearly verbatim in that text. The questions must be suitable for conference-poster readers: avoid deep theoretical proofs, reference lists, or citation minutiae. Follow all guidelines below precisely. Instructions: 1. Carefully read the Markdown in document_markdown. Each question must map to one clear sentence or phrase in the poster text. No duplicate or near-duplicate wording. 2. Write 50 factual, answerable-from-text questions. Vary difficulty from easy headline facts to specific numeric or procedural details. 3. Distribute the 50 questions across the following poster-friendly aspects, aiming for 25 questions per aspect and ensuring each aspect appears at least once: A. Title & authorship (title, author names, affiliations, keywords) B. Motivation / problem statement / research gap C. Objectives or hypotheses D. Dataset(s) or experimental materials E. Methodology (algorithms, model architecture, workflow steps) F. Key parameters or hyper-parameters (values, settings) G. Evaluation metrics or criteria H. Quantitative results (numbers in tables, charts) I. Qualitative findings, figures, or illustrative examples J. Comparative or ablation study results 29 K. Conclusions, implications, or contributions L. Limitations or future work M. Definitions of domain-specific terms or abbreviations 4. EXCLUDE references, citations, author acknowledgements, and any text that would not appear on standard poster. 5. Use the following JSON-for-each format (exact spelling & casing): { \"Question X\": { \"aspect\": \"<A-M>\", \"question\": \"<single sentence>\", \"options\": [ \"A. <choice 1>\", \"B. <choice 2>\", \"C. <choice 3>\", \"D. <choice 4>\" ], \"answer\": \"<Letter>. <exact correct option text>\" }, ... } 6. Output **only** the final JSON object containing 50 itemsno additional commentary. 7. Balance the correct answers roughly equally among options AD. Example Output: {\"Question 1\": {...}, \"Question 2\": {...}, ..., \"Question 50\": {...}} Think step by step and ensure full compliance with every guideline. ı Prompt: Generate Interpretive QA System Prompt: You are Question-Generation agent. Your task is to read the supplied Markdown text (document_markdown) and create exactly 50 multiple-choice questions that capture *high-level understanding* of the workits purpose, novelty, core approach, and overall findings. Every question must still be answerable by locating explicit sentences or phrases in the text; do not require inference that is absent from the poster-style content. Instructions: 1. Read the Markdown in document_markdown closely. Each question must map to explicit content in the text. Do not require inference beyond presented poster-level information. 2. Draft 50 factual questions probing the readers global grasp (e.g., What problem does the study address?). Avoid low-level numeric settings, code snippets, or reference lists. Vary wording and avoid duplicates. 3. Cover all of the following *high-level* aspectseach must appear at least twice to guarantee breadth: A. Research domain & background context B. Central problem / motivation / research gap C. Primary goal, hypothesis, or research question D. Key contributions or novelty statements E. Overall methodology or workflow (summarized) F. Principal findings or headline quantitative results G. Qualitative insights or illustrative examples H. Implications, applications, or significance I. Limitations or future-work directions J. Main conclusions or take-home messages 4. EXCLUDE citations, granular hyper-parameters, precise numeric tables, and acknowledgementsstick to poster-level overview content. 5. Return the questions in the following *strict* JSON schema: { \"Question X\": { \"aspect\": \"<A-J>\", \"question\": \"<one concise sentence>\", \"options\": [ \"A. <choice 1>\", \"B. <choice 2>\", \"C. <choice 3>\", \"D. <choice 4>\" ], \"answer\": \"<Letter>. <exact correct option text>\" }, ... } 6. Produce **only** the final JSON object with 50 entriesno commentary, headers, or extra lines. 7. The number of correct answers should be approximately balanced across AD. Document Markdown: {{ document_markdown }} Output ONLY the JSON with 50 questions below Evaluation Workflow. For each poster image, we query six VLM reader models to answer curated questions. These models include three open-source models (LLaVA-OneVision-Qwen2-7B-ov-hf, Phi-4-multimodal-instruct, and Llama-4-Scout-17B-16E-Instruct) and three closed-source models (o3, GPT-4o mini, and Gemini 2.0 Flash). Their outputs are evaluated according to two enforced rules: No external knowledge. Models must base answers solely on information present in the poster image. Visual citation. Each answer must include reference to the poster region supporting it (e.g., See Figure 2 caption); if no region contains the answer, the model responds NA. ı Prompt: Answer Questions System Prompt: You are an answering agent. You will be provided with: 1. An image of poster. 2. JSON object called questions which contains multiple questions. Each question has four possible answers: A, B, C, or D. 31 Your goal is to analyze the poster thoroughly and answer each question based on the information it provides. You should **NOT** use any external knowledge or context beyond the poster image. You must rely solely on the content of the poster to answer the questions. For each question: If you find enough evidence in the poster to decide on specific option (A, B, C, or D), then choose that option and include brief reference to the part of the poster that supports your answer (e.g., Top-left text, Event date section, etc.). If the poster does not offer sufficient information to confidently choose any of the options, respond with \"NA\" for both the answer and the reference. Instructions: 1. Study the poster image along with the questions provided. 2. For each question: Decide if the poster clearly supports one of the four options (A, B, C, or D). If so, pick that answer. Otherwise, if the poster does not have adequate information, use \"NA\" for the answer. 3. Provide brief reference indicating where in the poster you found the answer. If no reference is available (i.e., your answer is \"NA\"), use \"NA\" for the reference too. 4. Format your output strictly as JSON object with this pattern: { \"Question 1\": { \"answer\": \"X\", \"reference\": \"some reference or NA\" }, \"Question 2\": { \"answer\": \"X\", \"reference\": \"some reference or NA\" }, ... } 5. Do not include any explanations or extra keys beyond the specified structure. 6. You must provide an answer entry for all questions in the questions object. Example Output: { \"Question 1\": { \"answer\": \"B\", \"reference\": \"Description on the top-right of the poster\" }, \"Question 2\": { \"answer\": \"NA\", \"reference\": \"NA\" } } Scoring Metrics. Let sR be the raw accuracy (fraction of correctly answered questions) and the token count of the poster text. We define the density-augmented score sA = sR (cid:16) 1 + 1 max(1, l/w) (cid:17) , where is the median text length of ground-truth posters. The density multiplier is capped at 2 to penalize verbosity and reward concise, information-dense designs."
        },
        {
            "title": "G Human Evaluation Protocol",
            "content": "Instructions. Each human evaluator follows the instructions as follow, You will be given poster, as well as 6 text files containing the criteria to judge the poster. You need to read the poster and provide your scores according to the 6 text files criteria. Criteria. The criteria are the same as those outlined in PaperQuiz F.4."
        },
        {
            "title": "H Error Analysis",
            "content": "Generating scientific poster requires tight coupling of language understanding, visual synthesis, and spatial layout reasoning. Across the five pipelines we evaluate4o-Image, 4o-HTML, OWL-4o, PPTAgent, and our proposed PosterAgentwe consistently observe four high-level failure modes: text integrity issues, visual / layout flaws, missing visuals, and overflow issues. Below, we describe each class of error and highlight representative examples. H.1 Text Integrity Issues Legible text is crucial for conveying papers content. In imageonly generation (4o-Image), posters often contain garbled or unreadable text (Fig.22a) because pixel-level synthesis struggles with highresolution typography, underscores the fragility of text rendering when no explicit semantic control is applied. PPTAgent, as template-based method, exhibits different variant: placeholders are left intact or partly overwritten (Fig.22b), producing semantically corrupted content. H.2 Visual / Layout Flaws Pipelines without robust visual feedback frequently misplace or distort content. 4o-Image outputs can be truncated horizontally or vertically (Fig.23a, 23b) because the generator lacks hard spatial constraints. The same model sometimes hallucinates nonsensical figures (Fig.24a). Even with predefined template, PPTAgent may insert figures at unusably small scales (Fig.24b), or leave substantial blank regions when text or images are partially generated (Fig.25b). HTML-based agents such as OWL-4o also suffer from large empty areas (Fig. 25a) when their sequential code lacks iterative, visual validation. H.3 Missing Visuals Although OWL-4o is, in principle, able to invoke external toolkits for figure extraction, it fails to complete the full retrieval-insert cycle; the resulting posters remain purely textual (Fig. 26a) On the other hand, 4o-HTML 26b) by design is text-only, leading to similar issues. H.4 Overflow Issues Unlike HTML, where nested boxes naturally clip overflow, the PPTX format lacks strict parentchild containment. Consequently, both PPTAgent and PosterAgent sometimes produce text that spills beyond panel boundaries (Fig. 27b, 27a). Among the PosterAgent variants, the problem is relatively more pronounced in the Qwen variant, whose backbone (Qwen2.5-VL-7b) provides weaker visual grounding than GPT-4o, making its visual-feedback loop less reliable. 33 (a) poster generated by 4o-Image, where substantial corrupted text is generated. (b) poster generated by PPTAgent, where meaningless template placeholder text is remained. Figure 22: Examples of posters with corrupted text. (a) poster generated by 4o-Image, where the poster is cutoff horizontally due to incomplete generation. (b) poster generated by 4o-Image, where the poster is cutoff vertically due to incomplete generation. Figure 23: Examples of posters with cutoff. 34 (a) poster produced by 4o-Image, featuring figure that is low-resolution, visually corrupted, and unintelligible. (b) poster generated by PPTAgent, where figures are rendered too small to be legible. Figure 24: Examples of posters with obscure figures. (a) poster generated by OWL-4o, where there are large blanks on the poster. (b) poster generated by PPTAgent, where there are large blanks on the poster. Figure 25: Examples of posters with large blanks. (a) poster generated by OWL-4o, where no figures are inserted into poster. (b) poster generated by 4o-HTML, where no figures are inserted into poster. Figure 26: Examples of posters without figures. (a) poster generated by PosterAgent-Qwen, where there is text overflowing outside textbox. (b) poster generated by PPTAgent, where there is text overflowing outside textbox. Figure 27: Examples of posters with textual overflow."
        },
        {
            "title": "I Prompt Templates",
            "content": "I.1 Baseline Prompts We exhibit the prompt templates used to generate baselines: 4o-Image, 4o-HTML, and OWL-4o. ı Prompt: 4o-Image Carefully analyze the provided research paper and design professional, visually appealing academic conference poster. Include clear, informative text summaries, relevant figures, and tables that are neatly arranged and aligned. The poster should accurately represent the key findings, methods, and conclusions as if created by the original authors for presentation at scientific conference. Ensure the design includes all essential elements commonly found in academic posters. The layout should be engaging, easy to follow, and visually attractive, balancing textual clarity with graphic effectiveness. The poster should be of width widthpx and height heightpx. Generate through image generation. ı Prompt: OWL-4o Read the PDF file from: paper_path/paper.pdf Carefully analyze the provided research paper and design professional, visually appealing academic conference poster. Include clear, informative text summaries, relevant figures, and tables that are neatly arranged and aligned. The poster should accurately represent the key findings, methods, and conclusions as if created by the original authors for presentation at scientific conference. Ensure the design includes all essential elements commonly found in academic posters. The layout should be engaging, easy to follow, and visually attractive, balancing textual clarity with graphic effectiveness. You should approach the task by generating and executing python-pptx code to create singleslide PowerPoint presentation. You should save your code, as well as the generated PowerPoint file. ı Prompt: 4o-HTML System Prompt: You are document-to-poster generation agent. Your task is to read the supplied Markdown text (document_markdown) and design professional, visually appealing academic conference poster by generating an HTML file. Follow the guidelines below precisely. Instructions 1. Carefully read the Markdown in document_markdown. 2. Design full-page academic conference poster in HTML + CSS: Include prominent header with title, authors, and affiliations. [1ex] Break content into logical sections (Introduction, Methods, Results, Conclusions, etc.). Provide clear, informative text summaries. Embed relevant figures and tables, neatly arranged and aligned. Accurately represent key findings, methods, and conclusions. Ensure the layout is engaging, easy to follow, and visually attractive. Include all essential poster elements commonly found at scientific conferences. 3. Write complete HTML code (with inline or embedded CSS) that, when rendered, produces the poster layout. 5. The poster width should be poster_width px and height should be poster_height px. 37 4. **Output only** JSON object with single key \"HTML\", whose value is the entire HTML code for the poster. I.2 Parser Prompts We exhibit prompt templates used for parser: (1) The LLM summarization prompt; (2) The figure filtering prompt. ı Prompt: Paper Summarizer System Prompt: You are document content divider and extractor specialist, expert in dividing and extracting content from various types of documents and reorganizing it into two-level json format for later poster generation. Instruction: Based on given markdown document, generate JSON output for later poster generation, make sure the output is concise and focused. Step-by-Step Instructions: 1. Identify Sections and Subsections in document and identify sections and subsections based on the heading levels and logical structure. 2. Divide Content: Reorganize the content into sections and subsections, ensuring that each subsection contains approximately 500 words. 3. Refine Titles: Create titles for each section with at most 3 words. 4. Remove Unwanted Elements: Eliminate any unwanted elements such as headers, footers, text surrounded by \"\" indicating deletion. 5. Refine Text: For content, you should keep as much raw text as possible. Do not include citations. 6. Length: you should control the length of each section, according to their importance according to your understanding of the paper. For important sections, their content should be long. 7. Make sure there is poster title section at the beginning, and it should contain information like paper title, author, organization etc. 8. The \"meta\" key contains the meta information of the poster, where the title should be the raw title of the paper and is not summarized. 9. Ther **must** be section for the poster title. Example Output: { \"meta\": { \"poster_title\": \"raw title of the paper\", \"authors\": \"authors of the paper\", \"affiliations\": \"affiliations of the authors\" }, \"sections\": [ { }, { }, { } ] \"title\": \"Poster Title & Author\", \"content\": \"content of poster title and author\" \"title\": \"title of section1\", \"content\": \"content of section 1\" \"title\": \"title of section2\", \"content\": \"content of section 2\" } ı Prompt: Figure Filter System Prompt: You are an assistant that reviews posters JSON layout (json_content), along with corresponding image_information and table_information. Your task is to filter out any image or table entries that are irrelevant to the content described in json_content (for instance, if their captions or any provided details do not align with the topics, sections, or content in the poster). Specifically: 1. Read through the full poster data described in json_content. 2. Examine each entry within image_information and table_information. 3. Decide if each entry is relevant based on its caption, path, or any other information provided. - For example, if an image has caption that obviously does not fit into any section or does not relate to the posters content outline, deem it unimportant. 4. Keep only those images/tables you consider \"important\" for the poster (i.e., relevant to the topics, sections, or discussions mentioned in json_content). 5. Produce an output containing just two keys: \"image_information\" for the filtered images, and \"table_information\" for the filtered tables. Each of these keys should map to an array of filtered objects. You must output valid JSON containing only: { } \"image_information\": {...}, \"table_information\": {...} Instructions: The user will provide JSON: 1. \"json_content\": The content of the poster (sections, text, etc.). 2. \"image_information\": dict of images (each with caption, path, size constraints). 3. \"table_information\": dict of tables (each with caption, path, size constraints). Your task: 1. Read the poster outline (json_content). 2. Filter image_information and table_information so that only entries relevant to the poster content remain. Relevance is determined by matching or relating their captions to the posters sections or content. If an image or table does not clearly match or support any content in json_content, remove it. 39 3. Return JSON with the structure: { } \"image_information\": <filtered image information JSON>, \"table_information\": <filtered table information JSON> Output Format: Just return JSON object with the two keys: \"image_information\" and \"table_information\" each containing the filtered data. No additional keys or text. Both \"image_information\" and \"table_information\" should present even if they are empty. Note: If no entries remain for either images or tables, just return an empty dict for that key. Keep at most 5 entries in image_information and table_information respectively. Make sure the JSON you output is valid. Please provide only the JSON object as your final output."
        },
        {
            "title": "J Planner Prompts",
            "content": "We present the prompts used by the planner module, covering three components: (1) the asset matching prompt; (2) the painter prompt; and (3) the commenter prompt. ı Prompt: Asset Matching System Prompt: You are an expert assistant tasked with assigning images or tables to the most relevant poster sections. You will be given: JSON content of the poster outline, including each sections title and brief description. list of images (image_information) with captions and size constraints. list of tables (table_information) with captions and size constraints. Your goal is to produce JSON mapping of each top-level section to exactly zero or one image/table that best fits that sections content. For each top-level section (named in the provided JSON json_content), decide: Whether an image or table (or none) is most relevant to the sections theme or description. If relevant, select the single most appropriate image or table to assign. Base this selection on the conceptual content described in the section (research methods, results, conclusion, etc.) and compare it with the captions of the provided images or tables, choosing whichever fits best. If assigning an image, specify image: <id>, where <id>is the identifier of the chosen image from image_information. If assigning table, specify table: <id>, where <id>is the identifier of the chosen table from table_information. Include an additional reason field briefly explaining why this assignment was made (e.g., how the image/table relates to the section content). If no image or table is assigned to given section, omit that section from the final JSON (i.e., only list sections where you actually assign something). 40 Important Notes: The assignment should not be arbitrary. It must be logically consistent with the sections description and the provided caption for the image or table. Do not produce any layout properties or subsections here. The final output must be single JSON object, mapping from section names to the chosen image/table ID plus the reason field. If multiple images or tables are suitable, select the single best one and assign only that. If image_information or table_information is empty, you may end up assigning nothing to any section. Instructions: 1. Read and analyze the posters top-level sections from {{ json_content }}. 2. Look at {{ image_information }} and {{ table_information }}. Determine content-fit: If sections description or subject matter matches well with given image/table caption, consider assigning it. If multiple images or tables seem relevant, choose the single best fit. If none of the images or tables are relevant, or if none are provided, do not assign anything for that section. 3. Produce single JSON object. Each key is the exact name of top-level section (e.g., \"Introduction\", \"Methods\", \"Results\"), and the value is an object with: \"image\": \"reason\": short explanation describing why the image/table is image_id or \"table\": table_id assigned 4. If no assignment is made for section, exclude that section from the JSON. 5. No image can be reused for multiple sections. Each image/table can only be assigned to one section. 6. Ensure your final response strictly follows JSON syntax with no extra commentary. Example Output Format: { \"Introduction\": { \"image\": 1, \"reason\": \"Image 1 depicts the central concept introduced in this section.\" }, \"Results\": { \"table\": 2, \"reason\": \"Table 2 summarizes the key metrics discussed in the results.\" } } ı Prompt: Painter System Prompt: You are an expert assistant tasked with producing bullet-point summaries for given poster section. You will be given: JSON object summary_of_section that contains: { } \"title\": \"<section title>\", \"content\": \"<full text description>\" 41 An integer number_of_textboxes, which can only be 1 or 2. Your goal is to produce JSON object representing the bullet-point text for this poster section. Each textbox key (textbox1 or textbox2) maps to list of bullet-point entries. Each bulletpoint entry must be JSON object of the form: { \"alignment\": \"left\", \"bullet\": true, \"level\": <indent_level>, \"font_size\": <integer>, \"runs\": [ { } \"text\": \"<bullet point text>\" # optionally \"bold\": true or \"italic\": true if needed ] } Instructions: 1. If number_of_textboxes = 1, your final output must only have: { } \"title\": [ section title ], \"textbox1\": [ ... array of bullet items ... ] 2. If number_of_textboxes = 2, then you must produce two keys: textbox1 and textbox2, and each must have the same number of bullet items. For example: { } \"title\": [ section title ], \"textbox1\": [... bullet items ...], \"textbox2\": [... bullet items ...] where both arrays have identical length. 3. Each bullet point is JSON object with the structure shown above; you can create as many bullet points as needed (following the constraint about textbox count). 4. Make sure your final output is valid JSON, with no extra keys or additional formatting. 5. Return only the JSON object, nothing else. Example Output: Example when number_of_textboxes = 1: { \"title\": [ { \"alignment\": \"left\", \"bullet\": false, \"level\": 0, \"font_size\": 60, \"runs\": [ \"text\": \"Methodology\", \"bold\": true { } ] } ], \"textbox1\": [ 42 { \"alignment\": \"left\", \"bullet\": true, \"level\": 0, \"font_size\": 48, \"runs\": [ \"text\": \"Key point about domain-invariant component analysis.\" { } ] }, { \"alignment\": \"left\", \"bullet\": true, \"level\": 1, \"font_size\": 48, \"runs\": [ \"text\": \"Supporting detail.\", \"bold\": true { } ] } ] } Example when number_of_textboxes = 2: { \"title\": [ { \"alignment\": \"left\", \"bullet\": false, \"level\": 0, \"font_size\": 60, \"runs\": [ \"text\": \"Experimental results\", \"bold\": true { } ] } ], \"textbox1\": [ { \"alignment\": \"left\", \"bullet\": true, \"level\": 0, \"font_size\": 48, \"runs\": [ \"text\": \"Primary finding, bullet 1.\" { } ] }, { \"alignment\": \"left\", \"bullet\": true, \"level\": 0, 43 \"font_size\": 48, \"runs\": [ \"text\": \"Primary finding, bullet 2.\" { } ] } ], \"textbox2\": [ { \"alignment\": \"left\", \"bullet\": true, \"level\": 0, \"font_size\": 48, \"runs\": [ \"text\": \"Additional commentary, bullet 1.\" { } ] }, { \"alignment\": \"left\", \"bullet\": true, \"level\": 0, \"font_size\": 48, \"runs\": [ \"text\": \"Additional commentary, bullet 2.\" { } ] } ] } ı Prompt: Commenter System Prompt: You are an agent that is given three images: Negative Example: This image shows bounding box with text overflowing outside it (i.e., text crossing or cut off by the box). Positive Example: This image shows bounding box with text that fits completely (i.e., no text crossing or cut off). Target Image: This is the final image you must analyze. From the first two images, you learn to interpret: 1. Whether text is overflowing (text crossing, cut off, or otherwise cannot fully fit in the box). 2. Whether there is too much blank space in the bounding box (i.e., the text is significantly smaller than the box, leaving large unused space). 3. Whether the text and bounding box are generally well-aligned (no overflow, no large blank space). Then, for the Target Image, you must: If there is any overflow text, return \"1\". If there is too much blank space, return \"2\". If the text fits well (no overflow, no large blank space), return \"3\". 44 Instructions: 1. You are provided three images (negative example, positive example, and target). 2. Refer to the first two images (negative and positive examples) to understand: What text overflow looks like What too much blank space in bounding box means How generally well-fitted bounding box appears 3. Analyze the third (Target) images bounding box to check: If there is overflow text, return \"1\". If there is too much blank space, return \"2\". Otherwise (if everything looks good), return \"3\"."
        },
        {
            "title": "K Failure by Diffusion Models",
            "content": "In Fig. 28, we illustrate failure cases of Stable Diffusion Ultra [28]. We found that diffusion models suffer from the issues listed below and remain far from adequate for academic poster generation: (i) Severely inaccurate text rendering Generated text often appears blurry, misspelled, or semantically incoherent, failing to meet title, body, and caption requirements. (ii) Unpredictable layouts Models cannot reliably partition the page or align content blocks, resulting in disorganized visual hierarchy. (iii) Inconsistent styling Fonts sizes, spacing lack controllable parameters, making it impossible to conform to template guidelines. Illustration of In-context reference for Commenter In Fig. 29, we illustrate the in-context references used by our commenter during panel refinement to avoid undesirable cases such as overflow, too blank,. These examples are highlighted by red box as visual prompt. 45 Figure 28: Failure generation examples by Stable Diffusion Ultra model [28]. (a) Negative examples (b) Positive examples Figure 29: In-context references for the commenter help the VLM better identify whether the current panel falls into failure case."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Oxford",
        "University of Waterloo"
    ]
}