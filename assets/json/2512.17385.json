{
    "paper_title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
    "authors": [
        "Jiajun Wu",
        "Jian Yang",
        "Wei Zhang",
        "Lin Jing",
        "Yuqing Ma",
        "Ensheng Shi",
        "Yuchi Ma",
        "Zhoujun Li",
        "Xianglong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios."
        },
        {
            "title": "Start",
            "content": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models Jiajun Wu1, Jian Yang1, Wei Zhang1, Lin Jing1, Yuqing Ma2, Ensheng Shi2, Yuchi Ma2, Zhoujun Li2, Xianglong Liu1 1Beihang University; 2Huawei; {wuyuverse,jiayang}@buaa.edu.cn 5 2 0 2 9 1 ] . [ 1 5 8 3 7 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated strong capabilities in code generation, producing functional code from natural language descriptions. This progress has attracted substantial interest from both academia and industry due to its practical impact on software development. Closed-source Corresponding Author. Figure 1: Comparison between supervised and unsupervised paradigms for code generation. LLMS such as GPT-5 (OpenAI, 2025) and Claude4.5 (Anthropic, 2025) can generate file-level code with high accuracy, while open-source alternatives, including StarCoder (Li et al., 2023; Lozhkov et al., 2024), DeepSeek-Coder (Guo et al., 2024), and QwenCoder (Hui et al., 2024) have emerged as competitive solutions for code intelligence. Most existing approaches for improving code generation rely on supervised instruction tuning, where LLMs are fine-tuned on curated problemsolution pairs annotated by human experts or LLMbased annotated. However, creating high-quality instruction data requires substantial human effort in problem design, implementation, and verification, with costs increasing as model capabilities advance. The recent (Yue et al.; Ye et al., 2025; Chu et al., 2025) works emphasize that pre-training brings the knowledge, and post-training is weak at knowledge integration but focuses on knowledge utilization and alignment. These challenges motivate fundamental question: Can LLMs auFigure 2: Overview of the proposed six-stage self-bootstrapping framework for unsupervised code generation. tonomously improve their generation capabilities using post-training without any external corpus, relying only on pre-trained knowledge? In this work, we introduce an unsupervised framework that performs Internal Probing of LLMs for Code generation, enabling post-training without external corpora or human-annotated instruction data. Our approach exploits latent programming knowledge in LLMs and uses execution feedback as scalable, deterministic supervision signal grounded in program semantics. We implement six-stage self-bootstrapping process that generates diverse programming tasks, synthesizes test suites, samples candidate solutions, and applies execution-driven consensus clustering to identify correct implementations. High-consensus solutions are iteratively consolidated as training data, forming feedback loop that progressively improves model performance. Despite using no external data, UCoder achieves comparable performance to the supervised baseline across multiple benchmarks. The primary contributions of this work are: We successfully probe latent programming knowledge in LLMs by forcing models to generate programming problems and their solutions, then identify correct solutions by finding clusters of similar implementations. Then, the self-training method progressively improves the LLM by reinforcing solutions. Based on the self-generated data from the unsupervised framework using internal probing of LLMs (IPC) without any external data, UCoder (7B, 14B, 32B) achieves performance competitive with supervised baselines. We provide empirical analysis showing that self-generated data maintains rich lexical, semantic, and structural diversity, while consensus-based selection improves solution quality and exhibits inverse scaling behavior."
        },
        {
            "title": "2.1.1 Supervised Code Generation\nSupervised code generation is formulated as a\nsequence-to-sequence learning task. Given a train-\ning dataset D = {(xi, y)\ni }N\ni=1, the model parame-\nters are optimized by maximizing log-likelihood:",
            "content": "θ = arg max θ (cid:88) i=1 log pθ(yixi) (1) where xi denotes natural language query, yi represents the corresponding reference implementation, and pθ(yx) is the conditional distribution over code sequences parameterized by θ. We adopt Pass@k (Chen et al., 2021a) to evaluate code correctness, which measures the probability that at least one of independently sampled solutions passes all test cases."
        },
        {
            "title": "2.1.2 Unsupervised Code Generation\nUnsupervised code generation aims to improve\nLLM code generation capabilities without human-\nannotated supervision. Given an initial model\nM0 : X → Y, the objective is to develop a self-\nimprovement algorithm A producing an enhanced\nmodel M ∗ = A(M0) such that Pass@k(M ∗) >\nPass@k(M0) on held-out test sets, without ac-\ncess to paired training data (x, y) ∈ X × Y. This\npresents three fundamental challenges: (1) Prob-\nlem Space Construction. Automatically gen-\nerating diverse programming problems with ap-\npropriate difficulty distributions while maintain-\ning semantic clarity; (2) Unsupervised Correct-\nness Verification. Assessing functional correct-",
            "content": "ness without reference implementations; (3) SelfBootstrapping Signal Construction. Extracting reliable training signals from noisy candidates while ensuring iterative stability. We address these through an execution-driven consensus mechanism coupled with self-bootstrapping framework, detailed in the following sections."
        },
        {
            "title": "2.2 Probing Internal Knowledge in LLMs",
            "content": "LLMs encode extensive programming knowledge through pre-training, yet this knowledge remains implicit and difficult to elicit. We propose sixstage framework to surface and reinforce these latent capabilities, as shown in Figure 2. First, we probe the problem space (Stages 13) by prompting the model to generate algorithmic problems with complete specifications, revealing its understanding of programming paradigms and data structures. Representative examples of problem generation, difficulty assessment, and solution skeleton construction are illustrated in Figure 3. We then assess semantic understanding (Stage 4) by generating approximately 100 test cases per problem to identify boundary conditions and edge cases. At the core (Stage 5), we probe the solution space via dense sampling, where executiondriven consensus clustering reveals that correct implementations form tight clusters while incorrect ones are dispersed. We quantify solution quality using execution success rate e(r), consensus strength s(r), and code fluency (r). Finally (Stage 6), we consolidate high-consensus samples through supervised fine-tuning, reinforcing correct patterns. The process forms positive feedback loop: at iteration t, the improved LLM Mt produces higherquality candidates, enabling more reliable selection and further strengthening Mt+1. Our experimental results in subsection 3.2 validate that pre-trained models already contain the knowledge required to solve target tasks in implicit form."
        },
        {
            "title": "2.3 Execution-Driven Consensus Clustering",
            "content": "Our approach exploits that correctness is singular while incorrectness is diverse: correct implementations produce identical outputs, but incorrect ones fail heterogeneously. This clustering structure allows the maximum-consensus cluster to indicate correctness without ground truth, formalized in Theorem 2.4 and validated in subsection 4.2."
        },
        {
            "title": "2.3.1 Definitions",
            "content": "Consensus clustering has three definitions. Figure 3: Problem space probing proceeds through three stages: problem generation with function signatures and input-output contracts, difficulty rating assessment and categorization, and solution skeleton generation with implementation structure. Definition 2.1 (Execution Signature). Given candidates = {r1, . . . , rn} and tests = {t1, . . . , tm}, define Exec : {0, 1} as the pass indicator: Exec(ri, tj) = 1 if ri passes tj, and 0 otherwise. The execution signature of ri on is σ(ri; ) = Exec(ri, tj), (2) (cid:77) j=1 where (cid:76) denotes ordered concatenation. Thus σ(ri; ) {0, 1}m, and σ(ri; ) = 1m indicates that ri passes all tests. Definition 2.2 (Consensus Clusters). If σ(ri, ) = σ(rj, ), we can regard the ri and rj as equivalent solution. Given the value of σ(ri, ), we can partition into clusters = {C1, . . . , Cℓ} of behaviorally identical candidates. Definition 2.3 (Quality Metrics). Each candidate is scored by: e(r) = {t : Exec(r, t) =} , s(r) = {r : σ(r) = σ(r)}, (r) = exp 1 r (cid:88) i=1 (3) log p(xi x<i) , where e(r) measures execution success, s(r) consensus strength, and (r) code fluency."
        },
        {
            "title": "2.3.2 Hierarchical Selection\nWe select the valid candidates using three criteria:\n(1) Reliability Filtering. Candidates with low ex-\necution success are removed (threshold ρ = 0.8):\nR′ = {r ∈ R : e(r) ≥ ρ}.\n(2) Consensus Selection. We select the largest non-\ntrivial cluster: C∗ = arg maxC∈C′, |C|≥τ |C|",
            "content": "Model HumanEval HE HE+ MBPP MBPP+ MBPP CodeLlama-7B-Instruct DS-Coder-6.7B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B Qwen2.5-Coder-7B-Instruct UCoder-7B CodeLlama-13B-Instruct Starcoder2-15B-Instruct-v0. DS-Coder-V2-Lite-Instruct Qwen2.5-Coder-14B Qwen2.5-Coder-14B-Instruct UCoder-14B CodeLlama-34B-Instruct DS-Coder-33B-Instruct DS-Coder-V2-Instruct Qwen2.5-Coder-32B Qwen2.5-Coder-32B-Instruct UCoder-32B 40.9 74.4 83.5 61.6 88.4 83.5 40.2 67.7 81.1 64.0 89.6 87. 48.2 81.1 85.4 65.9 92.7 89.0 33.5 71.3 78.7 53.0 84.1 76.8 32.3 60.4 75.6 57.9 87.2 81.1 40.2 75.0 82.3 60.4 87.2 82.9 GPT-4o-2024-08-06 Claude-3.5-Sonnet92.1 92.1 86.0 86.0 39.9 74.9 79.1 76.9 83.5 85.2 60.3 78.0 82.8 81.0 86.2 86.5 61.1 80.4 89.4 83.0 90.2 89.7 86.8 91. 33.6 65.6 69.0 62.9 71.7 72.2 51.1 65.1 70.4 66.7 72.8 74.3 Full Hard Bench Bench 4.1 15.5 18.9 16.2 20.3 22.3 7.1 15.5 23.2 - 18.2 22.9 21.9 35.5 43.2 - 41.0 41.1 3.4 10.1 18.2 - 18.2 15.5 25.40 40.16 41.08 - 47.95 51.27 BCB-Complete BCB-Instruct LiveCodeFullStackHard Full 6B+ LLMs 25.7 43.8 50.9 45.8 48.8 52.0 13B+ LLMs 31.7 45.1 47.6 51.8 56.7 53.9 32B+ LLMs 35.6 51.1 59.7 53.6 58.0 55. 27.56 48.19 56.37 - 56.88 53.35 27.00 42.68 - - 55.28 52.52 8.8 17.6 24.3 - 27.0 17.6 10.8 20.9 29.7 26.4 33.8 27.7 29.0 42.0 48.2 - 49.6 45.7 8.4 21.3 27.9 - 31.4 21. 6.1 12.1 24.3 - 23.4 20.6 6.8 14.9 18.2 22.3 29.7 24.3 28.5 37.2 36.8 - 48.4 40.9 9.5 11.5 16.2 - 22.2 16.2 50.5 70.1 75.1 68.2 75.1 75.7 Closed-APIs 72.5 74.6 - 58.6 36.5 35.1 50.1 46.8 25.0 25.7 34.6 31. 58.89 60.70 Table 1: Performance comparison of Qwen2.5-Coder Base and Instruct models with our iterative SFT models across code generation benchmarks. All metrics represent Pass@1 execution rates (%). Complete split is reported for Base models and Instruct split for Instruct models. Bold indicates best performance within each size category. - denotes unavailable or inapplicable results. (3) Intra-Cluster Selection. Within C, we choose = arg maxrCe(r), (r)."
        },
        {
            "title": "2.3.3 Theoretical Guarantee",
            "content": "Theorem 2.4 (Consensus Convergence). Let = {r1, . . . , rn} be candidates sampled independently from model, and let denote set of unit tests. Assume that at least candidates in are functionally correct with probability at least 1 δ, and that any pair of incorrect implementations produces identical outputs on single test with probability at most < 1. If the test set size satisfies log(n/k) log , then the largest consensus cluster Cmax contains only correct implementations with probability at least (Cmax is correct) 1 δ n2pT . Definition 2.5 (Iterative Update). At iteration t, we construct training set Dt = {(qi, )}, where each is selected via consensus from candidates sampled from Mt, and update: θt+1 = arg max θ (cid:88) (q,r)Dt log pθ(r q). (4) Why Self-Training Improves Performance? Iterative self-training is effective because consensus selection acts as quality filter. Let Q(r) [0, 1] denote candidate quality. For independent samples, random selection yields expected quality ErMt[Q(r)], whereas consensus selection favors correct implementations that cluster by execution behavior, achieving (for some > 0): E[Q(r)] = ErMt [Q(r)] + , (5) Optimizing on Dt shifts the model toward higherquality samples. As Mt+1 increases pθ(r q) for above-average outputs: ErMt+1 [Q(r)] ErMt [Q(r)], (6) 2.4 Iterative Self-Training We formalize the iterative self-training procedure and explain why it yields consistent improvement. where it induces positive feedback loop where improved models generate higher-quality candidates and more reliable training signals. Iter HumanEval MBPP HE HE+ MBPP MBPP+ BCB-Complete Hard Full BCB-Instruct Full Hard LiveCodeFullStackBench Bench 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 77.4 81.7+4.3 84.1+6.7 84.1+6.7 84.1+6.7 81.7+4.3 83.5+6.1 83.5 85.4+1.9 84.8+1.3 84.8+1.3 84.8+1.3 87.8+4.3 87.2+3.7 86.0 87.8+1.8 86.6+0.6 87.8+1.8 89.0+3.0 88.4+2.4 88.4+2. 67.1 74.4+7.3 77.4+10.3 76.8+9.7 77.4+10.3 75.0+7.9 76.8+9.7 76.8 77.4+0.6 76.2-0.6 78.0+1.2 78.0+1.2 81.1+4.3 80.5+3.7 78.0 82.3+4.3 81.1+3.1 81.1+3.1 82.9+4.9 81.7+3.7 82.3+4.3 72.0 72.00.0 79.1+7.1 81.2+9.2 79.1+7.1 83.9+11.9 85.2+13.2 75.9 87.8+11.9 84.9+9.0 83.6+7.7 84.1+8.2 86.5+10.6 84.4+8.5 86.2 89.9+3.7 88.4+2.2 88.9+2.7 89.7+3.5 87.8+1.6 89.2+3. 63.0 63.00.0 66.4+3.4 69.0+6.0 66.4+3.4 71.2+8.2 72.2+9.2 64.0 73.3+9.3 70.6+6.6 72.2+8.2 72.8+8.8 74.3+10.3 71.7+7.7 72.2 75.9+3.7 74.6+2.4 74.6+2.4 75.7+3.5 73.3+1.1 74.1+1.9 Ucoder-7B 44.4 51.3+6.9 44.7+0.3 44.40.0 44.7+0.3 52.2+7.8 52.0+7.6 15.5 23.0+7.5 14.9-0.6 15.50.0 14.9-0.6 19.6+4.1 22.3+6.8 Ucoder-14B 53.3 53.1-0.2 50.4-2.9 51.6-1.7 51.8-1.5 53.9+0.6 53.30.0 Ucoder-32B 54.8 55.4+0.6 56.2+1.4 54.3-0.5 55.4+0.6 54.5-0.3 54.0-0. 23.6 21.6-2.0 23.60.0 23.0-0.6 18.2-5.4 24.3+0.7 23.0-0.6 28.4 23.6-4.8 23.0-5.4 22.3-6.1 27.7-0.7 24.3-4.1 23.6-4.8 34.6 42.2+7.6 35.8+1.2 34.5-0.1 35.8+1.2 40.7+6.1 41.1+6.5 43.2 41.0-2.2 40.9-2.3 41.8-1.4 41.1-2.1 40.9-2.3 43.5+0.3 44.6 44.5-0.1 44.8+0.2 43.8-0.8 45.7+1.1 43.8-0.8 44.1-0.5 14.9 20.9+6.0 12.2-2.7 13.5-1.4 12.2-2.7 14.2-0.7 15.5+0. 16.2 14.2-2.0 15.5-0.7 15.5-0.7 12.8-3.4 16.20.0 16.20.0 18.9 17.6-1.3 18.90.0 16.9-2.0 17.6-1.3 18.90.0 16.9-2.0 13.0 15.3+2.3 14.5+1.5 21.4+8.4 14.5+1.5 20.6+7.6 22.9+9.9 22.1 17.6-4.5 19.1-3.0 18.3-3.8 22.10.0 20.6-1.5 22.9+0.8 22.1 17.6-4.5 16.0-6.1 19.8-2.3 21.4-0.7 22.9+0.8 22.9+0.8 40.2 48.2+7.9 40.2-0.1 40.20.0 40.2-0.1 50.0+9.7 51.3+11. 50.1 53.6+3.5 49.9-0.2 49.8-0.3 50.4+0.3 52.5+2.4 51.6+1.5 53.0 54.3+1.2 52.4-0.6 54.7+1.7 53.4+0.3 53.3+0.2 53.8+0.7 Table 2: Performance (Pass@1) across iterative SFT rounds at different model scales using Qwen2.5-Coder as the base model. Orange-highlighted rows show Iter 0 (initial model trained on seed data), while subsequent iterations use self-generated synthetic data. Blue-highlighted rows indicate the best-performing iteration for each scale. Bold denotes best performance, underline denotes second-best performance for each metric within each scale, and subscripts show differences from Iter 0 (green for improvement, red for decline)."
        },
        {
            "title": "3.1 Training and Evaluation Details",
            "content": "Model Configuration. We experiment with Qwen2.5-Coder(Hui et al., 2024) models at 7B, 14B, and 32B scales, starting from base checkpoints without prior instruction tuning and applying identical self-bootstrapping procedures across all scales for fair comparison. Training Hyperparameters. We employ consistent training settings across experiments. Models are fine-tuned for 3 epochs per iteration using AdamW with learning rate of 5e-6 and cosine decay schedule. We use batch size of 128 with gradient accumulation to fit memory constraints. Evaluation Benchmarks. We evaluate on six benchmarks: HumanEval (Chen et al., 2021b) and MBPP/MBPP+ (Austin et al., 2021) assess classic Python programming; LiveCodeBench (Jain et al., 2024) provides contamination-free competitive programming problems; BigCodeBench (BCB) (Zhuo et al., 2024) evaluates function completion with broader context and API usage (both Complete and Instruct variants); and FullStackBench (Liu Figure 4: Lexical entropy distribution of 16,867 generated problems. Histogram with KDE shows perproblem entropy; CDF (green) and boxplot show cumulative coverage. et al., 2024) covers diverse real-world scenarios. We report Pass@1 accuracy with execution-based validation; solutions must pass all test cases."
        },
        {
            "title": "3.2 Main Results",
            "content": "Table 1 demonstrates that unsupervised selfbootstrapping achieves performance comparable to supervised instruction tuning across diverse code generation benchmarks. Compared with other instruction-tuned models of similar scale, UCoder exhibits competitive or superior performance, consistently matching or exceeding Qwen2.5-CoderInstruct on challenging benchmarks including"
        },
        {
            "title": "Iterative",
            "content": "Effectiveness. Framework selfbootstrapping consistently improves performance without external supervision. Across benchmarks, all model scales show substantial gains over seed-trained baselines (Iter 0), with improvements of +6.1 to +13.2 points at 7B, +4.3 to +10.6 at 14B, and +3.0 to +4.9 at 32B. Gains are most pronounced on benchmarks requiring diverse programming skills, such as MBPP, FullStackBench, and LiveCodeBench, rather than narrowly scoped tasks like HumanEval. This supports our hypothesis that self-generated problem diversity combined with consensus expands capability coverage beyond seed data. execution-driven Figure 5: Complexity versus semantic coverage distribution. Color encodes density; red line shows linear trend (r = 0.664). Inverse Scaling of Improvement. Performance gains exhibit an inverse scaling trend, with smaller models benefiting disproportionately. We attribute this effect to latent capability gaps, where pretrained knowledge is only partially accessible through standard prompting. Consensus-based selection mitigates this gap by reinforcing correct patterns that smaller models generate inconsistently. Notably, the self-improved 7B model reaches 85.2% on MBPP, approaching the 32B baseline (86.2%), highlighting self-bootstrapping as compute-efficient alternative to model scaling. Convergence Characteristics. The optimal number of iterations decreases with model scalesix for 7B, five for 14B, and four for 32B. Beyond these points, performance exhibits mild oscillation rather than degradation, reflecting trade-off between specialization on synthetic data and generalization to held-out distributions. These observations motivate early stopping based on validation performance."
        },
        {
            "title": "4.1 Diversity of Self-Generated Problems",
            "content": "Lexical Diversity. We quantify lexical diversity using Shannon entropy = (cid:80) pi log2 pi, where pi denotes token probability. Figure 4 shows near-Gaussian entropy distribution (mean µ = 3.64 bits, σ = 0.69, median 3.62), indicating natural variation rather than templated construction. The smooth CDF and moderate interquartile range further suggest balanced coverage from concise to elaborate specifications. Semantic Coverage. As shown in Figure 9 (Appendix), the generated problems contain 229 domain-specific terms across seven categories, with Figure 6: Quality distribution characterization. Top: perplexity distribution across 9,700 samples with quartile stratification and KDE overlay (truncated at 1.12). Bottom: perplexity versus execution success rate, showing high-quality samples (80%+ success) concentrated below perplexity 1.05. MBPP+, BigCodeBench-Complete, and FullStackBench at all model scales (7B, 14B, 32B). Although certain Instruct models retain advantages on HumanEval, our approach progressively narrows this gap as model scale increases. These results indicate that execution-driven self-training can effectively elicit the latent instruction-following capabilities embedded in pre-trained code models, achieving supervised-level performance without requiring any human annotations or curated instruction data."
        },
        {
            "title": "3.3 Effects of Iterative Self-Improvement",
            "content": "Table 2 reports performance across six selfbootstrapping iterations at three model scales, demonstrating both framework effectiveness and scale-dependent dynamics. Method HumanEval HE HE+ MBPP MBPP MBPP+ BCB-Complete Hard Full BCB-Instruct Hard Full LiveCodeFullStackBench Bench UCoder Random Cluster Low PPL Success Rate UCoder Random Cluster Low PPL Success Rate UCoder Random Cluster Low PPL Success Rate 77.4 73.8 73.8 77.4 75.0 83.5 81.7 82.3 82.9 82.3 86.0 84.8 83.5 84.1 82.3 67.1 64.6 66.5 70.7 67.7 76.8 76.2 76.8 77.4 75. 78.0 79.3 75.6 76.2 75.6 72.0 65.6 68.8 70.1 66.9 75.9 71.7 73.0 73.3 74.1 86.2 80.7 81.5 79.9 81.2 7B Models 44.4 45.5 41.3 45.6 41.4 14B Models 53.3 53.8 50.4 50.7 50. 32B Models 54.8 53.7 53.9 53.5 54.1 15.5 14.9 12.2 17.6 11.5 23.6 20.9 18.2 20.3 20.9 28.4 25.0 23.0 27.7 27.7 63.0 56.6 58.5 59.3 57.1 64.0 60.1 64.0 63.5 63. 72.2 69.6 71.2 67.5 69.6 34.6 37.5 36.5 37.3 33.0 43.2 43.3 42.4 42.2 41.3 44.6 46.6 45.1 45.4 44.2 14.9 13.5 12.2 12.2 12.2 16.2 17.6 15.5 23.0 14. 18.9 21.6 16.9 23.0 17.6 13.0 10.7 10.7 12.2 12.2 22.1 16.8 17.6 19.1 17.6 22.1 15.3 13.0 18.3 21.4 40.25 33.55 38.35 37.11 40.01 50.09 47.95 48.55 49.32 47. 53.05 39.18 49.91 50.09 50.50 Table 3: Ablation study comparing data selection strategies across model scales: UCoder (execution-driven consensus), Random (random sampling from successful solutions), Cluster (clustering-based), Low PPL (lowest perplexity), and Success Rate (weighted by execution success). All metrics show Pass@1 execution rates (%). Bold/Underline indicate best/second-best performance per size category. Data Structures (18.3%), Algorithms (14.8%), and String Processing (11.4%) being most prominent. No category exceeds 20%, demonstrating broad semantic coverage, while concrete algorithmic terms (e.g., dijkstra, greedy, traversal) indicate non-generic, verifiable challenges. Complexity Distribution. We assess problem difficulty and conceptual breadth using Complexity Score (010, aggregating parameter count, description length, algorithmic keywords, and constraints) and Semantic Coverage Score (weighted keyword matches across seven categories). Figure 5 shows moderate positive correlation (r = 0.664), indicating that more complex problems tend to integrate multiple concepts. Problems span the full score ranges (complexity: 3.55 1.60, semantic: 3.16 1.32) with continuous density profile, suggesting natural difficulty continuum suitable for curriculum learning."
        },
        {
            "title": "4.2 Execution-Driven Consensus Effectiveness",
            "content": "Solution Space Diversity. We first examine whether dense sampling with = 128 candidates yields sufficiently diverse solution pool. As shown in Figure 8, the abstract syntax tree (AST) node distribution spans 15 syntactic constructs across 2.6M generated samples (212M total nodes), indicating diverse implementation structures beyond primitive elements. The joint distribution of cyclomatic complexity and code length further shows broad dispersion (mean complexity 2.7 2.3, mean length 22.4 10.5 lines), confirming substantial heterogeneity in the solution space. Quality Distribution Characterization. Given this diversity, we examine whether solution quality exhibits sufficient separation for reliable selection. Figure 6 shows the perplexity distribution over 9,700 sampled candidates and its relationship with execution success. The distribution is rightskewed (mean 1.03, std 0.03) with clear stratification: high-quality samples concentrate at low perplexity values around 1.01, while lower-quality samples progressively shift toward higher perplexity, extending beyond 1.10. Consistently, solutions with execution success rates above 80% cluster predominantly below perplexity 1.05, with rapid performance degradation beyond this range. This sharp transition indicates that high-quality solutions form distinct and identifiable subset within the candidate pool. Quality Improvement. Consensus-based filtering yields substantial improvements across all quality metrics. In Figure 7, the filtered dataset consistently outperforms the full dataset in success rate, error-free rate, and code uniqueness. Besides, these gains are achieved without sacrificing diversity, where structural and semantic entropy remain early findings that source code exhibits statistical regularities similar to natural language (Rozière et al., 2023; Guo et al., 2024; Li et al., 2023; Lozhkov et al., 2024; Zhang et al., 2025a,b). Recent work on unsupervised code post-training has focused on leveraging unlabeled code snippets and use LLM to generate synthetic question-answering data. Magicoder (Wei et al., 2023) uses opensource code examples to teach LLMs how to create varied coding instructions and training data. Besides, WizardCoder (Luo et al., 2023) progressively evolves simple coding instructions into complex ones for training. Further, WaveCoder (Yang et al., 2024) and CodeArena (Yang et al., 2024) generate more diverse and high-quality instruction data from the open source code dataset. Besides, there are some works (Pravilov et al., 2021; Ahmad et al., 2023) that adopt supervised learning for code translation and code change tasks. Code Instruction Tuning. Instruction tuning (Huang et al., 2025; Hui et al., 2024; Yang et al., 2025) has emerged as an effective approach for improving LLMs by fine-tuning them on instructionbased datasets, enabling better generalization and more accurate instruction-following capabilities. To improve the capabilities of code LLM, researchers have enhanced multiple code tasks and benchmarks, such as code generation of multiple domains (e.g., BigCodeBench (Zhuo et al., 2024) and FullStackBench (Liu et al., 2024)), multilingual code generation (e.g., MultiPl-E (Cassano et al., 2023) and McEval (Chai et al., 2024)), and competitive programming problems (e.g., LiveCodeBench (Jain et al., 2024))."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce an execution-driven self-bootstrapping framework that removes the need for human-annotated instruction data in code generation. UCoder exploits latent programming knowledge in pre-trained models and uses execution feedback as scalable, deterministic supervision signal for autonomous improvement. Guided by execution-driven consensus clustering, iterative self-training identifies correct implementations via behavioral consistency and constructs high-quality training data. Our results demonstrate that code models can achieve performance competitive with supervised baselines through fully autonomous learning, highlighting scalable and cost-effective path for advancing code intelligence. Figure 7: Filtered dataset (green) improves quality over full dataset (gray) while maintaining diversity. comparable between the two datasets. This demonstrates that execution-driven consensus selection effectively resolves the qualitydiversity trade-off."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "To isolate the effect of data selection, we compare our consensus-based approach with four alternatives: Random uniform sampling, Cluster selection from dominant output-hash clusters, Low PPL selection based on minimal perplexity, and Success Rate filtering with 0.5 execution threshold. Table 3 shows our approach consistently outperforms all baselines across three model scales, achieving the best results on most benchmarks. The performance gap widens with scale, on FullStackBench, the margin over Random increases from 6.7 points at 7B to 13.9 points at 32B, suggesting larger models produce more diverse candidate pools where principled selection matters most. While individual baselines excel on specific benchmarks, none matches the robustness of consensus-based selection. Notably, Success Rate filtering fails to consistently outperform Random, indicating binary pass/fail criteria inadequately capture quality differences, whereas consensus-based selection leverages behavioral consistency across test inputs as richer quality signal."
        },
        {
            "title": "5 Related Work",
            "content": "Unsupervised Learning for Code Post-training. Unsupervised learning has become increasingly prominent in code generation through pre-training on vast unlabeled code repositories, building on"
        },
        {
            "title": "Limitations",
            "content": "Despite the promising results, our work has several limitations that warrant future investigation. First, the effectiveness of our consensus-based selection mechanism relies on the availability of executable test cases, which may not always be feasible for certain programming tasks or domains where formal specifications are difficult to construct. Second, while our method demonstrates strong performance on standard benchmarks, the computational cost of generating and evaluating 128 candidate solutions per problem remains substantial, potentially limiting its applicability in resource-constrained scenarios. Third, our approach primarily focuses on functional correctness through execution-based validation, which may not capture other important code quality attributes such as maintainability, documentation quality, or adherence to specific coding standards beyond those explicitly testable. Fourth, the iterative self-training process exhibits diminishing returns and potential overfitting to synthetic data distributions after certain number of iterations, requiring careful validation-based early stopping. Finally, our analysis is primarily conducted on Python programming tasks, and the generalizability of our findings to other programming languages with different execution characteristics and paradigms remains to be thoroughly validated. These limitations highlight important directions for future work in unsupervised code generation."
        },
        {
            "title": "Ethics Statement",
            "content": "This work on unsupervised code generation acknowledges several ethical considerations. Automated code generation systems can be misused to produce malicious code or security vulnerabilities, requiring appropriate safeguards, including content filtering and usage monitoring. All experiments used publicly available benchmarks and open-source models, ensuring transparency without collecting proprietary data. We advocate for responsible development with clear documentation, transparent methodologies, and adherence to intellectual property rights."
        },
        {
            "title": "Acknowledgment",
            "content": "This work was supported by State Key Laboratory of Complex & Critical Software Environment (SKLCCSE) of Beihang University and supported by the Fundamental Research Funds for the Central Universities. This work was supported in part by the National Natural Science Foundation of China (Grant Nos. 62276017, 62406033, U1636211, 61672081), and the State Key Laboratory of Complex & Critical Software Environment (Grant No. SKLCCSE-2024ZX-18)."
        },
        {
            "title": "References",
            "content": "Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2023. Summarize and generate to back-translate: Unsupervised translation of programming languages. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 15281542. Anthropic. 2025. Introducing claude sonnet 4.5. Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. CoRR, abs/2108.07732. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2023. Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):36753691. Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, et al. 2024. Mceval: Massively multilingual code evaluation. arXiv preprint arXiv:2406.07436. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating large language models trained on code. abs/2107.03374. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196. Siming Huang, Tianhao Cheng, Jason Klein Liu, Weidi Xu, Jiaran Hao, Liuyihan Song, Yang Xu, Jian Yang, Jiaheng Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Xianzhen Luo, Qiufeng Wang, YuanTao Fan, Qingfu Zhu, Zhaoxiang Zhang, Yang Gao, Jie Fu, Qian Liu, Houyi Li, Ge Zhang, Yuan Qi, Yinghui Xu, Wei Chu, and Zili Wang. 2025. Opencoder: The open cookbook for top-tier code large In Proceedings of the 63rd Anlanguage models. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 33167 33193. Association for Computational Linguistics. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour MoustafaFahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you! CoRR, abs/2305.06161. Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, ZY Peng, et al. 2024. Fullstack bench: Evaluating llms as full stack coder. arXiv preprint arXiv:2412.00535. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolinstruct. CoRR, abs/2306.08568. OpenAI. 2025. Introducing upgrades to codex: Gpt-5-codex. https://openai.com/index/ introducing-upgrades-to-codex/. Mikhail Pravilov, Egor Bogomolov, Yaroslav Golubev, and Timofey Bryksin. 2021. Unsupervised learning of general-purpose embeddings for code changes. In Proceedings of the 5th international workshop on machine learning techniques for software quality evolution, pages 712. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. CoRR, abs/2308.12950. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need. CoRR, abs/2312.02120. Jian Yang, Xianglong Liu, Weifeng Lv, Ken Deng, Shawn Guo, Lin Jing, Yizhi Li, Shark Liu, Xianzhen Luo, Yuyu Luo, Changzai Pan, Ensheng Shi, Yingshui Tan, Renshuai Tao, Jiajun Wu, Xianjie Wu, Zhenhe Wu, Daoguang Zan, Chenchen Zhang, Wei Zhang, He Zhu, Terry Yue Zhuo, Kerui Cao, Xianfu Cheng, Jun Dong, Shengjie Fang, Zhiwei Fei, Xiangyuan Guan, Qipeng Guo, Zhiguang Han, Joseph James, Tianqi Luo, Renyuan Li, Yuhang Li, Yiming Liang, Congnan Liu, Jiaheng Liu, Qian Liu, Ruitong Liu, Tyler Loakman, Xiangxin Meng, Chuang Peng, Tianhao Peng, Jiajun Shi, Mingjie Tang, Boyang Wang, Haowen Wang, Yunli Wang, Fanglin Xu, Zihan Xu, Fei Yuan, Ge Zhang, Jiayi Zhang, Xinhao Zhang, Wangchunshu Zhou, Hualei Zhu, King Zhu, Bryan Dai, Aishan Liu, Zhoujun Li, Chenghua Lin, Tianyu Liu, Chao Peng, Kai Shen, Libo Qin, Shuangyong Song, Zizheng Zhan, Jiajun Zhang, Jie Zhang, Zhaoxiang Zhang, and Bo Zheng. 2025. From code foundation models to agents and applications: comprehensive survey and practical guide to code intelligence. Jian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, and Junyang Lin. 2024. Evaluating and aligning codellms on human preference. arXiv preprint arXiv:2412.05210. Junjie Ye, Yuming Yang, Yang Nan, Shuo Li, Qi Zhang, Tao Gui, Xuan-Jing Huang, Peng Wang, Zhongchao Shi, and Jianping Fan. 2025. Analyzing the effects of supervised fine-tuning on model knowledge from token and parameter levels. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 471513. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv. org/abs/2504.13837. Wei Zhang, Jack Yang, Renshuai Tao, Lingzheng Chai, Shawn Guo, Jiajun Wu, Xiaoming Chen, Ganqu Cui, Ning Ding, Xander Xu, Hu Wei, and Bowen Zhou. 2025a. V-gamegym: Visual game generation for code large language models. Wei Zhang, Jian Yang, Jiaxi Yang, Ya Wang, Zhoujun Li, Zeyu Cui, Binyuan Hui, and Junyang Lin. 2025b. Turning the tide: Repository-based code reflection. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877."
        },
        {
            "title": "A Additional Analysis Figures",
            "content": "Figure 8: Solution space diversity analysis. Left: AST node type distribution across 2.6M samples totaling 212M nodes, spanning 15 distinct syntactic constructs. Right: Cyclomatic complexity versus code length with density contours, showing broad dispersion (complexity: 2.7 2.3, length: 22.4 10.5 lines). Figure 9: Semantic distribution of 229 frequent terms in generated problems. Word size indicates frequency; colors denote categories."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Huawei"
    ]
}