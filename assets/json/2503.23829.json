{
    "paper_title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "authors": [
        "Yi Su",
        "Dian Yu",
        "Linfeng Song",
        "Juntao Li",
        "Haitao Mi",
        "Zhaopeng Tu",
        "Min Zhang",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 9 2 8 3 2 . 3 0 5 2 : r Expanding RL with Verifiable Rewards Across Diverse Domains"
        },
        {
            "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
            "content": "Yi Su ,1,2 , Dian Yu1 , Linfeng Song1 , Juntao Li2 , Haitao Mi1 , Zhaopeng Tu1 , Min Zhang2 , and Dong Yu1 1Tencent AI Lab 2Soochow University"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by large margin, across domains in free-form answer settings. This also strengthens RLVRs robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels."
        },
        {
            "title": "Instruction",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has emerged as promising training strategy for improving the reasoning capabilities of large language models (LLMs) (Luong et al., 2024; Lambert et al., 2024), even in the absence of supervised fine-tuning stage (Guo et al., 2025). This is applied in reference-based setting, where training assumes access to an objective ground-truth label for each prompt. reward or verification function typically assigns binary reward, indicating whether models response aligns with the ground truth. Previous research has primarily demonstrated the effectiveness of this method in tasks with wellstructured answers in limited domains, such as mathematical reasoning and coding. In these settings, reference answers such as numerical values or predefined test cases enable straightforward rule-based verification. To improve the robustness of verifiers across tasks, recent studies have also explored training reward models within single domain. For instance, researchers utilize approximately 800k instances to train reward model for mathematical reasoning (Team et al., 2025). In this work, we extend the study of RL with verifiable rewards beyond well-structured mathematics and coding tasks, investigating its potential to improve LLMs complex reasoning abilities across diverse domains, such as medicine, chemistry, psychology, economics, and education. Since training separate reward model for each domain is impractical, we begin by querying an aligned, generaldomain LLM, inspired by previous reference-free studies that demonstrated the superiority of generative verifiers (Zhang et al., 2024a; Mahan et al., 2024) over discriminative reward models in selecting the highest-scoring response from multiple samples during inference. Surprisingly, The work was done during Yis internship at Tencent AI Lab. 1 Expanding RL with Verifiable Rewards Across Diverse Domains when provided with objective reference answers written by domain experts, we observe high agreement in binary judgments between closed-source and open-source LLMs (e.g., GPT-4o vs. Qwen2.5-72B-Instruct (Team, 2024)) across diverse domains. This high consistency suggests that, when high-quality, objective reference answers are available, reference-based grading is generally easier than reference-free assessment, which is inherently as difficult as identifying the first mistake in response (Lightman et al., 2023). As result, the necessity of large-scale annotation or synthesis of training data for single-domain reward models comes into question. Furthermore, previous RLVR studies have predominantly relied on binary reward scores (Gandhi et al., 2024; Zhang et al., 2024b; Lambert et al., 2024; Guo et al., 2025; Ma et al., 2025a), raising concerns about their robustness in complex reasoning tasks across diverse domains. Even within mathematical problem-solving, our analysis of real-world examination problems shows that only 60.3% of reference answers are single-word numerical values or expressions that can be relatively easily verified using rule-based reward functions. This proportion decreases further to 45.4% for multi-domain problems. These observations underscore the limitations of binary scoring in assessing broader range of reasoning-intensive tasks and highlight the potential benefits of exploring more expressive scoring mechanisms. To address this, we incorporate model-based soft reward scores into RLVR by leveraging the probability of single token that represents generative verifiers final judgment. Noticeably, we demonstrate that when objective reference answers are available, reasonably effective verifier can be trained for diverse domains using relatively small LLM, such as 7B model, while achieving downstream performance comparable to much larger off-the-shelf generative verifiers, such as Qwen2.5-72B-Instruct (Team, 2024). In particular, this is achieved without the need for task-specific annotations for reward models. Instead, we collect policys responses and their assessments from teacher verifier during online exploration. These judged responses, with varying performance levels and formatting noise, may further enhance the robustness of the learned reward model (Section 3.3). By fine-tuning base 7B model using three RL algorithms guided by our reward model, we obtain policies that outperform powerful open-source aligned LLMs such as Qwen2.5-72B-Instruct (Team, 2024) and DeepSeek-R1-Distill-Qwen-32B (Guo et al., 2025) by up to 8.0% in accuracy across various domains in free-form answer tasks. In addition to improving generalization, model-based rewards scale effectively with increasing data size. In contrast, rule-based rewards struggle, especially when addressing unstructured reference answers, and may even lead to performance degradation. Our main contributions are as follows. We extend RLVR to diverse domains, demonstrating its effectiveness beyond mathematics and coding, particularly in handling unstructured reference answers. We show that even with unstructured objective answers, relatively small cross-domain reward model can be trained without requiring task-specific annotations.1 We propose incorporating model-based soft rewards into RLVR and empirically demonstrate their superiority, scalability, and robustness over rule-based rewards across domains."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Reward Estimation in Reinforcement Learning with Verifiable Rewards For reasoning tasks such as mathematical reasoning, whether in constructing training data or at test time, solution is typically considered correct if it arrives at correct final answer (Cobbe et al., 2021a). This is because reliably assessing the correctness of individual steps remains an open challenge, particularly when these steps may lack ground-truth labels in real-world scenarios. Similarly, the correctness of solutions to coding problems is typically accessed based on whether all test cases 1Our reward model and free-form data will be released at https://huggingface.co/collections/ virtuoussy/rlvr-67ea349b086e3511f86d1c1f. 2 Expanding RL with Verifiable Rewards Across Diverse Domains pass (Austin et al., 2021; Hendrycks et al., 2021a; Gehring et al., 2024). Consequently, previous reference-based RL studies have primarily focused on mathematical reasoning and coding tasks. In most previous studies (Zelikman et al., 2022; Gandhi et al., 2024; Zhang et al., 2024b; Lambert et al., 2024; Guo et al., 2025; Ma et al., 2025a; Yu et al., 2025), given access to the reference answer a, the correctness label for response to prompt is typically binary value. can also take on value in the range [0, 1] to reflect varying degrees of correctness (Luong et al., 2024; Li et al., 2024; Ma et al., 2025b; Xie et al., 2025). Labels are assigned by deterministic function = (x, y, a), which operates based on predefined rules (e.g., exact match). These rules can also be combined with tools, such as Python library, for verification (Xiong et al., 2025; Luo et al., 2025). This method is particularly effective when the answer type is fixed and easily matchable, such as numerical value or multiple-choice option. Each response is rated individually, without considering any preference information. Besides using closed-source LLMs such as GPT-4o as verifiers (Chen et al., 2024), recent studies have also explored training reference-based reward models for mathematical reasoning (Team et al., 2025). However, these models are confined to single domain and still require large-scale training data (e.g., 800k instances for math) even within that domain. 2.2 Generative Reward Modeling Using next-token prediction for reward modeling has attracted great interest in recent years (Lightman et al., 2023; Zheng et al., 2023; Tian et al., 2024; Zhang et al., 2024a), as it enables LLMs to fully leverage their generative capabilities, not only to produce accurate rewards but also to provide rationales that justify their judgments. In this work, we explore applying generative, reference-based verifiers to reinforcement learning and investigate their effectiveness across variety of domains, an area that remains largely underexplored. Furthermore, we explore training generative reward models without the need for annotated or synthetic step-by-step rationales (Team et al., 2025; Zhang et al., 2024a) to justify the final assessment. Specifically, we leverage the confidence of generative verifiers to provide stable and informative reward signals, enhancing the robustness of RL training in the presence of noise and ambiguity. 2.3 Verifiable Reasoning Data Previous and on-going RLVR studies primarily focus on narrow tasks (Liu & Zhang, 2025; Xie et al., 2025) such as math word problem solving, code generation, and logic puzzles, where well-structured reference answers allow for straightforward rule-based verification. For example, SimpleRL (Zeng et al., 2025) and Tulu (Lambert et al., 2024) use math datasets GSM8K (Cobbe et al., 2021b) and MATH (Hendrycks et al., 2021b), in which each reference answer typically consists of fewer than two words. However, this reliance on well-structured data constrains the scale and diversity of resources that can be used for RLVR across broader domains. In this work, we explore RLVR using reasoning data spanning diverse domains, where reference answers are free-form, either written by domain experts for unbiased evaluation (Yu et al., 2021), extracted from pre-training corpora (Yue et al., 2024), or generated by LLMs (Yuan et al., 2025)."
        },
        {
            "title": "3 Method",
            "content": "We focus on setting where each prompt is accompanied by an expert-written reference answer a. Reference answers have been shown to play crucial role in providing accurate rewards for reinforcement learning in reasoning-intensive tasks such as coding and mathematics (Shao et al., 2024). Ideally, in these domains, response can be objectively verified against the given reference answer a. However, in practice, this verification process may be influenced by factors such as imperfect answer extraction and matching when pattern-based verifiers are used, as well as noise introduced by automated evaluation systems, such as reward model rϕ(x, a, y). 3 Expanding RL with Verifiable Rewards Across Diverse Domains Figure 1: Overview paradigm of RLVR with our cross-domain verifier. Nevertheless, we can still use this verifiable reward in policy gradient algorithm, with REINFORCE (Williams, 1992) as an example, as follows: J(θ) = E (x,a)D yiπθ (x) (cid:2)rϕ(x, a, yi) (cid:105) . (1) When the generation of an entire response is modeled as single action (Ahmadian et al., 2024), the gradient becomes (see Section A.3 for details): θ J(θ) = (x,a)D yiπθ (x) (cid:2)rϕ(x, a, yi)θ log πθ(yi x)(cid:3). (2) 3.1 Reward Estimation To ensure binary reward signal, we instruct generative LLM πϕ to output only 0 or 1 (see system prompt in Table 4). For notational simplicity, we assume that each response consists of exactly steps, where each step corresponds to non-empty line. Let yT denote the final step of response yi. The binary model-based reward function is then defined as: rϕ(x, a, yi) = 1(cid:0)ci = 1(cid:1), (3) where ci is sampled from πϕ( x, a, yT Using πϕ as verifier, we can also define soft reward function using the probability of the judgment tokens (i.e., 0 or 1): ), representing πϕs judgment on the correctness of yi. rϕ(x, a, yi) = πϕ(1 x, a, yT ) 1 πϕ(0 x, a, yT ) if ci = 1, if ci = 0, otherwise. (4) As shown in Equations 3 and 4, rϕ(x, a, yi) is bounded within [0, 1], ensuring consistency with the widely adopted binary reward scale. 4 Expanding RL with Verifiable Rewards Across Diverse Domains 3.2 Reward Normalization To ensure stable gradients and encourage improvement across all samples in batch that perform above average, we apply z-score normalization to rewards, inspired by prior studies such as GRPO (Shao et al., 2024) and REINFORCE++ (Hu, 2025). r(x, a, yi) = r(x, a, yi) µr σr , (5) where µr and σr denote the mean and standard deviation of the rewards within the batch containing yi, respectively. In the special case where σr = 0, we set all normalized rewards to zero, as these samples are either too difficult or too easy for the current policy. 3.3 Reward Model Training When considering generative verifiers, natural choice is to use an off-the-shelf aligned LLM as the reward model πϕ, inspired by prior work that employs LLMs as judges (Zheng et al., 2023). However, we observe noticeable performance gap on downstream tasks when using LLMs of different sizes. For example, the 72B reward model achieves 62.7% while the 7B model gets 58.8% on math data (see training details in Section 4). To address this, we explore training moderately sized reward model (e.g., 7B) for general domains, aiming to balance performance and efficiency. Since there are no ground-truth reward labels, for each (x, a, y) triple, we prompt fixed LLM to obtain the binary judgments {0, 1}, indicating whether matches the reference answer a. During the RL phase, we collect the data {(x, a, y, c)} from the exploration stages and use it to fine-tune our reward models with supervised learning on c. Unlike relying on fixed LLM to generate y, the improving actor policy produces responses with varying performance and potential formatting noise, which may enhance the robustness of the trained reward models."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Data Mathematics Data To ensure high-quality reference answers, we use large-scale dataset of 773k Chinese Question Answering (QA) pairs, collected under authorized licenses from educational websites. This dataset covers three educational levels: elementary, middle, and high school. Unlike well-structured yet small-scale benchmarks such as MATH (Hendrycks et al., 2021b) and GSM8K (Cobbe et al., 2021b), our reference answers are inherently free-form, often interwoven with rationales or involving several sub-questions yet lacking clear structural patterns. As result, rule-based reward functions that rely on clean, well-structured answers for verification struggle to process these unstructured reference answers effectively. We use GPT-4o-mini to translate questions and their corresponding responses into English. We randomly sample 3,000 QA pairs from each level and reserve them for testing. The average length of reference answers in the test set is 33.7, 36.3, and 53.9 words for elementary, middle, and high school levels, respectively. These are much longer than those in the GSM8K (1 word) and MATH (1.3 words) test sets. Multi-Subject Data Since no large-scale, free-form dataset with objective reference answers exists for general domains, we use multi-subject multiple-choice QA dataset ExamQA (Yu et al., 2021). Originally written in Chinese, ExamQA covers at least 48 first-level subjects. We remove the distractors and convert each instance into free-form QA pair. This dataset consists of 638k college-level instances, with both questions and objective answers written by domain experts for examination purposes. We also use GPT-4o-mini to translate questions and options into English. 5 Expanding RL with Verifiable Rewards Across Diverse Domains For evaluation, we randomly sample 6,000 questions from ExamQA as the test set, while the remaining questions are used as the training pool. Since subject labels are not provided for each QA pair, we use GPT-4o-mini to classify them into one of 48 subjects or mark them as unclassified if uncertain. The detailed classification prompt is provided in Table 5. Excluding unclassified instances (15.8% of the test data), the most frequent subjects include basic medicine, law, economics, management, civil engineering, mathematics, computer science and technology, psychology, and chemistry, as shown in Figure 2. For ease of analysis, we further categorize these subjects into four broad fields (STEM, social sciences, humanities, and applied sciences) as detailed in Table 6. See examples in Table 10. Figure 2: Distribution of subject occurrences in the test set of ExamQA (excluding unclassified). Data for Training the Reward Model We construct the data for training the reward model by extracting 20k samples from each training set of the two datasets, totaling 40k samples. Using the methodology in Section 3.3, we employ Qwen2.5-7B (Team, 2024) to conduct RL training. We use the RLOO (Kool et al., 2019; Ahmadian et al., 2024) algorithm and generate four online samples for each prompt. We use Qwen2.5-72B-Instruct as the reward model for hard label determination. By preserving all input-output pairs, this process yields 160k distilled training samples from Qwen2.572B-Instruct for reward model training. To verify the training approachs validity, we exclude these 40k original samples from the final training dataset. This strict separation ensures that the reward model never encounters any data used in previous training stages, thereby guaranteeing evaluation objectivity. 4.2 Baselines and Notations Base Directly use the base model to generate the response of the question. SFT Directly use the label (without CoT) to fine-tune the base model. Rule-based reward RL with the reward determined by predefined rules. 6 Expanding RL with Verifiable Rewards Across Diverse Domains Qwen2.5-72B-Instruct Instruct (Team, 2024). RL with the reward determined by the judgment of Qwen2.5-72BRM-7B (ours) RL with the reward determined by the judgment of the reward model trained on our 160k distilled data based on Qwen2.5-7B-Instruct (Team, 2024). Binary When using rule-based rewards, we directly judge if the label is in the answer. When using model-based rewards, we use the output of the model. The value of binary reward should be in {0, 1}. Soft When using rule-based rewards, we use Jaccard similarity (Jaccard, 1912) as the reward. When using model-based rewards, we use the probability of the first output token. The value of soft reward should be in [0, 1]. 4.3 Evaluation We begin by investigating majority voting using strong open-source LLM, Qwen2.5-72BInstruct (Team, 2024), as the reward model πϕ. The evaluation process follows the prompting template provided in Table 4. Given prompt and reference answer a, we generate evaluation samples and determine the correctness of response via majority voting. response is considered correct if at least half of the evaluations classify it as such, i.e., ϕ (x, yT, a) = 1(cid:3) 2 . 1(cid:2)π j=1 (j) We measure the agreement between the Qwen-based evaluation method (majority voting over samples) and GPT-4o (a single evaluation per response) using Cohens Kappa (κ). As shown in Figure 3, the two evaluation methods demonstrate almost perfect agreement (0.81 κ 1.00), with κ exceeding 0.86 for mathematics and 0.88 for multi-subject college-level problems. This high level of agreement remains consistent across varying values of m, indicating that the results are not highly sensitive to the number of evaluation samples. Based on this observation, we adopt = 1 in all subsequent evaluations to improve efficiency without compromising evaluation quality. 4.4 Implementation Details After obtaining the 160k distilled data from Qwen2.5-72B-Instruct, we perform supervised finetuning on Qwen2.5-7B-Instruct using this data, resulting in our reward model. We use different RL algorithms to validate the effectiveness of our method, including REINFORCE (Williams, 1992; Ahmadian et al., 2024), RLOO (Kool et al., 2019; Ahmadian et al., 2024), and REINFORCE++ (Hu, 2025). Following Stiennon et al. (2020); Ouyang et al. (2022); Hu (2025), we introduce KullbackLeibler (KL) divergence penalty between the RL model and the reference policy (i.e., base model) distributions to mitigate bias in the reward model. We update r(x, a, yi) as follows: r(x, a, yi) r(x, a, yi) β log (cid:32) πθ(yi x) πref(yi x) (cid:33) , (6) where β 0 controls the effect of the KL penalty, and πref represents the reference policy distribution. We set β = 0.01 for all experiments. For all algorithms, we apply reward normalization as introduced in Section 3.2. We use Qwen2.57B (Team, 2024) as the base model for our experiments. Despite not undergoing post-training, it demonstrates reasonable instruction-following capabilities, as shown by its zero-shot performance in Table 1. We also include the results of Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B to illustrate the difficulty level of our datasets. For both datasets, we select 30k samples as the training data. The training hyper-parameters of RL distilled data collection, reward model training, and the main experiments can be found in Table 9 in the Appendix. 7 Expanding RL with Verifiable Rewards Across Diverse Domains Method Reward Score Type Math Multi-Subject Qwen2.5-72B-Instruct DeepSeek-R1-Distill-Qwen-32B Base SFT rule based REINFORCE Qwen2.5-72BInstruct RM-7B (ours) rule based REINFORCE++ Qwen2.5-72BInstruct RLOO RM-7B (ours) rule based Qwen2.5-72BInstruct RM-7B (ours) binary soft binary soft binary soft binary soft binary soft binary soft binary soft binary soft binary soft 44.2 27. 43.1 53.6 58.5 46.0 64.4 62.5 63.8 62.9 56.4 49.4 63.0 62.7 63.1 62.7 58.2 49.6 63.0 63.8 63.4 63.3 57.7 34.8 53.9 50.5 66.5 47.7 72.1 71.2 71.7 70.7 65.5 52.9 71.3 70.4 71.3 70.3 67.0 50.3 70.8 71.0 71.8 71. Avg STEM Social Humanities Applied Others Avg 40.3 17.4 33.2 32. 46.7 31.5 51.6 53.1 51.9 53.0 47.6 36.2 50.4 50.5 51.5 50.8 50.2 33.9 51.1 52.4 53.8 53.6 47.4 26.6 43.4 45. 57.2 41.7 62.7 62.3 62.5 62.2 56.5 46.2 61.6 61.2 62.0 61.3 58.5 44.6 61.6 62.4 63.0 62.9 25.2 23.2 16.3 24. 25.3 22.0 27.9 32.2 29.0 32.7 26.1 22.5 30.7 30.8 30.2 29.5 28.2 16.7 29.4 32.9 29.3 31.0 20.1 21.8 14.9 22. 26.6 20.3 27.9 32.8 29.1 32.8 26.1 22.0 32.8 30.1 30.8 31.7 27.9 17.3 30.5 31.4 29.0 32.0 28.7 26.7 15.2 25. 27.7 23.1 30.7 36.0 28.4 35.6 26.4 25.7 34.3 33.7 31.0 33.7 27.4 18.8 33.7 34.7 33.3 35.6 20.5 20.5 13.3 20. 21.1 16.9 24.4 24.9 23.8 28.6 21.8 18.6 27.5 25.6 26.6 25.8 22.4 14.5 24.6 27.7 25.8 27.0 21.0 18.5 14.8 22. 20.7 20.5 23.2 27.9 24.8 27.4 24.7 20.2 27.8 25.4 26.3 26.2 24.5 16.9 26.1 26.8 25.6 27.0 22.6 21.7 15.0 23. 24.2 20.3 26.6 30.3 27.3 31.2 25.0 21.4 30.3 28.8 29.1 29.0 26.3 16.6 28.4 30.6 28.1 30.0 Table 1: Performance Comparison of Different Methods. Base model: Qwen2.5-7B. E: elementary. M: middle. H: high. 4.5 Main Results Table 1 shows the results on mathematics and multi-subject tasks. We have the following observations: Evaluation on Base Models Both our math and multi-subject data have demonstrated notable difficulty, with even strong open-source models like Qwen2.5-72B-Instruct (Team, 2024) and DeepSeekR1-Distill-Qwen-32B (Guo et al., 2025) performing unsatisfactorily, particularly on multi-subject tasks (21.7% for DeepSeek-R1-Distill-Qwen-32B and 22.6% for Qwen2.5-72B-Instruct). We believe that more challenging datasets will better facilitate exploration across the industry. SFT vs. RL SFT significantly underperforms RL on both math and multi-subject tasks. Notably on math, SFT merely improves the model performance from 43.4% to 45.7%, falling far short of rule-based reward RL (RLOO, 58.8%) and lagging even further behind model-based reward RL (RM-7B, 63.0%). These findings demonstrate RLs distinct advantages and potential in reasoning tasks when there is no high-quality Chain-of-Thoughts for training. Model-based Reward vs. Rule-based Reward From the table, we can conclude that modelbased reward consistently outperforms rule-based reward in free-form reference-based scenarios. For instance, RM-7B (ours) and Qwen-2.5-72b-Instruct with binary reward achieves 63.0% and 61.6% respectively on average with RLOO, while rule-based reward only gets 58.5%. Notably, our distilled 7B reward model exhibits competitive performance against its much larger predecessor, Qwen2.5-72B-Instruct. In multi-subject evaluations using REINFORCE, the model trained from RM-7B achieves 31.2% accuracy compared to the 72B models 30.3% significant improvement given the substantial parameter disparity. This enhanced capability likely emerges from stabilized response patterns developed during training, which better align with the generative reward models objectives compared to the base models more variable outputs. Binary Reward vs. Soft Reward For rule-based reward, soft reward consistently underperforms binary reward. This discrepancy may stem from redundant tokens between the models generated answers and reference labels, which can lower the reward scores for correct answers. potential improvement could involve adopting metrics like cosine similarity of sentence embeddings as soft rewards, as these may better capture semantic alignment. In contrast, for model-based reward, binary and soft rewards yield comparable results on math tasks. This suggests that the model likely produces judgments with extremely high confidence, as determining answer-label matches in 8 Expanding RL with Verifiable Rewards Across Diverse Domains mathematical problems is relatively easy. However, in multi-subject tasks, where reference labels exhibit greater diversity and consequently higher judgment complexity, soft rewards demonstrate more conservative scoring behavior. This conservatism in ambiguous cases enables soft rewards to outperform binary rewards in certain scenarios (31.2% vs. 27.3%, REINFORCE, RM-7B), as their soft scoring better accommodates the inherent uncertainty of open-domain evaluation. Summary Our method establishes new state-of-the-art performance in RLVR through three key innovations: (1) Our proposed model-based reward is much stronger than rule-based baseline, allowing various RL methods to obtain very accurate rewards in general domain scenarios. (2) Building upon the data distilled from Qwen2.5-72B-Instruct, we develop computationally efficient 7B model that can achieve comparable or even better performance. (3) We extend binary reward to soft reward, which can get more conservative scores for ambiguous cases, which can help get better performance when the reference answers exhibit greater diversity and consequently higher judgment complexity. 4.6 Scaling Experiments Method Scale Math Multi-Subject 58.9 61.5 62.6 62.4 52.6 64.9 65.6 66.0 66.6 67.1 68.1 69.4 69.8 68.2 57. 71.8 72.4 71.6 72.3 72.3 20k 40k 60k 80k 100k 20k 40k 60k 80k 100k Avg STEM Social Humanities Applied Others Avg 47.6 55.4 56.8 53.6 45.2 53.4 54.4 53.2 55.6 55.6 58.2 62.1 63.1 61.4 51.7 63.4 64.1 63.6 64.8 65.0 27.3 25.1 20.0 19.2 17.8 30.8 34.3 33.3 34.5 35. 28.0 24.8 21.9 18.3 18.2 34.6 33.7 36.6 38.6 38.5 31.4 27.4 26.4 26.7 20.5 31.7 36.3 37.3 38.3 39.3 23.5 21.0 16.6 15.1 13.4 28.0 29.5 31.5 31.6 32. 23.0 23.0 19.9 16.4 16.4 27.7 28.6 28.9 31.0 30.7 26.2 24.0 20.1 18.0 16.9 30.8 32.4 33.3 34.6 35.0 Rule based RM-7B (ours) Table 2: The results of the scaling experiments. We use RLOO as the RL algorithm and binary reward as the score type. Scalability has emerged as critical property in the RL-based training era. key question worthy of investigation is whether model performance can continue to improve as RL training progresses and data volume increases. To examine this, we conduct experiments using our trained reward model against rule-based reward while progressively scaling the dataset. We randomly sampled 100k samples from our training corpus as the baseline set, conducting evaluations on both mathematical reasoning and multi-subject tasks. Table 2 shows the experimental results. The results reveal significant differences in scaling capabilities. The rule-based reward demonstrates unstable scalability across both mathematical and multi-subject tasks, exhibiting substantial performance fluctuations and eventual degradation as RL training continues. In contrast, our learned reward model shows consistent improvement trends throughout the training process. This empirical evidence highlights the inherent scalability advantages of model-based rewards compared to rule-based rewards. 4.7 Out-of-Distribution Evaluation To further validate the effectiveness of our reward model, we conduct additional evaluations on two benchmarks: NaturalReasoning (Yuan et al., 2025) and WebInstruct (Yue et al., 2024). We compare the performance of the rule-based reward with our RM-7B using RLOO with binary reward. The base model is Qwen2.5-7B. The results are shown in Table 3. As can be seen from the table, the performance of RM-7B remains significantly superior to the rule-based reward on datasets from Expanding RL with Verifiable Rewards Across Diverse Domains Method Natural Reasoning WebInstruct Rule based RM-7B (ours) 29. 39.8 33.9 44.0 Table 3: The results of the Out-of-Distribution evaluation other domains. This demonstrates that our general-purpose reward model can extend to other domains while maintaining strong performance."
        },
        {
            "title": "5 Discussions and Conclusions",
            "content": "In this work, we simplify the verification task by instructing generative reward model to output either 1 or 0, without requiring chain-of-thought (CoT) reasoning (Nye et al., 2021; Wei et al., 2022). While CoT has proven useful in both reference-based (Team et al., 2025) and referencefree (Zhang et al., 2024a) settings, it remains an open question how necessary in-depth rationales are for assessing semantic equivalence between reference answers and model responses in the same language, particularly when focusing on the conclusive part of each response. This also raises related question for process reward modeling (Lightman et al., 2023) in RLVR: how should rewards be assigned when there is no direct supervision for intermediate steps, regardless of the step segmentation method? In addition, we do not consider format-based rewards (Guo et al., 2025) in this work. We revisit the role of format-related constraints and rewards in this context. In prior work, pattern-based functions are often used for scoring, making it critical to guide LLMs to enclose their final answers in an easily parsed format. These extracted answers are then compared with the reference answers for verification and evaluation. In contrast, by reintroducing reward model in RLVR without imposing any format constraints on reference answers or model responses, we reduce the need for extensive human effort in data standardization and pattern design."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ust un, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021a. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021b. Kanishk Gandhi, Denise HJ Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah Goodman. Stream of search (sos): Learning to search in language. In First Conference on Language Modeling, 2024. 10 Expanding RL with Verifiable Rewards Across Diverse Domains Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Paul Jaccard. The distribution of the flora in the alpine zone. New phytologist, 1912. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get baseline for free! 2019. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Long Li, Xuzheng He, Haozhe Wang, Linlin Wang, and Liang He. How do humans write code? large models do it the same way too. arXiv preprint arXiv:2402.15729, 2024. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. S2r: Teaching llms to self-verify and self-correct via reinforcement learning. arXiv preprint arXiv:2502.12853, 2025a. Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, and Bing Xie. Sorft: Issue resolving with subtask-oriented reinforced fine-tuning. arXiv preprint arXiv:2502.20127, 2025b. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. 2021. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 11 Expanding RL with Verifiable Rewards Across Diverse Domains Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Lei Han, Haitao Mi, and Dong Yu. Toward self-improvement of llms via imagination, searching, and criticizing. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 5272352748. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 5e5853f35164e434015716a8c2a66543-Paper-Conference.pdf. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. Self-rewarding correction for mathematical reasoning, 2025. Dian Yu, Kai Sun, Dong Yu, and Claire Cardie. Self-teaching machines to read and comprehend with large-scale multi-subject question-answering data. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 5668, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.6. URL https://aclanthology.org/2021.findings-emnlp.6/. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason Weston, et al. Naturalreasoning: Reasoning in the wild with 2.8 challenging questions. arXiv preprint arXiv:2502.13124, 2025. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. arXiv preprint arXiv:2405.03548, 2024. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. 12 Expanding RL with Verifiable Rewards Across Diverse Domains Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024a. Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Yuhang Wang, Jinlin Xiao, and Jitao Sang. Openrft: Adapting reasoning foundation model for domain-specific tasks with reinforcement fine-tuning. arXiv preprint arXiv:2412.16849, 2024b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 13 Expanding RL with Verifiable Rewards Across Diverse Domains"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Template Table 4 shows the template for the grading task. Table 5 shows the template for the classification task. Table 6 shows the classification of subjects into STEM (Science, Technology, Engineering, and Mathematics), Social Sciences, Humanities, and Applied Sciences. Given problem , determine whether the final answer in the provided ( incomplete ) solution process matches the reference answer . The reference answer may be one single option character ( . . , , , , ) , numerical value , an expression , or list of answers if multiple questions are involved . ** The reference answer may be in Chinese or another language , but your evaluation should be language - agnostic .** Your task : - Compare the final output of the solution process with the reference answer . - If they ** match exactly ** , output ** YES **. - If they ** do not match ** , output ** NO **. - If the solution process is unclear , incomplete , or ambiguous , assume it is incorrect and output ** NO **. Your output must be strictly ** YES ** or ** NO ** , with no additional words , punctuation , or explanation . --- ** Question :** { question } ** Solution Process ( Final Step Only ) :** { response } ** Reference Answer :** { reference } ** Output :** Table 4: Template for the grading task. A.2 Agreement Note that for each instance, we have only single decision from GPT-4o. While it may align more closely with an individual sampled decision from the reward model than with the majority vote (when > 1), the latter provides more stable and deterministic outcome by reducing randomness during grading. A.3 REINFORCE θ yiπθ (x) (cid:104) r(x, a, yi) (cid:105) = yi = yi = (cid:104) θ πθ(yx) (cid:105) r(x, a, yi) (cid:104) πθ(yx)θ log πθ(yx) (cid:105) r(x, a, yi) (7) (cid:104) θ log πθ(yix)r(x, a, yi) (cid:105) . yiπθ (x) A.4 Hyper parameters Table 9 shows the hyper parameters of our experiments. 14 Expanding RL with Verifiable Rewards Across Diverse Domains Based on the content of Question and Answer classify the subject into one of the following categories . Return only the corresponding subject ID . If classification is uncertain , return 999. ** Question :** { question } ** Answer :** { answer } 110 120 130 140 150 170 180 190 210 230 310 320 330 350 360 413 416 420 430 460 470 510 520 530 550 560 570 580 610 620 630 710 720 730 740 750 760 770 790 810 820 840 850 860 870 880 890 910 999 Mathematics Information Science and System Science Mechanics Physics Chemistry Earth Science Biology Psychology Agronomy Animal Husbandry and Veterinary Science Basic Medicine Clinical Medicine Preventive Medicine and Public Health Pharmacy Chinese Medicine and Chinese Materia Medica Information and System Science Related Engineering and Technology Natural Science Related Engineering and Technology Surveying and Mapping Science and Technology Materials Science Mechanical Engineering Power and Electrical Engineering Electronics and Communications Technology Computer Science and Technology Chemical Engineering Food Science and Technology Civil Engineering Water Conservancy Engineering Transportation Engineering Environmental / Resource Science and Technology Safety Science and Technology Management Marxism Philosophy Religious Studies Linguistics Literature Art History Economics Political Science Law Sociology Ethnology and Cultural Studies Journalism and Communication Library , Information , and Documentation Education Sports Science Statistics Unclassified Table 5: Template for the classification task, with subject names and IDs referenced from (Yu et al., 2021). 15 Expanding RL with Verifiable Rewards Across Diverse Domains Category STEM Social Sciences Humanities Applied Sciences Subject IDs 110 ( Mathematics ) , 120 ( Information Science and System Science ) , 130 ( Mechanics ) , 140 ( Physics ) , 150 ( Chemistry ) , 170 ( Earth Science ) , 180 ( Biology ) , 430 ( Materials Science ) , 460 ( Mechanical Engineering ) , 470 ( Power and Electrical Engineering ) , 510 ( Electronics and Communications Technology ) , 520 ( Computer Science and Technology ) , 530 ( Chemical Engineering ) , 560 ( Civil Engineering ) , 570 ( Water Conservancy Engineering ) , 580 ( Transportation Engineering ) , 610 ( Environmental / Resource Science and Technology ) , 620 ( Safety Science and Technology ) , 910 ( Statistics ) 190 ( Psychology ) , 790 ( Economics ) , 810 ( Political Science ) , 820 ( Law ) , 840 ( Sociology ) , 850 ( Ethnology and Cultural Studies ) , 860 ( Journalism and Communication ) , 870 ( Library , Information , and Documentation ) , 880 ( Education ) , 890 ( Sports Science ) , 630 ( Management ) 710 ( Marxism ) , 720 ( Philosophy ) , 730 ( Religious Studies ) , 740 ( Linguistics ) , 750 ( Literature ) , 760 ( Art ) , 770 ( History ) 210 ( Agronomy ) , 230 ( Animal Husbandry and Veterinary Science ) , 310 ( Basic Medicine ) , 320 ( Clinical Medicine ) , 330 ( Preventive Medicine and Public Health ) , 350 ( Pharmacy ) , 360 ( Chinese Medicine and Chinese Materia Medica ) , 413 ( Information and System Science Related Engineering and Technology ) , 416 ( Natural Science Related Engineering and Technology ) , 420 ( Surveying and Mapping Science and Technology ) , 550 ( Food Science and Technology ) Table 6: Classification of subjects into STEM (Science, Technology, Engineering, and Mathematics), Social Sciences, Humanities, and Applied Sciences. Level Agreement (κ ) = 1 = 10 elementary middle high average 0.844 0.885 0.849 0. 0.838 0.883 0.846 0.861 Table 7: Cohens Kappa agreement (κ) between GPT-4o and majority voting (m: the number of votes) using Qwen2.5-72B-Instruct as evaluator across different education levels of math problems. 16 Expanding RL with Verifiable Rewards Across Diverse Domains Figure 3: Agreement between GPT-4o and Majority Vote with Graders, measured by Cohens Kappa. Level Agreement (κ ) = 1 = 10 college-level 0.881 0.883 Table 8: Cohens Kappa agreement (κ) between GPT-4o and majority voting (m: the number of votes) using Qwen2.5-72B-Instruct as evaluator across college-level multi-subject problems. Hyperparameter Reward Training Main Experiments micro train batch size train batch size micro rollout batch size rollout batch size samples per prompt max samples max epochs prompt max len generate max len max len actor learning rate init kl coef RL 8 128 16 128 4 40000 1 1024 1024 5e-7 0.01 SFT 4 128 1600000 1 4096 RL 8 128 16 128 4 30000 1 1024 1024 5e-7 0.01 SFT 4 128 30000 1 4096 Table 9: Training hyper parameters. Other hyper parameters are the default configuration in OpenRLHF. Expanding RL with Verifiable Rewards Across Diverse Domains coarse fine question answer Social Sciences Psychology STEM Civil neering EngiHumanities Philosophy Applied Sciences Agronomy Setting up an activity for students to bomb each other with compliments belongs to ( ). gravity retaining wall meets the Rankine earth pressure conditions, = 3 m, top width 2 m, bottom width 3 m, fill = 0, ϕ = 30, γ = 18.0 kN/m3, the base friction coefficient is 0.4, the anti-sliding stability safety factor Ks and the antitilting stability safety factor Kt are respectively () Laozi pointed out in the Tao Te Ching, Without leaving the door, one knows the world; without peering through the window, one knows the way of heaven. The farther one goes, the less one knows. Therefore, the sage knows without traveling, sees without looking, and achieves without doing. Laozis view here Under light, the physiological processes that can occur in the mesophyll cells and vascular bundle sheath cells of wheat (C3) are Self-awareness guidance 2.67; 1.73 denies the decisive role of practice in understanding Production of ATP and [H] Table 10: Example question and reference answer pairs in ExamQA."
        }
    ],
    "affiliations": [
        "Soochow University",
        "Tencent AI Lab"
    ]
}