{
    "paper_title": "EuroLLM: Multilingual Language Models for Europe",
    "authors": [
        "Pedro Henrique Martins",
        "Patrick Fernandes",
        "João Alves",
        "Nuno M. Guerreiro",
        "Ricardo Rei",
        "Duarte M. Alves",
        "José Pombal",
        "Amin Farajian",
        "Manuel Faysse",
        "Mateusz Klimaszewski",
        "Pierre Colombo",
        "Barry Haddow",
        "José G. C. de Souza",
        "Alexandra Birch",
        "André F. T. Martins"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quality of open-weight LLMs has seen significant improvement, yet they remain predominantly focused on English. In this paper, we introduce the EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs capable of understanding and generating text in all official European Union languages, as well as several additional relevant languages. We outline the progress made to date, detailing our data collection and filtering process, the development of scaling laws, the creation of our multilingual tokenizer, and the data mix and modeling configurations. Additionally, we release our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on multilingual general benchmarks and machine translation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 5 3 2 6 1 . 9 0 4 2 : r a"
        },
        {
            "title": "EUROLLM\nMULTILINGUAL LANGUAGE MODELS FOR EUROPE",
            "content": "Pedro Henrique Martins1 Patrick Fernandes2,3 Joao Alves1 Nuno M. Guerreiro1,2,4 Ricardo Rei1 Duarte M. Alves2 Jose Pombal1,2 Amin Farajian1 Manuel Faysse4,5 Mateusz Klimaszewski6 Pierre Colombo4,7 Barry Haddow6,8 Jose G. C. de Souza1 Alexandra Birch6,8 Andre F. T. Martins1,2 1Unbabel 2Instituto de Telecomunicac oes, Instituto Superior Tecnico 3Carnegie Mellon University 4MICS, CentraleSupelec, Universite Paris-Saclay 5Illuin Technology 6University of Edinburgh 7Equall 8Aveni"
        },
        {
            "title": "ABSTRACT",
            "content": "The quality of open-weight LLMs has seen significant improvement, yet they remain predominantly focused on English. In this paper, we introduce the EuroLLM project, aimed at developing suite of open-weight multilingual LLMs capable of understanding and generating text in all official European Union languages, as well as several additional relevant languages. We outline the progress made to date, detailing our data collection and filtering process, the development of scaling laws, the creation of our multilingual tokenizer, and the data mix and modeling configurations. Additionally, we release our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct1 and report their performance on multilingual general benchmarks and machine translation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) are driving significant advancements in natural language processing and AI, as demonstrated by OpenAIs GPT series and Anthropics Claude. LLMs are pre-trained on vast amounts of unlabelled data to perform self-supervised task (e.g., next word prediction or missing word prediction), enabling them to develop deep understanding of language. These pre-trained LLMs can already perform various downstream tasks, often leveraging in-context learning techniques, but are typically fine-tuned to better follow natural language instructions, improve performance on specific tasks, and adhere to safety protocols. However, the most advanced models are owned by large corporations with piecemeal commitment to open science. Moreover, despite the growing availability of open-weight LLMs (e.g., LLaMA, Mistral, or Gemma (Touvron et al., 2023; Jiang et al., 2023; Team et al., 2024a)), these are predominantly limited to English and few high-resource languages, leaving out many European languages. To address this gap, we have started the EuroLLM project with the goal of creating suite of LLMs capable of understanding and generating text in all European Union languages (Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish, and Swedish) as well as some additional relevant languages (Arabic, Catalan, Chinese, Galician, Hindi, Japanese, Korean, Norwegian, Russian, Turkish, and Ukrainian). Thus far, we have explored the methodologies for training multilingual LLMs and have developed our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct. This process involved: Collecting and filtering large volume of text data for all the targeted languages from various sources, as detailed in 2. 1The EuroLLM models are available here. Corresponding author: pedro.martins@unbabel.com. 1 Defining the mixture of data that composes the training corpus used to train the model. We describe the decisions we took in 2. These decisions were based on scaling laws and on the data availability for each language. Developing multilingual tokenizer, which we depict in 3. Setting the models hyperparameters and performing pre-training, as described in 4. Fine-tuning the LLMs to follow natural language instructions, which we describe in 5. Evaluating the models performance. Results are reported in 6."
        },
        {
            "title": "2 DATA",
            "content": "To train the EuroLLM models, we collect and filter data from various sources for all supported languages. The data included in the final corpus can be divided into four categories: web data, parallel data, code / math data, and high-quality data. Figure 1 shows the percentage attributed to each data category. 2.1 DATA COLLECTION AND FILTERING Web Data Regarding web data, for English, we use the FineWeb-edu dataset (Lozhkov et al., 2024) which went through individual dump deduplication, heuristic filtering, and model filtering according to the educational level of the documents (we select documents with scores above 2). For other high-resource languages (German, Spanish, French, and Italian), we collect data from the RedPajama-Data-v2 (Computer, 2023), which has been pre-deduplicated. Additionally, we employ perplexity filter along with variety of heuristic filters. For the remaining languages, we collect data from several datasets: HPLT (de Gibert et al., 2024), MADLAD-400 (Kudugunta et al., 2023), CulturaX (Nguyen et al., 2023), and mC4 (Xue et al., 2021), which we concatenate. We then perform deduplication, language identification, perplexity filtering, and apply set of heuristic filters, using preprocessing pipeline based on CCNet (Wenzek et al., 2019). Figure 1: Percentage attributed to each data category in the first training phase (left) and annealing phase (right). Parallel Data Regarding parallel data, we collect to-English (xxen) and from-English (enxx) data from various public sources. We ensure translation quality by removing sentence pairs below quality thresholds for Bicleaner (Sanchez-Cartagena et al.; Ramırez-Sanchez et al., 2020) and COMETKIWI-22 (Rei et al., 2022b).2 Code / Math Data Regarding code and mathematical data, we collect data from the Stack (Kocetkov et al., 2022), the Algebraic-stack (Azerbayev et al., 2023), and the Open-web-math (Paster et al., 2023) datasets. High-quality Data Regarding high-quality data, we use the Wikipedia (Foundation) for all languages and the Arxiv (Clement et al., 2019), Books (Zhu et al., 2015), and Apollo (Wang et al., 2024) for English. Annealing Data In the last 10% of the pre-training we increase the predominance of high-quality data in the data mix. To do so, we filter the monolingual data using binary classifier, inspired by FineWeb-Edu (Lozhkov et al., 2024), which was trained to predict whether document has some educational value, and collect additional high-quality datasets for this phase: Cosmopedia-v2 2For Bicleaner we use threshold of 0.6 for Portuguese and of 0.5 for all the other languages. For COMETKIWI-22 we use threshold of 0.7. 2 Figure 2: Joint Scaling laws obtained when varying the percentage of parallel data. (Ben Allal et al., 2024) which is synthetic dataset composed of textbooks, blog posts, and stories generated by Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024); Python-Edu (Ben Allal et al., 2024) which is subset of Python data from the Stack that was filtered by its educational value; and the training sets of the Grade School Math 8K (GSM8K) (Cobbe et al., 2021) and of the Mathematics Aptitude Test of Heuristics (MATH) (Hendrycks et al., 2021). We also collect document-level parallel data from Europarl (Koehn, 2005) and ParaDocs (Wicks et al., 2024). 2.2 DATA MIXTURE Before starting the training of multilingual LLMs, it is crucial to carefully define the data mixture to be used. This involves deciding how much parallel data to include (2.2.1), determining whether to repeat high-quality data (2.2.2), and deciding how to allocate the total number of tokens among the different languages (2.2.3). 2.2.1 PARALLEL DATA Parallel data (sentences / documents with their translations in another language) can benefit multilingual LLMs in two aspects: improving the alignment between languages and enhancing the models machine translation capabilities. However, determining the optimal proportion of parallel data can be challenging. Joint Scaling Laws Recent research suggests that the performance of large LLMs can be predicted by function of the number of non-embedding parameters , using power-law (Kaplan et al., 2020). In particular, Fernandes et al. (2023) found that, for multilingual models, by training smaller models with varying weights for each language in the data mix, one can fit multilingual, joint scaling law that predicts performance for model trained with weight for language: L(N, p) = (p)βN α + L, with ratio function (p) = + c1pc2 (1 p)c3, and where α, β, and c{1,2,3} are empirically estimated parameters of the scaling law. This law can predict the language performance trade-off of larger models, even for novel language weightings not encountered during the fitting of the scaling law. Thus, to decide on the appropriate amount of parallel data, we re-purpose this law to predict the impact on performance as we change its weighting in training: we train models with varying numbers of non-embedding parameters (100M, 203M, and 341M) on 100B token corpus, for which parallel data constitutes different percentages (0%, 25%, and 37.5%) of the total data for each language, excluding English. Figure 2 reports the obtained scaling laws for test sets from several domains: web data, Wikipedia data, and parallel data. The results indicate that adding parallel data does not negatively impact performance on web and Wikipedia domains, while significantly enhancing the performance on parallel data. Moreover, increasing the percentage of parallel data from 25% to 37.5% yields diminishing returns. Therefore, we include 20% parallel data for each language in the final corpus. 3 Figure 3: Joint Scaling laws obtained when repeating vs not-repeating Wikipedia. Figure 4: Percentage of the training corpus attributed to each language, excluding English which accounts to 50% in the first phase and 32.5% during annealing. 5% of the corpus is left for datasets composed of code and math in the first phase and 7% during annealing. 2.2.2 REPEATING HIGH QUALITY DATA To determine whether it is beneficial to repeat datasets considered to be of higher quality, we analyze scaling laws using method similar to that described in 2.2.1. To do so, we train models on two 100B token corpora: one where Wikipedia data is repeated for all languages and one where it is not. Figure 3 shows the scaling laws for test sets from web and Wikipedia domains. The results clearly indicate that repeating Wikipedia data improves performance on the Wikipedia test sets without degrading performance on the web test sets. Therefore, we choose to repeat data from high-quality datasets. 2.2.3 DIVISION BETWEEN LANGUAGES Regarding the allocation of the corpus to each language, we designate 50% for English, as both highquality data and web data are predominantly in English, and include 5% of code / math data. The remaining 45% of the tokens are distributed among the other languages based on the amount of data obtained after the collection and filtering processes. In order to increase EuroLLMs multilinguality, in the annealing phase, we decrease the English allocation to 32.5% and distribute the surplus across the other languages. We also increase the code / math allocation to 7%. Figure 4 shows the exact percentage attributed to each language."
        },
        {
            "title": "3 TOKENIZER",
            "content": "To train the tokenizer, we adopt the approach used by the LLaMa-2 and Mistral models (Touvron et al., 2023; Jiang et al., 2023), training BPE tokenizer with byte-fallback. To do so, we use the SentencePiece framework (Kudo & Richardson, 2018). For an LLM to be efficient across large number of languages, it is crucial to have tokenizer with large vocabulary. However, this comes 4 with the drawback of having high number of embedding parameters. Through experimentation, we reach the conclusion that vocabulary of 128,000 pieces provides the best trade-off. We compare the fertility achieved by the EuroLLM tokenizer with those of Mistral, LLaMa-3, and Gemma tokenizers (Jiang et al., 2023; AI@Meta, 2024; Team et al., 2024a) which have vocabularies of 32,000, 128,256, and 256,000 pieces, respectively. Figure 5 presents the fertilities for subset of the languages included in EuroLLM. Compared to the Mistral tokenizer, the larger vocabulary of EuroLLM results in significantly lower fertilities. In comparison with the LLaMa-3 and Gemma tokenizers, the LLaMa-3 tokenizer shows the lowest fertility in English but higher fertility for most other languages, while the Gemma tokenizer seems to be better for Asian languages but very similar to EuroLLM for the European ones. Figure 5: Fertility (pieces / word) obtained with the Mistral, LLaMa-3, Gemma, and EuroLLM tokenizers for subset of the EuroLLM languages."
        },
        {
            "title": "4 MODELING",
            "content": "EuroLLM uses standard, dense Transformer architecture (Vaswani et al., 2017): We use grouped query attention (GQA; Ainslie et al. (2023)) with 8 key-value heads since it has been shown to increase speed at inference time while maintaining downstream performance (Team et al., 2024b). We use pre-layer normalization (Xiong et al., 2020), since it improves training stability, and use the RMSNorm (Zhang & Sennrich, 2019), which is faster than LayerNorm (Ba et al., 2016). We use the SwiGLU activation function (Shazeer, 2020) since it has been shown to lead to good results on downstream tasks (Shazeer, 2020; Le Scao et al., 2022). We use rotary positional embeddings (RoPE) (Su et al., 2024) in every layer since these have been shown to lead to good performances while allowing the extension of the context length. Sequence Length Number of Layers Embedding Size FFN Hidden Size Number of Heads Number of KV Heads (GQA) Activation Function Position Encodings Layer Norm Tied Embeddings Max Learning Rate Min Learning Rate Embedding Parameters LM Head Parameters Non-embedding Parameters Total Parameters 1.7B 4,096 24 2,048 5,632 16 8 SwiGLU RoPE (Θ=10,000) RMSNorm No 3 104 3 105 0.262B 0.262B 1.133B 1.657B Table 1: Overview of EuroLLM hyperparameters. Training We pre-train EuroLLM-1.7B on 4 trillion tokens, increasing the predominance of highquality data on the final 10% of the pre-training process. We use 256 Nvidia H100 GPUs of the Marenostrum 5 supercomputer, training the model with constant batch size of 3,072 sequences, which corresponds to approximately 12 million tokens, using the Adam optimizer (Kingma & Ba, 2014), and bfloat16 mixed precision. All relevant model and training hyperparameters are shown in Table 1."
        },
        {
            "title": "4.1 LEARNING RATE SCHEDULER",
            "content": "Regarding the learning rate scheduler, we experiment with two options. In the first option, we use cosine scheduler with warm-up phase corresponding to 10% of the steps. The second option consists of using trapezoid scheduler (Xing et al., 2018) (also named Warmup-Stable-Decay (Hu et al., 2024)). This scheduler has three phases: warm-up for 10% of the steps; constant learning rate; linear decay of the learning rate to the minimum learning rate in the final 10% of the pre-training process, phase in which we use the higher quality annealing data. To decide which option to use in future models we compare the two options on two multilingual general benchmarks: Hellaswag (Zellers et al., 2019; Lai et al., 2023) and Arc Challenge (Clark et al., 2018; Lai et al., 2023) and on machine translation on the FLORES-200 (Team et al., 2022), WMT-23 (Kocmi et al., 2023), and WMT-24 (Kocmi et al., 2024) datasets. The machine translation scores are obtained using COMET-22 (Rei et al., 2022a). The average results, reported on Table 2, show that using the trapezoid scheduler leads to scores consistently better on the multilingual benchmarks and on machine translation. MODEL GENERAL Hellaswag Arc Challenge TRANSLATION FLORES-200 WMT-23 WMTEuroLLM-1.7B - cosine EuroLLM-1.7B - trapezoid 0.4646 0.4744 0.3206 0.3268 86.48 86.75 82.88 83.13 78.87 79. Table 2: Comparison between models trained using the two learning rate scheduler options: cosine scheduler and trapezoid scheduler."
        },
        {
            "title": "5 POST TRAINING",
            "content": "EuroBlocks In order for EuroLLM-1.7B to be able to follow natural language instructions, we create multilingual dataset EuroBlocks which encompasses publicly available human-written and synthetic data. We use instruction-following conversations collected from OpenHermes-2.5 (Teknium, 2023) and Aya (Singh et al., 2024) datasets, as well as high-quality machine translation examples from NTREX-128 (Federmann et al., 2022), FLORES-200-DEV (Team et al., 2022), WMT-21 (Farhad et al., 2021), and WMT-22 (Kocmi et al., 2022). Overall the dataset is composed by 1M samples covering all supported languages and variety of tasks. Supervised fine-tuning (SFT) We fine-tune EuroLLM-1.7B on EuroBlocks to turn it into an instruction-following conversational model: EuroLLM-1.7B-Instruct. We use the standard crossentropy loss, enabling bfloat16 mixed precision and packing. We only calculate the loss on target tokens (thus masking loss on prompt tokens). We train for 4 epochs using learning rate of 7 106 over the course of around 6K steps. tokens (<im start> and Formatting EuroLLM is finetuned with chat format control <im end> tokens) that are included in the original tokenizer described in Section 3. As such, no tokens are added during SFT; however, we overwrite the end-of-sequence token with the <im end> token. These control tokens create clear separation between user/model turns. The chat template may also include system prompt that can be used for added functionality and steerability of the models responses. We show an example in Table 3."
        },
        {
            "title": "6 RESULTS",
            "content": "In order to evaluate our models we recur to: general benchmarks (6.1) to understand if EuroLLM1.7B acquired world knowledge during training; and machine translation (6.2) to understand if the model is able to understand and generate text in all the supported languages. 6.1 GENERAL BENCHMARKS Regarding general benchmarks, we consider commonsense natural language inference test set: Hellaswag (Zellers et al., 2019) and test set of science exam questions: Arc Challenge (Clark 6 System <s><im start>system User Model User Model Translate all user texts to English.<im end> <im start>user Comissao Europeia uma instituic ao politicamente independente que representa defende os interesses da Uniao Europeia na sua globalidade. <im end> <im start>assistant The European Commission is politically independent institution that represents and defends the interests of the European Union as whole. <im end> <im start>user La Comision Europea no consta unicamente de los 27 miembros que integran la institucion constitucional, el llamado Colegio de Comisarios. Esta integrada por todo un cuerpo administrativo de algo mas de 38 000 funcionarios cuya funcion es asistirlo actuar como brazo ejecutivo del Colegio. <im end> <im start>assistant The European Commission is not only composed of the 27 members who form the constitutional institution, the so-called College of Commissioners. It is composed of whole administrative body of more than 38,000 officials whose function is to assist it and act as the executive arm of the College. <im end> Table 3: Example of dialogue with EuroLLM-1.7B-Instruct. We mark the system, user and model control tokens. Figure 6: Results on the Hellaswag (top) and Arc Challenge (bottom) benchmarks. The results were obtained using 10-shot and 25-shot prompts for Hellaswag and Arc Challenge, respectively. et al., 2018). These benchmarks are originally English-only, so we recur to translations (Lai et al., 2023). As baselines, we use Gemma-2b (Team et al., 2024a) and TinyLlama (Zhang et al., 2024). Figure 6 reports the results. On Hellaswag, EuroLLM-1.7B matches or outperforms Gemma-2b and TinyLlama on all languages besides English, which showcases its increased multilinguality. On Arc Challenge, EuroLLM-1.7B outperforms TinyLlama on all languages but is worse than Gemma-2b. This can be caused by the lower number of parameters (EuroLLM-1.7B has 1.133B non-embedding parameters while Gemma-2B has 1.981B). Interestingly, EuroLLM-1.7B-Instruct leads to slightly better results than EuroLLM-1.7B for both benchmarks. 6.2 MACHINE TRANSLATION Regarding machine translation, we compare EuroLLM-1.7B-Instruct with Gemma-2b and Gemma7b (Team et al., 2024a) on three datasets: FLORES-200-TEST (Team et al., 2022), WMT-23 (Kocmi 7 Figure 7: COMET-22 scores on the FLORES-200 dataset on EN-XX (top) and XX-EN (bottom) language pairs. All models were fine-tuned with the EuroBlocks dataset and the translations were obtained using 0-shot prompts and greedy search. Figure 8: COMET-22 scores on the WMT-23 and WMT-24 datasets. All models were fine-tuned with the EuroBlocks dataset and the translations were obtained using 0-shot prompts and greedy search. et al., 2023), and WMT-24 (Kocmi et al., 2024) and evaluate the translations using COMET-22. To have fair comparison, we also fine-tune Gemma-2b and Gemma-7b on the EuroBlocks dataset. Figures 7 and 8 report the results. We can see that EuroLLM-1.7B-Instruct clearly outperforms Gemma-2b-Instruct on all languages pairs and datasets, and is competitive with Gemma-7b-Instruct despite the much lower number of parameters."
        },
        {
            "title": "7 CONCLUSIONS AND FUTURE WORK",
            "content": "In this paper, we present the work done so far in the EuroLLM project. We describe our data collection and filtering process, how we build multilingual tokenizer, and the data mixture and modeling configurations. We also release our initial models: EuroLLM-1.7B and EuroLLM-1.7BInstruct and report their performance on multilingual general benchmarks and machine translation. In future work, we will continue training multilingual LLMs for Europe, focusing on scaling up the number of model parameters and improving further the quality of our data. ACKNOWLEDGMENTS Part of this work was supported by the EUs Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), and by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Center for Responsible AI). We thank EuroHPC for the HPC resources used to support this work through grant EHPC-EXT-2023E01-04."
        },
        {
            "title": "REFERENCES",
            "content": "AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Conference on Empirical Methods in Natural Language Processing, 2023. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics, 2023. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von URL https://huggingface.co/datasets/ Smollm-corpus, 2024. Werra. HuggingFaceTB/smollm-corpus. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018. Colin B. Clement, Matthew Bierbaum, Kevin P. OKeeffe, and Alexander A. Alemi. On the use of ArXiv as dataset, 2019. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Together Computer. RedPajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Banon, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramırez-Sanchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, and Jorg Tiedemann. new massive multilingual dataset for highperformance language technologies, 2024. URL https://arxiv.org/abs/2403.14009. Akhbardeh Farhad, Arkhangorodsky Arkady, Biesialska Magdalena, Bojar Ondˇrej, Chatterjee Rajen, Chaudhary Vishrav, Marta Costa-jussa, Espana-Bonet Cristina, Fan Angela, Federmann Christian, et al. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, 2021. Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 news test references for MT In Proceedings of the First Workshop on Scaling Up Multilinevaluation of 128 languages. gual Evaluation, pp. 2124, Online, nov 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.sumeval-1.4. Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, and Orhan Firat. Scaling laws for multilingual neural machine translation. In International Conference on Machine Learning, pp. 1005310071. PMLR, 2023. Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. arXiv preprint arXiv:2103.03874, 2021. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The Stack: 3 TB of permissively licensed source code. Preprint, 2022. Tom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, et al. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT), 2022. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Proceedings of the Eighth Conference on Machine Translation, 2023. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. Preliminary WMT24 Ranking of General MT Systems and LLMs. arXiv preprint arXiv:2407.19884, 2024. Philipp Koehn. Europarl: parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pp. 7986, Phuket, Thailand, September 13-15 2005. URL https://aclanthology.org/2005.mtsummit-papers.11. Taku Kudo and John Richardson. Sentencepiece: simple and language independent subword tokenizer and detokenizer for Neural Text Processing. EMNLP 2018, pp. 66, 2018. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. MADLAD-400: multilingual and document-level large audited dataset, 2023. URL https: //arxiv.org/abs/2309.04662. Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 318327, 2023. Teven Le Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, et al. What language model to train if you In Findings of the Association for Computational Linguistics: have one million GPU hours? EMNLP 2022, pp. 765782, 2022. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu, 2024. URL https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages, 2023. URL https://arxiv.org/abs/2309. 09400. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. OpenWebMath: An open dataset of high-quality mathematical web text, 2023. Gema Ramırez-Sanchez, Jaume Zaragoza-Bernabeu, Marta Banon, and Sergio Ortiz-Rojas. Bifixer In Proceedings of the 22nd and Bicleaner: two open-source tools to clean your parallel data. Annual Conference of the European Association for Machine Translation, pp. 291298, Lisboa, Portugal, November 2020. European Association for Machine Translation. ISBN 978-989-330589-8. Ricardo Rei, Jose GC De Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andre FT Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Seventh Conference on Machine Translation, 2022a. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, Jose G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and Andre F. T. Martins. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Philipp Koehn, Loıc Barrault, Ondˇrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-juss`a, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, Andre Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aurelie Neveol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri (eds.), Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 634645, Abu Dhabi, United Arab Emirates (Hybrid), December 2022b. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.60. Vıctor M. Sanchez-Cartagena, Marta Banon, Sergio Ortiz-Rojas, and Gema Ramırez-Sanchez. In Proceedings of Prompsits submission to WMT 2018 parallel corpus filtering shared task. the Third Conference on Machine Translation. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Ustun, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024a. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024b. NLLB Team, Marta R. Costa-juss`a, James Cross, Onur elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzman, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation. 2022. Teknium. OpenHermes 2.5: An Open Dataset of Synthetic Data for Generalist LLM Assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5. 11 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, and Benyou Wang. Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People, 2024. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Extracting High-Quality Monolingual Datasets from Web Crawl Data, 2019. URL https://arxiv.org/abs/1911.00359. Rachel Wicks, Matt Post, and Philipp Koehn. Recovering document annotations for sentence-level bitext, 2024. URL https://arxiv.org/abs/2406.03869. Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. walk with sgd. arXiv preprint arXiv:1802.08770, 2018. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 1052410533. PMLR, 2020. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: massively multilingual pre-trained text-to-text transformer, 2021. URL https://arxiv.org/abs/2010.11934. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In International Conference on Computer Vision, 2015."
        }
    ],
    "affiliations": [
        "Aveni",
        "Carnegie Mellon University",
        "Equall",
        "Illuin Technology",
        "Instituto de Telecomunicacoes, Instituto Superior Tecnico",
        "MICS, CentraleSupelec, Universite Paris-Saclay",
        "Unbabel",
        "University of Edinburgh"
    ]
}