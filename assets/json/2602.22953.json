{
    "paper_title": "General Agent Evaluation",
    "authors": [
        "Elron Bandel",
        "Asaf Yehudai",
        "Lilach Eden",
        "Yehoshua Sagron",
        "Yotam Perlitz",
        "Elad Venezian",
        "Natalia Razinkov",
        "Natan Ergas",
        "Shlomit Shachor Ifergan",
        "Segev Shlomov",
        "Michal Jacovi",
        "Leshem Choshen",
        "Liat Ein-Dor",
        "Yoav Katz",
        "Michal Shmueli-Scheuer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents."
        },
        {
            "title": "Start",
            "content": "Elron Bandel Asaf Yehudai Lilach Eden Yehoshua Sagron Yotam Perlitz Elad Venezian Natalia Razinkov Natan Ergas Shlomit Shachor Ifergan Segev Shlomov Michal Jacovi Leshem Choshen 1 Liat Ein-Dor Yoav Katz Michal Shmueli-Scheuer"
        },
        {
            "title": "IBM Research",
            "content": "Abstract The promise of general-purpose agentssystems that perform tasks in unfamiliar environments without domain-specific engineeringremains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Solo Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as first-class research objective. We propose conceptual principles for such evaluation, Unified Protocol enabling agent-benchmark integration, and Exgentica practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish foundation for systematic research on general-purpose agents: www.exgentic.ai. 6 2 0 2 6 2 ] . [ 1 3 5 9 2 2 . 2 0 6 2 : r 1. Introduction The field of AI agents has witnessed remarkable progress, with agentic systems demonstrating impressive capabilities across diverse domainsfrom solving software engineering tasks to navigating web interfaces (Zhang et al., 2024; Deng et al., 2023). However, current progress largely relies on domain specialization and manual tuning; whereas, 1MIT-IBM. Correspondence to: Elron Bandel <elron.bandel@ibm.com>. Preprint. February 27, 2026. 1 # General Agent Model"
        },
        {
            "title": "1 OpenAI Solo",
            "content": "Claude Opus 4."
        },
        {
            "title": "2 Claude Code",
            "content": "Claude Opus 4."
        },
        {
            "title": "Smolagent",
            "content": "Claude Opus 4."
        },
        {
            "title": "4 ReAct Short",
            "content": "Gemini"
        },
        {
            "title": "5 ReAct Short",
            "content": "Claude Opus 4."
        },
        {
            "title": "7 ReAct",
            "content": "Gemini 3 Claude Opus 4."
        },
        {
            "title": "15 Smolagent",
            "content": "Gemini 3 Gemini 3 Gemini 3 GPT 5.2 GPT 5.2 GPT 5. GPT 5.2 GPT 5.2 Avg Success Avg Cost App World Browse Comp+ SWE BenchV Tau 2 Airline Tau 2 Retail Tau 2 Telecom .73 . .66 .62 .62 .61 .61 . .57 .56 .46 .41 .39 . .38 $8.5 $8.0 $4.4 $0.7 $3. $0.8 $5.8 $2.8 $2.5 $1.8 $0. $0.2 $0.2 $0.4 $0.4 .68 . .70 .55 .64 .51 .61 . .36 .13 .22 .00 .00 . .07 .61 .53 .61 .48 . .48 .49 .33 .51 .57 . .46 .48 .43 .26 .81 . .65 .71 .61 .71 .61 . .67 .76 .57 .57 .55 . .53 .74 .66 .72 .70 . .70 .66 .62 .70 .68 . .54 .50 .48 .60 .85 . .78 .82 .78 .82 .78 . .78 .76 .73 .73 .54 . .68 .84 .76 .58 .73 . .73 .76 .89 .69 .88 . .54 .53 .55 .71 Table 1. The Open General Agent Leaderboard comparing emerging general agents across standardized benchmarks. Average Success represents the mean success rate across benchmarks; Average Cost represents the mean cost per task. Performance is strongly influenced by backbone model choice. heterogeneous real-world settings demand general-purpose agents capable of scalable deployment without such manual customization (c.f., Marreed et al., 2025; Bandel et al., 2026). Despite their importance, current evaluation practices cannot adequately assess general-purpose agent capabilities. Existing agentic benchmarks like SWE-Bench Verified (Jimenez et al., 2023) and τ 2-Bench (Yao et al., 2024) provide valuable assessments of domain-specific agents. Yet, they impose two constraints preventing generalagent evaluation: they use bespoke communication protocols (Anonymous, 2026), and they implicitly assume agents have prior knowledge of benchmark-specific goals and environment semantics. Recent consolidation efforts like BrowserGym (Chezelles et al., 2025) and Harbor (Shaw, 2025) have integrated multiple benchmarks within single domains, by exposing to the agent the current goals and environment semantics (Fig. 2(B)). While step forward,"
        },
        {
            "title": "General Agent Evaluation",
            "content": "these frameworks still enforce single protocol (web-based for BrowserGym, CLI-based for Harbor), preventing agents from using their native integration mechanisms and effectively evaluating diminished version of the agent (Yehudai et al., 2025). We set general-purpose AI agents as research target, propose concrete method for evaluating them, and present the first systematic analysis of general agents across diverse environments  (Fig. 3)  . Specifically, our contributions are threefold. (1) We present the Unified Protocol, benchmarkagent mediation protocol (Fig. 2(C)). The Unified Protocol bridges the communication between agent interfaces (e.g., CLI, tool-calling APIs, MCP) and benchmarks through canonical task representation, decoupling evaluation from domain-specific implementations and communication protocols. (2) Based on the Unified Protocol, we release Exgentic an evaluation harness for general agents that supports modular insightscomparing architectures, analyzing language model impact, and optimizing agent-model pairings. (3) Running Exgentic we present the first public Open General Agent Leaderboard to guide general agent development, totaling in overall cost of $22K (See Table 1). Our analysis of the Open General Agent Leaderboard highlights both the capabilities and limitations of contemporary general-purpose agents. While these agents demonstrate notable cross-domain generalizationoften performing on par with domain-optimized baselinestheir success is primarily dictated by the underlying language model  (Fig. 1)  . Conversely, different agentic scaffolds exhibit comparable performance, despite substantial variance in cost. Together, these findings point to the potential of general agents. Ultimately, advancing general-purpose agents requires collective effort. We hope the Open General Agent Leaderboard serves as catalyst for approaches that transcend individual tasks and invite the research community to expand this ecosystem by contributing benchmarks that challenge generalization and novel evaluation protocols. 2. Unified Protocol Methodology This work provides an evaluation solution for any general agent on any agentic benchmark, overcoming the common case of incompatibility between agent and benchmark protocols that either prevent evaluation (Fig. 2(B)), or require costly pairwise adaptation for each agent and benchmark (Fig. 2(A)). To address these limitations, we introduce Unified Protocol that serves as mediation layer between agents and benchmarks. The Unified Protocol serves as narrow waist, adding new agent (or benchmark) only needs adhering to it rather than to all benchmarks (agents). Thus, it significantly reduces integration complexity, development effort and learnFigure 1. Cost-performance tradeoffs across agent-model configurations. The Pareto frontier (red dashed line) shows optimal tradeoffs: GPT 5.2 configurations offer the best cost-efficiency while Claude Opus 4.5 achieve the highest performance at 3-33 higher cost. ing curve. The Unified Protocol is not an imposed standard, but rather one derived from existing agent and benchmark communication patterns. As such, it naturally accommodates and unifies prevalent interaction protocolsenabling faithful translation between any agent-benchmark pair that employs these paradigms. 2.1. Agent Benchmark Unified Protocol The protocol defines instances that are passed between the benchmark and the agent. Each instance has three fields: task, context, and actions. Here we demonstrate them with τ 2-Bench as our running example (see other benchmark examples in Appendix B). 1. Task: What the agent should do? textual description of the task. In τ 2-Bench, it is You are customer service agent that helps the user according to the policy provided below. Try to be helpful and always follow the policy.. In addition, the first user utterance, such as Cancel my flight reservation AH3BDS, is passed to the agent separately as the first observation from the environment. 2. Context: What the agent should know? Additional information provided to the agent to accomplish the task. In τ 2-Bench, the context contains the policy. We note that the agent can use the context in different ways. For example, the agent can naively append it to the task or store it in dedicated agent memory or document store for conditional retrieval. 3. Actions: What can the agent do? set of environment actions. These actions con-"
        },
        {
            "title": "General Agent Evaluation",
            "content": "Figure 2. Evolution of Agentic Evaluation. (A) Collection of separate benchmarks, each requiring custom agent or an agent with specific adaptation per benchmark (HAL) (B) Multiple benchmarks consolidated through single protocol, such as CLI, or Web (C) Multiple benchmarks consolidated through common protocol that can be adapted to any agents protocol (Exgentic). stitute the complete set of operations the environment makes available for performing the task. Each action specifies typed set of parameters and may return one or more observations of arbitrary In τ 2-Bench for the airline domain, some types. actions are cancel reservation(reservation id) and search direct flight(origin, destination, date). Reviewing existing protocols and agents, we observed that many introduce special handling for two specific types of interactions with the environment: (1) sending message to user, and (2) submitting final answer to the benchmark, signaling that the agent has completed the task. To support these common interaction patterns, the Unified Protocol allows implementers to optionally designate one action as the message action and one as the final-answer action. 2.2. Methodology for Adapting Existing Benchmarks Existing agent benchmarks are typically coupled with specific interaction protocols, and often implicitly assume that agents possess prior knowledge of the benchmarks semantics, or that human will manually perform the integration. representative example is SWE-BENCH VERIFIED1. Each task specifies GitHub repo, base commit, and free-text bug description, with the expected output being patch. The benchmark does not define how agents should access the repo or submit fixesthose details are left to the integrator. For general-purpose agents without human intervention, this interface must be explicit. However, we cannot arbitrarily decide on setup; instead, we derive the interface from reference agent implementation. For SWE-BENCH VERIFIED, we examined MINI-SWE AGENT2 as the reference implementation. There, the agent is placed in bash environment where the repository has already been cloned. When the agent outputs COMPLETE TASK AND SUBMIT FINAL OUTPUT , the system automatically generates patch and submits it for 1SWE-Bench Verified 2 MINI-SWE AGENT 3 evaluation. This design fully specifies how the agent interacts with the benchmark, what actions it may take, and how it submits solutionsimplictly indicating that repository cloning and patch creation are not evaluation targets. Accordingly, in the Exgentic protocol for SWE-BENCH VERIFIED, we introduce two explicit actions: one for executing bash commands and another for submitting patch constructed from the agents code modifications. To define the protocols task and context fields, we review both the benchmark tasks and the reference implementation prompts. Many benchmark tasks include irrelevant implementation details, while key instructions appear only in the reference agents internal prompts. For instance, in τ 2-Bench, the reference prompt states: You are customer service agent that helps the user according to the <policy> below. Such essential information belongs in the benchmark task itself and is included in the Exgentic task definition. In contrast, instructions like Each turn you may either message the user or make tool call, but not both are excluded because they assume particular tool-calling protocol. In summary, we decouple each benchmark from its original protocol by making all agent-visible assumptions explicit. First, we inspect the reference agent to see how it interacts with the environment and what actions and observations it uses. Then we build task descriptions that include only the information needed for the agent to solve the task, omitting implementation-specific details and redundant signals. This yields tasks that preserve the benchmarks intended semantics while remaining independent of any particular agent architecture or communication protocol, making them suitable for evaluating any general agent implementation. 2.3. Methodology for Adapting Existing Agents Existing agents interface with existing environments through specific protocols such as MCP, python functions, or tool calls. They also receive the task description through some command line or programmatic API."
        },
        {
            "title": "General Agent Evaluation",
            "content": "Figure 3. Open General Agent Leaderboard is the first benchmark to consistently test general-agent architectures across key skills in diverse environments. Adapting agents to the Unified Protocol involves deciding how to map the task, context and actions of the protocol to the agents specific API. It is important to note that the agent adaptor is benchmark agnostic. The textual task descriptions are typically concatenated with the context fields to textual instructions passed to the model. While not implemented today, the context may be used in different ways. For example, an MCP-based agent may opt to store the context in MCP resources rather than add them to the instructions. Action adaptation is straightforward and largely reusable across agents using similar APIs, with each action mapped to single Python function, OpenAI tool, or MCP tool. More subtle adaptation is dealing with special actions. One special action type is interacting with user. Some agents, like tool-calling agents, natively interact with users using dedicated assistantand usermessages rather than through tool API. To preserve the principle of presenting the benchmark to the agent in the most natural way, the tool-calling agent adaptor converts user and assistant message to the corresponding message action. 3. Exgentic Framework General-purpose agents must operate across diverse environments, and hence viable evaluation frameworks must scale across many benchmarks and agents. The Exgentic framework enables running any currently supported agent on any supported benchmark task, with any LLM, using only few lines of standard Python code or dedicated GUI. The framework was built for use at scale and supports parallelism and caching. Every run is executed in an isolated environment and is reproducible. Benchmark results, interaction trajectories, and cost reports are created in standardized format for all benchmarks and agents. The main orchestration loop is illustrated in Figure 4. Each benchmark generates set of sessions, where each session corresponds to single benchmark task the agent must complete (e.g., resolving GitHub issue, or fulfilling specific user request). For each session, the orchestrator initializes the agent with the task description, contextual information, and the set of available actions. Following initialization, the agent receives the first observation from the session environment and responds by selecting one of the permissible actions. This action is executed by the environment, which returns new observation. The loop continues until either the session concludes or the agent terminates by emitting no further actions. We also terminate if the number of actions/observations exceeds some threshold to avoid deadlocks or excessive costs. 3.1. Solving the Integration Problem Adapting existing agents and benchmarks to the Unified Protocol and integrating them with the Exgentic orchestrator is conceptually straightforward but practically challenging. These components are developed independently by third-party authors who are unaware of the Unified Protocol, the orchestrators execution model, or each others design assumptions. Presumably, one possible solution is to make intrusive modifications to the benchmark and agent code bases to make them use the Unified Protocol. However, such changes may be extremely costly to implement, difficult to maintain, or even impossible when the agent or benchmark is closed-source. Instead, we use external adaptor code that handles synchronization and protocol translation. On the agent side, adaptors expose the Unified Protocol actions in whatever form the agent expectsPython functions, MCP server tools, or OpenAI tools. On the benchmark side, they translate each benchmarks task definition and agent interface into the Uni-"
        },
        {
            "title": "General Agent Evaluation",
            "content": "fied Protocol. Since many adaptations repeat across agents and benchmarks, we provide base adaptors that simplify building specific ones. We allow agents and benchmarks to run natively and independently in separate processes, while all communication between them is mediated by the orchestrator and the corresponding adaptor components. This design ensures that neither the benchmarks nor the agents are affected by the fact that they are running inside the Exgentic framework, preserving their original behavior. For more details, see Appendix A, which demonstrates complete interaction between code-generation agent such as SmolAgents and τ 2-Bench. 4. Experimental Setup Our experiments include evaluation of 5 agent architectures across 3 frontier LLMs (GPT 5.2, Claude Opus 4.5, and Gemini 3 Pro, with default parameters), and maximum of 100 turns per task, on 6 benchmark environments. Yielding 90 configurations with 100 tasks per benchmark environment. Appendix provides detailed descriptions of the benchmark adaptations to the Unified Protocol. 4.1. Benchmarks BrowseComp+ (Chen et al., 2025) is deep research benchmark to assess an agents ability to handle complex informationsearch tasks involving iterative search planning and multi-step reasoning. While the original benchmark jointly evaluates LLMs and retrieval components, we fix the retriever to isolate agent reasoning and decisionmaking. We use the authors provided retriever with either BM25 (Robertson et al., 1994) or Qwen3 Embedder-based dense retrieval (Zhang et al., 2025), and report results using the latter. τ 2-Bench evaluates customer-service agents across retail, airline, and telecom domains via LLM-simulated users, measuring both policy-compliant task completion and violation rejection. τ 2-Bench has bespoke python API, where the agent receives simulated user message and returns either message reply or calls to one or more predefined tools. We map these into message action and Exgentic actions respectively. SWE-Bench Verified human-validated subset of 500 real-world software engineering tasks from popular Python repositories. Each provides GitHub issue and repository snapshot; agents produce patches that are evaluated against hidden test suites. Following mini-swe-agent , we expose single bash action for repository interaction in sandboxed environment, generating patches via git diff for evaluation. This ensures uniform agent interaction. AppWorld is benchmark for evaluating user-assistance agents on realistic day-to-day digital tasks. In the original protocol, the agent interacts with the environment by writing Python code that is executed in dedicated interpreter with access to the AppWorld APIs. In our setup, we adopt this native interpreter-based interaction protocol and use the official task definitions and evaluation harness, ensuring consistent API access and evaluation conditions across all agent configurations. 4.2. Agents ReAct We implement two ReAct-style (Yao et al., 2023) agents: vanilla ReAct baseline using LiteLLMs toolcalling interface, and an extended version with tool shortlisting. Both are integrated with Exgentic by exposing benchmark actions as tool specifications, while the shortlisting variant is designed to handle large action spaces efficiently. Smolagent CodeAgent code-generation agent that produces Python code to invoke tools rather than calling them directly. We integrate Smolagents v1.24.0 (Roucher et al., 2025) with Exgentic by exposing benchmark actions as Python functions and adapting its termination behavior to use the benchmark-defined finish action. OpenAI Solo + MCP An agent built on OpenAIs SDK v0.7.0 in solo mode with Model Context Protocol integration (OpenAI Solo for short). The agent operates in solo mode, interacting with environments exclusively through MCP tool calls. We integrate it with Exgentic by implementing an adapter that translates benchmark actions into MCP tool specifications. Claude Code feature-rich command-line agent originally designed for software engineering tasks and recently claimed general effectiveness beyond coding3. We evaluate Claude Code v2.1.7 without modifying its internal logic, integrating it with Exgentic via MCP-exposed benchmark actions. The agent runs in Docker container to ensure isolation and reproducibility. 4.2.1. AGENT COMPONENTS Agents differ in implementation but share common conceptual components. To gain insight into agents internal behavior and its impact on performance, we adopt componentlevel view covering execution runtime, tool shortlisting, schema guards, communication protocols, memory, and planning. Appendix details their presence across agents. 4.3. Metrics To enable consistent comparison across agents and tasks, we adopt the following general metrics. 3Building agents with the Claude Agent SDK."
        },
        {
            "title": "General Agent Evaluation",
            "content": "Figure 4. Exgentic architecture. Exgentic defines unified protocol between agents and benchmarks. The Exgentic Orchestrator connects the agent and the benchmark, first passing the task definition and then mediates the observations and actions that are passed between the benchmark and the agent. Exgentic provides adaptors that convert the Unified Protocol into the specific protocols required by the agents and benchmarks. Finally, the benchmark provides the quality result metrics while the agent provides the agent runtime cost. Success Rate. The proportion of runs deemed successful according to the original success definition and evaluation procedure of the benchmark. Cost per Task. The average monetary cost of completing task, enabling comparison of agent efficiency in addition to performance. In our experiments, costs are reported using LiteLLMs pricing data4. Average Steps. The mean number of steps taken by an agent to reach task completion. 5. Results Our main results address three central questions: (1) Do agents generalize across domains? (2) What drives agent performance Model quality or Agent architectural design? (3) What architectural components enable crossdomain capabilities? 5.1. Key Leaderboard Findings Our benchmark evaluation reveals clear performance hierarchies at the model, agent, and configuration levels. These findings provide practitioners with actionable guidance for system selection and deployment (see Appendix for com4Model prices and context window. plete leaderboard results). Top Configurations: The leaderboard  (Table 1)  is split between Claude Opus 4.5 and Gemini 3 based pairings, with Claude Opus 4.5 occupying the top three positions. No GPT 5.2 configuration appears in the top-10. We assessed statistical significance using pooled McNemar test. While the top configuration (utilizing OpenAI Solo and Claude Opus 4.5) did not significantly outperform the second-ranked configuration, it demonstrated significant advantage over the third-ranked (p < 0.01) and all remaining configurations (p < 0.001). See Appendix for detailed statistical analysis. Model Performance: We compare models using their mean success rate across all agents and benchmarks, weighting τ 2-Bench subdomains equally (1/12 each) to balance benchmark representation. Claude Opus 4.5 ranks first with success rate of 0.66, followed by Gemini 3 at 0.60, while GPT 5.2 underperforms at 0.40. Pairwise statistical tests over all (benchmark, task, agent) configurations confirm that these performance differences are significant (p < 0.0001). Claude Opus 4.5s superiority is consistent across nearly all benchmarks, whereas GPT 5.2s low aggregate performance is largely driven by failures in tool-rich environments. Agent Performance: We compare agents by their mean"
        },
        {
            "title": "General Agent Evaluation",
            "content": "success rate across all models and benchmarks (weighting τ 2-Bench subdomains as 1/12 each to balance benchmark representation). ReAct Short leads with 0.57, closely followed by OpenAI Solo (0.57), ReAct (0.55), Claude Code (0.54), and Smolagent (0.53). Using paired McNemar test, comparing results over each pair of agents on all (benchmark, task, model) combinations, we saw that these differences not statistically significant (p > 0.1). where is the task success rate and is the grouping variable (model or agent). Model choice accounts for 28.2% of total success rate variance across all configurations, while agent architecture explains only 0.6%. The remaining 71.2% reflects task-level variancedifferences in benchmark difficulty, task characteristics, and stochastic execution. Model quality is by far the strongest single factor, dominating agent architecture by more than 85-fold. However, agent performance are model-dependent: OpenAI Solo excels with Claude Opus 4.5 (0.73) but struggles on GPT 5.2 (0.39), while ReAct Short performs more consistently across models. Notable Outliers: OpenAI Solo + Gemini 3 achieves the highest single-benchmark score (0.89 on τ 2-BenchTelecom), while four GPT 5.2 configurations score 0.00 on AppWorld without tool shortlisting, as GPT 5.2 is limited to 128 tools while AppWorld requires 468. The performance spread within models ranges from 11 percentage points (Claude Opus 4.5: best 0.73, worst 0.62) to 6 percentage points (Gemini 3: best 0.62, worst 0.56), indicating that agent choice matters significantly even for strong models. 5.2. No Single Agent Dominates Across Task Domains Table 2 shows the best-performing agent-model configuration for each benchmark. Table 2. Success rate (Score) of best agent-model configuration per benchmark. Top Score denotes the highest reported domainspecific agent performance on the original leaderboard (links are in App. D.1). Note: our results are on 100 randomly sampled benchmark instances, whereas the original leaderboard reports results on the full benchmark."
        },
        {
            "title": "Top Score",
            "content": "SWE-Bench Verified BrowseComp+ τ 2-Bench-Airline τ 2-Bench-Retail τ 2-Bench-Telecom AppWorld OpenAI Solo + Claude Opus 4.5 Smolagent+Claude Opus 4.5 OpenAI Solo + Claude Opus 4.5 OpenAI Solo + Claude Opus 4.5 OpenAI Solo + Gemini 3 Smolagent + Claude Opus 4.5 0.81 0.61 0.74 0.85 0.89 0.7 0.79 0.80 0.73 0.86 0.98 0.73 No single agent dominates: OpenAI Solo wins 4 benchmarks and ties on 1 (SWE-Bench Verified, τ 2-BenchAirline, τ 2-Bench-Retail, τ 2-Bench-Telecom, and BrowseComp+ tie), demonstrating particular strength on structured API interaction tasks and code generation. Smolagent wins 1 benchmark and ties on 1 (AppWorld and BrowseComp+ tie), excelling on web navigation and multi-application environments. 5.3. Model Quality Drive Performance We performed variance decomposition to isolate the relative contributions of model choice versus agent architecture. We compute variance explained as η2 = Var(E[Y X])/Var(Y ), 7 5.4. Model Stability to Agent Architectures Beyond average performance, model stability across different agent architectures is critical for practical agent development. stable model allows developers to iterate on agent design without model-specific tuning; an unstable model requires careful co-design of the agent-model pairing. We measure stability as the standard deviation of scores across agent architectures for each model. Claude Opus 4.5 exhibits the highest stability (Mean: 0.66, STD: 0.06), followed by GPT 5.2 (Mean: 0.40, STD: 0.071) and Gemini 3 (Mean: 0.59, STD: 0.09). Claudes standard deviation is slightly lower than Gemini 3s, indicating that agent performance with Claude varies minimally across architectural choices. Stability has practical implications for agent development workflows. With Claude Opus 4.5, developers may focus on agent architecture without extensive model-specific optimization. With Gemini, agent-model co-design may becomes necessary, increasing development cost and reducing modularity. For practitioners prioritizing development efficiency and deployment flexibility, model stability may be as important as absolute performance. 5.5. Agent Components Effect Several agent components discussed in Section 4.2.1 prove useful across agent implementations or models. Notably, the top three performing architecturesOpenAI Solo, Claude Code, and Smolagentall employ schema guard component: mechanism that detects when an action with an invalid schema is invoked and allows the agent to correct itself. This highlights the potential value of such self-correction mechanisms across agents based on Claude Opus 4.5. Tool shortlisting, when added to simple ReAct agent with tool calling, improves performance across all models in tool-rich environments. For GPT 5.2, shortlisting adds 5 percentage points overall, while for Claude Opus 4.5 the gain is more modest (1 percentage point) but comes with substantial cost reduction of $1.97 on average. These results underscore the importance of documenting agent components, sharing implementation details, and conducting ablation studies as primary means of advancing general agent development."
        },
        {
            "title": "General Agent Evaluation",
            "content": "5.6. Cross-Benchmarks Agent Stability To assess whether agent performance generalizes across task types, we computed Spearman rank correlations between benchmark scores across all agent-model configurations. Results reveal moderate to strong positive correlations across most benchmark pairs: τ 2-Bench-Airline vs τ 2Bench-Retail shows +0.85 (strong positive), SWE-Bench Verified vs τ 2-Bench-Telecom shows +0.78, and AppWorld vs τ 2-Bench-Retail shows +0.75. BrowseComp+ shows more moderate correlations with other benchmarks (0.32 to 0.74), suggesting it captures somewhat distinct capabilities while still following overall model quality trends. The predominantly positive correlations are driven by systematic model differencesGPT 5.2 underperforms across nearly all benchmarks (mean 0.40) while Claude Opus 4.5 excels broadly (mean 0.66). This creates cross-benchmark consistency at the model level but does not imply agentlevel generalization. Within-model analysis reveals that agent rankings vary substantially: on Claude, OpenAI Solo leads (0.73), while on GPT 5.2, ReAct Short leads (0.46). These findings challenge the notion of general-purpose agents. Current architectures do not achieve robust generalization but instead optimize for specific task distributions. An agents benchmark performance is poor predictor of its performance on dissimilar tasks. 5.7. Cost-Efficiency Tradeoffs We computed cost-efficiency as average score divided by average inference cost per task5. Table 3 shows that GPT 5.2 configurations dominate the efficiency rankings. For comparison, the best-performing configuration overallOpenAI Solo + Claude Opus 4.5 (0.73 average score)has substantially higher costs per task. Achieving state-of-the-art performance requires 30 higher cost than the most efficient configurations, representing fundamental tradeoff between absolute performance and cost. Table 3. Most and least cost-efficient configurations per model"
        },
        {
            "title": "Configuration",
            "content": "Score Cost/Task Efficiency ReAct + GPT 5.2 Claude Code + GPT 5.2 ReAct + Gemini 3 OpenAI Solo + Gemini 3 ReAct Short + Claude Opus 4.5 Claude Code + Claude Opus 4.5 0.41 0.38 0.62 0. 0.62 0.67 $0.17 $0.38 $0.66 $2.81 $3.78 $8.03 2.41 1.00 0.93 0. 0.16 0.08 This Pareto frontier shown in Figure 1 has practical implications. Cost-sensitive applications (internal tools, high5Costs computed using LiteLLMs pricing data as of January 2026, based on public API list prices. Enterprise pricing may differ substantially. 8 volume automation) may prefer GPT 5.2 configurations despite 27 point performance gaps. Performance-critical applications (customer-facing agents, high-stakes decisions) justify the premium for Claude-based configurations. Gemini 3 configurations may represent middle ground between cost and performance. There is no universal best choiceoptimal selection depends on application-specific cost-performance requirements. 5.8. Failure Patterns and Agent Behavioral Differences We examined whether failures are associated with increased number of interactions between the agent and the environment, resulting in higher computational cost, and whether this pattern is consistent across agents architectures. To this end, we compared successful and failed runs at the task level, aggregating across backbone models for each benchmark and agent architecture6. For each benchmark and agent architecture, we report the percent increase in mean steps for failed runs relative to successful runs (positive means failures are longer). Table 4 shows this gap is positive in most settings, with the largest overheads on interaction-heavy benchmarks such as AppWorld and BrowseComp+ (e.g., ReAct is +110.7% on AppWorld). few τ 2-Bench cells are near zero or negative, indicating some failures terminate early. The detailed interactions counts appear in App. D.2. Benchmark-weighted averages are positive for every agent architecture (Claude Code 38.8%, OpenAI Solo 20.0%, Smolagent 26.4%, ReAct 54.4%, ReAct Short 45.1%), implying that failed runs generally consume more stepsand there fore more costthan successful runs, amplifying the practical penalty of unreliability. This pattern indicates that tasks that ultimately fail tend to take longer, whereas easier tasks complete more quickly. While this trend appears across all architectures, they differ in the magnitude of the effect. These differences suggest the existence of variations along more subtle dimensions than overall performance and cost. Understanding these finer-grained behavioral characteristicsfor example, how architectures allocate interaction budget, manage uncertainty, or recover from partial progressmay be important when designing or selecting agent architectures. 5.9. The Current State of General-Purpose Agents Four overarching themes emerge from our evaluation. First, model quality creates cross-benchmark consistency. No agent achieves consistently strong performance across all 6We excluded zero-step sessions and capped step counts at 50 to reduce the influence of long-tail outliers, which accounts to 7% of total runs."
        },
        {
            "title": "General Agent Evaluation",
            "content": "Table 4. How much longer failed runs are than successful ones, measured by the percentage difference in number of interactions. Positive values mean failures take more interactions; negative values mean they take fewer."
        },
        {
            "title": "Benchmark",
            "content": "AppWorld BrowseComp+ SWE-Bench Verified τ 2-Bench-Airline τ 2-Bench-Retail τ 2-Bench-Telecom"
        },
        {
            "title": "Smolagent ReAct",
            "content": "63% 70% 16% 25% -2% -6% 39% 49% 18% 6% 34% -14% 2% 20% 33% 111% 67% 50% 21% 9% 31% 35% 7% -2% 20% 6% 26% 54%"
        },
        {
            "title": "ReAct\nShort",
            "content": "74% 67% 21% 31% 7% 20% 45% benchmarks, but strong positive correlations (0.75-0.85) across most benchmark pairs reflect systematic model differences. Claude Opus 4.5 excels broadly (mean 0.66), Gemini shows moderate performance (mean 0.60), and GPT 5.2 underperforms significantly (mean 0.40). Agent rankings vary within models, but model effects dominate overall patterns. Second, model quality dominates agent architecture. Model choice explains 28.2% of score variance while agent architecture explains only 0.6%. The model-agent interaction effect (5.0%) exceeds the agent main effect by more than 4.5-fold, indicating that optimal agent selection depends heavily on the model. However, model differencesparticularly GPT 5.2 failures on tool-rich environmentsdrive most performance variation. Agent architecture matters primarily for enabling model capabilities (e.g., ReAct Short for GPT 5.2) rather than as an independent performance driver. Third, practical deployment considerationscost, tool scalability, component complexityare not secondary concerns but fundamental constraints. Tool shortlisting transforms GPT 5.2 from unusable to viable in tool-rich environments. Cost-efficiency varies by 33 across configurations. Sophisticated components like memory and planning correlate with gains but increase implementation complexity. These tradeoffs define the space of viable deployments rather than being post-hoc optimizations. Fourth, general-purpose agents are competitive with benchmark-specific heavily customized agents. Our results show that across benchmarks, general agents largely match or exceed specialized systems (Tab. 2). Overall, although general agents have not yet been systematically pursued in full and still have substantial room to improve, these results establish general agents as promising direction for future research and development. Progress toward general-purpose agents requires addressing generalization explicitly through cross-benchmark evaluation. Improving single-benchmark performance does not yield generalizing agents. 6. Related Work Domain-Specific Agent Benchmarks. The rapid advancement of AI agents has led to proliferation of benchmarks (Zhou et al., 2023; Deng et al., 2023; Xie et al., 2024; Liu et al., 2023), each targeting specific domains such as software engineering (Jimenez et al., 2023; Merrill et al., 2026), customer service (Yao et al., 2024) and deep scientific research (Bragg et al., 2025). Each benchmark defines domain-specific protocols and task specifications. Attempts at Consolidation HAL (Kapoor et al., 2025) unifies infrastructure across benchmarks but requires perbenchmark agent adaptation. BrowserGym (Chezelles et al., 2025) and Harbor (Shaw, 2025) standardize interaction via fixed protocols (web/CLI) but restrict evaluation to single environment classes. AgentBeats7 models agents and benchmarks as interacting via A2A/MCP subsets, standardizing evaluation lifecycle components but leaving task semantics to individual benchmarks. Exgentic enables protocolpreserving evaluation across heterogeneous benchmarks, supporting consistent comparison without per-benchmark adaptation. 7. Discussion This work takes first step toward systematic research of general-purpose agentsa fundamental gap in the field. We develop Exgentic and the Unified Protocol to address the current landscape while providing infrastructure that evolves with emerging agents and benchmarks. Our initial results demonstrate that agents can generalize across domains without domain-specific adaptation, matching or exceeding domain-specific performance across most benchmarks and establishing general agents as viable alternative (Tab. 2). Our evaluation reveals promising opportunities for advancement: substantial performance headroom, domain variations suggesting architectural improvements, and clear cost-performance optimization targets. These findings point to exciting research directionsenhancing performance through better reasoning and planning, achieving cross-domain consistency, developing cost-effective solutions, and expanding to multimodal and safety-critical scenarios. The Open General Agent Leaderboard and Exgentic provide foundation for systematic comparison and iterative progress toward truly capable general-purpose agents."
        },
        {
            "title": "Impact Statement",
            "content": "The current research landscape for AI agents is fragmented by domain-specific benchmarks and communication protocols, which limit the development of general-purpose systems. This work introduces Exgentic and the Unified 7AgentBeats"
        },
        {
            "title": "General Agent Evaluation",
            "content": "Protocol to bridge these gaps, enabling the first systematic evaluation of general agents across diverse environments. By establishing the Open General Agent Leaderboard, we provide the research community with foundation for developing agents that transcend individual tasks and generalize across heterogeneous real-world settings. Our findings highlight that while model quality remains the primary driver of performance, standardized evaluation is essential for identifying the architectural components that enable scalable, cross-domain capabilities."
        },
        {
            "title": "References",
            "content": "Anonymous. Position: The ml community needs universal standard for agent benchmarks. In Proceedings of the International Conference on Machine Learning, 2026. Under submission. Bandel, E., Yehudai, A., Lacoste, A., Ghosh, A., Neubig, G., Mitchell, M., Shmueli-Scheuer, M., and Choshen, L. Agentic systems should be general. SSRN Electronic Journal, 2026. URL SSRN:https://ssrn.com/ abstract=6176178. Bragg, J., DArcy, M., Balepur, N., Bareket, D., Dalvi, B., Feldman, S., Haddad, D., Hwang, J. D., Jansen, P., Kishore, V., et al. Astabench: Rigorous benchmarking of ai agents with scientific research suite. arXiv preprint arXiv:2510.21652, 2025. Chen, Z., Ma, X., Zhuang, S., Nie, P., Zou, K., Liu, A., Green, J., Patel, K., Meng, R., Su, M., Sharifymoghaddam, S., Li, Y., Hong, H., Shi, X., Liu, X., Thakur, N., Zhang, C., Gao, L., Chen, W., and Lin, J. Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent, 2025. URL https://arxiv.org/abs/2508.06600. Chezelles, T. L. S. D., Gasse, M., Drouin, A., Caccia, M., Boisvert, L., Thakkar, M., Marty, T., Assouel, R., Shayegan, S. O., Jang, L. K., L`u, X. H., Yoran, O., Kong, D., Xu, F. F., Reddy, S., Cappart, Q., Neubig, G., Salakhutdinov, R., Chapados, N., and Lacoste, A. The browsergym ecosystem for web agent research, 2025. URL https://arxiv.org/abs/2412.05467. Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Kapoor, S., Stroebl, B., Kirgis, P., Nadgir, N., Siegel, Z. S., Wei, B., Xue, T., Chen, Z., Chen, F., Utpala, S., Ndzomga, F., Oruganty, D., Luskin, S., Liu, K., Yu, B., Arora, A., Hahm, D., Trivedi, H., Sun, H., Lee, J., Jin, T., Mai, Y., Zhou, Y., Zhu, Y., Bommasani, R., Kang, D., Song, D., Henderson, P., Su, Y., Liang, P., and Narayanan, A. Holistic agent leaderboard: The missing infrastructure for ai agent evaluation, 2025. URL https://arxiv. org/abs/2510.11977. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. Marreed, S., Oved, A., Yaeli, A., Shlomov, S., Levy, I., Akrabi, O., Sela, A., Adi, A., and Mashkif, N. Towards enterprise-ready computer using generalist agent, 2025. URL https://arxiv.org/abs/2503.01861. Merrill, M. A., Shaw, A. G., Carlini, N., Li, B., Raj, H., Bercovich, I., Shi, L., Shin, J. Y., Walshe, T., Buchanan, E. K., et al. Terminal-bench: Benchmarking agents on hard, realistic tasks in command line interfaces. arXiv preprint arXiv:2601.11868, 2026. Robertson, S. E., Walker, S., Jones, S., Hancock-Beaulieu, M. M., and Gatford, M. Okapi at trec-3. In Proceedings of the Third Text REtrieval Conference (TREC-3), pp. 109126. NIST, 1994. Roucher, A., del Moral, A. V., Wolf, T., von Werra, L., and Kaunismaki, E. smolagents: smol library to build great agentic systems. https://github.com/ huggingface/smolagents, 2025. Shaw, A. Harbor Framework, November 2025. URL https://github.com/laude-institute/ harbor. Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., Liu, Y., Xu, Y., Zhou, S., Savarese, S., Xiong, C., Zhong, V., and Yu, T. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In Advances in Neural Information Processing Systems, 2024. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language modarXiv preprint els resolve real-world github issues? arXiv:2310.06770, 2023. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023."
        },
        {
            "title": "General Agent Evaluation",
            "content": "Yao, S., Shinn, N., Razavi, P., and Narasimhan, K. Taubench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Yehudai, A., Eden, L., Li, A., Uziel, G., Zhao, Y., BarHaim, R., Cohan, A., and Shmueli-Scheuer, M. Survey on evaluation of llm-based agents, 2025. URL https: //arxiv.org/abs/2503.16416. Zhang, Y., Ruan, H., Fan, Z., and Roychoudhury, A. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 1592 1604, 2024. Zhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., Huang, F., and Zhou, J. Qwen3 embedding: Advancing text embedding and reranking through foundation models, 2025. URL https://arxiv.org/abs/2506.05176. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., Alon, U., and Neubig, G. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. A. Detailed Benchmark Agent Interaction Example"
        },
        {
            "title": "General Agent Evaluation",
            "content": "This section demonstrate complete interaction between code-generation agent such as SmolAgents and the τ 2-Bench benchmark. Agent Side. During initialization, the SmolAgent adaptor converts all Exgentic actions into lightweight Python wrapper functions. standard SmolAgent instance is then created using the sessions task definition and the set of wrapper functions. When the agent invokes one of these wrapper functions, the wrapper places the corresponding action into an action queue and blocks while waiting for response in observation queue. Later, when the orchestrator calls action = CodeAgentWrapper. react(observation), the adaptor stores the observation in the observation queue, unblocking the agent-side wrapper function. The wrapper retrieves the observation and returns it to the agent as the result of the function call. Meanwhile, react() waits for the next action to appear in the action queue. On the next invocation of wrapper function, the agent places new action in the action queue, which releases the blocked react() call. The action is then returned to the orchestrator, which forwards it to the benchmark session, obtains the next observation, and calls react() again. This cycle continues until either the agent produces no further actions or the benchmark provides no further observations, signaling the end of the session. Benchmark Side. During initialization in TauBenchBenchmark. start(), the list of available task names is retrieved from the τ 2-Bench codebase. When TauBenchBenchmark. next session() is invoked, Session wrapper object is constructed. This wrapper defines the textual task description for the selected task and translates τ 2-Benchs OpenAI tool specifications into Exgentic protocol actions. It then builds proxy agent compatible with τ 2-Benchs internal agent API and begins executing τ 2-Bench code for the selected task. When τ 2-Bench calls the proxy agent to obtain the next action given simulated user message, the proxy agent stores the message in an observation queue and waits for an action to appear in the action queue. Once the orchestrator executes observation = TauBenchBenchmark. step(action), the benchmark wrapper stores the action in the action queue, allowing the proxy agent to resume and forward the action to τ 2-Bench. Meanwhile, TauBenchBenchmark. step() blocks on the observation queue. When the proxy agent is called again by the τ 2-Bench code with the next simulated user message , it stores the message in the observation queue, enabling the observation to be returned to the orchestrator, which then passes it to the real agent. B. Benchmark Adaptation B.1. SweBench Task Definition Example"
        },
        {
            "title": "TASK",
            "content": "Resolve the given issue by editing the repository files directly on remote machine. Repository directory on the remote machine: /testbed ## Issue to resolve: Missing call make_hashable on through_fields in ManyToManyRel Description In 3.2 identity property has been added to all ForeignObjectRel to make it possible to compare them. hash is derived from said identity and its possible because identity is tuple. To make limit_choices_to"
        },
        {
            "title": "General Agent Evaluation",
            "content": "hashable (one of this tuple elements), theres call to make_hashable. It happens that through_fields can be list. In such case, this make_hashable call is missing in ManyToManyRel. For some reason it only fails on checking proxy model. think proxy models have 29 checks and normal ones 24, hence the issue, but thats just guess. Minimal repro: class Parent(models.Model): name = models.CharField(max_length=256) class ProxyParent(Parent): class Meta: proxy = True class Child(models.Model): parent = models.ForeignKey(Parent, on_delete=models.CASCADE) many_to_many_field = models.ManyToManyField( to=Parent, through=\"ManyToManyModel\", through_fields=[child, parent], related_name=\"something\" ) class ManyToManyModel(models.Model): parent = models.ForeignKey(Parent, on_delete=models.CASCADE, related_name=+) child = models.ForeignKey(Child, on_delete=models.CASCADE, related_name=+) second_child = models.ForeignKey(Child, on_delete=models.CASCADE, null=True, default=None) Which will result in: File \"manage.py\", line 23, in <module> main() File \"manage.py\", line 19, in main execute_from_command_line(sys.argv) ... File \".../django/db/models/fields/reverse_related.py\", line 140, in __hash__ return hash(self.identity) TypeError: unhashable type: list Solution: Add missing make_hashable call on self.through_fields in ManyToManyRel. ## Execution Environment & Access (STRICT): All commands are executed on remote machine that already contains the full repository and all required system dependencies and prerequisites. The remote machine is accessible ONLY via the bash action. All interactions with the filesystem and environment including reading files, editing files, and running scripts MUST be done through bash. Each command is executed in separate shell invocation; working directory changes and environment variables do not persist between commands. Only files written to disk persist. All bash commands are executed with the working directory set to /. ## Instructions: You must fix the issue by directly modifying files in this repository. ALL file edits MUST be performed using bash. Explanations or code snippets in chat are not sufficient. Only changes written to the actual repository files will be included in the final patch. Modify only non-test source files in /testbed."
        },
        {
            "title": "General Agent Evaluation",
            "content": "## Hard Boundaries: - MODIFY ONLY: regular source code files in /testbed - DO NOT MODIFY: tests or configuration files - DO NOT use interactive editors - DO NOT suggest changes without implementing them ## Patch & Submission Mechanics: Submission captures the entire working tree using: git add -A && git diff --staged C0 This means: - All intended fixes MUST appear in repository files - Any file present at submission time WILL be included in the patch - Temporary files must be removed before submission ## Recommended Workflow: 1. Analyze the codebase 2. Reproduce the issue 3. Edit using bash 4. Verify fix 5. Test edge cases ## Submission: Use submit_patch exactly once with short summary. ## Evaluation: The patch will be applied to hidden test suite and must pass all checks."
        },
        {
            "title": "Key",
            "content": "(no entries)"
        },
        {
            "title": "ACTIONS",
            "content": "bash finish B.2. BrowseComp Task Definition Example"
        },
        {
            "title": "TASK",
            "content": "Answer the provided question by performing search and document expansion as needed, and submit your final answer. Question: need you to name the first and last name of the production controller of specific Indian film. The director of this film made one other movie the same year, made his directorial debut in the 1980s, and has since directed over thirty films. The film features the debut of an actor known for starring in satirical show influenced by real-world events, which is loosely adapted from cartoon series published in famous magazine. This show has more than 4,000 episodes. This actor has also played small roles in over 60 films. The film was released between 1990 and 2020, and 1980s Hollywood movie inspired its storyline. Additionally, an actor in the main cast is featured in film with title connected to classic game of strategy, this film was released in the 2000s. Note: - The question has an answer discoverable through proper search."
        },
        {
            "title": "General Agent Evaluation",
            "content": "- The question requires putting together information from different sources. Your performance is scored based on: 1. Most importantly, the correctness of the answer you assembled from different searches. 2. Your effective use of search and your ability to retrieve all relevant information for the question. 3. How efficiently you find all the relevant information, using as few searches as possible. Important: During your work, Do NOT interact with the user or send any messages at any point - messages will be ignored and are NOT considered valid final answer. The ONLY acceptable way to finish is by calling submit with the required structured fields. Finish the session always by calling submit. If you fail to find the answer, submit with exact_answer: \"Cant find the answer.\"."
        },
        {
            "title": "Key",
            "content": "(no entries)"
        },
        {
            "title": "ACTIONS",
            "content": "search(query: str)"
        },
        {
            "title": "Value",
            "content": "submit(exact answer: str, explanation: str, confidence: float) get document(docid: str) B.3. Appworld Task Definition Example"
        },
        {
            "title": "TASK",
            "content": "Task from supervisor: have invited some of my friends to reunion party via phone messages. have made CSV to track who is coming or not in /documents/personal/ in my file system. Please update RSVPs in it as per their latest replies."
        },
        {
            "title": "Value",
            "content": "This environment provides set of applications, each exposing predefined set of APIs that may be used to perform tasks on behalf of the supervisor. The applications include: api docs, supervisor, amazon, phone, file system, spotify, venmo, gmail, splitwise, simple note, todoist. The available applications and their APIs are fixed for the task. Supervisor account credentials (such as emails, usernames, and passwords) are available through the supervisor applications APIs and are accessed from there when required. If an application requires an access token to perform authenticated operations, the access token is obtained by calling that applications authentication/login API using the credentials retrieved from the supervisor application. Access tokens are not provided by the supervisor application. References to people (e.g., friends, family, roommates) correspond to entries in the phone contacts application. References to files or storage correspond to the file system application, not the local machine filesystem. Time-based instructions (e.g., this month, yesterday) are interpreted with full calendar boundary ranges. If an API returns paginated results, all pages constitute the complete result. The environment consists only of the provided applications and their documented APIs and parameters. No additional endpoints, methods, arguments, or capabilities are assumed beyond those explicitly defined. When task execution is finished, the designated task-completion API is used to signal completion. If the task requires final answer value, the answer is returned through that completion API. If the task cannot be completed using the available applications and APIs, the task may be marked as failed. { first name: Ashley, last name: Moore, email: as moore@gmail.com, phone number: 7336094411 } 2023-05-18T12:00:"
        },
        {
            "title": "Key",
            "content": "policy supervisor datetime ACTIONS (OVERALL 468) finish supervisor.show profile supervisor.show addresses supervisor.show payment cards supervisor.show account passwords amazon.show account amazon.signup amazon.delete account amazon.update account name amazon.login amazon.logout amazon.clear browsing history amazon.search sellers amazon.show cart amazon.update product quantity in cart amazon.show wish list amazon.update address amazon.show product reviews amazon.write product review"
        },
        {
            "title": "General Agent Evaluation",
            "content": "amazon.show product questions .... many more tools ... phone.search contacts phone.send text message phone.show alarm phone.update alarm file system.create directory file system.show file spotify.show account spotify.search songs simple note.create note todoist.create task B.4. Tau2Bench Task Definition Example"
        },
        {
            "title": "TASK",
            "content": "You are customer service agent that helps the user according to the <policy> provided below. Try to be helpful and always follow the policy."
        },
        {
            "title": "Key",
            "content": "policy"
        },
        {
            "title": "Value",
            "content": "# Airline Agent Policy The current time is 2024-05-15 15:00:00 EST. As an airline agent, you can help users **book**, **modify**, or **cancel** flight reservations. You also handle **refunds and compensation**. Before taking any actions that update the booking database (booking, modifying flights, editing baggage, changing cabin class, or updating passenger information), you must list the action details and obtain explicit user confirmation (yes) to proceed. You should not provide any information, knowledge, or procedures not provided by the user or available tools, or give subjective recommendations or comments. You should only make one tool call at time, and if you make tool call, you should not respond to the user simultaneously. If you respond to the user, you should not make tool call at the same time. You should deny user requests that are against this policy. You should transfer the user to human agent if and only if the request cannot be handled within the scope of your actions. To transfer, first make tool call to transfer to human agents, and then send the message YOU ARE BEING TRANSFERRED TO HUMAN AGENT. PLEASE HOLD ON. to the user. ......"
        },
        {
            "title": "ACTIONS",
            "content": "message book reservation calculate"
        },
        {
            "title": "General Agent Evaluation",
            "content": "cancel reservation get reservation details get user details list all airports search direct flight search onestop flight send certificate transfer to human agents update reservation baggages update reservation flights update reservation passengers get flight status C. Agent Components We outline the key components and, in Table 5, analyze the components present in each agent. Execution Runtime. Agents may have access to sandboxed execution environments where they can run code dynamically. For example, SmolAgents provides Python interpreter, while Claude Code operates within Linux machine environment. These runtime environments enable agents to execute and test code as part of their problem-solving process. Tool Shortlisting. preprocessing component that filters the available tool set before each action step, selecting relevant subset based on current context. This improves efficiency and decision quality by focusing the agent on contextually appropriate tools, and addresses LLM constraints on tool countwhen the full tool set exceeds model limits, shortlisting becomes necessary for task completion. Tool Schema Guard. component that validates actions against expected schemas before execution. When an agent attempts to call tool or execute an environment action with incorrect parameters or structure, the schema validator raises an internal error, allowing the agent to detect and correct the mistake. This component is implemented differently across agent types: tool-calling agents typically lack explicit schema validation (relying on the LLM to generate correct calls), MCP-based agents include built-in schema validation as part of the MCP protocol, and Python-based agents receive runtime errors from the Python interpreter that serve similar validation function. Communication Protocol. The interface through which agents invoke tools and receive results. Agents may use direct tool-calling APIs (e.g., OpenAI function calling), code-generation approaches where the agent writes executable code, or standardized protocols like MCP. Protocol choice affects action expressiveness and error handling mechanisms available to the agent. Memory. Explicit storage and retrieval mechanisms beyond the conversation history. Memory components allow agents to maintain working state across turns, recall previous observations, and avoid redundant actions. Without explicit memory, agents rely solely on the LLMs context window. Planning. Components that decompose tasks into structured subgoals before execution. Planning modules may generate explicit task hierarchies or action sequences, enabling more directed problem-solving. Agents without planning components select actions reactively at each step based on immediate observations. D. Detailed Results Table 1 presents the complete leaderboard results, including the average number of steps."
        },
        {
            "title": "General Agent Evaluation",
            "content": "Table 5. Architectural components of evaluated agents. denotes an explicit, modular component; capability; denotes absence. denotes an implicit or non-modular"
        },
        {
            "title": "Execution Runtime Tool Shortlisting Tool Schema Guard Communication Protocol Memory Planning",
            "content": "Tool-calling Tool-calling Python-Functions"
        },
        {
            "title": "MCP\nMCP",
            "content": "Table 6. Agent-Model Configuration Leaderboard"
        },
        {
            "title": "SWE Airline Retail Telecom Mean\nScore",
            "content": "Steps Cost ($) (avg) OpenAI Solo Claude Opus 4.5 Claude Opus 4.5 Claude Code Claude Opus 4.5 Smolagent Gemini 3 ReAct Short Claude Opus 4.5 ReAct Short Gemini 3 ReAct ReAct Claude Opus 4.5 OpenAI Solo Gemini 3 Gemini 3 Claude Code Gemini 3 Smolagent GPT 5.2 ReAct Short ReAct GPT 5.2 OpenAI Solo GPT 5.2 GPT 5.2 Claude Code GPT 5.2 Smolagent 0.68 0.66 0.70 0.55 0.64 0.51 0.61 0.58 0.36 0.13 0.22 0.00 0.00 0.00 0.07 0.61 0.53 0.61 0.48 0.49 0.48 0.49 0.33 0.51 0.57 0.46 0.46 0.48 0.43 0.26 0.81 0.74 0.65 0.71 0.61 0.71 0.61 0.72 0.67 0.76 0.57 0.57 0.55 0.58 0.53 0.74 0.66 0.72 0.70 0.66 0.70 0.66 0.62 0.70 0.68 0.54 0.54 0.50 0.48 0. 0.85 0.83 0.78 0.82 0.78 0.82 0.78 0.73 0.78 0.76 0.73 0.73 0.54 0.51 0.68 0.84 0.76 0.58 0.73 0.76 0.73 0.76 0.89 0.69 0.88 0.54 0.54 0.53 0.55 0.71 0.73 0.67 0.66 0.62 0.62 0.61 0.61 0.60 0.57 0.56 0.46 0.41 0.39 0.38 0.38 30.7 31.7 29.2 18.8 24.5 18.6 25.0 21.3 29.0 32.2 12.3 9.8 11.3 10.7 22.2 8.54 8.03 4.39 0.66 3.78 0.81 5.75 2.81 2.47 1.85 0.26 0.17 0.19 0.38 0.36 D.1. References to Leaderboards For reference, SWE-Bench Verified leaderboard top reported domain-specific agent achieves 0.798, BrowseComp+ and AppWorld are 0.809, and 0.7310, respectively. τ 2-Bench Airline (0.73), Retail (0.86), and Telecom (0.98)11. D.2. Steps Counts Table 7. Average steps per benchmark and architecture, split by successful vs. failed sessions; models are aggregated, 0-step sessions are excluded, and steps are capped at 50."
        },
        {
            "title": "Smolagent Fail ReAct Succ ReAct Fail ReAct Short Succ ReAct Short Fail",
            "content": "AppWorld BrowseComp+ SWE-Bench Verified airline retail telecom weighted avg 23.67 13.90 27.69 10.43 11.73 12.98 19.24 38.56 23.64 32.21 13.02 11.54 12.25 26. 26.41 15.23 27.76 10.19 11.36 13.06 20.24 39.39 17.96 29.30 13.65 9.81 13.26 24.72 25.69 15.89 29.28 10.39 11.48 11.99 20. 34.17 23.83 32.03 14.06 11.31 12.75 25.68 13.24 9.28 28.38 9.31 10.81 13.23 15.50 27.91 15.53 34.22 12.18 11.52 15.84 22. 12.34 9.28 28.38 9.31 10.81 13.23 15.28 21.41 15.53 34.22 12.18 11.52 15.84 21.08 E. Statistical Significance We assess the statistical significance of the benchmark results. The evaluation consists of six benchmarks, each containing 100 independent instances with binary (0/1) success outcomes (except τ 2-Bench Airline, which contains 50 instances). 8SWE-Bench Verified 9BrowseComp+ 10AppWorld 11τ 2-Bench"
        },
        {
            "title": "General Agent Evaluation",
            "content": "For single benchmark with = 100 binary trials, the 95% Wilson confidence-interval half-width typically ranges from 7 to 9.5 percentage points when the observed success rate lies between 0.3 and 0.8the region where most leading models perform. This means that differences smaller than approximately 810 percentage points on individual benchmarks should be interpreted cautiously, as they fall within normal statistical uncertainty. To obtain more stable measure, we compute weighted aggregate score across all benchmark instances. Under the assumption that benchmarks are independent of one another, this yields an effective sample size of = 650. The corresponding 95% delta-method confidence-interval half-width for the aggregated score is substantially smallertypically in the range of 45 percentage points. Thus, while individual benchmark scores have relatively wide uncertainty due to limited sample sizes, the aggregated metric provides more reliable estimate of overall agent performance. It is important to note that these levels of statistical uncertainty are standard across existing agentic leaderboards. Most widely used agent-evaluation platforms report confidence intervals on the order of only few percentage points, reflecting the inherent variability of evaluations conducted on datasets of similar size. To enhance statistical power when comparing benchmarks, we employ McNemars test for pairwise analysis. This allows us to determine if one configuration significantly outperforms another by isolating performance discrepancies on identical tasks. F. Limitations While Exgentic provides clear methodology and reusable building blocks for adaptation, familiarity with these capabilities and addition development work is still required when integrating new agents or benchmarks. Currently, Exgentic focuses on evaluating tasks where all agentbenchmark interactions are text-based. Future extensions should support visual or web-based interaction. The Unified Protocol was designed around the APIs of subset of existing systems and may need to be expanded to accommodate these additional protocols Agent evaluation is expensive, more over for general-purpose agents that must be tested across many benchmarks. Due to cost constraints, our selection of agents and models is limited and does not cover the full range of open-source models or existing general-purpose agents. To enable further progress in the field, future work should therefore explore techniques such as intelligent sampling and early stopping to reduce evaluation costs when it is clear that certain agentmodel combinations underperform."
        }
    ],
    "affiliations": [
        "MIT-IBM"
    ]
}