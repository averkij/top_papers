{
    "paper_title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
    "authors": [
        "Yudong Wang",
        "Zixuan Fu",
        "Hengyu Zhao",
        "Chen Zhao",
        "Chuyue Zhou",
        "Xinle Lin",
        "Hongya Lyu",
        "Shuaikang Xue",
        "Yi Yi",
        "Yingjiao Wang",
        "Zhi Zheng",
        "Yuzhou Zhang",
        "Jie Zhou",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 3 0 0 9 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Data Science and Technology",
            "content": "Data Science and Technology Towards AGI Part I: Tiered Data Management Yudong Wang1, Zixuan Fu1, Hengyu Zhao2,3, Chen Zhao2, Chuyue Zhou2, Xinle Lin2,4, Hongya Lyu2, Shuaikang Xue2, Yi Yi2, Yingjiao Wang2, Zhi Zheng2, Yuzhou Zhang2, Jie Zhou2, Chaojun Xiao1, Xu Han1, Zhiyuan Liu1, Maosong Sun1 1Tsinghua University 2ModelBest Inc. 3Beijing Institute of Technology 4South China Agricultural University yudongwang@tsinghua.edu.cn zhoujie@modelbest.cn {xcj,han-xu,liuzy}@tsinghua.edu.cn https://huggingface.co/collections/openbmb/ultradata https://ultradata.openbmb.cn Abstract The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Despite remarkable progress, current large language model (LLM) research is dominated by paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of artificial general intelligence (AGI) is entering new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework explicitly balances data quality, acquisition cost, and marginal training benefit, providing systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies on math and web data, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community."
        },
        {
            "title": "Introduction",
            "content": "The development of artificial intelligence can be viewed as an evolution of data-driven strategies and data utilization paradigms (Zha et al., 2025). Each paradigm shift extends and restructures prior approaches while introducing new methods for utilizing and managing data. These transformations have consistently driven improvements in model capability, enabling the emergence of higher-level intelligence (Wei et al., 2022; Gan et al., 2026). Based on the primary data types that drive each era, the developmental trajectory can be divided into four phases. (Buchanan & Feigenbaum, 1981; Shortliffe, 2012; Rumelhart et al., 1986; Cortes & Vapnik, 1995; * Equal contribution. Corresponding authors. Project leaders."
        },
        {
            "title": "Data Science and Technology",
            "content": "Krizhevsky et al., 2012; He et al., 2016; Vaswani et al., 2017) (1) Symbolic Learning: the initial data era established paradigm driven by knowledge data, such as human-annotated rules. It relied on experts to codify world knowledge into static knowledge bases (Augusto, 2021) and implemented intelligence through explicit rules. (2) Supervised Learning: the era of statistical and deep learning established paradigm driven by labeled data (Krizhevsky et al., 2012). This stage witnessed the transition from manual feature engineering to end-to-end supervised training (Bengio et al., 2013; LeCun et al., 2015). The model performance became directly dependent on data scale, quality, and representational capacity (Sun et al., 2017). (3) Self-supervised Learning: the pre-training era further reduced dependence on labeled data and enabled self-supervised learning driven by unsupervised data. Training on massive corpora allows models to compress and internalize world knowledge, leading to strong generalization and emergent capabilities across modalities (Wei et al., 2022; Achiam et al., 2023; Dubey et al., 2024). (4) Feedback Learning: in the feedback-driven era, models leverage human and environmental feedback through reinforcement learning (RL) (Ziegler et al., 2019; Kaufmann et al., 2024). Continuous interaction enables active exploration of model behavior and capability improvement (Rafailov et al., 2023; Liu et al., 2024). This stage has strengthened decision-making and adaptability in complex settings and has laid an important foundation for progress toward artificial general intelligence (AGI) (Gan et al., 2026). As shown in Figure 1, current mainstream research primarily manifests as Data-Driven Learning, which emphasizes unidirectional enhancement of model capabilities through expansion of data scale (Zhou et al., 2025b). As model capabilities advance, we argue that AI development should transition toward Data-Model Co-Evolution, wherein models improve data management practices while high-quality data further refines model performance (Yuan et al., 2024), creating positive feedback cycle. To accommodate this paradigm shift, this paper focuses on presenting model-driven Tiered Data Management framework, aiming to provide systematic technical support for advancing toward artificial general intelligence. The necessity of implementing tiered data management stems from three core considerations. (1) High-quality public data resources are becoming increasingly scarce. Future model development cannot rely solely on expanding data scale (Villalobos et al., 2022). Instead, data science and technology must shift from pursuing scale toward more careful data management and utilization. (2) LLM training involves multiple different phases from knowledge acquisition during pre-training to behavioral alignment (Ouyang et al., 2022) during fine-tuning each with different requirements for data quality, quantity, and distribution (Mo et al., 2025; Zha et al., 2025). This necessitates designing specialized training datasets suited to each phases specific learning objectives. (3) Data management must balance the costs of data acquisition against the benefits to model performance. In the early stage of data management, lightweight and low-cost methods (such as heuristic filtering) should be adopted, while in deeper management stage, more fine-grained and Figure 1: Paradigm Shift in Data Organization and Utilization. The evolution of LMs and ultimately toward AGI fundamentally represents paradigm shift in the organization and utilization of data, progressing through Symbolic, Supervised, Self-supervised, and Feedback Learning phases. We argue that the field is transitioning toward Data-Model Co-Learning phase, which necessitates three critical research pillars: Scientific Data Value Assessment, Hierarchical Data Management, and Dynamic Data-Model Co-evolution to transcend current sustainability bottlenecks."
        },
        {
            "title": "Data Science and Technology",
            "content": "higher-cost approaches (such as LLM-based labeling) should be used (Zhou et al., 2025b). Since high-quality data typically requires significant investment, strategically deploying valuable data at critical training moments such as mid-training phases or annealing stages can maximize data effectiveness while keeping overall costs manageable. Despite significant academic progress in specific data processing tasks, such as filtering (Penedo et al., 2024c; Young et al., 2024; Soldaini et al., 2024b), selection (Chen et al., 2023b; Penedo et al., 2023; Xie et al., 2023; Soldaini et al., 2024b; Wettig et al., 2024; Engstrom et al., 2024; Dubey et al., 2024), and editing (Eldan & Li, 2023; Gunasekar et al., 2023; Li et al., 2023c; Wang et al., 2023; Taori et al., 2023; Peng et al., 2023; Xu et al., 2024a; Wang et al., 2024e; Ding et al., 2023; Cui et al., 2023), these approaches often fall short of addressing the systematic requirements of the LLM full-lifecycle training. To address this issue, we propose an L0-L4 tiered data management framework, evolving from raw resources to structured knowledge: (1) L0: Raw Data. L0 data comprises PB-scale, uncurated resources characterized by high redundancy and noise, such as raw web dumps containing advertisements. It is maintained in its original state without deep processing. Consequently, it is primarily utilized for archiving and traceability rather than direct model training. (2) L1: Filtered Data. L1 data features standardized text formatting and basic readability. It is usually produced via heuristic cleaning and deduplication to remove significant noise like web advertisements. As result, it serves as the foundational resource pool for subsequent data selection and evaluation. (3) L2: Selected Data. L2 data retains samples with distinct themes and high information density, suitable for knowledge learning and domain adaptation (e.g., high-quality academic papers, technical code repositories, or filtered encyclopedia articles). (4) L3: Refined Data. L3 data features structured content with clear reasoning and explicit educational intent, ensuring maximum learnability. It is usually produced through rewriting, synthetic generation, or human refinement to achieve textbook-quality standards. Consequently, it serves as the core resource for advanced training phases like mid-training. (5) L4: Organized Data. L4 data consists of trustworthy and verifiable knowledge. It is created by converting unstructured text into organized formats, such as knowledge graphs or databases, and rigorously verifying the facts. Consequently, these data can provide the solid factual support necessary for retrieval-augmented generation. To validate the effectiveness of the proposed tiered data management framework, we conduct comprehensive empirical study across four representative domains, including English web, Chinese web, mathematics, and code data. By systematically constructing tiered datasets and applying them across the LLM training lifecycle, we demonstrate that the performance improves as data quality ascends from L1 to L3. Specifically, our results reveal that high-tier data (e.g., Math-L3) not only achieves domain-specific superiority but also acts as fundamental driver for general reasoning, yielding significant cross-domain gains in language understanding and programming. Furthermore, addressing the growing complexity of multi-stage training, we explore the application of tiered data management for multi-stage training to mitigate the interference of low-quality samples that often hampers late-stage convergence. Our analysis reveals that tiered training strategy, which introduces higher-quality data in the later phases, effectively prevents performance saturation and consistently outperforms mixed-training strategy. These findings underscore the necessity of tiered data management as core element of data science and technology for AGI, establishing the granular quality control essential for modern scaling laws. As shown in Table 1, we have open-sourced an extensive collection of UltraData datasets and tools. Moving forward, we remain committed to the continuous release of these resources and invite the community to integrate our tiered management logic into the evolving landscape of data science and technology towards AGI."
        },
        {
            "title": "2 Tiered Data Management Framework",
            "content": "In the training of LLMs, data quality serves as textbooks for model learning. In recent LLM construction, data of varying quality are often indiscriminately mixed during training. This coarse-grained approach hinders the fully utilization of high-value data samples, leading to suboptimal model performance. Prior studies have demonstrated that concentrating high-quality data during specific phases, such as annealing or mid-training, can significantly enhance model performance (Hu et al., 2024; Wang et al., 2025c). In this section, we first review existing data management research from the perspectives of training stages and processing methodologies. Building on this summary, we propose fine-grained management framework with data quality-based tiering. This framework facilitates seamless transitions and dynamic optimization across the entire data lifecycle, encompassing collection, cleaning, filtering, and utilization."
        },
        {
            "title": "Link",
            "content": "Table 1: Open-source datasets and tools released in this paper."
        },
        {
            "title": "Description",
            "content": "Dataset Tool UltraData-Math-L1 UltraData-Math-L2 UltraData-Math-L3 Ultra-Fineweb-en (L2) Ultra-Fineweb-en-L3 Ultra-Fineweb-zh (L2) Ultra-Fineweb-zh-L3 Filtered math data using heuristic rules Model-selected math data Synthetic and refined math data Model-selected English web corpus Synthetic and refined English web data Model-selected Chinese web corpus Synthetic and refined Chinese web data UltraData-Math-Parser UltraData-Math-Generator Enhanced HTML parser for math content Synthetic math problem generator Ultra-FineWeb-en-Classifier Ultra-FineWeb-zh-Classifier Classifier for selecting English web data Classifier for selecting Chinese web data"
        },
        {
            "title": "Scale",
            "content": "170B 33B 88B 1,800B 200B 120B 200B / / / /"
        },
        {
            "title": "2.1 Existing Data Management Frameworks",
            "content": "Existing data management frameworks are typically organized by model training stages or specific data processing methodologies. In this section, we first introduce the stage-oriented management framework and the method-oriented management framework. Then we propose our tiered data management framework."
        },
        {
            "title": "2.1.1 Stage-Oriented Management Framework",
            "content": "This framework is tightly coupled with the entire model training lifecycle. Driven by distinct objectives, such as knowledge acquisition, domain enhancement, and capability alignment, it categorizes corpora and methods into specialized categories for pre-training, mid-training, and post-training phases. Given the significant variations in requirements for scale, diversity, and signal-to-noise ratio across these stages, the framework typically establishs parallel management standards and differentiated processing pipelines. Pre-training data management. Pre-training management strives for an optimal equilibrium between data scale and information density, while strictly preserving domain diversity. Its core objective is to endow models with broad general knowledge and establish solid foundation for linguistic representation. As data engineering techniques evolve, management paradigms are transitioning from early heuristic rule-based filtering to statistical deduplication and model-driven selection, and ultimately advancing toward active synthesis and generation. Foundational efforts such as C4 (Raffel et al., 2020) established the baseline for web data cleaning, relying predominantly on heuristic rules and language identification to eliminate low-quality text. Building on this, RefinedWeb (Penedo et al., 2023) demonstrated that large-scale fuzzy deduplication and strict URL filtering enable pure web corpora to surpass mixed datasets like The Pile (Gao et al., 2020). Moreover, FineWeb-Edu (Penedo et al., 2024c), DCLM (Li et al., 2024b), and Ultra-FineWeb (Wang et al., 2025b) employed model-based classifiers to score webpages for educational value, providing compelling evidence that filtering driven by powerful models significantly outperforms traditional heuristics. In response to the impending scarcity of high-quality natural corpora, the Phi series (Gunasekar et al., 2023; Abdin et al., 2024) introduced textbook-level synthetic paradigm, while Nemotron-CC (Su et al., 2025) expanded this frontier by leveraging strong models to generate nearly noise-free pre-training corpora. Mid-training data management. Mid-training management prioritizes specialized knowledge and structured reasoning to bolster model performance on vertical tasks. For mathematical data, the primary challenge involves reconstructing rigorous logical structures from unstructured web content. OpenWebMath (OWM) (Paster et al., 2023) and MegaMath (Zhou et al., 2025a) have substantially enhanced LaTeX formula extraction via optimized HTML parsing strategies, effectively mitigating issues of formula omission and misalignment that are prevalent in traditional extraction methods. To identify and retain high-quality mathematical content, MathScore (Paster et al., 2023) and DeepSeek-Math (Shao et al., 2024) develop domain-specific classifiers for semantic-level filtering. Building on these foundations, Nemotron-CC-Math (Mahabadi et al., 2025) pioneered parsing-then-editing paradigm. By preserving the visual layout of mathematical expressions through Lynx rendering and employing large language models (LLMs) as neural editors, this approach repairs fragmented reasoning steps, yielding high-fidelity, noise-free corpora. In the realm of code data, The Stack v2 (Lozhkov et al., 2024) establishes robust baseline by adopting strict heuristic filtering and deduplication pipelines. DeepSeek-Coder-V2 (Zhu et al., 2024) expands upon this by incorporating model-based filtering to"
        },
        {
            "title": "Data Science and Technology",
            "content": "retrieve high-quality code and technical documentation previously overlooked in Common Crawl. Furthermore, synthetic augmentation strategies such as Code Needs Comments (Song et al., 2024a) and AlchemistCoder (Song et al., 2024b) significantly refine code quality by generating detailed documentation and synthesizing programming tasks. Beyond general coding tasks, vertical domains such as law and medicine require more stringent management. Works such as SaulLM (Colombo et al., 2024) and PMC-LLaMA (Wu et al., 2024) have engineered citation-based deduplication strategies and entity-extraction-based filtering mechanisms to guarantee knowledge traceability and factual accuracy. Post-training data management. Post-training data management focuses on curating and filtering data during the instruction fine-tuning and reinforcement learning stages after base model pre-training, aiming to improve the models responsiveness to human instructions and adherence to intended behavioral norms. This stage also emphasizes balancing data quality and diversity, extensively leveraging strong-model synthesis and customized filtering strategies to achieve fine-grained control. During the instruction/supervised finetuning (SFT) phase, representative methodologies have focused on scaling and refining synthetic instructions. Self-Instruct (Wang et al., 2023) established the foundational paradigm of bootstrapping instruction data by guiding models with minimal set of human-authored seed tasks. UltraChat (Ding et al., 2023) extended this paradigm to multi-turn conversational settings, employing separate LLMs to iteratively simulate user and assistant roles and systematically covering diverse thematic sectors including world knowledge, creative writing, and document-grounded assistance. Evol-Instruct (Xu et al., 2023) advanced this by systematically escalating logical difficulty and coverage through evolutionary mechanisms along both depth and breadth dimensions. To address distributional bias, OSS-Instruct (Wei et al., 2023b) introduced external open-source code as high-quality logical prior. More recently, Magpie (Xu et al., 2024b) exploited the autoregressive properties of aligned models to facilitate seed-free, introspective instruction generation, effectively uncovering latent capability spaces without external reliance. In terms of SFT data selection, inspired by the less is more principle advocated by LIMA (Zhou et al., 2023), research has pivoted toward model-driven strategies to maximize the information density of instruction data. Approaches such as MoDS (Du et al., 2023) and DEITA (Liu et al., 2023a) quantify data value by rigorously evaluating samples across quality, coverage, and complexity dimensions. In the reinforcement learning (RL) stage, data management mainly focuses on the precise construction of preference signals. UltraFeedback (Cui et al., 2023) pioneered this approach by coupling multi-model sampling with fine-grained, multi-dimensional AI annotations. By evaluating criteria such as instruction following and honesty, this framework synthesizes high-quality alignment signals to refine model performance. More recently, RL data paradigms have evolved from emphasizing stylistic preferences to prioritizing logical correctness. In reasoning-intensive domains such as mathematics and coding, this trend manifests in the construction of closed-loop datasets featuring problems with verifiable answers, where deterministic reward functions are employed to adjudicate outcomes (Shao et al., 2024; Guo et al., 2025). This methodology reduces reliance on expensive process-level annotations (Lightman et al., 2023), enabling models to achieve sustained capability evolution and qualitative leaps through autonomous exploration within feedback loops."
        },
        {
            "title": "2.1.2 Method-Oriented Management Framework",
            "content": "This framework centers on processing methodologies. Rather than being delineated by training stages, it organizes the data pipeline according to the complexity of processing logic and the depth of intelligence involved, forming hierarchical workflow that ranges from basic cleaning to advanced data synthesis. Data Parsing. As the entry point of the management pipeline, data parsing transforms heterogeneous raw files into machine-interpretable, logically coherent structured assets. For HTML parsing, traditional tools like Trafilatura (Barbaresi, 2021a) employ heuristic rule-based matching algorithms to separate meaningful content from boilerplate elements. Recent advances reformulate content extraction as semantic understanding problem, leveraging language models to perform sequence labeling on DOM structures (e.g., MinerU-HTML (Liu et al., 2025)) or transform HTML into structured Markdown and JSON formats (e.g., ReaderLM-v2 (Wang et al., 2025a)). For mathematical content, specialized approaches such as OpenWebMath (Paster et al., 2023), MegaMath (Zhou et al., 2025a), and Nemotron-CC-Math (Mahabadi et al., 2025) achieve precise LaTeX formula extraction through optimized HTML processing strategies (detailed in the mid-training management section). For document parsing, pipelines rely on customized frameworks such as MinerU (Wang et al., 2024a) and PaddleOCR (Cui et al., 2025), end-to-end models such as Nougat (Blecher et al., 2023), or OCR solutions powered by Vision-Language Models (e.g., GOT-OCR (Wei et al., 2024), olmOCR (Poznanski et al., 2025), DeepSeek-OCR (Wei et al., 2026), Qwen3-VL (Bai et al., 2025), and DeepSeek-OCR2 (Zhong et al., 2026)) to transform complex-layout documents into structured formats. For audio parsing, models such as Whisper (Radford et al., 2023) enable high-fidelity transcription, converting audio streams into timestamped, speaker-diarized transcripts."
        },
        {
            "title": "Data Science and Technology",
            "content": "Data Filtering. Data filtering serves as the bedrock of the management pipeline, aiming to eliminate noise through low-cost, engineering-centric mechanisms and thereby ensure minimum quality threshold. Early paradigms, exemplified by C4 (Raffel et al., 2020), relied predominantly on heuristic rules. These methods utilize regular expressions, blacklists, and language identification models to excise low-quality, duplicated, or non-target language content. Building upon these foundations, RefinedWeb (Penedo et al., 2023) advanced this by implementing large-scale fuzzy deduplication via MinHash-LSH (Broder, 1997) and rigorous URL filtering, demonstrating that superior filtering significantly boosts pre-training efficiency. Beyond surface-level matching, SemDeDup (Abbas et al., 2023) leverages embeddings to identify semantic duplicates, enabling more aggressive data reduction with minimal performance loss. In multimodal contexts, filtering extends to cross-modal alignment techniques, such as image-text matching scoring (e.g., CLIP Score), ensuring precise semantic correspondence between modalities. Data Selection. Data selection utilizes model-based classifiers to perform multi-dimensional filtering based on data quality and thematic relevance, often enriching samples with semantic annotations in the process. DCLM (Li et al., 2024b), FineWeb-Edu (Penedo et al., 2024c), and Ultra-FineWeb (Wang et al., 2025b) employ classifiers to identify and prioritize corpora with high educational value. Similarly, DeepSeek-Math (Shao et al., 2024) and FineMath (Allal et al., 2025) construct specialized classifiers to pinpoint high-quality, reasoning-intensive samples in the mathematical domain. In the post-training stage, methods such as DEITA (Liu et al., 2023a) and MoDS (Du et al., 2023) assess data utility across dimensions of quality, coverage, and necessity, employing optimization-driven algorithms or influence functions to distill massive instruction pools into compact yet highly representative subsets. Beyond binary quality filtering, DecorateLM (Zhao et al., 2024) distills expert knowledge from large models into lightweight annotators, enabling efficient processing of hundreds of billions of tokens. Its three-level tagging system automatically appends hierarchical knowledge labels and performs standardized editing on raw text, significantly enhancing the structural quality of pretraining corpora. QuRating (Wettig et al., 2024) further advances quality assessment by eliciting pairwise comparisons from LLMs across dimensions such as writing style, required expertise, and educational value, training scalar rating models that enable fine-grained, continuous-valued data selection at scale. At finer granularity, Rho-1 (Lin et al., 2024) introduces Selective Language Modeling that operates at the token level, dynamically focusing training on high-value tokens while skipping uninformative ones. Data Editing. Unlike synthesis, which generates new content, data editing refines and restructures existing datasets to rectify defects and enhance logical coherence. ProX (Zhou et al., 2024) and RefineX (Bi et al., 2025) formulate data refinement as programming task, enabling models to autonomously generate fine-grained editing operations (e.g., string normalization and noise removal) for iterative corpus polishing. Additionally, Nemotron-CC-Math (Mahabadi et al., 2025) and Qwen3 (Yang et al., 2025) employ LLM-based editing to repair formula fragmentation and formatting inconsistencies in parsed documents. Data Synthesis. Data synthesis enables scalable production of high-quality data via generative models, spanning both pre-training and post-training phases. For pre-training, the Phi series (Gunasekar et al., 2023) pioneered the textbook-level corpus generation paradigm, while Nemotron-CC (Su et al., 2025) further scaled this approach by leveraging strong teacher models to synthesize large-scale, low-noise corpora. For posttraining, instruction synthesis has evolved from simple imitation to complex logical construction through methods such as Self-Instruct (Wang et al., 2023), Evol-Instruct (Xu et al., 2023), OSS-Instruct (Wei et al., 2023b), and Magpie (Xu et al., 2024b), as detailed in the post-training data management section. In summary, despite methodological diversity, existing practices remain hampered by the lack of systematic and hierarchical management. The absence of unified tiered standards makes it difficult to identify and prioritize high-value data effectively. Management strategies tend to be monolithic, failing to implement differentiated processing based on data value, intended use, or training stage. Moreover, data processing pipelines are severely fragmentedcollection, cleaning, selection, and validation are often conducted independently without unified quality metrics or closed-loop feedback mechanisms. Additionally, current approaches provide inadequate support for data lineage tracking, making it difficult to trace data provenance, processing paths, and modification histories, which further impedes the automation and intelligent evolution of data management workflows."
        },
        {
            "title": "2.2 Tiered Data Management Framework",
            "content": "To overcome the limitations of existing management frameworks, it is essential to establish fine-grained hierarchical standard centered on data quality and trustworthiness. Specifically, we define five distinct levels (L0L4). Each level represents progressive increase in data purity, albeit with corresponding rise in acquisition and computational costs. In what follows, we elaborate on the specific definitions and representative"
        },
        {
            "title": "Data Science and Technology",
            "content": "Figure 2: Tiered data management framework. This framework establishes fine-grained standard centered on data quality and trustworthiness by defining five distinct stages. The framework sequentially elevates data purity through specialized operators while managing the progression from raw data to high-density knowledge assets and their associated computational costs. datasets for each level, while providing typical case studies based on web and math data to illustrate their practical implementation. Furthermore, to provide concrete and actionable reference, we systematically organize representative open-source tools and publicly available datasets according to our tiered criteria, as summarized in Table 2."
        },
        {
            "title": "2.2.1 L0: Raw Data",
            "content": "The L0 level is the raw archival data level, which serves as the foundational reserve of full-scale data. Data at this level is maintained in its native format or undergoes minimal structural conversion. The corpus encompasses wide array of heterogeneous sources, including general web content (news, blogs, and social media), entertainment media, scholarly literature, and source code repositories. It also integrates multimodal assets such as audio, video, and imagery. Although the L0 level features extensive coverage, its inherent low information density and high noise level render it ineligible for direct use in model training. Acquisition of L0 data is achieved through basic operations including web crawling, batch downloading and format parsing. For different types of data sources, corresponding parsing technologies and tools are adopted. To facilitate HTML parsing, tools such as MinerU-HTML (Ma et al., 2025) and Resiliparse (Bevendorff et al., 2018) are utilized for efficient content extraction; for PDF documents, tools such as MinerU (Wang et al., 2024a), olmOCR (Poznanski et al., 2025) and Nougat (Blecher et al., 2023) can accomplish accurate parsing from PDF to textual data. Currently, representative data at the L0 level are generally classified into three primary categories. In the web data domain, Common Crawl1 is the largest open-source web-crawling dataset, released periodically as snapshot versions. The latest release (updated to January 2026) encompasses over 30 billion web pages spanning 15 years. It provides multiple data granularities for varied data management, including the Web ARChive (WARC) format preserving complete HTTP responses, the Web Archive Transform (WAT) format offering structured metadata and link graphs, and the Web Extracted Text (WET) format providing preprocessed plaintext. In addition, web data from vertical domains such as MathOverflow2 also acts as key raw data source for the L0 level. In the academic literature domain, arXiv3 provides LaTeX source files and PDF versions of preprint papers. In the code data domain, GitHub4, the worlds largest code hosting platform, houses complete code histories and version evolution records of millions of projects, and Stack Overflow5 provides abundant raw data for programming Q&A. 1https://commoncrawl.org 2https://mathoverflow.net/ 3https://arxiv.org/ 4https://github.com/ 5https://stackoverflow.com/"
        },
        {
            "title": "Data Science and Technology",
            "content": "The data management strategy for the L0 level centers on achieving full-scale data storage through data collection technologies, without requiring additional data quality optimization operations. Its core objective is to preserve data integrity and establish foundation for data traceability. Consequently, this provides abundant raw data reserves for subsequent levels of the data management framework, while endowing the framework itself with data retrospection capabilities and potential for secondary processing."
        },
        {
            "title": "2.2.2 L1: Filtered Data",
            "content": "The L1 level is the basic data-cleaning layer. Its core objective is to eliminate obvious noise and construct standardized foundational dataset. The data management philosophy for the L1 level centers on ensuring the basic usability of data via low-cost engineering methods, and its technical pipeline primarily comprises operations including URL filtering, text extraction, language identification, heuristic rule filtering, and global deduplication. In what follows, we elaborate on the L1 level data management practices through two representative works: FineWeb (Penedo et al., 2024c) and UltraData-Math-L1 (UltraData, 2026). FineWeb (Penedo et al., 2024c) validates and optimizes L1-level management strategies through empirical ablation experiments. In terms of text extraction, experiments demonstrate that extracting from WARC using Trafilatura significantly improves model performance compared to using WET files. For base filtering, it adopts URL blacklists from RefinedWeb, the fastText language classifier, and quality filtering rules, processing the data down to approximately 36 trillion tokens. Regarding deduplication strategies, findings indicate that global MinHash deduplication increases the sampling rate of low-quality data in older snapshots, leading to performance degradation. Therefore, FineWeb employs an independent snapshot deduplication strategy (5-gram, 112 hash functions, 14 buckets, targeting 75% similarity), deduplicating each of the 96 Common Crawl snapshots separately to produce about 20 trillion tokens. In terms of heuristic filtering, it integrates rules from the C4 dataset (such as removing lines containing \"javascript\" or \"cookie policy\" and filtering overly short documents) and develops custom filters by comparing statistical metrics of highand low-quality datasets. These custom filters include line-ending punctuation ratio filtering ( 0.12), duplicated line character ratio filtering ( 0.1), and short line ratio filtering ( 0.67). Collectively, these filters remove approximately 22% of tokens and improve the aggregate score by about 1%. Ultimately, the FineWeb dataset contains 15 trillion tokens and outperforms existing public datasets on multiple benchmarks. UltraData-Math (UltraData, 2026) implements its L1-level management through specialized suite of cleaning operators designed for mathematical content. This suite is organized into two primary categories: format repair mappers and content filters. The format repair mappers focus on text standardization without altering the record count. These operators perform fine-grained cleaning tasks, such as eliminating invisible characters and control codes, consolidating excessive consecutive line breaks, and stripping residual interface noise, including navigation bars and pagination buttons. These operations transform raw, noisy extractions into clean, readable text. The content filters focus on record-level pruning based on heuristic rules. Records that fail to meet basic usability standards are discarded entirely. Key filtering criteria include removing short articles that lack proper punctuation and documents with abnormal text lengths. By orchestrating these two types of operators, UltraData-Math-L1 ensures high format consistency and eliminates obviously low-quality samples, establishing standardized foundation for the subsequent model-driven selection and synthesis stages. Data at the L1 level is primarily applied to large-scale pre-training, furnishing models with foundational capacity for general knowledge comprehension and linguistic representation. Marked by low processing costs and high scalability, this level acts as pivotal stage in constructing high-quality pre-training corpora."
        },
        {
            "title": "2.2.3 L2: Selected Data",
            "content": "The L2 level is defined as the selected data layer, which enhances data information density through modeldriven selection mechanisms. This level marks paradigm shift from rule-based to model-driven data management, with its management strategies comprehensively adopting domain-specific classifiers, semanticlevel selecting, quality scoring, data labeling and other approaches. The selection process at this level is essentially value discovery process; it leverages the discriminative capabilities of models to identify and retain high-value data samples. We elaborate on the L2 level data management practices through two representative works: Ultra-FineWeb (Wang et al., 2025b) and FineMath (Allal et al., 2025). Ultra-FineWeb (Wang et al., 2025b) proposes more efficient method for training data selection models, based on the hypothesis that high-quality seed data benefits LLM training and fosters stronger classifiers to identify similar beneficial data. To reduce the prohibitive costs of traditional validation, the method introduces an efficient validation strategy based on weight-decay scheduler and two-stage annealing phase. This approach significantly reduces GPU hours required, enabling researchers to rapidly assess the impact of different data"
        },
        {
            "title": "Data Science and Technology",
            "content": "subsets. Leveraging this strategy, the researchers rapidly assess the actual impact of different data subsets on LLM performance from candidate pool, precisely selecting high-quality samples as seed data for lightweight fastText classifier. Compared to LLM-based classifiers, fastText significantly reduces inference overhead while maintaining high selection quality. Experimental results demonstrate that the Ultra-FineWeb dataset, filtered from FineWeb using this method, significantly outperforms the L1-level FineWeb dataset, validating the effectiveness of model-driven selection over rule-based filtering. FineMath (Allal et al., 2025) addresses the limitations of existing mathematical datasets, such as OpenWebMath (Paster et al., 2023) (14.7B tokens) and InfiMM-WebMath (Han et al., 2024) (40B tokens), which suffer from insufficient scale and lack of step-by-step reasoning content. The construction pipeline begins by extracting text from Common Crawl WARC files and using classifier trained on Llama-3.1-70B-Instruct for initial scoring (on 3-point scale) to identify high-quality mathematical domains, and then expanding the URL list to include OWM and InfiMM-WebMath. Subsequently, the OWM pipeline is utilized to re-extract all relevant pages, preserving LaTeX formatting while removing boilerplate content, resulting in 7.1B pages and 6.5T tokens. second classification pass (on 5-point scale) is then applied to specifically filter for pages containing reasoning and educational content ranging from middle school to early college levels. Finally, the process concludes with single-band MinHash LSH deduplication (10 hashes), fastText language classification (retaining only English), and benchmark decontamination. This yields two versions: FineMath-4+ (10B tokens, retaining scores of 4-5) and FineMath-3+ (34B tokens, retaining scores of 3-5). Experiments show that FineMath-4+ achieves 2 performance increase on GSM8K and 6 increase on MATH, significantly outperforming OWM and InfiMM-WebMath. Compared to L1 data, L2-level data achieves significant advancement in information density, professionalism, and task relevance. The practices of Ultra-FineWeb and FineMath demonstrate that model-driven quality distillationwhether implemented via classifiers trained on synthetic annotations, fastText models optimized through efficient validation strategies, or multi-stage scoring for specific domainscan effectively extract target-oriented high-quality data assets from general web content. This process provides more efficient training corpora for both foundational pre-training and continuous pre-training stages."
        },
        {
            "title": "2.2.4 L3: Refined Data",
            "content": "The L3 level is defined as the edited and synthetic data layer, representing an advanced optimization phase built upon the foundation of L1 and L2 data. This level employs editing, restoration, and synthetic enhancement techniques to eliminate semantic flaws and reinforce logical coherence, providing high-order corpora essential for breakthroughs in model capabilities. UltraFineWeb-L3 (UltraData, 2026) implements an editing refinement strategy for general web data. Although L2-level quality classifiers have filtered out high-scoring webpages, noise such as boilerplate text, navigation elements, and formatting inconsistencies remains prevalent due to limitations in parser accuracy. Drawing on the technical approach of Nemotron-CC (Su et al., 2025), this stage treats data processing as semantic distillation rather than simple filtering, aiming to reconstruct the underlying content in its purest form. Specifically, LLMs are employed to refine webpage text by systematically removing non-content elements such as sidebars, headers, footers, and residual advertisements, correcting OCR errors, fixing broken code indentation, and addressing grammatical inconsistencies. This process improves text coherence and readability while strictly preserving the original semantics. Documents failing to meet information density thresholds are discarded, and this \"filtering + editing\" paradigm effectively overcomes the brittleness of heuristic filtering, achieving secondary purification of data quality. UltraData-Math-L3 (UltraData, 2026) implements systematic synthesis pipeline to overcome the scarcity and homogeneity of web-mined mathematical data. The process begins by cleaning seed documents and standardizing them into unified LaTeX format, which removes formatting noise and ensures that the model focuses purely on mathematical logic during synthesis. To maximize model generalization, it employs multi-model ensemble strategy to transform these seeds into five diverse instructional formats: (1) difficultystratified Q&A pairs that follow curriculum-aligned progression from primary school to undergraduate levels, providing clear supervision signals for problem-solving; (2) multi-turn teacher-student dialogues between seven persona pairs to introduce structural complexity and context-maintenance, which are essential for complex reasoning; (3) multi-style rewrites that decouple mathematical core logic from presentation styles such as Wikipedia, blogs, or academic papers, preventing the model from over-fitting to narrow linguistic distributions; (4) knowledge-driven textbook modules that extract theorems and axioms to generate pedagogical explanations and multi-level practice problems; and (5) persona-integrated synthesis that simulates professional educational materials to further enhance the pedagogical value of the data. Finally, all synthetic outputs undergo rigorous"
        },
        {
            "title": "Data Science and Technology",
            "content": "Table 2: Representative open-source tools and datasets for L0L4 tiered data management. Level Open-Source Tools and Datasets L0 L1 L2 Open-Source Tools: Trafilatura (Barbaresi, 2021b), Resiliparse (Bevendorff et al., 2018), MinerU (Wang et al., 2024a), MinerU-HTML (Ma et al., 2025), Nougat (Blecher et al., 2023), Docling (Auer et al., 2024), Mathpix6, Magic-HTML7, Marker8, olmOCR (Poznanski et al., 2025) Open-Source Datasets: Web: Common Crawl, Mathoverflow PDF: Papers, Ebooks Code: GitHub, Stackoverflow Open-Source Tools: MinHash (Broder, 2000), DataTrove (Penedo et al., 2024b), Duplodocus (Olmo et al., 2025), Semdedup (Abbas et al., 2023), CCNet (Wenzek et al., 2020) Open-Source Datasets: Web: C4 (Raffel et al., 2020), DCLM-pool (Li et al., 2024b), FineWeb (Penedo et al., 2024c), FinePDFs (Kydlíˇcek et al., 2025) RefinedWeb (Penedo et al., 2023), RedPajama-V2 (Weber et al., 2024), RedPajama-V1 (Weber et al., 2024), Dolma (Soldaini et al., 2024a), WanJuan (He et al., 2023), MiChao-HuaFen 1.0 (Liu et al., 2023b), CulturaX (Nguyen et al., 2024), Txt360 (Tang et al., 2024), OSCAR 22.01 (Abadji et al., 2022), SlimPajama (Shen et al., 2023), CCAligned (El-Kishky et al., 2020), wudao (Yuan et al., 2021), SkyPile-150B (Wei et al., 2023a) Code: The Stack v2 (Lozhkov et al., 2024), The Stack v1 (Kocetkov et al., 2022), StarCoder (Li et al., 2023b), CommitPack (Muennighoff et al., 2023), RefineCode (Huang et al., 2025) Math: Proof-pile2 (Azerbayev et al., 2023), AlgebraicStack (Azerbayev et al., 2023) Open-Source Tools: Fasttext (Joulin et al., 2017), Data-Juicer (Chen et al., 2024), Dolma toolkit (Soldaini et al., 2024a), WebOrganizer (Wettig et al., 2025) Open-Source Datasets: Web: DCLM-baseline (Li et al., 2024b), FineWeb-Edu (Penedo et al., 2024c), FinePDFs-Edu (Kydlíˇcek et al., 2025), Ultra-FineWeb (Wang et al., 2025b), IndustryCorpus2 (Shi et al., 2024), TeleChatPTD (Wang et al., 2024f), CCI3 (Wang et al., 2024b), ChineseWebText (Chen et al., 2023a), ChineseWebText2.0 (Zhang et al., 2024), Dolma 3 Mix (Olmo et al., 2025), Chinese FineWeb-edu (Yu et al., 2025) Code: Stack-Edu (Allal et al., 2025), SWE-Gym (Pan et al., 2024), CodeContests (Li et al., 2022), Apps(Hendrycks et al., 2021a) Math: OpenWebMath (Paster et al., 2023), FineMath (Allal et al., 2025), Megamath-Web (Zhou et al., 2025a), MegaMath-Code (Zhou et al., 2025a), InfiMM-WebMath Han et al. (2024), MathPile Wang et al. (2024d) Open-Source Tools: ProX (Zhou et al., 2024), MoDS (Du et al., 2023), Self-Instruct (Wang et al., 2023), Evol-Instruct (Xu et al., 2023), OSS-Instruct (Wei et al., 2023b), RLVE (Zeng et al., 2025) Open-Source Datasets: Text for pre-training: Nemotron-CC and Nemotron-CC-HQ (Su et al., 2025), Nemotron-CC-Math3+/4+ (Mahabadi et al., 2025), MegaMath-Web-Pro (Zhou et al., 2025a), MegaMath-Synth (Zhou et al., 2025a), Dolma 3 Dolmino/Longmino Mix and Dolci (Olmo et al., 2025) QA for post-training: DEITA (Liu et al., 2023a), LIMA (Zhou et al., 2023), Magpie (Xu et al., 2024b), UltraFeedBack (Cui et al., 2024), MAmmoTH2 (Yue et al., 2024b), OpenCodeReasoning (Ahmad et al., 2025b), OpenThoughts (Guha et al., 2025), SAND-Math (Manem et al., 2025), SmolTalk (Allal et al., 2025), Magicoder-OSS-Instruct (Wei et al., 2023b), Nemotron-Math-v2 (Du et al., 2025), LIMO (Ye et al., 2025), LIMO-V2 (Ye et al., 2025), AugGSM8K and AugMATH (Li et al., 2024a), NuminaMath (Li et al., 2024c), Big-Math (Albalak et al., 2025), OpenMathInstruct-2 (Toshniwal et al., 2024), Ultrachat (Ding et al., 2023), DISC-Law-SFT (Yue et al., 2023), SynthLaw (Yue et al., 2025), FinQA (Chen et al., 2021), FinCoT (Qian et al., 2025), ConvFinQA (Chen et al., 2022), SMARTs Trajectory Dataset (Yue et al., 2024a), COIG (Zhang et al., 2023), OpenCodeInstruct (Ahmad et al., 2025a), WizardCoder (Luo et al., 2023), KodCode (Xu et al., 2025), CodeActInstruct (Wang et al., 2024c), SciLitIns (Li et al., 2024d), MegaScience (Fan et al., 2025), Skywork-OR1-RL-Data (He et al., 2025) Open-Source Tools: LangChain, LlamaIndex, Haystack, RAGatouille, EmbedChain Open-Source Datasets: Wikidata (Vrandeˇcic & Krötzsch, 2014), DBpedia, UltraData-arXiv. 6 https://mathpix.com/ 8 https://github.com/datalab-to/marker 7 https://github.com/opendatalab/magic-html"
        },
        {
            "title": "Data Science and Technology",
            "content": "filtering for LaTeX syntax errors and logical incompleteness, ensuring that the final corpus maintains the high information density required for large-scale pre-training. L3-level data exhibits strong adaptability, supporting multiple critical stages including mid-training, supervised fine-tuning (SFT), and reinforcement learning (RL), significantly enhancing core model capabilities such as logical reasoning, mathematical proficiency, and instruction following. Compared to L2 data, L3 data not only has higher information density but also achieves creative value augmentation through editing and synthesis, breaking through the inherent constraints of raw data distribution in terms of scale, quality, and diversity."
        },
        {
            "title": "2.2.5 L4: Organized Data",
            "content": "The L4 level is defined as the organized data layer, representing the most refined state of data within the management framework. While previous levels focus on the linguistic and semantic quality of continuous text, the L4 level emphasizes unified orchestration and rigorous normalization to transform fragmented information into structured, reliable, and searchable knowledge assets. This layer serves as the authoritative source of truth for models, providing the high-fidelity substrate necessary for knowledge-intensive tasks. The management of L4 data centers on two core operations: data orchestration and fact verification. Through orchestration, scattered data from diverse sources are unified under coherent thematic frameworks and interconnected knowledge structures. Simultaneously, fact verification ensures the integrity of the information by cross-referencing entries with trusted sources, thereby eliminating the factual inconsistencies often found in raw web corpora. Representative examples of L4 data include Wikidata (Vrandeˇcic & Krötzsch, 2014) and UltraData-arXiv. Wikidata provides highly structured, multilingual knowledge base that facilitates precise entity-relation querying. Similarly, Ultra-arXiv represents sophisticated reorganization of scholarly literature, where complex elements such as mathematical formulas, citations, and experimental results are standardized into searchable and interconnected format. The defining characteristics of the L4 level are its explicit structural rigor and exceptional credibility. These attributes make L4 data indispensable for advanced applications like retrieval-augmented generation (RAG). By enabling efficient and accurate retrieval, L4 data provides robust defense against model hallucinations and ensures the factual precision required for expert-level reasoning and decision-making. By establishing the fine-grained, hierarchical L0L4 management framework, data management undergoes paradigm shift from extensive accumulation to precise empowerment. This framework underpins progressive training strategy, enabling models to be precisely matched with data resources of incrementally improving quality at different stages of evolution. L0 data generally serves as archival reserve material and does not participate in actual model training. L1 data, processed through general cleaning rules and customized operators, can support large-scale pre-training. L2 data, filtered via classification models to possess high information density and meet diverse domain and quality requirements, is suitable for the Decay and MidTraining stages. Building upon this, L3 data, further refined through rewriting and synthesis to resolve semantic imperfections, provides high quality corpus for leaps in model logical reasoning capabilities, applicable across MidTraining, SFT, and RL stages. Meanwhile, L4 (Organized Data), through standardized verification and data orchestration, constructs trustworthy knowledge index that can directly serve downstream applications such as RAG. This hierarchical framework effectively decouples the complex challenges of data management, avoiding the inefficiency of traditional \"one-size-fits-all\" approach and enabling more rigorous quality control. Its structured framework provides modular resource library for downstream adaptation, allowing researchers to perform domain-targeted sampling from the L1 pool based on specific needs, or extract high-quality seed samples from L2 to guide the evolutionary synthesis of dedicated L3 corpora. Complemented by the trusted knowledge index constructed at the L4 level, this framework transforms data quality from vague, experience-based assessments into predictable engineering metrics. Consequently, building upon traceable data lineage, it drives model capabilities toward deterministic advancement, shifting from an experience-driven to value-driven paradigm."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we present comprehensive evaluation of our proposed tiered data management framework. We first detail the experimental setting in Section 3.1, including model configuration, various verification strategies (pre-training, efficient, and decay verification), and evaluation benchmarks. In Section 3.2, we perform granular quality analysis across four major corpora: English Web, Chinese Web, Math, and Code. Our results demonstrate that data quality improves progressively from L1 to L3, validating the effectiveness of the tiered"
        },
        {
            "title": "Data Science and Technology",
            "content": "management in enhancing corpus quality and purity. Subsequently, Section 3.3 provides specialized case study on mathematics to systematically validate the L1L3 framework. Finally, Section 3.4 presents case study on tiered management for model training. By exploring the deployment of different data tiers across various training stages, we reveal that higher-level data becomes increasingly critical as training progresses."
        },
        {
            "title": "3.1 Experimental Setting",
            "content": "Model Configuration. In our experiments, all models are trained via the Megatron-LM library (Shoeybi et al., 2019). We utilize the MiniCPM-1.2B model architecture with the MiniCPM3-4B tokenizer. Table 3 provides the detailed configurations, where Params., Vocab., dm, df , dh, nhead, nkv, and nLayer represent the total number of non-embedding parameters, vocabulary size, model hidden dimension, feedforward layer bottleneck dimension, attention head dimension, number of queries, number of key/values, and the number of layers, respectively. Table 3: Model configurations for the MiniCPM-1.2B. Name Params. Vocab. dm df MiniCPM-1.2B 1,247,442,432 73448 1,536 3,840 dh 64 nhead nkv nLayer 24 52 Efficient Verification. To efficiently evaluate data quality, we conduct two-stage annealing process on 10B token budget. We first train base model from scratch on 1.1T tokens using the MiniCPM-3-4B corpus. The training employs Warmup-Stable-Decay (WSD) scheduler, comprising 1T-token stable stage and 0.1T-token decay stage. Building upon this base, we perform annealing with data mixture composed of 30% verification data and 70% of the default distribution. Key training parameters include sequence length of 4096, weight decay of 0.1, and gradient clipping threshold of 1.0. We employ global batch size of 512 (micro batch size of 16). This configuration yields total of 10.5B tokens, calculated as SeqLen GBS TrainStep = 4096 512 5000 = 10.5B; for simplicity, we refer to it as 10B. The optimization for this stage employs an exponential decay schedule with 500-step warm-up, scaling the learning rate from peak of 1 103 to minimum of 5 105. To enhance training stability, we use Maximal Update Parameterization (µP) (Yang et al., 2022) across all experimental setups. Pre-train Verification. We perform pre-training verification on approximately 120B tokens to balance validation comprehensiveness with computational efficiency. Each verification comprises 15,000 steps with sequence length of 4,096 and global batch size of 2,048 (implemented with micro-batch size of 16). This configuration yields total of 125.8B tokens, calculated as SeqLen GBS TrainStep = 4096 2048 15000 = 125.8B tokens; for simplicity, we refer to it as 120B. The optimization process employs cosine learning rate decay schedule with 1,000-step warm-up phase. The learning rate initiates at 1 105, reaches peak of 1 102, and subsequently decays to 5 104. Additional hyperparameters include weight decay of 0.1 and gradient clipping threshold of 1.0. Similarly, µP is employed to improve the stability of the training process. Decay Verification. To evaluate data performance during the final pre-train phase, we conduct decay verification using base model trained on 1.3T tokens from the MiniCPM-4 corpus, having completed both warmup and stable stages. For this verification, we employ data mixture composed of 30% new verification data and 70% of the default distribution. The process involves 20,000 steps with sequence length of 4,096 and global batch size of 1,280 (micro-batch size of 10), yielding approximately 104.9B tokens (4096 1280 20000), which we refer to as 100B for brevity. We utilize an exponential decay schedule that scales the learning rate from the stable stages 7.5 104 down to minimum of 3.75 105. Other training hyperparameters remain consistent with our standard settings. Although efficient verification enables higher iteration efficiency with significantly lower resource requirements, the limited training budget may introduce higher variance in results. Decay verification addresses this by employing full-scale decay phase, providing more robust and definitive evaluation that closely reflects the final pre-training performance. Benchmarks. We adopt OpenCompass (Contributors, 2023) as our evaluation framework. The specific benchmarks, evaluation settings, and inference methods are detailed as follows: General English datasets: We evaluate our models on standard English benchmarks including MMLU (5-shot, PPL) (Hendrycks et al., 2020), ARC-C (0-shot, PPL) (Clark et al., 2018), ARC-E (0-shot, PPL) (Clark et al., 2018), BigBench Hard (BBH) (3-shot, Gen) (Suzgun et al., 2022), CommonSenseQA (8-shot, PPL) (Talmor et al., 2018), HellaSwag (0-shot, PPL) (Zellers et al., 2019), OpenbookQA (0-shot, PPL) (Mihaylov et al.,"
        },
        {
            "title": "Data Science and Technology",
            "content": "Table 4: Detailed data selection for L1, L2, and L3 across multiple domains. Domain L1 Filtered Data L2 Selected Data L3 Refined Data Web-en Web-zh Math Code FineWeb (Penedo et al., 2024a) Ultra-FineWeb-en (Wang et al., 2025b) Ultra-FineWeb-en-L3 Chinese FineWeb (Yu et al., 2025) Ultra-FineWeb-zh (Wang et al., 2025b) Ultra-FineWeb-zh-L3 UD-Math-L1 (UltraData, 2026) Stack-v2 (Lozhkov et al., 2024) UD-Math-L2 (UltraData, 2026) Stack-Edu (Allal et al., 2025) UD-Math-L3 (UltraData, 2026) Code Textbook 2018) (0-shot, PPL), PIQA (0-shot, PPL) (Bisk et al., 2020) (0-shot, PPL), SIQA (0-shot, PPL) (Sap et al., 2019) (0-shot, PPL), and Winogrande (0-shot, Loglikelihood) (Sakaguchi et al., 2021). General Chinese datasets: We evaluate our models on Chinese knowledge-intensive benchmarks, including C-Eval (5-shot, PPL) (Huang et al., 2023) and CMMLU (5-shot, PPL) (Li et al., 2023a). Math reasoning datasets: We evaluate our models on mathematics reasoning benchmarks, including MATH500 (4-shot, Gen) (Hendrycks et al., 2021b) and GSM8K (Cobbe et al., 2021) (4-shot, Gen). Code reasoning datasets: We evaluate our models on code reasoning benchmarks, including MBPP (3-shot, Gen) (Austin et al., 2021) and HumanEval (0-shot, Gen) (Chen, 2021)."
        },
        {
            "title": "3.2 Data Analysis",
            "content": "We first conduct an efficient verification method to confirm that the tiered data management framework produces meaningful quality differentiation. Specifically, we evaluate the data quality across the L1, L2, and L3 tiers for English web, Chinese web, Math, and Code domains. For each domain, the tiered composition is defined as follows. (1) In the English web domain, we select FineWeb (Penedo et al., 2024a) as L1 data, Ultra-FineWeb-en (Wang et al., 2025b) as L2, and Ultra-FineWeb-en-L3, synthetic dataset generated based on Ultra-FineWeb-en as L3. Following prior synthetic data construction practices (e.g., Nemotron-CC (Su et al., 2025)), the L3 data comprises five types of synthesized content spanning diverse supervision signals (i.e., diverse QA, distill, extract knowledge, knowledge list, and wiki style). (2) Similarly, for the Chinese web domain, Chinese FineWeb (source data from Chinese FineWeb-edu-v2 (Yu et al., 2025)) is defined as L1, Ultra-FineWeb-zh (Wang et al., 2025b) as L2, and Ultra-FineWeb-zh-L3, which is constructed via multi-type synthetic data generation on top of Ultra-FineWeb-zh as L3. (3) For the math domain, we uniformly adopt the UltraData-Math (UltraData, 2026) for data construction and management across all tiers. Specifically, Math-L1 consists of raw mathematical text parsed from web data using the UltraData-Math Parser and filtered with rule-based filter operators. Math-L2 is obtained by further selecting high-quality samples using the UltraData-Math Classifier, while Math-L3 is composed of synthetic mathematical data generated by the UltraData-Math Generator. (4) In the code domain, we select Stack-v2 (Lozhkov et al., 2024) as Code-L1 and Stack-Edu (Allal et al., 2025) as Code-L2, while Code-L3 is derived from Code-L2 through textbook-style rewriting, including code explanations and programming exercises. We apply the same efficient verification strategy across all domains and tiers to ensure fair and comparable quality assessment. The experimental results are shown in Table 5, demonstrating clear and consistent performance improvement from L1 to L3 across domains, validating that the proposed tiered data management framework effectively captures meaningful quality stratification. Across all four domains, downstream performance improves steadily from L1 to L3, demonstrating that data quality increases with each data tier. Specifically, average benchmark scores rise from 52.26 percentage points (pp) to 53.96pp in English (+1.70 pp), 49.44pp to 51.48pp in Chinese (+2.04 pp), 23.78pp to 30.84pp in Math (+7.06 pp), and 34.49pp to 36.28pp in Code (+1.79 pp). The strict performance hierarchy of L3 > L2 > L1 holds universally without exception. These results empirically validate the effectiveness of our tiered data management framework in producing high-quality data, which yields clear performance signals even under verification with constrained training budgets."
        },
        {
            "title": "3.3 Case Study on UltraData-Math",
            "content": "To further examine whether quality improvements in single domain can translate into broad downstream gains, we scale up to 100B tokens using the UltraData-Math corpus. This scaling experiment aims to verify whether the quality advantages of tiered data are scalable, ensuring sustained performance gains without premature saturation as the training volume increases. Specifically, we utilize the decay verification method to verify whether the quality-driven dividends observed during small-scale verification persist and scale linearly as the training trajectory extends. By training models separately on UltraData-Math-L1, UltraData-Math-L2,"
        },
        {
            "title": "Data Science and Technology",
            "content": "Table 5: Comparison of models trained with L1, L2, and L3 datasets across English web, Chinese web, math, and code domains. Web-En Data MMLU ARC-E ARC-C BBH CSQA Hella. OBQA PIQA SIQA Wino. Avg. English L1 L2 L3 46.88 46.73 47.25 Web-Zh Data 61.38 59.79 59.08 Chinese CMMLU C-Eval Avg. 37.63 39.32 37.97 35.36 34.94 34.51 57.82 57.99 58.56 56.87 56.72 57. 56.00 66.20 72.40 72.85 73.29 73.18 42.94 43.30 43.40 54.85 55.33 56.12 52.26 53.36 53.96 Math Data Math MATH500 GSM8K Avg. Code Data Code MBPP HumanEval Avg. L1 L2 L3 49.37 50.72 51.74 49.51 50.59 51.22 49.44 50.66 51.48 L1 L2 14.80 15.60 20.20 32.75 34.04 41.47 23.78 24.82 30.84 L1 L2 L3 43.97 44.36 45.73 25.00 26.22 26. 34.49 35.29 36.28 and UltraData-Math-L3 data, we evaluate their performance across diverse suite of benchmarks spanning mathematical reasoning, English and Chinese understanding, and code generation. Furthermore, the benefits of Math-L3 extend beyond mathematical reasoning and translate into consistent improvements across non-mathematical domains. On English benchmarks, Math-L3 improves the average score from 51.32pp and 51.69pp to 54.77pp, yielding gains of 3.45pp and 3.08pp over Math-L1 and Math-L2, respectively. Notably, substantial improvements are observed on reasoning-intensive tasks such as ARC-E (+5.29pp), ARC-C (+1.69pp), BBH (+5.87pp), and OpenbookQA (+15.40pp), indicating enhanced general reasoning and problem-solving capabilities induced by higher-quality math data. Similar trends are observed in the Chinese domain, where Math-L3 achieves an average score of 53.48pp, outperforming Math-L1 and Math-L2 by 1.89pp and 2.64pp, respectively, with consistent gains on both CMMLU and C-Eval. In the code domain, Math-L3 also demonstrates clear advantages, improving the average performance to 41.10pp, compared to 37.30pp and 38.41pp for Math-L1 and Math-L2. This improvement is reflected across both MBPP (+4.56pp over Math-L1) and HumanEval (+3.05pp over Math-L1), suggesting that high-quality mathematical data can enhance structured reasoning and abstraction skills beneficial to code generation tasks. Overall, these results demonstrate that progressively improving data quality within single domain can yield significant and transferable benefits across diverse evaluation domains. These cross-domain improvements underscore that high-quality mathematical data is fundamental driver of enhancing models general logical consistency and problem-solving capabilities across diverse languages and tasks."
        },
        {
            "title": "3.4 Tiered Data Management for Multi-Stage Training",
            "content": "To validate the effectiveness of the tiered data management framework, we design comparative experiment to evaluate the performance of mix training and tiered training strategies under the same training setting. Both training strategies adhere to consistent domain distribution consisting of 50% Web-en, 25% Web-zh, Table 6: Comparison of results across Math data quality levels (L1L3) on all benchmarks."
        },
        {
            "title": "MMLU",
            "content": "ARC-E ARC-C"
        },
        {
            "title": "CSQA",
            "content": "Hella."
        },
        {
            "title": "PIQA",
            "content": "SIQA Wino. Avg. Math-L1 50.57 Math-L2 50.93 Math-L3 51."
        },
        {
            "title": "Method",
            "content": "54.50 55.20 59."
        },
        {
            "title": "Chinese",
            "content": "37.29 36.95 38.98 37.75 39.27 43.62 60.44 60.20 61."
        },
        {
            "title": "Math",
            "content": "58.02 57.52 58.27 41.60 39.80 57.00 74.21 74.48 74.76 41.71 44.73 43.35 57.14 57.77 59.04 51.32 51.69 54."
        },
        {
            "title": "Code",
            "content": "All Avg. CMMLU C-Eval Avg. MATH500 GSM8K Avg. MBPP HumanEval Avg. Math-L1 51.28 Math-L2 51.13 Math-L3 52. 51.89 50.55 54.08 51.59 50.84 53.48 27.78 29.20 37.02 54.66 52.92 61.79 41.22 41.06 49.41 44.71 44.50 49. 29.88 32.32 32.93 37.30 38.41 41.10 48.39 48.59 58.27"
        },
        {
            "title": "Data Science and Technology",
            "content": "Table 7: Comparison of mix training and tiered training results across all benchmarks. Method"
        },
        {
            "title": "English",
            "content": "MMLU ARC-E ARC-C BBH CSQA Hella. OBQA PIQA SIQA Wino. Avg. Mix Tiered 28.26 29. 48.32 50.09 26.78 31.53 26.20 28.37 46.11 46.27 46.89 45.21 26.00 29. 71.44 70.62 39.76 39.20 54.30 53.43 41.41 42.29 Method Chinese Math Code All Avg. CMMLU C-Eval Avg. MATH500 GSM8K Avg. MBPP HumanEval Avg. Mix Tiered 25.47 26.71 23.97 28.37 24.72 27.54 1.60 4.20 3.11 5. 2.36 4.60 12.06 16.34 2.44 3.05 7.25 9.70 30.17 31.66 8% Math, and 17% Code. The mix training strategy utilizes single-stage approach, mixing 120B tokens with an equal 1:1:1 ratio of L1, L2, and L3 data into unified pool. In contrast, the tiered training strategy partitions the same 120B tokens into three consecutive 40B token stages (effectively equivalent to 1:1:1 ratio), transitioning from L1 to L2 and finally to L3 data. Both strategies employ the MiniCPM-1.2B model trained from scratch and are evaluated on multiple benchmarks across four major domains: English, Chinese, Math, and Code. Table 7 presents detailed comparative results of the two strategies across evaluation benchmarks. Tiered training achieves an improvement of 1.49pp in overall average performance compared to mix training (31.66pp vs 30.17pp), with significant gains across all four major evaluation domains. Specifically, the English domain shows an average improvement of 0.88pp, with reasoning-intensive tasks such as ARC-C, ARC-E, OpenbookQA, and BBH improving by 4.75pp, 1.77pp, 3.00pp, and 2.17pp respectively, and knowledge-intensive tasks such as MMLU improving by 0.89pp; the Chinese domain shows an average improvement of 2.82pp, with C-Eval and CMMLU improving by 4.40pp and 1.24pp respectively; the mathematics domain shows an average improvement of 2.24pp, with MATH500 and GSM8K improving by 2.60pp and 1.89pp respectively; the code domain shows an average improvement of 2.45pp, with MBPP and HumanEval improving by 4.28pp and 0.61pp respectively. In-depth analysis reveals that tiered training demonstrates more significant improvements on reasoning-intensive tasks (ARC-C, BBH, OpenbookQA) and knowledge-intensive tasks (MMLU, C-Eval, CMMLU), benefiting from the model-driven selection mechanism of the L2 that effectively identifies and retains high information density samples, while the editing and synthesis techniques of the L3 further enhance the logical coherence of data. The substantial improvements in the math and code domains validate the effectiveness of domain-specific management strategies: through L2s domain classifier selection and L3s editing enhancement, the model can more fully absorb domain knowledge. It is worth noting that tiered training shows slight decreases on few tasks (HellaSwag, PIQA, SIQA, Winogrande), which focus more on the breadth rather than depth of common sense reasoning and language understanding, potentially benefiting from the larger scale L1 data coverage in mix training. Figure 3: Comparison of average score between mix training and tiered training at each checkpoint. The tiered training strategy (40B tokens per stage) consistently outperforms the mix training baseline across most evaluation intervals. Figure 3 further reveals the advantages of tiered management from the dynamic perspective of the training process. In the early training stage, both strategies exhibit similar growth trends, with performance improving from approximately 24.7pp to around 28.3pp. At this stage, tiered training primarily uses L1 data, with data distribution relatively close to mix training. In the latter stages of training, the tiered training strategy gradually introduces high-quality data from L2 and L3, and the performance curve demonstrates sustained and stable growth trend, improving from 28.35pp to 31.66pp, an increase of 3.31pp. In contrast, the growth trend of mix training significantly slows down, improving from 28.26pp to 30.17pp, an increase of only approximately"
        },
        {
            "title": "Data Science and Technology",
            "content": "1.91pp. This significant difference fully embodies the core advantage of tiered governance: by introducing high-quality data that has undergone L2-model-driven selection and L3-editing and synthesis optimization in the later training stages, the model can continuously and efficiently learn complex knowledge and reasoning capabilities, avoiding the learning efficiency decline caused by low-quality data interference in mix training. Combining results on benchmarks and training curves, this experiment fully validates the effectiveness of the tiered data management framework. The tiered training strategy not only comprehensively surpasses mix training in final performance (overall average improvement of 1.49pp), but more importantly, demonstrates sustained and stable trend of learning capability improvement in the later training stages, with growth magnitude reaching 1.7 times that of the mix training strategy. This advantage stems from the core mechanism of tiered management: by introducing data of corresponding quality levels at different training stages, achieving precise alignment between data value and model learning demands. L1 data provides broad foundation of language representation in the early training stage, L2 data enhances the learning of high information density content in the mid-stage, and L3 data deepens the absorption of logical reasoning and domain knowledge in the final stage, thereby effectively avoiding the interference of low-quality data in the mix training strategy on the learning of advanced capabilities. The experimental results demonstrate that organizing 120B tokens of training data into tiered stages according to L1, L2, and L3 can more effectively enhance the models comprehensive performance across multiple dimensions, such as knowledge understanding, logical reasoning, and domain capabilities, providing solid empirical support for the stepwise optimization of data management."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we revisit the development of artificial intelligence through the lens of data organization and utilization, and argue that the dominant paradigm of data-driven learning is approaching fundamental sustainability limits. As model capabilities continue to advance, further progress can no longer rely solely on expanding data scale, but instead requires systematic rethinking of how data is managed, valued, and deployed throughout the training lifecycle. To this end, we propose data-model co-evolution perspective, in which models actively guide data management decisions while high-quality data, in turn, amplifies model capability in positive feedback loop. Under this paradigm, we introduce L0L4 tiered data management framework that structures data from raw resources to organized and verifiable knowledge. By explicitly aligning data quality, management cost, and training objectives across different learning stages, the proposed framework provides principled and scalable foundation for sustainable LLM data management. Through empirical studies on math and web data, we demonstrate that tier-aware data utilization can significantly improve training efficiency and model performance, validating the practical value of tiered data management beyond isolated data processing techniques. Our results suggest that effective data management should be treated as first-class engineering problem, rather than an auxiliary preprocessing step. Our future work will focus on deepening and operationalizing the data-model co-evolution paradigm. Specifically, we plan to develop more rigorous methods for scientific data value assessment, enabling models to quantitatively estimate the marginal utility of data across tiers and training stages. We will further explore dynamic datamodel feedback mechanisms, where model signals continuously inform data selection, refinement, and allocation during training. In addition, we aim to extend the tiered data management framework to broader modalities and application domains, and to integrate it more tightly with large-scale training systems. Through these efforts, we seek to establish data management as core, adaptive component of next-generation AI systems."
        },
        {
            "title": "References",
            "content": "Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. Towards cleaner document-oriented In Proceedings of the Thirteenth Language Resources and Evaluation multilingual crawled corpus. Conference, pp. 43444355, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.463. Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023. Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Wasi Uddin Ahmad, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Vahid Noroozi, Somshubra Majumdar, and Boris Ginsburg. Opencodeinstruct: large-scale instruction tuning dataset for code llms. arXiv preprint arXiv:2504.04030, 2025a. Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025b. Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, and Nick Haber. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models, 2025. URL https://arxiv.org/abs/2502.17387. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Nikolaos Livathinos, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Fabian Lindlbauer, Kasper Dinkla, et al. Docling technical report. arXiv preprint arXiv:2408.09869, 2024. Luis Augusto. From symbols to knowledge systems: A. newell and ha simons contribution to symbolic ai. 2021. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. 2025. URL https://arxiv.org/abs/2511.21631. Adrien Barbaresi. Trafilatura: web scraping library and command-line tool for text discovery and extraction. In Heng Ji, Jong C. Park, and Rui Xia (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 122131, Online, August 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-demo.15. URL https://aclanthology.org/2021.acl-demo.15/."
        },
        {
            "title": "Data Science and Technology",
            "content": "Adrien Barbaresi. Trafilatura: web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 122131, 2021b. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):17981828, 2013. Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi, and Benjamin Piwowarski (eds.), Advances in Information Retrieval. 40th European Conference on IR Research (ECIR 2018), Lecture Notes in Computer Science, Berlin Heidelberg New York, March 2018. Springer. Baolong Bi, Shenghua Liu, Xingzhang Ren, Dayiheng Liu, Junyang Lin, Yiwei Wang, Lingrui Mei, Junfeng Fang, Jiafeng Guo, and Xueqi Cheng. Refinex: Learning to refine pre-training data at scale from expertguided programs. arXiv preprint arXiv:2507.03253, 2025. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418, 2023. Andrei Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pp. 2129. IEEE, 1997. Andrei Broder. Identifying and filtering near-duplicate documents. In Annual symposium on combinatorial pattern matching, pp. 110. Springer, 2000. Bruce Buchanan and Edward Feigenbaum. Dendral and meta-dendral: Their applications dimension. In Readings in artificial intelligence, pp. 313322. Elsevier, 1981. Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, et al. Data-juicer: one-stop data processing system for large language models. In Companion of the 2024 International Conference on Management of Data, pp. 120134, 2024. Jianghao Chen, Pu Jian, Tengxiao Xi, Dongyi Yi, Qianlong Du, Chenglin Ding, Guibo Zhu, Chengqing Zong, Jinqiao Wang, and Jiajun Zhang. ChineseWebText: Large-scale high-quality chinese web text extracted with effective evaluation model. arXiv preprint arXiv:2311.01149, 2023a. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023b. Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al. Finqa: dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 36973711, 2021. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. arXiv preprint arXiv:2210.03849, 2022. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021."
        },
        {
            "title": "Data Science and Technology",
            "content": "Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre FT Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia Morgado, et al. Saullm-7b: pioneering large language model for law. arXiv preprint arXiv:2403.03883, 2024. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https: //github.com/open-compass/opencompass, 2023. Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273297, 1995. Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. arXiv preprint arXiv:2310.01377, 2023. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. In Proceedings of the 41st International Conference on Machine Learning, 2024. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. Qianlong Du, Chengqing Zong, and Jiajun Zhang. Mods: Model-oriented data selection for instruction tuning. arXiv preprint arXiv:2311.15653, 2023. Wei Du, Shubham Toshniwal, Branislav Kisacanin, Sadegh Mahdavi, Ivan Moshkov, George Armstrong, Stephen Ge, Edgar Minasyan, Feng Chen, and Igor Gitman. Nemotron-math: Efficient long-context distillation of mathematical reasoning from multi-mode supervision. arXiv preprint arXiv:2512.15489, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzmán, and Philipp Koehn. Ccaligned: massive collection of cross-lingual web-document pairs. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 59605969, 2020. Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023. Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024. Run-Ze Fan, Zengzhi Wang, and Pengfei Liu. Megascience: Pushing the frontiers of post-training datasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025. Zeyu Gan, Ruifeng Ren, Wei Yao, Xiaolin Hu, Gengze Xu, Chen Qian, Huayi Tang, Zixuan Gong, Xinhao Yao, Pengwei Tang, et al. Beyond the black box: Theory and mechanism of large language models. arXiv preprint arXiv:2601.02907, 2026. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023."
        },
        {
            "title": "Data Science and Technology",
            "content": "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. arXiv preprint arXiv:2409.12568, 2024. Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models. arXiv preprint arXiv:2308.10755, 2023. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of NeurIPS: Datasets and Benchmarks Track, 2021b. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. MiniCPM: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Siming Huang, Tianhao Cheng, Jason Klein Liu, Weidi Xu, Jiaran Hao, Liuyihan Song, Yang Xu, Jian Yang, Jiaheng Liu, Chenchen Zhang, et al. Opencoder: The open cookbook for top-tier code large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3316733193, 2025. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomáš Mikolov. Bag of tricks for efficient text In Proceedings of the 15th conference of the European chapter of the association for classification. computational linguistics: volume 2, short papers, pp. 427431, 2017. Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. survey of reinforcement learning from human feedback. 2024. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. Hynek Kydlíˇcek, Guilherme Penedo, and Leandro von Werra. Finepdfs. https://huggingface.co/datasets/ HuggingFaceFW/finepdfs_edu, 2025. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436444, 2015."
        },
        {
            "title": "Data Science and Technology",
            "content": "Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. Mugglemath: Assessing the impact of query and response augmentation on math reasoning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1023010258, 2024a. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023a. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024b. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024c. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b. Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, and Hengxing Cai. Scilitllm: How to adapt llms for scientific literature understanding. arXiv preprint arXiv:2408.15545, 2024d. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023c. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Mengjie Liu, Jiahui Peng, Pei Chu, Jiantao Qiu, Ren Ma, He Zhu, Rui Min, Lindong Lu, Wenchang Ning, Linfeng Hou, Kaiwen Liu, Yuan Qu, Zhenxiang Li, Chao Xu, Zhongying Tu, Wentao Zhang, and Conghui He. Dripper: Token-efficient main html extraction with lightweight lm, 2025. URL https://arxiv.org/ abs/2511.23119. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685, 2023a. Yidong Liu, FuKai Shang, Fang Wang, Rui Xu, Jun Wang, Wei Li, Yao Li, and Conghui He. Michaohuafen 1.0: specialized pre-trained corpus dataset for domain-specific large models. arXiv preprint arXiv:2309.13079, 2023b. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. Ren Ma, Jiantao Qiu, Chao Xu, Pei Chu, Kaiwen Liu, Pengli Ren, Yuan Qu, Jiahui Peng, Linfeng Hou, Mengjie Liu, et al. Aicc: Parse html finer, make models bettera 7.3 ai-ready corpus built by model-based html parser. arXiv preprint arXiv:2511.16397, 2025."
        },
        {
            "title": "Data Science and Technology",
            "content": "Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc-math: 133 billion-token-scale high quality math pretraining dataset. arXiv preprint arXiv:2508.15096, 2025. Chaitanya Manem, Pratik Prabhanjan Brahma, Prakamya Mishra, Zicheng Liu, and Emad Barsoum. Sandmath: Using llms to generate novel, difficult and useful mathematics questions and answers. arXiv preprint arXiv:2507.20527, 2025. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Kaixiang Mo, Yuxin Shi, Weiwei Weng, Zhiqiang Zhou, Shuman Liu, Haibo Zhang, and Anxiang Zeng. Mid-training of large language models: survey. arXiv preprint arXiv:2510.06826, 2025. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. In NeurIPS 2023 workshop on instruction tuning and instruction following, 2023. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 42264237, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.377. Team Olmo, Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, et al. Olmo 3. arXiv preprint arXiv:2512.13961, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024a. Guilherme Penedo, Hynek Kydlíˇcek, Alessandro Cappelli, Mario Sasko, and Thomas Wolf. Datatrove: large scale data processing, 2024b. URL https://github.com/huggingface/datatrove. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024c. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025. Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Han Yi, Yilun Zhao, Jimin Huang, Qianqian Xie, and Jian-yun Nie. Fino1: On the transferability of reasoning-enhanced llms and reinforcement learning to finance. arXiv preprint arXiv:2502.08127, 2025."
        },
        {
            "title": "Data Science and Technology",
            "content": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, et al. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21(140):167, 2020. David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representations by back-propagating errors. nature, 323(6088):533536, 1986. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions, 2019. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, et al. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818, 2023. Xiaofeng Shi, Lulu Zhao, Hua Zhou, and Donglin Hao. Industrycorpus2, 2024. URL https://huggingface. co/datasets/BAAI/IndustryCorpus2. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Edward Shortliffe. Computer-based medical consultations: MYCIN, volume 2. Elsevier, 2012. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024a. URL https://arxiv.org/abs/2402.00159. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: long papers), pp. 1572515788, 2024b. Demin Song, Honglin Guo, Yunhua Zhou, Shuhao Xing, Yudong Wang, Zifan Song, Wenwei Zhang, Qipeng Guo, Hang Yan, Xipeng Qiu, et al. Code needs comments: Enhancing code llms with comment augmentation. arXiv preprint arXiv:2402.13013, 2024a. Zifan Song, Yudong Wang, Wenwei Zhang, Kuikun Liu, Chengqi Lyu, Demin Song, Qipeng Guo, Hang Yan, Dahua Lin, Kai Chen, et al. Alchemistcoder: Harmonizing and eliciting code capability by hindsight tuning on multi-source data. Advances in Neural Information Processing Systems, 37:21852214, 2024b. Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, pp. 24592475, 2025."
        },
        {
            "title": "Data Science and Technology",
            "content": "Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pp. 843852, 2017. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-ofthought can solve them. arXiv preprint arXiv:2210.09261, 2022. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, et al. Txt360: top-quality llm pre-training dataset requires the perfect blend, 2024. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. UltraData. Ultradata-math, 2026. URL https://huggingface.co/datasets/openbmb/UltraData-Math. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325, 1:1, 2022. Denny Vrandeˇcic and Markus Krötzsch. Wikidata: free collaborative knowledgebase. Communications of the ACM, 57(10):7885, 2014. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839, 2024a. Feng Wang, Zesheng Shi, Bo Wang, Nan Wang, and Han Xiao. Readerlm-v2: Small language model for html to markdown and json. arXiv preprint arXiv:2503.01151, 2025a. Liangdong Wang, Bo-Wen Zhang, Chengwei Wu, Hanyu Zhao, Xiaofeng Shi, Shuhao Gu, Jijie Li, Quanyue Ma, TengFei Pan, and Guang Liu. Cci3.0-hq: large-scale chinese dataset of high quality designed for pre-training large language models, 2024b. URL https://arxiv.org/abs/2410.18505. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024c. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers), pp. 1348413508, 2023. Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng, Chaojun Xiao, et al. Ultra-fineweb: Efficient data filtering and verification for high-quality llm training data. arXiv preprint arXiv:2505.05427, 2025b. Zengzhi Wang, Xuefeng Li, Rui Xia, and Pengfei Liu. Mathpile: billion-token-scale pretraining corpus for math. Advances in Neural Information Processing Systems, 37:2542625468, 2024d. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025c."
        },
        {
            "title": "Data Science and Technology",
            "content": "Zifeng Wang, Chun-Liang Li, Vincent Perot, Long Le, Jin Miao, Zizhao Zhang, Chen-Yu Lee, and Tomas Pfister. Codeclm: Aligning language models with tailored synthetic data. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 37123729, 2024e. Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Zhongjiang He, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huihan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, and Shuangyong Song. Telechat technical report, 2024f. Maurice Weber, Dan Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, et al. Redpajama: an open dataset for training large language models. Advances in neural information processing systems, 37:116462116492, 2024. Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr 2: Visual causal flow. arXiv preprint arXiv:2601.20552, 2026. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, and Yahui Zhou. Skywork: more open bilingual foundation model, 2023a. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with oss-instruct. arXiv preprint arXiv:2312.02120, 2023b. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Édouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of The 12th Language Resources and Evaluation Conference, pp. 40034012, 2020. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. Qurating: Selecting high-quality data for training language models. arXiv preprint arXiv:2402.09739, 2024. Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. Organize the web: Constructing domains enhances pre-training data curation. arXiv preprint arXiv:2502.10341, 2025. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmc-llama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9):18331843, 2024. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:3420134227, 2023. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024a. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024b."
        },
        {
            "title": "Data Science and Technology",
            "content": "Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, and Ji Pei. Opencsg chinese corpus: series of high-quality chinese datasets for llm training. arXiv preprint arXiv:2501.08197, 2025. Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. Wudaocorpora: super large-scale chinese corpora for pre-training language models. AI Open, 2:6568, 2021. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. In Forty-first International Conference on Machine Learning, 2024. Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, and Zhongyu Wei. Disc-lawllm: Fine-tuning large language models for intelligent legal services, 2023. Shengbin Yue, Siyuan Wang, Wei Chen, Xuanjing Huang, and Zhongyu Wei. Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks. arXiv preprint arXiv:2407.09893, 2024a. Shengbin Yue, Ting Huang, Zheng Jia, Siyuan Wang, Shujun Liu, Yun Song, Xuanjing Huang, and Zhongyu Wei. Multi-agent simulator drives language models for legal intensive interaction. arXiv preprint arXiv:2502.06882, 2025. Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 37:9062990660, 2024b. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zhiyuan Zeng, Hamish Ivison, Yiping Wang, Lifan Yuan, Shuyue Stella Li, Zhuorui Ye, Siting Li, Jacqueline He, Runlong Zhou, Tong Chen, et al. Rlve: Scaling up reinforcement learning for language models with adaptive verifiable environments. arXiv preprint arXiv:2511.07317, 2025. Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-centric artificial intelligence: survey. ACM Computing Surveys, 57(5):142, 2025. Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. Chinese open instruction generalist: preliminary release, 2023. Wanyue Zhang, Ziyong Li, Wen Yang, Chunlin Leng, Yinan Bai, Qianlong Du, Chengqing Zong, and Jiajun Zhang. Chinesewebtext 2.0: Large-scale high-quality chinese web text with multi-dimensional and fine-grained information, 2024. URL https://arxiv.org/abs/2411.19668. Ranchi Zhao, Zhen Leng Thai, Yifan Zhang, Shengding Hu, Jie Zhou, Yunqi Ba, Jie Cai, Zhiyuan Liu, and Maosong Sun. Decoratelm: Data engineering through corpus rating, tagging, and editing with language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 14011418, 2024."
        },
        {
            "title": "Data Science and Technology",
            "content": "Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, and Zhixiong Zeng. Ocrverse: Towards holistic ocr in end-to-end vision-language models. arXiv preprint arXiv:2601.21639, 2026. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. Programming every example: Lifting pre-training data quality like experts at scale. arXiv preprint arXiv:2409.17115, 2024. Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, and Eric Xing. Megamath: Pushing the limits of open math corpora. arXiv preprint arXiv:2504.02807, 2025a. Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, et al. survey of llm data. arXiv preprint arXiv:2505.18458, 2025b. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "ModelBest Inc.",
        "South China Agricultural University",
        "Tsinghua University"
    ]
}