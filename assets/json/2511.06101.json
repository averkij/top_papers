{
    "paper_title": "Adapting Web Agents with Synthetic Supervision",
    "authors": [
        "Zhaoyang Wang",
        "Yiming Liang",
        "Xuchao Zhang",
        "Qianhui Wu",
        "Siwei Han",
        "Anson Bastos",
        "Rujia Wang",
        "Chetan Bansal",
        "Baolin Peng",
        "Jianfeng Gao",
        "Saravan Rajmohan",
        "Huaxiu Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent."
        },
        {
            "title": "Start",
            "content": "Zhaoyang Wang1,3* Yiming Liang2 Xuchao Zhang3 Qianhui Wu3 Siwei Han1 Anson Bastos3 Rujia Wang3 Chetan Bansal3 Baolin Peng3 Jianfeng Gao3 Saravan Rajmohan3 Huaxiu Yao1 2Purdue University 3Microsoft 1UNC-Chapel Hill {zhaoyang,huaxiu}@cs.unc.edu,{xuchaozhang,qianhuiwu}@microsoft.com 5 2 0 2 8 ] . [ 1 1 0 1 6 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) with multimodal capabilities have enabled new wave of web agents capable of autonomously completing complex tasks on the internet (Nakano et al., 2021; Hong et al., 2024; He et al., 2024; Agashe et al., 2025; Li et al., 2025). These agents take user instructions and then interact with websites to accomplish tasks, showing promising results on standardized benchmarks (Zhou et al., 2024; Yao et al., 2022; Wei et al., 2025a; Deng et al., 2023). However, persistent challenge is that web agents strug- *Work done during internship at Microsoft. 1 Figure 1: The web agent is expected to adapt and operate on new environment. Some existing works choose to train on tasks from test set due to the scarcity of environment specific data. In contrast, this paper offers an approach for fully synthetic supervision to adapt agents to such new environment without human involvement. gle to adapt to new websites not seen during training (Zhou et al., 2024; Pahuja et al., 2025; He et al., 2024), because new environments often lack sufficient task demonstrations. And issuing environment specific tasks and collecting trajectories demonstrated by human on every new website can be prohibitively expensive. Existing training datasets are either limited to few domains or lack diversity (Xu et al., 2024; Deng et al., 2023; Xie et al., 2024; Chen et al., 2024a), thus when an agent is deployed on new website, it could frequently encounter unfamiliar states or tasks for which it has no experience. This raises the challenge of how to effectively adapt web agents to new environments with minimal human involvement. straightforward way to improve an agents performance on new website environment is to collect more environment specific training data. However, traditional agentic data collection relies on human experts or manually scripted tasks (Zhou et al., 2024; Lu et al., 2024; Deng et al., 2023; Mitra et al., 2024). Such approaches are labor-intensive and time consuming which can not easily scale. This may lead to significant gaps between training data and the real-world environments where agents are deployed. Without abundant experience in the new and unfamiliar environment, it is challenging for agents to adapt and perform effectively. To address the data scarcity issue in web agents, as shown in Figure 1, synthetic data generation (Xu et al., 2024; Su et al., 2025; Sun et al., 2025; Pahuja et al., 2025) has emerged as promising solution, which uses LLMs to automatically generate and collect data for training agents. Traditional selfinstruct methods (Wang et al., 2023a; Taori et al., 2023) can be used for generating agentic tasks, while it is hard to directly generate information intensive trajectory which often contains series of observations such as HTML code and screenshots. Recently, Sun et al. (2025) propose generating diverse tasks based on the observations of the environment changes, while hallucinations in generated tasks can make it impossible to collect corresponding trajectory. Explorer (Pahuja et al., 2025) addresses this issue by iteratively refining tasks based on the agents observations during interaction with the environment (i.e., trajectory collection). However, since the initial tasks are often underspecified, the beginning trajectories tend to contain substantial noise. It is challenging to fully synthesize high-quality agentic training data for new environment without any human involvement. In this paper, we propose SynthAgent, synthetic supervision framework for adapting web agents to new environments. Compared to existing approaches, SynthAgent aims at ensuring the quality in synthesis by introducing dual refinement to both task and trajectory. Specifically, our method consists of four stages: (1) First, we synthesize domain specific agentic tasks by extensively exploring the environment and strategically categorizing web elements. (2) Then, we deploy an agent to collect trajectories for the synthesized tasks, while refining tasks based on new observations to mitigate hallucinations. (3) After collection, we further refine the collected trajectories using global context to mitigate potential noise or misalignments. (4) Finally, we fine-tune an open-source web agent on the refined synthetic data to adapt it to the target environment. In contrast to existing works focusing on data synthesis, our approach aims at improving the quality of synthetic data via dual refinement. In summary, our contributions are three-fold: (1) We propose SynthAgent, fully synthetic supervision framework for adapting web agents to new environments without test task leakage or human involvement. (2) We identify critical quality issues in existing synthetic data generation methods, i.e., hallucinations in tasks and noise in trajectories. We introduce dual refinement to substantially improve synthetic data quality. (3) Experimental results on controllable web environments demonstrate that SynthAgent outperforms existing synthetic data baselines, validating the importance of high-quality synthetic supervision for web agent adaptation."
        },
        {
            "title": "2.1 Web Agent",
            "content": "Recent advances in LLMs have driven interest in developing agents that combine reasoning and interaction (Wei et al., 2022; Yao et al., 2023a; Wang et al., 2022; Lightman et al., 2023; Guo et al., 2025). ReAct (Yao et al., 2023b) introduces the interleaving of reasoning and actions after observations, while following works (Gao et al., 2023; Hong et al., 2024; Wu et al., 2024; Nakano et al., 2021) explore the use of tools integration and planning to enhance agent capabilities. However, these works primarily rely on human-annotated data for training, which is costly to scale and difficult to adapt to new environments. Another series of works (Lu et al., 2025; Qiu et al., 2025; Wei et al., 2025b) use reinforcement learning to train web agents, but they often require supervision from expert annotation and are impractical for complex and realistic websites due to the high cost of online training. Using synthetic data to train web agents has also gained traction as scalable alternative, especially for new environments (Wang et al., 2024; Liu et al., 2025)."
        },
        {
            "title": "2.2 Data Synthesis",
            "content": "Data synthesis has emerged as powerful paradigm to address data scarcity across various fields, allowing models to learn from automatically generated examples rather than costly human annotations. Early works such as Self-Instruct (Wang et al., 2023a) and Alpaca (Taori et al., 2023) leverage advanced but closed-source LLMs to bootstrap instruction tuning data for training smaller opensource models. Meanwhile, other studies (Zelik2 man et al., 2022; Wang et al., 2023b; Ge et al., 2024; Zhao et al., 2025) explore synthesizing large-scale training data to enhance the reasoning performance of smaller LLMs. These successes demonstrate the promising potential of synthetic data for adapting models to new tasks and domains. However, in agent scenarios, many works focus solely on synthesizing trajectories while directly using tasks from the test set to train the model (Chen et al., 2024b; Zhou et al., 2024; Zhang et al., 2025), raising serious concerns about test set leakage. Fortunately, recent works (He et al., 2024; Su et al., 2025; Sun et al., 2025; Pahuja et al., 2025) have explored data synthesis for web agents from scratch, which includes both task and trajectory synthesis."
        },
        {
            "title": "2.3 Synthetic data for Web Agents",
            "content": "Inspired by the success of distilling knowledge from large LLMs into smaller models (Wang et al., 2023a; Taori et al., 2023; Xu et al., 2023; Wang et al., 2023b), synthetic data has been increasingly used to train web agents. He et al. (2024) introduce self-instruct for agentic task generation, where it mainly operates on the surface of websites, thus the synthesized tasks are often simple and repetitive. AgentTrek (Xu et al., 2024) collects training data by scraping web tutorials from offline corpus, which are often outdated and not feasible for new environments. WebSynthesis (Gao et al., 2025) employs learned world model of web interfaces to simulate interactions, but its reliance on LLMbased environment can introduce additional hallucinations. Explorer (Pahuja et al., 2025) synthesizes tasks and trajectories simultaneously and refines initial tasks based on observations. However, since these tasks are often underspecified at the beginning, the trajectories may contain substantial noise due to wandering. OS-Genesis (Sun et al., 2025) collects low-level actions through GUI exploration and infers high-level tasks from them. However, its task proposals rely solely on singlestep environment changes, often leading to severe hallucinations. Additionally, OS-Genesis explores environments via random actions, which produces repetitive tasks and inefficient exploration. To address these limitations in data quality and diversity, SynthAgent introduces dual refinement for both task and trajectory synthesis, effectively reducing hallucinations and noise. Further, SynthAgent uses elements categorization to guide exploration and task generation, enhancing both diversity and coverage of the synthesized data."
        },
        {
            "title": "3 Method",
            "content": "As shown in Figure 2, the proposed SynthAgent framework consists of four main steps: (1) Task Synthesis with Categorized Exploration, (2) Task Refinement during Trajectory Collection, (3) Trajectory Refinement, and (4) Agent Fine-tuning. In the following, we describe each step in detail. Problem Setup. We view website as partially observable environment E. The task τ specifies goal that requires web agent to interact with the environment to accomplish. At step t, the agent receives multimodal observation ot (i.e., textual accessibility tree (Zhou et al., 2024) and visual screenshot of the webpage) and outputs an action at (e.g., CLICK, TYPE, SCROLL and etc.). Let ht = (o1, a1, . . . , ot) denote the trajectory, i.e., the sequence of observations and actions taken by the agent to complete the task. In this paper, our goal is to synthesize dataset = {(τ i, hi)}N i=1 of tasks τ and corresponding trajectories hi on previously unseen environment, and then adapt an open-source web agent to this environment by supervised fine-tuning."
        },
        {
            "title": "3.1 Task Synthesis",
            "content": "Following OS-Genesis (Sun et al., 2025), we explore the environment to synthesize diverse tasks. It collects interaction triplets (ot, at, ot+1) by executing an action at on randomly selected element of the current page observation ot, resulting in new webpage ot+1. Then, it prompts an LLM to propose high-level tasks based on each triplet. However, such random exploration can lead to redundant interactions and limited task diversity. To address this issue, we propose categorized exploration to systematically cover different types of web elements and interactions. Specifically, at each visited page ot with element set containing text, images, inputs, buttons, links and etc., we classify elements into several categories (e.g., Account Management, Search & Filters, Shopping Content and etc. for shopping webpage) based on their functional intention. This partition is enabled by prompting an LLM with the page structure and simple heuristics such as element role and name. From each category we uniformly sample up to 2 unvisited elements to interact with, which can then yield batch of interaction triplets. We also maintain URL pool to record newly discovered pages, for resuming exploration to deeper pages later. 3 Figure 2: Overview of SynthAgent and its comparison with baseline methods: OS-Genesis (Sun et al., 2025) and Explorer (Pahuja et al., 2025). SynthAgent consists of four steps: (1) Task Synthesis with Categorized Exploration, which synthesizes diverse, environment specific tasks through categorized exploration. (2) Task Refinement during Trajectory Collection, which collects trajectories while refining task descriptions based on new observations to mitigate potential hallucinations. (3) Trajectory Refinement, which further improves collected trajectories using global context to edit noisy actions. (4) Agent Fine-tuning, which adapts the web agent to new environments under fully synthetic supervision. The right panel highlights the key advantage of SynthAgent over baselines, i.e., the proposed dual refinement of both task and trajectory synthesis can lead to better synthetic data. For each collected triplet (ot, at, ot+1), we prompt the LLM to propose high-level task τ describing users intention that (1) is achievable from ot by plausible multi-step interactions and (2) is highly possible to be completed in ot+1 or subsequent pages. The LLM is requested to imagine broader objective that requires multiple steps to accomplish, where the single action at is just one step towards the goal. These synthesized tasks are thus more environment-aware and diverse, covering various categories and depths of target website."
        },
        {
            "title": "3.2 Task Refinement",
            "content": "Why task refinement is necessary. After task proposal, OS-Genesis deploys the agent to interact with the environment to collect trajectories for each synthesized task. Note that its proposed task is based solely on single interaction, thus it often contains hallucinations (e.g., assuming an option or account state that does not exist), causing agent cannot complete the task. Explorer (Pahuja et al., 2025) combines task proposal and trajectory collection into one stage. It first proposes coarse task from the homepage and then continuously refines the task based on new observations during trajectory collection. This design implicitly acknowledges that the initial task is underspecified 4 which means following refinement is more about filling in details rather than correcting hallucinations. Empirically, we observe that Explorer frequently changes the task intent during execution (8.6 times vs. 2.0 times of SynthAgent), which often leads to overly long trajectories that fail to complete within the budget (68.3% samples vs. 6.3% samples of SynthAgent). To mitigate hallucinations in task synthesis, we separate task proposal and trajectory collection into two distinct steps, in order to avoid underspecified initial tasks. Building on our categorized exploration, the initial tasks are already more detailed, while it still may contain hallucinations due to limited observations. Therefore, we introduce the following task refinement during trajectory collection to reduce such noise. Refinement during Trajectory Collection. Let τt denote the task specification at time and ht = (o1, a1, . . . , ot) the partial trace. The task is likely to contain hallucinations if it conflicts with the observation on the realistic environment. Thus, we trigger refinement when detect conflict with lightweight predicate C(ht, τt)=true, where := ExistsUI(ht, τt) (cid:125) (cid:124) (cid:123)(cid:122) invalid goal MissingArgs(ht, τt) (cid:125) (cid:123)(cid:122) missing details (cid:124) Stall(ht) (cid:125) (cid:124) (cid:123)(cid:122) stalled (1) . The first term ExistsUI fires when element implied by τt are absent or contradict observations (e.g., referenced item does not exist). The second term MissingArgs detects when τt is underspecified and lacks essential parameters (e.g., username when login) that cannot be inferred from ht. The third term Stall detects lack of progress: three consecutive no-op transitions, or the same navigation/error loop encountered twice. Upon trigger, we call the LLM to follow four evidence-driven principles to refine the task: (1) concretize missing details, (2) align with actual observations, (3) downscope or simplify when blocked, and (4) preserve similar task category. The replacement task τt+1 is expected to resolve the conflict while remaining as close as possible to the original intent. The following actions are then taken to continue trajectory collection, in order to complete the new task τt+1. After execution, the last specification is the final task τ along with the collected trajectory hT . Note that refinement processes can introduce discontinuity and noise in the trajectory, which we address in the next step."
        },
        {
            "title": "3.3 Trajectory Refinement",
            "content": "Why Trajectory refinement is necessary. Refining task during execution inevitably produces trajectories whose segments were collected under earlier task variants and thus misaligned with the final refined task τ . In addition, even without task edits, the agent may wander through dead-ends, repeat no-op actions (e.g., consecutive SCROLL), or backtrack, naturally resulting in noisy trajectories for supervision. To convert such trajectory into more clean and suitable one that is aligned well with τ , SynthAgent introduces post-hoc offline trajectory refinement step after collection, without reinteracting the environment or fabricating new steps. Thanks to the global view of the full trajectory and finally defined task, we can consider small set of conservative edits: (1) Remove(i): is task-irrelevant or redundant (e.g., repeated SCROLL with identical observation). delete step (oi, ai) if it (2) Reorder(i, j): swap two locally commutable steps (oi, ai) and (oj, aj) when their targets are independent (e.g., open filter then set sort). (3) Drop(hT ): drop the entire trajectory if it is too noisy or fails to complete any part of τ . 5 (4) Keep(hT ): keep the trajectory as is if the trajectory is already aligned well with τ . These edits are proposed by prompting the LLM with (τ , hT ) and then applied to refine the trajecT . The refined pair (τ , tory into clean one ) is expected to be less noisy and better consistent, thus more suitable for fine-tuning. For ablations, we also retain the original trajectories to quantify the effect of post-hoc editing."
        },
        {
            "title": "3.4 Agent Fine-tuning",
            "content": "After obtaining high-quality dataset of refined task-trajectory pairs = {(τ , h)}N , we finetune the open-source web agent to adapt it to such new environment with fully synthetic supervision. We split sample with steps into sequence of training examples {(τ , {ot, a<t}, at)}T t=1, where the model learns to predict the next action at given the task description τ and the historical context of observations and actions {ot, a<t}. The historical context window is empirically set to 3 considering training cost and inference latency. By using the standard supervised fine-tuning (SFT) paradigm, we optimize the model as follows: LSFT = E(τ ,h)D (cid:34) (cid:88) log pθ(atτ , ot, a<t) . (cid:35) t=1 (2) After fine-tuning, the agent is expected to better understand the new environment and complete environment specific tasks more effectively, achieving the goal of adaptation."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Environment & Benchmark. We conduct experiments in WebArena (Zhou et al., 2024), fully controllable suite of five websites: e-commerce (Shopping), content management (CMS), social forum (Reddit), developer platform (Gitlab), and map navigation (Maps). The environment provides simplified html code (accessibility tree) and visual screenshots as observations, and accepts common browser interactions listed in Table 5. We deliberately choose it as our sole testbed for three reasons. First, compared with simplified environments such as WebShop (Yao et al., 2022), each website in WebArena exposes deep task structure (multi pages, authentication, and full functionality), which better matches the realistic scenarios. Second, WebArena provides Docker deployments, ensuring the reproducibility, in contrast to online benchmarks such as Mind2Web (Deng et al., 2023) where pages may change over time. Finally, since web agents extensively interact with browsers during both exploration and evaluation, running in local environment keeps experimental cost manageable and avoids the operational risks of live-web interaction. Baselines & Models. We compare our approach against several strong baselines for synthesis of web agent data, including: (1) Self-Instruct (Wang et al., 2023a), which directly generates tasks from few seed examples via prompting LLMs. (2) OS-Genesis (Sun et al., 2025), which synthesizes tasks from single-step environment changes with randomly exploring the environment. (3) Explorer (Pahuja et al., 2025), which synthesizes tasks and trajectories by iteratively refining tasks during trajectory collection. All baselines are reimplemented using the same LLM of GPT-4.1 1 for both task and trajectory synthesis. We select two popular open-source multimodal LLMs for agent fine-tuning: Qwen2.5-VL-7B-Instruct (Team, 2025) and UI-TARS-1.5-7B (Qin et al., 2025). Implementation Details. For both data synthesis and evaluation, we set maximum step budget of 30 per episode, following recommendations from (Zhou et al., 2024). All methods are synthesizing up to 500 task-trajectory pairs per website for agent fine-tuning. During execution and training, they are using the same prompt template and action space. We also use the same GPT-4.1 for both synthesis and refinement for SynthAgent. We use context window of 3 most recent steps for efficiency. For agent fine-tuning, we mix synthesized data from five websites to train single model with learning rate of 1e-5 and batch size of 32 for 3 epochs. For evaluation, we restart the environment for every run, and report the success rate of completing the task from test set. More details are in Appendix A.1."
        },
        {
            "title": "4.2 Main Results",
            "content": "The overall performance of different methods is shown in Table 1. From the results, we have the following observations: (1) Training on test set tasks (SFT) provides an informative upper bound for the following experiments using synthetic data. (2) Self-Instruct struggles without environment 1https://openai.com/index/gpt-4-1/ grounding, showing limited improvements over base models. This indicates the importance of interacting with the environment for creating environment specific tasks. (3) Explorer performs surprisingly poorly, even degrading base model performance. We find that this poor performance is due to: first, substantial portion of its collected trajectories (about 68.3%) exceed the execution step budget without completing the task, which will be discussed later. This is because continuous task refinement strategy frequently changes task goals and adds new details during execution, causing the agent to deviate from the original objective. Second, the original Explorer paper states that their method operates on web pages without authentication requirements. This limitation suggests that Explorer may be unsuitable for complex, realistic websites that involve complex functionality. (4) Our SynthAgent substantially improves web agent adaptation with fully synthetic supervision, consistently outperforming all baselines. Compared to base models and OS-Genesis, we achieve average absolute gains of +10.2 and +5.1, respectively. SynthAgent also shows closer gap to the upper bound SFT, demonstrating its effectiveness in synthesizing high-quality data. The success of SynthAgent over baselines highlights the importance of dual refinement in both task and trajectory synthesis, which ultimately mitigates hallucinations and noise, offering better agent fine-tuning."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we conduct further analyses to understand the effectiveness of SynthAgent."
        },
        {
            "title": "5.1 Synthetic Data Quality",
            "content": "We first analyze the quality and diversity of synthetic data in Table 2. To better understand the characteristics of synthesized tasks, we also visualize their embeddings using t-SNE (van der Maaten and Hinton, 2008) in Figure 3, where points closer in the plot indicate more similar tasks, while points farther apart indicate greater diversity. Task Diversity & Quality. Synthesized task diversity is the essential prerequisite for effective agent adaptation. Self-Instruct produces points heavily clustered in small region of the embedding space, which reflects: without environment grounding, the LLM imagined tasks results in repetitive and narrow task variations. In contrast, OS-Genesis achieves better diversity (83) through 6 Train on Shopping CMS Reddit Gitlab Maps Overall Method GPT-4.1 Qwen +SFT - - Test Dataset +Self-Instruct +OS-Genesis +Explorer +Ours Synthetic Data Synthetic Data Synthetic Data Synthetic Data UI-TARS +SFT - Test Dataset +Self-Instruct +OS-Genesis +Explorer +Ours Synthetic Data Synthetic Data Synthetic Data Synthetic Data 30. 13.71 27.27 18.18 14.55 10.91 20.00 12.73 25.45 20.00 21.82 11.43 20.00 24.56 15. 26.79 21.9 25.22 8.24 12.28 8.77 10.53 3.51 21.05 8.77 22. 8.77 12.28 4.88 14.04 9.43 19.23 3.85 11.54 0.00 15.38 3.85 19.23 7.69 7.69 0.00 19.23 6.18 10. 12.50 16.07 1.82 19.64 7.14 21.43 14.55 14.29 3.23 16.07 5.50 12.50 9.38 12.5 3.12 28.12 9.38 28. 12.50 12.50 0.00 18.75 8.80 16.37 11.50 13.27 4.44 20.80 8.85 23.45 13.33 14.60 4.96 17.26 Table 1: Performance comparison across different methods and websites. SFT means directly use the tasks from test set paired with trajectories collected by GPT-4.1 for agent fine-tuning. The best performance among synthetic methods is highlighted in bold. In most cases, ours SynthAgent outperforms other methods with synthetic data. Figure 3: t-SNE visualization of synthesized tasks. Tasks from test set are written by human. Task Trajectory Method Completed Failed Exceeded Costs $ Method Self-Instruct OS-Genesis Explorer SynthAgent w/o TR w/o JR Quality # Refine Diversity #Steps Quality 49.2 39.8 69.8 70.2 68.2 69.5 - - 8.6 2.0 - 2.0 69 83 95 86 95 6.3 5.1 20.5 7.5 7.0 8.8 56.6 54.1 48.1 92.5 78.1 86.3 Table 2: Statistics of synthesized data. Quality metric is rated on scale of 0-100 for every sample. Diversity metric is measured by 100 samples as whole. ALL these metrics are rated by GPT-4.1. TR and JR stand for task and trajectory refinement, respectively. Explorer SynthAgent w/o TR w/o JR 30.5 96.5 47.4 73.8 1.1 3.5 49.2 17.0 68. 0.0 3.4 9.2 0.22 0.13 0.09 0.12 Comparison between Explorer Table 3: and SynthAgent in terms of trajectory completion rates judged by GPT-4.1. Completed, Failed, and Exceeded are the percentages of trajectories that successfully completed the task, failed to complete the task, or exceeded the maximum step budgets, respectively. Costs are the average API cost per trajectory. random environment exploration. Explorer exhibits the poorest diversity (54) with highly clustered pattern, indicating that its initial coarse-grained task proposals from the homepage constrain the space of possible tasks. SynthAgent achieves the highest diversity score after task refinement, with points closely resembling the distribution of human written tasks. This suggests that our categorized exploration and task refinement strategies effectively covers different functions and interaction depths. Trajectory Quality. Table 2 shows that our method achieves the highest trajectory quality (92.5), substantially outperforming other methods. From the ablation, we can find the improvement 7 Method Shopping CMS Reddit Gitlab Maps Overall Qwen +SS +CE +TR +JR 13.71 14.55 20.00 18.18 20.00 8. 10.53 14.04 12.28 21.05 9.43 6.18 5.50 11.54 15.38 11.54 15.38 16.07 16.07 19.64 19. 12.50 12.50 21.88 28.12 8.80 13.27 15.93 16.81 20.80 Table 4: Ablation study of SynthAgent. SS and CE stand for naive synthetic supervision and categorized exploration, respectively. TR and JR stand for task and trajectory refinement, respectively. Indentation indicates the addition of new component based on the up-level. is enabled by our dual refinement. First, task refinement during trajectory collection ensures tasks are feasible and grounded in actual observations. Second, trajectory refinement with global context effectively mitigates noise from execution and task edits. To understand why Explorer underperforms despite similar refinement, we analyze trajectory completion rates in Table 3. Explorers continuous task refinement during execution changes goals and adds details at each step, causing agents to deviate from original objectives, resulting in 68.3% of trajectories exceeding the step budget without task completion. In contrast, SynthAgent has 73.8% success rate with task refinement and 92.5% after trajectory refinement, demonstrating the advantage of the proposed dual refinement design. In addition, compared with 0.22$ cost of Explorer, SynthAgent only costs only about 60% of it due to more efficient exploration and refinement. Figure 4: Performance of SynthAgent across different websites with varying synthesis data amounts."
        },
        {
            "title": "5.3 Data Scaling",
            "content": "To evaluate the scalability of SynthAgent, we measure across different data amounts in Figure 4, scaling from 4% (20 tasks per website) to 100% (500 tasks per website). The results demonstrate consistent performance gains as more synthetic data is used, with the average success rate increasing from approximately 10.6 at 4% data to 20.8 at 100% data. Notably, different websites exhibit varying performance gains, which may be attributed to their inherent task complexity and capacity. For example, Map environment reaches the peak at 40% data, while CMS and Gitlab continue to steadily improve. The overall scaling behavior shows that SynthAgent maintains data quality consistently as the synthesis scales with the website complexity."
        },
        {
            "title": "6 Conclusion",
            "content": "To quantify the contribution of each component in SynthAgent, we conduct systematic ablation study in Table 4. Starting from the base model, naive synthetic supervision (+SS) boosts it to 13.27%, showing data synthesis by simply interacting with the environment can help adaptation. Adding categorized exploration (+CE) significantly enhances performance of Shopping and Reddit websites, which need categorization to cover diverse functions from repetitive elements. Interestingly, adding only task refinement (+TR) yields marginal 0.88 improvement, which confirms that changing tasks during execution can introduce noise to trajectories. Thanks to the final trajectory refinement, +JR achieves the strongest performance at 20.80%. This big gap highlights the trajectory refinement is necessary for the success of prior task refinement, without which the contribution would be masked by noisy trajectories. In this paper, we study how to adapt web agents to new environments where environment-specific tasks and demonstrations are scarce. We identify that existing synthetic data generation methods suffer from severe quality issues: tasks often contain hallucinations and collected trajectories are noisy. To address these issues, we propose SynthAgent, fully synthetic supervision framework that improves data quality through dual refinement of both tasks and trajectories. In addition, we introduce categorized exploration to systematically cover web elements and interactions, enhancing the diversity and efficiency of task synthesis. Extensive experiments and analysis demonstrate the effectiveness of our approach in adapting web agents to new environments. Beyond adaptation, we believe SynthAgent is valuable resource for the rapidly emerging field of agentic reinforcement learning where large-scale and diverse tasks are crucial."
        },
        {
            "title": "References",
            "content": "Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. 2025. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906. Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, and 1 others. 2024a. Guiworld: dataset for gui-oriented multimodal llmbased agents. arXiv e-prints, pages arXiv2406. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024b. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards generalist agent for the web. Preprint, arXiv:2306.06070. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. Preprint, arXiv:2211.10435. Yifei Gao, Junhong Ye, Jiaqi Wang, and Jitao Sang. 2025. Websynthesis: World-model-guided mcts for efficient webui-trajectory synthesis. Preprint, arXiv:2507.04370. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Hongming Zhang, Tianqing Fang, Zhenzhong Lan, and Dong Yu. 2024. Openwebvoyager: Building multimodal web agents via iterative real-world exploration, feedback and optimization. arXiv preprint arXiv:2410.19609. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, and 1 others. 2024. Metagpt: Meta programming for multi-agent collaborative framework. International Conference on Learning Representations, ICLR. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, and 1 others. 2025. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, and 1 others. 2025. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990. Xing Han Lu, Zdenˇek Kasner, and Siva Reddy. 2024. WebLINX: Real-world website navigation with multiturn dialogue. In Forty-first International Conference on Machine Learning. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. 2025. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620. Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby Rosset, and 1 others. 2024. Agentinstruct: Toward generative teaching with agentic flows. arXiv preprint arXiv:2407.03502. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and 1 others. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra, Spencer Whitehead, Yu Su, and Ahmed Awadallah. 2025. Explorer: Scaling exploration-driven web trajectory synthesis for multimodal web agents. Preprint, arXiv:2502.11357. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, and 1 others. 2025. Uitars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326. Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, and 1 others. 2025. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286. Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Ö Arık. 2025. Learn-byinteract: data-centric framework for self-adaptive arXiv preprint agents in realistic environments. arXiv:2501.10893. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou 9 Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, and Zhiyong Wu. 2025. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. Preprint, arXiv:2412.19723. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Qwen Team. 2025. Qwen2.5-vl. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):25792605. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, and 1 others. 2024. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023a. Self-instruct: Aligning language models with self-generated instructions. Preprint, arXiv:2212.10560. Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and 1 others. 2023b. Democratizing reasoning ability: Tailored learning from large language model. arXiv preprint arXiv:2310.13332. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025a. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, and 1 others. 2025b. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, and 1 others. 2024. Autogen: Enabling next-gen llm applications via multiIn First Conference on Lanagent conversations. guage Modeling. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, and 1 others. 2024. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. 2024. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. Preprint, arXiv:2210.03629. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Agudelo, Peter Qian, and Tianlong Chen. 2025. Symbiotic cooperation for web agents: Harnessing complementary strengths of large and small llms. arXiv preprint arXiv:2502.07942. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. 2025. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. 2023. Judging 10 llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems, volume 36, pages 4659546623. Curran Associates, Inc. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. Webarena: realistic web environment for building autonomous agents. Preprint, arXiv:2307.13854."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Environment Action Description Clicks on an element with the given element_id. Moves the cursor over an element with the given element_id. click [id] type [id] [text] Types the specified text into the field with element_id. hover [id] press [key_comb] Presses keyboard shortcut (e.g., Ctrl+V, Cmd+V). scroll [updown] Scrolls vertically in the given direction. goto [url] Navigates directly to the specified URL. go_back Navigates to the previous page in browser history. go_forward Navigates to the next page in browser history. none [summary] Produces summary or final answer without any browser action. stop [reason] Stops execution when the task is impossible or inconsistent. Table 5: Available atomic low-level browser interactions and their descriptions for WebArena. Action Space. All the actions that are executable in the trajectories are shown in Table 5 with regular web actions. The none action signals the final step for answering the task. For example, if the task is to find the cheapest product on shopping website, the last action would be none with the summary of the cheapest product as the observation. The stop action is used to terminate the trajectory when the agent exceeds the maximum step budget or encounters an unrecoverable error. Evaluation. WebArena (Zhou et al., 2024) contains 812 challenging web navigation tasks derived from 241 human written task templates, including maps, e-commerce, Reddit forums, and software development. Due to the naturally high cost of time interacting with the browser, we follow prior work (Sun et al., 2025) and use 226 tasks for evaluation, where we include only one task per template and exclude tasks that require cross-website navigation. The numbers of tasks for each website are 55, 57, 26, 56, and 32 for Shopping, CMS, Reddit, Gitlab, and Maps, respectively. Note that for training with tasks from test set (SFT method in Table 1), we still use the full set of 812 tasks to enrich the training data as much as possible. A.2 Task Generation For SynthAgent, the generation process begins by categorizing low-level actions into distinct functional groups through systematic exploration of the web environment. The agent interacts with the interface and analyzes all visible elements, distinguishing between interactive and non-interactive components and assigning each to an appropriate category. This structured categorization enables the model to generate contextually grounded low-level Figure 5: Case study of Task Refinement. During task execution, SynthAgent detects when task becomes infeasible based on the progression of its execution and refines the task accordingly, guided by the observed failure and current environment state. actions within different functional groups, thereby enhancing its generalization ability. The detailed prompt for this process is shown in Table 7, which is largely adopted from OS-Genesis (Sun et al., 2025). Using the low-level actions, we further generate high-level tasks following Table 8. A.3 Trajectory Generation To ensure higher-quality trajectories, SynthAgent divides the collection process into three stages. In the first stage, given the set of collected high-level tasks, SynthAgent executes them in chain-ofthought (CoT) manner (Wei et al., 2022), as illustrated in Table 9. The model reasons step-bystep over the current web context to generate the next low-level action until the task is completed or deemed infeasible. During the second stage, as the agent interacts with the environment, each high-level task is dynamically analyzed and refined using the task refinement prompt (i.e., prompt at Table 10). SynthAgent continuously evaluates whether the task remains valid given the current observation and refines it when inconsistencies, missing parameters, or execution stalls are detected. In the final stage, SynthAgent assesses the collected trajectories and refines them by dropping those deemed unachievable or reordering intermediate steps when revised sequence better fulfills the task objective using prompt at Table 11. A.4 Agent Framework Our agent execution framework builds upon ReAct (Yao et al., 2023b), instantiating policy that interleaves CoT reasoning with environment actions in browser-based setting with textual accessibility tree and visual screenshot as observations. Specifically, given high-level instruction, the agent performs sequence of steps to accomplish the task. At each step, the agent first takes observations from the environment based on the history actions and current state which includes the URL, accessibility tree, set of candidate interactive elements and screenshot, then uses CoT reasoning to generate the next action to take. Execution continues until the model explicitly outputs termination (none action in Table 5), the task is judged unachievable by itself (stop action in Table 5), or preset step budget (30) is exhausted. The used prompt template is shown in Table 9. A.5 Tasks Analysis We use LLMs-as-a-judge (Zheng et al., 2023) to evaluate the quality of the generated tasks. We prompt the LLM with Table 12 to assesses how broadly and non-redundantly the tasks cover different user intentions for specific website. 12 Figure 6: Case study of Trajectory Refinement. After trajectory collection, SynthAgent evaluates the trajectory and refines it by reordering steps or removing unnecessary actions to ensure the trajectory consistently align with the task objective and ends with successful completion. webpage. The collected trajectory included this futile loop along with redundant scroll actions that did not contribute to task completion. Our trajectory refinement step, equipped with the global view of the full trajectory and final task objective, identified and removed the repetitive clicking attempts on the unresponsive sort interface, consolidated the necessary scroll actions. The refined trajectory retained only the essential 9 steps needed to successfully complete the task: clicking the Electronics category, navigating through sorting options, and scrolling to identify the cheapest product. This refinement ensures that the trajectory aligns precisely with the high-level task while removing noisy actions that would be harmful for the agent during fine-tuning."
        },
        {
            "title": "B Case Study",
            "content": "Task Refinement. Figure 5 illustrates an example of the task refinement process in SynthAgent. The original task Sort the vitamin supplements search results by price to find the cheapest product available, was initially executed based on the agents understanding of the web interface. However, during interaction, the agent detected that the page failed to redirect to the intended subcategory (Diet & Sports Nutrition), making the original goal inconsistent with the current observation. SynthAgent automatically analyzed this discrepancy and refined the task to align with the accessible context, updating it to Identify the product with the lowest listed price in the Health & Household category.. This refinement ensures that highlevel task remain executable and contextually valid for existing trajectory. Trajectory Refinement. Figure 6 presents the trajectory refinement process in SynthAgent. The agents intermediate sequence of the trajectory for the task Find the cheapest available product in the Electronics category by sorting results by price contained significant noise and inefficiencies. During execution, the agent became stuck attempting to interact with non-functional sort option, resulting in 19 repeated steps with no progress on the 13 You are GUI (Graphical User Interface) Web Agent expert capable of grouping interactive elements from web page into high-level user task categories. **Information** Current Page: - URL: {url} - Accessibility Tree (only current view, not full page): {page_context} - Elements: {elements} - Screenshot of the page (only current view, not full page): <image is provided in the attachment> Add {const_uninteractive_category} category (list[int]) for non-interactive elements that **Your Goal** 1. Fully explore the current page and its content to understand its functionality and layout. 2. Categorize ALL provided {element_num} elements into different categories (list[dict]) based on their natural purpose. 3. cannot be CLICK, TYPE, or HOVER. 4. For each category (except {const_uninteractive_category}), decide: { \"action\": choose from [CLICK, TYPE, HOVER], \"element_id\": id_of_element (int), \"value\": if TYPE, provide text to type; else , \"low-level_instruction\": concise description of the action } Example low-level instructions: - \"Click on the Add to Cart button next to the product to add it to your shopping cart.\" - \"Type OpenAI into the search bar to find relevant articles.\" - \"Scroll down to view the latest blog posts on the homepage.\" 5. Provide an appropriate and meaningful value for \"value\" if the action is TYPE. Examples: - For search box, generate realistic search query. - For textbox, generate plausible text according to context. **Output Requirements** Return ONLY JSON dictionary (no commentary) with the following format: { \"Analysis\": \"your analysis of the current page state and elements\", \"Categorization\": { \"category_1\": [ { \"action\": \"xxx\", \"element_id\": int, \"value\": \"xxx or empty string\", \"low-level_instruction\": \"a concise instruction\" }, ...other actions... ], \"category_2\": [...], \"category_3\": [...], ...{different categories}..., \"{const_uninteractive_category}\": [element_id_1, element_id_2, ...] } } RETURN ME THE DICTIONARY ASKED FOR WITHOUT ANY COMMENTARY. \"\"\" Table 7: Prompt for web elements categorization. 14 You are GUI (Graphical User Interface) expert capable of analyzing interface changes and envisioning executable tasks or instructions. Given GUI interface change caused by an action (e.g., clicking or typing) and the corresponding element highlighted in red boxes, you are required to analyze the interface and generate related tasks. Your task is to envision tasks based on the current action and the resulting changes in the screenshots. The output should include three components: 1. Sub-Instruction: Create natural language instruction for the current action based on the interface changes it caused. The instruction should be concise, clear, and actionable, incorporating specific details critical to the task, such as elements, file names, timestamps, or other relevant content visible in the screenshots. For example: - \"Click on the Add to Cart button next to the product to add it to your shopping cart.\" - \"Type OpenAI into the search bar to find relevant articles.\" - \"Scroll down to view the latest blog posts on the homepage.\" Analysis: Carefully analyze the before-and-after screenshots step by step, focusing on the 2. Then, examine key elements in both screenshots and consider changes caused by the action. For example: \"The previous screen displayed the possible operations based on these elements. main interface of shopping website, featuring multiple product categories and several showcased items. After clicking the Sign Up button, the interface transitioned to login page where an email and password can be entered to log into an account. The login page also provides other options, such as recovering password, creating new account, or logging in with Google account.\" 3. High-Level Instruction: Based on the before-and-after screenshots, the action, and the analysis, generate high-level task that you believe can be completed within the current interface. There are three types of tasks: - Information seeking: The user wants to obtain certain information from the webpage, such as product details, reviews, map information, or route comparisons. Please propose clear and specific questions that need an explicit answer, and avoid asking for summary-type questions, such as \"summarize the information about product.\" - Site navigation: The user wants to navigate to specific page or state. - Content modification: The user wants to modify the content of webpage or its settings. The high-level instruction should be creative. You need to deeply analyze the elements and executable actions on the interface to generate realistic, valuable, and executable tasks that can be completed within the current GUI. The instruction should be specific, actionable, and goal-oriented, ensuring the task can be completed on the current GUI by including all critical specifics such as file names, relevant timings, or required details. Below is brief description of the current website: {website_intro} Here are some examples of High-Level Instruction for reference: {task_examples} Current Action: {current_action_str} Website Name: {website_name} Before-action Screenshot: <image is provided in the first attachment> (the actions target element is highlighted in red box if applicable) After-action Screenshot: <image is provided in the second attachment> Please generate tasks that can be completed on the current platform, and avoid tasks that are unrelated to the current website. You ONLY need to return JSON dictionary formatted as follows (no extra commentary): { \"Sub-Instruction\": \"xxx\", \"Analysis\": \"xxx\", \"High-Level-Instruction\": \"xxx\" } RETURN ONLY THE DICTIONARY ASKED FOR. } Table 8: Prompt for agentic task generation, adopted from OS-Genesis (Sun et al., 2025). You are GUI (Graphical User Interface) Web Agent expert capable of long-horizon planning and executing high-level tasks on website. Based on the observations and the high-level task to complete, generate the next low-level instruction. **Information** 1. High-Level Task (your ultimate goal to finish): \"{high_level_task}\" 2. Current Page (only current view, not full page, you may need to scroll to see more): - URL: {url} - Accessibility Tree (Page Context): {page_context} - Elements (addressable in this view): {elements} - Screenshot (only current view, not full page): {img_info} 3. History of Actions ({hint_for_history}): {previous_state_action} **Critical Rules for Success** 1. Issue only actions valid for the current observation (elements, accessibility tree, screenshot). 2. Propose ONE atomic action per item in your Potential-Actions list; actions must be independently executable. 3. Prefer element IDs from the current Elements list for CLICK/TYPE/HOVER. 4. Provide meaningful non-empty value if action {TYPE, SCROLL, GOTO, NONE, STOP}. 5. If the task is complete, use NONE with the final answer in value; do not propose further actions. 6. Be concise, avoid redundant/risky actions; each action must advance the task. 7. observations/history. If the task is hallucinated/low-quality/impossible, cautiously choose STOP based on Pseudo-code for deciding STOP: if high_level_task lacks required info STOP if high_level_task contains hallucinations STOP if task is inappropriate/harmful STOP if multiple (3) similar attempts already failed STOP else consider NON-STOP actions You MUST actively decide the next step; First write \"state_observation_summary\", then do step-by-step \"reasoning\", then decide 8. \"next_action\". 9. Expect MULTIPLE steps; choose the next action that changes state; continue iteratively. 10. finish/impossibility. 11. In \"reasoning\", explicitly apply the STOP vs NON-STOP pseudo-code. 12. Actively explore alternatives before STOP if current approach stalls. 13. Choose elements strictly from \"Elements (addressable in this view)\"; justify this choice in \"reasoning\". 14. If the page doesnt change after an action, consider SCROLL to reveal more elements. 15. Special note: when typing date, use \"MM/DD/YYYY\". do not choose NONE/STOP unless certain of **Output Requirements** Return ONLY JSON dictionary (no commentary) with: { \"state_observation_summary\": \"13 sentence summary of the current state relevant to the task\", \"reasoning\": \"step-by-step reasoning to decide the next action; include rule-based justification and STOP check\", \"next_action\": { \"low-level_instruction\": \"concise string, e.g., click the button with ID submit\", \"action\": { \"type\": \"XXXX\", \"element_id\": <int or (if int, it MUST exist in Elements)>, \"value\": <string or per spec> } } Table 9: Prompt for long-horizon planning and next-action generation of web agent. 16 You are GUI (Graphical User Interface) Web Agent expert specializing in analyzing interface changes and determining whether high-level task should be refined based on series of agent actions and observations. ## High-Level Task Categories 1. **Information Seeking** User aims to retrieve specific information from the website. - Examples: - \"What is the most expensive product in the Electronics category?\" - \"What are the top 5 posts in the Technology forum?\" - \"Summarize the reviews for the product iPhone 11.\" 2. **Site Navigation** User aims to reach specific page or site state. - Examples: - \"Go to the billing page to check the latest transactions.\" - \"Navigate to the Contact Us page and fill out the form to express interest in joining the company.\" - \"Find the wiki page of the youngest person to receive Nobel Prize.\" 3. **Content Modification** User aims to change site content or settings. - Examples: - \"Create user account with username bob2134 and password 128nxc18zxv.\" - \"Post new article titled The Future of AI in the Technology forum.\" - \"Create code repo named Agent and add README with the text This is code repo for an intelligent agent.\" **Stalled or Repetitive Execution** three or more consecutive actions show no meaningful ## Refine Rules ### When to REFINE the task Refine the task if the following situations are observed (cite triggers in \"Analysis\"): 1. **Invalid or Inconsistent Goal** target entity/page/action does not exist, cannot be located, or conflicts with observed facts. 2. **Insufficient Executable Details** essential parameters are missing and cannot be inferred. 3. change, or same error repeats. ### When NOT to REFINE - Goal is valid and consistent with observations. - Essential parameters are available or can be inferred. - Actions show measurable progress. - No persistent or repetitive failures detected. ### How to REFINE If refinement is required (your analysis must reference the below rules): 1. **Concretize Missing Details** add essential parameters from history or observation. 2. **Align with Reality** replace hallucinated entities with actual ones found on the site. 3. **Downscope the Goal** adjust to the next achievable milestone. 4. **Preserve Task Type** keep within same category unless required otherwise. ## Goal Ensure the refined task is either already completed or highly likely to complete within the next 12 steps. ## Output Requirements - Format: JSON dictionary only, no commentary. - Fields: - \"Analysis\": Step-by-step reasoning. - \"Need-to-Refine\": \"yes\" or \"no\". - \"High-Level-Task\": Refined task if \"yes\", else empty string. ## Information 1. Current High-Level-Task: \"{current_high_level_task}\" 2. Previous High-Level-Tasks (oldest to newest): <start_previous_high_level_tasks> {previous_high_level_tasks} <end_previous_high_level_tasks> 3. History of Actions ({hint_for_history}): <start_action> {previous_state_action} <end_action> 4. Current Page (only current view): - URL: \"{curr_url}\" - Page Context: <start_context> {curr_state_context} <end_context> - Screenshot: \"{img_info}\" You ONLY need to return JSON dictionary formatted as follows (no commentary): { \"Analysis\": \"step-by-step reasoning\", \"Need-to-Refine\": \"yes or no\", \"High-Level-Task\": \"refined task if yes, otherwise empty\" } RETURN ONLY THE DICTIONARY ASKED FOR. Table 10: Prompt for refining tasks during trajectory collection. 17 You are GUI (Graphical User Interface) Web Agent expert. Your job is to analyze high-level task and its trajectory (sequence of states and actions), assign quality score, and decide one of: - \"keep\": keep the trajectory as-is (already minimal, ordered, and ends with correct NONE action with non-empty value). - \"refine\": reorder or delete steps to make the trajectory succeed (final step must be NONE with non-empty explanation). - \"drop\": discard the trajectory entirely (e.g., irreparable, hallucinatory, impossible, unsafe, or missing critical information). trajectory is structured as: \"Length of trajectory, High-level task, summary of state1, action1, summary of state2, action2, ...\" ### Scoring Rubric (0100) Evaluate the trajectory on: 1. Goal Alignment (025): Steps relevant to the high-level task. 2. Logical Order (025): Steps follow coherent and sensible sequence. 3. Efficiency (025): Avoids redundant or unnecessary actions. 4. Success Likelihood (025): Likely to end successfully with NONE (non-empty value). Note: The score is advisory; the final decision (keep/refine/drop) depends on qualitative judgment. ### Decision Policy - Always ensure kept/refined trajectories end with NONE action and non-empty value. - If refining, reorder or delete existing steps (do not add new ones). - Replace STOP with NONE if success is achievable. - If dropping, do not fabricate NONE; instead, provide clear drop_reason. **Indexing and Deletion Rules:** - Let the trajectory contain (observation, action) pairs, indexed 0..K-1. - Reordering: return indices in new order. - Deletion: omit indices. - No duplicates or out-of-range indices. Do not invent new steps. ### Input {trajectory} ### Output Requirement (STRICT) Return ONLY one JSON object (no extra text, no code fences): { \"task\": \"<exact high-level task string>\", \"score\": <int>, // 0100, advisory only \"decision\": \"keep\" \"refine\" \"drop\", \"order\": [<int>, ...], // indices in final order \"modify_end\": <truefalse>, \"append_end\": <truefalse>, \"final_none_value\": \"<non-empty explanation for final NONE>\", \"drop_reason\": \"<reason if dropped>\", \"modification_reason\": \"<brief rationale for keep/refine/drop>\" } ### Additional Constraints - \"task\" must match the high-level task exactly. - If decision = keep: order = [0,1,...,K-1], and final step already NONE. - If decision = refine: must end with NONE and valid order. - If decision = drop: order = [], final_none_value empty, provide drop_reason. Table 11: Prompt for trajectory quality evaluation and refinement. 18 You are GUI (Graphical User Interface) Web Agent evaluation expert. Your job is to evaluate the **diversity** of set of high-level tasks generated for single web environment. Each task represents distinct user goal that can be accomplished on the same webpage (e.g., \"Search for product\", \"Sort results by price\", \"View product details\"). Your evaluation should judge how **broad, non-overlapping, and complementary** these tasks are relative to one another. ## Input The input contains multiple high-level tasks that share the same web environment: {task_list_block} ## Scoring Rubric (0100) Evaluate the **diversity** of the provided task set using the following criteria: 1. **Intent Variety (025):** Do the tasks represent different user intents (e.g., information seeking vs. navigation vs. modification)? 2. **Action Diversity (025):** Do the tasks require different types of GUI interactions (e.g., clicking, typing, scrolling, submitting forms)? 3. **Goal Coverage (025):** Do the tasks explore different meaningful aspects or functionalities of the environment? 4. **Redundancy Minimization (025):** Are there minimal duplicate or near-duplicate tasks (i.e., no rephrasing of the same goal)? ## Output Requirement (STRICT) Return ONLY one JSON object (no extra text, no code fences): { \"score\": <int>, // 0100 total diversity score \"subscores\": { \"intent_variety\": <int>, // 025 \"action_diversity\": <int>, // 025 \"goal_coverage\": <int>, // 025 \"redundancy_minimization\": <int> }, \"analysis\": \"<short reasoning describing overall diversity and possible overlaps>\", \"representative_examples\": [\"<one or two tasks that illustrate high or low diversity>\"] } ## Additional Constraints - Evaluate ONLY diversity, not task quality or feasibility. - Consider whether tasks collectively span multiple distinct purposes, operations, or workflows. - Do NOT propose new tasks, rephrase them, or rewrite anything. - Do NOT remove or modify any input tasks. - The reasoning should briefly summarize what aspects contribute most to or detract from diversity. // 0 Table 12: Prompt for evaluating diversity of high-level tasks in single web environment."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Purdue University",
        "UNC-Chapel Hill"
    ]
}