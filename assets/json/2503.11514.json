{
    "paper_title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into Gradient Inversion Attacks",
    "authors": [
        "Pengxin Guo",
        "Runxi Wang",
        "Shuang Zeng",
        "Jinjing Zhu",
        "Haoning Jiang",
        "Yanran Wang",
        "Yuyin Zhou",
        "Feifei Wang",
        "Hui Xiong",
        "Liangqiong Qu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., \\textit{optimization-based} GIA (OP-GIA), \\textit{generation-based} GIA (GEN-GIA), and \\textit{analytics-based} GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Exploring the Vulnerabilities of Federated Learning: Deep Dive into Gradient Inversion Attacks Pengxin Guo*, Runxi Wang*, Shuang Zeng, Jinjing Zhu, Haoning Jiang, Yanran Wang, Yuyin Zhou, Feifei Wang, Hui Xiong, Fellow, IEEE, and Liangqiong Qu Project page: pengxin-guo.github.io/FLPrivacy 5 2 0 2 M 3 1 ] . [ 1 4 1 5 1 1 . 3 0 5 2 : r AbstractFederated Learning (FL) has emerged as promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generationbased GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks. Index TermsFederated Learning, Data Privacy, Gradient Inversion Attacks. I. INTRODUCTION Pengxin Guo, Runxi Wang, and Liangqiong Qu are with the School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China (e-mail: {guopx, u3637153}@connect.hku.hk, liangqqu@hku.hk). Shuang Zeng is with the Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China (e-mail: zengsh9@connect.hku.hk). Jinjing Zhu and Hui Xiong are with the Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China (email: jzhu706@connect.hkust-gz.edu.cn, xionghui@ust.hk). Haoning Jiang is with the Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China (e-mail: 12210308@mail.sustech.edu.cn). Yanran Wang is with the Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA (e-mail: joycewyr@stanford.edu). Yuyin Zhou is with the Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA (e-mail: yzhou284@ucsc.edu). Feifei Wang is with the Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China, and also with the Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China (e-mail: ffwang@eee.hku.hk). Pengxin Guo and Runxi Wang contributed equally to this work. Corresponding author: Liangqiong Qu. EDERATED Learning (FL) [1][3] is framework that enables multiple clients to collaboratively train model the need to disclose their private local data. The without distinctive features of FL make it particularly suitable for developing machine learning models in privacy sensitive scenarios such as healthcare [4][6] and financial services [7], [8]. Although FL is designed to protect data privacy by only sharing model gradients, several studies have shown that attackers can still extract sensitive information about private training data through Gradient Inversion Attacks (GIA) [9], [10], Membership Inference Attacks (MIA) [11], [12], or Property Inference Attacks (PIA) [13], [14]. Among these privacy attacks, GIA, which focuses on reconstructing input data via either an honest-but-curious server that follows the FL protocol but is interested in uncovering input data [9], [10], [15], or malicious server that may modify the model architecture or parameters sent to the user [16][18], has emerged as the most powerful and is the focus of this work. Many GIA methods have been proposed, but detailed analysis, evaluation, and summary of these methods is still lacking. Although there are various surveys [19][25] summarizing existing privacy attacks in FL, few studies have conducted extensive experiments to reveal the effectiveness of GIA and their associated limiting factors in this context. Among the limited research on this topic, the works [10], [26], [27] are the most closely related to ours. However, these methods focus solely on analyzing partial GIA methods, leaving the applicability of their findings to other types of GIA uncertain. To fill this gap, we aim to conduct comprehensive study of GIA, including literature review, categorization of existing methods, in-depth analysis, and extensive evaluation. Specifically, we first undertake systematic review of current GIA methods and divide them into three categories (Section II): (1) Optimization-based GIA (OP-GIA): OP-GIA works by minimizing the distance between the received gradients and the gradients computed from dummy data [9], [15], [28][33]. (2) Generation-based GIA (GEN-GIA): GEN-GIA utilizes generator to reconstruct input data [34][41]. (3) Analyticsbased GIA (ANA-GIA): ANA-GIA aims to recover input data in closed form using malicious server [16][18], [42][44]. Additionally, based on the different optimization components of the GEN-GIA methods, they can be further categorized into (i) optimizing latent vector [34][36], (ii) optimizing gener00000000/00$00.00 2021 IEEE JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 TABLE COMPARISON OF DIFFERENT TYPES OF GIA METHODS IN TERMS OF INFLUENCE FACTORS, RECONSTRUCTION RESULTS, AND EXTRA RELIANCE OF EACH TYPE OF METHOD. INFLUENCE FACTORS INCLUDE BATCH SIZE, IMAGE RESOLUTION, MODEL TRAINING STATE, THE NUMBER OF THE SAME LABELS IN ONE BATCH DATA, NETWORK ARCHITECTURE, AND PRACTICAL FEDAVG WITH MULTIPLE LOCAL TRAINING STEPS. RECONSTRUCTION RESULTS INCLUDE WHETHER THE RECONSTRUCTION RESULTS ARE THE ORIGINAL INPUTS AND THE VISUAL QUALITY OF THE RECONSTRUCTION RESULTS. Taxonomy Influence Factors Reconstruction Results Extra Reliance Batch Size Image Resolution # Same Label Model Training State Network Architecture Practical FedAvg Original Inputs? Visual Quality OP-GIA - GEN-GIA Opti. Lat. Vec. Opti. Gen. Para. Train. Inv. Mod. ANA-GIA Manip. Mod. Arch. Manip. Mod. Para. Yes No Yes Yes Yes Yes Low High Middle Low High Middle No Trained Generator Sigmoid Activation Auxiliary Dataset Malicious Server Malicious Server ators parameters [37][39], and (iii) training an inversion generation model using an auxiliary dataset [40], [41]. Based on the type of modification, the ANA-GIA methods can be further categorized into (i) manipulating model architecture [16], [18], and (ii) manipulating model parameters [17], [42] [44]. Moreover, we conduct comprehensive analysis and evaluation of the three types of GIA in FL, with the goal of providing actionable insights into the factors that influence their performance, practicality, and potential risks. Our study is structured to address the following research questions, progressing from foundational factors to comparative analysis of threats, and finally focusing on strategies for efficient fine-tuning and their implications: R1. What are the crucial factors that impact the performance of different GIA methods and their associated underlying mechanisms? R2. Among all types of GIA with both honest-but-curious and malicious adversaries, which type is the most practical and poses the greatest threat to FL? R3. With Parameter-Efficient Fine-Tuning (PEFT) technologies being widely used in FL to fine-tune foundation models [45][47], what is the potential privacy leakage of FL under PEFT? To answer the first two research questions, we conduct extensive experiments on the three types of GIA to uncover the factors that influence GIA performance (Section III). Our results, summarized in Table I, reveal that: (I) OP-GIA is the most practical attack setting, but the performance is not satisfactory. Specifically, OP-GIA relies on minimal assumptions (i.e., NO extra reliance), making it the most practical attack setting among the three types of GIA. However, it is influenced by common FL training parameters, making it challenging to achieve satisfactory attack performance. Moreover, practical FedAvg [1], where clients train the model locally for multiple iterations before sending updates, itself has the ability to resist OP-GIA (Section III-B). (II) GEN-GIA has many dependencies, making it pose minimal threat to FL. Specifically, some GEN-GIA methods (i.e., optimizing latent vector z) can only achieve semanticlevel recovery and heavily rely on the pre-trained generator. Other GEN-GIA methods (i.e., optimizing generators parameters and training an inversion model) can perform pixel-level attacks, but they have strong dependencies, such as reliance on the Sigmoid function and an auxiliary dataset (Section III-C). (III) ANA-GIA can achieve satisfactory attack performance but is easily detected and defended against by clients. Specifically, while ANA-GIA can achieve satisfactory performance by manipulating model architecture or parameters, the modifications it makes to the network structure make it easily detectable and defendable by clients, thus rendering it impractical (Section III-D). For the research question R3, we construct the attack methods for FL under PEFT (Section II-D) and evaluate the privacy leakage (Section III-E). Our experimental results reveal that: (IV) Attackers can breach privacy on low-resolution images but fail with high-resolution ones under PEFT. Specifically, as shown in Figure 12, the attackers achieve relatively good performance on CIFAR-10, CIFAR-100, and CelebA with small resolution but perform poorly on ImageNet with large resolution (Section III-E). Based on our experimental findings, we provide threestage defense pipeline for users when designing FL frameworks and protocols for better privacy protection: (1) avoid the Sigmoid activation function and use more complicated network architectures during network design, (2) adopt larger batch sizes and multi-step local training in the local training protocol, and (3) implement client-side validation to check for any potential malicious modification to the model architecture and parameters upon receiving the model from the server. By following this pipeline, users can better protect their data privacy when using FL without worrying about being attacked by current GIA methods. We summarize our contributions as follows. We undertake systematic review of GIA and categorize existing methods into three types: optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA) (Section II). We also provide public repository to continually track developments in this fast-evolving field: Awesome-GradientInversion-Attacks. We introduce an error bound analysis (Theorem 1) for data reconstruction in OP-GIA, which, for the first time, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 Optimization-based GIA (OP-GIA II-A) DLG [9], iDLG [28], SAPAG [49], IG [15], Geng et al. [50], GradInversion [29], CAFE [51], APRIL [52], GradViT [30], Dimitrov et al. [53], ROG [32], CPA [31], iLRG [54], Wang et al. [55], HFG [33], SEER [56], MGIC [57], AFGI [58], GI-SMN [59], GI-NAS [60], DLG-FB [61], TGIAs-RO [62]"
        },
        {
            "title": "GIA",
            "content": "Generation-based GIA (GEN-GIA II-B) Optimizing Generators Parameters (II-B2) GRNN [37], CI-Net [38], GIRG [39] Optimizing Latent Vector (II-B1) GIAS [34], GGL [35], GIFD [36] Training an Inversion Generation Model (II-B3) LTI [40], GLFA [41] Manipulating Model Architecture (II-C1) Robbing the Fed [16], LOKI [18] Analytics-based GIA (ANA-GIA II-C) Manipulating Model Parameters (II-C2) Pasquini et al. [42], Fishing [44], Trap Weights [17], MKOR [43] Fig. 1. Taxonomy of existing GIA methods. The existing GIA methods can be divided into three types: optimization-based GIA (OP-GIA), which works by minimizing the distance between received gradients and gradients computed from dummy data; generation-based GIA (GEN-GIA), which utilizes generator to reconstruct input data; and analytics-based GIA (ANA-GIA), which aims to recover input data in closed form. Moreover, GEN-GIA can be further divided into three categories: optimizing the latent vector z, optimizing the generators parameters , and training an inversion generation model. ANA-GIA can be further divided into two categories: manipulating model architecture and manipulating model parameters. theoretically proves that the OP-GIA performance is linearly related to the square root of the batch size and image resolution. Furthermore, we propose gradient similarity proposition (Proposition 1) to investigate the impact of complex FL training parameters, such as model training state and label distributions, on OP-GIA performance (Section II-A). We conduct extensive experiments on the three types of GIA to uncover the factors influencing their performance and many key interesting findings are provided and summarized (Section III). Based on our experimental findings, we provide three-stage defense pipeline for users when designing FL frameworks and protocols for better privacy protection (Section IV). We further investigate the privacy leakage in FL with PEFT in this work, revealing that attackers can breach privacy on low-resolution images but fail with highresolution ones under PEFT (Sections II-D & III-E). II. ANALYSIS OF GRADIENT INVERSION ATTACKS input data using the shared gradients, Gradient Inversion Attacks (GIA) are tailored attacks intended for shared gradients in FL [9]. They aim to reconstruct typically by potential adversary seeking to reconstruct clients private data. Adversaries are classified as honest-but-curious [48], following the FL training protocol while trying to recover private data, or malicious [16], modifying model architecture or parameters to worsen privacy leakage. We divide existing GIA methods into three categories: optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analyticsbased GIA (ANA-GIA), as illustrated in Figure 1. A. Optimization-based GIA Optimization-based GIA (OP-GIA) typically operates under the threat model of an honest-but-curious server to recover private data from clients [9]. It aims to reconstruct input data by initializing random dummy data and minimizing the distance between the received gradients and those computed Algorithm 1 Optimization-based GIA Input: Leaked gradients θL(x, y), model weight θ, loss function L, distance metric D(, ), regulation term ω(), regularization coefficient λ, optimization learning rate η, and optimization iterations I. Output: Restored batch data ( ˆx, ˆy). 1: Label Restoration: θL(x, y) ˆy; 2: Initialize ˆx0; 3: for = 1 to do 4: 5: 6: end for 7: ˆx = ˆxI ; 8: return ( ˆx, ˆy). (ˆxi1) = D(θL(x, y), θL( ˆxi1, ˆy)) + λω( ˆxi1); ˆxi = ˆxi1 η ˆxi1 ( ˆxi1); from the dummy data, process also known as gradient matching [9]. Formally, given neural network parameters θ and the gradients θL(x, y) computed with private data batch (x, y), the attacker randomly initializes (x, y) and aims to solve the following optimization problem: arg min x,y D(θL(x, y), θL(x, y)) + λω(x), (1) where D(, ) denotes distance metric, ω() is regularization term, and λ is regularization coefficient. Since simultaneously optimizing the input and the label in Eq. (1) is challenging [9], [28], [54], this has led to the development of methods that restore the label first, followed by input optimization [28], [29], [54], [55], [63] 1. Then, the whole procedure of OP-GIA can be summarized in Algorithm 1. Since the introduction of GIA in [9], various methods have been proposed to enhance attack performance by designing more powerful distance metrics or incorporating more complex regularization terms. For example, DLG [9] uses ℓ2-distance as D(, ) and does not use regularization term. IG [15] adopts cosine similarity as D(, ) and the Total Variance (TV) as ω(). GradInversion [29] adopts ℓ2-distance as D(, ) and divides ω() into four terms, TV, ℓ2-distance, Batch Normalization (BN) prior on the input x, and group consistency 1Label restoration is not the focus of this work, and we assume that label information is obtainable [15], [28], [29], [54]. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 regularization term. Moreover, CPA [31] uses Independent Component Analysis (ICA) to reconstruct features and aid further reconstruction. ROG [32] employs an encoder to reduce input resolution, shrinking the original optimization space. HFG [33] implements GIA stepwise to enhance reconstruction. Despite the progress, recent studies reveal that OP-GIA is often influenced by various FL training parameters, such as image resolution and batch size [9], [10]. However, comprehensive theoretical understanding detailing the reasons behind OP-GIAs susceptibility to these parameters as well as the extent of their impact (whether linear, exponential, or otherwise), remains unclear. To fill this gap, we provide the following theoretical analysis (the proof is provided in Section I-A in the Supplementary Material) obtained by Algorithm 1, aiming to shed light on the relationship between FL parameters and attack performance. Theorem 1. If is µ strong convex and L-smooth, choose µ+L , then Algorithm 1 obtains ˆx satisfying step-size η the following convergence guarantees: (cid:113) 2 ˆxx2 (1 µ µ + )I ˆx0x2+ (cid:112)2BCHW (µ + L) µ κ, (cid:80)T where C, H, denote the image resolution, is the batch size, and κ is the upper bound of ˆxf ( ˆx) 1 t=1 ˆxft( ˆx)2 for 2, where ft is constructed based on model weights θt at temporal. Here we assume there are communication rounds in FL, and θt denotes the model weights at time t. Remark 1. Intuitively, Theorem 1 provides convergence analysis and details the quantitative relationship between the reconstruction error upper bound and both image resolution and batch size. Specifically, the reconstruction error is linearly related to the square root of the batch size and image resolution. Therefore, as image resolution and batch size increase, the attack performance deteriorates, which is also demonstrated in Section III-B in our experiments and other works [9], [15], [62]. batch contains higher number of identical labels, the gradients become more similar to those of single image from that particular class, further degrading attack performance. These findings are also validated in Section III-B. In summary, Theorem 1 and Proposition 1 offer valuable insights into the factors affecting the performance of OPGIA. Theorem 1 lays strong theoretical foundation for understanding the impact of critical training parameters, such as batch size and image resolution, on given fixed model. In contrast, Proposition 1 delves deeper into the influence of more complex and general parameters on attack performance by comparing the shared gradients of two distinct models. This insight is particularly valuable as it reveals the interplay between the model training state and attack performance, enabling researchers to identify potential vulnerabilities and devise appropriate countermeasures. Together, these theoretical analyses serve as powerful tool for guiding future work on OP-GIA methods, allowing researchers to evaluate the effectiveness and limitations of their models attack performance more comprehensively. B. Generation-based GIA Another type of GIA utilizes generator to reconstruct the input data, which we refer to as generation-based GIA (GEN-GIA) [34][41]. Differing from OP-GIA, which directly optimizes the input data, this type of method uses generator to produce the input data. Based on the different optimization components of these methods, we divide them into the following categories: 1) optimizing the latent vector z; 2) optimizing the generators parameters ; and 3) training an inversion generation model using an auxiliary dataset. 1) Optimizing Latent Vector z: Due to the large search space complicating the optimization process in OP-GIA methods, optimizing the latent vector of pre-trained generator is proposed to mitigate this issue, as it significantly reduces the search space [34][36]. Formally, the optimizing process can be illustrated as: Proposition 1. For any two model weights θt1 and θt2 , if the leaked gradients of different batch data on θt1 are the cardinality of more similar than those on θt2, L(x,j, y,j) < the set {x,j : D(θt1 the set {x,j ϵ} is greater than the cardinality of : L(x,j, y,j) < ϵ} for any and D(θt2 ϵ > 0, then recovering the input data using the leaked gradients by Algorithm 1 on θt1 is harder than on θt2. L(x,i, y,i), θt1 L(x,i, y,i), θt2 i.e., Remark 2. Proposition 1 states that for two different models, if the leaked gradients of different batch data on one model are more similar than those on another, then recovering input data using the formers leaked gradients is much harder. This implies that, in addition to the batch size and image the performance of OP-GIA is also influenced resolution, by the model training state and the label distribution on each batch size. Specifically, well-trained model tends to exhibit more similarity in the gradients of different data points compared to an untrained model, resulting in worse attack performance on the well-trained model. Furthermore, when arg min D(θL(x, y), θL(G(z, y), y)) + λω(G(z, y)), (2) where denotes the latent vector fed into the generator and the dimension is usually small, is pre-trained generator. Here they [34][36] assume the label can be accurately recovered by other method [28], [29], [54], [64]. After reconstructing ˆz using Eq. (2), it is fed into the pre-trained generator to obtain reconstruction images ˆx via ˆx = G( ˆz, y). Despite its promise, there are two limitations for this type of method that cannot be ignored. First, the data distribution of the recovered data should be similar to the data that the generator was pretrained on [35]. Moreover, since the generator is pretrained and fixed, only semantically similar images can be reconstructed [34][36], as illustrated in Section III-C. 2) Optimizing Generators Parameters : Since fixing the generators parameters can only produce semantically similar images, some works attempt to optimize the generators parameters to achieve pixel-level attacks [37][39], resulting in the following optimization problem: JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 arg min D(θL(x, y), θL(G(W ; z, y), y)) + λω(G(W ; z, y)), (3) where denotes the parameters of the generator, and denotes the randomly initialized latent vector that remains fixed during training. Differing from the optimization of the latent vector in Eq. (2), the generators parameters are optimized in Eq. (3), allowing for pixel-level similar image reconstruction, as demonstrated in Section III-C. However, such methods heavily rely on the activation functions of the target model [38]. Specifically, this approach only works when the target model adopts the Sigmoid activation function and fails with other activation functions, as validated in Section III-C. This strong dependence limits its practicality. 3) Training an Inversion Generation Model: Unlike the previous two types of GEN-GIA methods that rely on gradient matching, another approach to utilizing the generator model is to train an inversion generation model to generate the input data [40], [41]. This involves optimizing the models parameters to learn the mapping from given set of gradients or other information to reconstruct the original input data. For example, Wu et al. [40] propose Learning to Invert (LTI), which trains an inversion model to reconstruct training samples from their gradients with the assistance of an auxiliary dataset. Xue et al. [41] introduce FGLA, which first extracts the features of each sample in batch and then directly generates the user data based on trained generator. Both methods can recover the input images directly based on the given gradients by solving an inverse problem. However, these methods require an auxiliary dataset to train the inversion model, which poses significant limitation. Fig. 2. Reconstruction results of the linear layer. Neuron is activated by single image, resulting in an accurate reconstruction, while neuron j, activated by two images, leads to reconstruction that is combination of both images. Based on the type of modification, these ANA-GIA methods can be categorized into 1) manipulating model architecture [16], [18] and 2) manipulating model parameters [17], [42] [44]. 1) Manipulating Model Architecture: Fowl et al. [16] first introduce Robbing the Fed which inserts specifically designed linear layer at the networks outset to exactly recover the input data, called binning. This layer, formulated as (x) = ReLU(Wx+b), uses weights to measure known continuous cumulative density function (CDF) of the input data (e.g., image brightness), with each neurons bias acting as cutoff. The goal of this method is to ensure that only one input activates each bin, where the bin is defined as the neuron with the largest cutoff that is activated. With welldesigned weights and biases b, the original input xi can be reconstructed as: (cid:16) (cid:17) (cid:17) (cid:16) i i+1 bi bi+1 , xi = (4) C. Analytics-based GIA In contrast to the OP-GIA and GEN-GIA that recover input data using gradient matching or inversion function, analyticsbased GIA (ANA-GIA) aims to reconstruct the input data in closed form [16][18], [42][44], [63], [65]. These approaches leverage the characteristic of the linear layer, where the linear composition of the input can be calculated based on the gradients of the weight and bias [65]. Specifically, consider linear layer = + b, where is weight matrix, is bias, and is the layers input. When only single image xi activates neuron i, the input xi can be directly computed as xi = iL biL. Here is the loss function, denotes entry-wise division, is the activated neuron, xi is the input that activates neuron i, and iL, biL are the weight gradient and bias gradient of the neuron respectively. If the linear layer is at the beginning of network, the reconstructed data will be the original input. However, exact reconstruction only occurs when single data sample activates neuron. If multiple inputs activate the same neuron, the reconstruction becomes combination of all activating images, as illustrated in Figure 2. To address this issue, malicious adversary is employed to manipulate model architecture or parameters, ensuring that each neuron is activated by only single sample whenever possible, thereby facilitating such attacks [16][18], [42][44]. where is the i-th row of W, is the activated bin and i+1 is the bin with the next higher cutoff bias. The proposition 1 in [16] (also provided in Section II in the Supplementary Material) shows that the number of exactly recovered data relates to the relationship between number of print bins and batch size B. However, if we choose relatively large number of bins, the influence of batch size becomes minimal, as shown in Table IV in our experiments. Later, Zhao et al. [18] propose LOKI, which inserts an attack module at the start of model. The module consists of convolutional layer followed by two fully connected layers, designed to perform successful attacks even under secure aggregation. LOKI sends customized convolutional kernels to each client as an identity mapping set, allowing the server to separate weight gradients from the clients despite the use of secure aggregation. The server then uses these weight gradients to reconstruct the original data points. Although these methods can achieve exact recovery, they involve modifications to the model architecture, making them easily detectable and defensible by clients [56]. 2) Manipulating Model Parameters: Since modifying model architecture is easily detectable, some works propose manipulating only the model parameters to facilitate such attacks [17], [42][44]. These methods can be understood as attacks based on gradient sparsity. In this context, while the aggregation protocol nominally runs as intended, all but JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 one of the data points return zero gradient for certain model parameters, allowing the averaging process to still produce these entries directly [44]. Formally, for batch of data = {x1, x2, ..., xB} and the corresponding labels = {y1, y2, ..., yB}, the gradients of the loss function with respect to the model parameters θ on this batched data are: where denotes generator parameterized by as used in GEN-GIA (Section II-B2), θL represents the parameters of the LoRA matrices, and ω() is the regularization term. The regularization term consists of two components: 1) Total Variation, which enhances the quality of reconstructed images, and 2) Patch Consistency, which addresses the misalignment of patches caused by the Vision Transformer [30]. θL (x, y) = = (cid:88) θL (cid:0)xk, yk(cid:1) k=1 θL (cid:0)xt, yt(cid:1) +"
        },
        {
            "title": "1\nB",
            "content": "θL (cid:0)xk, yk(cid:1) . (cid:88) k=1,k=t (5) By deliberately modifying the model parameters θ, the malik=1,k=t θL (cid:0)xk, yk(cid:1) 0. Thus, it cious server can make (cid:80)B can achieve: (6) θL (x, y) θL (cid:0)xt, yt(cid:1) ."
        },
        {
            "title": "1\nB\nAs a result, the gradient of a single data point is “isolated”\nfrom the large batch, breaking the aggregation. For exam-\nple, Pasquini et al. [42] propose distributing different model\nweights to different users to tamper with the updates so that\ntheir aggregation will leak information about the update of\na target user. Wen et al. [44] introduce Fishing, in which\nthe malicious server artificially decreases the confidence of\nthe network’s predictions for the target class, significantly in-\ncreasing the contribution of the gradient information calculated\nfrom data belonging to the selected class. Boenisch et al.\n[17] propose Trap Weights, which re-scale components in the\nmodel’s weight matrix to extract individual training data points\nfrom a chosen subset of participating users. These methods\nshare a similar idea: by manipulating model weights sent to\nthe client to isolate a data point from batch data. However, they\ncan only reconstruct the original input data when the first layer\nof the target model is a fully connected layer; otherwise, they\ncan only isolate the gradients of a single data point and must\nbe combined with OP-GIA methods to continue recovering\n[17], [44]. Moreover, due to the modifications to the model\nparameters, they can also be detected by clients [56].",
            "content": "D. Attacks under Parameter-Efficient Fine-Tuning The aforementioned works have primarily focused on the traditional scenario where clients share the gradients of the entire model with the server. In this section, we consider more prevalent situation where clients fine-tune foundation model using Parameter-Efficient Fine-Tuning (PEFT) technologies, such as Low-Rank Adaptation (LoRA) [66]. These approaches involve sharing only the gradients of small subset of parameters with the server, and there is lack of research on privacy leakage analysis in these scenarios. To fill this gap, we will explore these issues in this work. When client model is fine-tuned using LoRA, we can adopt the following optimization problem to recover the input data: arg min D(θL L(x, y), θL L(G(W ; z), y)) + λω(G(W ; z)), (7) E. Extension to Practical FedAvg The aforementioned works focused on reconstructing raw data from known gradients in ideal settings, rather than considering practical environments in production FL. Specifically, these studies often assume training with FedSGD, where clients compute gradient update on single local batch of data and then send it to the server. In contrast, real-world FL applications typically build on FedAvg [1], where clients train the model locally for multiple iterations before sending updates. Under FedAvg, reconstructing clients private data becomes significantly more challenging [53], [67], which is also demonstrated in our later evaluation section. III. EVALUATION In this section, we provide comprehensive evaluation of privacy leakage in FL across various types of GIA to answer the following research questions: R1. What are the crucial factors that impact the performance of different types of GIA? R2. Among all types of GIA, which type is the most practical and poses the greatest threat to FL? R3. Whats the privacy leakage of FL under PEFT? To answer the research question R1, we divide the influence factors into two types: data level and model level. At the data level, we investigate the influence of batch size, image resolution, and the number of the same labels within one batch on the performance of each type of GIA. At the model level, we explore the influence of model training state and network architectures on the performance of each type of GIA. For the research question R2, we utilize the additional reliance on each type of GIA to measure its practicality and combine this with the reconstruction results to demonstrate its threat to FL. For the research question R3, we test the privacy leakage of FL under LoRA fine-tuning. In the following sections, we examine each type of GIA separately in terms of their influence factors, practicality, and threats to FL. Then, we discuss them collectively and provide guidance for users on designing FL models and training protocols to enhance data privacy protection. A. Experimental Setup We evaluate the privacy leakage of FL in three nature image classification datasets: CIFAR-10 [68], CIFAR-100 [68], and ImageNet [69], and facial image dataset: CelebA [70]. For each dataset, we select subset of 64 images to evaluate the attack performance. There are no same labels in the subset of CIFAR-100, ImageNet, and CelebA. For the CelebA dataset, we choose the images of the first 1,000 celebrities and assign JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 the corresponding number of the celebrity as the label. The illustration of these selected subsets is shown in Figure III.1 in the Supplementary Material. We adopt ResNet-18 [71] as the baseline network, except for the experiments comparing different model architectures and attacks under PEFT. We experiment with both honest-but-curious and malicious adversary settings, and focus on image reconstruction tasks due to its widespread interest. For OP-GIA and GEN-GIA, the adversary is an honest-but-curious server, while for ANAGIA, the adversary is malicious server. For the OP-GIA, we choose IG [15] as the attack method for evaluation. For optimization, we optimize the attack for 24,000 iterations using Adam optimizer [72], with an initial learning rate 0.1. The learning rate is decayed by factor of 0.1 at 3/8, 5/8, 7/8 of the optimization. The coefficient of the TV regularization term is 1e-2 for CIFAR-10 and CIFAR-100, and 1e-6 for ImageNet. For the GEN-GIA, we employ GGL [35], which optimizes the latent vector z, CI-Net [38], which optimizes the generators parameters , and LTI [40], which learns an inversion generation model to evaluate the privacy leakage in FL. For GGL, similar to the experimental settings in [35], we use pre-trained BigGAN [73] as the generator and the gradientfree Covariance Matrix Adaptation Evolution Strategy (CMAES) [74] as the optimizer. For CI-Net, following the settings in [38], we use the CI-Net as the generator and optimize the attack for 2000 iterations using the Adam optimizer with learning rate 1e-3. For LTI, following the settings in [40], we adopt an MLP model as the generator and optimize the attack for 5000 epochs using the Adam optimizer with learning rate 1e-4. For the ANA-GIA, we choose Robbing the Fed [16], which manipulates model architectures, and Fishing [44], which manipulates model parameters, as the attack method for evaluation. For Robbing the Fed, we modify the ResNet18 to include an imprint module with different number of bins in front to perform attacks. For Fishing, we modify the parameters of the ResNet-18 model, enabling the isolation of gradients and the execution of optimization-based attacks on single image. All experiments are conducted on NVIDIA L40S and GeForce RTX 4090 GPUs. The peak signal to noise ratio (PSNR) [75], structural similarity (SSIM) [76], and learned perceptual image patch similarity (LPIPS) [77] are adopted as the metrics for evaluating the attack performance. Lower LPIPS, higher PSNR and SSIM of reconstructed images indicate better attack performance. B. Optimization-based GIA To validate the influence factors of OP-GIA, we evaluate the attack performance of IG [15] on various datasets with different image resolutions and batch sizes. Additionally, we conduct attacks using different network architectures and models at different training states. The reconstruction results are shown in Figures 3(a) and 3(b). Additional evaluation metrics such as PSNR and LPIPS are provided in Figures IV.2 and IV.3 in the Supplementary Material. The visualization of reconstruction results are provided in Section IV-A1 in the Supplementary Material. Based on these results, we conclude that: Takeaway 1: OP-GIA has no additional reliance, but its performance is not satisfactory and is affected by many factors. Larger batch size, higher image resolution, more complicated network architecture, better model training state, and more same labels in one batch lead to worse OP-GIA performance. (a) (b) Fig. 3. (a) Reconstruction results of IG evaluated on models in different training states on various datasets with different image resolutions and batch sizes. (b) Reconstruction results of IG with different network architectures on the CIFAR-100 dataset. The shaded region represents the standard deviation. These results show that larger batch size, higher image resolution, more complicated network architecture, and better model training state lead to worse OP-GIA performance. Larger batch size, worse OP-GIA performance. According to Figure 3(a), we can see that with the increase in batch size, the attack performance decreases, which is consistent the size of the optimization with Theorem 1. Intuitively, problem in Eq. (1) is O(B din), where is the batch size and din denotes the dimensionality of the input data. The difficulty of performing this optimization scales with the dimensionality of the input, preventing OP-GIA from scaling to high-dimensional inputs. Therefore, as increases, the attack performance will decrease [9], [15], [62], [78]. Higher image resolution, worse OP-GIA performance. Comparing the attack performance on CIFAR-10/100 (with resolution 3 32 32) and ImageNet (with resolution 3 224 224) in Figure 3(a), we observe that higher image resolution leads to worse attack performance. This phenomenon is consistent with Theorem 1, and the reasoning is similar to the influence of batch size. Moreover, the image resolution has more significant impact than the batch size. As as shown in Theorem 1, the reconstruction error for OP-GIA is linearly related to the square root of the batch size and image resolution. Since increasing the image resolution involves increasing both the height (H) and width (W ), the reconstruction error can be considered to have linear correlation with the image resolution. This suggests that the image resolution has more significant impact than the batch size. Moreover, the experimental results on ImageNet with different image resolutions in Figure IV.4 in the Supplementary Material further support this point. More complicated network architecture, worse OP-GIA performance. From the results in Figure 3(b), we can observe JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 that as the model architecture becomes more complicated (i.e., transitioning from ResNet-18 to ResNet-101), the attack performance degrades. Notably, when the client adopts ResNet50, the attack effectiveness against batch size of 1 is even worse than using ResNet-18 with batch size of 64. This is because more complicated network architectures can make the optimization process (as expressed in Eq. (1)) more prone to getting trapped in local minima [62]. As result, the attack performance decreases. Better model training state, worse OP-GIA performance. By comparing the attack performance on untrained and welltrained models in Figure 3(a), we observe that the attack performance is worse on the well-trained model. This is because the gradients tend to be more similar for well-trained models, as illustrated in Figure 4, making it difficult to recover private data by comparing gradient values. This observation is consistent with Proposition 1, which states that for two different models, if the leaked gradients of different batch data on one model are more similar than those on another, then recovering input data using the formers leaked gradients is much harder. Fig. 4. t-SNE visualization of gradients of different CIFAR-100 data points on untrained and trained models. It shows that the gradients are more similar for the trained model than the untrained model. More same labels in one batch, worse OP-GIA performance. We find that the attack performance on CIFAR-10 is inferior to that on CIFAR-100 when the batch size is large, as shown in Figure 3(a). Since both datasets share the same resolution, we hypothesize that this is due to the higher number of images with the same label in CIFAR-10. To validate this point, we compare the attack performance with different numbers of images having the same label within batch, and the results are shown in Figure 5. As illustrated in this figure, as the number of images with the same label increases, the attack performance decreases, which is consistent with our hypothesis. To analyze the reasons for this finding, we compare the gradient relationships between these batch images and individual images, which are shown in Table II. Ii denotes that there are identical labels in the batch data. This also corresponds to each batch images from left to right in Figure 5. denotes the j-th image in Ii. From Table II, we can observe that as the number of images with the same label increases, the cosine similarity between the gradients of single image and these batch images also increases (i.e., the first row in Table II). Furthermore, the cosine similarity between the gradients of individual images and the batch images with the same labels are all large (i.e., the last column in Table II). Therefore, we conclude that the reason for the difficulty in recovering batch images with the same labels comes from the similarity of gradients between the individual images and the batch images, which is consistent with the objective Eq. (1) and our gradient similarity Proposition 1. Since the objective function is based on gradient matching, when the gradient between batch images and single image is similar, its hard to distinguish them, which makes recovering the input based on gradient matching more difficult. TABLE II COSINE SIMILARITY BETWEEN PAIRWISE GRADIENTS. Ii DENOTES THAT THERE ARE IDENTICAL LABELS IN THE BATCH DATA. DENOTES THE j-TH IMAGE IN Ii. IT SHOWS THAT AS THE NUMBER OF IMAGES WITH THE SAME LABEL INCREASES, THE COSINE SIMILARITY BETWEEN THE GRADIENTS OF SINGLE IMAGE AND THESE BATCH IMAGES ALSO INCREASES (I.E., THE FIRST ROW). I1 I2 I3 I4 -0.0305 - - - 0.7443 - - - 0.9029 - - - 0.9541 0.9715 0.9623 0.9454 1 1/2/3/4 2 4 3 4 4 4 a) Extension to Practical FedAvg.: Previous evaluations are based on an ideal setting where training is performed using FedSGD where clients compute gradient update on single local batch of data, and then send it to the server. In this part, we evaluate the privacy leakage in practical FedAvg scenario locally for multiple [1] in which clients train the model iterations before sending the updated model back to the server. In this way, the server only observes the aggregates of the clients local updates. The experimental results of attacking practical FedAvg on the CIFAR-100 dataset are provided in Table III. Strong Simulation means that the server knows the local training information (i.e., learning rate, batch size, epochs, etc.) that can simulate the training process at the server side. Weak Simulation denotes that the server does not know the local training information and adopts different training hyperparameters to simulate the local training process. No Simulation means that the server makes no simulation at the server side and considers the local updates as FedSGD gradient to conduct attacks. From these results, we can conclude that practical FedAvg itself has the ability to resist OP-GIA. TABLE III RECONSTRUCTION RESULTS OF IG ON THE CIFAR-100 DATASET UNDER PRACTICAL FEDAVG. DENOTES THE NUMBER OF LOCAL TRAINING EPOCHS, AND REPRESENTS THE BATCH SIZE. THESE RESULTS DEMONSTRATE THAT PRACTICAL FEDAVG ITSELF HAS THE ABILITY TO RESIST OP-GIA. Strong Simulation Weak Simulation No Simulation PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 1 1 1 8 1 16 2 1 2 8 2 16 5 1 5 8 5 16 12.02 14.11 14.50 12.13 12.99 15.17 11.51 14.58 14. 0.3031 0.2049 0.5026 0.1295 0.5050 0.1315 0.3653 0.1701 0.4252 0.1486 0.5261 0.1138 0.3462 0.1816 0.5210 0.1232 0.5253 0.1137 12.26 11.75 12.26 11.08 12.36 13.67 10.93 11.63 12.22 0.3971 0.1522 0.3615 0.1681 0.3971 0.1522 0.2672 0.2120 0.3871 0.1624 0.4502 0.1409 0.2647 0.2363 0.3321 0.1928 0.3486 0.1702 11.40 11.53 11.57 11.30 11.44 11.62 11.48 10.48 11.03 0.2301 0.2252 0.1824 0.2512 0.1503 0.2751 0.2011 0.2372 0.1573 0.2877 0.1375 0.2880 0.1898 0.2482 0.1666 0.3045 0.1311 0.3281 Specifically, as shown in Table III, when the server does JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9 Fig. 5. Reconstruction results of IG on the CIFAR-10 dataset with batch size of 4. From left to right, the number of images with the same label are 0, 2, 3, and 4. The first row represents the ground truth, while the second row shows the reconstruction results. These results indicate that more same labels in one batch lead to worse OP-GIA performance. not have access to the training information on the client (i.e., weak simulation and no simulation), the attack performance is poor. Even when the server has knowledge of the local training information and can precisely simulate the training process on the server side, the reconstruction results are still unsatisfactory. This implies that practical FedAvg itself has the ability to resist OP-GIA. C. Generation-based GIA GEN-GIA can be further divided into three categories: 1) optimizing the latent vector z; 2) optimizing the generators parameters ; and 3) training an inversion generation model using an auxiliary dataset. In this section, we evaluate each type of method separately and then provide an overall observation. 1) Optimizing Latent Vector z: As mentioned above, by optimizing the input latent vector in Eq. (2), GEN-GIA can only generate semantically similar images, not pixellevel reconstructions. This limitation makes numerical metrics such as PSNR, SSIM, and LPIPS unsuitable for evaluating the attack performance. Thus, we primarily rely on visual comparisons. We choose GGL [35] to evaluate the privacy leakage of FL under GEN-GIA when optimizing the latent vector z. Partial reconstruction results are provided in Figure 6, while the complete results can be found in Section IV-B1 in the Supplementary Material. From these results we can conclude that when optimizing latent vector z, GEN-GIA can even generate semantically similar images when using random Gaussian noise instead of real gradients, as long as the label information is available, indicating that it is not affected by the factors influencing OP-GIA. However, it heavily relies on the pre-trained generator and only can achieve semantic-level recovery. As illustrated in Figure 6, the reconstruction results of the GEN-GIA are not influenced by the factors affecting OPGIA. This is likely because GEN-GIA only recovers latent vector and then use this vector along with label information as input to pre-trained generator to produce the reconstructed images via ˆx = G( ˆz, y). Consequently, the inferred label information and the pre-trained generator are crucial, while the obtained gradients are less important. To verify this hypothesis, we replace the real gradients in Eq. (2) with random Gaussian noise for gradient matching. As shown in Figure 6(c), even with random Gaussian noise, as long as the label information the images, is available, supporting our hypothesis. Note that we assume the label is still possible to reconstruct it Fig. 6. Reconstruction results of GGL. (a)-(c) Reconstruction results on the ImageNet dataset: (a) with different batch sizes and model training states; (b) under practical FedAvg scenario; (c) with random Gaussian noise. The ground truth for (b) and (c) is similar to (a) and is omitted. (d) Reconstruction results on the CIFAR-100 dataset with batch size of one and an untrained model. These results show that when optimizing the latent vector z, GEN-GIA can generate semantically similar images and is not affected by the factors influencing OP-GIA. However, it heavily relies on the pre-trained generator and only can achieve semantic-level recovery. information can be accurately recovered by other methods [28], [29], [54], [64], which is consistent with the assumptions in most GEN-GIA methods [34][39]. Moreover, the recovery of label information is not the focus of this work. Additionally, the results for the CIFAR-100 dataset in Figure 6(d) show that the reconstructed images are not even semantically similar to the input data. This suggests that GENGIA heavily relies on the pre-trained generator. In detail, the generator used in our experiment is pre-trained on the ImageNet dataset, which is dissimilar to CIFAR-100 data, resulting in poor reconstruction for CIFAR-100. This underscores the importance of the pre-trained generator for GENGIA. Therefore, despite the advantages of GEN-GIA with optimizing latent vector in not being influenced by many factors, it heavily relies on the pre-trained generator and can only generate data that is semantically similar, rather than reconstructing the original inputs. This implies that GEN-GIA, when optimizing the latent vector z, suffers from significant constraints and poses little threat to FL. 2) Optimizing Generators Parameters : When optimizing the generators parameters , GEN-GIA can reconstruct pixel-wise similar images, thereby enabling pixel-level attacks. We choose CI-Net [38] to evaluate the privacy leakage of FL under GEN-GIA when optimizing the generators parameters . The reconstruction results of CI-Net are provided in Figure 7. Additional evaluation metrics, such as PSNR JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 10 and LPIPS, are provided in Figures IV.12 and IV.13 in the Supplementary Material. More experimental results regarding in varying numbers of samples that share the same label one batch and other activation functions (e.g., Leaky-ReLU, RReLU, and GeLU) are shown in Figures IV.15 and IV.14 in the Supplementary Material. These results show that when optimizing the generators parameters , GEN-GIA can achieve pixel-level attacks, but is affected by the factors that influence OP-GIA. Moreover, it only works when the target model adopts the Sigmoid activation function and fails with other activation functions. (a) (b) Fig. 7. (a) Reconstruction results of CI-Net evaluated on ResNet-18 with different activation functions on various datasets with different batch sizes. (b) Reconstruction results of CI-Net on ImageNet with different resolutions under the Sigmoid activation function. These results show that GEN-GIA with optimizing the generators parameters is affected by the factors that influence OP-GIA. Moreover, it only works when the target model adopts the Sigmoid activation function and fails with other activation functions. As shown in Figure 7(a), CI-Net achieves satisfactory reconstruction results on the model with Sigmoid activation function, while it fails to reconstruct with other activation functions. To explore the reasons behind the vulnerability of the Sigmoid activation function, we start by analyzing to the activation function, as shown in Figure the input the inputs to the activation 8(a). These results show that function mostly lie in the range [2, 2]. Surprisingly, the Sigmoid activation function is approximately linear within this range, while other activation functions (e.g., ReLU and Tanh) are not, as illustrated in Figure 8(b). Thus, we attribute the vulnerability of the Sigmoid activation function to its local linearity. Specifically, the data transformation under the Sigmoid activation function is linear, whereas it is non-linear for other activation functions, making it more vulnerable under the Sigmoid activation function. parameters can achieve pixel-level attacks, it only works when the target model adopts the Sigmoid activation function and fails with other activation functions. Since most current models rarely utilize Sigmoid activation functions, this type of attack poses little threat to FL. 3) Training an Inversion Generation Model: Here, we choose LTI [40] to evaluate the privacy leakage of FL under GEN-GIA with training an inversion generation model. Reconstruction results are shown in Figure 9. Additional evaluation metrics, such as PSNR and LPIPS, are provided in Figures IV.16 and IV.17 in the Supplementary Material. Reconstruction results for the varying numbers of samples that share the same label in one batch are provided in Figure IV.18 in the Supplementary Material. From these results, we can conclude that when training an inversion generation model, GEN-GIA can achieve pixel-level attacks but is influenced by most of the factors that affect OP-GIA, except for the models training state. Moreover, such paradigm relies on an auxiliary dataset with data distribution similar to the local data to train the inversion model [40], which is difficult to achieve in real-world applications. (a) (b) (a) Reconstruction results of LTI evaluated on different models Fig. 9. with different training states on CIFAR-10 with different batch sizes. (b) Reconstruction results of LTI on different datasets with different resolutions on LeNet. These results show that when training an inversion generation model, GEN-GIA can achieve pixel-level attacks but is influenced by most of the factors that affect OP-GIA, except for the models training state. In summary, combining all the experimental results of GENGIA, including optimizing of latent vector and generators parameters , and training an inversion generation model using an auxiliary dataset, we conclude that: Takeaway 2: GEN-GIA has many dependencies, which makes it pose minimal threat to FL. Some GEN-GIA methods (i.e., optimizing latent vector z) can only achieve semantic-level recovery and heavily rely on the pre-trained generator. Other GEN-GIA methods (i.e., optimizing generators parameters and training an inversion model) can perform pixellevel attacks, but they have strong dependencies, such as reliance on the Sigmoid function and an auxiliary dataset. (a) (b) Fig. 8. (a) Distributions of inputs to different activation functions. It shows that the inputs mostly fall within the range of [-2, 2]. (b) Data transformation of different activation functions under the input range [-2, 2]. It shows that the Sigmoid activation function is approximately linear within this range, whereas other activation functions are not. Therefore, although GEN-GIA optimizes the generators D. Analytics-based GIA The success of ANA-GIA relies on malicious server that alters the model architecture or the model parameters sent to the client. In this section, we use Robbing the Fed [16], JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11 which manipulates model architecture, and Fishing [44], which manipulates model parameters, as examples to demonstrate the attack performance of ANA-GIA. 1) Manipulating Model Architecture: Since ANA-GIA with manipulating model architecture can achieve exact recovery, resulting in reconstruction results that are exactly the same as the original images, we choose the number of reconstruction images as the metric here. The experimental results of Robbing the Fed [16] are illustrated in Table IV. Reconstruction results for the varying numbers of samples that share the same label in one batch are provided in Figure IV.19 in the Supplementary Material. These results show that ANA-GIA, when manipulating model architecture, can achieve great attack performance irrespective of batch size, image resolution, model training state, or the number of same labels in one batch, provided it adopts relatively large number of bins. TABLE IV THE NUMBER OF RECONSTRUCTION IMAGES OF ROBBING THE FED WITH 1000 BINS ON DIFFERENT BATCH SIZES. IT SHOWS THAT ANA-GIA, WHEN MANIPULATING MODEL ARCHITECTURE, CAN ACHIEVE GREAT ATTACK PERFORMANCE IRRESPECTIVE OF BATCH SIZE, IMAGE RESOLUTION, OR MODEL TRAINING STATE, PROVIDED IT ADOPTS RELATIVELY LARGE NUMBER OF BINS. Batch Size CIFAR-10 CIFAR-100 ImageNet 1 8 32 64 64/64 63/64 60/64 60/ 64/64 64/64 61/64 60/64 64/64 64/64 64/64 61/64 As shown in Table IV, Robbing the Fed achieves impressive attack performance regardless of image resolution or model training state, which aligns with Proposition 1 in [16]. Furthermore, the influence of batch size is minimal if we choose relatively large number of bins, i.e., 1000. This results in the batch size being smaller than the number of bins, thereby minimizing the batch sizes influence. Additionally, unlike the OP-GIA which is affected by the number of same labels in batch, Robbing the Fed is not influenced by this factor, as illustrated in Figure IV.19 in the Supplementary Material. Furthermore, in the case of practical FedAvg scenario [1], the authors further propose sparse variant of Robbing the Fed utilizing two-sided activation function (such as Hardtanh) and weight/bias scaling to maintain the same leakage as in FedSGD [16]. However, despite these advantages, ANA-GIA with manipulating model architecture is easily detectable due to the modifications it makes to the network structure, rendering it impractical [56]. Moreover, introducing linear layer can lead to storage and communication overhead. As shown in Figure 10, increasing the number of perfectly reconstructed images requires increasing the number of imprint bins, which, in turn, necessitates an increase in the number of parameters. From these results, we can see that there exists trade-off between the attack performance and model size overhead in ANA-GIA when manipulating model architecture. 2) Manipulating Model Parameters: As discussed in Section II-C2, ANA-GIA that manipulates model parameters can isolate the gradients of single data point from batch, making its performance independent of batch size, and the Fig. 10. Proportion of exactly reconstructed images for batch size of 64 with different numbers of bins, where the upper horizontal axis represents the number of additional parameters introduced. This indicates that there exists trade-off between attack performance and model size overhead in ANA-GIA when manipulating model architecture. number of same labels in one batch. In this section, we utilize Fishing [44] as the attack method on the ImageNet dataset to examine other impact factors, such as image resolution, model training state, and network architectures. We further utilize several large batch sizes to show that the attack performance is indeed not influenced by batch size. To test the influence of image resolutions, we resize the data in the ImageNet dataset to 64 64 and 128 128 to compare the performance with the original resolution of 224 224. The experimental results of Fishing [44] on the ImageNet dataset are shown in Figure 11. Additional evaluation metrics, such as PSNR and LPIPS, are provided in Figures IV.20 and IV.21 in the Supplementary Material. (a) (b) Fig. 11. Reconstruction results of Fishing on ImageNet with different (a) image resolutions and model training states, and (b) network architectures. These results show that the attack performance of ANA-GIA, which manipulates model parameters, is not affected by batch size but worsens with larger image resolutions, worse model training states, and more complicated model architectures. As the results in Figure 11(a) show, ANA-GIA that manipulates model parameters can achieve satisfactory attack performance regardless of batch size. However, performance decreases with increasing image resolution and from trained to untrained models. The impact of the models training state has an inverse influence compared to OP-GIA, which may be because well-trained model has better feature extraction ability, making it more compatible with the feature-fishing strategy used in the Fishing attack. Moreover, as shown in Figure 11(b), the attack performance will also decrease with more complicated network architectures, which is similar to JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 12 OP-GIA. Additionally, under practical FedAvg scenario [1], performance is maintained because breaking the aggregation makes the batch size equivalent to 1, which eliminates the protection offered by practical FedAvg compared to OP-GIA [44]. Despite achieving satisfactory attack performance, the modifications to the model parameters can also make it detectable and defensible by clients [56]. Moreover, it can only isolate the gradients of single data point from batch and must be combined with OP-GIA methods to continue recovering [17], [44], making it not purely analytics-based method. Based on the aforementioned experimental results and the analysis of ANA-GIA involving the manipulation of model architecture and parameters, we conclude that: Takeaway 3: ANA-GIA can achieve satisfactory attack performance but is easily detected and defended against by clients. E. Attacks under Parameter-Efficient Fine-Tuning In the previous sections, we focused on the traditional scenario where clients share the gradients of the entire model with the server. In this section, we evaluate the potential privacy leakage under Parameter-Efficient Fine-Tuning (PEFT) for foundation models to answer the research question R3. We choose pre-trained ViT [79] as the base models and utilize Low-Rank Adaptation (LoRA) [66] to fine-tune them. In this paradigm, the server receives only the gradients of the LoRA parameters. The reconstruction results using Eq. (7) are shown in Figures 12, while the visualization results are provided in Section IV-D1 in the Supplementary Material. Additional evaluation metrics, such as PSNR and LPIPS, are provided in Figures IV.22 and IV.23 in the Supplementary Material. More experimental results on ImageNet with different image resolutions are shown in Figure IV.24 in the Supplementary Material. From these results, we can conclude that: Takeaway 4: Attackers can breach privacy on lowresolution images but fail with high-resolution ones under PEFT. Moreover, smaller pre-trained models are better at protecting privacy. Specifically, as shown in Figure 12(a), the attackers achieve relatively good performance on CIFAR-10, CIFAR-100, and CelebA with small resolution but perform poorly on ImageNet with large resolution. This indicates that attackers can breach privacy on low-resolution images but fail with highresolution ones. The reconstruction results on ImageNet at different image resolutions, shown in Figure IV.24 in the Supplementary Material, further support this point. Moreover, we evaluate the privacy leakage across different ViT architectures fine-tuned by LoRA, with the results shown in Figure 12(b). According to these results, we find that smaller pre-trained models are better at protecting privacy. This may be because, with smaller pre-trained model, the LoRA parameters are (a) (b) Fig. 12. (a) Reconstruction results of Eq. (7) evaluated on the ViT-base fine-tuned with LoRA on different datasets with different batch sizes. (b) Reconstruction results of Eq. (7) evaluated on different ViT architectures finetuned with LoRA on the CIFAR-100 dataset. These results show that attackers can breach privacy on low-resolution images but fail with high-resolution ones under PEFT. Moreover, smaller models are better at protecting privacy. fewer, resulting in relatively small leaked gradients and less information leakage. A. Discussion IV. OUTLOOK From the evaluation, we understand the influence factors of each type of GIA method and their corresponding practicality and threat to FL. Here, we provide an overall discussion of all GIA methods and offer insights on how to defend against each type of method. OP-GIA is the most practical setting since it has no additional reliance. However, its effectiveness is heavily influenced image by many FL training factors, such as batch size, resolution, the number of same labels in one batch, model training state, and network architectures. To defend against these attacks, we can increase the batch size and choose more complicated network architectures. Furthermore, we find that practical FedAvg with multiple local training steps can inherently resist OP-GIA, indicating that the actual threat posed by OP-GIA to FL is limited. Therefore, it is recommended to perform multiple local training iterations when using the FL algorithm. As for GEN-GIA, when optimizing the latent vector z, it can achieve semantic-level recovery as long as the label information is provided. If users do not care about semantic privacy leakage, they do not need to be concerned about such attacks. When optimizing the generators parameters , although it can achieve pixel-level attacks, it only works when the target model uses Sigmoid activation functions and fails with other activation functions. Users can simply choose different activation functions when designing local models to defend against this attack. The third variance of GENGIA, which involves training an inversion generation model, demands an auxiliary dataset with distribution similar to the local data. This requirement is difficult to satisfy in real applications since the server may not know the distribution of the local data. Furthermore, even with distributionally aligned auxiliary dataset, the performance of such attacks is unsatisfactory. In summary, these constraintsthe generation of semantically similar results, specificity to the Sigmoid activation function, and the unrealistic requirement for an auxiliary datasetsignificantly limit GEN-GIAs real-world threat to FL systems. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13 For ANA-GIA, although it can achieve satisfactory attack performance, both manipulating model architecture and parameters are easily detected by clients. The client only needs to conduct some simple checks when receiving the model sent from the server to defend against such attacks [56]. Overall, current GIA methods all have their limitations, and if users use the FL training protocol carefully, the privacy leakage of local data can be minimized. B. Defensive Guidelines Based on our extensive evaluation and analysis, we propose actionable strategies to protect FL systems against GIA without using complicated defense mechanisms, which usually lead to privacy-utility trade-offs [80]. Specifically, the defender we are discussing here is how to design FL training protocols to safeguard local data privacy. Our recommendations are organized into three-stage defense pipeline. First, for network design, users should avoid adopting the Sigmoid activation function, as it is particularly vulnerable to GEN-GIA. Moreover, employing more complicated network architectures can make many attack methods more challenging. Second, during the local training protocol, users are encouraged to use larger batch size and train locally for multiple steps before sending model updates to the server, as this can also hinder attacks. Third, simple client-side validation should be adopted: when receiving the model from the server, users should verify the model architecture and parameters to avoid being attacked by ANA-GIA. By integrating these practices, users can better protect their data privacy when using FL without worrying about being attacked by current GIA methods. The above guidelines can be summarized into the following key takeaway tips: Takeaway 5: Three-stage defense pipeline: (1) avoid the Sigmoid activation function and use more complicated network architectures during network design, (2) adopt larger batch sizes and multi-step local training in the local training protocol, and (3) implement clientside validation to check for any potential malicious modification to the model architecture and parameters upon receiving the model from the server. C. Implications for Attack Design While this work advocates for privacy protection, the experiments conducted may also provide some inspiration for improving attack strategies. Since attack and defense are interactive games that complement each other, better attack method can promote the development of better defense. For designing attack methods, researchers should not only consider attack performance but also practical applicability. For example, although current ANA-GIA methods can achieve high attack performance, modifications to the model architecture or parameters make them easily detected and defended against by clients, thus limiting their practicality [56]. Moreover, current GEN-GIA methods depend on factors such as pretrained generators, the Sigmoid activation function, and an auxiliary dataset, which limits their practicality. Thus, when designing attack methods in the future, researchers should prioritize their practicality and minimize reliance on external factors, similar to OP-GIA. Furthermore, researchers should pay more attention to practical scenarios like FedAvg, where clients train the model locally for multiple iterations before sending updates, rather than focusing solely on FedSGD. V. CONCLUSION In this work, we conduct comprehensive study on various types of GIA in FL. We thoroughly examine and assess existing GIA methods, dividing them into three categories: optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Theoretically, we provide an error bound analysis for data reconstruction and gradient similarity proposition in the context of OP-GIA. These theoretical analyses serve as powerful tools for guiding future work on OP-GIA methods, allowing researchers to evaluate the effectiveness and limitations of their models attack performance more comprehensively. Empirically, we conduct extensive evaluations, revealing that: (1) OP-GIA is the most practical attack setting, but its performance is not satisfactory; (2) GEN-GIA has many dependencies, posing minimal threat to FL; (3) ANA-GIA can achieve satisfactory attack performance but is easily detected and defended against by clients. Based on these experimental findings, we provide insights on how to defend against GIA in FL. We hope our study can assist researchers in designing more robust FL frameworks to defend against these attacks."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported by National Natural Science Foundation of China (62306253, U24A201397), Early career fund (27204623), and Guangdong Natural Science Fund-General Programme (2024A1515010233)."
        },
        {
            "title": "REFERENCES",
            "content": "[1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. Arcas, Communication-efficient learning of deep networks from decentralized data, in Proc. Int. Conf. Artif. Intell. Stat., 2017, pp. 12731282. [2] O. Gupta and R. Raskar, Distributed learning of deep neural network over multiple agents, J. Network Comput. Appl., vol. 116, pp. 18, 2018. [3] J. Zhang, S. Zeng, M. Zhang, R. Wang, F. Wang, Y. Zhou, P. P. Liang, and L. Qu, Flhetbench: Benchmarking device and state heterogeneity in federated learning, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 12 09812 108. [4] A. Sadilek, L. Liu, D. Nguyen, M. Kamruzzaman, S. Serghiou, B. Rader, A. Ingerman, S. Mellem, P. Kairouz, E. O. Nsoesie et al., Privacy-first health research with federated learning, npj Digital Med., vol. 4, no. 1, p. 132, 2021. [5] R. Yan, L. Qu, Q. Wei, S.-C. Huang, L. Shen, D. Rubin, L. Xing, and Y. Zhou, Label-efficient self-supervised federated learning for tackling data heterogeneity in medical imaging, IEEE Trans. Med. Imaging, 2023. [6] S. Zeng, P. Guo, S. Wang, J. Wang, Y. Zhou, and L. Qu, Tackling data heterogeneity in federated learning via loss decomposition, in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2024, pp. 707717. [7] G. Long, Y. Tan, J. Jiang, and C. Zhang, Federated learning for open Springer, banking, in Federated Learning: Privacy and Incentive. 2020, pp. 240254. [8] P. Chatterjee, D. Das, and D. B. Rawat, Federated learning empowered recommendation model for financial consumer services, IEEE Trans. Consum. Electron., 2023. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14 [9] L. Zhu, Z. Liu, and S. Han, Deep leakage from gradients, in Proc. Int. Conf. Neural Inf. Process. Syst., vol. 32, 2019. [10] Y. Huang, S. Gupta, Z. Song, K. Li, and S. Arora, Evaluating gradient inversion attacks and defenses in federated learning, in Proc. Int. Conf. Neural Inf. Process. Syst., vol. 34, 2021, pp. 72327241. [11] M. Nasr, R. Shokri, and A. Houmansadr, Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning, in Proc. IEEE Symp. Secur. Priv., 2019, pp. 739753. [34] J. Jeon, K. Lee, S. Oh, J. Ok et al., Gradient inversion with generative image prior, in Proc. Int. Conf. Neural Inf. Process. Syst., vol. 34, 2021, pp. 29 89829 908. [35] Z. Li, J. Zhang, L. Liu, and J. Liu, Auditing privacy defenses in federated learning via generative gradient leakage, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 10 13210 142. [36] H. Fang, B. Chen, X. Wang, Z. Wang, and S.-T. Xia, Gifd: generative gradient inversion method with feature domain optimization, in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 49674976. [12] O. Zari, C. Xu, and G. Neglia, Efficient passive membership inference attack in federated learning, in Proc. Int. Conf. Neural Inf. Process. Syst. Workshops, 2021. [37] H. Ren, J. Deng, and X. Xie, Grnn: generative regression neural networka data leakage attack for federated learning, ACM Trans. Intell. Syst. Technol., vol. 13, no. 4, pp. 124, 2022. [13] Z. Wang, Y. Huang, M. Song, L. Wu, F. Xue, and K. Ren, Poisoningassisted property inference attack against federated learning, IEEE Trans. Dependable Secure Comput., 2022. [14] X. Luo, Y. Wu, X. Xiao, and B. C. Ooi, Feature inference attack on model predictions in vertical federated learning, in Proc. Int. Conf. Data Eng., 2021, pp. 181192. [15] J. Geiping, H. Bauermeister, H. Droge, and M. Moeller, Inverting gradients-how easy is it to break privacy in federated learning? in Proc. Int. Conf. Neural Inf. Process. Syst., vol. 33, 2020, pp. 16 93716 947. [16] L. Fowl, J. Geiping, W. Czaja, M. Goldblum, and T. Goldstein, Robbing the fed: Directly obtaining private data in federated learning with modified models, in Proc. Int. Conf. Learn. Represent., 2022. [17] F. Boenisch, A. Dziedzic, R. Schuster, A. S. Shamsabadi, I. Shumailov, and N. Papernot, When the curious abandon honesty: Federated learning is not private, in Proc. IEEE Eur. Symp. Secur. Priv., 2023, pp. 175199. [18] J. C. Zhao, A. Sharma, A. R. Elkordy, Y. H. Ezzeldin, S. Avestimehr, and S. Bagchi, Loki: Large-scale data reconstruction attack against federated learning through model manipulation, in Proc. IEEE Symp. Secur. Priv. IEEE Computer Society, 2024, pp. 3030. [19] V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and G. Srivastava, survey on security and privacy of federated learning, Future Gener. Comput. Syst., vol. 115, pp. 619640, 2021. [20] K. N. Kumar, C. K. Mohan, and L. R. Cenkeramaddi, The impact of adversarial attacks on federated learning: survey, IEEE Trans. Pattern Anal. Mach. Intell., 2023. [21] C. Zhang, Y. Xie, H. Bai, B. Yu, W. Li, and Y. Gao, survey on federated learning, Knowledge-Based Syst., vol. 216, p. 106775, 2021. [22] R. Zhang, S. Guo, J. Wang, X. Xie, and D. Tao, survey on gradient inversion: Attacks, defenses and future directions, in Proc. Int. Joint Conf. Artif. Intell., 2022, pp. 5678685. [23] J. Wen, Z. Zhang, Y. Lan, Z. Cui, J. Cai, and W. Zhang, survey on federated learning: challenges and applications, Int. J. Mach. Learn. Cybern., vol. 14, no. 2, pp. 513535, 2023. [24] Y. Liu, Y. Kang, T. Zou, Y. Pu, Y. He, X. Ye, Y. Ouyang, Y.-Q. Zhang, and Q. Yang, Vertical federated learning: Concepts, advances, and challenges, IEEE Trans. Knowl. Data Eng., 2024. [25] Y. Shi, O. Kotevska, V. Reshniak, A. Singh, and R. Raskar, Dealing inversion attacks under doubt: Unveiling threat models in gradient federated learning, survey and taxonomy, arXiv:2405.10376, 2024. [26] J. Du, J. Hu, Z. Wang, P. Sun, N. Z. Gong, and K. Ren, Sok: Gradient leakage in federated learning, arXiv:2404.05403, 2024. [27] I. Baglin, X. Zhu, and S. Hadfield, Fedlad: Federated evaluation of deep leakage attacks and defenses, arXiv:2411.03019, 2024. [28] B. Zhao, K. R. Mopuri, and H. Bilen, idlg: Improved deep leakage from gradients, arXiv:2001.02610, 2020. [29] H. Yin, A. Mallya, A. Vahdat, J. M. Alvarez, J. Kautz, and P. Molchanov, See through gradients: Image batch recovery via gradinversion, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 16 337 16 346. [30] A. Hatamizadeh, H. Yin, H. R. Roth, W. Li, J. Kautz, D. Xu, and P. Molchanov, Gradvit: Gradient inversion of vision transformers, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 10 021 10 030. [31] S. Kariyappa, C. Guo, K. Maeng, W. Xiong, G. E. Suh, M. K. Qureshi, and H.-H. S. Lee, Cocktail party attack: Breaking aggregation-based privacy in federated learning using independent component analysis, in Proc. Int. Conf. Mach. Learn., 2023, pp. 15 88415 899. [32] K. Yue, R. Jin, C.-W. Wong, D. Baron, and H. Dai, Gradient obfuscation gives false sense of security in federated learning, in Proc. USENIX Secur. Symp., 2023, pp. 63816398. [38] C. Zhang, Z. Xiaoman, E. Sotthiwat, Y. Xu, P. Liu, L. Zhen, and Y. Liu, Generative gradient inversion via over-parameterized networks in federated learning, in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 51265135. [39] E. Sotthiwat, L. Zhen, C. Zhang, Z. Li, and R. S. M. Goh, Generative image reconstruction from gradients, IEEE Trans. Neural Networks Learn. Syst., 2024. [40] R. Wu, X. Chen, C. Guo, and K. Q. Weinberger, Learning to invert: Simple adaptive attacks for gradient inversion in federated learning, in Proc. Uncertain. Artif. Intell., 2023, pp. 22932303. [41] D. Xue, H. Yang, M. Ge, J. Li, G. Xu, and H. Li, Fast generation-based gradient leakage attacks against highly compressed gradients, in Proc. IEEE INFOCOM - IEEE Con. Comput. Commun., 2023, pp. 110. [42] D. Pasquini, D. Francati, and G. Ateniese, Eluding secure aggregation in federated learning via model inconsistency, in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., 2022, pp. 24292443. [43] F. Wang, S. Velipasalar, and M. C. Gursoy, Maximum knowledge orthogonality reconstruction with gradients in federated learning, in Proc. Winter Conf. Appl. Comput. Vis., 2024, pp. 38843893. [44] Y. Wen, J. A. Geiping, L. Fowl, M. Goldblum, and T. Goldstein, Fishing for user data in large-batch federated learning via gradient magnification, in Proc. Int. Conf. Mach. Learn., 2022, pp. 23 668 23 684. [45] Y. Sun, Z. Li, Y. Li, and B. Ding, Improving lora in privacy-preserving federated learning, in Proc. Int. Conf. Learn. Represent., 2024. [46] Y. Yang, G. Long, T. Shen, for adapter Dual-personalizing arXiv:2403.19211, 2024. J. Jiang, and M. Blumenstein, foundation models, federated [47] P. Guo, S. Zeng, Y. Wang, H. Fan, F. Wang, and L. Qu, Selective aggregation for low-rank adaptation in federated learning, arXiv:2410.01463, 2024. [48] O. Goldreich, Foundations of cryptography: volume 2, basic applications. Cambridge university press, 2009. [49] Y. Wang, J. Deng, D. Guo, C. Wang, X. Meng, H. Liu, C. Ding, and S. Rajasekaran, Sapag: self-adaptive privacy attack from gradients, arXiv:2009.06228, 2020. [50] J. Geng, Y. Mou, F. Li, Q. Li, O. Beyan, S. Decker, and C. Rong, Towards general deep leakage in federated learning, arXiv:2110.09074, 2021. [51] X. Jin, P.-Y. Chen, C.-Y. Hsu, C.-M. Yu, and T. Chen, Cafe: Catastrophic data leakage in vertical federated learning, in Proc. Neural Inf. Process. Syst., vol. 34, 2021, pp. 9941006. [52] J. Lu, X. S. Zhang, T. Zhao, X. He, and J. Cheng, April: Finding the achilles heel on privacy for vision transformers, in IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 10 05110 060. [53] D. I. Dimitrov, M. Balunovic, N. Konstantinov, and M. Vechev, Data leakage in federated averaging, Tran. Mach. Learn. Res., 2022. [54] K. Ma, Y. Sun, J. Cui, D. Li, Z. Guan, and J. Liu, Instance-wise batch label restoration via gradients in federated learning, in Proc. Int. Conf. Learn. Represent., 2023. [55] Y. Wang, J. Liang, and R. He, Towards eliminating hard label constraints in gradient inversion attacks, in Proc. Int. Conf. Learn. Represent., 2024. [56] K. Garov, D. I. Dimitrov, N. Jovanovic, and M. Vechev, Hiding in plain sight: Disguising data stealing attacks in federated learning, in Proc. Int. Conf. Learn. Represent., 2024. [57] C. Liu and J. Wang, Mgic: multi-label gradient inversion attack based on canny edge detection on federated learning, arXiv:2403.08284, 2024. [58] C. Liu, J. Wang, and D. Yu, Raf-gi: Towards robust, accurate inversion attack in federated learning, and fast-convergent gradient arXiv:2403.08383, 2024. [33] Z. Ye, W. Luo, Q. Zhou, Z. Zhu, Y. Shi, and Y. Jia, Gradient inversion attacks: Impact factors analyses and privacy enhancement, IEEE Trans. Pattern Anal. Mach. Intell., 2024. [59] J. Qian, K. Wei, Y. Wu, J. Zhang, J. Chen, and H. Bao, Gi-smn: Gradient inversion attack against federated learning without prior knowledge, in Proc. Int. Conf. Intell. Comput., 2024, pp. 439448. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15 [60] W. Yu, H. Fang, B. Chen, X. Sui, C. Chen, H. Wu, S.-T. Xia, and K. Xu, Gi-nas: Boosting gradient inversion attacks through adaptive neural architecture search, arXiv:2405.20725, 2024. [61] L. Leite, Y. Santo, B. L. Dalmazo, and A. Riker, Federated learning under attack: Improving gradient inversion for batch of images, arXiv:2409.17767, 2024. [62] B. Li, H. Gu, R. Chen, J. Li, C. Wu, N. Ruan, X. Si, and L. Fan, Temporal gradient inversion attacks with robust optimization, IEEE Trans. Dependable Secure Comput., 2025. [63] J. Zhu and M. B. Blaschko, R-gap: Recursive gradient attack on privacy, in Proc. Int. Conf. Learn. Represent., 2021. [64] T. Dang, O. Thakkar, S. Ramaswamy, R. Mathews, P. Chin, and F. Beaufays, Revealing and protecting labels in distributed training, in Proc. Int. Conf. Neural Inf. Process. Syst., vol. 34, 2021, pp. 1727 1738. [65] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai, Privacypreserving deep learning: Revisited and enhanced, in Proc. Appl. Tech. Info. Secur., 2017, pp. 100110. [66] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models, in Proc. Int. Conf. Learn. Represent., 2022. [67] F. Wang, E. Hugh, and B. Li, More than enough is too much: Adaptive defenses against gradient leakage in production federated learning, in Proc. IEEE INFOCOM - IEEE Con. Comput. Commun., 2023, pp. 110. [68] A. Krizhevsky, G. Hinton et al., Learning multiple layers of features from tiny images, 2009. [69] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, pp. 248255. [70] Z. Liu, P. Luo, X. Wang, and X. Tang, Deep learning face attributes in the wild, in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 37303738. [71] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770778. [72] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, in Proc. Int. Conf. Learn. Represent., 2015. [73] A. Brock, J. Donahue, and K. Simonyan, Large scale gan training for high fidelity natural image synthesis, in Proc. Int. Conf. Learn. Represent., 2019. [74] N. Hansen, The cma evolution strategy: tutorial, arXiv:1604.00772, 2016. [75] A. Hore and D. Ziou, Image quality metrics: Psnr vs. ssim, in Proc. Int. Conf. Pattern Recognit., 2010, pp. 23662369. [76] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, Image quality assessment: from error visibility to structural similarity, IEEE Trans. Image Process., vol. 13, no. 4, pp. 600612, 2004. [77] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, The unreasonable effectiveness of deep features as perceptual metric, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 586595. Springer, 1989, vol. [78] A. Torn and A. ˇZilinskas, Global optimization. 350. [79] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, in Proc. Int. Conf. Learn. Represent., 2021. [80] P. Guo, S. Zeng, W. Chen, X. Zhang, W. Ren, Y. Zhou, and L. Qu, new federated learning framework against gradient inversion attacks, arXiv:2412.07187, 2024. [81] S. Bubeck et al., Convex optimization: Algorithms and complexity, Found. Trends Mach. Learn., vol. 8, no. 3-4, pp. 231357, 2015. [82] R. Ge, F. Huang, C. Jin, and Y. Yuan, Escaping from saddle pointsonline stochastic gradient for tensor decomposition, in Proc. Conf. Learn. Theory, 2015, pp. 797842. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Exploring the Vulnerabilities of Federated Learning: Deep Dive into Gradient Inversion Attacks"
        },
        {
            "title": "Supplementary Material",
            "content": "I. THEORETICAL PROOF A. Proof of Theorem 1 For the second term in Eq. (I.1), since the dimension of xf (x) is , where is the batch size, C, H, denote the image resolution, then we have: ˆxi1 ft( ˆxi1)2 we have: Proof. In order to prove Theorem 1, we first assume that there exists temporal model weights θt and leaked gradients θtL(x, y). Consequently, multiple ft() can be constructed, i.e., ft(x) = D(θtL(x, y), θtL(x, ˆy))+λω(x). This assumption is practical since clients usually share gradients with the server multiple times, and any of these leaked gradients can be used to conduct an attack. Then, according to Algorithm 1, we have: ˆxi x2 = ˆxi1 η ˆxi1f ( ˆxi1) x2 = ˆxi1 η ˆxi1f ( ˆxi1) η"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 ˆxi1ft( ˆxi1) + η"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 ˆxi1 η"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 ˆxi1ft( ˆxi1)2 + η"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) ˆxi1 ft( ˆxi1) ˆxi1f ( ˆxi1)2 t=1 = ˆxi1 η ˆxi1 ( ˆxi1)2 ( ˆxi1) ˆxi1 ( ˆxi1)2, + η ˆxi1 where ˆxi ( ˆxi1) = 1 (cid:80)T t=1 ˆxi1 ft( ˆxi1). For the first term in Eq. (I.1), (I.1) ˆxi1 η ˆxi ( ˆxi1)2 2 = ˆxi1 x2 + η2 ˆxi1 2 2η ˆxi1 x, ˆxi1 ( ˆxi1)2 2. ( ˆxi1) Since is Lsmooth and µ strong convex, according to Lemma 3.11 in [81], we obtain: ˆxi1 x, ˆxi1 ( ˆxi1) 1 µ + ˆxi1 ( ˆxi1) 2 + µL µ + ˆxi1 x2 2. Let η (cid:113) 2 µ+L , then we get: ˆxi1 η ˆxi1 ( ˆxi1)2 2 ) ˆxi1 x2 2 + (η2 2 µ + ) ˆxi ( ˆxi1)2 2 ) ˆxi1 x2 2. (1 (1 2µ µ + 2µ µ +"
        },
        {
            "title": "Since",
            "content": "1 1 2 , we get: ˆxi1 η ˆxi1 ( ˆxi1)2 (1 µ µ + ) ˆxi1 x2. (I.2) ˆxi1 ( ˆxi1) ˆxi1 ( ˆxi1)2 ( ˆxi1)2 = ˆxi1 ( ˆxi1) ˆxi1 BCHW (cid:88) (cid:113) [ ˆxi1 ( ˆxi1) ˆxi ( ˆxi1)]2 (j) j="
        },
        {
            "title": "BCHW",
            "content": "(cid:118) (cid:117) (cid:117) (cid:116) BCHW (cid:88) j=1 BCHW κ, [ ˆxi1 ( ˆxi1) ˆxi1 ( ˆxi1)]2 (j) = = where κ is the upper bound of ˆxf ( ˆx) ˆx the inequality is due to CauchySchwarz inequality. Combine Eqs. (I.1), (I.2), and (I.3), and use η (I.3) ( ˆx)2, and (cid:113) 2 µ+L , ˆxi x2 (1 µ µ + ) ˆxi1 x2 + (cid:115) 2BCHW µ + κ. Apply Eq. (I.4) recursively for = 1, . . . , I, we obtain: (cid:112)2BCHW (µ + L) µ )I ˆx0 x2 + µ µ + ˆx x2 (1 (I.4) κ. (I.5) B. Proof of Proposition 1 Proof. Suppose the ground truth data (x, y) and reconstruction results ( ˆx, ˆy) obtained by Algorithm 1 satisfy: D(θL(x, y), θL( ˆx, ˆy)) < ϵ, ϵ > 0. Then, the set of all possible reconstruction results obtained by Algorithm 1 for model parameters θt1 and θt2 can be written as: L( ˆx, ˆy)) < ϵ}, L( ˆx, ˆy)) < ϵ}, ˆA1 = { ˆx : D(θt1 ˆA2 = { ˆx : D(θt2 L(x, y), θt1 L(x, y), θt2 where ˆA1 and ˆA2 represent the sets of all possible reconstruction results obtained by Algorithm 1 for θt1 and θt2, respectively. Then, all the elements in these sets can be considered local minima obtained by Algorithm 1, while only one of them is the optimal solution and the others are not. Thus, the presence of more elements in the set indicates that the reconstruction task is more challenging [82]. L(x,i, y,i), θt1 According to the assumption that : D(θt1 than the cardinality of the cardinality of L(x,j, y,j) < the set {x,j the set {x,j : ϵ} is greater L(x,j, y,j) < ϵ} for any and D(θt2 ϵ > 0, we have ˆA1 > ˆA2, where denotes the cardinality of the set A. This means recovering the input data using the leaked gradients by Algorithm 1 on θt1 is harder than on θt2. L(x,i, y,i), θt2 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17 II. PROPOSITION 1. IN ROBBING THE FED [16] If the server knows the cumulative Proposition 2. density function (assumed to be continuous) of some quantity associated with user data that can be measured with linear function : Rm R, then for batch of size and number of imprint bins > > 2, by using an appropriate combination of linear layer and ReLU activation, the server can expect to exactly recover 1 (cid:18) + 1 (cid:19) 1 B2 (cid:88) i=1 (cid:18) (cid:19) (cid:107) (cid:106) Bi 2 (cid:88) j=1 (cid:18) (cid:19) (cid:18) 1 (cid:19) 1 +r(B, k) (a) PSNR . (b) SSIM . Fig. IV.2. Reconstruction results of IG evaluated on models in different training states on various datasets with different image resolutions and batch sizes, where the shaded region represents the standard deviation. These results show that larger batch size, higher image resolution, and better model training state lead to worse OP-GIA performance. (c) LPIPS . samples of user data (where the data is in Rm) perfectly, where (cid:18) denotes the number of ways to select the bins that (cid:19) have exactly 1 element, and r(B, k) is correction term. III. VISUALIZATION OF SELECTED SUBSET"
        },
        {
            "title": "The illustration of these selected subsets is shown in Figure",
            "content": "III.1. (a) CIFAR-10. (b) CIFAR-100. (c) ImageNet. (d) CelebA. Fig. III.1. Visualization of each subset. We resize the images to 64 64 for the CelebA dataset. The image resolutions for CIFAR-10 and CIFAR-100 are 32 32, while they are 224 224 for ImageNet. IV. MORE EXPERIMENTAL RESULTS A. Optimization-based GIA The reconstruction results of IG with all evaluation metrics are shown in Figures IV.2 and IV.3. These results show that larger batch size, higher image resolution, more complicated network architecture, and better model training state lead to worse OP-GIA performance. (a) PSNR . (b) SSIM . Fig. IV.3. Reconstruction results of IG with different network architectures on the CIFAR-100 dataset, where the shaded region represents the standard deviation. These results show that more complicated network architecture lead to worse OP-GIA performance. (c) LPIPS . The reconstruction results of IG on the ImageNet dataset with different resolutions 2 and batch sizes are shown in Figure IV.4. From these results, we can see that doubling the image resolution leads to larger decrease in attack performance compared to doubling the batch size. This suggests that the image resolution has more significant impact on the performance of OP-GIA than the batch size. (a) PSNR . (b) SSIM . Fig. IV.4. Reconstruction results of IG with different image resolutions and batch sizes on the ImageNet dataset, where the shaded region represents the standard deviation. It shows that the image resolution has more significant impact on the performance of OP-GIA than the batch size. (c) LPIPS . 1) Visualization: The visualization of reconstruction results of IG on the CIFAR-10, CIFAR-100, and ImageNet datasets are shown in Figures IV.5, IV.6, and IV.7, respectively. (a) Batch size = 1. PSNR : 22.03, SSIM : 0.7410, LPIPS : 0.0559. Fig. IV.5. Visualization of reconstruction results of IG on the CIFAR-10 dataset of untrained ResNet-18. (c) Batch size = 64. PSNR : 12.47, SSIM : 0.2985, LPIPS : 0.2110. (b) Batch size = 32. PSNR : 12.59, SSIM : 0.3561, LPIPS : 0.1634. 2We resize the images to different resolutions. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 18 (a) Batch size = 1. PSNR : 21.39, SSIM : 0.7448, LPIPS : 0.0572. Fig. IV.6. Visualization of reconstruction results of IG on the CIFAR-100 dataset of untrained ResNet-18. (c) Batch size = 64. PSNR : 14.99, SSIM : 0.5131, LPIPS : 0.1222. (b) Batch size = 32. PSNR : 16.72, SSIM : 0.5787, LPIPS : 0.1172. (a) Batch size = 1. PSNR : 15.75, SSIM : 0.3979, LPIPS : 0.5933. Fig. IV.7. Visualization of reconstruction results of IG on the ImageNet dataset of untrained ResNet-18. (c) Batch size = 64. PSNR : 12.17, SSIM : 0.3087, LPIPS : 0.7204. (b) Batch size = 32. PSNR : 12.32, SSIM : 0.3136, LPIPS : 0.7005. B. Generation-based GIA 1) Optimizing Latent Vector z: The reconstruction results of GGL are shown in Figures IV.8, IV.9, IV.10, and IV.11. These results show that when optimizing latent vector z, GENGIA can even generate semantically similar images when using random Gaussian noise instead of real gradients, as long as the label information is available, indicating that it is not affected by the factors influencing OP-GIA. However, it heavily relies on the pre-trained generator and only can achieve semantic-level recovery. (a) Batch size = 1. (b) Batch size = 32. Fig. IV.8. Reconstruction results of GGL on the ImageNet dataset of untrained ResNet-18. These results show that the attack performance of GGL is not affected by batch size. (c) Batch size = 64. 2) Optimizing Generators Parameters : The reconstruction results of CI-Net with all evaluation metrics are shown in Figures IV.12, IV.13. Reconstruction results of CI-Net on the CIFAR-100 dataset with batch size of 64 under different numbers of images with the same label within one batch are shown in Figure IV.15. Reconstruction results of CI-Net on the CIFAR-100 dataset under various activation functions (e.g., Sigmoid, ReLU, Tanh, Leaky-ReLU, RReLU, and GeLU) are shown in Figure IV.14. These results show that when optimizing the generators parameters , GEN- (a) Batch size = 1. (b) Batch size = 32. Fig. IV.9. Reconstruction results of GGL on the ImageNet dataset of trained ResNet-18. These results show that the attack performance of GGL is not affected by model training states and batch size. (c) Batch size = 64. (a) Epoch = 1, batch size = 8. Fig. IV.10. Reconstruction results of GGL on the ImageNet dataset of untrained ResNet-18 under practical FedAvg scenario. These results show that the attack performance of GGL is not affected by practical FedAvg scenario. (b) Epoch = 2, batch size = 1. (c) Epoch = 5, batch size = 8. GIA can achieve pixel-level attacks, but is affected by the factors that influence OP-GIA. Moreover, it only works when the target model adopts the Sigmoid activation function and fails with other activation functions. 3) Training an Inversion Generation Model: The reconstruction results of LTI with all evaluation metrics are shown in Figures IV.16 and IV.17. Reconstruction results of LTI for the varying numbers of samples that share the same label in one batch are provided in Figure IV.18. These results show that when training an inversion generation model, GEN-GIA can achieve pixel-level attacks but is influenced by most of the factors that affect OP-GIA, except for the models training state. Furthermore, such paradigm relies on an auxiliary dataset where the data distribution is similar to the local data to train the inversion model, which is difficult to satisfy in real applications. C. Analytics-based GIA 1) Manipulating Model Architecture: Reconstruction results of Robbing the Fed for the varying numbers of samples that share the same label in one batch are provided in Figure IV.19. These results show that the reconstruction results of ANA-GIA with manipulating model architecture are now affected by the number of same labels in one batch. 2) Manipulating Model Parameters: The reconstruction results of Fishing with all evaluation metrics are shown in Figures IV.20 and IV.21. These results show that ANA-GIA which manipulates model parameters can achieve satisfactory attack performance regardless of batch size. However, performance decreases with increasing image resolution, from trained to untrained models, and complicated network architectures. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. IV.11. Reconstruction results of GGL on the ImageNet dataset with batch size of 4. From left to right, the number of images with the same label is 0, 2, 3, and 4. The first row represents the ground truth, while the second row shows the reconstruction results. It shows that the reconstruction results of GGL are not affected by the number of images with the same label within one batch. (a) PSNR . (b) SSIM . Fig. IV.12. Reconstruction results of CI-Net evaluated on ResNet-18 with different activation functions on various datasets with different batch sizes. These results show that GEN-GIA with optimizing the generators parameters is affected by the factors that influence OP-GIA. Moreover, it only works when the target model adopts the Sigmoid activation function and fails with other activation functions. (c) LPIPS . (a) No same label. (b) Randomly selected. (c) All same label. (a) PSNR . (b) SSIM . Fig. IV.13. Reconstruction results of CI-Net on ImageNet with different resolutions under the Sigmoid activation function. These results show that images with lower resolution are relatively easier to reconstruct for CI-Net. (c) LPIPS . D. Attacks under Parameter-Efficient Fine-Tuning The reconstruction results using Eq. (7) evaluated on different ViT architectures fine-tuned with LoRA on different datasets with all evaluation metrics are shown in Figures IV.22 and IV.23. Reconstruction results on ImageNet with different image resolutions are shown in Figure IV.24. These results show that attackers can breach privacy on low-resolution images but fail with high-resolution ones under PEFT. Moreover, smaller pre-trained models are better at protecting privacy. 1) Visualization: The visualization of reconstruction results using Eq. (7) is shown in Figures IV.25, IV.26, IV.27, and IV.28. (a) PSNR . (b) SSIM . Fig. IV.14. Reconstruction results of CI-Net on CIFAR-100 under various activation functions. These results further show that when optimizing the generators parameters , GEN-GIA only succeeds when the target model adopts the Sigmoid activation function. (c) LPIPS . same same (e) Randomly selected. PSNR : 17.87, SSIM : 0.7365, LPIPS : 0.0221. label. (d) No PSNR : 18.15, SSIM : 0.7556, LPIPS : 0.0184. Fig. IV.15. (a)-(c): Original images on CIFAR-100 with varying numbers of samples that share the same label. (d)-(f): Reconstruction results of CI-Net on CIFAR-100 with varying numbers of samples that share the same label in batches of size 64. These results indicate that more same labels in one batch lead to worse CI-Net performance. label. (f) All PSNR : 14.40, SSIM : 0.4712, LPIPS : 0.0769. (a) PSNR . (b) SSIM . Fig. IV.16. Reconstruction results of LTI on CIFAR-10 and different model architectures under different model training states. These results show that more complicated model architecture and better training state lead to worse LTI performance. (c) LPIPS . (a) PSNR . (b) SSIM . Fig. IV.17. Reconstruction results of LTI on various datasets with different resolutions. These results show that larger resolutions lead to worse LTI performance. (c) LPIPS . JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 20 Fig. IV.18. Reconstruction results of LTI on the CIFAR-10 dataset with batch size of 4. From left to right, the number of images with the same label are 0, 2, 3, and 4. The first row represents the ground truth, while the second row shows the reconstruction results. These results indicate that more same labels in one batch lead to worse LTI performance. Fig. IV.19. Reconstruction results of Robbing the Fed on the CIFAR-10 dataset with batch size of 4. From left to right, the number of images with same label is 0, 2, 3, and 4. The first row represents the ground truth, while the second row shows the reconstruction results. These results show that the reconstruction results of ANA-GIA with manipulating model architecture are now affected by the number of same labels in one batch. (a) PSNR . (b) SSIM . Fig. IV.20. Reconstruction results of Fishing on ImageNet with different image resolutions and model training states. These results show that the attack performance of ANA-GIA, which manipulates model parameters, is not affected by batch size but worsens with larger image resolutions and worse model training states. (c) LPIPS . (a) PSNR . (b) SSIM . Fig. IV.21. Reconstruction results of Fishing on ImageNet with different network architectures. These results show that the attack performance of ANAGIA, which manipulates model parameters, is not affected by batch size but worsens with more complicated model architectures. (c) LPIPS . (a) PSNR . (b) SSIM . Fig. IV.22. The reconstruction results using Eq. (7) evaluated on ViT-base fine-tuned with LoRA on different datasets with all evaluation metrics. These results show that attackers can breach privacy on low-resolution images but fail with high-resolution ones under PEFT. (c) LPIPS . (a) PSNR . (b) SSIM . Fig. IV.23. The reconstruction results using Eq. (7) evaluated on different ViT architectures fine-tuned with LoRA on CIFAR-100 with all evaluation metrics. These results show that smaller pre-trained models are better at protecting privacy. (c) LPIPS . (a) PSNR . (b) SSIM . Fig. IV.24. The reconstruction results using Eq. (7) evaluated on different ViT-base fine-tuned with LoRA on ImageNet with different resolutions with all evaluation metrics. These results show that attackers can breach privacy on low-resolution images but fail with high-resolution ones under PEFT. (c) LPIPS . (a) Batch size = 1. PSNR : 13.96, SSIM : 0.5185, LPIPS : 0.0803. Fig. IV.25. Visualization of reconstruction results using Eq. (7) evaluated on the ViT-base fine-tuned with LoRA on the CIFAR-10 dataset. (c) Batch size = 64. PSNR : 12.06, SSIM : 0.3812, LPIPS : 0.1432. (b) Batch size = 32. PSNR : 12.61, SSIM : 0.3886, LPIPS : 0.1334. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 21 (a) Batch size = 1. PSNR : 13.61, SSIM : 0.5539, LPIPS : 0.0781. Fig. IV.26. Visualization of reconstruction results using Eq. (7) evaluated on the ViT-base fine-tuned with LoRA on the CIFAR-100 dataset. (c) Batch size = 64. PSNR : 13.26, SSIM : 0.4918, LPIPS : 0.0965. (b) Batch size = 32. PSNR : 12.71, SSIM : 0.4632, LPIPS : 0.1243. (a) Batch size = 1. PSNR : 12.38, SSIM : 0.3875, LPIPS : 0.3432. Fig. IV.27. Visualization of reconstruction results using Eq. (7) evaluated on the ViT-base fine-tuned with LoRA on the ImageNet dataset. (c) Batch size = 64. PSNR : 11.54, SSIM : 0.3805, LPIPS : 0.4548. (b) Batch size = 32. PSNR : 11.76, SSIM : 0.3874, LPIPS : 0.4295. (a) Batch size = 1. PSNR : 10.27, SSIM : 0.4082, LPIPS : 0.4299. Fig. IV.28. Visualization of reconstruction results using Eq. (7) evaluated on the ViT-base fine-tuned with LoRA on the CelebA dataset. (c) Batch size = 64. PSNR : 8.38, SSIM : 0.3702, LPIPS : 0.5386. (b) Batch size = 32. PSNR : 8.79, SSIM : 0.3820, LPIPS : 0.4950."
        }
    ],
    "affiliations": [
        "Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA",
        "Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA",
        "Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China",
        "Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China",
        "Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China",
        "Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China",
        "School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China",
        "Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China"
    ]
}