{
    "paper_title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for State Space Models",
    "authors": [
        "Wonjun Kang",
        "Kevin Galim",
        "Yuchen Zeng",
        "Minjae Lee",
        "Hyung Il Koo",
        "Nam Ik Cho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning."
        },
        {
            "title": "Start",
            "content": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for State Space Models Wonjun Kang1,2* Kevin Galim2* Hyung Il Koo2,4 Yuchen Zeng3* Nam Ik Cho1 3 UW-Madison Minjae Lee"
        },
        {
            "title": "2 FuriosaAI",
            "content": "5 2 0 2 5 ] . [ 1 9 9 4 3 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, promptbased methods like Prompt Tuning and PrefixTuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/ furiosa-ai/ssm-state-tuning."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have gained significant attention for their strong performance in NLP tasks (Achiam et al., 2023; Brown et al., 2020), but suffer from the quadratic complexity of Transformer architectures (Vaswani et al., 2017). To mitigate this, subquadratic alternatives have gained interest (Katharopoulos et al., 2020; Peng et al., 2023; Sun et al., 2023), with State Space Models (SSMs) emerging as promising solution (Gu and Dao, 2024; Dao and Gu, 2024). Meanwhile, as LLMs scale up, full fine-tuning for downstream tasks becomes prohibitively expensive. Consequently, Parameter-Efficient FineTuning (PEFT) (Houlsby et al., 2019; Hu et al., *Equal contribution. 1 2021; He et al., 2021; Zaken et al., 2022; Liu et al., 2021, 2022; Zeng and Lee, 2024) has emerged, which aims to reduce the number of trainable parameters while achieving adaptation performance comparable to full fine-tuning. However, research on PEFT methods for SSMs remains limited despite their growing popularity. For instance, prompt-based PEFT methods, such as Prompt Tuning (Lester et al., 2021) and PrefixTuning (Li and Liang, 2021), have been widely applied to Transformers but fail to adapt effectively to SSMs (Galim et al., 2024). Therefore, new PEFT strategies tailored to SSMs are needed to fully leverage their architectural properties. To bridge this gap, we introduce state-based PEFT methods that leverage the intrinsic properties of SSMs, offering superior alternative to promptbased methods. Building on this concept, we propose State-offset Tuning. This method directly adjusts the state-related features rather than relying on external prompts, enabling more effective adaptation. In summary, our main contributions are: We introduce state-based methods, new family of PEFT techniques for SSMs, offering superior alternative to prompt-based approaches. We propose State-offset Tuning as new statebased PEFT method. We demonstrate the effectiveness of our method through experiments on variety of datasets, consistently outperforming existing fine-tuning techniques."
        },
        {
            "title": "2.1 State Space Models",
            "content": "Linear State-Space Layers (LSSL) are one of the earliest applications of SSMs in sequence modeling (Gu et al., 2021), leveraging HiPPO (Gu et al., 2020) to initialize the state matrix. However, its PEFT for SSMs Galim et al. (2024) showed that LoRA outperforms prompt-based methods on SSMs. Furthermore, they proposed Selective Dimension Tuning (SDT) for fine-tuning the SSM module while applying LoRA on the linear projection matrices when fine-tuning Mamba models. Yoshimura et al. (2024) suggested new PEFT method called Additional-scan, which increases the hidden state dimension of SSMs, fine-tuning only its additional parameters."
        },
        {
            "title": "3 PEFT Methods on SSMs",
            "content": "SSM Preliminaries Assuming single channel dimension, SSMs such as S4 (Gu et al., 2022) transform signal xt into yt through an Hdimensional latent state ht RH as below: ht = Aht1 + Bxt, yt = Cht, where RH1 controls input influence, RHH governs state dynamics, and R1H maps the state to the output. and represent discretized versions of and B, using learning step size R. In S6 (the SSM module of Mamba), input dependency is integrated by using input-dependent At, Bt, and Ct at every timestep. Specifically, given channels with xt RD, learnable parameters WB, WC RHD, and RDD compute Bt = WBxt, Ct = WCxt, and = Wxt. In this section, we consider S4 for simplicity."
        },
        {
            "title": "3.1 Prompt-based PEFT Methods on SSMs",
            "content": "Prefix-Tuning Can Update Only the Initial State of an SSM Generally, SSMs assume that the initial hidden state is h0 = 0. We can express ht with h0 as ht = (cid:80)t Bixi + h0. ti i=1 Assume we have virtual tokens x(V +1), . . . , x0. If we prepend virtual tokens as prefix to the input sequence, we can write the updated (cid:98)ht as below: (cid:98)ht = ht + (cid:88)V 1 i=0 Bxi = ht + hprefix. By introducing non-zero (cid:98)h0, we can substitute (cid:98)h0 for hprefix making Prefix-Tuning, or optimizing virtual tokens, equivalent to updating the initial state. As optimized virtual tokens only affect the initial state (cid:98)h0, Prefix-Tunings expressivity is upper-bounded by updating the initial state directly. Since Prefix-Tuning is an extended version of Prompt Tuning, this upper bound is applicable to Prompt Tuning as well. 2 Figure 1: Illustration of our proposed State-offset Tuning on Mamba block (Gu and Dao, 2024). State-offset Tuning injects trainable state-offset at each timestep in the SSM module while keeping other parameters frozen, enabling parameter-efficient fine-tuning and improved downstream performance. high computational overhead limits practicality. Gu et al. (2022) introduced Structured State Space Models (S4), which mitigate this by structuring the state matrix. Recently, Mamba (Gu and Dao, 2024; Dao and Gu, 2024) enhanced modeling capabilities by introducing an input-dependent S6 block."
        },
        {
            "title": "2.2 Paramter-Efficient Fine-Tuning",
            "content": "In this section, we review existing PEFT methods. For more details, see Sec. D. Parameter-based Methods One approach to parameter-based PEFT methods is to selectively fine-tune specific layers within the model while keeping the remaining layers frozen. BitFit (Zaken et al., 2022) is lightweight and effective strategy that focuses solely on fine-tuning models bias terms. Furthermore, LoRA (Hu et al., 2021) represents notable parameter-based PEFT method by introducing low-rank matrices for weight updates, facilitating efficient adaptation. Prompt-based Methods Instead of fine-tuning model parameters, Prompt Tuning (Lester et al., 2021) enhances models by prepending trainable soft embeddings to the prompt. Prefix-Tuning (Li and Liang, 2021) builds on this approach by injecting trainable embeddings into each Transformer layer, achieving strong adaptation results for Transformer-based LLMs. Galim et al. (2024) showed Initial State Tuning, an advanced version of Prefix-Tuning, which directly optimizes the channel-specific initial state RH , resulting in DH trainable parameters in total across all channels. The updated output (cid:98)yt for Initial State Tuning can be written as in Table 1."
        },
        {
            "title": "PEFT Methods for SSMs",
            "content": "We define state-based methods as new family of PEFT methods specifically designed for SSMs. These methods directly modify the intrinsic staterelated features within the SSM module. In contrast, prompt-based methods, such as Prefix-Tuning, influence the hidden state of the SSM module indirectly by introducing external virtual tokens. While both approaches adjust the hidden state of the SSM module, state-based methods operate within the SSM module itself, offering more direct and expressive adaptation strategy. Based on our definition, we classify Initial State Tuning as state-based method. While Initial State Tuning surpasses Prefix-Tuning (Galim et al., 2024), it still falls short compared to other finetuning methods on SSMs. To bridge this gap, we propose novel state-based method for enhanced performance. Initial State Tuning State-offset Tuning (h) State-offset Tuning (y) (cid:0)(cid:81)t i=1 Ai (cid:1)h (cid:98)yt = yt + Ct (cid:98)yt = yt + Cth (cid:98)yt = yt + Table 1: State-based Methods for S6. Our methods eliminate the time-dependent coefficient (cid:81)t i=1 Ai, ensuring uniform effect across timesteps. Prompt-based"
        },
        {
            "title": "Timestep T",
            "content": "Timestep +"
        },
        {
            "title": "Suffix",
            "content": "[prefix, x1, . . . , xT ] [prefix, x1, . . . , xT , xT +1] [x1, . . . , xT , suffix] [x1, . . . , xT , suffix, xT +1]"
        },
        {
            "title": "Iterative Suffix",
            "content": "[x1, . . . , xT , suffix] [x1, . . . , xT , xT +1, suffix] Table 2: Comparison of Prefix Tuning, Suffix Tuning, and Iterative Suffix Tuning."
        },
        {
            "title": "4 Proposed State-based PEFT Method",
            "content": "In this section, we propose State-offset Tuning as new state-based PEFT method. visual comparison with Initial State Tuning and Prefix-Tuning is provided in Sec. A."
        },
        {
            "title": "4.1 State-offset Tuning",
            "content": "for S4 and (cid:81)t Initial State Tuning introduces an additional term with coefficient i=1 Ai for S6. However, this coefficient, which varies for each timestep, tends to decrease over time, leading to inconsistent effects. This is related to the issue that SSMs struggle to recall early tokens (Fu et al., 2022). To address this and ensure consistent effect for each timestep, we introduce State-offset Tuning, which eliminates this coefficient. State-offset Tuning adds constant, learnable state-offset to the hidden state before obtaining the updated output (cid:98)yt  (Fig. 1)  . Therefore, unlike Initial State Tuning, State-offset Tuning does not alter the hidden state dynamics directly. Instead, State-offset Tuning adds constant repetitively for each timestep, ensuring uniform impact. We formulate State-offset Tuning (h) for S6 in Table 1, where we optimize RH . In S4, Ct does not depend on the input, simplifying to constant C. This allows us to optimize bias instead of (with := Ch for each dimension). We name this method State-offset Tuning (y). For S4, State-offset Tuning (y) and State-offset Tuning (h) are equivalent. In S6, opting for the simpler Stateoffset Tuning (y) enhances parameter efficiency by decreasing the tunable parameters from DH to D."
        },
        {
            "title": "4.2 Connection to Prompt-based Methods",
            "content": "To further validate the methodology of State-offset Tuning, we examine its connection to prompt-based methods and demonstrate its correspondence to Iterative Suffix-Tuning. Iterative Suffix-Tuning Li and Liang (2021) showed that in Transformers, inserting virtual tokens at the beginning (Prefix-Tuning) or the end (Suffix-Tuning, referred to as Infix-Tuning in their work) yields similar performance. However, for SSMs, the position of the inserted virtual tokens is crucial, as these models tend to forget early tokens. The effect of Prefix-Tuning and Suffix-Tuning diminishes as the model processes subsequent timesteps. This leads to the question: how can we maintain consistent influence of virtual tokens across all timesteps in SSMs? To achieve this, we propose Iterative SuffixTuning. As shown in Table 2, both Prefix-Tuning and Suffix-Tuning hold virtual tokens in fixed positions throughout all timesteps. Conversely, Iterative Suffix-Tuning shifts virtual tokens to the sequences last position at each timestep, ensurModel Size Dataset Type - Method Full Fine-tuning (All) Full Fine-tuning (S6) Parameter based LoRA BitFit Additional-scan Selective Dimension Tuning Prompt based Prompt Tuning Prefix-Tuning State based Initial State Tuning State-offset Tuning (h) State-offset Tuning (y) Mamba 1.4B Spider SAMSum All Easy Medium Hard Extra R1 R2 RL 66.2 56.7 56.3 51.3 26.9 19.8 43.6 39.7 51.8 57.4 53.0 84.3 76.6 75.0 74.2 44.4 38. 65.3 65.7 77.8 77.4 77.4 69.5 57.8 56.5 50.9 25.6 16.6 42.4 38.6 51.1 59.9 55. 53.4 46.0 50.6 43.1 21.3 16.1 33.3 31.0 35.1 44.8 40.8 43.4 34.9 33.7 26.5 10.2 4. 25.3 15.1 32.5 33.7 22.9 51.2 51.1 50.5 50.3 37.6 46.3 50.1 50.6 50.0 50.9 50. 27.3 26.9 26.4 25.7 17.5 21.5 25.6 26.5 26.0 26.5 26.1 42.9 42.2 42.2 41.9 30.9 37. 41.6 42.1 41.3 42.4 42.0 Mamba 130M DART GLUE MET. BLEU Avg. 71.0 70.3 69.9 67.0 60.6 67.5 66.2 66.6 69.1 70.0 66.8 51.8 48. 50.8 43.7 15.8 48.2 39.8 42.5 46.2 47.0 45.2 80.5 79.3 78.3 77.9 62.4 63.7 63.8 68. 77.4 78.5 77.7 Params (%) 100.00 4.31 0.92 0.06 0.68 0.51 0.04 22.69 0.45 0.45 0. Params (%) 100.00 4.46 0.46 0.03 0.34 0.26 0.01 12.81 0.23 0.23 0.01 Table 3: Experimental results of fine-tuning the SSM module (S6) using Mamba (Gu and Dao, 2024) models. We assess Spider and its subsets using execution accuracy, SAMSum with ROUGE-1/2/L scores, DART using METEOR and BLEU scores, and GLUE by calculating the average score. To demonstrate the effectiveness of our methods, we configured the hyperparameters of each method to ensure their parameter budget is comparable to or exceeds that of our methods. Bold and underline indicate the best and the second-best results, respectively, among all methods (excluding full fine-tuning). Our State-offset Tuning (h) outperforms all other methods on most datasets, and our State-offset Tuning (y) shows comparable or better performance than other methods despite its significantly fewer trainable parameters. ing uniform influence in SSMs. This method is akin to how State-offset Tuning eliminates the timevarying coefficient in Initial State Tuning, enforcing consistent effect at every timestep. We show that Iterative Suffix-Tuning in SSMs is equivalent to State-offset Tuning (as detailed in Sec. B)."
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "We conduct experiments for fine-tuning the SSM module (S6) using pretrained Mamba (Gu and Dao, 2024) and Mamba-2 (Dao and Gu, 2024) models on four datasets: Spider (Yu et al., 2018), SAMSum (Gliwa et al., 2019), DART (Nan et al., 2021), and GLUE (Wang et al., 2019). For further information on datasets, evaluation metrics, and experimental details, refer to Secs. and F. We use LoRA (Hu et al., 2021), BitFit (Zaken et al., 2022), Additionalscan (Yoshimura et al., 2024), and Selective Dimension Tuning (Galim et al., 2024) as parameter-based methods. For prompt-based methods, we employ Prompt Tuning (Lester et al., 2021) and PrefixTuning (Li and Liang, 2021). For state-based methods, we utilize Initial State Tuning (Galim et al., 2024), along with our proposed methods, Stateoffset Tuning (h) and State-offset Tuning (y)."
        },
        {
            "title": "5.2 Experimental Results",
            "content": "Table 3 shows the results on Mamba models. More results, including Mamba-2 results, are provided in Sec. G. Furthermore, in Sec. G, we compare the training speed and memory usage of LoRA and State-offset Tuning (h), demonstrating that ours is faster and more memory-efficient than LoRA. State-based Methods Outperform Promptbased Methods Table 3 shows that all statebased methods outperform prompt-based methods, supporting the claim that state-based methods are superior to prompt-based methods on SSMs. In particular, our State-offset Tuning (h) achieves the best results among all tested PEFT methods on most datasets. Our State-offset Tuning (y) outperforms Initial State Tuning on most datasets, using just 0.01% of the parameters compared to 0.23% by Initial State Tuning. State-offset Tuning Outperforms ParameterBased Methods State-offset Tuning (h) outperforms BitFit across all datasets and surpasses LoRA on most datasets. Notably, it also outperforms Additional-scan and Selective Dimension Tuning, which are methods specifically designed for finetuning SSM modules, across all datasets. Furthermore, State-offset Tuning (h) achieves performance comparable to full fine-tuning (S6), highlighting the effectiveness of state-based PEFT for SSM modules, despite using significantly fewer parameters. The results from Mamba-2  (Table 10)  further validate the effectiveness of our method."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce state-based methods as new family of PEFT methods for State Space Models, serving as superior alternative to promptbased methods. We propose State-offset Tuning as new state-based PEFT method and demonstrate its effectiveness through extensive experiments."
        },
        {
            "title": "7 Limitations",
            "content": "While we demonstrate that State-offset Tuning is effective for fine-tuning SSMs in the text domain, its applicability to other domains, such as vision or speech, remains unexplored. Existing PEFT methods, such as LoRA and Prompt Tuning, have been successfully applied across various domains (Jia et al., 2022; Gal et al., 2022; Ran et al., 2024). Extending State-offset Tuning to models in other domains, such as Vision Mamba (Zhu et al., 2024), is an interesting direction for future work. Potential Risks Our approach enables parameterefficient fine-tuning (PEFT) of pretrained SSMs, significantly reducing the computational cost of adaptation. While this is beneficial for resourceconstrained scenarios, it also presents potential risks. Specifically, adversaries could leverage our method to efficiently fine-tune pretrained SSMs on harmful or biased data, enabling the rapid adaptation of models for malicious purposes with minimal computational resources. This could lead to the proliferation of harmful or deceptive models that reinforce misinformation, bias, or toxicity. To mitigate these risks, future work should explore more robust safety measures, such as integrating ethical finetuning constraints and monitoring mechanisms."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan The second pascal recognisSzpektor. 2006. ing textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 1. Citeseer. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth pascal recognizing textual entailment challenge. TAC, 7(8):1. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are fewshot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine learning challenges workshop, pages 177190. Springer. Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized models and efficient algorithms through In International structured state space duality. Conference on Machine Learning. Bill Dolan and Chris Brockett. 2005. Automatically constructing corpus of sentential paraphrases. In Third international workshop on paraphrasing (IWP2005). Daniel Fu, Tri Dao, Khaled Kamal Saab, Armin Thomas, Atri Rudra, and Christopher Re. 2022. Hungry hungry hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. 2022. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618. Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, and Kangwook Lee. 2024. Parameter-efficient fine-tuning of state space models. arXiv preprint arXiv:2410.09016. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 19. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum corpus: humanannotated dialogue dataset for abstractive summarization. EMNLP-IJCNLP 2019, page 70. 5 Albert Gu and Tri Dao. 2024. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. 2020. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems, volume 33, pages 14741487. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. 2021. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Advances in Neural Information Processing Systems, volume 34, pages 572585. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. Towards unified view of parameter-efficient transfer learning. In International Conference on Learning Representations. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pages 27902799. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-rank adaptation of large In International Conference on language models. Learning Representations. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and SerNam Lim. 2022. Visual prompt tuning. In European Conference on Computer Vision, pages 709727. Springer. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 51565165. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 30453059. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. PTuning: Prompt Tuning Can Be Comparable to Finetuning Across Scales and Tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 6168. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT Understands, Too. arXiv:2103.10385. Ilya Loshchilov and Frank Hutter. 2019. coupled weight decay regularization. arXiv:1711.05101. DePreprint, Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Peft: State-of-the-art parameterBossan. 2022. https://github. efficient fine-tuning methods. com/huggingface/peft. Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. 2021. DART: Open-Domain Structured Data Record to Text Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 432447. Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. pages 311318. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. Preprint, arXiv:1912.01703. Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. 2023. RWKV: Reinventing RNNs for the transformer era. In The 2023 Conference on Empirical Methods in Natural Language Processing. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822. Lingmin Ran, Xiaodong Cun, Jia-Wei Liu, Rui Zhao, Song Zijie, Xintao Wang, Jussi Keppo, and Mike Zheng Shou. 2024. X-adapter: Adding universal compatibility of plugins for upgraded diffusion 6 and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 39113921. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 19. Yuchen Zeng and Kangwook Lee. 2024. The expressive power of low-rank adaptation. In International Conference on Learning Representations. Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. 2024. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417. model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87758784. Daniel G. A. Smith and Johnnie Gray. 2018. opt_einsum - python package for optimizing contraction order for einsum-like expressions. Journal of Open Source Software, 3(26):753. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631 1642. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: multi-task benchmark and analysis platform for natural language understandIn International Conference on Learning ing. Representations. Warstadt. 2019. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471. Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. broad-coverage challenge corpus for sentence understanding through inference. Preprint, arXiv:1704.05426. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Masakazu Yoshimura, Teruaki Hayashi, and Yota Maeda. 2024. Mambapeft: Exploring parameterefficient fine-tuning for mamba. arXiv preprint arXiv:2411.03855. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider: large-scale human-labeled dataset for complex 7 Visual Comparison of Prompt-based Methods and State-based Methods Fig. 2 compares prompt-based methods and statebased methods, including our proposed techniques, within the S6 block. Figure 2: Visual comparison of prompt-based methods and state-based methods in the S6 block. State-based Methods Operate within the SSM Module Fig. 2 shows that prompt-based methods, such as Prefix-Tuning, rely on virtual tokens external to the S6 block. In contrast, state-based methods, such as Initial State Tuning, State-offset Tuning (h), and State-offset Tuning (y), directly adjust state-related features within the S6 block. State-offset Tuning affects the Current Timestep Figure 2 illustrates how Prefix-Tuning and Initial State Tuning modify features at early timesteps, indirectly affecting the current state. However, this impact diminishes over time. In contrast, Stateoffset Tuning (h) and State-offset Tuning (y) directly influence the state at each timestep, resulting in more effective adaptation. Output: yt+1 Ct+1 At+1At At+1 Hidden State: ht"
        },
        {
            "title": "Btxt",
            "content": "Bt+1xt+1 (a) Iterative Suffix-Tuning (with + 1 as current timestep) Input: xt+1 Output: yt Ct A 1 t+1 At Hidden State: ht"
        },
        {
            "title": "Btxt",
            "content": "Bt+1xt+1 Input: xt (b) Iterative Suffix-Tuning (with as current timestep) Figure 3: Two different implementations of Iterative Suffix Tuning on SSM. We show that Fig. 3b is equivalent to State-offset Tuning. Iterative Suffix-Tuning on SSMs (S6) with virtual token (suffix) xt+1. Fig. 3a views + 1 as current timestep. In this case, input-dependent Ct+1 = WCxt+1 is determined solely by the suffix xt+1 RD, which is constant at inference time, thus the input dependency of is lost, reducing the expressive power of S6. To address this, we view as current timestep instead and interpret xt+1 as future token (Fig. 3b). Consequently, we time-shift xt+1 by multiplying it with the inverse of At+1. Fig. 3a: yt+1 = Ct+1(At+1ht + Bt+1xt+1), Fig. 3b: yt = Ct(ht + 1 t+1Bt+1xt+1). Therefore, according to the equation corresponding to Fig. 3b, Iterative Suffix-Tuning can be imple1 mented by updating only t+1Bt+1xt+1. Since this term depends solely on the constant suffix xt+1, we can directly replace it with learnable parame1 ter (h := t+1Bt+1xt+1), which is equivelant to State-offset Tuning (h)  (Table 1)  . Iterative Suffix-Tuning and State-offset Low-Rank State-offset Tuning"
        },
        {
            "title": "Tuning",
            "content": "In this section, we show that Iterative Suffix-Tuning for SSMs is equivalent to State-offset Tuning. State-offset Tuning is Iterative Suffix-Tuning Fig. 3 provides two different implementations of State-offset Tuning (h) shows superior parameter efficiency on Mamba versus other PEFT methods. To further reduce trainable parameters, we can represent the learnable state-offset as product of two low-rank matrices, inspired by LoRA (Hu et al., 2021). This is particularly useful for Mamba-2, 8 where the state dimension is larger than in Mamba, leading to an increased number of trainable parameters. In such cases, low-rank techniques can effectively mitigate the parameter overhead. Experimental results of State-offset Tuning (h) with lower rank on Mamba-2 are provided in Sec. G.2."
        },
        {
            "title": "D PEFT Baselines",
            "content": "focused solely on its S6 component (Selective Dimension Tuning) to ensure fair comparison. Additional-scan (Yoshimura et al., 2024). This approach enhances the models expressivity by expanding the state dimensions for A, WB, and WC. During training, only the added dimensions are marked as trainable. In this section, we provide more detailed description of the baseline methods."
        },
        {
            "title": "E Datasets",
            "content": "LoRA (Hu et al., 2021). LoRA aims to fine-tune large models by maintaining the bulk of pretrained parameters untouched while introducing trainable low-rank matrices within each Transformers layer. This method leverages linear algebra principles where large matrix can be effectively approximated by two low-rank matrices, thus reducing the number of parameters. LoRA includes scaling parameter to adjust the influence of original and LoRA weights during training. We use the Hugging Face version (Apache License 2.0, Mangrulkar et al. (2022)) of LoRA for our experiments. Prompt Tuning (Lester et al., 2021). This method involves freezing the entire model and adding trainable soft prompt to the input. The prompt consists of continuous virtual tokens that provide additional context. Prefix-Tuning (Li and Liang, 2021). Similar to Prompt Tuning, Prefix-Tuning adds trainable tokens but extends them across every Transformer layer by appending trainable embeddings to the attention matrices. To combat the instability in training these prefixes, an over-parameterized MLP is utilized, which can be discarded after training. BitFit (Zaken et al., 2022). This PEFT method simplifies fine-tuning by freeing only the bias terms while freezing the other model weights, drastically reducing trainable parameters. SDLoRA (Galim et al., 2024). SDLoRA (LoRA with Selective Dimension Tuning) employs sparse updating approach for the matrices A, B, and (WB and WC for S6), while additionally applying LoRA to the linear projection layers. All remaining layers are kept frozen. The process for determining which parameters to update involves warmup stage, during which parameters are flagged as updatable if they exhibit significant gradient magnitude. In our SDLoRA experiments, we excluded LoRA from the linear projection layers and Dataset #Train #Valid #Epochs Model size Metrics RTE MRPC CoLA SST-2 QNLI QQP MNLI Spider SAMSum DART 2490 3668 8551 67349 104743 363846 392702 6918 14732 277 408 1043 872 5463 40430 19647 1034 819 2768 10 10 10 10 10 3 3 10 10 10 130m 130m 130m 130m 130m 130m 130m 1.4B, 2.8B 1.4B 130m METEOR, BLEU Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. ROUGE Table 4: Dataset details. We report the number of training and validation samples, number of training epochs, employed model size and evaluation metrics. This paper examines four datasets across two domains: Natural Language Understanding (NLU) and Natural Language Generation (NLG). Table 4 presents detailed information for each dataset. GLUE (Wang et al., 2019). benchmark comprising nine tasks in English for assessing language understanding models, including sentiment analysis, linguistic acceptability, and question answering. We use the the following datasets: RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MRPC (Dolan and Brockett, 2005), CoLA (Warstadt, 2019), SST2 (Socher et al., 2013), QNLI (Rajpurkar et al., 2018), QQP1, and MNLI (Williams et al., 2018). Evaluation is mainly through accuracy, except for CoLA where Matthews correlation is used. The final metric is calculated as the average accuracy (Matthews correlation for CoLA) across all datasets. The individual datasets are available under different permissive licenses. We use the version hosted at https://huggingface.co/ datasets/nyu-mll/glue. SAMSum (Gliwa et al., 2019). dataset for dialogue summarization featuring about 16,000 synthetic conversations in English with summaries, created to simulate digital communications with 1https://quoradata.quora.com/ First-Quora-Dataset-Release-Question-Pairs 9 varied tones and styles. Its structure helps in developing systems that process conversational text. The dataset is evaluated via ROUGE score. This dataset is available under the CC BY-NC-ND 4.0 license. We use the version hosted at https:// huggingface.co/datasets/Samsung/samsum. Spider (Yu et al., 2018). text-to-SQL dataset with 10,000 annotated SQL queries across 200+ databases, classifying queries from easy to extra hard based on SQL operation complexity. It involves translating English questions to SQL, evaluated via execution accuracy. Execution accuracy considers the output correct if the models predicted SQL query and the ground truth SQL query yield the same results when executed on the database. This dataset is available under the CC BY-SA 4.0 license. We use the version hosted at https:// huggingface.co/datasets/xlangai/spider. DART (Nan et al., 2021). Comprising over 80,000 instances, DART focuses on English RDFto-text generation, organized by structured data triples and corresponding text summaries. Its assessed using METEOR and BLEU metrics. This dataset is available under the MIT license. We use the version hosted at https://huggingface.co/ datasets/Yale-LILY/dart."
        },
        {
            "title": "F Experimental Details",
            "content": "For every dataset, we select the model size based on how difficult the dataset is and conduct brief grid search for one epoch using subset of the data (1k-2k instances) with learning rates of {4 101, 2 101, 1 101, ..., 1 105}. The best learning rate is then selected as the rate that has the lowest training loss. In our experimental results, we report the metric from the best epoch observed on the validation set during training, employing early stopping. Each experiment is conducted once. We apply fine-tuning methods to the SSM module (S6) of Mamba (130M, 1.4B, 2.8B)2 and the SSM module (SSD) of Mamba-2 (130M, 1.3B)3 pretrained from Pile (MIT License, Gao et al. (2020)) using AdamW (Loshchilov and Hutter, 2019) with linear decay schedule for the learning rate. In general, we choose hyperparameters for each individual method to ensure that all 2Apache License 2.0, https://huggingface.co/statespaces/mamba-{130m,1.4b,2.8b} methods operate within similar parameter budget. Tables 5 and 6 show selected learning rates and chosen hyperparameters for each method. For assessing NLG tasks, we utilize beam search with five beams and maximum beam length of 1024. BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) metrics are computed using Hugging Faces evaluate library4. We use an NVIDIA RTX 3090 24GB for training models with less than 1 billion parameters, and an NVIDIA H100 80GB for larger models. We implemented our project in PyTorch (Modified BSD license, Paszke et al. (2019)), utilizing the Hugging Face trainer (Apache License 2.0, Wolf et al. (2020)). We train with batch size 4 for 10 epochs on all datasets except QQP and MNLI for which we use 3 epochs, allowing each training run to finish in under 16 hours. This project spanned three months, utilizing four NVIDIA RTX 3090 24GB GPUs and four NVIDIA H100 80GB GPUs, totaling approximately 17,000 GPU hours."
        },
        {
            "title": "G Additional Experimental Results",
            "content": "G.1 Mamba Results Training Speed and Memory Usage We conduct small experiment to compare the memory usage and training speed of State-offset Tuning (h) and LoRA, as they performed most similarly in terms of dataset metrics in our experiments. Using single H100 GPU, we train for 100 batch iterations with batch size of 4 and 1K context, continuously measuring memory usage and batch latency. Table 7 shows the training speed and maximum memory usage for different Mamba sizes for Stateoffset Tuning (h) and LoRA. State-offset Tuning (h) uses less memory and is faster, even with more trainable parameters. In this experiment, we selected hyperparameters to ensure LoRA has less trainable parameters than State-offset Tuning (h). We believe State-offset Tuning (h)s efficiency stems from our optimized einsum implementation, enhanced with the opt_einsum (MIT License, Smith and Gray (2018)) Python package to reduce memory usage and improve latency. 3Apache License 2.0, spaces/mamba2-{130m,1.3b} https://huggingface.co/state4Apache License 2.0, https://huggingface.co/spaces/evaluatemetric/{bleu,rouge,meteor} 10 Model Mamba MambaMethod / Dataset RTE MRPC CoLA SST-2 QNLI QQP MNLI DART SAMSum Spider DART SAMSum Spider LoRA Additional-scan Selective Dimension Tuning Initial State Tuning State-offset Tuning (h) State-offset Tuning (h) (low rank) State-offset Tuning (y) 2e-03 4e-03 1e-03 4e-04 1e-03 - 1e-03 2e-03 2e-03 4e-02 1e-03 2e-04 - 2e-03 4e-05 2e-03 1e-01 2e-03 2e-04 - 1e2e-03 1e-01 4e-02 2e-03 1e-04 - 1e-03 1e-03 2e-03 2e-02 2e-03 1e-04 - 2e-03 1e-03 4e-02 2e-02 2e-03 4e-05 - 1e-03 2e-03 4e-03 1e-01 2e-03 4e-04 - 1e-03 4e-03 4e-03 4e-02 2e-03 4e-04 - 4e-03 2e-03 4e-03 2e-02 2e-04 1e-04 - 1e4e-03 4e-03 4e-02 1e-03 2e-04 - 2e-03 4e-03 2e-02 2e-03 4e-03 1e-03 4e-03 1e-02 2e-03 4e-03 1e-03 2e-04 2e-05 2e-04 2e-04 4e-03 1e-02 1e-03 4e-04 2e-05 2e-04 1e-03 Table 5: Learning rates for each method and dataset. For Mamba and Mamba-2, learning rates for each method and dataset are determined via small grid search on dataset subset. The learning rate yielding the best training loss is chosen as the final rate. Method /Model Mamba 130M Mamba 1.4B Mamba-2 130M Mamba-2 1.3B LoRA Rank = 8 α = 8 Dropout = 0.1 Modules = all weight matrices in Rank = 8 α = 8 Dropout = 0.1 Modules = all weight matrices in S6 Rank = 16 α = 16 Dropout = 0.1 Modules = all weight matrices in SSD Rank = 16 α = 16 Dropout = 0.1 Modules = all weight matrices in SSD Additional-scan #States = 8 #States = #States = 32 #States = 32 Selective Dimension Tuning Initial State Tuning State-offset Tuning (h) State-offset Tuning (h) (low rank) State-offset Tuning (y) Freeze #Channels = 50.0% Freeze #States = 75.0% Freeze #Channels = 50.0% Freeze #States = 75.0% Freeze #Channels = 75.0% Freeze #States = 0.0% Freeze #Channels = 75.0% Freeze #States = 0.0% - - - - - - - - - - Rank = 32 - - - Rank = 64 - Table 6: Hyperparameter settings for each model and PEFT method. In general, we adjust hyperparameters to maintain similar number of trainable parameters. Model Method Params (%) Mem. (GB) Latency (s) G.2 Mamba-2 Results Table 10 shows experimental results using Mamba2 (Dao and Gu, 2024) models. State-offset Tuning (h) with low-rank adaptation (Sec. C) significantly reduces the number of trainable parameters. It outperforms existing methods on the Spider benchmark by large margin and achieves performance comparable to other approaches on the SAMSum and DART datasets. 130M 370M 790M 1.4B 2.8B State-offset Tuning (h) LoRA State-offset Tuning (h) LoRA State-offset Tuning (h) LoRA State-offset Tuning (h) LoRA State-offset Tuning (h) LoRA 0.45 0.35 0.42 0.32 0.3 0.23 0.23 0.17 0.19 0.14 4.2 5. 9.36 11.56 13.91 17.17 18.77 22.99 31.49 37.84 0.13 0.18 0.33 0. 0.49 0.61 0.67 0.8 1.13 1.33 Table 7: Training speed and memory usage. For each Mamba size, we compare the maximum memory usage and mean latency for processing single batch during training. Our State-offset Tuning (h) is compared against LoRA, as it demonstrated the most similar performance in the experiment section. We configure LoRA to use fewer trainable parameters than State-offset Tuning (h). Despite this, State-offset Tuning (h) still consumes less memory and is faster in training. Mamba 2.8B Results Table 8 shows the experimental results using Mamba 2.8B. Our State-offset Tuning (h) outperforms all methods except full fine-tuning. Mamba Results on GLUE Dataset Table 9 shows the full results on the GLUE dataset using Mamba 130M. Our State-offset Tuning (h) achieves the highest average score among all PEFT methods."
        },
        {
            "title": "Type",
            "content": "-"
        },
        {
            "title": "Method",
            "content": "Full Fine-tuning (All) Full Fine-tuning (S6)"
        },
        {
            "title": "Parameter\nbased",
            "content": "LoRA BitFit Additional-scan Selective Dimension Tuning"
        },
        {
            "title": "Prompt\nbased",
            "content": "Prompt Tuning Prefix-Tuning"
        },
        {
            "title": "State\nbased",
            "content": "Initial State Tuning State-offset Tuning (h) State-offset Tuning (y) Mamba 2.8B"
        },
        {
            "title": "Easy Medium Hard Extra",
            "content": "71.8 65.7 63.9 59.9 35.0 14.0 50.7 45.1 59.7 65.0 63.1 87.5 81.9 86.3 82.3 62.0 33. 75.4 75.0 82.3 89.1 85.9 73.5 68.8 68.2 60.8 31.9 8.7 53.8 45.1 62.3 65.9 64. 63.8 58.0 49.4 52.9 27.4 9.5 37.4 32.2 43.7 51.7 52.3 51.8 41.0 34.3 31.3 12.1 4. 19.3 13.9 35.5 40.4 37.3 Params (%) 100.00 4.44 0.38 0.02 0.28 0.21 0.01 10. 0.19 0.19 0.01 Table 8: Experimental results of fine-tuning the SSM module using pretrained Mamba 2.8B. State-offset Tuning (h) stands out as the most effective method among all PEFT approaches."
        },
        {
            "title": "Type",
            "content": "-"
        },
        {
            "title": "Method",
            "content": "Full Fine-tuning (All) Full Fine-tuning (S6)"
        },
        {
            "title": "Parameter\nbased",
            "content": "LoRA BitFit Additional-scan Selective Dimension Tuning"
        },
        {
            "title": "Prompt\nbased",
            "content": "Prompt Tuning Prefix-Tuning"
        },
        {
            "title": "State\nbased",
            "content": "Initial State Tuning State-offset Tuning (h) State-offset Tuning (y) Params (%) 100.00 4.31 0.92 0.06 0.68 0.51 0.01 0.03 0.45 0.45 0. Mamba 130M"
        },
        {
            "title": "GLUE",
            "content": "RTE MRPC CoLA SST-2 QNLI QQP MNLI Avg. 71.1 69.7 66.1 69.5 57.9 67.5 56.0 67.5 66.8 67.4 70.0 80.6 78. 78.7 80.4 74.0 69.4 71.6 75.7 78.4 80.8 79.6 63.2 59.1 57.8 54.7 38.6 14.6 12.0 43. 53.0 56.2 52.5 92.2 91.5 90.8 92.0 79.0 81.8 89.4 91.5 92.4 91.9 91.7 87.4 88. 87.8 86.2 79.9 64.0 76.8 83.4 86.4 87.7 86.3 87.9 87.5 86.9 85.3 70.5 82.4 79.6 83. 86.1 85.6 85.6 80.8 80.5 79.8 77.2 36.9 66.1 61.5 35.6 78.5 79.7 78.2 80.5 79. 78.3 77.9 62.4 63.7 63.8 68.6 77.4 78.5 77.7 Table 9: Full results of fine-tuning the SSM module on the GLUE dataset using pretrained Mamba 130M. Our State-offset Tuning (h) achieves the highest average score among all PEFT methods."
        },
        {
            "title": "Type",
            "content": "-"
        },
        {
            "title": "Method",
            "content": "Full Fine-tuning (All) Full Fine-tuning (SSD)"
        },
        {
            "title": "Parameter\nbased",
            "content": "LoRA BitFit Additional-scan Selective Dimension Tuning"
        },
        {
            "title": "Prompt\nbased",
            "content": "Prompt Tuning Prefix-Tuning"
        },
        {
            "title": "State\nbased",
            "content": "Initial State Tuning State-offset Tuning (h) State-offset Tuning (h) (low rank) State-offset Tuning (y) Mamba-2 1.3B Mamba-2 130M Params (%) 100.00 2.42 0.37 0.02 0.47 0. 0.01 6.99 1.84 1.84 0.35 0."
        },
        {
            "title": "Easy Medium Hard Extra",
            "content": "R1 R2 RL 64.8 55.1 45.4 50.9 31.9 47.5 45.2 47. 54.3 58.5 60.5 43.6 85.9 76.2 69.0 71.4 57.3 68.6 62.5 71.0 73.4 79.3 79.0 66.5 65.7 56. 44.4 51.6 30.5 47.8 46.9 48.2 57.2 61.6 65.7 42.1 54.0 42.5 37.4 45.4 23.0 35.7 34.5 32. 45.4 44.6 52.3 36.9 42.2 34.3 21.1 24.1 7.2 27.7 25.9 25.9 27.1 33.7 27.7 21.1 51.0 50. 49.7 50.9 43.0 47.9 49.6 50.8 50.4 48.8 50.4 50.3 26.9 26.3 25.9 26.5 20.1 24.8 26.1 26. 26.4 24.7 26.8 26.2 42.5 42.4 41.7 42.6 34.8 40.2 41.6 42.6 42.3 40.5 42.5 42.2 Params (%) 100.00 4.17 0.76 0.03 0.91 0.88 0.04 12.81 3.53 3.53 0.72 0."
        },
        {
            "title": "DART",
            "content": "MET. BLEU 66.6 65.7 70.3 66.2 58.5 68.2 65.5 69.2 65.3 70.0 69.8 65.9 34.9 39. 49.6 39.0 16.0 39.2 36.9 46.5 37.2 46.3 47.9 38.7 Table 10: Experimental results of fine-tuning the SSM module using pretrained Mamba-2 (Dao and Gu, 2024) models. We evaluate Spider and its subsets with execution accuracy, SAMSum using ROUGE-1/2/L scores, and DART through METEOR and BLEU scores. State-offset Tuning (h) with low-rank adaptation (Sec. C) significantly reduces trainable parameters. It outperforms existing methods on Spider by wide margin and matches the performance of other approaches on SAMSum and DART."
        }
    ],
    "affiliations": [
        "UW-Madison"
    ]
}