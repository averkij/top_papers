{
    "paper_title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation Benchmark",
    "authors": [
        "Viktor Moskvoretskii",
        "Alina Lobanova",
        "Ekaterina Neminova",
        "Chris Biemann",
        "Alexander Panchenko",
        "Irina Nikishina"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models' abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources."
        },
        {
            "title": "Start",
            "content": "Do look like cat.n.01 to you? Taxonomy Image Generation Benchmark Viktor Moskvoretskii1,2, Alina Lobanova2, Ekaterina Neminova1,2, Chris Biemann3, Alexander Panchenko1,4, Irina Nikishina3, 1Skoltech, 2HSE University, 3University of Hamburg, 4AIRI Correspondence: vvmoskvoretskii@gmail.com 5 2 0 2 3 ] . [ 1 7 5 3 0 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper explores the feasibility of using textto-image models in zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose comprehensive benchmark for Taxonomy Image Generation that assesses models abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) and Visual Language Models (VLMs) have demonstrated remarkable quality across wide range of singleand cross-domain tasks (Esfandiarpoor et al., 2024; Esfandiarpoor and Bach, 2023; Du et al., 2023; Jiang et al., 2024b). Their capabilities also expand to the tasks traditionally dominated by human input, such as annotation and data collection (Tan et al., 2024). At the very same time, the urge for manually created datasets and databases still remains popular, as more accurate and reliable (Zhou et al., 2023), even though they are time-consuming and expensive to be kept up-to-date. In this paper, we focus on taxonomies lexical databases that organize words into hierarchical 1 Figure 1: Comparison of generations of the Playground model for the input prompt from the DiffusionDB dataset and available inputs from the WordNet-3.0. It can be seen, that the input from the TTI dataset is more detailed and the inner model representation could be misguiding even when the difinition is given. structure of IS-A relationships. WordNet (Miller, 1998) is the most popular taxonomy for English, forming the graph backbone for many downstream tasks (Mao et al., 2018; Lenz and Bergmann, 2023; Fedorova et al., 2024). In addition to textual data, taxonomies also extend to visual sources, e.g. ImageNet (Deng et al., 2009). ImageNet is built upon the WordNet taxonomy by associating concepts or synsets (sets of synonyms, aka lemmas) with thousands of manually curated images. However, it covers very small portion of WordNet taxonomy (5,247 out of 80,000 synsets in total, 6.5%). From the visual perspective, Text-to-Image models are widely used for the visualizations (Ng et al., 2023; Sha et al., 2023), but only occasionally for taxonomies (Patel et al., 2024a). Therefore, there is limited knowledge about how well text-to-image models are capable of visualizing concepts of dif- (a) Kandinsky 3 (b) SD3 (c) Playground (d) Retrieval Figure 2: The example of generation and retrieval results for cigar lighter. As can be observed, the generation approach is significantly superior to the retrieval approach, as the retrieved image is quite unconventional. ferent level of abstraction in comparison to humans (Liao et al., 2024). Image generation for taxonomies could be quite specific and require additional research: Figure 1 highlights the key differences in prompt usage for the DiffusionDB dataset (Wang et al., 2023) and WordNet-3.0. Moreover, the output taxonomy-linked depictions should aim succinctly portraying the synsets core idea and/or sometimes revealing insights about the concept that are challenging to convey textually. Therefore, in this paper, we address this gap by investigating the use of automated methods for updating taxonomies in the image dimension (depicting). Specifically, we develop an evaluation benchmark comprising 9 metrics for Taxonomy Image generation using both human and automatic evaluation and Bradley-Terry model ranking in line with recent top-rated evaluation methodology (Chiang et al., 2024a; Zheng et al., 2023b). Suprisingly, our task yields different rankings for models compared to those in text-to-image benchmarks (Jiang et al., 2024a), higlighting the task importance. We also uncover that modern Text-to-Image models outperform traditional retrieval-based methods in covering broader range of concepts, highlighting their ability to better represent and visualize these previously underexplored areas. The contributions of the paper are as follows: We propose benchmark comprising 9 metrics, including several taxonomy-specific textto-image metrics grounded with theoretical justification drawing on KL Divergence and Mutual Information. We test on the dataset specifically designed for Taxonomy Image Generation task, which presents challenges that were previously unaddressed in text-to-image research. We are the first to evaluate the performance of the 12 publicly available Text-to-Image models to generate images for WordNet concepts on the developed benchmark. We perform pairwise preference evaluation with GPT-4 for text-to-image generation and analyze its alignment with human preferences, biases, and overall performance. We publish the dataset of the images generated by the best Text-to-Image approach from the benchmark that fully covers WordNet-3.0 extending the ImageNet dataset. We publish all datasets, generated wordnet images and collected preferences."
        },
        {
            "title": "2 Datasets",
            "content": "This section provides an overview of the datasets used to evaluate the performance of text-to-image (TTI) models. It includes the Easy Concepts dataset, the TaxoLLaMA test set derived from WordNet, and the predictions generated by the TaxoLLaMA model. The aim of the datasets is to assess the models sensitivity to easier/harder dataset and to existing/AI-generated entities. 2.1 Easy Concept Dataset The Easy Concepts dataset from Nikishina et al. (2023) comprises 22 synsets selected by the authors as common-sense concepts (e.g. coin.n.01, chromatic_color.n.01, makeup.n.01, furniture.n.01, etc.). We extend this list by including their direct hyponyms (children nodes), following the methodology outlined in the original paper and 1https://huggingface.co/ collections/VityaVitalich/ generated-image-wordnet-67d2c868ff1414ec2f8e0d3d 2 Model name Size Model Family Paper SD-v1-5 SDXL SDXL Turbo Kandinsky 3 Playground-v2-aesthetic Openjourney IF SD3 PixArt-Sigma Hunyuan-DiT FLUX 400M 6.6B 3.5B 12B 2.6B 123M 4.3B 2B 900M 1.5B 12B U-Net Diffusion Transformers Wikimedia Commons2 - Retrieval Rombach et al. (2022) Podell et al. (2023) Sauer et al. (2023) Arkhipkin et al. (2023) Li et al. (2023) Prompthero (2023) DeepFloyd.Lab (2023) Esser et al. (2024) Chen et al. (2024b) Li et al. (2024) BlackForestLabs (2024) Ferrada et al. (2018) Jones and Oyen (2022) Table 1: List of the approaches evaluated in the Taxonomy Image Generation benchmark. based on the English WordNet (Miller, 1998). The resulting dataset comprises 483 entities and represents broader set of common knowledge entities. 2.2 Random Split from WordNet To generate the second dataset, we use the algorithm from TaxoLLaMA (Moskvoretskii et al., 2024b). We randomly sample the nodes the following types of hierarchical relations between synsets: Hyponymy (Hypo): from broader word (working_dog.n.01) to more specific (husky.n.01). Here we take broader word for image generation. Hypernymy (Hyper): from more specific word (capuccino.n.01) to broader concept (coffee.n.01). Here we take more specific word for image generation. Synset Mixing (Mix): nodes created by mixing at least two nodes (e.g. milk.n.01 is beverage.n.01 and diary_product.n.01). Here we take the node created by mixing for depiction. The algorithm for sampling uses 0.1 probability for sampling Hyponymy, 0.1 probability for sampling Synset Mixing, and 0.8 probability for sampling Hypernymy. The dominance of Hypernymy is necessary because it is the most useful relation for training TaxoLLaMA (Moskvoretskii et al., 2024a). To mitigate this bias, the probabilities of occurrence in the test set differ between cases: for Hypernymy is set very low at 1 105, higher for Hyponymy at 0.05, and highest for Synset Mixing at 0.1, as these cases are rare. The resulting test set includes 1,202 nodes: 828 from Hypernymy relations, 170 from Synset Mixing relations, and 204 from Hyponymy relations. 2.3 LLM Predictions Datasets As our final goal is depicting of the new concepts for taxonomy extension, we should also test TTI models with LLM predictions rather than groundtruth synsets. Therefore, we finetune an LLM model on the Taxonomy Enrichment task to use its predictions and assess the sensitivity of text-toimage (TTI) models to AI-generated content. The workflow comprises three steps: (i) exclude the Easy Concept dataset and random split from the overall WordNet data for LLM model training; (ii) train the updated version of TaxoLLaMA with LLaMA-instruct-3.1 (Dubey et al., 2024); (iii) solve the Taxonomy Enrichment task for the test data to generate concepts for vizualization. When training the TaxoLLaMA-3.1 model, we follow the methodology outlined in Moskvoretskii et al. (2024b). This process resulted in 1,685 items. To match the original WordNet synsets, we generate definitions for every generated node with GPT4, described in Appendix B."
        },
        {
            "title": "3 Models",
            "content": "In this section, we describe ten TTI models and one Retrieval model (12 in total) and the details of image collection. Table 1 comprises the full list of the models compared in the evaluation benchmark, 3 Figure 3: LLM prompt example for evaluating text-to-image assistants. their description can be found in Appendix A. An example of prompt for image generation is demonstrated below, details are described in Appendix E. We perform experiments with two versions of the prompt: with and without definition. Figure 2 shows the example of generation and retrieval results guided by the prompt: TEMPLATE: An image of <CONCEPT> (<DEFINITION>) EXAMPLE: An image of cigar lighter (a lighter for cigars or cigarettes)"
        },
        {
            "title": "4 Evaluation",
            "content": "In this section, we describe the evaluation process and metrics. Our evaluation consists of 9 metrics that we assess to provide comprehensive evaluation using the latest methods. To formally define our metrics, let be finite set of concepts v, A(v) the set of hypernyms for v, and (v) the set of cohyponyms for v. Let be the set of all possible images xj in finite model space and = 12 in our case. We define mapping gj : for each model j, which assigns an image to each concept v. 4.1 Preferences Metrics In this section, we describe metrics based on pairwise preferences or those learned to mimic them. ELO Scores We evaluate the ELO scores of the model by first assigning pairwise preferences, similar to the modern evaluation of text models (Chiang et al., 2024a). Each object is assigned two uniformly sampled random models A, U[J] , and their outputs (xA, xB) engage in battle. Then, either human assessor or GPT-4 serves as function to assign win to model (0) or model (1), represented as (v, xA, xB) {0, 1}. Ties are omitted in both the notation and the BT model. To compute ELO scores, the likelihood is maximized with respect to the BT coefficients for each model, which correspond to their ELO scores. More details of the approach can be found in the Chatbot Arena paper (Chiang et al., 2024a). Following this methodology, we calculate the ELO score based on the Bradley-Terry (BT) model with bootstrapping to build 95% confidence in4 Metric Mean Ground Truth Predicted Easy Hypo Hyper Mix P-Easy P-Hypo P-Hyper P-Mix ELO GPT (w/ def) ELO GPT (w/o def) ELO Human (w def) ELO Human (w/o def) Reward Model (w/ def) Reward Model (w/o def) Lemma Similarity Hypernym Similarity Cohyponyms Similarity Specificity FID IS Playground Playground FLUX FLUX Playground Playground SDXL-turbo SDXL-turbo SDXL-turbo SD1.5 SD1.5 SD3 Playground Kandinsky3 Playground / DeepFloyd SD3 Playground Playground SDXL-turbo FLUX FLUX SD1.5 / Playground FLUX PixArt Playground PixArt* Kandinsky3 Kandinsky3 Playground Playground SDXL-turbo FLUX / SDXL-turbo SDXL-turbo SD1.5 FLUX Playground Playground Playground FLUX Playground Playground Playground SDXL-turbo SDXL-turbo SDXL-turbo SD1.5 / Playground FLUX Playground Playground FLUX Playground PixArt Playground Playground SDXL-turbo FLUX / SDXL-turbo SDXL-turbo Draw HDiT SD3 PixArt FLUX FLUX FLUX Playground Playground SDXL-turbo SDXL-turbo SDXL-turbo Draw FLUX SD3 PixArt Playground FLUX FLUX Playground Playground SDXL-turbo SDXL-turbo SDXL-turbo SDXL-turbo / SDXL FLUX FLUX Playground Playground Playground SDXL Playground Playground SDXL-turbo Draw SDXL-turbo SDXL-turbo FLUX FLUX / Retrieval SD3* Kandinsky PixArt SDXL Playground Playground SDXL-turbo Draw SDXL-turbo / SDXL SDXL-turbo DeepFloyd Playground Table 2: Summary of the Top-1 model for each metric and subset. Each cell shows the best-rated model. If two models tie, both are listed with slash; if more than two tie, \"Draw\" is written, indicating insufficient specificity. Results marked with * have negligible differences within the confidence interval. Subsets and models are described in Sections 2 and 3. πi+πj tervals. The Bradley-Terry model (Bradley and Terry, 1952) is probabilistic framework used to predict the outcome of pairwise comparisons between items or entities. It assigns latent strength parameter π to each item and the probability that item is preferred over item is given by: (i > j) = πi . Here, πi, πj > 0 represent the strengths of items and j, respectively. The parameters are typically estimated from observed comparison data using maximum likelihood estimation. We also adopt labeling technique that includes the Tie and Both Bad categories, indicating cases where the models are equally good or both produce poor outputs. We modify the prompt from previous studies evaluating text assistants (Zheng et al., 2023a) for images, as presented in Figure 3 (also see prompt 7 in Appendix for more details). We conduct the Human ELO Evaluation along with the GPT-4 ELO Evaluation on 3370 pair images from two different models ( 600 samples from each model). For Human ELO, we employ 4 assessors expert in computational linguistics, both male and female with at least bachelor degrees. The Spearman correlation between annotators is 0.8 (p-value 0.05) for the images generated with definitions. For the automatic calculation of the ELO score we use GPT-4, which is highly correlated with human evaluations (Zheng et al., 2023a) and has proven to be an effective image evaluator on its own (Cui et al., 2024), as well as great pairwise preferences evaluator (Chen et al., 2024a). Reward Model We utilize the reward model from recent study (Xu et al., 2024), which is trained to align with human feedback preferences, focusing on text-image alignment and image fidelity. This score demonstrates strong correlation with human annotations and outperformed the CLIP Score and BLIP Score. Formally, this metric is similar to ELO Scores, as the reward model was tuned using preferences and the BT model. However, the key difference is that each object is assigned real-valued score and takes only one model image xj as input: freward(v, xj) R. 4.2 Similarities In this section, we introduce novel similarity metrics that leverage taxonomy structure and are derived from KL Divergence and Mutual Information, with formal probabilistic definitions provided in Appendix C. They all have CLIP similarities under the hood, which have been already validated against human judgements (Hessel et al., 2021). This ensures that our metrics, by extension, are aligned with human judgements. In practice, we approximate the probabilities using CLIP similarity (Hessel et al., 2021), as it is the most reliable measure of text-image co-occurrence. Formally, CLIP model C(text or image) Rhidden_dim, we calculate the cosine similarity between the embedding of concept and the embedding of image xj, resulting in the score sim(C(v), C(xj)) Lemma Similarity reflects how well the image aligns with the lemmas textual description and is defined as Slemma(v, x) := (X = v) sim(C(v), C(xj)). (1) Hypernym Similarity reflects how similar the image is on average to the lemma hypernyms and is defined as Shyper(v, x) := (X = A(v)) = 1 A(v) (cid:88) (X = a) aA(v) (cid:88) sim(C(a), C(x)). (2) 1 A(v) 5 aA(v) Figure 4: ELO scores for human and GPT4 preferences. The prompt includes the definition. Overall Spearman correlation of model rankings remains significantly high at 0.92, p-value 0.05. Cohyponym Similarity measures how similar the image is, on average, to the cohyponyms and defined as Specificity helps to ensure that the image accurately represents the lemma rather than its cohyponyms with the relation of the CLIP-Score to the Cohyponym CLIP-Score Scohyponym(v, x) := (X = (v)) = (cid:88) (X = n) 1 (v) nN (v) (cid:88) sim(C(n), C(x)). (3) 1 (v) nN (v) This metric should be interpreted in conjunction with Specificity, as high Cohyponym Score paired with low Specificity does not necessarily indicate good generation. In the T2I domain, it is not feasible to define accuracy in the traditional sense. It is difficult to determine whether the reflection of concept is entirely correct or completely incorrect. This challenge is inherent to the nature of T2I tasks and is shared by other studies in this domain. To address this limitation, we propose an analogous measure to assess how well the image reflects the concept. We use the probability of the concept with respect to the generated image, denoted as (X = xv), which is derived from Lemma Similarity. To further refine this measure, we also consider how well the generated image fits into the surrounding conceptual space by evaluating Hypernym Similarity and Cohyponym Similarity. These additional metrics help capture how accurately the image represents the broader context of the concept. Shyper(v, x) Scohyponym(v, x) This metric generalizes the In-Subtree Probability, as proposed in (Baryshnikov and Ryabinin, 2023). The key advantage of our metric is that it does not depend on specific ImageNet classifier and can be applied to any type of taxonomy node. 4.3 FID and IS We evaluate the Inception Score (IS) (Salimans et al., 2016) and the Fréchet Inception Distance (FID) (Heusel et al., 2017). IS is primarily used to assess diversity, while FID measures image quality relative to true image distributions. In our case, we calculate FID based on retrieved images, meaning that in this specific setting, FID reflects the realness or closeness to retrieval rather than the semantic correctness of an image."
        },
        {
            "title": "5 Results & Analysis",
            "content": "The summary of the main results are presented in Table 2 and in Appendix G: they show the best model for each subset and each metric. Additionally, we provide an error analysis in Appendix and discuss the strengths and weaknesses of the best-performing models. ELO Scores The preferences of human evaluators and GPT-4 resulted in the ELO Scores are Figure 5: Distribution of preferences for Human and GPT across subsets in percentage. Prompt included definition. shown in Figure 4. FLUX and Playground rank the first and the second across both GPT-4 and human assessors, with PixArt securing the third place. While the other rankings are less consistentlikely due to the difficulty in distinguishing between middle-performing modelsthe overall Spearman correlation of model rankings remains significantly high at 0.88, p-value 0.05. Ranking without definitions is presented in Figure 8 in Appendix F, where FLUX ranks first for the Human preferences and Playground for the GPT Preference. However, the confidence intervals for the GPT Preference suggest it is not definitive winner, as it ranks similarly to PixArt. The correlation between human and GPT-4 rankings is 0.73, 0.05, which, while lower, is still strong. At the same time, we found no correlation between raw scores for individual battles. This issue stems from strong bias toward the first option, as illustrated in Figure 5 and the Confusion Matrix in Figure 11 in Appendix F, bias not exhibited by humans. Most TTI models benefit from definitions in their input which exposes high human-GPT alignment, as shown in Figure 6 in Appendix B. Reward Model The results from the Reward Model, introduced in previous study (Xu et al., 2024), show that Playground is the most preferred model, followed by PixArt and FLUX, with no significant differences between the latter, as shown in Figure 9 in Appendix F. Overall, the Reward Model demonstrates high correlation with human evaluations (0.79) and moderate correlation with GPT-4 (0.59). Playground is also the preferred model across all subsets, as illustrated in Figure 10 in Appendix F, while Figure 16 in Appendix highlighting the statistical significance of these comparisons. Similarities for lemmas, hypernyms, and cohyponyms consistently shows the dominance of SDXL-turbo across all subsets and FLUX for Easy Ground Truth subset. This result differs from AI preferences, possibly due to CLIP-Score focusing solely on text-image alignment without accounting for image quality. It is also noteworthy that SDXL-turbo ranks higher than SDXL, despite being distilled version of the latter. The distillation process may have preserved more of the image-text alignment features while reducing overall image quality, as suggested in the original paper, while other models are not distilled or are specifically tuned to match user preferences. Specificity shows no clear dominance, although the top models are SDXL-turbo, SD1.5, and Playground. SD1.5 ranks first in several subsets, though it performs poorly in terms of user preferences. Moreover, this result indicates that Playgrounds generations can be specific to the precise lemma, aligning both with preference and specificity. 7 FID results, presented in Table 8 and Table 10 in Appendix G, demonstrate that on average SD1.5 performs best, however FLUX dominates across nearly all subsets. We associate this performance with stronger focus on reconstructing open-source crawled images, rather than aligning with human preferences and text-image alignment, however FLUX balancing to also appeal to human judgments. Notably, all models, except for SDXLturbo, benefit from definitions, likely because the longer prompts may confuse the distilled SDXLturbo model. as MS-COCO (Lin et al., 2015), Fashion-Gen (Rostamzadeh et al., 2018) or ConceptBed (Patel et al., 2024b). There are also platforms for interactive comparison of AI models, based on ELO-rating: LMSYS Chatbot Arena (Chiang et al., 2024b) for LLM and GenAI Arena (Jiang et al., 2024a) for comparing text-to-image models. Moreover, due to latest AIs abilities, LLM-as-a-judge evaluation emerged: text or image generation outputs of different models are compared by another model, see (Zheng et al., 2023c; Wei et al., 2024; Chen et al., 2024a). IS results in Table 8 and Table 9 in Appendix indicate that SD3, Playground, and Retrieval rank first across different subsets, suggesting their generations are perceived as sharper and more distinct. All versions of SDXL and SD3 do not benefit from the definitions, likely due to the specific characteristics of the SD family. Overall Our results show that Playground and FLUX are among the top models across different metrics, both with and without definitions. While PixArt also demonstrates strong results, it is preferred by AI evaluations more than human preferences, indicating that the preference may be more AI-Judge specific. However, the results are more heterogeneous for specificity, which measures how well the model reflects the concept itself and not its neighbors. Models from the SD family perform differently on different metrics and subsets, indicating that even when models trained with CLIP alignment may not guarantee specificity to the precise concept and the ability to reflect more detailed information of the node."
        },
        {
            "title": "6 Related Work",
            "content": "In this section, we describe existing evaluation benchmarks for both texts and images and provide an overview of text-to-image generation models. We do not provide an overview on the existing taxonomy-related tasks and approaches and refer to Zeng et al. (2024) and Moskvoretskii et al. (2024b). 6.1 Evaluation Benchmarks Popular benchmarks for language models include GLUE (Wang et al., 2019) and SuperGLUE (Sarlin et al., 2020), MTEB (Muennighoff et al., 2023), SQuAD (Rajpurkar et al., 2016), MTBench (Zheng et al., 2023c) and others. For Text2Image Generation, there are benchmarks such 6.2 Text-to-Image Generation Models For Taxonomies Image generation has recently received significant attention in the field of machine learning. Previously, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Variational Autoencoders (VAEs) (Kingma and Welling, 2022) were primarily used for this purpose. However, diffusion-based methods have now become the dominant approach and are widely used for the visualizations (Ng et al., 2023; Sha et al., 2023), but only occasionally for taxonomies (Patel et al., 2024a). To the best of our knowledge, the existing work on the evaluation of images for taxonomies comprises the paper of Baryshnikov and Ryabinin (2023), which introduces In-Subtree Probability (ISP) and Subtree Coverage Score (SCS), which are revisited in our paper. Recently, Liao et al. (2024) introduced novel task of text-to-image generation for abstract concepts. The benchmark from Patel et al. (2024a) addresses grounded quantitative evaluations of text conditioned concept learners and the Zhang et al. (2024) also operates the notion of concepts for images when developping the concept forgetting and correction method."
        },
        {
            "title": "7 Conclusion",
            "content": "We have proposed the Taxonomy Image Generation benchmark as tool for the further evaluation of text-to-image models in taxonomies, as well as for generating images in existing and potentially automatically enriched taxonomies. It consists of 9 metrics and evaluates the ability of 12 open-source text-to-image models to generate images for taxonomy concepts. Our evaluation results show that Playground (Li et al., 2023) ranks first in all preference-based evaluations."
        },
        {
            "title": "Limitations",
            "content": "Our evaluation focuses on open-source textto-image models, as they are more convenient and cost-effective to use in any system than models relying on an API. Additionally, open-source models offer the flexibility for fine-tuning, which is not possible with closedsource models. However, it would be valuable to explore how closed-source models perform on this task, as our benchmark depends solely on the quality of the generated images. Preferences using GPT-4 were obtained with the use of Chain of Thought reasoning, following previous studies to optimize the prompt (Zheng et al., 2023a). However, we did not utilize multiple generations with majority vote to improve consistency, nor did we rename the models, which could help reduce positional bias. Additionally, we did not perform multiple runs with models alternating positions, as each model could appear in position or with equal probability. The Bradley-Terry model of preferences compensates for such inconsistencies and provides robust scoring, given sufficient number of preference labels, as noted in previous study (Zheng et al., 2023a). Our assumption is further supported by the high correlation in the resulting rankings, even though the correlation between raw preferences is close to zero. Metrics based on CLIP-Score may be biased toward the CLIP model and lack specificity if CLIP is unfamiliar with or unspecific to the precise WordNet concept. Additionally, models could be fine-tuned to optimize for this particular metric. To mitigate this bias, we propose incorporating preferences from AI feedback and also employ preferences from human feedback to provide more balanced and comprehensive evaluation. The Inception Score relies on the InceptionV3 model, which is specific to ImageNet1k. We included this metric as it is traditionally used to measure overall text-to-image performance. However, to address this potential bias, we introduced CLIP based metrics as well as generalization of the ISP metric from previous study (Baryshnikov and Ryabinin, 2023), which also relies on an ImageNet1k classifier, 9 but we supplemented it with the use of CLIP to provide broader evaluation."
        },
        {
            "title": "Ethical Considerations",
            "content": "In our benchmark, we utilized several text-to-image models as well as text assistants for evaluating and generating new concepts. While these models are highly effective for creating creative and novel content, both textual and visual, they may exhibit various forms of bias. The models tested in this benchmark have the potential to generate malicious or offensive content. However, we are not the creators of these models and focus solely on evaluating their capabilities; therefore, the responsibility for any unfair or malicious usage lies with the users and the models authors. Additionally, our fine-tuning of the LLaMA 3.1 model was conducted using safe prompts sourced from WordNet. Although applying quantization and further fine-tuning could potentially reduce the models safety, we did not observe any unsafe or offensive behavior during our testing."
        },
        {
            "title": "References",
            "content": "Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, and Denis Dimitrov. 2023. Kandinsky 3.0 technical report. arXiv preprint arXiv:2312.03511. Anton Baryshnikov and Max Ryabinin. 2023. Hypernymy understanding evaluation of text-to-image arXiv preprint models via wordnet hierarchy. arXiv:2310.09247. BlackForestLabs. 2024. Announcements. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024a. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with visionlanguage benchmark. Preprint, arXiv:2402.04788. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. 2024b. Pixartσ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. Preprint, arXiv:2403.04692. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024a. Chatbot arena: An open platform for evaluating llms by human preference. Preprint, arXiv:2403.04132. Computational Linguistics: ACL 2024, pages 5712 5724, Bangkok, Thailand. Association for Computational Linguistics. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024b. Chatbot arena: An open platform for evaluating llms by human preference. Preprint, arXiv:2403.04132. Xiao Cui, Qi Sun, Wengang Zhou, and Houqiang Li. 2024. Exploring GPT-4 vision for text-to-image synthesis evaluation. In The Second Tiny Papers Track at ICLR 2024. DeepFloyd.Lab. 2023. Deepfloyd if. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. 2023. Learning universal policies via text-guided video generation. In Advances in Neural Information Processing Systems, volume 36, pages 91569172. Curran Associates, Inc. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Reza Esfandiarpoor and Stephen Bach. 2023. Followup differential descriptions: Language models resolve ambiguities for image classification. arXiv preprint arXiv:2311.07593. Sebastián Ferrada, Nicolás Bravo, Benjamin Bustos, and Aidan Hogan. 2018. Querying wikimedia images In Companion Proceedings using wikidata facts. of the The Web Conference 2018, WWW 18, page 18151821, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial networks. Preprint, arXiv:1406.2661. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: referencefree evaluation metric for image captioning. arXiv preprint arXiv:2104.08718. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30. Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. 2024a. Genai arena: An open evaluation platform for generative models. Preprint, arXiv:2406.04485. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024b. survey on large language models for code generation. Preprint, arXiv:2406.00515. Shawn Jones and Diane Oyen. 2022. Abstract images have different levels of retrievability per reverse image search engine. In European Conference on Computer Vision, pages 203222. Springer. Diederik Kingma Auto-encoding variational bayes. arXiv:1312.6114. and Max Welling. 2022. Preprint, Reza Esfandiarpoor, Cristina Menghini, and Stephen Bach. 2024. If clip could talk: Understanding vision-language model representations through their arXiv preprint preferred concept descriptions. arXiv:2403.16442. Mirko Lenz and Ralph Bergmann. 2023. Case-based adaptation of argument graphs with wordnet and large language models. In Case-Based Reasoning Research and Development, pages 263278, Cham. Springer Nature Switzerland. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. 2024. Scaling rectified flow transformers Preprint, for high-resolution image synthesis. arXiv:2403.03206. Mariia Fedorova, Andrey Kutuzov, and Yves Scherrer. 2024. Definition generation for lexical semantic change detection. In Findings of the Association for Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail Doshi. 2023. Playground v2. Zhimin Li, Jiangfeng Jianwei Zhang, Qin Lin, Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen 10 Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. 2024. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. Preprint, arXiv:2405.08748. Jiayi Liao, Xu Chen, Qiang Fu, Lun Du, Xiangnan He, Xiang Wang, Shi Han, and Dongmei Zhang. 2024. Text-to-image generation for abstract concepts. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 3360 3368. AAAI Press. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. 2015. Microsoft coco: Common objects in context. Preprint, arXiv:1405.0312. Rui Mao, Chenghua Lin, and Frank Guerin. 2018. Word embedding and WordNet based metaphor identification and interpretation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1222 1231, Melbourne, Australia. Association for Computational Linguistics. George Miller. 1998. WordNet: An electronic lexical database. MIT press. Viktor Moskvoretskii, Ekaterina Neminova, Alina Lobanova, Alexander Panchenko, and Irina Nikishina. 2024a. Taxollama: Wordnet-based model for solving multiple lexical sematic tasks. arXiv preprint arXiv:2403.09207. Viktor Moskvoretskii, Alexander Panchenko, and Irina Nikishina. 2024b. Are large language models good at lexical semantics? case of taxonomy learning. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 14981510, Torino, Italia. ELRA and ICCL. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, Dubrovnik, Croatia. Association for Computational Linguistics. pre-trained transformers. In Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings), pages 134148. Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou Yang. 2024a. Conceptbed: Evaluating concept learning abilities of text-to-image diffusion models. In AAAI, pages 1455414562. AAAI Press. Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou Yang. 2024b. Conceptbed: Evaluating concept learning abilities of text-to-image diffusion models. Preprint, arXiv:2306.04695. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. Preprint, arXiv:2307.01952. Prompthero. 2023. Openjourney. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. Preprint, arXiv:2103.00020. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring the limits of transfer learning with unified text-to-text transformer. Preprint, arXiv:1910.10683. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. Preprint, arXiv:1606.05250. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695. Negar Rostamzadeh, Seyedarian Hosseini, Thomas Boquet, Wojciech Stokowiec, Ying Zhang, Christian Jauvin, and Chris Pal. 2018. Fashion-gen: The generative fashion dataset and challenge. Preprint, arXiv:1806.08317. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved techniques for training gans. Advances in neural information processing systems, 29. Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, and Tao Xiang. 2023. Dreamcreature: Crafting photorealistic virtual creatures from imagination. arXiv preprint arXiv:2311.15477. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. 2020. Superglue: Learning feature matching with graph neural networks. Preprint, arXiv:1911.11763. Irina Nikishina, Polina Chernomorchenko, Anastasiia Demidova, Alexander Panchenko, and Chris Biemann. 2023. Predicting terms in is-a relations with Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. 2023. Adversarial diffusion distillation. Preprint, arXiv:2311.17042. 11 Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. 2023. De-fake: Detection and attribution of fake images generated by text-to-image generation models. Preprint, arXiv:2210.06998. Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023c. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. Preprint, arXiv:2305.11206. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large language models for data annotation: survey. Preprint, arXiv:2402.13446. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Glue: multi-task benchmark and analysis platform for natural language understanding. Preprint, arXiv:1804.07461. Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 2023. DiffusionDB: large-scale prompt gallery dataset for text-to-image generative models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 893911, Toronto, Canada. Association for Computational Linguistics. Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, and Mei Han. 2024. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates. Preprint, arXiv:2408.13006. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2024. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36. Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Zhenyu Wu, Shangbin Feng, and Meng Jiang. 2024. Codetaxo: Enhancing taxonomy expansion with limited examples via code language prompts. Preprint, arXiv:2408.09070. Gong Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. 2024. Forget-me-not: Learning to forget in text-to-image diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024 - Workshops, Seattle, WA, USA, June 17-18, 2024, pages 17551764. IEEE. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems, volume 36, pages 4659546623. Curran Associates, Inc. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. In"
        },
        {
            "title": "A TTI Models Description",
            "content": "To generate the images, we employed ten models and one retrieval approach. It results in 12 systems in total. A.1 U-Net-based models Models based on the architecture: SD-v1-5 (400M) (Rombach et al., 2022) is SD-v1-2 fine-tuned on 595k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. SDXL (6.6B) (Podell et al., 2023). The U-Net within is 3 times larger comparing to classical SD models. Moreover, additional CLIP (Radford et al., 2021) text encoder is utilized increasing the number of parameters. SDXL Turbo (3.5B) (Sauer et al., 2023) is distilled version of SDXL-1.0. Kandinsky 3 (12B) (Arkhipkin et al., 2023). The sizes of U-Net and text encoders were significantly increased in comparison to the second generation. Playground-v2-aesthetic (2.6B) (Li et al., 2023) has the same architecture as SDXL, and is trained on dataset from Midjourney3. Openjourney (123M) (Prompthero, 2023) is also trained on Midjourney images. A.2 Diffusion Transformers models Diffusion Transformers (DiTs) models: IF (4.3B) (DeepFloyd.Lab, 2023). modular system consisting of frozen text encoder and three sequential pixel diffusion modules. SD3 (2B) (Esser et al., 2024) is Multimodal DiT (MMDiT). The authors used two CLIP encoders and T5 (Raffel et al., 2023) for combining visual and textual inputs. PixArt-Sigma (900M) (Chen et al., 2024b). The authors employed novel attention mechanism for the sake of efficiency and high-quality training data for 4K images. Hunyuan-DiT (1.5B) (Li et al., 2024) is text-to-image diffusion transformer designed for finegrained understanding of both English and Chinese, using custom-built transformer structure and text encoder. FLUX (12B) (BlackForestLabs, 2024) is rectified flow Transformer capable of generating images from text descriptions. It is based on hybrid architecture of multimodal and parallel diffusion transformer blocks. A.3 Retrieval We retrieved images from Wikimedia Commons4, following previous studies (Ferrada et al., 2018; Jones and Oyen, 2022). For 3370 total items, this process resulted in 1,790 unique images. For 20 concepts (32 dataset entities), no images were found. For 146 lemmas, the search returned images that had already been retrieved, likely due to the similarity of the concepts searched. We use the top-1 output from the main image search engine5. 3https://www.midjourney.com 4https://commons.wikimedia.org/ 5For the lemma coin, the search URL is https://commons.wikimedia.org/w/index.php?search=coin& title=Special:MediaSearch&go=Go&type=image"
        },
        {
            "title": "B Definitions Analysis",
            "content": "We also analyzed how different models benefit from the inclusion of definitions in the TTI prompt, examining the change in winning battles with definitions (all models are provided with definitions), as depicted in Figure 6. Most models benefit from definitions according to human evaluation, though the trend is milder in GPT-4 evaluations, with preferences for Kandinsky3 and SD1.5 even dropping significantly. Despite the outlier of Kandinsky3, the overall trend between GPT-4 and human evaluations highlights the alignment of GPT-4s judgments with human preferences. Figure 6: Summary change in battle wins with added definition in prompt."
        },
        {
            "title": "C Metrics Definition",
            "content": "Let be finite set of concepts (lemmas), where each represents semantic category. Let (X , A, µ) be measurable space representing the image domain, where is the set of all possible images, is σ-algebra of measurable subsets of , and µ is base measure. For each concept , we assume probability measure ( v) on (X , A) such that (X v) represents the probability that an image generated under the concept lies in the measurable set A. In other words, each concept defines probability distribution over the image space . C.1 Lemma Similarity Definition (Lemma Similarity). Given concept and an image , the Lemma Similarity is defined as: Slemma(v, x) := (X = v). This measures the likelihood of observing the image under the assumption that the concept is the generating source. According to Theorem 1, it also reflects the likelihood of the concept given the image x, offering principled way to assess how well an image aligns with the semantic properties of concept. Maximizing this metric implies stronger alignment between the concept and the generated image. Theorem 1. Let be finite set of concepts, and suppose the prior distribution is uniform (V = v) = 1 v . Then, v arg maxiV Slemma(v, x) arg maxiV (V = = x). Proof. By Bayes rule, the posterior probability of concept given an image is: (V = = x) = (X = i)P (V = i) vV (X = v)P (V = v) (cid:80) = (X = i) 1 vV (X = v) 1 (cid:80) = (X = i) vV (X = v) (cid:80) (X = i) 14 (4) To find the concept that maximizes the posterior (V = = x), we write: ˆi = arg max iV (V = = x) = arg max iV (X = i) = arg max iV Slemma(i, x). Thus, under uniform prior, maximizing the posterior probability is equivalent to maximizing the Lemma Similarity C.2 Hypernym Similarity & Cohyponym Similarity Definition (Hypernym Similarity). Let A(i) be the set of hypernyms of concept . For given image , we define the Hypernym Similarity as: Shyper(i, x) := (X = A(i)) = 1 A(i) (cid:88) hA(i) (X = h). Definition (Cohyponym Similarity). Let C(i) be the set of cohyponyms of concept . For given image , we define the Cohyponym Similarity as: Scohyponym(i, x) := (X = C(i)) = 1 C(i) (cid:88) chC(i) (X = ch). Hypernym Similarity and Cohyponym Similarity represent the likelihood of observing under the average distribution of its ancestor and cohyponyms concepts respectively. Intuitively, they measure how well the image fits into the neighboring concepts either broader, more general semantic category represented by the ancestors of or similar, slightly different concepts from the same ancestor of i. According further to Theorem 2, maximizing those similarities is proportional to minimizing distance between image space conditioned on concept and image space conditioned on specific neighbor space, therefore better reflecting neighbors semantic properties and covering tree structure. Theorem 2. With large enough Slemma(i, x) to properly represent our concept, max Shyper(i, x) and max Scohyponym(i, x) minP (XA(i)) DKL (cid:0)P (X i) (X A(i))(cid:1) . Proof. The proof will be based for the ancestor case, however proving for cohyponyms is similar with only change of ancestor set to cohyponyms set. Consider the KL divergence: DKL (cid:0)P (X i) (X A(i))(cid:1) = (X = i) log (X = i) (X = A(i)) . (cid:88) xX arg min (XA(i)) DKL (cid:0)P (X i) (X A(i))(cid:1) = (X = i) (X = A(i)) 1 x. As (X = i) is fixed in precise setting, achieving (X=xi) (X=xA(i)) 1 requires increasing (X = A(i)) subject to large enough (X = i). Since Shyper(i, x) = (X = A(i)), increasing Shyper(i, x) for the most probable under reduces the KL divergence. C.3 Specificity Definition (Specificity). Let C(i) be the set of cohyponyms of concept . Define: (X = C(i)) := 1 C(i) (cid:88) cC(i) (X = c). The Specificity of an image with respect to concept is: Spec(i, x) := (X = i) (X = C(i)) . Specificity measures how much more likely it is that was generated by concept compared to one of its cohyponyms. high Specificity value indicates that the probability of under is significantly larger than under C(i), the distribution over its cohyponyms. According to Theorems 3 and 4, Specificity highlights the uniqueness of the node representation by maximizing the distance to the cohyponyms nodes distribution and increasing the mutual information between the concept and image spaces. However, this metric relies on high Lemma Similarity value; otherwise, the reflected uniqueness may be misleading due to poor alignment with the target nodes distribution. Theorem 3. Proof. max Spec(i, x) max DKL(P (X i)P (X C(i))) Spec(i, x) = (X = i) (X = C(i)) . log(Spec(i, x)) = log (X = i) (X = C(i)) . DKL(P (X i)P (X C(i))) = (X = i) log (X = i) (X = C(i)) . (cid:88) For fixed (X = i), increasing (X=xi) (X=xC(i)) term. Thus, the sum DKL(P (X i)P (X C(i))) is maximized. (X=xC(i)) for all (i.e., maximizing Spec(i, x)) increases each log (X=xi) Theorem 4. Let and be random variables over concepts and images. max Spec(i, x)v , max I(V ; X). Proof. For = i: I(V ; X) = (cid:88) v,x (V = v, = x) log (X = = v) (X = x) . (X = i) (X = x) = (cid:80) (X = i) (V = v)P (X = v) . Consider the uniform prior (P (V = v) = 1 ): (X = x) = 1 (cid:88) (X = v), (X = C(i)) = 1 C(i) (cid:88) cC(i) (X = c). Spec(i, x) = (X = i) (X = C(i)) . Increasing Spec(i, x) (X=xi) (X=x) , raising each term (V = i, = x) log (X=xi) (X=x) , thus increasing I(V ; X). GPT-4 Prompts We show the technical style prompt for GPT-4 in Figure 7 for more clarity on how images and user prompt were provided. We employed gpt-4o-mini version with API calls with images in high resolution. The prompt for gpt-4o-mini to generate definitions for TaxoLLaMA3.1 predictions is presented below. (1) 16 Figure 7: Full prompt example for evaluating text-to-image assistants. Write definition for the word/phrase in one sentence. Example: Word: caddle Definition: act as caddie and carry clubs for player Word: bichon Definition:"
        },
        {
            "title": "E Technical Details",
            "content": "For text-to-image, we used the recommended generation parameters for each model, the HuggingFace Diffusers library, and single NVIDIA A100 GPU. All models were utilized in FP16 precision and produced images with resolutions of 512x512 or 1024x1024. Additionally, we experimented with prompting, adding definitions from the WordNet database to help with ambiguity resolution, as this has shown benefits for LLMs in the past (Moskvoretskii et al., 2024b)."
        },
        {
            "title": "F Additional Figures",
            "content": "In this appendix, we include graphs for our evaluation, which results outlined in the main text. Figure 8: ELO scores for human and GPT4 preferences. Prompt did not include the definition. 18 Figure 9: Distribution of rewards for each model, calculated with reward model described in Section 4 Figure 10: Distribution of rewards for each model across subsets, calculated with reward model described in Section 4 Figure 11: Confusion matrix for human and GPT preferences, excluding Tie labels to avoid distracting the analysis. GPT rarely assigns Ties, with fewer than 20 instances. The prompt included definition. 20 Model Ground Truth Predicted Easy Hypo Hyper Mix P-Easy P-Hypo P-Hyper P-Mix Playground FLUX PixArt SDXL HDiT Kandinsky3 Retrieval SD3 SDXL-turbo DeepFloyd Openjourney SD1.5 1125 (+61/-59) 1013 (+65/-78) 1050 (+43/-67) 960 (+75/-72) 981 (+61/-61) 1010 (+72/-70) 965 (+81/-76) 1056 (+78/-59) 1004 (+86/-69) 981 (+53/-63) 997 (+65/-59) 852 (+69/-90) 1139 (+137/-111) 1104 (+153/-151) 1125 (+181/-100) 1113 (+145/-149) 955 (+100/-97) 1035 (+103/-101) 884 (+98/-106) 949 (+118/-99) 999 (+102/-93) 909 (+76/-95) 849 (+102/-125) 933 (+102/-120) 1148 (+50/-56) 1088 (+48/-50) 1086 (+60/-40) 1056 (+63/-61) 1004 (+44/-51) 998 (+51/-55) 979 (+47/-58) 962 (+41/-53) 957 (+49/-48) 943 (+50/-43) 889 (+56/-62) 885 (+58/-55) 1066 (+97/-105) 982 (+105/-125) 1038 (+104/-80) 1063 (+134/-128) 1053 (+122/-137) 958 (+135/-82) 1014 (+119/-105) 983 (+122/-113) 960 (+114/-148) 1053 (+128/-110) 962 (+97/-107) 863 (+110/-115) 1072 (+87/-85) 1066 (+82/-60) 1135 (+82/-66) 1061 (+78/-67) 980 (+78/-59) 1051 (+74/-55) 953 (+65/-72) 997 (+49/-59) 917 (+74/-86) 931 (+56/-64) 987 (+59/-77) 842 (+65/-70) 1095 (+118/-89) 967 (+131/-107) 1159 (+143/-95) 1112 (+114/-86) 1046 (+138/-86) 999 (+100/-104) 880 (+111/-135) 1090 (+123/-120) 964 (+82/-117) 949 (+152/-158) 880 (+87/-162) 853 (+73/-130) 1141 (+53/-59) 1025 (+46/-41) 1107 (+47/-49) 1050 (+49/-48) 1074 (+52/-61) 1043 (+48/-40) 995 (+51/-54) 961 (+62/-57) 969 (+46/-47) 941 (+40/-65) 826 (+54/-55) 864 (+48/-64) 1047 (+130/-105) 1096 (+144/-132) 1063 (+101/-136) 1010 (+148/-120) 965 (+148/-134) 1005 (+102/-113) 971 (+138/-137) 1104 (+116/-85) 1025 (+110/-102) 1036 (+105/-109) 825 (+125/-225) 847 (+108/-133) Table 3: ELO score for GPT Preferences for subsets with definition in input. Model Ground Truth Predicted Easy Hypo Hyper Mix P-Easy P-Hypo P-Hyper P-Mix Playground PixArt Kandinsky3 FLUX HDiT SDXL SD3 Retrieval SDXL-turbo DeepFloyd SD1.5 Openjourney 1091 (+82/-47) 1037 (+77/-71) 1094 (+66/-74) 954 (+48/-65) 1028 (+58/-55) 1015 (+52/-61) 1076 (+66/-76) 926 (+79/-70) 1045 (+76/-56) 918 (+65/-63) 870 (+71/-105) 940 (+76/-50) 1110 (+154/-123) 1137 (+110/-105) 1065 (+95/-110) 1030 (+103/-113) 954 (+102/-129) 939 (+104/-120) 951 (+108/-103) 996 (+99/-110) 969 (+94/-94) 900 (+131/-90) 976 (+95/-134) 969 (+108/-135) 1116 (+45/-42) 1113 (+49/-50) 1090 (+44/-57) 1057 (+55/-57) 1040 (+45/-41) 1029 (+58/-50) 1006 (+50/-60) 973 (+53/-43) 906 (+56/-54) 903 (+46/-60) 888 (+58/-49) 874 (+43/-57) 1049 (+159/-101) 1094 (+103/-90) 1021 (+84/-107) 1119 (+135/-106) 1039 (+107/-102) 998 (+137/-156) 1099 (+129/-146) 1019 (+127/-109) 967 (+104/-150) 888 (+108/-112) 925 (+96/-115) 774 (+93/-139) 1069 (+72/-66) 1122 (+86/-73) 1051 (+76/-67) 1137 (+109/-64) 1014 (+92/-56) 1034 (+63/-64) 1014 (+62/-65) 917 (+81/-70) 950 (+66/-63) 862 (+56/-86) 894 (+69/-75) 930 (+60/-70) 1136 (+148/-118) 1048 (+122/-111) 1083 (+120/-72) 1097 (+122/-100) 1055 (+136/-86) 998 (+123/-105) 969 (+99/-100) 959 (+116/-104) 868 (+80/-146) 976 (+99/-149) 961 (+90/-80) 846 (+86/-129) 1127 (+66/-53) 1081 (+45/-46) 1089 (+46/-52) 1068 (+58/-46) 1028 (+51/-48) 1008 (+44/-45) 964 (+34/-56) 994 (+49/-45) 980 (+43/-45) 896 (+54/-61) 905 (+34/-45) 854 (+47/-60) 1093 (+167/-104) 1076 (+123/-102) 1117 (+112/-120) 1032 (+132/-128) 835 (+118/-177) 1033 (+133/-144) 1078 (+124/-83) 938 (+157/-138) 1073 (+115/-101) 980 (+125/-125) 866 (+107/-187) 872 (+91/-154) Table 4: ELO score for GPT Preferences for subsets with no definition in input."
        },
        {
            "title": "G Additional Tables",
            "content": "In this section we provide the detailed tables for every metric evaluated in the paper. FID results are demonstrated in Tables 8 and 10. IS results are shown in Tables 9 and 8. Lemma Similarity results are shown in Table 12 Hypernym Similarity results are shown in Table 11 Cohyponym Similarity results are shown in Table Specificity results are shown in Table 14 Reward model p-values of Mann-Whitney test on comparing means shown in Table 16 ELO Scores Tables 3, 4, 5 and 6. for human and GPT labeling within each subset with and without definitions are shown in 21 Model Ground Truth Predicted Easy Hypo Hyper Mix P-Easy P-Hypo P-Hyper P-Mix DeepFloyd Playground FLUX SDXL-turbo SD3 SDXL HDiT PixArt Kandinsky3 Retrieval Openjourney SD1.5 1056 (+45/-38) 1055 (+37/-46) 1050 (+48/-44) 1039 (+45/-46) 1033 (+48/-41) 1015 (+39/-36) 994 (+32/-43) 990 (+43/-65) 961 (+42/-48) 960 (+50/-48) 941 (+45/-41) 900 (+46/-59) 957 (+72/-77) 1102 (+96/-95) 1105 (+128/-73) 987 (+57/-71) 1003 (+76/-71) 1033 (+74/-113) 946 (+70/-59) 1027 (+69/-62) 1059 (+81/-63) 953 (+90/-83) 907 (+73/-66) 914 (+72/-61) 956 (+40/-27) 1078 (+36/-33) 1088 (+37/-38) 980 (+36/-41) 1002 (+33/-35) 1050 (+39/-31) 1031 (+33/-35) 1082 (+33/-31) 1014 (+38/-39) 940 (+39/-47) 885 (+37/-49) 889 (+38/-36) 945 (+89/-85) 1094 (+81/-64) 1048 (+104/-97) 945 (+89/-136) 996 (+58/-86) 1089 (+109/-85) 988 (+70/-76) 1050 (+78/-67) 1005 (+87/-60) 990 (+107/-104) 902 (+73/-102) 942 (+76/-67) 1013 (+60/-62) 1017 (+67/-53) 1218 (+77/-70) 1018 (+53/-56) 976 (+54/-47) 980 (+54/-52) 1034 (+67/-70) 1030 (+59/-57) 999 (+61/-87) 890 (+62/-62) 928 (+79/-70) 891 (+58/-70) 946 (+64/-90) 994 (+63/-62) 1070 (+74/-78) 1027 (+69/-71) 976 (+68/-58) 1060 (+73/-68) 1015 (+60/-58) 1052 (+71/-70) 992 (+51/-71) 1023 (+111/-63) 906 (+67/-70) 933 (+68/-79) 993 (+28/-41) 1075 (+39/-33) 1051 (+29/-34) 1033 (+36/-37) 963 (+35/-32) 1035 (+32/-32) 1019 (+33/-38) 1036 (+36/-35) 1063 (+40/-30) 947 (+37/-45) 874 (+42/-42) 904 (+30/-34) 1027 (+55/-60) 1019 (+70/-68) 1044 (+90/-78) 1053 (+79/-80) 959 (+57/-70) 997 (+80/-71) 953 (+80/-77) 1075 (+72/-68) 992 (+64/-97) 1030 (+81/-90) 988 (+72/-73) 857 (+61/-81) Table 5: ELO score for Human Preferences for subsets with definition in input. Model Ground Truth Predicted Easy Hypo Hyper Mix P-Easy P-Hypo P-Hyper P-Mix Playground PixArt Kandinsky3 SD3 SDXL FLUX HDiT Retrieval SDXL-turbo DeepFloyd SD1.5 Openjourney 1044 (+46/-33) 940 (+35/-41) 979 (+47/-33) 1048 (+37/-54) 1022 (+46/-41) 1011 (+51/-46) 964 (+42/-41) 1041 (+49/-49) 992 (+48/-41) 1000 (+37/-40) 968 (+52/-56) 983 (+48/-43) 1044 (+70/-68) 1036 (+64/-74) 1060 (+97/-62) 967 (+67/-69) 948 (+54/-52) 1015 (+75/-100) 988 (+61/-87) 1027 (+79/-72) 989 (+60/-65) 969 (+62/-54) 991 (+73/-69) 958 (+63/-52) 1043 (+28/-30) 1035 (+36/-30) 1027 (+39/-32) 1026 (+32/-31) 1017 (+39/-27) 1008 (+42/-43) 1001 (+28/-34) 995 (+32/-29) 984 (+37/-37) 970 (+35/-39) 958 (+36/-31) 930 (+32/-42) 1006 (+66/-67) 1138 (+84/-84) 1034 (+63/-86) 1074 (+80/-76) 939 (+114/-83) 1012 (+92/-84) 961 (+92/-93) 1051 (+71/-83) 986 (+90/-88) 1037 (+76/-78) 918 (+85/-79) 838 (+67/-96) 1006 (+62/-55) 1037 (+50/-46) 978 (+58/-49) 1044 (+69/-51) 1016 (+44/-49) 1144 (+75/-59) 1040 (+48/-41) 893 (+61/-57) 1010 (+40/-56) 959 (+40/-56) 935 (+48/-51) 931 (+63/-53) 972 (+76/-55) 954 (+68/-94) 1016 (+56/-72) 970 (+70/-52) 1026 (+92/-60) 1102 (+79/-64) 949 (+69/-70) 1093 (+86/-73) 1033 (+73/-68) 1013 (+64/-86) 947 (+54/-65) 918 (+56/-63) 1039 (+32/-29) 1020 (+26/-31) 1045 (+43/-37) 996 (+27/-32) 985 (+30/-24) 1043 (+32/-26) 1001 (+36/-34) 976 (+37/-38) 1053 (+37/-23) 979 (+34/-31) 948 (+25/-36) 908 (+33/-38) 973 (+61/-69) 1041 (+74/-75) 998 (+73/-80) 951 (+68/-76) 1125 (+75/-75) 941 (+64/-83) 983 (+63/-88) 1027 (+96/-84) 1049 (+64/-66) 963 (+57/-82) 970 (+71/-63) 973 (+83/-95) Table 6: ELO score for Human Preferences for subsets with no definition in input. Model Inception Score FID DeepFloyd Kandinsky3 PixArt Playground Openjourney SD1.5 SDXL-turbo SD3 HDiT SDXL FLUX Retrieval 19.6 19.4 19.8 20.9 15.4 18.0 10.9 21.2 18.2 19.1 20.9 19.1 62 64 73 71 68 59 89 63 67 63 68 - Table 7: FID and IS metrics for different models on the full dataset without repetitions Model DeepFloyd Kandinsky3 PixArt Playground Openjourney SD1.5 SDXL-turbo SD3 HDiT SDXL FLUX def 19.6 19.4 19.8 20.9 15.4 18.0 10.9 21.2 18.2 19.1 20.9 IS no_def def no_def FID 12.1 18.2 18.8 20.0 13.9 17.5 12.1 23.5 20.2 19.6 22.0 62 64 73 71 68 59 89 63 67 63 68 131 75 73 74 73 60 65 65 85 68 72 Table 8: Comparing FID and IS for datasets with and without definition Model DeepFloyd Kandinsky-3 PixArt Playground Openjourney SD1.5 SDXL-turbo SD3 HDiT SDXL FLUX Retrieval Ground Truth Predicted Hypo Hyper Mix Easy Hypo Hyper Mix Easy 8.2 7.4 7.6 8.6 6.7 7.1 5.2 8.4 8.0 7.6 8.3 7.8 9.3 10.3 10.5 11.0 8.2 9.0 6.7 9.5 10.1 10.4 10.5 10.5 6.6 6.6 6.6 7.0 6.3 6.9 4.7 7.2 6.8 7.1 7.5 6. 12.9 12.8 13.5 13.3 12.5 12.9 9.9 13.3 11.2 12.8 12.9 11.4 8.3 7.8 7.6 8.0 6.9 7.4 5.3 8.1 7.3 7.5 8.3 8.5 11.3 11.7 11.2 12.0 9.0 9.5 6.8 12.0 11.8 11.4 12.7 12.7 6.8 7.0 7.1 7.8 6.4 6.6 5.1 7.3 7.0 7.3 7.2 6.7 7.3 8.4 8.6 8.3 7.0 8.1 6.1 8.8 7.7 8.1 8.0 8.2 Table 9: Inception Score per subsets with definitions Model DeepFloyd Kandinsky3 PixArt Playground Openjourney SD1.5 SDXL-turbo SD3 HDiT SDXL FLUX Ground Truth Predicted Hypo Hyper Mix Easy Hypo Hyper Mix Easy 195 203 203 198 200 192 207 193 201 192 134 134 143 143 145 135 158 135 141 133 132 191 197 191 188 197 192 211 193 187 199 186 128 127 134 130 127 125 146 133 126 129 139 220 229 229 234 225 229 235 225 226 230 165 219 224 230 225 228 221 226 218 225 223 115 147 155 163 163 159 154 170 153 160 163 193 191 203 196 198 186 203 184 198 191 171 Table 10: Fréchet Inception Distance Score per subsets with definitions"
        },
        {
            "title": "Predicted",
            "content": "Easy Hypo Hyper Mix P-Easy P-Hypo P-Hyper P-Mix HDiT Retrieval Kandinsky3 Openjourney DeepFloyd SDXL-turbo Playground SDXL PixArt SD3 SD1.5 FLUX 0.65 0.63 0.66 0.63 0.64 0.67 0.66 0.66 0.65 0.66 0.64 0.70 0.60 0.56 0.61 0.59 0.60 0.62 0.60 0.60 0.60 0.61 0.60 0.62 0.61 0.57 0.61 0.59 0.60 0.62 0.61 0.61 0.60 0.61 0.60 0.61 0.61 0.60 0.62 0.60 0.62 0.63 0.61 0.61 0.62 0.62 0.61 0. 0.57 0.54 0.58 0.57 0.57 0.59 0.57 0.58 0.57 0.57 0.57 0.57 0.59 0.57 0.60 0.58 0.59 0.61 0.59 0.59 0.59 0.60 0.58 0.59 0.59 0.56 0.59 0.59 0.60 0.60 0.59 0.59 0.59 0.60 0.60 0.59 0.60 0.60 0.61 0.59 0.60 0.61 0.60 0.60 0.61 0.60 0.59 0.6 Table 11: Hypernym CLIPScore Across Different Subsets"
        },
        {
            "title": "Easy Hypo Hyper Mix Easy Hypo Hyper Mix",
            "content": "HDiT Retrieval Kandinsky3 Openjourney DeepFloyd SDXL-turbo Playground SDXL PixArt SD3 SD1.5 FLUX 0.73 0.68 0.73 0.71 0.71 0.76 0.74 0.73 0.72 0.73 0.73 0.71 0.66 0.60 0.67 0.66 0.67 0.71 0.67 0.67 0.66 0.68 0.68 0.65 0.67 0.62 0.67 0.66 0.66 0.71 0.68 0.67 0.67 0.67 0.67 0.66 0.68 0.66 0.69 0.68 0.69 0.73 0.70 0.69 0.69 0.70 0.70 0.68 0.72 0.63 0.73 0.71 0.70 0.75 0.72 0.73 0.72 0.72 0.72 0. 0.67 0.63 0.68 0.67 0.67 0.72 0.69 0.69 0.67 0.69 0.69 0.68 0.68 0.63 0.68 0.66 0.66 0.71 0.68 0.68 0.67 0.68 0.68 0.67 0.68 0.69 0.69 0.69 0.69 0.74 0.70 0.70 0.68 0.70 0.70 0.69 Table 12: Lemma CLIPScore Across Different Subsets"
        },
        {
            "title": "Model",
            "content": "CLIP-Score Hypernym CLIP-Score Cohyponym CLIP-Score Specificity HDiT Retrieval Kandinsky3 Openjourney DeepFloyd SDXL-turbo Playground SDXL PixArt SD3 SD1.5 FLUX 0.69 0.64 0.69 0.68 0.68 0.72 0.70 0.69 0.68 0.70 0.69 0.68 0.60 0.57 0.61 0.59 0.60 0.62 0.60 0.60 0.60 0.60 0.59 0.61 0.58 0.56 0.59 0.57 0.58 0.60 0.58 0.58 0.58 0.58 0.57 0.58 1.2 1.16 1.19 1.2 1.18 1.23 1.22 1.2 1.19 1.21 1.23 1. Table 13: Summary of CLIPscore Metrics Across Models"
        },
        {
            "title": "Predicted",
            "content": "Easy Hypo Hyper Mix P-Easy P-Hypo P-Hyper P-Mix HDiT Retrieval Kandinsky3 Openjourney DeepFloyd SDXL-turbo Playground SDXL PixArt SD3 SD1.5 FLUX 1.21 1.17 1.21 1.22 1.18 1.23 1.24 1.22 1.20 1.23 1.24 1.14 1.14 1.11 1.14 1.16 1.16 1.18 1.16 1.15 1.14 1.17 1.19 1.10 1.15 1.13 1.15 1.15 1.14 1.18 1.17 1.15 1.15 1.16 1.18 1.12 1.16 1.14 1.16 1.17 1.15 1.20 1.20 1.18 1.16 1.19 1.20 1. 1.34 1.24 1.32 1.32 1.29 1.34 1.34 1.33 1.34 1.34 1.34 1.32 1.18 1.15 1.18 1.18 1.16 1.22 1.21 1.20 1.17 1.20 1.22 1.18 1.18 1.15 1.19 1.18 1.16 1.22 1.20 1.19 1.18 1.20 1.21 1.18 1.18 1.18 1.18 1.21 1.18 1.24 1.21 1.20 1.17 1.20 1.23 1.20 Table 14: Specificity Scores Across Different Models and Subsets"
        },
        {
            "title": "Predicted",
            "content": "Easy Hypo Hyper Mix P-Easy P-Hypo P-Hyper P-Mix HDiT Retrieval Kandinsky3 Openjourney DeepFloyd SDXL-turbo Playground SDXL PixArt SD3 SD1.5 FLUX 0.61 0.59 0.61 0.59 0.61 0.62 0.60 0.61 0.60 0.61 0.60 0.63 0.59 0.55 0.59 0.58 0.58 0.60 0.58 0.58 0.59 0.59 0.58 0.59 0.58 0.55 0.59 0.57 0.58 0.60 0.58 0.58 0.58 0.58 0.57 0.59 0.60 0.59 0.61 0.59 0.61 0.62 0.60 0.60 0.61 0.60 0.60 0. 0.54 0.51 0.55 0.54 0.55 0.56 0.54 0.55 0.54 0.54 0.54 0.54 0.57 0.56 0.58 0.57 0.57 0.59 0.57 0.58 0.57 0.58 0.56 0.57 0.58 0.55 0.59 0.57 0.58 0.60 0.58 0.58 0.58 0.58 0.57 0.57 0.58 0.58 0.59 0.57 0.59 0.60 0.58 0.60 0.59 0.59 0.57 0.58 Table 15: Cohyponym CLIPScore Across Different Subsets SDXL-turbo Retrieval SD1.5 HDiT Playground Openjourney Kandinsky3 SDXL PixArt DeepFloyd SD3 FLUX SDXL-turbo Retrieval SD1.5 HDiT Playground Openjourney Kandinsky3 SDXL PixArt DeepFloyd SD3 FLUX 0 0 0 0.003 0 0 0.036 0 0 0.027 0.029 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.037 0 0 0 0 0 0.003 0 0 0 0 0 0 0 0 0 0.702 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.037 0 0 0 0 0 0 0 0 0 0.036 0 0 0 0 0 0 0 0.167 0.95 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.167 0 0 0.14 0 0.027 0 0 0 0 0 0.95 0 0.14 0 0 0 0.029 0 0 0.702 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Table 16: P-value of Mann-Whitney mean differences test in rewards for models. Values below 0.000 are marked as 0. To identify the side of difference, refer to the violin plot in Figure 9 26 Models Mistake Analysis This analysis is made for generation without definitions. All models struggle with depicting a. abstract concepts; b. nonfrequent and specific words (\"orifice.n.01\" with the lemma \"rima\"); c. notions of people with specific functional role (\"holder.n.02\" with the lemma \"holder\", for example). Abstract concepts are handled with the following: 1. Text in images (although not all models succeed in writing); (a) Openjourney, prevention.n.01, prevention (b) Openjourney, corporal punishment.n.01, corporal punishment (c) HDit, reform movement.n.01, labor union (d) SD3, murder.n.01, murder (e) SD3, abstinence.n.02, abstinence (f) PixArt, broadcast.n.01, headline Figure 12: Text in images 2. Abstract images; Other unwanted behaviors for the purposes of illustrating taxonomies include 1. Generating playing cards for the concepts (most seen in Openjourney, also present in SD1.5); 2. Abstract ornamental circles (also most found in Openjourne, and some in SD1.5). 3. Depicturing monsters when facing rare animal names (seen in Kandinsky3). Most importantly, models struggle closer to the leaves of taxonomy: they tend to create an image of parent concept without necessary features of the child (see figure 20). 27 (a) PixArt, binomial.n.01, binomial (b) PixArt, statement.n.01, statement (c) HDiT, disrespect.n.01, obscenity (d) SDXL, clearance.n.03, clearance (e) HDiT, financial gain.n.01, flow (f) SD3, somesthesia.n.02, somesthesia Figure 13: Abstract images (a) Openjourney, sovereign sovereign.n.01, (b) Openjourney, enthusiast.n.01, enthusiast (c) Openjourney, policyholder.n.01, policyholder (d) Openjourney, agreement agreement.n.01, (e) Openjourney, ground-shaker.n.01, ground-shaker (f) SD1.5, finisher.n.01, finisher Figure 14: Playing cards in Openjourney and SD1.5 (a) Openjourney, object.n.01, object (b) SD1.5, salat.n.01, salat (c) SD1.5, person.n.01, iranian (d) Openjourney, engineering.n.03, engineering (e) Openjourney, lingtpen.n.01, light pen (f) Openjourney, organization.n.01, baptist association Figure 15: Abstract ornamental circles (a) Kandinsky3, acanthocephalan.n.01, acanthocephalan (b) Kandinsky3, gelechiid gelechiid.n.01, Figure 16: Monsters for rare animal names in Kandinsky Figure 17: PixArt images for \"cheese\" and some of its hyponyms"
        },
        {
            "title": "I Demonstration System Examples",
            "content": "Figure 18: Subgraph starting from the root node entity.n.01. Images are generated with the best TTI model. Figure 19: Subgraph starting from the node biological_group.n.01. Images are generated with the best TTI model. 31 Figure 20: Node descriptions from the demonstration system with the generated image using the best-performing model."
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE University",
        "Skoltech",
        "University of Hamburg"
    ]
}