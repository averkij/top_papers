{
    "paper_title": "ATI: Any Trajectory Instruction for Controllable Video Generation",
    "authors": [
        "Angtian Wang",
        "Haibin Huang",
        "Jacob Zhiyuan Fang",
        "Yiding Yang",
        "Chongyang Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a unified framework for motion control in video generation that seamlessly integrates camera movement, object-level translation, and fine-grained local motion using trajectory-based inputs. In contrast to prior methods that address these motion types through separate modules or task-specific designs, our approach offers a cohesive solution by projecting user-defined trajectories into the latent space of pre-trained image-to-video generation models via a lightweight motion injector. Users can specify keypoints and their motion paths to control localized deformations, entire object motion, virtual camera dynamics, or combinations of these. The injected trajectory signals guide the generative process to produce temporally consistent and semantically aligned motion sequences. Our framework demonstrates superior performance across multiple video motion control tasks, including stylized motion effects (e.g., motion brushes), dynamic viewpoint changes, and precise local motion manipulation. Experiments show that our method provides significantly better controllability and visual quality compared to prior approaches and commercial solutions, while remaining broadly compatible with various state-of-the-art video generation backbones. Project page: https://anytraj.github.io/."
        },
        {
            "title": "Start",
            "content": "ATI: Any Trajectory Instruction for Controllable Video Generation"
        },
        {
            "title": "Chongyang Ma",
            "content": "ByteDance Intelligent Creation https://anytraj.github.io/ 5 2 0 2 8 2 ] . [ 1 4 4 9 2 2 . 5 0 5 2 : r Figure 1. ATI is able to generate video given an initial frame (left) and set of user-specified trajectories. Green dots denote the starting points, and red dots indicate the ending points of each trajectory. On the right, we show uniformly sampled frames from the generated video, with colored dots tracking the position of each trajectory point over time."
        },
        {
            "title": "Abstract",
            "content": "We propose unified framework for motion control in video generation that seamlessly integrates camera movement, object-level translation, and fine-grained local motion using trajectory-based inputs. In contrast to prior methods that address these motion types through separate modules or task-specific designs, our approach offers cohesive solution by projecting user-defined trajectories into the latent space of pre-trained image-to-video generation models via lightweight motion injector. Users can specify keypoints and their motion paths to control localized deformations, entire object motion, virtual camera dynamics, or combinations of these. The injected trajectory signals guide the generative process to produce temporally consistent and semantically aligned motion sequences. Our framework demonstrates superior performance across multiple video motion control tasks, including stylized motion effects (e.g., motion brushes), dynamic viewpoint changes, and precise local motion manipulation. Experiments show that our method provides significantly better controllability and visual quality compared to prior approaches and commercial solutions, while remaining broadly compatible with various state-of-the-art video generation backbones. 1. Introduction Recent advances in video generation models [14, 10, 13, 16, 17, 22, 23, 27, 33, 37, 44] have demonstrated remarkable capabilities in synthesizing realistic and diverse video content. However, precise control over motion remains significant challenge, particularly when users reFigure 2. ATI takes an image and user specified trajectories as inputs. The point-wise trajectories are injected into the latent condition for the generation. Videos are decoded from the latent denoised from the DiT. quire fine-grained direction over how specific elements move within the generated sequence. Current approaches [20, 25, 30, 31, 35, 41] typically address different types of motion controlsuch as camera movement, object translation, or local deformationthrough separate specialized modules, leading to fragmented workflows and inconsistent results. Motion control in video generation encompasses spectrum of manipulations, from camera operations (panning, zooming, rotation) to object-level translations and localized movements of specific regions. These motion types are inherently related and often need to be coordinated to achieve desired visual effects. The separation of these controls in existing systems limits creative expression and requires users to navigate multiple interfaces or models. In this paper, we propose unified trajectory-based framework that addresses this limitation by treating all forms of motion control through common lens. Our key insight is that diverse motion effects can be represented as trajectories of specific points within the scene, whether these points are anchored to local features, whole objects, or used to indicate camera perspective changes. By defining motion as trajectory paths for user-selected key points, we establish consistent, intuitive interface for motion specification. Our approach builds upon state-of-the-art image-tovideo generation models [23, 27], augmenting them with specialized motion injector module. This module processes trajectory information and projects it into the latent space of the pre-trained video generation model, effectively guiding the synthesis process to follow the specified motion paths. Importantly, our method does not require retraining the base video model, making it adaptable to different generation architectures. We demonstrate the versatility of our framework through extensive experiments across various motion control tasks. Our results show that the unified approach not only simplifies the user workflow but also produces higher quality motion than previous methods that handle different motion types separately. The framework excels in challenging scenarios that require coordination between camera movement and object motion, outperforming both specialized academic methods and commercial video generation products in both control precision and visual quality. The main contributions of our work include: unified framework for motion control in video generation that seamlessly integrates camera movements, object-level motion, and local deformations through trajectory-based guidance. motion injector module that effectively projects userspecified trajectory controls into the latent space of pretrained video generation models. Comprehensive evaluation demonstrating superior performance across various motion control tasks compared to previous methods and commercial products. Demonstration of compatibility with different base video generation models, highlighting the approachs flexibility and broad applicability. 2. Related Work Motion-controlled video generation aims to synthesize temporally coherent videos with user-defined motion guidance, which aims to manipulate the camera motion and object movement. Methods such as CamI2V [42], CameraCtrl [9], and CamCo [34] encode camera trajectories using Plucker coordinates to achieve fine-grained camera path conditioning. Others like ViewCrafter [40] and I2VControlCamera [7] leverage 3D scene reconstruction from single image to generate point cloud renderings that guide camera perspectives during generation. Additionally, there exist some training-free approach like [11, 39]. [14] proposes collaborative diffusion methods which address consistent multi-view synthesis with controllable cameras. Another important aspect of Motion-controlled video generation is object motion control. Various strategies are used to guide object trajectories, e.g., optical flowbased methods, such as DragNUWA [38], Image Conductor [15], DragAnything [32], and MotionBridge [26], utilize sparse or dense flow to control object displacement. Others like MOFA-Video [20] and Motion-I2V [24] directly learn dense motion fields to guide generation. Bounding-boxbased control is employed in Boximator [29] and Directa-Video [36], while methods like LeviTor [28] incorporate depth and clustering for accurate 3D motion guidance. Newer works like ReVideo [19], Peekaboo [12], and Trailblazer [18] expand this paradigm with interactive or trajectory-aware modules. Particularly, training-free method like [39] injects motion trajectories by decomposing the task into out-of-place and in-place motion animation and leverage layout-conditioned image generation for motion generation. Recent works further advances to motion control that simultaneously handles camera and object motion. MotionCtrl [30] introduces explicit modules to support concurrent control, while Motion Prompting [8] encodes motion tracks to guide both scene and subject dynamics. Perception-asControl [5] proposes 3D-aware representation that fuses motion perception with generation. VidCraft3 [43] builds unified, disentangled control across multiple motion modalities. 3. Method We propose ATI (Figure 2), diffusion-based video generation framework that enables Fine-grained feature-level Instruction of Trajectories. Specifically, ATI introduces Gaussian-based motion injector to encode trajectory signals, spanning local, object-level, and camera motion, directly into the latent space of pretrained image-to-video diffusion model. This enables unified and continuous control over both object and camera dynamics. 3.1. Conditional Video Generation Models Recent advances in diffusion models have revolutionized In the generative modeling for both images and videos. video domain, these models aim to synthesize realistic, temporally consistent sequences. When extended to conditional generation, the objective is to generate videos based on specific inputs such as text, images, or motion cues, enabling fine-grained control over content, appearance, and motion. Prominent architectures like Diffusion Transformers (DiT) achieve state-of-the-art performance by integrating spatiotemporal modeling with conditional guidance. Let video be denoted as x0 RT HW C, where is the number of frames. Let represent conditioning signal (e.g., text prompt, an image, or trajectory). The diffusion model defines forward noising process that progressively corrupts the video with Gaussian noise: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI) q(xtx0) = (xt; αtx0, (1 αt)I) (1) (2) where αt = (cid:81)t i=1(1 βi). neural network ϵθ learns to reverse the noising process by predicting the added noise, conditioned on c: Lϵ = ϵ ϵθ(xt, t, c)2 (3) In this report, we choose Seaweed-7B [23] and Wan2.1-14B [27] as the base video generation models, where the denoising model ϵθ is implemented using DiT architecture [21]. 3.2. Gaussian Model for Trajectory Instruction of"
        },
        {
            "title": "Feature",
            "content": "We propose Gaussian model for feature-level instruction of point trajectories. Specifically, for each trajectory point, we assign weight (f li,j,t) to every pixel (i, j) in the latent space. For point trajectory ϕt = (xt, yt) at frame t, we define: (cid:16) (f li,j,t) = exp ϕt(i,j)2 2 σ (cid:17) , where σ is predefined constant. In practice, we set σ = 1 440 so that the Gaussian weight decays to half its maximum at the nearest diagonal pixel. As illustrated in Figure 3, we first pass the input image through the VAE encoder Φ to obtain latent feature map LI = Φ(I) RHW C. (4) We then extract, for each trajectory point, C-dimensional feature vector at its initial position ϕ0 = (x0, y0) by bilinearly sampling from LI whenever (x0, y0) does not lie exactly on an integer grid coordinate. In Figure 3, the bottom-left panel depicts the latent feature grid as colored cells; arrows indicate the precise sub-pixel sampling locations for each trajectory point. The inset in the middle shows how these sampled values assemble into the latent feature vector . Finally, the bottom-right panels visualize the spatial Gaussian maskscentered at the trajectory locations ϕt in subsequent framescomputed as (cid:0)f li,j,t (cid:1) = exp (cid:0)ϕt (i, j)2/(2 σ)(cid:1), (5) which softly distributes the feature across neighboring latent pixels to guide the image-to-video generator with fine-grained control. 3.3. Tail Dropout Regularization In practice, when user-specified point trajectory terminates before the end of the video, the model often hallucinates spurious occluders around the final annotated frame. We attribute this to our training labels: any point that falls off its ground-truth track is marked as occluded or out of frame, which inadvertently teaches the model to introduce occlusions whenever trajectory ends. Figure 3. Trajectory Instruction module computes latent feature from points trajectory. During inference, given the points location in the first frame (i.e., the input image), we sample the feature at that location using bilinear interpolation. We then compute spatial Gaussian distribution for each visible point on its corresponding location in every subsequent frame. To mitigate this, we introduce Tail Dropout Regularizer. During training, with probability (set to 0.2 in our experiments), we sample dropout frame All trajectories and visibility flags are stored to support downstream model training and evaluation. During training, for each video clip, we randomly select 1 to 20 points. td U{0, 1, . . . , }, 4. Experiments where is the full trajectory length. We then truncate the trajectory via set the visibility of that point to 0 after frame td, effectively simulating an early termination. This encourages the model to learn that missing future points do not imply occlusion. Empirically, applying Tail Dropout significantly reduces visual distortions and the appearance of unintended occluders when trajectories end before the final frame at inference time. 3.4. Data Collection We create our training dataset by first processing 5 million highquality video clips, which are filted to contain no scene cuts and to meet strict aesthetic criteriaand then selecting 2.4 million clips exhibiting strong object motion. To generate point trajectory annotation, we apply TAP-Net [6] to each selected clip as follows: 1. On the first frame, uniformly sample = 120 points such that initial pairwise distances are approximately equal. 2. Track these seed points throughout the clip using TAPNet. 3. Record the trajectory of each point tracker on each frame t: Trajectory (xt, yt), the 2D coordinates. Visibility vt {0, 1}, indicating if the point is visible. We integrate ATI into two different video generation frameworks: Seaweed-7B [23] and Wan2.1-14B [27]. In Sec. 4.1, we detail our training and inference setups. We evaluate ATI on both frameworks, providing qualitative comparisons in Sec. 4.2 and quantitative analyses in Sec. 4.3. 4.1. Implementation Details We integrate ATI into two video generation frameworks: Seaweed-7B [23] and Wan2.1-14B [27]. Our implementations build on the pre-trained I2V model by injecting the trajectory instruction between the preprocessing stage and the patchify layer. For both models, we fine-tune all DiT parameters for 50,000 iterations using 64 GPUs with 80 GB of VRAM each. All other training hyperparameters follow the standard I2V fine-tuning setup. Training and inference time. Incorporating the ATI module into our video generation pipeline does not significantly affect training or inference times. After 15,000 iterations, both models achieve satisfactory trajectory-following performance. During inference, both the Seaweed-7B ATI and Wan2.1-14B ATI models generate five-second, 480p video in approximately 8 GPU-seconds. Wan2.1 ATI model details. In the Wan2.1 variant, we handle first-frame conditioning by inserting black frames Figure 4. Object Motion Control. Left: the input image overlaid with user-specified trajectoriesgreen dots mark each trajectorys start point, and arrows mark each end point. Endpoint color encodes trajectory length, indicating that some trajectories span only part of the generated video. Right: five frames uniformly sampled from the generated video. Dot colors serve only to distinguish between trajectories. immediately after the initial RGB image and feeding this RGB sequence into the VAE encoder to extract latent features. As result, the latent stream includes features corresponding to the black frames. We then blend the trajectory instruction features with these black-frame VAE features according to the probability scheme described in Sec. 3.2. Interactive trajectory editor. We provide an interactive trajectory editor for creating and refining point trajectories on single input image. The tool allows users to draw and adjust trajectories, place static points to denote stationary objects, and apply global camera motions such as horizontal panning or zooming in and out. 4.2. Qualitative Results We present video generation results from our ATI model using trajectories created with the tools described in Sec 4.1. Unless otherwise stated, all examples use the Seaweed-7B ATI model. Figure 4 illustrates outputs for trajectories that emphasize object motion and deformation. In the left-hand insets, we overlay the initial frame with the user-defined point trajectories: green dots mark each trajectorys start point, and arrows mark its end. The color of each endpoint also encodes trajectory length, since some paths span only part of the generated video. On the right, we show five frames uniformly sampled from the generated video. Dot colors in each frame serve only to distinguish between trajectories. Figure 5 demonstrates the camera control capabilities Figure 5. Video generation results with camera control. Left: Input image superimposed with user specified trajectories. Right: Five frames uniformly sampled from the generated video. of our ATI model. Moving set of points radially outward from the image center at constant speed creates smooth zoom-in effect. Combining this radial motion with uniform horizontal translation lets us target the zoom on specific region. By anchoring static trajectory on the subject while applying the zoom to the background, we reproduce classic dolly-zoom (as the last example shown). However, if all trajectories consist solely of planar horizontal shifts or zooms, the generated videos content may remain static, exhibiting only 2D camera movement. Figure 6 illustrates videos generated under simultaneous camera and object motion control. You can create these trajectories by first drawing camera-movement paths and then editing selected ones for object motion, or by defining object-motion trajectories first and subsequently applying the camera-movement transformation. Figure 7 shows qualitative comparison between the Wan2.1 ATI model and the Seaweed ATI model. Overall, we observe that Seaweed ATI demonstrates slightly better trajectory-instruction-following ability, which may be attributable to differences in how the input latent is zeroconditioned (see Sec. 4.1). On the other hand, we observer richer motion for the Wan2.1 ATI model on those unconstrained locations. Overall, we observe that ATI achieves high success rate in generating videos that follow the user-specified trajectories, except in the following cases: Very rapid movements (e.g., when point travels half the image width in two frames), which can prevent the model from accurately following the trajectory. Trajectories requiring object disassembly (e.g., forcing an object to split into multiple parts), leading to either failure to follow the trajectory or unnatural distortions (e.g., generating an extra cat head). Figure 6. Video generation results with coherent control of camera and object motion. Left: Input image superimposed with user specified trajectories. Right: Five frames uniformly sampled from the generated video. ATI Base Model Acc@0.05 Acc@0.01 App. Rate Seaweed-7B [23] Wan2.1-14B [27] 59.0 55.9 36.0 34.7 67.9 65.5 Table 1. Quantitative results for trajectory instruction following ability of ATI using different base models. Notably, our approach handles intersecting point trajectories successfully, even when tracking points overlap at certain time steps. We also observe an interesting phenomenon: the ATI model often finds alternative, realistic solutions to satisfy the users trajectory instructions (for example, rotating the camera rather than applying implausible object deformations). 4.3. Quantitative Results As shown in Table 1, we quantitatively evaluate the ability of ATI models to follow user-specified point trajectories. First, we collect 100 imagetrajectory pairs; for each image, we manually draw between one and ten point trajectories. We then evaluate all ATI models on this test set. For each generated video, we use TAP-Net to track the points from the first-frame user inputs and compute the perframe error distance between the TAP-Net outputs and the ground-truth trajectories. We introduce three metrics to assess tracking accuracy: Acc@0.01, the percentage of frames where the point distance is less than 0.01 the image diagonal; Acc@0.05, the percentage of frames where the point distance is less than 0.05 the image diagonal; and Appearance Rate, the proportion of frames in which the tracker correctly predicts point as visible whenever the user-specified trajectory is present. We report the average value of each metric over the entire test set. 5. Conclusion In this paper, we introduce ATI, unified trajectorybased control framework that seamlessly integrates camera movement, object translation, and fine-grained local motion within single latent-space injection module. Our experiments demonstrate that this cohesive approach not only outperforms prior modular methods and commercial systems in both controllability and visual quality, but also remains agnostic to the choice of underlying video Figure 7. Qualitative comparison for ATI video generation with different backend models. generation model. In the future, we will further enhance the control capabilities to ensure that object motion better follows both real-world physics and user inputs."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, and et.al. Lumiere: space-time diffusion model for video generation. 2024. 1 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint, 2023. [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. 2023. [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. 2024. 1 [5] Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Perception-as-control: Fine-grained controllable image animation with 3d-aware motion representation. arXiv preprint arXiv:2501.05020, 2025. 3 [6] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. TAP-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. 4 [7] Wanquan Feng, Jiawei Liu, Pengqi Tu, Tianhao Qi, Mingzhen Sun, Tianxiang Ma, Songtao Zhao, Siyu Zhou, I2vcontrol-camera: Precise video camera and Qian He. arXiv preprint control with adjustable motion strength. arXiv:2411.06525, 2024. 2 [8] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. 3 [9] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 2 [10] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1 [11] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. 2 [12] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via maskedIn Proceedings of the IEEE/CVF Conference diffusion. on Computer Vision and Pattern Recognition, pages 8079 8088, 2024. [13] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, and et.al. Hunyuanvideo: systematic framework for large video generative models, 2025. 1 [14] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2024. 2 [15] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Ying Shan, and Yuexian Zou. Image conductor: Precision control for interactive video synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 50315038, 2025. 2 [16] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 1 [17] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, , and et.al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model, 2025. 1 [18] Wan-Duo Kurt Ma, John Lewis, and Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video In SIGGRAPH Asia 2024 Conference Papers, generation. pages 111, 2024. [19] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. Advances in Neural Information Processing Systems, 37:1848118505, 2024. 3 [20] Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model. arXiv preprint arXiv:2405.20222, 2024. 2 [21] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 3 [22] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, , and et.al. Movie gen: cast of media foundation models, 2025. 1 [23] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. 1, 2, 3, 4, 7 [24] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion In ACM SIGGRAPH 2024 Conference Papers, modeling. pages 111, 2024. 2 [25] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, and Dasong andand et.al Li. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. SIGGRAPH 2024, 2024. [26] Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, and Nanxuan Zhao. Motionbridge: Dynamic video inbetweening with flexible controls. arXiv preprint arXiv:2412.13190, 2024. 2 [27] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, and et.al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 3, 4, 7 [28] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. arXiv preprint arXiv:2412.15214, 2024. 3 [29] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024. 3 [30] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. arXiv preprint, 2024. 2, 3 [31] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. arXiv preprint, 2024. [32] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anyIn European Conference thing using entity representation. on Computer Vision, pages 331348. Springer, 2024. 2 [33] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive autoregressive video diffusion models. 2024. 1 [34] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 2 [35] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn Special directed camera movement and object motion. Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers 24, page 112. ACM, 2024. 2 [36] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userdirected camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 [37] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, and et.al Yang. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1 [38] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2 [39] Shoubin Yu, Jacob Zhiyuan Fang, Jian Zheng, Gunnar Sigurdsson, Vicente Ordonez, Robinson Piramuthu, and Mohit Bansal. Zero-shot controllable image-to-video animation via In Proceedings of the 32nd ACM motion decomposition. International Conference on Multimedia, pages 33323341, 2024. 2, [40] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2 [41] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-tovideo diffusion models. arXiv preprint, 2023. 2 [42] Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024. 2 [43] Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu. Vidcraft3: Camera, object, and lighting control for image-to-video generation. arXiv preprint arXiv:2502.07531, 2025. 3 [44] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024."
        }
    ],
    "affiliations": [
        "ByteDance Intelligent Creation"
    ]
}