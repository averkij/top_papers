{
    "paper_title": "DreamPoster: A Unified Framework for Image-Conditioned Generative Poster Design",
    "authors": [
        "Xiwei Hu",
        "Haokun Chen",
        "Zhongqi Qi",
        "Hui Zhang",
        "Dexiang Hong",
        "Jie Shao",
        "Xinglong Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present DreamPoster, a Text-to-Image generation framework that intelligently synthesizes high-quality posters from user-provided images and text prompts while maintaining content fidelity and supporting flexible resolution and layout outputs. Specifically, DreamPoster is built upon our T2I model, Seedream3.0 to uniformly process different poster generating types. For dataset construction, we propose a systematic data annotation pipeline that precisely annotates textual content and typographic hierarchy information within poster images, while employing comprehensive methodologies to construct paired datasets comprising source materials (e.g., raw graphics/text) and their corresponding final poster outputs. Additionally, we implement a progressive training strategy that enables the model to hierarchically acquire multi-task generation capabilities while maintaining high-quality generation. Evaluations on our testing benchmarks demonstrate DreamPoster's superiority over existing methods, achieving a high usability rate of 88.55\\%, compared to GPT-4o (47.56\\%) and SeedEdit3.0 (25.96\\%). DreamPoster will be online in Jimeng and other Bytedance Apps."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 8 1 2 4 0 . 7 0 5 2 : r DreamPoster: Unified Framework for Image-Conditioned Generative Poster Design Xiwei Hu 1, Haokun Chen 1, Zhongqi Qi 1, Hui Zhang 1,2, Dexiang Hong 1 Jie Shao 1, Xinglong Wu 1 1Intelligent Creation Lab, ByteDance, 2Fudan University Equal contribution, Project lead"
        },
        {
            "title": "Abstract",
            "content": "We present DreamPoster, Text-to-Image generation framework that intelligently synthesizes high-quality posters from user-provided images and text prompts while maintaining content fidelity and supporting flexible resolution and layout outputs. Specifically, DreamPoster is built upon our T2I model, Seedream3.0 [9] to uniformly process different poster generating types. For dataset construction, we propose systematic data annotation pipeline that precisely annotates textual content and typographic hierarchy information within poster images, while employing comprehensive methodologies to construct paired datasets comprising source materials (e.g., raw graphics/text) and their corresponding final poster outputs. Additionally, we implement progressive training strategy that enables the model to hierarchically acquire multi-task generation capabilities while maintaining high-quality generation. Evaluations on our testing benchmarks demonstrate DreamPosters superiority over existing methods, achieving high usability rate of 88.55%, compared to GPT-4o [19] (47.56%) and SeedEdit3.0 [27] (25.96%). DreamPoster will be online in Jimeng and other Bytedance Apps. Date: July 8, 2025 Project Page: https://dreamposter.github.io/ Correspondence: Xiwei Hu, Jie Shao at {huxiwei, shaojie.mail}@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Text-to-Image (T2I) generation have yielded significant progress, facilitating breakthroughs across diverse domains ranging from artistic creation to advertising design. However, models that rely exclusively on text prompts to guide image generation often struggle to meet the nuanced and fine-grained demands of real-world creative tasks, particularly when users seek to build upon pre-existing visual content. In practical design contexts such as the creation of promotional posters, advertisements, and social media covers, there is growing need for Image-to-Image (I2I) generation capabilities that seamlessly integrate user-provided images with textual directives, resulting in cohesive, high-quality visuals. To effectively address these practical requirements, models must extend beyond mere textual comprehension and possess the ability to interpret and transform image content while maintaining both fidelity and design coherence. Despite the progress made in generative models, existing Image-to-Image methods, such as image editing [2, 7, 1 Figure 1 wide variety of posters generated by DreamPoster. Note that all input images are generated by Seedream3.0 [9] 2 11, 14, 34, 38] and unified image generation [10, 18, 19, 30], show several limitations. Prior methods struggle to fully integrate multimodal information, resulting in outputs that lack subject preservation, aesthetic appeal, or layout flexibility. For example, models such as Step1X-Edit [16] and SeedEdit1.6 [22] enforce rigid input-output aspect ratio constraints, restricting their adaptability and limiting their practical utility. Models like GPT-4o [19], while powerful, often produce unstable results in terms of aspect ratio and subject preservation, frequently yielding distorted or poorly composed outputs. Additionally, the design sense in its generated content tends to be suboptimal, often lacking visual harmony or balance. To this end, we present DreamPoster, unified text-to-image generation framework for image-conditioned poster design. DreamPoster generates high-quality posters from user-provided inputs. The inputs include base image, such as product photo or illustration, and text prompts like title, tagline, or description. The model integrates the visual and textual content into coherent and visually appealing layout. Unlike general-purpose generators, DreamPoster is specifically trained to maintain content fidelity while producing an aesthetically pleasing layout. Our approach supports flexible resolutions and aspect ratios, addressing the limitation of prior systems that forced fixed dimensions. DreamPoster is built upon powerful diffusion-based T2I model (Seedream3.0 [9]) and extends it with multimodal architecture and curated training strategy for the poster domain. The contributions of this work are threefold. First, we construct large-scale dataset of posters paired with their source components using novel data curation pipeline. This includes automated filtering for text visibility and aesthetics, and custom Poster Captioner that annotates each poster with detailed textual and layout descriptors. These annotations enable learning of fine-grained typographic hierarchy and design attributes. Second, we design unified generative architecture that concatenates text tokens, image embeddings, and noise in transformer-based diffusion model, allowing DreamPoster to jointly model layout, visual, and textual information. We fine-tune the base diffusion model with progressive multi-stage training regimen: starting with simple text addition, then multi-task mixed editing, and finally high-quality fine-tuning for aesthetic alignment. Third, we conduct comprehensive evaluations against state-of-the-art baselines. DreamPoster significantly outperforms competing methods in human evaluations of prompt following, subject preservation, and design sense. In the usability study, our framework achieved an 88.55% success rate, far higher than GPT-4o (47.56%) and our earlier SeedEdit3.0 model (25.96%). DreamPoster will be deployed in real-world content creation applications (e.g. ByteDances Jimeng platform), enabling users to generate professional-grade posters from minimal inputs."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Image-to-Image Generation Recent advances in Text-to-Image (T2I) models have enabled the generation of high-quality images from textual descriptions. However, T2I models often struggle to meet the fine-grained control and content preservation requirements of real-world creative tasks. To address this limitation, Image-to-Image (I2I) generation tasks have emerged, where the model is required to follow both the input image and the prompt, transforming the image into user-desired version while still preserving the key content or structure of the original image. Depending on the specific application scenario and intent, I2I tasks can be categorized into various types, including but not limited to: image editing [2, 7, 11, 14, 34, 38], subject-driven generation [3, 6, 18, 21, 23, 24, 36, 37], style transfer [26, 29, 31, 35], and so on. Traditionally, most methods have designed and trained specialized expert models for different I2I sub-tasks, resulting in limited flexibility in multi-task scenarios. Recently, some unified approaches [10, 13, 16, 19, 22, 27] have been proposed to cover multiple I2I tasks within single framework. For example, models such as GPT-4o [19], Gemini2.5Flash [10], the SeedEdit series [22, 27], and Step1X-Edit [16] integrate multimodal inputs (e.g., image and text) within unified generative architecture, achieving cross-task image generation and editing capabilities."
        },
        {
            "title": "2.2 Intelligent Poster Generation",
            "content": "Poster design demands not only seamless integration of visual and textual elements, but also imposes higher requirements on layout hierarchy, typographic style, and overall aesthetic harmony. Early automatic poster 3 generation systems [12, 15] were primarily based on rule-driven templates, resulting in limited creative expression and diversity constrained by the templates themselves. With the rapid advancement of diffusion models and large-scale multimodal models, some recent works [4, 5, 8, 17, 20, 25, 28, 32, 33] have attempted to build end-to-end unified frameworks for automated poster generation. However, these methods often struggle with overall aesthetic coordination and precise adherence to design elements, and are typically limited to handling only single sub-task of I2I generation(i.e. poster generation), lacking support for diversified editing operations or multi-style adaptation. To this end, we propose DreamPoster, which leverages unified diffusion transformer architecture to enable intelligent poster generation across multiple tasks, styles, and layouts. Our approach not only ensures content fidelity and layout flexibility, but also significantly enhances the design quality and aesthetic consistency of generated posters, better satisfying the diverse and high-standard requirements of real-world applications."
        },
        {
            "title": "3 Dataset Pipeline",
            "content": "In this section, we describe the systematic pipeline used to curate the DreamPoster dataset. Our pipeline involves data filtering, source-target pairing, and detailed re-captioning, ensuring each sample includes clear text, strong aesthetics, and fine-grained layout annotations. This provides high-quality, structured data for subsequent model training. Figure 2 The overview of our dataset pipeline, which consists of data filtering, data pair construction and re-captioning."
        },
        {
            "title": "3.1 Data Filtering",
            "content": "Training generative model for posters requires paired data of source content (images and text) and the final designed poster. However, such paired data is not readily available. We assemble large dataset of existing high-quality posters and devise an automated pipeline to extract source materials and annotations for each. To ensure data quality, we first filter collection of candidate poster images using OCR and aesthetic scoring. We discard images where OCR cannot recognize any text and those with low aesthetic appeal. Only images above certain aesthetic score and with clear textual content are retained as our clean poster dataset."
        },
        {
            "title": "3.2 Data pair Construction",
            "content": "To construct paired source-target training examples, we create for each poster an equivalent deconstructed source input. Using suite of advanced image processing techniques, we reverse-engineer each final poster into its constituent parts. Specifically, we apply context-aware inpainting to remove textual content from the poster image, producing clean background image that retains the main graphic elements but without any overlaid text. We also perform semantic segmentation to isolate key visual components (such as the primary product or figure in the poster) from the background if needed. The original text from the poster is saved as the source text prompt. In some cases, we retrieve or approximate the exact font and style of the text and render it separately. Through these diverse methods, we ensure the training pairs have structural correspondence while preserving semantic fidelity the model sees how given image and pieces of text can be combined to achieve the final designed poster. Overall, our data pipeline yields large number of training pairs covering diverse poster styles and layouts, all annotated with fine-grained captions for auxiliary guidance."
        },
        {
            "title": "3.3 Poster Captioner",
            "content": "We generate rich annotations for each poster. We train Poster Captioner, specialized image captioning model for text-heavy images. It is able to identify all text strings present and additionally describes their visual attributes such as font style, size, color, and arrangement. The Poster Captioner produces two complementary captions for every poster: I) glyph-focused caption detailing fine-grained typographic properties (e.g. Large bold red title text Summer Sale at top, smaller white subtitle below in cursive font.), and II) layout-focused caption describing the overall spatial arrangement (e.g. poster featuring toy as the main subject. The main title reads Cute Toy, and the subtitle says Buy One Get One Free. The composition is symmetrical.). By recaptioning each poster in this way, we provide the generative model with supervisory signals to learn both the visual appearance of glyphs and the hierarchical layout of design elements."
        },
        {
            "title": "4.1 Model Architecture",
            "content": "DreamPoster is built on diffusion-based image generation backbone that integrates both image and text conditions. We adopt transformer-based diffusion architecture, following recent Diffusion Transformer (DiT) designs that replace the usual UNet with transformer for improved scalability [1, 9]. In our implementation, we treat all inputs as single sequence of tokens: this sequence includes tokens encoding the conditional image (the input image), tokens encoding the text prompt, and tokens for the noisy target image latent. Respective positional embeddings are added so that the model can distinguish the roles of each token. By concatenating text and image embeddings along the sequence dimension, the model can attend to both modalities jointly through self-attention layers, achieving seamless fusion of visual and textual information. This design is inspired by prior works [24] that demonstrated the effectiveness of transformer backbones for multimodal generation, where image patches and text tokens are processed together to allow fine-grained alignment between text and image content. We initialize our model with strong pre-trained text-to-image diffusion model (Seedream 3.0 [9]) and then adapt it to the poster domain. During training, most of the transformer layers are initialized from the pre-trained models weights, but we fine-tune specific portions of the network to specialize in our task. Figure 3 The overview of our proposed DreamPoster. Left: We concatenate condition image embedding and text embedding to generate various posters with the given image. Right: The progressive training strategy of DreamPoster."
        },
        {
            "title": "4.2 Progressive Training Strategy",
            "content": "We train DreamPoster using three-stage curriculum designed to gradually endow the model with more complex design abilities. Each stage introduces new training data and objectives, building on the skills learned in the previous stage: Stage 1: Single-Task Pretraining (Text Addition). In the first stage, we focus on the simplest task: adding text to an image. The model is trained on subset of data where the goal is to place given text onto the input image in reasonable location and style, without significantly altering the image otherwise. This task teaches the model the basic alignment between textual content and background images, essentially learning to write on images. Stage 2: Multi-Task Mixed Training. In the second stage, we expose the model to mixture of different poster generation tasks. In addition to text addition, the training data now includes samples of text modification, text deletion, multi-aspect design, poster restyling, and other variations. We shuffle and mix these tasks during training so that the model learns to handle diverse and complex editing instructions. This multi-task training expands the models capabilities beyond simple caption placement it learns to, for example, replace old text with new text seamlessly, adjust layouts, or apply stylistic changes as directed by the prompt. By the end of this stage, DreamPoster has broad skill set for poster design operations, all under unified framework. Stage 3: Fine-Grained Aesthetic Alignment. The final stage finetunes the model on small high-quality dataset with expert-designed posters, focusing on the subtleties of professional design. Here we train the model to optimize for visual aesthetic quality and layout refinement. After this stage, DreamPoster can produce outputs that are not only correct in content but also visually appealing and coherent at professional level. Through these progressive stages, the training effectively follows curriculum from easy tasks to hard tasks. This strategy allows the model to maintain high generation quality while expanding its functionality. By Stage 3, DreamPoster has been optimized to handle complex design compositions and fine aesthetic details, achieving high level of creative autonomy in poster generation."
        },
        {
            "title": "5.1 Qualitative Results",
            "content": "As illustrated in Figure 4, we show representative qualitative comparisons between DreamPoster and several state-of-the-art poster generation models, including SeedEdit3.0 [27], GPT-4o [19], Gemini2.5Flash [10], Step1X-Edit [16], and SeedEdit1.6 [22]. These examples cover variety of poster types such as product promotion, event advertising, and creative artwork. Compared to all baselines, DreamPoster demonstrates consistently superior alignment between user prompts and generated outputs. In particular, it exhibits more accurate rendering of user-specified texts (e.g. main title, slogan), better spatial integration with input images, and stronger control over typographic hierarchy. In particular, DreamPoster is capable of generating diverse layouts that preserve the visual prominence of the subject while maintaining clean, professional aesthetics. In contrast, it has been observed that models such as GPT-4o and Gemini2.5Flash frequently struggle to maintain the intended aspect ratio or introduce layout artifacts, particularly when aligning multiple textual elements against complex backgrounds. While Step1X-Edit and SeedEdit1.6 enforce stringent input-output dimensional constraints, their inability to generate flexible layouts results in visually rigid outputs that often lack balance. Furthermore, SeedEdit3.0 predominantly treats text as simple overlays, which undermines overall design coherence. In contrast, DreamPoster effectively learns to reconcile content, typography, and layout through progressive training process coupled with rich design supervision. These qualitative observations substantiate our quantitative findings, demonstrating that DreamPoster achieves superior trade-off between prompt fidelity, visual consistency, and design quality across diverse array of real-world scenarios. 6 Figure 4 Qualitative comparisons between DreamPoster and other methods. Notice the advantage of DreamPoster in prompt following, subject preservation, and design sense. Note that GPT-4o and Gemini2.5 exhibit instability in target aspect ratio preservation during image synthesis, while Step1X-Edit and SeedEdit1.6 enforce rigid input-output dimensional alignment through fixed aspect ratio constraints."
        },
        {
            "title": "5.2 Quantitative Results",
            "content": "We conduct comprehensive evaluation of DreamPoster using an internally curated benchmark featuring diverse poster design scenarios, including movie posters, product advertisements, holiday cards, and event promotions. This dataset is designed to assess whether the model can effectively integrate user-provided images and prompts into well-formed, aesthetically pleasing posters. We define three expert evaluation criteriaPrompt Following, Subject Preservation, and Design Senseto assess whether the model (1) adheres to input instructions, (2) preserves the main content of the provided image, and (3) produces coherent and professional layout designs. In addition, we conduct large-scale user study involving 40 participants (30 with graphic design experience), who evaluate the outputs based on the Usability Rate (the percentage of results with fewer than 3 non-satisfaction points) and the Satisfaction Rate (the percentage of fully satisfactory results with no issues). As shown in Fig 5, DreamPoster consistently outperforms all baseline models across all five evaluation dimensions. It achieves an 88.55% Usability Rate, significantly surpassing GPT-4o (47.56%) and SeedEdit3.0 (25.96%). In expert-rated dimensions, DreamPoster attains the highest scores in Prompt Following (3.88/5.00) 7 Figure 5 Weight normalized radar chart comparing DreamPoster with baseline models on five evaluation metrics (higher is better for all). DreamPoster (red) achieves the highest scores across all metrics, indicating superior balance of instruction compliance, content preservation, and design quality, which translates into dramatically higher usability and user satisfaction. and Design Sense (3.19/5.00), while maintaining strong Subject Preservation (3.38/5.00). These results demonstrate that DreamPoster not only faithfully follows user instructions but also excels in visual fidelity and design quality, leading to significantly higher user satisfaction and real-world usability compared to all baselines."
        },
        {
            "title": "6 Conclusion",
            "content": "In this report, we present DreamPoster, unified framework for image-conditioned generative poster design that substantially advances the state-of-the-art in AI-driven poster design. We constructed high-quality dataset through novel pipeline that gathers and refines image-poster pairs, ensuring rich and diverse training data for the task. Building on this foundation, we developed unified generative model with multimodal fusion, seamlessly integrating visual and textual inputs. By gradually fine-tuning the model through stages of increasing task complexity, we achieve stable learning and enhanced creative fidelity in the generated posters. Extensive experiments and evaluations confirm the effectiveness of DreamPoster. The model delivers significant improvements in all key metrics, outperforming strong baselines such as GPT-4o and SeedEdit3.0 by wide margin."
        },
        {
            "title": "References",
            "content": "[1] Black Forest Labs. Flux.1 [dev]. https://huggingface.co/black-forest-labs/FLUX.1-dev, 2024. [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023. [3] Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon Wetzstein. Diffusion selfdistillation for zero-shot customized image generation. arXiv preprint arXiv:2411.18616, 2024. [4] Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu, Ying-Cong Chen, Lei Zhu, and Xinchao Wang. Posta: go-to framework for customized artistic poster generation. In CVPR, 2025. [5] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In ECCV, 2024. [6] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65936602, 2024. [7] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: Diffusion transformer for image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025. [8] Yifan Gao, Zihang Lin, Chuanbin Liu, Min Zhou, Tiezheng Ge, Bo Zheng, and Hongtao Xie. Postermaker: Towards high-quality product poster generation with accurate text rendering. In CVPR, 2025. [9] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. [10] Google DeepMind and Google AI. Gemini 2.5 flash: hybrid reasoning multimodal model. Model card and blog post, 2025. URL https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash. [11] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [12] Peidong Jia, Chenxuan Li, Yuhui Yuan, Zeyu Liu, Yichao Shen, Bohan Chen, Xingru Chen, Yinglin Zheng, Dong Chen, Ji Li, et al. Cole: hierarchical generation framework for multi-layered and editable graphic design. arXiv preprint arXiv:2311.16974, 2023. [13] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [14] Haonan Lin, Yan Chen, Jiahao Wang, Wenbin An, Mengmeng Wang, Feng Tian, Yong Liu, Guang Dai, Jingdong Wang, and Qianying Wang. Schedule your edit: simple yet effective diffusion noise schedule for image editing. Advances in Neural Information Processing Systems, 2024. [15] Jinpeng Lin, Min Zhou, Ye Ma, Yifan Gao, Chenxi Fei, Yangjian Chen, Zhang Yu, and Tiezheng Ge. Autoposter: highly automatic and content-aware design system for advertising poster generation. In ACMMM, 2023. [16] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [17] Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan. Glyph-byt5: customized text encoder for accurate visual text rendering. In ECCV, 2024. [18] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. [19] OpenAI. Introducing 4o image generation, 2025. URL https://openai.com/index/ introducing-4o-image-generation/. [20] Yifan Pu, Yiming Zhao, Zhicong Tang, Ruihong Yin, Haoxing Ye, Yuhui Yuan, Dong Chen, Jianmin Bao, Sirui Zhang, Yanbin Wang, et al. Art: Anonymous region transformer for variable multi-layer transparent image generation. In CVPR, 2025. [21] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. [22] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. [23] Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. Large-scale text-to-image model with inpainting is zero-shot subject-driven image generator. arXiv preprint arXiv:2411.15466, 2024. [24] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [25] Yuxiang Tuo, Yifeng Geng, and Liefeng Bo. Anytext2: Visual text generation and editing with customizable attributes. arXiv preprint arXiv:2411.15245, 2024. [26] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. [27] Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025. [28] Zhendong Wang, Jianmin Bao, Shuyang Gu, Dong Chen, Wengang Zhou, and Houqiang Li. Designdiffusion: High-quality text-to-design image generation with diffusion models. In CVPR, 2025. [29] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [30] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [31] Youcan Xu, Zhen Wang, Jun Xiao, Wei Liu, and Long Chen. Freetuner: Any subject in any style with training-free diffusion. arXiv preprint arXiv:2405.14201, 2024. [32] Hui Zhang, Dexiang Hong, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, and Yu-Gang Jiang. Creatilayout: Siamese multimodal diffusion transformer for creative layout-to-image generation. arXiv preprint arXiv:2412.03859, 2024. [33] Hui Zhang, Dexiang Hong, Maoke Yang, Yutao Chen, Zhao Zhang, Jie Shao, Xinglong Wu, Zuxuan Wu, and Yu-Gang Jiang. Creatidesign: unified multi-conditional diffusion transformer for creative graphic design. arXiv preprint arXiv:2505.19114, 2025. [34] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 2023. [35] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1014610156, 2023. [36] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. [37] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. [38] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 2024."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Intelligent Creation Lab, ByteDance"
    ]
}