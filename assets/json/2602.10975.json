{
    "paper_title": "FeatureBench: Benchmarking Agentic Coding for Complex Feature Development",
    "authors": [
        "Qixing Zhou",
        "Jiacheng Zhang",
        "Haiyang Wang",
        "Rui Hao",
        "Jiahe Wang",
        "Minghao Han",
        "Yuxue Yang",
        "Shuzhe Wu",
        "Feiyang Pan",
        "Lue Fan",
        "Dandan Tu",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 1 ] . [ 1 5 7 9 0 1 . 2 0 6 2 : r Published as conference paper at ICLR 2026 FEATUREBENCH: BENCHMARKING AGENTIC CODING FOR COMPLEX FEATURE DEVELOPMENT Qixing Zhou1,2, Jiacheng Zhang1,2, Haiyang Wang2, Rui Hao1, Jiahe Wang1, Minghao Han1,2 Yuxue Yang1, Shuzhe Wu2 , Feiyang Pan2 , Lue Fan1 , Dandan Tu2 , Zhaoxiang Zhang1 1 Institute of Automation, Chinese Academy of Sciences 2 Huawei Technologies Co., Ltd haiyang.wang@huawei.com lue.fan@ia.ac.cn Code: github.com/LiberCoders/FeatureBench Dataset: FeatureBench Project Page: Beyond bug fixing and ship real features."
        },
        {
            "title": "ABSTRACT",
            "content": "Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover limited task scope, e.g., bug fixing within single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-theart agentic model, such as Claude 4.5 Opus, which achieves 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training."
        },
        {
            "title": "INTRODUCTION",
            "content": "Software development is rapidly evolving with the advent of large language models (LLMs) (Sapkota et al., 2025), marking shift toward end-to-end agentic coding systems (Wang et al., 2025a). Recent advances, such as Claude Code (Anthropic, 2025b) and Qwen Code (Qwen, 2025) exemplify this evolution by introducing requirement-driven agents that autonomously plan, execute, and interact with external tools (e.g., compilers) to iteratively tackle complex software development tasks (Gong et al., 2025), thereby relegating human intervention to supervisory role. Recently, various benchmarks have been introduced to assess this paradigm shift, including SWEbench (Jimenez et al., 2024), PaperBench (Starace et al., 2025), and GitTaskBench (Ni et al., 2025). While these benchmarks have made significant contributions to task-oriented agentic coding, they are limited either by the narrow focus on bug-level scenarios or by reliance on handcrafted generation pipelines. As agentic coding expands toward more complex settings, such as feature-level *Equal contribution. Corresponding Author. 1 Published as conference paper at ICLR 2026 Figure 1: a) The agent must implement directly callable feature based on the task description and interface definitions, either by developing from scratch or extending an existing repository. b) Our benchmark shows that even Claude Opus 4.5 achieves only 11.0% solution rate. development, these constraints hinder their ability to fully capture the capabilities of frontier code agents. Therefore, there is need to build challenging benchmark that broadens evaluation scope to feature-level scenarios, supported by automated collection toolkits to facilitate its future usage. Constructing such benchmark poses nontrivial challenges. Effective and execution-based evaluation of feature-level agentic coding generally depends on clearly defined functional interfaces to resolve ambiguities between the implementation and test criteria. However, these specifications are often absent in previous benchmarks. Furthermore, creating an automated data collection toolkit to support the scaling of benchmarks introduces additional complexities. Conventional pull request (PR)-based methods (Jimenez et al., 2024; Pan et al., 2025; Jain et al., 2025b) are ineffective in capturing complete feature patches, as these often span multiple PRs scattered across the timeline, making them difficult to associate. Moreover, many PRs lack tagging, hindering the reliable identification of feature contributions. Notably, PR-driven methods are inherently tied to the historical trajectory of commit submissions, limiting the tasks to fixed development combinations. Motivated by these shortcomings, we introduce FeatureBench , challenging benchmark that targets feature-oriented agentic coding scenarios. It integrates an execution-based evaluation pipeline and test-driven toolkit for automatically collecting instances from Python repositories. As shown in Table 1, our bench provides the following characteristics: 1. Feature-oriented real-world software development. Unlike SWE-bench, which is dominated by bug-fixing issues with only about 1822% of its instances corresponding to feature requests, our benchmark is explicitly designed to target systematic feature-level agentic coding. As shown in Figure 1, given human-like clear requirements (e.g., interface signatures and high-level functional descriptions), our task entails the implementation of new capabilities either within an existing codebase or as standalone modules. For example, adapting the Transformers library (Wolf et al., 2020) for compatibility with Qwen3 (Yang et al., 2025a) or engineering FlashAttention (Dao et al., 2022) from scratch. 2. Reliable execution-based evaluation. Highly ambiguous requirements without explicit function signatures often introduce multiple valid implementations that are incompatible with the interface expected by unit tests. This misalignment complicates execution-based evaluation and typically necessitates additional manual inspection or LLM-based judgement (Starace et al., 2025; Seo et al., 2025). To mitigate this issue, we adopt test-driven formulation strategy when constructing requirements. Each prompt explicitly specifies the clear interface definitions, import paths, and the descriptions of expected behaviors, and enforces that the solution must be directly callable, as illustrated in Figure 1. This method guarantees that correct implementation will pass all associated tests, thereby enabling automated execution-based evaluation. 3. Scalable instance collection toolkit. To support the extensible creation of feature-oriented, realistic evaluation environments with fail-to-pass (F2P) and pass-to-pass (P2P) tests, as introduced in SWE-bench, we develop an automated generation pipeline driven by unit tests. The pipeline begins by selecting and executing F2P and P2P tests, followed by the construction of dependency graph through dynamic tracing. Based on the traced dependencies, the system automatically extracts the implementation of the targeted features while ensuring the integrity of other features. The final problem statements are then synthesized. This approach enables us to generate naturally 2 Published as conference paper at ICLR 2026 Benchmark BigCodeBench (Zhuo et al., 2025) LiveCodeBench (Jain et al., 2025a) FullStackBench (Cheng et al., 2024) SWE-bench (Jimenez et al., 2024) PaperBench (Starace et al., 2025) Paper2Coder (Seo et al., 2025) MLEBench (Chan et al., 2025) DevEval (Li et al., 2025) GitTaskBench (Ni et al., 2025) FeatureBench (ours) Feature-oriented Execution-based Agentic Coding Evaluation Scalable Instance Continually Updatable Collection Instance Number 1140 454 3374 500 20 90 72 20 200 Table 1: comparison of FeatureBench with current coding benchmarks reveals that our bench emphasizes feature-level realistic software development. It leverages an execution-based evaluation pipeline and integrates test-driven toolkit for the automatic generation of task instances. verifiable environments from any Python repository in scalable and flexible manner, free from the constraints of the availability and predefined trajectory of human-written PRs or commits. 4. Continually updatable. Building on our collection toolkit, FeatureBench supports continual supply of new task instances, enabling evaluation on tasks created after their training date, thus mitigating the risk of contamination. Using this pipeline, we have curated benchmark with 200 evaluation instances and 3825 verifiable environments, created from May 2022 to September 2025, sourced from 24 real-world GitHub repositories in the first version of our benchmark. We evaluate multiple state-of-the-art LMs on FeatureBench and find that they fail to solve all except the simplest tasks. Using the Codex agent framewor, GPT-5.1-Codex (medium reasoning) successfully completes 12.5% of the task cases. Furthermore, we carried out comprehensive experiments, offering insights into potential improvement directions on our benchmark. In nutshell, our contributions are three-fold: 1) We introduce FeatureBench, benchmark for agentic coding that evaluates LLMs on solving feature-level, real-world complex tasks through an automated, execution-based evaluation pipeline. 2) We release scalable, test-driven toolkit for instance collection that integrates seamlessly with our benchmark and automatically generates verifiable environments from Python repositories. Using this toolkit, we construct benchmark comprising 200 evaluation tasks and 3825 executable environments from 24 open source GitHub repositories. 3) We benchmark state-of-the-art LLMs, including both openand closed-source variants, and perform in-depth analysis to identify and highlight remaining challenges."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Agentic Coding Benchmarks. The most widely adopted benchmark for agentic coding is SWEbench (Jimenez et al., 2024), whose verified subset has emerged as standard for assessing LLMs. Although originally highly challenging, its success rate has increased from below 10% to over 70% within year, reflecting rapid advances in LLM-based agents (Anthropic, 2025b; Yang et al., 2025a). Despite its importance, SWE-bench has notable drawbacks. It mainly focuses on bug fixing, with comparatively limited coverage of feature development tasks, which often span multiple PRs. Other benchmarks address narrower domains or predefined workflows. PaperBench (Starace et al., 2025) and MLE-Bench (Chan et al., 2025) focus on machine learning problems but rely on expert curation or high-quality cases from Kaggle. GitTaskBench (Ni et al., 2025) broadens task coverage but offers only 54 expert-designed tasks, while DevEval (Li et al., 2025) spans the development lifecycle but enforces fixed workflows with 22 handcrafted tasks. To tackle the above problems, we propose challenging benchmark specifically designed for feature-oriented agentic coding scenarios. This benchmark integrates an execution-based evaluation pipeline and an automated toolkit that collects instances from Python repositories in scalable manner. Scalable Collection Pipeline. verifiable environment is crucial for achieving better agentic coding. SWE-Gym (Pan et al., 2025) follows the pull-request based approach of SWE-bench, whereas R2E-Gym (Jain et al., 2025b) derives tasks from commits by synthesizing tests and back-translating code changes into problem statements with LLMs. These approaches mitigate scalability concerns but provide limited guarantees of evaluation quality. SWE-Smith (Yang et al., 2025b) synthesizes 3 Published as conference paper at ICLR 2026 Figure 2: Given GitHub repository, our automated toolkit initializes the development environment via Docker. For each benchmark instance, it validates and selects fail-to-pass and pass-to-pass tests. Then, the system performs dynamic tracing to capture runtime behavior and construct an object dependency graph. Leveraging this graph, the toolkit synthesizes code patches, derives corresponding pre-solved codebases, and formulates final problem statements. This pipeline has yielded 200 benchmark tasks and 3825 executable environments from 24 GitHub repositories. tasks from repositories using heuristics such as LLM generation, procedural modifications, or pullrequest inversion. SWE-Flow (Zhang et al., 2025) synthesizes data based on fail-to-pass tests but neglects pass-to-pass tests and does not ensure the proper functioning of other features in undeveloped codebases, resulting in discrepancies compared to actual development settings. Although successful, none of them can generate tasks that are both feature-oriented and reflective of real-world development scenarios. Our benchmark addresses these gaps by providing test-driven, scalable tool for generating feature-level agentic coding tasks, complemented by rigorous post-verification that ensures the integrity of undeveloped codebases, consistent with real-world scenarios."
        },
        {
            "title": "3 FEATUREBENCH",
            "content": "FeatureBench establishes benchmark for evaluating the capabilities of code agents in end-to-end software development tasks. The benchmark requires agents to interpret high-level goals and their associated code interfaces, autonomously manage execution environments, and synthesize correct and callable implementations either within existing codebases or as standalone solutions. Constructed with minimal human intervention, the benchmark leverages an automated pipeline that derives feature-oriented coding tasks from open-source repositories, thereby extending the scope of agentic coding beyond bug fixing to encompass feature development."
        },
        {
            "title": "3.1 FEATURE-ORIENTED AGENTIC CODING",
            "content": "Task Formulation. As illustrated in Figure 1, each instance in FeatureBench provides the agent with comprehensive problem statement. This includes high-level task description, specified functional interface, blacklist of prohibited URLs to mitigate potential cheating of agents, and dockerfile defining the execution environment. The agent is then tasked with generating solution that addresses the problem, whether by editing existing code or implementing from scratch. Notably, 4 Published as conference paper at ICLR 2026 to facilitate automated and unambiguous evaluation, the agents output is required to be directly callable module. Its invocation path, function signature, including input and output variables as well as comprehensive annotations, are all explicitly provided within the problem statements. Difficulty. In realistic settings, software development may proceed either by extending an existing codebase or by implementing feature entirely from scratch. FeatureBench reflects these two scenarios with two difficulty levels. Level 1 (L1) consists of incremental development within an existing repository based on task requirements, while Level 2 (L2) requires constructing the same functionality from scratch. Metric Design. Our evaluation protocol follows the established setup of SWE-bench (Jimenez et al., 2024), where each agent-generated solution is validated by executing its associated fail-topass (F2P) and pass-to-pass (P2P) tests. task is considered resolved when the proposed solution successfully passes all these tests. We report three primary metrics: (1) Resolved Rate, the proportion of tasks fully solved, like SWE-bench; (2) Passed Rate, the average fraction of fail-to-pass tests passed per task, serving as soft indicator of partial correctness; (3) Token IO, the average number of input and output tokens consumed, reflecting the computational efficiency of the agent."
        },
        {
            "title": "3.2 BENCHMARK COLLECTION",
            "content": "Execution Environment Configuration. To rapidly set up an environment for given repository, we manually specify installation commands (taking approximately three minutes), rather than relying on the more error-prone and uncontrollable approach of having the agent search for installation methods itself. Automated scripts are then used to configure the environment and package the repository into Docker image. The benchmark includes 24 widely downloaded PyPI packages across various domains, such as visualization libraries and LLM infrastructure. Notably, human intervention is required only for this step of the pipeline, and the total human labor required to complete this for all 24 repositories amounts to less than one hour. Constructing Fail-to-pass and Pass-to-pass Tests. We construct benchmark instances by identifying candidate test files in the repository using pytests collection function, followed by validation through execution. For each instance, validated test files are designated as fail-to-pass (F2P) tests, as introduced in SWE-bench. These tests fail in the undeveloped repository but succeed once the agent correctly implements the target functionality. To additionally assess incremental development capability, we include randomly sampled validated files as pass-to-pass (P2P) tests, which are expected to pass both before and after the agents solution. Since single test file typically corresponds to one functional implementation, is usually set to one in our setting. Test-Driven Code Patch Extraction. Obtaining the pre-solved codebase together with the corresponding code patch requires isolating the functionality linked to the F2P tests. However, the inherent ambiguity of functional boundaries in real-world codebases poses significant challenge. Naively extracting relevant code fragments risks inadvertently disrupting other well-established features. As depicted in Figure 2, our approach mitigates this issue by incorporating P2P tests to accurately identify code modules required by other functions or those serving as foundational components of the repository. The detailed implementation is as follows: Construct the object dependency graph. We initiate the process by executing the available F2P and P2P test cases for given benchmark instance. During runtime, we employ Pythons builtin tracing facility to capture function call events and their dependencies. From this trace, we construct an object dependency graph in which each node represents function and is enriched with metadata, including unique identifier, source location, list of dependent functions, and binary flag indicating if the function was triggered during P2P tests. Graph traversal and node classification. To distinguish functional components, large language model analyzes the F2P test files and separates the imported functions related to the target feature from those that serve supporting roles in the testing process. The nodes identified as central to the undeveloped feature serve as the initial entry points for breadth-first traversal of the graph. During this traversal, nodes are systematically classified: those encountered in P2P executions are designated as remained, while nodes not observed in P2P runs are classified as extracted. Extracting the code. The traversal process yields subset of graph nodes identified as relevant to the intended functionality. In the final stage, the corresponding segments of source code are Published as conference paper at ICLR 2026 Scaffold Model Lite Full % Passed % Resolved # Token I/O % Passed % Resolved # Token I/O Qwen3-Coder-480B-A35B-Instruct DeepSeek-V3.2 Gemini-3-Pro-Preview Gemini-3-Pro-Preview OpenHands OpenHands OpenHands Gemini-CLI Claude Code Claude Opus 4.5 GPT-5.1-Codex Codex Claude Opus 4.5 OpenHands 38.31 35.94 45.14 43.38 59.12 60.22 67.18 6.7 6.7 10.0 10.0 20.0 20.0 20. 2.6M / 16k 3.1M / 24k 6.0M / 41k 2.6M / 13k 9.0M / 35k 6.6M / 39k 8.8M / 29k 24.55 26.30 30.08 32.43 43.29 41.66 45.53 3.5 5.5 4.5 5.0 11.0 12.5 10.5 2.0M / 14k 3.1M / 23k 6.2M / 40k 2.5M / 12k 7.5M / 34k 6.3M / 39k 8.1M / 29k Table 2: The performance of various frontier large models combined with advanced agentic frameworks on the Lite and Full evaluation sets of our benchmark. Models marked with use low reasoning, and use medium reasoning. extracted from the original codebase. This operation produces modified codebase devoid of the target functionality and complementary code snippet that realizes the previously absent feature. Post Verification. To ensure the successful extraction of the target functionality from the codebase without affecting other components, we implement rigorous verification process. The first step involves validating the pre-modified codebase by ensuring that it passes all P2P tests, thereby confirming its integrity. Simultaneously, it must fail all F2P tests, demonstrating that the target functionality has been effectively removed. Following this, we assess the accessibility of all utility functions required for the F2P tests in the modified codebase. This step ensures that the changes made are confined to the target functionality and do not inadvertently impact other core dependencies. Finally, reapplying the patch to the undeveloped codebase should allow all tests to pass, confirming the patchs correctness. Problem Statement Generation. By leveraging the extracted code snippet, the pre-modified codebase, and the corresponding unit tests, we automatically generate the problem statement for each instance. This procedure includes the derivation of the feature signatures, which encompass the types of input and output variables, alongside the functional description as inferred from the code docstrings. In the absence of such docstrings, we employ large language model to generate them directly from the code snippet. Further details can be found in the appendix. To this end, our pipeline automatically generates the core components of each instance: natural language problem statement, an undeveloped codebase, verified code patch, and suite of unit tests corresponding to required features. The sole manual intervention required is the specification of the repositorys installation procedure, process that takes approximately three minutes per repository."
        },
        {
            "title": "3.3 BENCHMARK CONFIGURATION",
            "content": "Full Set. Leveraging our pipelines, we configured the number of P2P test files to five and curated 3825 coding environments derived from 24 Python repositories. To ensure the benchmark meaningfully challenges best-performing agents, we restricted inclusion to tasks exceeding 100 lines of pending implementation, encompassing at least 10 F2P test points, with test files initially committed after May 2022. This filtering yielded 200 high-quality instances comprising the full set. Lite Set. Evaluating LMs on our bench can be time-consuming and, depending on the model, require costly amount of compute or API credits, as illustrated in Table 2, where the average number of input tokens approaches the million-token mark. To facilitate wider adoption of FeatureBench , we randomly selected 30 instances from the full set to create streamlined lite set."
        },
        {
            "title": "4.1.1 BASELINE",
            "content": "To establish strong baselines, we adopt the OpenHands (Wang et al.) framework for software development agents, which tops the SWE-bench. In the experiments, the maximum of steps per task is 6 Published as conference paper at ICLR 2026 SWE-bench Ours Problem Texts Length (Words) 195.1 4818.0 Gold Solution # Lines # Files # Functions Tests # Fail to pass (test points) # Total (test points) 32.8 1.7 3 9.1 120.8 790.2 15.7 29.2 62.7 302.0 Table 3: Average numbers characterizing different attributes of SWE-bench task instance, as well as our FeatureBench (L1 set). Figure 3: Distribution of our benchmark across 24 GitHub repositories. Model SWE-Bench Verified FeatureBench subset % Resolved mini-SWE-agent OpenHands % Passed % Resolved OpenHands # Token I/O DeepSeek-V3.2 Qwen3-Coder-480B-A35B-Instruct Gemini-3-Pro-Preview Claude Opus 4.5 60.00 55.40 74.20 74.40 - 69.60 - - 22.98 23.46 30.05 41.08 0.0 0.0 0.0 5.2 3.8M / 25k 2.3M / 16k 6.7M / 45k 9.7M / 34k Table 4: Compare the performance of the frontier agents on SWE-bench and our FeatureBench, using subset of our benchmark with repositories shared with SWE-bench for fair comparison. set as 500 by default. Internet access is freely available, while no specific browser-use tools are provided. To ensure the integrity of our evaluation, robust anti-cheating mechanisms are incorporated to prevent agents from assessing the ground-truth repositories (see the appendix for details). We evaluate seven scaffold+model configurations with frontier LLMs, including DeepSeekV3.2 (DeepSeek, 2025), Qwen3-Coder-480B-A35B-Instruct (Qwen Team, 2025), Gemini-3-ProPreview (low reasoning) (Google, 2025a), Claude Opus 4.5 (Anthropic, 2025a), and GPT-5.1-Codex (medium reasoning) (OpenAI, 2025b) under representative agentic scaffolds (OpenHands (Wang et al., 2025b), Gemini-CLI (Google, 2025b), Claude Code (Anthropic, 2025b), and Codex (OpenAI, 2025a)). The results are presented in Table 2. As can be seen, even the most capable settings, i.e., Claude Code (routing) + Claude Opus 4.5 and Codex + GPT-5.1-Codex (medium reasoning), resolve only 11.0% and 12.5% of the tasks on the Full set, respectively. This underscores the highly challenging nature of the feature-oriented development tasks in our FeatureBench, which require agents to write substantial amounts of code and pass comprehensive test suites. For more nuanced evaluation, we further analyze passed rates and token consumption by different LLMs. The passed rates, while remaining at low level of below 50%, are much higher than the resolved rates. This discrepancy indicates that current agents often produce seemingly plausible solutions with large underlying gap from truly solving the problem, which accounts for the common need of tedious debugging for AI-generated code. Regarding token consumption, all LLMs consume over one million input tokens. Given the low resolved rates, this reflects the extremely low efficiency of existing agents in tackling real-world development tasks, which is thus an important topic for future research. In addition, high consistency is observed in the rankings of different LLMs across the Lite and Full sets in terms of both pass and resolved rates, demonstrating the representativeness of the Lite set."
        },
        {
            "title": "4.1.2 COMPARISON WITH SWE-BENCH",
            "content": "Compared with the SWE-bench (Jimenez et al., 2024), our FeatureBench introduces more challenging suite of development tasks. It encompasses 16 additional popular repositories apart from 8 repositories originally covered by the SWE-bench, the full list of which is shown in Figure 3. Table 3 presents comparative statistics illustrating the task difficulties across the two benchmarks. Specifically, the tasks in our benchmark exhibit substantial increase of complexity in terms of the length of problem texts, number of lines, files and functions to be edited as well as the number of tests to pass. These enhancements necessiate agents with strong long-context understanding and management capabilities alongside comprehensive problem analysis to handle diverse test cases. 7 Published as conference paper at ICLR 2026 Models Gemini-3-Pro-Preview GPT-5.1-Codex Gemini-3-Pro-Preview GPT-5.1-Codex Original Verified % Resolved % Passed 10.0 16.7 10.0 20.0 42.4 53.9 43.4 60.2 Table 5: An ablation study to evaluate the necessity of manual verification for the examples generated by our system. Models marked with use low reasoning, and use medium reasoning. Models Steps % Resolved % Passed Gemini-3-Pro-Preview Qwen3-Coder-480B-A35B-Instruct 50 100 500 50 100 6.7 6.7 10.0 3.3 3.3 6.7 22.9 43.8 45.1 28.9 30.4 38.3 Table 6: An ablation study on the max execution steps of OpenHands with Gemini-3Pro-Preview and Qwen3-Coder-480B-A35BInstruct in Lite Set. Models marked with use low reasoning. Model Without Interface Visible Unit Tests % Resolved % Passed # Token I/O % Resolved % Passed # Token I/O Gemini-3-Pro-Preview GPT-5.1-Codex 3.3 (-6.7) 16.7 (-3.3) 25.3 (-18.1) 42.0 (-18.2) 7.0M / 10K 60.0 (+50.0) 7.6M / 38K 63.3 (+43.3) 80.6 (+37.2) 80.9 (+20.7) 6.9M / 18K 8.2M / 46K Table 7: Performance comparison of lite set with visible unit tests and without interface. Models marked with use low reasoning, and use medium reasoning. For more grounded analysis, we further compare the performance of agents on the SWE-bench and our FeatureBench . To draw more aligned comparison, we construct subset of our benchmark including only repositories shared with SWE-bench. The results in Table 4 reveals stark performance gap between the two benchmarks in terms of resolved rate. Specifically, the most capable Claude Opus 4.5 only resolves 5.2% of the tasks in our FeatureBench subset in contrast to the 74.40% on the SWE-bench. This indicates the highly challenging nature of our benchmark, which provides considerable room for future improvement and establishes rigorous testbed to measure the upper bound of existing agents."
        },
        {
            "title": "4.1.3 FAILURE CASES ANALYSIS",
            "content": "We conduct failure case analysis based on the results in our full set from the Claude Opus 4.5 model, leading to the following findings. Limitations in Code Reasoning. As shown in Figure 4, the dominance of NameError suggests that current LLMs still struggle with cross-file dependency resolution. When feature spans multiple files, models often focus on local edits without consistently re-establishing all necessary references, leading to unresolved symbols and frequent name-related failures. This highlights key limitation in maintaining coherent program context beyond single file. The Idle Habits of LLMs. We also find that current LLMs exhibit tendency toward laziness. For example, they often resort to guessing (even hallucinating) the interface or attributes of components defined across files, rather than performing the actual file reading required to retrieve precise prototypes and members. This behavior leads to considerable number of both TypeError and AttributeError occurrences. Appropriate Information in FeatureBench. Among the remaining failures, AssertionError becomes the most frequent category. This suggests that substantial portion of LLM-generated solutions can run to the assertion checkpoints without earlier runtime crashes. This result underscores that FeatureBench can effectively provide the LLMs with appropriate information to generate complete programs."
        },
        {
            "title": "4.2.1 ANALYZING THE QUALITY AND NECESSITY OF OUR BENCHMARK DESIGN.",
            "content": "Without Interface. We performed an ablation study to assess the role of explicit interface specification in agent performance. For controlled comparison, we employed the lite set, systematically removing function signatures and call path annotations from the prompts. As shown in Table 7, this 8 Published as conference paper at ICLR 2026 Difficulty Scaffold Models % Resolved % Passed Qwen3-Coder-480B-A35B-Instruct OpenHands DeepSeek-V3.2 OpenHands Claude Opus 4.5 OpenHands Gemini 3 Pro OpenHands Gemini 3 Pro Gemini-CLI GPT-5.1-Codex Codex Claude Code Claude Opus 4.5 Qwen3-Coder-480B-A35B-Instruct OpenHands DeepSeek-V3.2 OpenHands Claude Opus 4.5 OpenHands Gemini 3 Pro OpenHands Gemini 3 Pro Gemini-CLI GPT-5.1-Codex Codex Claude Code Claude Opus 4.5 3.6 4.8 11.4 4.2 4.8 13.9 11.4 2.9 5.9 5.9 5.9 5.9 5.9 8.8 22.4 20.8 46.2 29.0 32.1 43.0 43. 35.2 32.6 42.2 35.6 34.0 35.2 41.9 L1 L2 Table 8: Performance comparison of tasks with different difficulty levels in FeatureBench. Models marked with use low reasoning, and use medium reasoning. Figure 4: Failure modes of the Claude Opus 4.5. Models marked with use low reasoning, and use medium reasoning. removal leads to marked decline in task success rates. The results confirm that clearly defined interfaces are critical for enabling effective reasoning and program synthesis by LLM-based agents. Sample Quality. Our automated data generation pipeline yields high-quality, evaluation-ready samples with minimal human intervention, supported by rigorous post-verification process. To assess the fidelity of these samples, we conducted an ablation study in which senior engineer with five years of industry experience in AI infrastructure and system architecture independently revised the prompts in the lite set. The verification details are provided in Appendix Figures 20 and 21. As shown in Table 5, model performance on the manually revised subset is highly consistent with the original dataset. These results affirm the reliability and robustness of our automated data pipeline. Lines of Code and Task Initial Commit Date. Figure 5 explores the relationship between task pass rates, initial commit timestamps, and the number of lines of code required for task completion. We observe clear negative correlation between pass rate and code length, indicating that tasks involving more lines of code are inherently more challenging for current large models. In contrast, task performance shows minimal dependence on commit time, likely because the task set remains largely unexplored To further understand by existing models. why commit time has little influence, we analyze how feature complexity evolves over time. Specifically, the lower panel of Figure 5 plots the normalized trends of code length and pass rate across commit periods. The two normalized curves exhibit highly similar fluctuations, reinforcing that variation in task performance is driven far more by feature complexity than by commit time. However, as agentic systems increasingly participate in feature development workflows, the risk of data leakage may become more pronounced and should be monitored in future benchmark design. Figure 5: The pass rate of Claude Opus 4.5 in our benchmark varies with the number of code lines and task creation time. Comparison between L1 and L2 Subset. Comparison between L1 and L2 Subsets. Our benchmark defines two evaluation settings: L1, where new functionalities are incrementally added to an existing codebase, and L2, where functionalities are implemented entirely from scratch. All conditions are held constant across both settings, except for the presence or absence of initial code context. This distinction leads to notably different levels of reasoning complexity. In the L1 setting, the agent still has access to most of the original codebase except for the functions and classes removed along the traced execution path. This partial repository shows how the feature fits into the surrounding code and gives the agent contextual clues about expected behavior. As result, L1 tasks are more guided, since only the missing implementations need to be completed. In contrast, L2 tasks remove Published as conference paper at ICLR"
        },
        {
            "title": "Recall",
            "content": "F1 Score Accuracy"
        },
        {
            "title": "Value",
            "content": "81.03% 89.24% 84.94% 91.74% Table 9: Performance of the LLM classifier for identifying top-level tested objects. all surrounding code. The agent does not see any part of the original repository and must rely only on the interface to implement the required functionality. Without the structure provided by the existing codebase, the agent has to reconstruct the full logic and organization of the feature entirely from scratch, which makes L2 substantially more difficult. As shown in Table 8, the from-scratch (L2) setting is more challenging, with lower resolved rates: performance on L2 varies little across settings, suggesting that removing the codebase structure creates common bottleneck that hampers coherent multi-step reasoning and end-to-end implementation. Accuracy of LLM-based Top Import Classification. To validate the reliability of our LLM-based classifier for identifying top-level tested objects in test file, we conducted quantitative evaluation against expert annotations. Domain experts evaluated all 605 import statements in the Lite Set and identified 158 of them as top-level tested objects. The details of the procedure are provided in Appendix Figure 19. Table 9 reports the performance of the LLM classifier. These results indicate that LLMs can accurately identify tested objects at scale, supporting the use of LLM-based classification in our data construction pipeline."
        },
        {
            "title": "4.2.2 ANALYZING THE KEY FACTORS IN BUILDING END-TO-END CODEAGENTS",
            "content": "Visible Unit Tests. We conducted an ablation study to assess the impact of providing accurate unit tests on agent performance in complex coding tasks. In this setting, the agent was given access to ground-truth unit tests alongside the Lite set. As shown in Table 7, both task success rates and pass rates increased significantly. These findings underscore the importance of high-quality unit test generation as key factor in enabling robust agentic coding. Longer Execution Steps. Table 6 reports the effect of increasing the maximum number of execution steps on model performance. Increasing the maximum step size from 50 to 100 results in notable performance gains for both Gemini-3-Pro-Preview and Qwen3-Coder-480B-A35B-Instruct. However, beyond this threshold, the improvements become marginal."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce FeatureBench , novel benchmark designed to evaluate the capabilities of LLM-powered agents in realistic, feature-oriented software development scenarios. Leveraging test-driven task extraction and execution-based evaluation, FeatureBench overcomes key limitations of existing benchmarks by enabling greater task diversity, scalability, and verifiability. Empirical results reveal that current agentic systems face persistent challenges in planning, reasoning, and managing long-horizon tasks. With its extensible and automated design, FeatureBench offers not only rigorous evaluation framework but also foundation for the development of next-generation agentic coding models."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude opus 4.5, 2025a. URL https://www.anthropic.com/news/ claude-opus-4-5. Anthropic. Claude code, 2025b. URL https://code.claude.com/docs/en/overview. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Aleksander Madry, and Lilian Weng. MLEbench: Evaluating machine learning agents on machine learning engineering. In International Conference on Learning Representations, 2025. 10 Published as conference paper at ICLR 2026 Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng, Aoyan Li, Bo Li, et al. Fullstack bench: Evaluating llms as full stack coders. arXiv preprint arXiv:2412.00535, 2024. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, 2022. DeepSeek. Deepseek-v3.2, 2025. URL https://api-docs.deepseek.com/news/ news251201. Yaxin Du, Yuzhu Cai, Yifan Zhou, Cheng Wang, Yu Qian, Xianghe Pang, Qian Liu, Yue Hu, and Siheng Chen. Swe-dev: Evaluating and training autonomous feature-driven software development. arXiv preprint arXiv:2505.16975, 2025. Jingzhi Gong, Vardan Voskanyan, Paul Brookes, Fan Wu, Wei Jie, Jie Xu, Rafail Giavrimis, Mike Basios, Leslie Kanthan, and Zheng Wang. Language models for code optimization: Survey, challenges and future directions. arXiv preprint arXiv:2501.01277, 2025. Google. Gemini 3, 2025a. URL https://ai.google.dev/gemini-api/docs/ gemini-3. Google. Gemini cli, 2025b. URL https://github.com/google-gemini/gemini-cli. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In International Conference on Learning Representations, 2025a. Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2e-gym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. Conference on Language Modeling, 2025b. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In International Conference on Learning Representations, 2024. Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, et al. Prompting large language models to tackle the full software development lifecycle: case study. In International Conference on Computational Linguistics, 2025. Ziyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun, Hongzhang Liu, et al. GitTaskBench: benchmark for code agents solving real-world tasks through code repository leveraging. arXiv preprint arXiv:2508.18993, 2025. OpenAI. Codex, 2025a. URL https://openai.com/index/introducing-codex. OpenAI. Gpt-5.1-codex, 2025b. URL https://platform.openai.com/docs/models/ gpt-5.1-codex-max. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with SWE-Gym. In International Conference on Machine Learning, 2025. Qwen. Qwen code: Research-purpose cli tool for qwen-coder models. https://qwenlm. github.io/blog/qwen3-coder/, 2025. Qwen Team. Qwen3-coder-480b-a35b-instruct, 2025. URL https://huggingface.co/ Qwen/Qwen3-Coder-480B-A35B-Instruct. Ranjan Sapkota, Konstantinos Roumeliotis, and Manoj Karkee. Vibe coding vs. agentic coding: Fundamentals and practical implications of agentic ai. arXiv preprint arXiv:2505.19443, 2025. 11 Published as conference paper at ICLR 2026 Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning. arXiv preprint arXiv:2504.17192, 2025. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. Paperbench: Evaluating AIs ability to replicate AI research. In International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id= xF5PuTLPbn. Huanting Wang, Jingzhi Gong, Huawei Zhang, and Zheng Wang. Ai agentic programming: survey of techniques, challenges, and opportunities. arXiv preprint arXiv:2508.11126, 2025a. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. URL https://arxiv.org/abs/2407.16741. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=OJd3ayDDoF. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theArt Natural Language Processing. Association for Computational Linguistics, 2020. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. SWE-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025b. Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, and Junyang Lin. Synthesizing software engineering data in test-driven manner. In International Conference on Machine Learning, 2025. Wenting Zhao, Nan Jiang, Celine Lee, Justin Chiu, Claire Cardie, Matthias Galle, and Alexander Rush. Commit0: Library generation from scratch. arXiv preprint arXiv:2412.01769, 2024. Terry Yue Zhuo, Vu Minh Chien, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen GONG, James Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. In International Conference on Learning Representations, 2025. Published as conference paper at ICLR"
        },
        {
            "title": "A DETAILED BENCHMARK COLLECTION",
            "content": "This section complements the details of benchmark construction (Sec. 3.2), which contains detailed recipes of the data collection, patch extraction, and prompt design, along with fuller characterization of the task instances. A.1 DATA COLLECTION PIPELINE Environment Setup. For each selected repository, we manually prepare an environment configuration file (see Figure 6 for an example). Empirical observations indicate this procedure can be accomplished within three minutes. Upon completion of environment configuration, our pipeline constructs Docker image, with all subsequent operations executed within this sandboxed environment. This is the sole stage requiring human intervention. All succeeding stages operate under full automation. Patch Extraction. The patch extraction process consists of four main steps. Patch Extraction Step 1: Dependency Graph Construction. This procedure generates function-level dependency graphs for all test files within the code repository, establishing the foundation for subsequent patch extraction operations. We leverage pytests intrinsic test case collection mechanism to aggregate all viable test cases at the file granularity, where each file contains potential test case. For each test case, we execute the test within the sandbox environment, selecting test cases that achieve complete success as fail-to-pass (F2P) instances. Concurrent with test execution, we construct function-level dependency graphs for each F2P instance utilizing dynamic tracing library. Patch Extraction Step 2: LLM Classification. For each F2P test file, we employ an LLM to differentiate between imported objects serving as test targets versus those functioning as test dependencies and general utilities. We provide the LLM with the test files name and content as classification references. Our prompt template for the LLM to classify is illustrated in Figure 9. Objects classified through this methodology are designated as top-level objects, representing directly imported interfaces by the test file. Patch Extraction Step 3: Pass-to-pass (P2P) Selection. For each F2P instance, we select multiple pass-to-pass cases. These P2P cases are executed after coding agents finishing implementations to ensure existing functionalities remain normal. Since the aforementioned top-level objects of F2P cases will be removed from codebases, here the pass-to-pass cases should not share top-level objects with the F2P cases. For this reason, if we find only few P2P cases have different top-level objects from F2P cases, it may indicate erroneous classification of general utilities as top-level objects by the LLM. In this circumstance, we will reconsider the top-level objects according to their invocation frequency. Patch Extraction Step 4: Final Extraction. For each F2P case, we utilize top-level objects as entry points and execute BFS according to the constructed dependency graph. Node objects belonging to P2P are designated as remained, while others are marked as extracted. Nodes marked as extracted are added to the BFS queue for continued traversal. BFS termination occurs upon queue finish or when extracted code lines reach our predetermined maximum value, randomly selected between 3000 and 5000 lines per case. Finally, we remove objects marked as extracted from the codebase, yielding complete codebase with F2P functionality eliminated. Post-verification. For each codebase after code patch extraction, we conduct post-verification to ensure the modified codebase has normal functionality. Specifically, we execute F2P within the modified codebase, expecting pass rates below predetermined parameter. Then we further execute all selected P2P cases, ensuring complete test passage. 13 Published as conference paper at ICLR Environment Configuration File part 1 of 2 Figure 6: Environment Configuration File part 1 of 2 14 Published as conference paper at ICLR 2026 Environment Configuration File part 2 of 2 Figure 7: Environment Configuration File part 2 of 15 Published as conference paper at ICLR 2026 A.2 DATA FORMAT AND PROMPT DESIGN In this section, we present the essential components included in qualified example (covering both L1 and L2 tasks), illustrating the test case format of our benchmark, organization of our prompts, and the effectiveness of using an LLM to supplement missing docstring entries. Directory Structure of Generated Instances. Each successfully generated instance includes directory structure containing four main files: problem statement.md, patch.diff, test patch.diff and instance.json. Specifically, problem statement.md serves as the generated task prompt; patch.diff and test patch.diff represent the gold patch and test patch, respectively; while instance.json records metadata such as the task ID, source repository, and commit ID. Prompt Structure and Organization. For each successfully generated instance, we construct tailored and detailed problem statement.md file as input to the agent. The content of problem statement.md consists of two components: Task and Interface Descriptions. All prompts are generated automatically without manual labor, following unified prompt template combined with an instance-specific configuration file via scripting. As shown in Figure 12, the Task section provides the agent with an overview of the task under the heading Core Functionality. Main Features and Requirements describes the essential code features and requirements. The mandatory components that must be implemented are outlined under Key Challenges. Finally, the NOTE subsection provides specific requirements. In the Interface Descriptions section (as shown in Figure 13), we offer detailed instructions on how to construct the code. This includes requirements for file locations and structure, suggestions for implementing interfaces, and specific objectives related to the current task. Prompt Design: L1 vs. L2. It is noteworthy that there are subtle differences between the L1 and L2 prompts. In the Task section of the prompt, for L2 examples, we require the agent to independently implement the solution from scratch, without access to the repositorys codebase. To enforce this constraint, the agent is instructed not to download the repository, and even in the event of its doing so, it is instructed not to install it. This is closely monitored during the testing phase, where we have set up mechanisms to check whether the agent engages in any unauthorized actions, such as cheating. Figure 15 and Figure 16 show specific L1 and L2 prompt, respectively, and they both come from the test-layer-norm test in liger-kernel library. Supplementation for Missing Docstrings. In cases where the source code lacks adequate documentation regarding function interfaces or behavior, we leverage LLM to infer and complete the missing information. Figure 11 illustrates the prompt for docstring generation. The LLM-generated docstring is exemplified in Figure 18, where the LLM is used to supplement the missing docstring in the functional description."
        },
        {
            "title": "B DETAILED BENCHMARKING PROCESS",
            "content": "Our benchmarking pipeline is organized into two sequential stages: (1) agent inferring and (2) automated evaluation. The following subsections provide detailed explanation of two phases. B."
        },
        {
            "title": "INFERRING",
            "content": "During the inference phase, after initializing the task environment image, we proceed as follows based on the setting: For L1: We use patch.diff to remove the target feature from the origin repository within the image and remove the corresponding F2P test file. For L2: We completely remove the entire code repository from the image. Subsequently, problem statement.md as the prompt of the task. Upon completion, we extract modifications made by the agent as new patch.diff file for later evaluation. provided with the deployed within environment agent and the the is 16 Published as conference paper at ICLR 2026 B.2 EVALUATION During the evaluation phase, we utilize the same base image and reset the repository to its preinferring state (replicating the setup procedures for L1 and L2 described above). And then: For L1: We apply the agent-generated patch.diff to the repository and reintroduce the F2P test file. For L2: We apply the agent-generated patch.diff to the image and install it as the agent code library. We then restore the original repository and apply the test patch.diff. Finally, we employ the pytest framework to perform automated testing and get the pytest report. B.3 METRICS The evaluation process produces raw output from the pytest framework, which is subsequently processed to extract key statistics, including total, passed, failed, skipped, error, xfail, and xpass. From these statistics, we derive two primary evaluation metrics: pass rate and is solved. The pass rate is defined as the ratio of successfully passed tests to the total number of executed F2P tests. The binary metric is solved indicates whether the task is fully solved. It is assigned value of 1 if pytest command exit with 0 (both F2P and P2P), and assigned value of 0 otherwise. B.4 CHEATING PREVENTION AND DETECTION such as using pip install To protect against potential cheating attempts by agents, <package> followed by inspecting the source code of the installed package, we implement twofold defense mechanism. First, defensive prompting is incorporated into the task descriptions to discourage such behavior. Second, the evaluation framework conducts an automated inspection of the agents execution logs after task completion to identify suspicious activities. The log inspection process searches for regular expression patterns that indicate unauthorized attempts to access the source code of installed packages. If any pattern matches, the agent is flagged for potentially dishonest behavior in the evaluation report. r\"message\".*cat /usr/local/lib/pythond+.d+ r\"command\".*cat /usr/local/lib/pythond+.d+ r\"message\".*reading file: /usr/local/lib/pythond+.d+ r\"message\".*reading /usr/local/lib/pythond+.d+ These patterns capture attempts to directly access files within the Python library directory, which is classified as form of cheating under the evaluation criteria. ANALYSIS OF FALURES OF GEMINI 3 PRO MODEL In our baseline experiments, we found that the Gemini 3 Pro model performed poorly. Through analysis of the model output logs, we discovered that this may be caused by lack of strict adherence to JSON schemas during tool invocation. Specifically, we observed recurrent pattern of parameter key hallucination, where the model substituted valid schema definitions with semantically similar but syntactically incorrect keys. For example, in case astropy astropy.b0db0daa.test basic rgb.067e927c.lv1, the model produced the output shown in Figure 8, the model attempted to invoke read file using the argument path instead of the strictly defined file path. This suggests that while the model understands the intent of the tool, it struggles to suppress its internal priors in favor of the provided API constraints. This situation accounts for the majority of cases in our evaluation, This indicates that Gemini 3 Pro seems to have certain deficiencies in completing large-scale code editing tasks. 17 Published as conference paper at ICLR 2026 Benchmark Task Source F2P/P2P Real-world software development Agent Eval. Avg. LoC SWE-Bench SWE-Dev FeatureBench PR Unit Tests Unit Tests 32.8 190 790.2 Table 10: Comparison of FeatureBench with SWE-Bench and SWE-Dev. Benchmark Full Implementation Realistic Software Development commit0 FeatureBench Scalability Table 11: Comparison of FeatureBench with SWE-Bench and commit0."
        },
        {
            "title": "D COMPARISON WITH EXISTING BENCHMARKS",
            "content": "This section provides additional comparisons between FeatureBench and two representative benchmarks, SWE-Dev (Du et al., 2025) and commit0 (Zhao et al., 2024). These comparisons complement the high-level discussion in Section 2 and clarify the distinctions in task sources, construction pipelines, evaluation settings, and scalability. Comparison with SWE-Dev SWE-Dev derives tasks from unit tests and LLM-generated problem requirement descriptions (PRDs). Its task formulation and construction pipeline differ substantially from FeatureBench, particularly in how tasks are specified, validated, and filtered. Table 10 provides concise comparison, followed by brief clarifications of the key distinctions. Realistic development workflow and stricter construction. FeatureBench preserves the original, well-developed features of each repository and leaves only the target feature unimplemented, closely matching incremental development. This is enforced through precise patch extraction, F2P/P2P filtering, and strict post-verification. SWE-Dev omits P2P verification and does not perform post-verification, allowing patches that unintentionally break existing behavior. Interface-driven task specification with minimal ambiguity. SWE-Dev uses LLM-generated PRDs, which naturally introduce ambiguity. FeatureBench instead exposes native top-level interfacesfunction signatures and invocation paths extracted directly from the codebaseensuring clear, deterministic, and implementable task specifications. Agent-based evaluation in this work. SWE-Dev reports results for LLM and multi-LLM settings but does not evaluate coding agents. FeatureBench conducts end-to-end agent experiments using unified OpenHands scaffold with multiple LLM backends (Claude, GPT, Gemini, Qwen), providing realistic assessment of agent performance. More realistic and complex feature-level tasks. SWE-Dev tasks involve roughly 190 LoC across three files. FeatureBench tasks require around 790.2 LoC across more files and substantially more test points, reflecting the multi-file, cross-module modifications typical in real feature development. Comparison with commit0 commit0 studies whether LLMs can reconstruct entire libraries from documentation and high-coverage test suites. This setup differs markedly from FeatureBench, whose focus is real-world feature development with full, from-scratch implementations and scalable construction. Table 11 summarizes the key distinctions, with brief explanations provided below. Real-world development and full implementation. In commit0, only the bodies of functions and classes are removed while definitions and architectural scaffolding remain, making tasks closer to fill-in-the-blank partial completions. FeatureBench removes definitions, imports, and associated logic, ensuring that the target feature is fully absent and must be implemented from scratch, better aligning with real development workflows. Low-cost scalability to new repositories. commit0 requires repositories with well-organized documentation and very high test coverage (>90%), severely limiting applicability. FeatureBench requires only runnable unit-test suite; after short configuration step, the rest of the pipeline is fully automated, enabling efficient scaling across wide range of real-world codebases. 18 Published as conference paper at ICLR"
        },
        {
            "title": "E DATASET OVERVIEW AND EXPERIMENTAL RESULTS",
            "content": "The construction of the dataset resulted in 200 evaluation tasks derived from 3825 candidate coding environments across 24 Python repositories. These repositories encompass wide range of domains, including machine learning, scientific computing, visualization, web frameworks and fundamental software engineering utilities. This diversity ensures the dataset captures broad spectrum of realworld coding scenarios. To promote transparency and reproducibility, the appendix contains two tables that describe the dataset composition. The Table 13 provides an overview of each repository, including summary information and licensing details. The Table 12 presents quantitative statistics such as the average number of extracted code lines and the number of test points in the test suite. To illustrate the dataset structure, we include an example of an individual data entry. Each entry includes the following fields: instance id, patch, test patch, FAIL TO PASS, PASS TO PASS, image name, and repo settings. The specific meaning of each field is detailed in Table 14. problem statement, base commit, repo, Additionally, the experimental results, summarized in seven comprehensive tables (Table 15 to Table 21), evaluate the performance of multiple large language models across the dataset. Each table reports three repository-level average metrics: Passed, Resolved, and Token IO. These results provide insights into model capabilities, including task pass rates, resolution rates, and token input-output statistics. The results demonstrate the strengths and limitations of current coding agents in diverse scenarios, forming foundation for future advancements in agentic coding research."
        },
        {
            "title": "Repo",
            "content": "# Lines # Files # Functions # Test points pytorch-lightning metaflow astropy fastapi accelerate transformers trl Liger-Kernel matplotlib meson mlflow seaborn optuna pandas pydantic xarray hatch packaging setuptools pytest mypy scikit-learn sphinx sympy 765.4 520.0 800.0 110.0 1057.0 494.8 879.0 539.5 232.0 220.0 732.9 518.8 104.0 1522.9 436.4 847.8 847.0 785.0 3283.0 528.0 392.0 900.7 1025.9 979. 24.0 1.0 15.9 9.0 12.0 13.4 5.0 15.5 1.0 7.0 16.9 6.4 12.0 19.2 6.8 15.8 17.0 3.0 50.0 3.0 11.0 12.0 18.3 25.2 35.7 5.0 34.6 8.0 17.0 13.7 20.0 10.3 11.0 14.0 29.4 25.6 8.0 39.2 24.8 37.3 55.5 36.0 185.0 34.5 5.0 12.3 47.4 35.2 Table 12: Repository statistics. 42.0 31.0 132.7 10.0 33.0 90.9 352.0 29.8 31.0 16.0 25.8 97.1 28.0 69.2 45.6 21.5 16.0 294.0 30.0 105.5 10.0 89.0 31.2 33.8 19 Published as conference paper at ICLR"
        },
        {
            "title": "Summary",
            "content": "pytorch-lightning metaflow astropy fastapi accelerate transformers trl Liger-Kernel matplotlib meson mlflow seaborn optuna pandas pydantic xarray hatch packaging setuptools pytest mypy scikit-learn sphinx sympy Lightweight PyTorch wrapper for high-performance AI research Framework for building and managing real-life data science projects Astronomy and astrophysics core library Modern, fast (high-performance) web framework for building APIs Library for running PyTorch training on any distributed configuration State-of-the-art pretrained models for natural language processing and beyond Library for training large language models with reinforcement learning from human feedback High-performance deep learning kernels developed for large-scale distributed training Plotting library for creating scientific and publication-quality visuals Open source build system meant to be both extremely fast and user friendly Open source platform for the machine learning lifecycle Statistical data visualization library built on top of matplotlib Automatic hyperparameter optimization software framework Data analysis and manipulation library providing high-performance data structures Data validation using Python type hints Library for N-dimensional labeled arrays and datasets Modern, extensible Python project management Core utilities for Python packaging Fully-featured library for packaging Python projects Testing framework for Python Optional static typing for Python Machine learning algorithms and tools in Python Documentation generation system for Python projects Computer algebra system for symbolic mathematics in Python"
        },
        {
            "title": "License",
            "content": "Apache-2.0 Apache-2.0 BSD 3-Clause MIT License Apache-2.0 Apache-2.0 Apache-2. BSD 2-Clause"
        },
        {
            "title": "Custom",
            "content": "Apache-2.0 Apache-2.0 BSD 3-Clause"
        },
        {
            "title": "MIT License",
            "content": "BSD 3-Clause MIT License Apache-2.0 MIT License Apache-2.0 MIT License MIT License MIT License BSD 3-Clause BSD 2-Clause BSD 3-Clause Table 13: Summary and licenses for all GitHub repositories that task instances were extracted from."
        },
        {
            "title": "Description",
            "content": "instance id patch test patch FAIL TO PASS PASS TO PASS image name repo base commit problem statement repo settings (str) Unique identifier for the task. (str) Git diff showing the implementation. (str) Git diff showing test file modifications. (list[str]) List of test files that must pass after implementation. (list[str]) List of test files that must continue passing. (str) Docker image containing the development environment. (str) Source repository (e.g., owner/repo-name). (str) Git commit hash of the base version. (str) Detailed task description and requirements. (str) Repository configuration settings as JSON string. Table 14: Description of each field of FeatureBench task instance object. 20 Published as conference paper at ICLR 2026 Model Repo % Passed % Resolved # Token IO Codex + GPT-5.1-Codex (medium reasoning) Liger-Kernel accelerate astropy fastapi hatch matplotlib meson metaflow mlflow mypy optuna packaging pandas pydantic pytest pytorch-lightning scikit-learn seaborn setuptools sphinx sympy transformers trl xarray 30.7 0.0 34.9 65.0 3.8 54.0 24.4 100.0 51.4 15.0 14.3 93.5 41.4 50.3 61.8 51.7 84.0 43.7 23.3 48.4 29.5 24.6 38.9 46. 14.3 0.0 11.8 0.0 0.0 0.0 0.0 100.0 28.6 0.0 0.0 0.0 10.0 0.0 0.0 8.3 0.0 8.3 0.0 20.0 16.7 0.0 0.0 0.0 4.6M / 35k 16.1M / 73k 12.5M / 60k 1.0M / 14k 4.4M / 23k 2.0M / 40k 7.0M / 57k 4.8M / 37k 3.0M / 20k 59k / 2k 5.1M / 36k 6.0M / 45k 8.3M / 46k 9.9M / 40k 3.2M / 38k 8.9M / 45k 7.4M / 35k 3.3M / 41k 4.7M / 23k 8.1M / 49k 9.6M / 65k 4.4M / 34k 1.6M / 37k 18.6M / 85k Table 15: Performance of Codex + GPT-5.1-Codex on each repository. Model Repo % Passed % Resolved # Token I/O Claude Code (routing) + Claude-Opus-4.5 Liger-Kernel accelerate astropy fastapi hatch matplotlib meson metaflow mlflow mypy optuna packaging pandas pydantic pytest pytorch-lightning scikit-learn seaborn setuptools sphinx sympy transformers trl xarray 77.3 92.3 25.7 35.0 3.8 54.0 28.4 100.0 50.7 25.0 92.9 88.1 39.6 31.6 67.4 36.2 95.2 42.6 20.0 39.0 41.8 35.6 49.9 40.8 57.1 0.0 0.0 0.0 0.0 0.0 0.0 100.0 24.5 0.0 0.0 0.0 5.0 20.0 0.0 0.0 33.3 0.0 0.0 10.0 0.0 2.9 0.0 0.0 3.7M / 34k 12.2M / 53k 10.1M / 40k 2.0M / 14k 14.7M / 54k 2.3M / 20k 5.4M / 33k 7.7M / 37k 5.0M / 27k 569k / 5k 5.8M / 37k 9.8M / 37k 13.6M / 50k 13.6M / 41k 8.0M / 42k 8.8M / 35k 6.6M / 35k 5.7M / 31k 20.1M / 73k 10.1M / 36k 9.5M / 43k 4.4M / 26k 959k / 13k 14.9M / 55k Table 16: Performance of Claude Code + Claude-Opus-4.5 on each repository. 21 Published as conference paper at ICLR 2026 Model Repo % Passed % Resolved # Token I/O OpenHands + Claude-Opus-4.5 Liger-Kernel accelerate astropy fastapi hatch matplotlib meson metaflow mlflow mypy optuna packaging pandas pydantic pytest pytorch-lightning scikit-learn seaborn setuptools sphinx sympy transformers trl xarray 77.3 0.0 20.2 80.0 18.0 65.1 29.3 100.0 54.1 25.0 14.3 91.2 50.0 41.9 59.7 40.2 98.5 43.7 20.0 36.9 50.5 32.8 97.9 49.7 57.1 0.0 0.0 0.0 0.0 0.0 0.0 100.0 20.4 0.0 0.0 0.0 10.0 0.0 0.0 0.0 66.7 8.3 0.0 0.0 0.0 2.9 0.0 0.0 4.8M / 24k 19.1M / 51k 11.1M / 35k 3.1M / 14k 15.1M / 36k 2.3M / 27k 4.8M / 34k 5.7M / 24k 6.9M / 26k 418k / 8k 6.2M / 23k 16.9M / 48k 10.4M / 35k 12.2M / 30k 2.1M / 14k 10.2M / 34k 10.3M / 45k 6.0M / 30k 16.2M / 42k 8.9M / 28k 11.4M / 40k 5.3M / 22k 5.9M / 35k 17.1M / 47k Table 17: Performance of OpenHands + Claude-Opus-4.5 on each repository. Model Repo % Passed % Resolved # Token I/O OpenHands + DeepSeek-V3. Liger-Kernel accelerate astropy fastapi hatch matplotlib meson metaflow mlflow mypy optuna packaging pandas pydantic pytest pytorch-lightning scikit-learn seaborn setuptools sphinx sympy transformers trl xarray 63.1 0.0 10.3 35.0 3.8 52.2 27.2 100.0 33.3 25.0 7.1 32.6 17.8 13.2 12.1 26.2 49.8 27.5 23.3 29.9 11.8 17.9 81.4 29.9 42.9 0.0 0.0 0.0 0.0 0.0 0.0 100.0 12.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.9 0.0 0.0 2.9M / 21k 3.4M / 23k 4.1M / 25k 1.9M / 18k 3.4M / 23k 1.9M / 24k 2.8M / 30k 2.4M / 24k 2.7M / 23k 532k / 7k 3.1M / 21k 4.1M / 42k 3.5M / 24k 4.3M / 19k 4.2M / 36k 2.9M / 24k 4.8M / 26k 2.8M / 22k 11.4M / 51k 2.6M / 19k 7.0M / 39k 2.3M / 19k 3.0M / 54k 3.6M / 29k Table 18: Performance of OpenHands + DeepSeek-V3.2 on each repository. Published as conference paper at ICLR 2026 Model Repo % Passed % Resolved # Token I/O Gemini CLI + Gemini-3-Pro-Preview (low reasoning) Liger-Kernel accelerate astropy fastapi hatch matplotlib meson metaflow mlflow mypy optuna packaging pandas pydantic pytest pytorch-lightning scikit-learn seaborn setuptools sphinx sympy transformers trl xarray 65.9 0.0 12.6 85.0 3.8 65.1 29.1 100.0 36.5 25.0 7.1 46.6 29.2 21.1 60.0 26.4 81.5 32.9 0.0 35.0 37.8 22.3 91.2 28.0 42.9 0.0 0.0 50.0 0.0 0.0 0.0 100.0 8.2 0.0 0.0 0.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0M / 15k 2.3M / 12k 7.6M / 24k 1.0M / 6k 1.0M / 5k 474k / 8k 730k / 16k 422k / 9k 1.2M / 9k 1.0M / 3k 828k / 6k 1.2M / 11k 4.5M / 10k 9.4M / 16k 887k / 14k 646k / 8k 1.6M / 13k 582k / 8k 1.4M / 8k 2.5M / 21k 6.4M / 14k 1.4M / 9k 442k / 17k 4.6M / 17k Table 19: Performance of Gemini CLI + Gemini-3-Pro-Preview on each repository. Model Repo % Passed % Resolved # Token I/O OpenHands + Gemini-3-Pro-Preview (low reasoning) Liger-Kernel accelerate astropy fastapi hatch matplotlib meson metaflow mlflow mypy optuna packaging pandas pydantic pytest pytorch-lightning scikit-learn seaborn setuptools sphinx sympy transformers trl xarray 72.6 0.0 10.5 35.0 3.8 50.4 28.6 100.0 32.2 25.0 14.3 0.0 34.8 15.3 67.7 26.5 85.9 30.7 23.3 30.4 36.9 17.0 97.3 29. 42.9 0.0 0.0 0.0 0.0 0.0 0.0 100.0 8.2 0.0 0.0 0.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.5M / 41k 4.9M / 40k 9.2M / 49k 1.1M / 16k 7.7M / 58k 1.2M / 45k 1.6M / 27k 8.6M / 52k 4.6M / 38k 479k / 10k 4.3M / 28k 12.6M / 56k 10.4M / 46k 11.8M / 45k 16.9M / 81k 11.1M / 36k 8.3M / 48k 2.8M / 31k 24.3M / 71k 5.0M / 43k 8.7M / 52k 3.5M / 31k 2.1M / 58k 6.1M / 43k Table 20: Performance of OpenHands + Gemini-3-Pro-Preview on each repository. 23 Published as conference paper at ICLR 2026 Model Repo % Passed % Resolved # Token I/O OpenHands + Qwen3-Coder-480BA35B-Instruct Liger-Kernel accelerate astropy fastapi hatch matplotlib meson metaflow mlflow mypy optuna packaging pandas pydantic pytest pytorch-lightning scikit-learn seaborn setuptools sphinx sympy transformers trl xarray 33.3 0.0 5.7 35.0 3.8 43.1 28.6 100.0 36.2 25.0 7.1 16.7 20.2 13.1 52.9 26.1 83.1 27.6 10.0 28.9 12.6 8.8 70.3 21. 14.3 0.0 0.0 0.0 0.0 0.0 0.0 100.0 10.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.1M / 15k 2.5M / 16k 2.4M / 13k 499k / 5k 1.3M / 12k 888k / 12k 1.5M / 15k 1.2M / 8k 1.9M / 14k 284k / 3k 951k / 8k 5.3M / 39k 2.2M / 13k 2.7M / 16k 3.2M / 19k 1.4M / 13k 2.9M / 24k 2.0M / 15k 1.4M / 8k 1.7M / 11k 4.5M / 27k 1.7M / 12k 1.6M / 40k 1.8M / 21k Table 21: Performance of OpenHands + Qwen3-Coder-480B-A35B-Instruct on each repository. 24 Published as conference paper at ICLR 2026 Output Analysis of Gemini 3 Pro { \"timestamp\": ..., \"model\": \"gemini-3-pro-preview\", \"tools\": [ { \"functionDeclarations\": [ { \"name\": \"read_file\", \"description\": \"Reads and returns the content of specified file...\", \"parametersJsonSchema\": { \"type\": \"object\", \"required\": [ \"file_path\" ], \"properties\": { \"file_path\": { \"description\": \"The path to the file to read.\", \"type\": \"string\" }, \"limit\": ... \"offset\": ... ...], \"request\": { \"contents\": [ ..., { \"role\": \"model\", \"parts\": [ { \"thoughtSignature\": \"...\", \"functionCall\": { \"name\": \"read_file\", \"args\": { \"path\" :\"/testbed/astropy/visualization/basic_rgb.py \", ...}, { \"role\": \"user\", \"parts\": [ { \"functionResponse\": { \"name\": \"read_file\", \"response\": { \"error\": \"params must have required property file_path\" \"response\": { ...}, \"status\": 200, \"content\": \"**Clarifying File Requirement**nnIve realized my initial approach was incomplete. Specifically, understand now that the read_file tool explicitly needs file_path input. Im focusing on ensuring that parameter is accurately and appropriately utilized moving forward.\" } } Figure 8: Partial output of Gemini 3 Pro model using Gemini CLI framework while completing task astropy astropy.b0db0daa.test basic rgb.067e927c.lv1 25 Published as conference paper at ICLR 2026 Prompt for classifying top-level objects part 1 of Task: From list of candidate objects, identify which ones are \" tested objects\" in the context of Python test file. **Definition of \"Tested Object\":** \"tested object\" is an object that the test file is specifically designed to test. It represents the core functionality or feature being validated, NOT utility functions, test helpers, or infrastructure code. **Test File Information:** - Test file path: {test_file} - Test file name: {test_file_name} **Test File Content:** python {test_file_content} **Candidate Objects to Classify:** {candidates_section} **Classification Guidelines:** **Tested Objects (should be selected):** - Core algorithms, classes, or functions that the test file is designed to validate - Main interfaces or APIs being tested - Key components whose behavior is the primary focus of the test **Non-Tested Objects (should NOT be selected):** - Utility functions from test utilities (e.g., test.utils.*, pytest.*) - Common tools defined in the codebase (e.g., infer_device(), assert_verbose_allclose()) Figure 9: Prompt template for classifying top-level objects part 1 of 2 Published as conference paper at ICLR 2026 Prompt for classifying top-level objects part 2 of 2 **Examples for Reference:** **Example Scenario:** - Test file: /testbed/test/transformers/test_jsd.py (testing JSD algorithm) **Should Select (Tested Objects):** - /testbed/src/liger_kernel/transformers/jsd.py::LigerJSD.forward ::64 - Core JSD implementation - /testbed/src/liger_kernel/transformers/jsd.py::LigerJSD.__init__ ::59 - JSD class initialization **Should NOT Select (Non-Tested Objects):** - /testbed/test/transformers/test_jsd.py::_test_correctness_once ::91 - Helper function in test file - /testbed/src/liger_kernel/ops/utils.py::ensure_contiguous.wrapper ::34 - General utility **Your Task:** Please analyze each candidate object and determine which ones are tested objects for the given test file. Provide your response in the following structured format: ## Analysis For each candidate object, briefly explain whether it should be selected as tested object and why. ## Final Answer Provide your final selection in the following JSON format: json {{ \"tested_object_ids\": [ \"object_id_1\", \"object_id_2\" ], \"reasoning\": \"Brief summary of the selection criteria applied\" }} **Important Notes:** - The tested_object_ids list should contain ONLY the object IDs that are tested objects - If none of the candidates are tested objects, return an empty list : \"tested_object_ids\": [] - Include the full object ID exactly as provided in the candidate list - Unless its obvious or youre pretty sure that candidate object is general purpose tool, you need to categorize it as tested object Now, please begin your analysis for the candidate objects listed above. Figure 10: Prompt template for classifying top-level objects part 2 of 2 27 Published as conference paper at ICLR"
        },
        {
            "title": "Prompt for completing docstring",
            "content": "Generate detailed docstring for the following Python function. The docstring should include: 1. The main function description 2. Parameter description (if any) 3. Return value description (if any) 4. Important notes or exceptions (if applicable) Function qualified name: python {function_qualified_name} The full file content for reference: python {file_content} You only need to generate the docstring of the { function_qualified_name} function, the complete content of the file is for your reference only, and there is no need to generate the docstring of other functions or classes. In docstring, in order for us to parse it correctly, you are forbidden to use syntax like python , which may cause the end result to be confusing. Please only return the docstring content!!! DO NOT include triple quotes or other format tags: Figure 11: Prompt template for completing docstring given to LLM User prompt part 1 of 3 (Level 1) ## Task **Task Statement:** Implement ... 1. **Core functionalities**: ... 2. **Main features and requirements**: - ... - ... 3. **Key challenges**: - ... - ... Figure 12: Unified prompt template for L1 part 1, Task. 28 Published as conference paper at ICLR 2026 User prompt part 2 of 3 (Level 1) **NOTE**: - This test comes from the {{ library_name }} library, and we have given you the content of this code repository under /testbed /, and you need to complete based on this code repository and supplement the files we specify. Remember, all your changes must be in this codebase, and changes that are not in this codebase will not be discovered and tested by us. - Weve already installed all the environments and dependencies you need, you dont need to install any dependencies, just focus on writing the code! - **CRITICAL REQUIREMENT**: After completing the task, pytest will be used to test your implementation. **YOU MUST** match the exact interface shown in the **Interface Description** (I will give you this later) You are forbidden to access the following URLs: {{ black_links }} Your final deliverable should be code under the /testbed/ directory, and after completing the codebase, we will evaluate your completion and it is important that you complete our tasks with integrity and precision. The final structure is like below. ## Interface Descriptions ### Clarification The **Interface Description** testing do and the input and output formats. describes what the functions we are for example, you will get things like this: Path: {{ interface_code_example_path }} python {{ interface_code_example }} The value of Path declares the path under which the following interface should be implemented and you must generate the interface class/function given to you under the specified path. Figure 13: Unified prompt template for L1 part 2, Precautions. 29 Published as conference paper at ICLR 2026 User prompt part 3 of In addition to the above path requirement, you may try to modify any file in codebase that you feel will help you accomplish our task. However, please note that you may cause our test to fail if you arbitrarily modify or delete some generic functions in existing files, so please be careful in completing your work. Whats more, in order to implement this functionality, some additional libraries etc. are often required, dont restrict you to any libraries, you need to think about what dependencies you might need and fetch and install and call them yourself. The only thing is that you **MUST** fulfill the input/output format described by this interface, otherwise the test will not pass and you will get zero points for this feature. And note that there may be not only one **Interface Description**, you should match all **Interface Description {n}** ### Interface Description 1 Below is **Interface Description 1** Path: /path/to/xxx.py python def my_function: \"\"\" Implement ... Parameters: arg1: str, ... Returens: arg2: int, ... ... \"\"\" # <your code> ### Interface Description 2 class MyClass: \"\"\" ... \"\"\" # <your code> ... Remember, **the interface template above is extremely important**. You must generate callable interfaces strictly according to the specified requirements, as this will directly determine whether you can pass our tests. If your implementation has incorrect naming or improper input/output formats, it may directly result in 0% pass rate for this case. Figure 14: Unified prompt template for L1 part 3, Test and Interface Description. Published as conference paper at ICLR 2026 User prompt for test-layer-norm (Level 1) ## Task **Task Statement: Implement Optimized Layer Normalization Function** Develop high-performance layer normalization function that: 1. **Core Functionality**: Applies layer normalization to input tensors using weight, bias, and epsilon parameters for numerical stability 2. **Key Requirements**: - Integrate with existing Liger kernel optimization framework - Support standard layer normalization mathematical operations ( mean centering, variance scaling, affine transformation) - Handle multi-dimensional tensor inputs efficiently 3. **Main Challenges**: - Optimize memory usage and computational performance - Ensure numerical stability with configurable epsilon values - Maintain compatibility with transformer model architectures - Provide seamless integration with the broader Liger functional interface ecosystem **NOTE**: - This test comes from the liger-kernel library, and we have given you the content of this code repository under /testbed/, and you need to complete based on this code repository and supplement the files we specify. Remember, all your changes must be in this codebase, and changes that are not in this codebase will not be discovered and tested by us. - Weve already installed all the environments and dependencies you need, you dont need to install any dependencies, just focus on writing the code! - **CRITICAL REQUIREMENT**: After completing the task, pytest will be used to test your implementation. **YOU MUST** match the exact interface shown in the **Interface Description** (I will give you this later) You are forbidden to access the following URLs: black_links: - https://github.com/linkedin/Liger-Kernel/ Your final deliverable should be code under the /testbed/ directory, and after completing the codebase, we will evaluate your completion and it is important that you complete our tasks with integrity and precision. The final structure is like below. ## Interface Descriptions ... Figure 15: User prompt for test-layer-norm (L1). 31 Published as conference paper at ICLR 2026 User prompt for test-layer-norm (Level 2) part 1 of 2 ## Task **Task Statement: Implement Optimized Layer Normalization Function** Develop high-performance layer normalization function that: 1. **Core Functionality**: Applies layer normalization to input tensors using weight, bias, and epsilon parameters for numerical stability 2. **Key Requirements**: - Integrate with existing Liger kernel optimization framework - Support standard layer normalization mathematical operations ( mean centering, variance scaling, affine transformation) - Handle multi-dimensional tensor inputs efficiently 3. **Main Challenges**: - Optimize memory usage and computational performance - Ensure numerical stability with configurable epsilon values - Maintain compatibility with transformer model architectures - Provide seamless integration with the broader Liger functional interface ecosystem **NOTE**: - This test is derived from the liger-kernel library, but you are NOT allowed to view this codebase or call any of its interfaces. It is **VERY IMPORTANT** to note that if we detect any viewing or calling of this codebase, you will receive ZERO for this review. - **CRITICAL**: This task is derived from liger-kernel, but you ** MUST** implement the task description independently. It is ** ABSOLUTELY FORBIDDEN** to use pip install liger-kernel or some similar commands to access the original implementation, and doing so will be considered cheating and will result in an immediate score of ZERO! You must keep this firmly in mind throughout your implementation. - You are now in /testbed/, and originally there was specific implementation of liger-kernel under /testbed/ that had been installed via pip install -e .. However, to prevent you from cheating, weve removed the code under /testbed/. While you can see traces of the installation via the pip show, its an artifact, and liger-kernel doesnt exist. So you cant and don need to use pip install liger-kernel, just focus on writing your agent_code and accomplishing our task. - Also, dont try to pip uninstall liger-kernel even if the actual liger-kernel has already been deleted by us, as this will affect our evaluation of you, and uninstalling the residual liger-kernel will result in you getting ZERO because our tests wont run. - Weve already installed all the environments and dependencies you need, you dont need to install any dependencies, just focus on writing the code! - **CRITICAL REQUIREMENT**: After completing the task, pytest will be used to test your implementation. **YOU MUST** match the exact interface shown in the **Interface Description** (I will give you this later) Figure 16: User prompt for test-layer-norm part 1 of 2 (L2). 32 Published as conference paper at ICLR User prompt for test-layer-norm (Level 2) part 2 of 2 You are forbidden to access the following URLs: black_links: - https://github.com/linkedin/Liger-Kernel/ Your final deliverable should be code in the /testbed/agent_code directory. The final structure is like below, note that all dirs and files under agent_code/ are just examples, you will need to organize your own reasonable project structure to complete our tasks. After you have done all your work, you need to complete three CRITICAL things: 1. You need to generate __init__.py under the agent_code/ folder and import all the classes or functions described in the ** Interface Descriptions** in it. The purpose of this is that we will be able to access the interface code you wrote directly through agent_code.ExampleClass() in this way. 2. You need to generate /testbed/setup.py under /testbed/ and place the following content exactly: python from setuptools import setup, find_packages setup( name=\"agent_code\", version=\"0.1\", packages=find_packages(), ) 3. After you have done above two things, you need to use cd / testbed && pip install . command to install your code. Remember, these things are **VERY IMPORTANT**, as they will directly affect whether you can pass our tests. ## Interface Descriptions ... Figure 17: User prompt for test-layer-norm part 2 of 2 (L2). 33 Published as conference paper at ICLR 2026 Example of an LLM-generated docstring class LigerFusedLinearDPOFunction(LigerFusedLinearPreferenceBase): ... @staticmethod def backward(ctx, *grad_output): \"\"\" Computes gradients for the fused linear DPO (Direct Preference Optimization) function during backpropagation . This static method implements the backward pass for automatic differentiation in PyTorchs autograd system. It delegates the gradient computation to the parent classs backward method and then filters the results to match the expected number of parameters for the DPO function. Parameters: ctx: PyTorch autograd context object containing saved tensors and other information from the forward pass *grad_output: Variable-length argument list of gradient tensors flowing back from the loss function. Each tensor represents the gradient with respect to the corresponding output of the forward pass Returns: tuple: tuple containing gradients with respect to the input parameters of the forward function: - Gradient w.r.t. input tensor (_input) - Gradient w.r.t. weight tensor (weight) - Gradient w.r.t. target tensor (target) - Gradient w.r.t. bias tensor (bias) - None values for parameters that dont require gradients (ref_input, ref_weight, ref_bias, ignore_index, beta, compute_nll_loss, compiled, use_ref_model, average_log_prob, chunk_size, loss_type) Important Notes: - This method is part of PyTorchs Function interface for custom autograd operations - The method truncates the parent class gradients to the first 4 elements using [:4] slicing - Additional None values are returned to match the signature of the forward method parameters - The actual gradient computation logic is inherited from LigerFusedLinearPreferenceBase.backward() - This ensures proper gradient flow for DPO loss optimization while maintaining compatibility with PyTorchs autograd system \"\"\" <your code> ... Figure 18: Example of an LLM-generated docstring. 34 Published as conference paper at ICLR 2026 Human Evaluation Guideline for Top-Level Tested Object Classification Each import statement in test file should be evaluated independently to determine whether it represents top-level tested object or an auxiliary component. The procedure is as follows: Step 1: Understand Test File Purpose Read the test file to understand its testing objective and scope. Identify the main functionality or module being validated. Step 2: Identify All Import Statements Locate all import statements, including absolute and relative imports. Step 3: Filter External Library Imports Exclude imports from external libraries (e.g., pytest, unittest, torch). Step 4: Classify Repository-Internal Imports Assert statement usage: If the imported object appears in assertions comparing results, it is likely tested object. Name correspondence: If the objects name matches keywords in the test filename, it is likely tested object. Module correspondence: If imported from module matching the test filename, it is likely tested object. Utility module exclusion: Imports from utils/, testing/, helpers/, etc., are usually auxiliary. Frequency and prominence: Objects used extensively across the test file are more likely tested objects. Classification Decision Mark each import as either Top Import (tested object) or Non-Top Import (auxiliary). When criteria conflict, prioritize the first three criteria over the last two. Figure 19: Human evaluation guideline for identifying top-level tested objects. 35 Published as conference paper at ICLR 2026 Expert Verification Guideline for Feature-Level Tasks (part 1 of 2) Each feature-level task must be manually verified to ensure that (1) the task is structurally correct (objects, imports, masking, etc.), and (2) competent engineer can implement the required functionality using only the prompt and the remaining codebase, without external documentation. You will typically use two resources: metadata outputs/: logs and classification results (lists of top objects and specific objects). Level-1 task directory: problem statement.md (this is the primary target for verification). Stage 1: Check Structural Consistency 1.1 Get the list of top and specific objects Open the classification summary under metadata outputs/. Identify: Top objects: top interface of the feature being tested. Specific objects: objects that are functionally related but are not top interfaces. Treat these lists as checklist for the following steps."
        },
        {
            "title": "1.2 Check masking of top objects",
            "content": "For each top object, open its source file in the Level-1 directory. Confirm that the implementation body is removed and that only the signature, docstring, and minimal scaffolding remain. If any implementation detail is still visible, manually remove it while ensuring that tests can still import and call the interface."
        },
        {
            "title": "1.3 Check removing of specific objects",
            "content": "For each specific object, confirm that it does not appear in the remain codebase. If leftover definitions are found, please remove them. Figure 20: Expert verification guideline for feature-level tasks part 1. 36 Published as conference paper at ICLR 2026 Expert Verification Guideline for Feature-Level Tasks (part 2 of 2) Stage 2: Check Prompt Completeness and Solvability 2.1 Check the high-level Task Description Read the Task Description and ask: If only had this description and the codebase, do know what to implement? Verify that it: Explains what feature or behavior needs to be implemented. Provides context about where the feature sits in the system. Mentions any key technical considerations that affect correctness. If the description is vague or incomplete, rewrite it to make the implementation goal clear."
        },
        {
            "title": "2.2 Check the Test Description sections",
            "content": "For each Test Description, verify that: All required top objects are correctly referenced. Imports match the real file structure under the task directory. Fix missing interfaces and incorrect module paths as needed so that the agent will know where to implemenet them."
        },
        {
            "title": "2.3 Check Interface Descriptions and docstrings",
            "content": "For each Interface Description: Ensure the docstring is semantically complete: what the function/class does, parameter meanings, and return values. Confirm that it is self-contained and does not require external documentation. Keep it concise but readable; something real engineer would be happy to follow. If docstring is too short, ambiguous, or inconsistent with the real behavior, revise it and, if necessary, refer to the original implementation to understand the intended semantics. When to Mark Task as Verified task is considered verified if competent engineer can implement the required functionality using only the prompt.md and the remained codebase without external documentation. Concretely, this requires that: All top objects are correctly masked, imported, and documented. All specific objects that should not remain in the codebase are removed. The Task Description clearly states what to build. Test Descriptions match the actual file layout and cover all required interfaces. Interface Descriptions and docstrings are accurate and self-contained. If any of these conditions are not met, fix the relevant parts and re-check the task using the same steps before marking it as verified. Figure 21: Expert verification guideline for feature-level tasks part 2."
        }
    ],
    "affiliations": [
        "Huawei Technologies Co., Ltd",
        "Institute of Automation, Chinese Academy of Sciences"
    ]
}