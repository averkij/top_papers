{
    "paper_title": "Void in Language Models",
    "authors": [
        "Mani Shemiranifar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite advances in transformer-based language models (LMs), a fundamental question remains largely unanswered: Are all layers activated during inference? We investigate this question by detecting unactivated layers (which we refer to as Voids) using a non-trainable and parameter-free adaptive computation method called L2 Adaptive Computation (LAC). We adapt LAC from its original efficiency-focused application to trace activated layers during inference. This method monitors changes in the L2-norm of activations to identify voids. We analyze layer activation in instruction-tuned LMs across two phases: Prompt Processing (PP), where we trace activated layers for each token in the input prompts, and Response Generation (RG), where we trace activated layers for each generated token. We further demonstrate that distinct layers are activated during these two phases. To show the effectiveness of our method, we evaluated three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an improvement from 69.24 to 71.29 while the model uses only 30% of the layers. Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to 18.36 when using 70% of the layers during both the PP and RG phases. These results show that not all layers contribute equally during inference, and that selectively skipping most of them can improve the performance of models on certain tasks."
        },
        {
            "title": "Start",
            "content": "Mani Shemiranifar Independent Researcher m.shemiranifar@gmail.com Abstract 5 2 0 2 0 2 ] . [ 1 7 6 4 4 1 . 5 0 5 2 : r Despite advances in transformer-based language models (LMs), fundamental question remains largely unanswered: Are all layers activated during inference? We investigate this question by detecting unactivated layers (which we refer to as Voids) using non-trainable and parameter-free adaptive computation method called L2 Adaptive Computation (LAC). We adapt LAC from its original efficiency-focused application to trace activated layers during inference. This method monitors changes in the L2-norm of activations to identify voids. We analyze layer activation in instruction-tuned LMs across two phases: Prompt Processing (PP), where we trace activated layers for each token in the input prompts, and Response Generation (RG), where we trace activated layers for each generated token. We further demonstrate that distinct layers are activated during these two phases. To show the effectiveness of our method, we evaluated three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an improvement from 69.24 to 71.29 while the model uses only 30% of the layers. Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to 18.36 when using 70% of the layers during both the PP and RG phases. These results show that not all layers contribute equally during inference, and that selectively skipping most of them can improve the performance of models on certain tasks."
        },
        {
            "title": "1 Introduction",
            "content": "In recent years, there have been remarkable advances in transformer-based language models (Vaswani, 2017), broadening their capabilities from natural language processing tasks to complex reasoning problems (OpenAI et al., 2024; DeepSeek-AI et al., 2025). However, despite these achievements, these models remain largely black boxes. Several approaches have been developed to make LMs more transparent, including circuit analysis (Elhage et al., 2021; Geva et al., 2020), activation analysis (Gurnee et al., 2023; Burns et al., 2022), probing (Alain and Bengio, 2018; Orgad et al., 2024), and mechanistic interpretability (Wang et al., 2022; Elhage et al., 2022; Bricken et al., 2023). In addition to the methods mentioned above, analyzing layer activation patterns in Language Models (LMs) offers valuable insights into how these models process inputs of varying complexity. We can pinpoint which layers get activated during the inference by employing Adaptive Computation techniques. These methods were originally designed to improve the efficiency (Dehghani et al., 2022) of neural networks (Bengio et al., 2016), recurrent neural networks (Graves, 2017; Banino et al., 2021), language models (Dehghani et al., 2019; Schuster et al., 2022; yikun jiang et al., 2024), and vision models (Bolukbasi et al., 2017; Wang et al., 2018; Xue et al., 2023). However, these techniques are not straightforward to use when analyzing layer activation patterns in LMs, since they need to be trained specifically for this purpose and often lead to degraded model performance. To address these limitations, we utilize L2 Adaptive Computation (LAC) (Shemiranifar and Dehghani, 2023), which offers simple approach compared to existing methods. LAC operates without requiring any training, making it particularly well-suited for analyzing LMs. This method functions by monitoring changes in the L2norm of layer activations to identify halting points. LAC provides three distinct halting stages: per-batch, perexample, and per-token. We focus on the per-token setting since we aim to observe which layers are activated while LMs process and generate tokens 1The code is available in https://github.com/manishemirani/void_in_language_models Our analysis examines layer activation patterns across two distinct phases: Prompt Processing (PP): We use LACs per-token setting to track activated layers as the model processes each token within the input prompts. Response Generation (RG): We use LACs per-token setting to track activated layers as the model generates each token based on the given input prompt. We evaluate our method on three distinct instruction-tuned Language Models from the Llama, Mistral, and Qwen families, across range of challenging benchmarks: MMLU, GPQA Diamond, and BoolQ. Our findings reveal that Language Models do not consistently activate all layers during inference. Surprisingly, we demonstrate that selectively skipping these detected Voids can lead to performance improvements on certain tasks, even when utilizing significantly reduced subset of the models total layers. For instance, on MMLU with zero-shot setting, skipping Voids in Qwen2.5-7B-Instruct resulted in an improvement from 69.24 to 71.29 accuracy while using only 30% of the models layers. Similarly, Mistral-7B-Instructv0.3 on GPQA Diamond saw an improvement from 13.88 to 18.36 accuracy when using approximately 70% of its layers across both PP and RG phases. Furthermore, we demonstrate that distinct layers are activated during the Prompt Processing and Response Generation phases, highlighting task-dependent computational allocation within these models. The consistent presence of Voids across different models and tasks, coupled with the observed performance gains when selectively skipping them, strongly indicates that layers contribute unequally during inference. Our method, which precisely identifies these Voids, thus provides valuable insights into the inner workings of Language Models. This approach opens new avenues for interpretability, targeted knowledge editing, and hallucination detection by pinpointing layers that are less crucial for specific computations."
        },
        {
            "title": "2 Background",
            "content": "In this section, we will review the core concept of L2 adaptive computation, detailing how measurements of the L2-norm (Section 2.1) can be applied at various levels of granularity. Next, we examine LACs halting mechanism (Section 2.2), which leverages these measurements to halt computations. We then analyze the threshold sensitivity (Section 2.3). Finally, we discuss the practical implementation of granular halting through masking (Section 2.4)."
        },
        {
            "title": "2.1 L2 Adaptive Computation",
            "content": "L2 Adaptive Computation (LAC) considers the models progress as halting criterion, where this algorithm defines the progress based on the changes in the L2-norm made by each layer applied sequentially. LAC requires step function to perform; consider St(ht) = ht+1 for = {1, ..., } where represents the maximum number of layers and RBLD denoting the hidden state at step with B, and representing batch size, length, and depth, respectively. LAC offers three distinct levels for halting the computational steps: 1. Per-Batch: In this level, the L2-norm will be calculated for the whole given batch and accordingly halt the computation for this level: h2 = (cid:113) Σb i=1Σl j=1Σd k=1h2 ijk h2 R1 (1) 2. Per-Example: In this level, L2-norm will be calculated for each example in batch, and the halting mechanism will stop computation for each example separately: (cid:113) (cid:113) (cid:113) k=1h2 3. Per-Token: In this level. L2-norm will be calculated for each token in examples, therefore, the halting h2 = { 2jk, ..., j=1Σd j=1Σd j=1Σd k=1h k=1h2 h2 RB1 bjk} 1jk, Σl Σl Σl (2) mechanism stops the computation for each token individually: (cid:113) h2 = { Σd k=1h2 11k, (cid:113) Σd k=1h2 12k, ..., (cid:113) Σd k=1h2 blk} h2 RBL1 (3)"
        },
        {
            "title": "2.2 LAC’s Halting Mechanism",
            "content": "Having defined how the L2-norms are calculated at various granularities (Section 2.1), we now describe how these measurements are used to decide whether to halt computation for given input (batch, example, or token). The halting decision is based on the models \"progress\" during each computational step t. Progress at step is defined as the change in the L2-norm of the hidden state from step 1 to t: δt = ht2 ht12 (4) where δt denotes the progress of the model at step t. Depending on how to compute the L2-norm (2.1), the shape of δ changes, e.g., δ R1 for the per-batch setting, δ Rb1 for the per-example setting and δ Rbl1 for the per token setting. LAC keeps track of all progress measurements from the first step to the last step(t) before halting computations: Then is used to calculate dynamic threshold λt at step t: = {δ1, δ2 . . . , δt} λt = αmax(t) min(t) (5) (6) Here, λt represents the minimum required change in activation (progress) for the activations to continue to step + 1. The parameter α (0, 1] acts as knob to control the decisiveness of the threshold. LAC halts the computation for specific unit (batch, example, or token) before executing step + 1 if the progress made at step falls below the calculated threshold λt for that unit: δt < λt. This condition is evaluated independently for each unit at the chosen granularity."
        },
        {
            "title": "2.3 Threshold Sensitivity",
            "content": "The α parameter in Equation 6 directly controls the threshold sensitivity. lower value of α results in smaller λt, creating more permissive threshold that allows more computational steps. Conversely, higher α results in larger λt, leading to more decisive threshold and fewer steps. The LAC framework distinguishes between αt used during training and αe used during evaluation. The optimal α value depends on factors like the complexity of the input data. For our analysis, we use αe to control the halting behavior of pre-trained models during inference."
        },
        {
            "title": "2.4 Masking",
            "content": "LAC can dynamically halt computations at different granularities: for the entire batch, for individual examples within the batch, or for individual tokens within examples. Per-batch halting is relatively straightforward since the computation for the entire batch must proceed until the halting condition is met for all active examples; therefore, it is possible to halt the computations. However, implementing per-example and per-token halting efficiently on hardware accelerators is challenging due to poor performance with arbitrary sparse operations (i.e., skipping computations for arbitrary examples or tokens). Therefore, LAC employs masking to achieve this granular halting mechanism without requiring highly sparse operations. We define δE and λE as the change in the L2-norm and the corresponding threshold for examples, respectively. Similarly, δT and λT are defined for tokens. In per-example halting, masking is used to zero out the activations for an entire example when its specific halting condition is met. Let hi represent the activation corresponding to the i-th example in the batch. The masking operation is defined as follows: hi = (cid:40) 0 hi if δi < λE otherwise 1 (7) 3 where 0 is zero tensor with the same shape as hi, and is the maximum number of examples in the batch. When condition δi < λE is met, for example, i, its activations hi are set to zero, effectively halting further computation for this example in subsequent layers. Per-token requires finer-grained masking approach. Here, activations corresponding to individual tokens are zeroed out if their specific halting condition is met. Let hij represent the activation for the j-th token within the i-th example. The masking operation is defined as: hij = (cid:40) 0 hij if δij < λT otherwise 1 l, 1 (8) Here, 0 represents zero tensor of the same dimension as hij, is the maximum number of examples, and is the maximum sequence length in the batch. When condition δij < λT is met for specific token, its corresponding activations hij are set to zero, halting further processing for that particular token position within the example."
        },
        {
            "title": "3 Detecting and Skipping Voids in LMs",
            "content": "In this section, we modify the LAC threshold formula to precisely identify activated layers (voids) during LM inference. We then demonstrate the impact of selectively skipping these identified voids on the performance of three distinct instruction-tuned LMs across three benchmarks. Furthermore, we analyze the effect of the parameter α on layer usage and model performance when skipping voids, focusing on the BoolQ(Clark et al., 2019) dataset. Finally, we delve into the specific layer usage patterns and the underlying L2-norm dynamics that characterize voids."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "For our experiments, we considered three instruction-tuned LMs, Qwen2.5-7B-Instruct (Qwen et al., 2025), Llama3-8B-Instruct (Grattafiori et al., 2024), and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023). For our evals, we used Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021a), common world-knowledge and problem-solving benchmark, Graduate-Level Google-Proof Q&A (GPQA Diamond) (Rein et al., 2023), graduate-level multiple-choice benchmark in the biology, physics, and chemistry domains, and BoolQ (Clark et al., 2019), common yes/no reading comprehension benchmark. We evaluated these models in two configurations for each benchmark. First, we evaluated the LMs using their full layer capacity (i.e., without skipping any layers). Second, we evaluated the LMs when selectively skipping the detected unactivated layers (voids) using our modified LAC method. For the main performance evaluation across all benchmarks, we set the α parameter to 0.8. However, for the specific analysis of αs effect on the BoolQ benchmark, we explored range of α values from 0.1 to 1.0 to illustrate how varying the threshold impacts the amount of skipped layers and model behavior. All experiments were performed using single 24GB RTX 3090 GPU."
        },
        {
            "title": "3.2 Modifying LAC",
            "content": "To precisely identify unactivated layers (voids) during LM inference, we modified the standard LAC threshold formula (Section 6) by removing the absolute value: λt = α(max(t) min(t)) (9) The original LAC formulation used the absolute value to compute threshold aimed at efficiently halting computations during training. In contrast, for the purpose of detecting voids at inference time in pre-trained models, we found that removing the absolute value allows the threshold λt to better capture the subtle changes in L2-norm that indicate when layers contribution falls below significant level, thereby identifying it as void. 4 Figure 1: BoolQ accuracy and Percentage of total layers used (relative to total model layers) as function of α. Left: Llama3-8B-Instruct; Middle: Mistral-7B-Instruct-v0.3; Right: Qwen2.5-7B-Instruct. In the top row, the blue line shows performance when skipping voids, and the red horizontal line indicates the full models performance. In the bottom row, the red line represents average layer usage during Response Generation (RG), and the green line shows average layer usage during Prompt Processing (PP). Usage is shown as percentage of total model layers."
        },
        {
            "title": "3.3 Skipping Voids",
            "content": "Table 1 presents the performance scores of the LMs on the three benchmarks when using their full layer capacity (\"Not Skipped\") compared to when selectively skipping the detected voids (\"Skipped\"), using α = 0.8. The table also shows the average percentage of layers utilized per token during the Prompt Processing (PP) and Response Generation (RG) phases when skipping is enabled. Model Llama-3-8B-Instruct Qwen2.5-7B-Instruct Mistral-7B-Instruct-v0.3 Usage Llama-3-8B-Instruct(Layers=32) Qwen2.5-7B-Instruct(Layers=28) Mistral-7B-Instruct-v0.3(Layers=32) MMLUzeroshot GPQA Diamond BoolQ Not Skipped Skipped Not Skipped Skipped Not Skipped Skipped 61.18 69.24 59.70 60.42 71.29 59.29 29.11 34.78 13. 30.53 33.33 18.36 76.38 86.40 84.98 MMLUzeroshot GPQA Diamond BoolQ PP 0.53 0.29 0.71 RG 0.43 0.30 0.72 PP 0.62 0.32 0.74 RG 0.65 0.31 0.74 PP 0.64 0.36 0.71 75.92 83.81 83.18 RG 0.72 0.36 0. Table 1: Evaluation scores and average percentage of layers used when skipping voids for the three LMs on MMLU, GPQA Diamond, and BoolQ benchmarks in zero-shot setting, with α = 0.8. Results are shown for full model capacity (\"Not Skipped\") and when selectively skipping voids (\"Skipped\"). Average layer usage is presented separately for the Prompt Processing (PP) and Response Generation (RG) phases, normalized by the total number of layers and shown as percentage. Notably, we observe performance gains on some benchmarks despite skipping significant proportion of layers. For example, Qwen2.5-7B-Instruct shows 2.05 percentage point improvement in the MMLU zero-shot score (from 69.24 to 71.29) while using only approximately 30% of its total layer capacity (average of 29% for PP and 30% for RG). Similarly, Mistral-7B-Instruct-v0.3 achieves performance gain of 4.48 percentage points on GPQA Diamond (from 13.88 to 18.36) while utilizing approximately 74% of its layers during both phases. These results demonstrate that selective skipping of voids, identified by LAC, can lead to performance improvements on certain tasks, even when significantly reducing the computation per token. 5 Figure 2: Usage percentage of layers across proposed LMs while evaluating them on stated benchmarks. Red lines represent the usage of each layer for the Prompt Processing phase, where blue lines show the usage percentage of layers for the Response Generation phase. For ease of demonstration, we normalized the usage for each layer. Figure 1 illustrates the effect of varying the α parameter (from 0.1 to 1.0) on both the percentage of layers used and the BoolQ accuracy for the three LMs. As detailed in Section 2.3, higher values of α result in more stringent threshold λt, leading to lower layer usage (more skipped voids). This relationship is clearly evident in the second row of Figure 1. The first row of Figure 1 shows how performance changes with varying α when skipping voids. For Llama3-8B-Instruct and Mistral-7B-Instruct-v0.3, increasing α (and thus decreasing layer usage) generally leads to decrease in BoolQ accuracy across the evaluated α range. In contrast, Qwen2.5-7B-Instruct exhibits different behavior. Its performance generally improves as α increases, reaching its peak BoolQ accuracy at α = 1.0 while using only approximately 20% of its layers. This demonstrates how the choice of α impacts the balance between computational cost and performance when skipping voids, and shows that for some models, aggressive skipping (high α) can even lead to better performance than less aggressive skipping."
        },
        {
            "title": "3.4 Layers Usage",
            "content": "This section delves into the patterns of layer usage observed in the evaluated LMs during inference, specifically focusing on which layers are frequently activated and which appear as \"Voids\" unactivated layers that can potentially be skipped. Table 1 presents the average percentage of layers used per token during both the Prompt Processing (PP) and Response Generation (RG) phases when Voids are skipped. The results clearly show that, on average, LMs operate well below their full capacity. For example, Qwen2.5-7B-Instruct, despite having 28 layers, uses only around 29% for PP and 30% for RG on MMLU. Similarly, Llama-3-8B-Instruct (32 layers) averages about 53% usage for PP and 43% for RG on MMLU, while Mistral-7B-Instruct-v0.3 (32 layers) uses around 71% for PP and 72% for RG on the same benchmark. These low percentages highlight the substantial presence of Voids during standard inference. Crucially, Table 1 also shows that selectively skipping these detected Voids can lead to performance improvements on certain tasks. For instance, Qwen2.5-7B-Instructs MMLU zero-shot score improves from 69.24 to 71.29 when only using approximately 30% of its layers. Mistral-7B-Instruct-v0.3 sees notable jump on GPQA Diamond from 13.88 to 18.36 while using around 70% of its layers. These results strongly support the hypothesis that not all layers contribute equally, and skipping Voids could be beneficial. Figure 2 shows that the specific layers activated during PP and RG phases. This suggests shift in computational requirements between processing the input context and generating new tokens. Notably, for Qwen2.57B-Instruct, Figure 2 indicates that many layers, particularly those in the middle (around layers 4 to 17, and especially layers 10 to 13), are activated less than 20% of the time, often nearing 0% usage. These layers represent significant \"Voids\" where computations are frequently skipped by LAC. Although Llama-3-8B and Mistral show fluctuations, they do not exhibit such prominent, sustained low-usage in their middle layers as observed in Qwen. 6 Figure 3: Mean L2 Norm (top row) and mean change in L2 Norm (bottom row) for each layer of Llama-3-8B-Instruct, Mistral7-B-Instruct-v0.3, and Qwen2.5-7B-Instruct, averaged across the evaluated benchmarks. Blue lines represent metrics during the Prompt Processing (PP) phase, and red lines represent metrics during the Response Generation (RG) phase. The change in L2 norm for layer is calculated from the L2 norm of its output minus the L2 norm of its input (see Section 2.2)."
        },
        {
            "title": "3.5 L2 Norms",
            "content": "Since the LAC mechanism detects voids based on changes in the L2-norm of activations (Section 2.2), we analyze the mean L2-norm of activations and the mean change in L2-norm across each layer for the evaluated LMs and benchmarks. Figure 3 shows these metrics. In the top row, we observe that the mean L2-norm of activations generally increases as they pass through layers sequentially. The mean L2-norm for the PP phase is consistently higher than for the RG phase across all LMs, suggesting differences in activation magnitudes between processing and generation. The bottom row of Figure 3 plots the mean change in L2-norm contributed by each layer. We observe that middle layers (e.g., layers 5 to 15, depending on the model) tend to exhibit smaller mean changes in the L2-norm compared to earlier and later layers. This reduced contribution to the change in L2-norm provides insight into *why* these layers are more frequently skipped by the LAC halting mechanism, as their \"progress\" (change in L2norm) often falls below the dynamic threshold λt. This reduced contribution to progress aligns with the pattern seen in Figure 2, where these same layers exhibit lower usage percentages, thus appearing more frequently as \"Voids\"."
        },
        {
            "title": "4.1 Model pruning",
            "content": "The model pruning (LeCun et al., 1989; Hassibi and Stork, 1992) is classic method for reducing the size and complexity of trained neural network by removing redundant modules(e.g. neurons and layers). while the initial focus of model pruning involved magnitude-based pruning (Han et al., 2015), which remove the redundant parameters by setting weights with small values to zero. Structured pruning on the other hand, focuses on removing the non-critical structures from models (Li et al., 2016; Luo et al., 2017). Some studies uses this method to compress LMs, Men et al. (2024) uses metric called Block Influence (BI) which use the similarity between layers input and output to measure the importance of each layer, Gromov et al. (2025) study the knowledge stored in LMs weight via layer pruning on common questionanswering benchmarks, indicating that weights in unnecessary layers are not storing the knowledge needed to answer those questions. Zhang et al. (2024) propose new form of fine-grained layer, which considers all self-attention and feed-forward network (FFN) layers within blocks as individual pruning candidates."
        },
        {
            "title": "4.2 Adaptive Computation",
            "content": "Adaptive computation is technique that allows neural networks to dynamically adjust the amount of computation performed for different inputs. Instead of having fixed computational budget, adaptive computation models can allocate more resources to complex inputs and fewer to simpler ones. The concept of adaptive computation has been explored in various neural network architectures and tasks. For recurrent neural networks (RNNs), Graves (2017) introduced Adaptive Computation Time (ACT) to allow the network to learn how many computational steps to take before producing an output. Building upon ACT, Banino et al. (2021) introduced PonderNet, which allows models to dynamically adjust the number of computational steps and improves the halting mechanisms for deciding when to stop processing. Several studies have explored adaptive computation in the context of language models. Schuster et al. (2022) introduced framework called Confident Adaptive Language Modeling (CALM) that dynamically allocates varying amounts of computation for both input and generated tokens. Their work demonstrated that using early-exit decoding at the token level can enhance the efficiency of language models. Similarly, yikun jiang et al. (2024) proposed token-adaptive computing resource allocation strategy for large language models called D-LLM. This method involves dynamic decision module within each transformer layer that determines whether to execute or skip the layer for given token."
        },
        {
            "title": "4.3 Layer Effectiveness",
            "content": "Research into language models has increasingly focused on understanding the role and effectiveness of individual layers. Several studies have examined how knowledge is stored and processed across the different layers of these complex networks. One notable observation is that not all layers contribute equally to the models performance Several studies have examined the effectiveness of layers in Language Models (LMs). Sun et al. (2025) suggests that significant portion of layers, particularly in deeper parts of the network, might be less effective than expected. This phenomenon has been referred to as the \"Curse of Depth\" in Large Language Models. One proposed explanation for this reduced effectiveness in deeper layers is the widespread use of Pre-Layer Normalization (Pre-LN). While Pre-LN helps stabilize training in Transformer models, its output variance can grow exponentially with the models depth. This can lead to the derivative of deeper Transformer blocks approaching an identity matrix, effectively minimizing their contribution to training. Jin et al. (2024) suggests that basic concepts might be learned in shallower layers, while more complex concepts require deeper layers for full acquisition. Ju et al. (2024) indicates that knowledge related to entities might be primarily encoded in knowledge-related entity tokens at lower layers, with this knowledge expanding to other tokens in upper layers."
        },
        {
            "title": "5 Limitation",
            "content": "While our work effectively identifies and leverages voids for potential performance gains, it is important to acknowledge some limitations. Our current implementation utilizes masking to zero out activations of inactive layers, as detailed in Section 2.4, rather than explicitly enabling dynamic halting of computation for specific layers or tokens. This means that, in its present form, our method does not directly translate to an increase in computational efficiency during inference. Although layers are effectively skipped for masked units, the forward pass still needs to be executed through all layers to calculate the L2 norm changes required by LACs halting mechanism at each step. Future work will focus on developing hardware-aware implementations that can leverage the identified voids for true dynamic execution and computational speedups."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we explored fundamental, yet understudied, aspect of transformer-based language models: the extent to which all layers are activated during inference. To address this question, we employed modified version of L2 Adaptive Computation (LAC). This non-trainable, parameter-free adaptive computation method 8 was adapted to precisely detect unactivated layers, which we termed \"Voids,\" by monitoring changes in the L2norm of activations. Our analysis focused on two key phases of processing tokens in LMs: Prompt Processing (PP) and Response Generation (RG). Through experiments on three distinct instruction-tuned LMs (Llama3-8B-Instruct, Mistral-7B-Instruct-v0.3, and Qwen2.5-7B-Instruct) evaluated across three benchmarks (MMLU, GPQA Diamond, and BoolQ), we demonstrate that LMs do not consistently activate all layers during inference. Importantly, we showed that selectively skipping these detected Voids can lead to performance improvements on certain tasks, even when utilizing significantly reduced subset of the models total layers. For example, skipping voids in Qwen2.5-7B-Instruct improved its MMLU zero-shot score while utilizing approximately 30% of its layers. Similarly, Mistral-7B-Instruct-v0.3 saw performance enhanced on GPQA Diamond using around 70% of its layers across both PP and RG phases. We observed distinct layer activation patterns between the PP and RG phases, indicating task-dependent computational allocation. The layer usage profiles also differed significantly across different model families, with Qwen2.5-7B-Instruct showing particularly low usage percentages in its middle layers, representing significant \"Voids.\" Analysis of the L2 norms further supported these observations, indicating reduced progress contributions from layers with lower usage. This work highlights significant opportunities for enhancing model efficiency through dynamic inference strategies. Furthermore, our method provides novel tool for probing layer-specific function, which could open new avenues for interpretability, targeted knowledge editing, hallucination detection, and more sophisticated pruning techniques. Specifically, by providing method to detect which layers are less consistently activated (i.e., appear as Voids) and contribute less to the L2-norm change during inference, our work offers valuable insights for improving layer pruning strategies. Identifying and potentially removing these less critical layers, followed by targeted fine-tuning, could lead to more efficient and potentially less performance-degrading pruning compared to methods that prune layers uniformly or based on less direct metrics."
        },
        {
            "title": "Acknowledgments",
            "content": "The author wishes to thank Mohammad Mashayekhi for his initial collaboration on this work."
        },
        {
            "title": "References",
            "content": "G. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes, 2018. URL https: //arxiv.org/abs/1610.01644. A. Banino, J. Balaguer, and C. Blundell. Pondernet: Learning to ponder, 2021. URL https://arxiv.org/ abs/2107.05407. E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup. Conditional computation in neural networks for faster models, 2016. URL https://arxiv.org/abs/1511.06297. T. Bolukbasi, J. Wang, O. Dekel, and V. Saligrama. Adaptive neural networks for efficient inference, 2017. URL https://arxiv.org/abs/1702.07811. T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T. Maxwell, N. Joseph, Z. Hatfield-Dodds, A. Tamkin, K. Nguyen, B. McLean, J. E. Burke, T. Hume, S. Carter, T. Henighan, and C. Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformercircuits.pub/2023/monosemantic-features/index.html. C. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022. C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, 9 H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Łukasz Kaiser. Universal transformers, 2019. URL https: //arxiv.org/abs/1807.03819. M. Dehghani, A. Arnab, L. Beyer, A. Vaswani, and Y. Tay. The efficiency misnomer, 2022. URL https:// arxiv.org/abs/2110.12894. N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, mathematical framework for transformer circuits. https://transformercircuits.pub/2021/framework/index.html. J. Kaplan, S. McCandlish, and C. Olah. Transformer Circuits Thread, 2021. J. Clark, N. Elhage, T. Hume, C. Olsson, N. Nanda, T. Henighan, S. Johnston, S. ElShowk, N. Joseph, N. DasSarma, B. Mann, D. Hernandez, A. Askell, K. Ndousse, A. Jones, D. Drain, A. Chen, Y. Bai, D. Ganguli, L. Lovitt, Z. HatfieldDodds, J. Kernion, T. Conerly, S. Kravec, S. Fort, S. Kadavath, J. Jacobson, E. Tran-Johnson, J. Kaplan, J. Clark, T. Brown, S. McCandlish, D. Amodei, and C. Olah. Softmax linear units. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/solu/index.html. M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Wyatt, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Guzmán, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Thattai, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. A. Ibarra, I. Kloumann, I. Misra, I. Evtimov, J. Zhang, J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Prasad, K. Upasani, K. Plawiak, K. Li, K. Heafield, K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu, K. Bhalla, K. Lakhotia, L. Rantala-Yeary, L. van der Maaten, L. Chen, L. Tan, L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat, L. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri, M. Kardas, M. Tsimpoukelli, M. Oldham, M. Rita, M. Pavlova, M. Kambadur, M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi, N. Bashlykov, N. Bogoychev, N. Chatterji, N. Zhang, O. Duchenne, O. Çelebi, P. Alrassy, P. Zhang, P. Li, P. Vasic, P. Weng, P. Bhargava, P. Dubal, P. Krishnan, P. S. Koura, P. Xu, Q. He, Q. Dong, R. Srinivasan, R. Ganapathy, R. Calderer, R. S. Cabral, R. Stojnic, R. Raileanu, R. Maheswari, R. Girdhar, R. Patel, R. Sauvestre, R. Polidoro, R. Sumbaly, R. Taylor, R. Silva, R. Hou, R. Wang, S. Hosseini, S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie, S. Narang, S. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang, S. Vandenhende, S. Batra, S. Whitman, S. Sootla, S. Collot, S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha, T. Georgiou, T. Scialom, T. Speckbacher, T. Mihaylov, T. Xiao, U. Karn, V. Goswami, V. Gupta, V. Ramanathan, V. Kerkez, V. Gonguet, V. Do, V. Vogeti, V. Albiero, V. Petrovic, W. Chu, W. Xiong, W. Fu, W. Meers, X. Martinet, X. Wang, X. Wang, X. E. Tan, X. Xia, X. Xie, X. Jia, X. Wang, Y. Goldschlag, Y. Gaur, Y. Babaei, Y. Wen, Y. Song, Y. Zhang, Y. Li, Y. Mao, Z. D. Coudert, Z. Yan, Z. Chen, Z. Papakipos, A. Singh, A. Srivastava, 10 A. Jain, A. Kelsey, A. Shajnfeld, A. Gangidi, A. Victoria, A. Goldstand, A. Menon, A. Sharma, A. Boesenberg, A. Baevski, A. Feinstein, A. Kallet, A. Sangani, A. Teo, A. Yunus, A. Lupu, A. Alvarado, A. Caples, A. Gu, A. Ho, A. Poulton, A. Ryan, A. Ramchandani, A. Dong, A. Franco, A. Goyal, A. Saraf, A. Chowdhury, A. Gabriel, A. Bharambe, A. Eisenman, A. Yazdan, B. James, B. Maurer, B. Leonhardi, B. Huang, B. Loyd, B. D. Paola, B. Paranjape, B. Liu, B. Wu, B. Ni, B. Hancock, B. Wasti, B. Spence, B. Stojkovic, B. Gamido, B. Montalvo, C. Parker, C. Burton, C. Mejia, C. Liu, C. Wang, C. Kim, C. Zhou, C. Hu, C.-H. Chu, C. Cai, C. Tindal, C. Feichtenhofer, C. Gao, D. Civin, D. Beaty, D. Kreymer, D. Li, D. Adkins, D. Xu, D. Testuggine, D. David, D. Parikh, D. Liskovich, D. Foss, D. Wang, D. Le, D. Holland, E. Dowling, E. Jamil, E. Montgomery, E. Presani, E. Hahn, E. Wood, E.-T. Le, E. Brinkman, E. Arcaute, E. Dunbar, E. Smothers, F. Sun, F. Kreuk, F. Tian, F. Kokkinos, F. Ozgenel, F. Caggioni, F. Kanayet, F. Seide, G. M. Florez, G. Schwarz, G. Badeer, G. Swee, G. Halpern, G. Herman, G. Sizov, Guangyi, Zhang, G. Lakshminarayanan, H. Inan, H. Shojanazeri, H. Zou, H. Wang, H. Zha, H. Habeeb, H. Rudolph, H. Suk, H. Aspegren, H. Goldman, H. Zhan, I. Damlaj, I. Molybog, I. Tufanov, I. Leontiadis, I.-E. Veliche, I. Gat, J. Weissman, J. Geboski, J. Kohli, J. Lam, J. Asher, J.-B. Gaya, J. Marcus, J. Tang, J. Chan, J. Zhen, J. Reizenstein, J. Teboul, J. Zhong, J. Jin, J. Yang, J. Cummings, J. Carvill, J. Shepard, J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu, K. H. U, K. Saxena, K. Khandelwal, K. Zand, K. Matosich, K. Veeraraghavan, K. Michelena, K. Li, K. Jagadeesh, K. Huang, K. Chawla, K. Huang, L. Chen, L. Garg, L. A, L. Silva, L. Bell, L. Zhang, L. Guo, L. Yu, L. Moshkovich, L. Wehrstedt, M. Khabsa, M. Avalani, M. Bhatt, M. Mankus, M. Hasson, M. Lennie, M. Reso, M. Groshev, M. Naumov, M. Lathi, M. Keneally, M. Liu, M. L. Seltzer, M. Valko, M. Restrepo, M. Patel, M. Vyatskov, M. Samvelyan, M. Clark, M. Macey, M. Wang, M. J. Hermoso, M. Metanat, M. Rastegari, M. Bansal, N. Santhanam, N. Parks, N. White, N. Bawa, N. Singhal, N. Egebo, N. Usunier, N. Mehta, N. P. Laptev, N. Dong, N. Cheng, O. Chernoguz, O. Hart, O. Salpekar, O. Kalinli, P. Kent, P. Parekh, P. Saab, P. Balaji, P. Rittner, P. Bontrager, P. Roux, P. Dollar, P. Zvyagina, P. Ratanchandani, P. Yuvraj, Q. Liang, R. Alao, R. Rodriguez, R. Ayub, R. Murthy, R. Nayani, R. Mitra, R. Parthasarathy, R. Li, R. Hogan, R. Battey, R. Wang, R. Howes, R. Rinott, S. Mehta, S. Siby, S. J. Bondu, S. Datta, S. Chugh, S. Hunt, S. Dhillon, S. Sidorov, S. Pan, S. Mahajan, S. Verma, S. Yamamoto, S. Ramaswamy, S. Lindsay, S. Lindsay, S. Feng, S. Lin, S. C. Zha, S. Patil, S. Shankar, S. Zhang, S. Zhang, S. Wang, S. Agarwal, S. Sajuyigbe, S. Chintala, S. Max, S. Chen, S. Kehoe, S. Satterfield, S. Govindaprasad, S. Gupta, S. Deng, S. Cho, S. Virk, S. Subramanian, S. Choudhury, S. Goldman, T. Remez, T. Glaser, T. Best, T. Koehler, T. Robinson, T. Li, T. Zhang, T. Matthews, T. Chou, T. Shaked, V. Vontimitta, V. Ajayi, V. Montanez, V. Mohan, V. S. Kumar, V. Mangla, V. Ionescu, V. Poenaru, V. T. Mihailescu, V. Ivanov, W. Li, W. Wang, W. Jiang, W. Bouaziz, W. Constable, X. Tang, X. Wu, X. Wang, X. Wu, X. Gao, Y. Kleinman, Y. Chen, Y. Hu, Y. Jia, Y. Qi, Y. Li, Y. Zhang, Y. Zhang, Y. Adi, Y. Nam, Yu, Wang, Y. Zhao, Y. Hao, Y. Qian, Y. Li, Y. He, Z. Rait, Z. DeVito, Z. Rosnbrick, Z. Wen, Z. Yang, Z. Zhao, and Z. Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. A. Graves. Adaptive computation time for recurrent neural networks, 2017. URL https://arxiv.org/ abs/1603.08983. A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts. The unreasonable ineffectiveness of the deeper layers, 2025. URL https://arxiv.org/abs/2403.17887. W. Gurnee, N. Nanda, M. Pauly, K. Harvey, D. Troitskii, and D. Bertsimas. Finding neurons in haystack: Case studies with sparse probing. arXiv preprint arXiv:2305.01610, 2023. S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. B. Hassibi and D. Stork. Second order derivatives for network pruning: Optimal brain surgeon. Advances in neural information processing systems, 5, 1992. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009.03300. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. M. Jin, Q. Yu, J. Huang, Q. Zeng, Z. Wang, W. Hua, H. Zhao, K. Mei, Y. Meng, K. Ding, et al. Exploring concept depth: How large language models acquire knowledge and concept at different layers? arXiv preprint arXiv:2404.07066, 2024. 11 T. Ju, W. Sun, W. Du, X. Yuan, Z. Ren, and G. Liu. How large language models encode context knowledge? layer-wise probing study. arXiv preprint arXiv:2402.16061, 2024. Y. LeCun, J. Denker, and S. Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989. H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016. J.-H. Luo, J. Wu, and W. Lin. Thinet: filter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, pages 50585066, 2017. X. Men, M. Xu, Q. Zhang, B. Wang, H. Lin, Y. Lu, X. Han, and W. Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. OpenAI, :, A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, A. Iftimie, A. Karpenko, A. T. Passos, A. Neitz, A. Prokofiev, A. Wei, A. Tam, A. Bennett, A. Kumar, A. Saraiva, A. Vallone, A. Duberstein, A. Kondrich, A. Mishchenko, A. Applebaum, A. Jiang, A. Nair, B. Zoph, B. Ghorbani, B. Rossen, B. Sokolowsky, B. Barak, B. McGrew, B. Minaiev, B. Hao, B. Baker, B. Houghton, B. McKinzie, B. Eastman, C. Lugaresi, C. Bassin, C. Hudson, C. M. Li, C. de Bourcy, C. Voss, C. Shen, C. Zhang, C. Koch, C. Orsinger, C. Hesse, C. Fischer, C. Chan, D. Roberts, D. Kappler, D. Levy, D. Selsam, D. Dohan, D. Farhi, D. Mely, D. Robinson, D. Tsipras, D. Li, D. Oprica, E. Freeman, E. Zhang, E. Wong, E. Proehl, E. Cheung, E. Mitchell, E. Wallace, E. Ritter, E. Mays, F. Wang, F. P. Such, F. Raso, F. Leoni, F. Tsimpourlas, F. Song, F. von Lohmann, F. Sulit, G. Salmon, G. Parascandolo, G. Chabot, G. Zhao, G. Brockman, G. Leclerc, H. Salman, H. Bao, H. Sheng, H. Andrin, H. Bagherinezhad, H. Ren, H. Lightman, H. W. Chung, I. Kivlichan, I. OConnell, I. Osband, I. C. Gilaberte, I. Akkaya, I. Kostrikov, I. Sutskever, I. Kofman, J. Pachocki, J. Lennon, J. Wei, J. Harb, J. Twore, J. Feng, J. Yu, J. Weng, J. Tang, J. Yu, J. Q. Candela, J. Palermo, J. Parish, J. Heidecke, J. Hallman, J. Rizzo, J. Gordon, J. Uesato, J. Ward, J. Huizinga, J. Wang, K. Chen, K. Xiao, K. Singhal, K. Nguyen, K. Cobbe, K. Shi, K. Wood, K. Rimbach, K. Gu-Lemberg, K. Liu, K. Lu, K. Stone, K. Yu, L. Ahmad, L. Yang, L. Liu, L. Maksin, L. Ho, L. Fedus, L. Weng, L. Li, L. McCallum, L. Held, L. Kuhn, L. Kondraciuk, L. Kaiser, L. Metz, M. Boyd, M. Trebacz, M. Joglekar, M. Chen, M. Tintor, M. Meyer, M. Jones, M. Kaufer, M. Schwarzer, M. Shah, M. Yatbaz, M. Y. Guan, M. Xu, M. Yan, M. Glaese, M. Chen, M. Lampe, M. Malek, M. Wang, M. Fradin, M. McClay, M. Pavlov, M. Wang, M. Wang, M. Murati, M. Bavarian, M. Rohaninejad, N. McAleese, N. Chowdhury, N. Chowdhury, N. Ryder, N. Tezak, N. Brown, O. Nachum, O. Boiko, O. Murk, O. Watkins, P. Chao, P. Ashbourne, P. Izmailov, P. Zhokhov, R. Dias, R. Arora, R. Lin, R. G. Lopes, R. Gaon, R. Miyara, R. Leike, R. Hwang, R. Garg, R. Brown, R. James, R. Shu, R. Cheu, R. Greene, S. Jain, S. Altman, S. Toizer, S. Toyer, S. Miserendino, S. Agarwal, S. Hernandez, S. Baker, S. McKinney, S. Yan, S. Zhao, S. Hu, S. Santurkar, S. R. Chaudhuri, S. Zhang, S. Fu, S. Papay, S. Lin, S. Balaji, S. Sanjeev, S. Sidor, T. Broda, A. Clark, T. Wang, T. Gordon, T. Sanders, T. Patwardhan, T. Sottiaux, T. Degry, T. Dimson, T. Zheng, T. Garipov, T. Stasi, T. Bansal, T. Creech, T. Peterson, T. Eloundou, V. Qi, V. Kosaraju, V. Monaco, V. Pong, V. Fomenko, W. Zheng, W. Zhou, W. McCabe, W. Zaremba, Y. Dubois, Y. Lu, Y. Chen, Y. Cha, Y. Bai, Y. He, Y. Zhang, Y. Wang, Z. Shao, and Z. Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. H. Orgad, M. Toker, Z. Gekhman, R. Reichart, I. Szpektor, H. Kotek, and Y. Belinkov. Llms know more than they show: On the intrinsic representation of llm hallucinations. arXiv preprint arXiv:2410.02707, 2024. Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduatelevel google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Q. Tran, Y. Tay, and D. Metzler. Confident adaptive language modeling, 2022. URL https://arxiv.org/abs/2207.07061. M. Shemiranifar and M. Dehghani. L2 norm guided adaptive computation, 2023. openreview.net/forum?id=qW_GZYyn7C. URL https:// W. Sun, X. Song, P. Li, L. Yin, Y. Zheng, and S. Liu. The curse of depth in large language models, 2025. URL https://arxiv.org/abs/2502.05795. 12 A. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022. X. Wang, F. Yu, Z.-Y. Dou, T. Darrell, and J. E. Gonzalez. Skipnet: Learning dynamic routing in convolutional networks, 2018. URL https://arxiv.org/abs/1711.09485. F. Xue, V. Likhosherstov, A. Arnab, N. Houlsby, M. Dehghani, and Y. You. Adaptive computation with elastic input sequence, 2023. URL https://arxiv.org/abs/2301.13195. yikun jiang, H. Wang, L. Xie, H. Zhao, C. Zhang, H. Qian, and J. C. Lui. D-LLM: token adaptive computing resource allocation strategy for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=UIOjGTKHQG. Y. Zhang, Y. Li, X. Wang, Q. Shen, B. Plank, B. Bischl, M. Rezaei, and K. Kawaguchi. Finercut: Finer-grained interpretable layer pruning for large language models. arXiv preprint arXiv:2405.18218, 2024."
        },
        {
            "title": "A Hallucination Correction",
            "content": "As discussed in Section 3.3, our method led to performance improvements on certain benchmarks. Here, we provide specific examples demonstrating how this mechanism can also mitigate hallucination, particularly in mathematical reasoning tasks. The following questions are taken from the MATH dataset (Hendrycks et al., 2021b) with zero-shot prompting (temperature 0.6, and top-p 0.9). In each case, the Not Skipped model produces hallucinated answer, while the Skipped model uses reduced set of layers, provides the correct solution. A.1 Llama-3-8B-Instruct Question and True answer Question: At which point do the lines = 9 2t and = 3s + 1 intersect? Give your answer as an ordered pair in the form (s, t). Answer: We can substitute the second equation into the first equation to get Moving the variable terms to the left-hand side and the constants to the right-hand side, we find = 9 2(3s + 1) = 9 6s 2. This gives = 1 which we may plug into either equation to get t. For example, + 6s = 7. So the lines intersect at the point (1, 4) . = 3(1) + 1 = 4. Not Skipped have = 3s+1 and = 92t. Substituting the second equation into the first, we obtain = 3(92t)+1, so = 27 6t, so 7t = 27 and = 3. Plugging this into the second equation, we have = 9 2(3) = (3, 3) . Final Answer: The final answer is (3,3). hope it is correct. Skipped (α = 0.98) find the point of intersection of the lines given by the equations = 9 2t and = 3s + 1 we need to solve these equations simultaneously. First, lets substitute from the second equation into the first equation: = 9 2(3s + 1). Now, simplify the right-hand side: = 9 6s 2. Combine like terms: = 7 6s. Add 6s to both sides to isolate s: 7s = 7. Divide both sides by 7: = 1. Now that we have = 1, substitute this value back into the second equation to find t: = 3(1) + 1. Simplify: = 3 + 1 = 4. Thus, the point of intersection is (s, t) = (1, 4) . So, the final answer is: (1, 4) . 14 A.2 Mistral-7B-Instruct-v0.3 Question and True answer Question: Let (x) = (cid:40) x2 + 9 3x 8 if < 5, if 5. Answer: We begin by looking at each of the two possible cases; either < 5 and (x) = x2 + 9 = 10, or 5 and (x) = 3x 8 = 10. Tackling the first case, we find that the only possible values of that could satisfy x2 + 9 = 10 x2 = 1 are 1 and -1, neither of which are less than -5, thus yielding no possible solutions. In the second case, the only possible value of that satisfies 3x 8 = 10 is 6. Since this value is greater than or equal to -5, it satisfies both conditions. Thus, the only possible value of for which (x) = 10 is 6, which means the sum of all possible values is also 6 . Not Skipped find the solutions, we need to find the values of that satisfy the equation (x) = 10. We have two cases : 1 ) For < 5, (x) = x2 + 9. We need to find the value of such that x2 + 9 = 10. Sol ving this equation gives x2 = 1, which has solutions = 1. However , since < 5, the only solution in this case is = 1 2 ) For 5, (x) = 3x 8. We need to find the value of such that 3x 8 = 10. Solving this equation gives = (10 + 8)/3 6.33. So, the sum of all possible values of is approximately 6.06 + 6.33 = 0.27. 10 6.06. Skipped (α = 0.80) find the values of such that (x) = 10 , we need to solve the following equations : 1 ) For < 5 : x2 + 9 = 10x2 = 10 9x2 = 1x = 1 But < 5 , so = 1 is not possible since - 1 squared is positive. Therefore , = 1 is the solution for < 5. 2 ) for >= 5 : 3x 8 = 103x = 18x = 18/3x = 6 So , there are two solutions : = 1 and = 6 . However , = 1 viol ates the condition >= 5 , so the only solution is = 6 . Since we are asked to find the sum of all possible values of x, there is only one value : 6. A.3 Qwen2.5-7B-Instruct Question and True answer Question: For how many positive integer values of does kx2 + 10x + = 0 have rational solutions? Answer: By considering the expression for the solutions of ax2 + bx + = 0, we find that the solutions are rational if and only if the discriminant b2 4ac has rational square root. Therefore, the solutions of kx2 + 10x + = 0 are rational if and only if 100 4(k)(k) is perfect square. (Recall is irrational). By writing the discriminant that if is an integer which is not perfect square, then as 4(25 k2), we see that we only need to check the integers 1 5. Of these, 3, 4, and 5 work, for total of 3 integer values of k. b24ac 2a 15 Not Skipped determine the number of positive integer values of for which the quadratic equation kx2 +10x+k = 0 has rational solutions, we need to ensure that the discriminant of the quadratic equation is perfect square. The general form of quadratic equation is ax2 + bx + = 0, and its discriminant is given by = b2 4ac. For the equation kx2 + 10x + = 0, we have = k, = 10, and = k. The discriminant is: = 102 4 = 100 4k2 For the quadratic equation to have rational solutions, the discriminant must be perfect square. Lets denote this perfect square by m2, where is an integer. Therefore, we have: 100 4k2 = m2 This can be rewritten as: 100 m2 = 4k2 or This is difference of squares, which can be factored as: 100 m2 = (2k)2 (10 m)(10 + m) = (2k) Since (2k)2 is perfect square, 10 and 10 + must be pairs of factors of perfect square. Lets list the factor pairs of 100 (since 100 = (10 m)(10 + m)): (1, 100), (1, 100), (2, 50), (2, 50), (4, 25), (4, 25), (5, 20), (5, 20), (10, 10), (10, 10) We need to check which of these pairs can be written in the form (10 m, 10 + m) and ensure that is positive integer. 1. For the pair (1, 100): 10m = 1 = = 9 and 10+m = 100 = = 91 (not possible since must be the same) 2. For the pair (2, 50): 10 = 2 = = 8 and 10 + = 50 = = 40 (not possible since must be the same) 3. For the pair (4, 25): 10 = 4 = = 6 and 10 + = 25 = = 15 (not possible since must be the same) 4. For the pair (5, 20): 10 = 5 = = 5 and 10 + = 20 = = 10 (possible) 5. For the pair (10, 10): 2k = 10 = = 10 = 10 = = 0 and 10 + = 10 = = 0 (possible) 2k = 10 = = 5 6. For the pair (1, 100), (2, 50), (4, 25), and (5, 20), the calculations will be the same as above but with negative m, which will not change the value of k. Thus, the only positive integer value of that satisfies the condition is = 5. Therefore, the number of positive integer values of is: 1 . Skipped (α = 0.98) determine how many positive integer values of make the quadratic equation ax2 + bx + = 0 have rational solutions, we need to use the condition that the discriminant = b2 4ac must be perfect square. For the given equation kx2 +peater solution steps: 1. Identify coefficients: For the equation kx2 + 10x + = 0, we have: = k, = 10, = 16 2. Calculate the discriminant: The discriminant is given by: = b2 4ac = 102 4 = 100 4k2 3. condition: For the solutions to be rational, must be non-negative perfect square: 100 4k2 = m2 for some integer 4. Rearrange the equation: 100 m2 = 4k 4k2 + m2 = 100 5. Solve for integer solutions: We need to find pairs (k, m) such that 4k2 + m2 = 100. We will check possible values of since must be positive integer. - For = 1: 4(1)2 + m2 = 100 = 4 + m2 = 100 = m2 = 96 (not perfect square) - For = 2: 4(2)2 + m2 = 100 = 16 + m2 = 100 = m2 = 84 (not perfect square) - For = 3: 4(3)2 + m2 = 100 = 36 + m2 = 100 = m2 = 64 = = 8 ( perfect square) - For = 4: 4(4)2 + m2 = 100 = 64 + m2 = 100 = m2 = 36 = = 6 ( perfect square) - For = 5: 4(5)2 + m2 = 100 = 100 + m2 = 100 = m2 = 0 = = 0 ( perfect square) - For = 6: - For 7: 4(6)2 + m2 = 100 = 144 + m2 = 100 = m2 = (not possible) 4k2 4(72) = 196 = 4k2 + m2 > 100 (not possible) 6. List valid values: The positive integer values of that satisfy the condition are = 3, 4, 5. Therefore, there are three positive integer values of for which the equation kx2 + 10x + = 0 has rational solutions."
        },
        {
            "title": "B Layers Usage Visualization",
            "content": "This appendix provides visualizations of the layer usage patterns observed during the inference process for the specific examples discussed in Appendix A. These plots offer detailed view of which layers of the Language Model are activated for each token in the PP and RG phases. Each plot displays the sequence of tokens on the x-axis (we only show the tokens for the PP phase since, for the RG phase, due to the large amount of generated tokens, we do not show the tokens) and the models layers (from bottom to top) on the y-axis. White pixels indicate that specific layer was activated while processing the corresponding token. Black pixels represent unactivated layers, which we refer to as Voids. By examining these visualizations for Llama-3-8B, Mistral-7B, and Qwen2.5-7B on the Hallucination Correction examples, we can visually confirm the token-specific and layer-specific sparsity of computation during inference. The presence of significant black areas (Voids) throughout the plots demonstrates that not all layers are activated for every token. These visualizations provide direct evidence of the \"Voids\" that the LAC identifies. B.1 Llama-3-8B-Instruct 18 B.2 Mistral-7B-Instruct-v0. B.3 Qwen2.5-7B-Instruct"
        }
    ],
    "affiliations": []
}