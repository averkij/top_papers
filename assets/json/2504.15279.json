{
    "paper_title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models",
    "authors": [
        "Weiye Xu",
        "Jiahao Wang",
        "Weiyun Wang",
        "Zhe Chen",
        "Wengang Zhou",
        "Aijun Yang",
        "Lewei Lu",
        "Houqiang Li",
        "Xiaohua Wang",
        "Xizhou Zhu",
        "Wenhai Wang",
        "Jifeng Dai",
        "Jinguo Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning. Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 7 2 5 1 . 4 0 5 2 : r VisuLogic: Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models Weiye Xu1,3, Jiahao Wang2,3, Weiyun Wang3, Zhe Chen3, Wengang Zhou1, Aijun Yang2, Lewei Lu4, Houqiang Li1, Xiaohua Wang2, Xizhou Zhu3, Wenhai Wang3, Jifeng Dai5,3(cid:12), Jinguo Zhu3(cid:12) 1University of Science and Technology of China, 2Xian Jiaotong University, 3Shanghai Artifcial Intelligence Laboratory, 4SenseTime Research, 5Tsinghua University ustcxwy0271@mail.ustc.edu.cn, wjhwdscience@stu.xjtu.edu.cn, lechatelia@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Visual reasoning is core component of human intelligence and critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow languagebased reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracyonly slightly above the 25% random baseline and far below the 51.4% achieved by humansrevealing significant gaps in visual reasoning. Furthermore, we provide supplementary training dataset and reinforcement-learning baseline to support further progress. Code, data, and baselines are available at https://visulogic-benchmark.github.io/VisuLogic."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: Composition of the VisuLogic benchmark and performance of representative MLLMs. The left figure shows the distribution of the 6 categories and their subcategories in VisuLogic. The right figure shows accuracies (%) achieved by MLLMs and by human on each category of VisuLogic. equal contribution; interns at OpenGVLab, Shanghai AI Laboratory; (cid:12) corresponding author. (a) Pipeline of MLLM descriptionLLM for Question in MMMU [89]. It is trivial that SOTA MLLMs extract key visual details, thereby enabling the LLM to answer questions solely based on language reasoning. (b) Pipeline of MLLM descriptionLLM for Question in VisuLogic. Even SOTA MLLMs struggle to describe images precisely, leading to ambiguous interpretations. Figure 2: Comparison of the MLLM descriptionLLM pipeline on two benchmarks. In MMMU, detailed descriptions lead to correct solutions, while in VisuLogic, critical visual cues (e.g., symmetry, rotation) can be easily lost, causing the LLM to misinterpret the image. This highlights that textual reasoning alone is insufficient, underscoring the benchmarks demand for robust and in-depth visual reasoning. Reasoning, as fundamental component of human intelligence, has become critical criterion in evaluating progress toward Artificial General Intelligence (AGI) [26, 74]. Recent advancements in Large Language Models (LLMs) have demonstrated substantial improvements in reasoning capabilities across complex domains such as mathematics [61, 82, 81, 58], logical reasoning [68, 79, 23, 47] and coding [2, 35, 42, 32]. Techniques like Chain-of-Thought (CoT) [75] prompting and test-time compute scaling (e.g., OpenAI o1 [34] and Deepseek-R1 [18]) have significantly enhanced the reasoning performance of LLMs [18, 26, 74]. Along with the rapid development of language reasoning research for LLMs, considerable progress [84, 61, 58, 11, 50, 72, 51, 63, 73] has been made in improving multimodal reasoning capability of Multimodal Large Language Models (MLLMs). These methods, which often incorporate reinforcement learning techniques [11, 50, 61] to enhance the reasoning capabilities of MLLMs, have achieved some early successes [84, 61, 58, 11, 50, 51, 63]. However, they typically rely on existing multi-modal benchmarks that struggle to accurately capture models core visual reasoning ability. For example, VLM-R1 [63] assesses visual reasoning with referring expression comprehension tasks [88, 55, 38], yet these tasks primarily focus on object localization, demanding only basic perceptual skills rather than more advanced visual cognitive processes. Meanwhile, several works [58, 61, 84] adopt mathematical problem-solving benchmarks that include diagramssuch as MathVista [52], MathVerse [91], and MathVision [69]to evaluate visual reasoning. In practice, however, as [91] observes, many MLLMs translate these visual clues into textual descriptions and then rely on standard language reasoning. This approach can incorrectly 2 Figure 3: Comparison of questions from different Benchmarks. Compared to MathVista [52], MathVision [69], and MMMU [89], VisuLogic focuses more explicitly on assessing pure visual reasoning capabilities. attribute language-driven results to visual reasoning, resulting in misleading assessment of the models visual reasoning capabilities [91, 30]. Consequently, designing new benchmarks that explicitly focus on vision-centric reasoningrather than conflating it with text-based reasoningremains critical for advancing MLLMs visual reasoning capacities. To address this limitation, we propose VisuLogic, novel benchmark specifically designed to evaluate visual reasoning abilities in multimodal models without mixing them with purely textbased reasoning (see Figure 3). VisuLogic comprises carefully constructed tasks that span multiple reasoning categories (see Figure 1). As shown in Figure 5, these tasks are classified into six key types, such as Quantitative Reasoning, which requires understanding and deducing shifts in the quantity of certain elements within an image. In contrast to existing benchmarks, as demonstrated in Figure 2, state-of-the-art (SOTA) MLLMs often omit crucial visual details when describing VisuLogic problems, making it difficult for them to rely solely on text-based inference shortcut. Indeed, even humans would find it challenging to capture every essential visual cue in single description, so effectively tackling VisuLogic demands more robust, vision-centric reasoning. By reducing reliance on textual inference shortcuts, VisuLogic thus provides stringent evaluation of MLLMs genuine visual reasoning capabilities. We conducted comprehensive evaluation and systematic analysis to assess current models visual reasoning capabilities. When leading text-only LLMs were supplied with detailed descriptions in place of raw images, their accuracyDoubao-1.5-Pro (26.6%), Claude-3.7-Sonnet (25.9%) and Qwen2.5-72B-Instruct [83] (28.0%)barely exceeded the random-chance baseline of 24.9%. This clearly demonstrates that textual reasoning alone are insufficient for solving our VisuLogic tasks. Even state-of-the-art multimodal arge language models (MLLMs)including GPT-4o [33], Doubao1.5-Vision-Pro, Gemini-2.0-Pro-Exp [64] and InternVL3-78B [94]achieve only 26.3%, 28.1%, 28.0% and 27.7%, respectively, whereas human participants reached 51.4%. The substantial gap between these results and human performance underscores the challenge of robust visual reasoning in current MLLMs. Furthermore, we applied simple reinforcement-learning (RL) fine-tuning step on our supplementary training dataset: this boosted the baseline models accuracy from 25.5% to 31.1%, outperforming both open-source and closed-source counterparts. These findings illustrate the promise of the RL technique for strengthening MLLMs visual reasoning capabilities. In summary, our contributions are as follows: We propose challenging visual reasoning benchmark that is inherently difficult to articulate using language, providing more rigorous evaluation of the visual reasoning capabilities of MLLMs. We conduct comprehensive experiments to evaluate and analyze the benchmark, including extensive evaluations and comparative studies of various MLLMs under different setting. We identify the RL technique as promising direction for improving the visual reasoning capabilities of MLLMs. Furthermore, we release both the training code and data to facilitate future research."
        },
        {
            "title": "2 Related Work",
            "content": "Multi-modal Large Language Models. Recent years have witnessed substantial advancements in Multi-modal Large Language Models (MLLMs). Early works like BLIP [41, 40] and Flamingo [5] introduce lightweight parameters between vision transformer [21] (ViT) and LLMs, laying the groundwork for multimodal perception. Subsequent efforts, such as LLaVA [45] and MiniGPT-4 [93], integrate instruction tuning, further enhancing the performance of MLLMs. Proprietary models like GPT-4o [33] and Gemini-Pro [64] have advanced MLLM performance on complex multimodal tasks, while open-source models such as Qwen-VL series [7, 70, 8] and InternVL series [15, 16, 24, 14, 94] achieve competitive results through optimized architectural design, dataset expansion and training paradigm improvements. Meanwhile, some related studies further advance the ability of large models by incorporating new modalities (e.g., audio [22, 19, 77], point clouds [27, 9], video [92, 12]) and by supporting more tasks (e.g., grounding [80, 71], computer usage [60, 6]). Notably, limited research attempts to enhance the reasoning capabilities of MLLMs. Some pioneering works, such as R1Onevision [84], LMM-R1 [61], MM-EUREKA [58], R1-V [11], Visual-rft [50], Visualprm [72], OThink-MR1 [51], VLM-R1 [63], and Open-r1-Video [73] have explored the visual reasoning capabilities of MLLMs through Reinforcement Learning (RL), but they are still in the nascent stage. Multimodal Benchmarks. With the development of MLLMs, multimodal benchmarks have also evolved significantly [43]. Early benchmarks primarily address visual perception tasks through simple tasks like visual question answering (VQA) [13, 44, 36, 78], image captioning [59, 20, 37] and referring expression comprehension [88, 55]. Subsequent works expand the capability coverage of benchmarks into more specialized domains: OCRBench [49], Chartqa [56] and DocVQA [57] assess textual content extraction; AgentBench [48] and ToolEyes [86] test tool usage capabilities; and egocentric perception benchmarks [54, 17] quantify first-person scene interpretation. Despite the progress, they ignore the evaluation of visual reasoning abilities [90, 89]. Recently, some benchmarks have made explorations in examining MLLMs visual reasoning abilities, but methodological deficiencies still cause limitations to assess the intrinsic visual reasoning capabilities [30, 4, 76]. InfiMM-Eval [29] test reasoning abilities around daily life, lacking deep-level reasoning scenarios. MMMU [89] and Emma [30] provide benchmarks demanding advanced reasoning abilities in fields such as chemistry and physics, but they ignore questions around the images fundamental visual components (e.g., shapes, elements). While mathematical benchmarks [69, 52, 31, 62, 91, 28] evaluate mathematical reasoning with geometric and diagram problems included, they focus on math capabilities but disregard logical analysis about the vision information. LogicVista [76] provides multimodal logical reasoning benchmark, its visual questions lack analytical depthdominated by single-hop, superficial queries in limited data scope. Unlike previous works, we introduce challenging benchmark focused specifically on the domain of visual logical reasoning."
        },
        {
            "title": "3 VisuLogic",
            "content": "In this section, we first describe the VisuLogic data-curation pipeline, which comprises three key stages: data collection, quality control, and the detailed taxonomy. We then report the benchmarks construction statistics, including total size, answer-option distributions, and category-level proportions. Finally, we introduce supplementary training datasetconsisting of questions analogous to those in VisuLogicdesigned to bolster future research and facilitate community engagement. 3.1 Data Curation Pipeline Data Collection. We construct the VisuLogic dataset by sourcing all questions from publicly available online resources in compliance with relevant licenses and regulations. As shown in Figure 4, our automated data processing pipeline comprises three stages: 1) Fetching: We employ Playwright 1 to systematically scrape raw web content, supplemented by custom parsing scripts that extract questionanswer pairs. 2) Cleaning: We remove noise, irrelevant content, and extraneous HTML markup (e.g., <div>) to ensure the integrity of the textual data. 3) Structuring: We standardize the cleaned text and images by structuring all information (such as question text, metadata) in JSON Lines (JSONL) format. 1https://github.com/microsoft/playwright 4 Figure 4: Data curation pipeline of VisuLogic. The pipeline includes Data Collection, Quality Control and Data Taxonomy. Quality Control. To ensure the reliability of the benchmark dataset, we employ three-stage data validation procedure: 1) Image Verification: Each image referenced in the questions is checked for existence and correct formatting; any item that fails to meet the criteria is removed following human review. 2) Duplicate Removal: We eliminate redundant entries at both the text and image levels by (i) detecting lexical overlap among text segments and (ii) applying perceptual hashing (pHash) to identify visually similar images. 3) Manual Checking: After automated filtering, we perform thorough human-led review of every remaining entry to confirm its validity and ensure dataset reliability. Data Taxonomy. We categorize all collected data into taxonomy of six primary classes based on expert human annotation of the reasoning skills each question requires. Annotators first tag questions according to the targeted reasoning competency; these annotated tags are then analyzed and merged into five primary categories. subsequent human review ensures that every question is accurately classified, with any ambiguous instances consolidated under the Other category. Specifically, we define each category as follows. Quantitative Reasoning focuses on changes in the number or count of graphical elements (for example, points, lines and angles) and on arithmetic relationships among shapes. Spatial Reasoning requires mentally reconstructing three-dimensional shapes from two-dimensional figures, folding or unfolding surfaces, and integrating three-dimensional structures. Positional Reasoning examines transformations such as translation, rotation and reflection of objects while preserving their fundamental elements. Attribute Reasoning involves intrinsic properties of shapes, including symmetry (axial or central), curvature and measures of openness or closedness. Stylistic Reasoning entails alterations in stylistic features such as overlay, subtraction and assessments of shape similarity or difference. Other encompasses questions that fall outside the preceding categories, including those involving letters, alphanumeric symbols or other specialized characters. 3.2 Dataset Statistics Following data curation and validation, VisuLogic comprises 1,000 single-choice questions. Figure 1 (left) illustrates the category distribution: Quantitative Reasoning (35.3%), Spatial Reasoning (23.1%), Positional Reasoning (13.6%), Attribute Reasoning (8.2%), Stylistic Reasoning (9.0%), and Other (10.8%). Correct answer options are evenly balanced, with the proportions distributed as follows: (23.1%), ( 26.7%), (25.2%), and (25.0%). 3.3 Supplementary Training Dataset To facilitate further investigation of visual reasoning, we provide an auxiliary training set of 4,296 questionanswer pairs drawn from the same domains and subjected to identical validation procedures to prevent overlap with the benchmark. The training split mirrors the primary taxonomy, with category proportions of Quantitative Reasoning (30.7%), Spatial Reasoning (25.5%), Positional Reasoning (13.0%), Attribute Reasoning (8.8%), Stylistic Reasoning (9.9%), and Other (12.1%). 5 Figure 5: Question examples of different categories in our VisuLogic Benchmark. VisuLogic contains 6 categories of questions, which require models abilities in visual logic reasoning."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present comprehensive evaluation of the VisuLogic benchmark. We first describe the experimental setup in Section 4.1, followed by overall performance results in Section 4.2. We then analyze systematic errors in Section 4.3 and provide qualitative insights in Section 4.4. 4.1 Experiment Setup References Performance. To fully investigate models performance, we establish two reference points: 1) Human Performance: We invited 100 graduate students majoring in science and engineering to solve 10 randomly sampled VisuLogic questions each, allowing 25 minutes per question. The aggregate accuracy over all participants constitutes the human benchmark. 2) Random Selection: We simulate random guessing by sampling answers uniformly over 10 independent runs and report the average accuracy as the random baseline. Evaluated Models. We evaluate total of 28 models on VisuLogic, comprising 8 large language models (LLMs) and 20 multimodal large language models (MLLMs). For open-source LLMs, we test Deepseek-R1 [18], Qwen2.5-72B-Instruct [83] and Qwen-QwQ [67], and for close-source LLMs we evaluate GPT-4 [1], o3-mini, Gemini-2.0-Flash-Thinking [64], Claude3.7-Sonnet and Doubao-1.5-Pro-32k. Open-source MLLMs include Qwen2.5-VL-7B-Instruct [8], Qwen2.5-VL-72B-Instruct [8], QvQ-72B-Preview [66], InternVL2.5-38B [16], InternVL2.5-78B [16], InternVL3-38B [94], InternVL3-78B [94], LLaVA-v1.5-7B [46], LLaVA-OneVision-7B (SI) [39], ShareGPT4V [10], MiniCPM-o-2.6 [85], GLM-4v-9B [25], Ovis2-8B [53] and mPLUG-Owl3-7B [87], while close-source MLLMs consist of GPT-4o [33], GPT-4o-mini, Kimi-latest [65], Doubao-1.5Vision-Pro-32k, Gemini-2.0-Pro [64] and Claude-3.7-Sonnet. We further include two reinforcementlearning baselines built on Qwen2.5-VL-7B-Instruct [8] and InternVL2.5-38B [16], respectively, trained via our rule-based RL procedure on our supplementary training dataset. Fully supervised fine-tuning (SFT) experiments on the same datasets serve as controls to isolate the effect of RL optimization. All model hyperparameters, training regimes, and implementation details are provided in the Appendix. LLM Evaluation Protocol. For language-only models, we generate an auxiliary image description using GPT-4o and prepend it to the question. Specifically, each question is formatted as Following is detailed caption describing an image: [IMAGE DESCRIPTION]. Based on the provided description, select the best answer from the four options:. This combined prompt is fed directly into the target LLMs for inference. Figure 6: Solution examples generated by different models. Reference solution and outputs generated by GPT-4o [33], Qwen2.5VL-72B-Instruct [8], InternVL2.5-38B [16], and InternVL2.538B with RL. Additionally, the image description and solution from LLMs (o3-mini) are also illustrated. Prompts Setting. We apply three distinct prompting paradigms to investigate model reasoning capabilities: 1) Non-CoT prompt evaluation: Models receive concise instruction: Answer the question using single word or phrase, following this format: Answer: boxed{$LETTER}. 2) CoT prompt evaluation: We prompt models to articulate intermediate reasoning steps: Solve the complex visual logical reasoning problem through step-by-step reasoning. Think about the reasoning process first and answer the question following this format: Answer: boxed{$LETTER}. 3) Hint prompts evaluation: Leveraging GPT-4o, we generate question-specific hints derived from the reference solutions. As shown in Figure 7, solution-related hints are provided alongside the CoT prompt to guide reasoning without revealing the final answer directly. Notably, unless otherwise specified, CoT prompt evaluation is employed by default for assessing model performance. 4.2 Overall Results LLM Performance. Table 1 reports that all evaluated LLMs attain rather low accuracy on VisuLogic. The best-performing LLM, Qwen2.5-72B-Instruct, reaches only 28.0%, while GPT-4 and DeepseekR1 achieve 23.6% and 26.6%, respectively. These findings underscore that reasoning based solely on 7 Table 1: Cross-Modal performance with CoT prompts on VisuLogic. The table shows the evaluation scores of baseline references, LLMs, and MLLMs, which illustrates gap between humans and models capabilities. Top performers per category are bolded, with secondary leaders underlined. Overall Quantity Spatiality Position Attribute Style Other Models Human Random References 51.4 24.9 45.3 25. 52.7 25.4 71.1 22.7 Open Source LLM (MLLM DescriptionLLM) Deepseek-R1 [18] Qwen2.5-72B-Instruct [83] QwQ-32B [67] 26.6 28.0 22.8 27.7 30.2 24. 23.5 24.4 20.1 24.0 27.5 25.4 Close Source LLM (MLLM DescriptionLLM) GPT-4 (20240613) [1] o3-mini (20250131) Gemini-2.0-Flash-Thinking (20250121) [64] Claude-3.7-Sonnet (20250219) Doubao-1.5-Pro-32k (20250115) GPT-4o-mini (20240718) GPT-4o (20240806) [33] Kimi-latest [65] Doubao-1.5-Vision-Pro-32k (20250115) Gemini-2.0-Pro (20250205) [64] Claude-3.7-Sonnet (20250219) LLaVA-v1.5-7B [46] LLaVA-OneVision-7B (SI) [39] ShareGPT4V [10] MiniCPM-o-2.6 [85] GLM-4v-9B [25] Ovis2-8B [53] mPLUG-Owl3-7B-241101 [87] Qwen2.5-VL-7B-Instruct [8] Qwen2.5VL-72B-Instruct [8] QvQ-72B-Preview [66] InternVL2.5-38B [14] InternVL2.5-78B [14] InternVL3-38B [94] InternVL3-78B [94] Qwen2.5-VL-7B-Instruct-SFT Qwen2.5-VL-7B-Instruct-RL InternVL2.5-38B-SFT InternVL2.5-38B-RL 23.6 24.6 23.4 25.9 26.6 24.3 26.3 25.9 28.1 28.0 24.8 24.6 25.3 23.4 25.3 24.3 25.6 18.9 26.0 26.2 23.0 25.5 27.3 27.1 27.7 25.5 28.0 27.9 31.1 21.2 27.8 23.2 26.6 30. 22.5 18.8 26.0 22.5 22.5 Close Source MLLMs 27.2 28.6 24.9 28.1 29.7 22.7 23.4 24.7 29.4 23.8 24.2 27.3 Open Source MLLMs 26.1 22.4 24.9 25.6 22.4 26.1 21.5 27.6 25.2 24.2 24.4 26.6 28.7 27. 24.4 26.6 30.6 31.2 24.2 27.3 22.1 23.0 23.7 23.8 15.2 20.9 23.8 17.0 26.4 26.0 27.6 26.1 26.4 33.8 29.4 31.2 21.3 24.5 16.9 25.0 25.0 23.5 27.2 26.5 29.1 27.9 27.9 23.5 33.1 23.5 27.3 28.3 27.2 16.2 25.2 27.2 24.4 27.2 26.5 26.1 31. 27.2 29.4 20.6 26.5 50.0 23.4 27.8 26.5 19.0 25.6 21.7 17.1 28.0 25.6 18.3 26.8 28.0 25.1 30.5 28.0 17.1 23.2 19.5 21.9 26.0 28.0 20.7 23.2 25.6 21.0 23.2 26.8 21.4 26. 23.2 23.2 25.6 30.5 47.5 24.3 23.0 26.8 20.7 23.3 25.6 21.1 25.6 30.0 31.1 20.0 16.7 32.1 22.2 22.2 31.1 25.6 28.9 24.5 24.1 25.6 18.9 37.8 25.6 24.4 25.6 31.1 23.9 21. 25.6 18.9 30.0 30.0 44.2 26.1 35.0 30.8 24.0 35.2 28.4 33.3 30.6 24.1 16.7 25.9 26.9 35.0 33.3 22.2 22.2 22.2 19.4 29.9 25.3 24.1 20.4 25.0 34.3 30.6 26.9 30.6 28.5 32. 26.9 29.6 25.0 38.9 textual descriptions is insufficient to capture the rich visual information required by our benchmark, causing failures to resolve visual logical reasoning problems. MLLM Performance. As shown in Table 1, current multimodal LLMs also perform poorly on VisuLogic. The highest score is 28.1% by Doubao-1.5-Vision-Pro-32k, which remains substantial 23.3 points below human performance. Advanced models such as GPT-4o and Gemini-2.0-Pro attain only 26.3% and 28.0%, respectively, revealing marked gap between existing MLLMs and human-level visual reasoning. Overall, these results indicate that current MLLMs have serious deficiencies in visual reasoning and that significant advances are still required. Effectiveness of CoT Prompts. Contrary to expectations, chain-of-thought (CoT) prompting yields minimal improvements in visual reasoning. As detailed in Table 2, GPT-4o-mini benefits most, with only 1.2-point gain under CoT compared to direct-answer prompts; all other models exhibit gains below 1.0 point. We speculate that this limited effect likely stems from current CoT training being 8 Table 2: Influence of Chain-of-Thought on model performance. Positive value changes are highlighted in red, negative changes in green, and statistically insignificant variations (delta < 1%) are denoted in gray. With CoT prompts, MLLMs only exhibit tiny improvements in visual reasoning. Spatiality Attribute Quantity Position Overall Models Other Style CoT GPT-4o (20240806) Kimi-latest GPT-4o-mini (20240718) Qwen2.5-VL-Instruct-7B InternVL2.5-38B (cid:33) (cid:35) (cid:33) (cid:35) (cid:33) (cid:35) (cid:33) (cid:35) (cid:33) (cid:35) 26.3 26.0(0.3) 28.6 26.9(1.7) 24.7 24.2(0.5) 27.2 26.5(0.7) 26.8 23.2(3.6) 20.0 24.0(+4.0) 25.9 29.6(+3.7) 25.9 25.1(0.8) 24.9 22.9(2.0) 29.4 22.5(6.9) 26.5 25.0(1.5) 28.0 19.5(7.5) 16.7 35.6(+18.9) 26.9 24.1(2.8) 24.3 23.1(1.2) 27.2 23.8(3.4) 23.4 22.9(0.5) 23.5 24.3(+0.8) 18.3 17.1(1.2) 31.1 30.0(1.1) 16.7 18.5(+1.8) 26.0 25.9(0.1) 27.6 25.5(2.1) 20.9 22.8(+1.9) 25.2 26.4(+1.2) 23.2 25.3(+2.1) 37.8 20.6(17.2) 25.0 38.2(+13.2) 24.9 25.0(+0.1) 24.1 24.6(+0.5) 26.4 25.5(0.9) 27.2 22.1(5.1) 23.2 22.0(1.2) 25.6 26.7(+1.1) 22.2 29.6(+7.4) Table 3: Influence of hint prompts on model performance. MLLMs exhibit measurable performance enhancements with hint integration, yet retain significant gaps against human performance. In comparison, humans achieve task mastery on VisuLogic with hints. Value changes are color-coded with red indicating positive shifts and green denoting negative variations. Models Human GPT-4o (20240806) Claude-3.7-Sonnet (20250219) Gemini-2.0-Pro (20250205) Doubao-1.5-Vision-Pro-32k (20250115) Hint Overall Quantity Spatiality Position Attribute Style Other (cid:35) (cid:33) (cid:35) (cid:33) (cid:35) (cid:33) (cid:35) (cid:33) (cid:35) (cid:33) 51.4 83.6(+32.2) 45.3 85.1(+39.8) 52.7 68.5(+15.8) 71.1 100.0(+28.9) 50.0 95.7(+45.7) 47.5 78.6(+31.1) 44.2 90.5(+46.3) 26.3 30.0(+3.7) 24.8 33.5(+8.7) 28.0 36.5(+8.5) 28.1 37.0(+8.9) 28.6 25.4(3.2) 24.7 31.5(+6.8) 22.7 37.3(+14.6) 27.3 33.3(+6.0) 29.7 44.8(+15.1) 24.2 33.3(+9.1) 27.2 29.2(+2.0) 27.9 37.5(+9.6) 27.9 25.0(2.9) 28.1 46.3(+18.2) 23.8 25.9(+2.1) 29.1 54.2(+25.1) 26.8 28.6(+1.8) 28.0 23.8(4.2) 30.5 38.1(+7.6) 25.1 33.3(+8.2) 20.0 30.8(+10.8) 25.9 42.9(+17.0) 22.2 15.4(6.8) 22.2 15.4(6.8) 32.1 23.1(9.0) 22.2 38.1(+15.9) 33.3 42.9(+9.6) 35.0 28.6(6.4) based only on pure-text corpora; future works should explore CoT techniques tailored to multimodal data to better support visual reasoning tasks. Effectiveness of Hint Prompts. Table 3 shows that hint prompts can boost model performanceClaude-3.7-Sonnet, Gemini-2.0-Pro, and Doubao-1.5-Vision-Pro-32k all improve by over 8 points, reaching accuracies above 35%. However, even with explicit guidance, models still fail to construct coherent, reliable reasoning chains. This suggests that simply augmenting training data with similar tasks is insufficient (which can help MLLMs come up with specific directions for solving the problem); future efforts must focus on enhancing the reliability and correctness of reasoning procedures of MLLMs to achieve more accurate reasoning inference. Impact of Model Scaling. In Table 1, we observe positive correlation between parameter size and model performance. With in the same model series, Qwen2.5-VL-72B-Instruct achieves 26.2 % outperforming Qwen2.5VL-7B-Instruct (26.0%) by 0.2%. Furthermore, InternVL2.5-78B (27.3%) surpasses InternVL2.5-38B (25.5%) by margin of 1.8%. Open-Source vs Close-Source. Table 1 further compares openand closed-source models. The top open-source MLLM, InternVL3-78B, attains 27.7%, trailing the closed-source leader (Doubao-1.5Vision-Pro-32k, 28.1%) by only 0.4% points and outperforming other proprietary competitors such as GPT-4o and Claude-3.7-Sonnet. Overall, both openand closed-source models exhibit uniformly low performance, highlighting widespread neglect of visual reasoning objectives in current multimodal model training and data collection. Behaviors of RL Trained models. As shown in Table 1, MLLMs with reinforcement learning optimization can yield obvious improvements in visual reasoning performance. Qwen2.5-VL-7BInstruct-RL attains 28.0%, 2.0 percentage point boost over its non-RL counterpart. More strikingly, InternVL2.5-38B-RL reaches 31.1%, surpassing the original non-RL model by 5.6% and establishing new state-of-the-art on VisuLogic. Furthermore, compared to supervised fine-tuning (SFT) on idenFigure 7: Hint prompts visualization. Hint prompts examples, which supply solution guidance for MLLMs, are shown in the image, with solution-critical elements highlighted in red. tical datasets, RL-enhanced models demonstrate substantially larger performance gains, underscoring the promise of targeted RL methods for advancing multimodal visual reasoning. 4.3 Fine-grained Comparison We systematically analyze model capabilities by examining error distributions across reasoning categories for different models. Figure 8 presents the error rates of LLMs, MLLMs, and human participants over six distinct reasoning categories. Figure 8a reveals that LLMs struggle most with Spatial Reasoning questions, indicating that textonly descriptions are insufficient to infer three-dimensional structures or spatial transformations. In contrast, their performance on Quantitative Reasoning tasks is comparatively stronger, suggesting that quantitative relationships are more readily conveyed through language. As shown in Figure 8b, Stylistic Reasoning presents the greatest difficulty for MLLMs, with error rates exceeding 75%worse than random guessing (25% accuracy). This result underscores fundamental limitation of current MLLM architectures in capturing subtle visual cues such as overlays, contours, and shape variations. Figure 8c reveals that human error patterns form distinct cluster, separate from both LLMs and MLLMs. Human participants maintain error rates below 30% on Positional Reasoning tasks, reflecting robust position-based visual inference. By contrast, both model classes struggle with positional reasoning, highlighting fundamental divergence in visualcognitive processes between humans and MLLMs. 4.4 Qualitative Analysis LLM Failures. As shown in Figure 6(h), text-only LLMs that rely on externally generated image captions often omit critical visual details required for multi-step logical deductionsuch as the counts, shapes, and progression patterns of the black and white dots in Figure 6(a). Consequently, their reasoning diverges from the correct solution and frequently yields hallucinations or irrelevant responses. MLLM Failures. Figure 6 also presents cases in which MLLMs correctly describe static visual content yet fail to infer the evolving relationships among shapes, instead resorting to superficial cues like object counts. While these models can recognize individual shapes and tally items, they struggle to reason over inter-element relations, which limits their ability to solve complex visual-logic problems. RL-Based Improvements. As illustrated in Figure 6(g), reinforcement learning (RL) encourages deeper, stepwise logical reasoning. The RL-enhanced model successfully captures state transitions (e.g., the movements of chess pieces in Figure 6(a)) and accurately predicts subsequent configurations. Moreover, it learns to iteratively revise intermediate hypothesesakin to trial-and-erroruntil (a) LLMs error distribution. (b) MLLMs error distribution. (c) Humans error distribution. Figure 8: Error distribution analysis. The figure demonstrates distinct error type allocations across Humans, LLMs and MLLMs, revealing differences among their cognition patterns. coherent deduction emerges (see additional examples in the Appendix). These findings highlight the potential of RL methods to bolster performance on visual reasoning tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present VisuLogic, novel benchmark designed to evaluate the visual reasoning capabilities of Multi-modal Large Language Models (MLLMs). The benchmark consists of 1,000 vision-centric reasoning tasks distributed across six distinct categories. We conduct comprehensive evaluation of several advanced LLMs and MLLMs on this benchmark and provide an in-depth analysis of their performance. Our findings reveal that even the most advanced models fall short of human performance, highlighting substantial opportunities for advancement in visual logical reasoning. Through further experiments, we find that reinforcement learning (RL) is promising approach for enhancing the vision reasoning capabilities of MLLMs. To promote further research and innovation, we open-source the evaluation code, training scripts, and datasets associated with this work."
        },
        {
            "title": "References",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] W. U. Ahmad, S. Narenthiran, S. Majumdar, A. Ficek, S. Jain, J. Huang, V. Noroozi, and B. Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025. [3] A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, O. Pietquin, A. Üstün, and S. Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. [4] S. N. Akter, S. Lee, Y. Chang, Y. Bisk, and E. Nyberg. Visreas: Complex visual reasoning with unanswerable questions, 2024. [5] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [6] H. Bai, Y. Zhou, L. E. Li, S. Levine, and A. Kumar. Digi-q: Learning q-value functions for training device-control agents. arXiv preprint arXiv:2502.15760, 2025. [7] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [8] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [9] G. Chen, M. Wang, Y. Yang, K. Yu, L. Yuan, and Y. Yue. Pointgpt: Auto-regressively generative pre-training from point clouds. Advances in Neural Information Processing Systems, 36:29667 29679, 2023. [10] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [11] L. Chen, L. Li, H. Zhao, Y. Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [12] S. Chen, H. Li, Q. Wang, Z. Zhao, M. Sun, X. Zhu, and J. Liu. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. Advances in Neural Information Processing Systems, 36:7284272866, 2023. [13] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server, 2015. [14] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [15] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [16] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [17] S. Cheng, Z. Guo, J. Wu, K. Fang, P. Li, H. Liu, and Y. Liu. Egothink: Evaluating first-person perspective thinking capability of vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429114302, 2024. [18] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 12 [19] A. Défossez, L. Mazaré, M. Orsini, A. Royer, P. Pérez, H. Jégou, E. Grave, and N. Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. [20] H. Dong, J. Li, B. Wu, J. Wang, Y. Zhang, and H. Guo. Benchmarking and improving detail image caption. arXiv preprint arXiv:2405.19092, 2024. [21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [22] Q. Fang, S. Guo, Y. Zhou, Z. Ma, S. Zhang, and Y. Feng. Llama-omni: Seamless speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. [23] J. Feng, R. Xu, J. Hao, H. Sharma, Y. Shen, D. Zhao, and W. Chen. Language models can be logical solvers. arXiv preprint arXiv:2311.06158, 2023. [24] Z. Gao, Z. Chen, E. Cui, Y. Ren, W. Wang, J. Zhu, H. Tian, S. Ye, J. He, X. Zhu, et al. Mini-internvl: flexible-transfer pocket multi-modal model with 5% parameters and 90% performance. Visual Intelligence, 2(1):117, 2024. [25] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. [26] B. Goertzel and C. Pennachin. Artificial general intelligence, volume 2. Springer, 2007. [27] Z. Guo, R. Zhang, X. Zhu, Y. Tang, X. Ma, J. Han, K. Chen, P. Gao, X. Li, H. Li, et al. Pointbind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615, 2023. [28] H. Gupta, S. Verma, U. Anantheswaran, K. Scaria, M. Parmar, S. Mishra, and C. Baral. Polymath: challenging multi-modal mathematical reasoning benchmark, 2024. [29] X. Han, Q. You, Y. Liu, W. Chen, H. Zheng, K. Mrini, X. Lin, Y. Wang, B. Zhai, J. Yuan, et al. Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models. arXiv preprint arXiv:2311.11567, 2023. [30] Y. Hao, J. Gu, H. W. Wang, L. Li, Z. Yang, L. Wang, and Y. Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. [31] C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [32] D. Huang, Q. Bu, Y. Qing, and H. Cui. Codecot: Tackling code syntax errors in cot reasoning for code generation. arXiv preprint arXiv:2308.08784, 2023. [33] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [34] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [35] X. Jiang, Y. Dong, L. Wang, Z. Fang, Q. Shang, G. Li, Z. Jin, and W. Jiao. Self-planning code generation with large language models. ACM Transactions on Software Engineering and Methodology, 33(7):130, 2024. [36] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman. The kinetics human action video dataset, 2017. [37] L. Ke, W. Pei, R. Li, X. Shen, and Y.-W. Tai. Reflective decoding network for image captioning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 88888897, 2019. 13 [38] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, and J. Jia. Lisa: Reasoning segmentation via large language model, 2024. [39] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu, and C. Li. Llava-onevision: Easy visual task transfer, 2024. [40] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [41] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. [42] J. Li, G. Li, Y. Li, and Z. Jin. Structured chain-of-thought prompting for code generation. ACM Transactions on Software Engineering and Methodology, 34(2):123, 2025. [43] J. Li, W. Lu, H. Fei, M. Luo, M. Dai, M. Xia, Y. Jin, Z. Gan, D. Qi, C. Fu, Y. Tai, W. Yang, Y. Wang, and C. Wang. survey on benchmarks of multimodal large language models, 2024. [44] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollár. Microsoft coco: Common objects in context, 2015. [45] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [46] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning, 2023. [47] H. Liu, Z. Teng, L. Cui, C. Zhang, Q. Zhou, and Y. Zhang. Logicot: Logical chain-of-thought instruction-tuning. arXiv preprint arXiv:2305.12147, 2023. [48] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. [49] Y. Liu, Z. Li, M. Huang, B. Yang, W. Yu, C. Li, X.-C. Yin, C.-L. Liu, L. Jin, and X. Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [50] Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [51] Z. Liu, Y. Zhang, F. Liu, C. Zhang, Y. Sun, and J. Wang. Othink-mr1: Stimulating multimodal generalized reasoning capabilities through dynamic reinforcement learning. arXiv preprint arXiv:2503.16081, 2025. [52] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [53] S. Lu, Y. Li, Q.-G. Chen, Z. Xu, W. Luo, K. Zhang, and H.-J. Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. [54] K. Mangalam, R. Akshulakov, and J. Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. [55] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. [56] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [57] M. Mathew, D. Karatzas, and C. Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [58] F. Meng, L. Du, Z. Liu, Z. Zhou, Q. Lu, D. Fu, B. Shi, W. Wang, J. He, K. Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. 14 [59] T. Nguyen, S. Y. Gadre, G. Ilharco, S. Oh, and L. Schmidt. Improving multimodal datasets with image captioning. Advances in Neural Information Processing Systems, 36:2204722069, 2023. [60] R. Niu, J. Li, S. Wang, Y. Fu, X. Hu, X. Leng, H. Kong, Y. Chang, and Q. Wang. Screenagent: vision language model-driven computer control agent. arXiv preprint arXiv:2402.07945, 2024. [61] Y. Peng, G. Zhang, M. Zhang, Z. You, J. Liu, Q. Zhu, K. Yang, X. Xu, X. Geng, and X. Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [62] R. Qiao, Q. Tan, G. Dong, M. Wu, C. Sun, X. Song, Z. GongQue, S. Lei, Z. Wei, M. Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [63] H. Shen, Z. Zhang, K. Zhao, Q. Zhang, R. Xu, and T. Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. https://github.com/om-ai-lab/VLM-R1, 2025. Accessed: 2025-02-15. [64] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [65] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [66] Q. Team. Qvq: To see the world with wisdom, December 2024. [67] Q. Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [68] Y. Wan, W. Wang, Y. Yang, Y. Yuan, J.-t. Huang, P. He, W. Jiao, and M. R. Lyu. Logicasker: Evaluating and improving the logical reasoning ability of large language models. arXiv preprint arXiv:2401.00757, 2024. [69] K. Wang, J. Pan, W. Shi, Z. Lu, H. Ren, A. Zhou, M. Zhan, and H. Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [70] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [71] S. Wang, D. Kim, A. Taalimi, C. Sun, and W. Kuo. Learning visual grounding from generative vision and language model. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 80578067. IEEE, 2025. [72] W. Wang, Z. Gao, L. Chen, Z. Chen, J. Zhu, X. Zhao, Y. Liu, Y. Cao, S. Ye, X. Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. [73] X. Wang and P. Peng. Open-r1-video. https://github.com/Wang-Xiaodong1899/ Open-R1-Video, 2025. [74] Y. Wang, W. Chen, X. Han, X. Lin, H. Zhao, Y. Liu, B. Zhai, J. Yuan, Q. You, and H. Yang. Exploring the reasoning abilities of multimodal large language models (mllms): comprehensive survey on emerging trends in multimodal reasoning, 2024. [75] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [76] Y. Xiao, E. Sun, T. Liu, and W. Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. [77] Z. Xie and C. Wu. Mini-omni: Language models can hear, talk while thinking in streaming, 2024. URL https://arxiv. org/abs/2408.16725, 2024. [78] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. 15 [79] F. Xu, Z. Wu, Q. Sun, S. Ren, F. Yuan, S. Yuan, Q. Lin, Y. Qiao, and J. Liu. Symbol-llm: Towards foundational symbol-centric interface for large language models. arXiv preprint arXiv:2311.09278, 2023. [80] R. Xu, Z. Huang, T. Wang, Y. Chen, J. Pang, and D. Lin. Vlm-grounder: vlm agent for zero-shot 3d visual grounding. arXiv preprint arXiv:2410.13860, 2024. [81] Y. Xu, X. Liu, X. Liu, Z. Hou, Y. Li, X. Zhang, Z. Wang, A. Zeng, Z. Du, W. Zhao, et al. Chatglm-math: Improving math problem-solving in large language models with self-critique pipeline. arXiv preprint arXiv:2404.02893, 2024. [82] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [83] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [84] Y. Yang, X. He, H. Pan, X. Jiang, Y. Deng, X. Yang, H. Lu, D. Yin, F. Rao, M. Zhu, B. Zhang, and W. Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [85] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [86] J. Ye, G. Li, S. Gao, C. Huang, Y. Wu, S. Li, X. Fan, S. Dou, Q. Zhang, T. Gui, et al. Tooleyes: fine-grained evaluation for tool learning capabilities of large language models in real-world scenarios. arXiv preprint arXiv:2401.00741, 2024. [87] J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models, 2024. [88] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context in referring expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. [89] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. [90] C. Zhang, F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu. Raven: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [91] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou, P. Lu, K.-W. Chang, Y. Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [92] Q. Zhao, S. Wang, C. Zhang, C. Fu, M. Q. Do, N. Agarwal, K. Lee, and C. Sun. Antgpt: Can large language models help long-term action anticipation from videos? arXiv preprint arXiv:2307.16368, 2023. [93] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [94] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, Y. Duan, H. Tian, W. Su, J. Shao, Z. Gao, E. Cui, Y. Cao, Y. Liu, X. Wei, H. Zhang, H. Wang, W. Xu, H. Li, J. Wang, D. Chen, S. Li, Y. He, T. Jiang, J. Luo, Y. Wang, C. He, B. Shi, X. Zhang, W. Shao, J. He, Y. Xiong, W. Qu, P. Sun, P. Jiao, H. Lv, L. Wu, K. Zhang, H. Deng, J. Ge, K. Chen, L. Wang, M. Dou, L. Lu, X. Zhu, T. Lu, D. Lin, Y. Qiao, J. Dai, and W. Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025."
        },
        {
            "title": "A Overview of the Appendix",
            "content": "In the appendix, we provide additional details and supplementary information to further elaborate on sections mentioned above. In Section B, we analyze the statistical features of the dataset, meanwhile providing examples of questions ranging from different categories. Section contains experiments details, including the evaluation of LLMs, the evaluation of hint prompts and RL experiments. Some examples of model outputs are also illustrated."
        },
        {
            "title": "B Benchmark Analysis",
            "content": "B.1 Statistical analysis As shown in Figure 10, the text length of questions in VisuLogic is mostly concentrated around 40 tokens (calculated by Llama-3.1s and InternVL2.5s tokenizer). We also analyze the distribution of image sizes, as shown in Figure 9. The image widths range from 200 to 700 pixels, with an average of 592.3 pixels, while the heights range from 90 to 825 pixels, with an average of 327.9 pixels. Figure 9: Image size distribution. The size of images is limited to within the same order of magnitude. B.2 More Examples of VisuLogic To provide thoroughly presentation of our benchmark, we include more examples of questions from different categories in the Figure 11 and Figure 12. Evaluation & Experiment C.1 Evaluation of LLMs Caption generation for LLMs Evaluation. In our experiment, we employ large language models (LLMs) for comparative analysis. Specifically, when setting up the LLM-based experiment, we initially utilize GPT-4o to generate captions for images with the following prompt: Please describe the fine-grained content of the image or figure based on this question, including scenes, objects, relationships, and any text present. Please note that you do not need to answer this question directly, just describe the information of this picture. Additional examples of generated image captions are presented in Figure 14 and Figure 15. 17 Figure 10: Distribution of text token length in VisuLogic. More Examples of Captions. We provide additional image captions for six categories, as illustrated in Figures 14 and 15. Even SOTA MLLM (GPT-4o) encounters difficulties in accurately describing the details of images from VisuLogic. C.2 More Solutions from Models We provide more solutions generated from different LLMs/MLLMs on our benchmark, as shown in Figure 16, Figure 17 and Figure 18. For the majority of questions, almost all models fail to provide accurate solutions. Sometimes even when the final answer is correct, methodological wrong may persist. C.3 Hint Prompts Evaluation Details We first generate hint prompts with GPT-4o, combining reference solutions with question data as inputs. All outputs undergo manual validation to prevent solution leakage. More examples are shown in Figure 19. After that, we input the hint prompts along with the same CoT prompt in CoT experiments (Solve the complex visual logical reasoning problem through step-by-step reasoning. Think about the reasoning process first and answer the question following this format: Answer: boxed{$LETTER}.) to MLLMs. C.4 RL Experiments Comparative SFT Experiments. To verify the effectiveness of RL method, we arrange the comparative SFT experiments on the same dataset as RL experiments. The instruction consists of questions and Non-CoT prompts, and the responses are formatted direct answers. RL Algorithm. We employ REINFORCE Leave-One-Out (RLOO) [3] in our reinforcement learning training phase. As critic-model-free algorithm,rloo is at low computational cost while maintaining more robustness to noise and KL constraints. 18 Figure 11: More examples in VisuLogic of Quantitative Reasoning, Spatial Reasoning, Positional Reasoning. 19 Figure 12: More examples in VisuLogic of Attribute Reasoning, Stylistic Reasoning, and Other. 20 Figure 13: Distribution of tokens length in LLM evaluation settings, including image description. Reward Modeling. Inspired by Deepseek-R1 [18], we design our rule-based reward system that mainly consists of two types of rewards: 1. Format rewards: To clarify models outputs, we design format rule that forces model to put its thinking process between <think> and </think> tags and put its final answer between <answer> and </answer> tags. Regular expression is applied to judge whether outputs conform to the format rule. 2. Accuracy rewards: The accuracy reward is decided by the responses correctness. The model should generate the response in right format, then the answer is extracted and judged whether it is matched to the correct option. Hyperparameter settings. Our two RL models are trained with the hyperparameter configuration detailed in Table 5. And the hyperparameters used in SFT training stage are listed in Table 4. Table 4: Hyperparameter Settings for SFT Training Stage. Qwen2.5-VL-7B-Instruct-SFT InternVL2.5-38B-SFT pretrain model Qwen2.5-VL-7B-Instruct InternVL2.5-38B learning rate batch size optimizer lr scheduler 0.5e-5 64 AdamW cosine 2e-5 128 AdamW cosine image strategy image_max_pixels= max_dynamic_patch=6 warmup ratio max epochs bf16 0.1 True 21 0.03 1 True Table 5: Hyperparameter Settings for RL Training Stage. Qwen2.5-VL-7B-Instruct-RL InternVL2.5-38B-RL pretrain model Qwen2.5-VL-7B-Instruct InternVL2.5-38B"
        },
        {
            "title": "RL Algorithm",
            "content": "train batch size rollout batch_size temperature samples per prompt prompt max len generate max len bf16 actor learning rate init kl coef rloo 128 1 16"
        },
        {
            "title": "True",
            "content": "1e-6 0 rloo 64 128 8"
        },
        {
            "title": "True",
            "content": "1e-6 0 Other Details. The training environment consists of CentOS Linux release 7.6.1810 operating system with CUDA 12.1. For Qwen2.5-VL-7B-Instruct-RL, we train for 80 steps on 18 A800 GPUs and for InternVL2.5-38B-RL we train for 100 steps on 68 A800 GPUs. C.5 RL models Evaluation Details As mentioned above, we apply format rewards in RL experiments. Thus, to fully investigate the models latent reasoning abilities, we utilize implement training-aligned prompts during evaluation in VisuLogic, which is shown as follows: Solve the complex visual logical reasoning problem through step-by-step reasoning. Think about the reasoning process first and answer the question following this format: <think> THINKING </think><answer> ANSWER </answer>. C.6 Effectiveness of RL Experiments Figures 20, 21, 22, 23, 24 and 25 demonstrate qualitative differences in model outputs between baseline and RL optimized models. It illustrates reinforcement learning (RL) training enables the model to perform fundamental-level analysis of reasoning tasks embedded in graphical representations. 22 Figure 14: Part of image caption in LLM evaluation. 23 Figure 15: Part of image caption in LLM evaluation. Figure 16: Solution examples generated by different models. Reference solution and outputs generated by GPT-4o, Qwen2.5VL-72B-Instruct, Gemini-2.0-pro-exp-02-05, Doubao-1.5-VisionPro-32K and Claude-3.7-sonnet-thinking. Additionally, the image caption and solution from LLMs (Qwen2.5-72B-Instruct) are also illustrated. 25 Figure 17: Solution examples generated by different models. Reference solution and outputs generated by GPT-4o, Kimi-latest, Gemini-2.0-pro-exp-02-05 and Doubao-1.5-Vision-Pro-32K. Additionally, the image caption and solution from LLMs (Qwen2.5-72B-Instruct) are also illustrated. 26 Figure 18: Solution examples generated by different models. Reference solution and outputs generated by GPT-4o, Qwen2.5VL-72B, Gemini-2.0-pro-exp-02-05 and Doubao-1.5-Vision-Pro-32k. Additionally, the image caption and solution from LLMs (o3-mini) are also illustrated. Figure 19: Examples of hint prompts. Hint prompts are provided to guide reasoning without revealing the final answer directly. 28 Figure 20: Comparison of model outputs before and after RL training stage for Qwen2.5-VL-7B. 29 Figure 21: Comparison of model outputs before and after RL training stage for Qwen2.5-VL-7B. Figure 22: Comparison of model outputs before and after RL training stage for Qwen2.5-VL-7B. 31 Figure 23: Comparison of model outputs before and after RL training stage for InternVL-2.5-38B. 32 Figure 24: Comparison of model outputs before and after RL training stage for InternVL-2.5-38B. Figure 25: Comparison of model outputs before and after RL training stage for InternVL-2.5-38B."
        }
    ],
    "affiliations": [
        "SenseTime Research",
        "Shanghai Artificial Intelligence Laboratory",
        "Tsinghua University",
        "University of Science and Technology of China",
        "Xian Jiaotong University"
    ]
}