{
    "paper_title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation",
    "authors": [
        "Shaojin Wu",
        "Mengqi Huang",
        "Wenxu Wu",
        "Yufeng Cheng",
        "Fei Ding",
        "Qian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation."
        },
        {
            "title": "Start",
            "content": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation Shaojin Wu Mengqi Huang Wenxu Wu Yufeng Cheng Fei Ding Qian He Intelligent Creation Team, ByteDance {wushaojin, huangmengqi.98, wuwenxu.01, chengyufeng.cb1, dingfei.212, heqian}@bytedance.com Project Page: https://bytedance.github.io/UNO 5 2 0 2 2 ] . [ 1 0 6 1 2 0 . 4 0 5 2 : r Figure 1. Our UNO evolves as an universal customization from single to multi-subject."
        },
        {
            "title": "Abstract",
            "content": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal roIt is multi-image conditioned tary position embedding. subject-to-image model iteratively trained from text-toimage model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation. Code and model: https://github.com/bytedance/UNO. 1. Introduction As the material medium for abstract linguistic semantics and spatial embodiments of concrete visual subjects, images constitute the foundational modality in intelligent content generation. In recent years, customized image generation, which aims to create images that align with both the text semantics and the subjects in the reference images, has garnered significant interest across academic and industrial communities. This task unifies the flexibility of text control and the accuracy of visual controls, providing foundational infrastructure for diverse real-world applications ranging from film production to industrial design. As the field advances, the research challenge in customized image generation now centers on developing stable and scalable paradigm for unlocking more controllability, i.e., continuously increasing the amount of visual subject control without compromising the original text controllability. Data, while serving as the foundation for training generative models, has long been the bottleneck in customized generation. An ideal model should be capable of generating visual subjects in diverse poses, locations, sizes, and other attributes based on text prompts. This necessitates data that encompasses multi-perspective subject variations, requirement hindered by the impracticality of acquiring such comprehensive real paired datasets. Corresponding author Project lead Figure 2. The illustration of our motivation. We propose novel model-data co-evolution paradigm, where less-controllable preceding models systematically synthesize better customization data for successive more-controllable variants, enabling persistent coevolution between enhanced model and enriched data. Existing customized generation methods can be categorized into two streams based on how they utilize the data to design the model, i.e., few-data fine-tuning and large-data training stream. Early few-data fine-tuning approaches [8, 33] primarily employ per-subject optimization through model fine-tuning [16, 33] or textual inversion [8], incurring substantial computational overhead and timeconsuming during inference, which hinders real-world deployment. Therefore, more recent researches focus on the latter stream, which train adapters or image encoders on large set of visual subjects to achieve real-time customization. Their corresponding data used for training are either real images with diversity limitations (eg, restricted subject variations [45, 47]), or synthetic data with limited image quality (typically, 512 512) and narrow domain coverage. Therefore, these methods often exhibit trade-off between subject similarity and text controllability [45], or unstable generation [14]. Essentially, the existing customized models are designed based on their corresponding available customized data, resulting in limited scalability due to their data bottleneck. Diverging from the existing data-driven model design, research on large language models (LLMs) demonstrates their capacity for strategic synthetic data generation toward model self-enhancement. Their bidirectional data-model knowledge transfer manifests through either high-performance models providing supervisory signals for weaker counterparts[1, 9], or conversely, less capable models could provide supervision to elicit higher capabiliInspired by ties leading to stronger variant[3, 23, 35]. 2 this this LLMs synthetic data-driven self-improvement, study proposes that achieving stable and scalable customized generation necessitates an analogous model-data co-evolution paradigm, where less-controllable preceding customized models systematically synthesize better customization data for successive more-controllable variants, enabling persistent co-evolution between enhanced customized models and enriched customization data, as illustrated in Figure. 2. Technically, to achieve the model-data co-evolution, this study addresses two fundamental challenges, i.e., (1) how to establish systematic synthetic data curation framework that reliably harnesses knowledge distillation from lesscontrollable models; and (2) how to develop generalized customization model framework capable of hierarchical controllability adaptation, ensuring seamless scalability across varying degrees of controllability. To be specific, as for the synthetic data curation framework, we introduce progressive synthesis pipeline that transitions from singlesubject to multi-subject in-context generation, combined with multi-stage filtration mechanism to curate unprecedented high-resolution, high-quality paired customization data through fine-grained ensembled filtering. As for the customization model framework, we develop UNO to fully unlock the multi-condition contextual capabilities of Diffusion Transformers (DiT) through iterative simple-to-hard training, preserving the base architectures scalability with minimal modifications. Moreover, we propose universal Rotary Position Embedding (UnoPE) to effectively equip UNO with the capability of mitigating the attribute confusion issue when scaling visual subject controls. Our contributions are summarized as: Conceptual Contribution. We identify that current data-driven approaches in customized model design inherently suffer from scalability constraints rooted in fundamental data bottlenecks. To address this limitation, we pioneer model-data co-evolution paradigm that achieves enhanced controllability while enabling stable and scalable customized generation. Technical Contribution. (1) We develop systematic framework for synthetic data curation that produces high-fidelity, high-resolution paired customization datasets through progressive in-context synthesis and multi-stage filtering. (2) We propose UNO, universal customization architecture that enables seamless scalability across multicondition control through minimal yet effective modification of DiT. Experimental Contribution.We conduct extensive experiments on DreamBench [33] and multi-subject driven generation benchmarks. Our UNO achieve the highest DINO and CLIP-I scores among these two tasks. This demonstrates its strong subject similarity and text controllability, showcasing its capability to deliver state-of-the-art (SOTA) results. 2. Related Work 2.1. Text-to-image generation Recent years have witnessed explosive growth in text-toimage (T2I) models [6, 7, 17, 25, 28, 30, 31, 44, 46]. Apart from some work that chooses the GAN or autoregressive paradigm, most of current text-to-image work chose the denoising diffuison [11, 37] as their image generation framework. Early exploratory work [21, 29, 34] have validated the feasibility of using diffusion models for text-toimage generation and demonstrated their superior performance compared to other methods. The efficiency, quality, and capacity for T2I diffusion models are keeping improved in the following work. LDM [31] suggests training the diffusion model in latent space significantly improves the efficiency and output resolution, which become the default choice for many subsequent works such as Stable Diffusion series [7, 25], Imagen3 [2] and Flux [17]. Recent work [7, 17, 24] replaces the unet [32] to transformer and shows the impressive quality and scalability of the transformer backbone. 2.2. Subject-driven generation Subject-driven generation has been widely studied in the context of diffusion models. Dreambooth [33], textual inversion [8] and LoRA [12] introduce the subject-driven generation capability by introducing lightweight new parameters and perform parameter efficiency tuning for each subject. The major drawback of those methods is the cumbersome fine-tuning process for each new-coming subject. IP-adapter [45], BLIP Diffusion [18] use an extra image encoder and new layers to encode the reference image of the subject and inject it into the diffusion model, achieving subject-driven generation without further finetuning for new concept. For DiT, IC LoRA [13] and Ominicontrol [38] have explored the inherent image reference capability in the transformer, pointing that the DiT itself can be used as the image encoder for the subject reference. Many further works follow this reference image injection approach and have made improvements in various aspects like facial identity [10, 40], image-text joint controlability [14], multiple reference subject support [20, 41]. Despite these advances, the aforementioned work heavily relies on paired images, which are hard to collect, especially for multi-subject scenes. 3. Methodology This section introduces our proposed model-data coevolution paradigm, encompassing the systematic synthetic data curation framework detailed in Sec. 3.2 and the generalized customization model framework (i.e., UNO) exing. As depicted in Fig. 3, we introduce high-resolution, highly-consistent data synthesis pipeline to tackle this challenge, capitalizing on the intrinsic in-context generation capabilities of DiT-based models. Through the utilization of meticulously crafted text inputs, DiT models exhibit the capacity to generate subject-consistent grid outcomes. In contrast to prior methodologies like OminiControl [38], which generate single-subject consistent data at resolution of 512 512, our approach establishes more comprehensive pipeline that progresses from single-subject to multisubject data generation. This advancement enables the direct production of three distinct high-resolution image pairs (i.e., 1024 1024, 1024 768, and 768 1024), significantly broadening the application spectrum and diversity of the synthesized data. We emphasize that the superior quality of the synthesized data can significantly enhance model performance. To substantiate this assertion, we developed filtering mechanism based on the Vision-Language Model (VLM) to evaluate the quality of the generated image pairs. Subsequently, we conducted experiments utilizing synthesized data across various quality score levels. As depicted in Fig. 5, highquality scores image pairs can significantly enhance the subject similarity of the results with higher DINO [22] and CLIP-I score, which verifies that our automated data curation framework can continuously supplement high-quality data and improve model performance. Single-Subject In-Context Generation. In order to increase dataset diversity, we initially formulated taxonomy tree comprising 365 overarching classes sourced from Object365 [36], alongside finer-grained categories encompassing distinctions in age, profession, and attire styles. Within each category, we leverage the capabilities of Large Language Model (LLM) to generate an extensive array of subjects and varied settings. Through the amalgamation of these outputs with predefined text templates, we are able to derive millions of text prompts for the T2I model, facilitating the generation of subject-consistent image pairs. Initially generated image pairs often encounter several issues, such as subject inconsistency and missing subjects. To efficiently filter the data, we first split the image pair into the reference image 1 ref and the target image Itgt, then calculate the DINOv2 [22] between the two images. This method is effective in filtering out images with significantly lower consistency. Subsequently, VLM will be further employed to provide score list evaluating different aspects (i.e., appearance, details, and attributes), which can be represented by the following equation: = VLM(Iref, Itgt, cy) RN 1, score = Average(S), (2) (3) where cy represents the input text to VLM, denotes the number of evaluated dimensions that are automatically genFigure 3. framework based on in-context data generation. Illustration of our proposed synthetic data curation pounded upon in Sec. 3.3. Specifically, the foundational work of DiT [24] is elucidated in Sec. 3.1. Section 3.2 provides an in-depth exploration of the construction of our innovative subject-consistent dataset, comprising meticulously curated single-subject and multi-subject image pairs. Furthermore, Sec. 3.3 outlines our methodology for transforming Text-toImage (T2I) DiT model into Subjectto-Image (S2I) model, showcasing its contextual generation capabilities. This adaptation involves an iterative training framework designed to facilitate multi-image perception, textual comprehension, and condition generation conducive to subject-driven synthesis. 3.1. Preliminary The original DiT architecture focuses solely on classconditional image generation. It departs from the commonly used U-Net backbone, instead employing full transformer layers that operate on latent patches. More recently, image generators such as Stable Diffusion 3 [7] and FLUX.1 [17] are built upon MM-DiT, which incorporates multi-modal attention mechanism and takes as input the concatenation of the embeddings of text and image inputs. The multi-modal attention operation projects positionencoded tokens into query Q, key K, and value representations, enabling attention computation across all tokens: Attention ([zt, c]) = softmax (cid:19) (cid:18) QK V, (1) where = [zt, c] denotes the concatenation of image and text tokens. This allows both representations to function within their own respective spaces while still taking the other into account. 3.2. Synthetic Data Curation Framework The paucity of high-quality, subject-consistent datasets has long presented formidable obstacle for subject-driven generation, severely constraining the scalability of model train4 Figure 4. Illustration of the training framework of UNO. It introduces two pivotal enhancements to the model: progressive cross-modal alignment and universal rotary position embedding(UnoPE). The progressive cross-modal alignment is divided into two stages. In the Stage I, we use single-subject in-context generated data to finetune the pretrained T2I model into an S2I model. In the Stage II, we continue training on generated multiple-subject data pairs. The UnoPE can effectively equip UNO with the capability of mitigating the attribute confusion issue when scaling visual subject controls. Figure 5. Model performance on Dreambench [33]. We conduct experiments under different quality score levels. erated by VLM, signifies the output which is parsed into score list, and score indicates the final consistency score of generated image pairs. Multi-Subject In-Context Generation. The comprehensive dataset from the preceding phase will be utilized to train S2I model, which is conditioned on both singleimage and text inputs. Subsequently, this trained S2I model, along with the dataset, will be employed to generate multisubject consistent data in the current stage. Illustrated in Fig. 3(b), we will initially employ an open-vocabulary detector (OVD) to identify subjects beyond those present in 1 ref. The extracted cropped images and their corresponding subject prompts will then be input into our trained S2I model to derive new results for 2 ref. Traditional approaches often encountered significant failure rates as models struggled to preserve the original subjects identity. However, models trained using our proposed in-context training methodology can effectively surmount this challenge, yielding highly consistent outcomes with ease. Further elaboration on this topic will be provided in the subsequent section. Some may question the necessity of generating new data, suggesting that the cropped part could be treated simply as 2 ref. However, we contend that relying solely on cropped images as the training dataset may introduce copy-paste issues. This scenario arises when the model fails to adhere to the textual prompt and merely pastes\" the input reference image onto the resulting image. Our proposed pipeline effectively mitigates this concern. 3.3. Customization Model Framework (UNO) In this section, we will provide detailed explanation of how to iteratively train multi-image conditioned S2I model from DiT-based T2I model. It should be noted that all the training data we used originate from images generated by our in-context data generation method proposed in the previous chapter. Progressive cross-modal alignment. Original T2I models gradually transform pure Gaussian noise into text-adherent images through an iterative denoising process. During this process, the VAE encoder E() first encodes the target image Itgt into noisy latent zt = E(Itgt). zt is then concatenated with the encoded text token c, forming the input for the DiT model. This process can be formulated as: = Concatenate(c, zt), (4) Iref conditions To ref, 2 ref, . . . , incorporate multi-image = [I 1 ref], we introduce progressive training paradigm that progresses from simpler to more complex scenarios. We view the training phase with single-image conditions as the initial phase for cross-modal alignment. Given that the original input comprises solely text tokens and noisy latents, the introduction of noise-free reference image tokens could potentially disrupt the original convergence distribution. Such disruption may result in training instability or suboptimal outcomes. Hence, we opt for gradual complexity approach rather than directly exposing the model to multiple reference image inputs. In Stage I, depicted in Fig. 4, only single image serves as the reference image. We utilize z1 as the input multi-modal tokens for the DiT model: z1 = Concatenate(c, zt, E(I 1 ref)), (5) 5 Figure 6. Qualitative comparison with different methods on single-subject driven generation. Figure 7. Qualitative comparison with different methods on multi-subject driven generation. 6 After Stage training, the model is capable of processing single-subject driven generation tasks. We then train the model with multi-image conditions to tackle more complex multi-subject driven generation scenarios. z2 can be described as follows: ref = E(I zi ref), = 1, . . . , N, z2 = Concatenate(c, zt, ref, z2 ref, . . . , zN ref), (6) (7) where is set to 2 in our paper. During Stage I, the T2I model is trained to refer to the input reference image and prompt, with the goal of generating single subjectconsistent results. Stage II is designed to enable the S2I model to refer to multiple input images and inject information into corresponding latent spaces. Through iterative training, the inherent in-context generation capability of T2I model is unlocked, eliciting more controllability from single text-to-image models. Universal rotary position embedding(UnoPE). An important consideration for incorporating multi-image conditions into DiT-based T2I model pertains to the aspect of position encoding. In the context of FLUX.1 [17], the utilization of Rotary Position Embedding (RoPE) necessitates the assignment of position indices (i, j) to both text and image tokens, thereby influencing the interaction among multimodal tokens. Within the original model architecture, text tokens are assigned consistent position index of (0, 0), while noisy image tokens are allocated position indices (i, j) where [0, 1] and [0, 1]. Here, and denote the height and width of the noisy latent, respectively. Our newly introduced image conditions reuse the same format to inherit the implicit position correspondence of the original model. However, we start from the maximum height and width of the noisy image tokens, as shown in Fig. 4, which begins with the diagonal position. The position index for the latent zN ref is defined as: (i, j) = (i + w(N 1), + h(N 1)), (8) where [0, wN ), [0, hN ), with wN and hN representing the width and height of the latent zN ref, respectively. Here, and are the adjusted position indices. To prevent the generated image from over-referencing the spatial structure of the reference image, we adjust the position indices within certain range. In the scenario of multi-image conditions, different reference images inherently have semantic gap. Our proposed UnoPE can further prevent the model from learning the original spatial distribution of reference images, thereby focusing on obtaining layout information from text features. This enables the model to improve its performance in subject similarity while maintaining good text controllability. 4. Experiments 4.1. Experiments Setting Implementation Details. To self-evolution our base DiTbased T2I model, we firstly take the FLUX.1 dev [17] as the pretrained model. We train the model with learning rate of 105 and total batch size of 16. For the progressive cross-modal alignment, we first train the model using single-subject pair-data for 5, 000 steps. Then, we continue training on multi-subject pair-data for another 5, 000 steps. Specifically, we generated 230k and 15k data pairs for these two stages respectively using the in-context data generation method mentioned above. We conduct the entire experiment on 8 NVIDIA A100 GPUs and trained the model using LoRA [12] rank of 512 throughout the training process. Comparative Methods. As tuning-free method, our model is capable of handling both single-subject and multisubject driven generation. We compare it with some leading methods in these two tasks respectively, including Omnigen [44], Ominicontrol [38], FLUX IPAdapter v2 [39], Msdiffusion [41], MIP-Adapter [15], RealCustom++ [20], and SSR-Encoder [48]. Evaluation Metrics. Following previous works, we use standard automatic metrics to evaluate both subject similarity and text fidelity. Specifically, we employ cosine similarity measures between generated images and reference images within CLIP [27] and DINO [22] spaces, referred to as CLIP-I and DINO scores, respectively, to assess subject similarity. Additionally, we calculate the cosine similarity between the prompt and the image CLIP embeddings (CLIP-T) to evaluate text fidelity. For single-subject driven generation, we measure all methods on DreamBench [33] for fairness. For multi-subject driven generation, we follow previous studies [15, 19] that involve 30 different combinations of two subjects from DreamBench, including combinations of non-live and live objects. For each combination, we generate 6 images per prompt using 25 text prompts from DreamBench, resulting in 4, 500 image groups for all subjects. 4.2. Qualitative Analyses We compare with various state-of-the-art methods to verify the effectiveness of our proposed UNO. We show the comparison of single-image condition generation results in Fig. 6. In the first two rows, our UNO nearly perfectly keeps the subject detail (e.g., the numbers on the dial of the clock) in the reference image, while other methods struggle to maintain the details. In the following two rows, we demonstrate the editability. UNO can maintain subject similarity while editing attributes, specifically colors, whereas other methods either fail to maintain subject similarity or do not follow the text edit instructions. As contrast, OminiControl [38] has good retention ability but may encounter Method DINO CLIP-I CLIP-T Oracle(reference images) Textual Inversion [8] DreamBooth [33] BLIP-Diffusion [18] ELITE [43] Re-Imagen [5] BootPIG[26] SSR-Encoder[48] RealCustom++ [14, 20] OmniGen [44] OminiControl [38] FLUX.1 IP-Adapter UNO (Ours) 0.774 0.569 0.668 0.670 0.647 0.600 0.674 0.612 0.702 0.693 0.684 0.582 0.760 0.885 0.780 0.803 0.805 0.772 0.740 0.797 0.821 0.794 0.801 0.799 0.820 0.835 - 0.255 0.305 0.302 0.296 0.270 0.311 0.308 0.318 0.315 0.312 0.288 0.304 Table 1. Quantitative results for single-subject driven generation on Dreambench. We present the oracle results in the first row and compare both tuning methods and tuning-free methods. We highlight the best and second-best values for each metric. Method DINO CLIP-I CLIP-T DreamBooth [33] BLIP-Diffusion [18] Subject Diffusion [19] MIP-Adapter [15] MS-Diffusion [41] OmniGen [44] UNO (Ours) 0.430 0.464 0.506 0.482 0.525 0.511 0.542 0.695 0.698 0.696 0.726 0.726 0.722 0.733 0.308 0.300 0.310 0.311 0.319 0.331 0. Table 2. Quantitative results for multi-subject driven generation. Our method achieves state-of-the-art performance among both tuning methods and tuning-free methods. copy-paste risks, e.g., red robot in the last row of Fig. 6. We show the comparison results of multi-image condition generation in Fig. 7. Our method can keep all reference images while adhering to text responses, whereas other methods either fail to maintain subject consistency or miss the input text editing instructions. 4.3. Quantitative Evaluations Automatic scores. Tab. 1 compares our UNO on DreamBench [33] against both tuning-based and tuning-free methods. UNO has significant lead over previous methods with the highest DINO and CLIP-I scores of 0.760 and 0.835 respectively in zero-shot scenarios, and leading CLIP-I score of 0.304. We also compare our method in the multiimage condition scenario in Tab. 2. UNO achieves the highest DINO and CLIP-I scores and has competitive CLIP-T scores compared to existing leading methods. This shows that UNO can greatly improve subject similarity while adhering to text descriptions. User study. We further conduct user study via online Figure 8. Radar charts of user evaluation of methods for singlesubject driven and multi-subject driven generation on different dimensions Method w/o generated 2 ref w/o cross-modal alignment w/o UnoPE UNO (Ours) DINO CLIP-I CLIP-T 0.529 0.511 0.386 0.542 0.730 0.721 0.674 0.733 0.308 0.322 0.323 0. Table 3. Ablation Study of our proposed in-context data generation and in-context training method. We report the results on the multi-subject driven generation benchmark. Method DINO CLIP-I CLIP-T w/ single subject pair-data w/ cross-modal alignment 0.730 0.760 0.821 0.835 0.309 0.304 Table 4. Effect of progressive cross-modal alignment. The model exhibits superior performance on DreamBench [33] after undergoing progressive cross-modal alignment, in contrast to being trained exclusively on single-subject pair-data, despite both models undergoing an identical number of training steps. questionnaires to showcase the superiority of UNO. For subjective assessment, 30 evaluators including both domain experts and non-experts, assessed 300 image combinations covering both single-subject and multi-subject driven generation tasks. For each case, evaluators rank the best results across five dimensions, including text fidelity at the subject level, text fidelity at the background level, subject similarity, composition quality, and visual appeal. As shown in Fig. 8, the results reveal that our UNO not only excels in subject similarity and text fidelity but also achieves strong performance in other dimensions. 4.4. Ablation Study Effect of synthetic data curation framework. Tab. 3 shows the effects of different modules of UNO. When using augmented cropped part images from the target image instead of generated 2 ref, we observe significant decline in all metrics in Tab. 3. In Fig. 9, the results tend to merely 8 Method DINO CLIP-I CLIP-T w/o offset w/ width-offset w/ height-offset UNO (Ours) 0.470 0.717 0.678 0. 0.722 0.813 0.797 0.821 0.308 0.304 0.308 0.309 Table 5. Comparison with different forms of position index offsets. We report the results on DreamBench[33]. Method DINO CLIP-I CLIP-T w/o offset w/ width-offset w/ height-offset UNO (Ours) 0.386 0.508 0.501 0.542 0.674 0.724 0.719 0.733 0.323 0.321 0.306 0.322 Table 6. Comparison with different forms of position index offsets. We report the results on the multi-subject driven generation benchmark. copy-paste the subjects and almost do not respond to the text prompt description. Effect of progressive cross-modal alignment. As shown in Tab. 3 and Fig. 9, there is significant drop in both DINO and CLIP-I scores, as well as in subject similarity, when the model is directly exposed to multiple reference image inputs without progressive cross-modal alignment. Furthermore, as shown in Tab. 4, progressive cross-modal alignment can increase the upper limit of the model in singleimage condition scenarios. Effect of UnoPE. As shown in Tab. 3, there is significant drop in both DINO and CLIP-I scores when cloning the position index from the target image without using UnoPE. In Fig. 9, the generated images can follow the text descriptions but hardly reference the input images. We further compared with different forms of position index offsets, as shown in Tabs. 5 and 6, and our method achieves the best results, which demonstrates the superiority of our proposed UnoPE. 5. Conclusion In this paper, we present UNO, universal customization architecture that unlock the multi-condition contextual capabilities of diffusion transformer. This is achieved through progressive cross-modal alignment and universal rotary position embedding. The training of UNO consists of two steps. The first step uses single-image input to elicit the subject-to-image capabilities in diffusion transformers. The next step involves further training on multiple-subject data pairs. Our proposed universal rotary position embedding can also significantly improves subject similarity. Additionally, we present progressive synthesis pipeline that evolves from single-subject to multi-subject in-context generation. This pipeline generates high-quality synthetic data, Figure 9. Ablation study of UNO. Zoom in for details. effectively reducing the copy-paste phenomenon. Extensive experiments show that UNO achieves high-quality similarity and controllability in both single-subject and multiplesubject customization."
        },
        {
            "title": "References",
            "content": "[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 2 [2] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. 3 [3] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. 2 [4] Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon Wetzstein. Diffusion self-distillation for zero-shot customized image generation. arXiv preprint arXiv:2411.18616, 2024. 13 [5] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022. 8 [6] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NIPS, 34:1982219835, 2021. 3 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 3, 4 [8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 3, [9] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, 9 Jonathan Uesato, Po-Sen Lucy Campbell-Gillingham, Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soˇna Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements, 2022. 2 [10] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Zhang, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. In NIPS, 2024. 3 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NIPS, 33:68406851, 2020. 3 [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3, [13] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 3 [14] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: narrowing real text word for real-time open-domain text-to-image customization. In CVPR, pages 74767485, 2024. 2, 3, 8 [15] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion arXiv for finetuning-free personalized image generation. preprint arXiv:2409.17920, 2024. 7, 8 [16] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, pages 19311941, 2023. 2 [17] Black Forest Labs. Flux: Official inference repository for flux.1 models, 2024. Accessed: 2025-02-07. 3, 4, 7, 12, 13 [18] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36:3014630166, 2023. 3, 8 [19] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 7, [20] Zhendong Mao, Mengqi Huang, Fei Ding, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom++: Representing images as real-word for real-time customization. arXiv preprint arXiv:2408.09744, 2024. 3, 7, 8 [21] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [22] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 4, 7, 12, 13 [23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 2 [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 3, 4 [25] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 3 [26] Senthil Purushwalkam, Akash Gokul, Shafiq Joty, and Nikhil Naik. Bootpig: Bootstrapping zero-shot personalized image generation capabilities in pretrained diffusion models. arXiv preprint arXiv:2401.13974, 2024. 8 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PmLR, 2021. 7 [28] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821 8831. Pmlr, 2021. 3 [29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3 [30] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In ICML, pages 10601069. PMLR, 2016. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 3 [32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In CVPR, pages 2250022510, 2023. 2, 3, 5, generation. 7, 8, 9 [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NIPS, 35:3647936494, 2022. 3 10 [35] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing arXiv preprint models for assisting human evaluators. arXiv:2206.05802, 2022. [36] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In CVPR, pages 84308439, 2019. 4, 12 [37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In ICML, pages 2256 nonequilibrium thermodynamics. 2265. pmlr, 2015. 3 [38] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 3, 2024. 3, 4, 7, 8, 12 [39] XLabs AI team. x-flux, 2025. Accessed: 2025-02-07. 7 [40] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 3 [41] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. MS-diffusion: Multi-subject zero-shot image personalization with layout guidance. In ICLR, 2025. 3, 7, 8 [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 12, [43] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In CVPR, pages 1594315953, 2023. 8 [44] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 3, 7, 8 [45] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3 [46] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 3 [47] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: largescale dataset of multi-view images. In CVPR, pages 9150 9161, 2023. 2 [48] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, pages 80698078, 2024. 7, 11 Less-to-More Generalization: Unlocking More Controllability by In-Context Generation Supplementary Material F. In-Context Data Generation Pipeline In this section, we give detailed description of our in-context data generation pipeline. We first build taxonomy tree in Sec. F.1 to obtain various subject instances and scenes. Then we generate subject-consistent image-pair data with the incontext ability of pretrained Text-to-Image (T2I) model and utilize Chain-of-Thought (CoT) [42] to filter the synthesized data in Sec. F.2. Finally, for multi-subject data, we train Subject-to-Image (S2I) model to generate subject-consistent reference image instead of the cropped one to avoid the copy-paste issue in Sec. F.3. F.1. Taxonomy Tree Generation To ensure the diversity of the generated dataset, we first construct taxonomy tree that includes common categories of people and objects, as shown in Fig. 10. Specifically, we use the 365 general classes from Object365 [36] as the basis for our taxonomy tree. To obtain more diverse categories, we employ Large Language Model (LLM) to generate various subject instances and diverse scenes. The instructions in Fig. 12 make LLM generate subject instances according to the given asset category in creative, realistic, and text-decorated ways. In addition, we instruct LLM to generate scene descriptions according to the given subject with system prompt in Fig. 13. Following the steps above, we build taxonomy tree and get plenty of diverse subjects and scene descriptions. F.2. Single-Subject In-Context Data Generation In-context ability of T2I model has been proved in OminiControl [38] and we also utilize it to generate subject-consistent image-pair data as the basic part of our final in-context synthesized data in Sec. F.2.1. Moreover, we filter out low quality data with bad subject consistency according to the similarity from DINOv2 [22] and Vision-Language Model (VLM) in Sec. F.2.2. Figure 10. Illustration of the taxonomy tree. F.2.1. Subject-Consistent Image-Pair Generation Combining the constructed taxonomy tree with the predifined diptych text template in Fig. 11, we utilize the inherent incontext ability of FLUX.1 [17], one of the state-of-the-art T2I model, to generate subject-consistent image-pair. Since FLUX.1 [17] has multi-resolution generation ability, we directly produce three different high-resolution (i.e., 10241024, 1024768, 7681024) image-pairs, with great balance of quality and efficiency. F.2.2. Subject-Consistent Image-Pair Filter Though FLUX.1 [17] shows great in-context generation ability, synthesized image-pairs suffer several issues, especially subject inconsistency and missing subjects. We highlight that the high quality of synthesized data can notably accelerate the convergence and improve the subject consistency. To efficiently filter synthesized data, we first split the diptych image-pair into reference image 1 ref and Itgt ref and target image Itgt with Hough Transform. According to the template in Fig. 11, both 1 12 Figure 11. Diptych text template for generating subject-consistent image-pair with FLUX.1[17]. contain the same subject1 while Itgt has another subject2. To ensure 1 cosine similarity with DINOv2 [22] and set threshold to filter out image-pairs with significantly low consistency. ref and Itgt have consistent subject1, we then calculate However, since the reference image 1 ref and the target image Itgt have different scene settings, the subject1 in the imagepairs may not be spatially aligned, resulting in incorrect cosine similarity with feature from DINOv2 [22]. We further employ VLM to provide fine-grained score list evaluating various aspects adaptively, i.e. appearance, details, and attributes. We only keep the data with highest VLM score, which indicating the highest quality and subject consistency in the synthesized data. Specifically, inspired by [4], we utilize CoT [42] for better discrimination of the subject1 in 1 ref and Itgt, as shown in Fig. 14. To demonstrate the effectiveness of the CoT filter, we sample data from different VLM score intervals in Fig. 15. Image-pairs with low score suffer severe subject inconsistency while those with highest score (i.e. score is 4) show highly consistent subject in the reference image and the target image. We also count the amount of data in each score interval as shown in Fig. 17, indicating that around 35.43% data would be remained with the VLM CoT filter. Also, there are seldom of data with extremely low VLM score after DINOv2 filter, showing its effectiveness. (a) System prompt of LLM used to generate subject instances in creative type. (b) System prompt of LLM used to generate subject instances in realistic type. (c) System prompt of LLM used to generate subject instances in text-decorated type. Figure 12. System prompt of LLM used to generate subject instances. F.3. Multi-Subject In-Context Data Generation Following the above pipeline, we construct single-subject in-context data containing subject-consistent image-pairs (I 1 ref, Itgt). Both the reference image and the target image have subject1 while only the target image contain subject2. Since Itgt has multisubject, the simplest way to build multi-subject in-context data is utilizing open-vocabulary detector (OVD) to identify and crop the subject2 in Itgt as the second reference image 2 ref would make severe copypaste issue. To alleviate the issue, S2I model is trained with the single-subject in-context data and then used to generate new reference image 2 ref but different scenes. Thus we have high quality multi-subject in-context data with subject-consistent image-pairs (I 1 ref, Itgt) after very similar filter pipeline for the synthesized 2 ref. There are some case randomly sample from our final data in Fig. 16. Interestingly, we find that small part of image-pairs have more than 2 reference images, due to the randomness of T2I generation and OVD, empowering generalization for more-subject generation. ref, which has the consistent subject with the cropped 2 ref, 2 ref. However, we find that the cropped 2 13 Figure 13. System prompt of LLM used to generate scene descriptions. (a) System prompt of the filter VLM. (b) Prompt for the first round CoT of the filter VLM. (c) Prompt for the second round CoT of the filter VLM. (d) Prompt for the third round CoT of the filter VLM. Figure 14. CoT prompt of the filter VLM. G. Analysis on LoRA Rank In this section, we further conduct an ablation study on the LoRA rank. Since training parameters are strongly related to the final performance, we scale the rank from 4 to 512. As shown in the Fig. 18, increasing the rank gradually brings sustained gains, but when the rank reaches 128, the performance improvement slows down. Finally, considering both performance and resource consumption, we set UNO to rank of 512. H. More Qualitative Results H.1. Qualitative Results on Multi-Subject Driven Generation We show more qualitative comparison of multi-subject driven generation in Fig. 19. It is clear that UNO generate images with best multi-subject consistency, following edit instructions to the subject and background. 14 Figure 15. Sampled data from different VLM score intervals. H.2. Application Scenarios We evaluated our UNO model across diverse multi-image conditional scenarios, such as identity preservation, virtual tryon, and stylized generation. We found that UNO demonstrated exceptional generalization capabilities, even with minimal exposure to such data during training. Multi-subject Driven Generation: we have showcased additional results from our UNO model in Fig. 20. Beyond effectively handling multi-subject scenarios, UNO excels in complex applications like logo design and the integration of virtual and real elements, demonstrating its strong generalization capabilities. Virtual Try-on: as shown in Fig. 21, UNO performs exceptionally well in virtual try-on scenarios, despite the absence of specialized training on such datasets. This demonstrates that UNO has learned to understand relationships between objects rather than simply performing copy-paste operations. It also suggests that UNO could provide novel optimization strategies for virtual try-on applications, promising direction we leave to further exploration. Identity Preservation: another notable observation is that UNO performs well in both pure ID scenarios and ID-subject combinations in Fig. 22. This flexibility reduces reliance on additional ID plugins, fostering opensource community development. We attribute this capability to our systematic training data construction. As mentioned in Sec. F.1, our taxonomy tree covers extensive human-object combinations, enabling this versatile performance. Stylized Generation: as depicted in Fig. 23, UNO has inherited stylization ability inherited from the original DiT model, despite the lack of specific paired data in our training set. This stems from our training approach, which smoothly transitions from T2I to S2I, allowing the model to evolve multi-condition control while maintaining strong semantic alignment. I. Limitation and Discussion Although we have established an automated data curation framework, this paper primarily focuses on subject-driven generation. Our dataset currently contains limited editing and stylization data. While UNO is unified and customizable framework Figure 16. Sampled data from our final multi-subject in-context data. Figure 17. Amount of data in each VLM score interval. Figure 18. Analysis of model performance under different LoRA ranks. with sufficient generalization capabilities, the types of synthetic data may somewhat restrict its abilities. In the future, we plan to expand our data types to further unlock UNOs potential and cover broader range of tasks. 16 Figure 19. More comparison with different methods on multi-subject driven generation. We italicize the subject-related editing part of the prompts. 17 Figure 20. More multi-subject generation results from our UNO model. 18 Figure 21. More virtual try-on results from our UNO model. 19 Figure 22. More identity preservation results from our UNO model. 20 Scenarios One2One Two2One Many2One Stylized Generation Virtual Try-on Product Design Identity-preservation Story Generation Figure 23. More stylized generation results from our UNO model. Prompt clock on the beach is under red sun umbrella\" doll holds UNO sign under the rainbow on the grass\" The figurine is in the crystal ball\" The boy and girl are walking in the street\" penguin doll, car and pillow are scattered on the bed\" boy in red hat wear sunglasses\" Ghibli style, woman\" Ghibli style, man\" man wears the black hoodie and pants\" girl wears the blue dress in the snow\" The logo and words Let us unlock! are printed on the clothes\" The logo is printed on the cup\" The figurine is in the crystal ball\" penguin doll, car and pillow are scattered on the bed\" boy in green is in the arcade\" man strolls down bustling city street under moonlight\" The man and boy in green clothes are standing among the flowers by the lake\" The man met boy dressed in green at the foot of the tower\" Table 7. Text prompts used in Fig. 1."
        }
    ],
    "affiliations": [
        "Intelligent Creation Team, ByteDance"
    ]
}