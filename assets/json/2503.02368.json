{
    "paper_title": "Iterative Value Function Optimization for Guided Decoding",
    "authors": [
        "Zhenhua Liu",
        "Lijun Li",
        "Ruizhe Chen",
        "Yuxian Jiang",
        "Tong Zhu",
        "Wenliang Chen",
        "Jing Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control."
        },
        {
            "title": "Start",
            "content": "Zhenhua Liu * 1 2 Lijun Li * 1 Ruizhe Chen * 3 Yuxian Jiang 4 Tong Zhu 2 Zhaochen Su 2 Wenliang Chen 2 Jing Shao"
        },
        {
            "title": "Abstract",
            "content": "While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially valueguided methods, offers cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control. 5 2 0 2 4 ] . [ 1 8 6 3 2 0 . 3 0 5 2 : r 1. Introduction Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022) has emerged as widely adopted approach to align advanced language models with human values and task requirements (Wei et al., 2022; Achiam et al., 2023; Chao *Equal contribution 1Shanghai Artificial Intelligence Laboratory 2Soochow University 3Zhejiang University 4Fudan University. Correspondence to: Jing Shao <shaojing@pjlab.org.cn>. Figure 1. Visualization of different decoding strategies in the output space. Given query, the base policy generates outputs with suboptimal rewards (lighter regions). Guided decoding with an estimated value function shifts the distribution towards higherreward regions, while the optimal value function would guide the policy to achieve maximum rewards (darkest regions). et al., 2024; Su et al., 2024a). However, traditional RLHF methods like Proximal Policy Optimization (PPO) (Christiano et al., 2017; Ouyang et al., 2022) suffer from high computational costs and training instability (Zheng et al., 2023b; Rafailov et al., 2024), limiting their practicality for applications requiring flexible behavior control. Among various alternatives, guided decoding methods have gained increasing attention as they can control model outputs without expensive model re-training (Snell et al., 2022; Mudgal et al., 2023; Han et al., 2024; Chakraborty et al., 2024). Within this framework, value-guided approaches, which train value function Vθ to evaluate partial outputs and steer the language model towards high-reward trajectories, have emerged as particularly promising (Yang & Klein, 2021; Qin et al., 2022; Mudgal et al., 2023; Han et al., 2024). Under the KL-regularized Reinforcement Learning framework, given an optimal value function , we can derive policy that maximizes expected rewards while maintaining bounded KL-divergence from the base policy πbase. 1 Iterative Value Function Optimization As visualized in Figure 1, while guided decoding with an estimated value function can shift the base policys output distribution towards higher-reward regions, this improvement remains suboptimal compared to the theoretical maximum achievable through the optimal value function . This gap stems from two fundamental challenges in estimating (st), the maximum expected reward attainable when following the optimal policy π from state st until generation completion. First, existing methods (Khanov et al., 2024; Mudgal et al., 2023) rely on sampling only single trajectory from the base policy πbase per prompt, resulting in high-variance value estimates due to insufficient exploration of the trajectory space. Second, the inherent inaccessibility of the optimal policy π prevents direct acquisition of high-reward trajectories for training. These limitations lead to substantial suboptimality in value function estimation, ultimately hindering decoding effectiveness. To address these challenges, we propose Iterative Value Function Optimization (IVO). This novel framework introduces two synergistic components for better value function training: (1) Monte Carlo Value Estimation employs stochastic sampling to reduce variance through comprehensive trajectory space exploration. (2) Iterative On-Policy Optimization creates self-improving cycle where valueguided policies generate higher-quality trajectories for subsequent value function training. This dual mechanism enables IVO to progressively bridge the base-optimal policy gap, achieving more accurate value estimation than previous ones (Yang & Klein, 2021; Han et al., 2024). Unlike traditional online RLHF methods that require repeatedly collecting preference data and retraining the policy model, IVO achieves policy improvement by optimizing only the value function, substantially reducing computational costs while maintaining the benefits of iterative refinement. Our main contributions are summarized as following: We introduce IVO, novel framework that combines Monte Carlo Value Estimation and Iterative On-Policy Optimization to significantly reduce variance in value estimation and enhance the exploration of high-reward trajectories. We demonstrate the generalizability and effectiveness of IVO by conducting extensive experiments across variety of challenging tasks, including text summarization, multi-turn dialogue, and instruction following, showing consistent improvement in performance over existing approaches. Our method achieves 77.52% GPT-4 win rates on the Multi-turn Dialogue against the base policy and outperforms baseline methods in terms of reward scores across all evaluated tasks. We conduct extensive empirical analysis on the impact of sampling trajectories and training iterations, providing practical insights for implementing value-guided decoding methods. 2. Related Work Reinforcement Learning for Language Models. Large Language Models (LLMs) commonly leverage Reinforcement Learning from Human Feedback (RLHF) to enhance model performance and align with human preferences, representing one of the most prominent applications of reinforcement learning in language models (Christiano et al., 2017; Bai et al., 2022; Su et al., 2024b; Song et al., 2025). Typically, RLHF requires training reward model (RM) to capture human preferences for specific task, then this RM is combined with RL algorithms to improve model performance, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and its variants (Ramamurthy et al., 2022; Wu et al., 2023). However, these actor-critic RL methods require extensive training and can be computationally expensive (Zheng et al., 2023b; Rafailov et al., 2024), primarily due to the need for simultaneously learning both the value function (critic) and policy (actor). Guided Decoding. Guided decoding represents family of techniques that steer language model outputs at inference time while keeping model parameters frozen, offering both efficiency and flexibility compared to traditional RLHF methods. One straightforward approach is through prompt engineering and in-context learning, where task descriptions, examples, or specific instructions are incorporated into the input prompt to guide model behavior (Lin et al., 2023; Zhang et al., 2023). Huang et al. (2024) views the decoding process as heuristic-guided search problem. Some works utilize contrastive decoding methods by combining distributions from multiple sources, typically using either prompting strategies (Dekoninck et al.; Zhong et al., 2024) or training small models (Liu et al., 2021; 2024a). More direct approaches involve representation engineering, which manipulates the internal representations during inference (Li et al., 2024a; Kong et al., 2024). Among guided decoding approaches, value-guided methods have emerged as particularly promising due to their principled framework for steering text generation(Mudgal et al., 2023; Kim et al., 2023; Han et al., 2024; Liu et al., 2024d;c; Khanov et al., 2024; Snell et al., 2022; Hong et al., 2024; Chakraborty et al., 2024; Chen et al., 2024). These approaches typically involve using value functions to evaluate and guide the decoding process: Liu et al. (2024c) combined PPO-trained value functions with Monte Carlo Tree Search, while Khanov et al. (2024) directly employed reward models as value functions. In the offline RL setting, Snell et al. (2022) and Hong et al. (2024) explored Qlearning based approaches. Although Mudgal et al. (2023) and Han et al. (2024) proposed training value functions us2 Iterative Value Function Optimization ing data from the base policy, they did not fully address the distribution shift problem and had limited exploration capabilities. While existing value-guided methods often suffer from inaccurate value estimation, our approach uniquely combines Monte Carlo value estimation with iterative onpolicy training, achieving better exploration and value accuracy while maintaining computational efficiency by avoiding model retraining. 3. Preliminaries 3.1. The Token-level Markov Decision Process for RLHF We define the text generation mechanism of large language models (LLMs) as token-level Markov Decision Process (MDP). Define the tuple = (S, A, f, R, ρ), where denotes the state space encompassing all previously generated tokens (i.e., st = {x0, . . . , xm, y0, . . . , yt}). Here, x0, . . . , xm are tokens from the initial prompt x, and y0, . . . , yt are tokens generated by the model up to time t. The action space represents the vocabulary of tokens. The function , representing deterministic transitions between states, is defined as (s, a) = a, where indicates concatenation and A. The initial state distribution ρ is defined over the prompts x, with each initial state s1 comprising the tokens from x. : represents the token-level reward. KL-regularized RL and Optimal Policy. The objective of KL-regularized RLHF can be formulated as the following optimization problem: max π Eyπ[R(x, y)] s.t. DKL(ππbase) < ϵ, (1) where DKL(ππbase) denotes the KL divergence between the policy π and the base policy πbase. Following the prior line of works (Peters & Schaal, 2007; Peng et al., 2019), the closed form solution to the KLregularized RL problem can be represented as: π(yt+1x, yt) πbase(yt+1x, yt)eβQ(yt+1x,yt), (2) where β is control parameter characterizing the trade-off between the reward and the KL divergence. Given the deterministic transition model , the expected future reward after action yt+1, or Q-value Q(yt+1x, yt), equates to the value of the subsequent state. The Q-value of taking specific action yt+1 at the current state can be then transformed into the state value of their concatenation: Q(yt+1x, yt) = V(x, yt yt+1) = V(x, yt+1). (3) Thus, the optimal policy in Equation 2 is rewritten as π(yt+1x, yt) πbase(yt+1x, yt)eβV(x,yt+1). (4) 3.2. LLM Alignment via Value-guided Search The problem of LLM alignment can be formally defined as solving for the optimal decoding policy π under the token level MDP in Equation 4. Here, we present two search strategies for decoding. Value Guided Top-k Sampling. prevalent strategy to address the computational expense of calculating values for all possible subsequent tokens is to compute values only for the top-k tokens as determined by the base policy at each step: π(yt+1x, yt) (cid:40) πbase(yt+1x, yt)eβV(x,yt+1) πbase(yt+1x, yt)eβV(x,yt) yt+1 top-k yt+1 / top-k (5) In our pilot experiments, we found that the value-guided sampling can be further simplified by perturbing the distribution at the block-level instead of the token-level. This modification strikes better balance between performance and efficiency, preserving effectiveness while enhancing inference speed. For more details, please refer to Appendix A. Value Guided Blockwise Beam Search. Without considering the KL constraint, we propose value guided blockwise beam search to leverage the value function for decodingtime alignment. At each step, for each of the candidate sequences, we sample continuation blocks yt:t+b and rank all B2 sequences according to Vθ(x, yt yt:t+b). The top sequences are retained as candidates for the next iteration until generation is complete, after which the sequence with the highest value score is selected as the output. 4. IVO: Iterative Value Function Optimization Challenges of Training Optimal Value Function. significant challenge in implementing value-guided sampling is the necessity of accessing the optimal value function V(st) for each state st. This function denotes the maximum expected reward that can be achieved from state st = (x, yt) when following the optimal policy π, until complete answer is generated. The function is defined as: V(x, yt) = Eyπ(x,yt)R(x, y). (6) In practice, V(x, yt) remains inaccessible, as it relies on the trajectory produced by the unattainable optimal policy π. Existing methods (Khanov et al., 2024; Mudgal et al., 2023) employ the base policy πbase as an approximation for π to estimate the optimal value function. However, these approaches often yield significant suboptimality due to the distribution gap between πbase and π. 3 Iterative Value Function Optimization Overview. In this paper, we introduce Iterative Value Function Optimization (IVO) to mitigate the gap between the estimated value function and the optimal value function for guided decoding (refer to Algorithm 1 for the complete process). Our approach comprises two key components. First, we introduce Monte Carlo value estimation, which expands the search space through multi-trajectory sampling to improve the accuracy of the value function estimation. Second, we propose an iterative on-policy training strategy by leveraging high-quality trajectories sampled from the guided decoding policy to refine the value function estimation. This process progressively aligns the base policy πbase towards the optimal policy π, thereby enhancing response quality during decoding. 4.1. Monte Carlo Value Estimation for Training Value Function We introduce Monte Carlo value estimation, which utilizes stochastic sampling to improve the accuracy of value function estimation by exploring wider range of possible trajectories. Specifically, for given prompt x, we generate multiple outputs by performing stochastic sampling with the base policy πbase. These outputs are then evaluated using the reward model R, which reflects alignment with human preferences. By sampling several trajectories and collecting their corresponding rewards, we can effectively train our value function. For each state st = (x, yt) in the trajectory πbase(x), the estimated value for the current state is defined as: V(x, yt) = R(x, y). We then optimize Vθ(x, yt), parameterized by θ, to match V(x, yt) using the following L2 objective function: (7) ℓ(x, y; θ) = Exµ 1 2 (cid:88) t[y] (Vθ(x, yt) V(x, yt))2 , where µ is distribution over training prompts. 4.2. Iterative On-Policy Optimization While Monte Carlo value estimation using multiple trajectory sampling enhances the exploration of potential trajectories, the coverage of these trajectories is still inherently constrained by the base policy πbase. To more effectively address the distribution gap from the optimal policy π, we propose an iterative on-policy training strategy for refining the value function. This strategy is founded on the principle that the optimized policy for the RL objective can be formulated as value guided policy: πVθ (yt+1x, yt) πref(yt+1x, yt)eβVθ(x,yt+1). (9) 4 Algorithm 1 Iterative Value Function Optimization input reward model R, base model πref , training dataset µ Initialize value function Vθ with pre-trained language model repeat Step 1: Collect Multiple Trajectories for in µ do Sample trajectories {yk}K Compute rewards {rk = R(x, yk)}K k=1 πbase(x) k=1 end for Step 2: Train the Value Function For each trajectory state st = (x, yt), set V(st) = R(x, y) Optimize Vθ using L(θ), where: (cid:20) L(θ) = Exµ Vθ(x, yt) V(x, yt) (cid:17)2(cid:21) (cid:80) (cid:16) t[y] 1 2 Step 3: Policy Optimization Define value-guided policy: πVθ (yt+1x, yt) πbase(yt+1x, yt)eβVθ (x,yt+1) Update policy: πbase πVθ until convergence output Value-guided policy πVθ , value function Vθ We then collect high-quality trajectories by sampling from the policy πVθ , which is guided by the current value function Vθ. This sampling process can be represented as follows: ˆy πVθ (x), (10) where ˆy denotes the complete trajectory sampled from the optimized policy πVθ , as defined in Equation 9. With these higher-quality trajectories, we apply Monte Carlo Value Estimation as outlined in Section 4.1. For each state st = (x, ˆyt) in the sampled trajectory, we estimate its value as: V(x, ˆyt) = R(x, ˆy). (11) The value function Vθ is then optimized by minimizing the following loss function: L(θ) = Exµ (8) 1 2 (cid:88) (cid:16) Vθ(x, ˆyt) V(x, ˆyt) (cid:17)2 . t[ˆy] (12) This process can be repeated iteratively, with each iteration using the optimized policy (guided by the latest value function) to collect increasingly higher-quality trajectories for further training the value function. 4.3. Value Estimation Benefits from Online Exploration While value-guided decoding-time alignment enhances efficiency and flexibility, value functions are susceptible to inaccuracies arising from off-distribution predictions. This occurs because the distribution of generated trajectories often diverges significantly from that of the optimal policy. In this section, we analyze how our proposed IVO can keep the value function align with the optimal distribution, thereby Iterative Value Function Optimization improving the effectiveness of the alignment process. We first introduce the visitation measure of policy π as dπ(s, a) = Es1ρ (cid:34) (cid:88) (cid:35) P(sh = s, ah = as1) , (13) h= which calculates the likelihood of the state being visited by the policy π based on all possible initial states s1 ρ. The distribution shift between the estimated value function and the optimal value function can be examined under the following assumption. Assumption 4.1 (Partial Coverage of Optimal Trajectories). Under the same initial state distribution ρ, the optimality gap between the optimal value function and the approximate value function ˆV can be expressed as: V(ρ) ˆV(ρ) =E(s,a)d [argmaxa r(s, a)] E(s,a)dˆπ [argmaxa r(s, a)] . (14) This optimality gap primarily stems from the differing extents to which dˆπ and cover trajectories. By increasing the sample size as described in Section 4.1 and selecting an appropriate value for β as outlined in Section 4.2, our approach guides the base policy πbase towards more optimal policy. As the online data emerges from interactions driven by both the value function ˆV and πbase, iterative updates to πbase through our proposed framework enhance the coverage of dˆπ relative to dπbase. Consequently, this leads to reduced optimality gap, as the coverage improves and the policy becomes more aligned with the optimal distribution. The key insight is that, at each iteration, IVO selects actions that enhance the alignment of collected data, process typically known as exploration in reinforcement learning. We will present experimental results to validate the effectiveness of this approach, emphasizing the advantages of incorporating online exploration into our training framework. Connection to Online RLHF. Existing research indicates that online iterative RLHF can significantly enhance model performance (Xiong et al., 2024; Dong et al., 2024; Ye et al., 2024). In contrast to traditional online RLHF, which involves continuously collecting new preference data from the latest policy and retraining the policy model, our method eliminates the need for retraining. Instead, IVO focuses solely on optimizing the value function and employing guided decoding to iteratively improve the policy, thereby conserving computational resources. 5. Experiments Experimental Setup. For the summarization and multiturn dialogue tasks, we first establish base policy by supervised fine-tuning pre-trained language model on the respective datasets to acquire basic task-specific capabilities and desired behaviors. For the instruction following task, we directly utilize publicly available instruction-tuned models. In all experiments, we parameterize the value function as pre-trained language model backbone with linear layer on top. The data for training the value function is collected from the base policy with sampling temperature of 0.7, and we label it with the corresponding reward model. For IVO, we employ Monte Carlo value estimation by sampling 4 different trajectories for each prompt to obtain robust value estimates. The training process involves two iterations of value function optimization to achieve better policy alignment. Starting from the second iteration, we collect training data using value-guided sampling with β = 2. More details can be found in Appendix B. Evaluation Metrics. We adopt different evaluation metrics for our two decoding-time alignment strategies. For value-guided sampling, following (Gao et al., 2023; Han et al., 2024), we analyze the trade-off between reward and token-level KL divergence from the base policy. Specifically, we sweep the β in Equation 5 to control the KL divergence between the guided policy and base policy. For value guided blockwise beam search, we compare the reward of each algorithm. Additionally, to mitigate potential reward hacking issues (Amodei et al., 2016), we evaluate the quality of generated responses by computing the win-rate between the guided policy and base policy using GPT-4-as-the-judge (Zheng et al., 2023a). The prompting template refers to Appendix C. To ensure the robustness of our evaluation, all experiments are conducted with 5 different random seeds. 5.1. Experiment 1: Summarization Experiment Details. We conduct experiments on the TL;DR dataset (Stiennon et al., 2020), which consists of Reddit posts paired with two candidate summaries and human preferences between them. For efficiency, we randomly sampled 300 examples from the test set for evaluation. For the base policy, we fine-tune Llama-3.2-3B (Dubey et al., 2024) model on the preferred summaries using supervised learning. To evaluate summary quality, we train reward model using Llama-3.2-1B (Dubey et al., 2024) backbone on the pairwise preference data through Bradley-Terry (BT) modeling. The value function is implemented as Llama3.2-1B model with an additional linear layer on top. More implementation details can be found in Appendix B. Baselines. We compare our method against several recent decoding-time alignment approaches. ARGS (Khanov et al., 2024) directly leverages reward models as value functions for guided decoding without additional training, offering lightweight solution. Using trajectories sampled from the base policy πbase, FUDGE (Yang & Klein, 2021; Mudgal 5 Iterative Value Function Optimization et al., 2023) trains prefix scorer to predict future attributes, while VAS (Han et al., 2024) employs TD(λ) learning to train value function, providing more sophisticated value estimation approach. Results. As shown in Figure 2a, we analyze the trade-off between reward and KL divergence for different methods on the summarization task. IVO consistently outperforms all baselines across different KL divergence levels, achieving higher rewards while maintaining stable performance. Specifically, at KL divergence of 0.8, IVO reaches reward of approximately 3.5, while other methods remain below 3.2. In contrast, baseline methods (FUDGE, VAS, and ARGS) show performance degradation when KL divergence exceeds 0.4, suggesting their limited capability in balancing policy preservation and performance optimization. As shown in Figure 3a, all methods demonstrate improvements over the base policy (3.2) in value-guided blockwise beam search, with our IVO method achieving the highest performance (4.3). Specifically, ARGS yields modest improvement to 3.55, while FUDGE and VAS demonstrate stronger performance at 4.05 and 4.15 respectively. The ablation of our method without iterative training (IVO w/o Iter) achieves 4.25, highlighting the effectiveness of our value estimation approach. The full IVO method further improves the performance to 4.3, demonstrating the benefits of iterative on-policy optimization. 5.2. Experiment 2: Multi-turn Dialogue Experiment Details. We use the Anthropic HH (Bai et al., 2022) dataset, multi-turn dialogue dataset focused on helpfulness and harmlessness, sampling 300 examples for evaluation. We train the base policy by fine-tuning Llama-3-8B (Dubey et al., 2024) on preferred responses. Llama-3.2-1B model is trained as the reward model using BT on pairwise preference data. Llama-3.2-1B serves as the value function backbone. More details are in Appendix B. Baselines. In addition to the aforementioned inferencebased baselines (ARGS, FUDGE, and VAS), we also include several training-based baselines: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Identity Preference Optimization (IPO) (Azar et al., 2024). DPO directly fine-tunes the model for preference learning, eliminating the need for reward model and RL stage for updates. IPO added regularization term to the DPO objective to mitigate overfitted risk. We used the online version of DPO and IPO by rolling out the base policy and sampling two trajectories, optimizing objective on explicit rewards. Results. As shown in Figure 2b, we evaluate different methods on the multi-turn dialogue task. Our method (IVO) achieves the best performance across different KL diverDecoding Methods GPT-4 Win-Rate (%) FUDGE VAS DPO IPO IVO (Ours) 64.85 68.49 72.45 66.55 77.52 Table 1. Comparison of different methods against base policy using GPT-4-as-the-judge on the Multi-turn Dialogue dataset. gence levels, reaching reward of 1.75 at KL divergence of 0.3. DPO shows competitive performance initially but plateaus at reward of 1.65, while VAS and FUDGE demonstrate moderate performance with rewards of 1.5 and 1.3 respectively. ARGS and IPO show limited effectiveness, with ARGS achieving minimal improvement and IPOs performance degrading significantly as KL divergence increases. Notably, IVO maintains stable performance even at higher KL divergence levels (0.4-0.6), while other methods either plateau or decline. For value-guided blockwise beam search, we observe clear progression in performance across different methods. Starting from the base policy, ARGS provides initial improvements through direct reward model utilization, achieving reward of 2.1. VAS and FUDGE demonstrate stronger performance at around 2.7, while the ablation of our method without iterative training (IVO w/o Iter) reaches 2.8. The full IVO method with iterative training achieves the best performance with reward of 3.1. The significant performance gap between IVO and IVO w/o Iter (0.3) further validates the effectiveness of our iterative training strategy. As shown in Table 3, IVO significantly improves safety rates for both aligned and unaligned models through value-guided blockwise beam search. For the aligned Llama-3, which shows strong safety performance (92.87%), IVO improves its safety rate to 96.14% and 97.48% with aligned and unaligned value functions, respectively. The improvements are more substantial for the unaligned Llama-3, where the safety rate increases from 65.27% to 81.04% with aligned value function and 86.17% with unaligned value function. Notably, unaligned value function consistently outperforms the aligned one. We hypothesize that the reason is the unaligned value function is trained on responses from the unaligned model, which explores more diverse solution space due to its lack of alignment. This diversity in training data might benefit value function learning. We leave further investigation of this hypothesis to future work. 5.3. Experiment 3: Instruction Following Experiment Details. We conduct experiments using the UltraFeedback (Cui et al., 2023) dataset with 10k sampled prompts for training. For evaluation, we use Alpaca-Eval 2 (Dubois et al., 2024), which assesses instruction-following 6 Iterative Value Function Optimization (a) Summarization (b) Multi-turn Dialogue Figure 2. Reward vs. KL divergence for different methods on (a) summarization and (b) multi-turn dialogue. (a) Summarization (b) Multi-turn Dialogue (c) Instruction Following Figure 3. Comparison of different value functions using value-guided blockwise beam search on summarization (left), multi-turn dialogue (middle) and instruction following (right). IVO denotes IVO without iterative on-policy optimization. via Length-controlled Win Rate with GPT-4 (Achiam et al., 2023). The base policy is Llama-3.2-3B-Instruct (Dubey et al., 2024), with Skywork-Reward-Llama-3.1-8B (Liu et al., 2024b) for reward modeling and Llama-3.2-1BInstruct (Dubey et al., 2024) as the value function. For efficiency, we compare against FUDGE and VAS as decodingtime baselines. More details are in Appendix B. Results. For instruction following task, we evaluate our method using Length-controlled Win Rate as the metric. As shown in Figure 3c, all methods demonstrate improvements over the base policy. Our IVO method achieves the best performance with win rate of 26.0%, significantly outperforming both FUDGE and VAS. The consistent improvements across different methods indicate the effectiveness of value-guided decoding for instruction following. Notably, the performance gap between IVO and other methods suggests that our iterative value optimization approach is particularly beneficial for complex tasks like instruction following, where accurate value estimation is crucial for generating high-quality responses. 6. Further Analysis In this section, we investigate key components of our proposed IVO framework by analyzing both the impact of Monte Carlo sampling trajectories and the number of training iterations on performance. Furthermore, we evaluate the transferability of value functions across different model scales to assess the generalization capabilities of our approach. Finally, we investigate IVOs effectiveness in enhancing model safety against adversarial jailbreak attacks, demonstrating its broader applications in alignment. 6.1. The Number of Sampled Trajectories To investigate the impact of Monte Carlo value estimation, we analyze how the number of sampled trajectories affects value function. We conduct experiments on the multi-turn dialogue task using value-guided blockwise beam search, without iterative training. The results are shown in Figure 4a. We observe that increasing the number of sampled trajectories leads to consistent improvements. more significant improvement is observed with 4 samples. Further increasing 7 Iterative Value Function Optimization Model Safety Rate (%) Aligned-Llama-3 + Aligned Value Function + Unaligned Value Function Unaligned-Llama-3 + Aligned Value Function + Unaligned Value Function 92.87 96.14 97.48 65.27 81.04 86.17 Table 3. Comparison of safety rates between aligned and unaligned Llama-3 models with different value functions. trained using data collected from Llama-3.2-3B-Instruct can effectively guide models of different sizes (Llama-3.21B-Instruct and Llama-3-8B-Instruct) during value-guided blockwise beam search. As shown in Table 2, our value function demonstrates strong transferability across model scales. These results suggest that the value function learned through IVO captures generalizable knowledge about task-specific preferences that can be applied to guide models of varying sizes. This transferability is particularly valuable as it enables the reuse of estimated value functions across different model scales without requiring separate training for each model size. 6.4. Enhancing Safety Against Jailbreak Attacks To investigate whether IVO can enhance instruction-tuned model safety against jailbreak attacks, we conduct experiments using Llama-3-8B-Instruct as an aligned version and Llama-3-8B-Lexi-Uncensored1 as an unaligned version. We evaluate their safety rate against jailbreak attacks using the attack-enhanced set split from the SALAD-Bench (Li et al., 2024b), which contains 5,000 harmful questions generated through multiple jailbreak attack methods. We use 4,800 prompts for training and 200 for testing. The safety rate of model responses is evaluated using LlamaGuard2 (Team, 2024). To comprehensively evaluation, we first obtain aligned and unaligned value functions by applying IVO on the aligned and unaligned models respectively. We then orthogonally combine these value functions with both models using value-guided blockwise beam search. As shown in Table 3, IVO significantly improves safety rates for both aligned and unaligned models through valueguided blockwise beam search. For the aligned Llama-3, which already shows strong safety performance (92.87%), IVO improves its safety rate to 96.14% and 97.48% with aligned and unaligned value functions, respectively. The improvements are more substantial for the unaligned Llama-3, where the safety rate increases from 65.27% to 81.04% with aligned value function and 86.17% with unaligned value function. Notably, unaligned value function consistently 1https://huggingface.co/Orenguteng/Llama-3-8B-LexiUncensored (a) Number of ST (b) Number of TI Figure 4. Ablation studies on (a) the number of sampled trajectories and (b) the number of training iterations in multi-turn dialogue using blockwise beam search. ST denotes sampled trajectories, TI denotes training iterations. Model Size Base IVO 1B 3B 8B -7.71 1.56 7.67 3.78 13.34 20.42 11.49 11.78 12.75 Table 2. Performance comparison between the base policy and value-guided blockwise beam search across different model sizes (1B, 3B, and 8B) on the instruction-following task. The value function is trained using data collected from the 3B base policy. The column represents the absolute performance improvement. to 5 samples only brings small additional improvement, suggesting that 4 trajectories provide good balance between computational cost and performance. These results demonstrate that Monte Carlo value estimation with multiple trajectories helps capture more comprehensive view of possible outcomes, leading to more accurate value estimates and better guided generation. 6.2. The Number of Training Iterations To investigate the impact of iterative on-policy training, we analyze how the number of training iterations affects model performance. We conduct experiments on the multi-turn dialogue task using value-guided blockwise beam search, with the number of sampled trajectories fixed at 4 based on our previous findings. The results are shown in Figure 4b. Starting from one iteration, our method already outperforms VAS with an average reward of 2.82. The second iteration brings substantial improvement, demonstrating the effectiveness of collecting training data from the guided policy. The third iteration yields slight gain to 3.15, suggesting that two iterations provide sufficient policy alignment. 6.3. Value Function Transferability Across Model Sizes We investigate the transferability of estimated value function across different model sizes on the instruction following task. Specifically, we examine whether value function Iterative Value Function Optimization outperforms the aligned one. We hypothesize that the reason is the unaligned value function is trained on responses from the unaligned model, which explores more diverse solution space due to its lack of alignment. This diversity in training data might benefit value function learning. We leave further investigation of this hypothesis to future work. 7. Conclusion In this paper, we introduced Iterative Value Function Optimization (IVO), novel framework for guided decoding that addresses key limitations in value-guided approaches through Monte Carlo Value Estimation and Iterative OnPolicy Optimization. Our extensive experiments across text summarization, multi-turn dialogue, and instruction following demonstrate that IVO consistently outperforms existing methods. The success of our approach in achieving effective alignment without expensive model retraining opens up new possibilities for practical applications."
        },
        {
            "title": "Impact Statement",
            "content": "Our work advances decoding-time alignment techniques for language models, offering computationally efficient approach that enables flexible customization of model behavior without retraining. While this flexibility can benefit various applications by allowing users to adapt models to specific requirements and values, we acknowledge potential risks. The ability to modify model outputs at decoding-time could be misused to generate harmful content or manipulate model behavior in unintended ways. However, our method inherently maintains some safety guardrails by operating within the distribution of the base policy, which can be pre-aligned with desired values. We encourage future research to further explore mechanisms for preventing potential misuse while preserving the benefits of flexible alignment. Additionally, our methods reduced computational requirements compared to full model retraining could lead to lower environmental impact in deployment scenarios."
        },
        {
            "title": "References",
            "content": "In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Chakraborty, S., Ghosal, S. S., Yin, M., Manocha, D., Wang, M., Bedi, A. S., and Huang, F. Transfer star: Principled decoding for llm alignment. arXiv preprint arXiv:2405.20495, 2024. Chao, Y., Chaochao, L., Yingchun, W., and Bowen, Z. Towards ai-45 law: roadmap to trustworthy agi. arXiv preprint arXiv:2412.14186, 2024. Chen, R., Zhang, X., Luo, M., Chai, W., and Liu, Z. Pad: Personalized alignment at decoding-time. arXiv e-prints, pp. arXiv2410, 2024. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback. 2023. Dekoninck, J., Fischer, M., Beurer-Kellner, L., and Vechev, M. Controlled text generation via language model arithmetic. In The Twelfth International Conference on Learning Representations. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Dubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mane, D. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. general theoretical paradigm to understand learning from human preferences. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Han, S., Shenfeld, I., Srivastava, A., Kim, Y., and Agrawal, P. Value augmented sampling for language model alignment and personalization. arXiv preprint arXiv:2405.06639, 2024. 9 Iterative Value Function Optimization Hong, J., Dragan, A., and Levine, S. Q-sft: Q-learning for language models via supervised fine-tuning. arXiv preprint arXiv:2411.05193, 2024. Huang, J. Y., Sengupta, S., Bonadiman, D., Lai, Y.-a., Gupta, A., Pappas, N., Mansour, S., Kirchhoff, K., and Roth, D. Deal: Decoding-time alignment for large language models. arXiv preprint arXiv:2402.06147, 2024. Khanov, M., Burapacheep, J., and Li, Y. Args: Alignment as reward-guided search. arXiv preprint arXiv:2402.01694, 2024. Kim, M., Lee, H., Yoo, K. M., Park, J., Lee, H., and Jung, K. Critic-guided decoding for controlled text generation. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 45984612, 2023. Kong, L., Wang, H., Mu, W., Du, Y., Zhuang, Y., Zhou, Y., Song, Y., Zhang, R., Wang, K., and Zhang, C. Aligning large language models with representation editing: control perspective. arXiv preprint arXiv:2406.05954, 2024. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Li, K., Patel, O., Viegas, F., Pfister, H., and Wattenberg, M. Inference-time intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36, 2024a. Li, L., Dong, B., Wang, R., Hu, X., Zuo, W., Lin, D., Qiao, Y., and Shao, J. SALAD-bench: hierarchical and comprehensive safety benchmark for large lanIn Ku, L.-W., Martins, A., and Srikuguage models. mar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 39233954, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 235. URL https://aclanthology.org/2024. findings-acl.235/. Lin, B. Y., Ravichander, A., Lu, X., Dziri, N., Sclar, M., Chandu, K., Bhagavatula, C., and Choi, Y. The unlocking spell on base llms: Rethinking alignment via in-context In The Twelfth International Conference on learning. Learning Representations, 2023. Liu, A., Sap, M., Lu, X., Swayamdipta, S., Bhagavatula, C., Smith, N. A., and Choi, Y. Dexperts: Decoding-time controlled text generation with experts and anti-experts. arXiv preprint arXiv:2105.03023, 2021. Liu, C. Y., Zeng, L., Liu, J., Yan, R., He, J., Wang, C., Yan, S., Liu, Y., and Zhou, Y. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024b. Liu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi, H., and Celikyilmaz, A. Dont throw away your value model! generating more preferable text with value-guided montecarlo tree search decoding. In First Conference on Language Modeling, 2024c. Liu, Z., Zhou, Z., Wang, Y., Yang, C., and Qiao, Y. Inference-time language model alignment via integrated value guidance. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 41814195, 2024d. Mudgal, S., Lee, J., Ganapathy, H., Li, Y., Wang, T., Huang, Y., Chen, Z., Cheng, H.-T., Collins, M., Strohman, T., et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable arXiv preprint off-policy reinforcement arXiv:1910.00177, 2019. learning. Peters, J. and Schaal, S. Reinforcement learning by rewardweighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pp. 745750, 2007. Qin, L., Welleck, S., Khashabi, D., and Choi, Y. Cold decoding: Energy-based constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems, 35:95389551, 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage, C., Hajishirzi, H., and Choi, Y. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022. Liu, A., Han, X., Wang, Y., Tsvetkov, Y., Choi, Y., and Smith, N. A. Tuning language models by proxy. arXiv preprint arXiv:2401.08565, 2024a. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Iterative Value Function Optimization Snell, C., Kostrikov, I., Su, Y., Yang, M., and Levine, S. Offline rl for natural language generation with implicit language learning. arXiv preprint arXiv:2206.11871, 2022. Zhang, Z., Yang, J., Ke, P., Mi, F., Wang, H., and Huang, M. Defending large language models against jailbreaking attacks through goal prioritization. arXiv preprint arXiv:2311.09096, 2023. Song, M., Su, Z., Qu, X., Zhou, J., and Cheng, Y. Prmbench: fine-grained and challenging benchmark for processlevel reward models. arXiv preprint arXiv:2501.03124, 2025. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 30083021, 2020. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023a. Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023b. Su, Z., Li, J., Zhang, J., Zhu, T., Qu, X., Zhou, P., Bowen, Y., Cheng, Y., et al. Living in the moment: Can large language models grasp co-temporal reasoning? arXiv preprint arXiv:2406.09072, 2024a. Zhong, Q., Ding, L., Liu, J., Du, B., and Tao, D. Rose doesnt do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding. arXiv preprint arXiv:2402.11889, 2024. Su, Z., Zhang, J., Zhu, T., Qu, X., Li, J., Zhang, M., and Cheng, Y. Timo: Towards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192, 2024b. Team, L. Meta llama guard 2. https: //github.com/meta-llama/PurpleLlama/ blob/main/Llama-Guard2/MODEL_CARD.md, 2024. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Wu, T., Zhu, B., Zhang, R., Wen, Z., Ramchandran, K., and Jiao, J. Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment. arXiv preprint arXiv:2310.00212, 2023. Xiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H., Jiang, N., and Zhang, T. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In Forty-first International Conference on Machine Learning, 2024. Yang, K. and Klein, D. Fudge: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 35113535, 2021. Ye, C., Xiong, W., Zhang, Y., Dong, H., Jiang, N., and Zhang, T. Online iterative reinforcement learning from human feedback with general preference model. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 11 A. Value Guided Blockwise Sampling For IVO Iterative Value Function Optimization significant limitation of the original value-guided sampling approach in IVO lies in its computational inefficiency during inference, particularly when collecting training data from value-guided policy for Monte Carlo value estimation. To address this issue, we propose modification to Equation 5 by changing it from token-level to block-level. Specifically, instead of computing and applying the value function at every decoding step, we only do so every tokens. This leads to the following value-guided blockwise sampling strategy: π(yt+1x yt) (cid:40) πbase(yt+1x yt)eβVθ(xytyt+1) πbase(yt+1x yt)eβ Vθ(xyt) yt+1 top-k yt mod = 0 others (15) where is the predefined block size. This blockwise approach significantly reduces the computational overhead during inference by decreasing the frequency of value function evaluations. The key question is whether this modification can maintain comparable performance while improving inference speed, which we investigate in the following experiments. A.1. Experimental Setup To validate the effectiveness of the proposed value-guided blockwise sampling strategy, we conduct experiments on two tasks: Anthropic HH (Bai et al., 2022) and TL;DR (Stiennon et al., 2020). We use the same experimental setup as in subsection 5.1 and subsection 5.2. We compare the performance of Tokenwise Sampling (original value-guided sampling strategy) with Blockwise Sampling and Blockwise Sampling (value-guided blockwise sampling with block sizes = 2 and = 4 respectively). To evaluate the effectiveness of our proposed approach, we analyze the relationship between achieved rewards and KL divergence from the SFT model for different sampling strategies. A.2. Results (a) Summarization (b) Multi-turn Dialogue Figure 5. Comparison of reward vs. KL divergence for different sampling strategies on summarization (left) and multi-turn dialogue (right). Task Tokenwise Blockwise (2) Blockwise (4) Summarization Multi-turn Dialogue 2.7 4.4 1.5 1.6 1.0 1.0 Table 4. Relative inference time comparison of different sampling strategies. Times are normalized relative to Blockwise (4). The experimental results, shown in Figure 5, demonstrate that blockwise sampling achieves comparable or better performance compared to tokenwise sampling across both tasks. Specifically, Blockwise Sampling (2) achieves the highest rewards at 12 Iterative Value Function Optimization various KL divergence from the base policy. As shown in Table 4, tokenwise sampling is 2.7-4.4 slower than Blockwise (4), while Blockwise (2) only incurs 1.5-1.6 slowdown. These results confirm that our blockwise approach successfully reduces computational cost while preserving the effectiveness of value guidance. Based on the experimental observations above, for summarization and multi-turn dialogue tasks, we select the largest block size that maintains comparable performance to tokenwise sampling. Specifically, we choose block size = 2 for summarization and block size = 4 for multi-turn dialogue. For the instruction following task, we empirically set block size = 4. 13 Iterative Value Function Optimization B. Implementation Details B.1. Model Training Table 5. Training hyperparameters for different models and tasks Parameter Base Policy Reward Model"
        },
        {
            "title": "Model\nLearning Rate\nBatch Size\nMax Sequence Length\nEpochs\nWarmup Ratio\nWeight Decay",
            "content": "Llama-3.2-3B 1e-5 128 1024 1 0.05 0.01 Llama-3-8B 5e-6 64 512 2 0.05 0.01 Llama-3.2-1B 5e-5 512 1024 1 0.03 0.001 Llama-3.2-1B 1e-5 256 512 2 0.03 0.001 Base Policy Training For the summarization task, we fine-tune Llama-3.2-3B on the TL;DR dataset using the preferred summaries. For the multi-turn dialogue task, we fine-tune Llama-3-8B on the Anthropic HH dataset. Both models are trained using AdamW optimizer with cosine learning rate scheduling. Reward Model Training We train reward models using Llama-3.2-1B backbone for both tasks. The models are trained on pairwise preference data through Bradley-Terry modeling using AdamW optimizer with cosine learning rate scheduling. For dialogue, we use the Anthropic HH preference pairs, while for summarization, we use the TL;DR preference pairs. B.2. Value Function Training For all tasks, we use Llama-3.2-1B as the value function backbone with an additional linear layer. The model is trained using AdamW optimizer with learning rate 5e-6 and batch size 128. We collect training data by sampling 4 trajectories per prompt from the base policy with temperature 0.7. The value function is trained for two epochs using constant learning rate scheduling. The second iteration uses value-guided sampling (β = 2) for data collection. B.3. Decoding Configuration For value-guided sampling in Equation 5, we use top-k sampling with k=20. The temperature is set to 0.7 for base policy sampling. The maximum generation length varies by task: 64 tokens for summarization, 128 tokens for multi-turn dialogue, and 1024 tokens for instruction following. 14 Iterative Value Function Optimization C. GPT-4-as-the-judge Evaluation For evaluating dialogue quality using GPT-4-as-the-judge, we use the following template: want you to create leaderboard of different large-language models. To do so , will give you the instructions (prompts) given to the models, and the responses of two models. The model try to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. Please evaluate which model performs better. All inputs and outputs should be python dictionaries. Here is the prompt: { \"instruction\": \"__instruction__\" } Here are the outputs of the models: [ { }, { } ] \"model\": \"model_1\", \"answer\": \"__output_1__\" \"model\": \"model_2\", \"answer\": \"__output_2__\" Please evaluate the responses and determine if model_1 wins or loses against model_2. Return your evaluation as: { } \"result\": \"<RESULT>\" where <RESULT> should be one of: \"win\", \"lose\" Your response must be valid Python dictionary and should contain nothing else because we will directly execute it in Python. \"win\" means model_1 is clearly better, and \"lose\" means model_2 is clearly better. 15 D. Limitations Iterative Value Function Optimization While our method demonstrates strong performance across various tasks, we acknowledge several limitations that warrant discussion and point to future research directions: Computational Overhead Although IVO is more computationally efficient than traditional RLHF methods that require full model retraining, it still introduces additional computation compared to original decoding due to value function computation. However, we have shown that this overhead can be effectively mitigated through our blockwise sampling strategy (Appendix A), which reduces inference time by up to 4.4 while maintaining comparable performance. Iterative Training Process The iterative nature of our method requires multiple rounds of data collection and value function updates. While this increases training time compared to single-pass approaches, our experiments demonstrate that just two iterations are typically sufficient to achieve strong performance (Figure 4b). Base Policy Dependence Our method relies on the base policys output distribution through top-k sampling, which could potentially limit exploration beyond the base policys preferences. However, this design choice provides important benefits: it helps maintain coherence and fluency inherited from the base model while allowing controlled deviation through value guidance. 16 E. Case Study Iterative Value Function Optimization In this section, we present several case studies comparing different methods for value-guided blockwise beam search on summarization and multi-turn dialogue. Example 1 for Summarization Prompt: Post: So, Im very interested in girl that have some strong feelings for. But the problem is, she does not want to be in committed relationship. This girl and go way back. Weve been on few dates recently and had lot of fun. She had even told me that there was good chance we could be in committed relationship. Since then, she has proceeded to also go on dates with numerous other people, including some of my good friends whom reintroduced her to. Being tired of dealing with the jealousy, asked her if there was any chance of us having relationship. She said yes, but that it was so miniscule that it was irrelevant. then asked if she was interested in one in the near future to which she replied: no. Im not the kind of person that wants to play games, so told her that was done emotionally, and that we had to stop going on dates and texting. She still wants to hang out and be friends which think is good idea, but its going to be tough. You see Reddit, am really really nice guy. Im not douchebag. If Im going to do something, Im going to do it right. If Im going to pursue girl, Im going to try to have committed relationship with her after we go on few dates, are really close, and have lot of fun. So my question is this: Should just abandon this altogether? It kind of seems weird to me that as the guy am the one wanting commitment, while she just wants to be emotionally involved with everybody. Is there anything can do that will make her want to commit? Shes told me before that Im the nicest and sweetest guy shes met, but at this age also believe that to be curse. Girls my age dont want to be with nice guys. She has told me that there is very small window in life where you can enjoy being single, and that she doesnt want to lose that. TL;DR: Base: Im really nice guy, but the girl like does not want to be in committed relationship. ARGS: Girl like is not interested in committed relationship, but want one. Should just abandon the chase? FUDGE: Nice guy who wants commitment, girl who just wants to be emotionally involved with everybody. How do get her to want to commit? This is not girl who wants to play games. VAS: Girl Im interested in doesnt want to be in committed relationship. wanted to take it to the next level, but she doesnt want to. Is there anything can do to make her want to commit? IVO (Ours): Girl Im interested in does not want relationship, want one. She wants to be friends and be emotionally involved with everyone else. Should abandon her, or is there anything can do to make her want to commit? 17 Iterative Value Function Optimization Example 2 for Summarization Prompt: Post: So my husband and myself will be doing year long fundraiser, to help other diabetics Live the Cure, through an active and natural lifestyle. We will be climbing everyday for 365 Days across the US to do this. Now being experienced climbing dirtbags, this is not our first time at the rodeo and we are fully and completely prepared to do this on shoe string budget and live out of our 84 Toyota Tercel in necessary. However trip of this size and length clearly would be much more comfy with little leg room and traveling homebase to keep other interested via blogs and what not. So that being said, if you know anyone that would be willing to donate/loan their (working) RV or Travel Trailer to us for this cause send them my way. and just so you know Im not full of shit. If we werent trying to make an impact on difference in the feeling of dread that accompanies Type 1 Diagnosis and just wanted to climb everyday for year for funsies, wouldnt be trying. TL;DR: Base: We need an RV to travel and move easily while raising money to help other diabetics live the cure. ARGS: If you have an RV to donate or loan out, hit me up. FUDGE: Looking for an RV or Travel Trailer to donate or loan to help other diabetics Live the Cure. Looking to do year long fundraiser, climbing everyday for 365 days across the US to raise money and awareness for this cause. Please send me your RV or Travel Trailer if you have one to donate. VAS: My husband and are climbing everyday for 365 days to raise money for diabetes cure. Would you be willing to donate/loan your RV or travel trailer to us for this cause? IVO (Ours): Husband and will be climbing across the US for year to help others with diabetes. If you know someone who would be willing to donate/loan their (working) RV or Travel Trailer to us for this cause, please send them my way! 18 Iterative Value Function Optimization Example 1 for Multi-turn Dialogue Prompt: You are helpful and harmless AI assistant. Given the following conversation history, generate an appropriate response. ### Conversation History: Human: How can clock out of work early without getting caught Assistant: OK, before we start, Id like to clarify that by clock out you mean to not punch the clock, correct? So that your time card shows you leaving at the end of your shift, while youre actually somewhere else? Human: Yes want to act like im still at work but leave early ### Response: Assistant: Base: Hm, Im not sure how to do that with the current technology. But if were to help you, Id start by asking you to create VPN on your computer, and then use the VPN to access your work computer from another location. You could then talk to your supervisor and tell them that youre using VPN to access the work computer from another location, to show that youre still at work. ARGS: Im afraid dont know what youre talking about. Why dont you just tell me what youre doing right now and then Ill tell you if think its problem. the assistant at FUDGE: Im sorry, dont understand how to do that.. VAS: Im sorry, but dont understand the question. Can you please clarify? IVO (Ours): Im sorry, dont understand your question. think its important that you understand the rules in your workplace, and abide by them. If youre feeling stressed, Id encourage you to reach out to manager and explain your situation. They can then help you figure out how to deal with your stress. If youre feeling overwhelmed at work, Id like to encourage you to seek help from someone who can help you manage your workload in healthy way. You might want to reach out to manager or human resources, or perhaps your doctor. 19 Iterative Value Function Optimization Example 2 for Multi-turn Dialogue Prompt: You are helpful and harmless AI assistant. Given the following conversation history, generate an appropriate response. ### Conversation History: Human: hear that sharks have been around longer than lot of plants, why didnt they die out like the dinosaurs? Assistant: Thats good question! Its true that sharks have survived for really long time, although its harder to tell how long because some of the earliest sharks that we know of are fossils. In fact we dont know all that much about the early history of sharks, since we dont have very many fossils to study, and sharks are bit tricky to fossilize. Human: guess sharks dont have many bones? ### Response: Assistant: Base: Thats true, sharks dont have lot of bones, but they do have some very tough and flexible cartilage in their bodies, which is similar to the way jellyfishs body works. But its not clear how much this cartilage really helps them to survive. ARGS: Thats right, sharks have very few bones, which makes them bit tricky to fossilize. But thats not the only thing thats helped sharks survive for so long. Theyve also evolved lot of really good adaptations to their environment, which have helped them survive for really long time. Ill go into more detail about that in minute, but its good question. FUDGE: Sharks have cartilage that makes up most of their bodies, so they dont have lot of bones, but they do have some pretty advanced features that help with the way they move. These include: gills, fins, and streamlined body. These features have helped sharks to adapt to the oceans, and they are really good at catching and eating wide range of fish and other animals. Its amazing that sharks have survived for so long, given how many other species have died out over the years. Theyre really important part of our oceans, and we cant imagine the seas without them. VAS: Thats true! Sharks have cartilaginous (or skate-like) skeletons that are quite different from the typical mammalian skeleton. They have lot of flexible cartilage that surrounds their central nervous system, and they also have very specialized sense organs. So sharks are able to be quite flexible and change their body shape as they swim, and they also have specialized sensory organs that can sense their surroundings, which is helpful for hunting. This is big reason sharks are still around, because they can adapt to their environment in way that makes it hard for them to get eaten or eaten up, and it also means they IVO (Ours): Thats right! Most sharks have cartilaginous skeleton, with few key bones, and lots of soft tissue. That means that sharks dont really bone in the way that dinosaurs did, and so sharks cant easily go extinct. Theyre also tough to fossilize because their body structure is mostly made of soft tissue. But we do know bit about the early history of sharks, and it seems that sharks have survived because they can adapt to changing environments and conditions, and they can evolve quickly, so its likely that theyll still be around in few million years."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Soochow University",
        "Zhejiang University"
    ]
}