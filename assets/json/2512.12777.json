{
    "paper_title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
    "authors": [
        "Mosh Levy",
        "Zohar Elyoseph",
        "Shauli Ravfogel",
        "Yoav Goldberg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 7 7 2 1 . 2 1 5 2 : r State over Tokens: Characterizing the Role of Reasoning Tokens Zohar Elyoseph University of Haifa Shauli Ravfogel New York University Mosh Levy Bar-Ilan University moshe0110@gmail.com Yoav Goldberg Bar-Ilan University Allen Institute for AI"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not faithful explanation of the models actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as linguistic narrative, but as an externalized computational statethe sole persistent information carrier across the models stateless generation cycles. This explains how the tokens can drive correct reasoning without being faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state."
        },
        {
            "title": "Introduction",
            "content": "The assertion that Large Language Models (LLMs) can reason now appears unremarkable (Mitchell, 2025; Maslej et al., 2025). key factor to achieve this was letting models generate sequence of tokens before their final answer, which significantly improves performance (Wei et al., 2022; Zelikman et al., 2022; DeepSeek-AI et al., 2025). We refer to this sequence of symbols, which includes phrases such as therefore, consider and it follows that as the reasoning tokens, and explicitly distinguish this name from reasoning text, which is the same tokens when interpreted by reader according to their English semantics. The combination of (a) utility in improving the answer; and (b) appearance as readable English text, may lead to the following inference: the reasoning text is faithful explanation of the models reasoning process. This is strengthened by metaphors like Chain-of-Thought, which imply that the steps in the text are thoughts that explain the process. Yet empirical findings contradict this inference (see Section 2.1): the reasoning text is not faithful explanation of the models reasoning process. While those findings clarify what the reasoning tokens are not, they leave conceptual vacuum as to what they are. Our aim in this paper is to help fill that vacuum. Drawing on the idea that metaphors structure understanding and guide thinking (Lakoff & Johnson, 1980), we believe that adopting more apt descriptions and metaphors can steer researchers and practitioners toward more fruitful directions and surface new set of questions that are less salient under the prevailing view of the reasoning text as an explanation. To understand reasoning tokens, we must focus on the functional role they play, rather than their appearance, which empirical research has found to be deceiving. To this end, we advocate viewing them as representing State over Tokens (SoT), which characterizes the 1 reasoning tokens as computational device that enables the persistence of process across separate and stateless computation cycles. We argue that in order to understand the role of the reasoning tokens, we should interpret this sequence of tokens not using their semantics when read as English text, but as the state carriers of computational process. The Whiteboard Analogy Consider hypothetical scenario: you are placed in room with problem written on whiteboard. Your task is to solve it, but under peculiar constraint: every 10 seconds, your memory is completely wiped and resets to the same state as it was when you first entered the room. Within each interval, you can read what is on the board and add single word. These rounds repeat until you finally write down the solution. How might you solve problem under such constraints? You may write intermediate results on the board: numbers, conclusions, or partial computationsthat you can use when you return after being reset. You might perform several mental calculations before writing down just the result so the whiteboard may not capture every calculation that you did within each cycle. Moreover, you may use an encoding scheme when writing on the board: abbreviations, symbols, or even apparent gibberish that will mean something specific to you when you encounter it in the next cycle. All in all, an outside observer may interpret the whiteboard text incorrectly. The whiteboard analogy mirrors the models operation: the words are the reasoning tokens, you are the model, and the ten-second interval represents the models limited capacity per cycle. Motivated by this intuition, we present the SoT framework (Section 3) and use it to demonstrate two common misconceptions that underlay the belief that the text is faithful explanation (Section 4) and to examine its novel ontological divergence (Section 5). The SoT framework naturally gives rise to new research questions (Section 6), challenging current interpretability methods to account for multiple cycles, and questioning whether language is special computational medium that can theoretically act both as state and as faithful explanation. Finally, this perspective compels us to reconsider established metaphors. While Chain-ofThought captures the sequential nature of generation, it misleadingly suggests the text contains the complete reasoning process; SoT clarifies that tokens are the state passed between computations, and do not necessarily express the computations themselves. Similarly, the Scratchpad metaphor obscures crucial distinction: while humans use scratchpad to supplement internal memory, for LLMs, the tokens are the sole carrier of state between cycles. By establishing reasoning tokens as externalized computational state, the SoT framework characterizes how LLMs employ what seems to us as natural language as computational medium, and provides foundation for future work on decoding how models use language tokens to reason."
        },
        {
            "title": "2 The Subject of Inquiry: Reasoning Tokens",
            "content": "The subject of our discussion is the sequence of tokens that LLMs emit before their final answer, which we refer to as reasoning tokens (which we distinguish from reasoning text which is the English interpretation of these same tokens). This includes scenarios where LLMs were guided to generate reasoning tokens using examples, instructions like lets think step by step (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022) or that they were trained to do so regardless of the input (Zelikman et al., 2022; OpenAI, 2024; Muennighoff et al., 2025; DeepSeek-AI et al., 2025; Comanici et al., 2025). However, LLMs operate in the same way for the generation of the final answer as they do for the generation of reasoning tokens. Why separate the two when aiming to characterize the role of the produced tokens? 2 The first reason is techno-sociological: reasoning tokens appear to offer second product beyond the answer itself. When interpreted as English, the reasoning text reads like description of reasoning work, encouraging readers to see not only what the model answers, but also how it seemingly arrived there, serving as an apparatus for building trust (Jacovi et al., 2021; Ferrario & Loi, 2022). The second reason is functional: we can conceptually distinguish the two stages by their training objectives. The final answer tokens are generated to be interpreted by humans (or external systems) as the solution to the users query. During training, the answer text is optimized to reflect the solution, and thus has an additional constraint: it cannot explicitly reflect computation. In contrast, the reasoning tokens are not constrained in this way, and their purpose is to support producing the final answer. Indeed, their existence improves the accuracy of the final answer (Kojima et al., 2022; Zelikman et al., 2022; Muennighoff et al., 2025; Comanici et al., 2025). This distinction is especially pronounced in models trained with methods that optimize only the final answer (Lambert et al., 2024; DeepSeek-AI et al., 2025), where the reasoning tokens are not enforced to any specific constraints as long as they serve the final prediction. 2.1 Empirical evidence: plausibility without faithfulness The reasoning text often reads like logical derivation. As Perrier (2025) notes, it can correspond to an explanation according to some deductive proof system, representing sequence of steps that leads to conclusion. In that sense, it serves as plausible explanation: it is convincing to human readers and presents valid path to the solution. However, while the sequence of outputs may correspond to valid proof, it does not necessarily correspond to the actual computation performed by the model. In other words, it fails to reliably be faithful explanation (Jacovi & Goldberg, 2020)one that accurately reflects the true causal process driving the models generation. Incompleteness. The reasoning text can be incomplete and omit critical factors that affected the models final answer (Turpin et al., 2023; Yee et al., 2024; Chua & Evans, 2025; Lindsey et al., 2025; Arcuschin et al., 2025; Marioriyad et al., 2025). study conducted in controlled settings showed that LLMs can even seem to follow alignment goals while omitting the undesired topics from the reasoning text (Skaf et al., 2025). Semantic mismatch between LLM and reader. The semantic content of the reasoning text can be meaningless or misleading from human perspective. Some studies have found that LLMs can disregard specific details in the text they generate (Lanham et al., 2023; Paul et al., 2024; Chen et al., 2025b). Other work has shown that LLMs can be trained to produce irrelevant reasoning text and still provide accurate final answers (Stechly et al., 2025; Bhambri et al., 2025; Zolkowski et al., 2025). This disconnect is also apparent to human readers, who fail to identify causal relations in the generation process of the reasoning text (Levy et al., 2025). The peril lies in how apparent rationality can increase trust without justifying it, fostering over-reliance on model outputs. When reasoning text mimics systematic deliberation, users in high-stakes scenarios may be misled into unwarranted confidence precisely because the text implies rigorous process (Ehsan & Riedl, 2024; Rosenbacke et al., 2024). 2.2 What can be said on reasoning tokens Following this growing body of evidence and the practical risks it highlights, some have already argued that we should not treat reasoning text as faithful explanation of the models reasoning process (Agarwal et al., 2024; Sarkar, 2024; Barez et al., 2025; Kambhampati et al., 2025). However, we believe that we need to characterize the role of reasoning tokens in positive way, beyond merely stating that they are not an explanation. What can be said about the reasoning tokens? The following three statements hold: 3 The reasoning tokens are an important part of the process that lead to the final answer. The reasoning tokens can sometimes be read as series of steps that logically lead to the answer. The reasoning tokens are not faithful explanation of the computation that led to the final answer. We would like metaphor that account for all three of these points. In what follows, we provide the State over Tokens view, which we believe does that."
        },
        {
            "title": "3 Conceptual Framework: State over Tokens",
            "content": "Rather than explaining the reasoning tokens by how they are perceived by human readers (how they look), we consider their functional role within the mechanism of generation (what they do). We view reasoning tokens as an accumulated computational substratea medium through which the computation state is encoded in tokens. 3.1 The formal framework Building on the intuition from the whiteboard analogy in the introductionand consistent with how LLMs are implementedwe view the reasoning tokens not as text with humanreadable semantics, but rather as the evolving state of the reasoning process. The token sequence functions as state for the model, though it can also be interpreted as natural language text. This state is what allows the LLM to maintain coherent process across multiple computation cycles, each bounded in capacity, to solve problems that require more computation than any single cycle can provide. To capture this architecture precisely, we view the autoregressive generation of the LLM as recursive application of pure function M() on token sequence (technically the LLM call is not pure as it involves random sampling of tokens, we can assume the random seed is part of the call to remedy this). Each computation of has limited computational capacity. While the computation capacity of the Transformer (Vaswani et al., 2017) does increase slightly with each input token (Merrill et al., 2022; Pfau et al., 2024), this is fixed increase that does not depend on the content of the tokens. The function M() is repeatedly applied to sequence of inputs following deterministic process. The initial input to the function is the users input, sequence of tokens we denote as S0 (commonly referred to as the context in LLM literature; we use input to emphasize its functional role as the argument to M): S0 = user input At each subsequent cycle k, the function takes the current sequence Sk as its input and produces new token as output. This token is appended to Sk to form Sk+1, the input for the next call: Sk+1 = Sk M(Sk) Here, denotes the concatenation operation. This continues until the final answer is produced. Note that each prefix of the Sk is an input (a computation state) of one of the prior cycles. The function of the tokens. This formalization reveals three key observations about the functional role of the tokens: The tokens are the only persistent artifact. The sequence Sk is the sole carrier of information between cycles. Internal states of the LLM exist only within each cycle and do not persist to the next, and will be recreated from scratch, exactly the same, in each cycle (modern Transformers use key-value (KV) cache to avoid this recomputation, but the KV cache does not carry information beyond what would be recomputed from the tokens). Nothing else persists, and the LLM must reconstruct any needed information from Sk alone. The tokens exclusively dictate future computation. The only information that the next computation of receives is Sk. Thus, Sk fully dictates what computation can perform next. Encoding and decoding are internal to the LLM. The way that Sk affects the next computation and the overall process that leads to the answer is dependent on M. What the tokens enable. The tokens are what enable the mechanism of process that is composed of multiple cycles. Each individual computation is limitedit can perform only as much work as the LLMs depth allows in single cycle. However, by encoding results into Sk and building upon them in subsequent cycles, the computational capacity of multiple computations of the LLM is utilized. The tokens accumulation transforms the process from being bounded to one cycle to utilize the computation done in multiple cycles. This conceptual view is supported by theoretical findings, which demonstrate thatat least for specific constructionsaccumulating tokens formally increases computational capacity (Merrill & Sabharwal, 2024; Li et al., 2024). Following those observations, we argue that an accurate and revealing name for this is State over Tokens (SoT). We will use this term for the remainder of the paper. 3.1.1 Implications of the State over Tokens view Viewing the reasoning tokens as state means we can attribute to them the properties inherent to the concept of state in computation. This reframing has several immediate implications for how we understand the tokens: state is forward-looking. state enables future computation, not describing past computations. While it is created by past computation, it does not determine it: various computations can lead to the same state. Unlike log that records past cycles, state encodes what is needed to continue the process. change in the state will lead to different process. Altering the state will change the trajectory of future computations. States are discrete snapshots. The tokens seem to describe flowing narrative, text. But the LLM does not process this flowat each cycle, it operates on single prefix as discrete state. Reading continuously obscures how each prefix functions in the process, which may differ from what the tokens appear to mean as narrative. state is necessarily partial. As state, the tokens need only contain what is required for the next computation to proceed with the overall process. The bulk of each cycles computation does not need to be externalized (see Section 4.1 for further discussion). state is created to be used by its creator. What matters is not what generated token means to human readers, but how the LLM decodes it uses it in subsequent cycles. The LLM can use its own semantics independent of natural language interpretation (see Section 4.2)."
        },
        {
            "title": "4 Two Misconceptions Behind the Illusion of Explanation",
            "content": "To examine the nature of the relation between sequence of states and the computation that produced it, it is instructive to take step back and consider recursive iterative processes where is not an LLM but simpler function. 5 As an example, consider process that computes the Catalan numbers, sequence of natural numbers occurring in various counting problems. The Nth Catalan number is defined recursively by the formula Cn = n1 i=0 CiCn1i with base case C0 = 1. The recursive definition can be translated into an iterative one, in which pure function takes prefix of the sequence and computes the next value according to the recursive definition. When each number is computed, it is added to the state that becomes the input of the next function call. The function takes as input target index and prefix of values (the first Catalan numbers), and returns the (N + 1)-th Catalan number. The computation state in our case is the sequence of already computed numbers. Each computed value is concatenated to the input sequence; the resulting sequence becomes the state for the next call. For example, computing the 6th Catalan number generates the sequence of states: S0 = 6th? S1 = S0 M(S0) = S0 M(6th?) S2 = S1 M(S1) = S1 M(6th?, 1) S3 = S2 M(S2) = S2 M(6th?, 1, 1) S4 = S3 M(S3) = S3 M(6th?, 1, 1, 2) S5 = S4 M(S4) = S4 M(6th?, 1, 1, 2, 5) S6 = S5 M(S5) = S5 M(6th?, 1, 1, 2, 5, 14) = 6th? = 6th?, 1 = 6th?, 1, 1 = 6th?, 1, 1, 2 = 6th?, 1, 1, 2, 5 = 6th?, 1, 1, 2, 5, 14 1 1 2 5 14 Note that while the numbers 1,1,2,5 and 14 are all needed for computing 42, and were used in the process of computing 42, we intuitively do not consider them as an explanation for how 42 was computed. Indeed it actually seems odd for us to treat them as an explanation. But why would it be different for the intermediate steps of an LLM? This form of alienation is already instructive. But lets now consider two concrete implications. 4.1 The misconception of completeness It should be clear to any reader that while the intermediate sequence 1, 1, 2, 5, 14 is crucial for computing 42, this sequence results from the computation and allows it to continue, but it is not the computation itself. The sequence of values does not equal the computation that took place for creating them. Additionally, this sequence does not determine computation: we cannot infer the computation from the sequence (we can guess but cannot be sure), and it is clear that the computation steps that were involved in computing each number are not reflected in the output sequence. We see results of intermediate computations, not the algorithm that is used for producing them. We can also imagine function that is only called 3 times and not 6, with the intermediate values 1,2,5. This function computes two following numbers at time, and only outputs the second one. In each invocation, it recomputes the missing numbers, and continues the sequence. Even if we do choose to treat the steps as an explanation, there is no guarantee that this explanation is complete. The SoT functions as scaffolding that propels the process forward; misconception arises when we mistake this scaffolding for the building. 4.2 The misconception of shared meaning Even if we accept that the state shows only partial results, we might assume the LLM interprets them as humans do. However, this assumption is also mistaken. Consider an alternative function for computing the N-th Catalan number, whose final state is not 1, 1, 2, 5, 14, 42 but rather 11, 11, 12, 15, 24, 52. Such function could be just as effective: at each step, will subtract 10 from each of its inputs, perform the computation, add 10 to the result, and return it as output. At the final (N-th) stage, it will subtract 10 from the last item and report it as the answer. 6 This simple additive transformation demonstrates that the reasoning tokens in the SoTthe very symbols we read and interpretmay function in way entirely different from what human readers naturally ascribe to them. Even elementary encoding schemes can render the surface meaning of text opaque to human interpretation while remaining fully usable for the LLM. Beyond numerical encoding. This encoding arbitrariness extends beyond numerical computation to the full range of SoT. When an LLM writes need to reconsider this approach, the phrase might function not as genuine meta-cognitive reflection, but as an encoding for something else. Given empirical findings on how models can embed functionally relevant information in ways that remain opaque to human readers (Cloud et al., 2025), it is likely that real-world encodings are far more intricate than simple additive shift. This view, together with Section 4.1, may also give intuition to the phenomenon where LLMs succeed at evading mentioning relevant topics in their reasoning text (Emmons et al., 2025; Li et al., 2025)."
        },
        {
            "title": "5 The Ontological Divergence: Text vs. State",
            "content": "We argue that the divergence between form and function in reasoning LLMsthat the same tokens can be interpreted (by humans) as English text while at the same time also interpreted (by the LLM) as an encoding of computation state for an iterative functionis novel, and hence foreign phenomenon in human experience. While we are accustomed to symbols that can be interpreted in multiple ways (the word apple can refer to either fruit or company, the same sequence of bytes can be interpreted as either floating point number or memory address), the State over Tokens case is fundamentally different. The reasoning tokens go beyond admitting multiple interpretations; they inhabit distinct ontological categories. To human reader, the sequence is text, parsed according to linguistic conventions to extract semantic meaning. To the model, however, the same tokens serve as computational substrate, accumulated one token at time to mechanically drive the next step of process. These are not different perspectives on the same underlying content, but completely different kinds of entitiesa communicative medium and functional statethat happen to share the same embodiment. We believe this is how the tokens can function as computational substrate without becoming about the computation in way that human readers can discern by reading. The LLM uses sequence of tokens that can be interpreted by humans according to one semantic system, while at the same time it is interpreted by the LLM using an alternative semantic system."
        },
        {
            "title": "6.1 What SoT means for interpretability",
            "content": "Rather than solving the challenges that existing interpretability work addressessuch as understanding model internals, tracing how features emerge, or identifying mechanisms (Alain & Bengio, 2016; Li et al., 2023; Bereska & Gavves, 2024)the SoT view surfaces distinct, new challenge: investigating how LLMs construct and use computational state through tokens. We call for recognizing the encoded state as first-class object of study. This approach neither reads the reasoning tokens as ordinary English text nor dismisses them as an inadequate explanation, but rather acknowledges that understanding how LLMs use state requires work to decode the structure and function of the token sequence. The new overarching question becomes How is the computation state encoded in the tokens being maintained and used by the LLM? This shift in perspective opens new questions about the structure and behavior of the encoded state itself: How do LLMs decide what information to externalize at each cycle? What information is actually encoded in the state at different points in the sequence? Do they use consistent encodings across the solution of problem and across different problems? 7 How does information propagate through the sequence? Understanding these properties requires decoding the relationship between internal computation and the externalized token-based state. Recent interpretability work provides starting pointfor instance, studying which parts of the sequence are most critical for the final answer (Bogdan et al., 2025) or analyzing model components activity during different cycles (Dutta et al., 2024; Chen et al., 2025a; Zhao et al., 2025). However, the grand challenge of understanding how LLMs manage state through tokens remains largely open. 6.2 Is language special for SoT? The SoT framework also raises the question of the arbitrariness of the medium for discrete, token-based computation. We previously argued that SoT can, in principle, encode arbitrary computation regardless of its surface semantics. But to what extent is this actually the case? Are all media equally expressive, or is using natural language sequences particularly well-suited for encoding the kinds of computations that LLMs perform? One can hypothesize that the massive pretraining stage on natural language data induces an inductive bias to reason (Venhoff et al., 2025) in ways that are in accordance with its semantics, which makes the reasoning text read as plausible explanation. Under this view, elaborate encoding schemes may either contradict the training distributionwhere SoT updates often gradually lead to the correct continuationor require additional computation that is disfavored given the pretraining data. If this hypothesis is correct, LLMs possess at least an inductive bias to represent state in manner that reflects their internal computation. In this context, recent works have explored alternative media for state representation, ranging from simple vectors (Hao et al., 2025; Butt et al., 2025; Hwang et al., 2025) to structured formats (Domingos, 2025). However, the question of whether natural language offers unique advantages over these alternatives remains open. 6.3 Can SoT ever be faithful explanation? Finally, the SoT view sharpens the question of whether the faithfulness of reasoning tokens can be improved. If we aim to make the text faithful explanation, there will be fundamental tension: the reasoning tokens will have to serve two distinct masters. Functionally, as state, the tokens are optimized to encode the information necessary to drive the next computation cycles. Interpretively, as an explanation, we want them to also describe the computation itselfwhether they describe the computation that has already occurred in past cycles, or the computation that will occur in future cycles. This dual requirement creates bottleneck. The tokens are the only medium available for both the computation and its description. Requiring an LLM to explain its own computation while simultaneously performing it amounts to form of metacognitive capability; the LLM must reason about its own reasoning process within the same sequence that sustains it. For the explanation to be faithful, the information the model encodes to solve the problem (the state) must be identical to the information human extracts by reading the text (the explanation). However, an optimal computational state might require encoding information that is redundant, non-linear, or semantically opaque to human reader. Conversely, constraining the state to be coherent, linear English narrative might strip it of the information needed for the reasoning process. Thus, the challenge is not merely whether LLMs can be trained to produce plausible explanations, but whether the medium of natural language tokens has the capacity to simultaneously function as an efficient computational substrate and transparent descriptive record. This forces us to confront fundamental question: can the same sequence of symbols simultaneously carry the full weight of machines computation and transparently reveal the logic of that process to humans? 8 The question extends also to viewing reasoning texts as rationalizations: texts that consist of sequence of steps leading to the answer, which can be used by human reader to verify it. To what extent can sequence be used concurrently both as carrying state and as providing an effective rationalization?"
        },
        {
            "title": "7 Conclusion",
            "content": "The reasoning tokens generated by LLMs before an answer are best understood not as narrative of thought, but as an externalized computational state, role captured by the term State over Tokens (SoT). Adopting this perspective resolves the persistent misconceptions that underlie the illusion of explanation. The Misconception of Completeness is explained by recognizing that SoT externalizes only what is functionally necessary for the next cycle, not full account of the computation that occurred. The Misconception of Shared Meaning is exposed by the insight that the texts human-readable meaning can be incidental to its computational function, which may rely on arbitrary, model-specific encodings. This semantic mismatchwhere the token sequence functions as computational substrate while appearing as natural languagerepresents an unprecedented ontological divergence: two fundamentally incompatible modes of interpretation coexisting within the same textual artifact. While much research has focused on what this text is not, the SoT view attempts to explain what it is. This opens concrete research directionsunderstanding how LLMs construct and use state through tokens, investigating whether natural language possesses unique advantages as computational medium, and exploring whether SoT can ever faithfully explain the underlying computation. Recognizing the reasoning tokens as computational state provides clear basis for calibrating trust and for investigating the new questions this view opens."
        },
        {
            "title": "References",
            "content": "Chirag Agarwal, Sree Harsha Tanneru, and Himabindu Lakkaraju. Faithfulness vs. plausibility: On the (un) reliability of explanations from large language models. arXiv preprint arXiv:2402.04614, 2024. Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. stat, 1050:14, 2016. Ivan Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and Arthur Conmy. Chain-of-thought reasoning in the wild is not always faithful. arXiv preprint arXiv:2503.08679, 2025. Fazl Barez, Tung-Yu Wu, Ivan Arcuschin, Michael Lan, Vincent Wang, Noah Siegel, Nicolas Collignon, Clement Neo, Isabelle Lee, Alasdair Paren, Adel Bibi, Robert Trager, Damiano Fornasiere, John Yan, Yanai Elazar, and Yoshua Bengio. Chain-of-thought is not explainability, 2025. Leonard Bereska and Efstratios Gavves. Mechanistic interpretability for ai safetya review. arXiv preprint arXiv:2404.14082, 2024. Siddhant Bhambri, Upasana Biswas, and Subbarao Kambhampati. Interpretable traces, unexpected outcomes: Investigating the disconnect in trace-based knowledge distillation, 2025. URL https://arxiv.org/abs/2505.13792. Paul C. Bogdan, Uzay Macar, Neel Nanda, and Arthur Conmy. Thought anchors: Which llm reasoning steps matter?, 2025. URL https://arxiv.org/abs/2506.19143. Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, and Yann Ollivier. Soft tokens, hard truths, 2025. URL https://arxiv.org/abs/2509.19170. 9 Xi Chen, Aske Plaat, and Niki van Stein. How does chain of thought think? mechanistic interpretability of chain-of-thought reasoning with sparse autoencoding, 2025a. URL https://arxiv.org/abs/2507.22928. Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, and Ethan Perez. Reasoning models dont always say what they think, 2025b. URL https://arxiv.org/abs/2505.05410. James Chua and Owain Evans. Are deepseek r1 and other reasoning models more faithful?, 2025. URL https://arxiv.org/abs/2501.08156. Alex Cloud, Minh Le, James Chua, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, and Owain Evans. Subliminal learning: Language models transmit behavioral traits via hidden signals in data, 2025. URL https://arxiv.org/abs/2507.14805. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, and Haowei Zhang et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. Pedro Domingos. Tensor logic: The language of ai, 2025. URL https://arxiv.org/abs/ 2510.12269. Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, and Tanmoy Chakraborty. How to think step-by-step: mechanistic understanding of chain-of-thought reasoning, 2024. URL https://arxiv.org/abs/2402.18312. Upol Ehsan and Mark Riedl. Explainability pitfalls: Beyond dark patterns in explainable ai. Patterns, 5(6), 2024. Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat, and Rohin Shah. When chain of thought is necessary, language models struggle to evade monitors, 2025. URL https://arxiv.org/abs/2507.05246. Andrea Ferrario and Michele Loi. How explainability contributes to trust in ai. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 2022. URL https: //api.semanticscholar.org/CorpusID:246458160. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space, 2025. URL https://arxiv.org/abs/2412.06769. Hyeonbin Hwang, Byeongguk Jeon, Seungone Kim, Jiyeon Kim, Hoyeon Chang, Sohee Yang, Seungpil Won, Dohaeng Lee, Youbin Ahn, and Minjoon Seo. Latent reasoning via sentence embedding prediction, 2025. URL https://arxiv.org/abs/2505.22202. Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness? Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 41984205, 2020. URL https://aclanthology.org/2020. acl-main.386/. Alon Jacovi, Ana Marasovic, Tim Miller, and Yoav Goldberg. Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 624635, 2021. Subbarao Kambhampati, Kaya Stechly, Karthik Valmeekam, Lucas Saldyt, Siddhant Bhambri, Vardhan Palod, Atharva Gundawar, Soumya Rani Samineni, Durgesh Kalwar, and Upasana Biswas. Stop anthropomorphizing intermediate tokens as reasoning/thinking traces!, 2025. URL https://arxiv.org/abs/2504.09762. 10 Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Lakoff and Johnson. Metaphors we live by. 1980. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamil Lukoˇsi ut e, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac HatfieldDodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023. URL https://arxiv.org/abs/2307.13702. Mosh Levy, Zohar Elyoseph, and Yoav Goldberg. Humans perceive wrong narratives from ai reasoning texts, 2025. URL https://arxiv.org/abs/2508.16599. Chloe Li, Mary Phuong, and Noah Siegel. Llms can covertly sandbag on capability evaluations against chain-of-thought monitoring. arXiv preprint arXiv:2508.00943, 2025. Kenneth Li, Aspen Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring sequence model trained on synthetic task. ICLR, 2023. Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 1, 2024. Jack Lindsey, Wes Gurnee, Emmanuel Ameisen, Brian Chen, Adam Pearce, Nicholas L. Turner, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. On the biology of large language model. Transformer Circuits Thread, 2025. URL https://transformer-circuits.pub/2025/attribution-graphs/biology.html. Arash Marioriyad, Shaygan Adim, Nima Alighardashi, Mahdieh Soleymani Banghshah, and Mohammad Hossein Rohban. Unspoken hints: Accuracy without acknowledgement in llm reasoning, 2025. URL https://arxiv.org/abs/2509.26041. Nestor Maslej, Loredana Fattorini, Raymond Perrault, Yolanda Gil, Vanessa Parli, Njenga Kariuki, Emily Capstick, Anka Reuel, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, Russell Wald, Tobi Walsh, Armin Hamrah, Lapo Santarlasci, Julia Betts Lotufo, Alexandra Rome, Andrew Shi, and Sukrut Oak. Artificial intelligence index report 2025, 2025. URL https://arxiv. org/abs/2504.07139. William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. International Conference on Learning Representations 2024, 2024. William Merrill, Ashish Sabharwal, and Noah Smith. Saturated transformers are constantdepth threshold circuits. Transactions of the Association for Computational Linguistics, 10: 843856, 2022. Melanie Mitchell. Artificial intelligence learns to reason. Science, 387(6740):eadw5211, 2025. doi: 10.1126/science.adw5211. URL https://www.science.org/doi/abs/10.1126/ science.adw5211. 11 Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114. OpenAI. Openai o1 system card, openai-o1-system-card/. Accessed: 2024. 2024. URL https://openai.com/index/ Debjit Paul, Robert West, Antoine Bosselut, and Boi Faltings. Making reasoning matter: Measuring and improving faithfulness of chain-of-thought reasoning, 2024. URL https: //arxiv.org/abs/2402.13950. Elija Perrier. Typed chain-of-thought: curry-howard framework for verifying llm reasoning, 2025. URL https://arxiv.org/abs/2510.01069. Jacob Pfau, William Merrill, and Samuel R. Bowman. Lets think dot by dot: Hidden computation in transformer language models, 2024. URL https://arxiv.org/abs/2404. 15758. Rikard Rosenbacke, Asa Melhus, Martin McKee, and David Stuckler. How explainable artificial intelligence can increase or decrease clinicians trust in ai applications in health care: systematic review. Jmir Ai, 3:e53207, 2024. Advait Sarkar. Large language models cannot explain themselves. arXiv preprint arXiv:2405.04382, 2024. Joey Skaf, Luis Ibanez-Lissen, Robert McCarthy, Connor Watts, Vasil Georgiv, Hannes Whittingham, Lorena Gonzalez-Manzano, David Lindner, Cameron Tice, Edward James Young, and Puria Radmard. Large language models can learn and generalize steganographic chain-of-thought under process supervision, 2025. URL https://arxiv.org/abs/ 2506.01926. Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, and Subbarao Kambhampati. Beyond semantics: The unreasonable effectiveness of reasonless intermediate tokens, 2025. URL https://arxiv.org/abs/2505.13775. Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models dont always say what they think: Unfaithful explanations in chain-of-thought prompting. In Advances in Neural Information Processing Systems, volume 36, 2023. URL https://arxiv. org/abs/2305.04388. Paper examining the faithfulness of chain-of-thought explanations in language models. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Constantin Venhoff, Ivan Arcuschin, Philip Torr, Arthur Conmy, and Neel Nanda. Base models know how to reason, thinking models learn when, 2025. URL https://arxiv. org/abs/2510.07364. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, and Denny et al. Zhou. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Evelyn Yee, Alice Li, Chenyu Tang, Yeon Ho Jung, Ramamohan Paturi, and Leon Bergen. Dissociation of faithful and unfaithful reasoning in llms. arXiv preprint arXiv:2405.15092, 2024. 12 E. Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In Proceedings of the Conference, 2022. URL https://api.semanticscholar.org/ CorpusID:247762790. Zheng Zhao, Yeskendir Koishekenov, Xianjun Yang, Naila Murray, and Nicola Cancedda. Verifying chain-of-thought reasoning via its computational graph, 2025. URL https: //arxiv.org/abs/2510.09312. Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy, Roland S. Zimmermann, and David Lindner. Early signs of steganographic capabilities in frontier llms, 2025. URL https://arxiv.org/abs/2507.02737."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "Bar-Ilan University",
        "New York University",
        "University of Haifa"
    ]
}