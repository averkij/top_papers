{
    "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
    "authors": [
        "Ziru Chen",
        "Shijie Chen",
        "Yuting Ning",
        "Qianheng Zhang",
        "Boshi Wang",
        "Botao Yu",
        "Yifei Li",
        "Zeyi Liao",
        "Chen Wei",
        "Zitong Lu",
        "Vishal Dey",
        "Mingyi Xue",
        "Frazier N. Baker",
        "Benjamin Burns",
        "Daniel Adu-Ampratwum",
        "Xuhui Huang",
        "Xia Ning",
        "Song Gao",
        "Yu Su",
        "Huan Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1 with direct prompting and self-debug, which demonstrates the effectiveness of increasing inference-time compute. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 2 0 8 0 5 0 . 0 1 4 2 : r Preprint. Under review. SCIENCEAGENTBENCH: TOWARD RIGOROUS ASSESSMENT OF LANGUAGE AGENTS FOR DATA-DRIVEN SCIENTIFIC DISCOVERY Ziru Chen1, Shijie Chen1, Yuting Ning1, Qianheng Zhang3, Boshi Wang1, Botao Yu1, Yifei Li1, Zeyi Liao1, Chen Wei3, Zitong Lu4, Vishal Dey1, Mingyi Xue5, Frazier N. Baker1,6, Benjamin Burns1, Daniel Adu-Ampratwum2, Xuhui Huang5, Xia Ning1,2,6, Song Gao3, Yu Su1, Huan Sun1 1Department of Computer Science and Engineering, OSU 2College of Pharmacy, OSU 3Department of Geography, UWMadison 5Department of Chemistry, UWMadison Website: https://osu-nlp-group.github.io/ScienceAgentBench/ 4Department of Psychology, OSU 6Department of Biomedical Informatics, OSU"
        },
        {
            "title": "ABSTRACT",
            "content": "The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1 with direct prompting and self-debug, which demonstrates the effectiveness of increasing inference-time compute. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have shown remarkable capabilities beyond text generation, including reasoning (Wei et al., 2022; Yao et al., 2023), tool learning (Schick et al., 2023; Wang et al., 2024a), and code generation (Chen et al., 2021; Yang et al., 2024a). These abilities have piqued significant research interests in developing LLM-based language agents to automate scientific discovery end-to-end. For instance, Majumder et al. (2024a) urge the community to build automated systems for end-to-end data-driven discovery, an increasingly important workflow in many disciplines (Hey et al., 2009) that leverages existing datasets to derive new findings. More recently, Lu et al. (2024) claim to have built The AI Scientist, an agent that is capable of automating the entire research workflow, from generating ideas to running experiments and writing papers. This ambitious claim has sparked both excitement and skepticism about the true capabilities of such agents. Correspondence to: {chen.8336,chen.10216,sun.397}@osu.edu 1 Preprint. Under review. Figure 1: Top: Distribution of sub-tasks in ScienceAgentBench. Each task in our benchmark consists of one or more of these sub-tasks and requires successful completion of all sub-tasks to achieve the task goal. Bottom: Heterogeneous datasets involved: (a) cell image in Bioinformatics, (b) molecular activity visualization in Computational Chemistry, (c) flooding risk map in Geographical Information Science, and (d) an EEG time series in Psychology and Cognitive Neuroscience. In this work, we contend that for language agent to fully automate data-driven discovery, it must be able to complete all essential tasks in the workflow, such as model development, data analysis, and visualization. Thus, we advocate careful evaluations of the agents performance on these tasks, before claiming they can automate data-driven discovery end-to-end. Such an assessment strategy helps grasp more solid understanding of an agents strengths and limitations than purely relying on end-to-end evaluations, e.g., using an LLM-based reviewer to assess generated papers (Lu et al., 2024). Yet, high-quality benchmarks focusing on individual tasks in real-world scientific workflows are lacking for objective assessment and continued development of agents for data-driven discovery. To this end, we present ScienceAgentBench, new benchmark for evaluating language agents for data-driven discovery. The construction of ScienceAgentBench follows three key design principles. (1) Scientific authenticity through co-design with subject matter experts: We ensure the authenticity of tasks in our benchmark by directly extracting them from peer-reviewed publications and engaging nine subject matter experts (incl. senior Ph.D. students and professors) from the respective disciplines to validate them. This approach also minimizes the generalization gap for agents developed on our benchmark to real-world scenarios. In total, we curate 102 diverse tasks from 44 peer-reviewed publications in four disciplines: Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology & Cognitive Neuroscience (Figure 1). (2) Rigorous graded evaluation: Reliable evaluation for language agents is notably difficult due to the openendedness and complexity of data-driven discovery tasks. We first unify the target output for every task as self-contained Python program, and then employ an array of evaluation metrics that examine the generated programs, execution results (e.g., rendered figures or test set predictions), and costs. We also provide step-by-step rubrics specific to each task to enable graded evaluation. (3) Careful multi-stage quality control: Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns due to LLM pre-training. We comprehensively evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct (Wang et al., 2024c), and self-debug. In addition, we evaluate OpenAI o1 with direct prompting and self-debug. Since OpenAI o1 generates additional reasoning tokens for extensitve inference-time compute, we only report its performances as references and focus on analyzing other LLMs: Surprisingly, without expert-provided knowledge, Claude-3.5Sonnet using self-debug can successfully solve 10.8% more tasks than using OpenHands CodeAct while costing 17 times less API fees. This result resonates with recent findings that agent designs should jointly consider costs and performance to maximize their practical utility (Kapoor et al., 2024). Still, given three attempts for each task, the best agent can only solve 32.4% of the tasks independently and 34.3% of them with expert-provided knowledge. These results also suggest language agents cannot yet automate essential tasks in data-driven discovery nor the research pipelines end-to-end, in contrast to claims in recent work such as Lu et al. (2024). 2 Preprint. Under review. Figure 2: An example Computational Chemistry task in ScienceAgentBench with four components. Despite their current mediocre performance, we believe language agents hold significant potential in augmenting human scientists productivity: For each task in our benchmark, it takes trained annotator at least 2.53 hours on average to adapt an existing program from public sources, and potentially much longer for subject matter scientist to write the program from scratch. In contrast, language agent can usually generate meaningful program draft within 10 minutes. In the long run, ScienceAgentBench will serve as benchmark for rigorously measuring progress toward developing language agents to assist scientists in data-driven scientific discovery."
        },
        {
            "title": "2 SCIENCEAGENTBENCH",
            "content": "In this section, we introduce ScienceAgentBench, which aims to evaluate agents on essential tasks in data-driven discovery workflow. Before automating the entire workflow end-to-end, we envision language agents to first serve as science co-pilots that can write code to process, analyze, and visualize data. Similar to co-pilots for software development, we target scientist users who might know how to write such code but want to save hours of programming effort with language agents. Hence, we formulate each task as code generation problem, whose output is easily verifiable and directly usable by scientist without additional modification efforts."
        },
        {
            "title": "2.1 PROBLEM FORMULATION",
            "content": "Given natural language instruction, dataset, and some optional expert-provided knowledge, an agent shall generate program to complete the assigned task and save it to Python source code file. Each instance in our benchmark contains four components (Figure 2): (a) Task Instruction, which describes the goal of an essential task in data-driven discovery and its output requirements. To resemble real-world settings, we keep the instructions concise and avoid unnecessary details when describing task goals. This setup also retains the open-endedness of datadriven discovery and encourages the development of practical agents that do not rely on prescriptive directions from scientists. We provide example task instructions in Appendix for each discipline. (b) Dataset Information, which contains the datasets directory structure and preview of its content. For agents without file navigation tools, they need such information to correctly use the dataset in their generated programs. For agents that can navigate file systems, it also helps them save few turns of interactions to read datasets from the programming environment. (c) Expert-Provided Knowledge, which includes explanations for scientific terms, formulas to conduct analysis, and example usages of programming tools. These pieces of knowledge are provided by subject matter experts, including senior Ph.D. students and professors, and are optional inputs to 3 Preprint. Under review. an agent. In Section 4, we show that while with such information, language agents knowledge gap in involved disciplines can be mitigated to some extent, they still fall short utilizing it effectively. (d) Annotated Program, which is adapted from an open-source code repository released by peerreviewed scientific publication. As shown in Figure 2, each program is self-contained with package imports, function and class implementations, and main procedure to carry out the task. An agent is expected to produce similar programs that can be executed independently, e.g. by Python interpreter, but not necessarily using the same tools as those in the annotated programs."
        },
        {
            "title": "2.2 DATA COLLECTION",
            "content": "Task Annotation. We start by forming group of nine graduate students to annotate the tasks in four disciplines: Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology & Cognitive Neuroscience. Within each discipline, we search for peer-reviewed publications that release their code and data under permissive licenses (Appendix I). Then, we follow five steps to annotate each task: (1) Identify reasonably documented code example that is selfcontained and convert it into task in our benchmark. (2) Collect and preprocess datasets used in the code. (3) Annotate the reference program by revising the referred code to analyze datasets in our benchmark. (4) Implement task-specific success criteria as an executable script and use GPT-4o to draft fine-grained rubrics for evaluation. (5) Write the instruction and dataset information for this task. We gathered 110 tasks initially but discarded four because their programs require long execution time or nontrival environment setup. This leaves us with 106 tasks for validation. Data Contamination and Shortcut Mitigation. In our preliminary studies, we have noticed that some agents, such as OpenHands, may take shortcuts to solve task. For example, when asked to develop machine learning model, they may directly read and report the ground-truth labels in the test set without writing the training code. Such perfect results are actually cheating and will hurt evaluation validity. In addition, because datasets and programs in our benchmark are open-sourced, they are subject to data contamination in LLM training. To mitigate these issues, we devise two strategies to modify the datasets: (1) For each dataset, we randomly remove five data points from its test set. If an LLM-generated program uses automatic data loaders that appeared in the training corpora, it will produce results misaligned to our setup and fail the success criteria. In some cases, we have to skip this step if it would break the completeness of dataset, e.g., if it results in an incomplete geographical map. (2) For tasks involving model development, we re-split the dataset, keep the test set labels only for evaluation, and replace them with dummy values, such as -1 for classification tasks. These two strategies effectively mitigate data contamination and agent shortcut concerns by failing agents that recite memorized code or attempt to directly report test set labels. See Appendix E.2: Example E.4 for case study. Expert Validation. We engage nine subject matter experts, including senior Ph.D. students and professors from the four involved disciplines, to validate each task and provide additional knowledge. For each task, we present to experts with its instruction, dataset information, annotated program, and task rubrics. The experts are asked to validate the tasks by completing questionnaire (Appendix F), which can be summarized as four steps: (1) Validate if an annotated task represents realistic task in their data-driven discovery workflow. (2) Review whether task instruction gives an accurate high-level description of the program and uses professional languages in their disciplines. (3) Provide up to three pieces of knowledge that might be needed for solving each task. (4) Make necessary revisions to the rubrics for grading the program. Then, following the experts feedback, we revise 41 task instructions and remove three tasks that are not representative enough for scientific workflows in their disciplines. With 103 tasks remaining, our publication-oriented annotation strategy is shown to be effective in collecting real-world tasks. Annotator Verification. To ensure data quality, we work with the nine annotators for another round of task verification. We ask the annotators to verify tasks that are not composed by themselves and execute programs to reproduce the results. During this process, we refine 29 task annotations and discard one more task whose result is hard to replicate with the same program due to randomness. We finalize ScienceAgentBench with 102 high-quality tasks for data-driven scientific discovery."
        },
        {
            "title": "2.3 EVALUATION",
            "content": "While it is preferable feature, the open-endedness of tasks in our benchmark introduces crucial evaluation challenge. Specifically, our evaluation strategy has to accommodate diverse setup 4 Preprint. Under review. Table 1: Representative examples of task-specific success criteria in ScienceAgentBench. To keep the table concise, we omit output requirements in the task instructions and show the task goals."
        },
        {
            "title": "Success Criteria",
            "content": "Train multitask model on the Clintox dataset to predict drugs toxicity and FDA approval status. Develop drug-target interaction model with the DAVIS dataset to repurpose the antiviral drugs for COVID. Analyze the inertial measurement unit (IMU) data collected during sleep and compute sleep endpoints: time of falling asleep, time of awakening, and total duration spent sleeping. Analyze Toronto fire stations and their service coverage. Visualize the results to identify coverage gaps."
        },
        {
            "title": "Feature Engineering\nDeep Learning",
            "content": "The trained model gets 0.77 ROC-AUC score on the test set."
        },
        {
            "title": "Feature Engineering\nDeep Learning",
            "content": "The top-5 repurposed drugs match the gold top-5 drugs."
        },
        {
            "title": "Computational\nAnalysis",
            "content": "Each computed endpoint is close (math.isclose in Python) to the corresponding gold answer."
        },
        {
            "title": "Map Visualization",
            "content": "The resulting figure gets 60 score by the GPT-4o Judge. requirements of programs generated by different agents. To address this challenge, we implement pipeline to set up conda environment flexibly for any program. Before evaluation, the conda environment is initialized with seven basic Python packages: numpy, pandas, matplotlib, pytorch, tensorflow, rdkit, and tf keras. To evaluate each program, we first use pipreqs1 to analyze it and generate file listing all packages used. Then, according to the file, we use pip-tools2 and handcrafted rules to update the conda environment and properly configure the packages. We execute each program in the customized environment and calculate the evaluation metrics. Program Evaluation. We comprehensively evaluate each generated program with four metrics. (1) Valid Execution Rate (VER) checks if the program can execute without errors and save its output with the correct file name. (2) Success Rate (SR) examines whether program output meets the success criteria for each task goal  (Table 1)  , such as test set performance, prediction-answer matches, and visualization quality. To automatically check these criteria, we implement them as evaluation programs for each task during annotation. By nature, SR is conditioned on valid execution: If program has execution errors or does not save its output correctly, its SR will be 0. Both VER and SR are binary metrics. (3) CodeBERTScore (CBS) measures how closely the generated program resembles the annotated one with contextual embeddings and calculates the F1 metric for matched token embeddings (Zhou et al., 2023). If SR = 1 for program, we change its CBS to 1.0 as well to reflect task success. (4) API Cost (Cost) calculates the average cost (in USD) to complete one task in our benchmark, since it is important for language agents to control their cost and optimize their design for better practical utility (Kapoor et al., 2024). Figure Evaluation. If the task output is figure, we follow existing work (Wu et al., 2024; Yang et al., 2024b) to evaluate its quality using GPT-4o as judge, which is shown to correlate reasonably well with human raters. We use Yang et al. (2024b)s prompt to request GPT-4o to compare the program-produced figure with the ground-truth and respond with score on its quality. For evaluation stability, we sample 3 responses and use the average score to compute success rates. Rubric-Based Evaluation. Outcome-based evaluation metrics, which require program to correctly implement all steps for the task, can sometimes be too stringent. For example, an agent would be underrated by these metrics if it gets all steps right but output formatting wrong. As complement to the outcome-based metrics, we introduce rubric-based evaluation to assess the generated programs at more fine-grained levels. Considering the characteristics of data-driven discovery tasks, we structure the rubrics into five stages: Data Loading, Data Processing, Modeling or Visualization, Output formatting, and Output Saving. To accelerate the annotation process, we first use GPT-4o to generate the rubrics by designating multiple milestones with scores for the five stages. Then, each rubric is refined by an expert (Appendix G). In this work, we leverage the rubrics to conduct 1https://github.com/bndr/pipreqs 2https://github.com/jazzband/pip-tools 5 Preprint. Under review. Table 2: Comparison of ScienceAgentBench to representative benchmarks. DiscoveryBench-Real is evaluating the quality of generated programs indirectly through the natural language hypothesis, while ScienceAgentBenchs focus is to rigorously assess the programs and their execution results. Benchmark TaskBench (Shen et al., 2024) SWE-Bench (Jimenez et al., 2024) BioCoder-Py (Tang et al., 2024b) ML-Bench (Tang et al., 2024a) MLAgentBench (Huang et al., 2024b) DiscoveryBench-Real (Majumder et al., 2024b) SciCode (Tian et al., 2024) BLADE (Gu et al., 2024) Code Gen Complexity No Code Gen File-Level Edit Function-Level Line-Level File-Level Edit Task Sources Synthetic GitHub GitHub GitHub Kaggle Code Gen 27 Publications Function-Level Function-Level Publications 31 Publications ScienceAgentBench (Ours) File-Level Gen 44 Publications Heterogeneous Data Processing Shortcut Prevention Scientific Subjects 0 1 1 1 1 5 6 4 # Test Tasks 28,271 2,294 1,126 260 13 239 80 102 human evaluation for generated programs (Section 4.2). We deem that automating this rubric-based evaluation approach, such as developing an LLM-based judge, is meaningful future direction."
        },
        {
            "title": "2.4 COMPARISON WITH EXISTING BENCHMARKS",
            "content": "ScienceAgentBench differs from other benchmarks with unique ensemble of research challenges  (Table 2)  . (1)Tasks in our benchmark require an agent to generate standalone program file from scratch, in contrast to JSON API calls in TaskBench, abstract workflow descriptions in DiscoveryBench, or few lines of code completion or edits in other benchmarks. To do so, an agent needs to have deep understanding of the task, decompose it into classes and functions appropriately, and implement them. (2) Our benchmark adapts 44 peer-reviewed publications and covers variety of real-world datasets in four different disciplines. Compared to ML-Bench and DiscoveryBench, our ScienceAgentBench includes more heterogeneous datasets that have complex structures (Figure 1), such as cell images, chemical structure-activity relationships, and geographical maps with multiple layers. (3) ScienceAgentBench is also one of the two benchmarks that tries to mitigate data contamination and agent shortcut issues, which helps establish valid evaluation. (4) Our benchmark has medium scale of 102 tasks. Although smaller than benchmarks with synthetic or easier tasks, this scale is reasonable to evaluate agents, considering the annotation difficulty and evaluation cost. We will discuss the limitations of our benchmark and other relate work in Appendix and B."
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "We experiment with three open-weight LLMs, Llama-3.1-Instruct-70B, 405B (Dubey et al., 2024), and Mistral-Large-2 (123B) (MistralAI, 2024), and two proprietary LLMs, GPT-4o (OpenAI, 2024a) and Claude-3.5-Sonnet (Anthropic, 2024). For all experiments, we use the same hyperparameters, temperature = 0.2 and top = 0.95, and perform 0-shot prompting3 via the APIs. In addition, we evaluate OpenAI o14 (OpenAI, 2024b) with its default hyperparameters. The prompts are included in Appendix H. We evaluate the LLMs under three different (agent) frameworks: Direct Prompting. Direct prompting is simple framework that does not interact with any programming environment. Given the task inputs, it prompts an LLM to generate corresponding program in one pass. We use this framework to show the basic code generation capability of each LLM. OpenHands CodeAct. OpenHands5 (Wang et al., 2024c) is generalist agent development framework for code generation and software engineering. It provides three kinds of tools in the environment and defines different actions for an agent to interact with the tools: Python code interpreter, bash shell, and web browser. Among the agents developed with OpenHands, we use the bestperforming CodeActAgent v1.9 (Wang et al., 2024b) in our experiments. This agent unifies all 3OpenHands has built-in 1-shot example to demonstrate response formats, tool usages, and other plugins like web browser. We do not provide any examples from our benchmark when evaluating OpenHands. 4As of 10/24/2024, OpenAI o1 (o1-preview-2024-09-12) does not allow hyperparameter changes and is not yet compatible with OpenHands, so we only evaluate it with direct prompting and self-debug. 5https://github.com/All-Hands-AI/OpenHands, originally named as OpenDevin. 6 Preprint. Under review. actions in OpenHands, including the agent-computer interface commands (Yang et al., 2024a) to read and edit local files, into large action space of different Python API calls. We experiment with the CodeActAgent v1.9 using different LLMs to test the effectiveness of OpenHands framework design for code generation tasks in data-driven discovery. For simplicity, we shorten the name of this agent framework as OpenHands CodeAct. Self-Debug. Self-debug (Chen et al., 2024a) is code generation framework for LLMs to execute their generated programs, access execution results, and then reflect on the results to improve each program iteratively. In this work, we re-implement self-debug with three modifications. First, we do not instruct the LLMs to generate reflections before debugging the code, since self-reflection may not always yield better results (Chen et al., 2024b; Huang et al., 2024a; Jiang et al., 2024). Second, we allow early exits if the backbone LLM generates the same program for two consecutive debugging turns. Finally, before running each program, we use pipreqs and pip-tools to set up the environment. We do not initialize the self-debug environment with any of the basic packages or provide the rules to configure some packages that are used for evaluation (Section 2.3). Even though self-debug might not be able to use some packages due to this design choice, we want to ensure fair comparisons with other baselines, which also have no access to these information. To improve evaluation stability, we repeat each task with three independent runs in all experiments. Then we select the best run according to the metrics in the following order: maximum SR, maximum VER, maximum CBS, and minimum Cost. We refer to the next metric in this order to break ties. For example, if two programs generated for task both have SR = 0, we pick the one with higher VER. Finally, we report each metric based on the average performance of selected runs. We also include the mean performances out of three runs and standard deviations in Appendix D."
        },
        {
            "title": "4 RESULTS AND ANALYSIS",
            "content": "Through comprehensive experiments  (Table 3)  , we show that the latest LLMs and agents can only achieve low-to-moderate task success rates. Although OpenAI o1 shows better performance, we do not compare it with other LLMs due to its additional inference-time reasoning and different hyperparameters. We provide case study on OpenAI o1 in Appendix E.3 and focus on analyzing agents based on other LLMs in this section: Given three attempts for each task, Claude-3.5-Sonnet with self-debug demonstrates the best performance (34.3% SR) when using expert-provided knowledge. This result underline that LLM-based agents are not yet capable of fully addressing realistic and challenging data-driven discovery tasks, such as those in ScienceAgentBench."
        },
        {
            "title": "4.1 MAIN RESULTS",
            "content": "Direct Prompting vs. Self-Debug: Execution feedback is necessary for LLMs to generate useful programs. As shown in Table 3, directly prompting LLMs cannot unleash their full potential in programming for data-driven discovery tasks. Without executing its code, even the best performing LLM, Claude-3.5-Sonnet, can only solve 16.7% of the tasks independently and 20.6% with additional knowledge. For most failed tasks, we share similar findings with Liang et al. (2024) that LLM-generated programs have correct high-level structures but implementation-level errors, such as missing steps or wrong API usage. Compared to direct prompting, self-debug can nearly double Claude-3.5-Sonnets success rate (16.7 32.4; 1.94) without extra knowledge. With expertprovided knowledge, Claude-3.5-Sonnet using self-debug also shows decent improvement over direct prompting. It achieves 13.7 absolute gains on SR (20.6 34.3; 1.67) and 45.1 absolute gains on VER (41.2 86.3; 2.09). These results highlight the effectiveness of the simple self-debug framework and the importance of enabling LLMs to execute and revise their code for complex tasks. OpenHands CodeAct vs. Self-Debug: Agent designs should consider costs and capabilities of LLMs. For four of the five LLMs evaluated, self-debug demonstrates better performance than OpenHands CodeAct, with GPT-4o as the only exception  (Table 3)  . By examining the trajectories, we find that GPT-4o is better at leveraging tools in OpenHands than other LLMs. For instance, it is the only LLM that search for more details about the provided knowledge with the web browser. In contrast, other LLMs are still struggling with specialized bash commands in OpenHands to edit programs correctly (Example in Appendix E.1). We hypothesize that GPT-4o may have been trained to better follow instructions for language agents and to better use complex tools like web browser. 7 Preprint. Under review. Table 3: Results on ScienceAgentBench. The best performances (with and without knowledge) for each framework are in bold. The overall best performances are underlined. OpenAI o1 generates additional reasoning tokens for extensive inference-time compute (with different hyperparameters), so it might be unfair to compare the model with other LLMs, yet we include it for reference."
        },
        {
            "title": "With Knowledge",
            "content": "SR CBS VER Cost SR CBS VER Cost Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o (2024-05-13) Claude-3.5-Sonnet (2024-06-20) OpenAI o1 Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o (2024-05-13) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o (2024-05-13) Claude-3.5-Sonnet (2024-06-20) OpenAI o"
        },
        {
            "title": "Direct Prompting",
            "content": "5.9 3.9 13.7 11.8 17.7 34.3 81.5 79.4 83.2 82.6 83.6 87.1 29.4 35.3 47.1 52.9 51.0 70. 0.001 0.010 0.009 0.011 0.017 0."
        },
        {
            "title": "OpenHands CodeAct",
            "content": "6.9 5.9 9.8 19.6 21.6 13.7 14.7 23.5 22.6 32.4 42.2 63.5 65.8 72.5 83.1 83. 30.4 52.0 53.9 78.4 87.3 Self-Debug 82.7 82.9 85.1 84.4 86.4 88.4 80.4 78.4 83. 83.3 92.2 92.2 0.145 0.383 0.513 0.803 0.958 0.007 0.047 0.034 0.047 0.057 0.636 4.9 2.9 16. 10.8 21.6 31.4 2.9 8.8 13.7 27.5 24.5 16.7 13.7 27.5 23.5 34.3 41.2 82.1 81.3 84. 83.8 85.4 87.4 65.7 71.4 78.8 86.3 85.1 83.4 83.6 86.8 85.6 87.1 88.9 27.5 25.5 39. 41.2 41.2 63.7 25.5 58.8 50.0 73.5 88.2 73.5 79.4 78.4 71.6 86.3 91.2 0.001 0.011 0. 0.012 0.017 0.236 0.252 0.740 0.759 1.094 0.900 0.008 0.055 0.036 0.046 0.061 0.713 When it comes to self-debug, which has more straightforward design, GPT-4o loses its advantage and underperforms Mistral-Large-2 and Claude-3.5-Sonnet, both of which are trained for better code generation according to their reports (MistralAI, 2024; Anthropic, 2024). Most surprisingly, without the help of expert-provided knowledge, Claude-3.5-Sonnet using self-debug can successfully solve 10.8% more tasks (21.6 32.4 SR) than using OpenHands while costing 17 times less API fees ($0.958 $0.057), which is critical factor to consider for practical applications. Overall, our results resonate with recent findings on agent design (Kapoor et al., 2024; Xia et al., 2024): (1) LLM-based agents do not always benefit from large action space with complex tools; and (2) both cost and performance should be considered when designing or selecting agent frameworks. With vs. Without Expert-Provided Knowledge: Expert-provided knowledge does not always lead to metric improvement. On one hand, we observe that expert-provided knowledge leads to consistent improvements on SR and CBS for most agents  (Table 3)  . These agents can effectively leverage helpful information in the knowledge, such as API names and some concrete steps in the task, to generate high-quality program draft that closely resembles the annotated gold program and then use execution feedback to address implementation errors. On the other hand, we notice that there are performance decreases on VER for most agents. These decreases can be attributed to two reasons. (1) Expert-provided knowledge specifies some specific tools that are less familiar to the agents. Originally, they would only use basic tools like rdkit and sklearn in their generated programs, which are free of execution errors. With provided knowledge, the agents would use those specified tools to generate programs, which often contain incorrect API usage and hallucinated API calls. (2) The agents do not know how to solve some tasks without expert-provided knowledge and would generate some executable but less meaningful programs, e.g., to produce an empty figure. While additional knowledge helps them to produce more concrete modeling or analysis, such programs are error-prone and hard to fix with execution feedback (Appendix E.2). For these reasons, despite decreases in VER, we argue that expert-provided knowledge helps agents to generate more useful programs from scientist users perspective, as reflected by SR and CBS, and future AI agents should improve their abilities to better leverage such information. 8 Preprint. Under review. Figure 3: Task performance analysis of Claude-3.5-Sonnet with self-debug and expert-provided knowledge. Left: Distribution of lines in gold programs for succeeded and failed tasks. The red vertical line marks the average length (58.6 lines) of all gold programs in the benchmark. Right: Task error rates for each sub-task category in each discipline. Language agents cannot solve complex data-driven discovery tasks yet. Our further analysis on the best performing agent, Claude-3.5-Sonnet with self-debug and expert-provided knowledge, show that it is not yet capable of addressing complex tasks in data-driven discovery. To estimate the complexity of tasks, we visualize the number of lines in their corresponding gold programs using box plot (Figure 3; Left). More than 75% of succeeded tasks lean to the simpler side because their gold programs have less than 58.6 lines, which is the mean length of all gold programs in the benchmark. In other words, language agents still fail on many tasks with complex gold programs. To understand the task failures, we break them down by different disciplines and sub-task categories (Figure 3; Right). For Bioinformatics and Computational Chemistry, the agent mostly fails on tasks involving data processing and model development. This is because data in these two disciplines are highly heterogeneous, including cell images, molecules, and genes, which can be hard to process. Without correctly processed data, the agent would also not be able to develop and train functioning model, not to mention choosing appropriate configurations for various models such as Convolutional or Graph Neural Networks used in the tasks. For Geographical Information Science and Psychology & Cognitive Neuroscience, their tasks usually require discipline-specific tools, such as Geopandas and Biopsykit, to analyze the datasets. However, existing LLMs fall short of using these tools and can generate incorrect or hallucinated API usage in the programs. Given these shortcomings, we argue that current language agents cannot yet automate data-driven discovery tasks or the full research pipeline, in contrast to claims made in recent work such as Lu et al. (2024)."
        },
        {
            "title": "4.2 HUMAN EVALUATION",
            "content": "Evaluation Setup. To further investigate the performance of Claude-3.5-Sonnet with self-debug (the best-performing agent), we conduct rubric-based human evaluation of all the 102 programs generated using expert-provided knowledge. With the task-specific rubrics validated by experts (examples in Appendix G) and gold programs as references, each generated program is rated by two different evaluators who participated in data collection. To reduce possible noises in ratings, the evaluators only mark whether rubric item is met by the LLM-generated program. For each stage, we add up points for satisfied rubric items and normalize them by total available points to the range of 0100. Similarly, we calculate the overall score considering all items. The final score of each program is the average of two evaluators ratings. Additionally, one purpose of this human evaluation is to assign partial credits to the generated program even if it is not correct (Section 2.3). Therefore, we do not provide the evaluators with program execution results and hide task success outcomes. Although this setup encourages evaluators to examine LLM-generated programs carefully, it also introduces some noise. For example, there are tasks where both feed-forward neural network and random forest model can achieve satisfying performance on the test set. While the gold program implements the neural network, the agent chooses to use random forest. Since each rubric is derived from gold program and reflect its implementation, there are chances that the evaluator overlooks such equivalence. Also, for output formatting, we observe some subjective variance when judging the formats of figures, such as colors, 9 Preprint. Under review. Figure 4: Rubric-based human ratings for 102 programs generated by Claude-3.5-Sonnet with selfdebug and expert-provided knowledge. We show the overall distributions and those for the five stages in our rubrics (Section 2.3). The blue boxes are distributions for failed tasks, and the orange ones are for succeeded tasks. The open dots represent outliers in the distributions. scales, and text labels, according to the rubrics and gold programs. As result, successful programs would not always receive perfect human rating. Results and Analysis. As shown in Figure 4, data loading and processing, the first two stages in data-driven discovery tasks, can distinguish successful programs from failed ones. Except for few outliers, almost all successful programs receive perfect human rating for data loading. In contrast, 25% of the failed programs have their rating below 50 in the first stage. For data processing, the rating distribution of successful programs skews toward the full score, while that of failed programs skews toward score between 20 and 50. These human evaluation results correspond to an intuitive explanation: If the dataset were not loaded or processed correctly, it would be impossible to solve task successfully, regardless of the code implementation for consequent stages. In the third stage, modeling or visualization, human ratings for successful and failed programs are also different: The median score of successful programs is already at the 75th percentile of failed program ratings. This indicates that human evaluators agree with the SR metric and prefer programs passing all success criteria for the task, even though they may have some minor issues. For output formatting and saving, we find no difference between the two groups of programs, indicating that LLMs like Claude-3.5-Sonnet can follow such instructions reasonably well. Overall, human ratings for succeeded and failed programs form two overlapped but distinguishable distributions, which meets our motivation to complement outcome-based metrics with fine-grained evaluation. These ratings agree with our main result and suggest that some LLM-generated programs are close to success but hindered by some bottlenecks, such as data loading and processing. Future research may, for example, improve language agents capability to better process scientific data."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORK",
            "content": "We introduce ScienceAgentBench, new benchmark to evaluate language agents for data-driven scientific discovery. We compile 102 diverse, real-world tasks from 44 peer-reviewed publications across four scientific disciplines and engage nine subject matter experts to ensure data quality. Through comprehensive experiments on five LLMs and three frameworks, we show that the bestperforming agent, Claude-3.5-Sonnet with self-debug, can only solve 34.3% of the tasks when using expert-provided knowledge. Our results and analysis suggest that current language agents cannot yet automate tasks for data-driven discovery or whole research pipeline. By introducing ScienceAgentBench, we advocate the use of language agents to assist human scientists with tedious tasks in their workflows and call for more rigorous assessments of such agents. We plan to continually expand our benchmark into more disciplines and facilitate future research in two ways: (1) ScienceAgentBench will serve as necessary testbed for developing future language agents with stronger capabilities to process scientific data or to utilize expert-provided knowledge. (2) ScienceAgentBench will help future research to design new automatic graded metrics, such as an LLM judge based on task-specific rubrics, to assess language agents for data-driven discovery. 10 Preprint. Under review."
        },
        {
            "title": "AUTHOR CONTRIBUTIONS",
            "content": "Z. Chen led the project, formulated the benchmark, organized data collection and human evaluation, implemented programs and evaluation scripts for experiments, conducted all experiments on GPT-4o/Claude/Mistral, and wrote the manuscript. S. Chen designed and implemented rubric-based evaluation, and helped to run direct prompting, self-debug, and OpenHands experiments on Llama 3.1 70B/405B, optimize the evaluation scripts, and revise the manuscript. Y. Ning helped to run some experiments on OpenHands with Llama 3.1 70B/405B. Z. Chen, S. Chen, Y. Ning, Q. Zhang, B. Wang, B. Yu, Y. Li, Z. Liao, and C. Wei worked as the student annotators to collect the benchmark, verify the tasks, and evaluate generated programs based on rubrics. Z. Lu, V. Dey, M. Xue, F. Baker, B. Burns, D. Adu-Ampratwum, X. Huang, X. Ning, and S. Gao are subject matter experts who validated the tasks and provided task-specific knowledge. In addition, S. Gao provided substantial guidance in Geographical Information Science task collection and constructive feedback on the project during weekly discussions. Y. Su and H. Sun are senior authors who oversaw this project, contributed to the core ideas, and revised the manuscript. H. Sun conceived the research problem."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "The authors would thank colleagues from the OSU NLP group and the NSF-funded ICICLE AI Institute for constructive feedback. This research was sponsored in part by NSF OAC 2112606, Amazon, and Ohio Supercomputer Center (Center, 1987). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein."
        },
        {
            "title": "REFERENCES",
            "content": "Mathijs Andeweg and Tom Kuijpers. 2024. ing, model-how-land-subsidence-affects-flooding/."
        },
        {
            "title": "Model how land subsidence",
            "content": "affects floodhttps://learn.arcgis.com/en/projects/"
        },
        {
            "title": "URL",
            "content": "Anthropic. Claude 3.5 sonnet. claude-3-5-sonnet. Jun 2024. URL https://www.anthropic.com/news/ Gordon Bell, Tony Hey, and Alex Szalay. Beyond the data deluge. Science, 323(5919):12971298, 2009. doi: 10.1126/science.1170411. URL https://www.science.org/doi/abs/10. 1126/science.1170411. Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624:570578, 2023. doi: https://doi.org/10.1038/ s41586-023-06792-0. Cedric Bouysset and Sebastien Fiorucci. ProLIF: library to encode molecular interactions as fingerprints. Journal of Cheminformatics, 13(1):72, 2021. doi: 10.1186/s13321-021-00548-6. Daniel Brand, Nicolas Riesterer, Hannah Dames, and Marco Ragni. Analyzing the differences in human reasoning viajoint nonnegative matrix factorization. Proceedings of the Annual Meeting of the Cognitive Science Society, 42, 2020. URL https://escholarship.org/uc/item/ 0br9k22g. Danila Bredikhin, Ilia Kats, and Oliver Stegle. Muon: multimodal omics analysis framework. Genome Biology, 23(1), 2022. doi: 10.1186/s13059-021-02577-8. O. J. M. Bequignon, B. J. Bongers, W. Jespers, A. P. IJzerman, B. van der Water, and G. J. P. van Westen. Cellprofiler: image analysis software for identifying and quantifying cell phenotypes. Journal of Cheminformatics, 15(3), 2023. doi: 10.1186/s13321-022-00672-x. Longbing Cao. Data science: comprehensive overview. ACM Comput. Surv., 50(3), jun 2017. ISSN 0360-0300. doi: 10.1145/3076253. URL https://doi.org/10.1145/3076253. 11 Preprint. Under review. Anne E. Carpenter, Thouis R. Jones, Michael R. Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A. Guertin, Joo Han Chang, Robert A. Lindquist, Jason Moffat, Polina Golland, and David M. Sabatini. Cellprofiler: image analysis software for identifying and quantifying cell phenotypes. Genome Biology, 7(R100), 2006. doi: 10.1186/gb-2006-7-10-r100. Ohio Supercomputer Center. Ohio supercomputer center, 1987. URL http://osc.edu/ark: /19495/f5s1ph73. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=KuPixIqPiq. Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search useful for LLM planning? it depends on the discriminator. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1365913678, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.acl-long.738. Simone Ciuti, Tyler B. Muhlym, Dale G. Paton, Allan D. McDevitt, Marco Musiani, and Mark S. Boyce. Human selection of elk behavioural traits in landscape of fear. Proceedings of the Royal Society B, 279:44074416, 2012. doi: 10.1098/rspb.2012.1483. Andrew Dawson. eofs: library for eof analysis of meteorological, oceanographic, and climate data. Journal of Open Research Software, 4(1), 2016. doi: https://doi.org/10.5334/jors.122. Pierre-Paul De Breuck, Matthew Evans, and Gian-Marco Rignanese. Robust model benchmarking and bias-imbalance in data-driven materials science: case study on modnet. Journal of Physics: Condensed Matter, 33(40):404002, jul 2021a. doi: 10.1088/1361-648X/ac1280. URL https: //dx.doi.org/10.1088/1361-648X/ac1280. Pierre-Paul De Breuck, Geoffroy Hautier, and Gian-Marco Rignanese. Materials property prediction for limited datasets enabled by feature selection and joint learning with modnet. npj Computational Materials, 7(1), 2021b. doi: 10.1038/s41524-021-00552-2. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and In Thirty-seventh Conference on Yu Su. Mind2web: Towards generalist agent for the web. Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=kiYqbO3wqw. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael 12 Preprint. Under review. Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mo13 Preprint. Under review. hammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. ESRI."
        },
        {
            "title": "Predict",
            "content": "deforestation 2024a. predict-deforestation-in-the-amazon-rain-forest/."
        },
        {
            "title": "URL",
            "content": "in January https://learn.arcgis.com/en/projects/ amazon forest, rain the ESRI. Assess access to public transit, February 2024b. URL https://learn.arcgis.com/ en/projects/assess-access-to-public-transit/. ESRI. gust build-a-model-to-connect-mountain-lion-habitat/. to Auhttps://learn.arcgis.com/en/projects/ model URL mountain connect habitat, 2024c."
        },
        {
            "title": "Build",
            "content": "lion ESRI. Assess burn scars with satellite imagery, June 2024d. URL https://learn.arcgis. com/en/projects/assess-burn-scars-with-satellite-imagery/. Sunny Fleming. Model animal home range, March 2024. URL https://learn.arcgis. com/en/projects/model-animal-home-range/. Adam Gayoso, Romain Lopez, Galen Xing, Pierre Boyeau, Valeh Valiollah Pour Amiri, Justin Hong, Katherine Wu, Michael Jayasuriya, Edouard Mehlman, Maxime Langevin, Yining Liu, Jules Samaran, Gabriel Misrachi, Achille Nazaret, Oscar Clivio, Chenling Xu, Tal Ashuach, Mariano Gabitto, Mohammad Lotfollahi, Valentine Svensson, Eduardo da Veiga Beltrame, Vitalii Kleshchevnikov, Carlos Talavera-Lopez, Lior Pachter, Fabian J. Theis, Aaron Streets, Michael I. Jordan, Jeffrey Regier, and Nir Yosef. python library for probabilistic analysis of single-cell omics data. Nature Biotechnology, 40:163166, 2022. doi: 10.1038/s41587-021-01206-w. David E. Graff, Eugene I. Shakhnovich, and Connor W. Coley. Accelerating high-throughput virtual screening through molecular pool-based active learning. Chemical Science, 12(22):78667881, 2021. doi: 10.1039/D0SC06805E. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, Tianmai M. Zhang, Lanyi Zhu, Mike A. Merrill, Jeffrey Heer, and Tim Althoff. Blade: Benchmarking language model agents for data-driven science, 2024. URL https://arxiv.org/abs/2408.09667. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. WebVoyager: Building an end-to-end web agent with large multimodal modIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Anels. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14 Preprint. Under review. 68646890, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.371. Tony Hey, Stewart Tansley, Kristin Tolle, and Jim Gray. The Fourth Paradigm: DataISBN 978-0-9825442URL https://www.microsoft.com/en-us/research/publication/ Intensive Scientific Discovery. Microsoft Research, October 2009. 0-4. fourth-paradigm-data-intensive-scientific-discovery/. Tom Hourigan. NOAA Deep Sea Corals Research and Technology Program, 1 2023. URL https: //www.gbif.org/dataset/df8e3fb8-3da7-4104-a866-748f6da20a3c. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, In The Twelfth and Denny Zhou. Large language models cannot self-correct reasoning yet. International Conference on Learning Representations, 2024a. URL https://openreview. net/forum?id=IkmD3fKBPQ. Kexin Huang, Tianfan Fu, Lucas Glass, Marinka Zitnik, Cao Xiao, and Jimeng Sun. Deeppurpose: deep learning library for drug-target interaction prediction. Bioinformatics, 2020. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, 2024b. URL https://openreview.net/forum?id=1Fs1LvjYQW. Ryan Jacobs, Tam Mayeshiba, Ben Afflerbach, Luke Miles, Max Williams, Matthew Turner, Raphael Finkel, and Dane Morgan. The materials simulation toolkit for machine learning (mastml): An automated open source toolkit to accelerate data-driven materials research. Computational Materials Science, 176:109544, 2020. ISSN 0927-0256. doi: https://doi.org/10. 1016/j.commatsci.2020.109544. URL https://www.sciencedirect.com/science/ article/pii/S0927025620300355. Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, and Daniel Khashabi. Self-[in]correct: Llms struggle with discriminating self-generated responses, 2024. URL https://arxiv.org/abs/2404.04298. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZıdek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino RomeraParedes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold. Nature, 596:583589, 2021. doi: https://doi.org/10.1038/s41586-021-03819-2. Sayash Kapoor, Benedikt Stroebl, Zachary S. Siegel, Nitya Nadgir, and Arvind Narayanan. Ai agents that matter, 2024. URL https://arxiv.org/abs/2407.01502. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. VisualWebArena: Evaluating multimodal agents on realistic visual web tasks. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 881905, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.50. Eric Krause. Analyze urban heat using kriging, July 2024. URL https://learn.arcgis. com/en/projects/analyze-urban-heat-using-kriging/. 15 Preprint. Under review. Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. BioMistral: collection of open-source pretrained large language modIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findels for medical domains. ings of the Association for Computational Linguistics ACL 2024, pp. 58485864, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.348. Zekun Li, Wenxuan Zhou, Yao-Yi Chiang, and Muhao Chen. GeoLM: Empowering language models for geospatially grounded language understanding. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 52275240, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.317. URL https://aclanthology.org/2023. emnlp-main.317. Jenny T. Liang, Carmen Badea, Christian Bird, Robert DeLine, Denae Ford, Nicole Forsgren, and Thomas Zimmermann. Can gpt-4 replicate empirical software engineering research? Proc. ACM Softw. Eng., 1(FSE), jul 2024. doi: 10.1145/3660767. URL https://doi.org/10.1145/ 3660767. Guanyu Lin, Tao Feng, Pengrui Han, Ge Liu, and Jiaxuan You. Paper copilot: self-evolving and efficient llm system for personalized academic assistance, 2024. URL https://arxiv.org/ abs/2409.04593. Anika Liu, Moritz Walter, Peter Wright, Aleksandra Bartosik, Daniela Dolciami, Abdurrahman Elbasir, Hongbin Yang, and Andreas Bender. Prediction and mechanistic analysis of druginduced liver injury (dili) based on chemical structure. Biology Direct, 16(6), 2021. doi: 10.1186/s13062-020-00285-0. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https://arxiv.org/ abs/2408.06292. Zitong Lu and Julie Golomb. Generate your neural signals from mine: individual-to-individual eeg converters. Proceedings of the Annual Meeting of the Cognitive Science Society, 45, 2023. URL https://escholarship.org/uc/item/5xn0885t. Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, and Peter Clark. Position: Data-driven discovery with large generative models. In Forty-first International Conference on Machine Learning, 2024a. URL https://openreview.net/ forum?id=5SpjhZNXtt. Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. Discoverybench: Towards data-driven discovery with large language models, 2024b. URL https://arxiv.org/abs/2407.01725. Dominique Makowski, Tam Pham, Zen J. Lau, Jan C. Brammer, Francois Lespinasse, Hung Pham, Christopher Scholzel, and S. H. Annabel Chen. NeuroKit2: Python toolbox for neurophysiological signal processing. Behavior Research Methods, 53(4):16891696, February 2021. doi: 10.3758/s13428-020-01516-y. Mariia Matveieva and Pavel Polishchuk. Benchmarks for interpretation of qsar models. Journal of Cheminformatics, 41(13), 2021. doi: 10.1186/s13321-021-00519-x. F. Maussion, A. Butenko, N. Champollion, M. Dusch, J. Eis, K. Fourteau, P. Gregor, A. H. Jarosch, J. Landmann, F. Oesterle, B. Recinos, T. Rothenpieler, A. Vlug, C. T. Wild, and B. Marzeion. The open global glacier model (oggm) v1.1. Geoscientific Model Development, 12(3):909 doi: 10.5194/gmd-12-909-2019. URL https://gmd.copernicus.org/ 931, 2019. articles/12/909/2019/. MistralAI."
        },
        {
            "title": "Large",
            "content": "enough. Jul 2024. URL https://mistral.ai/news/ mistral-large-2407. 16 Preprint. Under review. Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent L. Chevrier, Kristin A. Persson, and Gerbrand Ceder. Python materials genomics (pymatgen): robust, open-source python library for materials analysis. Computational Materials Science, 68:314319, 2013. ISSN 0927-0256. doi: https: //doi.org/10.1016/j.commatsci.2012.10.028. URL https://www.sciencedirect.com/ science/article/pii/S0927025612006295. OpenAI. Hello gpt-4o. May 2024a. URL https://openai.com/index/hello-gpt-4o. OpenAI. Introducing openai o1-preview. Sep 2024b. URL https://openai.com/index/ introducing-openai-o1-preview/. Bharath Ramsundar, Bowen Liu, Zhenqin Wu, Andreas Verras, Matthew Tudor, Robert P. SheriJournal of Chemidan, and Vijay Pande. cal Information and Modeling, 57(8):20682076, 2017. doi: 10.1021/acs.jcim.7b00146. URL https://doi.org/10.1021/acs.jcim.7b00146. PMID: 28692267. Is multitask deep learning practical for pharma? Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande, Karl Leswing, and Zhenqin Wu. Deep Learning for the Life Sciences. OReilly Media, 2019. https://www.amazon.com/ Deep-Learning-Life-Sciences-Microscopy/dp/1492039837. Sebastian Raschka, Anne M. Scott, Mar Huertas, Weiming Li, and Leslie A. Kuhn. Automated Inference of Chemical Discriminants of Biological Activity, pp. 307338. Springer New York, New York, NY, 2018. ISBN 978-1-4939-7756-7. doi: 10.1007/978-1-4939-7756-7 16. URL https://doi.org/10.1007/978-1-4939-7756-7_16. Robert Richer, Arne Kuderle, Martin Ullrich, Nicolas Rohleder, and Bjoern M. Eskofier. Biopsykit: python package for the analysis of biopsychological data. Journal of Open Source Software, 6 (66):3702, 2021. doi: 10.21105/joss.03702. URL https://doi.org/10.21105/joss. 03702. Nicolas Riesterer, Daniel Brand, Hannah Dames, and Marco Ragni. Modeling human syllogistic reasoning:the role of no valid conclusion. Proceedings of the Annual Meeting of the Cognitive Science Society, 41, 2019. URL https://escholarship.org/uc/item/5xm1m8h8. AShlee Robinson. 2023. October chart-coral-and-sponge-distribution-factors-with-python/."
        },
        {
            "title": "URL",
            "content": "sponge and python, https://learn.arcgis.com/en/projects/ factors with distribution"
        },
        {
            "title": "Chart",
            "content": "coral Stuart Russell and Peter Norvig. Artificial Intelligence: Modern Approach. Prentice Hall, 3 edition, 2010. Jason Schatz and Christopher Kucharik. Urban climate effects on extreme temperatures in madison, wisconsin, usa. Environmental Research Letters, 10(9):094024, 2015. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH. Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. URL automation. https://openreview.net/forum?id=ZUbraGNpAq. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers, 2024. URL https://arxiv.org/abs/ 2409.04109. Gregor Sturm, Tamas Szabo, Georgios Fotakis, Marlene Haider, Dietmar Rieder, Zlatko Trajanoski, and Francesca Finotello. Scirpy: Scanpy extension for analyzing single-cell T-cell receptorsequencing data. Bioinformatics, 36(18):48174818, 07 2020. ISSN 1367-4803. doi: 10. 1093/bioinformatics/btaa611. URL https://doi.org/10.1093/bioinformatics/ btaa611. 17 Preprint. Under review. Kyle Swanson, Parker Walther, Jeremy Leitz, Souhrid Mukherjee, Joseph Wu, Rabindra Shivnaraine, and James Zou. ADMET-AI: machine learning ADMET platform for evaluation of large-scale chemical libraries. Bioinformatics, 40(7):btae416, 06 2024. ISSN 1367-4811. doi: 10. 1093/bioinformatics/btae416. URL https://doi.org/10.1093/bioinformatics/ btae416. Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, and Mark Gerstein. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code, 2024a. URL https://arxiv.org/abs/ 2311.09835. Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, and Mark Gerstein. BioCoder: benchmark for bioinformatics code generation with large language models. Bioinformatics, 40 (Supplement 1):i266i276, 2024b. ISSN 1367-4811. doi: 10.1093/bioinformatics/btae230. URL https://doi.org/10.1093/bioinformatics/btae230. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: research coding benchmark curated by scientists, 2024. URL https: //arxiv.org/abs/2407.13168. Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Philipp Angerer, Volker Bergen, Pierre Boyeau, Maren Buttner, Gokcen Eraslan, David Fischer, Max Frank, Justin Hong, Michal Klein, Marius Lange, Romain Lopez, Mohammad Lotfollahi, Malte D. Luecken, Fidel Ramirez, Jeffrey Regier, Sergei Rybakov, Anna C. Schaar, Valeh Valiollah Pour Amiri, Philipp Weiler, Galen Xing, Bonnie Berger, Dana Peer, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle, Fabian J. Theis, and Scverse Community. The scverse project provides computational ecosystem for single-cell omics data analysis. Nature Biotechnology, 41(5), 2023. doi: 10.1038/ s41587-023-01733-8. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. In Towards understanding chain-of-thought prompting: An empirical study of what matters. Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2717 2739, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.153. URL https://aclanthology.org/2023.acl-long.153. Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. LLMs in the imaginarium: Tool learning through simulated trial and error. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1058310604, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.acl-long.570. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Veliˇckovic, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the age of artificial intelligence. Nature, 620:4760, 2023b. doi: https://doi.org/10.1038/s41586-023-06221-2. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better LLM agents. In Forty-first International Conference on Machine Learning, 2024b. URL https://openreview.net/forum?id=jJ9BoXAfFa. 18 Preprint. Under review. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Opendevin: An open platform for ai software developers as generalist agents, 2024c. URL https://arxiv.org/abs/2407.16741. Logan Ward, Alexander Dunn, Alireza Faghaninia, Nils E.R. Zimmermann, Saurabh Bajaj, Qi Wang, Joseph Montoya, Jiming Chen, Kyle Bystrom, Maxwell Dylla, Kyle Chard, Mark Asta, Kristin A. Persson, G. Jeffrey Snyder, Ian Foster, and Anubhav Jain. Matminer: An open source toolkit for materials data mining. Computational Materials Science, 152:6069, 2018. ISSN 0927-0256. doi: https://doi.org/10.1016/j.commatsci.2018.05.018. URL https: //www.sciencedirect.com/science/article/pii/S0927025618303252. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf. F. Alexander Wolf, Philipp Angerer, and Fabian J. Theis. Scanpy: large-scale single-cell gene expression data analysis. Genome Biology, 19(15), 2018. doi: 10.1186/s13059-017-1382-0. Felix Wong, Erica J. Zheng, Jacqueline A. Valeri, Nina M. Donghia, Melis N. Anahtar, Satotaka Omori, Alicia Li, Andres Cubillos-Ruiz, Aarti Krishnan, Wengong Jin, Abigail L. Manson, Jens Friedrichs, Ralf Helbig, Behnoush Hajian, Dawid K. Fiejtek, Florence F. Wagner, Holly H. Soutter, Ashlee M. Earl, Jonathan M. Stokes, Lars D. Renner, and James J. Collins. Discovery of structural class of antibiotics with explainable deep learning. Nature, 626:177185, 2024. doi: 10.1038/s41586-023-06887-8. Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, and Ping Luo. Plot2code: comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots, 2024. URL https://arxiv.org/abs/ 2405.07990. Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. Moleculenet: benchmark for molecular machine learning. Chem. Sci., 9:513530, 2018. doi: 10.1039/C7SC02664A. URL http://dx.doi.org/ 10.1039/C7SC02664A. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llmbased software engineering agents, 2024. URL https://arxiv.org/abs/2407.01489. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent computer interfaces enable software engineering language models, 2024a. Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi, and Maosong Sun. Matplotagent: Method and evaluation for llm-based agentic scientific data visualization, 2024b. URL https: //arxiv.org/abs/2402.11453. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=WE_vluYUL-X. Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, and Huan Sun. LlaSMol: Advancing large language models for chemistry with large-scale, comprehensive, high-quality instruction tuning dataset. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=lY6XTF9tPv. 19 Preprint. Under review. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=yLClGs770I. Paul Zandbergen. Run geoprocessing tools with python, March 2024. URL https://learn. arcgis.com/en/projects/run-geoprocessing-tools-with-python/. Amanda Zellmer and Barbara Goto. Urban wildlife corridors: Building bridges for wildlife and people. Frontiers in Sustainable Cities, 4:954089, 2022. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, and Jiawei Han. comprehensive survey of scientific large language models and their applications in scientific discovery, 2024. URL https://arxiv.org/abs/2406.10833. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. GPT-4V(ision) is generalist web agent, if grounded. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 6134961385. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/zheng24e.html. Zhiling Zheng, Oufan Zhang, Ha L. Nguyen, Nakul Rampal, Ali H. Alawadhi, Zichao Rong, Teresa Head-Gordon, Christian Borgs, Jennifer T. Chayes, and Omar M. Yaghi. Chatgpt research group for optimizing the crystallinity of mofs and cofs. ACS Central Science, 9(11): 21612170, 2023. doi: 10.1021/acscentsci.3c01087. URL https://doi.org/10.1021/ acscentsci.3c01087. Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. CodeBERTScore: Evaluating code generation with pretrained models of code. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1392113937, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.859. URL https://aclanthology.org/2023. emnlp-main.859. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realIn The Twelfth International Conferistic web environment for building autonomous agents. ence on Learning Representations, 2024. URL https://openreview.net/forum?id= oKn9c6ytLx. Abubakr Ziedan, Cassidy Crossland, Candace Brakewood, Philip Pugliese, and Harrison Ooi. Investigating the preferences of local residents toward proposed bus network redesign in chattanooga, tennessee. Transportation Research Record, 2675(10):825840, 2021. 20 Preprint. Under review."
        },
        {
            "title": "APPENDICES",
            "content": "We provide more details omitted in the main text as follows: Appendix A: Limitations Appendix B: Related Work Appendix C: Example Task Instructions Table C.1: Example Instructions for Bioinformatics and Computational Chemistry"
        },
        {
            "title": "Tasks",
            "content": "Table C.2: Example Instructions for Geographical Information Science and Psychology & Cognitive Neuroscience Tasks Appendix D: More Details about Main Results Table D.1: Mean performances of each agent and standard deviations on ScienceAgentBench without expert-provided knowledge Table D.2: Mean performances of each agent and standard deviations on ScienceAgentBench with expert-provided knowledge. Appendix E: Case Studies Appendix E.1: Action Space of OpenHands Appendix E.2: Influence of Expert-Provided Knowledge Appendix E.3: OpenAI o1 Reasoning Appendix F: Expert Validation Details Appendix F.1: Questionnaire for Subject Matter Experts Appendix F.2: Program Example for Subject Matter Experts Appendix F.3: Knowledge Example for Subject Matter Experts Appendix G: Rubric Examples Appendix G.1: An example rubric of Computational Chemistry task generated by GPT-4o without expert revision Appendix G.2: An example rubric revised by an expert by adding the available points to two items Appendix G.3: An example rubric of Geographical Information Science task generated by GPT-4o without expert revision Appendix G.4: An example rubric of Geographical Information Science task revised by an expert by reducing the available points for several items Appendix H: Prompt Templates Table H.1: Prompt Template for Direct Prompting Table H.2: Prompt Template for Self-Debug Table H.3: Prompt Template for OpenDevin Appendix I: Publications, Repositories, and Licenses Table I.1: List of Bioinformatics and Computational Chemistry Publications Table I.2: List of Geographical Information Science and Psychology & Cognitive"
        },
        {
            "title": "Neuroscience Publications",
            "content": "Table I.3: List of Repositories and Licenses Table I.4: Copyright Information for rasterio/rasterio Table I.5: Copyright Information for ackingmaterials/matminer. 21 Preprint. Under review."
        },
        {
            "title": "A LIMITATIONS",
            "content": "Capabilities and Evaluation of Language Agents for Science. In this work, we have developed benchmark focusing on tasks in data-driven discovery and formulate them as code generation (1) Data-driven discovery is an increasingly important workflow problems due to two reasons. for science (Hey et al., 2009). While plenty of computational tools (Cao, 2017) and AI models (Wang et al., 2023b) have been developed, the sheer amount and heterogeneity of data are already overwhelming for scientists (Bell et al., 2009), not to mention the programming efforts to access these tools and models for processing, analyzing, and visualizing scientific data. language agent that can automate such tedious tasks in data-driven discovery would help to save hours of effort for scientists. (2) We aim to rigorously assess the capabilities of existing language agents as science copilots that can write code to process, analyze, and visualize data. Hence, we formulate each task as code generation problem, whose output shall be easily verifiable using well-established automatic metrics and directly usable by scientist without additional efforts to modify or implement. As result, we only focus on the code generation capability of language agents. We encourage future studies to carefully examine the agents other capabilities that can help with scientific discovery, such as summarizing literature (Lin et al., 2024), suggesting ideas (Si et al., 2024), or planning experiments (Boiko et al., 2023). Specifically, we advocate rigorous, comprehensive assessments of one such capability at time, as we need to deeply understand the strengths and limitations of current language agents for each aspect of scientific discovery. In addition, while we only use wellestablished evaluation methods in our benchmark, such as CodeBERTScore (Zhou et al., 2023) and GPT-4o judge for figures (Wu et al., 2024; Yang et al., 2024b), we acknowledge that they are not perfect yet. Future research may leverage the diverse set of tasks in our benchmark to develop better automatic evaluation metrics or human evaluation protocols for data-driven discovery tasks and code generation problems. Diversity of Tasks, disciplines, and Programs. Although we strive to include diverse set of tasks and programs from different scientific disciplines in ScienceAgentBench, we devise several compromises to make data collection more practical. First, when collecting publications, we have indeed found more with programs written in R, Stata, or Matlab. However, because our annotators are not familiar with these programming languages, we focus on collecting Python programs, which all annotators can adapt confidently. Second, for evaluation efficiency, we only collect programs that can accomplish the task within 10 minutes. As result, the final benchmark includes relatively fewer tasks that process large-scale data and develop complex methods. Finally, we choose the four representative disciplines considering their abundance of open-source data and the availability of experts we can easily contact. With these limitations in mind, we have designed principled, extensible data collection process and expert validation protocol. Future work is encouraged to expand ScienceAgentBench with programs in other languages and tasks in other disciplines. Ethical and Safety Considerations. Our benchmark is constructed by adapting open-source code and data, to which we respect their creators ownership and intellectual property. In Appendix I, we have made our best effort to cite the original papers, list the repositories, and provide their licenses. Still, we acknowledge that two repositories are copyrighted and believe their terms for use are compatible with our research purpose (Table I.4, I.5). We welcome requests from the original authors to modify or remove relevant tasks if needed. Meanwhile, agents developed with ScienceAgentBench should consider potential safety issues in deployment, such as adversarial attacks and confidential data leakage. Instead of solely relying on language agents for scientific discovery, it is important for users to have control over the agents through intervention and feedback mechanisms. 22 Preprint. Under review."
        },
        {
            "title": "B RELATED WORK",
            "content": "AI for Science. Since deep learning unlocks the power of data, AI algorithms and models have been increasingly used to accelerate scientific discovery (Wang et al., 2023b). One of the most prominent examples is AlphaFold (Jumper et al., 2021), which can predict protein structures with high accuracy and save biologists months to years of effort. More recently, tremendous number of language models has been developed for different disciplines, including math (Yue et al., 2024), chemistry (Yu et al., 2024), biology (Labrak et al., 2024), geography (Li et al., 2023), and so on. 6 To automate data-driven discovery end-to-end, it is necessary for language agents to write code to access these AI models and other computational tools (Cao, 2017). Our work aims to develop language agents with this essential ability, which can help scientists save hours of programming effort, and rigorously evaluate such agents to grasp more solid understanding of their strengths and limitations. Agents and Benchmarks for Task Automation. Developing agents for task automation is longestablished challenge in AI research (Russell & Norvig, 2010). Built upon LLMs, new generation of agents has shown new promise to automatically perform many tasks in web navigation (Deng et al., 2023; He et al., 2024; Koh et al., 2024; Zheng et al., 2024; Zhou et al., 2024), software development (Jimenez et al., 2024; Wang et al., 2024c; Yang et al., 2024a), or scientific discovery (Boiko et al., 2023; Zheng et al., 2023; Lu et al., 2024). To evaluate the performance of these new agents, many benchmarks have been recently proposed. For example, TaskBench (Shen et al., 2024) is one of the first benchmarks for evaluating language agents with large-scale synthetic tasks. The output of these tasks are formatted as JSON API tool calls, which hinders the generalization of agents developed on this benchmark to constantly changing tools or new ones in practice. To resemble real-world use cases with more flexibility, many benchmarks formulate their tasks as code generation and unifies their outputs as Python programs, such as SWE-Bench (Jimenez et al., 2024), BioCoder-Py (Tang et al., 2024b), ML-Bench (Tang et al., 2024a), and MLAgentBench (Huang et al., 2024b). Yet, these benchmarks only consist of tasks found on secondary sources, such as GitHub and Kaggle. To fill the gap in evaluating language agents for scientific tasks, few benchmarks start to use scientific publications as their task sources, including SciCode (Tian et al., 2024), BLADE (Gu et al., 2024), and DiscoveryBench-Real (Majumder et al., 2024b). Among them, DiscoveryBench-Real is the most similar to our ScienceAgentBench. However, DiscoveryBench-Real asks the agents to output abstract steps to complete task in natural language, which is hard to evaluate rigorously and maybe practically less useful. With ScienceAgentBench, we advocate careful evaluations of language agents performance on individual tasks, instead of purely relying on end-to-end evaluations of these agents, e.g., using an LLM-based reviewer to assess generated papers (Lu et al., 2024). In the long run, ScienceAgentBench will serve as high-quality benchmark focusing on essential tasks that involve code generation in real-world data-driven discovery workflows for objective assessment and continued development of future language agents. 6We refer to Zhang et al. (2024) for comprehensive survey on scientific language models. Preprint. Under review."
        },
        {
            "title": "C EXAMPLE TASK INSTRUCTIONS",
            "content": "Table C.1: Example instructions of Bioinfomatics and Compuational Chemistry tasks (Section 2.2)."
        },
        {
            "title": "Computational Chemistry",
            "content": "Train cell counting model on the BBBC002 datasets containing Drosophila KC167 cells. Save the test set predictions as single column count to pred results/cell-count pred.csv. Train drug-target interaction model using the DAVIS dataset to determine the binding affinity between several drugs and targets. Then use the trained model to predict the binding affinities between antiviral drugs and COVID-19 target. Rank the antiviral drugs based on their predicted affinities and save the ordered list of drugs to pred results/davis dti repurposing.txt, with one SMILES per line. Plot the Tanimoto similarities of the fingerprint between the frames. Specifically, the interaction fingerprints between selected ligand and protein for the first 10 trajectory frames. Save the png file into pred results/ligand similarity pred.png. Train VAE model on the given data and perform 1-vs-all differential expression test for each cell type. Extract top markers for each cell type using the results. Visualize them as dotplot with the cell types organized using dendrogram. Save the figure to pred results/hca cell type de.png. Train multitask model on the Clintox dataset to predict drugs toxicity and FDA approval status. Save the test set predictions, including the SMILES representation of drugs and the probability of positive labels, to pred results/clintox test pred.csv. Generate features for the given diffusion data based on material composition and use the SHAP feature selection approach to select 20 features. Save the selected features as CSV file mat diffusion features.csv to the folder pred results/. Filter the compounds in hits.csv and save the SMILES representations of the left ones. Compounds to be kept should have no PAINS or Brenk filter substructures and have maximum tanimoto similarity of less than 0.5 to any of the active compounds in train.csv. Save the SMILES of left compounds to pred results/compound filter results .txt, with each one in line. Train graph convolutional network on the given dataset to predict the aquatic toxicity of compounds. Use the resulting model to compute and visualize the atomic contributions to molecular activity of the given test example compound. Save the figure as pred results/aquatic toxicity qsar vis.png. 24 Preprint. Under review. Table C.2: Example instructions of Geographical Information Science and Psychology & Cognitive Neuroscience tasks (Section 2.2)."
        },
        {
            "title": "Geo Information Science",
            "content": "Psy & Cognitive Neuroscience Analyze and visualize Elk movements in the given dataset. Estimate home ranges and assess habitat preferences using spatial analysis techniques. Identify the spatial clusters of Elk movements. Document the findings with maps and visualizations. Save the figure as pred results/Elk Analysis.png. Analyze the impact of land subsidence on flooding based on future elevation data of the study area. Identify flood-prone areas and estimate potential building damage to support urban planning and mitigation strategies. Save the results to pred results/flooding analysis.png. Calculate the deforestation area percentage in the Brazilian state of Rondˆonia within the buffer zone of 5.5km around road layers. Save the percentage result in CSV file named pred results/deforestation rate.csv with column title percentage deforestation. Load North America climate data in NetCDF file and extract temperature data along the time series, then perform quadratic polynomial fit analysis on the temperature data, and output the fitting results by year in pred results/polynomial fit pred.csv. Process and visualize the given ECG data by perform peak detection and outlier correction. Plot an overview of the data and save the final figure as pred results/ecg processing vis1 pred result.png. Analyze the inertial measurement unit (IMU) data collected during sleep and compute sleep endpoints. Load the given data and compute the following sleep endpoints: time of falling asleep, time of awakening, and total duration spent sleeping. The three values should be saved in JSON file pred results/imu pred.json, and the keys for them are sleep onset, wake onset, and total sleep duration, respectively. Analyze cognitive theories using pattern similarity. Process CSV files containing model predictions for various syllogistic reasoning tasks. Calculate similarity scores between these models and pre-computed high-conscientiousness and high-openness patterns. The results will contain similarity scores for each cognitive model with respect to the personality trait patterns. Save the results to pred results/CogSci pattern high sim data pred.csv. Train linear model to learn the mapping of neural representations in EEG signals from one subject (Sub 01) to another (Sub 03) based on the preprocessed EEG data from Sub 01 and Sub 03. Then use the test set of Subject 1 (Sub 01) to generate EEG signal of Subject 3 (Sub 03). Save the generated EEG signal of Subject 3 to pred results/linear sub01tosub03 pred.npy. 25 Preprint. Under review."
        },
        {
            "title": "D MORE DETAILS ABOUT MAIN RESULTS",
            "content": "In Section 4.1, we present our results by selecting the best of three independent runs for each task in all experiments (Section 3). For comprehensiveness, we show the mean performances of each agent and standard deviations below, which demonstrate the same findings as in our main results. Table D.1: Mean performances of each agent and standard deviations on ScienceAgentBench without expert-provided knowledge."
        },
        {
            "title": "Models",
            "content": "SR"
        },
        {
            "title": "VER",
            "content": "Cost Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o Claude-3.5-Sonnet OpenAI o"
        },
        {
            "title": "Direct Prompting",
            "content": "3.6 (2.0) 3.6 (0.5) 10.1 (1.2) 7.5 (0.5) 11.8 (2.1) 23.9 (0.5) 81.0 (0.4) 79.3 (0.1) 82.5 (0.2) 81.7 (0.1) 82.5 (0.4) 84.5 (0.1)"
        },
        {
            "title": "OpenHands CodeAct",
            "content": "22.2 (0.9) 32.0 (0.5) 36.6 (0.9) 42.2 (1.6) 36.0 (1.2) 56.2 (1.7) 0.001 (0.000) 0.011 (0.000) 0.010 (0.000) 0.011 (0.000) 0.017 (0.000) 0.237 (0.003) Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o Claude-3.5-Sonnet 3.3 (0.5) 2.6 (0.9) 7.5 (0.9) 13.1 (2.6) 14.1 (1.2) 59.9 (1.6) 59.0 (4.9) 70.4 (1.1) 80.6 (1.2) 81.2 (0.8) 17.0 (1.2) 34.3 (9.2) 42.8 (1.7) 62.8 (2.9) 63.4 (6.5) 0.234 (0.026) 0.576 (0.108) 0.735 (0.025) 1.093 (0.071) 1.122 (0.056) Self-Debug Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o Claude-3.5-Sonnet OpenAI o1 7.2 (1.2) 8.8 (1.4) 16.0 (1.7) 14.7 (3.2) 22.9 (2.0) 27.1 (1.2) 81.2 (0.3) 80.8 (0.5) 83.2 (0.4) 82.6 (0.6) 84.2 (0.3) 84.9 (0.4) 67.3 (2.4) 67.0 (2.8) 70.3 (2.6) 71.2 (1.2) 84.0 (1.2) 84.6 (0.5) 0.009 (0.000) 0.054 (0.005) 0.043 (0.001) 0.057 (0.006) 0.066 (0.005) 0.701 (0.008) Table D.2: Mean performances of each agent and standard deviations on ScienceAgentBench with expert-provided knowledge."
        },
        {
            "title": "Models",
            "content": "SR"
        },
        {
            "title": "VER",
            "content": "Cost Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o Claude-3.5-Sonnet OpenAI o1 Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o Claude-3.5-Sonnet"
        },
        {
            "title": "Direct Prompting",
            "content": "2.6 (0.5) 2.9 (0.0) 11.4 (1.2) 8.2 (1.8) 16.7 (2.4) 18.3 (0.5) 81.7 (0.1) 81.3 (0.0) 83.8 (0.2) 83.2 (0.4) 84.5 (0.4) 84.6 (0.1)"
        },
        {
            "title": "OpenHands CodeAct",
            "content": "1.6 (0.9) 4.3 (2.0) 9.2 (0.9) 16.7 (2.8) 15.7 (2.1) 60.5 (0.9) 62.9 (6.3) 74.1 (2.9) 83.7 (0.7) 82.8 (0.3) Self-Debug 19.3 (1.7) 24.5 (0.0) 28.8 (2.3) 35.6 (1.8) 33.0 (1.2) 45.4 (2.4) 0.001 (0.000) 0.011 (0.000) 0.010 (0.000) 0.012 (0.000) 0.017 (0.000) 0.247 (0.009) 16.7 (0.8) 35.6 (1.7) 35.3 (0.8) 60.8 (2.4) 68.0 (3.3) 0.296 (0.003) 0.653 (0.072) 0.757 (0.049) 1.402 (0.055) 1.095 (0.087) Llama-3.1-Instruct-70B Llama-3.1-Instruct-405B Mistral-Large-2 (2407) GPT-4o Claude-3.5-Sonnet OpenAI o1 9.8 (2.1) 8.2 (0.9) 18.3 (0.5) 15.0 (2.0) 27.8 (2.0) 27.8 (1.7) 82.0 (0.4) 82.2 (0.1) 84.9 (0.1) 83.8 (0.4) 85.5 (0.5) 85.8 (0.4) 60.8 (2.1) 61.1 (3.8) 62.8 (0.0) 61.4 (1.7) 81.1 (0.9) 83.0 (3.9) 0.011 (0.000) 0.072 (0.002) 0.051 (0.001) 0.063 (0.001) 0.072 (0.005) 0.853 (0.023) Preprint. Under review."
        },
        {
            "title": "E CASE STUDIES",
            "content": "E.1 ACTION SPACE OF OPENHANDS In Section 4, our results show that, for four of the five LLMs evaluated, self-debug demonstrates better performance than OpenHands CodeAct, with GPT-4o as the only exception. For GPT-4o, which may have been trained to better follow instructions for language agents, we indeed observe that it can better leverage tools in OpenHands, such as the web browser commands (lines 33, 60, and 87 in Listing E.1). In sharp contrast, as shown in Listing E.2, Claude-3.5-Sonnet is still struggling with the specialized bash command in OpenHands to update the program file without duplication (line 11 in Listing E.2), and after five steps of trials, it ends up simply using the Python function open() instead to edit the file (line 146 in Listing E.2). Thus, LLM-based agents do not always benefit from large action space with complex tool usage, such as the design of OpenHands. Listing E.1: An example of GPT-4o using the web browser in OpenHands. Browsing actions in lines 3134, 5861, 8588. 1 { 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 35 36 37 38 39 40 41 42 43 44 45 ......(first seven turns omitted), [ { \"id\": 17, \"timestamp\": \"2024-09-19T14:13:52.407648\", \"source\": \"agent\", \"message\": \"Im asking BrowsingAgent for help with this task.\", \"action\": \"delegate\", \"args\": { \"agent\": \"BrowsingAgent\", \"inputs\": { \"task\": \"It seems that the rsp_rrv function does not accept peaks argument. Lets check the NeuroKit2 documentation to find the correct usage of the rsp_rrv function.nn### Step 8: Check NeuroKit2 DocumentationnWe will browse the NeuroKit2 documentation to find the correct usage of the rsp_rrv function.. should start with: Search for \"NeuroKit2 rsp_rrv function documentation\"\" }, \"thought\": \"\" } }, { \"message\": \"No observation\", \"observation\": \"null\", \"content\": \"\", \"extras\": {} ], [ } { \"id\": 20, \"timestamp\": \"2024-09-19T14:13:53.634697\", \"source\": \"agent\", \"message\": \"Executing browser actions: goto(https://neurokit2.readthedocs.io /)\", \"action\": \"browse_interactive\", \"args\": { \"browser_actions\": \"goto(https://neurokit2.readthedocs.io/)\", \"thought\": \"In order to accomplish my goal, need to search for the NeuroKit2 rsp_rrv function documentation. will start by navigating to the NeuroKit2 documentation website.\", \"browsergym_send_msg_to_user\": \"\" } }, { \"id\": 21, \"timestamp\": \"2024-09-19T14:13:56.398162\", \"source\": \"agent\", \"message\": \"Visited https://neurokit2.readthedocs.io/en/legacy_docs/\", \"cause\": 20, \"observation\": \"browse\", \"content\": \"<::before> NeuroKit nnlegacy_docs nn * Documentation nnn n__ NeuroKit nn<::before><::after>n * [ <::before> Edit on GitHub ](https://github.com/neuropsychology/NeuroKit/blob/ <::after>u00b6 legacy_docs/docs/index.rst)nnnn* * *nn# n<::before>Warning nnThe **NEW DOCUMENTATION** website is at [ https:// neuropsychology.github.io/NeuroKit ](https://neuropsychology.github.io/NeuroKit Revision bd105f7f /)nn* * *nnu00a9 Copyright 2020, Dominique Makowski * Documentation n"
        },
        {
            "title": "Documentation",
            "content": "u00bb * Docs 27 Preprint. Under review. [ legacy_docs ](https://neurokit2.readthedocs.io/en/ v: legacy_docs <::before>n [ PDF ](//neurokit2.readthedocs.io/_/downloads [ HTML ](//neurokit2.readthedocs.io/_/downloads/en/ [ Epub ](//neurokit2.readthedocs.io/_/downloads/en/ [ Project Home ](//readthedocs.org [ Builds ](//readthedocs.org/projects/neurokit2/ . nn<::before><::after> <::before> Read the Docs nVersions legacy_docs/)nnDownloads /en/legacy_docs/pdf/)n legacy_docs/htmlzip/)n legacy_docs/epub/)nnOn Read the Docs /projects/neurokit2/)n builds/)n nOn GitHub legacy_docs/docs/index.rst)n NeuroKit/edit/legacy_docs/docs/index.rst)nnSearch [ Read the Docs ](https://readthedocs.org) u00b7 docs.readthedocs.io/page/privacy-policy.html)n\", \"extras\": { [ Downloads ](//readthedocs.org/projects/neurokit2/downloads/)n [ View ](https://github.com/neuropsychology/NeuroKit/blob/ [ Edit ](https://github.com/neuropsychology/ nn* * *nnHosted by [ Privacy Policy ](https:// \"url\": \"https://neurokit2.readthedocs.io/en/legacy_docs/\", ...... (webpage information omitted), } ], [ } { \"id\": 22, \"timestamp\": \"2024-09-19T14:13:58.344923\", \"source\": \"agent\", \"message\": \"Executing browser actions: click(43)nfill(43, rsp_rrv) npress(43, Enter)\", \"action\": \"browse_interactive\", \"args\": { \"browser_actions\": \"click(43)nfill(43, rsp_rrv)npress(43, Enter) \", \"thought\": \"To find the correct usage of the rsp_rrv function, should search for it in the NeuroKit2 documentation. The current page has search box that can use for this purpose.nnAction:n1. Click on the search textbox.n2. Fill the search textbox with the query \"rsp_rrv\".n3. Press Enter to initiate the search.\", \"browsergym_send_msg_to_user\": \"\" } }, { \"id\": 23, \"timestamp\": \"2024-09-19T14:14:02.341831\", \"source\": \"agent\", \"message\": \"Visited https://neurokit2.readthedocs.io/en/legacy_docs/search.html ?q=rsp_rrv&check_keywords=yes&area=default\", \"cause\": 22, \"observation\": \"browse\", \"content\": \"[ <::before> NeuroKit ](index.html)nnlegacy_docs nn__ [ * Search * [ Docs ](index.html) u00bb * [ <::before> Edit on GitHub ](https://github.com/ NeuroKit ](index.html)nn<::before><::after>n neuropsychology/NeuroKit/blob/legacy_docs/docs/search.rst)nnnn* * *n nPlease activate JavaScript to enable the search functionality. nn## Searching .. nnnnn* * *nnu00a9 Copyright 2020, Dominique Makowski Revision bd105f7f . nn<::before><::after> <::before> Read the Docs v: legacy_docs <::before>nnVersions readthedocs.io/en/legacy_docs/search.html)nnDownloads neurokit2.readthedocs.io/_/downloads/en/legacy_docs/pdf/)n neurokit2.readthedocs.io/_/downloads/en/legacy_docs/htmlzip/)n neurokit2.readthedocs.io/_/downloads/en/legacy_docs/epub/)nnOn Read the Docs ](//readthedocs.org/projects/neurokit2/builds/)n readthedocs.org/projects/neurokit2/downloads/)nnOn GitHub https://github.com/neuropsychology/NeuroKit/blob/legacy_docs/docs/search.rst)n [ Edit ](https://github.com/neuropsychology/NeuroKit/edit/legacy_docs/docs/ [ Project Home ](//readthedocs.org/projects/neurokit2/)n [ legacy_docs ](https://neurokit2. [ Downloads ](// [ HTML ](// [ Epub ](// [ PDF ](// [ View ]( [ Builds search.rst)nnSearch readthedocs.org) u00b7 privacy-policy.html)n\", \"extras\": { nn* * *nnHosted by [ Read the Docs ](https:// [ Privacy Policy ](https://docs.readthedocs.io/page/ \"url\": \"https://neurokit2.readthedocs.io/en/legacy_docs/search.html?q=rsp_rrv &check_keywords=yes&area=default\", ...... (webpage information omitted), } ], [ } { \"id\": 24, \"timestamp\": \"2024-09-19T14:14:03.570167\", \"source\": \"agent\", \"message\": \"Executing browser actions: scroll(0, 300)\", \"action\": \"browse_interactive\", \"args\": { 28 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 73 74 75 76 77 78 79 80 81 82 83 84 85 86 Preprint. Under review. \"browser_actions\": \"scroll(0, 300)\", \"thought\": \"To find the correct usage of the rsp_rrv function, need to review the search results. Since the search results are likely listed in the main section, will scroll down to view them.\", \"browsergym_send_msg_to_user\": \"\" } }, { \"id\": 25, \"timestamp\": \"2024-09-19T14:14:05.356163\", \"source\": \"agent\", \"message\": \"Visited https://neurokit2.readthedocs.io/en/legacy_docs/search.html ?q=rsp_rrv&check_keywords=yes&area=default\", \"cause\": 24, \"observation\": \"browse\", \"content\": \"[ <::before> NeuroKit ](index.html)nnlegacy_docs nn__ [ <::before>nnVersions bd105f7f . nn<::before><::after> <::before> Read the Docs * Search NeuroKit ](index.html)nn<::before><::after>n neuropsychology/NeuroKit/blob/legacy_docs/docs/search.rst)nnnn* * *n nPlease activate JavaScript to enable the search functionality. nn## Searching nnnnn* * *nnu00a9 Copyright 2020, Dominique Makowski * [ <::before> Edit on GitHub ](https://github.com/ * [ Docs ](index.html) u00bb Revision v: legacy_docs [ legacy_docs ](https://neurokit2.readthedocs.io [ PDF ](//neurokit2.readthedocs [ HTML ](//neurokit2.readthedocs.io/_ [ Epub ](//neurokit2.readthedocs.io/_/ /en/legacy_docs/search.html)nnDownloads .io/_/downloads/en/legacy_docs/pdf/)n /downloads/en/legacy_docs/htmlzip/)n downloads/en/legacy_docs/epub/)nnOn Read the Docs readthedocs.org/projects/neurokit2/)n /neurokit2/builds/)n downloads/)nnOn GitHub NeuroKit/blob/legacy_docs/docs/search.rst)n neuropsychology/NeuroKit/edit/legacy_docs/docs/search.rst)nnSearch [ View ](https://github.com/neuropsychology/ [ Downloads ](//readthedocs.org/projects/neurokit2/ [ Edit ](https://github.com/ [ Builds ](//readthedocs.org/projects [ Project Home ](// nn* * *nnHosted by [ Read the Docs ](https://readthedocs.org) u00b7 Policy ](https://docs.readthedocs.io/page/privacy-policy.html)n\", \"extras\": { [ Privacy \"url\": \"https://neurokit2.readthedocs.io/en/legacy_docs/search.html?q=rsp_rrv &check_keywords=yes&area=default\", ...... (webpage information omitted), } } ], ......(19 more turns omitted), 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 } Listing E.2: An example of Claude-3.5-Sonnet that struggles with specialized bash commands in OpenHands to edit programs correctly. See line 11 for failed edit example. 1 { 2 3 4 5 6 7 8 9 10 11 ......(first seven turns omitted), [ { \"id\": 15, \"timestamp\": \"2024-09-16T21:45:01.776300\", \"source\": \"agent\", \"message\": \"......\", \"action\": \"run_ipython\", \"args\": { \"code\": \"code = nimport pandas as pdnimport numpy as npnfrom sklearn. model_selection import train_test_splitnfrom sklearn.preprocessing import StandardScaler, OneHotEncodernfrom sklearn.linear_model import LogisticRegressionnfrom sklearn.metrics import accuracy_scorenfrom sklearn. compose import ColumnTransformernfrom sklearn.pipeline import Pipelinenfrom mlxtend.feature_selection import SequentialFeatureSelector as SFSnimport matplotlib.pyplot as pltnn# Load the datandata = pd.read_csv(/workspace/ benchmark/datasets/dkpes/dkpes_train.csv)nn# Prepare features and targetnX = data.drop([index, Signal-inhibition], axis=1)ny = data[Signal-inhibition ]nn# Identify numeric and categorical columnsnnumeric_features = X. select_dtypes(include=[int64, float64]).columnsncategorical_features = X. select_dtypes(include=[object]).columnsnn# Binarize the target variable nthreshold = y.median()ny_binary = (y > threshold).astype(int)nn# Split the datanX_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size =0.2, random_state=42)nn# Create preprocessing stepsnpreprocessor = ColumnTransformer(n numeric_features),n categorical_features)n logistic regressionnlr_pipeline = Pipeline([n ),n backward feature selectionnsfs = SFS(lr_pipeline,n (X.columns)),n scoring=accuracy,n (classifier, LogisticRegression(random_state=42))n])nn# Perform (cat, OneHotEncoder(drop=first, sparse=False), ])nn# Create pipeline with preprocessing and (preprocessor, preprocessor n_jobs=-1)nnsfs = sfs.fit( (num, StandardScaler(), k_features=(1, len floating=False,n forward=False,n transformers=[n cv=5,n 29 Preprint. Under review. X_train, y_train)nn# Get the resultsnresults = pd.DataFrame.from_dict(sfs. get_metric_dict()).Tnresults.sort_index(inplace=True)nn# Plot the results nplt.figure(figsize=(10, 6))nplt.plot(results.index, results[avg_score], marker=o)nplt.title(Backward Feature Selection: Accuracy vs Number of Features)nplt.xlabel(Number of Features)nplt.ylabel(Cross-validation Accuracy)nplt.grid(True)nplt.tight_layout()nn# Save the plotnplt.savefig (/workspace/pred_results/dkpes_feature_selection_analysis_pred.png)nn# Print the best feature subsetnbest_subset = list(sfs.k_feature_names_)nprint(f\" Best feature subset: {best_subset}\")nprint(f\"Number of selected features: { len(best_subset)}\")nprint(f\"Best cross-validation accuracy: {sfs.k_score_:.4f }\")nn# Evaluate on the test set using the best feature subsetnbest_pipeline = sfs.estimatornbest_pipeline.fit(X_train, y_train)ny_pred = best_pipeline. predict(X_test)nntest_accuracy = accuracy_score(y_test, y_pred)nprint(f\"Test accuracy with selected features: {test_accuracy:.4f}\")nn to_replace=import pandas as pdnimport numpy as np nedit_file_by_replace(/workspace/pred_programs/pred_dkpes_visualization_3.py, nfrom sklearn.model_selection import train_test_splitnfrom sklearn. preprocessing import StandardScaler, OneHotEncodernfrom sklearn.linear_model import LogisticRegressionnfrom sklearn.metrics import accuracy_scorenfrom sklearn.compose import ColumnTransformernfrom sklearn.pipeline import Pipeline nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS nimport matplotlib.pyplot as plt, new_content=code)\", 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 \"thought\": \"Thank you for updating the file. see that there are some duplicate sections in the code. Let me clean it up and remove the unnecessary parts:\", \"is_confirmed\": \"confirmed\", \"kernel_init_code\": \"from agentskills import *\" } }, { \"id\": 16, \"timestamp\": \"2024-09-16T21:45:01.983049\", \"source\": \"agent\", \"message\": \"Code executed in IPython cell.\", \"cause\": 15, \"observation\": \"run_ipython\", \"content\": \"......\", \"extras\": { \"code\": \"......\" } ], [ } { \"id\": 17, \"timestamp\": \"2024-09-16T21:45:20.373176\", \"source\": \"agent\", \"message\": \"......\", \"action\": \"run_ipython\", \"args\": { \"code\": \"......\", \"thought\": \"I apologize for the confusion. It seems that the file still contains duplicate content. Let me clean it up one more time to remove all duplicates and ensure the code is correct:\", \"is_confirmed\": \"confirmed\", \"kernel_init_code\": \"from agentskills import *\" } }, { \"id\": 18, \"timestamp\": \"2024-09-16T21:45:20.397554\", \"source\": \"agent\", \"message\": \"Code executed in IPython cell.\", \"cause\": 17, \"observation\": \"run_ipython\", \"content\": \" Cell In[1], line 88n new_content=n ˆ nSyntaxError: incomplete inputn[Jupyter current working directory: /workspace/ benchmark/datasets]n[Jupyter Python interpreter: /openhands/poetry/openhands-ai -5O4_aCHf-py3.11/bin/python]\", \"extras\": { \"code\": \"......\" } ], [ } { \"id\": 19, \"timestamp\": \"2024-09-16T21:45:35.261112\", \"source\": \"agent\", \"message\": \"......\", \"action\": \"run_ipython\", 30 Preprint. Under review. 64 65 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 \"args\": { \"code\": \"......\", \"thought\": \"I apologize for the syntax error. Let me correct it and update the file properly:\", \"is_confirmed\": \"confirmed\", \"kernel_init_code\": \"from agentskills import *\" } }, { \"id\": 20, \"timestamp\": \"2024-09-16T21:45:35.480240\", \"source\": \"agent\", \"message\": \"Code executed in IPython cell.\", \"cause\": 19, \"observation\": \"run_ipython\", \"content\": \"......\", \"extras\": { \"code\": \"......\" } ], [ } { \"id\": 21, \"timestamp\": \"2024-09-16T21:45:52.166468\", \"source\": \"agent\", \"message\": \"......\", \"action\": \"run_ipython\", \"args\": { \"code\": \"......\", \"thought\": \"I apologize for the confusion. It seems that the file still contains duplicate content. Let me clean it up one final time to remove all duplicates and ensure the code is correct:\", \"is_confirmed\": \"confirmed\", \"kernel_init_code\": \"from agentskills import *\" } }, { \"id\": 22, \"timestamp\": \"2024-09-16T21:45:52.190187\", \"source\": \"agent\", \"message\": \"Code executed in IPython cell.\", \"cause\": 21, \"observation\": \"run_ipython\", \"content\": \" Cell In[1], line 88n new_content=n ˆ nSyntaxError: incomplete inputn[Jupyter current working directory: /workspace/ benchmark/datasets]n[Jupyter Python interpreter: /openhands/poetry/openhands-ai -5O4_aCHf-py3.11/bin/python]\", \"extras\": { \"code\": \"......\" } ], [ } { \"id\": 23, \"timestamp\": \"2024-09-16T21:46:10.802743\", \"source\": \"agent\", \"message\": \"......\", \"action\": \"run_ipython\", \"args\": { \"code\": \"......\", \"thought\": \"I apologize for the repeated mistake. Let me correct it and update the file properly:\", \"is_confirmed\": \"confirmed\", \"kernel_init_code\": \"from agentskills import *\" } }, { \"id\": 24, \"timestamp\": \"2024-09-16T21:46:11.030574\", \"source\": \"agent\", \"message\": \"Code executed in IPython cell.\", \"cause\": 23, \"observation\": \"run_ipython\", \"content\": \"......\", \"extras\": { \"code\": \"......\" } } ], 31 Preprint. Under review. 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 [ { \"id\": 25, \"timestamp\": \"2024-09-16T21:46:26.874027\", \"source\": \"agent\", \"message\": \"......\", \"action\": \"run_ipython\", \"args\": { \"code\": \"with open(/workspace/pred_programs/pred_dkpes_visualization_3.py, f.write(nimport pandas as pdnimport numpy as npnfrom w) as f:n sklearn.model_selection import train_test_splitnfrom sklearn.preprocessing import StandardScaler, OneHotEncodernfrom sklearn.linear_model import LogisticRegressionnfrom sklearn.metrics import accuracy_scorenfrom sklearn. compose import ColumnTransformernfrom sklearn.pipeline import Pipelinenfrom mlxtend.feature_selection import SequentialFeatureSelector as SFSnimport matplotlib.pyplot as pltnn# Load the datandata = pd.read_csv(/workspace/ benchmark/datasets/dkpes/dkpes_train.csv)nn# Prepare features and targetnX = data.drop([index, Signal-inhibition], axis=1)ny = data[Signal-inhibition transformers=[n (cat, OneHotEncoder(drop=first, sparse=False), ]nn# Identify numeric and categorical columnsnnumeric_features = X. select_dtypes(include=[int64, float64]).columnsncategorical_features = X. select_dtypes(include=[object]).columnsnn# Binarize the target variable nthreshold = y.median()ny_binary = (y > threshold).astype(int)nn# Split the datanX_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size =0.2, random_state=42)nn# Create preprocessing stepsnpreprocessor = ColumnTransformer(n numeric_features),n categorical_features)n logistic regressionnlr_pipeline = Pipeline([n ),n backward feature selectionnsfs = SFS(lr_pipeline,n (X.columns)),n scoring=accuracy,n X_train, y_train)nn# Get the resultsnresults = pd.DataFrame.from_dict(sfs. get_metric_dict()).Tnresults.sort_index(inplace=True)nn# Plot the results nplt.figure(figsize=(10, 6))nplt.plot(results.index, results[avg_score], marker=o)nplt.title(Backward Feature Selection: Accuracy vs Number of Features)nplt.xlabel(Number of Features)nplt.ylabel(Cross-validation Accuracy)nplt.grid(True)nplt.tight_layout()nn# Save the plotnplt.savefig (/workspace/pred_results/dkpes_feature_selection_analysis_pred.png)nn# Print (classifier, LogisticRegression(random_state=42))n])nn# Perform ])nn# Create pipeline with preprocessing and (preprocessor, preprocessor n_jobs=-1)nnsfs = sfs.fit( (num, StandardScaler(), k_features=(1, len floating=False,n forward=False,n cv=5,n the best feature subsetnbest_subset = list(sfs.k_feature_names_)nprint(f\" Best feature subset: {best_subset}\")nprint(f\"Number of selected features: { len(best_subset)}\")nprint(f\"Best cross-validation accuracy: {sfs.k_score_:.4f }\")nn# Evaluate on the test set using the best feature subsetnbest_pipeline = sfs.estimatornbest_pipeline.fit(X_train, y_train)ny_pred = best_pipeline. predict(X_test)nntest_accuracy = accuracy_score(y_test, y_pred)nprint(f\"Test accuracy with selected features: {test_accuracy:.4f}\")n)nnprint(\"File content updated successfully.\")\", \"thought\": \"I apologize for the persistent issue. Let me fix this once and for all by directly editing the file content:\", \"is_confirmed\": \"confirmed\", \"kernel_init_code\": \"from agentskills import *\" } }, { \"id\": 26, \"timestamp\": \"2024-09-16T21:46:26.901372\", \"source\": \"agent\", \"message\": \"Code executed in IPython cell.\", \"cause\": 25, \"observation\": \"run_ipython\", \"content\": \"File content updated successfully.n[Jupyter current working directory: /workspace/benchmark/datasets]n[Jupyter Python interpreter: / openhands/poetry/openhands-ai-5O4_aCHf-py3.11/bin/python]\", \"extras\": { \"code\": \"......\" } } ], ......(17 more turns omitted) 160 161 162 163 164 165 166 } 32 Preprint. Under review. E.2 INFLUENCE OF EXPERT-PROVIDED KNOWLEDGE In Section 4, we also find that expert-provided knowledge does not always lead to metric improvement because: (1) Expert-provided knowledge specifies some advanced tools that are less familiar to the agents. (2) When the agents do not know how to solve some tasks without expert-provided knowledge, they tend to generate some executable but less meaningful programs. As show in Listing E.3, without expert-provided knowledge, Claude-3.5-Sonnet using self-debug would only use basic tools like pandas and sklearn to develop random forest model, which is not accurate enough to find the best drugs to repurpose. To help the agent, an expert provided the following knowledge: Typical encoders for drug include ECFP fingerprint or messagepassing neural networks over the 2D molecular graph representation and for target include 1D convolutional neural networks over the amino acid sequence. With the help of expert-provided knowledge, the same agent successfully generates program to install the DeepPurpose package and use it do develop an advanced neural network with MPNN drug encoding and CNN target encoding (lines 4757 in Listing E.4). However, we noticed some signs of data contamination in this generated program, such as the use of automatic data loader in DeepPurpose (lines 3338 in Listing E.4). This issue is effectively captured by our strategies to mitigate data contamination and agent shortcut concerns (Section 2.2): The generated program (Listing E.4) is not executable as the automatic data loader does not read the modified local dataset correctly. Despite this issue, from scientist users perspective, the program generated with expert-provided knowledge tries to develop more helpful model and resembles the ground-truth program (Listing E.5) more closely. Listing E.3: Program generated by Claude-3.5-Sonnet using self-debug without expert-provided knowledge. with open(file_path, r) as f: return [line.strip() for line in f] return pd.read_csv(file_path, header=None).values 1 import numpy as np 2 import pandas as pd 3 from sklearn.model_selection import train_test_split 4 from sklearn.ensemble import RandomForestRegressor 5 from sklearn.preprocessing import StandardScaler 6 import json 7 import os 8 9 # Load and preprocess data 10 def load_data(file_path): 11 12 13 14 def load_affinity_data(file_path): 15 16 17 def load_target_sequences(file_path): 18 19 20 21 def encode_sequence(seq, max_length=100): 22 23 24 25 26 27 28 29 # Load data 30 drugs_train = load_data(benchmark/datasets/dti/DAVIS/drug_train.txt) 31 affinity_train = load_affinity_data(benchmark/datasets/dti/DAVIS/affinity_train.csv amino_acids = ACDEFGHIKLMNPQRSTVWY encoding = np.zeros(max_length) for i, aa in enumerate(seq[:max_length]): encoding[i] = amino_acids.index(aa) + 1 with open(file_path, r) as f: if aa in amino_acids: return json.load(f) return encoding ) 32 target_sequences = load_target_sequences(benchmark/datasets/dti/DAVIS/target_seq. json) return [smiles.count(c) for in CNOFPS] 33 34 # Encode drugs (simple fingerprint) 35 def encode_drug(smiles): 36 37 38 X_drugs = np.array([encode_drug(drug) for drug in drugs_train]) 39 X_targets = np.array([encode_sequence(seq) for seq in target_sequences.values()]) 40 41 # Prepare data for training 42 = np.column_stack([np.repeat(X_drugs, X_targets.shape[0], axis=0), 43 44 = affinity_train.flatten() np.tile(X_targets, (X_drugs.shape[0], 1))]) 33 Preprint. Under review. 45 46 # Scale features 47 scaler = StandardScaler() 48 X_scaled = scaler.fit_transform(X) 49 50 # Split data 51 X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) 52 53 # Train model 54 model = RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state =42) 55 model.fit(X_train, y_train) 56 57 # Load COVID-19 target and antiviral drugs 58 covid_seq = load_data(benchmark/datasets/dti/covid_seq.txt)[1] # Second line contains the sequence 59 antiviral_drugs = pd.read_csv(benchmark/datasets/dti/antiviral_drugs.tab, sep=t) 60 61 # Predict binding affinities 62 covid_encoding = encode_sequence(covid_seq) 63 X_pred = np.column_stack([np.array([encode_drug(smiles) for smiles in antiviral_drugs [SMILES]]), np.tile(covid_encoding, (len(antiviral_drugs), 1))]) 64 65 X_pred_scaled = scaler.transform(X_pred) 66 67 predictions = model.predict(X_pred_scaled) 68 69 # Rank drugs based on predicted affinities 70 ranked_drugs = antiviral_drugs.copy() 71 ranked_drugs[Predicted_Affinity] = predictions 72 ranked_drugs = ranked_drugs.sort_values(Predicted_Affinity) 73 74 # Save results 75 os.makedirs(pred_results, exist_ok=True) 76 with open(pred_results/davis_dti_repurposing.txt, w) as f: 77 78 79 80 print(\"Predictions saved to pred_results/davis_dti_repurposing.txt\") for smiles in ranked_drugs[SMILES]: f.write(f\"{smiles}n\") Listing E.4: Program generated by Claude-3.5-Sonnet using self-debug with expert-provided knowledge. 1 import os 2 import sys 3 import subprocess 4 5 # Function to install required packages using pip 6 def install_pip(package): 7 8 9 # Check and install required packages 10 required_packages = [pandas, rdkit, DeepPurpose, git+https://github.com/bpsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package]) kelley/descriptastorus, pandas-flavor] try: else: else: except ImportError: install_pip(package) install_pip(rdkit-pypi) if package.startswith(git+): __import__(package.split(==)[0]) print(f\"{package} not found. Installing...\") if package == rdkit: 11 for package in required_packages: 12 13 14 15 16 17 18 19 20 21 22 23 24 # Now that weve ensured all packages are installed, import the required modules 25 import numpy as np 26 import pandas as pd 27 from DeepPurpose import DTI as models 28 from DeepPurpose import utils, dataset 29 30 # Create directories if they dont exist 31 os.makedirs(pred_results, exist_ok=True) 32 33 # Load and preprocess DAVIS dataset 34 X_drug, X_target, = dataset.load_process_DAVIS( 35 # Use rdkit-pypi for pip installation path=benchmark/datasets/dti/DAVIS, install_pip(package) 34 Preprint. Under review. # Changed to Label binary=False, convert_to_log=True drug: X_drug, target: X_target, Label: 36 37 38 ) 39 40 # Convert numpy arrays to pandas DataFrames 41 train = pd.DataFrame({ 42 43 44 45 }) 46 47 # Load drug-target interaction model 48 config = utils.generate_config( drug_encoding=MPNN, 49 target_encoding=CNN, 50 cls_hidden_dims=[1024, 1024, 512], 51 train_epoch=5 52 53 ) 54 model = models.model_initialize(**config) 55 56 # Train the model 57 model.train(train) 58 59 # Load antiviral drugs 60 antiviral_drugs = pd.read_csv(benchmark/datasets/dti/antiviral_drugs.tab, sep=t) 61 62 # Load COVID-19 target sequence 63 with open(benchmark/datasets/dti/covid_seq.txt, r) as f: 64 65 66 # Prepare data for prediction 67 X_drug_repurpose = antiviral_drugs[SMILES].tolist() 68 X_target_repurpose = [covid_seq] * len(X_drug_repurpose) 69 70 # Make predictions 71 y_pred = model.predict(X_drug_repurpose, X_target_repurpose) 72 73 # Create dataframe with predictions 74 results = pd.DataFrame({ 75 76 77 78 }) 79 80 # Sort by predicted affinity (lower values indicate stronger binding) 81 results_sorted = results.sort_values(Predicted_Affinity) 82 83 # Save results 84 results_sorted[SMILES].to_csv(pred_results/davis_dti_repurposing.txt, index=False Drug: antiviral_drugs[Name], SMILES: antiviral_drugs[SMILES], Predicted_Affinity: y_pred covid_seq = f.read().strip().split(n)[0] , header=False) 85 86 print(\"Prediction completed. Results saved to pred_results/davis_dti_repurposing.txt\" ) Listing E.5: Ground-truth program in the benchmark. 1 from DeepPurpose import utils, dataset 2 from DeepPurpose import DTI as models 3 from pathlib import Path 4 from shutil import copyfile 5 6 import os 7 import json 8 import numpy as np 9 import pandas as pd 10 11 drug_encoding, target_encoding = MPNN, CNN 12 13 def make_dataset(drug_fname, affinity_fname, target): 14 15 16 17 18 19 20 21 22 23 24 SMILES = [] Target_seq = [] = [] drug = [l.rstrip() for in f] for in range(len(target)): with open(drug_fname) as f: for in range(len(drug)): affinity = pd.read_csv(affinity_fname, header=None) 35 Preprint. Under review. 25 26 27 28 29 30 31 32 33 34 35 36 37 def main(): 38 39 40 41 42 SMILES.append(drug[i]) Target_seq.append(target[j]) y.append(affinity.values[i, j]) = utils.convert_y_unit(np.array(y), nM, p) return utils.data_process(np.array(SMILES), np.array(Target_seq), np.array(y), drug_encoding, target_encoding, split_method=no_split) with open(benchmark/datasets/dti/DAVIS/target_seq.json) as f: target = json.load(f) target = list(target.values()) train = make_dataset(benchmark/datasets/dti/DAVIS/drug_train.txt, benchmark/ datasets/dti/DAVIS/affinity_train.csv, target) val = make_dataset(benchmark/datasets/dti/DAVIS/drug_val.txt, benchmark/ datasets/dti/DAVIS/affinity_val.csv, target) config = utils.generate_config(drug_encoding = drug_encoding, target_encoding = target_encoding, cls_hidden_dims = [1024,1024,512], train_epoch = 10, LR = 5e-4, batch_size = 128, hidden_dim_drug = 128, mpnn_hidden_size = 128, mpnn_depth = 3, cnn_target_filters = [32,64,96], cnn_target_kernels = [4,8,12] ) model = models.model_initialize(**config) model.train(train, val, val) t, t_name = [l.rstrip() for in open(benchmark/datasets/dti/covid_seq.txt)] df = pd.read_csv(benchmark/datasets/dti/antiviral_drugs.tab, sep = t) r, r_name, r_pubchem_cid = df.SMILES.values, df[Name].values, df[Pubchem CID ].values out_fpath = Path(\"./pred_results/result/\") if not out_fpath.exists(): os.mkdir(out_fpath) y_pred = models.repurpose(X_repurpose = r, target = t, model = model, drug_names = r_name, target_name = t_name, result_folder = \"./pred_results/result/\", convert_y = True) 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 66 67 68 69 70 71 72 with open(\"./pred_results/result/repurposing.txt\") as f_in: lines = [l for in f_in] with open(\"./pred_results/davis_dti_repurposing.txt\", \"w+\") as f_out: f_out.write(\"\".join(lines[3:-1])) 73 74 75 76 77 78 79 80 81 if __name__ == \"__main__\": 82 main() 36 Preprint. Under review. E.3 OPENAI O1 REASONING In our experiments (Section 4), OpenAI o1 has demonstrated significant gain over other LLMs with both direct prompting and self-debug. Since its API does not return any intermediate reasoning information, we access OpenAI o1 through ChatGPT (https://chatgpt.com/), which returns summary of OpenAI o1s inference-time reasoning, and present case study. We hypothesize that the low-level plans generated for each task during reasoning are essential for writing correct programs and identify three strategies OpenAI o1 might have been trained to use: (1) Reiterate Task Goal and Requirements: As its first step, OpenAI o1 usually repeats and paraphrases the task instruction to emphasize the goal (Im tasked with writing Python program to load Total Electron Content (TEC) data from space weather NetCDF file, visualize it, and save it as an image. and Im working on Python program to train graph convolutional network (GCN) for predicting the blood-brain barrier permeability of compounds using the given SDF files for training and testing.). In the subsequent steps, it also repeats other parts of the prompt, such as constraints for the output program: Im tasked with creating full Python script, avoiding incomplete or partial code descriptions, and ensuring no interactive Python commands are included. (2) Sketch Implementation Plan: With the goal and requirements identified, OpenAI o1 then sketches detailed implementation plan to import necessary packages, load the dataset, implement the model or analysis, and save the outcome. During plan generation, we find that OpenAI o1 frequently uses bridging objects to maintain the relevance and coherence of each step, similar to how chain-of-thought prompting functions (Wang et al., 2023a), e.g., Im tasked with writing Python program to load Total Electron Content (TEC) data from space weather NetCDF file, visualize it, and save it as an image. Im pulling together Python program to load NetCDF data... (3) Refine the Plan In-Place: When sketching the plan, OpenAI o1 also tries to refine its previous steps in-place by describing some adjustments or improvements: Adjusting the approach: reshaped the latitude and longitude arrays for accurate mapping and ensured the TEC variable is 2D. and Simplifying atom features: Im considering atom features using one-hot encoding of atomic numbers up to 20 or the atomic number itself. Task: Load Total Electron Content (TEC) data from space weather NetCDF file and visualize it as filled contour map with geographical and coastline background. Loading and visualizing Im tasked with writing Python program to load Total Electron Content (TEC) data from space weather NetCDF file, visualize it, and save it as an image. Loading and examining NetCDF data Im pulling together Python program to load NetCDF data and create filled contour map with geographic and coastline background. Crafting the solution Im tasked with creating full Python script, avoiding incomplete or partial code descriptions, and ensuring no interactive Python commands are included. Weighing visualization tools Im assessing libraries for visualizing spatial data, focusing on Cartopy for coastlines and xarray for NetCDF files. Considering other options too, but these stand out. Leveraging Cartopy Im noting the transition from Basemap to Cartopy for data visualization, ensuring the use of xarray, matplotlib, and Cartopy, and mapping TEC data with coastlines. Importing and loading start by importing libraries and loading the dataset. The variables TEC, latitude, and longitude are extracted for visualization. Adjusting the approach reshaped the latitude and longitude arrays for accurate mapping and ensured the TEC variable is 2D. Using pcolormesh for plotting, included coastlines and features, saving the figure as TEC vis.png. Mapping the process Im confirming the use of TEC as an xarray DataArray. Noting that x=rLon, y=rLat ensures accurate plotting with coordinates in TEC.plot.pcolormesh. Ensuring plot accuracy Im verifying variable existence and adjusting the coordinate reference system. The code readily supports global data visualization. Preprint. Under review. Task: Train graph convolutional network on the given dataset to predict the blood-brain barrier permeability of compounds. Training the model Im working on Python program to train graph convolutional network (GCN) for predicting the blood-brain barrier permeability of compounds using the given SDF files for training and testing. Laying out the steps Im mapping out Python program to read SDF files, convert molecules to graph representations, prepare data for GCN, define the GCN model, train, evaluate, and save results. Piecing together Im integrating predictions into pred results/brain blood qsar.csv. Using RDKit for molecule handling and PyTorch Geometric for GCN is acceptable. Progressing towards extracting insights effectively. Setting up imports Im picking essential imports for RDKit, PyTorch, and PyTorch Geometric to handle molecules and graphs effectively. The subsequent steps will involve defining custom dataset class and GCN model. Crafting custom Dataset Im piecing together custom Dataset class to handle train and test datasets from separate SDF files. This involves extracting molecular features like atom types and edge indices, defining function to convert molecules into Data objects, and setting up DataLoaders for efficient data handling. Converting molecules Im working through converting molecules into torch geometric Data objects. This involves converting atomic features, setting up edge indices and attributes, and ensuring labels are included if they exist. Transforming features OK, let me see. Converting non-numeric values like hybridization and IsAromatic to numbers. Using PyTorch Geometric for molecular graphs seems promising. Focusing on functionality Rather than making the code complex, simplifying and ensuring accuracy is key. Training set labels are under <logBB class> while test set labels might be absent. Setting up the dataset Im creating MoleculeDataset class to handle graph-based data from an SDF file. The class initializes with an SDF file, processes molecules, and retrieves data for given index, including optional labels. Charting the outcomes Im charting the architecture (GCN model) with two graph convolutional layers and single output neuron. Using torch.nn.functional.global mean pool for pooling the node features, the final layer outputs single value, ideally for binary classification. Simplifying atom features Im considering atom features using one-hot encoding of atomic numbers up to 20 or the atomic number itself. For bond features, Im leaning towards bond type as double. Converting molecular data Im assembling function to transform molecular data into graph structure using torch tensors. Atom features come from atomic numbers, while edges are determined by bond indices. Integrating atomic embeddings Im integrating an embedding layer to map atomic numbers from 1 to 53 into GCN model. This embarks on journey to enhance feature representation and improve model performance. Preparing the pipeline Im assemblying the datasets and loaders to get the data ready. Using Graph Convolutional Network model, Im managing the training loop with BCEWithLogitsLoss criterion and an Adam optimizer. Evaluating predictions Im noting that the task involves evaluating model on test set and saving the predictions under specific column in CSV file. This requires extracting and storing molecule IDs from the test set. Including mol id Im working on the mol to graph data obj function to ensure it includes mol id, which helps with test set prediction and results storage. This change will streamline handling test set predictions effectively. Gathering predictions and mol ids Adjusting batch size Im working through setting batch size=1 for the test loader to ensure mol ids are correctly collected from each data point. This approach seems effective for the task. Organizing the data Im compiling mol id and label into pandas DataFrame. Setting up pred results directory, Ill save the CSV file as brain blood qsar.csv. 38 Preprint. Under review."
        },
        {
            "title": "F EXPERT VALIDATION DETAILS",
            "content": "In this section, we provide details about the expert validation process in Section 2.2. We include the questionnaire (Section F.1) for subject matter experts and two examples used in it (Section F.2 and F.3). F.1 QUESTIONNAIRE FOR SUBJECT MATTER EXPERTS Thanks for providing feedback on our AI4Science benchmark called ScienceAgentBench. We are developing an AI agent to assist you! Given task instruction and dataset, the agent will help you write computer program to fulfill the task you have in mind. To develop and evaluate such an AI agent, we have collected benchmark by adapting some tasks from peer-reviewed publications with open-source codes. Each data sample in our benchmark consists of the following main components: Task Instruction: Describes (1) the goal of task or scientific hypothesis and (2) output requirements. Dataset Information: Contains (1) the dataset directory structure and (2) helpful metadata or few examples from the dataset. Annotated Program: The reference solution adapted from each publications open-source code. Evaluation Script: The code to evaluate AI agents performance by comparing the execution results of its generated programs with those of the annotated programs. To ensure that each task is formulated and described correctly and professionally, we would like you to give us hand by reviewing our collected data samples. In addition, we are also seeking some additional information from you as domain expert, including writing down some task-related domain knowledge and revising rubric to score the generated programs. Please follow the guidelines below to review each task. First, please enter the Task ID you are reviewing: [task id] Guidelines for Data Reviewing First, bit more background: once you give the AI agent task instruction, it will try to automatically complete everything without seeking additional help from you. This is similar to the scenario where you give the task to junior student in your lab/class who will complete it as an assignment. For each task, please first spend few minutes reading the given task information (instruction, dataset information, and source GitHub repository) and our annotated program to have rough understanding of the task and relevant concepts. Then, please comment on the following two parts. Note that you may iteratively revise your answer to each question to help us improve the task instructions and programs. 1. Program Is the program valid solution (not necessarily the best solution) to the given task instruction? Here is an example: [google doc link] If there are only minor issues, please comment on how the program should be modified below. However, if you believe there is major issue (e.g., the program is doing sth irrelevant or more than two lines of code need to be revised in order to make it correct), please let us know the task ID and do NOT fill the rest of the form. Is the program valid solution to the given task instruction? [] Yes [] Need Modification (comment below) [] No (report and continue to the next task) How should the program be modified? Please mention the line numbers that need to be inspected. [Long Text Answer] 2. Task Instruction The task instructions were created by non-experts and thus might contain some misused terms, awkward expressions, or inaccurate descriptions of the task that do not adhere to your domains scientific language. Do you see such issues for this task instruction? If so, please revise or rewrite the task instruction for any issues you can find. Finally, if needed, please help make the task instruction more fluent and natural sounding. Please enter your revised task instruction below. If there are no changes, please skip this question and leave the answer text blank. [Long Text Answer] aThe example for program and instruction validation is provided in Section F.2. 39 Preprint. Under review. 3. Domain Knowledge Suppose the AI agent fails to fulfill the task based solely on the task instruction, perhaps due to lack of some background knowledge, we want to provide some additional information to help it succeed. This is similar to the situation where you give an exam problem to student in your class and they might not be able to do it just based on the problem description, but you can provide some hints to help them. Please write down at most three important pieces of knowledge that are related to the task and the program. For example ([google doc link]): Concepts and details in the task description or program that may need further explanation or extra attention, e.g., term definition on wikipedia you would send to new student (without much domain expertise) working on this task, or common practice for such tasks in your field. Information about the python packages and/or functions used in the program. For example, you would copy and paste snippet of package description/function documentation to help the new student working on the task. You may assume the AI agent has general sense of your domain, like new graduate student with undergrad-level knowledge but not much about the specific task. Please help it by providing some knowledge to write the program for this task. You can search online for more details about the dataset, packages, and functions used in each task before writing. Please try not to leak the annotated program directly. You may imagine that you dont have the direct answer but could provide some helpful information to your junior colleague so that they can derive the program. For example: Instead of copying/describing few lines in the program, you may copy the documentations of packages/functions used in that program. Instead of specifying the variables and parameters, you may suggest range (e.g., 1e-3 to 1e-4 for learning rate). Instead of saying columns A,B,C are related to the target attribute Y, you may try to find knowledge snippet describing what is correlated to Y. However, in some rare cases, there may be need to provide minimal leak of the annotated program, e.g., the decision boundary of is 0.6 instead of 0.5. Still, it would be great if you could think about its necessity before annotating such knowledge. For each piece of domain knowledge related to this task, please write 1-5 sentences. If you believe the task instruction is self-contained and needs no further explanations, please enter None. [Long Text Answer] 4. Scoring Rubric Once the AI agent generates program, we need an evaluation method to review the generated program. To do it, we need task-specific scoring rubric, which assigns partial credits for more comprehensive evaluation of the generated program. Right now we have already got an initial draft of the rubric with five major components: (1) data loading, (2) data processing, (3) modeling, analysis or visualization, (4) output formatting, (5) saving output. Please review our initial draft of the rubric. Imagine that you will use this rubric to score the programs produced by your junior students. Please modify the rubric items that you think are incorrect, should be described with more/less details, or should be reweighed with higher/lower credits for each component. Please also add any missing but necessary rubric item you would use to assess programs correctness, or remove redundant rubric items. Please enter your revised scoring rubric below. If there are no changes, please skip this question and leave the answer text blank. [Long Text Answer] aThe example for domain knowledge annotation is provided in Section F.3. 40 Preprint. Under review. F.2 PROGRAM EXAMPLE FOR SUBJECT MATTER EXPERTS Task Instruction: Train graph convolutional network on the given dataset to predict the aquatic toxicity of compounds. Use the resulting model to compute and visualize the atomic contributions to molecular activity of the given test example compound. Save the figure as pred results/aquatic toxicity qsar vis.png. Program: wt[atom] = df.loc[mol.GetProp(\"_Name\"),\"Contrib\"][n] for n,atom in enumerate(range(mol.GetNumHeavyAtoms())): wt[atom] = df.loc[Chem.MolToSmiles(mol),\"Contrib\"][n] return SimilarityMaps.GetSimilarityMapFromWeights(mol,wt) Chem.rdmolfiles.CanonicalRankAtoms(mol) ) ): if smi_or_sdf == \"sdf\": for n,atom in enumerate( DATASET_FILE = os.path.join( mols = [ for in Chem.SDMolSupplier(DATASET_FILE) if is not None wt = {} if smi_or_sdf == \"smi\": ] loader = dc.data.SDFLoader( benchmark/datasets/aquatic_toxicity, Tetrahymena_pyriformis_OCHEM.sdf 1 import os 2 os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" 3 4 from rdkit import Chem 5 from rdkit.Chem.Draw import SimilarityMaps 6 7 import pandas as pd 8 import deepchem as dc 9 10 def vis_contribs(mol, df, smi_or_sdf = \"sdf\"): 11 12 13 14 15 16 17 18 19 20 21 22 def main(): 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 ) frag_dataset = loader.create_dataset( ][0] test_dataset = loader.create_dataset( tasks=[], featurizer=dc.feat.ConvMolFeaturizer( 1, mode=\"regression\", batch_normalize=False ) m.fit(dataset, nb_epoch=40) tasks=[\"IGC50\"], featurizer=dc.feat.ConvMolFeaturizer(), sanitize=True TEST_DATASET_FILE, shard_size=5000 TEST_DATASET_FILE, shard_size= TEST_DATASET_FILE = os.path.join( = dc.models.GraphConvModel( ), sanitize=True per_atom_fragmentation=True loader = dc.data.SDFLoader( ) test_mol = [ benchmark/datasets/aquatic_toxicity, Tetrahymena_pyriformis_OCHEM_test_ex.sdf ) for in Chem.SDMolSupplier(TEST_DATASET_FILE) if is not None 41 ) dataset = loader.create_dataset(DATASET_FILE, shard_size=5000) Preprint. Under review. tr = dc.trans.FlatteningTransformer(frag_dataset) frag_dataset = tr.transform(frag_dataset) pred = m.predict(test_dataset) pred = pd.DataFrame( pred, index=test_dataset.ids, columns=[\"Molecule\"] ) ) 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 if __name__ == \"__main__\": main() ) ) pred_frags = m.predict(frag_dataset) pred_frags = pd.DataFrame( pred_frags, index=frag_dataset.ids, columns=[\"Fragment\"] df = pd.merge(pred_frags, pred, right_index=True, left_index=True) df[Contrib] = df[\"Molecule\"] - df[\"Fragment\"] vis = vis_contribs(test_mol, df) vis.savefig( \"pred_results/aquatic_toxicity_qsar_vis.png\", bbox_inches=tight Explanation: In this example, there are three key points in the instruction: (1) GCN training (lines 28-45), (2) calculating atomic contribution (lines 76-91), and (3) visualizing atomic contribution (lines 10-20). In this case, you can select Yes for the first question and move on. Suppose the given program is not training GCN at line 33 but, say, simple feed-forward neural network, you may select Need Modification and comment Line 33 in the follow-up question. However, if more than three lines of code have errors, please select No. More clarifications: (1) The annotated program should be treated as reference solution to the task. As you may have already noticed, these tasks are open-ended and can have multiple valid solutions. So, although the annotated program may import certain classes and packages, we dont want to force the agent to necessarily do the same in the task instruction. But, if you find the classes and packages helpful, feel free to mention them as domain knowledge. (2) The agents will be able to install packages for themselves via pip. For example, if it chooses to use mastml, it should use pip install mastml to set itself up. During annotation, we tried to make sure that all packages are distributed via pip so that the agent should be able to install, but there might be few mistakes. If the program uses something that is not available via pip but is critical to completing the task, please let us know. 42 Preprint. Under review. F.3 KNOWLEDGE EXAMPLE PROVIDED TO SUBJECT MATTER EXPERTS DURING"
        },
        {
            "title": "ANNOTATION",
            "content": "Task Instruction: Train (1) graph convolutional network on the given dataset to predict the aquatic toxicity of compounds. Use the resulting model to (2) compute and (3) visualize the atomic contributions to molecular activity of the given test example compound. Save the figure as pred results/aquatic toxicity qsar vis.png. Program: 1 import os 2 os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" 3 4 from rdkit import Chem 5 from rdkit.Chem.Draw import SimilarityMaps 6 7 import pandas as pd 8 import deepchem as dc 9 10 ###### 11 # (3) This part defines function for visualizing atomic contributions. One relevant piece of domain knowledge you might want to provide to the AI agent or your junior student working on this task is about how to draw atomic contributions ( with rdkit), e.g., by mentioning the required functions. Chem.rdmolfiles.CanonicalRankAtoms(mol) wt[atom] = df.loc[mol.GetProp(\"_Name\"),\"Contrib\"][n] for n,atom in enumerate(range(mol.GetNumHeavyAtoms())): wt[atom] = df.loc[Chem.MolToSmiles(mol),\"Contrib\"][n] return SimilarityMaps.GetSimilarityMapFromWeights(mol,wt) ): if smi_or_sdf == \"sdf\": for n,atom in enumerate( wt = {} if smi_or_sdf == \"smi\": 12 13 def vis_contribs(mol, df, smi_or_sdf = \"sdf\"): 14 15 16 17 18 19 20 21 22 23 24 25 ###### 26 27 def main(): 28 29 30 31 32 33 34 benchmark/datasets/aquatic_toxicity, Tetrahymena_pyriformis_OCHEM.sdf DATASET_FILE = os.path.join( ) ###### # (1) This part loads the data and trains GCN. One relevant piece of domain knowledge you might want to provide to the AI agent or your junior student working on this task is about what IGC50 means and why that column is the gold label for aquatic toxicity. 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 mols = [ for in Chem.SDMolSupplier(DATASET_FILE) if is not None ] loader = dc.data.SDFLoader( tasks=[\"IGC50\"], featurizer=dc.feat.ConvMolFeaturizer(), sanitize=True ) dataset = loader.create_dataset(DATASET_FILE, shard_size=5000) = dc.models.GraphConvModel( 1, mode=\"regression\", batch_normalize=False ) m.fit(dataset, nb_epoch=40) ###### TEST_DATASET_FILE = os.path.join( benchmark/datasets/aquatic_toxicity, Tetrahymena_pyriformis_OCHEM_test_ex.sdf ) test_mol = [ for in Chem.SDMolSupplier(TEST_DATASET_FILE) 43 Preprint. Under review. 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 if is not None ][0] test_dataset = loader.create_dataset( TEST_DATASET_FILE, shard_size=5000 ) loader = dc.data.SDFLoader( tasks=[], featurizer=dc.feat.ConvMolFeaturizer( per_atom_fragmentation=True ), sanitize=True ) frag_dataset = loader.create_dataset( TEST_DATASET_FILE, shard_size=5000 ) tr = dc.trans.FlatteningTransformer(frag_dataset) frag_dataset = tr.transform(frag_dataset) ###### # (2) This part uses the trained GCN to predict the test examples toxicity and calculate the atomic contributions. One relevant piece of domain knowledge you might want to provide to the AI agent or your junior student working on this task is about how atomic contributions may be calculated, i.e. predicting the toxicity of the complete compound and those of compound fragments (with one atom removed), then making subtraction to find the contribution of the removed atom. pred_frags = m.predict(frag_dataset) pred_frags = pd.DataFrame( pred_frags, index=frag_dataset.ids, columns=[\"Fragment\"] pred = m.predict(test_dataset) pred = pd.DataFrame( pred, index=test_dataset.ids, columns=[\"Molecule\"] ) 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 if __name__ == \"__main__\": 115 ###### main() ) ) vis = vis_contribs(test_mol, df) vis.savefig( \"pred_results/aquatic_toxicity_qsar_vis.png\", bbox_inches=tight df = pd.merge(pred_frags, pred, right_index=True, left_index=True) df[Contrib] = df[\"Molecule\"] - df[\"Fragment\"] 44 Preprint. Under review."
        },
        {
            "title": "G RUBRIC EXAMPLES",
            "content": "In this section, we show two rubrics generated by GPT-4o (Listing G.1, G.3) and their final versions revised by subject matter experts (Listing G.2, G.4). Listing G.1: An example rubric of Computational Chemistry task generated by GPT-4o without expert revision. 1 { 2 3 4 5 \"data_loading\": [ { \"name\": \"Initialize Data Loader for Training\", \"description\": \"Successfully initializes the MyClintoxLoader object for training data with correct parameters: featurizer=ECFP, tasks=[FDA_APPROVED, CT_TOX], feature_field=smiles, and correct file path benchmark/datasets/ 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 clintox/clintox_train.csv.\", \"points\": 5 }, { \"name\": \"Load Training Dataset\", \"description\": \"Successfully loads the training dataset using the train_loader object and correctly assigns the dataset to train_dataset.\", \"points\": 5 }, { \"name\": \"Initialize Data Loader for Testing\", \"description\": \"Successfully initializes the MyClintoxLoader object for test data with correct parameters: featurizer=ECFP, tasks=[FDA_APPROVED, CT_TOX ], feature_field=smiles, and correct file path benchmark/datasets/clintox/ clintox_test.csv.\", \"points\": }, { \"name\": \"Load Testing Dataset\", \"description\": \"Successfully loads the test dataset using the test_loader object and correctly assigns the dataset to test_dataset.\", \"points\": 5 } ], \"data_processing\": [ { \"name\": \"Transform Data\", \"description\": \"Applies required transformations to the training dataset using the specified transformers (e.g., balancing).\", \"points\": 5 } ], \"modeling_or_analysis_or_visualization\": [ { \"name\": \"Initialize Model\", \"description\": \"Successfully initializes the MultitaskClassifier with parameters: number of tasks equal to length of CLINTOX_TASKS, n_features=1024, layer_sizes=[1000], dropouts=[0.25], learning_rate=0.001, and batch_size=50.\", \"points\": 10 }, { \"name\": \"Fit Model\", \"description\": \"Successfully fits the model using the train_dataset.\", \"points\": }, { \"name\": \"Predict Using Model\", \"description\": \"Successfully uses the trained model to predict scores on the test_dataset, correctly applying any necessary test_transformers.\", \"points\": 10 } ], \"output_formatting\": [ { \"name\": \"Format Output DataFrame\", \"description\": \"Creates pandas DataFrame named test_scores_df containing smiles, FDA_APPROVED, and CT_TOX columns with correctly assigned test scores.\", \"points\": 5 } ], \"output_saving\": [ { \"name\": \"Save Predictions to CSV\", 45 Preprint. Under review. \"description\": \"Correctly saves the test_scores_df to CSV file at pred_results/clintox_test_pred.csv without an index.\", \"points\": 5 } ], \"total_points\": 65 58 59 60 61 62 63 } Listing G.2: An example rubric of Computational Chemistry task revised by an expert. 1 { 2 3 4 5 \"data_loading\": [ { \"name\": \"Initialize Data Loader for Training\", \"description\": \"Successfully initializes the MyClintoxLoader object for training data with correct parameters: featurizer=ECFP, tasks=[FDA_APPROVED, CT_TOX], feature_field=smiles, and correct file path benchmark/datasets/ 6 7 8 9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 52 53 54 55 56 clintox/clintox_train.csv.\", \"points\": 10 }, { \"name\": \"Load Training Dataset\", \"description\": \"Successfully loads the training dataset using the train_loader object and correctly assigns the dataset to train_dataset.\", \"points\": 5 }, { \"name\": \"Initialize Data Loader for Testing\", \"description\": \"Successfully initializes the MyClintoxLoader object for test data with correct parameters: featurizer=ECFP, tasks=[FDA_APPROVED, CT_TOX ], feature_field=smiles, and correct file path benchmark/datasets/clintox/ clintox_test.csv.\", \"points\": 10 }, { \"name\": \"Load Testing Dataset\", \"description\": \"Successfully loads the test dataset using the test_loader object and correctly assigns the dataset to test_dataset.\", \"points\": 5 } ], \"data_processing\": [ { } \"name\": \"Transform Data\", \"description\": \"Applies required transformations when loading training and test datasets using the specified transformers (i.e., balancing).\", \"points\": 5 ], \"modeling_or_analysis_or_visualization\": [ { \"name\": \"Initialize Model\", \"description\": \"Successfully initializes the MultitaskClassifier with parameters: number of tasks equal to length of CLINTOX_TASKS, n_features=1024, layer_sizes=[1000], dropouts=[0.25], learning_rate=0.001, and batch_size=50.\", \"points\": }, { \"name\": \"Fit Model\", \"description\": \"Successfully fits the model using the train_dataset.\", \"points\": 10 }, { \"name\": \"Predict Using Model\", \"description\": \"Successfully uses the trained model to predict scores on the test_dataset, correctly applying any necessary test_transformers.\", \"points\": } ], \"output_formatting\": [ { \"name\": \"Format Output DataFrame\", \"description\": \"Creates pandas DataFrame named test_scores_df containing smiles, FDA_APPROVED, and CT_TOX columns with correctly assigned test scores.\", \"points\": } ], \"output_saving\": [ { 46 Preprint. Under review. \"name\": \"Save Predictions to CSV\", \"description\": \"Correctly saves the test_scores_df to CSV file at pred_results/clintox_test_pred.csv without an index.\", \"points\": 5 } ], \"total_points\": 80 57 58 59 60 61 62 63 } 47 Preprint. Under review. Listing G.3: An example rubric of Geographical Information Science task generated by GPT-4o without expert revision. 1 { 2 3 4 5 \"data_loading\": [ { \"name\": \"Load Bathymetry Data\", \"description\": \"Correctly loads the bathymetry raster data from the path benchmark/datasets/CoralSponge/CatalinaBathymetry.tif.\", 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \"points\": 10 }, { \"name\": \"Load Coral and Sponge Data\", \"description\": \"Correctly reads the coral and sponge data from the path benchmark/datasets/CoralSponge/CoralandSpongeCatalina.geojson.\", \"points\": 10 }, { \"name\": \"CRS Transformation\", \"description\": \"Correctly transforms the CRS of the GeoDataFrame to EPSG :4326.\", \"points\": 5 } ], \"data_processing\": [ { \"name\": \"Elevation Conversion\", \"description\": \"Correctly converts elevation values by multiplying with -1.\", \"points\": 10 }, { \"name\": \"Calculate Gradient\", \"description\": \"Accurately calculates the gradient (grad_x, grad_y) using numpy gradient function.\", \"points\": 10 }, { \"name\": \"Calculate Slope\", \"description\": \"Correctly calculates the slope in degrees from the gradients.\", \"points\": 10 }, { \"name\": \"Calculate Aspect\", \"description\": \"Correctly calculates the aspect in degrees and adjusts any negative values.\", \"points\": 10 }, { \"name\": \"Coordinate to Raster Index Conversion\", \"description\": \"Correctly implements the function to convert coordinates to raster grid indices.\", \"points\": 5 }, { \"name\": \"Extract Slope and Aspect\", \"description\": \"Extracts slope and aspect values for each point in the GeoDataFrame correctly.\", \"points\": 10 }, { \"name\": \"Add Slope and Aspect to GeoDataFrame\", \"description\": \"Successfully adds the extracted slope and aspect values as new columns to the GeoDataFrame.\", \"points\": 5 }, { \"name\": \"Group by VernacularNameCategory\", \"description\": \"Correctly groups the GeoDataFrame by VernacularNameCategory and computes mean values for slope and aspect.\", \"points\": 5 } ], \"modeling_or_analysis_or_visualization\": [ { \"name\": \"Bar Plot for Mean Slope\", \"description\": \"Correctly creates bar plot showing the mean slope per species .\", \"points\": 10 }, { \"name\": \"Bar Plot for Mean Aspect\", Preprint. Under review. 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 \"description\": \"Correctly creates bar plot showing the mean aspect per species.\", \"points\": 10 } ], \"output_formatting\": [ { } \"name\": \"Plot Descriptions\", \"description\": \"Properly sets plot titles, axis labels, and ensures x-ticks are rotated for readability.\", \"points\": ], \"output_saving\": [ { \"name\": \"Save Plots\", \"description\": \"Saves the plots as mean_slope_per_species.png, mean_aspect_per_species.png, and pred_results/CoralandSponge.png.\", \"points\": 5 } ], \"total_points\": 84 85 86 87 88 } Listing G.4: An example rubric of Geographical Information Science task revised by an expert. 1 { 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 \"data_loading\": [ { \"name\": \"Load Bathymetry Data\", \"description\": \"Correctly loads the bathymetry raster data from the path benchmark/datasets/CoralSponge/CatalinaBathymetry.tif.\", \"points\": 5 }, { \"name\": \"Load Coral and Sponge Data\", \"description\": \"Correctly reads the coral and sponge data from the path benchmark/datasets/CoralSponge/CoralandSpongeCatalina.geojson.\", \"points\": 5 }, { \"name\": \"CRS Transformation\", \"description\": \"Correctly transforms the CRS of the GeoDataFrame to EPSG :4326.\", \"points\": 5 } ], \"data_processing\": [ { \"name\": \"Elevation Conversion\", \"description\": \"Correctly converts elevation values by multiplying with -1.\", \"points\": 5 }, { \"name\": \"Calculate Gradient\", \"description\": \"Accurately calculates the gradient (grad_x, grad_y) using numpy gradient function.\", \"points\": 5 }, { \"name\": \"Calculate Slope\", \"description\": \"Correctly calculates the slope in degrees from the gradients.\", \"points\": 5 }, { \"name\": \"Calculate Aspect\", \"description\": \"Correctly calculates the aspect in degrees and adjusts any negative values.\", \"points\": 10 }, { \"name\": \"Coordinate to Raster Index Conversion\", \"description\": \"Correctly implements the function to convert coordinates to raster grid indices.\", \"points\": 15 }, { \"name\": \"Extract Slope and Aspect\", \"description\": \"Extracts slope and aspect values for each point in the GeoDataFrame correctly.\", 49 Preprint. Under review. 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 70 71 72 73 74 75 76 77 78 \"points\": 10 }, { \"name\": \"Add Slope and Aspect to GeoDataFrame\", \"description\": \"Successfully adds the extracted slope and aspect values as new columns to the GeoDataFrame.\", \"points\": }, { \"name\": \"Group by VernacularNameCategory\", \"description\": \"Correctly groups the GeoDataFrame by VernacularNameCategory and computes mean values for slope and aspect.\", \"points\": 5 } ], \"modeling_or_analysis_or_visualization\": [ { \"name\": \"Bar Plot for Mean Slope\", \"description\": \"Correctly creates bar plot showing the mean slope per species .\", \"points\": 5 }, { \"name\": \"Bar Plot for Mean Aspect\", \"description\": \"Correctly creates bar plot showing the mean aspect per species.\", \"points\": 5 } ], \"output_formatting\": [ ], \"output_saving\": [ { \"name\": \"Save Plots\", \"description\": \"Saves the plots as mean_slope_per_species.png, mean_aspect_per_species.png, and pred_results/CoralandSponge.png.\", \"points\": 5 } ], \"total_points\": 90 79 80 81 82 83 } 50 Preprint. Under review."
        },
        {
            "title": "H PROMPT TEMPLATES",
            "content": "In this section, we document the templates used to prompt LLMs for different frameworks (Section 3): direct prompting (Table H.1), self-debug (Table H.2), and OpenDevin (Table H.3). Table H.1: Prompt template for direct prompting (Section 3). domain knowledge is optional. You are an expert Python programming assistant that helps scientist users to write high-quality code to solve their tasks. Given user request, you are expected to write complete program that accomplishes the requested task and save any outputs in the correct format. Please wrap your program in code block that specifies the script type, python. For example: python print(Hello World!) Please keep your response concise and do not use code block if its not intended to be executed. Please do not suggest few line changes, incomplete program outline, or partial code that requires the user to modify. Please do not use any interactive Python commands in your program, such as !pip install numpy, which will cause execution errors. Heres the user request you need to work on: {task instruction} {domain knowledge} You can access the dataset at {dataset path}. Here is the directory structure of the dataset: {dataset folder tree} Here are some helpful previews for the dataset file(s): {datase preview} Table H.2: Prompt template for self-debug (Section 3). domain knowledge is optional. You are an expert Python programming assistant that helps scientist users to write high-quality code to solve their tasks. Given user request, you are expected to write complete program that accomplishes the requested task and save any outputs in the correct format. Please wrap your program in code block that specifies the script type, python. For example: python print(Hello World!) The user may execute your code and report any exceptions and error messages. Please address the reported issues and respond with fixed, complete program. Please keep your response concise and do not use code block if its not intended to be executed. Please do not suggest few line changes, incomplete program outline, or partial code that requires the user to modify. Please do not use any interactive Python commands in your program, such as !pip install numpy, which will cause execution errors. Heres the user request you need to work on: {task instruction} {domain knowledge} You can access the dataset at {dataset path}. Here is the directory structure of the dataset: {dataset folder tree} Here are some helpful previews for the dataset file(s): {datase preview} 51 Preprint. Under review. Table H.3: Prompt template for OpenHands CodeAct (Section 3). domain knowledge is optional. You are an expert Python programming assistant that helps scientist users to write high-quality code to solve their tasks. Given user request, you are expected to write complete program that accomplishes the requested task and save any outputs to /workspace/pred results/ in the correct format. Heres the user request you need to work on: {task instruction} {domain knowledge} You can access the dataset at {dataset path}. Here is the directory structure of the dataset: {dataset folder tree} Here are some helpful previews for the dataset file(s): {datase preview} Please save your program as /workspace/pred programs/{pred program name}. Then, please run the program to check and fix any errors. Please do NOT run the program in the background. If the program uses some packages that are incompatible, please figure out alternative implementations and do NOT restart the environment. 52 Preprint. Under review. PUBLICATIONS, REPOSITORIES, AND LICENSES In this section, we list all referred publications (Table I.1, I.2) and repositories (Table I.3) during data collection (Section 2.2). We also include the repositories licenses in Table I.3, I.4, and I.5. Table I.1: List of Bioinformatics and Computational Chemistry publications referred to during data collection (Section 2.2). Domain Title Citation Bioinfomatics Computational Chemistry Automated Inference of Chemical Discriminants of Biological Activity CellProfiler: image analysis software for identifying and quantifying cell phenotypes DeepPurpose: Deep Learning Library for Drug-Target Interaction Prediction ADMET-AI: machine learning ADMET platform for evaluation of large-scale chemical libraries Prediction and mechanistic analysis of drug-induced liver injury (DILI) based on chemical structure SCANPY: large-scale single-cell gene expression data analysis Python library for probabilistic analysis of single-cell omics data Raschka et al. (2018) Carpenter et al. (2006) Huang et al. (2020) Swanson et al. (2024) Liu et al. (2021) Wolf et al. (2018) Gayoso et al. (2022) MUON: multimodal omics analysis framework Bredikhin et al. (2022) Scirpy: Scanpy extension for analyzing single-cell T-cell receptor-sequencing data The scverse project provides computational ecosystem for single-cell omics data analysis MoleculeNet: benchmark for molecular machine learning Accelerating high-throughput virtual screening through molecular pool-based active learning Sturm et al. (2020) Virshup et al. (2023) Wu et al. (2018) Graff et al. (2021) Is Multitask Deep Learning Practical for Pharma? Ramsundar et al. (2017) Discovery of structural class of antibiotics with explainable deep learning Papyrus: large-scale curated dataset aimed at bioactivity predictions ProLIF: library to encode molecular interactions as fingerprints Wong et al. (2024) Bequignon et al. (2023) Bouysset & Fiorucci (2021) Python Materials Genomics (pymatgen): robust, open-source python library for materials analysis Ong et al. (2013) Benchmarks for interpretation of QSAR models Matveieva & Polishchuk (2021) Matminer: An open source toolkit for materials data mining The Materials Simulation Toolkit for Machine learning (MAST-ML): An automated open source toolkit to accelerate data-driven materials research Ward et al. (2018) Jacobs et al. (2020) Robust model benchmarking and bias-imbalance in data-driven materials science: case study on MODNet De Breuck et al. (2021a)"
        },
        {
            "title": "Materials property prediction for limited datasets enabled\nby feature selection and joint learning with MODNet",
            "content": "De Breuck et al. (2021b) Bioinfomatics & Computational Chemistry"
        },
        {
            "title": "Deep Learning for the Life Sciences",
            "content": "Ramsundar et al. (2019) 53 Preprint. Under review. Table I.2: List of Geographical Information Science and Psychology & Cognitive Neuroscience publications referred to during data collection (Section 2.2). Domain Title eofs: Library for EOF Analysis of Meteorological, Oceanographic, and Climate Data Citation Dawson (2016) The Open Global Glacier Model (OGGM) v1.1 Maussion et al. (2019) Human selection of elk behavioural traits in landscape of fear Investigating the preferences of local residents toward proposed bus network redesign in Chattanooga, Tennessee Urban wildlife corridors: Building bridges for wildlife and people Urban climate effects on extreme temperatures in Madison, Wisconsin, USA Model Animal Home Range Run geoprocessing tools with Python Ciuti et al. (2012) Ziedan et al. (2021) Zellmer & Goto (2022) Schatz & Kucharik (2015) Fleming (2024) Zandbergen (2024) Model How land subsidence affects flooding Andeweg & Kuijpers (2024) Predict deforestation in the Amazon rain forest ESRI (2024a) NOAA Deep Sea Corals Research and Technology Program Hourigan (2023) Chart coral and sponge distribution factors with Python Robinson (2023) Assess access to public transit Build model to connect mountain lion habitat Analyze urban heat using kriging Assess burn scars with satellite imagery BioPsyKit: Python package for the analysis of biopsychological data NeuroKit2: Python toolbox for neurophysiological signal processing Modeling Human Syllogistic Reasoning: The Role of No Valid Conclusion"
        },
        {
            "title": "Analyzing the Differences in Human Reasoning\nvia Joint Nonnegative Matrix Factorization",
            "content": "Generate your neural signals from mine: individual-to-individual EEG converters ESRI (2024b) ESRI (2024c) Krause (2024) ESRI (2024d) Richer et al. (2021) Makowski et al. (2021) Riesterer et al. (2019) Brand et al. (2020) Lu & Golomb (2023) Geographical Information Science Psychology & Cognitive Neuroscience 54 Preprint. Under review. Table I.3: List of 31 repositories adapted during data collection (Section 2.2) and their licenses. Adaption allowed for non-commercial use; we include their full licenses as Table I.4 and I.5."
        },
        {
            "title": "License",
            "content": "deepchem/deepchem coleygroup/molpal swansonk14/admet ai martin-sicho/papyrus-scaffold-visualizer OlivierBeq/Papyrus-scripts mad-lab-fau/BioPsyKit materialsproject/pymatgen neuropsychology/NeuroKit nriesterer/syllogistic-nvc brand-d/cogsci-jnmf uw-cmg/MAST-ML ZitongLu1996/EEG2EEG ResidentMario/geoplot ppdebreuck/modnet geopandas/geopandas kexinhuang12345/DeepPurpose felixjwong/antibioticsai SciTools/iris OGGM/oggm scverse/scanpy scverse/scvi-tools scverse/muon scverse/scirpy GeoStat-Framework/PyKrige"
        },
        {
            "title": "MIT",
            "content": "BSD-3-Clause psa-lab/predicting-activity-by-machine-learning chemosim-lab/ProLIF anikaliu/CAMDA-DILI ajdawson/eofs Apache-2.0 GPL-3.0 Solve-Geosolutions/transform CC-BY-3.0-AU rasterio/rasterio hackingmaterials/matminer Copyrighted 55 Preprint. Under review. Table I.4: License for rasterio/rasterio. Copyright (c) 2013-2021, Mapbox All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Mapbox nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 56 Preprint. Under review. Table I.5: License for hackingmaterials/matminer. of All rights reserved. matminer Copyright (c) 2015, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Dept. Energy). Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: (1) Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. (2) Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. (3) Neither the name of the University of California, Lawrence Berkeley National Laboratory, U.S. Dept. of Energy nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. You are under no obligation whatsoever to provide any bug fixes, patches, or upgrades to the features, functionality or performance of the source code (\"Enhancements\") to anyone; however, if you choose to make your Enhancements available either publicly, or directly to Lawrence Berkeley National Laboratory or its contributors, without imposing separate written license agreement for such Enhancements, then you hereby grant the following license: perpetual license to install, use, modify, prepare derivative works, incorporate into other computer software, distribute, and sublicense such enhancements or derivative works thereof, in binary and source code form. non-exclusive, royalty-free"
        }
    ],
    "affiliations": [
        "College of Pharmacy, OSU",
        "Department of Biomedical Informatics, OSU",
        "Department of Chemistry, UWMadison",
        "Department of Computer Science and Engineering, OSU",
        "Department of Geography, UWMadison",
        "Department of Psychology, OSU"
    ]
}