{
    "paper_title": "Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding",
    "authors": [
        "Yanzheng Xiang",
        "Lan Wei",
        "Yizhen Yao",
        "Qinglin Zhu",
        "Hanqi Yan",
        "Chen Jin",
        "Philip Alexander Teare",
        "Dandan Zhang",
        "Lin Gui",
        "Amrutha Saseendran",
        "Yulan He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality."
        },
        {
            "title": "Start",
            "content": "Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding Yanzheng Xiang * 1 Lan Wei * 2 Yizhen Yao 1 Qinglin Zhu 1 Hanqi Yan 1 Chen Jin 3 Philip Alexander Teare 3 Dandan Zhang 2 Lin Gui 1 Amrutha Saseendran 3 Yulan He 1 4 6 2 0 2 5 ] . [ 1 1 6 1 6 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality. 1. Introduction Autoregressive language models (Abhimanyu Dubey et al., 2024; Brown et al., 2020; Radford et al., 2019; Radford & *Equal contribution 1Kings College London, UK 2Imperial College London, UK 3Centre for AI, Data Science & Artificial Intelligence, BioPharmaceuticals R&D, AstraZeneca, UK 4The Alan Turing Institute, UK. Correspondence to: Yulan He <yulan.he@kcl.ac.uk>. Preprint. February 9, 2026. Figure 1. Flip-flop behaviour on HumanEval for Dream-Instruct7B and LLaDA-Instruct-8B under two revocable baselines (Saber, WINO) and ours (COVER). Unlike baselines that repeatedly ReMask, COVER uses context-preserving in-place verification to reduce oscillatory revisions while maintaining generation quality. Narasimhan, 2018) generate text token by token and remain the dominant paradigm for high quality generation. Yet this sequential decoding is persistent inference bottleneck, and early errors can propagate through the remainder of the output (Valmeekam et al., 2023; Stechly et al., 2023). Diffusion large language models (dLLMs) offer an appealing alternative: they denoise an initially masked sequence and can, in principle, update many positions in parallel (Li et al., 2022b). In practice, however, aggressive parallel unmasking often harms generation quality (Hong et al., 2025; Dong et al., 2025), so dLLMs frequently revert to conservative decoding that unmasks only one position per step, largely sacrificing the promised speed gains (Nie et al., 2025; Ye et al., 2025; Xie et al., 2025). Recent work attempts to bridge this gap with revocable parallel diffusion decoding. These methods draft multiple tokens in parallel and then revisit subset of previously unmasked positions using the newly available context, optionally revoking them by resetting to [MASK]. WINO (Hong et al., 2025) performs verification through an auxiliary shadow block, whereas Saber (Dong et al., 2025) triggers remasking based on confidence drops. Although revocation imStop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding proves robustness, existing verification mechanisms introduce substantial overhead. WINO increases effective sequence length and memory footprint, and both methods depend on explicit remasking, which replaces content tokens with [MASK] for all queries and can destabilise subsequent drafts, leading to slower net denoising progress. In this work, we highlight an inefficiency of revocable decoding that standard accuracy metrics do not capture. We observe flip-flop oscillations, where position is remasked and later re-unmasked to exactly the same token. Figure 1 shows that such oscillations occur frequently under existing revocable baselines across dLLMs, indicating that many verification actions consume iterations without producing correction. This creates two coupled inefficiencies. First, remasking replaces content bearing embedding with [MASK], weakening the conditioning context used by other positions during parallel drafting. Second, each ineffective remask spends future unmask budget merely to restore the same token, reducing net denoising progress under any fixed step or unmask budget. To solve this, we propose COVER (Cache Override Verification for Efficient Revision), context-preserving single-pass verification mechanism for revocable parallel diffusion decoding. At each step, COVER rechecks small seed set by masking these positions in the input while overriding their KV states with cached values from the previous step. This dual view computation keeps the drafting context for all non seed queries unchanged, yet enables faithful leave one out verification on the seeds via diagonal correction that removes self leakage. To make cache reuse reliable, COVER chooses seeds using stability aware score that trades off uncertainty against estimated influence on the remaining masked positions, adapts the verification token number per step, and updates verified positions by KEEP, REPLACE, or REMASK to avoid ineffective remasking cycles. Our contributions are as follows: We identify flip-flop oscillations as dominant inefficiency in revocable diffusion decoding and show how explicit remasking weakens drafting context and wastes the revision budget. We introduce an in place KV cache override verification mechanism with diagonal correction, enabling faithful leave-one-out checks and stable parallel drafting within single forward pass. We propose stability aware and adaptive seed selection that prioritises uncertain and influential positions while avoiding unstable cache reuse, enabling efficient multitoken verification. We show that COVER improves accuracy while substantially reducing decoding steps, yielding consis2 tent end-to-end speedups of up to 11.64 (Dream-Ins7B), which supports reliable multi-token drafting via context-preserving in-place verification. 2. Related Work Diffusion Large Language Models (dLLMs). Diffusion language models generate text by iteratively denoising partially masked sequence, enabling multi token generation in principle. Early work studied both continuous diffusion for text (Li et al., 2022a; Gong et al., 2022; Han et al., 2023) and discrete formulations (Ou et al., 2024; Lou et al., 2023; Austin et al., 2021a; Sahoo et al., 2024). Among these, masked discrete diffusion models have proven most amenable to large scale training and deployment (Sahoo et al., 2024). Recent releases include open models such as LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025), as well as commercial systems such as Mercury (Labs et al., 2025) and Gemini Diffusion (Deepmind, 2025). Despite their potential, practical inference remains challenging: aggressive parallel unmasking often degrades generation quality, while bidirectional attention and the lack of stable KV cache make each decoding step expensive. Closing this gap between multi token capacity and reliable fast inference is an active research direction. dLLMs Acceleration. Existing acceleration methods mainly follow two directions: reducing per step compute via KV reuse (Liu et al., 2025; Wu et al., 2025; Song et al., 2025) and reducing the number of steps via parallel decoding (Israel et al., 2025; Wang et al., 2025b). On the systems side, Fast dLLMs (Wu et al., 2025) observes that KV states change smoothly across diffusion steps under full attention and proposes to cache and update them blockwise, amortising recomputation. On the algorithmic side, parallel decoding unmasks multiple positions per step, typically guided by confidence criteria, and relies on verification with optional remasking to correct erroneous drafts (Wang et al., 2025a; Kong et al., 2025; Dong et al., 2025). WINO (Hong et al., 2025) performs verification using an auxiliary shadow block with stricter criterion than drafting, which improves selectivity but introduces additional computation. dParallel (Chen et al., 2025) combines self-distillation with entropy threshold-based remasking to reduce steps, but it requires retraining the diffusion model to obtain the high certainty drafts needed for aggressive parallel unmasking. Instead, COVER achieves faithful leave-one-out verification in place via KV cache override with diagonal correction, and selects verification seeds with stability aware rule, enabling fast parallel decoding without extra blocks or retraining. Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding 3. Revocable Parallel Diffusion Decoding Let be vocabulary and let [MASK] be special token. We consider conditional generation with prompt and response of fixed length L. At step {0, . . . , }, the 1 , . . . , y(t) partial state is (t) = (y(t) ) (V {[MASK]})L with (0) = [MASK]L. We denote masked and unmasked indices by Mt := {i [L] : y(t) = [MASK]} and Ut := [L] Mt. Given (X, (t1)), the diffusion model outputs per-position token distributions {p(i) θ ( X, (t1))}L i=1. Decoding protocol. Revocable parallel diffusion decoding iterates for = 1, . . . , . Step takes as input the current state (t1) and seed set St1 Ut1 (with S0 = ), where St1 contains previously unmasked positions scheduled to be rechecked at step t. The procedure is specified by three step-dependent rules: drafting rule (choose Dt), verification rule (produce updates and remask set), and seed selection rule (choose St). Drafting. drafting rule selects set Dt Mt1 of currently masked positions to unmask in parallel. For each Dt, it proposes token ˆy(t) θ (v X, (t1)). typical instantiation ranks masked positions by confidence c(t1) := maxvV p(i) θ (v X, (t1)) and selects the top ones, optionally subject to budget Dt B. := arg maxvV p(i) Verification with optional revocation. Given the newly drafted context, verification rule revisits each seed position St1. It either outputs an updated token y(t) V, or revokes the position by resetting it to [MASK]. The revoked indices form the remask set Rt St1. State update. Initialize (t) (t1), then apply the drafting and verification outcomes: y(t) = ˆy(t) , [MASK], y(t1) , Dt, Rt, otherwise. Equivalently, Ut = (Ut1 Dt) Rt and Mt = [L] Ut. Seed selection. seed selection algorithm chooses the next seed set St Ut, which will be verified at step + 1. Decoding terminates when Mt = or when step budget is reached. 4. Flip-Flop Oscillations Revocable decoding improves quality by allowing previously unmasked tokens to be remasked and refined under richer context. However, in practice we observe pathological behavior that we call flip-flop oscillations, which can substantially slow down inference without providing meaningful corrections. 3 Definition. During revocable diffusion decoding, position can be unmasked, later remasked to [MASK], and then unmasked again. Fix position and record the token predicted each time transitions from [MASK] to concrete token. We say flip-flop event occurs at position if two consecutive such unmaskings predict the same token, meaning that the intermediate remask does not change the models discrete decision. Equivalently, flip flop corresponds to an ineffective remask action that is later undone by restoring the same token. Let Fi denote the number of flip flop events at position i, and define the total flip flop count for the sequence as = (cid:80)L i=1 Fi. In our empirical study, flip flops dominate revocation in existing methods: over 99% of Sabers ReMask operations are ineffective, and for WINO the ineffective fraction remains close to 90% across datasets (Section 6.3). Flip-flop oscillations slow down decoding. Flip-flop oscillations reduce efficiency even when revocation rarely changes the final discrete prediction. We highlight two sources of overhead: 1. Remasking weakens the conditioning context for parallel drafting. When previously unmasked token is reset to [MASK], the input replaces content bearing embedding with an uninformative placeholder, so other positions that attend to it temporarily lose semantic signal. Empirically, many revoked positions are later repredicted as exactly the same token, which means this transient context deletion often provides no corrective benefit. Nevertheless, it still lowers confidence at remaining masked positions, typically shrinking the drafted set Dt and slowing the net rate at which new tokens can be committed. 2. Remasking consumes the decoding budget and reduces net progress. From the state update Ut = (Ut1 Dt) Rt, the net expansion per step is Ut Ut1 = Dt Rt. Each flip-flop event increases Rt without producing new assignment and forces later step to spend an unmask slot merely to restore the same token, thereby wasting iterations under any fixed step or unmask budget. We formalize this overhead in Appendix (Lemma A.1), proving that any additional remask or flip-flop event increases the required number of decoding steps under fixed per-step unmask budget. 5. Method We propose COVER, an in-place single-pass verification mechanism for revocable parallel diffusion decoding (Figure 2). At each step, COVER performs parallel drafting on masked positions while simultaneously verifying small seed set from the previous step. Verification is implemented Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding Figure 2. Overview of our single-pass revocable diffusion decoding. At step t, the model drafts multiple masked positions in parallel and verifies seed set selected from step t1. Verification masks the seeds in the input but injects their cached K, states so non-seed queries see an unchanged context. An attention diagonal correction is applied at the masked seed positions to prevent self-leakage and enable re-prediction from the surrounding context. Each seed is then updated by KEEP, REPLACE, or REMASK, and stability-aware score based on uncertainty and in/out influence selects the next seed set via top-k. by masking the selected positions in the input but overriding their key value states with cached activations, together with diagonal correction that prevents self-leakage. This dual view computation preserves stable conditioning context for drafting, and yields faithful leave-one-out checks for the verified positions. Each verified position is then assigned KEEP, REPLACE, or REMASK to avoid ineffective flip-flop cycles. Finally, stability aware seed selection rule prioritises high-risk positions, and an adaptive revision rate controls how many positions are verified per step. 5.1. Dual-View Feed Forward through KV Cache Override At denoising step t, COVER takes as input the current partial state (t1) and seed set St1 Ut1 selected at the end of step 1 (Sec. 5.3). Positions in St1 are previously unmasked tokens scheduled to be rechecked at step t, and the verification rule will optionally output remask set Rt St1. Our goal is to obtain two types of predictions within single forward pass: (i) faithful leave one out style verification distribution at each seed position St1, where the model re-predicts y(t1) from surrounding context with the input at set to [MASK]; and (ii) stable drafting distributions for all non seed positions, including masked positions to be drafted, whose queries should still condition on the same seed representations as in step 1. Masked seed input for verification. We first construct verification input by masking only the seed positions: y(t1) = (cid:40) [MASK], y(t1) , St1, otherwise. Let (Qℓ, Kℓ, Vℓ) denote the query, key, and value states computed from (t1) at transformer layer ℓ. KV cache override yields stable drafting view. Naively masking St1 would delete their information from the context of every other query, weakening parallel drafting. COVER preserves the seed context by overriding only the memory columns at the seed positions with their cached key value states from step 1. Concretely, when St1 is selected, we cache the per layer key and value states {( (t1) , (t1) )}jSt1 . At step t, we form an overridℓ,j ℓ,j ℓ, den memory (K ℓ ) by (cid:40) (K ℓ,j, ℓ,j) = ( (t1) , (t1) ℓ,j ℓ,j (Kℓ,j, Vℓ,j), ), St1, otherwise. We then run attention once for all positions using this overridden memory: Oovr ℓ = Attn(Qℓ, ℓ, ℓ ) = softmax (cid:18) QℓK ℓ (cid:19) ℓ , denote the resulting output vector at position by oovr ℓ,i . Here, is the key dimension per attention head. For any query position / St1, the seed columns in memory are exactly the cached representations from step 1, so non seed queries continue to condition on stable seed context even though the seed tokens are masked in the input. 4 Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding Diagonal correction for faithful verification. The KV override attention run above is sufficient for stable drafting, but it is not faithful for verifying seed position St1. Although y(t1) is masked in (t1), naively overriding the seed columns would still place the cached pair (k i) on the diagonal column = i, creating direct self conditioning path that can leak the token being verified. i, i To obtain leave one out view for each seed query i, we keep the overridden columns for all = but restore the diagonal column to the key and value computed from the masked input: (k(i) , v(i) ) = (cid:40) (ki, vi), j, (k j), = i, = i. This modification changes only the diagonal attention score in row i. However, since attention probabilities are normalized by softmax, changing the diagonal score also rescales the entire attention distribution in that row. We therefore apply post-hoc diagonal correction. Let αi be the diagonal attention weight under the overridden run and let δi be the diagonal score shift after restoring (ki, vi). Then the corrected attention weights are obtained by single row wise rescaling: wi,j = wovr i,j ri (j = i), wi,i = wovr i,i exp(δi) ri , ri = 1 + αi (cid:0) exp(δi) 1(cid:1). We then update the attention output at accordingly, while leaving all non seed queries unchanged. The full derivation and implementation details are provided in Appendix B. 5.2. Drafting: Multiple Token Unmasking We adopt the parallel drafting scheme described in Sec. 3. At decoding step t, let M(t) be the set of masked positions, and let c(t) denote the confidence score (as defined in Sec. 3) for each M(t). We draft new tokens by selecting all masked positions whose confidence exceeds threshold: D(t) = (cid:8) M(t) : c(t) > τdraft (cid:9). To avoid overly aggressive updates within single step, which can introduce many errors, we additionally cap the number of drafted positions by maximum budget B. When D(t) > B, we keep only the positions with the largest confidence values. Revision outcomes for previously verified positions. In the same forward pass, the model re-predicts the seed positions St1 from the previous step under the verification view. For each St1, let y(t) (v) and c(t) ). We apply three-way revision rule = arg maxv p(t) = p(t) (y(t) (KEEP/REPLACE/REMASK): y(t) = , y(t1) y(t) [MASK], , = y(t1) y(t) = y(t1) y(t) otherwise. , , c(t) τdraft, c(t) τdraft, The revision rule is designed to avoid unnecessary revocations. KEEP skips ReMask when the verified token matches the current assignment, and REPLACE commits confident correction in place. Both actions reduce the remask set Rt := {i St1 : y(t) = [MASK]}, thereby suppressing flip-flop revisions and preserving net progress under fixed unmasking budget. Decoding terminates once (t) contains no [MASK] tokens; for Instruct models, we stop early when the end-of-sequence token is generated. 5.3. Stability Aware Seed Selection and Adaptive Revision Rate Verifying too many positions in parallel can be harmful when their cached representations are unstable, as overriding such KV states may perturb the predictions of other tokens. We therefore restrict verification to small, adaptively chosen seed set St Ut. Stability aware seed scoring. For each Ut, we score its verification priority by combining (i) risk of being incorrect, (ii) how much the remaining masked positions rely on it as context, and (iii) how likely its cached KV state will drift after the current draft. Let A(t) [0, 1]LL denotes the last layer attention matrix (averaged over heads) from the current forward pass. We define three signals: u(t)(j) = log p(t) (cid:16) (cid:17) , y(t) d(t) in (j) = (cid:88) A(t) qj, qMt d(t) out(j) = A(t) ji. (cid:88) iDt Here, u(t)(j) is the uncertainty of the currently assigned token at position j, so larger values indicate higher verification risk. The term d(t) in (j) measures the downstream influence of on the not yet generated tokens, namely the total attention mass from still masked queries to j. The term d(t) out(j) measures the draft sensitivity of j, namely how strongly attends to newly drafted positions; large value suggests that the representation at is likely to change after drafting, making KV reuse less stable. 5 Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding"
        },
        {
            "title": "We combine them as",
            "content": "Score(t)(j) = u(t)(j) 1 + d(t) 1 + d(t) in (j) out(j) . Thus, we prioritise seeds that are uncertain and influential, while penalising those whose cached states are likely to drift under the newly introduced context. Adaptive revision rate. Rather than fixing the seed number for revision per step, we adapt it to the empirical score distribution. Let nt = Ut and {sj}jUt be the scores. We define the empirical cumulative distribution function (CDF) Ft(s) = 1 nt (cid:88) jUt I{sj s}. Define the empirical mean st = 1 nt (cid:80) sj and tail mass πt = 1 Ft(st). We then set the verification number as St = nt πt , and select the top-scoring positions accordingly. position cannot be selected as seed in two consecutive steps, since its cached KV is outdated. 6. Experiment 6.1. Experimental Settings Implementation Details. We conduct experiments on four different dLLMs, namely LLaDA-8B-Base, LLaDA-8BInstruct(Nie et al., 2025), LLaDA-1.5-8B (Zhu et al., 2025a), and Dream-7B-Instruct (Ye et al., 2025). For consistency and robustness, we set the decoding temperature to zero and greedily unmask the token with the lowest entropy at each step. We adopt the semi-autoregressive sampling strategy (Nie et al., 2025), which segments the output sequence into set of blocks that are generated sequentially in left-to-right order. In our evaluation, we set the generation length to 256 and 512 and the block length to 64. We set the per-step drafting budget to = 15 and tune the drafting threshold τdraft {0.7, 0.8, 0.9}. All experiments are conducted on four NVIDIA H200 GPUs. Datasets. We evaluate our approach on four benchmarks covering mathematical reasoning and code generation. For mathematical reasoning, we consider GSM8K (Cobbe et al., 2021), which contains grade-schoollevel word problems, and the more demanding MATH500 (Lightman et al., 2023), composed of competition-style mathematics questions. For code generation, we benchmark on MBPP (Austin et al., 2021b), which focuses on introductory Python programming tasks, and HumanEval (Mark Chen et al., 2021), collection of hand-written problems designed to assess program synthesis ability. All Instruct variant models are evaluated in the zero-shot setting, while standard few-shot protocols are adopted on the LLaDA-Base model specific to each benchmark: zero-shot for HumanEval, three-shot for MBPP, four-shot for MATH500, and eight-shot for GSM8K (Zhu et al., 2025b). Metrics. To evaluate the effectiveness and efficiency of our approach, we utilise three primary metrics: Acc., Steps, and Speed. For performance assessment, we report standard accuracy on mathematical reasoning benchmarks and the pass@1 rate for code generation tasks. Efficiency is quantified by tracking the average number of decoding steps required per sample across the entire dataset. Finally, we measure relative speedup by calculating the ratio of the total inference time: specifically, the total runtime of standard greedy decoding divided by the total runtime of the parallel diffusion decoding methods. To characterise flipflop behaviour, we additionally report: (1) No. Total ReMask, the total number of ReMask operations; (2) No. Eff. ReMask, the number of effective ReMask operations where the token after re-unmasking differs from that before re-masking; and (3) Ratio := No. Eff. ReMask/No. Total ReMask.1 The remaining remasking operations are ineffective and correspond to flip flop events (Section 4). Baselines. We evaluate our approach relative to standard greedy diffusion decoding and two training free revocable parallel diffusion decoding baselines, WINO 2 (Hong et al., 2025) and Saber 3 (Dong et al., 2025). For both baselines, we use the authors original implementations and evaluate them under the same experimental settings as ours for fair comparison. 6.2. Main Results Performance on Benchmarks. Table 1 shows that COVER consistently improves task performance across four benchmarks and multiple diffusion models. Across both code generation (HumanEval, MBPP) and math reasoning (GSM8K, MATH500), COVER achieves the strongest or near-strongest accuracy in each model and length setting, thereby avoiding the noticeable quality degradation of naive multi-token unmasking. The gains are particularly clear on code benchmarks: for LLaDA-Base-8B, COVER improves HumanEval from 32.93% to 35.37% at length 512 and MBPP from 40.80% to 42.40% at length 256; for LLaDA-Ins-8B at length 256, it improves HumanEval from 37.20% to 41.46% and MBPP from 37.20% to 39.00%. We observe similar improvements on reasoning tasks. For ex1For COVER, we treat REPLACE operations as effective since they modify the previously assigned token. Accordingly, for COVER the denominator of Ratio is #ReMask + #Replace. 2WINO:https://github.com/Feng-Hong/ WINO-DLLM 3Saber: https://github.com/zhaoyMa/Saber 6 Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding Table 1. Main results across four benchmarks and multiple diffusion models. We report accuracy (%) and the average number of decoding steps (lower is better). Speed denotes relative runtime (baseline = 1.00), where larger values are faster. Rows with pink background indicate ours, and the best result within each block is bolded. Model Len Method HumanEval MBPP GSM8K MATH500 Acc.(%) Steps Speed Acc.(%) Steps Speed Acc.(%) Steps Speed Acc.(%) Steps Speed LLaDA-Base-8B LLaDA-Ins-8B LLaDA-1.5-8B Dream-Ins-7B 256 256 512 256 512 256 33.54 baseline 32.930.61 Saber WINO 33.54+0.00 COVER 34.76+1.22 32.93 baseline 33.54+0.61 Saber 34.76+1.83 WINO COVER 35.37+2.44 37.20 baseline 37.20+0.00 Saber WINO 39.63+2.44 COVER 41.46+4.27 46.95 baseline 47.56+0.61 Saber WINO 47.56+0.61 COVER 48.17+1.22 43.29 baseline 40.852.44 Saber 42.071.22 WINO COVER 43.29+0.00 48.78 baseline 48.170.61 Saber 48.78+0.00 WINO COVER 48.78+0.00 54.88 baseline 54.88+0.00 Saber WINO 54.88+0.00 COVER 55.49+0.61 54.27 baseline 52.441.83 Saber WINO 53.051.22 COVER 57.93+3.66 256 93.75162.25 71.43184.57 46.40209.60 512 114.81397.19 103.18408.82 70.90441.10 256 93.44162.56 62.44193.56 53.68202.32 512 191.29320.71 178.44333.56 132.63379. 256 100.93155.07 83.93172.07 75.09180.91 512 225.11286.89 205.16306.84 140.44371.56 256 186.1669.84 83.76172.24 71.56184.44 512 448.8563.15 99.37412.63 71.27440.73 40.80 1.00 2.67 41.20+0.40 2.19 41.80+1.00 2.98 42.40+1.60 1.00 4.45 40.20+0.40 2.75 40.40+0.60 4.64 41.00+1.20 39.80 37. 1.00 2.61 37.60+0.40 3.28 36.201.00 3.51 39.00+1.80 1.00 2.58 37.400.20 1.59 38.40+0.80 2.72 38.80+1.20 37.60 38.60 1.00 2.38 37.601.00 2.41 37.800.80 2.44 37.800.80 1.00 2.22 38.40+0.20 1.39 38.000.20 2.66 39.40+1.20 38.20 56. 1.00 1.45 57.00+0.20 2.70 57.00+0.20 2.77 58.00+1.20 1.00 1.24 55.401.80 4.73 56.201.00 5.78 57.60+0.40 57.20 256 107.11148.89 82.08173.92 75.37180.63 512 167.99344.01 125.35386.65 94.03417.97 256 114.86141.14 87.00169.00 69.75186.25 512 168.44343.56 131.29380.71 115.24396.76 256 124.42131.58 89.99166.01 70.52185.48 512 185.50326.50 139.51372.49 123.48388.52 256 207.1748.83 57.08198.92 42.35213.65 512 461.5750.43 62.11449.89 35.84476. 70.58 1.00 2.34 69.620.96 1.55 70.430.15 2.42 70.96+0.38 1.00 3.05 70.130.61 1.92 70.050.69 3.82 71.42+0.68 70.74 74.91 1.00 2.25 75.66+0.75 1.64 77.33+2.42 2.35 77.26+2.35 1.00 2.96 79.61+0.53 1.94 79.45+0.37 3.08 79.98+0.90 79. 82.11 1.00 2.38 81.430.68 1.65 81.120.99 2.45 81.730.38 1.00 2.70 81.350.99 1.84 82.030.31 2.83 82.56+0.22 82.34 75.82 1.00 1.31 71.044.78 4.23 74.001.82 4.36 78.92+3.11 1.00 1.22 72.104.78 7.82 74.072.81 11.64 79.30+2.43 76. 256 135.36120.64 106.76149.24 97.21158.79 512 166.08345.92 128.30383.70 105.15406.85 256 120.48135.52 49.47206.53 51.65204.35 512 128.75383.25 74.96437.04 65.47446.53 256 107.32148.68 85.03170.97 63.24192.76 512 117.53394.47 74.73437.27 63.84448.16 256 203.5552.45 83.00173.00 52.11203.89 512 450.3061.70 96.98415.02 51.48460.52 1.00 1.72 1.09 1.78 1.00 3.07 1.49 3.17 1.00 2.47 3.01 3.25 1.00 3.87 3.61 5.52 1.00 2.33 2.44 2.65 1.00 4.29 3.72 5.41 30.60 31.80+1.20 30.80+0.20 32.00+1.40 31.80 30.201.60 31.600.20 32.40+0.60 30.50 31.80+1.30 32.40+1.90 32.80+2.30 36.90 38.20+1.30 39.20+2.30 40.80+3.90 35.20 35.80+0.60 34.600.60 36.00+0.80 40.00 41.00+1.00 41.40+1.40 42.60+2.60 1.00 40.00 1.34 42.00+2.00 3.01 42.40+2.40 3.51 43.60+3.60 1.00 42.40 1.26 45.00+2.60 5.05 44.40 +2.00 8.48 45.40+3.00 256 123.91132.09 102.30153.70 87.66168.34 512 181.98330.02 139.45372.55 124.29387. 256 130.66125.34 75.49180.51 68.05187.95 512 170.12341.88 115.69396.31 99.97412.03 256 132.51123.49 96.73159.27 66.17189.83 512 213.55298.45 122.32389.68 111.05400.95 256 172.7683.24 139.64116.36 105.30150.70 512 352.11159.89 204.58 307.42 143.91368.09 1.00 2.02 1.16 2.08 1.00 2.80 1.76 2.91 1.00 2.25 1.96 2.52 1.00 2.92 2.33 3.62 1.00 2.34 2.05 2.66 1.00 2.37 2.59 2.82 1.00 1.56 1.77 1.84 1.00 1.59 2.38 2.95 ample, on Dream-Ins-8B at length 512, COVER reaches 79.30% on GSM8K and 45.40% on MATH500, exceeding the baseline and surpassing the other two revocable methods. Overall, these results indicate that COVER enables aggressive parallel drafting with faithful in place verification, yielding consistent quality gains rather than trading accuracy for speed. Efficiency and Decoding Speed. Beyond accuracy, COVER substantially reduces the number of diffusion steps and delivers faster end-to-end decoding. Within each model block, the speedups are measured relative to the standard one token per step baseline (Speed = 1.00), and COVER is consistently among the fastest methods while maintaining the best accuracy. For LLaDA-Base-8B at length 256, COVER cuts HumanEval steps from 256 to 46.40 (2.98 speedup) and reduces MATH500 steps to 87.66 (2.08). For LLaDA-Ins-8B at length 512, COVER achieves 5.52 speedup on GSM8K with only 65.47 steps, while also improving accuracy. On Dream-Ins-7B, the acceleration is even more pronounced, reaching up to 11.64 speedup on MBPP at length 512. These efficiency gains support the central claim of COVER: by reusing cached representations to stabilise parallel drafting and verifying only small set of high-risk positions, we improve net denoising progress per step and avoid spending steps on ineffective oscillations. 6.3. Flip-Flop Oscillations: Empirical Analysis Table 2 shows that existing revocable decoders spend most revision operations on ineffective ReMask events. Saber exhibits extremely low efficiency, with the ratio below 1% on all datasets, meaning almost every ReMask is wasted and corresponds to flip-flop event. WINO improves the situation via leave-one-out style verification, but still wastes large fraction of revisions, with Ratio only around 8% to 13%. In contrast, COVER makes revision substantially more selective and effective. Across all four datasets, COVER reduces the total number of ReMask operations by one to two orders of magnitude, for example, from 173030 to 1436 on GSM8K, while maintaining comparable number of effective revisions. As result, COVER achieves consistently high Ratio of roughly 58% to 65%, indicating that most revision actions lead to actual token changes rather than oscillatory remasking. These results support our claim that stabilised in-place verification avoids spending the unmask budget on repeated flip-flop cycles, which directly translates into fewer decoding steps and faster inference. 7 Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding Table 2. Flipflop statistics of revocable decoding on four datasets with LLaDA-1.5-8B (generation length 256). Table 3. Ablation on LLaDA-Instruct-8B with generation length 256. Speed is relative runtime (COVER = 1.00). Dataset HumanEval MBPP GSM8K MATH500 Method No. Eff. Remask No. Total Remask Ratio 0.76% 7.74% 62.79% 0.66% 9.53% 57.68% 0.34% 13.11% 65.46% 0.34% 11.96% 63.23% Saber WINO COVER Saber WINO COVER Saber WINO COVER Saber WINO COVER 17367 2107 258 35785 6529 1061 173030 38432 1436 35570 9241 133 163 162 237 622 612 590 5039 940 122 1105 650 Dataset HumanEval MBPP GSM8K MATH Method Acc.(%) COVER w/o kv w/o seed COVER w/o kv w/o seed COVER w/o kv w/o seed COVER w/o kv w/o seed 41.46 38.413.05 39.022.44 39.00 38.600.40 38.001.00 77.26 76.500.76 76.800.46 32.80 30.602.20 32.000.80 Steps 53.68 105.73+52.05 70.17+16.49 69.75 147.85+78.10 82.78+13.02 51.65 123.28+71.63 65.10+13.45 68.05 96.37+28.32 83.63+15.58 Speed 1.00 0.73 0.79 1.00 0.68 0.92 1.00 0.63 0.83 1.00 0.80 0.89 6.4. Ablation Study Table 3 ablates two core components of COVER on LLaDAInstruct-8B with generation length 256. Effect of KV cache override. The variant w/o kv removes KV cache override and diagonal correction, and instead verifies by masking the seed positions in the input directly. This forces all queries to attend to degraded context in which the recently drafted tokens are absent, so parallel drafting loses the conditioning signals it needs to remain stable. The consequence is immediate in both progress and runtime: the decoder revisits the same positions more often, spending iterations on low-value revoke and re-unmask cycles. Across all datasets, w/o kv nearly doubles or more the step count and consistently slows inference. For example, on GSM8K, steps increase from 51.65 to 123.28, and speed drops to 0.63; on HumanEval steps rise from 53.68 to 105.73 with 3.05% accuracy drop. These results isolate KV cache override as the main mechanism that preserves stable drafting context while still enabling faithful verification. Effect of stability aware seed selection. The variant w/o seed keeps KV override verification but replaces stability aware seed selection with naive confidence drop heuristic. While verification remains in place, the selected seeds are less compatible with cache reuse: the method more often verifies positions whose cached representations are likely to drift after the current draft, making the overridden memory less reliable as conditioning context for other positions. Empirically, this primarily hurts efficiency rather than causing catastrophic failures. Across datasets, w/o seed increases the step count and reduces speed, for instance from 51.65 to 65.10 on GSM8K and from 53.68 to 70.17 on HumanEval, with smaller but consistent accuracy drops. This shows that seed selection is not merely verification policy, but prerequisite for making cache reuse robust under multi-token drafting. Figure 3. Spearman rank correlation between the proposed stability proxy dout and measured KV drift across diffusion models and tasks. Cell colour and the value indicate the correlation coefficient; values above 0.5 suggest strong monotonic relationship, supporting dout as stability proxy. 6.5. Empirical validation of the drift proxy dout Our stability aware seed selection penalises candidates with large dout, which serves as proxy for how much their cached KV states may change after the current step update. To validate this proxy, we measure the true KV drift of each position as the average change in its key and value states before versus after the update, averaged across layers and heads, and compute the Spearman rank correlation between dout and the measured drift (averaged over steps and examples). Figure 3 shows consistently positive correlations across all models and tasks (from 0.540 to 0.716, mean = 0.637); correlations above 0.5 indicate strong monotonic relationship, confirming that larger dout reliably corresponds to larger KV drift and supporting its use for avoiding unstable cache reuse. 7. Conclusion DLLMs support parallel unmasking, but revocable decoding can waste computation through flip-flop oscillations that 8 Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding repeatedly remask tokens that would be restored unchanged. We introduce an in-place KV cache override verification mechanism with diagonal correction, enabling leave-oneout style checks while preserving stable context for parallel drafting within single forward pass. We also propose stability aware and adaptive seed selection that targets uncertain positions while avoiding unstable cache reuse, enabling efficient multi-token verification. Across benchmarks on different dLLMs, COVER improves accuracy while reducing decoding steps and inference time, delivering better speed quality tradeoff than prior revocable methods."
        },
        {
            "title": "Impact Statement",
            "content": "Our work highlights flip-flop oscillations as common source of wasted computation in revocable diffusion decoding, where many remasking actions do not change the final token but still remove useful context and consume decoding steps. We propose training-free inference procedure that performs in-place verification using KV cache override with diagonal correction and stability aware seed selection rule. Because COVER does not modify model weights, it does not change the underlying models capabilities or introduce new content risks beyond those already present in the base dLLM. We hope this analysis and method provide clearer lens for evaluating revocable diffusion decoders beyond accuracy, and offer practical building block for more efficient multi token decoding in future dLLM algorithms."
        },
        {
            "title": "Acknowledgments",
            "content": "In Advances in neural information processing systems, volume 33, pp. 18771901, 2020. Chen, Z., Fang, G., Ma, X., Yu, R., and Wang, X. dparallel: Learnable parallel decoding for dllms. arXiv preprint arXiv:2509.26488, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. Deepmind. Gemini diffusion, 2025."
        },
        {
            "title": "URL",
            "content": "https://deepmind.google/models/ gemini-diffusion/. Dong, Y., Ma, Z., Jiang, X., Fan, Z., Qian, J., Li, Y., Xiao, J., Jin, Z., Cao, R., Li, B., et al. Saber: An efficient sampling with adaptive acceleration and backtracking enhanced remasking for diffusion language model. arXiv preprint arXiv:2510.18165, 2025. Gong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. Han, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semiautoregressive simplex-based diffusion language model In Proceedfor text generation and modular control. ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1157511596, 2023. This work was supported in part by the UK Engineering and Physical Sciences Research Council through Turing AI Fellowship (grant no. EP/V020579/1, EP/V020579/2) and the Prosperity Partnership scheme (grant no. UKRI566). Hong, F., Yu, G., Ye, Y., Huang, H., Zheng, H., Zhang, Y., Wang, Y., and Yao, J. Wide-in, narrow-out: Revokable decoding for efficient and effective dllms. arXiv preprint arXiv:2507.18578, 2025."
        },
        {
            "title": "References",
            "content": "Abhimanyu Dubey, Abhinav Jauhri, A. P. et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021a. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V., and Sutton, C. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021b. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Israel, D., Broeck, G. V. d., and Grover, A. Accelerating diffusion llms via adaptive parallel decoding. arXiv preprint arXiv:2506.00413, 2025. Kong, F., Zhang, J., Liu, Y., Wu, Z., Tian, Y., Zhou, G., et al. Accelerating diffusion llm inference via local determinism propagation. arXiv preprint arXiv:2510.07081, 2025. Labs, I., Khanna, S., Kharbanda, S., Li, S., Varma, H., Wang, E., Birnbaum, S., Luo, Z., Miraoui, Y., Palrecha, A., Ermon, S., Grover, A., and Kuleshov, V. Mercury: Ultrafast language models based on diffusion, 2025. URL https://arxiv.org/abs/2506.17298. Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022a. 9 Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding J., Gulrajani, I., Liang, P., Li, X. L., Thickstun, Diffusion-lm improves conand Hashimoto, T. ArXiv, abs/2205.14217, trollable text generation. 2022b. URL https://api.semanticscholar. org/CorpusID:249192356. Valmeekam, K., Marquez, M., and Kambhampati, S. Can large language models really improve by selfArXiv, abs/2310.08118, critiquing their own plans? 2023. URL https://api.semanticscholar. org/CorpusID:263909251. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. ArXiv, abs/2305.20050, 2023. Wang, G., Schiff, Y., Sahoo, S. S., and Kuleshov, V. Remasking discrete diffusion models with inference-time scaling, 2025a. URL https://arxiv.org/abs/ 2503.00307. Liu, Z., Yang, Y., Zhang, Y., Chen, J., Zou, C., Wei, Q., Wang, S., and Zhang, L. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. Wang, X., Xu, C., Jin, Y., Jin, J., Zhang, H., and Deng, Z. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192, 2025b. Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., Luo, P., Han, S., and Xie, E. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Xie, Z., Ye, J., Zheng, L., Gao, J., Dong, J., Wu, Z., Zhao, X., Gong, S., Jiang, X., Li, Z., and Kong, L. Dream-coder 7b: An open diffusion language model for code. ArXiv, abs/2509.01142, 2025. Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., and Kong, L. Dream 7b: Diffusion large language models. ArXiv, abs/2508.15487, 2025. Zhu, F., Wang, R., Nie, S., Zhang, X., Wu, C., Hu, J., Zhou, J., Chen, J., Lin, Y., Wen, J., and Li, C. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. ArXiv, abs/2505.19223, 2025a. Zhu, Q., Yao, Y., Zhao, R., Xiang, Y., Saseendran, A., Jin, C., Teare, P. A., Liang, B., He, Y., and Gui, L. Latent refinement decoding: Enhancing diffusion-based language models by refining belief states. ArXiv, abs/2510.11052, 2025b. Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Mark Chen, Jerry Tworek, H. J. et al. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021. Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., ZHOU, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Ou, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li, C. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Radford, A. and Narasimhan, K. Improving language understanding by generative pre-training. 2018. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136 130184, 2024. Song, Y., Liu, X., Li, R., Liu, Z., Huang, Z., Guo, Q., He, Z., and Qiu, X. Sparse-dllm: Accelerating diffusion llms with dynamic cache eviction. arXiv preprint arXiv:2508.02558, 2025. Stechly, K., Marquez, M., and Kambhampati, S. Gpt-4 doesnt know its wrong: An analysis of iterative prompting for reasoning problems. ArXiv, abs/2310.12397, 2023. URL https://api.semanticscholar. org/CorpusID:264305982. 10 Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding A. Flip Flop Overhead and Step Lower Bound Each flip flop at position forces at least one additional unmask event beyond the first unmask of that position. Therefore, if is the total flip flop count, then the total number of unmask events is at least + . Since drafting selects at most positions per step, the total number of unmask events is at most BT , which yields Lemma A.1. Lemma A.1 (Unmask budget overhead from flip-flop). Assume decoding terminates with no [MASK] tokens, and drafting unmasks at most positions per step, namely Dt for all t. Let be the total flip flop count defined above. Then the number of decoding steps satisfies (cid:24) + (cid:25) . Proof. Let ni := Ti be the number of times position is unmasked. Completion implies ni 1 for all i, hence (cid:80) ni L. By construction, Fi ni 1, so ni 1 + Fi and thus (cid:80) Fi = + . Moreover, each unmask event corresponds to selecting one position into some Dt, so (cid:80) t=1 Dt BT . Combining yields BT + , giving the claim. ni + (cid:80) ni = (cid:80)T B. Post-hoc diagonal correction for faithful verification This appendix derives the closed form diagonal correction used in Sec. 5.1 to obtain faithful leave-one-out verification at seed queries without additional attention passes. where (ki, vi) are the key and value from (Kℓ, Vℓ) computed on the masked input. All off diagonal columns remain unchanged. B.2. Single score update lemma Lemma B.1 (Softmax under single score change). Let = softmax(s) RL. If we change only the ith score by si si + δ, then the updated distribution satisfies = wj 1 + wi(exp(δ) 1) = i, i = wi exp(δ) 1 + wi(exp(δ) 1) . Proof. Write wj = exp(sj)/Z where = (cid:80) exp(sk). After the change, = exp(si) + exp(si + δ) = Z(cid:0)1+wi(exp(δ)1)(cid:1). Substituting into j)/Z yields the claimed formulas. = exp(s B.3. Closed form correction of attention weights In our setting, restoring the diagonal key replaces only the diagonal score in row i: δi = qik qik . Let αi = wovr the scalar rescaling factor i,i be the overridden diagonal weight and define ri = 1 + αi (cid:0)exp(δi) 1(cid:1). B.1. Notation and objective Consider specific transformer layer ℓ and one attention head. Let (Qℓ, Kℓ, Vℓ) be computed from the verification input (t1) where seed positions are masked. Let (K ℓ, ℓ ) be the overridden memory formed by replacing the seed columns with their cached states from step 1. Define the overridden attention scores, weights, and outputs as Applying Lemma B.1 gives the corrected attention distribution for row i: wi,j = wovr i,j ri = i, wi,i = αi exp(δi) ri . This shows explicitly that correcting the diagonal score changes the entire row through the shared normalizer. sovr i,j = qik , i,: = softmax(cid:0)sovr wovr i,: (cid:88) i,j oovr j, = wovr (cid:1) , B.4. Closed form correction of attention outputs The corrected output is oi = (cid:88) j=i wi,jv + wi,ivi. where qi is the query at position i, and (k ridden key and value at position j. j, j) is the overUsing the identities above and oovr we obtain the single expression = (cid:80) j=i wovr i,j + αiv i, For seed query St1, faithful verification requires restoring only the diagonal entry: oi = αiv oovr + αi exp(δi) vi ri . (k(i) , v(i) ) = (cid:40) (ki, vi), j, (k j), = i, = i, 11 For non seed queries / St1, no correction is applied and we keep oi = oovr . i"
        }
    ],
    "affiliations": [
        "Centre for AI, Data Science & Artificial Intelligence, BioPharmaceuticals R&D, AstraZeneca, UK",
        "Imperial College London, UK",
        "Kings College London, UK",
        "The Alan Turing Institute, UK"
    ]
}