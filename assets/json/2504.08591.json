{
    "paper_title": "ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration",
    "authors": [
        "Yongsheng Yu",
        "Haitian Zheng",
        "Zhifei Zhang",
        "Jianming Zhang",
        "Yuqian Zhou",
        "Connelly Barnes",
        "Yuchen Liu",
        "Wei Xiong",
        "Zhe Lin",
        "Jiebo Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces a critical trade-off between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, a novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs a highly compressed latent representation that compresses image 32x, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that structures the latent space into sub-bands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 1 9 5 8 0 . 4 0 5 2 : r ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration Yongsheng Yu1,2*, Haitian Zheng2, Zhifei Zhang2, Jianming Zhang2, Yuqian Zhou2, Connelly Barnes2, Yuchen Liu2, Wei Xiong2, Zhe Lin2, Jiebo Luo1 1University of Rochester, 2Adobe Research"
        },
        {
            "title": "Abstract",
            "content": "Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces critical tradeoff between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs highly compressed latent representation that compresses image 32, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose Latent Pyramid VAE (LP-VAE) design that structures the latent space into subbands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs. 1. Introduction Recent advanced generative models, such as GANs [16] and diffusion models [19, 38], have dramatically improved image restoration (IR). These models leverage long-range context modeling [1, 44, 47], enhanced architectural designs [19, 30], and greater model capacity to effectively restore complex image structures from severely degraded or downsampled inputs. However, existing IR methods, often relying on UNet-based diffusion models [34, 38], are pretrained on an 8 compressed latent space. While being effective, they face efficiency challenges when restoring ultra high-resolution outputs, due to the quadratic computational demands associated with the number of spatial tokens. Not only scaling up the resolution of models is challenging, but deploying such models for ultra high-resolution IR also presents significant challenges. At high level, *Work done during an internship at Adobe. there seems to be fundamental dilemma between quality and efficiency. On one hand, long-range attention modeling is crucial for both visual understanding [1, 21, 22, 35] and synthesis [5, 6, 9, 19], facilitating the recent success of the Diffusion Transformer (DiT)[31] in both image and video generation[17, 28]. On the other hand, such capacity comes with an extensive computational overhead with the number of spatial tokens, dramatically limiting the scalability of these methods for ultra high-resolution IR. As shown in Fig. 1, existing diffusion-based IR methods [50, 56] take approximately one minute to process 2K image, and if tiled-based inference is employed, the runtime increases further. This limitation also impedes the exploration of more scalable models like DiT. In this work, we introduce ZipIR, novel framework designed to enhance model capacity, efficiency, and longrange modeling for high-quality, high-resolution diffusionbased image restoration. We start with building highly compressed latent representation [13, 38] with spatial downsampling factor of = 32. Differing from existing methods [25, 46, 50, 56], our design effectively reduces the number of latent tokens and offers benefits: it enables the use of advanced models like DiT, facilitates training on the entire images rather than local patches for improved holistic modeling, and increases efficiency during both training and inference phases. As result, ZipIR achieves up to 10 times faster inference than SeeSR [50] at 2K resolution and provides enhanced restoration for severely degraded inputs (downsampled by 20 or 16). However, designing the 32 latent space for image restoration introduces several challenges. First, naively trained latent space is susceptible to minor perturbation and low-level degradation [29, 42], complicating the restoration process on latent space and leading to instability. Second, decoding from such compressed latent code often distorts essential low-level details. To address these issues, we develop novel Latent Pyramid VAE (LP-VAE) inspired by the Laplacian pyramid representation from image processing literature [3]. We train the latent space sequentially from lower to higher resolutions: early channels encode lower- (a) 20 super-resolution at 20482 px resolution. (b) 16 super-resolution at 20482 px resolution. (c) 8 restoration at 20482 px resolution. (d) Inference time at 20482 px resolution (in seconds). (e) Model scalability measured by the diffusion model parameters (in Millions). Figure 1. Our ZipIR demonstrates strong capacity in restoring severely degraded images, such as 20, 16 downsampled or 8 degraded inputs to restore 20482 resolution output. Compared to different diffusion-based methods, ZipIR enjoys (d) an up to 10x running time advantage over SeeSR [50] while (e) maintaining higher learning capacity for producing high-quality and ultra high-resolution images from severely degraded inputs. resolution information, and subsequent channels capture residual details necessary for reconstructing high-resolution images. This sub-band decomposition effectively separates high-level image structures from low-level details. It ensures that the low-level degradation primarily affects the finer-level latent features, while the coarser-level codes remain consistent, thereby simplifying the learning process for the diffusion model. Building upon LP-VAE, we finally designed novel architecture based on DiT [31] to scale the capacity of the diffusion model for high-resolution IR. Trained on the entire image at up to 2K resolution and benefiting from the long-range modeling capacity, our method shows stronger generation capacity, capable of upsampling across wide range of scale factors (from 8 to 20) directly at 2K resolution while achieving significant speedup over previous diffusion-based image restoration methods including the recent SUPIR [56], without sacrificing the quality. To summarize, we introduce ZipIR, novel diffusionbased framework designed for high-quality and efficient high-resolution image restoration. Leveraging the highly compact and structured LP-VAE latent space, along with scaled-up diffusion model trained on full high-resolution images, ZipIR seamlessly reconstructs 2K images with both globally coherent structures and fine local fidelity from heavily degraded inputs, outperforming existing diffusionbased approaches in both efficiency and quality. 2. Related Work 2.1. High-Resolution Image Restoration High-resolution image restoration (HR-IR) aims to enhance degraded images, often requiring models capable of generating fine-grained details at high fidelity. Early approaches to image restoration targeted specific degradations independently, such as super-resolution (SR) [12, 53], denoising [10, 60], and deblurring [51, 52], often relying on fixed assumptions about degradation patterns. While effective within constrained conditions, these methods lacked the flexibility to handle real-world complexities. Recently, blind IR methods have gained popularity [48, 56, 58], integrating multiple restoration tasks within unified frameworks that can generalize across diverse degradations, as exempliFigure 2. Overview of ZipIR: (a) Latent Pyramid VAE (LP-VAE) compresses raw images into 32 downsampled latent space through pyramidal design that captures sub-band information across multiple scales (32, 4, original), ensuring high compression rate while maintaining well-structured latent space with improved representation invariance under degradation. (b) Our transformer diffusion model is trained and operates on the compressed latent space of the entire image, supporting resolutions up to 20482 pixels, enriching semantic understanding and synthesis of holistic structure. Furthermore, the pixel-space decoder (Sec. 3.1) further enhances restoration quality. fied by DiffBIR [25]. tional load for high-resolution image restoration. GAN-based methods have achieved realistic restoration results on real-world degraded images [48, 58]. However, these methods have limitations, particularly in preserving fine details under extreme scaling factors. Diffusion-based approaches like StableSR [46] and SUPIR [56], which leverage pre-trained models like Stable Diffusion [34, 38], have demonstrated notable improvements in restoration quality through multi-step processes, though these can be computationally intensive. Scaling up restoration models has shown promise, especially with advancements in large-scale architectures [56]. Our proposed method leverages the scalability of diffusion transformers [31] to tackle the complex, high-dimensional nature of HR-IR. 2.2. Efficient Diffusion Models Diffusion models are powerful generative tools but face challenges with high computational demands and slow sampling speeds, limiting their practicality [19, 38]. Samplingefficient methods [27, 39, 40, 55] reduce the number of sampling steps, thereby shortening runtime, while model-based optimizations refine model architecture, using strategies like pruning [7, 14] and linear-complexity modules [15, 26] to create faster, more compact models. As diffusion models scale for high-resolution tasks, memory limitations and inference latency also become pressing issues. Our method addresses these with LP-VAE, compact latent encoding approach that intensifies compression, reducing the spatial dimensions of feature maps and thus easing the computa3. Methodology 3.1. Latent Pyramid VAE (LP-VAE) To enable billion-scale DiT models to operate at 2K resolution and beyond, our priority is to optimize latent channel capacity and deepen the latent space mapping by adding more downsampling layers. This reduces the token count, lowering the quadratic complexity of DiT built on selfattention. As spatial compression increases, the spatial resolution of the latent representation shrinks, necessitating corresponding increase in the latent channel count to mitigate information loss. For an input image R3HW , the encoder maps it to latent code RC . Despite the increased latent channels, raising the compression ratio still significantly impacts reconstruction quality, as evident from our ablation tests in Table 4. Pyramid Cascade Encoders. Cascading networks have proven effective in other generative models [20, 32, 43], which allows different networks to independently learn representations at different resolutions, optimizing overall pipeline performance. Accordingly, our architecture employs three-level pyramid VAE encoder to capture fine-level which encodes image high-frequency details, coarse-level features which encode lower-res structures, and macro-level semantics, with cascaded latent codes serving as highly compressed image representation. The pyramid latent structure is shown on the left side of Fig. 2. The fine and coarse-level encoders independently encode σ 32 32 representations from different resolutions. For = 32, the fine-level encoder operates on the original image without downsampling, producing 52-channel latent encoding zfine R52 32 . The coarse-level encoder captures lower-resolution features with an 4 downsampled 4 input, I4 R3 4 , resulting in 9-channel latent encoding zcoarse R9 32 . To incorporate macro-level semantics, we use downsampled I32 as 3-channel image zmacro = I32µ , where µ and σ are the mean and standard deviation calculated from the entire training dataset. Finally, the concatenated latent code across all levels, denoted by = [zmacro; zcoarse; zfine], serves as the final highlycompressed 64-channel representation. Progressive Training. We employ progressive training approach. Training begins with the coarse-level encoder Ec, which requires decoder Dc to reconstruct the 12-channel latent [zmacro; zcoarse] into pixel space. After completing this training stage, the coarse-level decoder Dc is discarded. Progressively, the next stage involves training the fine-level autoencoder to achieve full-level compression. The left side of Fig. 2 illustrates this stage, the fine-level decoder Df is trained to reconstruct from 64-channel latent back to pixel space, while the coarse-level encoder remains frozen. For both training phases, we use combination of discriminator loss and LPIPS loss as recommended in [13]. Based on empirical findings, we observed that attention layers did not significantly improve performance and added unnecessary overhead for both encoding and decoding. Therefore, our entire LP-VAE is designed as pure convolutional network. Once training is complete, Ec and Ef serve as sub-networks of the LP-VAE Encoder, cascading three types of compressed mappings into highly-compression representation R64 32 32 . Finally, non-pyramidal decoder network Df decodes to obtain the RGB image. Pixel-aware Decoder-only Finetuning. Reconstructing from our highly compressed latents space achieves notable quality in high-resolution image reconstruction. However, without access to the full-resolution input, the decoder remains suboptimal for image processing applications, particularly restoration tasks that demand high pixel fidelity and detailed quality. Therefore, after obtaining the LPVAE with its encoder Ef , decoder Df , and the associated 64-channel latent space, we incorporate pixel-level details through skip connections to add spatial information during LP-VAE decoding, leading to pixel-aware decoder . To capture spatial features, we replicate an LP-VAE subencoder, Ef , initializing it with weights from the pretrained Ef . This degradation-aware feature extractor specifically handles degraded images, such as those blurred, noisy, or affected by JPEG artifacts. Additional residual layers are inserted between upsampling blocks in each layer of the LP-VAE decoder Df to pass multiscale spatial information from the degradation-aware feature extractor, effectively capturing details from low-quality inputs. To train the pixel-aware decoder , we freeze the original Ef , but unlock the degradation-aware feature extractor and the entire decoder. With image reconstruction as the learning objective, this set-up enables the decoder to learn how to utilize low-quality images to complement the highly compressed latent code with pixel-level details. Note that decoder finetuning can occur after diffusion training to enhance quality, since the latent space is not altered by freezing Ef . 3.2. Diffusion Transformer for Image Restoration With the LP-VAE trained, we use its encoder to represent an input image I. Our model, scaled-up diffusion transformer architecture of 3B parameters, G, is optimized for high-resolution image restoration. As illustrated in Fig. 2b, our framework uses two conditioning inputs: low-quality image ILQ R3HW and text embedding y, integrating both visual and semantic guidance for restoration. Low-Quality Image Conditioning. Unlike traditional restoration methods relying on pixel-level low-quality (LQ) inputs, we resize ILQ to the target resolution, compress it using our LP-VAE Encoder, and concatenate it with noisy latent zt. Despite sharing the same latent space, these latents differ significantly, necessitating separate, parameterindependent Patch Embedders for tokenization, which are arranged in parallel within the token sequence. Text Semantic Guidance. Text embeddings aid in reconstructing degraded images by refining regions based on contextual cues [56]. We train on paired text-image data, where the text prompt is caption of the original image, encoded by T5 language model [36], and integrated via cross-attention layers within the Diffusion Transformer. To support classifier-free guidance, we randomly drop the text embedding with probability of 0.05 during training. Additionally, we annotated low-quality images with negative prompts, enabling the model to produce clearer, more realistic outputs during inference with added negative text prompts. The effects of varying text prompt strengths are further analyzed in our supplementary material. Learning HR-IR with DiT. We train our model on synthetic data degraded using methods similar to [48], resizing the generated low-quality images to match the highquality images for training. This approach aligns with our focus on high-resolution restoration while ensuring robustness to various degradation patterns. Leveraging the highlycompressed latent space, we only need to process 6464 latent map even for an original image resolution of 2048. This allows us to train DiT models on high-resolution images more effectively than existing methods [50, 56], facilitating the global-range semantic understand. During DiT training, we mix crop patches ranging from 5122 to 40962. Figure 3. Our DiT-based ZipIR serves as 16 upsampler, completing super-resolution from 1282 to 20482 in only 6.9 seconds. We compare it with SUPIR [56] and SeeSR [50], evaluating both their direct 2K inference and SeeSRs default 512px output upscaled to 2K via Real-ESRGAN [48]. The gold dust on the face is real, not an artifact. 4. Experiments 4.1. Experimental Settings For training both LP-VAE and DiT, we use 300 million curated stock images paired with text as the data source. For the coarse-scale sub-model of LP-VAE, we use batch size of 512 and run for 50K iterations. For the fine-scale submodel, we start with batch size of 160 on 5122 cropped patches for 100K iterations, followed by 1K patch adaptation with batch size of 32 on 10242 cropped patches for 50K iterations. For DiT, we mix resolutions and aspect ratios to sample training images, similar to [34], using batch size of 128 over 250K iterations, with the standard learning objective [31] guiding the training process. Inference for ZipIR employs the DDIM sampler with 25 denoising steps. In recent benchmarking of IR, medium-resolution samples at 10242 or 5122 serve as HQ images, often from limited sets with fewer than 250 images, such as RealSet65 [57] and DrealSR [49]. These datasets are not ideally suited for distribution-based evaluation metrics like Frechet Inception Distance (FID) [18] due to their scale. To enable more robust benchmarking for IR at higher resolutions, we collected comprehensive set of 3000 2K-resolution photos from Pexels [33], facilitating thorough evaluation across tasks like mixture degradation restoration and superresolution across varying scale factors from 8 to 16. In the Appendix, we present an analysis of the test set. Figure 4. Our DiT-based ZipIR functions as an 8 upsampler, enhancing images from 2562 to 20482 in just 6.9 seconds, while simultaneously restoring details through deblurring, denoising, and JPEG artifact removal. The input image is degraded with Gaussian blur (σ = 1), noise (σ = 15), and JPEG compression (q = 65). The reconstructed hand retains biological features without merging with the sack texture. Quantitatively, we primarily use FID and Kernel Inception Distance (KID) [2] to measure output distribution realism. Given the HQ resolution of 2048, we additionally report Patch FID, inspired by [8, 37]. Text prompting for our model is provided via InternVL-26B [11], which generates consistent image caption. We continue to report PSNR, LPIPS [59] for benchmark purposes, despite the acknowledged misalignment of pixel-wise similarity metrics with human perception in evaluation [41, 56]. Noreference image quality metrics, such as MANIQA [54], are omitted from the main quantitative experiments in Table 1 because they downsample images to 2242, which may not adequately capture high-resolution restoration performance. To provide more comprehensive evaluation, we employ the real-world LQ dataset RealPhoto60 [56]. For fair comparisonmirroring SUPIR [56], which downsamples its 1K results to 5122we downsample our 1K results to 5122 (note that despite differences in model output resolutions, all methods use the same input images). This approach allows us to compare no-reference image quality metrics, including MANIQA [54], CLIP-IQA [45], and MUSIQA [23]. We conduct three main experiments to demonstrate our methods performance and assess the contributions of each component. First, we benchmark traditional restoration on RealPhoto60 [56] and high-resolution image restoration on Table 1. Quantitative comparison of image restoration methods under various degradation types. Mixture degradation denotes that the input image undergoes 8 downsampling, Gaussian blur with σ = 2, noise with σ = 40, and JPEG artifacts with = 50. Method PSNR LPIPS FID pFID KID103 ) 6 1 ( ) 8 ( Real-ESRGAN [48] StableSR [46] DiffBIR [25] SeeSR [50] SUPIR [56] Ours Real-ESRGAN [48] StableSR [46] DiffBIR [25] SeeSR [50] SUPIR [56] Ours a g Real-ESRGAN [48] StableSR [46] DiffBIR [25] SeeSR [50] SUPIR [56] Ours . 25.55 26.41 25.92 25.22 23.85 24.44 27.47 28.93 28.03 27.77 26.35 27. 22.24 22.15 22.45 22.06 21.65 20.41 0.5535 0.5683 0.4405 0.4321 0.4377 0.3978 0.4122 0.4238 0.3503 0.3444 0.3508 0.3374 0.5919 0.7593 0.5806 0.6085 0.6335 0.5791 19.32 24.40 21.13 13.20 15.23 9.89 10.52 5.17 9.26 4.35 7.25 3. 73.32 123.87 59.29 78.09 81.14 35.10 29.12 54.22 29.90 21.20 20.55 18.17 21.90 19.51 18.03 17.05 15.74 13.95 76.08 172.62 64.35 49.72 70.35 31.08 3.23 8.47 4.40 1.23 0.81 0.63 1.96 0.29 1.93 0.43 0.80 0. 36.07 73.25 26.19 29.47 37.75 11.23 our newly proposed validation set of 3000 images (Section 4.2). Second, we analyze the inference efficiency and model parameters among series of diffusion-based image restoration methods (Section 4.3). Third, an ablation study illustrates the effectiveness of each technical component by adding them incrementally (Section 4.4). 4.2. Comparison with Existing Methods Quantitative Evaluations. Table 1 illustrates the comparisons analysis of high-resolution (20482) image restoration across 16 to 8 super-resolution scale factor and kind of mixture degradation by 8 downsampling, Gaussian blur σ = 2, noise σ = 40, JPEG artifacts = 50. We evaluate the proposed ZipIR and recent advanced image restoration methods via PSNR, LPIPS [59], FID [18], Patch FID (pFID) and KID [2]. The method ZipIR demonstrates strong performance in high-resolution 16 and 8 scenarios. For the 16 super-resolution, ZipIR achieves notable LPIPS and FID improvements (0.3978 and 9.89, respectively), indicating superior perceptual quality and fidelity, while maintaining competitive PSNR score. Its KID score (0.63 103) also emphasizes the reduced distributional discrepancy compared to other models. For the 8 superresolution task, ZipIR continues to show robustness, with the lowest FID (3.24) and best LPIPS (0.3374), affirming its quality consistency across different scales. Under mixed degradation, although the evaluation on pixel-wise similarity of ZipIR is lower, its LPIPS (0.5791), FID (35.10), Patch FID (31.08) and KID (11.23 103) reflect an ability to preserve perceptual quality and distributional consistency in challenging conditions. Furthermore, in no-reference image quality assessment, as in Table 2, our method achieves the best or secondbest performance across all metrics. Specifically, while Table 2. Quantitative evaluation of real-world LQ images from RealPhoto60 [56] using no-reference image quality metrics. Metrics BSRGAN Real-ESRGAN StableSR DiffBIR SeeSR SUPIR Ours CLIP-IQA MUSIQ MANIQA 0.4119 55.64 0.1585 0.5174 59.42 0. 0.7654 70.70 0.3035 0.6983 69.69 0.2619 0.7721 72.21 0.5596 0.8232 73.00 0.4295 0.8154 72.75 0.6681 Table 3. Efficiency comparison of recent diffusion-based image restoration methods at 20482 resolution, including Neural Function Evaluations (NFEs), latency per denoising step, total inference time per image, and trainable parameters in diffusion models. Model Type NFEs Denoising Latency (ms) Inf. Time # Trainable Param. SeeSR [50] UNet SUPIR [56] UNet Ours DiT 50 50 1420 901 250 73.736s 52.994s 6.923s 0.5 1.2 3.1 CLIP-IQA [45] and MUSIQ [23] scores are slightly lower than those of SUPIR [56], they remain highly comparable (0.8154 vs. 0.8232 for CLIP-IQA [45] and 72.75 vs. 73.00 for MUSIQ [23]). Moreover, our method outperforms all others in MANIQA [54], highlighting the effectiveness of our approach in real-world LQ image restoration. Qualitative Evaluations. Figures 3 and 4 present the visual comparison across existing the most advanced image restoration baselines. For facial portrait restoration at 16 SR, ZipIR produces sharper, more natural results than competing models, capturing intricate local details like the nose In comparison, SUPIR [56] and structure and piercings. SeeSR [50] fall short of ZipIR in preserving clarity, exhibiting noticeable distortions. We also evaluate an alternative approach where SeeSR processes its default resolution (512px), followed by upsampling with the efficient Real-ESRGAN [48]. While this results in slightly sharper outputs compared to SeeSRs direct 2K inference, it still introduces artifacts, compromising overall visual fidelity. In the 8 IR task, as shown in Fig. 4, the LQ input suffers from blur σ = 1, noise σ = 15, and JPEG artifacts = 65. SeeSR [50] introduces over-sharpening artifacts, while SUPIR [56] over-smooths textures, leading to unnatural hallucinations. In contrast, our ZipIR effectively restores fine-grained structures such as skin details, fabric texture, and citrus surfaces while minimizing artifacts. 4.3. Efficiency Analysis Table 3 summarizes comparison of ZipIR with several advanced baseline methods in terms of denoising latency, processing time per image, and trainable diffusion model parameters. ZipIR achieves much lower denoising latency of 250 ms, outperforming all baselines, with the closest competitor, SUPIR, showing latency of 901 ms. This efficiency is due to our proposed LP-VAE, which achieves Figure 5. Effect of HR training for high-res restoration. Figure 6. Comparison for w/ and w/o the pixel-aware decoder. 32 compression rate, significantly reducing the input token count in the diffusion transformer. Even at 2K resolution, each diffusion denoising step requires substantially less time. Consequently, ZipIR demonstrates exceptional efficiency in image processing, taking only 6.92 seconds per imagea significant improvement over other models like SeeSR (73.73 seconds) and SUPIR (52.99 seconds). Our LP-VAE introduces minimal overhead when encoding or decoding 2K images, highlighting its design efficiency. Despite ZipIRs larger model size of 3.1 billion parameters, it performs inference 10.7 times faster than SeeSR, which has 1.4 billion parameters in the diffusion model. These results emphasize the superior efficiency and scalability of ZipIR for practical applications. 4.4. Effectiveness of Proposed Components We quantitatively demonstrate the impact of our proposed components through an ablation study in Table 4. To facilitate the experiments, we sampled 100 images from benchmark dataset of 3000 for the ablation study, performing 8 super-resolution from LQ 1282 px to HQ 10242 resolution. We report the metrics FID, Patch FID (pFID), and pixel-wise similarity PSNR. Starting with baseline 0.68B DiT model paired with the original f8c4-SDVAE, we observe that switching directly to f32c64-SDVAE results in decline in FID and pFID. This indicates that naively stacking networks or increasing channel dimensions in VAE does Table 4. Ablation on our model design, including latent space choices, model scaling, and different diffusion training schemes. For various VAEs, represents the compression factor, while denotes the dimensionality of the latent channels. Model 0.68B DiT + f8c4-SDVAE + f32c64-SDVAE + f32c64-LP-VAE 3B DiT f32c64-LP-VAE FID pFID PSNR 30.74 35.83 28.14 53.45 59.47 51.73 28.41 29.06 28.50 + Pyramid Cascade Encoders + 1K Patch Adapation + Pixel-aware Decoder + HR Training 22.84 21.09 20.95 18. 41.12 39.94 38.73 34.85 26.90 26.75 27.94 27.75 not guarantee robust improvement, as the latent code is susceptible to low-level perturbation and complicates the diffusion training. Next, by introducing our f32c64-LP-VAE, we achieve notable performance gains across all metrics, underscoring the impact of an optimized VAE design. Due to the lack of pre-trained f32c64-SDVAE checkpoint, we trained it with the same settings as f32c64-LP-VAE for fair comparison. Scaling up to 3B DiT model, we incrementally add each of our proposed components. Notably, as we scale up the diffusion transformer, we observe significant boosts in perceptual quality and fidelity. Each addition, from Pyramid Cascade Encoders progressively enhances performance, with consistent reductions in FID and pFID alongside increases in PSNR. HR Training. Our high-compression LP-VAE encoder (f32) allows DiT models to be trained on global image above 2K resolution. We conduct qualitative study to demonstrate the effect of the HR training technique. As illustrated in Fig. 5, HR training facilitates sharper and more accurate local details, such as the structure of accessories and textures of fur, compared to its counterpart. Pixel-aware Decoder. The pixel-aware decoder is introduced as complementary module to restore the spatial information of the input image at the pixel level. This proposed module enables ZipIR to capture spatial details directly from the original image, rather than relying solely on latent-level information. As shown in Fig. 6, the use of the pixel-aware decoder enhances clarity in textual and structural details, demonstrating its effectiveness. 5. Conclusion and Future Work We present ZipIR, framework that tackles efficiency, scalability, and quality in ultra-high-resolution image restoration. We developed the Latent Pyramid VAE (LP-VAE) to compress images into structured latent space, enabling the training of the high-capacity Diffusion Transformer (DiT) on entire images. Tested on full images up to 2K resolution, ZipIR demonstrates remarkable improvement over existing diffusion-based methods, highlighting the advantages of enhanced latent representation and scalable generative models for image restoration. We plan to explore even higher compression rates and larger capacity diffusion models for improved high-resolution image restoration."
        },
        {
            "title": "References",
            "content": "[1] Dosovitskiy Alexey. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020. 1 [2] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In ICLR, 2018. 6, 7 [3] Peter Burt and Edward Adelson. The laplacian pyramid as compact image code. In Readings in computer vision, pages 671679. Elsevier, 1987. 1 [4] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: new benchmark and new model. In ICCV, 2019. 12 [5] Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction. In CVPR, 2022. 1 [6] Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang, Radu Timofte, and Yulun Zhang. Retinexformer: One-stage retinexbased transformer for low-light image enhancement. In ICCV, 2023. 1 [7] Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi. Ld-pruner: Efficient pruning of latent diffusion models using task-agnostic insights. In CVPR, 2024. [8] Lucy Chai, Michael Gharbi, Eli Shechtman, Phillip Isola, and Richard Zhang. Any-resolution training for highresolution image synthesis. In ECCV, 2022. 6 [9] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative In International conference on machine learning, model. pages 864872. PMLR, 2018. 1 [10] Yunjin Chen and Thomas Pock. Trainable nonlinear reaction diffusion: flexible framework for fast and effective image restoration. TPAMI, 2016. 2 [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 6 [12] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295307, 2015. 2 [13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 1, 4 [14] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. In NeurIPS, 2023. 3 [15] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkvarXiv preprint like architectures for diffusion models. arXiv:2404.04478, 2024. 3 [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and In Advances Yoshua Bengio. Generative adversarial nets. in neural information processing systems, pages 26722680, 2014. 1 [17] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for imIn European Conference on Computer Viage generation. sion, pages 3755. Springer, 2025. 1 [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. 5, [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. 1, 3 [20] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 2022. 3 [21] Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Zhifei Zhang, Yilin Wang, Jianming Zhang, and Jiebo Luo. Finecaption: Compositional image captioning focusing on wherever you want at any granularity. arXiv preprint arXiv:2411.15411, 2024. 1 [22] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo Luo. Mmcomposition: Revisiting the compositionality arXiv preprint of pre-trained vision-language models. arXiv:2410.09733, 2024. 1 [23] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. MUSIQ: multi-scale image quality transformer. In ICCV, 2021. 6, 7 [24] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In ICCV, 2021. [25] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior, 2024. 1, 3, 7, 12 [26] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. 3 [27] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionbased text-to-image generation. In ICLR, 2024. 3 [28] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. CoRR, abs/2401.03048, 2024. 1 [29] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 25742582, 2016. 1 [30] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. [48] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In ICCVW, 2021. 2, 3, 4, 5, 7, 12 [49] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, and Liang Lin. Component divide-and-conquer for real-world image super-resolution. In ECCV, 2020. 5, 12 [50] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. Seesr: Towards semantics-aware real-world image super-resolution. In CVPR, 2024. 1, 2, 4, 5, 7, 12 [51] Li Xu, Jimmy Ren, Ce Liu, and Jiaya Jia. Deep convolutional neural network for image deconvolution. Advances in neural information processing systems, 27, 2014. 2 [52] Li Xu, Xin Tao, and Jiaya Jia. Inverse kernels for fast spatial deconvolution. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 3348. Springer, 2014. 2 [53] Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution as sparse representation of raw image patches. In 2008 IEEE conference on computer vision and pattern recognition, pages 18. IEEE, 2008. 2 [54] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In CVPR, 2022. 6, [55] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 3 [56] Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, and Chao Dong. Scaling up to excellence: Practicing model scaling for photorealistic image restoration in the wild. In CVPR, 2024. 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16 [57] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. image superResshift: resolution by residual shifting. NeurIPS, 2023. 5 Efficient diffusion model for [58] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind image super-resolution. In ICCV, 2021. 2, 3, 12 [59] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6, 7 [60] Yaping Zhao, Haitian Zheng, Zhongrui Wang, Jiebo Luo, improving video denoising and Edmund Lam. Manet: with multi-alignment network. In ICIP, 2022. 2 [31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 1, 2, 3, 5 [32] Pablo Pernias, Dominic Rampas, Mats Richter, Christopher Pal, and Marc Aubreville. Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. In ICLR, 2024. [33] Pexels. Pexels. https://www.pexels.com. 5 [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 1, 3, 5 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 13 [36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 4 [37] Jingjing Ren, Wenbo Li, Haoyu Chen, Renjing Pei, Bin Shao, Yong Guo, Long Peng, Fenglong Song, and Lei Zhu. Ultrapixel: Advancing ultra high-resolution image synthesis to new peaks. In NeurIPS, 2024. 6 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 3, 12 [39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, 2020. [40] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 3 [41] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In WACV, 2022. 6 [42] Szegedy. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 1 [43] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. In ICLR, 2024. [44] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 1 [45] Jianyi Wang, Kelvin C. K. Chan, and Chen Change Loy. Exploring CLIP for assessing the look and feel of images. In AAAI, 2023. 6, 7 [46] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. IJCV, 2024. 1, 3, 7, 11 [47] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 77947803, 2018. 1 Supplementary ZipIR: Latent Pyramid Diffusion Transformer for High-Efficiency High-Resolution Image Restoration A. Additional Implementation Details Training Configuration. The optimization process employs AdamW with initial learning rate 5 105 (decaying to 5 106), weight decay 0.05, and betas (0.9, 0.95). We implement DDPM loss with ϵ-prediction objective, coupled with linear noise schedule (βstart = 0.00085, βend = 0.012) and logit-normal time-step sampling for enhanced convergence. Architecture Specifications. Our LP-VAE is constructed as UNet-based architecture with 128 base channels. Specifically, the encoders Ec/Ef leverage residual blocks with channel multipliers [1, 2, 4, 4]/[1, 2, 4, 4, 4, 4] respectively, and these configurations are mirrored in the decoders Dc/Df . Each scale integrates two residual blocks powered by Swish activations. Our DiT adopts 24-layer transformer, featuring hidden dimension of 2048, 16 attention heads, and strategy that incorporates adaptive layer normalization. Lastly, the f32c64-SDVAE used in ablation studies is modified from the LDM baseline, using zchannels = 64 and chmult = [1, 2, 4, 4, 4, 4], thereby achieving 32 spatial compression ratio. Pixel-aware Decoder. While previous work [46] adopts similar approach by fine-tuning auxiliary networks for skip connections within the VAE decoder, we propose joint optimization of the feature extractor and VAE decoder. As illustrated in Fig. 8, our pixel-aware decoder builds upon this design. B. Ultra-High Image Super-Resolution In the main text, we comprehensively present cases of 2K image restoration. To further evaluate whether our method can generalize effectively to ultra-high-resolution image super-resolution, such as 4K and even 8K, we conducted additional experiments. As shown in Figures 14-15, our ZipIR achieved 16 upscale, enhancing 256-pixel and 512-pixel images to 4K and 8K resolutions, respectively. This demonstrates the capability of our method to handle ultra-high-resolution image super-resolution effectively. We have supplemented the evaluation of inference latency for ZipIR during diffusion denoising at each time step when synthesizing ultra-high-resolution images, including 4K and 8K resolutions. All efficiency evaluations were conducted on an A100-80G GPU. As shown in Fig. 7, our ZipIR demonstrates significant advantages across all resolutions. In contrast, the second-best method, SUPIR, fails to infer images larger than 29442 resolution due to outFigure 7. Denoising latency across ultra-high resolutions. of-memory errors. Furthermore, ZipIR achieves lower inference latency at 40962 resolution compared to SeeSR at 20482. C. Real-World HR Image Restoration We collected randomly sampled image thumbnails from the internet, capturing diverse real-world degradations, and used them as LQ inputs for high-resolution image restoration experiments. As shown in Figures 16 and 17, our ZipIR effectively removes compression artifacts, reduces noise, and enhances fine details, producing high-resolution restored images with improved perceptual quality. These results demonstrate ZipIRs robustness in handling in-thewild degradations across varying content and degradation Figure 8. The pixel-aware decoder extracts high-res features from raw pixels with degradation-aware feature extractor, enhancing the low-level fidelity of the decoded result during inference. Figure 9. Qualitative comparison of VAE image reconstruction. The first row visualizes the latent space representations, while the second row shows the reconstructed results. types, highlighting its practical applicability in real-world scenarios. D. Comparisons at 4 Scale Factor Our goal is not to achieve state-of-the-art performance on traditional settings (e.g., medium-resolution outputs at 4 scale factor on previous benchmarks), but rather to enable efficient image restoration (IR) at ultra-high resolutions. Nevertheless, for fair comparisons and more comprehensive evaluation of our proposed method, we provide experiments on previous real-world image restoration benchmarks, including the DrealSR [49] and RealSR [49] datasets, for 4 scale factor. Specifically, we restore LQ images of 2562 px to HQ resolutions of 10242 px. Following the experimental setup described in the main text, we adopt FID, patch FID (pFID), PSNR, SSIM, and LPIPS as evaluation metrics. As shown in Table 5, even on traditional real-world image restoration benchmarks, our ZipIR achieves the best FID (61.35 and 24.62) and pFID (78.07 and 61.08) across both datasets, demonstrating superior perceptual quality. On DrealSR, it also achieves the highest PSNR (25.12), reflecting exceptional clarity and detail. These results validate the robustness and effectiveness of ZipIR for real-world 4 image restoration. E. VAE Reconstruction To intuitively highlight the differences between our proposed LP-VAE and straightforward deepening of SDVAE, we present qualitative comparison of LP-VAE and SD-VAE f32c64 on 2048-resolution image reconstruction. Both LP-VAE and SD-VAE f32c64 perform 32 image compression and use 64-channel dimensionality to represent the latent space. As shown in Figure 9, our proposed LP-VAE faithfully reconstructs the images, while SD-VAE f32c64 struggles to recover high-frequency details such as text and facial features. Table 5. Comparative analysis of 4 image restoration methods on real-world low-quality datasets. F. Additional Qualitative Comparisons Dataset Method FID pFID PSNR SSIM LPIPS BSRGAN [58] Real-ESRGAN [48] SwinIR [24] SD Upscaler [38] DiffBIR [25] SeeSR [50] SUPIR [56] Ours BSRGAN [58] Real-ESRGAN [48] SwinIR [24] SD Upscaler [38] DiffBIR [25] SeeSR [50] SUPIR [56] Ours 78.17 82.41 78.43 68.33 68.75 68.96 61.84 61. 41.57 45.10 43.60 33.96 36.33 35.05 38.93 24.62 95.84 88.18 84.59 81.39 90.62 79.42 84.99 78.07 69.96 71.51 66.47 68.83 62.19 65.38 69.42 61.08 25.35 24.70 24.86 24.60 25.62 25.06 24.04 24.19 24.88 24.05 24.19 23.91 25.03 24.59 23.97 25.12 0.7385 0.7384 0.7444 0.6644 0.7149 0.7209 0.6673 0. 0.6969 0.6861 0.6905 0.6276 0.6701 0.6701 0.6193 0.6843 0.2809 0.2865 0.2732 0.3598 0.3896 0.2874 0.3425 0.2750 0.2174 0.2273 0.2209 0.2992 0.2175 0.2064 0.2884 0.2541 RealSR [4] DrealSR [49] In the main text, we provide only two qualitative comparisons. To offer more comprehensive and intuitive evaluation of our methods performance, we present additional qualitative results. 2K 3000-Sample Test Set. Following the experimental setup described in the main text, Figures 18-20 showcase 16 super-resolution. Our ZipIR faithfully reconstructs fine details, such as the nose ring in Figure 18, and textures, like the frogs chin in Figure 20, while avoiding hallucination of unrealistic structures, as seen in the human chin in Figure 19. Figures 21-22 illustrate 8 image restoration, where the inputs suffer from blur (σ=1), noise (σ=15), and JPEG arTable 6. Quantitative evaluation of configurations with and without text prompts, including varying CFG strengths under the w/ text prompt setting. The default configuration is CFG strength = 3.5, and its values are equivalent to those for w/ text prompt. Text Prompt CFG Strength w/ text prompt Default (3.5) 1.5 6. w/o text prompt Metrics FID pFID PSNR 18.05 19.73 20.41 34.85 35.12 36.88 19.97 35.40 27.75 27.90 28.25 28. tifacts (q=65). ZipIR effectively recovers realistic textures, such as the grass in Figure 21, and restores sharper details, exemplified by the chess piece in Figure 22. 512px RealPhoto60 [56]. As qualitative counterpart to Table 2 in the main text, Figures 11 to 13 present visual comparisons on the RealPhoto60 test set. SeeSR and DiffBIR use their default training resolutions, while both SUPIR and our ZipIR process images at 1K resolution, following SUPIRs approach, before downsampling to 512px. G. Effectiveness of Text Prompt Table 6 compares configurations with and without text prompts, highlighting the influence of CFG strength on FID, pFID, and PSNR. The default setting, with CFG strength of 3.5, achieves balanced performance, yielding an FID of 18.05, pFID of 34.85, and PSNR of 27.75. Reducing CFG strength to 1.5 slightly enhances PSNR but worsens FID and pFID, while increasing it to 5.5 maximizes PSNR (28.25) at the expense of perceptual fidelity. Without text prompts, FID and pFID degrade, though PSNR (28.04) marginally surpasses the default. This underscores the role of text prompts in enhancing perceptual fidelity, while CFG strength tuning mediates the trade-off between fidelity and quality. H. Test Set Construction We curate high-resolution test set of 3,000 images randomly sampled from Pexels, ensuring comprehensive covThe dataset enerage of real-world visual concepts. compasses diverse semantic content, validated through CLIP [35] text similarity analysis, as illustrated in Fig. 10, spanning six major categories. Special attention is given to maintaining balanced representation across visual domains while preserving natural image statistics. Figure 10. Semantic category distribution of our 3,000-sample test dataset, classified using CLIP [35]. Figure 11. The visual comparison on the real-world LQ dataset RealPhoto60 dataset [56] at resolution 5122. Figure 12. The visual comparison on the real-world LQ dataset RealPhoto60 dataset [56] at resolution 5122. Figure 13. The visual comparison on the real-world LQ dataset RealPhoto60 dataset [56] at resolution 5122. Figure 14. 4K Image Super-Resolution Result by ZipIR. The input is 256px low-resolution image, and the output achieves 4096px (16Mpix) resolution with 16 scaling. Figure 15. 8K Image Super-Resolution Result by ZipIR. The input is 512px low-resolution image, and the output achieves 8192px (67Mpix) resolution with 16 scaling. Figure 16. Real-world image restoration results by ZipIR. The inputs are low-resolution thumbnails sourced from the internet, featuring in-the-wild degradations. Figure 17. Real-world image restoration results by ZipIR. The inputs are low-resolution thumbnails sourced from the internet, featuring in-the-wild degradations. Figure 18. Our DiT-based ZipIR achieves 16 super-resolution, enhancing images from 1282 to 20482 in just 6.9 seconds. Fine details, such as the nose ring and earrings, are faithfully restored without artifacts. Please zoom in for detailed comparison. Figure 19. Our DiT-based ZipIR serves as 16 upsampler, completing super-resolution from 1282 to 20482 in only 6.9 seconds. Fine details, such as the chin, are accurately restored without artifacts. Please zoom in for detailed comparison. Figure 20. Our DiT-based ZipIR serves as 16 upsampler, completing super-resolution from 1282 to 20482 in only 6.9 seconds. The texture of the frogs chin is faithfully reconstructed without blur. Please zoom in for detailed comparison. Figure 21. Our DiT-based ZipIR performs 8 super-resolution, enhancing images from 2562 to 20482 in just 6.9 seconds, while simultaneously restoring details through deblurring, denoising, and JPEG artifact removal. The input image is degraded with Gaussian blur (σ = 1), noise (σ = 15), and JPEG compression (q = 65). Fine textures, such as the grass, are faithfully reconstructed. Zoom in for detailed comparison. Figure 22. Our DiT-based ZipIR performs 8 super-resolution, enhancing images from 2562 to 20482 in just 6.9 seconds, while simultaneously restoring details through deblurring, denoising, and JPEG artifact removal. The input image is degraded with Gaussian blur (σ = 1), noise (σ = 15), and JPEG compression (q = 65). The generated chess piece base is sharp and free from blurring."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Rochester"
    ]
}