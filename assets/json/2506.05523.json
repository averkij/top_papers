{
    "paper_title": "MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning",
    "authors": [
        "Zikui Cai",
        "Andrew Wang",
        "Anirudh Satheesh",
        "Ankit Nakhawa",
        "Hyunwoo Jae",
        "Keenan Powell",
        "Minghui Liu",
        "Neel Jay",
        "Sungbin Oh",
        "Xiyao Wang",
        "Yongyuan Liang",
        "Tom Goldstein",
        "Furong Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research."
        },
        {
            "title": "Start",
            "content": "MORSE-500: Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning Zikui Cai 1 Andrew Wang 1 Keenan Powell 1 Minghui Liu 1 Neel Jay 1 Anirudh Satheesh 1 Ankit Nakhawa 1 Hyunwoo Jae 1 Sungbin Oh 1 Xiyao Wang 1 Yongyuan Liang 1 Furong Huang 1,2 Tom Goldstein 1 University of Maryland, College Park 2 Capital One Despite rapid advances in visionlanguage models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problemsolving, neglecting the broader spectrum of reasoning skillsincluding abstract, physical, planning, spatial, and temporal capabilitiesrequired for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamicsenabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systemsincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research. Project: https://morse-500.github.io/ Datasets: https://huggingface.co/datasets/video-reasoning/morse-500 Video Viewer: https://huggingface.co/datasets/video-reasoning/morse-500-view Code: https://github.com/morse-benchmark/morse-500-code Contact: zikui@umd.edu 5 2 0 2 J 5 ] . [ 1 3 2 5 5 0 . 6 0 5 2 : r 1. Introduction Multimodal reasoning is the frontier. Recent advances in visionlanguage models (VLMs) have pushed the boundaries of perception and retrieval [Alayrac et al., 2022, Li et al., 2023, OpenAI, 2023], but robust reasoning remains elusive [Zhang et al., 2023, Lu et al., 2024]. As these models are increasingly deployed in domains requiring inference, planning, and interactionfrom embodied agents [Shridhar et al., 2023] to scientific assistants [Shen et al., 2023]there is growing need to evaluate and develop their capacity for genuine reasoning over multimodal inputs. This shift demands capabilities that go beyond recognition or retrieval, toward causal, temporal, abstract, and physically grounded understanding. MORSE: Multimodal Reasoning Stress-test Environment (a) (b)"
        },
        {
            "title": "Math Abstract Spatial Temporal Physical Planning",
            "content": "(c) MathVista [Lu et al., 2024] MathVision [Wang et al., 2024a] DynaMath [Zou et al., 2024] Mementos [Wang et al., 2024b] HourVideo [Chandrasegaran et al., 2024] LongVideoBench [Wu et al., 2024] EMMA [Hao et al., 2025] PhysBench [Chow et al., 2025] PHYBench [Qiu et al., 2025] SeePhys [Xiang et al., 2025] MORSE-500 (Ours) Figure 1: (a) Task distribution of MORSE-500. (b) Performance of the best-performing models on MORSE-500. (c) Comparison of benchmarks across six reasoning categories. Only MORSE-500 spans all categories and supports programmatic control over difficulty and content. Benchmark evolutionand persistent blind spots. The trajectory of evaluation has mirrored model capabilities: early benchmarks focused on recognition (e.g., TextVQA [Singh et al., 2019], DocVQA [Mathew et al., 2021], OCR-VQA [Mishra et al., 2019]), then knowledge retrieval (e.g., MMMU [Yue et al., 2023], ScienceQA [Lu et al., 2022]), and more recently mathematical reasoning [Lu et al., 2024, Wang et al., 2024a, Zou et al., 2024], and physical reasoning [Chow et al., 2025, Qiu et al., 2025, Xiang et al., 2025]. However, most of these datasets rely on static images and narrowly scoped question types, overlooking reasoning in dynamic, interactive environments where the ability to process sequences, anticipate outcomes, and generalize abstract patterns is essential. MORSE: Multimodal Reasoning Stress-test Environment Limitations of current benchmarks. Despite recent progress, todays reasoning benchmarks suffer from three structural limitations: Static modality bias: Most benchmarks rely on single-frame images, ignoring the temporal evolution and causality inherent to many real-world tasks. Narrow reasoning spectrum: They often focus heavily on math word problems [Lu et al., 2024, Wang et al., 2024a, Zou et al., 2024], underrepresenting reasoning types such as spatial logic, temporal inference, physical causality, abstraction, and multi-step planning. Rapid saturation: Many benchmarks are quickly saturated by current models [Lu et al., 2024], offering little diagnostic signal once performance plateaus. Moreover, current benchmarks often conflate reasoning with perception and retrieval [Zhang et al., 2023], making it difficult to assess whether models are genuinely reasoning or merely pattern-matching. This highlights the need for principled evaluation framework that systematically varies difficulty while explicitly controlling for perceptual and knowledge-based confounds. Introducing MORSE-500. To address these challenges, we present MORSE-500a video benchmark explicitly designed to evaluate diverse forms of multimodal reasoning across time. The key advantages of MORSE-500 include: Diverse Reasoning Categories: MORSE-500 comprises 500 fully-scripted videos, each embedding question within its visual narrative. These span six complementary reasoning types: mathematical, abstract, spatial, temporal, physical, and planning, offering comprehensive and balanced stress test across the full spectrum of reasoning challenges. Truly Vision-Centric: Questions are embedded directly within the video content, rather than provided as separate textual prompts. This ensures models must extract and reason over information grounded in visual input alone, eliminating shortcut cues and better simulating real-world visual understanding. Scalable Difficulty: core innovation of MORSE-500 is its programmatic controllability. All videos are deterministically generated via Python scripts that combine Manim Community [2025], Matplotlib, MoviePy, generative video models, and curated real footage, enabling fine-grained control over scene complexity, distractor density, and temporal duration. Difficulty can be precisely scaled by adjusting script parameters, allowing MORSE-500 to serve as stress test that evolves with model capabilities. We plan to release ever-harder versions as models approach saturation. In our initial SOTA Models Demonstrate Significant Room for Improvement in Multimodal Reasoning. evaluation, state-of-the-art models OpenAI-o3 [OpenAI, 2025], Gemini 2.5 Pro [Google, 2025]show relatively low performance across all reasoning categories in the MORSE-500 benchmark, with especially pronounced challenges in abstract reasoning and planning tasks (see Table 2). These results underscore the need for models with improved temporal memory, compositionality, and generalization across dynamic contexts. Testbed for Long-Term Progress. MORSE-500 offers scalable and reproducible foundation for advancing research in multimodal reasoning. By releasing the full corpus, programmatic generation scripts with ground-truth annotations, and lightweight evaluation harness, we aim to support transparent and forwardcompatible benchmarking. As dynamic, extensible benchmark, MORSE-500 is well positioned to catalyze 3 MORSE: Multimodal Reasoning Stress-test Environment Figure 2: Representative examples from MORSE-500 demonstrating different reasoning categories. Each row shows sequential frames sampled from video task: (top) abstract reasoning with ARC-AGI2 pattern recognition requiring rule induction from visual transformations, (2nd) spatial reasoning through cube rotation testing 3D transformation understanding and mental rotation abilities, (3rd) planning reasoning via rope tying sequences assessing multi-step procedural understanding, (4th) physical reasoning using real vs. AI-generated video discrimination to test intuitive physics understanding, and (bottom) planning reasoning through maze navigation testing spatial pathfinding and goal-directed behavior. Questions are embedded directly within the video content, requiring models to extract relevant information from the temporal sequence rather than from separate text prompts. Models are simply prompted with \"Answer the question in this video\" with no additional context, ensuring evaluation of true multimodal reasoning capabilities across the temporal dimension. Visit our Website and HuggingFace dataset to view more videos. 4 MORSE: Multimodal Reasoning Stress-test Environment the development of reasoning-centric architectures, evaluation methodologies, and failure diagnostics for the next generation of visionlanguage models. 2. MORSE-500 2.1. Design Principles The development of MORSE-500 was guided by four foundational principles aimed at addressing critical limitations in existing multimodal reasoning benchmarks while establishing robust framework for systematic evaluation and future extensibility. Our design emphasizes temporal-first evaluation through video-based tasks that require genuine temporal understanding, truly vision-centric assessment where questions are embedded directly within visual content rather than provided as separate text, comprehensive reasoning taxonomy grounded in established cognitive frameworks spanning six complementary reasoning categories, and programmatic generation with scalable difficulty enabling systematic complexity control and forward compatibility as model capabilities advance. Comprehensive Reasoning Taxonomy. MORSE-500 spans six complementary reasoning categories (see table 1), each grounded in established cognitive science frameworks and designed to evaluate distinct cognitive capabilities essential for robust multimodal intelligence. Our taxonomy draws from the Cattell-Horn-Carroll (CHC) theory of cognitive abilities [Carroll, 1993], dual-process theory [Evans and Stanovich, 2013], and computational models of reasoning [Holyoak and Morrison, 2013]. Abstract reasoning targets pattern recognition, logical inference, and symbolic reasoning associated with fluid intelligence, requiring operation at multiple levels of abstraction [Gentner, 1983]. Mathematical reasoning evaluates fluid reasoning (Gf) and quantitative knowledge (Gq), assessing arithmetic operations, algebraic relations, and quantitative analysis through dynamic visualizations that integrate visual-spatial information with numerical processing [Dehaene, 2011]. Physical reasoning examines intuitive physics understanding through object dynamics and causal interactions, bridging perceptual experience with abstract physical knowledge [McCloskey et al., 1983]. Planning reasoning evaluates executive function and goal-directed behavior, emphasizing multi-step reasoning and sequential decision-making central to cognitive control theories [Miyake et al., 2000]. Spatial reasoning corresponds to visual-spatial processing (Gv), testing 3D transformations, perspective understanding, and object relationships through tasks requiring mental model construction and manipulation [Shepard and Metzler, 1971]. Temporal reasoning addresses sequence understanding and causal inference over time, evaluating temporal order tracking and future state prediction aligned with event segmentation theory [Zacks et al., 2007]. Programmatic Generation with Scalable Difficulty and Forward Compatibility. To ensure extensibility, reproducibility, and precise experimental control, all videos are generated through deterministic Python scripts utilizing established libraries including Manim for mathematical visualizations and 2D/3D object rendering and animation, Matplotlib for statistical graphics, MoviePy for image transforming effects, and video generative models for realistic scenario generation. This programmatic foundation enables fine-grained manipulation of complexity parameters including entity count, reasoning depth, distractor density, temporal dynamics (static to highly dynamic sequences), and visual complexity (minimal to high-noise environments). core innovation of MORSE-500 lies in its systematic difficulty progression that can evolve alongside model capabilities. Unlike static benchmarks that quickly saturate and become obsolete, MORSE-500 functions as living evaluation framework where new instances can be generated with precisely specified complexity profiles. 5 MORSE: Multimodal Reasoning Stress-test Environment Difficulty scaling operates across multiple orthogonal dimensions: structural complexity (number of entities, interaction patterns), cognitive demands (reasoning depth, abstraction level), environmental challenges (visual noise, occlusion, temporal irregularity), and task-specific parameters (plan length for planning tasks, transformation complexity for spatial reasoning). The deterministic generation process ensures perfect reproducibility while supporting systematic difficulty scaling as model capabilities advance, enabling the identification of specific reasoning weaknesses and targeted evaluation of architectural improvements. This scalable architecture ensures that MORSE-500 remains diagnostically valuable as models improve, supporting the generation of arbitrarily challenging instances on demand while maintaining consistent evaluation standards and serving as an effective stress test for next-generation multimodal systems. 2.2. Dataset Statistics MORSE-500 comprises 500 carefully curated video instances with embedded reasoning questions, systematically distributed across six complementary reasoning categories to ensure comprehensive cognitive coverage and balanced evaluation. Table 1: Dataset Distribution by Reasoning Domain"
        },
        {
            "title": "Physical Reasoning",
            "content": "12.8% 64 instances 16.8% 84 instances 12.8% 64 instances Pattern recognition, logical inference, symbolic reasoning Arithmetic operations, algebraic geometric analysis, relations, quantitative comparisons Object dynamics, causal interactions, physics laws, physical commonsense"
        },
        {
            "title": "Temporal Reasoning",
            "content": "20.0% 100 instances 21.6% 108 instances 16.0% 80 instances Multi-step reasoning, directed problem solving goalObject relationships, spatial transformations, 3D reasoning Sequence understanding, causal inference over time Category Distribution and Strategic Allocation. As shown in table 1, the dataset employs purposeful distribution across reasoning domains, with category allocation reflecting both cognitive importance and evaluation priorities: Spatial reasoning (21.6%, 108 instances) receives the largest allocation given its fundamental role in multimodal understanding; Planning reasoning (20.0%, 100 instances) emphasizes multi-step reasoning capabilities critical for autonomous systems; Mathematical reasoning (16.8%, 84 instances) covers structured problem-solving across arithmetic, algebraic, and geometric domains; Temporal reasoning (16.0%, 80 instances) evaluates sequence understanding and causal inference over time; while Abstract reasoning (12.8%, 64 instances) and Physical reasoning (12.8%, 64 instances) provide focused assessment of pattern recognition and physics-based inference respectively. Within each category, tasks span multiple specialized subcategories: mathematical reasoning includes arithmetic operations, algebraic relations, geometric analysis, and quantitative comparisons; abstract reasoning encompasses pattern recognition, logical inference, and symbolic reasoning; spatial reasoning covers object relationships, spatial transformations, and 3D reasoning; temporal reasoning evaluates sequence understanding and causal inference over time; physical reasoning tests object dynamics, causal interactions, and physics laws; and planning reasoning examines multi-step reasoning and goal-directed problem solving. 6 MORSE: Multimodal Reasoning Stress-test Environment (a) Distribution of Video Durations (s) (b) Distribution of Video Frame Rates (c) Distribution of Video Frame Counts (d) Duration Distribution by Video Resolution Figure 3: Dataset statistics including duration, fps, frame count, and resolution. Video Characteristics. Our dataset comprises 500 videos with total duration of 3.1 hours and an aggregate file size of 1.4 GB. As shown inn Figure 3, the videos exhibit considerable diversity in temporal characteristics, with durations ranging from 5.1 to 140.0 seconds (mean: 22.1s, median: 18.0s, std: 19.3s), indicating predominance of short-form content with high information density. Frame rates vary significantly across the dataset, spanning from 5.0 to 60.0 FPS with mean of 35.6 FPS and median of 30.0 FPS. Notably, 60 FPS emerges as the most frequent frame rate, reflecting modern high-quality video capture standards. In terms of spatial resolution, the dataset demonstrates multi-modal distribution: 45.2% of videos are recorded in Full HD (19201080), while 27.0% are in standard definition (854480), and 16.0% utilize an intermediate resolution of 800533 pixels. The remaining videos span various resolutions including square formats (19201620, 16001600), collectively representing diverse recording devices and platform requirements. File sizes exhibit high variability, ranging from less than 0.1 MB to 68.2 MB (mean: 2.9 MB, median: 0.8 MB), with the substantial difference between mean and median suggesting right-skewed distribution dominated by smaller files with occasional larger outliers. This heterogeneous composition reflects the natural diversity of developer-generated content across different software environments and conditions. Complexity Scaling and Difficulty Gradation. Each reasoning category incorporates systematic difficulty variation through programmatically controlled parameters. Complexity is measured along multiple orthogonal dimensions: (1) entity complexity, ranging from 2-3 simple objects to 15+ interconnected elements; (2) 7 MORSE: Multimodal Reasoning Stress-test Environment reasoning depth, spanning 1-2 step direct inference to 5+ step multi-hop reasoning chains; (3) distractor density, varying from minimal noise to high-distraction environments with 8+ irrelevant elements; (4) temporal complexity, from static sequential presentation to dynamic concurrent processes; and (5) visual complexity, including occlusion patterns, perspective changes, and rendering fidelity variations. Difficulty distributions are calibrated to span from tasks solvable by current models (ensuring non-trivial baseline performance) to challenges requiring advanced reasoning capabilities. Approximately 20% of instances target current model capabilities, 50% represent moderate extensions requiring improved reasoning, and 30% constitute stress tests for next-generation systems. Figure 4: Programmatic difficulty scaling in MORSE-500. Each row demonstrates how task complexity can be systematically increased while maintaining the core reasoning category: (top) maze navigation tasks with increasing path complexity; (middle) abstract shape transformation tasks with more complex geometric patterns and more reasoning steps; (bottom) 3D cube visualization with increasing structural complexity and color patterns. The datasets programmatic foundation enables systematic expansion: new instances can be generated with specified difficulty profiles, novel parameter combinations can explore untested reasoning scenarios, and category-specific stress tests can be developed as model capabilities advance, ensuring MORSE-500 remains relevant evaluation framework for future multimodal reasoning research. 2.3. Data Generation and Validation Process MORSE-500 employs systematic data generation pipeline that transforms high-quality reasoning tasks into challenging video-based assessments through programmatic generation and rigorous validation. Our approach ensures both scalable content creation and reliable evaluation standards across all reasoning categories. 8 MORSE: Multimodal Reasoning Stress-test Environment Math Benchmarks, ARC-AGI 3Blue1Brown Youtube Animation Websites, Robotics Manim, Matplotlib Generative Models OpenCV, MoviePy"
        },
        {
            "title": "Reasoning\nTaxonomy",
            "content": "Task Identification & Adaptation Vibe Coding & Prototyping"
        },
        {
            "title": "Technical Issues",
            "content": "Figure 5: MORSE-500 data generation and validation pipeline. The process flows systematically from reasoning taxonomy through task adaptation, prototyping, and implementation to final validation. External inspiration sources (blue dashed) inform task design and implementation, while quality assurance loops (red) enable iterative refinement at technical and cognitive validation stages. 2.3.1. Programmatic Content Creation Pipeline Our data generation process follows principled four-stage pipeline designed to create diverse, challenging, and reproducible video reasoning tasks. Task Identification and Adaptation. Beginning with our established six-category reasoning taxonomy, we systematically identify high-quality exemplars from existing benchmarks [Wang et al., 2024a, Chollet et al., 2025, Motamed et al., 2025] and expand them into dynamic video formats. For abstract reasoning, we adapt ARC-AGI pattern recognition tasks Chollet [2019] into animated sequences showing rule transformations over time, and included other tasks such as symbolic reasoning with anagram transformation. Mathematical reasoning draws inspiration from dynamic visualizations in educational content, particularly the 3Blue1Brown YouTube channels approach to mathematical explanation through animation, adapting static problems from math benchmarks [Lu et al., 2024] into temporal mathematical narratives. Spatial reasoning extends traditional mental rotation tasks from [Wang et al., 2024a] into continuous 3D transformations with occlusion and perspective changes. Physical reasoning incorporates real-world physics scenarios from established datasets [Motamed et al., 2025] while generating synthetic alternatives using state-of-the-art video generation models for discrimination tasks. Planning reasoning leverages maze and frozen-lake environment Brockman et al. [2016], Ivanitskiy et al. [2023], animated rope knots tying database [Grog, 2025], and robotic manipulation datasets [Zhu et al., 2023, Wu et al., 2025, Haldar et al., 2023, Wu et al., 2023, Wang et al., 2023, Bahl et al., 2023], transforming sequential action demonstrations into temporal reasoning challenges. Temporal reasoning creates novel sequence understanding tasks through procedural animations and cause-effect chains. Programmatic Implementation. We employ \"vibe coding\" approach where domain experts first create conceptual prototypes that capture the essential reasoning challenges, then iteratively refine these into fully functional, parameterized generation scripts. Each reasoning category utilizes specialized libraries: MORSE: Multimodal Reasoning Stress-test Environment Manim for mathematical animations, 3D objects and abstract pattern generation, Matplotlib for statistical visualizations, OpenCV for computer vision processing, and MoviePy for video composition and editing. The implementation process involves multiple iterations of prototype development, parameter tuning, and edge case handling to ensure robust generation across the full complexity spectrum. Complexity Parameterization. Each generation script incorporates systematic complexity controls spanning multiple orthogonal dimensions. Entity complexity ranges from 2-3 simple objects to 15+ interconnected elements; reasoning depth spans 1-2 step direct inference to 5+ step multi-hop reasoning chains; distractor density varies from minimal noise to high-distraction environments with 8+ irrelevant elements; temporal complexity progresses from static sequential presentation to dynamic concurrent processes; and visual complexity includes occlusion patterns, perspective changes, and rendering fidelity variations. These parameters enable precise difficulty calibration and systematic scaling as model capabilities advance. Content Diversification. To ensure comprehensive coverage within each category, we generate multiple task variants with independently sampled parameters. This approach balances challenge variety with experimental reproducibility, creating diverse reasoning scenarios while maintaining consistent evaluation standards. The programmatic foundation supports rapid iteration and expansion, enabling the creation of new task variants on demand. 2.3.2. Validation and Quality Assurance To ensure the reliability and quality of MORSE-500, we implemented rigorous multi-stage validation pipeline encompassing both automated verification and human evaluation. Automated Technical Validation. All generated videos undergo comprehensive automated validation to ensure technical integrity and content completeness. Our validation pipeline systematically checks for: (1) technical quality metrics including resolution consistency (512p minimum) and proper codec encoding; (2) content completeness verification ensuring all necessary visual elements for question answering are present and clearly visible throughout the temporal sequence; and (3) ground-truth label accuracy through automated cross-referencing with generation parameters. This automated process identifies and flags potential issues including rendering artifacts, incomplete animations, and parameter-label mismatches, which are subsequently addressed through iterative script refinement. Expert Human Evaluation. To validate the benchmarks cognitive validity and eliminate ambiguous cases, we conducted systematic human evaluation by paper authors on randomly sampled data slices. Each evaluator independently assessed video-question pairs for: (1) question clarity and unambiguity, ensuring that questions have single, well-defined correct answers derivable from the visual content; (2) visual-semantic alignment, verifying that all information necessary to answer questions is present and interpretable in the video sequence; and (3) difficulty appropriateness, confirming that tasks are challenging yet solvable for competent human reasoners. Iterative Refinement. Based on validation feedback, we implemented systematic improvements to our generation pipeline. Common issues identified during validationsuch as temporal misalignment between question presentation and relevant visual events, insufficient visual contrast for critical elements, and 10 MORSE: Multimodal Reasoning Stress-test Environment edge cases in procedural generationwere addressed through targeted script modifications and parameter adjustments. This iterative process ensures that MORSE-500 provides reliable, unambiguous, and cognitively valid assessment of multimodal reasoning capabilities across dynamic visual contexts. The complete pipeline from task identification to validated content typically requires 2-3 iterations per reasoning category, with the programmatic approach enabling rapid refinement and consistent quality across the entire dataset. 3. Experiments 3.1. Settings Models and Baselines. To evaluate the current frontier of multimodal reasoning, we benchmarked diverse set of visionlanguage models on MORSE-500. Our selection spans both proprietary and open-source models with varying architectural backbones and pretraining objectives. include Gemini 2.5 Pro [Google, 2025], Google DeepMinds strongest model with Closed-source models advanced video and visual reasoning capabilities, and other models from Gemini family, including Gemini 2.5 Flash, Gemini 2.0 Flash, Gemini 2.0 Flash-Lite, and Gemini 1.5 Pro[Google, 2024]. And OpenAI o3 [OpenAI, 2025], OpenAIs strongest LMM with improved temporal and spatial understanding. We also include GPT 4o [Hurst et al., 2024], o1 [Jaech et al., 2024], and o4-mini[OpenAI, 2025]. encompass wide spectrum of scale and design. We evaluate multiple model variants Open-source models of one of the strongest open source model family - Qwen2.5 VL [Bai et al., 2023] of different sized (3B, 7B, 32B, 72B) with and without quantization (AWQ), these model are all with video support. Other video supporting models include Qwen2.5-Omni-7B [Xu et al., 2025], which is able to perform audio understanding and generation tasks beyond vision and language. We also include LLaVA-NeXT-Video-7B [liu, 2024], video-capable variant of LLaVA; and MiniCPM-o-2_6 [Yao et al., 2024], lightweight vision-language model designed for efficiency. For VLMs that only has image support, we include InternVL3 8B [Zhu et al., 2025], strong multilingual VL baseline; and Gemma 3 [Gemma Team et al., 2025]. These open models vary widely in capability, allowing us to assess how scale, quantization, and architecture impact multimodal reasoning under the same benchmark. Evaluation Protocol. All models were given the original video clips from MORSE-500. For models with native video understanding, we used the entire clip. For image-only models, frames were sampled at 0.5second intervals (2fps) and with max 32-frame context. We also provide ablation studies on the influence of the fps and max number of frames in section 3.4. All models were prompted with the minimal instruction: \"Answer the question in this video.\" No few-shot examples or format-specific guidance were provided to isolate the models intrinsic reasoning abilities. Note for Gemini 2.5 series models (2.5 Pro and 2.5 Flash) and OpenAI models (o4-mini, o3, o1, 4o), the api does not yet support video input, so we provide image frames. Videos were downsampled to maximum side length of 512 pixels (preserving aspect ratio). Metrics. We report accuracy as the primary evaluation metricthe percentage of correctly answered questions over the benchmark. We provided detailed instructions on the output formatting in the video, and 11 MORSE: Multimodal Reasoning Stress-test Environment Table 2: Accuracy (%) on MORSE-500 across all six reasoning categories and overall average. Closed-source models are marked in purple, and open-source models are marked in blue."
        },
        {
            "title": "All",
            "content": "Abstract Math."
        },
        {
            "title": "Spatial Temporal",
            "content": "Human o3 o4-mini Gemini 2.5 Pro o1 Gemini 2.5 Flash Gemini 1.5 Pro Qwen2.5 VL 72B GPT 4o Qwen2.5 VL 32B AWQ Qwen2.5 VL 72B AWQ Gemini 2.0 Flash Qwen2.5 VL 32B Gemma 3 27b Gemini 2.0 Flash-Lite MiniCPM-o 2.6 Qwen2.5 Omni 7B Qwen2.5 VL 7B InternVL3 8B Qwen2.5 VL 3B LLaVA-NeXT-Video 7B 55.4 23.6 22.2 21.8 19.8 19.2 18.8 17.8 17.4 16.8 16.4 16.0 15.6 14.6 14.2 11.6 11.4 11.2 7.8 7.6 5.0 37.5 23.4 21.9 18.8 17.2 9.4 12.5 6.2 17.2 14.1 12.5 12.5 9.4 20.3 17.2 4.7 6.2 7.8 6.2 9.4 1.6 45.5 27.4 23.8 36.9 22.6 35.7 21.4 21.4 20.2 23.8 11.9 29.8 19.0 20.2 21.4 10.7 9.5 11.9 6.0 3.6 11.9 56.3 28.1 29.7 29.7 28.1 28.1 26.6 34.4 34.4 34.4 29.7 28.1 29.7 25.0 21.9 23.4 21.9 25.0 14.1 18.8 6.2 56.0 5.0 5.0 3.0 5.0 1.0 1.0 1.0 4.0 1.0 2.0 0.0 2.0 1.0 2.0 1.0 2.0 2.0 1.0 1.0 0. 73.1 29.6 27.8 16.7 23.1 24.1 26.9 22.2 12.0 15.7 27.8 13.0 16.7 13.0 14.8 16.7 15.7 12.0 11.1 9.3 5.6 55.2 31.2 28.7 32.5 26.2 18.8 26.2 25.0 25.0 18.8 16.2 18.8 21.2 15.0 12.5 15.0 15.0 12.5 10.0 7.5 5.0 we extract the answers from the model prediction using LLM (e.g. Qwen2.5 72B AWQ) and perform string matching for accuracy calculation, following MathVista [Lu et al., 2024]. Reproducibility. To ensure transparency and future benchmarking, the full evaluation code, the intermediate evaluation results of different models, and video generation scripts are released on github. The benchmark data is available on huggingface, including all 500 videos, ground-truth answers, and usage instructions. 3.2. Quantitative Results Table 2 presents accuracy across reasoning categories for all evaluated models. Overall performance remains substantially below human-level capabilities, with even the strongest systems averaging below 25% accuracya significant gap compared to human performance at 55.4%. Proprietary Model Performance. Among proprietary models, OpenAIs o3 achieves the highest overall score of 23.6%, demonstrating relatively balanced performance across categories with particular strength in temporal reasoning (31.2%). Gemini 2.5 Pro follows closely at 21.8%, exhibiting notable proficiency in mathematical reasoning (36.9%) and temporal understanding (32.5%), but struggling significantly with abstract reasoning (18.8%) and planning tasks (3.0%). Interestingly, while Gemini 2.5 Flash achieves similar mathematical performance (35.7%), it shows markedly weaker performance in abstract reasoning (9.4%) 12 MORSE: Multimodal Reasoning Stress-test Environment and planning (1.0%), suggesting that model scale and optimization strategies significantly impact reasoning capabilities across different cognitive domains. The performance patterns reveal interesting trade-offs: models optimized for mathematical reasoning tend to excel in structured, rule-based tasks but struggle with open-ended abstract reasoning. Conversely, models with stronger general reasoning capabilities (like o3) show more balanced performance but may sacrifice peak performance in specific domains. Open-Source Model Analysis. The open-source landscape demonstrates clear scaling relationship between model size and reasoning performance. Among the Qwen2.5 VL family, the 72B model achieves 17.8% overall accuracy, substantially outperforming smaller variants (32B: 16.8%, 7B: 11.2%, 3B: 7.6%). However, quantization effects are mixed: while the 72B AWQ model shows slightly lower overall performance (16.4%) compared to its full-precision counterpart, the 32B AWQ variant actually outperforms the standard 32B model (16.8% vs. 15.6%), suggesting that quantization impacts vary with model scale. Specialized models show domain-specific strengths: MiniCPM-o 2.6, despite lower overall performance (11.6%), demonstrates competitive physical reasoning capabilities (23.4%), indicating that targeted optimization can yield focused improvements. However, highly specialized models like LLaVA-NeXT-Video 7B, despite being designed for video understanding, achieve only 5.0% overall accuracy, highlighting the gap between video comprehension and video-based reasoning. Category-Specific Performance Patterns. Performance varies dramatically across reasoning categories, revealing systematic weaknesses in current models: Mathematical Reasoning: This category shows the highest performance across most models, with several systems exceeding 20% accuracy. The structured nature of mathematical problems and their prevalence in training data likely contributes to this relative strength. Physical Reasoning: Models demonstrate moderate competency (20-35% for top performers), suggesting that intuitive physics concepts are partially captured in current training paradigms. However, the gap from human performance (56.3%) remains substantial. Spatial and Temporal Reasoning: Performance in these categories is moderate but inconsistent across models, with some showing surprising deficits (e.g., Gemini 2.5 Pros 16.7% spatial accuracy despite strong mathematical performance). Abstract Reasoning: This category proves most challenging for all models, with even the best performers struggling to exceed 25% accuracy. The poor performance suggests fundamental limitations in pattern recognition, analogical thinking, and rule inductioncore components of general intelligence. Planning Reasoning: Perhaps most concerning, planning tasks show near-random performance across all models (0-5% accuracy), indicating critical gap in multi-step reasoning and goal-directed behavior. This weakness has significant implications for real-world deployment in autonomous systems. Implications and Model Limitations. The uniformly low performance across all reasoning categories, particularly in abstract reasoning and planning, suggests that current multimodal models suffer from fundamental architectural limitations rather than mere training inefficiencies. The inability to perform multi-step reasoning, maintain temporal coherence, and engage in abstract pattern matching indicates that these models may be primarily engaging in sophisticated pattern matching rather than genuine reasoning. 13 MORSE: Multimodal Reasoning Stress-test Environment Furthermore, the substantial human-model performance gaps (30+ percentage points in most categories) underscore that achieving human-level multimodal reasoning remains significant challenge. The particularly poor performance on planning tasks raises questions about the suitability of current models for autonomous decision-making applications. 3.3. Qualitative Results Table 6 illustrates representative model outputs on challenging examples from MORSE-500, and more examples can be found in Appendix C. We observe consistent failure patterns in tasks requiring abstract reasoning and multi-step planning, where even large-scale models falter at integrating temporal cues or executing compositional logic. Smaller models often rely on superficial visual features, anchoring on salient but irrelevant distractors. In contrast, more capable modelssuch as Gemini 2.5 Pro and o3attempt partial reasoning chains but frequently fall short of arriving at the correct solution. These qualitative trends reinforce the quantitative gaps reported earlier and highlight the need for better memory, inference, and grounding in dynamic visual contexts. Mathematical Reasoning - 3D Extrema Plot Figure 6: Example of mathematical reasoning failure across state-of-the-art models. All three models hallucinate the maximum at the origin, whereas the ground truth is (0, 4). Query: Answer the question in this video. Question text: At what point (u, v) does the surface achieve its local maximum? Ground truth: (0,4) Response from Qwen3-235B-A22B To determine the point (u, v) at which the surface achieves its local maximum, lets analyze the given 3D surface plot step by step. The surface is 3D plot with coordinates (u, v, z), where represents the height of the surface. The goal is to find the point (u, v) where the surface reaches its local maximum. 14 MORSE: Multimodal Reasoning Stress-test Environment The surface appears to have peak or highest point. The peak is visually identifiable as the point where the surface rises the most and then begins to descend in all directions. the surface has distinct peak. The peak is located near the origin of the From the plot: uv plane, but it is not exactly at the origin. The surface slopes downward in all directions from this peak. The peak appears to be at point where both and are close to zero but not exactly zero. Based on the symmetry and the shape of the surface, the peak is likely at point where and are small positive or negative values. The exact coordinates of the peak cannot be determined precisely without numerical data, but visually, the peak is very close to the origin. Therefore, the point (u, v) where the surface achieves its local maximum is approximately: (0, 0) Response from o3 (summarized) The model begins by sampling video frames at regular intervals (e.g., 1 fps or 0.5 fps) using OpenCV: import cv2, numpy as np, matplotlib.pyplot as plt video_path = '/mnt/data/LocalExtreme3DQuestion1.mp4' cap = cv2.VideoCapture(video_path) fps = cap.get(cv2.CAP_PROP_FPS) total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) duration = total / fps sample_rate = 30 frames = [] for in range(0, total, sample_rate): # 2 fps cap.set(cv2.CAP_PROP_POS_FRAMES, i) ret, frame = cap.read() if ret: frames.append((i, frame)) cap.release() print(len(frames)) # 15 frames It then inspects the sampled frames to locate the overlay text asking, At what point (u, v) does the surface achieve its local maximum?, and attempts to zoom in for clarity. Finally, it visually estimates the peak coordinates. **Answer:** (0,0) Response from Gemini-2.5-Pro-Preview (summarized) The video displays 3D surface, which appears to be an elliptic paraboloid opening downwards. The question asks for the point (u, v) where the surface achieves its local maximum. 15 MORSE: Multimodal Reasoning Stress-test Environment The axes are labeled: One horizontal axis is labeled u. The other horizontal axis is labeled v. The vertical axis is labeled f(u,v) (implied by the function notation at the top, though only (v,u)f is visible). local maximum is point on the surface that is higher than all its immediate neighbors. For smooth, downward-opening paraboloid like the one shown, the local maximum is unique and occurs at its vertex. By visually inspecting the graph, the vertex (the highest point) of the paraboloid is located directly above the origin of the uv plane. The origin is where the u-axis and v-axis intersect, which corresponds to = 0 and = 0. Therefore, the surface achieves its local maximum at the point (0, 0). 3.4. Analysis Performance on Challenging Tasks To illustrate the complexity of the tasks evaluated, two-by-two grid of example images from each categoryMazes, Rope Knots, Physical Commonsense, and ARC-AGI-2is presented in 7. These tasks are designed to push the limits of current models, particularly those relying on image-based inputs. For Mazes, we assess the models ability to determine the optimal path from start to finish while navigating obstacles, count the number of viable paths, identify holes, and calculate the connected area of holes. The task is intensified by restricting the visible area around the agent, making it dependent on temporal information and increasing difficulty. In Rope Knots, we randomize the sequence of tying steps and challenge the model to predict the remaining steps, deduce the order after partial completion, and reconstruct the full sequence, with added image transformations to heighten complexity. For Physical Commonsense, we generate videos from initial images and scene descriptions using state-of-the-art image-to-video models, blending these with real camera-captured footage. The model must then distinguish the physically realistic video, testing its sensitivity to subtle physical inconsistencies. Finally, for ARC-AGI-2, we adapt the original tasks for vision-language models, presenting before-and-after patterns and either asking the model to select the correct completion image (if any) or count specific colored cells in the solution, posing significant challenge due to abstract reasoning demands. Table 3 reveals the performance of different models across these tasks. Humans achieve the highest scores, with 58.3 in Mazes, 53.0 in Rope Knots, 63.6 in Physical Commonsense, and 47.1 in ARC-AGI-2, reflecting their robust reasoning and contextual understanding. In contrast, existing frontier models lag considerably: o3 scores 10.0, 3.8, 13.6, and 14.0 across the respective tasks, while Gemini 2.5 Pro records near-zero performance (0.0 to 4.2). This stark gap underscores that even the most advanced models struggle to match human-level proficiency, particularly in tasks requiring temporal reasoning, physical intuition, and abstract pattern recognition, highlighting critical area for future model development."
        },
        {
            "title": "Model",
            "content": "Mazes Rope Knots Physical Commonsense ARC-AGI-2 Human o3 Gemini 2.5 Pro 58.3 10.0 0.0 53.8 3.8 1.3 63.6 13.6 4.2 47.1 14.0 0. Table 3: Performance of top frontier models across 4 challenging tasks. 16 MORSE: Multimodal Reasoning Stress-test Environment (a) Planning Reasoning - Maze with Different Endpoints (b) Physical Commonsense - Tennis Ball Rolling (c) Planning Reasoning - Knots of Tying Bow Tie (d) Abstract Reasoning - ARC-AGI-2 Figure 7: Example videos for challenging tasks: Mazes, Rope Knots, Physical Commonsense, and ARC-AGI-2. Impact of FPS and Max Frames on Video-to-Image Frame Sampling for Model Performance To evaluate image-based models on video inputs, we analyzed how different sampling strategies, specifically varying frames per second (FPS) and maximum number of frames, affect model performance. We tested configurations with FPS {1, 2, 4, 8} and max frames {16, 32, 64, 128} for Gemini 2.5 Flash. As shown in Table 4, the configuration with FPS 2 and 32 max frames achieves the highest overall performance, with an \"All\" score of 19.2. Increasing FPS and max frames beyond this point, such as to FPS 4 with 64 frames or FPS 8 with 128 frames, does not consistently improve performance and can even lead to slight declines (e.g., \"All\" score drops to 19.0 at FPS 8, max 128). This suggests saturation point where additional frames may introduce noise or redundant information, particularly for tasks like Planning and Spatial reasoning, where scores remain low or inconsistent (e.g., Planning scores range from 1.0 to 4.0). For models with smaller context windows, the FPS 1, max 16 frames configuration yields marginally lower \"All\" score of 18.4 but outperforms in Physical tasks (34.4), indicating robustness for resource-constrained settings. Based on these findings, we adopt FPS 2 and max 32 frames as the default configuration, as highlighted in Table 4, balancing performance and computational efficiency."
        },
        {
            "title": "Abstract Math Physical Planning Spatial Temporal",
            "content": "1 2 4 8 16 32 64 128 18.4 19.2 19.2 19.0 17.2 9.4 17.2 15.6 23.8 35.7 29.8 29.8 34.4 28.1 26.6 28. 4.0 1.0 3.0 1.0 19.4 24.1 21.3 21.3 17.5 18.8 21.2 22.5 Table 4: Performance of models with different FPS and max frames settings when converting videos to frames for image-based input. Analyses based on Gemini 2.5 Flash. Static vs. Temporal Input. To investigate how temporal structure affects multimodal reasoning, we conduct an ablation study on the MathVista dataset using the Qwen-2.5 VL model series under three input settings: (1) single image paired with textual question (the original setting), (2) sequence of static images simulating temporal progression (with textual questions encoded in images), and (3) full video input (as mp3 file format). We construct multiple images from the original image and question pair by putting the MORSE: Multimodal Reasoning Stress-test Environment question text on images that are the same size as the original. Then, each of these is either passed into the model separately (2) or stitched together into video frames with 1 fps (3). Results are summarized in Table 5. These results reveal consistent decline in performance as input complexity increases. The models performs best with static image-text pairs and degrades when required to reason over temporal sequences. This highlights critical limitation of current VLMs: while optimized for joint spatial encoding, they remain brittle when handling distributed temporal information. Table 5: Ablation study on the MathVista dataset showing the effect of different input modalities on Qwen-2.5 VL 7B, 32B AWQ, and 72B AWQ. Static inputs perform better than temporally distributed ones."
        },
        {
            "title": "Input Modality",
            "content": "Image + Question Text Multi-Image Context Video Input Qwen-2.5 VL 7B Qwen-2.5 VL 32B Qwen-2.5 VL 72B 62.4 57.5 57.3 72.1 64.2 60.9 69.0 65.1 62.8 4. Related Work Reasoning Benchmarks. Most existing multimodal reasoning benchmarks operate on static images or short textimage pairs. Early static VQA datasets (e.g., TextVQA [Singh et al., 2019], DocVQA [Mathew et al., 2021], OCRVQA [Mishra et al., 2019]) evaluate text extraction but largely test surface-level perception. More recent efforts broaden the scope to mathematical (MathVista [Lu et al., 2024], Math-Vision [Wang et al., 2024a], DynaMath [Zou et al., 2024]), commonsense (VisualCOMET [Park et al., 2020]), and multi-disciplinary exam reasoning (MMMU [Yue et al., 2023], ScienceQA [Lu et al., 2022]). Nonetheless these datasets share three structural limitations: (i) they are image-centric and ignore temporal dynamics; (ii) they concentrate on either knowledge retrieval or single reasoning faculty (e.g., arithmetic) leading to skewed skill coverage; and (iii) their manually curated question banks saturate quickly, offering little head-room for diagnosing new models. Beyond supervised corpora, synthetic abstract-reasoning suites such as ARC-AGI-2 [Chollet et al., 2025] and CLEVRER [Johnson et al., 2017] test compositional and causal inference, yet they remain confined to toy worlds and still images. In sum, unified benchmark that spans diverse reasoning types and evaluates robustness under temporally unfolding evidence is still absent. MORSE-500 fills this gap by providing programmatically generated video tasks across six reasoning categories with scalable difficulty. Video Understanding Benchmarks. Video-centric datasets have traditionally emphasized descriptive understandinge.g., action recognition or dense captioningrather than explicit reasoning. Recent longcontext suites such as HourVideo [Chandrasegaran et al., 2024] and LongVideoBench [Wu et al., 2024] introduce hour-long or movie-length clips but probe summarization and retrieval rather than isolating causal or mathematical inference. EMMA [Hao et al., 2025] adds multi-choice questions over short sequences but still relies on crowd-sourced videos with fixed difficulty. Large-scale quantitative benchmarks like VideoMME [Fu et al., 2024] and Video-MMMU [Hu et al., 2025] evaluate factual knowledge in videos; however many tasks can be answered correctly by sampling single salient frame, blurring the line between perception and reasoning. Similarly, MVBench [Li et al., 2024] focuses on content understanding with limited task diversity. Mementos [Wang et al., 2024b] moves towards temporal reasoning but lacks programmatic control to escalate complexity as systems improve. By contrast, MORSE-500 synthesizes video clips via deterministic scripts, allowing precise modulation of 18 MORSE: Multimodal Reasoning Stress-test Environment motion speed, distractor density, and reasoning depth. This design avoids the single-frame shortcut and provides living benchmarknew, harder instances can be generated on demand to keep pace with rapidly advancing VLMs. 5. Conclusion and Discussion We introduce MORSE-500, programmatically generated, video-centric benchmark that systematically evaluates six core forms of multimodal reasoning: mathematical, abstract, spatial, temporal, physical, and planning. Unlike existing benchmarks, which are often static, narrow in scope, or quickly saturate, MORSE500 offers fine-grained control over difficulty, scene complexity, and distractor patterns, enabling stress tests that remain relevant as models improve. By embedding questions directly into temporally unfolding video narratives, our benchmark isolates reasoning performance from perception or retrieval shortcuts, providing more faithful diagnostic lens for genuine multimodal intelligence. Our evaluation reveals critical gap in current AI capabilities: even state-of-the-art models lag dramatically behind human performance across all reasoning categories, with particularly concerning deficits in planning tasks (0-5% accuracy). These systematic failures indicate fundamental architectural limitations rather than scaling issues, suggesting that achieving human-level multimodal intelligence requires advances in temporal memory systems, compositional reasoning mechanisms, and hierarchical planning capabilities. MORSE-500s programmatic foundation positions it as both diagnostic tool and research catalystits scalable difficulty generation prevents benchmark saturation while enabling targeted study of specific reasoning failures. The benchmarks extensibility to additional modalities and its grounding in cognitive science frameworks make it well-suited for driving the development of next-generation AI systems capable of flexible, robust reasoning in dynamic environments. By releasing the complete framework, we aim to accelerate progress toward AI systems that genuinely understand and reason about our complex world rather than merely matching patterns in static data. 6. Limitations and Broader Impact Limitations. While MORSE-500 offers richly structured and extensible framework for video-based reasoning evaluation, several limitations warrant acknowledgment. First, all videos are generated programmatically or curated from constrained domains, which may limit the visual diversity and realism found in real-world footage. Although we incorporate generative models and curated clips to enhance visual fidelity, the scenarios may not fully reflect the complexity of natural human environments with their inherent unpredictability and contextual richness. Second, our benchmark currently focuses exclusively on visual reasoning without incorporating audio modalities. While questions are embedded within video content, the absence of speech, ambient sounds, or audio-visual synchronization tasks represents significant gap, as real-world reasoning often requires multimodal integration across sensory channels. Future extensions could incorporate audio cues, spoken questions, or audio-visual reasoning tasks to provide more comprehensive evaluation. Third, while our benchmark focuses on isolating reasoning from perception, complete separation remains challenging. Tasks involving complex visual scenes, occlusion, or visual clutter may conflate reasoning failures with perceptual limitations, making it difficult to attribute poor performance to specific cognitive deficits. Additionally, the programmatic generation approach, while ensuring reproducibility, may inadvertently create artificial regularities that differ from the statistical patterns models encounter in natural training data. MORSE: Multimodal Reasoning Stress-test Environment Broader Impact. MORSE-500 is designed to rigorously evaluate the reasoning capabilities of large multimodal models (LMMs), particularly in dynamic, temporally grounded contexts. By providing scalable and reproducible stress-test, MORSE-500 may help identify reasoning deficits in current LMMs, thereby informing safer and more robust deployment in high-stakes domains such as robotics, education, and healthcare. At the same time, misuse of powerful video reasoning modelssuch as for surveillance, disinformation, or behavioral manipulationremains broader societal risk. By releasing MORSE-500 with full transparency, reproducible code, and benchmark harnesses, we aim to promote responsible development and auditing of multimodal AI systems."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, DARPA HR001124S0029-AIQ-FP-019, National Science Foundation NSF-IIS-2147276 FAI, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, National Science Foundation NAIRR240045, ONR MURI program, the National Science Foundation (IIS-2212182), and the NSF TRAILS Institute (2229885). Private support was provided by Capital One Bank, the Amazon Research Award program, Open Philanthropy, and Peraton. We thank Kaiyu Yue, Yuxin Wen, Vasu Singla, Yuhanng Zhou and other members from FurongLab and TomLab for the helpful discussions."
        },
        {
            "title": "References",
            "content": "Llava-next: Open large multimodal models. 2024. URL https://github.com/LLaVA-VL/LLaVA-NeXT. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. URL https://arxiv.org/abs/2204.14198. Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from human videos as versatile representation for robotics, 2023. URL https://arxiv.org/abs/2304.08488. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. URL https://arxiv.org/abs/2308.12966. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. URL https://arxiv.org/abs/1606. 01540. John Bissell Carroll. Human cognitive abilities: survey of ber 1. Cambridge university press, 1993. human-cognitive-abilities/F83D5EADF14A453F6350FF3DD39631C8. NumURL https://www.cambridge.org/core/books/ factor-analytic studies. Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2024. URL https://arxiv. org/abs/2411.04998. 20 MORSE: Multimodal Reasoning Stress-test Environment François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. URL https: //arxiv.org/abs/1911.01547. Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831, 2025. URL https:// arxiv.org/abs/2505.11831. Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. URL https://arxiv.org/abs/2501.16411. Stanislas Dehaene. The number sense: How the mind creates mathematics. OUP USA, 2011. Jonathan St B. T. Evans and Keith E. Stanovich. Dual-process theories of higher cognition: Advancing the debate. Perspectives on psychological science, 8(3):223241, 2013. doi: 10.1177/1745691612460685. URL https://doi.org/10.1177/1745691612460685. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. URL https://arxiv.org/ abs/2405.21075. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. URL https://arxiv.org/abs/2503.19786. Dedre Gentner. Structure-mapping: theoretical framework for analogy. Cognitive Science, 7(2):155170, 1983. doi: 10.1207/s15516709cog0702_3. URL https://doi.org/10.1207/s15516709cog0702_3. Anastasis Germanidis. Introducing gen-3 alpha: new frontier for video generation. https://runwayml. com/research/introducing-gen-3-alpha, 2024. Runway Research blog, Accessed 22 May 2025. Google. Our next-generation model: Gemini 1.5. https://blog.google/technology/ai/ google-gemini-next-generation-model-february-2024/, 2024. Accessed: 2025-05-16. Google. Gemini 2.5: Our most intelligent ai model. https://blog.google/technology/ google-deepmind/gemini-model-thinking-updates-march-2025/, 2025. Accessed: 2025-0516. Google DeepMind. State-of-the-art video and image generation with veo 2 and imagen 3. https://blog. google/technology/google-labs/video-image-generation-update-december-2024/, 2024. Accessed 22 May 2025. Grog. Animated knots. https://www.animatedknots.com, 2025. Accessed: 2025-05-16. Hailuo AI. Transform idea to visual with ai. https://hailuoai.video/, 2025. Accessed 22 May 2025. Siddhant Haldar, Vaibhav Mathur, Denis Yarats, and Lerrel Pinto. Watch and match: Supercharging imitation with regularized optimal transport, 2023. URL https://arxiv.org/abs/2206.15469. 21 MORSE: Multimodal Reasoning Stress-test Environment Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. In Proceedings of the 42nd International Conference on Machine Learning (ICML 2025), 2025. URL https://arxiv. org/abs/2501.05444. Keith J. Holyoak and Robert G. Morrison, editors. The Oxford handbook of thinking and reasoning. Oxford University Press, 2013. ISBN 9780199730013. URL https://academic.oup.com/edited-volume/ 34559. Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Videommmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. URL https://arxiv.org/abs/2501.13826. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. URL https://arxiv.org/abs/2410.21276. Michael Igorevich Ivanitskiy, Rusheb Shah, Alex F. Spies, Tilman Räuker, Dan Valentine, Can Rager, Lucia Quirke, Chris Mathwin, Guillaume Corlouer, Cecilia Diniz Behn, and Samy Wu Fung. configurable library for generating and manipulating maze datasets, 2023. URL https://arxiv.org/abs/2309.10498. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. URL https://arxiv.org/abs/2412.16720. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2901 2910, 2017. doi: 10.1109/CVPR.2017.311. URL https://openaccess.thecvf.com/content_ cvpr_2017/html/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.html. Kling AI. Kling ai: Next-generation ai creative studio. https://klingai.com/, 2025. Accessed 22 May 2025. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training In Proceedings of the 40th International with frozen image encoders and large language models. Conference on Machine Learning (ICML 2023), volume 202 of Proceedings of Machine Learning Research, pages 1910919133. PMLR, jul 2023. URL https://arxiv.org/abs/2301.12597. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition mark. URL https: (CVPR), pages 2219522206, 2024. //openaccess.thecvf.com/content/CVPR2024/html/Li_MVBench_A_Comprehensive_ Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.html. 10.1109/CVPR52733.2024.02049. doi: Pan Lu, Michel Galley, Hao Cheng, Kai-Wei Chang, and Jianfeng Gao. Learn from science textbooks: Retrieval-augmented science question answering. arXiv preprint arXiv:2207.05275, 2022. URL https: //arxiv.org/abs/2207.05275. MORSE: Multimodal Reasoning Stress-test Environment Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR 2024), 2024. URL https://arxiv.org/abs/2310.02255. Manim Community. Manim mathematical animation framework, 2025. URL https://zenodo.org/ records/14699705. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. for vqa Docvqa: the IEEE/CVF Winter Conference on Applications on document of Computer Vision (WACV), pages 22002209, 2021. 10.1109/WACV48630.2021. 00224. URL https://openaccess.thecvf.com/content/WACV2021/html/Mathew_DocVQA_A_ Dataset_for_VQA_on_Document_Images_WACV_2021_paper.html."
        },
        {
            "title": "A dataset",
            "content": "images. doi: Michael McCloskey, Allyson Washburn, and Linda Felch. Intuitive physics: the straight-down belief and its origin. Journal of Experimental Psychology: Learning, Memory, and Cognition, 9(4):636649, 1983. doi: 10.1037/0278-7393.9.4.636. URL https://doi.org/10.1037/0278-7393.9.4.636. Ankush Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question In 2019 International Conference on Document Analysis and answering by reading text in images. Recognition (ICDAR), pages 947952. IEEE, 2019. Akira Miyake, Naomi P. Friedman, Michael J. Emerson, Alexander H. Witzki, Amy Howerter, and Tor D. Wager. The unity and diversity of executive functions and their contributions to complex frontal lobe tasks: latent variable analysis. Cognitive psychology, 41(1):49100, 2000. doi: 10.1006/cogp.1999.0734. URL https://doi.org/10.1006/cogp.1999.0734. Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models understand physical principles? arXiv preprint arXiv:2501.09038, 2025. URL https://arxiv.org/ abs/2501.09038. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/abs/ 2303.08774. OpenAI. Sora is here. https://openai.com/index/sora-is-here/, 2024. Accessed 22 May 2025. OpenAI. Introducing o3 and o4-mini. https://openai.com/index/introducing-o3-and-o4-mini/, 2025. Accessed: 2025-05-16. Jae Sung Park, Dong Huk Kim, Rowan Zellers, Yonatan Bisk, Ani Kembhavi, and Yejin Choi. Visualcomet: In European Conference on Computer Vision Reasoning about the dynamic context of still image. (ECCV), pages 508524. Springer, 2020. URL https://arxiv.org/abs/2004.10796. Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, et al. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074, 2025. URL https://arxiv.org/abs/2504.16074. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. URL https://arxiv.org/abs/2303.17580. 23 MORSE: Multimodal Reasoning Stress-test Environment Roger N. Shepard and Jacqueline Metzler. Mental rotation of three-dimensional objects. Science, 171(3972): 701703, 1971. doi: 10.1126/science.171.3972.701. URL https://doi.org/10.1126/science.171. 3972.701. Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: multi-task transformer for robotic manipulation. In Conference on Robot Learning (CoRL 2022), 2023. URL https://proceedings.mlr. press/v205/shridhar23a.html. arXiv preprint year 2022. Amanpreet Singh, Vivek Natarajan, Yu Jiang, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Textvqa: Towards image-text reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 83178326, 2019. doi: 10.1109/CVPR.2019. 00852. URL https://openaccess.thecvf.com/content_CVPR_2019/html/Singh_Towards_ VQA_Models_That_Can_Read_CVPR_2019_paper.html. Consolidated entry for TextVQA CVPR paper. Wan Team, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. URL https://arxiv.org/abs/2503.20314. Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play, 2023. URL https://arxiv.org/ abs/2302.12422. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024a. URL https://arxiv.org/abs/2402.14804. Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024b. URL https://arxiv.org/abs/2401.10529. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. URL https://arxiv.org/abs/2407.15754. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: personalized robot assistance with large language models. Autonomous Robots, 47(8):10871102, nov 2023. ISSN 1573-7527. doi: 10.1007/s10514-023-10139-z. URL https://doi.org/10.1007/s10514-023-10139-z. Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, Shichao Fan, Xinhua Wang, Fei Liao, Zhen Zhao, Guangyu Li, Zhao Jin, Lecheng Wang, Jilei Mao, Ning Liu, Pei Ren, Qiang Zhang, Yaoxu Lyu, Mengzhen Liu, Jingyang He, Yulin Luo, Zeyu Gao, Chenxuan Li, Chenyang Gu, Yankai Fu, Di Wu, Xingyu Wang, Sixiang Chen, Zhenyu Wang, Pengju An, Siyuan Qian, Shanghang Zhang, and Jian Tang. Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation, 2025. URL https://arxiv.org/abs/2412.13877. Kun Xiang, Heng Li, Terry Jingchen Zhang, Yinya Huang, Zirong Liu, Peixin Qu, Jixi He, Jiaqi Chen, Yu-Jie Yuan, Jianhua Han, et al. Seephys: Does seeing help thinking?benchmarking vision-based physics reasoning. arXiv preprint arXiv:2505.19099, 2025. URL https://arxiv.org/abs/2505.19099. MORSE: Multimodal Reasoning Stress-test Environment Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. URL https://arxiv.org/abs/2503.20215. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. URL https://arxiv.org/abs/2408.01800. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. URL https://arxiv.org/ abs/2311.16502. Jeffrey M. Zacks, Nicole K. Speer, Khena M. Swallow, Todd S. Braver, and Jeremy R. Reynolds. Event segmentation in perception and memory. Trends in Cognitive Sciences, 11(2):8086, 2007. doi: 10.1016/ j.tics.2006.12.001. URL https://doi.org/10.1016/j.tics.2006.12.001. Haotian Zhang, Jingyang Li, Zhiwei Li, Zhang Kanchor, Zhengkai Liu, Chen-Yu Lee, Chunyuan Su, Chunyuan Li, Percy Liang, and Ahmed H. Awadallah. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. URL https://arxiv.org/abs/2302.00923. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Weiye Xu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. URL https://arxiv.org/abs/2504.10479. Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. Viola: Imitation learning for vision-based manipulation with object proposal priors, 2023. URL https://arxiv.org/abs/2210.11339. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. URL https://arxiv.org/abs/2411.00836. 25 MORSE: Multimodal Reasoning Stress-test Environment"
        },
        {
            "title": "Contents",
            "content": "A Benchmark Overview & Multi-modal Reasoning Taxonomy"
        },
        {
            "title": "B Example Question and Code",
            "content": "27 29 B.1 Abstract Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B.2 Mathematical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 B.3 Physical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 B.4 Planning Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Spatial Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 B.6 Temporal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C More Qualitative Results",
            "content": "59 C.1 Abstract Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 C.2 Mathematical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 C.3 Physical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 C.4 Planning Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 C.5 Spatial Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.6 Temporal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 26 MORSE: Multimodal Reasoning Stress-test Environment A. Benchmark Overview & Multi-modal Reasoning Taxonomy We construct MORSE-500 around six major categories of multi-modal reasoning, each designed with specific task instantiations and complexity controls, as detailed in Table A.1. Our approach combines established benchmarks with novel adaptations to create challenging multi-modal scenarios that require both visual understanding and temporal reasoning. Task Design Methodology. For each reasoning category, we employ systematic approach to task creation that balances coverage and difficulty. We adapt existing datasets where appropriate (e.g., maze navigation [Ivanitskiy et al., 2023, Brockman et al., 2016], ARC-AGI [Chollet, 2019, Chollet et al., 2025]), introduce environmental challenges (e.g., fog effects, visual noise), and curate real-world scenarios (e.g., robotic manipulation sequences and rope knot tying procedures). Complexity is controlled through multiple dimensions specific to each category, including structural parameters (number of objects, plan length), environmental factors (visual noise, temporal irregularity), and cognitive demands (rule depth, interaction complexity). Novel Adaptations. Several categories feature innovative task designs. For planning reasoning, we introduce fog effects to standard maze and FrozenLake environments [Brockman et al., 2016] and curate action sequence recognition tasks from robotic manipulation videos [Zhu et al., 2023, Wu et al., 2025, Haldar et al., 2023, Wu et al., 2023, Wang et al., 2023, Bahl et al., 2023]. Physical reasoning incorporates novel real-vsgenerated video discrimination task that leverages the improving quality of video generation models. We use the videos in Physics IQ Benchmark Motamed et al. [2025] as real footage, we use image-to-video models, including Sora [OpenAI, 2024], Runnway Gen-3 [Germanidis, 2024], Kling 1.6 [Kling AI, 2025], Hailuo AI [Hailuo AI, 2025], Wan 2.1 [Wan Team et al., 2025], and Veo2 [Google DeepMind, 2024] to generate 5s videos conditioned on one video frame and textual description on the object and camera motion. Abstract reasoning repurposes ARC-AGI patterns into multiple-choice and free-form visual reasoning questions based on color patterns and spatial arrangements. 27 MORSE: Multimodal Reasoning Stress-test Environment"
        },
        {
            "title": "Reasoning Category Description",
            "content": "Abstract Reasoning (12.8%) Mathematical Reasoning (16.8%) Physical Reasoning (12.8%) Planning Reasoning (20.0%) Spatial Reasoning (21.6%) Temporal Reasoning (16.0%) It targets pattern recognition, logical inference, and symbolic reasoning through adapted benchmarks and novel task designs. For logical reasoning, we repurpose ARC-AGI 2 patterns into multiple-choice questions and free-form responses based on color patterns, spatial arrangements, and cell counts, requiring rule induction from minimal examples and pattern extrapolation under abstract transformations. For symbolic reasoning, we design novel tasks including anagram word transformations, symbolic equation solving, and visual-textual symbol mapping challenges. Complexity is controlled by number of visual elements (4-25 cells), rule depth (1-4 nested transformations), color diversity (2-10 colors), symbolic abstraction level, and cross-modal symbol correspondence requirements. It evaluates arithmetic operations, algebraic relations, and quantitative comparisons through visual and textual integration. Tasks include dynamic word problems with chart interpretation, visual equation solving with geometric constraints, proportional comparisons across multiple data modalities, and geometric reasoning requiring spatial-numerical synthesis. Complexity is controlled through variable count (2-8 unknowns), operation depth (1-4 nested operations), visual noise levels, and crossmodal information density. It tests understanding of object dynamics and causal interactions governed by physical laws through simulation and real-world discrimination. Tasks involve predicting structural collapse in block towers, estimating force effects on object motion, forecasting collision trajectories, and novel real-vs-generated video discrimination task where models must identify authentic physics from increasingly sophisticated generated alternatives. Complexity is determined by interaction complexity (1-8 interacting objects), diversity of physical principles (gravity, friction, momentum, elasticity), and realism of generated distractors. It emphasizes multi-step, goal-directed reasoning through adapted environments and real-world scenarios. Tasks include maze navigation with fog effects (adapted from standard maze datasets), FrozenLake traversal under partial observability, robotic manipulation sequence recognition from curated online videos, and rope-tying action ordering tasks. We test action sequencing, goal inference, and sequential decisionmaking with complexity defined by plan length (3-15 steps), environmental uncertainty (fog density, partial observability), constraint density, and branching factor (2-6 alternative paths). It tests understanding of object relationships, spatial transformations, and 3D reasoning across multiple viewpoints and reference frames. Tasks include mental rotation with occlusion handling, multi-view inference requiring perspective integration, spatial pathfinding through complex 3D environments, and relative positioning under dynamic transformations. Complexity is determined by number of objects (3-12), degree of transformation (0-360 rotations), scene dimensionality (2D/3D), and presence of visual distractors. It assesses sequence understanding and causal inference over time through multi-frame visual narratives and process documentation. Tasks include event reordering from shuffled image sequences, duration comparison across parallel processes, future-state prediction in dynamic scenes, and cause-effect relationship identification in temporal chains. Difficulty is influenced by temporal irregularity (non-uniform time intervals), number of concurrent events (1-5), sequence length (4-20 frames), and presence of temporal red herrings. Table A.1: Definitions, task instantiations, and proportions of six multimodal reasoning categories in MORSE500. Each category incorporates specific complexity controls and novel adaptations to create challenging multimodal scenarios. 28 MORSE: Multimodal Reasoning Stress-test Environment B. Example Question and Code B.1. Abstract Reasoning Abstract Reasoning - ARC-AGI Video Question Query: Answer the question in this video. Question Text: How many pink squares should appear in the output grid? Answer with only one multiple choice option. Ground truth:"
        },
        {
            "title": "Code",
            "content": "The python code summarization is pasted below, the full code is available on github. def __init__ ( self , data_path , p_type ) : super () . __init__ () self . p_type = p_type self . DATA_PATH = data_path 1 from manim import * 2 import json , random , copy 3 import os 4 from pathlib import Path 5 from collections import Counter 6 from itertools import chain 7 8 class ARCScene ( Scene ) : 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ] # 0 - background / empty BLACK , ManimColor . from_hex ( \" #0074 D9 \" ) , ManimColor . from_hex ( \" # FF4136 \" ) , ManimColor . from_hex ( \" #2 ECC40 \" ) , ManimColor . from_hex ( \" # FFDC00 \" ) , ManimColor . from_hex ( \" # AAAAAA \" ) , ManimColor . from_hex ( \" # F012BE \" ) , ManimColor . from_hex ( \" #7 FDBFF \" ) , ManimColor . from_hex ( \" #870 C25 \" ) , WHITE , # 9 - white # - - - - - - - - - - Visual constants - - - - - - - - - - self . CELL_SIZE = 0.35 self . MAX_GRID = 3.0 self . TOP_SCALE = 0.8 self . COLOR_TABLE = [ # path to ARC json file # max side length after scaling for training grids # additional shrink factor for test ( top ) grids # 1 - blue # 2 - orange # 3 - green # 4 - yellow # 5 - light gray # 6 - pink # 7 - light blue # 8 - dark red 29 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 MORSE: Multimodal Reasoning Stress-test Environment self . NUM_TO_STR = [ \" black \" , \" blue \" , \" orange \" , \" green \" , \" yellow \" , \" light gray \" , \" pink \" , \" light blue \" , \" dark red \" , \" white \" , ] self . TRAIN_STAY = 1.5 self . TRANSITION = 0.5 self . TEST_BIG_STAY = 1.5 self . TEST_STAY = 4. # how long to hold full - size test pair # after options appear # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # Helpers # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - def gri d_to_vgroup ( self , grid ) : \"\"\" Convert 2 - list of ints into VGroup of colored squares . \"\"\" print ( grid ) vg = VGroup () for in range ( len ( grid ) ) : for in range ( len ( grid [ ]) ) : val = grid [ ][ ] val = val % 10 color = self . COLOR_TABLE [ val ] sq = Square ( side_length = self . CELL_SIZE , stroke_width =0) sq . set_fill ( color , opacity =1) # top - left origin mapping = ( + 0.5) * self . CELL_SIZE = -( + 0.5) * self . CELL_SIZE sq . move_to ([ , , 0]) vg . add ( sq ) # center on origin then return vg . move_to ( ORIGIN ) return vg def scale_grid ( self , vg , max_side ) : \"\"\" Scale * vg * so its larger dimension equals * max_side *. \"\"\" sf = min ( max_side / vg . width , max_side / vg . height ) vg . scale ( sf ) return vg def load_task ( self ) : with open ( self . DATA_PATH , \" \" ) as fp : task = json . load ( fp ) train_pairs = [( [ \" input \" ] , [ \" output \" ]) for in task [ \" train \" ]] test_input = task [ \" test \" ][0][ \" input \" ] test_output = task [ \" test \" ][0][ \" output \" ] return task , train_pairs , test_input , test_output def color_perturb ( self , grid , p_changes =0.1) : = copy . deepcopy ( grid ) rows , cols = len ( ) , len ( [0]) n_changes = int ( p_changes * rows * cols ) for _ in range ( n_changes ) : allowed = set () while len ( allowed ) == 0: allowed = set () , = random . randrange ( rows ) , random . randrange ( cols ) orig = [ ][ ] dirs = [( -1 , 0) , (1 , 0) , (0 , -1) , (0 , 1) ] # up , down , left , right for dr , dc in dirs : if 0 <= + dr < rows and 0 <= + dc < cols and grid [ + dr ][ + dc ]!= orig : allowed . add ( grid [ + dr ][ + dc ]) [ ][ ] = random . choice ( list ( allowed ) ) return def show_colors ( self , colors , names ) : cols = VGroup () all_squares = VGroup () all_arrows = VGroup () 30 MORSE: Multimodal Reasoning Stress-test Environment all_labels = VGroup () for start in range (0 , len ( colors ) , 4) : squares = VGroup ( *[ ] Square (1.0) . set_fill ( col , 1) . set_stroke ( color = WHITE , width =2) for col in colors [ start : start + 4] ) squares . arrange ( DOWN , buff =0.5 , aligned_edge = LEFT ) . to_edge ( LEFT , buff =2) labels = VGroup ( *[ Text ( nm , font_size =32) for nm in names [ start : start + 4]] ) for sq , lbl in zip ( squares , labels ) : lbl . next_to ( sq , RIGHT , buff =1.2) arrows = VGroup ( *[ Arrow ( start = sq . get_right () , end = lbl . get_left () , buff =0.05 , stroke_width =4 , ) for sq , lbl in zip ( squares , labels ) ] ) col_group = VGroup ( squares , arrows , labels ) cols . add ( col_group ) all_squares . add (* squares ) all_arrows . add (* arrows ) all_labels . add (* labels ) cols . arrange ( buff =1.7 , aligned_edge = UP ) . move_to ( ORIGIN ) . ca e_ o_ t_ id ( 0.9 * config . frame_width ) # Title title = Text ( \" Remember the following color names \" , font_size =40) title . to_edge ( UP ) # Animation sequence self . play ( Write ( title ) ) self . play ( Succession ( FadeIn ( all_squares ) , AnimationGroup (*[ GrowArrow ( ar ) for ar in all_arrows ] , lag_ratio =0.1) , AnimationGroup (*[ Write ( label ) for label in all_labels ]) , ) , run_time =1.5 , ) self . wait (2) self . play ( FadeOut ( title , cols ) ) self . wait (0.5) # - - - - - - - - - - Main construct - - - - - - - - - - def construct ( self ) : self . add ( Rectangle ( height = config . frame_height , width = config . frame_width ) . set_color ( color_gradient ( random . sample ( self . COLOR_TABLE , 2) , 5) ) . set_opacity (0.7) ) self . show_colors ( self . COLOR_TABLE , self . NUM_TO_STR ) prompt = Text ( \" Observe the following inputs / outputs \" , color = WHITE , font_size =36 ) . move_to ( ORIGIN ) self . play ( FadeIn ( prompt ) , run_time =0.5) self . wait (1.5) self . play ( FadeOut ( prompt ) , run_time =0.5) self . wait (1) task_raw , train_pairs , test_input , test_output = self . load_task () arrow_proto = Arrow ( LEFT , RIGHT , color = WHITE , buff =0.2) train_L = LEFT * 3 train_R = RIGHT * 3 # - - - - - - - - - - Training slideshow - - - - - - - - - - for inp , out in train_pairs : lgrid = self . scale_grid ( self . grid_to_vgroup ( inp ) , self . MAX_GRID ) . move_to ( 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 31 MORSE: Multimodal Reasoning Stress-test Environment train_L ) rgrid = self . scale_grid ( self . grid_to_vgroup ( out ) , self . MAX_GRID ) . move_to ( train_R ) arr = arrow_proto . copy () self . play ( FadeIn ( lgrid , shift = DOWN * 0.2) , FadeIn ( arr ) , FadeIn ( rgrid , shift = UP * 0.2) , run_time = self . TRANSITION , ) self . wait ( self . TRAIN_STAY ) self . play ( FadeOut ( lgrid ) , FadeOut ( arr ) , FadeOut ( rgrid ) , run_time = self . TRANSITION ) # - - - - - - - - - - Test : show full - size pair - - - - - - - - - - L_big_anchor = LEFT * 3 R_big_anchor = RIGHT * 3 test_in_big = self . scale_grid ( self . grid_to_vgroup ( test_input ) , self . MAX_GRID ) . move_to ( L_big_anchor ) blank_grid = [[0 for _ in row ] for row in test_output ] blank_big = self . scale_grid ( self . grid_to_vgroup ( blank_grid ) , self . MAX_GRID ) . move_to ( R_big_anchor ) q_big = Text ( \" ? \" , font_size =160 , color = WHITE ) . move_to ( blank_big ) arrow_big = arrow_proto . copy () self . play ( FadeIn ( test_in_big ) , FadeIn ( arrow_big ) , FadeIn ( q_big ) , run_time = self . TRANSITION , ) self . wait ( self . TEST_BIG_STAY ) ca = Counter ( chain . from_iterable ( test_input ) ) cb = Counter ( chain . from_iterable ( test_output ) ) changed = { for in set ( ca ) set ( cb ) if ca [ ] != cb [ ]} - {0 , 9} color = random . choice ( list ( changed ) ) color_str = self . NUM_TO_STR [ color ] title = \" How many { color_str } squares should appear in the output grid ? nAnswer with single integer \" 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 para = Paragraph (* lines , alignment = \" center \" , font_size =36 , line_spacing =0.8) para . to_edge ( UP ) if para . width > 0.9 * config . frame_width : para . al _t _ fi _w th ( config . frame_width * 0.9) self . play ( Write ( para ) , run_time =1) self . answer = sum ( row . count ( color ) for row in test_output ) self . question_text = title . replace ( \" \" , \" \" ) lines = title . split ( \" \" ) 234 235 236 237 238 239 240 241 242 243 244 245 246 247 if __name__ == \" __main__ \" : N_EXAMPLES = 20 248 p_type = \" count \" # { mc , count } 249 path_name = \" arcagi2_ { p_type } \" 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 ) as : . write ( str ( scene . answer ) ) os . makedirs ( \" media / videos /1080 p60 /{ path_name }/ questions \" , exist_ok = True ) os . makedirs ( \" media / videos /1080 p60 /{ path_name }/ solutions \" , exist_ok = True ) os . makedirs ( \" media / videos /1080 p60 /{ path_name }/ question_text \" , exist_ok = True ) folder = Path ( \" arcagi2 \" ) random . seed (1) paths = random . sample ( list ( folder . iterdir () ) , N_EXAMPLES ) for path in paths : config . output_file = \" { path_name }/ questions /{ path . stem } \" scene = ARCScene ( path , p_type ) scene . render () with open ( \" media / videos /1080 p60 /{ path_name }/ solutions /{ path . stem }. txt \" , \" \" MORSE: Multimodal Reasoning Stress-test Environment 267 268 269 270 with open ( \" media / videos /1080 p60 /{ path_name }/ question_text /{ path . stem }. txt \" , \" \" ) as : . write ( scene . question_text ) 33 MORSE: Multimodal Reasoning Stress-test Environment B.2. Mathematical Reasoning Mathematical Reasoning - Area Under Fluctuating Sine Wave Query: Answer the question in this video. Question Text: What is the total area under one period of this moving wave? Please answer as single integer. Ground truth:"
        },
        {
            "title": "Code",
            "content": "The python code summarization is pasted below, the full code is available on github. 1 # s n _ e _a ea . py 2 from manim import * 3 import numpy as np 4 import random 5 import os 6 7 class s nWaveArea ( Scene ) : def construct ( self ) : 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # ensure questions_text exists os . makedirs ( \" questions_text \" , exist_ok = True ) # 1) Randomize wave parameters = random . randint (1 , 5) = random . choice ([1 , 2]) C0 = random . uniform (0 , 2 * np . pi ) # initial phase offset period = 2 * np . pi / # amplitude # frequency # 2) Set up phase tracker for animation phase = ValueTracker (0.0) speed = 4. # units per second # 3) Set up axes with ticks every 1/ * pi maybe , but - range is one period x_min , x_max = 0 , period axes = Axes ( x_range =[ x_min , x_max , period /4] , y_range =[ - - 1 , + 1 , 1] , x_length =6 , y_length =4 , tips = True ) . to_edge ( DOWN ) axes_labels = axes . get_axis _labels ( x_label = \" \" , y_label = \" ( ) \" ) # 4) Plot and animate the wave via updater graph = axes . plot ( lambda : * np . sin ( * ( - phase . get_value () ) + C0 ) , x_range =[ x_min , x_max ] , color = BLUE ) def update_graph ( mob , dt ) : phase . increment_value ( speed * dt ) new_graph = axes . plot ( lambda : * np . sin ( * ( - phase . get_value () ) + C0 ) , x_range =[ x_min , x_max ] , color = BLUE ) 34 MORSE: Multimodal Reasoning Stress-test Environment mob . become ( new_graph ) graph . add_updater ( update_graph ) self . add ( axes , axes_labels , graph ) self . wait (1) # 5) Compute the total area under one period # Integral of ( ) dx over one period = 4 / correct = int (4 * / ) # 6) Save question prompt and answer prompt = ( \" What is the total area under one period of this moving wave ? \" ) with open ( \" questions_text / question . txt \" , \" \" ) as qf : qf . write ( prompt + \" nAnswer as single integer . \" ) with open ( \" answer . txt \" , \" \" ) as af : af . write ( \" { correct } \" ) # 7) Display question prompt on - screen ( two lines ) q1 = Text ( \" What is the total area under one period of this moving wave ? \" , font_size = ) . to_edge ( UP ) q2 = Text ( \" Answer as single integer . \" , font_size =24 ) . next_to ( q1 , DOWN , buff =0.2) self . play ( Write ( q1 ) , Write ( q2 ) ) self . wait (2) # 8) Clean up graph . remove_updater ( update_graph ) phase . clear_updaters () self . wait (1) 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 MORSE: Multimodal Reasoning Stress-test Environment Mathematical Reasoning - Region Area Comparison Query: Answer the question in this video. Question Text: Which region is larger, R1 or R2? Ground truth: R"
        },
        {
            "title": "Code",
            "content": "The python code summarization is pasted below, the full code is available on github. # 1) Define two wave functions and A1 , B1 , C1 = random . randint (1 ,3) , random . randint (1 ,3) , random . uniform (0 , 2* np . pi ) A2 , B2 , C2 = random . randint (1 ,3) , random . randint (1 ,3) , random . uniform (0 , 2* np . pi ) = lambda : A1 * np . sin ( B1 * + C1 ) = lambda : A2 * np . cos ( B2 * + C2 ) x_range =[ x_min , x_max , PI /2] , y_range =[ -( max ( A1 , A2 ) +1) , max ( A1 , A2 ) +1 , 1] , x_length =8 , y_length =5 , tips = True ) labels = axes . get_axis_labels ( \" \" , \" \" ) self . play ( Create ( axes ) , Write ( labels ) , run_time =1) def construct ( self ) : # 2) Setup domain and axes x_min , x_max = -PI , PI axes = Axes ( 1 from manim import * 2 import random 3 import numpy as np 4 5 class i o r e o ( Scene ) : 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 else : # 4) Shade R1 over [ - pi , 0] xs1 = np . linspace ( x_min , 0 , 200) polys1 = VGroup () for in range ( len ( xs1 ) -1) : x0 , x1 = xs1 [ ] , xs1 [ +1] y0f , y1f = ( x0 ) , ( x1 ) y0g , y1g = ( x0 ) , ( x1 ) mid = ( (( x0 + x1 ) /2) - (( x0 + x1 ) /2) ) if mid > 0: lower0 , upper0 = y0g , y0f lower1 , upper1 = y1g , y1f color = BLUE lower0 , upper0 = y0f , y0g lower1 , upper1 = y1f , y1g # 3) Plot ( blue ) and ( red ) graph_f = axes . plot ( lambda : ( ) , x_range =[ x_min , x_max ] , color = BLUE ) graph_g = axes . plot ( lambda : ( ) , x_range =[ x_min , x_max ] , color = RED ) self . play ( Create ( graph_f ) , Create ( graph_g ) , run_time =2) 36 MORSE: Multimodal Reasoning Stress-test Environment color = RED poly = Polygon ( axes . c2p ( x0 , lower0 ) , axes . c2p ( x0 , upper0 ) , axes . c2p ( x1 , upper1 ) , axes . c2p ( x1 , lower1 ) , fill_color = color , fill_opacity =0.5 , stroke_opacity =0 ) polys1 . add ( poly ) self . play ( Create ( polys1 ) , run_time =2) r1_label = Text ( \" R1 \" , font_size =24 , color = WHITE ) . next_to ( axes . c2p (( x_min +0) /2 , max ( A1 , A2 ) /2) , UP , buff =0.2 ) self . play ( Write ( r1_label ) ) # 5) Shade R2 over [0 , pi ] ( fresh ) xs2 = np . linspace (0 , x_max , 200) polys2 = VGroup () for in range ( len ( xs2 ) -1) : x0 , x1 = xs2 [ ] , xs2 [ +1] y0f , y1f = ( x0 ) , ( x1 ) y0g , y1g = ( x0 ) , ( x1 ) mid = ( (( x0 + x1 ) /2) - (( x0 + x1 ) /2) ) if mid > 0: lower0 , upper0 = y0g , y0f lower1 , upper1 = y1g , y1f color = BLUE else : lower0 , upper0 = y0f , y0g lower1 , upper1 = y1f , y1g color = RED poly = Polygon ( axes . c2p ( x0 , lower0 ) , axes . c2p ( x0 , upper0 ) , axes . c2p ( x1 , upper1 ) , axes . c2p ( x1 , lower1 ) , fill_color = color , fill_opacity =0.5 , stroke_opacity =0 ) polys2 . add ( poly ) # Fade out R1 and its label , then draw R2 self . play ( FadeOut ( polys1 ) , FadeOut ( r1_label ) , Create ( polys2 ) , run_time =2) r2_label = Text ( \" R2 \" , font_size =24 , color = WHITE ) . next_to ( axes . c2p ((0+ x_max ) /2 , max ( A1 , A2 ) /2) , UP , buff =0.2 ) self . play ( Write ( r2_label ) ) # 6) Compute areas area1 = np . trapz ( np . abs ( ( xs1 ) - ( xs1 ) ) , xs1 ) area2 = np . trapz ( np . abs ( ( xs2 ) - ( xs2 ) ) , xs2 ) correct = \" R1 \" if area1 > area2 else \" R2 \" # 7) Show question question = Text ( \" Which region is larger , R1 or R2 ? \" , font_size =36) . to_edge ( UP ) self . play ( Write ( question ) ) # 8) Show choices choices = VGroup ( Text ( \" . R1 \" , font_size =24) , Text ( \" . R2 \" , font_size =24) ) . arrange ( DOWN , aligned_edge = LEFT , buff =0.5) . next_to ( axes , RIGHT , buff =1) self . play ( Write ( choices ) ) # 9) Save answer with open ( \" answer . txt \" ,\" \" ) as out : out . write ( \" Region : { correct } \" ) self . wait (2) 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 37 MORSE: Multimodal Reasoning Stress-test Environment B.3. Physical Reasoning Physical Reasoning - Teapot Rotating Query: Answer the question in this video. Question Text: From the scenarios previewed, which video aligns with observable physical interactions? teapot is placed on rotating display that rotates clockwise. Static shot with no camera movement. Choose letter (e.g., S). If none of them are realistic, return None. Ground truth: E"
        },
        {
            "title": "Code",
            "content": "The python code summarization is pasted below, the full code is available on github. 1 import csv 2 import random 3 import argparse 4 from pathlib import Path 5 from typing import List , Optional , Tuple 6 import cv2 7 import numpy as np 8 from moviepy . editor import VideoFileClip , CompositeVideoClip , ColorClip 9 import textwrap 10 11 12 class e i Ge er or : 13 14 15 16 17 18 19 self . target_width = target_width self . target_fps = target_fps self . es io _ te pl te = [ def __init__ ( self , target_width : int = 1920 , target_fps : int = 30) : \"\"\" class to generate video grids with physics questions \"\"\" \" Which of the earlier videos reflects realistic physics in this 20 21 situation ? { prompt } nAnswer with single letter ( . . , ) . If none of them are realistic , return None . \" , \" From the clips you viewed , which one accurately simulates natural motion / behavior here ? { prompt } nRespond using one letter ( . . , ) . If none of them are realistic , return None . \" , \" Which previously shown video demonstrates plausible physics for this event ? { prompt } nSubmit one letter ( . . , ) . If none of them are realistic , return None . \" , 38 MORSE: Multimodal Reasoning Stress-test Environment \" Which of the earlier examples exhibits scientifically valid motion / behavior ? { prompt } nReply with single letter ( . . , ) . If none of them are realistic , return None . \" , \" Which previously shown video adheres to the laws of physics in this experiment ? { prompt } nAnswer with single letter ( . . , ) . If none of them are realistic , return None . \" , \" { prompt } nWhich of the earlier videos aligns with real - world physics in this scenario ? nRespond with single letter ( . . , ) . If none of them are realistic , return None . \" , \" { prompt } nFrom the clips viewed earlier , which one adheres to the laws of physics in this situation ? nChoose the correct letter ( . . , ) . If none of them are realistic , return None . \" , \" { prompt } nWhich demonstrated video exhibits physical plausibility for the scenario above ? nIndicate your answer as single letter ( . . , ) . \" , \" { prompt } nConsidering the scene described , which of the clips shown prior follows realistic physical principles ? nProvide your answer in single letter ( . . , ) . If none of them are realistic , return None . \" , \" { prompt } nAmong the options previewed earlier , which video is consistent with real - world physics in this context ? nSubmit your answer as single letter ( . . , ) . If none of them are realistic , return None . \" , ] def _ e _ p i ( self , video_path : str ) -> Tuple [ float , float , int , int ]: \"\"\" Get video properties using MoviePy \"\"\" try : clip = VideoFileClip ( video_path ) fps = clip . fps duration = clip . duration width , height = clip . size clip . close () return fps , duration , width , height except Exception as : print ( \" Error reading video { video_path }: { } \" ) raise def _ t _ _ e ( self , video_clip : VideoFileClip , letter : str ) -> VideoFileClip : \"\"\" Add letter marker in black strip at the bottom right of the video \"\"\" def add_letter ( frame ) : frame_copy = frame . copy () # Create black strip at the bottom of the frame strip_height = 60 strip_width = # Calculate position for bottom right placement x_start = frame_copy . shape [1] - strip_width y_start = frame_copy . shape [0] - strip_height # Create black rectangle cv2 . rectangle ( frame_copy , ( x_start , y_start ) , ( frame_copy . shape [1] , frame_copy . shape [0]) , (0 , 0 , 0) , -1) # Calculate text position to center in the black strip text_size = cv2 . getTextSize ( letter , cv2 . FONT_HERSHEY_DUPLEX , 2 , 2) [0] text_x = x_start + ( strip_width - text_size [0]) // 2 text_y = y_start + ( strip_height + text_size [1]) // 2 # Add shadow for better visibility cv2 . putText ( frame_copy , letter , ( text_x +2 , text_y +2) , cv2 . FONT_HERSHEY_DUPLEX , 2 , (50 , 50 , 50) , 3 , cv2 . LINE_AA ) # Add the letter in white cv2 . putText ( frame_copy , letter , ( text_x , text_y ) , cv2 . FONT_HERSHEY_DUPLEX , 2 , (255 , 255 , 255) , 2 , cv2 . LINE_AA ) return frame_copy return video_clip . fl_image ( add_letter ) def wrap_text ( self , text : str , max_width : int = 40) -> str : \"\"\" Wrap text to fit within maximum width \"\"\" lines = [] for paragraph in text . split ( ' ') : wrapped_lines = textwrap . wrap ( paragraph , width = max_width ) lines . extend ( wrapped_lines ) # Add an empty line between paragraphs if there 's another paragraph coming if paragraph != text . split ( ' ') [ -1]: lines . append ( ' ') return ' '. join ( lines ) 22 23 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 MORSE: Multimodal Reasoning Stress-test Environment def _ i _ s n ( self , final_clip : VideoFileClip , question_text : str , fade_duration : float = 3) -> Co os te de Cl : \"\"\" Add fading question frame at the end of the video \"\"\" ap ed_que stion = self . wrap_text ( question_text ) def e _ s n _ me ( ) : # Create black frame frame = np . zeros (( final_clip .h , final_clip .w , 3) , dtype = np . uint8 ) # Split the text into lines lines = wrappe d_question . split ( ' ') # Calculate positions for centered text y_position = final_clip . // 2 - ( len ( lines ) * 50) // 2 # Add each line of text for line in lines : # Get text size text_size = cv2 . getTextSize ( line , cv2 . FONT_HERSHEY_DUPLEX , 1.5 , 2) [0] # Calculate position to center text x_position = ( final_clip . - text_size [0]) // # Add shadow cv2 . putText ( frame , line , ( x_position +2 , y_position +2) , cv2 . FONT_HERSHEY_DUPLEX , 1.5 , (50 , 50 , 50) , 4 , cv2 . LINE_AA ) # Add main text cv2 . putText ( frame , line , ( x_position , y_position ) , cv2 . FONT_HERSHEY_DUPLEX , 1.5 , (255 , 255 , 255) , 2 , cv2 . LINE_AA ) # Move to next line y_position += 60 # Add fade - in effect if < fade_duration /2: alpha = / ( fade_duration /2) return ( frame * alpha ) . astype ( np . uint8 ) return frame # Create ColorClip with the question text question_clip = ColorClip ( size =( final_clip .w , final_clip . ) , color =[0 , 0 , 0] , duration = fade_duration ) # Override the make_frame method to use our custom text function question_clip . make_frame = e _ s n _ m question_clip = question_clip . set_fps ( final_clip . fps ) question_clip = question_clip . set_start ( final_clip . duration ) # Combine with the main video final_video = om os eV de li ( [ final_clip , question_clip ] , size = final_clip . size ) final_video = final_video . set_duration ( final_clip . duration + fade_duration ) return final_video def c t _ d _ o ( self , num_videos : int ) -> Tuple [ int , int ]: \"\"\" Calculate optimal grid layout for given number of videos \"\"\" if num_videos <= 2: return num_videos , 1 elif num_videos <= 4: return 2 , 2 elif num_videos <= 6: return 2 , 3 elif num_videos <= 9: return 3 , 3 else : 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 MORSE: Multimodal Reasoning Stress-test Environment # For more than 9 videos , use more general approach cols = int ( np . ceil ( np . sqrt ( num_videos ) ) ) rows = int ( np . ceil ( num_videos / cols ) ) return cols , rows def a e_ vi deo _g rid ( self , video_paths : List [ str ] , output_path : str , ending_question : str , solutions_dir : Optional [ Path ] = None , ue io s_ xt _ di : Optional [ Path ] = None , num_videos : Optional [ int ] = None ) -> None : \"\"\" Create video grid with specified number of videos \"\"\" # Limit number of videos if specified if num_videos is not None and len ( video_paths ) > num_videos : video_paths = video_paths [: num_videos ] if not video_paths : raise ValueError ( \" No video paths provided \" ) print ( \" Creating grid with { len ( video_paths ) } videos \" ) # Load video clips ori ginal_clips = [] for path in video_paths : try : clip = VideoFileClip ( path ) original_clips . append ( clip ) except Exception as : print ( \" Error loading video { path }: { } \" ) continue if not original_clips : raise ValueError ( \" No valid video clips could be loaded \" ) # Find the minimum duration among all clips min_duration = min ( clip . duration for clip in original_clips ) # Trim all clips to the same duration clips = [ clip . subclip (0 , min_duration ) for clip in original_clips ] # Calculate the grid layout _ id eo s_a ct ual = len ( clips ) num_columns , num_rows = self . c t _ d _ o ( nu _v ide os _ac tu al ) # Calculate the width for each video clip clip_width = self . target_width // num_columns # Resize all clips while preserving aspect ratio resized_clips = [] for clip in clips : resized_clip = clip . resize ( width = clip_width ) resized_clips . append ( resized_clip ) # Find the correct answer ( video with ' full ' in the name ) cor rect_answer = self . _ d _ r _ w ( video_paths ) # Save the correct answer and question text self . _ e _ w _ _ s n ( output_path , correct_answer , ending_question , solutions_dir , es on _t t_ ir ) # Add letter markers to each clip marked_clips = [ self . dd _ t _ _ e ( clip , chr (65 + ) ) for , clip in enumerate ( resized_clips ) ] # Calculate grid dimensions row_heights = self . _ c t _ _ g ( resized_clips , num_columns , num_rows ) total_height = sum ( row_heights ) final_size = ( self . target_width , total_height ) # Create composite clips arrangement omp osite_clips = self . _ a _ p _ _ d ( marked_clips , num_columns , num_rows , clip_width , row_heights ) # Create the final composite video background = ColorClip ( size = final_size , color =(0 , 0 , 0) , duration = min_duration ) background = background . set_fps ( self . target_fps ) final_grid = po it id oC ([ background ] + composite_clips , size = final_size ) 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 MORSE: Multimodal Reasoning Stress-test Environment final_grid = final_grid . set_fps ( self . target_fps ) # Add the ending question with fade final_clip = self . _ i _ s n ( final_grid , ending_question ) # Write the result to file try : final_clip . write_videofile ( output_path , codec = ' libx264 ') print ( \" Successfully created video : { output_path } \" ) except Exception as : print ( \" Error writing video file : { } \" ) raise finally : # Clean up resources self . _cleanup_clips ( original_clips + resized_clips + marked_clips + [ final_grid , final_clip ]) def _ d _ r _ w ( self , video_paths : List [ str ]) -> Optional [ str ]: \"\"\" Find the correct answer based on video filename containing ' full ' \"\"\" for , path in enumerate ( video_paths ) : if ' full ' in Path ( path ) . name . lower () : return chr (65 + ) # Convert to letter (A , , , etc .) return None def _ e _ w _ _ s n ( self , output_path : str , correct_answer : Optional [ str ] , ending_question : str , solutions_dir : Optional [ Path ] , ue io s_ xt _ di : Optional [ Path ]) -> None : \"\"\" Save the correct answer and question text to files \"\"\" if orrect_answer and solutions_dir : utput_filename = Path ( output_path ) . stem answer_file = solutions_dir / \" { output_filename }. txt \" try : with open ( answer_file , 'w ') as : . write ( correct_answer ) except Exception as : print ( \" Error saving answer file : { } \" ) if s ti s_ ex _ di : utput_filename = Path ( output_path ) . stem question_file = es on _t t_ ir / \" { output_filena me }. txt \" try : with open ( question_file , 'w ') as : . write ( ending_question ) except Exception as : print ( \" Error saving question file : { } \" ) def _ c t _ _ g ( self , resized_clips : List [ VideoFileClip ] , num_columns : int , num_rows : int ) -> List [ int ]: \"\"\" Calculate the height of each row in the grid \"\"\" row_heights = [] num_videos = len ( resized_clips ) for in range ( num_rows ) : row_start_idx = * num_columns row_end_idx = min ( row_start_idx + num_columns , num_videos ) if row_end_idx > row_start_idx : row_clips = resized_clips [ row_start_idx : row_end_idx ] row_height = max ( clip . for clip in row_clips ) row_heights . append ( row_height ) else : row_heights . append (0) return row_heights def _ a _ p _ _ d ( self , marked_clips : List [ VideoFileClip ] , num_columns : int , num_rows : int , clip_width : int , row_heights : List [ int ]) -> List [ VideoFileClip ]: \"\"\" Arrange clips in grid layout \"\"\" omp osite_clips = [] current_y = 0 num_videos = len ( marked_clips ) for row in range ( num_rows ) : row_start_idx = row * num_columns row_end_idx = min ( row_start_idx + num_columns , num_videos ) row_height = row_heights [ row ] if row_end_idx <= row_start_idx : 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 col = - row_start_idx clip = marked_clips [ ] composite_clips . append ( clip . set_position (( col * clip_width , current_y ) ) ) MORSE: Multimodal Reasoning Stress-test Environment # Handle the last row with potentially fewer videos videos_in_row = row_end_idx - row_start_idx if videos_in_row == 1 and num_columns > 1: # Center single video in last row center_x = ( self . target_width - clip_width ) // 2 clip = marked_clips [ row_start_idx ] composite_clips . append ( clip . set_position (( center_x , current_y ) ) ) break try : else : except : clip . close () return composite_clips current_y += row_height if hasattr ( clip , ' close ') : def _cl eanup_clips ( self , clips : List ) -> None : \"\"\" Clean up video clips to free memory \"\"\" for clip in clips : # Regular row arrangement for in range ( row_start_idx , row_end_idx ) : 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 def d _ c rip ti ons ( csv_path : str ) -> List [ List [ str ]]: 362 363 364 365 366 367 368 369 370 371 372 def a _ p _ e r ( base_dirs : List [ str ]) -> List [ Path ]: 373 374 375 376 377 378 379 380 381 382 def main () : 383 \"\"\" Create output directories if they don 't exist \"\"\" paths = [] for dir_name in base_dirs : \"\"\" Generate random question using the prompt \"\"\" template = random . choice ( self . qu ti n_ mp at ) return template . format ( prompt = prompt ) print ( \" Error loading descriptions from { csv_path }: { } \" ) return [] path = Path ( dir_name ) path . mkdir ( exist_ok = True ) paths . append ( path ) \"\"\" Load descriptions from CSV file \"\"\" try : def e at e_ que st ion ( self , prompt : str ) -> str : with open ( csv_path , 'r ' , encoding = 'utf -8 ') as : reader = csv . reader ( ) return list ( reader ) except Exception as : return paths pass 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 parser = argparse . ArgumentParser ( description = ' Generate physics question videos from input videos ') parser . add_argument ( ' -- descriptions ' , default = ' descriptions / descriptions . csv ' , help = ' Path to descriptions CSV file ') parser . add_argument ( ' -- video - root ' , default = ' videos_gen erated ' , parser . add_argument ( ' -- frames - root ' , default = ' frames_selected ' , help = ' Root directory containing selected frames ') help = ' Root directory containing generated videos ') parser . add_argument ( ' -- num - videos ' , type = int , help = ' Number of videos to include in each grid ( optional ) ') parser . add_argument ( ' -- target - width ' , type = int , default =1920 , help = ' Target width for output videos ') parser . add_argument ( ' -- target - fps ' , type = int , default =30 , parser . add_argument ( ' -- shuffle ' , action = ' store_true ' , help = ' Target FPS for output videos ') help = ' Shuffle video order randomly ') args = parser . parse_args () # Load prompt data prompt_data = loa d_ des cr ipt io ns ( args . descriptions ) if not prompt_data : 43 MORSE: Multimodal Reasoning Stress-test Environment print ( \" No prompt data loaded . Exiting . \" ) return # Create output directories questions_dir , questions_text_dir , solutions_dir = a _ p _ e r ([ ' questions ' , ' questions_text ' , ' solutions ' # Initialize video grid generator generator = de Gr dG er to ( target_width = args . target_width , target_fps = args . target_fps ) # Process images and generate videos image_root = Path ( args . frames_root ) images = sorted ( image_root . glob ( \" *. jpg \" ) ) video_root = Path ( args . video_root ) prefix = image . stem . split ( '_ ') [0] video_paths = video_root . glob ( \" { prefix }*. mp4 \" ) video_paths = [ str ( ) for in sorted ( video_paths ) ] if not video_paths : print ( \" No videos found for prefix { prefix } \" ) continue # Shuffle videos if requested if args . shuffle : random . shuffle ( video_paths ) # Get prompt for this index idx = int ( prefix ) if idx >= len ( prompt_data ) : print ( \" No prompt data for index { idx } \" ) continue prompt = prompt_data [ idx ][1] question = generator . gen er ate _q ues ti on ( prompt ) # Generate output path de o_o ut put _p ath = questions_dir / \" physics_iq_ { prefix }. mp4 \" ]) try : for image in images : 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 if __name__ == \" __main__ \" : 463 464 465 466 467 468 469 470 471 472 473 474 475 main () ) # Create the video grid generator . cr eat e_ vid eo _gr id ( video_paths = video_paths , output_path = str ( vi deo _o utp ut _pa th ) , ending_question = question , solutions_dir = solutions_dir , ue io s_ xt _ di = questions_text_dir , num_videos = args . num_videos except Exception as : print ( \" Error processing { image }: { } \" ) continue # # bash # Use only 3 videos per grid # python script . py -- num - videos 3 # Shuffle videos and use 6 videos per grid # python script . py -- num - videos 6 -- shuffle # # Custom resolution and fps # python script . py -- target - width 1280 -- target - fps 24 # # Use all available videos ( original behavior ) # python script . py 44 MORSE: Multimodal Reasoning Stress-test Environment B.4. Planning Reasoning Planning Reasoning - Frozen Lake Path Selection Query: Answer the question in this video. Question Text: Which of the listed move sequences carries the agent from Start to Goal without stepping onto hole? (A) (B) (C) (D) (E) Please return single letter (e.g. A). Nothing preceding or following it. Ground truth: D"
        },
        {
            "title": "Code",
            "content": "The python code summarization is pasted below, the full code is available on github. \"\"\" Generate spatial reasoning question with controllable difficulty 1 # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 2 # FOGGY FROZEN LAKE MAZE GENERATION 3 # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 4 5 def e e _ e _ s n ( di ff icu lt y_p ar ams ) : 6 7 8 9 10 11 12 13 14 15 16 17 Difficulty Parameters : - maze_size : Grid dimensions (4 x4 to 20 x20 ) - i ity_ range : How far agent can see (1 -5 cells ) - num_holes : Number of obstacles / holes (0 to size /4) - num_options : Multiple choice options (3 -8) - path_length : Minimum path length required \"\"\" # STEP 1: Generate Base Environment MORSE: Multimodal Reasoning Stress-test Environment maze_size = di ffi cu lty _p ara ms [ ' maze_size '] num_holes = di ffi cu lty _p ara ms [ ' num_holes '] # 4 -20 ( larger = harder ) # 0 - size /4 ( more = harder ) grid = eat e_ ran do m_m ap ( size = maze_size ) for _ in range ( num_holes ) : c _ e _ d y ( grid ) env = z a n o n ( grid ) start = (0 , 0) goal = ( maze_size -1 , maze_size -1) # Bottom - right corner # Top - left corner # STEP 2: Apply Fog - of - War ( Limited Visibility ) si i y_ range = if fic ul ty_ pa ram [ ' visibility_ range '] # 1 -5 ( less = harder ) def apply_fog ( frame , agent_position , visibili ty_range ) : \"\"\" Hide cells beyond visibility distance from agent \"\"\" foggy_frame = frame . copy () for each cell in grid : distance = an at n_ is nc ( cell , agent_position ) if distance > visibility _range : apply_gray_fog ( foggy_frame , cell ) return foggy_frame # STEP 3: Generate Exploration Animation def explore_maze () : \"\"\" Agent explores maze with limited visibility \"\"\" frames = [] agent_pos = start visited_cells = set () while t_ ul _e pl re () : # Find unvisited neighbors within visibility unvisited = d _ i e _ g r ( agent_pos , visibility_range ) if unvisited : # Move to random unvisited neighbor next_pos = random . choice ( unvisited ) agent_pos = move_to ( next_pos ) else : # Use BFS to find path to nearest unvisited area path = d _ h _ _ i e _ a ( agent_pos , visited_cells ) agent_pos = follow_path ( path ) visited_cells . add ( agent_pos ) # Render frame with fog applied frame = en er _ nv ro en ( env , agent_pos ) foggy_frame = apply_fog ( frame , agent_pos , visibili ty_range ) frames . append ( foggy_frame ) return frames # STEP 4: Find Valid Solutions def d _ r t _ h () : \"\"\" Use BFS to find all shortest paths from start to goal \"\"\" queue = [( start , []) ] visited = { start : 0} sho rtest_paths = [] min_length = infinity # ( position , path_so_far ) while queue : pos , path = queue . pop_front () if pos == goal : if len ( path ) < min_length : min_length = len ( path ) shortest_paths = [ path ] elif len ( path ) == min_length : shortest_paths . append ( path ) continue for action in [ LEFT , DOWN , RIGHT , UP ]: next_pos = apply_action ( pos , action ) 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 46 MORSE: Multimodal Reasoning Stress-test Environment if is_ va lid _p osi ti on ( next_pos ) and not_hole ( next_pos ) : new_path = path + [ action ] if next_pos not in visited or visited [ next_pos ] >= len ( new_path ) : visited [ next_pos ] = len ( new_path ) queue . append (( next_pos , new_path ) ) return shortest_paths # STEP 5: Generate Distractor Paths def e e _ n _ h ( correct_length , num_distractors ) : \"\"\" Create plausible but incorrect paths of similar length \"\"\" wrong_paths = [] while len ( wrong_paths ) < nu m_distractors : # Generate random path of same length path = [ random_action () for _ in range ( correct_length ) ] # Check if path fails to reach goal if not reaches_goal ( path ) and is_plausible ( path ) : wrong_paths . append ( path ) return wrong_paths # STEP 6: Create Multiple Choice Question num_options = dif fi cul ty _pa ra ms [ ' num_options '] # 3 -8 ( more = harder ) correct_paths = d _ r st _ h () correct_path = random . choice ( correct_paths ) wrong_paths = e e _ n _ h ( len ( correct_path ) , num_options - 1) # Shuffle options all_options = wrong_paths + [ correct_path ] random . shuffle ( all_options ) cor rect _answer = find_index ( correct_path , all_options ) # Convert to human - readable format act ion_ symbols = { LEFT : \" \" , DOWN : \" \" , RIGHT : \" \" , UP : \" \" } question_text = \" Which path leads to the goal without falling into holes ? \" options_text = \" \" for , path in enumerate ( all_options ) : letter = chr (65 + ) path_symbols = \" \" . join ([ action_symbols [ action ] for action in path ]) options_text += \" ({ letter }) { path_symbols } \" # , , , ... # STEP 7: Create Professional Animation def a e_animation ( frames , question_text , options_text ) : \"\"\" Generate video with intro , maze exploration , and question display \"\"\" total_frames = ( intro_frames + len ( frames ) + outro_frames ) # Title and setup # Maze exploration # Question and options for frame_num in range ( total_frames ) : if frame_num < intro_frames : n _ r _ h _ l () elif frame_num < intro_frames + len ( frames ) : maze_frame_idx = frame_num - intro_frames re nd er_ ma ze_ fr ame ( frames [ maze_frame_idx ]) else : d _ s n _ _ i ( question_text , options_text ) # STEP 8: Save Output Files question_name = \" maze_sz { maze_size } _vis { visibi lity_range } _holes { num_holes } \" # Save video as MP4 video_path = \" questions /{ question_name }. mp4 \" a _ f i l _ m o ( frames = explore_maze () , output_path = video_path , question_text = question_text , options_text = options_text ) 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 47 MORSE: Multimodal Reasoning Stress-test Environment # Save ground truth answer as TXT solution_path = \" solutions /{ question_name }. txt \" with open ( solution_path , \" \" ) as : # Single letter : , , , etc . # Save question text as TXT l _ s n_ ex = \" { question_text } { options_text } nPlease return single letter ( . . ) \" s n _ t_ at = \" questions_text /{ question_name }. txt \" with open ( question_text_path , \" \" ) as : ' video_path ': video_path , ' solution_path ': solution_path , ' s io _t t_ at ': question_text_path , ' question_text ': question_text , ' options ': options_text , ' cor rect_answer ': chr (65 + correct_answer ) , ' metadata ': { ' maze_size ': maze_size , ' visibility _range ': visibility_range , ' num_holes ': num_holes , ' path_length ': len ( correct_path ) , ' difficulty _score ': c t _ f l _ r ( iff ic ult y_ par am ) } } . write ( ul _ qu st on _ ex ) . write ( chr (65 + correct_answer ) ) # RETURN COMPLETE QUESTION PACKAGE return { 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 207 # DIFFICULTY CONTROL MECHANISMS 208 # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 209 210 def c t _ f l _ r ( params ) : 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 239 # EXAMPLE DIFFICULTY CONFIGURATIONS 240 # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 241 242 F L _ S ET = { 243 244 245 246 247 248 249 250 251 252 253 254 ' maze_size ': 6 , ' si bility _range ': 3 , ' num_holes ': 2 , ' num_options ': 4 , ' e d _ f l ': 25 ' maze_size ': 10 , ' si bility _range ': 2 , ' num_holes ': 8 , return min (100 , total_score ) ' medium ': { ' easy ': { } , \"\"\" Combine multiple factors into overall difficulty score (0 -100) \"\"\" base_score = 0 # Maze size contribution (0 -30 points ) size_factor = min (30 , ( params [ ' maze_size '] - 4) * 2) # Visibility contribution (0 -25 points , inverse relationship ) i i y_f ac tor = max (0 , 25 - params [ ' visibility_ range '] * 5) # Obstacles contribution (0 -20 points ) st e _ de nsity = params [ ' num_holes '] / ( params [ ' maze_size '] ** 2) bst ac e_ factor = min (20 , obstacle_dens ity * 100) # Path complexity contribution (0 -15 points ) path_factor = min (15 , params . get ( ' path_length ' , 0) - params [ ' maze_size ' ]) # Options contribution (0 -10 points ) opt ions _factor = max (0 , ( params [ ' num_options '] - 3) * 2) total_score = ( size_factor + vis ib ili ty _fa ct or + obstacle_factor + path_factor + options_factor ) 48 MORSE: Multimodal Reasoning Stress-test Environment ' num_options ': 5 , ' e d _ f l ': 55 ' maze_size ': 15 , ' si bility _range ': 1 , ' num_holes ': 20 , ' num_options ': 7 , ' e d _ f l ': 85 } } , ' hard ': { 255 256 257 258 259 260 261 262 263 264 265 266 } 267 268 # Usage Example : 269 if __name__ == \" __main__ \" : 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 # Create necessary directories os . makedirs ( \" questions \" , exist_ok = True ) os . makedirs ( \" solutions \" , exist_ok = True ) os . makedirs ( \" questions_text \" , exist_ok = True ) # Generate question with medium difficulty es n _ pa ckage = e e _ e _ s n ( DI IC LT _ PR SE [ ' medium ' ]) print ( \" Generated files : \" ) print ( \" print ( \" print ( \" print ( \" Video : { uestion_ package [ ' video_path ']} \" ) Answer : { question_packa ge [ ' solution_path ']} \" ) Question : { qu esti on_package [ ' ue io _t t_ at ']} \" ) Difficulty : { ques tion_p ackage [ ' metadata '][ ' difficulty _score ']}/100 \" ) # Files saved : # - questions / e _ 1 0 _ 2 _ e 8 . mp4 # - solutions / e _ 1 0 _ 2 _ e 8 . txt # - ques tions_text / e _ 1 0 _ 2 _ e 8 . txt ( full question text ) ( video with question ) ( single letter answer ) 49 MORSE: Multimodal Reasoning Stress-test Environment B.5. Spatial Reasoning Spatial Reasoning - Matching the Missing Shape Query: Answer the question in this video. Question Text: Observe the following structure. Which shape matches the missing cubes? Answer with only one multiple choice option. Ground truth: D"
        },
        {
            "title": "Code",
            "content": "The python code summarization is pasted below, the full code is available on github. 1 from manim import * 2 import random 3 import json 4 import math 5 import os 6 7 class Cubes ( ThreeDScene ) : 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def construct ( self ) : # Add background bg = ( ) : def __init__ ( self , p_type , grid_size , p_removed , cfg_path = \" ../ templates / prisms . json \" , max_iters =25 , super () . __init__ () self . p_type = p_type self . grid_size = grid_size self . total = math . prod ( grid_size ) self . n_removed = int ( self . total * p_removed ) self . max_iters = max_iters with open ( cfg_path , \" \" ) as : self . cfg = json . load ( ) Rectangle ( height = config . frame_height , width = config . frame_width ) . set_color ( 50 MORSE: Multimodal Reasoning Stress-test Environment color_gradient ([ d _ g _ ol () , d _ g _ o () ] , 5) ) . set_opacity (0.6) ) self . _ e _ _ m _ j s ( bg ) VALID_COLORS = { \" blue \" : BLUE , \" red \" : RED , \" green \" : GREEN , \" yellow \" : YELLOW } # Show opening prompt prompt = Text ( \" Observe the following structure \" , color = WHITE , font_size =36 ) . move_to ( ORIGIN ) self . play ( FadeIn ( prompt ) , run_time =0.5) self . wait (1.5) self . play ( FadeOut ( prompt ) , run_time =0.5) self . wait (1) # Create initial grid of cubes rows , cols , depth = self . grid_size cubes_vgroup = VGroup () cube_list = [ [[ None for in range ( depth ) ] for in range ( cols ) ] for in range ( rows ) ] colors = [ [[ None for in range ( depth ) ] for in range ( cols ) ] for in range ( rows ) ] unique_colors = set () for in range ( rows ) : for in range ( cols ) : for in range ( depth ) : color = random . choice ( list ( VALID_COLORS . keys () ) ) cube = Cube ( side_length =0.75) cube . set_fill ( color = VALID_COLORS [ color ] , opacity =1) cube . set_stroke ( color = VALID_COLORS [ color ] , width =2) cube . x_idx = cube . y_idx = cube . z_idx = cube . shift (0.75 * ( * RIGHT + * UP + * OUT ) ) cube_list [ ][ ][ ] = cube colors [ ][ ][ ] = color cubes_vgroup . add ( cube ) unique_colors . add ( color ) max_height = config . frame_height * 0.6 # 5% buffer top and bottom if cubes_vgroup . height > max_height : cubes_vgroup . scale ( max_height / cubes_vgroup . height ) cubes_vgroup . move_to ( ORIGIN ) # Create and start rotating cube structure cubes_vgroup . rotate (45 * DEGREES , axis = UP ) cubes_vgroup . rotate (20 * DEGREES , axis = RIGHT ) cubes_vgroup . add_updater ( lambda , dt : . rotate (1 * dt , axis = UP , about_point = ORIGIN ) ) self . play ( Write ( cubes_vgroup ) , runtime =1) self . wait (1) # Randomly remove cubes heights = [[ depth - 1 for in range ( cols ) ] for in range ( rows ) ] iters = 0 ube s_to_remove = set () idx s_to_remove = set () done = False while iters < self . max_iters and not done : for row in range ( rows ) : for col in range ( cols ) : old_height = heights [ row ][ col ] if random . uniform (0 , 1) < 0.5: new_height = random . randint (0 , heights [ row ][ col ]) else : new_height = old_height heights [ row ][ col ] = new_height 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 51 MORSE: Multimodal Reasoning Stress-test Environment for layer in range ( old_height , new_height - 1 , -1) : cubes_to_remove . add ( cube_list [ row ][ col ][ layer ]) idxs_to_remove . add (( row , col , layer ) ) if len ( cubes_to_remove ) == self . n_removed : done = True break if done : break if done : break iters += 1 if not done : raise ValueError ( \" Failed to find valid configuration in { iters } attempts \" ) # Fade out cubes with updater vt = ValueTracker (10) for cube in cubes_to_remove : cube . add_updater ( lambda : . set_opacity ( vt . get_value () / 10) ) self . play ( vt . animate . set_value (0) , run_time =1.5 , rate_func = smooth ) for cube in cubes_to_remove : cube . clear_updaters () cubes_vgroup . remove (* cubes_to_remove ) unique_colors = list ( unique_colors ) color = random . choice ( unique_colors ) = random . randint (1 , 4) # Create random variants to actual missing shape correct = idxs_to_remove all_idxs = [ (i , , ) for in range ( rows ) for in range ( cols ) for in range ( depth ) ] avail_to_add = set ( all_idxs ) - correct variants = [] while len ( variants ) < 3: # random numbers to add and remove na = random . randint ( 1 , min ( len ( avail_to_add ) , max (1 , len ( correct ) ) ) // 4 ) nr = ( random . randint (1 , min ( len ( correct ) , max (1 , len ( correct ) - 1) ) // 4) if len ( correct ) > 1 else 1 ) to_add = set ( random . sample ( avail_to_add , na ) ) to_remove = set ( random . sample ( correct , nr ) ) variant = ( correct - to_remove ) to_add if variant != correct and variant not in variants : variants . append ( variant ) options = [ correct ] + variants random . shuffle ( options ) labels = [ \" \" , \" \" , \" \" , \" \" ] # Draw each option as mini - diagram at the bottom option_groups = VGroup () positions = [ LEFT * 3 , LEFT , RIGHT , RIGHT * 3] for pos , inds , lbl in zip ( positions , options , labels ) : mini = VGroup () for , , in inds : = Cube ( side_length =0.2) . set_fill ( colors [ ][ ][ ] , opacity =1) . set_stroke ( colors [ ][ ][ ] , width =1) . shift ( * 0.2 * RIGHT + * 0.2 * UP + * 0.2 * OUT ) mini . add ( ) mini . scale (0.8) mini . move_to ( pos + DOWN * 3) mini . rotate ( -45 , axis = UP ) mini . rotate (20 , axis = RIGHT ) mini . add_updater ( lambda , dt : . rotate (1.57 * dt , axis = UP ) ) label = Text ( lbl ) . scale (0.2) . next_to ( mini , UP ) option_groups . add ( VGroup ( mini , label ) ) 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 MORSE: Multimodal Reasoning Stress-test Environment para = Paragraph (* lines , alignment = \" center \" , font_size =36 , line_spacing =0.8) para . move_to ( ORIGIN ) if para . width > 0.9 * config . frame_width : self . wait (3) self . play ( para . animate . to_edge ( UP , buff =0.2 * config . frame_height ) ) para . al _t _ fi _w th ( config . frame_width * 0.9) self . _ e _ _ m _ j s ( para ) self . play ( Write ( para ) , run_time =1.5) option_groups . to_edge ( DOWN ) self . answer = labels [ options . index ( correct ) ] option_groups . l _ _f _ g ( config . frame_height * 0.4) option_groups . al _ to _ fi _ wi th ( config . frame_width * 0.9) # Show answer page title self . play (*[ FadeOut ( mob ) for mob in self . mobjects if mob != bg ]) title_text = random . choice ( self . cfg [ \" text \" ][ self . p_type ]) title_text = title_text . replace ( \" <C > \" , color ) title_text = title_text . replace ( \" <N > \" , str ( ) ) lines = title_text . split ( \" \" ) 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def cre ate_p roblem ( grid_size , p_type , p_removed , path , file_name ) : 222 223 224 225 226 227 228 229 230 231 if __name__ == \" __main__ \" : 232 233 234 235 236 os . makedirs ( \" media / videos /1080 p60 / cubes / questions \" , exist_ok = True ) os . makedirs ( \" media / videos /1080 p60 / cubes / solutions \" , exist_ok = True ) os . makedirs ( \" media / videos /1080 p60 / cubes / question_text \" , exist_ok = True ) self . wait (3) self . question_text = \" Observe the following structure . {} \" . format ( self . wait (0.5) self . play ( Write ( option_groups , run_time =1) ) title_text . replace ( \" \" , \" \" ) . write ( scene . question_text ) . write ( str ( scene . answer ) ) ) config . output_file = \" { path }/ questions /{ file_name } \" scene = Cubes ( p_type , grid_size , p_removed ) scene . render () with open ( \" media / videos /1080 p60 /{ path }/ solutions /{ file_name }. txt \" , \" \" ) as : with open ( \" media / videos /1080 p60 /{ path }/ question_text /{ file_name }. txt \" , \" \" ) as : cre ate_ problem ((5 , 5 , 3) , \" missing_shape \" , 0.45 , \" cubes \" , \" missing_shape \" ) # Change grid params as needed 53 MORSE: Multimodal Reasoning Stress-test Environment B.6. Temporal Reasoning Temporal Reasoning - Dominoes Query: Answer the question in this video. Question Text: How many dominoes fell before the fourth green domino? Ground truth:"
        },
        {
            "title": "Code",
            "content": "The python code summarization is pasted below, the full code is available on github. \" What event caused the dominoes to change direction ? \" , \" Did the direction change occur before or after the 30 th domino fell ? \" , \" Insert your third question here \" , \" Insert your fourth question here \" , # 1) Question text question_text = \" How many dominoes were red ? \" question = Text ( question_text , font_size =24) . to_edge ( UP ) def construct ( self ) : 1 from manim import * 2 3 # List your question texts here ( one per video ) 4 i _ s n _ t = [ 5 6 7 8 9 ] 10 11 from manim import * 12 import random 13 14 class domino1 ( Scene ) : 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 2) Parameters num_dominoes = 30 turn_index = 20 domino_width = 0.2 domino_height = 0.6 spacing = 0. # determine position on screen = -6 + * spacing = 0 # 4) Build and display dominoes one by one dominoes = [] for in range ( num_dominoes ) : # 3) Precompute which 7 positions will be red red_positions = set ( random . sample ( range ( num_dominoes ) , 7) ) 54 MORSE: Multimodal Reasoning Stress-test Environment # choose color color = RED if in red_positions else random . choice ([ BLUE , GREEN , YELLOW , PURPLE , ORANGE , TEAL , PINK # create domino domino = Rectangle ( width = domino_width , height = domino_height , fill_opacity =1 , color = color ) . move_to ([ , , 0]) dominoes . append ( domino ) # animate its appearance self . play ( FadeIn ( domino ) , run_time =0.05) # 5) Wait moment before the chain reaction self . wait (0.2) # 6) Simulate the dominoes falling in order for , domino in enumerate ( dominoes ) : # choose falling direction based on branch angle = - PI /2 if < turn_index else PI /2 # pivot around the bottom - left edge pivot = domino . get_left () self . play ( Rotate ( domino , angle = angle , about_point = pivot ) , run_time =0.1) # 7) Done self . wait (1) self . play ( FadeIn ( question ) ) def construct ( self ) : # 1) Question text question_text = \" How many dominoes were green ? \" question = Text ( question_text , font_size =24) . to_edge ( UP ) # 2) Parameters num_dominoes = 30 turn_index = 25 domino_width = 0.2 domino_height = 0.6 spacing = 0.3 # 3) Precompute which 7 positions will be red ree n_positions = set ( random . sample ( range ( num_dominoes ) , 3) ) # 4) Build and display dominoes one by one dominoes = [] for in range ( num_dominoes ) : if < turn_index : = -6 + * spacing = 0 = -6 + ( turn_index - 1) * spacing = -( - turn_index + 1) * spacing # choose color color = GREEN if in green_positions else random . choice ([ BLUE , GREEN , YELLOW , PURPLE , ORANGE , TEAL , PINK # create domino domino = Rectangle ( width = domino_width , height = domino_height , fill_opacity =1 , color = color ) . move_to ([ , , 0]) dominoes . append ( domino ) # animate its appearance self . play ( FadeIn ( domino ) , run_time =0.05) # 5) Wait moment before the chain reaction ]) 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class domino2 ( Scene ) : 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 else : ]) 55 MORSE: Multimodal Reasoning Stress-test Environment # 6) Simulate the dominoes falling in order for , domino in enumerate ( dominoes ) : # choose falling direction based on branch angle = - PI /2 if < turn_index else PI /2 # pivot around the bottom - left edge pivot = domino . get_left () self . play ( Rotate ( domino , angle = angle , about_point = pivot ) , run_time =0.1) # 7) Done self . wait (1) self . play ( FadeIn ( question ) ) def construct ( self ) : # 1) Question text question_text = \" How many dominoes fell before the fourth green domino ? \" question = Text ( question_text , font_size =24) . to_edge ( UP ) # 3) Precompute which 7 positions will be red ll w_posi tions = set ( random . sample ( range ( num_dominoes ) , 3) ) # 4) Build and display dominoes one by one dominoes = [] for in range ( num_dominoes ) : if < turn_index : = -6 + * spacing = = -6 + ( turn_index - 1) * spacing = -( - turn_index + 1) * spacing # choose color color = YELLOW if in yello w_positions else random . choice ([ BLUE , GREEN , YELLOW , PURPLE , ORANGE , TEAL , PINK # create domino domino = Rectangle ( width = domino_width , height = domino_height , fill_opacity =1 , color = color ) . move_to ([ , , 0]) dominoes . append ( domino ) # animate its appearance self . play ( FadeIn ( domino ) , run_time =0.05) # 5) Wait moment before the chain reaction self . wait (0.2) # 6) Simulate the dominoes falling in order for , domino in enumerate ( dominoes ) : # choose falling direction based on branch angle = - PI /2 if < turn_index else PI /2 # pivot around the bottom - left edge pivot = domino . get_left () self . play ( Rotate ( domino , angle = angle , about_point = pivot ) , run_time =0.1) # 2) Parameters num_dominoes = 30 turn_index = 25 domino_width = 0.2 domino_height = 0.6 spacing = 0.3 self . wait (0.2) else : 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class domino3 ( Scene ) : 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 # 7) Done self . wait (1) ]) # 2) Parameters num_dominoes = 10 turn_index = 11 domino_width = 0.2 domino_height = 0.6 spacing = 0.3 56 MORSE: Multimodal Reasoning Stress-test Environment # 3) Precompute which 7 positions will be red ll w_posi tions = set ( random . sample ( range ( num_dominoes ) , 2) ) # 4) Build and display dominoes one by one dominoes = [] for in range ( num_dominoes ) : if < turn_index : = 2 + * spacing = 0 = -6 + ( turn_index - 1) * spacing = -( - turn_index + 1) * spacing # choose color color = GREEN if in yello w_positions else random . choice ([ BLUE , GREEN , YELLOW , PURPLE , ORANGE , TEAL , PINK # create domino domino = Rectangle ( width = domino_width , height = domino_height , fill_opacity =1 , color = color ) . move_to ([ , , 0]) dominoes . append ( domino ) # animate its appearance self . play ( FadeIn ( domino ) , run_time =0.05) # 5) Wait moment before the chain reaction self . wait (0.5) # 6) Simulate the dominoes falling in order for , domino in enumerate ( dominoes ) : # choose falling direction based on branch angle = - PI /2 if < turn_index else PI /2 # pivot around the bottom - left edge pivot = domino . get_left () self . play ( Rotate ( domino , angle = angle , about_point = pivot ) , run_time =0.1) # 1) Question text question_text = \" How many dominoes fell before the fall of the second red domino ? \" question = Text ( question_text , font_size =24) . to_edge ( UP ) self . play ( FadeIn ( question ) ) def construct ( self ) : # 2) Parameters num_dominoes = 30 turn_index = 20 domino_width = 0.2 domino_height = 0.6 spacing = 0.3 # 3) Precompute which 7 positions will be red red_positions = set ( random . sample ( range ( num_dominoes ) , 7) ) # 4) Build and display dominoes one by one dominoes = [] for in range ( num_dominoes ) : # determine position on screen = -6 + * spacing = 0 # choose color color = RED if in red_positions else random . choice ([ BLUE , GREEN , YELLOW , PURPLE , ORANGE , TEAL , PINK # create domino domino = Rectangle ( width = domino_width , height = domino_height , ]) else : 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 class domino4 ( Scene ) : 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 # 7) Done self . wait (1) ]) 57 MORSE: Multimodal Reasoning Stress-test Environment 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 fill_opacity =1 , color = color ) . move_to ([ , , 0]) dominoes . append ( domino ) # animate its appearance self . play ( FadeIn ( domino ) , run_time =0.05) # 5) Wait moment before the chain reaction self . wait (0.2) # 6) Simulate the dominoes falling in order for , domino in enumerate ( dominoes ) : # choose falling direction based on branch angle = - PI /2 if < turn_index else PI /2 # pivot around the bottom - left edge pivot = domino . get_left () self . play ( Rotate ( domino , angle = angle , about_point = pivot ) , run_time =0.1) # 7) Done self . wait (1) self . play ( FadeIn ( question ) ) 58 MORSE: Multimodal Reasoning Stress-test Environment C. More Qualitative Results We present reasoning CoTs from various frontier VLMs: Qwen3-235B-A22B, o3, Gemini 2.5 Pro. Samples are selected to represent each type of reasoning category. For failure modes, incorrect reasoning is highlighted in red. C.1. Abstract Reasoning Here we show the full thinking trace for Qwen3-235B-A22B and Gemini-2.5-Pro-Preview 05-06. For o3 we only present some of the key CoTs due to its excessively long reasoning and function calling. The full trace can be found in https://chatgpt.com/share/68301c0a-78a4-8003-aeff-eeb5cee85221. Abstract Reasoning - ARC-AGI2 Query: Answer the question in this video. Question Text: How many blue squares should appear in the output grid? Answer with single integer Ground truth: 118 Qwen3-235B-A22B"
        },
        {
            "title": "Thinking",
            "content": "Okay, lets see. The video starts by showing color chart with different colors labeled. Then there are some pixelated images that change from one to another. The last frame asks how many blue squares should appear in the output. First, need to figure out the pattern or rule thats being applied to the pixelated images. Let me look at the examples given. The first input is blue square with some black pixels around it. The output shows the same blue square but with some blue pixels added in the corners. Then another example where the input is blue square with smaller blue square inside, and the output adds more blue pixels around the smaller one. The third example has blue shape thats more spread out, and the output seems to fill in some areas. 59 MORSE: Multimodal Reasoning Stress-test Environment Wait, maybe the rule is that each time, the number of blue squares increases by one. Let me check. In the first example, the input has one blue square, and the output has two. The second input has two blue squares, and the output has three. The third input has three blue squares, so the output should have four. But wait, the last frame shows an input with three blue squares and asks how many should be in the output. So following this pattern, the answer would be four. But should make sure theres not another pattern. Maybe its about the arrangement or the direction of the squares. But looking at the examples, each time the number increases by one. So the answer is probably four blue squares. The video presents series of pixelated images where the number of blue squares increases by one in each subsequent output. Observing the examples: 1. First Input : 1 blue square Output : 2 blue squares. 2. Second Input : 2 blue squares Output : 3 blue squares. 3. Third Input : 3 blue squares Output : ? Following the established pattern, the next output should contain 4 blue squares . Answer: 4 (Incorrect) o3 Thought for 9m 17s Im thinking about how to inspect the frames in the video. The goal here is to understand what the video shows, so maybe Ill examine the first few frames or perhaps the last frame where the question appears. It seems likely that the text was graphically created, so Ill check for that. Ill load the video and start with the frames to see what stands out and where the relevant visual or text elements are located. [. . . ] (1615 words omitted) It seems like the task involves counting items, possibly within grid of shapes, and the question could be something like, How many black squares are there? The answer might be number, like \"4.\" To be sure, Id probably need some clarification, but the user didnt ask for that. So, think it would be best to use OCR to detect text in the frames. Ill process the frames at low resolution to make it easier and redefine the video path as needed. [. . . ] (2921 words omitted) It seems the user might be expecting direct answer as part of an evaluation. Based on analysis, think 3 is reasonable guess, but Im cautious because cant visually confirm the content in the video. Even though could extract frames, the user simply asked for an answer, not video frames. Ill mention Done 60 MORSE: Multimodal Reasoning Stress-test Environment 3 (Incorrect) Gemini-2.5-Pro-Preview Interpreting Visual Data Im working on understanding the videos core question. Ive noted the color names and the grid examples, hoping to extract pattern or rule. Now, Im focusing on the input/output grid pairs, trying to find correlation between them. Perhaps they encode color mapping rules? Im examining each grid, seeking transformations."
        },
        {
            "title": "Unveiling Grid Transformations",
            "content": "Im now zeroing in on the hypothesized rule involving isolated black pixels on blue background. Im actively testing the proposed transformation rules with the example grids. Im carefully analyzing how the light blue borders are drawn around the isolated black pixels, while paying close attention to the preservation of existing black regions. Im checking if the proposed rule accurately explains the observed transformations in the examples."
        },
        {
            "title": "Revising Rule Assumptions",
            "content": "[. . . ] (1719 words omitted)"
        },
        {
            "title": "Counting Blue to Light Blue",
            "content": "Ive determined there are five trigger pixels, each causing 8 blue pixels to become light blue. My previous misinterpretation of \"isolated\" black pixels led to some confusion. With confidence, Im ready to calculate the total number of light blue squares."
        },
        {
            "title": "Decoding Pixel Patterns",
            "content": "Im now counting the total number of light blue squares generated. The test input has five isolated black pixels; each triggers an 8-pixel change to light blue based on Example 2 logic. am now certain of the solution. The rule for the transformation, derived from the examples, is as follows: 1. Identify Trigger Pixels: black pixel in the input grid is \"trigger pixel\" if and only if all 8 of its surrounding neighbors (horizontally, vertically, and diagonally) are blue. Black pixels that are part of larger black structures (i.e., have at least one black neighbor) are not triggers and remain unchanged (they are \"inert\"). 2. Transformation Rule: Each trigger black pixel remains black in the output. All 8 of its neighboring blue pixels (which, by definition of trigger pixel, must all be blue) change to light blue in the output. Blue pixels that are not neighbors of any trigger black pixel remain blue. 61 MORSE: Multimodal Reasoning Stress-test Environment Inert black pixels remain black and do not cause any changes to their neighbors. Lets apply this to the test input grid. The grid is 10x10. denotes black, denotes blue. Input Grid: Row 0: K K Row 1: K B Row 2: K K Row 3: K B Row 4: B B Row 5: B K Row 6: B B Row 7: K B Row 8: K K Row 9: K B Step 1: Count initial blue squares. Row 1: 4 blue Row 2: 3 blue Row 3: 4 blue Row 4: 10 blue Row 5: 7 blue Row 6: 10 blue Row 7: 4 blue Row 8: 3 blue Row 9: 4 blue Total initial blue squares = 4 + 3 + 4 + 10 + 7 + 10 + 4 + 3 + 4 = 49. Step 2: Identify trigger black pixels. We need to find black pixels (K) whose 8 neighbors are all blue (B). at (2,7): Neighbors: (1,6)B, (1,7)B, (1,8)B, (2,6)B, (2,8)B, (3,6)B, (3,7)B, (3,8)B. All 8 are blue. This is trigger pixel. at (5,1): Neighbors: (4,0)B, (4,1)B, (4,2)B, (5,0)B, (5,2)B, (6,0)B, (6,1)B, (6,2)B. All 8 are blue. This is trigger pixel. at (5,4): Neighbors: (4,3)B, (4,4)B, (4,5)B, (5,3)B, (5,5)B, (6,3)B, (6,4)B, (6,5)B. All 8 are blue. This is trigger pixel. at (5,7): Neighbors: (4,6)B, (4,7)B, (4,8)B, (5,6)B, (5,8)B, (6,6)B, (6,7)B, (6,8)B. All 8 are blue. This is trigger pixel. at (8,7): Neighbors: (7,6)B, (7,7)B, (7,8)B, (8,6)B, (8,8)B, (9,6)B, (9,7)B, (9,8)B. All 8 are blue. This is trigger pixel. Other black pixels (e.g., at (0,0), (1,0)-(1,5), etc.) are part of larger black structures and have black neighbors, so they are not trigger pixels. There are 5 trigger black pixels. Step 3: Determine how many blue squares change to light blue. Each trigger black pixel causes its 8 blue neighbors to change to light blue. We need to check if the 3x3 regions of influence (centered on each trigger pixel) overlap. 62 MORSE: Multimodal Reasoning Stress-test Environment Region 1 (around (2,7)): Rows 1-3, Cols 6-8. Region 2 (around (5,1)): Rows 4-6, Cols 0-2. Region 3 (around (5,4)): Rows 4-6, Cols 3-5. Region 4 (around (5,7)): Rows 4-6, Cols 6-8. Region 5 (around (8,7)): Rows 7-9, Cols 6-8. These regions are all disjoint. Therefore, the sets of blue pixels changing to light blue are also disjoint. Number of blue squares changing to light blue = 5 trigger pixels * 8 blue neighbors/trigger = 40. Step 4: Calculate the number of blue squares remaining in the output. Number of blue squares in output = (Initial blue squares) - (Blue squares changed to light blue) Number of blue squares in output = 49 - 40 = 9. So, 9 blue squares should appear in the output. Final Answer: The final answer is 9 (Incorrect) Figure C.1: Example failure case in Abstract Reasoning using ARC-AGI pattern recognition. Three frontier models attempt to count blue squares in transformed grid following pattern rules. Despite detailed reasoning traces, all models fail: Qwen incorrectly applies simple increment pattern (answer: 4), o3 struggles with holistic visual understanding and provides most verbose yet ineffective reasoning leading to an arbitrary guess (answer: 3), and Gemini performs complex rule analysis but miscounts the final result (answer: 9). The correct answer is 118, demonstrating the challenge of visual pattern recognition and rule application in abstract reasoning tasks. 63 MORSE: Multimodal Reasoning Stress-test Environment C.2. Mathematical Reasoning Mathematical Reasoning - Radius Increase Query: Answer the question in this video. Question Text: By what factor was the radius scaled? Ground truth: 0.75 Qwen2-235B-A22B Okay, lets see. [. . . ] First, examine the starting figure: horizontal baseline marked with tick marks, and semicircle drawn above this line. The baseline represents the diameter of that semicircle. Next, the semicircle is transformed into full circle. Crucially, the baselines length does not change, so it now equals the diameter of the entire circle. semicircle is exactly half of full circle. To turn it into full circle, the vertical dimension (i.e. the radius) must double while the horizontal diameter stays fixed. Hence the radius is scaled by factor of 2. Answer: 2 The user-supplied video (8 at 60 fps) appears to show circle that grows rapidly in the first second and then stays large. To uncover the question, Sampled frames at 1 fps and ran pytesseract OCR. From frame 360 onward the overlay text consistently reads: By what factor was the radius scaled? Detected circles in each frame with Hough transform. Early frames (#2030) contain several concentric circles; the largest reliable radius is about 331 px. From frame 40 onward the dominant radius jumps to about 961 px. Computed the ratio: 961 331 2.90. Small measurement noise and pixel quantisation suggest the intended factor is exactly 3. 64 MORSE: Multimodal Reasoning Stress-test Environment Hence the video shows the original circle instantly expanding so its radius triples. Answer: 3 Gemini-2.5-Pro-Preview Okay, lets see. [. . . ] 00:00 00:01 An upper semicircle is drawn. It meets the x-axis at roughly = 4 and = +4, with its apex at (0, 4). That implies full circle of radius 4 centred at the origin. 00:01 00:03 The animation completes the lower semicircle, forming full circle that still spans = 4 to = +4 and = 4 to = +4. Thus the final circles radius is also 4. Key observation: Both the original (implied by the first semicircle) and the final circle share the same radius, so no size change occurred. Scaling calculation: Answer: Scaling factor = Final radius Original radius = 4 4 = 1. Figure C.2: Example failure case in Mathematical Reasoning involving radius scaling. Three frontier models attempt to determine the scaling factor when semicircle transforms into full circle. All models fail to correctly identify the geometric transformation: Qwen incorrectly reasons about doubling the radius (factor of 2), o3 uses image analysis to detect circle expansion (factor of 3), and Gemini claims no size change occurred (factor of 1). The correct answer is 0.75, representing the radius reduction needed when completing semicircle into full circle with the same diameter. MORSE: Multimodal Reasoning Stress-test Environment C.3. Physical Reasoning Here we show the full thinking trace for Qwen3-235B-A22B and Gemini-2.5-Pro-Preview 05-06. For o3 we only present some of the key CoTs due to its excessively long reasoning and function calling. The full trace can be found in https://chatgpt.com/share/683022c2-1400-8003-8f66-94f26ddaa311. All models struggle with distinguishing the physical nuances of different scenarios. Physical Reasoning - Tennis Rolls Towards Mirror Query: Answer the question in this video. Question Text: 30lb kettlebell resting on wooden table next to mirror. tennis ball rolls towards the kettlebell. Static shot with no camera movement. Which of the earlier examples aligns with the principles of physics in the context provided? Use single letter to respond (e.g., J). If none of them are realistic, return None. Ground truth: Qwen3-235B-A22B Thinking Okay, lets see. The question is asking which of the earlier examples aligns with the principles of physics in the context provided. The setup is 30lb kettlebell resting on wooden table next to mirror, and tennis ball rolls towards the kettlebell. The shot is static with no camera movement. First, need to look at the images labeled to and figure out which one makes sense physically. Let me go through each image step by step. In image A, theres one tennis ball moving towards the kettlebell. That seems straightforward. If ball rolls towards stationary object, it should hit it unless something else happens. 66 MORSE: Multimodal Reasoning Stress-test Environment Image shows the ball bouncing off the kettlebell. But wait, if the kettlebell is heavy (30lb) and stationary, the ball would just bounce off with some force. However, since the table is wooden and the surface might be smooth, the ball could roll away after hitting the kettlebell. But in B, the ball is shown mid-air, which might not be accurate because the collision would cause it to bounce, but maybe not that high. Also, the position of the ball relative to the kettlebell needs to make sense. Image has two balls. One is near the kettlebell, and another is rolling towards it. But the original setup mentions only one tennis ball. So maybe is incorrect because there are two balls here. Unless the first ball split or something, but thats not physical. So might be out. Image shows two balls again, one near the kettlebell and another further away. Similar to C, this introduces an extra ball, which isnt part of the original scenario. So is probably not correct. Image has one ball near the kettlebell and another smaller one rolling towards it. Again, two balls, so same issue as and D. Unless the small one is reflection, but the mirror is on the right side, and the reflection would show the balls image, not another ball. So is likely wrong. Image shows one ball rolling towards the kettlebell, similar to A, but the position is slightly different. The ball is closer to the kettlebell here. But does the path make sense? If the ball is rolling straight towards the kettlebell, it should hit it. However, in F, the ball is still moving towards it, so maybe its just matter of timing. But the question is about which image aligns with physics. Wait, the key point here is that the kettlebell is heavy and stationary. When the tennis ball collides with it, the ball should bounce back because the kettlebell doesnt move much. So in image B, the ball is shown bouncing off, which would be expected. However, the height of the bounce might be an issue. If the collision is elastic, the ball would bounce back with the same speed, but the height depends on the angle and surface. But since the table is flat, the ball might not bounce very high. Alternatively, if the ball hits the kettlebell and then rolls away, thats also possible. But looking at image B, the ball is in the air, which suggests it bounced. However, if the collision is head-on, the ball would reverse direction but stay on the table. So maybe image is not accurate because the ball is shown too high in the air. Alternatively, if the ball hit the edge of the kettlebell and bounced sideways, that could be possible. But the images dont show the direction of the bounce. Alternatively, image shows the ball still approaching the kettlebell, which would be before the collision. So if the question is about the moment after the collision, then would be the answer. But the problem is that the question says \"a tennis ball rolls towards the kettlebell. Static shot with no camera movement.\" So the image should capture the state after the ball has rolled towards the kettlebell. If the ball has already collided, then is the result. But if the ball is still rolling towards it, then or F. Wait, the initial setup is that the ball is rolling towards the kettlebell. The static shot is taken when the ball is in motion. So the correct image would be one where the ball is in the process of rolling towards the kettlebell, not after the collision. But the options include images where the ball has already hit the kettlebell (like B) or is still approaching (A, F). But the question is which example aligns with physics. If the ball is rolling towards the kettlebell, then the physical principle is that the ball will continue moving until it hits the kettlebell. So the MORSE: Multimodal Reasoning Stress-test Environment image should show the ball in motion towards the kettlebell. However, in the images, and show the ball moving towards the kettlebell, while shows it bouncing off. But the problem is that the question is asking which of the earlier examples (A-F) aligns with the principles of physics. The correct answer would be the one where the ball is moving towards the kettlebell, as thats the described scenario. However, if the ball has already hit the kettlebell, then would be the result, but the setup says the ball is rolling towards it, implying that the collision hasnt happened yet. Wait, the description says \"a tennis ball rolls towards the kettlebell. Static shot with no camera movement.\" So the image should capture the ball in the act of rolling towards the kettlebell. Therefore, the correct image would be one where the ball is in motion towards the kettlebell, not after the collision. So images A, C, E, show the ball moving towards the kettlebell, while and show after the collision. But among A, C, E, F, which ones are physically correct? The original setup has one ball. So C, D, have two balls, which is incorrect. So only and are left. In A, the ball is moving towards the kettlebell. In F, the ball is closer but still moving towards it. Both are possible, but the question is which one aligns with physics. Since the ball is rolling, the image should show the ball in position thats consistent with its motion. However, without knowing the exact timing, its hard to say. But the answer is likely or F. Wait, but the problem might be that in some images, the balls position relative to the kettlebell is impossible. For example, if the ball is too close to the kettlebell in F, but given that its rolling, its possible. Alternatively, the answer could be if the collision is considered. But the setup says the ball is rolling towards the kettlebell, not that it has collided. So the static shot is of the ball in motion towards the kettlebell. Therefore, the correct image would be or F. But looking back at the images, in A, the ball is on the left side, moving towards the kettlebell. In F, the ball is closer to the kettlebell but still moving towards it. Since the question is about which example aligns with physics, both and are possible, but perhaps the answer is because its closer, indicating the ball is in the process of rolling. However, without more information, its hard to tell. Wait, the text after the images says: \"A 30lb kettlebell resting on wooden table next to mirror. tennis ball rolls towards the kettlebell. Static shot with no camera movement.\" So the correct image should show the ball in motion towards the kettlebell. The images that show the ball moving towards the kettlebell are A, C, E, F. But C, have two balls, which is incorrect. So and F. Between and F, which one is correct? The answer might be because the ball is closer to the kettlebell, indicating its further along in its roll. But the problem is that the question is about which image aligns with physics. Since the ball is rolling, the image should show the ball in position thats consistent with its motion. However, all images are static, so they just capture moment. Alternatively, the answer could be if the collision is part of the scenario. But the setup says the ball is rolling towards the kettlebell, not that it has collided. So the correct image is one where the ball is in motion towards the kettlebell. Therefore, the answer is or F. 68 MORSE: Multimodal Reasoning Stress-test Environment But looking at the images again, in A, the ball is on the left, moving towards the kettlebell. In F, the ball is closer but still moving towards it. The problem is that the answer is supposed to be single letter. The user might expect that the correct answer is because after the collision, the ball bounces, but the setup says the ball is rolling towards the kettlebell, not after the collision. Wait, the question is phrased as \"Which of the earlier examples aligns with the principles of physics in the context provided?\" The context is 30lb kettlebell on table next to mirror, tennis ball rolls towards the kettlebell, static shot with no camera movement. So the image should show the ball in the process of rolling towards the kettlebell. Therefore, the correct image is or F. But why are there multiple images? Maybe the answer is because the ball is closer, indicating its further along in its path. However, without knowing the exact timing, its hard to say. Alternatively, the answer could be None if none of the images are correct. But that seems unlikely. Wait, looking at image B, the ball is shown in the air, which would be the result of the collision. But if the ball is rolling towards the kettlebell, the collision hasnt happened yet. So is after the event. The correct image should be before the collision. Therefore, the answer is or F. But the problem is that the user might expect the answer to be because thats the result of the physics (the ball bouncing off). But the setup says the ball is rolling towards the kettlebell, not that it has collided. So the answer should be an image where the ball is in motion towards the kettlebell. Given that, the answer is either or F. But which one? The images are labeled to F, and the user might have intended one of them. Since the ball is rolling, the image should show the ball in position thats consistent with its motion. However, all images are static, so they just capture moment. Alternatively, the answer could be None if none of the images correctly depict the scenario. But thats not likely. Wait, the text after the images says: \"Use single letter to respond (e.g., J). If none of them are realistic, return None.\" So if none of the images are correct, the answer is None. But lets re-examine the images. Image A: One ball moving towards the kettlebell. This is possible. Image B: Ball bouncing off. This would be after the collision. Image C: Two balls. Incorrect. Image D: Two balls. Incorrect. Image E: Two balls. Incorrect. Image F: One ball moving towards the kettlebell, closer than in A. MORSE: Multimodal Reasoning Stress-test Environment So between and F, which is correct? The answer depends on the exact moment captured. Since the question is about which aligns with physics, both and are possible, but the answer is likely because its closer, indicating the ball is in the process of rolling. However, without more information, its hard to be certain. Alternatively, the answer could be if the collision is considered part of the scenario. But the setup says the ball is rolling towards the kettlebell, not that it has collided. Therefore, the correct answer is either or F. But since the user expects single letter, and considering that shows the ball closer to the kettlebell, which is more in line with the action of rolling towards it, the answer is F. Wait, but in the images, shows the ball still moving towards the kettlebell, just closer. So yes, is valid depiction. Therefore, the answer is F. The correct answer is F. Explanation: The scenario describes tennis ball rolling towards stationary 30lb kettlebell on wooden table. The static shot must capture the ball in motion towards the kettlebell, not after collision. Images , , and are invalid due to the presence of two balls, violating the setup. Images and show one ball moving towards the kettlebell. While both are plausible, depicts the ball closer to the kettlebell, aligning better with the dynamic of rolling motion. Image incorrectly shows the ball bouncing off the kettlebell, which would occur after the collision, contradicting the described action. Thus, most accurately reflects the physical scenario. Answer: (Incorrect) o3 Thought for 7m 42s need to set up some code to sample frames from video and possibly display them for analysis. To do this, Ill use Python with libraries like cv2 or moviepy. Ill start by extracting few frames and gathering metadata, just for analysis. No need to dive into advanced stuff just yet; this is more about laying the groundwork. Ill focus on being clear on the process and use it to guide what comes next! Analyzed import cv2, os, json, math, numpy as np, base64, PIL.Image as Image, io, textwrap need to write some code to sample frames from video, maybe display them for analysis. Ill use Python with libraries like cv2 or moviepy. First step: Ill need to load the video, then extract metadata and few frames. This will help in analyzing how the data changes over time. Its just the initial step, so no fancy stuff just yet! Lets keep things simple and focused for now. 70 MORSE: Multimodal Reasoning Stress-test Environment video_path = \"/mnt/data/physical_reasoning_physical_commonsense_physics_iq_0156.mp4\" cap = cv2.VideoCapture(video_path) if not cap.isOpened(): print(\"Could not open video\") else: fps = cap.get(cv2.CAP_PROP_FPS) frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) duration = frame_count / fps if fps else 0 print(\"fps\", fps, \"frame_count\", frame_count, \"duration\", duration) # output: fps 30.0 frame_count 240 duration 8.0 To start, Ill write code to sample frames from video, maybe display them for analysis. Ill use Python, with libraries like cv2 or moviepy, to load the video and extract frames at specific times (0s, 2s, 4s, 6s). could save frames to file or possibly convert them to ASCII, but displaying text using PIL might be easier. Another option: try OCR, though its bit heavy for now. The alternative might be analyzing the audio for speech. [. . . ] (2040 words omitted) After showing the scenarios, the text on the final screen has question and list of scenario descriptions. The OCR only captures the first scenario, with others cut off. We need to capture the entire screen at 6.5 seconds, save the image, and extract more text. Ill crop the grayscale image and apply text recognition to capture everything. The question seems to ask which earlier example aligns with physics principles, so should focus on descriptions that refer back to them. [. . . ] (6297 words omitted) Im stuck. was thinking of binarizing the image to locate the \"J\" template, but it seems like my search for the letter didnt work. At this point, the easiest solution might be to say theres no match, but the task still needs letter. Maybe the letter corresponds not to the final scene but to earlier examples? The scenario description at the end might just summarize prior content. It looks like the task involves matching principles of physics, not necessarily focusing on specific letter. [. . . ] (516 words omitted) Ill calculate the pixel difference between frame 0 and 3500 for each tile to detect movement. The middle rows seem to have the highest motion, possibly due to ball scenario. Tile (1,3) stands out because of its high motion, but it might be an unrealistic scenario. Ill inspect it closely and save the tile image for further analysis. Lets see what we get from this! Analyzed r,c=1,3 y=r*tile_h x=c*tile_w 71 MORSE: Multimodal Reasoning Stress-test Environment tile=frame[y:y+tile_h,x:x+tile_w] cv2.imwrite(\"/mnt/data/tile_13.png\", tile) # output: True Lets calculate the pixel"
        },
        {
            "title": "Done",
            "content": "Answer: (Incorrect) Gemini-2.5-Pro-Preview"
        },
        {
            "title": "Assessing Video Relevance",
            "content": "Im focusing on the core physics principles at play: the tennis balls momentum transfer to the kettlebell, the influence of gravity, and the mirrors reflective properties. My current line of thought is to methodically eliminate videos that blatantly misrepresent these factors. This process will highlight those videos with accurate depictions of the physics, and will be able to rule out each video based on whether it adheres to real-world physics principles."
        },
        {
            "title": "Refining the Evaluation Process",
            "content": "Im now refining the evaluation process. Ill need to carefully examine each video, starting with the balls initial motion. The static table and mirror setup offer clear physical reference points. will be looking closely at the balls behavior after impact to determine if theres any movement, and if the reflection in the mirror appears accurate with respect to all of the criteria. My primary focus is to identify any deviations from these basic physics principles, particularly in the impact dynamics and reflective properties. Pinpointing Plausible Examples My current focus is on the impact dynamics and mirror reflections. Im carefully analyzing Example C, which appears most promising so far. The ball rebounds upon impact, and the kettlebell stays put, which is good. The mirror reflection aligns with the real-world physics, but Ill continue evaluating for anything that appears unrealistic. Im focusing on Examples D, E, and next, for any other acceptable answers."
        },
        {
            "title": "Determining Plausible Outcomes",
            "content": "Im now zeroing in on Example C. Its adherence to the basic physics principles is promising. The rebound of the tennis ball and the kettlebells immobility are both realistic. The mirror reflection appears consistent, but will still carefully assess for any other deviations. Ill also evaluate if the bounce of the ball is correct, and not an exaggeration. The focus is to look for any subtle discrepancies, even though it may seem accurate at first glance."
        },
        {
            "title": "Identifying the Correct Video",
            "content": "72 MORSE: Multimodal Reasoning Stress-test Environment Im now fully immersed in the examples, having systematically assessed each based on the principles of momentum and reflection. Example stands out as the most accurate representation. My focus now is solidifying this assessment. Im double-checking the videos details: the initial roll, the impact dynamics (rebound and kettlebell immobility), and, crucially, the mirror reflection. will confidently conclude that Example adheres most closely to the real-world physics principles at play. Answer: (Incorrect) Figure C.3: Example failure case in Physical Reasoning requiring discrimination between realistic and unrealistic physics scenarios. Models must identify which video shows plausible physics for tennis ball rolling toward 30lb kettlebell. Despite lengthy reasoning traces, all models fail: Qwen focuses on ball-kettlebell interaction timing but selects the wrong option (F), o3 struggles with video analysis and provides minimal justification (A), and Gemini attempts systematic physics analysis but misidentifies the realistic scenario (C). The correct answer is E, highlighting difficulties in understanding physical plausibility in dynamic scenes. MORSE: Multimodal Reasoning Stress-test Environment C.4. Planning Reasoning Planning Reasoning - Robot Manipulation Query: Answer the question in this video. Question Text: What is the correct sequence of events? Respond with comma-separated sequence of numbers (e.g. text. \"1,2,3...\"). Dont add any other Ground Truth: 3,2,1 Qwen2-235B-A22B Okay, lets see. [. . . ] (44 words omitted) First, Ill look at each frame. Frame 1 shows the robot arm reaching towards notepad thats held by two white clips. The lamp is turned off. In Frame 2, the robot arm is now holding the notepad, and the lamp is still off. Then Frame 3 shows the robot arm moving away, and the lamp is now on. the robot picks up the notepad (Frame 1 to Frame 2), then So the sequence seems to be: turns on the lamp (Frame 2 to Frame 3). But wait, the lamp being on in Frame 3 might be separate action. Let me check again. In Frame 2, the lamp is still off while the robot is picking up In Frame 1, the lamp is off. the notepad. Then in Frame 3, the lamp is on. So the correct order would be picking up the notepad first (Frames 1-2), then turning on the lamp (Frame 3). Therefore, the sequence should be 1, 2, 3. But wait, maybe the lamp turning on is part of the same action? No, because in Frame 2 the lamp is still off. So the robot first picks up the notepad (1-2), then turns on the lamp (3). So the correct sequence is 1, 2, 3. Answer: 1,2,3 (Incorrect) 74 MORSE: Multimodal Reasoning Stress-test Environment It looks like need to extract frames from the last few seconds of the video to identify the question. Ill try using library to load the video file and extract those frames. Once have the frames, might then try OCR (if available) or simply read the text manually. cant show images to the user, but can inspect the output visuals privately to find the question. Lets proceed with this method and check if OCR works. [. . . ] (586 words omitted) It looks like the question is focused on the order of events, and earlier in the video, they may have labeled events with numbers or shown clips. The answer could be sequence like \"321.\" To find that, should look at frames earlier in the video where labels such as \"1)\", \"2)\", or \"3)\" appear. Ill focus on frames between 300 and 400 to try and capture those numbered events, doing OCR on every 30th frame to reduce noise. cap = cv2.VideoCapture(video_path) numbers_found = [] for fi in range(total_frames): if fi % continue cap.set(cv2.CAP_PROP_POS_FRAMES, fi) ret, frame = cap.read() if not ret: continue gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) text = pytesseract.image_to_string(gray, config='--psm 6') text_clean = text.strip() if any(digit in text_clean for digit in ['1','2','3']): numbers_found.append((fi, text_clean)) cap.release() numbers_found[:20] Im looking for number overlays in frames, focusing on detecting digits like 1, 2, and 3. If sample every second of frames, Ive found 1 in the first frame, 2 at frame 120, and 3 in frame 270. These could mark the segments, but the real question is about the original sequence of events, not just the labels. It seems the user might want to reorder the segments to their original order, 1, 2, 3. The frames are labeled, starting with 1 at frame 0, which suggests no shuffle in the original order. However, considering the earlier script, the frame numbers correspond to shuffled segments, meaning that 1 could appear earlier in the sequence due to the shuffle. The labels reflect this shuffled order, not the original one, so the task is to identify the correct chronological order based on the original segmentation. The user probably expects an answer based on the original event order, though its tricky without the key directly from the shuffle script. [. . . ] (237 words omitted) The labels 1, 2, and 3 appear in order at frames 0, 120, and 270, respectively, suggesting the sequence is \"123\". Theres no earlier 3 or 2 found, and sequence alignment seems confirmed. 75 MORSE: Multimodal Reasoning Stress-test Environment Ive reviewed frames carefully, with 1 appearing first, then 2, and 3 last. Therefore, Im confident that the answer is \"123\". Answer: 1,2,3 (Incorrect) Gemini-2.5-Pro-Preview The video displays three segments, labeled 1, 2, and 3. We need to determine the correct chronological sequence of the events shown in these segments. Segment 1 (0:00-0:03): Shows the robot arm already in contact with notebook, drawing line from its right to its left (right to left on the page from the perspective of something being written conventionally), and then lifting its \"pen.\" This is an action of writing/drawing. Segment 2 (0:04-0:08): Shows the robot arm performing an action nearly identical to Segment 1. Its in contact with the notebook, draws line similarly, and lifts its \"pen.\" This is also an action of writing/drawing. Segment 3 (0:09-0:12): Shows the robot arm initially to the right of the notebook. It then moves over the notebook and positions its \"pen\" down towards the paper, as if to begin writing. This is preparatory action, an approach to start writing. Chronological order of events: The robot arm must first approach the notebook and position itself to write. This action is shown in Segment 3. After positioning, the robot arm performs the writing action. This is shown in Segment 1. If there is subsequent writing action, or if Segment 2 represents continuation or second stroke, it would follow. Segment 2 shows writing action very similar, if not identical, to Segment 1. The preparation/approach (Segment 3) must come first. Then, the writing occurs. Segment 1 shows writing action. Segment 2 also shows writing action. If this is second stroke or continuation, it would follow Segment 1 (assuming the numerical labels 1 and 2 imply an order for these similar actions). Thus, the logical sequence of these clips to form coherent event is: Segment 3 (approach) -> Segment 1 (write first part) -> Segment 2 (write second part/repeat). The sequence of numbers is 312. Answer: 3,1,2 (Incorrect) Figure C.4: Example failure case in Planning Reasoning involving temporal sequence understanding. Models are provided shuffled video from the MimicPlay dataset Wang et al. [2023], and tasked with determining the correct chronological order of the robots actions (switching off light and shelving notebook) from shuffled video segments labeled 1, 2, 3. While Qwen correctly identifies individual frame states, all models fail at sequential reasoning: o3 and Qwen both provide the displayed order (1,2,3) without recognizing the shuffling, while Gemini attempts logical sequencing but incorrectly orders the preparation and execution phases (3,1,2). The correct sequence is 3,2,1. 76 MORSE: Multimodal Reasoning Stress-test Environment C.5. Spatial Reasoning Spatial Reasoning - Count the Cubes Query: Observe the following structure. How many cubes are left? Answer with single integer. Ground Truth: 42 o3 The user uploaded video named \"count_2.mp4\" and wants an answer to question from it. However, this video is not accessible through regular tools here. Ill need to use Python-based approach to extract frames or audio, and maybe process them with available tools like OpenCV and ffmpeg. My goal is to examine the content of the video to locate the question and provide the answer. [. . . ] (92 words omitted) The user likely uploaded video \"count_2.mp4\" with counting question. We dont need to show the frames unless necessary, but will inspect them to understand the question. Ill open the video and check few frames to see what its asking. Ill use tools available, like imageio or OpenCV, to load the video and analyze it. Once we have an idea from the frames, we can provide clearer answer to the users question. import cv2 cap = cv2.VideoCapture('/mnt/data/count_2.mp4') frames = [] fps = cap.get(cv2.CAP_PROP_FPS) frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) duration = frame_count / fps if fps else None 77 MORSE: Multimodal Reasoning Stress-test Environment duration [. . . ] (400 words omitted) wonder if the question in the video is actually contained Okay, lets give this another shot. in the audio. We can extract the audio using ffmpeg (if available) and then perhaps transcribe it. Though dont have access to speech recognition libraries offline, might try to manually guess whats being said. The file is likely small, so it shouldnt take too long to inspect it. Time to load the audio and see what we can figure out! [. . . ] (1695 words omitted) Okay, Im going to plot the red pixel counts over time and print the list of values out. This should help me figure out any patterns, like when red objects appear or disappear. Its simple way to visually track the changes across frames. Hopefully, that will give me clearer idea about the objects in the video and help me make more accurate guess. Lets see what the data shows! for t, rp in zip(times, [red_pixels(f) for in frames_sampled]): print(t, rp) Im going to plot the red pixel counts over time and print the list. The red pixels appear around 4s to 11s, which could indicate red object across several frames. To answer the question, Ill focus on counting the red objects. Lets take look at frame 8 (around t=8), which has high count of red pixels. Ill check for connected components of the red clusters there to help identify distinct red objects. This should give us better idea. [. . . ] (1701 words omitted) It seems like theres pattern of red clusters in the frames where the red objects appear. also notice green clusters, though they might represent background or axis. Maybe the question relates to how many times the red ball hits the green one, but Im not totally sure. The pattern of red clusters isnt strictly increasing or decreasing, it seems bit erratic. Ill print the counts to analyze the sequence. The animation could be showing some kind of calculation too. Lets see what we find."
        },
        {
            "title": "Done",
            "content": "Answer: 6 Qwen3-235B-A22B Okay, lets see. The question is asking how many cubes are left after some structure is observed. The video shows 3D cube made up of smaller cubes. The main cube is divided into smaller sections, each colored differently. First, need to figure out the total number of small cubes in the structure. The cube appears to be 3x3x3 Rubiks cube, which typically has 27 small cubes. But wait, maybe its not 78 MORSE: Multimodal Reasoning Stress-test Environment standard Rubiks cube. Let me check again. Looking at the images, the cube is shown from different angles. Each face has mix of colors, but the structure seems to have layers. If its 3x3x3 cube, there are 3 layers along each axis. So, 3*3*3 equals 27. But sometimes, these puzzles might have fewer cubes if some are missing. However, in the images, the cube looks complete, with all sides filled. Wait, but the question says \"how many cubes are left.\" Maybe some cubes are removed? But in the video frames provided, the cube is intact. Theres no indication of any cubes being taken away. The last frame just asks the question. So perhaps the answer is the total number of small cubes in the structure as shown. If its 3x3x3 cube, the answer would be 27. But let me make sure. Sometimes, these questions can be tricky. For example, if the cube is hollow, the number would be less. But the images show solid cube with colored faces. Each small cube is visible on the surface, and since its 3D structure, the inner cubes are also present. Another way to count: each layer (front, middle, back) has 9 cubes (3x3). Three layers would be 27. Yes, that makes sense. So unless theres trick here, like overlapping or something, the answer should be 27. But the video might be showing different structure. Wait, looking at the third image, the cube is rotating, and it seems like there are some gaps or maybe overlapping cubes? No, actually, its just the perspective making it look like that. The cube is still solid 3x3x3. Therefore, the number of cubes left is 27. The answer should be 27. Answer: Gemini-2.5-Pro-Preview"
        },
        {
            "title": "Assessing Cube Quantity",
            "content": "Im currently focused on determining the missing cubes within the 3x3x3 structure. The videos rotation makes visualization bit tricky, but Im mentally parsing the layers to count the vacancies. plan to refine the count by checking for overlapping voids and hidden blocks."
        },
        {
            "title": "Refining Cube Count",
            "content": "Im now breaking down the structure further, layer by layer, considering each face. At 0:04, it looks like the top layer is missing one cube in the center, and the rotation is helping me clarify the positions. The video timing is making it hard to follow. Analyzing each face individually could provide more precise count."
        },
        {
            "title": "Identifying Cube Voids",
            "content": "Im now counting the existing cubes more meticulously, layer by layer, and trying to extrapolate from the visible faces. Analyzing the structures rotation and combining that with individual face counts, Im noting the apparent locations of missing cubes in each layer. Im focusing on \"seeing through\" the rotation to pin down the gaps. 79 MORSE: Multimodal Reasoning Stress-test Environment"
        },
        {
            "title": "Counting Remaining Cubes",
            "content": "Im now revisiting the structure, attempting precise count of remaining cubes. The initial visual assessment suggests hole running through the center along all three axes. If each axis has 3 missing cubes and they overlap in the central cube, thats 7 cubes missing. Analyzing from 0:04, the top layer appears to be missing center cube, but the structures rotation adds another layer of complexity. Subtracting 7 from the total of 27 cubes is my current method, but must avoid duplicates in the count."
        },
        {
            "title": "Determining Remaining Cubes",
            "content": "If we remove Ive determined structure with seven missing cubes is valid structure. central cross from each layer of cube, we arrive at seven removed cubes: the central one plus one along each of the three axes. This leaves us with 20 cubes. Now, Im verifying this math against the visual cues, as it is rotating. [. . . ] (443 words omitted) Finalizing Structure Count Ive meticulously analyzed the video, confirming that seven cubes are missing from the initial 3x3x3 structure. By removing the central cube and the center of each face, the final count settles at 20 remaining cubes. This result is consistent with my visual inspection and layer-by-layer assessment. Answer: 20 Figure C.5: Example failure case in Spatial Reasoning requiring 3D structure analysis. Models must count remaining cubes in rotating 3D structure after some have been removed. All models struggle with the spatial visualization: o3 attempts pixel-based analysis but grossly underestimates (answer: 6), Qwen assumes complete 333 structure without recognizing removals (answer: 27), and Gemini performs detailed layer analysis but still undercounts significantly (answer: 20). The correct answer is 42, demonstrating challenges in 3D spatial reasoning and occlusion handling. 80 MORSE: Multimodal Reasoning Stress-test Environment C.6. Temporal Reasoning Temporal Reasoning - House Tour Query: Answer the question in this video. Question Text: We see stairs to the basement: Choices: (A) right before entering the kitchen. (B) while learning the kitchen. (C) between walking from the bedroom to the office. (D) right after entering the living room. Respond with single letter (e.g. \"A\"). Dont add any other text. Ground Truth: Qwen3-235B-A22B Okay, lets tackle this question [. . . ] (46 words omitted) First, need to recall the layout of the house shown in the video. The video starts with hallway leading into different rooms. The kitchen is visible early on, with white cabinets and large island. The living room is adjacent, featuring couch, coffee table, and large clock. Theres also dining area near the kitchen. The bedrooms are shown later, with one having bathroom en suite. Now, looking for the stairs. In the initial hallway, theres staircase visible on the right side as you enter. The question is about the basement stairs. From the video, the stairs are seen right when you first enter the house, before reaching the kitchen. The kitchen is further down the hallway, so the stairs are before that. Option says \"Right before entering the kitchen,\" which matches this observation. The other options mention leaving the kitchen, moving between bedroom and office, or entering the living room. The living room is more towards the back, and the stairs are at the entrance. So the correct answer should be A. Answer: (Incorrect) 81 MORSE: Multimodal Reasoning Stress-test Environment o3 Im thinking about how to extract text from video frames. Maybe could use OpenCV or MoviePy to sample the last frames and apply OCR via pytesseract. But, its not clear if the environment supports OCR libraries. simpler approach might be to extract frames using FFmpeg and analyze them with EasyOCR. Since cant display videos, Id have to issue the necessary commands behind the scenes. Handling video content this way seems like the most effective route to answer the users question. [. . . ] (804 words omitted) Im considering using pixel variance to detect text in the frames, but thats not very helpful. could also try transcribing audio with openAI, but that wont work offline. Asking the user directly could be an option, but its better to give OCR shot first. Ill check if Tesseract is installed, then run it on one of the frames to see if we can extract any text! texts=[] for in range(len(frames)): img = Image.open(f\"/mnt/data/frame_{i}.jpg\") texts.append(pytesseract.image_to_string(img)) texts [. . . ] (71 words omitted) for text in texts: print(\"------\") print(text) After running OCR, it seems weve identified that the question asks about the visibility of basement stairs, with options A-D as time points. To answer accurately, Ill need to examine earlier frames to pinpoint when the basement stairs first appear. Unfortunately, automatic detection of the stairs is difficult. Ill likely need to manually inspect the earlier segments of the video and watch for when the stairs become visible. Lets proceed with this plan and check the frames in more detail! The video seems to follow typical real estate walkthrough, and the question asks when the basement stairs first appear. could try detecting stairs through image analysis, but thats... Answer: (Incorrect) Gemini-2.5-Pro-Preview The video shows the stairs to the basement at several points. Lets analyze the relevant timestamps: 1. 0:00: The video starts in hallway. Stairs leading downwards are clearly visible on the right-hand side. The hallway ahead leads towards the living room and kitchen area. At this point, we are in the hallway, before entering the kitchen. 2. 0:07: After tour of an office and bathroom, the camera is again in the hallway, moving towards the kitchen/living area. The stairs to the basement are visible on the right. This is still before entering the kitchen. 82 MORSE: Multimodal Reasoning Stress-test Environment 3. 0:09: The camera is at the threshold of the open-plan kitchen and living area. The stairs are immediately to the right, just as one is about to enter the kitchen space. Considering these observations: A) Right before entering the kitchen. This accurately describes the location of the stairs as seen in the hallway leading up to and at the entrance of the kitchen. B) While leaving the kitchen. If leaving the kitchen to go back down the hallway, the stairs would be on the left. The primary views show approaching or being at the entrance. C) Between walking from the bedroom to the office. The stairs are in the main hallway near the kitchen entrance, not specifically shown on path between bedroom and the office. D) Right after entering the living room. If one has already entered the living room, the stairs would typically be behind them or to the side in the hallway they just came from, not the primary viewing context. The most consistent and accurate description of when we see the stairs to the basement, based on the flow towards the main living areas, is right before entering the kitchen. Answer: Figure C.6: Example failure case in Temporal Reasoning involving spatial-temporal navigation. Models analyze Matterport 3D house tour to identify when basement stairs are visible relative to kitchen navigation. Both Qwen and Gemini incorrectly identify the stairs as being visible right before entering the kitchen (option A), while o3 provides right after entering the living room (option D). All models fail to recognize that the stairs are actually seen while leaving the kitchen (correct answer B), demonstrating difficulty with spatial-temporal reasoning in dynamic navigation scenarios."
        }
    ],
    "affiliations": [
        "Capital One",
        "University of Maryland, College Park"
    ]
}