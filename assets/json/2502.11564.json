{
    "paper_title": "Continuous Diffusion Model for Language Modeling",
    "authors": [
        "Jaehyeong Jo",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at \\href{https://github.com/harryjo97/RDLM}{https://github.com/harryjo97/RDLM}."
        },
        {
            "title": "Start",
            "content": "Jaehyeong Jo 1 Sung Ju Hwang 1 2 5 2 0 2 7 1 ] . [ 1 4 6 5 1 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose simulation-free training framework based on radial symmetry and simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM. 1. Introduction Discrete diffusion models (Austin et al., 2021; Lou et al., 2024) emerged as promising competitor to autoregressive models for the generative modeling of discrete data. These models have demonstrated competitive performance on tasks such as language modeling (Shi et al., 2024; Sahoo et al., 2024) and code generation (Gat et al., 2024). Unlike 1Korea Advanced ogy (KAIST) hyeong <sungju.hwang@kaist.ac.kr>. Institute 2DeepAuto.ai. Jo <harryjo97@kaist.ac.kr>, of Science Correspondence to: Sung and TechnolJaeJu Hwang Preprint. Under review. 1 autoregressive models that generate data sequentially, diffusion models generate the sequence in parallel, allowing for bidirectional controllable generation and faster sampling. However, discrete diffusion models do not fully leverage the power of iterative refinement which is the key to generative modeling of continuous data, for example, image synthesis (Saharia et al., 2022; Esser et al., 2024) and video generation (Polyak et al., 2024; Brooks et al., 2024). In discrete diffusion models, the progressive corruption during the forward process is modeled by stochastic jumps between states in Markov chains. Since denoising is achieved by jumping between states, discrete diffusion loses valuable signals during refinement which limits the generative performance and controllability. Several efforts were made to adapt continuous diffusion models for discrete data, but their performance falls short of discrete diffusion models, demonstrating significant gap compared to autoregressive models. Past works applied diffusion models for images to discrete data through continuous relaxation without constraint (Han et al., 2022; Li et al., 2022). Other lines of works (Avdeyev et al., 2023; Stärk et al., 2024) map discrete data to the probability simplex which exerts strong prior assumption on Dirichlet distribution, but often fails to model complex patterns. This led to recent works (Cheng et al., 2024; Davis et al., 2024) that apply flow matching to learn the categorical distributions using the structure of the statistical manifold, but these methods are limited to small sequences and categories. In particular, the link between discrete and continuous diffusion remains unclear, hindering the development of coherent diffusion framework for discrete data. In this work, we present Riemannian Diffusion Language Model (RDLM), continuous diffusion framework for language modeling that incorporates the geometry of the statistical manifold in the diffusion processes. We establish connection between continuous flow on the statistical manifold and the discrete diffusion process, showing that the trajectory of the transition distribution can be modeled to conditional flow on the manifold. Based on the analogy, we introduce simple design of the diffusion processes on the manifold that generalizes previous discrete diffusion models. We further present simulation-free training scheme using radial symmetry that integrates simple parameterization and Continuous Diffusion Model for Language Modeling maximum likelihood-based training objectives. Through experiments on language modeling tasks, image modeling, and biological sequence design, we validate that our framework outperforms existing discrete diffusion models. 2. Background 2.1. Discrete diffusion models Discrete diffusion models (Austin et al., 2021; Lou et al., 2024; Sahoo et al., 2024; Shi et al., 2024) define the diffusion process directly on discrete structures using the Markov chains. The forward process describes the transition from the current state to other states, which is formalized by multiplying the transition matrix Qt: q(xtxt1) = Cat(xt; Qtxt1), (1) where xt is the random variable for the discrete states and Cat() denotes the categorical distribution. This induces the marginal distribution that corresponds to repeatedly multiplying the transition matrices over time steps: q(xtx) = Cat(xt; Qtx) = Cat(xt; Qt Q1x). (2) Austin et al. (2021) introduced several designs of the transition matrices, including the masked (absorbing state) diffusion and the uniform diffusion, and the continuous-time Markov chains (CTMC) (Austin et al., 2021; Campbell et al., 2022) extends the framework to continuous-time. 2.2. Statistical Manifold of Categorical Distribution Let = {1, , d} denote the discrete data space and d1 = {(p1, , pd) Rd (cid:80) pi = 1, pi 0} denote the (d 1)-dimensional probability simplex. d-class categorical distribution over can be parameterized by the parameters p1, , pd such that {(cid:80) pi = 1, pi 0}. Then the statistical manifold P(X ) of the categorical distribution corresponds to d1 equipped with the FisherRao metric (Rao, 1992; Amari, 2016) (see Appendix A.1). Moreover, there exists diffeomorphism from P(X ) to the positive orthant of (d 1)-dimensional sphere Sd1 + : π : P(X ) Sd1 + ; pi (cid:55) ui = pi, (3) which induces the following geodesic distance on Sd1 + : dg(u, v) = cos1u, v, (4) where denotes the Euclidean inner product. We provide further explanation in Appendix A.1. 2.3. Riemannian Diffusion Mixture Riemannian diffusion mixture framework (Jo & Hwang, 2024) provides simple approach to generative modeling on general manifolds. The construction of the generative model starts with defining bridge process Qz on the manifold with endpoint z: dX , t)dt + σtdBM = ηz(X where BM is the Brownian motion defined on M. The diffusion process transporting an initial distribution to the data distribution is modeled as mixture of bridge processes: (cid:20)(cid:90) dXt = ηz(Xt, t) pz (Xt) pt(Xt) (cid:21) dt + σtdBM p(dvolz) (5) where denotes the data distribution, pz is the marginal distribution of the bridge Qz, and pt() := (cid:82) pz ()p(dvolz). The drift of this process is regressed by neural network ηθ with the bridge matching objective: (cid:34)(cid:90) 0 (cid:16) (cid:13) (cid:13)σ1 (cid:13) zp XQz ηθ(Xt, t) ηz(Xt, t) (cid:17)(cid:13) 2 (cid:13) (cid:13) (cid:35) dt (6) We provide further details in Appendix A.4. 3. Riemannian Diffusion Language Model We introduce novel continuous diffusion model for language modeling. In this section, we first present single token generation framework, which we generalize to modeling sequences of tokens in Section 5. 3.1. Generalization of Discrete Diffusion Continuous Reparameterization of Discrete Data To incorporate the geometry of the underlying categorical distribution, we leverage the statistical manifold to parameterize discrete data (Cheng et al., 2024; Davis et al., 2024). Each point on the statistical manifold P(X ) corresponds to the parameters of categorical distribution over the discrete sample space = {1, , d}. Thus discrete data can be represented as continuous parameters of categorical distribution on the manifold. Yet the Fisher-Rao metric is ill-defined on the boundary of the manifold where the initial distribution of the parameterized data lies, incurring numerical issues near the boundary. To address this, we leverage the diffeomorphism π (Eq. (3)) which maps P(X ) to the positive orthant of hypersphere Sd1 + (Cheng et al., 2024; Davis et al., 2024), + corresponds to Cat(; π-1(u)). Therefore, where Sd1 discrete data can be reparameterized to continuous states on Sd1 while preserving the geometry of the categorical distribution. In the case of masked diffusion, discrete sample space is augmented with an additional mask state, and the reparameterization results in d-dimensional sphere. Our key observation is that the transition distribution qt(xtx0) of discrete diffusion process is categorical distribution on (Eq. (2)). Therefore, modeling qt is equivalent to modeling the probability path on the statistical manifold P(X ). From the following proposition, we show that 2 Continuous Diffusion Model for Language Modeling discrete diffusion models over can be modeled by continuous flow on P(X ) and further on Sd1 + (we defer the proof to Appendix A.2). denotes the Brownian motion defined on Sd1 where Bd and ϕt denotes the geodesic distance between the current state and the endpoint. Proposition 3.1. The transition distribution of discrete diffusion processes can be modeled by the probability path on the statistical manifold, and further on the hypersphere. proof scketch. continuous flow on Sd1 and as geodesic is described by the following ODE: + that interpolates (7) = exp-1 Yt (u), Y0 = v, log κt dt dYt dt where exp-1 denotes the logarithm map. Then for welldesigned schedule κt and u, the process Zt := π(Yt) on P(X ) corresponds to the transition distribution of the discrete diffusion process. In particular, we obtain the masked diffusion process for = em, i.e., the masked token, and the uniform diffusion process for = (cid:80)d d. i=1 ei/ Although discrete diffusion processes can be represented as probability path on the statistical manifold, this flow cannot be learned by neural network. The network fails to generalize to points outside the geodesic that interpolates the prior and the data distribution, producing an incorrect vector field. While previous works (Cheng et al., 2024; Davis et al., 2024) use the uniform distribution on the simplex as the prior, this does not directly relate to discrete diffusion models. Therefore, we present simple design for the continuous diffusion model that generalizes existing discrete diffusion models. 3.2. Generative Process on Hypersphere With the reparameterization, the task of modeling the distribution of discrete data can be reformulated to modeling distribution on the hypersphere. The reparameterized data distribution can be represented as follows: p(x) = (cid:88) k= pkδ(x ek), (8) where pk and ek denotes the probability and the one-hot vector of the k-th token, respectively. To model p, we build upon the Riemannian Diffusion Mixture framework (Jo & Hwang, 2024) to construct generative process on the hypersphere. Due to the simple nature of Sd1, we can derive the logarithm bridge process (Jo & Hwang, 2024) from an arbitrary point Sd1 to the k-th token ek as follows (we provide the derivation in Appendix A.3): dX = γt ϕt(ek cos ϕtX ) sin ϕt dt + σtdBd , γt := σ2 (cid:82) σ2 ds , ϕt := cos-1X , ek, 0 = u, (9) 3 Intuitively, the current state Xt moves in the direction that minimizes the geodesic distance to the endpoint, resulting in process that bridges the starting and end points. While different forms of the bridge process exist, for example, scaling the drift or the diffusion coefficients, Eq. (9) yields specific transition distribution that enables simulation-free training, which we explain in Section 4. From the bridge processes, we construct diffusion process on Sd1 using the diffusion mixture representation (Proposition A.7) with mixing distribution (see Appendix A.4 for the formal definition of the representation): dXt = (cid:34) (cid:88) k=1 pT t(ekXt) ηk(Xt, t) dt + σtdBd , (10) (cid:35) where ηk denote the drift of the bridge process in Eq. (9). pT t(ekXt) represents the probability that the token ek will be the final outcome of the process, given the current state Xt at time t. Note that the construction guarantees the terminal distribution of the process to be p. An ideal generative process is one that gradually refines the uninformative states to recover the original tokens. We analyze the convergence of the bridge process through its radial process rk := dg(Xt, ek) described by the following SDE (see Appendix A.3 for the derivation using Itôs formula): drk = (cid:20) γtrk + (cid:21) cot rk σ2 2 dt + σtdWt, (11) where Wt is 1-dimensional Wiener process. For σ0 > σT , the radial process converges rapidly in early time steps, making it difficult for neural network to approximate accurately. We empirically find that the geometric schedule σt = σ1t with σ0 < σT leads to gradual convergence. 0 σt Masked Diffusion From Proposition 3.1, fixing the initial distribution to be the mask token em yields mixture process that generalizes the masked discrete diffusion process. The resulting process starts from mask token and moves to one of the tokens following the drift. In the perspective of discrete diffusion, our process smoothly interpolates the jump from the mask token to the final token via through the continuous states Xt, with pT t(ekXt) determining the direction of the process. The generalized framework shares similar properties with the masked discrete diffusion (Sahoo et al., 2024): (1) Zero Mask Probabilities. Our parameterization in Eq.(16) sets the probability pT t(emXt) to zero, indicating that the final token cannot be mask token. (2) Carry-Over Unmasking. If Xt converges to token ek before the terminal time, the Continuous Diffusion Model for Language Modeling drift in Eq. (9) also converges to zero and the state Xt is carried over without changing to different token. Yet, the fundamental difference is that discrete diffusion directly jumps from token to the mask token and vice versa where wrong jump is non-revokable, making the generation process uneditable. On the other hand, our continuous approach offers numerous chances to correct wrong directions during the process, leading to more accurate modeling of the data distribution. Uniform Diffusion From Proposition 3.1, the generalization of the uniform diffusion can be achieved by setting the starting point to be the barycenter of the simplex mapped (cid:17) to Sd1, i.e., π d. We further extend the uniform diffusion so that the transition to subset of tokens gets different probability ζ: i=1 ei/d i=1 ei/ = (cid:80)d (cid:16)(cid:80)d π (cid:88) iS ζei + 1 ζS (cid:88) /S ej , 0 ζ 1 . (12) For = {m} and ζ = 1, we obtain the masked diffusion. Mixture Paths Since masked diffusion and uniform diffusion have different initial conditions, they yield different convergence behaviors. We empirically observe that under the same noise schedule, uniform diffusion is easier to learn in the early time steps compared to masked diffusion, whereas the opposite holds in later stages. This suggests that diffusion process mixing masked and uniform processes could result in an improved generative model. Therefore, we derive new family of generative processes by mixing the probability path of generative processes {Q,i : 1 n} sharing the same noise schedule σt (see Appendix A.4 for detailed derivation of mixture path): Qmix := (cid:88) i=1 λi Q,i ; (cid:88) i=1 λi = 1, (13) where λn denotes the mixing schedule. From the perspective of diffusion mixture representation, this corresponds to creating mixture of generative processes {Q,i } with mixing distribution λi t. One example is to create mixture path from the masked bridges and uniform bridges: λtQmask + (1 λt)Qunif , (14) with initial distribution λ0δ(em)+(1λ0)δ((cid:80)d d), which generalizes the mixture paths used in discrete flow matching (Shaul et al., 2024). i=1 ei/ Generalizing Flow Matching Our framework generalizes flow matching methods on the statistical manifold (Cheng et al., 2024; Davis et al., 2024). By designing the noise schedule in Eq. (9) to be σt := σ0 0, we obtain the conditional vector field of the flow matching models. 4. Simulation-Free Training with Radial"
        },
        {
            "title": "Symmetry",
            "content": "Next, we introduce our training scheme. We derive the likelihood bound for our model and present simple parameterization and objectives. Further, we present simulation-free training method based on the radial symmetry of Sd. Likelihood Bound Our approach yields simple form of evidence-lower bound (ELBO) by using the Girsanov theorem on compact manifolds (De Bortoli et al. (2022), Corollary H.3). For point Sd, we can upper bound the negative log-likelihood of our model (Eq. (10)) by the KL divergence between the approximated mixture process and the bridge process with endpoint z: log pθ(z) = DKL(δ(z)pθ(XT ) = z) DKL(QzQθ) = EXQz (cid:34) 1 (cid:90) 0 (cid:13) (cid:13) σ1 (cid:13) (cid:13) (cid:0)ηθ(Xt, t) ηz(Xt, t)(cid:1) (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) dt where Qz and Qθ denote the probability measure of the bridge and mixture processes, respectively, and ηθ denotes the drift of Eq. (10). The point-wise likelihood bound provides an upper bound on the NLL Ezp [ log pθ(z)]: (cid:34) 1 2 (cid:90) (cid:13) (cid:13) σ1 (cid:13) (cid:13) Eekp XQk (cid:0)ηθ(Xt, t) ηk(Xt, t)(cid:1) (cid:35) dt , (15) (cid:13) 2 (cid:13) (cid:13) (cid:13) where Qk and ηk denote the probability measure and the drift of the bridge process with endpoint ek, respectively. Parameterization and Objective The drift of the mixture process diverges near the terminal time, which makes it challenging to learn. Therefore, instead of approximating the drift function directly, we propose to model the probability pT t(XT Xt) with neural network sθ as follows: pθ(Xt, t) := softmax (sθ(Xt, t)) (cid:104) pT t(e1Xt), , pT t(edXt), 0 (cid:105)T , = (16) where we force the probability pT t(emXt) to be zero. Then the drift of the mixture process can be represented by the parameterization as follows: ηθ(Xt, t) = (cid:88) k=1 (cid:68) pθ(Xt, t), ek (cid:69) ηk(Xt, t), (17) Continuous Diffusion Model for Language Modeling Based on the ELBO of Eq. (15), we derive maximum likelihood training objective with the parameterized drift: This is possible since Eq. (9) is obtained by applying the time change (Øksendal, 2003) to simple bridge: L(θ) = Eekp XQk (cid:34) 1 (cid:90) 0 (cid:35) σ2 Ek θ (Xt, t)dt (18) Ek θ (x, t) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) l=1 (cid:68) pθ(x, t), el (cid:69) (cid:13) 2 (cid:13) ηl(x, t) ηk(x, t) (cid:13) (cid:13) (cid:13) , which can be interpreted as minimizing the mean squared error of the drift approximation. key observation is that ELBO can be minimized by reducing the cross-entropy between the probability vector pθ(Xt, t) and the target one-hot vector ek. Therefore we introduce cross-entropy-based training objective similar to that used in discrete diffusion models (Sahoo et al., 2024; Shi et al., 2024): LCE(θ) = Eekp XQk (cid:20) (cid:90) 0 log (cid:10)pθ(Xt, t), ek (cid:11)dt (cid:21) . (19) We experimentally find that the cross-entropy-based loss yields faster convergence in training and leads to better performance than the mean squared error-based loss. Importance Sampling The difficulty of approximating the probability pT t(XT Xt) varies significantly across different time points t. While predicting XT is fairly easy in the later stage of the process, it is challenging to do so during the middle of the process. The training objective can be improved by training more on the challenging time points. We achieve this by using an importance sampling technique on which modifies the time distribution to focus on specific interval, resulting in an equivalent objective: Lq(θ) = Eekp XQk (cid:104) Etq q(t) log (cid:10)pθ(Xt, t), ek (cid:11)(cid:105) (20) where is the normalized proposal distribution for t. We find that simple density q(t) = 1 ϵ if [a, b] else ϵ to be effective. Approximation of Transition Distribution The training objective requires sampling Xt from the bridge processes at each iteration. Since the diffusion process on the d-dimensional sphere does not yield tractable transition distribution, it requires simulating the process which becomes significant bottleneck during training. Therefore, we introduce an approximation sampling method that enables simulation-free training, which makes our framework scalable to large vocabulary. We approximate the distribution p(XtX0, XT ) as the pushforward measure of Gaussian distribution on the tangent space by the exponential map, i.e., the Riemannian normal. ˆXt ="
        },
        {
            "title": "1\nT − t",
            "content": "ϕt(ek cos ϕt ˆXt) sin ϕt dt + dBd , (21) for ϕt := cos-1 ˆXT , ˆXt, which yields transition distribution similar to Riemannian normal. We parameterize the mean µt and the covariance Σt of the Riemannian normal approximating p(XtX0 = u, XT = v) with the parameters αt and ρt as follows: (cid:19) (cid:18)(cid:113) = + 1 α2 αt cos ϕ0 sin ϕ αt sin ϕ0 (Xt)(cid:3) = ρ2 EXt µt = EXt Σt = Cov (cid:2)exp-1 I, µt for ϕ0 := cos-1u, v. Intuitively, µt represents the normalized centroid of the samples Xt and ρ2 corresponds to the covariance of the lifted samples in the tangent space Tµt. (22) Connection to Projected Processes While the parameters αt and ρt are generally intractable, we derive them from the 1-dimensional projections of the diffusion process. Our main idea is to represent the parameters using the projected processes cw := Xt, for = X0 and X1. For bridge process from to v, the projected process cv = Xt, is modeled by 1-dimensional SDE derived from the Itôs formula and the radial symmetry of Sd (see Appendix A.5 for the derivation): dcv = b(cv b(c, t) = γt cos-1c , t)dt + σt (cid:112) (cid:112)1 (cv dσ2 2 1 c2 )2 dWt, (23) where Wt is 1-dimensional standard Wiener process. Similarly, cu = Xt, is described by SDE that depends on cv (see Appendix A.5 for the derivation): dcu = b(cu , cv b(cu, cv, t) = γt , t)dt + σt cos-1cv (cid:112)1 (cv)2 (cid:112)1 (cu (cid:16) )2 dWt, u, cucv(cid:17) (24) dσ2 2 cu From the initial conditions cv 0 = 1, we obtain the connection between the projections and the parameters of Riemannian normal (see Appendix A.6): 0 = u, and cu (cid:18) Ecv = (cid:112)1 u, v2αt + u, (cid:19) (cid:113) 1 α2 Fd(ρt), (cid:113) Ecu = 1 α2 Fd(ρt), Fd(ρ) := eρ2 2 1f (cid:18) 2 , 1 2 , (cid:19) ρ2 where 1f1 denotes the confluent hypergeometric function. Therefore, the parameters of the Riemannian normal can be 5 Continuous Diffusion Model for Language Modeling derived from the mean projections Ecu and Ecv as follows: (cid:115) αt = (rt u, v)2 1 u, v2 + (rt u, v)2 , rt = (cid:19) (cid:18) (cid:113) Ecv Ecu (25) ρt = 1 Ecu / 1 α2 , and Ecv denotes the inverse function of Fd. For small d, where 1 we calibrate ρt by scaling up with constant. While the mean projections Ecu generally do not have closed-form solutions, they can be easily obtained from simulating the 1-dimensional processes Eq. (23) and Eq. (24). In particular, for masked and uniform diffusion, is fixed to single point for which u, ek is the same for all the non-masked tokens., Due to the radial symmetry, Ecek is identical for all and the bridge processes Qk share the same αt and ρt. Therefore, before training our model, we pre-compute αti and ρti only once for ti := i/N with sufficiently large , by simulating the 2-dimensional process (cu ). Then with the pre-computed parameters, we can easily sample Xt from the Riemannian normal during training without expensive simulation of the bridge processes, achieving 50 faster speed up compared to the simulation-based training. We experimentally demonstrate that our approach provides an accurate approximation of the distribution Xt in Section 7.4. , ce1 5. Generation of Token Sequences Sequence of Tokens Now we generalize the result of single token modeling to the generation of token sequences. Since each token in the sequence is reparameterized to ddimensional spheres, sequence of length is modeled on product manifold (Sd)n := Sd Sd. The diffusion processes on each hypersphere are dependent on each other, described by the following system of SDEs: dX = (cid:88) k=1 p(X = ekX 1:n )ηk(X , t) + σtdBd , (26) for 1 n, where ηk denotes the drift of the bridge on Sd with endpoint ek. Note that p(X ) denotes the probability of the i-th token being the k-th state which relies on the current intermediate sequence 1:n , and we train neural network to predict the probabilities. = ekX 1:n Our framework allows generating sequences of arbitrary lengths smaller than the maximum length. Using the tokens [BOS] and [EOS] that denote the start and the end of the sequence, we can generate sequence of the desired length by fixing the position of these tokens. Dimension Splitting of Statistical Manifold For large vocabulary set, the corresponding statistical manifold has 6 high dimension which results in two challenges: (1) Abrupt convergence. Bridge processes on high-dimensional sphere converge abruptly near the end of the process, which makes them hard to learn with neural network. (2) Large input dimension. Since the input of the network is of high dimension, the hidden dimensions of the network should be sufficiently large to encode them properly. Models with small capacity fail to learn the probabilities of Eq.(16). To address these challenges, we introduce dimension splitting, simple technique to reduce the dimension of the parameterized manifold. Instead of directly mapping the k-th token to Sd, we represent the index in base which is then mapped to the product manifold (Sb)m where = for masked diffusion and otherwise 1, and := logb d. Dimension splitting reparameterizes sequence of length to product manifold (Sb)mL, and the bridge processes defined on Sb with small yield gradual convergence that can be easily learned by neural network. Dimension splitting significantly enhances the likelihood of our model when used together with the mixture path (Eq. (14)). 6. Related Work Discrete Diffusion Models Discrete diffusion directly models the Markov chain on discrete data space. The onehot data distribution is gradually corrupted to stationary distribution with specific transition matrices, where the noising process corresponds to the stochastic jumps between states in the Markov chain. D3PM (Austin et al., 2021) introduces discrete-time Markov forward processes with uniform and absorbing state transition matrices and has been generalized to continuous-time Markov chain framework (Campbell et al., 2022). SEDD (Lou et al., 2024) proposes learning the score entropy of the discrete states instead of the mean prediction. Recent works (Shi et al., 2024; Sahoo et al., 2024) introduce continuous-time masked diffusion models with simpler form of likelihood bounds. Continuous Diffusion Models for Discrete Data Early works approached by fully relaxing the discrete data into continuous space (Han et al., 2022) or embedding the tokens in latent space (Li et al., 2022; Dieleman et al., 2022), without any constraint. However, continuous relaxation without constraint fails to accurately model the discreteness of the categorical distribution. Recent works utilize the logit space (Hoogeboom et al., 2021; Graves et al., 2023) or the probability simplex (Avdeyev et al., 2023; Stärk et al., 2024) based on the Dirichlet distribution, which require strong assumptions on the diffusion noising processes. Flow matching has been applied to the probability simplex by using the statistical manifold on categorical distribution (Cheng et al., 2024; Davis et al., 2024) but has limited performance lagging behind discrete diffusion models. Continuous Diffusion Model for Language Modeling Table 1: Bits Per Character (BPC) results on Text8 test set. Results are taken from the corresponding papers. Bold denotes the best result in autoregressive or diffusion models. Table 2: Test perplexity (PPL) results on LM1B dataset. Baseline results are taken from Sahoo et al. (2024). Method # Param. PPL ()"
        },
        {
            "title": "Method",
            "content": "Autoregressive IAF/SCF AR Argmax Flow Transformer AR Discrete Flow Any-order Autoregressive ARDM MAC Discrete Diffusion Multinomial Diffusion D3PM Uniform D3PM Absorb SEDD Absorb MDLM MD4 Continuous Diffusion Plaid BFN RDLM (Ours) BPC () 1.88 1.39 1.23 1. 1.43 1.40 1.72 1.61 1.45 1.39 1.40 1.37 1.48 1.41 1.32 7. Experiments 7.1. Text Generation We evaluate our Riemannian Diffusion Language Model (RDLM) for text generation tasks on two language benchmarks: Text8 (Mahoney, 2006) and One Billion Words Dataset (Chelba et al., 2013). Baselines We compare against state-of-the-art autoregressive and diffusion models. Multinomial Diffusion (Hoogeboom et al., 2021), D3PM (Austin et al., 2021), SEDD (Lou et al., 2024), MDLM (Sahoo et al., 2024), MD4 (Shi et al., 2024) are discrete diffusion models. Plaid (Gulrajani & Hashimoto, 2024) and Bayesian Flow Network (BFN) (Graves et al., 2023) are continuous diffusion models. IAF/SCF (Ziegler & Rush, 2019), AR Argmax Flow (Hoogeboom et al., 2021), and Discrete Flow (Tran et al., 2019) are flow-based models, and ARDM (Hoogeboom et al., 2022) and MAC (Shih et al., 2022) are any-order autoregressive models. We also compare with transformer AR model (Vaswani et al., 2017). We provide further details on the baselines in Appendix B.1 Implementation Details For all experiments, we use the same data split and context size following Lou et al. (2024) and Sahoo et al. (2024). For Text8, we randomly sample contiguous chunks of length 256 as done in previous works (Austin et al., 2021; Lou et al., 2024). For One Bil7 Autoregressive Transformer-X Base OmniNetT Transformer Discrete Diffusion BERT-Mouth D3PM Absorb DiffusionBert SEDD MDLM Continuous Diffusion Diffusion-LM RDLM (Ours) 0.46B 100M 110M 110M 70M 110M 110M 110M 80M 110M 23.5 21.5 22.32 142.89 76.90 63.78 32.79 27.04 118.62 29. lion Words, we use the same tokenizer as in He et al. (2023) with context size 128. We use diffusion transformer architecture (Peebles & Xie, 2023) with rotary positional embeddings (Su et al., 2024) for all the experiments and match the number of parameters as used in the previous works (Lou et al., 2024; Sahoo et al., 2024). For our model, we use the mixture path of masked and uniform diffusion (Eq. (14)) and apply dimension splitting for large vocabulary. We provide more details in Appendix B.1. Text8 We first evaluate on small character-level language modeling task. Text8 (Mahoney, 2006) dataset is character-level text modeling benchmark extracted from English Wikipedia. We train the models on short text chunks of length 256 and evaluate the models using Bits Per Character (BPC). As shown in Table 1, our framework outperforms all previous diffusion models, both the discrete and continuous methods. We also outperform the any-order autoregressive models that generate texts in flexible decoding order similar to discrete diffusion models. We achieve similar generative perplexity and entropy compared to existing discrete diffusion models. We provide the generated texts from RDLM in Appendix C.1. One Billion Words We further evaluate on One Billion Words Dataset (LM1B) (Chelba et al., 2013), mediumsized real-world language benchmark. We evaluate the models using perplexity (PPL) and the results are summarized in Table 2. RDLM outperforms most of the diffusion models and is comparable to the state-of-the-art discrete diffusion model (Shi et al., 2024). In particular, we significantly outperform the existing continuous diffusion model (Li et al., 2022) demonstrating the effectiveness of incorporating the geometry of the underlying categorical distribution. We provide the generated texts in Appendix C.2. Continuous Diffusion Model for Language Modeling Table 3: Bits Per Dimension (BPD) results on CIFAR-10 dataset. Baseline results are taken from Shi et al. (2024). Table 4: MSE results on the generated promoter DNA sequences. Baseline results are taken from Davis et al. (2024). Method # Param. BPD () Method MSE () Autoregressive PixelRNN Gated PixelCNN PixelCNN++ PixelSNAIL Image Transformer Sparse Transformer Discrete Diffusion D3PM Absorb D3PM Gauss τ LDR MD4 Continuous Diffusion RDLM (Ours) 3.00 3.03 2.92 2.85 2.90 2.80 4.40 3.44 3.59 2.78 2.74 53M 46M 59M 37M 36M 36M 28M 35M 7.2. Pixel-level Image Modeling We further explore applications beyond the text domain. We train our model on order-agnostic image data where each image is represented as set of discrete tokens with vocabulary of size 256. This removes the information of relative proximity between different pixels. We compare RDLM against autoregressive models and discrete diffusion models that directly work on raw pixel space, which we describe in Appendix B.2. As shown in Table 3, our method achieves the lowest BPD outperforming the discrete diffusion models (Austin et al., 2021; Shi et al., 2024) and autoregressive models (Chen et al., 2018; Child et al., 2019). 7.3. DNA Sequence Design We show that our framework can be applied to the generation of biological sequences. We evaluate our method for the promoter DNA sequence design task, which aims to generate valid promoter DNA sequences conditioned on transcription profiles. We provide further details of the task in Appendix B.3. We measure the mean squared error (MSE) between the predicted regulatory activity of the generated sequence and that of the original sequence corresponding to the transcription profile. Table 4 shows that our framework achieves the lowest MSE, outperforming the flow matching methods (Stärk et al., 2024; Davis et al., 2024) and the discrete diffusion diffusion model (Austin et al., 2021). 7.4. Analysis Approximation of Transition Distribution In Figure 3, we measure the maximum mean discrepancy (MMD) (Gretton et al., 2012) distance between the simulated transition distribution and the approximated distribution. The approximated distributions show almost the same MMD as the 8 Bit-Diffusion (bit) Bit-Diffusion (one-hot) D3PM Uniform DDSM DirichletFM Language Model Fisher-Flow RDLM (Ours) 0.041 0.040 0.038 0.033 0.034 0.034 0.029 0. simulated distributions, indicating that the approximation is reliable. In particular, the discrepancy becomes close to zero in the high-dimensional manifold, where the simulation of the SDE becomes expensive. Training Objective We validate the effectiveness of the cross-entropy-based loss of Eq. (19) in Table 1. Compared to the mean-squared error-based loss of Eq. (18), the crossentropy loss provides faster convergence in training and better NLL. Furthermore, Table 1 shows that applying importance sampling (Eq. (20)) improves the performance. Dimension Splitting For datasets with large vocabulary, for example, LM1B dataset, splitting the dimension of the manifold yields significant improvement. Table 2 shows that the generative model on the high-dimensional manifold cannot be trained due to the large input dimension. While adding additional information to the model does improve the result, the abrupt convergence of bridge processes on high dimensions makes them challenging to learn. For large vocabulary, we achieve the best result by splitting the dimensions into smaller ones and modeling the generative process using mixture path. 8. Conclusion In this work, we introduced Riemannian Diffusion Language Model (RDLM), continuous diffusion model for language and discrete data. We present simple framework that generalizes discrete diffusion models building on the connection between the transition distribution of the diffusion process and the probability path on the statistical manifold. We provide general designs of the diffusion processes and introduce simulation-free training scheme leveraging the radial symmetry of the hypersphere. We validate through experiments on language benchmarks that RDLM outperforms previous discrete and continuous diffusion models for language modeling. Further, we explore applications to other modalities including images and biological sequences achieving state-of-the-art results. Continuous Diffusion Model for Language Modeling Impact Statement This paper presents work whose goal is to advance the field of deep generative models for language modeling and discrete data. We believe our work can enhance our understanding of various scientific fields dealing with discrete data."
        },
        {
            "title": "References",
            "content": "Amari, S.-i. Information geometry and its applications, volume 194. Springer, 2016. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems, 2021. Avdeyev, P., Shi, C., Tan, Y., Dudnyk, K., and Zhou, J. Dirichlet diffusion score model for biological sequence In International Conference on Machine generation. Learning, 2023. Ay, N., Jost, J., Vân Lê, H., and Schwachhöfer, L. Information geometry, volume 64. Springer, 2017. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., et al. Video generation models as world simulators, 2024. Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. continuous time framework for discrete denoising models. In Advances in Neural Information Processing Systems, 2022. Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013. Chen, T., Zhang, R., and Hinton, G. E. Analog bits: Generating discrete data using diffusion models with selfconditioning. In International Conference on Learning Representation, 2023. Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P. Pixelsnail: An improved autoregressive generative model. In International Conference on Machine Learning, 2018. Cheng, C., Li, J., Peng, J., and Liu, G. Categorical flow matching on statistical manifolds. In Advances in Neural Information Processing Systems, 2024. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv:1904.10509, 2019. Davis, O., Kessler, S., Petrache, M., Ceylan, I. I., Bronstein, M. M., and Bose, A. J. Fisher flow matching for generative modeling over discrete data. In Advances in Neural Information Processing Systems, 2024. De Bortoli, V., Mathieu, E., Hutchinson, M., Thornton, J., Teh, Y. W., and Doucet, A. Riemannian score-based generative modelling. In Advances in Neural Information Processing Systems, 2022. Dhariwal, P. and Nichol, A. Q. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systemsl, 2021. Dieleman, S., Sartran, L., Roshannai, A., Savinov, N., Ganin, Y., Richemond, P. H., Doucet, A., Strudel, R., Dyer, C., Durkan, C., et al. Continuous diffusion for categorical data. arXiv:2211.15089, 2022. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., and Rombach, R. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. Gat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T. Q., Synnaeve, G., Adi, Y., and Lipman, Y. Discrete flow matching. In Advances in Neural Information Processing Systems, 2024. Graves, A., Srivastava, R. K., Atkinson, T., and Gomez, F. Bayesian flow networks. arXiv:2308.07037, 2023. Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., and Smola, A. kernel two-sample test. The Journal of Machine Learning Research, 13(1):723773, 2012. Gulrajani, I. and Hashimoto, T. B. Likelihood-based diffusion language models. In Advances in Neural Information Processing Systems, 2024. Han, X., Kumar, S., and Tsvetkov, Y. Ssdlm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv:2210.17432, 2022. He, Z., Sun, T., Tang, Q., Wang, K., Huang, X., and Qiu, X. Diffusionbert: Improving generative masked language models with diffusion models. In Annual Meeting of the Association for Computational Linguistics, 2023. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv:2207.12598, 2022. Hon, C.-C., Ramilowski, J. A., Harshbarger, J., Bertin, N., Rackham, O. J., Gough, J., Denisenko, E., Schmeier, S., Poulsen, T. M., Severin, J., et al. An atlas of human long non-coding rnas with accurate 5 ends. Nature, 543(7644): 199204, 2017. Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P., and Welling, M. Argmax flows and multinomial diffusion: Learning 9 Continuous Diffusion Model for Language Modeling categorical distributions. In Advances in Neural Information Processing Systems, 2021. Hoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B., Berg, R. v. d., and Salimans, T. Autoregressive diffusion models. In International Conference on Learning Representation, 2022. Hsu, E. P. Stochastic analysis on manifolds. Number 38 in Graduate studies in mathematics. American Mathematical Society, 2002. Jo, J. and Hwang, S. J. Generative modeling on manifolds through mixture of riemannian diffusion processes. In International Conference on Machine Learning, 2024. Jo, J., Kim, D., and Hwang, S. J. Graph generation with diffusion mixture. In International Conference on Machine Learning, 2024. Jung, H., Park, Y., Schmid, L., Jo, J., Lee, D., Kim, B., Yun, S., and Shin, J. Conditional synthesis of 3d molecules In Advances in Neural with time correction sampler. Information Processing Systems, 2024. Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves controllable text generation. In Advances in Neural Information Processing Systems, 2022. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv:1711.05101, 2017. Lou, A., Meng, C., and Ermon, S. Discrete diffusion language modeling by estimating the ratios of the data distribution. In International Conference on Machine Learning, 2024. Mahoney, M. Large text compression benchmark. https: //www.mattmahoney.net/dc/text.html, 2006. . Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and Tran, D. Image transformer. In International Conference on Machine Learning, 2018. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. Peluchetti, S. Non-denoising forward-time diffusions. Openreview, 2021. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C., Chuang, C., Yan, D., Choudhary, D., Wang, D., Sethi, G., Pang, G., Ma, H., Misra, I., Hou, J., Wang, J., Jagadeesh, K., Li, K., Zhang, L., Singh, M., Williamson, M., Le, M., Yu, M., Singh, M. K., Zhang, P., Vajda, P., Duval, Q., Girdhar, R., Sumbaly, R., Rambhatla, S. S., Tsai, S. S., Azadi, S., Datta, S., Chen, S., Bell, S., Ramaswamy, S., Sheynin, S., Bhattacharya, S., Motwani, S., Xu, T., Li, T., Hou, T., Hsu, W., Yin, X., Dai, X., Taigman, Y., Luo, Y., Liu, Y., Wu, Y., Zhao, Y., Kirstain, Y., He, Z., He, Z., Pumarola, A., Thabet, A. K., Sanakoyeu, A., Mallya, A., Guo, B., Araya, B., Kerr, B., Wood, C., Liu, C., Peng, C., Vengertsev, D., Schönfeld, E., Blanchard, E., Juefei-Xu, F., Nord, F., Liang, J., Hoffman, J., Kohler, J., Fire, K., Sivakumar, K., Chen, L., Yu, L., Gao, L., Georgopoulos, M., Moritz, R., Sampson, S. K., Li, S., Parmeggiani, S., Fine, S., Fowler, T., Petrovic, V., and Du, Y. Movie gen: cast of media foundation models. arXiv:2410.13720, 2024. Rao, C. R. Information and the accuracy attainable in the estimation of statistical parameters. In Breakthroughs in Statistics: Foundations and basic theory, pp. 235247. Springer, 1992. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, S. K. S., Lopes, R. G., Ayan, B. K., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Photorealistic text-to-image diffusion models with In Advances in Neural deep language understanding. Information Processing Systems, 2022. Sahoo, S. S., Arriola, M., Gokaslan, A., Marroquin, E. M., Rush, A. M., Schiff, Y., Chiu, J. T., and Kuleshov, V. Simple and effective masked diffusion language models. In Advances in Neural Information Processing Systems, 2024. Salimans, T., Karpathy, A., Chen, X., and Kingma, D. P. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations, 2017. Shaul, N., Gat, I., Havasi, M., Severo, D., Sriram, A., Holderrieth, P., Karrer, B., Lipman, Y., and Chen, R. T. Flow matching with general discrete paths: kinetic-optimal perspective. arXiv:2412.03487, 2024. Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. K. Simplified and generalized masked diffusion for discrete In Advances in Neural Information Processing data. Systems, 2024. Shih, A., Sadigh, D., and Ermon, S. Training and inference on any-order autoregressive models the right way. In Advances in Neural Information Processing Systems, 2022. Stärk, H., Jing, B., Wang, C., Corso, G., Berger, B., Barzilay, R., and Jaakkola, T. S. Dirichlet flow matching with applications to DNA sequence design. In International Conference on Machine Learning, 2024. 10 Continuous Diffusion Model for Language Modeling Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Tran, D., Vafa, K., Agrawal, K. K., Dinh, L., and Poole, B. Discrete flows: Invertible generative models of discrete In Advances in Neural Information Processing data. Systems, 2019. van den Oord, A., Kalchbrenner, N., Espeholt, L., Kavukcuoglu, K., Vinyals, O., and Graves, A. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, 2016a. van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, In International K. Pixel recurrent neural networks. Conference on Machine Learning, 2016b. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 59986008, 2017. Ziegler, Z. M. and Rush, A. M. Latent normalizing flows for discrete sequences. In International Conference on Machine Learning, 2019. Øksendal, B. Stochastic Differential Equations. Universitext. Springer Berlin Heidelberg, 2003. 11 Continuous Diffusion Model for Language Modeling"
        },
        {
            "title": "Appendix",
            "content": "A. Derivations A.1. Preliminaries Statistical Manifold of Categorical Distributions For discrete sample space = {1, 2, , d}, d-class categorical distribution over is parameterized by number of parameters p1, , pd 0 such tat (cid:80)d i=1 pi = 1. The parameter space corresponds to the (d 1)-dimensional probability simplex: d1 := {(p1, , pd) Rd (cid:88) i= pi = 1, pi 0}, (27) natural choice of Riemannian metric on the simplex is the Fisher-Rao metric (Rao, 1992; Amari, 2016). For an interior point d1, the Fisher-Rao metric is defined as follows: gF R(p)[x, y] := x, yp := (cid:29) (cid:28) , = (cid:88) i=1 xiyi pi , x, Tpd1, (28) where the normalization by simplex defined as follows: in the inner product is performed component-wise. This induces geodesic distance on the d(p, q) = 2 cos (cid:32) (cid:88) i=1 (cid:33) piqi , p, d1, (29) where and corresponds to the parameters of categorical distributions. The probability simplex d1 equipped with the Fisher-Rao metric is Riemannian manifold called the statistical manifold of categorical distribution, denoted as P(X ) throughout the paper. The tangent space at an interior point is identified as Tp(P(X )) = {x Rd (cid:80)d i=1 xi = 0}. For further details on the geometry of the statistical manifold, we refer the reader to Ay et al. (2017). Hypersphere The hypersphere Sd1 denotes the (d 1)-dimensional sphere {u = (u1, , ud) (cid:80) + = {u = (u1, , ud) (cid:80) Sd1 embedded into the ambient Euclidean space Rd, which induces canonical inner product (cid:10)x, y(cid:11) := (cid:80)d the tangent space at point Sd1: Tu(Sd1) := {xx, = 0}. For discrete sample space = {1, 2, , d}, there exists diffeomorphism from P(X ) to Sd1 = 1} and = 1, ui 0} denotes positive orthant of Sd1. The hypersphere Sd1 can be i=1 xiyi for x, in + defined as follows: u2 u2 π : P(X ) Sd1 + ; pi (cid:55) ui = pi, π1 : Sd1 + P(X ) ; ui (cid:55) pi = u2 . The diffeomorphism induces the the geodesic distance on Sd1 + : dg(u, v) = cos1u, v, u, Sd1 + , (30) (31) for which the geodesic corresponds to the great circle connecting two points and v. The corresponding exponential and logarithm maps can be computed as follows: expu = cos(x)u + sin(x) , Sd1, Tu(Sd1), expu (v) = cos-1u, sin cos-1u, (cid:16) u, vu (cid:17) , u, Sd1. (32) (33) 12 Continuous Diffusion Model for Language Modeling Additionally, define the radial distance rv(x) := dg(x, v) where dg denotes the geodesic distance defined on Sd1. Then we have the following identities: rv(x) = v, xx (cid:112)1 v, x2 , rv(x) = (d 1) cot(rv(x)), (cid:68) rv(x), rw(x) (cid:69) = v, v, xw, (cid:112)(1 v, x2) (1 w, x2) = v, cos rv(x) cos rw(x) sin rv(x) sin rw(x) . In particular, the logarithm map in Eq. (33) can be represented in radial distance: exp-1 (v) = rv(x)rv(x), (34) (35) (36) (37) A.2. Connection Between Discrete Diffusion Models and Continuous Flow on Hypersphere In this section, we derive the connection between the discrete diffusion models and the continuous flow on hypersphere. Continuous Flow on Hypersphere We first derive useful lemmas for the continuous flows defined on hyperspheres. Lemma A.1. Define flow Yt on Sd in the time horizon [0, ] as follows: dYt dt = log κt dt exp1 Yt (yT ), Y0 = y0, (38) where κt : [0, ] [0, 1] is scalar function satisfying κ0 = 1 and κT = 0, and yT Sd. Then Yt has closed form solution: Yt = sin(θ0 θt) sin θ0 yT + sin θt sin θ0 y0, θt := κt cos-1y0, yT , with the endpoint YT = yT , which corresponds to the spherical linear interpolation, i.e., slerp: Yt = expyT (cid:16) κt exp1 yT (y0) (cid:17) (cid:16) = expy κT exp1 y0 (yT ) (cid:17) . Proof. Let θt := cos-1Yt, yT . Then Yt can be written as follows: Yt = cos θtyT + sin θtwt, where wt Rd+1 is an unit vector. From the definition of θt, we have the following identity: dθt dt = 1 sin θt 1 sin θt (cid:28) dYt dt log κt dt (cid:29) , yT = (cid:28) 1 sin θt log κt dt θt(yT Yt cos θt) sin θt , yT (cid:29) θt 1 cos2 θt sin θt = log κt dt θt, = which yields representation of the flow Yt in Eq. (38) with respect to θ: dYt dt = dθt dt yT Yt cos θt sin θt . Using the result of Eq. (44), we can see that wt is constant vector independent of t: dwt dt = = 1 sin2 θt 1 sin2 θt (cid:20)(cid:18) dYt dt (cid:104) dθt dt cos θt dt (cid:19) yT sin θt (Yt cos θtyT ) (cid:21) sin θt dt (yT Yt cos θt) + sin2 θtyT cos θtYt + cos2 θtyT (cid:105) = 0. 13 (39) (40) (41) (42) (43) (44) (45) (46) Therefore we get the closed form solution for Yt: Continuous Diffusion Model for Language Modeling Yt = cos θtyT + sin θt y0 cos θ0yT sin θ0 = sin(θ0 θt) sin θ0 yT + sin θt sin θ0 y0, (47) where θt = κtθ0 from Eq. (43). Note that the solution Eq. (39) is well-defined in the sense that sin θ0 > 0 always holds. This is because Yt, yT 1 as Yt and yT are on Sd +. Finally, using the definition of θt, we can show the following: exp1 YT (Yt) = θt Yt YT cos θt sin θt = κtθ0wt = κtθ0w0 = κt exp1 YT (Y0), (48) which proves Eq. (40). The following lemma describes the reverse process of the continuous flow Yt described in Lemma A.1. Lemma A.2. For flow Yt on Sd + in the time horizon [0, ]: dYt dt = log κt dt exp1 Yt (yT ), Y0 = y0, the following ODE describes the reverse process Xt := YT t: dXt dt = log κT dt exp1 Xt (y0), X0 = yT . Xt is also spherical linear interpolation with scheduler κt: Xt = expX0 (cid:16) κT exp1 (cid:17) (XT ) = expXT (cid:16) κt exp1 XT (X0) (cid:17) . (49) (50) (51) Masked Diffusion Model Now we show that masked diffusion models correspond to continuous flow on the statistical manifold that starts from an absorbing state. Proposition A.3. Define flow Yt on Sd + in the time horizon [0, ] as follows: dYt dt = log κt dt exp1 Yt (em), Y0 = ek, κt = sin1( αt) 2 π (52) where em denotes the mask token and αt [0, 1] is some differentiable noise schedule satisfying α0 1 and α1 0. Then the random variable Zt := π (Yt) Rd+1 satisfies the following: Zt = αtek + (1 αt)em, (53) which interpolates em and ek on the probability simplex d. Proof. Using Lemma A.1 with θ0 = cos-1em, ek = π/2, we have the following representation of Yt: Yt = sin(θ0 θt)em + sin θtek = 1 αtem + αtek, since θt = sin1( αt). Therefore, Zt has the following closed form: which is random variable on which interpolates between em and ek in straight line. Zt = (1 αt)em + αtek, (54) (55) Continuous Diffusion Model for Language Modeling Note that Zt is random variable on representing the categorical distribution Cat(αtex0 +(1αt)em). This corresponds to the transition distribution q(xtx0) of masked discrete diffusion model, where the transition matrix for the mask diffusion process is given as follows:"
        },
        {
            "title": "Qabsorb",
            "content": "t = αt 0 ... 0 1 αt 0 αt ... 0 1 αt . . . 0 0 ... αt 1 αt 0 0 ... 0 1 αt (56) Corollary A.4. The masked discrete diffusion process can be modeled by continuous flow on Sd the absorbing state em. + that starts from Uniform Diffusion Model We also show that uniform diffusion models correspond to continuous flow on the statistical manifold that starts from the barycenter of the simplex. Proposition A.5. Define flow Yt on Sd1 + in the time horizon [0, ] as follows: dYt dt = log κt dt exp1 Yt (cid:32) (cid:88) i= 1 (cid:33) ei , Y0 = ek, κt = 1 sin-1 (cid:16) d1 cos-1(1/ (1 αt) d) (cid:17) (57) where αt [0, 1] is differentiable noise schedule satisfying α0 1 and α1 0. Then the random variable Zt := π (Yt) Rd satisfies the following: which interpolates (cid:80)d i=1 ei/ Zt = 1 αt (cid:88) i=k ei + 1 + (d 1)αt ek, and ek on the probability simplex d1. (58) Proof. Using Lemma A.1 with θ0 = cos-1 (cid:68)(cid:80)d i=1 (cid:69) 1 ei, ek = cos-1(1/ d), we have the following representation of Yt: Yt = sin(θ0 θt) sin θ0 (cid:88) i=1 1 ei + sin θt sin θ0 ek = (cid:88) i=k sin(θ0 θt) ei + (cid:32) sin θt 1 (cid:33) + sin(θ0 θt) ek. (59) Due to the definition of κt, Zt has the following closed form: Zt = 1 αt (cid:88) i=k ei + 1 + (d 1)αt ek, which is random variable on that interpolates between (cid:80)d i=1 1 ei and ek in straight line. Note that Zt is random variable on d1 representing the categorical distribution: Cat (cid:88) i=x0 1 αt ei + 1 (d 1)α ex0 , 15 (60) (61) Continuous Diffusion Model for Language Modeling which corresponds to the transition distribution q(xtx0) of uniform discrete diffusion model. The transition matrix for the uniform diffusion process is given as follows: Qabsorb = 1 1 ... 1 1 . . . ... 1 1 1 ... 1 (62) Corollary A.6. The uniform discrete diffusion process can be modeled by continuous flow on Sd the barycenter of the simplex. + that starts from A.3. Generative Process on Hypersphere On general manifold M, the logarithm bridge process (Jo & Hwang, 2024) which bridges and is defined as follows: dX t = σ2 (cid:82) σ2 ds exp-1 Xt (v)dt + σtdBM , X0 = where exp-1 () denotes the logarithm map on at point and BM In the case of = Sd, we can derive the logarithm bridge process from to ek: is the Brownian motion defined on M. dX = σ2 (cid:82) σ2 ds ϕt(ek cos ϕtX ) sin ϕt dt + σtdBM where we used the logarithm map of Eq. (33). , ϕt := cos-1X , ek, 0 = u, (63) (64) Radial Process represented as follows: Let rw := dg(w, Xt) for arbitrary point Sd. Then the bridge process from to ek can be dXt = γt (ek cos rk rk t ) sin rk dt + σtdBd+1 , X0 = u, (65) is Brownian motion defined on Sd. The SDE of the radial process rw can be derived using the := rek where rk Itôs formula as follows: and Bd+1 drw = (cid:20)(cid:28) rw , γt (ek cos rk rk t ) sin rk (cid:29) + σ2 2 (cid:21) rw dt + (cid:68) rw , σtdBt (cid:69) , (66) where and denote the Riemannian gradient and the Laplace-Beltrami operator on Sd, respectively. From the identities in Appendix A.1 and the fact that rw is 1-dimensional Brownian motion ((Hsu, 2002) Example 3.3.3), we get the following result: , dBd+1 drw = (cid:20) γt rw ek, cos rk sin rk sin rw cos rw + (d 1)σ2 2 (cid:21) cot(rw ) dt + σtdWt, rw 0 := cos1u, w, (67) where Wt denotes 1-dimensional Brownian motion. For = el, we obtain simplified formulation: drl = (cid:20) σ2 τT τt C(rk , rl t)rl + (d 1)σ2 2 (cid:21) cot(rl t) dt + σtdWt, rl 0 = π 2 δk,l C(rk , rl t) = (cid:40) 1 cot(rk ) cot(rl t) if = otherwise . 16 (68) (69) Continuous Diffusion Model for Language Modeling A.4. Diffusion Mixture Representation We provide the statement of the diffusion mixture representation from Jo & Hwang (2024), which extends Peluchetti (2021) to Riemannian manifolds. We refer the readers to Jo & Hwang (2024) for the derivation. Proposition A.7. For collection of diffusion processes on Riemannian manifold {Qλ : λ Λ} and mixing distribution on Λ, there exists diffusion process on with marginal distribution pt satisfying the following: (cid:90) pt(x) = pλ (x)L(dλ), p0(x) = (cid:90) pλ 0 (x)L(dλ), (70) where pλ denotes the marginal distribution of Qλ. This process is described by the following SDE: (cid:20)(cid:90) dXt = ηλ(Xt, t) pλ (Xt) pt(Xt) (cid:21) L(dλ) dt + (cid:115)(cid:90) σλ(Xt, t)2 pλ (Xt) pt(Xt) L(dλ) dBM , X0 p0 (71) where ηλ and σλ denote the drift and diffusion coefficient of Qλ, respectively. Mixture Paths We derive new family of generative processes by mixing the probability paths of diffusion processes. From the diffusion mixture representation, we construct mixture process by mixing the probability paths {Qi : 1 n} with mixing distribution {λi : 1 n} as follows: Qt := (cid:88) i=1 λi Qi ; pt(x) = (cid:88) i=1 tpi λi t(x) dXt = (cid:88) i=1 Ri(Xt, t)ηi(Xt, t)dt + (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i= Ri(Xt, t)(σi t)2dBt, Ri(x, t) := tpi λi t(x) pt(x) . (72) (73) One example is creating mixture process from the masked bridge mixture and the uniform bridge mixture that share the same noise schedule σt, with mixing probability 1 λt and λt, respectively: (x) + λtpunif λtQmask + (1 λt)Qunif : pt(x) = (1 λt)pmask (74) (x) (cid:32) dXt = γt (cid:88) (cid:20) k= (1 R(Xt, t))pmask (ekXt) + R(Xt, t)punif (cid:21) (ekXt) exp-1 Xt (cid:33) (ek) dt + σtdBt, (75) R(x, t) := λtpunif (x) (x) + λtpunif (1 λt)pmask t (x) (76) A.5. Projected Processes For bridge process from to on Sd, we can derive the projection cv as follows: = Xt, using the Itôs formula for fv(z) := z, dcv = (cid:20)(cid:28) fv(Xt), γt ϕt(ek cos ϕtX ) sin ϕt (cid:29) + 1 2 (cid:21) σ2 fv(Xt) dt + σt (cid:68) fv(Xt), dBd+1 (cid:69) (cid:34)(cid:42) = v, XtXt, γt cos-1cv (cid:112)1 (cv )2 (cid:16) v, XtXt (cid:43) (cid:17) (cid:35) dσ2 cv dt + σt (cid:112)1 (cv )2dWt (cid:20) γt cos-1cv = (cid:112)1 (cv )2 (cid:21) dσ2 2 cv dt + σt (cid:112)1 (cv )2dWt, where we have used the following identities: fv(z) = v, zz, fv(z) = dv, z. (77) (78) (79) (80) Continuous Diffusion Model for Language Modeling Note that the last term dσ2 the radial symmetry of the hypersphere. 2 in the drift corresponds to the Laplacian of the inner product, which has simple form due to Similarly, cu = Xt, can be derived using Itôs formula for fu(z) := z, as follows: (cid:34)(cid:42) dcu = u, XtXt, γt cos-1cv (cid:112)1 (cv )2 (cid:16) v, XtXt (cid:43) (cid:17) (cid:35) dσ2 2 cu dt + σt (cid:112)1 (cu )2dWt (cid:34) = γt cos-1cv (cid:112)1 (cv ) (cid:16) u, cu cv (cid:35) (cid:17) dσ2 2 cu dt + σt (cid:112)1 (cu )2dWt. (81) (82) Masked Diffusion Since the masked bridge process has = em and = ek satisfying em, ek = 0 for all = m, the projected processes are described by the following SDEs: (cid:34) dcl = γt (cid:18) cos-1 ck (cid:112)1 (ck )2 δl,k cl tck (cid:19) (cid:35) dσ2 2 cl (cid:113) dt + σt 1 (cl t)2dW , (83) with initial condition c1:d 0 = 0 where are 1-dimensional standard Wiener processes. Uniform Diffusion The uniform bridge process has = (cid:80)d simple form: i=1 1 ei and = ek, and the projected processes have (cid:34) dcl = γt (cid:18) cos-1 ck (cid:112)1 (ck )2 Al,k cl tck (cid:19) (cid:35) dσ2 2 cl (cid:113) dt + σt 1 (cl t)2dW , (84) with initial condition cl 0 = 1/ d, where Al,k = 1/ for = and Ak,k = 1. A.6. Simulation-Free Training with Radial Symmetry Here we derive the parameters of the Riemannian normal distribution from the projected processes. From the definition cv := Xt, v, we can derive the following: Ecv = EXt, Ez (cid:10) expµt(ρtz), v(cid:11), NTµt Sd (0, I) (cid:28) Eq. (33) = Ez cos(ρtz)µt + sin(ρtz) (cid:29) , (cid:18) = Ez cos(ρtz) µt, (cid:19) (cid:18) + Ez sin(ρtz) (cid:29) (cid:19) (cid:28) , (cid:125) (cid:124) (cid:123)(cid:122) =0 (cid:32)(cid:113) Eq. (22) = Ez cos(ρtz) = Ez cos(ρtz) (cid:18) (cid:42) αt (cid:112)1 u, v2 (cid:112)1 u, v2αt + u, + 1 α (cid:113) 1 α2 (cid:19) , αtu, (cid:112)1 u, (cid:33) (cid:43) u, where the last term in Eq. (87) is zero due to the radial symmetry of z. Similarly, Ecu Ezexpµt(ρtz), = Ez cos(ρtz) (cid:113) 1 α2 , Notably, we have the following identity for NTµt Sd (0, I): Ez cos(ρtz) = eρ2 / 1f1( 2 , 1 2 , ρ2 ) := Fd(ρt), 18 (85) (86) (87) (88) (89) (90) (91) where 1f1 denotes the confluent hypergeometric function. Therefore we have: Continuous Diffusion Model for Language Modeling Ecv = αt (cid:112)1 u, v2 Fd(ρt), Ecu = (cid:32)(cid:113) 1 α αtu, (cid:112)1 u, v2 (cid:33) Fd(ρt), and the parameters αt and ρt can be derived from the mean projections Ecv (cid:115) αt = (rt u, v)2 1 u, v2 + (rt u, v)2 , rt = Ecv Ecu B. Experimental Details B.1. Text Generation and Ecu : (cid:32) Ecu , ρt = 1 (cid:112)1 α2 (cid:33) , (92) (93) Baselines We compare against state-of-the-art diffusion models. Multinomial Diffusion (Hoogeboom et al., 2021), D3PM (Austin et al., 2021), SEDD (Lou et al., 2024), MDLM (Sahoo et al., 2024), MD4 (Shi et al., 2024) are discrete diffusion models. Plaid (Gulrajani & Hashimoto, 2024) and Bayesian Flow Network (BFN) (Graves et al., 2023) are continuous diffusion models. We do not use existing works for flow matching on the statistical manifold (Cheng et al., 2024; Davis et al., 2024) as do not provide likelihood computation applicable for language modeling. We also use the transformer AR model (Vaswani et al., 2017) and the following autoregressive models as baselines: IAF/SCF (Ziegler & Rush, 2019), AR Argmax Flow (Hoogeboom et al., 2021), and Discrete Flow (Tran et al., 2019) are flow-based models, and ARDM (Hoogeboom et al., 2022) and MAC (Shih et al., 2022) are any-order autoregressive models. Text8 Text8 (Mahoney, 2006) is small character-level text modeling benchmark extracted from English Wikipedia. Following the previous works (Austin et al., 2021; Lou et al., 2024; Sahoo et al., 2024), we split the dataset into 90M/5M/5M with fixed sequence length of 256. We use vocabulary size of 28, comprising 26 lowercase letters, white space token, and mask token. We use 12-layer diffusion transformer (Peebles & Xie, 2023) following Lou et al. (2024) with 92.4M trainable parameters. We train our model for 1M iterations with batch size 512 as done in previous works, using the same learning rate, optimizer AdamW (Loshchilov & Hutter, 2017), and exponential moving average (EMA) with decay rate 0.9999. One Billion Words One Billion Word Benchmark is dataset extracted from the WMT 2011 News Crawl dataset comprised of single sentences from news articles. Following Sahoo et al. (2024), we use the bert-base-uncased tokenizer and pad and truncate the sequences to length 128. We use 12-layer diffusion transformer (Peebles & Xie, 2023) with hidden dimension of 768 and 12 attention heads, following Sahoo et al. (2024) with 110M trainable parameters. We train our model for 1M iterations with batch size 512 as done in previous works, using the same constant learning rate, optimizer AdamW (Loshchilov & Hutter, 2017), and exponential moving average (EMA) with decay rate 0.9999. B.2. Pixel-level Image Modeling Baselines We compare against autoregressive models and diffusion models that directly model raw pixel space. PixelRNN (van den Oord et al., 2016b), Gated PixelCNN (van den Oord et al., 2016a), PixelCNN++ (Salimans et al., 2017), PixelSNAIL (Chen et al., 2018), Image Transformer (Parmar et al., 2018), and Sparse Transformer (Child et al., 2019) are autoregressive models. D3PM (Austin et al., 2021), τ LDR (Campbell et al., 2022), and MD4 (Shi et al., 2024) are discrete diffusion models. Implementation Details We represent each image as set of discrete tokens with vocabulary size of 256. We use the 10-layer diffusion transformer (Peebles & Xie, 2023) for our model with 35M trainable parameters. We train 100k iterations with batch size 128 and AdamW (Loshchilov & Hutter, 2017) optimizer following Shi et al. (2024). B.3. DNA Sequence Design The dataset contains 100k promoter DNA sequences each paired with transcription signal profile. Each sequence consists of 1024 base pairs centered at the annotated transcription start site position (Hon et al., 2017), and the base pair has 4 categories (ATGC) conditioned on the profile. 19 Continuous Diffusion Model for Language Modeling Figure 1: Comparison between the training objectives. We compare Bits Per Character (BPC) on the Text8 test set. Figure 2: Analysis of the dimension splitting (Section 5). We compare NLL on LM1B test set. Top-K Feat. denotes adding additional features of top-k indices of the input state."
        },
        {
            "title": "Method",
            "content": "Drift MSE (Eq. (18)) Cross Entropy (Eq. (19)) Cross Entropy + Importance Sampling BPC () 1.40 1.39 1.32 Method w/o dimension splitting w/o dimension splitting + Top-K Feat. w/ dimension splitting NLL () 11996.9 661.1 434.2 Figure 3: Maximum mean discrepancy (MMD) distance between the simulated distribution p(XtX0, X1) and the approximated distribution. We report the results for dimension 4, 256, and 30522. Baselines We compare our model against diffusion models and language models. Bit Diffusion (Chen et al., 2023) is continuous diffusion model, D3PM (Austin et al., 2021) is discrete diffusion model, DDSM (Avdeyev et al., 2023) and Dirichlet Flow Matching (Stärk et al., 2024) are diffusion model and flow matching model using the probability simplex, respectively. Fisher-Flow (Davis et al., 2024) is flow matching model using statistical manifold. Implementation Details Following the previous work (Stärk et al., 2024; Davis et al., 2024), we use the same data split of 88,470/3,933/7,497 and identical model architecture consisting of 20-layer 1-D CNN with 13.3M trainable parameters. We train our model for 100k iterations with batch size 256 and AdamW (Loshchilov & Hutter, 2017) optimizer. We evaluate the MSE on the generated samples conditioned on the prescription signals from the test set, using 300 generation steps following the previous work (Davis et al., 2024). C. Generated Samples C.1. Text We provide uncurated text samples generated by our RDLM trained on the Text8 dataset. zero one british single payrock neurologically related condition is member of the original playboys oriental pbkr cat ii boob one card featured in the late one zero dippie dons as it became pigus in the cir the monoseur engine shair which became th delivered from the new meeting the construction of modern shooting begins kinington resurrects the hark or corped hopper nightlife subjecting to turn his attention at joyable moment he is able to explain that he is in recovery with new orleans baby wilder unrefreshed bup of lightmarks was pertified only at the head of sinar Continuous Diffusion Model for Language Modeling joseph avaret in the cetleben key in one nine nine seven this report has been portrayed as shrinking feathor of the civil directs against urban rumour as that he was ana eichy seven two chromosomes regainally regular and contain number of mignain gnaning pros zopods or cells whose podic configuration divided agong the faces of dna generally replaced by as therus group are non mit and elanisten special cayits regularly are ca nine four although portrayals of frel appearance the novel include leaked to bratally targeted audiences largely by steve roper dart mer upick and pernan durk born one nine four zero but stillly not they are created the western master and mag both idment indicates two different types drop tales have different charges which train structures having rare and light weight variations have lower weight impedients such as chawings starges and groove gloves shorter holes can be jumpliten don badld horse deliberately rejected this different post however saw al sh ibn misha rody was revealed to be the lord curses of jesus one nine one nine he handled his journey to its historical map of the egyptians and was still nodged as he committed to reproete he ovincial governors regelrant cursami governor granted to spanish cominic in one seven eight three mateo teltacheutes lebmo alexius jeano and pan dosien dostre of ruguen de cosst originating specifically the treaty of st louis the extinctions remain C.2. One Billion Words We provide uncurated text samples generated by our RDLM trained on the LM1B dataset. [CLS] social recklessly the obvious support 2013. [CLS] they were elected off by the english authorities, whose party subsequently named as principal when lawrence tang had to hold the property until they were turned to down their heads in the back - sky of which sank from matthewss doorstep. been pouring gladly with work and along the motorway, where certified sales will follow new bone in the next several days to avoid commercial production problems, according to recommendations from both workplace and tropical mod. [CLS] he said he plans watchsty will greens the old draft plunging sara, but have medics announced she would make you the taxpayer? [CLS] duchess [CLS] [CLS] it has [CLS] of lieberman. [CLS] analysts say since 5, 000 people have held established council in 120 forums and levels, some have returned to the villages of the british capital, mideast and sprint. ironing his body they forbid forrest. subcontinent and two development employees suffered injuries in securing of greece, spokeswoman said immediately, while tneye wedang. has already been considered. [CLS] jackie has an hopeful major interest for dirty potter, pilots bullocks show, whether they have what hugh and mariusa other, no - shame roots [CLS] [CLS] seven babies missing and 27 french [CLS] his friends ring between [CLS] both questions [CLS] is the problem that worth most of marriage to have single car he doesnt need. [CLS] mr obama will carry out more casualties however than president obamas followers, and it mild to form the first cumulative current division ofers holding the guantanamo men that arches to injustice. phillips said : \" designer kaia kangaroo, 27, and herself rubbed jim reyes, the [CLS] 21 Continuous Diffusion Model for Language Modeling general patron of france light, have organized building aimed at gunning film houses. exhibit mall in fasside, marked since the work are new sport, smaller schools racing has more [CLS] [CLS] at riding, london graduate college in edinburgh and temporary [CLS]aceous that in spain had submitted one time the main website on mass wireless, in carpcsllo. [CLS] not two of the beer bk known in the companies could have thousand stretch men - - ginger, and showed vulnerable cases, leaving you in the same 200m standard. [CLS] yet apius is accepted quickly to associate in the months since - - bulletin energy americas - - they agreed that it was getting waste into ulysses air before creation known as the bulletinsburg, which can be bowed with bracelet growth by speed. energetic first - turn victory. [CLS] more than 2, 000 people arrived, out [CLS] [CLS] rely will get another less [CLS] more steadily increasing transit facilities with murrays tax breaks. [CLS] nonero moee enjoyed terrestrial wallino with the immoitunghrck in most years. [CLS] those who run on hard sling are good with childhood often or later in short - term temperatures. and isatin out in stanford. [CLS] downing : didnt say in new hampshire and arkansas four years ago, vaclav with worldwide gains. his way to combat [CLS] [CLS] even if the huckabee god had \" the black annesies \" chosen to go on [CLS] top - seeded henin is shark seventh richard finally happy huckabee, who [CLS] high school, was potyas poker high - george she - former congressional class - flicked was prosecutor. [CLS] coln has won the services of the sub - area tustiw university, near fort dodge, pa. [CLS] one is the daughter of metro with problem but tough neighborhood, retirement campus which, on that day, was published by hyde for the little - class united states attorney. [CLS] lets sell floral parachute in civil court on lutheran case. virginia government says the ad, which will add its new poll kind wednesday, had 10, drastically supervisors and 25 people. [CLS] [CLS] [CLS] the [CLS] memorandum posted to the university : model google, which makes the copies to sell patients seem off significant stake in every final - ep you programmes similar. [CLS] almost no day cbees will homemadei. raf had sincerity at her twins guilty of battling \" apology from the bishops. \" [CLS] the courts have replayled their option forwelcome when the fed tends its view of the aec investorschance. for the milestone but on wednesday with their hay at jade bridge, was doing the champagne board without everyone quarter mips visit overnight. [CLS] that veteran, who claimed aredell mol [CLS] many in the [CLS] [CLS] the bbcs george washington is the first of 15, 000 people to put the calraircer range. [CLS] the uks \" arp \" drilled fence in the construction of eu hospitals on the trunk network as one of africas most damaging places. [CLS] all looked after world over just um occasionallytau, which takes place victorious for schizophrenia consumed near the doc centre. profits, not the greek pilot anchors, some of whom the very top cruise lay in the deep west of britain, which threatens developing dozens, and joined conference in america to provide full grand theft pad to [CLS] [CLS] it is complicated by D. Future Directions While our experiments were conducted with models of small parameter size, scaling up the number of parameters would demonstrate new possibilities, in particular on reasoning or planning abilities. Moreover, our framework can be extended to controllable text generation utilizing the guidance methods (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) of continuous 22 Continuous Diffusion Model for Language Modeling diffusion models, which we leave as future work. Another interesting direction is developing autoregressive-like diffusion language model, which could be stuided by controling the noise schedule. In this work, we use the same noise schedule for the bridge processes for simplicity. Yet, the noise scheduler could be used to control the convergence speed of the tokens in different positions, for example, converging in order from left to right as in autoregressive models. Lastly, while we focus specifically on language modality, our experiments show that RDLM could be used on different modalities such as image modeling or DNA sequence design. Promising directions would be exploring applications to domains where continuous diffusion models have been successful, for example, graph generation (Jo et al., 2024) or molecule synthesis (Jung et al., 2024)."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "Korea Advanced Institute of Science and Technology (KAIST)"
    ]
}