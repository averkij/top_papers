{
    "paper_title": "Modular Neural Image Signal Processing",
    "authors": [
        "Mahmoud Afifi",
        "Zhongling Wang",
        "Ran Zhang",
        "Michael S. Brown"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM"
        },
        {
            "title": "Start",
            "content": "Ran Zhang Michael S. Brown AI Center-Toronto, Samsung Electronics {m.afifi1, z.wang2, ran.zhang, michael.b1}@samsung.com 5 2 0 2 9 ] . [ 1 4 6 5 8 0 . 2 1 5 2 : r Figure 1. We present modular neural image signal processing (ISP) framework that offers full control over every stage of the pipeline and can handle unseen cameras without requiring re-training. On top of this framework, we built user-interactive tool that supports post-editable re-rendering, allowing users to re-process saved outputs with different picture styles and manual adjustments. The shown image was captured in raw format using the iPhone 13 main camera, then denoised and processed by our modular ISP, with intermediate stages and multiple picture-style and manual-adjustment results displayed. None of our models were trained on data from iPhone cameras."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction and Related Work This paper presents modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces high degree of modularity, providing full control over multiple intermediate stages of the rendering process. This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is fully learning-based framework with variants of different capacities, all of moderate size (ranging from 0.5 to 3.9 parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Image signal processing (ISP) is set of computational operations, typically organized as sequential pipeline, that transform linear raw sensor data into display-referred, highquality images [34]. These operations include raw image enhancement [23, 32, 57, 59, 79], white balancing and color-space conversion [14, 18, 22, 24, 46], and various quality enhancement [2, 39, 51, 77, 78, 83, 90], each targeting specific stage of the pipeline and collectively often requiring careful calibration and engineering to function as unified ISP system. Recent learning-based methods model the entire ISP pipeline as single black-box neural network trained endto-end to map raw images from specific camera to their display-referred outputs (e.g., [44, 48, 49, 69, 70, 85, 87, 91]). However, such monolithic designs tend to generalize poorly to unseen cameras, as their learned mappings are tightly coupled to the characteristics of the training camera [6, 68]. Moreover, many of these networks (e.g., [44, 48, 70]) have high memory and computational requirements, limiting their practicality for deployment scenarios such as on-device processing or real-time photo ren1 dering. Beyond computational cost, these black-box ISPs are difficult to interpret, debug, or extend in real-world deployments, where continuous improvement and scalability are critical (e.g., supporting new picture styles or handling user-specific corner cases [3, 4, 17, 27, 41]). few recent attempts have explored multi-stage or modular ISP designs (e.g., [10, 33, 53, 58, 60, 76]); however, these approaches remain limited in several ways. Some adopt overly generic stage definitions (e.g., restoration vs. enhancement [58], or local vs. global stages [10, 53]), while others require post-training fine-tuning providing no assurance that the stages retain their intended functionality and thereby reducing interpretability [60]. Another line of work relies on commercial raw-processing pipeline to generate training data for each stage [76]. However, the pipeline is closed-source, and intermediate-stage outputs are difficult to access, with limited information available about its internal processing order. Others operate on non-linear display-referred images, without clear scheme for adapting them to the linear raw domain [33]. This research gap motivates us to develop learningbased modular ISP framework that achieves high-quality results while maintaining high degree of modularity and interpretability to facilitate scalability, debugging, and flexibility. Unlike conventional neural ISPs, our method explicitly decomposes the pipeline into well-defined, learnable stages, each responsible for specific part of the image rendering process. This structure not only preserves functional transparency but also allows individual modules to be improved, replaced, or reused across cameras and picture styles without retraining the entire system. With this modular design, we achieve competitive image quality while gaining the ability to analyze and isolate the causes of corner cases, replace camera-specific modules with generic ones for unseen cameras, and extend the framework to additional picture styles without duplicating the full pipeline. Moreover, the modularity provides foundation for interactive and user-controllable image rendering. To showcase this flexibility, we integrate several image-editing operators directly into the learnable pipeline, allowing users to interactively adjust the final image appearance (see Fig. 1). Contributions. We introduce fine-grained modular ISP framework that provides explicit control over the entire raw rendering process through well-defined, interpretable components, while supporting diverse picture styles with lightweight to moderate model capacity. Our framework achieves state-of-the-art results across multiple picture styles and offers full user control over the rendering pipeline for further customization. To demonstrate its practicality, we develop an interactive photo-editing tool built upon our modular ISP, enabling users to process raw images from unseen cameras, select or interpolate between picture styles, and apply photo-editing adjustmentsall within fast and lightweight system. Furthermore, we describe how the tool supports unlimited re-rendering of saved images by embedding raw data within output JPEGs, and how it extends to editing standard sRGB images produced by unknown thirdparty cameras or software. 2. Method Given demosaiced raw image Iraw RHW 3, our objective is to render high-quality, display-referred version Iout RHW 3 in standard color space (assumed to be sRGB in this paper) that closely matches groundtruth reference IGT rendered from the same raw image under specific picture style. Beyond high-quality rendering, our goal is to maintain high degree of modularity across the rendering process, with interpretable stages that enhance scalability, facilitate debugging, and provide finegrained control over the entire pipeline. To this end, we propose learning-based modular ISP framework, illustrated in Fig. 2. The framework consists of raw enhancement stage (Sec. 2.1), followed by color correction stage (Sec. 2.2) that outputs ILsRGB in linear sRGB color space. This image is then downsampled to 4 3 and processed by modular photofinishing module (Sec. 2.3) to render the image, after which guided upsampling step (Sec. 2.4) produces the photofinished result at the original input resolution. Lastly, this image is refined through detail-enhancement process (Sec. 2.5) to generate the final output Iout. Each learnable stage in the pipeline is trained independently to preserve stage-level modularity, allowing individual stages to be replaced or updated without retraining the entire framework. The remainder of this section describes our framework pipeline, whereas network architecture details are provided in Sec. of the supplementary material. LsRGB 4 2.1. Raw Enhancement Raw images often exhibit noticeable noise and detail loss, particularly in dark or underexposed regions, due to low photon counts and sensor imperfections [28]. Among various raw degradations, we focus on denoising, as it is critical for preserving fine details in subsequent ISP stages [1]. While multi-frame burst denoising [35, 40, 42, 66] achieves high quality, it requires multiple captures of the same scene. To maintain broad applicability (supporting both on-device and software-based rendering), our framework adopts single-image raw denoising: Ienh-raw = fenh-raw(Iraw) , (1) where fenh-raw() denotes our denoising function, implemented as fully convolutional network Draw. We train Draw in supervised manner using pixel-wise ℓ1 loss between the predicted and ground-truth raw images. 2 Figure 2. Overview of our modular framework. The pipeline begins with image denoising, followed by color correction to map the denoised raw image to the linear sRGB space. The photofinishing module then processes downsampled version of the linear sRGB image through five parametric stages, where neural networks predict image-based parameters for each stage: digital gain map, global tone mapping (GTM), local tone mapping (LTM), chroma mapping, and gamma correction. guided upsampling step, using the full-resolution linear sRGB image as guidance, reconstructs the full-resolution photofinishing output, which is then refined by detail-enhancement stage to produce the final image. The shown example is from the S24 dataset [16]. Ground-truth images are expected to have minimal noise and can be generated using third-party denoisers (e.g., the AI-based Adobe Lightroom denoiser) rather than physically captured references [1], which are time consuming to acquire and often lack scene diversity. This practical choice balances accuracy and generalization, allowing training on broader range of raw data. Although such pseudo ground truths are imperfect, they provide stable supervision and help preserve fine image structures. workflow for generating pseudo ground truths when third-party denoisers are unavailable is described in Sec. B.2 of the supplementary material, with additional training details in Sec. G.1. 2.2. Color Correction After raw denoising, we apply color-correction function frawLsRGB() that maps the denoised raw image to the linear sRGB domain. This function places the image in camera-agnostic color space, making subsequent operators less dependent on camera-specific characteristics unlike denoising and color correction, which are inherently camera-dependent. The function frawLsRGB is defined as: ILsRGB(x,y) = MCCM (cid:0)DWB Ienh-raw(x,y) (cid:1) , (2) where DWB is diagonal matrix encoding the red and blue white-balance (WB) gains, and MCCM is the color correction matrix (CCM) interpolated based on these gains using pre-calibrated camera-specific matrices [52]. WB gains are typically stored in DNG metadata and estimated by the cameras on-board auto white balance (AWB) module, but can also be predicted using learning-based AWB methods (e.g., [20, 52, 62]). Further discussion of learning-based AWB integration is provided in Sec. I.3 of the supplementary material. 2.3. Photofinishing The photofinishing module finalizes the images overall look and feel, covering both perceptual enhancements and artistic picture styles. Rather than single black-box network, we design it as modular module, since this stage largely determines the final visual appearance and is key to maintaining flexibility. As illustrated in Fig. 2, the module operates on the downsampled image LsRGB for efficiency and comprises five steps: 1) digital gain to adjust brightness, 2) global tone mapping (GTM) to refine global contrast, maintain perceptual brightness, and preserve highlights, 3) local tone mapping (LTM) to enhance local contrast and details, 4) chroma mapping to adjust chromaticities, and 5) gamma correction to produce display-referred output. These are implemented by the functions fgain, fGTM, fLTM, fchroma, and fgamma, each parameterized by image-specific coefficient(s) predicted by lightweight neural networks (200K parameters in total): Dgain, DGTM, DLTM, Dchroma, and Dgamma. key challenge is the lack of ground-truth supervision for individual photofinishing functions, making independent optimization infeasible. We therefore train the entire module end-to-end, with the main difficulty being to ensure that each function performs its intended role (e.g., GTM adjusts global tone rather brightness). Our design encourages such separation and interpretability, as detailed below. The process begins with digital gain adjustment: gain = fgain (cid:16) LsRGB; dg (cid:17) = dg LsRGB, (3) where dg is global gain factor predicted by Dgain. Next, tone mapping refines image contrast while maintaining perceptual brightness. Although one could apply tone mapping only to the luminance channel (i.e., leaving chroma to be modified solely by fchroma), we found this design to 3 Figure 3. User-interactive photo-editing tool built on our modular ISP, providing full control over the rendering process, picture styles, and editing options. The interface supports selecting or interpolating between styles and adjusting white balance, exposure, color, and overall appearance. See the supplementary material (Sec. I) and the video (click to view) for details. yield suboptimal results (see Sec. K.1.4 of the supplementary material). Instead, tone mapping is applied directly to the scaled linear sRGB channels of gain, treating all channels equally. The GTM function, fGTM, is formulated as: I(ρ) GTM(x,y) = fTM (cid:0)I(ρ) gain(x,y); aGTM, bGTM, cGTM (cid:1), (4) where (x, y) indexes spatial locations and ρ {R, G, B}. Here, fGTM is simply an application of the shared tonemapping function fTM using global parameters. The tonemapping function fTM is defined as: fTM(x; aTM, bTM, cTM) = aTM aTM + (cid:0)cTM (1x)(cid:1) bTM , (5) where [0, 1] is the normalized RGB intensity. The image-specific parameters aGTM, bGTM, and cGTM, predicted by DGTM, primarily control: 1) midtone contrast through the exponent aGTM, 2) shadow compression via the slope parameter bGTM, and 3) highlight roll-off by scaling cGTM, which modulates the curvature at the upper end of the tonemapping curve. 4 The LTM stage complements the GTM by providing spatially adaptive tone control. To achieve this, DLTM comprises two subnetworks: 1) multi-scale guidance subnetwork that outputs guidance map Gguide 4 1, and 2) grid-prediction subnetwork that processes both gain and GTM (concatenated along channels) to predict coarse grid of tone-mapping parameters MLTM RNgNgNc5, where Ng, Nc H/4, W/4. Including both inputs provides the grid-prediction subnetwork with cues about the global tone-mapping behavior (from GTM) and the pre-tonemapped image content (from gain), helping it predict the coefficients applied to both inputs by the LTM function fLTM, defined as: LTM(x,y) = (1 WLTM(x,y)) I(ρ) I(ρ) (cid:0)X(ρ) BLTM(x,y), CLTM(x,y) + WLTM(x,y) fTM GTM(x,y) (cid:1), LTM(x,y); ALTM(x,y), (6) where XLTM(x,y) is the locally scaled version of the gainadjusted linear sRGB image: LTM(x,y) = I(ρ) X(ρ) gain(x,y) GLTM(x,y), (7) and the spatial coefficient maps: ALTM, BLTM, CLTM, GLTM, LTM 4 are generated by sampling the grid MLTM via trilinear interpolation using the guidance map Gguide. The preliminary map LTM is then passed through sigmoid activation to obtain WLTM. This formulation enables the LTM to build upon the globally tone-mapped image while flexibly re-tone-mapping the gain-adjusted image as needed. See Sec. K.1.4 in the supplementary material for ablations. After tone mapping, chroma mapping refines the images color components for enhancement or artistic stylization. We implement this step using fchroma, an imagespecific learnable 2D chroma LuT operating in the CbCr space. We opt for 2D chroma LuT instead of predicting an image-specific 3D RGB LuT to reduce memory usage and ensure that this stage affects only chromaticity. Specifically, the tone-mapped image LTM is converted to YCbCr1. The chroma network Dchroma computes differentiable 2D histogram of the CbCr channels (see Sec. in the supplementary material for details), which is processed by learnable encoder-decoder network. The encoder extracts chroma features, which are modulated by latent vector derived from the channel. This vector is produced by lightweight luminance-guidance subnetwork whose sigmoid-activated output scales the encoded chroma features for brightness-dependent modulation. The modulated features are decoded to predict residual 2D LuT in CbCr, added to learnable global base LuT to form the final Lchroma. The LuT is applied to the chroma channels via bilinear interpolation, producing chroma-mapped CbCr values that are combined with and converted back to pre-gamma, quasi-linear sRGB image, chroma. For artistic picture styles that involve stronger or more stylized color manipulations, we found that augmenting fchroma with an image-independent learnable 3D LuT improves color expressiveness. This optional step performs 1We use the YCbCr RGB conversion matrices defined in ITUR BT.709, assuming 2.2 gamma-encoded sRGB space. Although our input is not strictly 2.2 gamma, we adopt this approximation for simplicity and differentiability. trilinear lookup over an 111111 LuT (LRGB) on the tone-mapped RGB values before Dchroma and fchroma. The 3D LuT can capture rich color transformations that complement the subsequent 2D chroma LuT (Lchroma), enhancing expressiveness in artistic picture modes. However, since learning LRGB can interfere with the intended role of other tone-mapping stages, we keep it optional. At the final stage of photofinishing, we apply gamma correction using fgamma, defined as: gamma = fgamma (cid:16) chroma; γ (cid:17) (cid:16) chroma = (cid:17)(1/γ) , (8) where γ is an image-specific factor predicted by Dgamma. We train the five networks of our photofinishing module (Dgain, DGTM, DLTM, Dchroma, and Dgamma), and optionally LRGB, jointly in an end-to-end manner by minimizing: Ltotal = λ1 ℓ1 + λSSIM ℓSSIM + λE ℓE + λperc ℓperc + λCbCr ℓCbCr + λLuT-s ℓLuT-s + λTM ℓTM (9) + λLTM-s ℓLTM-s + λluma ℓluma. The total loss integrates fidelity, perceptual, and regularization components. Low-level terms (ℓ1, ℓSSIM, ℓCbCr) ensure pixeland structure-level accuracy; ℓCbCr measures chroma error between the predicted and de-gammaed (using the predicted γ) ground-truth CbCr channels. Perceptual terms (ℓE and ℓperc) enforce perceptual color and feature similarity, where ℓE is differentiable CIE metric and ℓperc is VGG-based. Regularization terms (ℓLuT-s, ℓLTM-s, ℓTM, and ℓluma) stabilize learning and improve interpretability: ℓLuT-s and ℓLTM-s are total-variation smoothness penalties for Lchroma and the LTM coefficient maps. ℓTM enforces luminance consistency between the downsampled global and full-resolution local tone-mapped channels and the channel of the de-gammaed ground truth, balancing global contrast and local detail refinement, while encouraging the GTM and LTM subnetworks to complement each other (avoiding dominance by either). Finally, ℓluma regularizes the GTM stage to preserve the average brightness of the gain-adjusted image, ensuring contrast refinement without altering global brightness. The coefficients λj control the relative strength of each term. Details of the loss definitions, weighting factors, training setup, and ablation studies are provided in the supplementary material (Secs. F, G.2, and K.1.3). 2.4. Upsampling LsRGB, and produces For efficiency, photofinishing is performed on downscaled image, gamma, which is then upsampled using the high-resolution linear sRGB image ILsRGB as guidance. We adopt bilateral grid upsampling (BGU) [29], which computes an affine transform per grid cell by solving regularized least-squares system. The original Halide BGU constrains regularization to be achromatic (a single scalar gain across channels) and uses grid blurring to handle empty cellsboth of which introduce limitations: 1) enforced achromaticity causes color crosstalk, and 2) grid blurring trades off detail for smoothness. We address these issues with per-channel gated regularization that removes the need for grid blurring. Each cell is regularized independently by channel, while empty cells fall back to global per-channel gains derived from pooled grid statistics. This design yields sharper, more faithful reconstructions (see Sec. of the supplementary material for details). The guided upsampling produces the photofinished image gamma RHW 3, which is then refined by the detailI enhancement stage. 2.5. Detail Enhancement The final stage of our pipeline applies detail-enhancement step to compensate for residual artifacts from denoising and guided upsampling. The output image Iout is obtained as: Iout = fenh (cid:0)I gamma (cid:1) , (10) where fenh() is implemented as compact fully convolutional network Denh. To train Denh, we first generate gamma for each training image using all preceding stages (with pretrained models), and use them as inputs. The network is optimized with pixel-wise ℓ1 loss between the predicted and ground-truth sRGB images. Additional details are provided in Sec. G.3 of the supplementary material. We explored merging the rawand detail-enhancement stages by fine-tuning Draw after training the photofinishing module, aiming to pre-enhance image details before resampling and thereby mitigate detail loss. However, this approach was unstable and often failed to converge. Using Denh as separate final stage (with only 50K parameters) proved more robust and easier to train. 2.6. Photo-Editing Tool To demonstrate the advantages of our modular design, we developed user-interactive photo-editing tool built on top of our neural ISP. The tool provides full control over the entire rendering process, supporting multiple picture styles and additional editing adjustments (e.g., highlights, shadows, contrast, and exposure) directly within the ISP pipeline (see Fig. 3). To enhance camera-agnostic usability, we integrated generic denoiser trained on diverse set of synthetic and real noisy images, aimed at improving robustness to unseen noise patterns (see Sec. of the supplementary material). The modular design also allows users to either apply the cameras AWB estimates or recompute WB gains using recent illuminant estimation models [11, 16, 92], including both camera-specific [16] and cross-camera variants [11]. Figure 4. Qualitative comparison between our method and recent neural ISP methods (ISPDiffuser [70], LiteISP [91], and ParamISP [53]) on an example from the S24 test set [16]. Results are shown for the default picture style (Style #0) and the remaining artistic styles (Styles #15). PSNR values with respect to the ground truth are shown in the lower-left corner of each image. We draw inspiration from recent work on raw image compression [15, 56, 80] and use the learning-based method of [15] to embed the compressed raw data into the final JPEG image. This enables unlimited post-editable re-rendering under new settings with only modest file size increase. Additionally, lightweight linearization network [10] is included to synthesize raw-like representation from external sRGB images, allowing the tool to process native DNG files, sRGB JPEGs saved by our tool with embedded raw, or standard sRGB inputs. Despite its versatility, the entire tool (including photofinishing networks for multiple picture styles) requires only 3.9 parameters, far fewer than competing neural ISPs (e.g., ISPDiffuser [70], 20.9 parameters for single style; see Table 1 for parameter comparisons across methods). Comprehensive details and additional demonstrations are provided in the supplementary material (Sec. I) and in the accompanying video (click to view). 3. Experimental Results We evaluated our modular ISP framework against recent neural ISP methods, including black-box end-to-end models (PyNet [48], LAN [69], LiteISP [91], InvertibleISP [85], FourierISP [44], MicroISP [49], ISPDiffuser [70]) and multi-stage architectures [10], ParamISP [53], and FlexISP [60]). We used the S24 (CIE XYZ Net dataset [16], which provides all data needed to train and evaluate our framework. The dataset includes pseudo ground-truth denoised raw images generated by Adobe Lightrooms AI-based denoiser (for training our denoising module) and six ground-truth sRGB images per raw input, corresponding to one default style (Style #0) and five artistic styles (Styles #15), making it suitable benchmark for evaluating the scalability of our method in handling multiple styles. The dataset comprises 2,619 training, 205 validation, and 400 test pairs. For results on the MIT-Adobe FiveK dataset [26], see Sec. K.2.2 in the supplementary material. We trained three denoising variants (Draw): lite (0.25 parameters), base (0.93 M), and large (3.6 M), described in Sec. C.1 of the supplementary material. These yield three configurations of our full pipeline. Each module (denoising, photofinishing, and detail enhancement) was trained separately. Owing to our modular design, only the photofinishing and detail-enhancement networks are style-specific, while the denoiser is shared across all styles. This greatly reduces training and memory requirements when supporting multiple picture styles, unlike prior methods that must retrain and load the entire ISP pipeline into memory to adapt to new styles. All baselines were trained per style using their official implementations (see Sec. in the supplementary material for details). 6 Figure 5. Comparison among Project Indigo [55], the iPhone native camera ISP, and our method (using the generic denoiser and cross-camera auto white balance). The image was captured using the iPhone 13 Pro Max main camera. 3.1. Quantitative Results We report the peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) [81], LPIPS [89], and E2000 [72] for quantitative evaluation. Table 1 summarizes results on the default S24 style along with each models parameter count. We present results for our three variants (lite, base, and large) and ablations with/without detail enhancement. Further ablations are detailed in Sec. K.1 of the supplementary material, covering: 1) denoising (Sec. K.1.1), 2) guided upsampling (Sec. K.1.2), 3) photofinishing loss design (Sec. K.1.3), and 4) photofinishing design (Sec. K.1.4). As shown in Table 1, our approach achieves state-of-the-art results across all variants (even the lite model with 0.5M parameters), while LiteISP [91], the closest competitor, uses 9M parameters and yields 2 dB lower PSNR. Table 2 reports PSNR for all artistic styles (Styles #15), with full metrics (SSIM, LPIPS, and E2000) provided in Sec. K.2.4 of the supplementary material. For completeness, we also compare with cmKAN [67], lightweight color-matching method trained to transfer colors from the default style to the remaining styles. At test time, we rendered images using our pipeline in the default style and applied cmKAN as post-processing color transfer (denoted PP (cmKAN) in Table 2). This experiment aims to highlight the advantage of performing style rendering directly from raw data vs. applying color transfer as postprocessing step after rendering to the default style. We further report the total parameters required to support all five artistic styles. Because our denoiser is shared, the added cost per style is minimala key benefit of our modular design. Table 2 lists results for our lite, base, and large variants with/without detail enhancement and the optional 3D LuT (LRGB). As shown, our method achieves state-of-the-art results across all styles while requiring moderate number of parameters to support multiple styles compared to other methods. Furthermore, learning LRGB consistently improves the final results. However, we found that when training for the default stylewhich primarily targets natural color rendering and high image quality rather than aggressive artistic Table 1. Results on the S24 test set [16]. We report PSNR, SSIM [81], LPIPS [89], and 2000 [72], along with the total number of parameters for each method. Our method is evaluated with different denoising model capacities (lite, base, and large), with and without the enhancement network. The best results are highlighted in yellow. Our method achieves state-of-the-art results, offers modular design with full ISP control, requires moderate number of parameters, and runs efficiently on single GPU (0.7 sec with the lite denoising model, 0.95 sec with the base model, and 1.4 sec with the large model on an NVIDIA GeForce RTX 4080 SUPER). Method ISPDiffuser [70] PyNet [48] CIE XYZ Net [10] FlexISP [60] Invertible-ISP [85] LAN [69] MicroISP [49] ParamISP [53] LiteISP [91] FourierISP [44] Ours (lite, w/o enhancement) Ours (base, w/o enhancement) Ours (large, w/o enhancement) Ours (lite, w/ enhancement) Ours (base, w/ enhancement) Ours (large, w/ enhancement) PSNR 24.03 23.80 23.32 24.01 22.87 23.11 20.55 24.32 25.49 24.50 26.36 26.48 26.51 27.37 27.52 27.57 S24 Test Set SSIM LPIPS 2000 0.881 0.869 0.860 0.886 0.820 0.812 0.775 0.841 0.897 0.913 0.878 0.883 0.884 0.916 0.922 0.923 5.918 6.277 7.024 6.938 7.374 6.765 11.012 6.135 5.521 5.928 4.413 4.282 4.253 4.059 3.938 3.913 0.159 0.100 0.124 0.110 0.147 0.116 0.180 0.115 0.074 0.096 0.071 0.065 0.064 0.060 0.055 0.054 # params 20,938,890 47,548,170 1,348,789 25,705,065 1,413,760 46,847 13,560 1,420,000 9,094,000 7,589,736 452,447 1,139,907 3,841,547 503,082 1,190,542 3,892, Table 2. Results across the S24 dataset picture styles (Styles #1 5) [16]. We report the average PSNR for each target style, along with the total number of parameters required to support all five styles. The best results in each column are highlighted in yellow. Method ISPDiffuser [70] PyNet [48] CIE XYZ Net [10] PP (cmKAN) [67] FlexISP [60] Invertible-ISP [85] LAN [69] MicroISP [49] ParamISP [53] LiteISP [91] FourierISP [44] Ours (lite, w/o enh.) Ours (base, w/o enh.) Ours (large, w/o enh.) Ours (lite, w/o enh., w/ 3D LuT) Ours (base, w/o enh., w/ 3D LuT) Ours (large, w/o enh., w/ 3D LuT) Ours (lite, w/ enh., w/ 3D LuT) Ours (base, w/ enh., w/ 3D LuT) Ours (large, w/ enh., w/ 3D LuT) #1 25.60 24.36 22.40 20.93 24.86 23.48 22.98 20.30 24.97 26.66 25.19 25.16 25.28 25.31 26.39 26.52 26.56 26.56 26.71 26.75 S24 Test Set #3 25.02 24.70 22.00 21.85 25.23 23.84 23.47 21.45 24.77 26.31 25.38 25.66 25.73 25.75 26.69 26.79 26.83 26.78 26.78 26.83 #4 25.93 24.34 22.26 20.91 23.96 23.33 22.80 20.34 24.18 25.04 24.74 25.47 25.55 25.58 26.35 26.47 26.51 26.66 26.79 26. #2 27.30 25.94 24.05 23.04 27.47 26.35 23.74 23.66 27.11 28.33 28.03 28.09 28.19 28.22 29.21 29.29 29.31 28.92 28.99 29.01 # params (for all styles) 104,694,450 237,740,850 6,743,945 384,535 128,525,325 7,068,800 234,235 67,800 7,100,000 45,470,000 37,948,680 1,281,343 1,968,803 4,670,443 1,347,950 2,035,410 4,737,050 1,550,490 2,237,950 4,939,590 #5 26.83 26.32 24.67 21.3 24.65 24.90 25.38 22.68 25.43 28.07 27.41 27.08 27.19 27.23 27.93 28.13 28.19 28.73 28.95 29.03 stylizationlearning the 3D LuT may not provide benefits and can even lead to slight degradations in some metrics. See Sec. K.1.4 of the supplementary material for further ablation analysis. 3.2. Qualitative Results Figure 4 shows qualitative comparison between our method and recent neural ISP approaches [53, 70, 91] on an example from the S24 test set. Rendered images are shown for the default picture style (Style #0) and the artistic styles, along with their corresponding ground truths. Our method consistently delivers higher visual quality across all styles. Additional examples are provided in Sec. K.2.5 of the supplementary material. 7 Cross-Camera Generalization. As described in Sec. 2.6, our photo-editing tool integrates generic denoisers and cross-camera AWB models to extend applicability to unseen cameras. Figure 5 presents qualitative comparison on an image captured with the iPhone 13 Pro Max main camera, comparing our result (using the generic denoiser and cross-camera AWB) with the native iPhone ISP and Project Indigo [55]. Our method delivers visual quality comparable to both, despite not using any iPhone data during training. This capability is further illustrated in Fig. 1, showing results on another camera unseen during training. Such generalization arises from our modular design, which allows switching between camera-specific models trained on the S24 dataset) and generic ones for (e.g., unseen devices, while maintaining visually pleasing output. Additional examples and discussion on cross-camera generalization are provided in the supplementary material (Secs. and I.3). User Study. To evaluate perceptual quality, we conducted user study comparing our method against the Samsung S24 native camera ISP and Adobe Lightroom. We captured 45 scenes with the S24 main camera in Pro mode (saving DNG files) and re-captured the same scenes using the native app. The captured scenes covered indoor, outdoor daylight, sunset, and low-light conditions. DNG files were processed using Adobe Lightroom (auto enhancement) and our method. For each scene, participants viewed three versions (ours, native ISP, and Lightroom) in random order and selected their preferred image under four criteria: color quality, brightness & contrast, sharpness & detail, and overall preference. Twenty participants took part in the study. Our method was consistently preferred, achieving 53.2% in color quality, 46.4% in brightness & contrast, 43.4% in sharpness & detail, and 51.4% in overall preference, out of three versions compared, outperforming the closest competitor by +13.927.0%. More details are provided in Sec. of the supplementary material. 4. Conclusion and Discussion In this paper, we have presented learning-based modular ISP framework that offers fine-grained control across the raw-to-sRGB image rendering process. We evaluated three variants of our frameworklite (0.5 parameters), base (1.2 M), and large (3.9 M)all of which have lightweight to moderate capacity and can process 12-megapixel image in about second or less on single GPU. Across all variants, our method achieves stateof-the-art results on nearly all picture styles in the S24 dataset [16], and competitive results on the MIT-Adobe FiveK dataset [26] (see Sec. K.2.2 in the supplementary material), while requiring significantly fewer parameters than the closest competing methods. Beyond high-quality Figure 6. Our pipeline may produce halo artifacts or color inconsistencies in flat regions near edges. These artifacts can be mitigated via multi-scale (MS) processing and post-refinement of LTM maps. Image captured using the S24 main camera. rendering, our framework consists of interpretable stages, making debugging and handling corner cases easier compared to previous black-box, non-interpretable end-to-end neural ISP designs. Furthermore, the modularity of our design makes scalability more practical and enables better generalization to unseen cameras. Our framework also offers strong flexibility throughout the rendering process. To highlight this flexibility, we implemented user-interactive photo-editing tool built on top of our modular ISP that enables fine-grained control over the rendering pipeline, including picture styles and editing options. Despite these advantages, our method faces few challenges. First, in certain corner cases, halo artifacts may appear near edges in backlit scenes. Because of the modular structure of our framework, we were able to analyze this issue and identify its origin in the LTM process. These artifacts can be mitigated by applying multi-scale (MS) processing and post-processing refinement of the predicted LTM maps. We discuss this in detail in the supplementary material (Sec. B.1) and show an example in Fig. 6, comparing the original artifact and the mitigated result using MS and refinement. Another challenge in training our framework is the need for reference denoised images to supervise the denoising modules, along with camera AWB and CCM data for color correction. While pseudo ground-truths for denoising can be obtained using AI-based third-party denoisers, the absence of DNG files makes it difficult to generate such pseudo ground-truth data or to obtain the necessary colorcorrection data for training. In Sec. B.2 of the supplementary material, we discuss this issue and present practical strategy for obtaining the missing data when DNG files are unavailable. Using the Zurich Raw-to-sRGB dataset [48] as case study, we demonstrate how our workflow can train the proposed method effectively, achieving results comparable to the top-performing methods despite the lack of DNG files and with roughly half or fewer parameters compared to existing alternativeswhile maintaining high degree of modularity not offered by other methods."
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 7. Intermediate outputs of our modular neural ISP, including denoising, color correction, digital gain, global tone mapping, local tone mapping, chroma mapping, gamma correction and detail enhancement. Different picture styles and edits are also shown (right). From top to bottom and left to right (for the styles and edits on the right), the first five images correspond to Styles #15 of the S24 dataset [16], while the remaining ones show results with additional edits and style adjustments. The first example was captured using an iPhone 15 main camera, the second using an iPhone 13 main camera, and the third using Samsung S24 main camera. Note that none of the training images were captured with, or include any synthetic data derived from, iPhone devices, underscoring the robustness and generalization capability of our method to unseen cameras. 1 In the main paper, we presented our modular neural image signal processing (ISP) framework, which enables accurate raw-to-sRGB rendering through modular design that provides full control over the rendering process and supports different picture styles. This modularity allows the framework to be flexibly tuned to produce desired capturetime outputs or to function as an interactive photo editor (see Fig. 7). We provide this supplementary material to further clarify how our method is developed, how it can be configured for capture-time deployment, and how it can alternatively be used as an interactive image-processing tool. Specifically, in Sec. A, we describe our GPU-accelerated iterative bilateral solver, which we use to mitigate halo artifacts that may occur in some corner cases. Handling challenging artifacts (with the help of the GPU-accelerated bilateral solver) and training with incomplete datasets are discussed in detail in Sec. B. We then elaborate on the design of the deep networks used in this work in Sec. and provide additional information about the differentiable histogram used in the chroma mapping network in Sec. D. Afterwards, we describe the guided upsampling regularization used in our implementation in Sec. E. Details of the photofinishing loss functions are provided in Sec. F, and the training details of our networks are presented in Sec. G. In Sec. H, we discuss our efforts to improve the generalization of our method across cameras. Sec. describes our graphical user interface tool, built on top of our method, which includes additional editing capabilities. In Sec. J, we present further details of the user study conducted as part of our evaluation. Lastly, in Sec. K, we provide extensive ablation studies of the pipeline components, along with additional results and comparisons. Further evaluation details for both the main paper and this supplementary material are provided in Sec. L. A. GPU-Accelerated Iterative Bilateral Solver To mitigate residual artifacts that occasionally appear in corner cases of our local tone-mapping (LTM) stage, we add an optional guided refinement step. Edge-aware smoothing is natural choice for this purpose. The Fast Bilateral Solver (FBS) [19] is particularly effective: it minimizes quadratic objective with bilateral affinities, thereby enforcing edge adherence while propagating low-frequency information. However, the original solver is not GPU-friendly. FBS constructs sparse bilateral grid and solves large symmetric positive definite (SPD) system using preconditioned conjugate gradient (PCG). This involves repeated gather-scatter operations between pixel and grid spaces, dominated by memory access rather than arithmetic, and converges slowly under the Jacobi preconditioner. These factors make direct GPU acceleration of FBS inefficient. A.1. Quadratic Objective in Image Space We propose lightweight image-space variant that preserves the FBS energy while avoiding the bilateral grid. Given an initial tensor and guidance image Z, we solve for refined output by minimizing the following energy: min λ (cid:88) Yp Mp2 2 + (cid:88) (cid:88) qNk(p) Wpq(Z) Yp Yq2 2, (11) where Nk(p) denotes local kk neighborhood around pixel p, and Wpq(Z) are bilateral affinities defined based on the guidance image Z: Wpq(Z) = exp (cid:16) q2 2 2σ2 (Zp Zq)2 2σ2 (cid:17) , (12) with (σs, σr) controlling the spatial and range scales. The guidance image, Z, is computed as the luminance of the input RGB image using weighted combination of the red, green, and blue channels, with respective weights of 0.2989, 0.5870, and 0.1140. A.2. Iterative Solver Instead of solving Eq. 11 with PCG, we pre-compute the bilateral weights Wpq(Z) once (per image) and apply fixed number of successive over-relaxation (SOR) updates [86]. Initializing Y(0) = M, each pixel is updated as: Y(t+1) = λ Mp + (cid:80) λ + (cid:80) qNk(p) Wpq(Z) Y(t) qNk(p) Wpq(Z) (cid:16) Y(t+1) Y(t)(cid:17) , , (13) (14) Y(t+1) = Y(t) + ω with relaxation ω [1, 2). In our implementation, Wpq(Z) are normalized such that (cid:80) qNk(p) Wpq(Z) = 1, making the denominator λ + 1. The same normalized weights are reused across channels and iterations. A.3. GPU Implementation and Efficiency All steps in Eq. 14 are implemented using dense tensor primitives (unfold, pointwise operations, and reductions) that are highly optimized on GPUs (see Algorithm 1). We use reflective padding to avoid border bias and pre-compute Wpq(Z) once per image, reusing it across channels and iterations. Unless otherwise stated, our implementation uses the following default values: k=7, σs=3.0 px, σr=0.01, λ=103, niter=80, and ω=1.6. With niter=80, our GPU-based iterative solver achieves an 11 wall-clock speedup over the original CPU-based FBS while producing visually comparable refinements. On CPU, however, it is slower than FBS due to the overhead Algorithm 1 GPU-Accelerated Iterative Bilateral Solver Require: Guidance Z, input tensor M, kernel size k, scales (σs, σr), smoothness λ, iterations niter, relaxation ω 1: Pad with reflect, extract kk neighborhoods using unfold 2: Compute range weights exp((Z)2/2σ2 ) 3: Compute spatial weights exp(p2/2σ2 ) 4: Form bilateral affinities Wpq and normalize so (cid:80) Wpq = 1 5: Initialize Y(0) 6: for = 0 to niter 1 do 7: Extract kk neighborhoods of Y(t) with unfold Compute smooth term (cid:80) Compute target (λ Mp + smooth)/(λ + 1) Update Y(t+1) Y(t) + ω(target Y(t)) Wpq Y(t) 8: 9: 10: 11: end for 12: return Y(niter) of repeated dense tensor operations (see Fig. 8 for runtime vs. niter on 7501000 guidance image). Since our framework assumes GPU availability in most deployment scenarios, the GPU-accelerated solver offers clear practical advantage. Our GPU-accelerated solver retains the FBS energy [19] but replaces the bilateral-grid PCG solver with local SOR scheme. While this sacrifices exact convergence guarantees, it enables efficient and straightforward GPU implementation with sufficient quality in practice. Beyond mitigating artifacts in our LTM module (see Fig. 9), the proposed GPU-accelerated refinement can also serve as general edge-aware post-processing method (see Fig. 10). Providing thorough evaluation of our modified solver across different datasets and tasks is beyond the scope of this paper, but we consider this an important direction for future work. B. Challenges and Potential Solutions In this section, we discuss key challenges of the proposed framework along with potential solutions. We begin with artifacts in the LTM stage, where in rare cases (particularly under strong backlighting), halo artifacts may appear. To address this, we introduce two optional lightweight steps that can be user-enabled, since applying them indiscriminately may reduce accuracy (as explained in the next subsection). We next consider the case of training on an incomplete dataset that lacks some of the essential information required by our method. For this purpose, we use the Zurich Raw-to-sRGB dataset [48], which, in addition to the aforementioned challenges, introduces further difficulties due to unaligned raw and ground-truth sRGB training pairs. 3 Figure 8. Runtime of the guided refinement process on 7501000 guide image for different iteration counts niter. We compare the Fast Bilateral Solver (FBS) [19] on CPU with our modified bilateral refinement on both CPU and GPU. Since our pipeline is primarily intended for GPU deployment, the modified bilateral refinement provides an efficient and practical replacement for FBS. Runtimes were measured on an Intel Core i7-14700K CPU and an NVIDIA GeForce RTX 4080 SUPER GPU (16 GB VRAM). Specifically, we describe how our framework can still be trained when key data are missing, such as ground-truth denoised raw images or DNG metadata (e.g., illuminant color and color correction matrices). B.1. Mitigating Artifacts We optionally apply two lightweight steps that suppress halos while preserving edges: 1) multi-scale aggregation of LTM coefficient predictions, followed by 2) the edge-aware bilateral refinement discussed in Sec. A. We adopt multi-scale strategy to improve robustness and suppress halo artifacts in challenging cases (e.g., strong backlighting). Specifically, the input image after digital gain gain and its global tone-mapped version GTM are processed at progressively downsampled resolutions using scale factors: = {1.0, 0.5, 0.25, 0.125, 0.0625}. At each scale S, if = 1.0, both GTM are bilinearly downsampled; otherwise the original resolution is used. From the input gain(s) at each scale s, we then generate guidance map using our multi-scale guidance subnetwork (see Sec. C.5), followed by smoothing step. In particular, the guidance map is first extended using reflection padding. We then apply average pooling over 55 spatial window, producing gently varying guidance signal that stabilizes the slicing coefficients while preserving the overall luminance structure needed for high-quality upsampling. gain and The pair (cid:0)I gain(s), GTM(s) (cid:1) is concatenated and fed into Table 3. Results of our method with and without multi-scale processing and refinement of local tone mapping in the photofinishing module. Input images are linear sRGB generated from pseudo ground-truth denoised images in the S24 test set [16], downsampled to one-quarter resolution. Outputs are compared against the ground truth at the same resolution. The best results are highlighted in yellow. Method Default (w/o multi-scale and refinement) w/ multi-scale and refinement S24 Test Set 1/4 (LsRGB/sRGB) SSIM PSNR 0.939 27.49 0.929 26.28 with and without the multi-scale processing and refinement designed to mitigate the rare halo artifacts observed in challenging scenes. As shown, applying these steps indiscriminately degrades accuracy, so they remain user-controlled and can be enabled or disabled as needed. B.2. Misaligned and Incomplete Datasets Training our method requires access to scene illuminant vectors and color correction matrices (CCMs) for the training images (information typically available in DNG files) along with denoised ground-truth images. The latter can be generated from DNG files using AI-based blind denoisers such as the one in Adobe Lightroom, as in the S24 dataset [16]. Missing DNG files (or the extracted data they contain) pose challenge for training our method. In addition, if the ground-truth sRGB images are misaligned with the corresponding raw inputs, this introduces another source of difficulty. The Zurich Raw-to-sRGB dataset [48] provides an ideal test case for studying these issues, since it lacks both DNG files and the required metadata, and its raw-sRGB pairs are not spatially aligned. We therefore expect degradation in accuracy, as our design does not explicitly handle misaligned training pairs. Nevertheless, this scenario is valuable for illustrating how our framework can be adapted to operate with missing data and for analyzing the resulting impact on accuracy. The Zurich dataset consists of raw images captured with smartphone camera and their corresponding JPEGs from DSLR camera, but the pairs are not strictly pixel-aligned due to lens distortion, hand-held capture, and other factors. Although such dataset is useful for evaluating robustness, it does not reflect the typical scenario, as obtaining aligned paired datasets is feasible in practice, as demonstrated by the S24 dataset [16] and the MIT-Adobe FiveK dataset [26]. The remainder of this section describes how we address these missing elements to enable training of our framework and analyzes the effect of misalignment in the Zurich dataset on the resulting performance. Figure 9. Comparison of 1) multi-scale processing for generating LTM coefficient maps, 2) Fast Bilateral Solver (FBS) [19] as post-processing step, 3) our modified bilateral refinement in place of FBS, and 4) our refinement applied after multi-scale processing. The input is linear sRGB image generated from pseudo ground-truth denoised image in the S24 validation set [16]. We report PSNR between the photofinishing output (downsampled to one-quarter resolution) and the corresponding ground truth, along with the runtime overhead for each approach, measured on an Intel Core i7-14700K CPU and an NVIDIA GeForce RTX 4080 SUPER GPU. the grid-prediction subnetwork (see Sec. C.5) to produce coefficient grid. Bilateral slicing using the guidance map Gguide(s), after the aforementioned smoothing step, yields scale-specific coefficient maps, which are upsampled to full resolution when = 1.0. The upsampled coefficient maps from all scales are then averaged to obtain the final output. This strategy integrates information from coarse-tofine representations, improving stability and reducing artifacts. After multi-scale processing, we apply our GPUaccelerated bilateral refinement (Sec. A) to further refine the predicted coefficient maps, using gain as the guide. These two steps together significantly mitigate halo artifacts in difficult scenes (see Fig. 11). Applying these steps blindly was found to reduce accuracy, as they also affect the strength of the LTM. To quantify this effect, we report results in Table 3 using the photofinishing module. We use pseudo ground-truth denoised images at one-quarter resolution as input (after white balancing and color correction) and evaluate against the corresponding sRGB ground-truth images at the same resolution from the S24 test set [16]. We compare the accuracy of our trained photofinishing module (Style #0, the default style) Figure 10. Comparison of our modified bilateral refinement with the original Fast Bilateral Solver (FBS) [19]. The proposed modified bilateral refinement achieves comparable or improved results while running efficiently on GPU. The reference image is from the S24 test set [16]; the high-resolution source was generated in Adobe Photoshop and downsampled to 10075 to obtain the low-resolution input. Runtimes were measured on an Intel Core i7-14700K CPU and an NVIDIA GeForce RTX 4080 SUPER GPU (16 GB VRAM). where fCM is computed via linear regression with polynomial kernel expansion, after saturated pixels are discarded. The pseudo ground-truth denoised raw is then obtained as: Ipseudo = fCM(Ilin). (16) Illuminant Estimation. Since no illuminant metadata is provided in the Zurich dataset, we estimate the camera illuminant as the mean RGB of the demosaiced raw image, equivalent to applying the gray-world assumption [25]. While this provides only rough estimate, the resulting error can be compensated for by the computed CCM described below. Color Correction. The Zurich dataset also lacks CCMs, so we solve for 33 matrix that maps white-balanced raw colors (with the estimated gray-world illuminant) to the corresponding sRGB values s: min rC s2 2, (17) subject to each row of summing to 1 (to approximately preserve intensity). We solve this constrained least-squares optimization with non-negativity bounds using sequential least squares programming (SLSQP). In practice, the computed CCM compensates for errors in the estimated illuminant to produce colors close to the target images. After these steps, we obtain the main components required to train our framework: input raw images, pseudo ground-truth denoised images, illuminant vectors, and CCMs to map the denoised raw images into linear sRGB space before training the photofinishing module. This allows us to train the framework as described in the main paper (more details of the training are available in Sec. G). Specifically, we train the denoising network and Figure 11. Halo artifact suppression using the proposed multiscale (MS) processing and guided refinement of the predicted LTM coefficients. Images were captured with the S24 main camera. Pseudo Ground-Truth Denoised Images. To generate pseudo ground-truth denoised images, we apply an inverse 2.2 gamma to the ground-truth sRGB image, producing linearized estimate Ilin. The use of 2.2 gamma is practical approximation; in reality, camera ISPs typically apply sequence of non-linear operators that cannot be fully inverted by this simple correction [9]. We then compute global non-linear color mapping (CM) function fCM : R3 R3 that maps Ilin to the input raw domain by solving the following optimization problem: arg min fCM (cid:13) (cid:13)fCM(Ilin) Iraw (cid:13) 2 2, (cid:13) (15) Figure 12. Comparison of predicted local tone mapping coefficients under different configurations: 1) without multi-scale processing, 2) with multi-scale processing, and 3) with multi-scale processing and guided refinement. For each configuration, we show intermediate outputs after LTM (first row) and the final sRGB outputs. The input images to the LTM module (linear sRGB after digital gain and after global tone mapping) and the ground truth are shown in the top row. The shown example is from the S24 test set [16]. the photofinishing module separately, then generate semifinal images before training the detail-enhancement network. Since the Zurich dataset provides 448448 semialigned patches, we train the detail-enhancement network on patches of the same size, rather than the 512512 patches used in the main paper experiments. Moreover, we disable the downsampling step before the photofinishing module and the guided upsampling after it when generating paired examples for the enhancement network and during evaluation, since testing is also performed on 448448 patches. We report the results of our method in Table 4. As shown, our method does not achieve state-of-the-art results on the Zurich dataset, unlike on the S24 dataset [16]. Nevertheless, our workflow enables training on incomplete datasets such as Zurich and still yields competitive results, outperforming methods with comparable number of parameters that lack the modularity of our framework (e.g., [10, 69]). Compared 6 Table 4. Results on the Zurich Raw-to-sRGB dataset [48]. We report PSNR, SSIM [81], LPIPS [89], and 2000 [72], along with the total number of parameters for each method. While the Zurich dataset poses challenges for our framework due to misalignment and missing metadata, our method still achieves competitive results, with about 1 dB drop in PSNR, while requiring significantly fewer parameters. The best results are highlighted in yellow. Method PyNet [48] CIE XYZ Net [10] LiteISP [91] FourierISP [44] LAN [69] Ours (lite, w/o enhancement) Ours (base, w/o enhancement) Ours (large, w/o enhancement) Ours (lite, w/ enhancement) Ours (base, w/ enhancement) Ours (large, w/ enhancement) PSNR 21.19 19.75 21.55 21.65 19.46 19.57 19.57 19.73 20.68 20.71 20.76 Zurich Raw-to-sRGB Test Set SSIM LPIPS 2000 0.747 0.697 0.749 0.755 0.730 0.717 0.718 0.721 0.726 0.727 0.729 0.193 0.408 0.187 0.182 NA 0.401 0.405 0.397 0.390 0.393 0.386 NA 9.283 NA NA NA 9.448 9.415 9.257 8.306 8.246 8. # params 47,548,170 1,348,789 9,094,000 7,589,736 46,847 452,447 1,139,907 3,841,547 503,082 1,190,542 3,892,182 to the best-performing method [44] in As shown in Table 4, our model requires substantially fewer parameters (500 with the lite denoising network and 3.9 with the large variant; see Sec. C.1 for configurational details of the denoising network variants), while achieving less than 1 dB drop in PSNR (compared to 7.6 parameters for the reference method [44]). Moreover, the modular design of our framework provides greater flexibility, making the rendering pipeline easier to control, scale, debug, and customize. C. Design of Networks This section details the architectures of the networks that form our proposed framework. C.1. Raw-Denoising Network For the raw-denoising network (Draw), we employ three variants of the NAFNet architecture [30], which differ only in their base channel width and thus span different points on the accuracy-efficiency trade-off. The lite, base, and large variants differ in their initial channel width (4, 8, and 16 channels, respectively), which is doubled after each encoder stage. This scaling leads to progressively larger bottleneck widths (64, 128, and 256 channels, respectively) and total capacities of approximately 0.25 M, 0.93 M, and 3.6 parameters. All three networks share the same overall design: four-stage encoderdecoder with skip connections and middle stage of four residual blocks. Each network operates in residual-learning manner, predicting correction that is added back to the input raw image. The encoder and decoder are composed of [2, 2, 4, 8] and [2, 2, 2, 2] NAF blocks per stage, respectively. Each block consists of lightweight NAF modules with depthwise convolutions, channel attention, and simple gating to realize an activationfree nonlinearity. Layer normalization and learnable scale parameters are applied to stabilize training. The networks 7 are fully convolutional and maintain compatibility with raw images of arbitrary resolution. C.2. Attention and Multi-Branch Blocks the networks employ In our photofinishing module, lightweight attention mechanism and multi-branch convolutional (MBConv) blocks. Specifically, we adopt the coordinate attention (CA) mechanism [45] with some modifications. Specifically, we replace Batch Normalization [50] with Group Normalization [82] to ensure consistent behavior under small batch sizes. We employ the LeakyReLU activation in all CA blocks. To prevent overly narrow bottlenecks in low-channel layers, we impose lower bound of eight channels on the reduced dimensionality, computed as max(8, Cin-CA/rCA), where Cin-CA is the input channel count and rCA is the reduction ratio. The reduction ratio is set to 4 by default, except for the luminance-guidance subnetwork within the chroma-mapping network, where we use rCA=2. The MBConv block is designed to capture features at multiple receptive-field scales while maintaining low computational cost. As shown in Fig. 13, the block consists of three depthwise convolution branches that operate in parallel. The first branch uses 33 kernel without dilation to capture fine local details. The second branch also uses 33 kernel but with dilation set to 2, allowing the convolution to gather broader contextual information without increasing parameters. The third branch employs larger 55 kernel to further expand the receptive field and capture smoother structural variations. Each branch begins with reflection padding, followed by depthwise convolution and LeakyReLU activation function. The outputs from all branches are summed and then fused by 11 convolution to produce the final aggregated feature map. This design effectively enhances receptive-field diversity without increasing the number of parameters or channels. We provide ablation studies on the impact of using the MBConv blocks and the CA mechanism on the final photofinishing results in Sec. K.1.4. C.3. Gain and Gamma-Correction Networks Both the digital-gain (Dgain) and gamma-correction (Dgamma) networks share the same lightweight convolutional architecture, designed to predict single global scalar factor that adjusts exposure or that is used for gamma correction. The two networks differ only in their output ranges. As illustrated in Fig. 14, the input image is first resized to fixed resolution of 128128 using bilinear interpolation. The network then begins with 33 convolution layer with reflection padding, followed by Group Normalization (two groups) and LeakyReLU activation. The features are processed by an MBConv block to capture multi-scale spatial context, followed by CA block to encode long-range Figure 13. Structure of the multi-branch convolutional block (MBConv). The input feature map is processed in three parallel depthwise convolution branches. Each branch applies reflection padding before the convolution and LeakyReLU activation afterward. The outputs from all branches are summed and then fused through convolution layer to produce the final aggregated feature map. This block preserves the number of channels (i.e., Cin = Cout). dependencies along both spatial directions. Another 33 convolution and LeakyReLU activation refine the features before global pooling. Two stages of average pooling are used to progressively compress spatial information: first to one-quarter of the input resolution (i.e., 1281283232), and then to single 11 feature vector. The resulting vector is passed through fully connected layer with Sigmoid activation to produce normalized scalar value in the range [0, 1]. Reflection padding is applied to all convolutions to prevent border artifacts, and the total number of trainable parameters for each network (digital-gain and gamma-correction) is 6,587. For the digital-gain stage, the predicted scalar is linearly mapped to the range [0.25, 4.0] (equivalent to approximately exposure value [EV] [2, +2]), producing gain factor dg that scales image intensities before tone and color processing. For gamma correction, the normalized output is mapped to the range [1.2, 3.0], yielding gamma factor γ. C.4. Global Tone Mapping Network The global tone-mapping (GTM) network (DGTM), consisting of 28,369 learnable parameters, predicts three positive parameters that define parametric tone curve applied to the linear sRGB image after digital gain (I gain). As shown in Fig. 15, the input image is first resized to 128128 using bilinear interpolation. The backbone begins with 33 convolution (10 channels) with reflection padding, followed by Group Normalization (two groups) and LeakyReLU activation. The resulting features are processed by an MBConv block and CA block to enhance spatial context modeling. Two subsequent 33 convolutions (20 channels each, reflection padding) with LeakyReLU activations further refine the features. Spatial information is then proFigure 14. Architecture of the network used for both the digital gain and gamma correction (6,587 parameters). The input image is first resized to fixed resolution 128128, then processed by convolutional layers with Group Normalization (2 groups) and LeakyReLU activation, followed by multi-branch convolutional and coordinate-attention blocks. After two stages of adaptive pooling, fully connected layer with Sigmoid activation predicts scalar in [0, 1], which is linearly mapped to the target range for either digital gain ([0.25, 4.0]) or gamma ([1.2, 3.0]). The number of channels at each stage is indicated in green. gressively aggregated through adaptive average pooling to 1616, followed by 33 convolution (40 channels, reflection padding) and LeakyReLU activation, another adaptive pooling to 44, and final 33 convolution (40 channels, reflection padding) with LeakyReLU activation. final global average pooling reduces the feature map to 11. The flattened feature vector is passed through fully connected layer that produces three outputs, followed by Softplus activation to ensure (aGTM, bGTM, cGTM) > 0. C.5. Local Tone-Mapping Network The LTM network, DLTM (120,215 learnable parameters), predicts spatially varying coefficients that locally modulate the tone-mapping behavior applied to the linear sRGB image after digital gain (I gain) and global tone mapping (I GTM). Unlike the global tone-mapping network (Sec. C.4), which predicts single set of global parameters (aGTM, bGTM, and cGTM), the LTM network produces per-pixel coefficient maps (ALTM, BLTM, CLTM, GLTM, and WLTM) that enable locally adaptive tone mapping and exposure adjustment. The LTM network consists of two main components: 1) multi-scale guidance subnetwork and 2) grid-prediction subnetwork, as shown in Fig. 16. The multi-scale guidance subnetwork derives guidance map from one of the color channels of gain. It processes three progressively downsampled versions of the luminance channel (1, 1/2, and 1/4 scales) using parallel convolutional branches, each composed of 33 convolution (reflection padding), Group Normalization (2 groups), LeakyReLU activation, an MBConv block, and CA block, followed by four additional 8 Figure 15. Architecture of the global tone-mapping (GTM) network (28,369 parameters). The input linear sRGB image after digital gain (I gain) is first resized to 128128 and processed by convolutional layers with Group Normalization (2 groups) and LeakyReLU activations, followed by multi-branch convolutional and coordinate-attention blocks. Two additional convolutional layers refine the features before series of adaptive pooling operations (1616 44 11). fully connected layer with Softplus activation outputs the three positive parameters (aGTM, bGTM, cGTM) that define the GTM curve. The number of channels at each stage is shown in green. convolutional layers. The outputs from the three scales are upsampled (bilinear), concatenated, and fused using convolutional layers followed by Tanh activation to produce the final guidance map Gguide [1, 1]. This multi-scale guidance design is intended to improve robustness against scale and contrast variations (see Sec. K.1.4 for ablation study). gain and In parallel, the grid-prediction subnetwork estimates coarse grid of tone-mapping coefficients conditioned on both GTM. The concatenated image pair is downsampled to 384384 and processed by series of convolutional layers (with Group Normalization and LeakyReLU activations), MBConv and CA blocks, and average pooling to form latent grid representation of size NgNg with depth Nc. final 11 convolution outputs Nc5 feature channels corresponding to five coefficient volumes, activated by Softplus to ensure non-negativity. The predicted NgNgNc5 coefficient grid, MLTM, is then sampled via trilinear interpolation using the guidance map Gguide as the depth coordinate, producing the spatial coefficient maps (ALTM, BLTM, CLTM, GLTM, and WLTM). In our implementation, we set Nc=18 and Ng=64, corresponding to 6464 spatial grid with 18 depth slices. See Sec. K.1.4 for ablation results on the impact of grid size. During training, the spatial coefficient maps are regularized using the LTM smoothness loss, ℓLTM-s (see Sec. F). At inference time, an optional multi-scale processing and bilateral refinement step can be applied to smooth the coefficient maps and mitigate artifacts (see Sec. B.1 for details). Figure 16. Architecture of the local tone-mapping (LTM) network (120,215 parameters). It consists of two main components: multi-scale guidance subnetwork that processes single channel of the linear sRGB image after digital gain to predict the guidance map Gguide, and grid-prediction subnetwork that processes the concatenated () linear sRGB image after digital gain and the globally tone-mapped image to estimate the coefficient grid MLTM. The predicted grid is trilinearly sampled using Gguide to produce five spatial coefficient maps that control local tone-mapping behavior. The number of channels at each stage is indicated in green. C.6. Chroma-Mapping Network The chroma-mapping network (Dchroma) predicts an imagespecific 2D lookup table (LuT) that transforms the chrominance components (CbCr) of the tone-mapped image produced by the preceding stages. The LuT is learned in an end-to-end fashion and applied via differentiable grid sampling to enable backpropagation through the photofinishing module. The network operates in the YCbCr color space, using the luminance channel of the tone-mapped image (YLTM) as an auxiliary guidance signal. Given the tone-mapped image (processed by both GTM and LTM operators) in YCbCr format, the image is first resized to fixed spatial resolution of 128128. differentiable 2D histogram representation of the CbCr channels, ˆHCbCr, is then computed (see Sec. for details). This histogram encodes the joint distribution of chroma values across NhNh bins (with value range of [0.5, 0.5]) and provides compact, differentiable summary of the chrominance content. The histogram ˆHCbCr is processed by shallow convolutional subnetwork (hereafter referred to as the hist subnetwork) to extract chroma features (four channels). These features are concatenated with an identity 2D meshgrid, denoted as Hpos RNhNh2, which encodes the normalized CbCr coordinates and provides the convolutional layers with explicit knowledge of the histogram bin locations [11]. The resulting six-channel tensor is then used as input to an 9 encoderdecoder network with luminance-guided attention, as illustrated in Fig. 17. The encoder consists of three convolutional stages. The first encoder stage applies 33 convolution with reflection padding, Group Normalization (four groups), and LeakyReLU activation. The second encoder stage includes 33 convolution with reflection padding followed by LeakyReLU activation and an MBConv block, while the third stage applies only 33 convolution with reflection padding followed by LeakyReLU. All stages preserve the channel dimensionality (28) and spatial resolution. The final encoder output is passed through CA block to encode long-range dependencies along both chroma dimensions. In parallel, the luminance channel (YLTM) is processed by lightweight auxiliary subnetwork that produces 28D attention vector. This luminance-guidance subnetwork begins with 33 convolution (8 channels) followed by Group Normalization (two groups) with LeakyReLU activation, CA block (reduction ratio 2), and an MBConv block. After adaptive average pooling to 88, the features are refined by another 33 convolution and LeakyReLU activation, followed by global pooling and fully connected layer with Sigmoid activation. The resulting normalized vector ([0, 1]28) modulates the encoder bottleneck features by channel-wise multiplication, allowing luminancedependent adaptation of the predicted chroma mapping. The decoder mirrors the encoder structure, consisting of three convolutional stages. The final layer uses 11 convolution with two output channels and Tanh activation to generate the residual chroma LuT (NhNh2). The predicted LuT (Lchroma-res) is added to learnable, imageindependent base LuT (Lchroma-base) to produce the final 2D LuT. The base LuT, Lchroma-base, is initialized as an identity mapping in the CbCr space before training. During training, the final constructed LuT is regularized using the chroma LuT smoothness loss, ℓLuT-s (Sec. F). The total number of learnable parameters in the chroma-mapping network, including the base LuT, is 45,466. C.7. Detail-Enhancement Network For the detail-enhancement network (Denh), we employ lightweight variant of the NAFNet architecture [30], which predicts residual correction added back to the input image. The detail-enhancement network follows the same encoder-decoder design with skip connections as our denoising models, but with significantly reduced depth and width to minimize complexity. Specifically, it is configured with an initial channel width of 8, two encoder stages, two decoder stages, and four blocks in the middle stage. This compact setup results in approximately 50 trainable parameters while providing sufficient capacity for fine-detail enhancement within our pipeline. Figure 17. Architecture of the chroma-mapping network (45,466 parameters). The CbCr channels are first converted into differentiable 2D histogram ( ˆHCbCr; see Sec. D), which is processed by shallow subnetwork (hist). The resulting chroma features are concatenated () with an identity meshgrid (Hpos) to provide subsequent layers with explicit awareness of the histogram bin positions. The encoder-decoder backbone (with MBConv and CA blocks) predicts residual NhNh2 LuT that is then added to learnable base LuT. luminance-guided subnetwork processes the YLTM channel to produce an attention vector that modulates the encoder bottleneck features. The number of channels at each stage is indicated in green. D. Differentiable Histogram Computation As described in the main paper and in this supplemental material (Sec. C.6), our chroma mapping network (Dchroma) relies on differentiable histogram representation of the chrominance channels to enable end-to-end training of the photofinishing module. Given an input image with Cb and Cr channels, we construct 2D histogram by softly assigning each pixel to its corresponding histogram bins [7, 12]. Let Nh denote the number of bins in the 2D histogram (and, accordingly, in the chroma 2D LuT constructed by the chroma-mapping network), and let [vmin, vmax] denote the value range. We place Nh uniformly spaced bin centers for both Cb and Cr channels as follows: ci = vmin + Nh1 (vmax vmin), = 0, . . . , Nh 1. (18) 10 Each pixel (cb, cr) contributes to all bins with Gaussian weighting: Wij(cb, cr) = exp (cid:18) (cb ci)2 + (cr cj)2 (cid:19) 2σ hist , (19) where cb and cr denote the chrominance values of pixel, and ci, cj are the uniformly spaced bin centers along the Cb and Cr axes, respectively. The term σhist controls the softness of the bin assignment. This formulation follows the differentiable histogram approach in [7, 12]. The histogram is computed by summing over all pixels:"
        },
        {
            "title": "HCbCr\nij",
            "content": "= (cid:88) Wij(cbp, crp), (20) then normalized to unit sum and square-rooted: (cid:115) ˆHCbCr = HCbCr i,j HCbCr ij (cid:80) . + ϵ (21) relationship between the low-resolution input gamma and the guide ILsRGB within grid cell m, such that Am models local affine mapping that transfers structure and edges from the guide to the upsampled output. The regularization matrix Rm controls the per-channel stability of this fitting. Thus, the design of Rm directly affects how well color and fine details are preserved in the final gamma result. Original Halide Regularization. The Halide implementation of BGU [29] tries to solve: Tblur Sblur + λmRluma = Am where (cid:0)Sblur Sblur + λmIC+1 (cid:1), (23)"
        },
        {
            "title": "Rluma",
            "content": "m = (cid:2)diag(rluma , rluma , rluma ) 0(cid:3) , applied uniformly to (24) that uses single luma gain rluma all RGB channels: In our implementation, we set the default values to Nh=24 bins (see Sec. K.1.4 for ablation results with Nh=12), vmin= 0.5, vmax=0.5, and σhist=0.075. rluma = E. Guided Upsampling Regularization 4 gamma In the main paper, we explained that our method employs bilateral guided upsampling (BGU) [29] with modified regularization that yields better results. Here, we provide additional details of our regularization. Our photofinishing module predicts low-resolution gammacorrected output, 4 3. To recover full resolution, we apply BGU using the full-resolution linear sRGB guide image ILsRGB RHW 3. The highresolution photofinishing result (after upsampling) is degamma RHW 3. The fitting grid is connoted as structed in the bilateral space using the low-resolution guide LsRGB 4 3, while the local affine fitting statistics are accumulated from the corresponding low-resolution input-output pair (I gamma), with guidance derived from the high-resolution image ILsRGB during upsampling. 64 The grid size is set to 64 16 to achieve good balance between speed and accuracy. LsRGB, 4 Let Sm R(C+1)(C+1) and Tm RC(C+1) denote the accumulated statistics in grid cell m, with cm samples. For RGB image, = 3. The affine transform for cell m, Am RC(C+1), is estimated by solving: TmS + λmRm = Am (cid:0)SmS λm = λ(cm + 1), + λmIC+1 (cid:1), (22) rglob = where IC+1 is the (C+1) (C+1) identity matrix and λ is scalar scaling factor. Intuitively, Sm and Tm encode the [0.25, 0.5, 0.25] Tblur [0.25, 0.5, 0.25] Sblur (cid:16) (cid:88) (cid:17) cblur + 1 , λglob, blur = λ [1 : C, +1] + λglob, blur [1 : C, +1] + λglob, blur , (25) and Sblur where Tblur Sm respectively, and cblur blurred grid m. Intuitively, are the blurred version of Tm and is the number of samples in the this achromatic assumption causes color crosstalk. Although empty cells are handled via spatial grid blurring, this process can oversmooth edges and fine details. Our Gated Regularization. Instead of applying grid blurring that may create artifacts, we adopt the original BGU [29] objective (Eq. 22) and use per-channel adaptive gains: Rgate = (cid:2)diag(rm) 0(cid:3) , where rm = [rm,1, rm,2, rm,3] is defined as: rm,c = Tm[c, + 1] Sm[c, + 1] + λm rglob , , if cm > 0, if cm = 0, with global per-channel fallback gains: (cid:80) Tn[c, + 1] (cid:80) Sn[c, + 1] + λglob (cid:17) (cid:16) (cid:88) , cn + 1 . λglob = λ 11 (26) (27) (28) (29) This gated regularization applies accurate local perchannel constraints when sufficient samples exist in grid cell, while sparsely populated cells are robustly handled using global statistics. Unlike the original Halide scheme, our regularization eliminates the need for grid blurring, avoids color crosstalk, and preserves sharp details in the upsampled result (see Sec. K.1.2 for ablation studies). F. Loss Function Details We provide the implementation details of the loss terms used to train our photofinishing module. Each loss is designed to complement specific stage of the photofinishing module, ensuring perceptually faithful reproduction of tone, color, and structure. The overall objective is given in Eq. 9 of the main paper. SSIM Loss. We adopt differentiable variant of the structural similarity index [81], computed independently per color channel. Given our output and the ground-truth sRGB images GT, we compute local statistics over an 1111 Gaussian window (standard deviation σ=1) using depthwise convolution: gamma and µ1 = gamma w, µ2 = GT w, 2 1 = (I σ2 σ12 = (I gamma gammaI GT w) µ1µ2. w) µ2 1, 2 = (I σ2 GT 2 w) µ2 2, (30) In Eq. 30, denotes the Gaussian weighting kernel. The SSIM map is then computed as: SM(I gamma, GT) = (2µ1µ2 + C1)(2σ12 + C2) 1 + µ 2 + C1)(σ2 1 + σ2 2 + C2) (µ2 (31) , where C1=0.012 and C2=0.032. The final loss is defined as ℓSSIM = 1 mean(SM). Perceptual Loss. We compute the perceptual loss using pretrained VGG-19 network [73] with frozen weights. We extract features from layers relu3 3 and relu4 2, following the convention of perceptual supervision in photo enhancement tasks. Both predicted and ground-truth RGB images are resized to 224224 and normalized by the ImageNet mean and standard deviation. The loss is given by the following equation: ℓperc = (cid:88) ϕl(I gamma) ϕl(I GT)1, (32) l{relu3 3, relu4 2} where ϕl() denotes the activation map of layer l. 12 Color Loss. To enforce perceptual color accuracy, we compute differentiable approximation of the CIE 1976 ab (i.e., E76) metric [71] in the CIE Lab color space. Both the predicted and ground-truth images are evaluated before gamma correction. For the groundtruth, we first remove the sRGB gamma encoding using the gamma scale, γ, predicted by our gamma-correction network (Dgamma), so that both images are evaluated under consistent, image-specific gamma setting. The resulting pre-gamma, quasi-linear sRGB images are then converted to the CIE Lab color space through differentiable transformation chain: linear sRGB XYZ Lab. The XYZ conversion uses the standard transformation matrix: MLsRGBXYZ = 0.4124564 0.2126729 0.0193339 0.3575761 0.7151522 0. 0.1804375 0.0721750 0.9503041 . (33) For the XYZ Lab mapping, the standard CIE Lab definition uses piecewise cubic-root function as follows: fcubic(t) = (cid:40)t1/3, > δ3, 3δ2 + 4 29 , otherwise. (34) where δ=6/29. Since this function is non-differentiable at t=δ3, we replace it with smooth blend defined as: fsoft(t) = σs (cid:0)αs(t δ3)(cid:1) t1/3 + (cid:2)1 σs (cid:0)αs(t δ3)(cid:1)(cid:3)(cid:16) 3δ2 + 4 29 (35) (cid:17) , where σs() denotes the sigmoid function used in our differentiable Lab conversion, and αs=150 controls the sharpness of the transition. The resulting Lab tensors, denoted as Lout and LGT, represent the full three-channel (L, a, b) values for our output and the ground-truth images, respectively. The per-pixel E76 distance is then computed as: ℓE = 1 (cid:88) (cid:13) (cid:13)Lout LGT (cid:13) (cid:13)2, (36) where = is the total number of pixels. If the 3D LuT (pre-chroma mapping network) is included as learnable component of the photofinishing module, we additionally incorporate an auxiliary color-difference term equal to half the E76 between the de-gammaed groundtruth image and the 3D LuT output. This auxiliary term encourages the 3D LuT to approximate the target color rendering prior to subsequent chroma mapping. Chroma Loss. This loss enforces chroma consistency by comparing the CbCr channels of the predicted output (after chroma mapping) with those of the de-gammaed ground truth, where the de-gammaing is performed using the image-specific gamma factor γ predicted by our gamma-correction network. The chroma loss, ℓCbCr, is defined as: ℓCbCr = (cid:13) (cid:13) ˆCLuT CGT (cid:13) (cid:13)1, (37) where CGT denotes the CbCr channels of the de-gammaed ), and ˆCLuT represents the mapped ground-truth image (I GT CbCr channels obtained using the predicted 2D LuT Lchroma. γ Tone-Mapping Loss. The tone-mapping loss enforces joint consistency between the global and local tonemapping modules and the ground-truth tone-mapped image. Let YGT denote the ground-truth luminance (Y) channel, de-gammaed using our image-specific gamma factor (γ). Similarly, let YGTM and YLTM denote the luminance channels obtained after the global and local tone-mapping stages, respectively. The tone-mapping loss is defined as: ℓTM = 0.6 (cid:13) (cid:13)Y8 GTM Y8 GT (cid:13)1 + (cid:13) (cid:13) (cid:13)YLTM YGT (cid:13) (cid:13)1, (38) where 8 indicates 1/8 downsampling via bilinear interpolation. The downsampled term penalizes deviations in the global tone and overall luminance of the tone-mapped output produced by the GTM network, while the second term encourages the LTM network to refine local tonal and luminance variations to better align with the ground truth. Overall Loss Combination. The total training objective is defined as weighted sum of all loss components: Ltotal = (cid:88) λj ℓj, (41) where each weighting coefficient λj balances the relative contribution of its corresponding loss term. Empirically, we found that combining low-level (ℓ1, ℓSSIM, ℓCbCr), perceptual (ℓE, ℓperc), and regularization (ℓLuT-s, ℓLTM-s, ℓluma, ℓTM) losses yields stable convergence and perceptually consistent results across different picture styles (see Sec. K.1.3 for ablation studies). G. Training Details As mentioned in the main paper, we train each module separately. In this section, we provide further details of training our framework. G.1. Denoising Training We trained the denoising model (Draw) to minimize the ℓ1 loss between the pseudo denoising ground truth on 256256 patches and the model outputs. The network was optimized using AdamW [64] with β1 = 0.9, β2 = 0.9, and no weight decay. The learning rate was initialized at 103 and decayed to 105 using cosine annealing [63]. Training was performed for 100 epochs with batch size of 32, and gradient clipping with maximum norm of 0.01 was applied to stabilize optimization. For each dataset, including the generic denoisers (Sec. H), we trained three model variantslite, base, and large. The architectural configurations of these variants are provided in Sec. C.1. Luminance Consistency Loss. We regularize the tonemapping process to preserve the average image brightness: G.2. Photofinishing Training ℓluma = (cid:12) (cid:12) E[YGTM] E[Ygain] (cid:12) (cid:12), (39) where Ygain denotes the channel of the linear sRGB image after applying the predicted image-specific digital gain dg in its YCbCr representation. This loss prevents exposure drift and encourages the tone-mapping modules to refine contrast while preserving global brightness. Total Variation Regularization Loss. The ℓLuT-s and ℓLTM-s terms promote spatial coherence in the chroma LuT and LTM maps. We implement these terms as isotropic total variation (TV) losses to encourage smoothness in the learned chroma LuT and local tone-mapping coefficient maps: ℓTV(X) = 1 2 (cid:0)hX1 + wX (cid:1), (40) where and denote horizontal and vertical spatial gradients, respectively. 13 We trained the photofinishing module (Dgain, DGTM, DLTM, Dchroma, Dgamma, Lchroma-base, and optionally the 3D LuT, LRGB) for 600 epochs with batch size of 8. The input images were the pseudo ground-truth denoised raw images mapped to linear sRGB space (ILsRGB), and the corresponding ground-truth images were the final sRGB targets (IGT). Both input and ground-truth images were resized to 512512, with standard geometric augmentations applied during training. Optimization was performed using Adam [54] with parameters β1 = 0.9, β2 = 0.999, and an ℓ2 regularization term of 107. The learning rate was initialized at 104 and decayed to 106 using cosine annealing schedule [63]. The loss function combined multiple terms as described in the main paper and in Sec. F. The weights of these losses were empirically set as follows: λ1 = 2.5 for ℓ1, λSSIM = 0.5 for ℓSSIM, λE = 0.02 for ℓE, λperc = 0.01 for the perceptual loss ℓperc, λCbCr = 1.0 for the CbCr loss ℓCbCr, λLuT-s = 0.06 for the LuT smoothness loss ℓLuT-s, λTM = 0.5 for the tone mapping loss ℓTM, λLTM-s = 0.6 for the LTM smoothness loss ℓLTM-s, and λluma = 0.2 for the luminance consistency loss ℓluma. Note that in the loss function details section (Sec. F), the resized photofinishing output and ground truth are denoted as gamma (after applying the predicted gamma correction) and GT, respectively, at resolution of 512512. gamma and GT refer to the photofinishing output and ground-truth images at the native 1/4 raw resolution. In all other contexts, G.3. Detail-Enhancement Training After training the denoising models and the photofinishing module, we trained our final detail-enhancement network (Denh), which refines the perceptual quality of the final output. To construct the training data, we first applied our trained denoisers (lite, base, and large variants, used jointly to enlarge the training set as an augmentation strategy). The denoised raw images were converted to linear sRGB using the metadata of each image (illuminant color vector and CCM), downsampled to one quarter of the resolution, processed by the trained photofinishing module, and then upsampled using the bilateral guided upsampling method [29] with our regularization (Sec. E). We extracted nonoverlapping 512512 patches from the guided-upsampled results and their corresponding ground-truth images. We used normalized image values in [0, 1] (without quantization) to construct the paired training set. For the Zurich dataset (Sec. B.2), we did not apply the downsampling and guided-upsampling procedure since the dataset provides only cropped 448448 patches, and thus we trained directly on these patches. Training was performed for 50 epochs with batch size of 16. The network was optimized using Adam [54] with parameters β1 = 0.9, β2 = 0.999, and ℓ2 regularization 107. The learning rate was initialized at 104 and decayed to 106 using cosine annealing schedule [63]. The pixel-wise ℓ1 loss between predicted and ground-truth sRGB patches was used as the loss function. H. Generalization Across Cameras One of the main advantages of our modular design is the ability to decouple the camera-specific modules of our pipeline from the generic ones that can operate across cameras. Specifically, the raw image denoising module is one of the primary camera-specific components, as noise characteristics differ from camera to camera. Other cameraspecific factors include color correction (i.e., auto white balancing and color correction matrices), which can be addressed using recent cross-camera illuminant estimation methods (e.g., [11]) or by reading metadata from DNG filessee Sec. I.3 for details on how we handle crosscamera auto white-balance correction. To overcome the limitations of camera-dependent denoising, we trained generic denoiser in addition to our camera-specific models trained for the S24 main camera, as reported in the main paper. To train the generic denoiser, we first modeled the noise characteristics of several cameras and then synthesized noise using these models on the pseudo ground-truth denoised raw images from the S24 dataset [16] to generate noisy raw inputs with synthetic noise from different cameras. These synthetic noisy raw images, along with the original noisy raw images from S24, were used to train our generic denoiser models under the same configurations described in Sec. G.1. Specifically, we constructed hybrid S24 dataset comprising 20% clean pseudo ground-truth denoised images, 40% real noisy S24 images, and 40% synthetic noisy images generated using the aforementioned cross-camera noise models. For each synthetic noisy sample, the ISO value was inherited from its corresponding real image to maintain realistic noise-exposure relationships. We adopted the widely used heteroscedastic Gaussian noise model [37, 38, 61]: ngd (cid:0)0, σ2 gd (cid:1) , σ2 gd = βgd,1 xgd + βgd,2, (42) where the noise variance σ2 gd increases linearly with the underlying clean pixel intensity xgd. The parameters βgd,1 and βgd,2 represent the shotand read-noise components, respectively. To synthesize noise under this model, we collected (xgd, σ2 gd) pairs and calibrated βgd,1 and βgd,2 for each ISO and color channel via linear regression. The shotnoise parameter βgd,1 corresponds to the slope, while the read-noise parameter βgd,2 corresponds to the intercept of the linear fit. During calibration, we performed 14-18 captures of an X-Rite ColorChecker chart (24 color patches). Each capture was taken at different ISO setting, ranging from ISO 50 to ISO 3200. For each ISO, we extracted the 24 color patches and subdivided them into smaller sub-patches to increase the number of samples for fitting. This process yielded multiple small patches of size (3535) per ISO and color channel, with the total number varying depending on the charts coverage within the image. Next, we computed the mean and variance of each small patch. Using these means as xgd and their corresponding variances as σ2 gd, we fitted linear regression model to estimate the slope (βgd,1) and intercept (βgd,2). This produced one pair of βgd,1 and βgd,2 values for each ISO in the calibration data. For ISOs not present, we interpolated the existing parameters (using linear interpolation for βgd,1 and cubic interpolation for βgd,2) to enable noise synthesis for all intermediate ISOs within the calibration range. To synthesize diverse noise patterns during training, we used noise models calibrated for total of 12 sensors: S20 FE (main), S20 Ultra (main), Pixel 6 (main), Pixel 9 Pro (main), Pixel 9 Pro (telephoto), S22 Plus (front), S22 Plus (telephoto), S22 Plus (ultra-wide), S22 Plus (main), 14 Table 5. Comparison of camera-specific and generic denoising models (lite, base, and large) on the S24 [16] and MIT Adobe 5K [26] test sets (noisy/denoised raw). The best results are highlighted in yellow. Denoising Model Camera-specific (lite) Camera-specific (base) Camera-specific (large) Generic (lite) Generic (base) Generic (large) S24 Test Set (Noisy/Denoised) SSIM PSNR 0.998 53.39 0.999 55.97 0.999 57.33 0.997 51.19 0.999 55.15 0.999 56.46 Adobe 5K Test Set (Noisy/Denoised) SSIM PSNR 0.998 55.57 0.999 57.36 0.999 59.85 0.996 51.84 0.997 52.86 0.997 53. Table 6. Comparison of denoising models (lite, base, large) trained on S24 [16], MIT Adobe 5K [26], and on wide range of noise profiles for generic denoising, evaluated on the SSID dataset [1]. The best results are highlighted in yellow. Denoising Model S24 (lite) S24 (base) S24 (large) Adobe (lite) Adobe (base) Adobe (large) Generic (lite) Generic (base) Generic (large) SSID Test Set (Raw Noisy/Raw Clean) PSNR 46.53 47.28 47.47 48.05 48.28 48.12 49.09 50.00 49.55 SSIM 0.940 0.952 0.956 0.966 0.967 0.971 0.971 0.978 0.979 (ultra-telephoto), S22 (ultra-wide), and S22 Ultra (main). These sensors were selected to represent diverse range of sensor types (main, telephoto, ultra-wide, and front), enabling the generic denoiser to learn from heterogeneous noise distributions. Similar to the camera-specific denoiser, we trained three variants of the generic denoiser (lite, base, and large). Table 5 compares the accuracy of camera-specific models each trained and evaluated on its corresponding dataset (S24 or MIT Adobe 5K [26])against the generic denoisers evaluated on the same datasets. The denoised groundtruth images for Adobe 5K were obtained by running Adobe Lightrooms AI denoiser on the raw images, following the same procedure used for the S24 dataset. As expected, camera-specific models performed better on their corresponding cameras, while the generic models were designed to generalize to unseen devices. To further validate this, we evaluated both model types on the Smartphone Image Denoising Dataset (SIDD) [1], which contains raw images from smartphones not included in the S24, Adobe 5K, or generic model training sets. Results in Table 6 show that the generic models outperformed the camera-specific ones, demonstrating stronger cross-camera generalization. Figure 18 shows qualitative results of our entire pipeline on unseen cameras (Samsung Galaxy S9 and iPhone X) 15 from the Raw-to-Raw dataset [6], using the photofinishing module trained on the S24 dataset. We report results with both the S24-trained denoiser and generic denoiser, combined with either the camera AWB or the cross-camera AWB model (C5) [11], as well as with the post-AWB userpreference mapping [92] (see Sec. I.3 for more details). For comparison, we include outputs from Adobe Lightroom, both with and without its built-in auto enhancement and AIbased denoiser features. Our method achieves high-quality cross-camera rendering, competitive with commercial software. For the iPhone X, we observed that raw images are substantially darker than those from most other cameras; thus, we applied the auto-exposure adjustment before the digital gain step (see Sec. I.2 for details). This adjustment was facilitated by the modularity of our pipeline, as further discussed in Sec. I. Figure 19 shows qualitative comparison between our method (using the generic denoiser along with the photofinishing and detail-enhancement modules trained on the S24 dataset [16]) and the LiteISP [91] and ISPDiffuser [70] models, both trained on the same dataset. The comparison is conducted on images captured by various iPhone devices, none of which were included in the training of any model, including ours. The results clearly demonstrate the superior cross-camera generalization ability of our approach, producing consistent and visually pleasing results with greater controllability (e.g., the ability to adjust white balance), while requiring significantly fewer parameters. Specifically, LiteISP contains approximately 9 parameters and ISPDiffuser around 21 M, whereas our lite versionwhich was used in this qualitative comparisonrequires only about 0.5 parameters. Additional results on cameras not seen during training are provided in Figs. 1 and 5 of the main paper, as well as in Fig. 7 of this supplemental material. Further qualitative examples are shown in Figs. 20, 22, 31, 32, and 49 in the following sections. I. User Interface for Modular ISP Control As described in the main paper, we implemented graphical user interface (GUI) to enable intuitive interaction with the modular components of our pipeline. Our photo-editing tool allows users to control all stages of the pipeline by enabling or disabling modules, adjusting their strengths, selecting from different picture styles, and even blending between styles or intermediate stages. In addition, it provides optional operators (e.g., contrast, highlights, shadows, and others) that can be applied within the pipeline, offering full flexibility to adjust the look and feel of the rendered images. Users can also export their manual edits as new styles and apply them to multiple images in batches (see Fig. 20). Figure 21 illustrates where these additional operators are integrated within the main pipeline described in the paper. Figure 18. Qualitative results on unseen cameras from the Raw-to-Raw dataset [6]. The first row shows an iPhone example and the second Samsung Galaxy S9 example. For each case, we include the input raw image, the reference sRGB rendered by the capturing app, and Adobe Lightroom outputs (default, built-in auto enhancement with/without AI denoising, and results with camera AWB and Lightroom auto AWB). Our outputs are shown with two denoisers (trained on the S24 dataset [16] and generic one), using camera AWB, C5 (neutral) AWB [11], and user-preference mapping [92]. All results are generated with the photofinishing module trained on the S24 dataset. 16 Figure 19. Qualitative comparison on unseen cameras (iPhone devices) between our method and the LiteISP [91] and ISPDiffuser [70] models, all trained on the S24 dataset [16]. Our method uses the generic denoiser, and none of the training data included iPhone captures. In this example, we show our results under the cameras automatic white balance (AWB) and after applying learned AWB models [11, 92] (see Sec. I.3 for details). Figure 20. Our photo-editing tool offers full control over the modular neural ISP pipeline and includes several interactive editing operators. Our tool supports DNG raw inputs from any camera, as well as rendered sRGB images previously processed by our tool, where compressed raw data is embedded within the JPEG for accurate re-rendering. For sRGB inputs from other cameras, the tool synthesizes raw-like data on the fly for rendering. Users can save custom edits and batch-apply them to multiple images. The shown reference image was captured with Canon EOS 90D, while other raw and rendered images (with embedded raw) were taken using iPhone 13 and Canon EOS 90D; all sRGB images were captured by the iPhone 13 main camera. Figure 21. Overview of the operators available in our graphical user interface. In addition to the main pipeline processes, the tool provides optional user-controlled operators (e.g., exposure, contrast, highlights, shadows, saturation, and sharpening), allowing fine-grained control over the final rendered image. In our implementation, the tool uses the photofinishing models trained on the S24 dataset [16] for the default picture style, as we found this model produces more visually plausible results (see Fig. 22). For the LTM, we adopt the proposed multi-scale processing and refinement (Sec. B.1) as the default configuration, while still allowing the user to disable this option if desired. In addition to the default picture style, we employ models trained on the S24 dataset that support five additional artistic picture styles (Style #15). In our GUI, we refer to them as Warm, Moody, Cinematic, Greenish, and Retro, respectively. For denoising, we rely on the S24-specific denoising base model for images captured by the S24 main camera, and generic base denoising model to support images captured by other cameras. For white balancing (Sec. I.3), the tool includes two models: camera-specific model for the S24 main camera and cross-camera model to support unseen cameras. Our tool accepts DNG raw images, where we first apply black-level normalization and then perform Menon demosaicing [65] using parallel, tile-based strategy with partial overlaps between tiles. In practice, we divide the Bayer image into overlapping tiles of 512512 pixels with 16pixel overlap to avoid boundary artifacts. Each tile is demosaiced independently using multiple threads and seamlessly blended into the final full-resolution RGB raw image. This tiled parallel processing improves runtime efficiency (up to 5 speed-up on 12-megapixel images compared to the standard full-frame implementation). The tool further supports post-editable image rerendering, where compressed raw data is embedded into the final rendered JPEG image (Sec. I.10). This design allows recovering the raw data for unlimited post-editable re-rendering under different settings, always starting from the raw image and thus avoiding accumulated degrada18 I.1. Denoising Adjustment The tool provides flexible control over the raw denoising strength. This control is implemented as linear interpolation between the image before and after the denoising operation, modulated by user-specified scalar parameter in the range [0, 1]. value of 0 preserves the unprocessed raw image, value of 1 applies the full denoising operation, and intermediate values yield proportionally mixed results. In addition to this raw-domain adjustment, we introduce two denoising operators that act in the color-corrected domainspecifically on the linear sRGB imageafter the color correction stage. These two operators are applied in the YCbCr representation of the linear sRGB image (where, for simplicity and similar to the photofinishing module, we use the standard RGB-to-YCbCr conversion matrix, which is originally defined assuming the input is in the non-linear sRGB space). Luma Denoising. To reduce luminance noise while preserving structural edges, we apply an edge-preserving guided filter [43] to the luma channel, YLsRGB, controlled by radius ry-d and regularization term ϵy-d. The output LsRGB is combined with the original YLsRGB through: Yy-d = (1 λy-d) YLsRGB + λy-d LsRGB, (43) where λy-d [0, 1] determines the luma denoising strength. Both ry-d and ϵy-d scale proportionally with λy-d, allowing smooth transition from minimal smoothing at low strengths to stronger spatial denoising as λy-d increases. This preserves local contrast and fine structure while progressively suppressing luminance grain. In our implementation, we used ry-d = 2 + 10 λy-d and ϵy-d = (0.001 + 0.03 λy-d)2, which provide balanced trade-off between structure preservation and noise reduction. Chroma Denoising. For the chroma components, we apply spatial Gaussian smoothing to suppress color blotches while retaining perceptual color consistency. The filtered channels Cb LsRGB are defined as LsRGB and Cr Cb Cr LsRGB = G(CbLsRGB; σc-d), LsRGB = G(CrLsRGB; σc-d), (44) where G(; σc-d) denotes separable Gaussian convolution with standard deviation σc-d. The blur scale σc-d is adaptively determined based on the user-defined chroma denoising strength λc-d and the image resolution: σc-d = (cid:0)3 + 12 λc-d (cid:1) (cid:18) (cid:19)0.9 , (45) where refers to the image height. This adaptive scaling maintains consistent perceptual smoothing across varying image resolutions. For high chroma denoising strength 19 Figure 22. Output of our method using photofinishing and detailenhancement models trained on the S24 dataset [16] (default picture style; Style #0) and the MIT Adobe 5K dataset [26] (Expert style; see Sec. K.2.2 for details). The image was captured with the iPhone 14 main camera. In both cases, our generic base denoising model was used. tions. Before saving, the raw data is pre-processed by learned compression model to improve efficiency and keep the JPEG file size compact. To broaden applicability, the tool can also process sRGB images not originally rendered with our system (and therefore lacking embedded raw data). For this case, we include learning-based linearization module that maps camerarendered sRGB images into camera-agnostic space and synthesizes raw-like representation, allowing the tool to handle sRGB input images produced outside our pipeline (Sec. I.11). This capability to process sRGB images from third parties, together with the batch-processing option, enables the tool to handle extracted sRGB video frames in frame-by-frame manner (see Fig. 23). To improve temporal consistency across video frames, we apply simple temporal smoothing over the current rendering parameters using information from the previous frames within 9-frame window. More advanced temporal smoothing approaches are beyond the scope of this paper and are left for future work. In total, our tool requires loading 3,902,604 parameters (approximately 14.9 MB / 0.015 GB) into GPU memory. This includes the denoising models (camera-specific and generic), photofinishing models for six picture styles (including the default), white balance models (cross-camera and camera-specific), single detail-enhancement model (we use the one trained for Style #0 for simplicity, instead of loading style-specific models), the raw pre-processing model (for raw embedding), and the linearization model. As result, the system remains practical to deploy without heavy GPU memory requirementsespecially compared to other methods such as ISPDiffuser [70], which alone requires 20,938,890 parameters to render raw to single picture style (6 for six styles), or LiteISP [91], which requires 9,094,000 parameters (6 for six styles). The remainder of this section describes the functionalities of our tool. Figure 23. Our tool can process sRGB input images (or video frames) and perform in-batch rendering with specific picture styles and edits. Shown are example frames from sRGB video sequences processed by our method, where each row corresponds to distinct picture style and edits. See the supplemental video (click to view). I.2. Exposure Adjustment In addition to manual exposure adjustment (defined by an exposure value, EV, applied directly to the raw image and computed as 2EV), we provide an optional auto-exposure (AE) adjustment, implemented as digital brightness compensation applied after conversion to linear sRGB and before the digital gain in the first stage of our photofinishing module. This step is particularly useful in cross-camera scenarios, where some devices produce raw images that are considerably darker than those encountered during training. Without such correction, these inputs may be misinterpreted as near-black, leading to insufficient boosting by the photofinishing network. Our AE approach follows histogram-based formulation inspired by prior work on exposure correction and tone reproduction [21, 75]. To reduce computation and suppress noise, we first downsample the input to 128128 pixels using area pooling. From the downsampled linear sRGB image, we compute luminance representation YE [0, 1]128128 and construct 1D histogram with 96 bins. We then define Gaussian target histogram over bin centers [0, 1] as follows: (cid:18) (b) exp 1 2 (cid:16) bgE σE (cid:17)2(cid:19) , (47) Figure 24. Example of luma and chroma denoising. The shown image was captured using Samsung S24 main camera and rendered with our tool. We show our output without luma/chroma denoising and with both enabled at 80% strength. (λc-d > 0.4), an additional Gaussian pass is applied to further enhance color uniformity in low-texture regions. The final chroma outputs are blended with the original channels using non-linear mixing rule: Cbc-d = (1 λ1.5 Crc-d = (1 λ1. c-d) CbLsRGB + λ1.5 c-d) CrLsRGB + λ1.5 c-d Cb c-d Cr LsRGB, LsRGB. (46) The denoised luma and chroma channels are merged to form the denoised YCbCr version of the linear sRGB image, which is then transformed back to the linear sRGB space to be processed by the next procedure in our pipeline. Figure 24 shows an example. centered at mid-gray value gE=0.08 with fixed spread σE=0.05. This target distribution encodes the desired tonal balance, with mid-tones dominating while highlights and shadows are preserved. To determine the exposure correction, we search over discrete set of EV candidates 20 [E, +E] with E=1.8, corresponding to multiplicative scalings of 2. For each candidate, the luminance matrix YE is scaled, histogram is recomputed, and the ℓ2 distance to the target distribution is evaluated. The EV that minimizes this distance is selected: = arg min [E,+E] (cid:13) (cid:13)H E(2 YE) t (cid:13) 2 2. (cid:13) (48) The final output is obtained by scaling the input image by 2 . We apply the EV correction in linear sRGB space, since this stage precedes the non-linear operations in the photofinishing module. Applying exposure adjustment after nonlinear mapping can limit the ability to adjust exposure effectively [13]. Figure 25 illustrates comparison between applying our AE in linear sRGB versus applying it at the end of the pipeline. I.3. White Balance Estimation and Adjustment In all of our main experiments, we relied on the cameras auto white balance (AWB)the scene illuminant color produced by the on-device AWB estimator embedded in the conventional ISP. Nevertheless, our tool also offers the ability to compute AWB using state-of-the-art illuminant estimation methods. For images captured by the Samsung S24 main camera, we adopted the camera-specific illuminant estimator of [16], which we re-trained after removing time and location features since location information is not always available in DNG files. For images captured by other cameras, we employed the cross-camera illuminant estimator C5 [11]. Following [52], we used modified C5 model that required only the testing image, without the need for additional images from the same camera as in the original C5 method. Specifically, we employed single-encoder variant that received 4848 histogram as input. This model was trained on the NUS dataset [31], the Cube++ dataset [36], and the S24 dataset. We followed the training instructions in the original works for both the cameraspecific and cross-camera estimators. Both AWB models (camera-specific and cross-camera) target neutral white balance, where the goal is for achromatic surfaces to appear gray. However, such neutral estimates do not always match human perceptual preference [8], where bias is often desirable. To account for this, we optionally integrate post-processing step that applies non-linear mapping from the neutral AWB estimate to perceptual preference-aware AWB, following [92]. Our tool also allows manual adjustment of the correlated color temperature (CCT) and tint. For this, we rely on Robertsons method [84] to map between CIE XYZ, CCT, and tint coordinates, combined with an interpolation of the Planckian locus. In this context, the CCT corresponds to the point on the Planckian locus that is closest to the illuminant chromaticity, while the tint represents the signed 21 perpendicular offset of the illuminant from this locus in the CIE 1960 (u, v) chromaticity diagram. By parameterizing AWB in terms of both CCT and tint, we enable intuitive user control: CCT adjusts the overall warmth or coolness of the image, while tint provides fine-grained correction of residual color casts that cannot be addressed by CCT alone. Finally, for the case where the AWB gains are taken from the camera (i.e., the ISP-provided AWB stored in the DNG files), we use the interpolated color correction matrix (CCM) already provided in the DNG files. For all other AWB settings (whether estimated using the cameraspecific model or the cross-camera C5 model, in either neutral or preference-biased form, or manually adjusted via CCT and/or tint), we re-compute the CCM that maps the white-balanced raw RGB to linear sRGB. This is achieved by interpolating between the pre-calibrated CCMs available in the DNG metadata, following the DNG specification [5]. Figure 26 shows an example output from our tool, illustrating different AWB renderings. I.4. Contrast Adjustment After applying local tone mapping, we optionally include contrast adjustment step that can be controlled by the user before the chroma mapping stage in our photofinishing module. This operation modifies the luminance channel YLTM around mid-gray (0.5) to either increase or decrease the perceived image contrast. Specifically, given userspecified adjustment factor αcontrast [1, 1], we compute: ˆYLTM = (YLTM 0.5) (1 + 0.5 αcontrast) + 0.5. (49) Positive values of αcontrast increase image contrast by expanding the difference between dark and bright regions, while negative values reduce contrast by compressing luminance values toward mid-gray. This adjustment allows users to fine-tune the global contrast of the image in addition to the automatic tone mapping provided by the pipeline. We find that applying contrast adjustment in the luminance channel at this stage leads to more visually pleasing results compared to applying it directly to the three channels of the processed linear sRGB either after local tone mapping or after the full photofinishing module (see Fig. 27). I.5. Highlights and Shadows Adjustment We allow the user to optionally adjust highlights and shadows after the local tone-mapping stage. Both operations are applied to the luminance channel (YLTM) of the YCbCr local tone-mapped image. We first construct smooth masks that isolate highlight and shadow regions: Mhigh = ς(YLTM; τhigh, τhigh + ϵhigh), Mshad = 1 ς(YLTM; τshad, τshad + ϵshad), (50) (51) Figure 25. We allow applying auto exposure (AE) after conversion to linear sRGB and directly before the photofinishing module. This figure shows qualitative example from the S24 validation set [16], including the input raw image, our output without AE, and with AE applied either at the end of the pipeline or in linear sRGB. The ground-truth sRGB image from the S24 dataset is shown for reference. Figure 26. Our tool provides different white-balance options. The image can be rendered using the cameras AWB (referred to as as shot in our tool), or we can re-estimate the illuminant color using learning-based auto white-balance model. For the latter, we provide two modes: neutral white-balance correction that enforces achromatic objects to appear gray, and preference-aware AWB that applies bias to better match human perceptual preference. In addition, the user may manually adjust the CCT and tint. Example shown from the test set of the S24 dataset [16]. where τhigh and τshad are thresholds for highlight and shadow regions, ϵhigh and ϵshad control the transition smoothness, and ς denotes the smoothstep interpolation defined as: ς(x; e0, e1) = , (52) (cid:19)2 (cid:18) e0 e1 e0 with e0 and e1 denoting the lower and upper transition edges. To prevent over-amplification near saturation, adjustments are limited to the valid luminance range [0.1, 0.9]. In our implementation, we set τhigh = 0.7, τshad = 0.3, and ϵhigh = ϵshad = 0.1. The adjusted luminance channels are then computed as follows: high = YLTM + shad = YLTM + αhigh 20 αshad YLTM Mhigh, (53) (1 YLTM) Mshad, (54) where αhigh, αshad [1, 1] are user-controlled parameters. Positive αhigh values boost highlights, while negative values compress highlights to recover detail. Positive αshad values lift dark regions, whereas negative values deepen them. Finally, the adjusted luminance is combined with the original chroma channels to reconstruct the full YCbCr image, which is then propagated through the remainder of the pipeline. This provides intuitive, user-controlled tonal adjustments while preserving color fidelity. See Fig. 28 for visual examples. I.6. Saturation and Vibrance Adjustment We optionally allow user-controlled color saturation and vibrance adjustments in the HSV color space after gamma correction in our photofinishing module. Working in the HSV color space is numerically stable because its saturation channel is bounded in [0, 1], and it provides an intuitive axis for perceptual editing. For each pixel, given its HSV representation (ph, ps, pv), the saturation adjustment rescales the saturation channel uniformly across all colors as: = min (cid:0)ps (1 + αsat), 1(cid:1), (55) where αsat is the saturation control parameter. Positive values of αsat increase global saturation, while negative values move the image toward grayscale. Figure 27. We implement user-controlled contrast adjustment on the luminance channel after local tone mapping. This figure shows qualitative example from the S24 validation set [16], including the input raw image, our output without adjustment (αcontrast = 0.0), and with reduced contrast (αcontrast = 1.0). For comparison, results are shown when the adjustment is applied after local tone mapping in linear sRGB, after the full photofinishing module, and to the channel after local tone mapping (YLTM) in YCbCr (our choice). Figure 28. We allow the user to optionally adjust highlights and shadows after applying local tone mapping. Shown here is an example from the S24 validation set [16], including the input raw image and our outputs with different values of the highlight and shadow control parameters, αhigh and αshad, respectively. In contrast, vibrance selectively boosts muted colors while preserving already saturated regions. The vibrance transformation is defined as: = min (cid:0)ps (1 + αvib (1 ps)), 1(cid:1), where αvib is the vibrance strength. Positive values of αvib increase vibrance by amplifying low-saturation colors more strongly than highly saturated ones, whereas negative values reduce vibrance. (56) After applying either transformation, the adjusted HSV values (ph, s, pv) are converted back to the sRGB space to continue processing in the pipeline. Figure 29 illustrates an example from the S24 test set [16], rendered with our photofinishing pipeline using different saturation and vibrance settings. As shown, the saturation adjustment uniformly scales the chroma across all colors, whereas vibrance selectively boosts muted colors while preserving already saturated regions. I.7. Detail-Enhancement Control Similar to the denoising strength control, our tool provides continuous control over the detail-enhancement strength. This is implemented as linear interpolation between the image before and after the detail enhancement operation, modulated by scalar parameter in the range [0, 1]. value of 0 preserves the unprocessed image, value of 1 applies the full operation, and intermediate values yield proportional effects. I.8. Sharpening Adjustment We apply an optional, user-controlled, edge-aware sharpening step at the final stage of our pipeline, operating on the full-resolution sRGB image. Let Iin denote the input to the sharpening operator (equivalent to Iout, the final output of our pipeline). We first compute smoothed base layer Bsharp using Gaussian blur with kernel size ρsharp = 3 Figure 29. We allow the user to optionally adjust the saturation and vibrance in our pipeline. Shown here is an example from the S24 test set [16], including the input raw image, our output without adjustment (αsat = αvib = 0.0), and results with different saturation and vibrance settings (α {0.5, +0.5, 0.95, +0.95}). Saturation uniformly scales chroma across all colors, whereas vibrance selectively enhances muted colors while preserving already saturated regions. and standard deviation σsharp = 1.0. The high-frequency detail layer is then defined as: Dsharp = Iin Bsharp. (57) To prevent uniform sharpening of noisy flat regions, we construct an edge-aware mask. Horizontal and vertical gradients are estimated by convolving the input image with simple derivative kernels, producing response maps Gx and Gy. Gradients are computed independently per channel using [1, 0, 1] derivative kernels, and the resulting magnitudes are averaged to form normalized edge mask. The edge magnitude map is then computed as: Esharp(x,y) = (cid:113) G2 x(x,y) + G2 y(x,y), (58) which is normalized to form the edge mask Msharp [0, 1]. This mask ensures that sharpening is applied predominantly along edges. The final sharpened image is obtained as: sharp = Iin + αsharpDsharpMsharp, where αsharp controls the sharpening strength. Higher values of αsharp increase edge contrast, while setting αsharp = 0 disables sharpening. Figure 30 shows qualitative results of the sharpening operator applied to an example image. (59) 24 I.9. Editing Picture Styles As our photofinishing module governs the overall appearance and perceived look and feel of the final image, and given its modular design, we provide users with finegrained control over each operator and enable flexible style editing. Specifically, our tool allows: 1) disabling individual operators (e.g., tone mapping, gamma correction) within selected picture style, 2) interpolating operators parameters between multiple picture styles, or 3) replacing specific operators of one style with those of another target style. We implement style mixing through simple linear interpolation of the operator parameters predicted by the corresponding photofinishing networks of each selected style. The interpolated parameters are then applied once through our photofinishing module, ensuring consistent and efficient rendering. Figure 31 shows an example image captured with an iPhone 13 (an unseen camera in our training) and processed using our generic denoiser and the photofinishing module (trained on images taken by the S24s main camera) with different picture styles. The figure demonstrates three types of style mixing: 1) operator replacement (e.g., rendering the image with Style #0 while substituting chroma mapping from another style), 2) operator interpolation (e.g., performthe initial rendering, allowing unlimited post-saving edits without needing to access the original raw DNG file. For the Raw-JPEG Adapter [15], we provide two quality settings: raw-JPEG quality = 95, which typically adds about 2-3 MB to the sRGB JPEG file size, and raw-JPEG quality = 75, which adds around 1-2 MB. These values represent modest storage overhead compared to alternatives such as HEIC on iPhone, which also supports post-capture re-rendering through Apples Photos editing tool but often produces larger files (exceeding 10 MB in some scenes or picture styles) and DNG files, which typically range from 12 MB to 35 MB per 12-megapixel image. In contrast, for 12-megapixel images, our approach of appending the JPEG-compressed raw data to the final image requires, on average, only about 5-6 MB in total at raw-JPEG quality = 95 (including approximately 3 MB for the sRGB JPEG) and about 3-4 MB at raw-JPEG quality = 75. To generate raw-JPEG compressed file, we pre-process the raw images before JPEG compression using trained models corresponding to each target JPEG quality, as described in [15]. During decoding, and prior to JPEG decompression, we inverted the Raw-JPEG Adapter operators using the stored metadata of the operator parameters embedded in the raw JPEG file. In our implementation, we employed the Raw-JPEG Adapter model variant without the DCT component, which was recommended for better generalization to unseen cameras during training [15]. The Raw-JPEG Adapter model is lightweight, with only 32,076 parameters, and introduces an average overhead of about 0.3 seconds for raw image processing before encoding and 0.1 seconds for restoration. With this design, our tool can re-render or edit images with the same level of functionality as when operating directly on the original DNG raw files during the post-render stage, even after multiple re-rendering operations without cumulative degradation in accuracy. This is achieved by preserving the complete raw data within the final JPEG file, with only slight loss in accuracy due to compression. Figure 32 demonstrates this capability, showing an example where an image saved using our tool is later rerendered both with the same settings and with different styles and parameter configurations. quantitative evaluation is provided in Sec. K.2.3, where we compare our design against prior re-rendering alternatives, showing that embedding the raw data within the JPEG enables higher-quality re-rendering results. I.11. Processing Input sRGB Images To broaden the applicability of our framework beyond raw images, we also support standard 8-bit sRGB inputs (e.g., JPEG/PNG) that are not accompanied by embedded raw data. Since our pipeline is designed to operate on raw images, we first linearize the input sRGB image using the Figure 30. We allow the user to apply sharpening as the final step of our pipeline. The example shown is from the validation set of S24 [16], including the input raw image and our rendered outputs with different sharpening strengths αsharp. Increasing αsharp enhances edge contrast, with αsharp = 0 corresponding to no sharpening. ing chroma mapping by interpolating 50% from Style #3 and 50% from Style #5), and 3) operator disabling (e.g., rendering with all operators of Style #5 except global tone mapping, which is disabled). I.10. Re-Rendering with Embedded Raw After rendering the raw image to sRGB, our tool embeds the original raw data within the JPEG container of the rendered output. This was achieved by first compressing the raw image as JPEG using the Raw-JPEG Adapter [15], and then appending this raw-JPEG file (along with the operator parameters, DNG metadata, and the current editing configuration of the image in the tool) after the end-of-image (EOI) marker of the sRGB JPEG. In this way, the visible JPEG remains fully standards-compliant and decodable by any conventional viewer, while our tool can still access the embedded raw data for re-rendering and further adjustments. This design offers several advantages. First, it ensures backward compatibility, as the sRGB JPEG remains viewable on all platforms without modification. At the same time, it enables the distribution of single JPEG file that contains both the standard viewable image (compatible with any JPEG decoder) and the corresponding raw data (accessible by our tool), with small increase in storage size compared to saving the raw data separately without compression (e.g., in DNG format). Lastly, by embedding the operator parameters, the user can later reset all manual adjustments and reapply new set exactly as applied during Figure 31. Our modular photofinishing design provides users with full control over the picture style of the final image. In addition to selecting from pre-defined picture styles (each rendered by pre-trained photofinishing module), users can also mix operators from different styles or disable specific operators entirely. The shown example was captured with the iPhone 13 main camera, which is an unseen camera for our pre-trained photofinishing modules. method of [10], where lightweight network maps the input sRGB image to the CIE XYZ linear space. We then follow the CIE XYZ-to-raw mapping strategy described in Sec. 4.2.4 of [10]. In this step, we fix the CIE XYZ-to-raw 33 matrix to the one corresponding to the S24 main camera under D50 illumination (CCT=5000K). The mapped raw image is then multiplied by the D65 light color in the raw space, yielding synthetic raw-like representation of the input sRGB image within the S24 main cameras raw space. Unlike the original design in [10], which employed two networks to map between sRGB and CIE XYZ in both directions, we only use single network to map from sRGB to CIE XYZ. To further reduce computational cost and memory usage, we use lightweight variant of the original architecture. Specifically, we reduced the local sub-network depth in [10] from 16 to 8 convolutional blocks, and the number of channels per convolutional layer from 32 to 24. For the global sub-network in [10], we 26 Figure 32. Our framework enables post-editable re-rendering of saved JPEG images with the same functionality available to the original raw data. This is achieved by embedding the raw image in compact form alongside the final sRGB rendering. Shown here is an example image captured with an iPhone 13 and processed by our tool. The first result shows the default rendering, while the remaining results are obtained by re-rendering from the restored raw image using different rendering settings. use depth of 4 (instead of 5) with 24 channels (instead of 64), and reduce the final three fully connected layers from [1024, 1024, 1024] to [512, 256, 256], while disabling the dropout layer. After these modifications, the total number of parameters in the linearization model is 693,493 (2.7 MB), resulting in only light additional load on the GPU. We trained this lightweight linearization model for 300 epochs using the Adam optimizer [54] on 512512 nonoverlapping patches from the sRGB-to-CIE XYZ dataset [10], with mini-batch size of 8 and an L2 regularization factor of 106 (in contrast to 103 used in the original work). With this synthetic raw conversion process, our tool remains applicable to sRGB inputs rendered by any camera or software ISP, as well as to synthetically generated sRGB imagesalbeit with reduced accuracy due to residual nonlinearities that cannot be fully removed by linearization. Figure 33 illustrates two examples: the first is an sRGB image captured and rendered by the iPhones native camera ISP, and the second is generated by the Gemini 2.5 Flash Image model. Both images are linearized, mapped to the S24 camera raw space, and subsequently rendered with different settings using our tool. J. User Study Details As discussed in the main paper, we conducted user study to compare our method with the Samsung S24 native camera ISP and Adobe Lightroom (using its built-in autoenhancement feature). We captured 45 scenes with the S24 main camera in Pro mode (which outputs DNG raw files) and re-captured the same scenes using the native camera application. The images covered variety of conditions, including indoor, outdoor daylight, sunset, and low-light scenes. The DNG files were processed with Adobe Lightroom (auto enhancement) and with our method, resulting in three versions for each scene: ours, native ISP, and Lightroom. Participants were presented with these three versions side-by-side in custom user-study GUI. The order of the images was randomized for each trial to prevent bias, and the internal order of the methods was randomized as well. For each scene, participants were asked to select the best image under four criteria: color quality How good and natural do the colors look?, bright27 Figure 33. To broaden the applicability of our tool to sRGB images saved without embedded raw data (i.e., outside our framework), we linearize the input sRGB images (whether produced by unknown ISPs or generated by AI models) into synthetic raw-like representations, thereby enabling the full functionality of our pipeline. The top example shows an image captured and rendered by the iPhone camera ISP, from which we generate raw-like image and process it with different settings using our tool. The second example is an image generated by the Gemini 2.5 Flash Image model with the prompt: Generate an ancient Egyptian scene with pyramids, the Nile, and palm trees glowing under golden sunset. We convert it into raw-like image and apply our tool with different rendering options. 28 Table 7. Raw image denoising results on the S24 noisy/denoised test set [16] using NafNet [30] and Restormer [88]. The best results are highlighted in yellow. Method NafNet (large, ours) [30] Restormer [88] S24 Test Set (Noisy/Denoised) SSIM PSNR 0.999 57.33 0.999 55.42 Table 8. Ablation study of raw denoising and detail enhancement on the S24 [16] test set. Inputs are noisy raw images, and outputs are compared against ground-truth sRGB images. Denoising was performed using the base model (933K parameters). The best results are highlighted in yellow. Framework Variation w/o raw denoising raw denoising w/ raw denoising + enhancement w/ S24 Test Set PSNR 24.48 26.48 27.52 SSIM 0.731 0.883 0.922 uated against the pseudo ground truth provided in the S24 dataset. The large NafNet variant achieves superior results while requiring substantially fewer parameters. Table 8 reports results on the S24 test set, where the inputs are noisy raw images and both the outputs and ground truth are sRGB-rendered images. We compare three configurations of our framework (with the photofinishing module enabled in all cases): without denoising, with denoising only (excluding the detail-enhancement network), and with both denoising and detail enhancement, using the base denoising model. The results show that image denoising yields significant improvements. K.1.2. Upsampling Ablations In both the main paper and this supplementary material (Sec. E), we have introduced our regularized variant of BGU [29]. Here, we present ablation results comparing different guided upsampling approaches, including BGU without our regularization. For completeness, we also compare against the regularization used in the original BGU Halide implementation, which we denote as Halide regularization. key challenge in this evaluation arises from the ground-truth sRGB images in the S24 test set. These ground-truth images include sharpening and detail enhancements that are not present in the pseudo ground-truth denoised raw images. Since the guide image (denoised raw or any of its variants, e.g., linear sRGB) lacks such details, no guided upsampling method can fully recover them. To address this, we conducted experiments under two groundtruth settings: 1. Original S24 ground truth: sRGB images with sharpening, detail enhancement, and compression (as provided in the original dataset). 2. Alternative ground truth: sRGB images rendered diFigure 34. User study results. Preference rates (%) for each criterion comparing our method, the Samsung S24 native ISP, and Adobe Lightroom (auto enhancement). Our method is consistently preferred by participants across all four criteria. ness & contrast Is the image clear, with good balance between light and dark sharpness & detail How crisp and areas?, detailed does the image look?, overall preference Which image do you simply like more overall?. total of 20 participants took part in the study. and As shown in Fig. 34, our method was consistently preferred across all four criteria: 53.2% in color quality, 46.4% in brightness & contrast, 43.4% in sharpness & detail, and 51.4% in overall preference, with margins of +13.927.0% over the closest competitor. These results demonstrate that users consistently favor our rendering pipeline over both the commercial Samsung ISP and Lightroom auto enhancement. See Fig. 35 for examples from the captured images and the rendering results from the native camera ISP, Adobe Lightroom (auto enhancement), and our method. K. Ablation and Additional Results K.1. Ablation Studies We conducted series of ablation studies to validate the effectiveness of each component in our method. In this subsection, we describe these experiments in detail and present the corresponding results. K.1.1. Denoising Ablations As discussed in the main paper, we evaluated three NafNet lite (245 pavariants [30] for raw image denoising: rameters), base (933 K), and large (3.63 M). To compare NafNet with an alternative architecture for raw image denoising, Table 7 shows results for the large NafNet model and Restormer [88] (26.1 M), both trained on the S24 raw/denoised pairs [16]. The input to each model is noisy raw image, and the output is denoised raw image, evalFigure 35. Example comparisons shown to participants during the user study. For each scene, participants evaluated three versions: our method, the Samsung S24 native ISP, and Adobe Lightroom (auto enhancement). The order was randomized in the GUI for every trial. rectly from the denoised raw DNG files using Adobe Photoshop, with sharpening and detail enhancements disabled and no compression applied. For both ground-truth settings, we evaluated two guide image versions: 1) the denoised raw image and 2) its linear sRGB conversion. The corresponding source image was generated by downsampling the guide image to one-quarter of its original resolution, while the target image was obtained by downsampling the ground-truth sRGB image to the same resolution. We utilized ground-truth sRGB images in these experiments, rather than outputs from our photofinishing module, to isolate the evaluation of the guided upsampling methods themselves under the best-case scenario, without indirect effects from the photofinishing stage. Table 9 reports results comparing BGU [29] with our regularization, BGU without regularization, and BGU with the Halide regularization, along with two alternative guided upsampling methods: guided linear upsampling (GLU) [74] and classical guided image filtering (GF) [43]. Figure 36 provides visual comparison of BGU results without and with our proposed regularization. Overall, our regularization produces reconstructions that are both visually and quantitatively closer to the ground truth. 30 Table 9. Comparison of different guided upsampling methods on the S24 test set [16] under two ground-truth settings: original S24 ground truth (with detail enhancements) and alternative ground truth (without enhancements). Results are reported for two guide image versions: denoised raw and linear sRGB (LsRGB). The best results are highlighted in yellow. Method GLU [74] GF [43] BGU (w/o regularization) [29] BGU (w/ Halide regularization) [29] BGU (w/ our regularization) [29] Original S24 GT (Denoised Raw/sRGB) PSNR 33.46 31.58 33.73 32.49 33.85 SSIM 0.919 0.907 0.928 0.916 0.927 Alt. S24 GT (Denoised Raw/sRGB) PSNR 37.95 33.74 40.82 37.28 41.69 SSIM 0.966 0.943 0.986 0.977 0. Original S24 GT (LsRGB/sRGB) SSIM PSNR 0.919 33.47 0.912 31.97 0.928 33.77 0.918 32.78 0.928 33.92 Alt. S24 GT (LsRGB/sRGB) SSIM PSNR 0.971 38.42 0.959 34.84 0.988 41.00 0.980 38.26 0.989 42.01 Figure 36. Visual comparison of guided upsampling on an example from the S24 dataset [16]. We compare BGU [29] without regularization against our proposed regularization. Our regularization better preserves fine details and produces results closer to the ground truth. K.1.3. Photofinishing Loss Ablations To analyze the contribution of each loss term in our photofinishing module, we conducted detailed ablation study on the default picture style (Style #0) of the S24 dataset [16] (Tables 10 and 11). For these experiments, we chose to process images starting from the pseudo ground-truth denoised raw inputs, which were subsequently mapped to the linear sRGB color space. This setup allows us to isolate and evaluate the photofinishing module independently, without the influence of potential degradations introduced by the denoising stage in our full pipeline. When training with all loss terms enabled, our model achieves the best accuracy in both PSNR and SSIM, as highlighted in Table 10. Removing any single loss term slightly decreases the accuracy. Conversely, progressively adding the losses one by one  (Table 11)  results in steady improvements, indicating that each term contributes positively to the overall quality. Figure 37 shows that the tone-mapping loss (ℓTM) and luminance-consistency loss (ℓluma) lead to more balanced tone reproduction across the image, encouraging the global and local tone-mapping stages of the photofinishing module to function as intended. The global stage refines the overall tone distribution of the gain-adjusted linear sRGB image, while the local stage further adjusts contrast in spatially varying regions. Both stages maintain comparable overall luminance level, which encourages the local tone-mapping stage to focus on localized adjustments rather than global Table 10. Ablation study showing the impact of removing each loss term individually on the results of our photofinishing module, evaluated on the S24 test set [16]. The input images are linear sRGB frames generated from pseudo ground-truth denoised images in the S24 test set, downsampled to one-quarter resolution. The outputs are compared against the ground truth at the same resolution. The best results are highlighted in yellow. S24 Test Set (1/4 LsRGB/sRGB) ℓ1 ℓSSIM ℓperc ℓE ℓCbCr ℓLuT-s ℓluma ℓTM ℓLTM-s PSNR SSIM 0.934 27.01 0.931 27.01 0.934 26.89 0.934 26.96 0.934 26.99 0.934 27.18 0.934 27.20 0.934 27.13 0.934 27.05 0.939 27.49 Table 11. Ablation study showing the cumulative effect of adding each loss term progressively to our photofinishing module, evaluated on the S24 test set [16]. Starting from baseline trained with ℓ1 only, additional loss terms are introduced one at time. Input images are linear sRGB frames generated from pseudo groundtruth denoised images in the S24 test set, downsampled to onequarter resolution. The outputs are compared against the ground truth at the same resolution. The best results are highlighted in yellow. S24 Test Set (1/4 LsRGB/sRGB) ℓSSIM ℓ1 ℓperc ℓE ℓCbCr ℓLuT-s ℓluma ℓTM ℓLTM-s PSNR SSIM 0.930 26.80 0.935 27.03 0.935 27.07 0.935 27.08 0.934 27.09 0.934 27.09 0.935 27.13 0.934 27.05 0.939 27.49 shifts. Similarly, Fig. 38 shows that our chosen loss weight for the local tone-mapping smoothness term (λLTM-s=0.6) 31 Table 12. Effect of different values of the local tone-mapping smoothness loss weight (λLTM-s) on the S24 test set [16]. Input images are pseudo ground-truth denoised raw images mapped to the linear sRGB space at one-quarter of the original raw resolution. The best results are highlighted in yellow. Loss weight (λLTM-s) 0.06 0.6 6.0 S24 Test Set (1/4 LsRGB/sRGB) SSIM PSNR 0.935 27.13 0.939 27.49 0.933 26.19 Table 13. Ablation on the design of our photofinishing module. For this ablation, we remove one network of our photofinishing module at both training and inference to evaluate its contribution. Results are shown on the S24 test set [16] (default style; Style #0), where the input images are pseudo ground-truth denoised raw images mapped to the linear sRGB space and downsampled to onequarter of the original resolution, and the ground truth is the corresponding sRGB image at the same resolution. The best results are highlighted in yellow. S24 Test Set (1/4 LsRGB/sRGB) Gain GTM LTM Chroma Gamma PSNR 26.68 26.84 24.66 26.45 26.91 27.49 SSIM 0.931 0.934 0.898 0.928 0.935 0.939 achieves good balance between suppressing local artifacts and preserving fine details (see Table 12 for the quantitative comparison). K.1.4. Photofinishing Design Ablations We conducted set of ablation studies to evaluate our design against alternative configurations. As described earlier, the proposed photofinishing module consists of five networks: digital gain, GTM, LTM, chroma mapping, and gamma correction. Each network predicts the parameters of its corresponding operator. The rest of this subsection details the contribution of each component. Overall Module Composition. Table 13 shows the results of different variants of our photofinishing module, where one network is removed at both training and testing to evaluate the contribution of each stage. As shown, the proposed full design not only provides greater modularity and user control but also achieves the best quantitative results. Figure 39 shows qualitative comparison. Table 14. Ablation on the local tone-mapping (LTM) design. We progressively add each predicted coefficient map (ALTM-GLTM) to evaluate their contribution. The All (w/o multi-scale guide) variant includes all five parameters but omits the multi-scale guidance network, using single guidance network in its place. Results are on the S24 test set [16] using pseudo ground-truth denoised raw inputs mapped to linear sRGB at one-quarter resolution. The best results are highlighted in yellow. LTM Design ALTM ALTM, BLTM ALTM, BLTM, CLTM ALTM, BLTM, CLTM, WLTM All (w/o multi-scale guide) Ours (all) S24 Test Set (1/4 LsRGB/sRGB) SSIM PSNR 0.927 26.49 0.928 26.54 0.927 26.58 0.931 26.86 0.933 26.87 0.939 27.49 ALTM, BLTM, CLTM, WLTM, and GLTM. We progressively enabled these coefficients to evaluate their contribution, and we further examined the effect of the LTM grid size and the design of the chroma LuT used for chroma mapping. Table 14 reports results of training our photofinishing module with different subsets of the LTM parameters. We began by predicting single-channel pixel-wise map representing ALTM to modulate the tone-mapping exponent. Next, we introduced BLTM to allow adaptive control over the compression strength in the denominator of the tonemapping function, followed by adding CLTM to model asymmetric behavior between bright and dark regions. We then incorporated the blending map WLTM to locally mix the LTM output with the globally tone-mapped result GTM, and subsequently added the gain map GLTM, which adjusts local exposure before tone mapping. This full set of parameters was tested both without and with the multi-scale guidance network to evaluate its contribution. See Fig. 40 for qualitative example. Table 15 complements this analysis by showing the results of modifying the LTM grid size and the chroma mapping design. We first reduced the LTM grid size from 6464185 to 323295 to examine the impact of spatial resolution of the LTM parameter grid. Next, we evaluated predicting the chroma LuT directly instead of learning it as residual to the image-independent learnable LuT. Lastly, we evaluated smaller chroma LuT with Nh=12 bins (i.e., LuT size of 12 12 2) instead of Nh=24 (i.e., 24242) to study the effect of chroma quantization granularity. The results demonstrate that both the residual formulation and higher grid/LuT resolution contribute to improved reconstruction accuracy, with our full configuration achieving the best overall results. Local Tone-Mapping and Chroma Design. We conducted additional experiments to ablate the design choices of our LTM and chroma mapping components. The LTM network predicts five spatially varying coefficient maps: Multi-Branch and Coordinate Attention. We further evaluated the contribution of the multi-branch convolutional (MBConv) and coordinate attention (CA) [45] blocks 32 Figure 37. Visual comparison showing the impact of training with and without the tone mapping loss (ℓTM) and the luma energy loss (ℓluma). Shown are the input raw image, the color-corrected linear sRGB image after digital gain, the results after global and local tone mapping, and the final photofinishing output compared against the ground truth at the same resolution (i.e., one-quarter of the original raw resolution). The input sample is from the validation split of the S24 dataset [16], where the photofinishing result shown here is obtained by processing the pseudo ground-truth denoised raw image mapped to the linear sRGB space. Table 15. Ablation on the design of the chroma mapping and LTM networks. We evaluate 1) predicting the chroma LuT directly instead of as residual, 2) using smaller number of chroma bins (Nh=12 instead of Nh=24), and 3) reducing the LTM grid size to Ng=32 and depth to Nc=9, instead of Ng=64 and Nc=18. Results are shown on the S24 test set [16], where the input images are pseudo ground-truth denoised raw images mapped to linear sRGB and downsampled to one-quarter of the original resolution. The best results are highlighted in yellow. Variant Chroma LuT (no residual) Chroma LuT bins (Nh) = 12 LTM grid size (Nc = 9, Ng = 32) Ours S24 Test Set (1/4 LsRGB/sRGB) SSIM PSNR 0.935 27.07 0.932 26.88 0.934 26.94 0.939 27. (Sec. C.2) used across the networks of our photofinishing module. For this ablation, we trained and tested variants with each component removed independently. Table 16 shows that both the multi-branch structure and the coordinate attention blocks improve the result of our photofinishing module. Table 16. Ablation on the effect of multi-branch (MBConv) and coordinate attention (CA) [45] in our photofinishing module. Results are reported on the S24 test set [16] (1/4 linear sRGB/sRGB). The best results are highlighted in yellow. Photofinishing Networks w/o MBConv w/o CA Ours (w/ MBConv & CA) S24 Test Set (1/4 LsRGB/sRGB) SSIM PSNR 0.934 26.95 0.933 26.93 0.939 27.49 Tone-Mapping Domain. In our tone-mapping design, we apply both the global and local tone-mapping operators directly to the linear sRGB representation instead of to the luma (Y) channel of the YCbCr representation at each stage. While operating in the luma domain may seem more intuitive (since it isolates luminance from chroma), we found that this separation leads to performance degradation. Specifically, applying tone mapping (global and local) to the channel and then allowing the chroma-mapping network to process the CbCr components resulted in noticeably poorer qualitative and quantitative results. Table 17 compares our proposed design against the luma-based vari33 Figure 38. The impact of different values of λLTM-s on the final output of the photofinishing module. Our chosen value (λLTM-s=0.6) achieves good balance between smoothing the local tone-mapping maps and maintaining both qualitative and quantitative quality. The shown image is from the test set of the S24 dataset [16]. The displayed result corresponds to the final output of our photofinishing module, obtained by processing the pseudo ground-truth denoised raw image mapped to the linear sRGB space. Both the prediction and ground truth are shown at one-quarter of the original raw resolution. Table 17. Comparison between our tone-mapping design and luma-based tone-mapping design. In the luma-based design, global and local tone-mapping are applied only to the (luminance) channel of the YCbCr color space. In our design, tonemapping is applied jointly to all RGB channels in the linear sRGB domain. Results are reported on the S24 test set [16] (1/4 linear sRGB/sRGB). The best results are highlighted in yellow. Design Luma-based TM design Ours S24 Test Set (1/4 LsRGB/sRGB) SSIM PSNR 0.922 25.80 0.939 27. ant on the S24 test set [16]. As shown, applying tone mapping in the linear RGB domain yields superior PSNR and SSIM values. See Fig. 41 for qualitative comparison. Figure 39. Output of the photofinishing module trained without one of the processing stages (digital gain, global tone mapping, local tone mapping, chroma mapping, and gamma correction) compared to the full model trained with all stages enabled. The shown image is from the validation set of the S24 dataset [16]. The results are obtained by processing the pseudo ground-truth denoised raw image provided in the dataset, mapped to the linear sRGB space at one-quarter of the original raw resolution, and compared against the ground-truth sRGB image at the same resolution. Figure 40. Qualitative results from our photofinishing module trained with progressively added LTM parameters (ALTM, BLTM, CLTM, WLTM, and GLTM) during both training and testing. The input is pseudo ground-truth denoised raw image mapped to linear sRGB (1/4 resolution), and the output is compared against the corresponding sRGB ground truth. Examples are taken from the S24 validation set [16]. 3D Lookup Table. In the main paper, we described an optional learnable global 3D LuT designed to improve the rendering of artistic picture styles. Specifically, we learn global, image-independent 111111 RGB LuT, LRGB, which is applied before the chroma-mapping operator. This 3D LuT helps capture more aggressive color transformations that the 2D chroma LuT, Lchroma, constructed by our chroma-mapping network is limited in modeling (see Fig. 42 for qualitative example). However, we observed that when learning simpler picture stylessuch as the default color style used in the 34 Figure 41. Qualitative comparison between the luma-based tonemapping design and our proposed design. Inputs to the photofinishing module are pseudo ground-truth denoised raw images mapped to linear sRGB and downsampled to one-quarter of the original resolution. Results are shown from the validation set of the S24 dataset [16]. dataset [16] (Style #0)the 3D LuT provides negligible benefit. We evaluated our photofinishing module with and without the 3D LuT, where the input is the linear sRGB image derived from the pseudo ground-truth denoised raw data at one-quarter of the original raw resolution. Similar to our other ablations, we perform this analysis in isolation from other components such as guided upsampling or denoising to focus on the photofinishing performance. Table 18 reports the results on the artistic picture styles of the S24 dataset (Styles #1-5). As shown, incorporating the 3D LuT provides consistent improvement across styles, with only minor increase of approximately 4,000 parameters per style. In contrast, Table 19 shows that for the default picture style (Style #0), the 3D LuT yields no significant improvement and even slightly degrades SSIM. Comparison with HDRNet [39]. While both our LTM network and HDRNet [39] aim to predict locally adaptive tone-mapping operators, their formulations and architectures are fundamentally different. Both methods employ low-resolution bilateral grid of coefficients that are sliced using high-resolution guidance image to produce pixelwise operator parameters. However, the tone-mapping operators modeled by our LTM differ from those in HDRNet, and our network design is entirely distinct (Sec. C.5). To better highlight these differences, we conducted two ablation experiments. In the first, we replaced our LTM network and its operators with HDRNet, while keeping all In the second, other photofinishing networks unchanged. we replaced the entire photofinishing module with single HDRNet model. All models were trained jointly following Figure 42. Impact of incorporating global 3D LuT when learning artistic picture styles. Results are shown on the S24 test set [16], where the input to the photofinishing module is the linear sRGB version of the pseudo ground-truth denoised raw image downsampled to one-quarter of the original resolution. Figure 43. Qualitative comparison between our method and HDRNet [39]. We show results when HDRNet replaces 1) only the LTM network/operator and 2) the entire photofinishing module. The example is from the S24 test set [16]. In our result, multiscale processing and refinement (Sec. B.1) are applied within the LTM stage. the same configuration used for our photofinishing module on the S24 dataset [16]. Table 20 reports the quantitative results on the S24 test set. As shown, our design achieves higher PSNR and SSIM while requiring fewer parameters. See Fig. 43 for qualitative comparison. 35 Table 18. Ablation on the effect of the learned 3D LuT for rendering artistic picture styles using our photofinishing module. Results are shown on the S24 test set [16], where the input consists of pseudo ground-truth denoised raw images mapped to the linear sRGB space and downsampled to one-quarter of the original resolution, and the ground truth is the corresponding sRGB image in the target picture style at the same resolution. The evaluation covers five distinct artistic picture styles (Styles #15). The best results are highlighted in yellow. Photofinishing Module Style # Style #2 S24 Test Set (1/4 LsRGB/sRGB) Style #3 Style #4 Style #5 # params (per style) Without 3D LuT With 3D LuT PSNR 26.08 27.61 SSIM 0.926 0.935 PSNR 28.70 30.02 SSIM 0.923 0.935 PSNR 26.36 27.78 SSIM 0.924 0. PSNR 26.24 27.52 SSIM 0.916 0.932 PSNR 28.03 29.45 SSIM 0.949 0.968 207,224 211,217 Table 19. Ablation on the effect of using the 3D LuT in our photofinishing module for the default style (Style #0) on the S24 dataset [16]. The input consists of pseudo ground-truth denoised raw images mapped to the linear sRGB space and downsampled to one-quarter of the original resolution, and the ground truth is the corresponding sRGB image at the same resolution. The 3D LuT mainly benefits target picture styles with artistic appearance, whereas for simpler styles that mainly enhance color and overall tone, the gain is marginal. The best results are highlighted in yellow. Photofinishing Module Without 3D LuT With 3D LuT S24 Test Set (1/4 LsRGB/sRGB) PSNR 27.49 27.53 SSIM 0.939 0.935 Table 20. Alternative designs using HDRNet [39] as replacement for our local tone-mapping network and operator, or by substituting the entire photofinishing module with single HDRNet. Results are reported on the S24 test set [16], where the input consists of pseudo ground-truth denoised raw images mapped to the linear sRGB space and downsampled to one-quarter of the original resolution, and the ground truth is the corresponding sRGB image at the same resolution. The best results are highlighted in yellow. Method LTM HDRNet Photofinishing HDRNet Ours S24 Test Set (1/4 LsRGB/sRGB) PSNR # params 570,462 25.35 483,453 24.87 27.49 207,224 SSIM 0.920 0.911 0.939 K.2. Additional Results In this subsection, we provide additional results, including visual outputs of intermediate stages, quantitative results on an additional dataset, detailed evaluations across the S24 dataset styles, and further qualitative examples. K.2.1. Visualization of Photofinishing Stages Figure 44 visualizes intermediate stages of our pipeline (after denoising and color correction, digital gain, tone mapping, chroma mapping, gamma correction, and detail enhancement) using the default picture style (Style #0). Another example of the default style is shown in Fig. 45. Figure 46 shows two examples rendered with different artistic styles (Styles #3 and #4), illustrating how the modules internal representations adapt across styles. Figure 47 compares the intermediate outputs of the same scene across difTable 21. Results on the MIT-Adobe 5K dataset [26]. We report PSNR, SSIM [81], LPIPS [89], and 2000 [72], along with the total number of parameters for each method. The best results are highlighted in yellow, and the second best in green. Method ISPDiffuser [70] PyNet [48] CIE XYZ Net [10] Invertible-ISP [85] LAN [69] MicroISP [49] ParamISP [53] LiteISP [91] FourierISP [44] Ours (lite, w/o enhancement) Ours (base, w/o enhancement) Ours (large, w/o enhancement) Ours (lite, w/ enhancement) Ours (base, w/ enhancement) Ours (large, w/ enhancement) PSNR 19.44 19.18 20.25 20.06 16.46 19.84 21.17 21.45 20.29 21.10 21.10 21.11 21.27 21.28 21.29 Adobe 5K Test Set SSIM LPIPS 2000 0.711 0.695 0.715 0.693 0.618 0.708 0.735 0.740 0.737 0.733 0.733 0.736 0.739 0.740 0.742 11.030 12.300 10.352 10.800 14.817 12.188 9.729 9.687 10.582 11.021 11.020 11.005 10.765 10.762 10.743 0.333 0.219 0.189 0.205 0.275 0.217 0.184 0.169 0.208 0.223 0.226 0.225 0.219 0.221 0.220 # params 20,938,890 47,548,170 1,348,789 1,413,760 46,847 13,560 1,420,000 9,094,000 7,589,736 452,447 1,139,907 3,841,547 503,082 1,190,542 3,892,182 ferent picture styles, highlighting how the tone and color transformations learned by each network component vary with style. K.2.2. Results on MIT-Adobe 5K Dataset In the main paper, we reported our results on the S24 dataset, which includes multiple picture styles, including artistic styles. To further evaluate our method against other alternatives, we trained and tested it, along with competing methods, on the MIT-Adobe FiveK dataset [26], using Expert as the ground truth. For our method, we generated pseudo ground-truth denoised images following the same procedure used for the S24 dataset, employing the AI-based denoiser available in Adobe Lightroom. Further details on how we trained and evaluated other methods on this dataset are provided in Sec. L. The results are reported in Table 21. Our method achieves promising results, ranking second in PSNR and first in SSIM, while maintaining fully modular design with controllable rendering stages. Moreover, the lite version of our method surpasses several recent methods despite having significantly fewer parameters (21.27 dB with 0.5 parameters vs. ISPDiffuser [70] at 19.44 dB with 21 parameters, and ParamISP [53] at 21.17 dB with 1.4 parameters). K.2.3. Re-Rendering Results with Embedded Raw In Sec. I.10, we described our approach for embedding JPEG-compressed raw data alongside the rendered sRGB 36 Figure 44. Outputs of the intermediate stages in our pipeline, including raw denoising, color correction (CC), and the photofinishing module, which comprises digital gain, global tone mapping (GTM), local tone mapping (LTM), chroma mapping, and detail enhancement (enh.). We show enlarged patches of the final result when raw denoising is disabled, when detail enhancement is disabled, and when both are enabled. The example is taken from the S24 test set [16]. Figure 45. Intermediate outputs of our photofinishing module for the default picture style (Style #0). The example is from the S24 test set [16]. The input to the photofinishing module is the pseudo ground-truth denoised raw image mapped to the linear sRGB space at one-quarter of the original resolution, and the ground-truth reference is the corresponding Style #0 sRGB image at the same resolution. image within the saved file. This design enables an unlimited number of post-capture re-rendering operations without cumulative degradation in accuracy or the need to reconstruct raw data from the rendered sRGB image, while introducing only moderate increase in the final JPEG file size compared to alternatives such as iPhones HEIC format for picture styles or saving the original DNG file. Here, we present quantitative comparison against alternative methods that enable image re-rendering through raw reconstruction. Specifically, we compare our embedded-raw approach (Sec. I.10) with InvISP [85] and ParamISP [53], both of which reconstruct raw data from the rendered image to allow re-rendering with different configurations (i.e., using different model weights corresponding to distinct picture styles). Our evaluation scheme is as follows. Each test image in the S24 dataset [16] is first rendered into target style (referred to here as the source picture style) using InvISP or ParamISP, followed by raw reconstruction and subsequent re-rendering into the remaining five picture styles using the models trained for those target picture styles. For our method, we directly extract and decode the embedded raw data, then re-render it using our pipeline with the photofinishing and detail-enhancement models of the target picture style. In this experiment, we used the RawJPEG Adapter [15] with raw-JPEG quality setting of 75, which introduces approximately 1-2 MB of overhead for the embedded encoded raw data. As shown in Tables 2227, our method achieves consistently superior accuracy and avoids the variability observed in other methods, whose reFigure 46. Intermediate outputs of our photofinishing module for two artistic picture styles (Style #3 and Style #4). Each example is taken from the S24 test set [16]. Inputs are pseudo ground-truth denoised raw images mapped to linear sRGB space at one-quarter of the original resolution, and the corresponding sRGB ground-truth references are shown at the same resolution. rendering quality depends on the source picture style of the initially rendered sRGB image. SSIM [81], LPIPS [89], and 2000 [72]. The detailed results for Styles #15. K.2.4. Detailed Results on S24 Styles #15 K.2.5. Additional Qualitative Results In the main paper, we reported the PSNR scores of our method compared to alternative approaches across different picture styles available in the S24 dataset. Tables 28 32 provide more comprehensive evaluation, including We present additional qualitative results obtained using our method. Figure 48 shows qualitative comparison between our method and recent state-of-the-art neural ISP methods on the S24 test set [16], along with the PSNR results of 38 Figure 47. Comparison of intermediate-stage outputs for the same scene rendered with different picture styles. Images are taken from the S24 test set [16]. The examples illustrate how the photofinishing stages adapt their tone and chroma adjustments depending on the target picture style. each method for every style. As shown, our method consistently produces high-quality results while using fewer parameters overall, since only the photofinishing and detailenhancement modules employ style-specific weights, owing to the modularity of our design. Lastly, Figure 49 presents additional results on images captured by different iPhone devices. None of our training data includes iPhone images, further demonstrating the cross-device generalization ability of our method. Figure 48. Additional qualitative comparison between our method and recent neural ISP methods (ISPDiffuser [70], LiteISP [91], and ParamISP [53]). Results are shown for the default style of the S24 dataset [16] (Style #0) and the remaining artistic picture styles (Styles #1 5). The shown images are from the S24 test set. L. Evaluation Details Except for the reported results on the Zurich Raw-to-sRGB dataset [48] (Sec. B.2), we trained and evaluated all other methods ourselves, following their original papers and released codebases. We generally followed the training procedures and hyperparameters described in the original works, with minor modifications where necessary (as elaborated in the following). For the S24 dataset [16], only demosaiced linear RGB raw images are available. For methods requiring single-channel mosaiced inputs, we simulated the mosaicing process by applying fixed Bayer pattern to the RGB images. For three-channel input methods (such as ours), the raw input images from the S24 [16], MITAdobe FiveK [26], and Zurich Raw-to-sRGB [48] datasets were generated by applying black-level normalization (using black and saturation levels specified in the DNG files or dataset metadata), followed by demosaicing with the Menon algorithm [65] when RGB demosaiced raw images were not provided. 40 Figure 49. Additional results produced by our method. The shown images were rendered from raw captures taken with different iPhone devices (13, 13 Pro Max, 14, and 15) and across various camera modules (main, ultra-wide, and telephoto). For these results, we used the photofinishing and detail-enhancement models trained on the S24 dataset [16], along with the generic denoising and cross-camera AWB models. None of the models were trained on data from any iPhone device. 41 Table 22. Re-rendering results for target picture style #0 on the S24 dataset [16]. We report PSNR, SSIM [81], LPIPS [89], and 2000 [72]. For Invertible-ISP [85] and ParamISP [53], images were first rendered to sRGB using the source picture style (as indicated in the table) and then reconstructed to raw before rerendering to the target style. For our method, the embedded raw data were used directly for re-rendering, with the base denoising model applied. The best results are highlighted in yellow. Method InvISP (source: Style #1) InvISP (source: Style #2) InvISP (source: Style #3) InvISP (source: Style #4) InvISP (source: Style #5) ParamISP (source: Style #1) ParamISP (source: Style #2) ParamISP (source: Style #3) ParamISP (source: Style #4) ParamISP (source: Style #5) Ours (base denoiser) S24 Test Set (Target: Style #0) PSNR 22.69 22.51 22.74 22.50 22.82 21.71 22.67 21.50 22.40 16.88 26. SSIM LPIPS 2000 0.817 0.822 0.812 0.809 0.801 0.789 0.805 0.786 0.798 0.730 0.901 7.319 7.419 7.438 7.550 7.882 8.072 7.478 9.844 7.937 16.532 4.256 0.152 0.154 0.160 0.160 0.185 0.181 0.154 0.217 0.170 0.402 0.066 Table 23. Re-rendering results for target picture style #1 on the S24 dataset [16]. For Invertible-ISP [85] and ParamISP [53], images were first rendered to sRGB using the source picture style and then reconstructed to raw before re-rendering. For our method, the embedded raw data were used directly for re-rendering, with the base denoising model applied. The best results are highlighted in yellow. Method InvISP (source: Style #0) InvISP (source: Style #2) InvISP (source: Style #3) InvISP (source: Style #4) InvISP (source: Style #5) ParamISP (source: Style #0) ParamISP (source: Style #2) ParamISP (source: Style #3) ParamISP (source: Style #4) ParamISP (source: Style #5) Ours (base denoiser) Ours (base denoiser + 3D LuT) S24 Test Set (Target: Style #1) PSNR 21.38 23.27 23.46 23.29 22.85 23.11 22.80 21.84 22.74 17.36 26.68 26.04 SSIM LPIPS 2000 0.810 0.833 0.830 0.824 0.828 0.814 0.814 0.796 0.810 0.754 0.897 0.890 8.676 6.509 6.530 6.609 7.881 7.065 7.135 8.687 7.517 14.479 4.417 4.989 0.211 0.164 0.166 0.165 0.181 0.169 0.166 0.215 0.180 0.374 0.088 0.100 Table 24. Re-rendering results for target picture style #2 on the S24 dataset [16]. The best results are highlighted in yellow. Method InvISP (source: Style #0) InvISP (source: Style #1) InvISP (source: Style #3) InvISP (source: Style #4) InvISP (source: Style #5) ParamISP (source: Style #0) ParamISP (source: Style #1) ParamISP (source: Style #3) ParamISP (source: Style #4) ParamISP (source: Style #5) Ours (base denoiser) Ours (base denoiser + 3D LuT) S24 Test Set (Target: Style #2) PSNR 26.17 26.14 26.23 26.19 25.91 25.67 24.73 24.09 24.94 19.86 29.48 28.74 SSIM LPIPS 2000 0.843 0.853 0.851 0.847 0.841 0.843 0.832 0.827 0.834 0.743 0.923 0.915 5.301 5.260 5.280 5.283 5.611 5.647 6.179 7.455 6.423 11.855 3.605 4. 0.124 0.121 0.129 0.125 0.152 0.131 0.157 0.179 0.148 0.330 0.067 0.082 For the S24 and Zurich Raw-to-sRGB datasets, training was performed using each datasets respective training 42 Table 25. Re-rendering results for target picture style #3 on the S24 dataset [16]. The best results are highlighted in yellow. Method InvISP (source: Style #0) InvISP (source: Style #1) InvISP (source: Style #2) InvISP (source: Style #4) InvISP (source: Style #5) ParamISP (source: Style #0) ParamISP (source: Style #1) ParamISP (source: Style #2) ParamISP (source: Style #4) ParamISP (source: Style #5) Ours (base denoiser) Ours (base denoiser + 3D LuT) S24 Test Set (Target: Style #3) PSNR 23.82 23.67 23.56 23.64 23.39 23.31 22.49 23.15 22.89 18.64 26.89 26.35 SSIM LPIPS 2000 0.851 0.853 0.858 0.852 0.840 0.834 0.821 0.836 0.828 0.768 0.905 0.897 7.316 7.327 7.394 7.413 7.651 7.126 7.781 7.267 7.469 12.829 4.660 5.318 0.146 0.149 0.147 0.151 0.181 0.151 0.179 0.150 0.160 0.303 0.085 0.101 Table 26. Re-rendering results for target picture style #4 on the S24 dataset [16]. The best results are highlighted in yellow. Method InvISP (source: Style #0) InvISP (source: Style #1) InvISP (source: Style #2) InvISP (source: Style #3) InvISP (source: Style #5) ParamISP (source: Style #0) ParamISP (source: Style #1) ParamISP (source: Style #2) ParamISP (source: Style #3) ParamISP (source: Style #5) Ours (base denoiser) Ours (base denoiser + 3D LuT) S24 Test Set (Target: Style #4) PSNR 23.30 23.24 23.27 23.31 22.85 22.90 22.01 22.88 22.09 18.64 26.44 26.21 SSIM LPIPS 2000 0.842 0.843 0.846 0.841 0.828 0.824 0.806 0.825 0.811 0.744 0.897 0.893 7.647 7.686 7.605 7.639 7.881 6.585 7.398 6.622 7.441 12.482 4.428 4. 0.149 0.148 0.146 0.151 0.181 0.141 0.171 0.140 0.167 0.323 0.080 0.087 Table 27. Re-rendering results for target picture style #5 on the S24 dataset [16]. The best results are highlighted in yellow. Method InvISP (source: Style #0) InvISP (source: Style #1) InvISP (source: Style #2) InvISP (source: Style #3) InvISP (source: Style #4) ParamISP (source: Style #0) ParamISP (source: Style #1) ParamISP (source: Style #2) ParamISP (source: Style #3) ParamISP (source: Style #4) Ours (base denoiser) Ours (base denoiser + 3D LuT) S24 Test Set (Target: Style #5) PSNR 24.89 24.91 24.82 24.83 24.78 24.47 24.20 24.50 23.95 24.18 27.75 28. SSIM LPIPS 2000 0.870 0.870 0.872 0.863 0.863 0.841 0.834 0.847 0.833 0.840 0.916 0.921 5.844 5.840 5.902 5.941 5.960 4.963 5.122 5.000 5.088 5.098 3.537 3.332 0.168 0.165 0.162 0.175 0.174 0.160 0.180 0.153 0.170 0.159 0.102 0.094 split. For the MIT-Adobe FiveK dataset, we followed the train/validation/test split provided in [47]; specifically, the split consists of 493 validation images, 989 test images, and the remaining images are used for training. Since the MIT-Adobe FiveK dataset [26] (Sec. K.2.2) contains images captured with multiple DSLR cameras, training and evaluating black-box neural ISP methods can often be unstable. To ensure fairness and stability, we trained and tested other methods on linear sRGB inputs, obtained by converting the raw images using the illuminant and CCM provided in the DNG metadata. For methods that require single-channel mosaiced inputs, we used mosaiced Table 28. Detailed results for Style #1 on the S24 test set [16]. Our method is evaluated with different denoising model capacities (lite, base, and large), with and without the enhancement network and optional 3D LuT. The best results are highlighted in yellow and the second best in green. Method ISPDiffuser [70] PyNet [48] CIE XYZ Net [10] PP (cmKAN) [67] FlexISP [60] Invertible-ISP [85] LAN [69] MicroISP [49] ParamISP [53] LiteISP [91] FourierISP [44] Ours (lite, w/o enhancement) Ours (base, w/o enhancement) Ours (large, w/o enhancement) Ours (lite, w/o enhancement, w/ 3D LuT) Ours (base, w/o enhancement, w/ 3D LuT) Ours (large, w/o enhancement, w/ 3D LuT) Ours (lite, w/ enhancement, w/ 3D LuT) Ours (base, w/ enhancement, w/ 3D LuT) Ours (large, w/ enhancement, w/ 3D LuT) PSNR 25.60 24.36 22.40 20.93 24.86 23.48 22.98 20.30 24.97 26.66 25.19 25.16 25.28 25.31 26.39 26.52 26.56 26.56 26.71 26.75 S24 Test Set (Style #1) SSIM LPIPS 2000 0.910 0.875 0.859 0.875 0.891 0.832 0.812 0.747 0.856 0.915 0.925 0.866 0.873 0.874 0.874 0.880 0.882 0.906 0.914 0.916 5.100 5.759 8.291 8.436 6.150 6.642 6.669 11.298 5.724 4.702 5.432 5.609 5.483 5.463 4.486 4.348 4.329 4.807 4.692 4.668 0.117 0.097 0.176 0.139 0.108 0.157 0.126 0.183 0.123 0.067 0.099 0.124 0.116 0.114 0.086 0.078 0.076 0.092 0.085 0. Table 29. Detailed results for Style #2 on the S24 test set [16]. The best results are highlighted in yellow and the second best in green. Method ISPDiffuser [70] PyNet [48] CIE XYZ Net [10] PP (cmKAN) [67] FlexISP [60] Invertible-ISP [85] LAN [69] MicroISP [49] ParamISP [53] LiteISP [91] FourierISP [44] Ours (lite, w/o enhancement) Ours (base, w/o enhancement) Ours (large, w/o enhancement) Ours (lite, w/o enhancement, w/ 3D LuT) Ours (base, w/o enhancement, w/ 3D LuT) Ours (large, w/o enhancement, w/ 3D LuT) Ours (lite, w/ enhancement, w/ 3D LuT) Ours (base, w/ enhancement, w/ 3D LuT) Ours (large, w/ enhancement, w/ 3D LuT) PSNR 27.30 25.94 24.05 23.04 27.47 26.35 23.74 23.66 27.11 28.33 28.03 28.09 28.19 28.22 29.21 29.29 29.31 28.92 28.99 29.01 S24 Test Set (Style #2) SSIM LPIPS 2000 0.912 0.890 0.872 0.875 0.911 0.861 0.782 0.801 0.869 0.922 0.928 0.890 0.893 0.893 0.898 0.901 0.901 0.920 0.923 0.924 0.116 0.095 0.120 0.131 0.089 0.115 0.112 0.155 0.101 0.064 0.082 0.094 0.090 0.089 0.068 0.065 0.064 0.078 0.075 0. 4.600 5.432 6.634 7.613 4.554 5.229 6.436 8.885 4.947 4.284 4.488 4.589 4.516 4.503 3.641 3.594 3.588 4.153 4.135 4.130 data from the MIT-Adobe FiveK dataset only after applying white balancing and color correction. For FlexISP [60], which employs three-stage framework (denoising, white balance, and Bayer-to-sRGB mapping), we first trained the denoising stage on the S24 dataset using paired noisy raw and denoised ground-truth images, and then fine-tuned it for each picture style in the S24 dataset, following the training scheme described in the original FlexISP paper [60]. Among all methods, ParamISP [53] and ISPDiffuser [70] required special handling during training compared to the default configurations described in their original papers. For ParamISP [53], originally designed for two-stage training (multi-camera pretraining followed by camera-specific finetuning), we found that omitting the pretraining stage yielded 43 Table 30. Detailed results for Style #3 on the S24 test set [16]. The best results are highlighted in yellow and the second best in green. Method ISPDiffuser [70] PyNet [48] CIE XYZ Net [10] PP (cmKAN) [67] FlexISP [60] Invertible-ISP [85] LAN [69] MicroISP [49] ParamISP [53] LiteISP [91] FourierISP [44] Ours (lite, w/o enhancement) Ours (base, w/o enhancement) Ours (large, w/o enhancement) Ours (lite, w/o enhancement, w/ 3D LuT) Ours (base, w/o enhancement, w/ 3D LuT) Ours (large, w/o enhancement, w/ 3D LuT) Ours (lite, w/ enhancement, w/ 3D LuT) Ours (base, w/ enhancement, w/ 3D LuT) Ours (large, w/ enhancement, w/ 3D LuT) PSNR 25.02 24.70 22.00 21.85 25.23 23.84 23.47 21.45 24.77 26.31 25.38 25.66 25.73 25.75 26.69 26.79 26.83 26.78 26.78 26.83 S24 Test Set (Style #3) SSIM LPIPS 2000 0.899 0.882 0.854 0.876 0.901 0.852 0.830 0.792 0.872 0.913 0.919 0.877 0.881 0.882 0.884 0.889 0.890 0.913 0.913 0.914 5.600 5.999 8.906 8.077 5.807 7.347 6.902 10.751 6.417 5.220 5.703 5.930 5.894 5.883 4.727 4.675 4.665 5.209 5.213 5.196 0.125 0.096 0.162 0.130 0.106 0.144 0.118 0.169 0.121 0.073 0.100 0.113 0.110 0.110 0.085 0.081 0.080 0.090 0.090 0. Table 31. Detailed results for Style #4 on the S24 test set [16]. The best results are highlighted in yellow and the second best in green. Method ISPDiffuser [70] PyNet [48] CIE XYZ Net [10] PP (cmKAN) [67] FlexISP [60] Invertible-ISP [85] LAN [69] MicroISP [49] ParamISP [53] LiteISP [91] FourierISP [44] Ours (lite, w/o enhancement) Ours (base, w/o enhancement) Ours (large, w/o enhancement) Ours (lite, w/o enhancement, w/ 3D LuT) Ours (base, w/o enhancement, w/ 3D LuT) Ours (large, w/o enhancement, w/ 3D LuT) Ours (lite, w/ enhancement, w/ 3D LuT) Ours (base, w/ enhancement, w/ 3D LuT) Ours (large, w/ enhancement, w/ 3D LuT) PSNR 25.93 24.34 22.26 20.91 23.96 23.33 22.80 20.34 24.18 25.04 24.74 25.47 25.55 25.58 26.35 26.47 26.51 26.66 26.79 26.84 S24 Test Set (Style #4) SSIM LPIPS 2000 0.904 0.876 0.846 0.852 0.878 0.842 0.790 0.750 0.853 0.894 0.906 0.868 0.872 0.873 0.880 0.884 0.885 0.905 0.910 0.911 5.100 5.745 7.624 8.394 5.965 7.694 6.736 11.499 6.138 5.517 5.591 5.486 5.437 5.424 4.414 4.365 4.355 4.601 4.564 4. 0.083 0.094 0.147 0.140 0.108 0.145 0.116 0.175 0.118 0.082 0.100 0.107 0.104 0.104 0.079 0.076 0.075 0.081 0.077 0.075 better results on the S24 dataset. For ISPDiffuser [70], we observed that training on image patches both reduced training time and improved accuracy. In both cases, we report the best-performing configurations we identified. For the Zurich dataset (Sec. B.2), results are taken from prior publications, except for CIE XYZ Net [10], which we trained and tested ourselves. LAN [69] results are reported from Rawformer [68], while the remaining numbers are taken from [44, 48]. We report quantitative results for all methods using PSNR, SSIM [81], LPIPS [89], and E2000 [72]. For LPIPS, both outputs and ground-truth images are resized to 10241024 prior to computation."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Luxi Zhao for the help in developing the generic denoiser and running the user study, and Raghav Goyal for the help during the early stages of the denoiser experTable 32. Detailed results for Style #5 on the S24 test set [16]. The best results are highlighted in yellow and the second best in green. S24 Test Set (Style #5) SSIM LPIPS 2000 0.929 0.920 0.897 0.880 0.912 0.875 0.878 0.823 0.867 0.935 0.947 0.898 0.906 0.908 0.911 0.921 0.924 0.930 0.938 0.941 3.800 3.982 5.270 7.338 8.448 5.927 4.662 10.500 4.499 3.434 3.561 3.868 3.831 3.820 3.398 3.336 3.323 3.234 3.177 3.160 0.116 0.089 0.133 0.147 0.124 0.163 0.108 0.221 0.134 0.071 0.089 0.112 0.108 0.107 0.096 0.088 0.086 0.090 0.084 0.082 Method ISPDiffuser [70] PyNet [48] CIE XYZ Net [10] PP (cmKAN) [67] FlexISP [60] Invertible-ISP [85] LAN [69] MicroISP [49] ParamISP [53] LiteISP [91] FourierISP [44] Ours (lite, w/o enhancement) Ours (base, w/o enhancement) Ours (large, w/o enhancement) Ours (lite, w/o enhancement, w/ 3D LuT) Ours (base, w/o enhancement, w/ 3D LuT) Ours (large, w/o enhancement, w/ 3D LuT) Ours (lite, w/ enhancement, w/ 3D LuT) Ours (base, w/ enhancement, w/ 3D LuT) Ours (large, w/ enhancement, w/ 3D LuT) PSNR 26.83 26.32 24.67 21.30 24.65 24.90 25.38 22.68 25.43 28.07 27.41 27.08 27.19 27.23 27.93 28.13 28.19 28.73 28.95 29.03 iments."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael Brown. high-quality denoising dataset for smartphone cameras. In CVPR, 2018. 2, 3, 15 [2] Abdullah Abuolaim and Michael Brown. Defocus deblurring using dual-pixel data. In ECCV, 2020. 1 [3] Adobe Community. raw DNG outSamsung expert shows green / over-exposed photo in Lightput https : / / community . adobe . com / room. t5 / lightroom - ecosystem - cloud - based - discussions / - samsung - expert - raw - dng - outputshowsgreenover- exposedphotoinlightroom/mp/14714898, 2024. Accessed: 2025-11-07. 2 [4] Adobe Community. Denoise of HDR merge file results https : / / community . adobe . in color artifacts. com/t5/lightroomclassicdiscussions/pdenoise - of - hdr - merge - file - results - in - color - artifacts / td - / 14915059, 2024. Accessed: 2025-11-07. [5] Adobe Systems Inc. Digital negative (DNG) specification. https://helpx.adobe.com/camera-raw/ digital - negative . html, 20042025. Accessed: 2025-11-05. 21 [6] Mahmoud Afifi and Abdullah Abuolaim. Semi-supervised raw-to-raw mapping. In BMVC, 2021. 1, 15, 16 [7] Mahmoud Afifi and Michael Brown. Sensor-independent illumination estimation for DNN models. In BMVC, 2019. 10, 11 [8] Mahmoud Afifi and Michael Brown. Deep white-balance editing. In CVPR, 2020. [9] Mahmoud Afifi, Brian Price, Scott Cohen, and Michael Brown. When color constancy goes wrong: Correcting improperly white-balanced images. In CVPR, 2019. 5 44 [10] Mahmoud Afifi, Abdelrahman Abdelhamed, Abdullah Abuolaim, Abhijith Punnappurath, and Michael Brown. CIE XYZ Net: Unprocessing images for low-level computer vision tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):46884700, 2021. 2, 6, 7, 26, 27, 36, 43, 44 [11] Mahmoud Afifi, Jonathan Barron, Chloe LeGendre, YunTa Tsai, and Francois Bleibel. Cross-camera convolutional color constancy. In ICCV, 2021. 5, 9, 14, 15, 16, 17, 21 [12] Mahmoud Afifi, Marcus Brubaker, and Michael Brown. HistoGAN: Controlling colors of GAN-generated and real images via color histograms. In CVPR, 2021. 10, 11 [13] Mahmoud Afifi, Konstantinos Derpanis, Bjorn Ommer, and Michael Brown. Learning multi-scale photo exposure correction. In CVPR, 2021. 21 [14] Mahmoud Afifi, Zhenhua Hu, and Liang Liang. Optimizing illuminant estimation in dual-exposure HDR imaging. In ECCV, 2024. 1 [15] Mahmoud Afifi, Ran Zhang, and Michael Brown. RawJPEG adapter: Efficient raw image compression with JPEG. arXiv preprint arXiv:2509.19624, 2025. 6, 25, [16] Mahmoud Afifi, Luxi Zhao, Abhijith Punnappurath, Mohammed Abdelsalam, Ran Zhang, and Michael Brown. Time-aware auto white balance in mobile photography. In ICCV, 2025. 3, 5, 6, 7, 8, 1, 4, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44 [17] Pesala Bandara. Influencers complain about unflattering iPhone 15 selfie camera. https://petapixel.com/ 2023 / 10 / 04 / influencers - complain - about - unflattering - iphone - 15 - selfie - camera/, 2023. Accessed: 2025-11-07. 2 [18] Jonathan Barron. Convolutional color constancy. In ICCV, 2015. 1 [19] Jonathan Barron and Ben Poole. The fast bilateral solver. In ECCV, 2016. 2, 3, 4, 5 [20] Jonathan Barron and Yun-Ta Tsai. Fast Fourier color constancy. In CVPR, 2017. 3 [21] Jarosław Bernacki. Automatic exposure algorithms for digital photography. Multimedia Tools and Applications, 79(19): 1275112776, 2020. 20 [22] Simone Bianco, Arcangelo Bruna, Filippo Naccari, and Raimondo Schettini. Color correction pipeline optimization for digital cameras. Journal of Electronic Imaging, 22(2): 023014023014. 1 [23] Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, and Jonathan Barron. Unprocessing images for learned raw denoising. In CVPR, 2019. [24] Michael Brown. Color processing for digital cameras. Fundamentals and applications of colour engineering, pages 81 98, 2023. 1 [25] Gershon Buchsbaum. spatial processor model for object colour perception. Journal of the Franklin Institute, 310(1): 126, 1980. 5 [26] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo Durand. Learning photographic global tonal adjustment with database of input/output image pairs. In CVPR, 2011. 6, 8, 4, 15, 19, 36, 40, 42 [27] DL Cade. Smartphones are ruining wildfire sky photos with auto white balance. https://petapixel.com/2020/ 09/10/smartphonesareruiningwildfiresky-photos-with-auto-white-balance/, 2020. Accessed: 2025-11-07. 2 [28] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In CVPR, 2018. [29] Jiawen Chen, Andrew Adams, Neal Wadhwa, and Samuel Hasinoff. Bilateral guided upsampling. ACM Transactions on Graphics (TOG), 35(6):18, 2016. 5, 11, 14, 29, 30, 31 [30] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. In ECCV, 2022. 7, 10, 29 [31] Dongliang Cheng, Dilip Prasad, and Michael Brown. Illuminant estimation for color constancy: Why spatialdomain methods work and the role of the color distribution. Journal of the Optical Society of America A, 31(5):1049 1058, 2014. 21 [32] Marcos Conde, Florin Vasluianu, and Radu Timofte. BSRAW: Improving blind raw image super-resolution. In WACV, 2024. 1 [33] Marcos Conde, Zihao Lu, and Radu Timofte. PixTalk: Controlling photorealistic image processing and editing with language. In ICCV, 2025. 2 [34] Mauricio Delbracio, Damien Kelly, Michael Brown, and Peyman Milanfar. Mobile computational photography: tour. Annual review of vision science, 7(1):571604, 2021. 1 [35] Akshay Dudhane, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan, and Ming-Husan Yang. Burst image restoration and enhancement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [36] Egor Ershov, Alexey Savchik, Illya Semenkov, Nikola Banic, Alexander Belokopytov, Daria Senshina, Karlo Koˇsˇcevic, Marko Subaˇsic, and Sven Lonˇcaric. The Cube++ illumination estimation dataset. IEEE access, 8:227511227527, 2020. 21 [37] Alessandro Foi. Clipped noisy images: Heteroskedastic modeling and practical denoising. Signal Processing, 89 (12):26092629, 2009. 14 [38] Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, and Karen Egiazarian. Practical Poissonian-Gaussian noise modeling and fitting for single-image raw-data. IEEE Transactions on Image Processing, 17(10):17371754, 2008. 14 [39] Michael Gharbi, Jiawen Chen, Jonathan Barron, Samuel Hasinoff, and Fredo Durand. Deep bilateral learning for realtime image enhancement. ACM Transactions on Graphics (TOG), 36(4):112, 2017. 1, 35, 36 [40] Clement Godard, Kevin Matzen, and Matt Uyttendaele. Deep burst denoising. In ECCV, 2018. 2 [41] Jeremy Gray. transform iPhone photography. How Apples Next-Gen photographic https : / / styles petapixel . com / 2024 / 10 / 24 / how - apples - nextgenphotographicstyles- transformiphone-photography/, 2024. Accessed: 2025-11-07. 2 Marc Levoy. Burst photography for high dynamic range and low-light imaging on mobile cameras. ACM Transactions on Graphics (ToG), 35(6):112, 2016. 2 [43] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image filtering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(6):13971409, 2012. 19, 30, 31 [44] Xuanhua He, Tao Hu, Guoli Wang, Zejin Wang, Run Wang, Qian Zhang, Keyu Yan, Ziyi Chen, Rui Li, Chengjun Xie, et al. Enhancing raw-to-sRGB with decoupled style structure in Fourier domain. In AAAI, 2024. 1, 6, 7, 36, 43, 44 [45] Qibin Hou, Daquan Zhou, and Jiashi Feng. Coordinate attention for efficient mobile network design. In CVPR, 2021. 7, 32, 33 [46] Yuanming Hu, Baoyuan Wang, and Stephen Lin. FC4: Fully convolutional color constancy with confidence-weighted pooling. In CVPR, 2017. [47] Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, and Stephen Lin. Exposure: white-box photo post-processing framework. ACM Transactions on Graphics (TOG), 37(2): 117, 2018. 42 [48] Andrey Ignatov, Luc Van Gool, and Radu Timofte. Replacing mobile camera ISP with single deep learning model. In CVPRW, 2020. 1, 6, 7, 8, 3, 4, 36, 40, 43, 44 [49] Andrey Ignatov, Anastasia Sycheva, Radu Timofte, Yu Tseng, Yu-Syuan Xu, Po-Hsiang Yu, Cheng-Ming Chiang, Hsien-Kai Kuo, Min-Hung Chen, Chia-Ming Cheng, et al. MicroISP: Processing 32MP photos on mobile devices with deep learning. In ECCV, 2022. 1, 6, 7, 36, 43, 44 [50] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 7 [51] Eric Kee, Adam Pikielny, Kevin Blackburn-Matzen, and Marc Levoy. Removing reflections from raw photos. In CVPR, 2025. 1 [52] Dongyoung Kim, Mahmoud Afifi, Dongyun Kim, Michael Brown, and Seon Joo Kim. CCMNet: Leveraging calibrated color correction matrices for cross-camera color constancy. In ICCV, 2025. 3, [53] Woohyeok Kim, Geonu Kim, Junyong Lee, Seungyong Lee, Seung-Hwan Baek, and Sunghyun Cho. ParamISP: Learned forward and inverse ISPs using camera parameters. In CVPR, 2024. 2, 6, 7, 36, 37, 40, 42, 43, 44 [54] Diederik P. Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 13, 14, 27 [55] Marc Levoy and Florian Kainz. Project Indigo: computational photography camera app. https://research. adobe . com / articles / indigo / indigo . html, 2025. Accessed: 2025-10-15. 7, 8 [56] Leyi Li, Huijie Qiao, Qi Ye, and Qinmin Yang. Metadatabased raw reconstruction via implicit neural functions. In CVPR, 2023. 6 [57] Chih-Hung Liang, Yu-An Chen, Yueh-Cheng Liu, and Winston Hsu. Raw image deblurring. IEEE Transactions on Multimedia, 24:6172, 2020. [42] Samuel Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan Barron, Florian Kainz, Jiawen Chen, and [58] Zhetong Liang, Jianrui Cai, Zisheng Cao, and Lei Zhang. CameraNet: two-stage framework for effective camera 45 ISP learning. IEEE Transactions on Image Processing, 30: 22482262, 2021. 2 [59] Orly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks, Tianfan Xue, Nikhil Karnad, Qiurui He, Jonathan Barron, Dillon Sharlet, Ryan Geiss, et al. Handheld mobile photography in very low light. ACM Transactions on Graphics (TOG), 38 (6):1641, 2019. 1 [60] Shuai Liu, Chaoyu Feng, Xiaotao Wang, Hao Wang, Ran Zhu, Yongqiang Li, and Lei Lei. Deep-FlexISP: threeIn stage framework for night photography rendering. CVPRW, 2022. 2, 6, 7, 43, [61] Xinhao Liu, Masayuki Tanaka, and Masatoshi Okutomi. Practical signal-dependent noise parameter estimation from single noisy image. IEEE Transactions on Image Processing, 23(10):43614371, 2014. 14 [62] Yi-Chen Lo, Chia-Che Chang, Hsuan-Chao Chiu, Yu-Hao Huang, Chia-Ping Chen, Yu-Lin Chang, and Kevin Jou. CLCC: Contrastive learning for color constancy. In CVPR, 2021. 3 [63] Ilya Loshchilov and Frank Hutter. tic gradient descent with warm restarts. arXiv:1608.03983, 2016. 13, 14 SGDR: StochasarXiv preprint [64] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 13 [65] Daniele Menon, Stefano Andriani, and Giancarlo Calvagno. Demosaicing with directional filtering and posteriori decision. IEEE Transactions on Image Processing, 16(1):132 141, 2007. 18, 40 [66] Ben Mildenhall, Jonathan Barron, Jiawen Chen, Dillon Sharlet, Ren Ng, and Robert Carroll. Burst denoising with kernel prediction networks. In CVPR, 2018. 2 [67] Artem Nikonorov, Georgy Perevozchikov, Andrei Korepanov, Nancy Mehta, Mahmoud Afifi, Egor Ershov, and Radu Timofte. Color matching using hypernetwork-based Kolmogorov-Arnold networks. 2025. 7, 43, 44 [68] Georgy Perevozchikov, Nancy Mehta, Mahmoud Afifi, and Radu Timofte. Rawformer: Unpaired raw-to-raw translation for learnable camera ISPs. In ECCV, 2024. 1, 43 [69] Daniel Wirzberger Raimundo, Andrey Ignatov, and Radu Timofte. LAN: Lightweight attention-based network for In CVPRW, raw-to-RGB smartphone image processing. 2022. 1, 6, 7, 36, 43, [70] Yang Ren, Hai Jiang, Menglong Yang, Wei Li, and Shuaicheng Liu. ISPDiffuser: Learning raw-to-sRGB mappings with texture-aware diffusion models and histogramIn AAAI, 2025. 1, 6, 7, 15, 17, guided color consistency. 19, 36, 40, 43, 44 [71] Alan Robertson. The CIE 1976 color-difference formulae. Color Research & Application, 2(1):711, 1977. 12 [72] Gaurav Sharma, Wencheng Wu, and Edul N. Dalal. The CIEDE2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations. Color Research & Application, 30(1):2130, 2005. 7, 36, 38, 42, 43 [73] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 12 [74] Shuangbing Song, Fan Zhong, Tianju Wang, Xueying Qin, and Changhe Tu. Guided linear upsampling. ACM Transactions on Graphics (TOG), 42(4):112, 2023. 30, [75] SaiKiran Tedla, Beixuan Yang, and Michael Brown. ExIn ICCV, amining autoexposure for challenging scenes. 2023. 20 [76] Ethan Tseng, Yuxuan Zhang, Lars Jebe, Xuaner Zhang, Zhihao Xia, Yifei Fan, Felix Heide, and Jiawen Chen. Neural photo-finishing. ACM Transactions on Graphics (TOG), 41 (6):2381, 2022. 2 [77] Yael Vinker, Inbar Huberman-Spiegelglas, and Raanan Fattal. Unpaired learning for high dynamic range image tone mapping. In ICCV, 2021. 1 [78] Tengfei Wang, Jiaxin Xie, Wenxiu Sun, Qiong Yan, and Qifeng Chen. Dual-camera super-resolution with aligned attention modules. In ICCV, 2021. 1 [79] Yuzhi Wang, Haibin Huang, Qin Xu, Jiaming Liu, Yiqun Liu, and Jue Wang. Practical deep raw image denoising on mobile devices. In ECCV, 2020. [80] Yufei Wang, Yi Yu, Wenhan Yang, Lanqing Guo, Lap-Pui Chau, Alex Kot, and Bihan Wen. Raw image reconstruction with learned compact metadata. In CVPR, 2023. 6 [81] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. 7, 12, 36, 38, 42, 43 [82] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. 7 [83] Yicheng Wu, Qiurui He, Tianfan Xue, Rahul Garg, Jiawen Chen, Ashok Veeraraghavan, and Jonathan Barron. How to train neural networks for flare removal. In ICCV, 2021. 1 [84] Gunther Wyszecki and Walter Stanley Stiles. Color science: concepts and methods, quantitative data and formulae. John wiley & sons, 2000. 21 [85] Yazhou Xing, Zian Qian, and Qifeng Chen. Invertible image signal processing. In CVPR, 2021. 1, 6, 7, 36, 37, 42, 43, 44 Iterative methods for solving partial difference equations of elliptic type. Transactions of the American Mathematical Society, 76(1):92111, 1954. 2 [86] David Young. [87] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. CycleISP: Real image restoration via improved data synthesis. In CVPR, 2020. 1 [88] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. 29 [89] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 7, 36, 38, 42, 43 [90] Xuaner Zhang, Qifeng Chen, Ren Ng, and Vladlen Koltun. Zoom to learn, learn to zoom. In CVPR, 2019. 1 [91] Zhilu Zhang, Haolin Wang, Ming Liu, Ruohao Wang, Jiawei Zhang, and Wangmeng Zuo. Learning raw-to-sRGB mappings with inaccurately aligned supervision. In ICCV, 2021. 1, 6, 7, 15, 17, 19, 36, 40, 43, 46 [92] Luxi Zhao, Mahmoud Afifi, and Michael Brown. Learning camera-agnostic white-balance preferences. In ICCVW, 2025. 5, 15, 16, 17,"
        }
    ],
    "affiliations": [
        "AI Center-Toronto, Samsung Electronics"
    ]
}