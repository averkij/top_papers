{
    "paper_title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
    "authors": [
        "Kung-Hsiang Huang",
        "Haoyi Qiu",
        "Yutong Dai",
        "Caiming Xiong",
        "Chien-Sheng Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 6 3 5 0 0 . 0 1 5 2 : r GUI-KV: EFFICIENT GUI AGENTS VIA KV CACHE WITH SPATIO-TEMPORAL AWARENESS Kung-Hsiang Huang1 Haoyi Qiu2 Yutong Dai1 Caiming Xiong1 Chien-Sheng Wu1 1Salesforce AI Research 2University of California, Los Angeles {kh.huang, cxiong, wu.jason}@salesforce.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Graphical user interface (GUI) agents built on visionlanguage models have emerged as promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers.This insight motivates simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames keys onto the current frames key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance."
        },
        {
            "title": "INTRODUCTION",
            "content": "Graphical user interface (GUI) agents, which automate tasks by interacting with graphical user interfaces, have emerged as crucial application of vision-language models (VLMs) (Qin et al., 2025; Wang et al., 2025; Gou et al., 2025). These agents navigate complex digital environments by processing sequences of screenshots and generating actions to accomplish user-specified goals. However, the computational demands of processing high-resolution GUI screenshots with VLMs present significant efficiency challenges. For example, running inference UI-TARS-1.5-7B (Qin et al., 2025) with bfloat16 and flash attention 2 using Huggingface inference on 5-screenshot example for single step from OSWorld-Verified (Xie et al., 2024) can take over 15 seconds on H200 on average. Some work address the inference challenge by merging input visual tokens (Lin et al., 2025) or pruning visual representations at higher layers (Chen et al., 2025). However, such approaches require re-training GUI agents. more desirable approach is key-value (KV) cache, which offers plugand-play solution where previously computed key and value representations from attention layers are stored in memory to avoid redundant computations during autoregressive generation. Nevertheless, caching all KV states can require extensive GPU memory. For instance, UI-TARS-1.5-7B can consume over 80GB GPU memory on single inference when feeding 5 screenshots with maximum steps of 50. This can easily trigger out-of-memory error for most of the consumer GPUs. Work done during internship at Salesforce AI Research. 1 While recent work has introduced various KV cache compression techniques (Xiao et al., 2024; Zhang et al., 2023; Cai et al., 2025; Yang et al., 2024; Li et al., 2024b; Tu et al., 2025), their effectiveness on GUI agent tasks remains unexplored. GUI agent tasks present unique characteristics that distinguish them from typical vision-language understanding. Screenshots contain substantial spatial redundancylarge regions of uniform backgrounds, repeated UI elements, and static components that persist across time steps. Moreover, the sequential nature of GUI interactions introduces temporal redundancy, as consecutive screenshots often share significant visual overlap. These properties suggest that existing KV cache approaches, developed primarily for natural images and documents, may not be optimal for GUI environments. In this work, our first contribution is the first systematic analysis for understanding if current KV cache budget allocation strategies across transformer layers remain effective for GUI agents (2). Results show that existing approaches fail to capture the unique properties of GUI scenarios. Our analysis indicate GUI screenshots exhibit uniformly extreme and flat attention sparsity across layers (> 0.99), whereas natural images can have sparsity reaches as low as 0.88. Figure 1). This causes per-layer budget schedules that normalize sparsity (e.g., PyramidKV (Cai et al., 2025) and VL-Cache (Tu et al., 2025)) to over-amplify tiny differences and misallocate cache. Thus, we posit that it is best to assign uniform KV cache budget to each layer for GUI agents. Furthermore, our second contribution is GUI-KV, KV cache compression method tailored for GUI agents (3). Our approach introduces two key innovations. First, we develop residual stream-based saliency guidance mechanism that augments attention-based scoring with the L2 norm of visual token hidden states. This design is inspired by insights from previous work showing that certain tokens act as information sinks (Darcet et al., 2024). Second, motivated by the fact that screenshots from prior steps contain overlapping information as the current frame, we introduce temporal redundancy scoring mechanism that performs QR decomposition over the current screenshots to determine redundant visual information from prior steps. The two components work synergistically to determine the spatio-temporal saliency of each token. Finally, our third contribution is the comprehensive experiments on two leading GUI agent models, UI-TARS-1.5-7B (Qin et al., 2025) and OpenCUA-7B (Wang et al., 2025), across six GUI agents benchmarks covering visual grounding and end-to-end evaluation in web, desktop, and mobile (4). Our findings show that GUI-KV enables GUI agents to operate with significantly reduced memory, while maintaining high performance across six benchmarks. Empirically, GUI-KV recovers near full-cache accuracy with modest budgets (typically 1020%) and matches or outperforms the best competing compression baselines across tasks, occasionally even exceeding the full-cache accuracy at intermediate budgets. On efficiency, pre-filling overhead is negligible (increase < 0.29%), while decoding compute drops substantiallyfor example, at 40% budget with five screenshots we reduce FLOPs per decoded token by 38.9% and simultaneously improve step accuracy by +4.1%; the savings grow with more screenshots. Ablations further show complementary benefits: spatial saliency is more effective when the number of screenshots is small, temporal redundancy becomes increasingly beneficial as more screenshots are provided, and combining both yields the best performance across settings."
        },
        {
            "title": "2 PRELIMINARY & ANALYSIS",
            "content": "2.1 GUI AGENT PROBLEM DEFINITION GUI agents are designed for GUI navigation tasks, where they sequentially generate actions and interact with the GUI environment to achieve specified goal (Qin et al., 2025; Wang et al., 2025). This interaction can be formulated as sequential decision-making process. More formally, GUI agent task can be modeled as Partially Observable Markov Decision Process (POMDP), defined by tuple (S, A, F, R, O). At each step t, the agent is in state st S, which represents the true state of the GUI environment (e.g., the current desktop environment). The agent receives partial observation ot O, which typically includes screenshot of the GUI. The agent also has access to the natural language instruction or goal G. Based on the observation ot and its history of past observations and actions, the agent performs an action at A, such as clicking or typing text. The environment then transitions to new state st+1 F(st, at), where is the state transition function. The agent receives new observation ot+1 O(st+1) and reward rt = R(st, at). For interactive settings such as OSWorld-Verified, the interaction loop continues until terminal action is generated or maximum number of steps is reached. 2 Figure 1: Attention sparsity for UI-TARS-1.5-7B across layers for screenshots from ScreenSpot-V2. All scenarios exhibit extremely high sparsity (mostly > 0.99) across all layers."
        },
        {
            "title": "2.2 KV CACHE IN VISION-LANGUAGE MODELS",
            "content": "When GUI agents process sequences of observations and generate actions, the underlying VLMs must efficiently handle the multimodal inputs. critical component in this process is the key-value (KV) cache mechanism, which avoids redundant processing. Below, we provide an overview of KV caching. Prefill phase. When VLM encodes multimodal input, the visual encoder processes the screenshot to generate visual tokens, which are then mapped to unified embedding space through projection layer. Concurrently, any language input, such as the task goal, is tokenized and embedded. The language model component of VLM processes these tokens in parallel. During this process, the key and value states are cached in GPU memory to avoid recomputation in subsequent steps. Decoding phase. After the prefill phase, the model generates action tokens auto-regressively. Each step produces new key-value pairs corresponding to the newly generated token, which are appended to the cache. For GUI agents that maintain history containing multiple high-resolution screenshots and long action outputs, the KV cache can grow substantially, creating memory bottlenecks that limit the agents ability to process long interaction sequences. To address this memory bottleneck, researchers have developed KV cache compression techniques that maintain only subset of the full cache while minimizing accuracy loss. These algorithms operate along two main dimensions: (1) budget allocation across layers, and (2) token selection policies. Budget Allocation. Given the transformers layered architecture, early work allocates equal KV cache slots to each layer (Xiao et al., 2024; Zhang et al., 2023), while recent work has shown that different layers have varying sensitivity to cache reduction (Cai et al., 2025; Yang et al., 2024). Token Scoring Policy. For given cache budget γ (0, 1] (i.e., keeping γ of KV states for layer), we need scoring function ψ to rank token importance. In practice, this scoring is typically computed using observation tokens, the last ω input tokens that serve as queries to determine which KV states are most relevant to retain (Li et al., 2024b). Let be the number of total input prompt tokens, with indices = {1, 2, . . . , n}. The scoring function ψ : Rn assigns importance scores to each token based on the attention they receive from these observation tokens. Based on the scoring function ψ and budget γ, we select the indices with top scores: Cψ = {i : {j : ψ(i) > ψ(j)} < γ n} (1) 2.3 HIGH SPATIAL REDUNDANCY IN GUI GUI screenshots exhibit fundamentally different visual characteristics compared to natural images, particularly in terms of spatial redundancy. Unlike natural scenes that contain rich textures and continuous variations, GUI interfaces are dominated by large uniform regions, such as blank spaces, solid-colored backgrounds, and repeated UI elements. This structural difference has profound implications for attention patterns in VLMs and, consequently, for KV cache compression strategies. To investigate whether current KV cache budget allocation strategies remain effective for GUI agents, we analyze attention sparsity patterns across different GUI environments. We measure the sparsity of the attention score matrix in different transformer layers during the prefill and decoding phases, where the attention scores are computed from observation tokens (the most recent decoder tokens) to all cached tokens. First, for each head h, we let Qh Rωdh denote the queries of the last ω input tokens, Kh R(nω)dh be the previous key states, and Ah = softmax( ) Rω(nω) be the subset of attention scores from the last ω tokens to the previous tokens. Then, we apply filter QhKh dh Figure 2: Performance comparison of KV cache methods on ScreenSpotV2. SnapKVs uniform allocation outperforms approaches that allocate varying budgets. with relative threshold to the attention score matrix Ah: ThresholdFilter(Ah, p)ij = (cid:40) Ah ij 0 ij maxj(Ah ij) if Ah otherwise (2) where threshold (0, 1) controls the strength of induced sparsification. Following Tu et al. (2025), we heuristically set = 1%, such that the filtered-out scores have an insignificant impact on the output of the transformer layer. After filtration, we calculate sparsity ς [0, 1] as: ς := (cid:80) i+nωj 1[ThresholdFilter(Ah, p)ij = 0] {Ah ij : + ω j} (3) which represents the percentage of zero entries. This metric captures how concentrated the attention distribution ishigher sparsity indicates that the model focuses on fewer tokens. We compute ς across transformer layers using GUI agent trajectories from ScreenSpotV2 (Wu et al., 2025), which provides diverse interaction scenarios spanning web, mobile, and desktop environments. Figure 1 reveals striking finding: attention sparsity in GUI screenshots consistently averages above 0.99 across all transformer layers, with the lowest values around 0.98. These are significantly higher than natural images, which average 0.92 with minimums around 0.88 (Tu et al., 2025). This extreme sparsity remains remarkably stable across layers, forming an almost flat line when plotted against layer depth. In contrast, natural images exhibit decreasing sparsity in deeper layers, reflecting the models need to integrate increasingly diverse visual features for complex scene understanding. This uniformly high sparsity suggests that existing budget allocation strategies may be suboptimal for GUI agents. Methods like PyramidKV (Cai et al., 2025) and VL-Cache (Tu et al., 2025) allocate cache budgets based on the assumption that attention sparsity varies significantly across layers. However, when sparsity differences are minimal, their normalization procedures can artificially create budget variations that do not reflect actual attention patterns. Specifically, VL-Cache normalizes attention sparsity scores before computing per-layer budgets, which amplifies small numerical differences into substantial budget disparities even when the underlying attention patterns are nearly identical. To validate this hypothesis, we compare three budget allocation strategies on ScreenSpotV2 tasks: (1) Li et al. (2024b), which allocate equal budget across all layers; (2) PyramidKV, which heuristically assigns more budget to shallower layers; and (3) VL-Cache, which computationally determines budgets based on normalized attention sparsity. As shown in Figure 2, SnapKV achieves significantly better performance than both PyramidKV and VL-Cache across various compression ratios. This result confirms that uniform budget allocation is indeed more appropriate for GUI environments, where the extreme spatial redundancy leads to consistently high attention sparsity across all layers. The performance gap widens at higher compression ratios, particularly when retaining only 5% to 20% of the original cache budget, suggesting that misallocated budgets become increasingly detrimental when cache resources are scarce. These findings demonstrate that prior studies on budget allocation strategies, developed primarily for natural images and text, are not transferable to GUI agent tasks. 4 Figure 3: Illustration of our temporal redundancy scoring mechanism. We first perform QR decomposition on the current screenshot at step to obtain subspace spanned by Qh . Then, we project key vectors . The magnitude of the residual ρ (the component orthogonal to the subspace) quantifies the visual tokens non-redundancy. In this example, k3 and k4 are less redundant than k1 and k2 since they have larger residuals orthogonal to Qh . is of visual tokens from previous frames onto Qh"
        },
        {
            "title": "3 GUI-KV",
            "content": "In this section, we introduce our method GUI-KV, which enhances KV cache compression for GUI agents through two novel mechanisms: (1) spatial saliency guidance that leverages the L2 norm of visual token hidden states to better identify important tokens (3.1), and (2) temporal redundancy-aware scoring that exploits the sequential nature of GUI interactions (3.2). 3.1 SPATIAL SALIENCY GUIDANCE GUI agents process screenshots, where only small fraction of the visual tokens are typically relevant to the task at hand. While attention mechanisms are designed to identify these salient regions, they are not always sufficient. Furthermore, studies on vision transformers suggest that not all visual tokens contribute equally to the models understanding. Some tokens act as register-like information sinks, aggregating content from across the visual input, while others are less informative (Darcet et al., 2024). This motivates us to supplement attention-based selection with more direct measure of tokens content strength, allowing us to better identify salient visual information. Let It and Tt denote the set of visual token indices and text token indices in the current step (step t), and let xi Rd be the residual-stream hidden state of token at given layer. We define per-token magnitudes with ri = xi2 and µr with sample mean of σr and standard deviation of r. Saliency score can be computed by applying softmax to the standardized norms: (cid:104) Si = softmax (cid:16) µr (σr + ϵ) τ (cid:17)(cid:105) , (4) where = [ri]iIt , τ > 0 is temperature hyperparameter, and ϵ = 108 for numerical stability. Intuitively, if attention is demand, then Si is the normalized payload. The raw norm ri remains reliable proxy for the strength of the information carried by token because the learned value and output projections are approximately norm-preserving (Xiong et al., 2020). Computing ri on the residual-stream hidden state before self-attention ensures that xi captures the accumulated content up to the given layer. The subsequent standardization and softmax preserve the ordering induced by ri while yielding saliency weights that are comparable across samples. By combining this normalized measure of absolute content strength with the relational signal in the attention scores Aj,i, our method maintains both content magnitude and contextual relevance in token selection. We therefore score the current visual tokens by linear combination of attention and norm: ψh = + αSi (cid:40) Ah Ah if It (visual tokens) if Tt (text tokens) (5) where α > 0 balances the two signals. We prefer an additive form over product Ai Si for numerical stability and for easier calibration of α. B.3 discusses the various Si we experimented."
        },
        {
            "title": "3.2 TEMPORAL REDUNDANCY SCORING",
            "content": "Consecutive GUI screenshots share substantial structure (e.g., static backgrounds, persistent UI elements), while only small subset of regions changes between steps. We capitalize on this temporal redundancy by preferentially retaining tokens from earlier frames whose content is not already represented by the most recent frame. At step t, let It1, . . . , Itk denote the token indices of previous images. For head of dimensionality dh, let Kh RT dh be the key matrix at the selection layer for all tokens in the prompt. Let nt = It denote the number of tokens in the last image, and let Kh be the submatrix of Kh containing the rows for indices in It. We compute It rank-r QR decomposition: (K It ) Qh Rh , Qh Rdhr, (6) where is fixed rank and Qh the last images key space. We define the projector onto this subspace as For any previous-image token Itℓ, let kh redundancy score relative to the last-image subspace as has orthonormal columns spanning low-dimensional subspace of Rdhdh. R1dh denote the i-th row of Kh. We define its Qh := Qh := (cid:13) ρh (cid:13)kh (I )(cid:13) (cid:13)2 , (7) so that tokens whose information lies largely in the span of the last image (small ρh dundant, while large ρh head we form Qh cations per image, Kh Ij ) are deemed rei indicates genuinely new directions worth keeping. Computationally, for each once from the last image and score previous-image tokens with two matrix multiplifor < t. Figure 3 illustrates an overview of our method. (cid:55) (Kh Ij )Qh Qh 3.3 SPATIAL AND TEMPORAL COMBINATION To combine spatial saliency with temporal redundancy, we compute ρh Itk and determine percentile threshold per head. γ, we set the threshold as for all tokens in It1 ρh = Percentile1γ (cid:0){ρh : It1 Itk}(cid:1). (8) We then define the final scoring function that combines both spatial saliency and temporal redundancy: ˆψh = + αSi + αSi) 1[ρh ρh] Ah (Ah Ah if It (current frame visual tokens) if It1 Itk (previous frames visual tokens) if (text tokens) For each head h, we select the top-scoring tokens: ˆψh = {i : {j : ˆψh Ch > ˆψh } < γ}, (9) (10) where γ (0, 1] is the budget applied uniformly across all heads. The two mechanisms work synergistically: spatial saliency guidance identifies important visual tokens based on their attention reception and representational strength, while temporal redundancy scoring filters out previous-frame tokens that duplicate information already present in the current frame. This combination ensures we retain tokens that are both intrinsically important and temporally unique, maximizing the information content of the compressed KV cache. We provide detailed algorithmic description of our method in Algorithm 1 and hyper-parameter selection in C."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Benchmarks. We assess the effectiveness of GUI-KV on six benchmarks. For GUI visual grounding, which evaluates the ability to locate specific UI elements from natural language descriptions, we use ScreenSpotV2 (Wu et al., 2025) and ScreenSpot-Pro (Li et al., 2025). For offline agent evaluation, which tests action prediction on pre-collected interaction trajectories, we employ AndroidControl (Li et al., 2024a), Multimodal-Mind2Web (Deng et al., 2023), and AgentNetBench (Wang et al., 2025). For online agent evaluation, which measures real-time task completion in live environments, we evaluate on OSWorld-Verified (Xie et al., 2024). 6 Table 1: Performance comparison of different KV cache compression methods across GUI agent benchmarks. represents the average absolute improvement of our method (GUI-KV) over the best performing baseline across all budget levels, excluding the 100% (full cache) budget. Dataset Model Method Budget 1% 3% 5% 10% 15% 20% 40% 80% 100% ScreenSpotPro ScreenSpotV2 AndroidControl MultimodalMind2Web AgentNetBench OSWorldVerified UI-TARS-1.5-7B OpenCUA-7B UI-TARS-1.5-7B OpenCUA-7B UI-TARS-1.5-7B OpenCUA-7B UI-TARS-1.5-7B OpenCUA-7B UI-TARS-1.5-7B OpenCUA-7B UI-TARS-1.5-7B OpenCUA-7B SnapKV PyramidKV VL-Cache GUI-KV 7.0 4.6 0.1 6.8 10.1 SnapKV PyramidKV 15.4 1.6 VL-Cache 10.1 GUI-KV SnapKV 16.3 PyramidKV 17.3 1.3 VL-Cache 16.7 GUI-KV 41.9 SnapKV PyramidKV 16.1 1.1 VL-Cache 41.9 GUI-KV 18.5 SnapKV PyramidKV 11.1 2.1 VL-Cache 18.4 GUI-KV SnapKV PyramidKV VL-Cache GUI-KV SnapKV PyramidKV VL-Cache GUI-KV SnapKV PyramidKV VL-Cache GUI-KV SnapKV PyramidKV VL-Cache GUI-KV SnapKV PyramidKV VL-Cache GUI-KV SnapKV PyramidKV VL-Cache GUI-KV SnapKV PyramidKV VL-Cache GUI-KV 3.1 1.9 0.0 3.1 0.5 0.9 0.0 0. 0.5 0.0 0.0 0.4 0.3 0.5 0.5 0.2 1.2 0.2 0.0 1.6 2.2 1.9 0.5 2.5 0.5 0.5 0.5 0.5 23.5 13.9 10.3 24. 16.9 14.2 13.8 16.7 62.6 46.6 28.0 63.6 70.7 34.6 50.3 72.1 26.0 20.7 6.0 25.7 15.1 11.1 0.2 15.9 3.6 2.3 0.1 3. 2.8 0.6 0.0 3.1 0.7 0.4 2.6 1.0 4.5 0.9 0.6 7.6 3.2 2.8 0.0 3.4 0.5 0.5 0.5 0.8 30.0 20.8 15.8 30. 18.2 14.5 15.1 19.4 77.0 57.1 52.1 79.1 73.4 41.1 57.2 72.8 28.9 24.7 8.6 30.1 18.5 15.1 2.5 20.1 5.7 3.7 0.0 6. 5.8 1.7 0.0 4.4 1.6 0.8 1.9 1.8 4.6 1.1 0.7 8.2 6.9 1.9 0.0 8.9 0.5 0.5 0.5 0.6 32.4 26.4 18.7 33. 17.0 11.4 15.2 18.5 83.4 67.6 63.4 83.5 73.4 41.0 60.4 75.1 39.0 28.3 24.5 41.2 21.3 21.3 11.4 23.8 9.8 5.7 0.4 12. 10.0 5.3 0.0 9.4 3.0 1.5 5.7 6.1 7.7 2.8 1.8 12.6 16.1 6.3 0.0 16.1 1.1 0.5 1.0 0.8 35.7 29.5 20.3 36. 15.3 14.5 12.8 17.3 85.3 76.6 70.4 85.7 73.7 41.0 67.8 74.5 41.9 31.4 39.5 44.7 27.6 27.3 12.7 30.2 14.1 9.0 3.3 16. 12.6 6.5 0.1 13.7 8.1 2.4 2.2 11.9 11.2 4.5 0.5 14.1 16.6 9.5 0.0 18.0 1.3 0.8 1.3 1.0 37.8 33.5 23.3 38. 17.1 12.5 16.3 19.3 86.8 82.8 76.9 87.9 63.6 63.0 55.5 67.9 44.5 36.8 42.7 46.0 23.7 27.4 19.9 23.3 16.4 10.1 5.4 18. 14.2 9.5 0.0 14.2 10.6 4.4 2.2 16.3 10.1 6.7 4.5 16.7 18.4 15.0 16.7 20.7 1.3 1.6 1.4 1.6 40.8 38.8 25.3 41. 14.7 13.3 15.2 18.3 88.2 85.8 88.1 88.2 67.3 57.2 65.0 69.5 47.4 44.3 46.1 45.9 28.6 21.8 27.5 30.9 20.5 19.1 19.7 21. 16.6 0.0 13.2 15.6 19.0 10.0 2.9 21.6 11.1 17.9 0.4 18.7 19.3 15.9 15.2 25.1 1.3 1.7 1.3 1.7 42.3 42.3 26.8 42. 27.1 14.5 14.5 28.0 88.5 88.5 87.5 89.3 74.6 74.4 76.2 75.9 46.5 46.5 45.9 46.8 23.9 24.5 3.5 31.7 22.7 22.7 22.8 22. 0.1 0.0 0.3 13.1 19.0 19.0 2.3 22.6 28.3 29.0 0.0 27.6 22.7 20.0 19.8 22.8 16.8 13.4 15.4 17.5 42.6 42.6 42.6 42. 44.6 44.6 44.6 44.6 88.3 88.3 88.3 88.3 92.7 92.7 92.7 92.7 49.9 49.9 49.9 49.9 41.1 41.1 41.1 41.1 23.2 23.2 23.2 23. 34.2 34.2 34.2 34.2 17.5 17.5 17.5 17.5 66.8 66.8 66.8 66.8 26.0 26.0 26.0 26.0 21.4 21.4 21.4 21.4 +0.4 +1.4 +0.7 +1.4 +0.8 +2. +1.2 +1.4 +2.3 +3.6 +1.5 +0. Metrics. We employ different evaluation metrics tailored to each benchmark. For ScreenSpot-V2 and ScreenSpot-Pro, we measure click accuracy, defined as the proportion of test samples where the models predicted coordinate falls within the ground truth bounding box. For OSWorld-Verified, we use success rate, determined by whether series of operations are successfully executed and specific milestones are achieved. For the remaining benchmarks, we adopt step accuracy, which assesses whether single predicted step contains the correct operation (e.g., click, write) and arguments (e.g., coordinate or textual content). We follow the same evaluation settings as UGround (Gou et al., 2025). Models. We evaluate two top-performing GUI agent models, UI-TARS-1.5-7B (Qin et al., 2025) and OpenCUA-7B (Wang et al., 2025). These two models share the same vision encoder but employ distinct language models, position encoding strategies, and training methods. UI-TARS-1.5-7B uses Qwen2.5 as the language model and was trained with supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) (Rafailov et al., 2023), where as OpenCUA-7Bs language model is based on Qwen2 and trained with SFT only. 7 Table 2: Efficiency analysis conducted on AgentNetBench, assessed by MFLOPs per decoded token. # Screenshots KV Cache γ (%) MFLOPs/ decoded token 3 5 10 Full Cache 100 GUI-KV GUI-KV 20 40 Full Cache 100 GUI-KV GUI-KV 20 40 Full Cache 100 GUI-KV GUI-KV 20 40 213.2 123.6 (-42.0%) 145.3 (-31.8%) 290.4 139.5 (-52.0%) 177.4 (-38.9%) 471. 175.3 (-62.8%) 249.9 (-47.0%) Figure 4: Ablation studies on AgentNetBench with various number of screenshots. Baselines. We consider three competitive and representative baselines: (1) SnapKV (Li et al., 2024b) is classic and effective method that assigns fixed budgets across all layers (2) PyramidKV (Cai et al., 2025) assigns pyramid-shaped budgets to each layer based on herustics (3) VL-Cache (Tu et al., 2025) determines the budget for each layer by computing the corresponding attention sparsity. 4.2 ACCURACY EVALUATION Table 1 displays the comparison between GUI-KV and baseline methods on the evaluated six benchmarks. We have the following observations: GUI-KV is the most effective in retaining Full KV cache performance. Across six benchmarks and budgets from 1% to 80%, GUI-KV consistently matches or surpasses the best competing compression schemes, recovering near full-cache accuracy with modest budgets (often 1020% for UI-TARS-1.5-7B). On ScreenSpotPro and ScreenSpotV2, 1020% already closes most of the gap, and by 40% the gap is negligible. On four of six benchmarks, GUI-KV achieves positive average absolute gains over the best baseline at sub-100% budgets, ranging from +0.7 to +3.6, with the largest improvements on online and offline agent evaluation tasks (e.g., +2.2 on AndroidControl and +3.6 on AgentNetBench for OpenCUA-7B). Notably, on multiple datasets such as ScreenSpotV2 and AgentNetBench, GUI-KV slightly exceeds the full-cache accuracy at 40-80% budgets, suggesting that targeted KV pruning can reduce long-context distraction. Compared to baselines, GUI-KV exhibits smoother, near-monotonic gains as the budget increases and avoids the high-budget regressions that several baselines show; at ultra-low budgets (13%), baseline may edge out GUI-KV on isolated cells, but GUI-KV reliably overtakes by 510% and then dominates across mid and high budgets. UI-TARS-1.5-7B is more robust than OpenCUA-7B against evicting KV states. UI-TARS-1.5-7B retains higher fraction of full-cache performance under aggressive compression, reaching near saturation by 1020% budget on visual grounding (e.g., 8389% on ScreenSpotV2) and maintaining strong step accuracy on action prediction (e.g., 41.2% at 10% and 46.0% at 20% on AndroidControl with GUI-KV. In contrast, OpenCUA-7B is more sensitive to KV eviction, showing larger drops and non-monotonic behavior across budgets for multiple methods (e.g., pronounced fluctuations on ScreenSpotPro and Multimodal-Mind2Web). Even with GUI-KV, OpenCUA-7B typically requires more budget to close the gap to full-cache performance and still trails UI-TARS-1.5-7B at comparable budgets on the harder action-prediction tasks (e.g., 23.8% at 10% and 23.3% at 20% on AndroidControl). These trends indicate that UI-TARS-1.5-7Bs architecture and training methods are intrinsically more tolerant to KV state pruning, while OpenCUA-7B benefits disproportionately from GUI-KV but remains less robust overall. 4.3 EFFICIENCY ANALYSIS We quantify efficiency by reporting MFLOPs per token for pre-filling and decoding on UI-TARS1.5-7B evaluated on AgentNetBench. We vary the budget γ as well as the number of screenshots (including current and the most recent previous frames). As shown in Table 2, when γ = 40% in the 5 screenshot settings, we observe 38.9% reduction in MFLOPs per decoded tokens and an absolute increase in step accuracy by 4.1% compared to the full cache baseline (see Table 1). The savings grow with more screenshots, indicating larger gains under image-heavy contexts. Pre-filling overhead is negligible (increase < 0.29% across all settings) as the QR step operates on the (dh nt) 8 block for the last image and costs O(d2 hnt) per head, where dh is the head dimension and nt = It is the number of visual tokens in the current image. This is dominated by the O(n2dh) attention cost per head during pre-filling with total sequence length n, so the added computation is negligible, while decoding compute drops substantially. Overall, GUI-KV achieves significant decoding FLOP reductions with minimal pre-filling overhead."
        },
        {
            "title": "4.4 ABLATION STUDIES",
            "content": "We analyze the contribution of the two components of GUI-KVspatial saliency guidance (3.1) and temporal redundancy scoring (3.2)on AgentNetBench using UI-TARS-1.5-7B with budget of γ = 20% while varying the number of screenshots. As shown in Figure 4, among the variants, the spatial-only guidance is more effective with fewer screenshots ( 7), while the temporal-only guidance excels as more screenshots are added, capitalizing on cross-frame redundancy. Combining both guidance, GUI-KV achieves the best performance, confirming the synergistic nature of the two components. All variants of our method consistently outperform the SnapKV baseline, demonstrating the overall effectiveness of our approach. Notably, with just 20% KV cache budget, in the 7screenshot setting, GUI-KV matches the performance of the full-cache counterpart that , highlighting its efficiency. Performance for all methods peaks at 7 screenshots, suggesting that excessive visual context can introduce distraction and dilute useful cues for GUI agents."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Vision Token Compression. Recent approaches to reducing vision token computational burden fall into two categories: architectural modifications and adaptive token pruning. Chu et al. (2024) employ lightweight projector architectures with average pooling layers to compress visual tokens. Adaptive methods like LLaVA-PruMerge (Shang et al., 2024) and MADTP (Cao et al., 2024) dynamically reduce tokens based on importance scores derived from attention patterns. Chen et al. (2024) combine adaptive attention in early layers with token pruning in later stages. Recently, Chen et al. (2025) extends these ideas to GUI agents through context-aware simplification strategies. KV Cache Compression. Post-training KV cache compression methods fall into four categories. Token-wise eviction strategies like StreamingLLM (Xiao et al., 2024) retain attention sinks and recent tokens for infinite-length generation, while Zhang et al. (2023) and Li et al. (2024b) identify heavy-hitter tokens, though potentially sacrificing context. Token-wise merging approaches preserve more informationZhang et al. (2024) consolidate information from multiple tokens, and Wan et al. (2024a) employ dynamic discriminative operations based on semantic similarity. Static layer-wise reduction methods like Cai et al. (2025) apply uniform compression ratios across layers following pyramidal structure but may overlook varying layer importance. Quantization techniques reduce memory through precision reduction, with Liu et al. (2024) demonstrating effective 2-bit asymmetric quantization and Kang et al. (2024) achieving near-lossless compression through error reduction frameworks. Most methods focus on text-based compression, overlooking multimodal challenges. While Wan et al. (2024b) address multimodal compression, they use fixed allocation strategies that ignore inter-layer attention variations. Recent works by Wan et al. (2025) and Tu et al. (2025) introduce dynamic allocation, advancing toward more adaptive multimodal compression strategies."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper studies KV cache compression for GUI agents and introduces GUI-KV, plug-and-play approach that exploits the spatio-temporal structure of GUI interactions. Our analysis reveals that attention patterns on GUI screenshots are highly sparse and relatively uniform across layers, making heuristic layer-wise schedules designed for natural images suboptimal. Building on these insights, GUI-KV combines residual-stream saliency to better surface semantically important visual tokens with redundancy-aware temporal scoring rule that preserves information not already captured by the most recent frame. Across six benchmarks spanning visual grounding, offline action prediction, and online evaluation, GUI-KV recovers nearfull-cache accuracy at modest budgets and consistently outperforms strong baselines over wide range of compression ratios. On UI-TARS-1.5-7B, 1020% of the cache typically closes most of the performance gap on visual grounding, and competitive step accuracy is maintained on action-prediction tasks. Moreover, decoding compute drops substantially with negligible prefill overhead, enabling deployment of GUI agents under tight memory constraints."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "This work studies inference-time KV cache compression for GUI agents and adheres to the ICLR Code of Ethics. The research does not involve new data collection from human subjects, user studies, or crowdsourcing. All experiments are conducted on publicly available benchmarks for GUI agents and web automation, including ScreenSpotV2 and ScreenSpot-Pro for GUI grounding (Wu et al., 2025; Li et al., 2025), AndroidControl and Multimodal-Mind2Web for offline action prediction (Li et al., 2024a; Deng et al., 2023), AgentNetBench (Wang et al., 2025), and OSWorld-Verified for online evaluation (Xie et al., 2024). We used these datasets strictly under their respective licenses and terms of use and did not modify or augment them with additional data containing personally identifiable information. To our knowledge, the benchmarks do not include sensitive personal data; our study focuses on efficiency of inference-time caching and does not attempt to extract, infer, or reconstruct private information. For model evaluation, we rely on existing GUI agents, UI-TARS-1.5-7B (Qin et al., 2025) and OpenCUA-7B (Wang et al., 2025), and do not introduce additional training or fine-tuning on user data. For the online setting (OSWorld-Verified), we followed the benchmarks official evaluation protocol, confined tasks to benign workflows, and did not attempt to circumvent security controls, access private accounts, or perform actions beyond the scope of the benchmark. Our method improves computational efficiency and memory usage for GUI agents; like other advances in automation, such capabilities could be misused if deployed without adequate safeguards. We encourage responsible use aligned with the Code of Ethics, organizational policies, and applicable laws, and we discourage applications that violate privacy, fairness, or safety norms. We disclose the use of large language models only for polishing the writing of this paper (see Appendix), not for conducting experiments or generating data or labels."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "We aim to make the study reproducible by specifying models, datasets, metrics, and implementation details in the main text and appendix. The proposed method is defined in 3, with algorithmic details provided in (Algorithm 1). Experimental settings, including benchmarks, evaluation metrics, compared baselines, and model backbones (UI-TARS-1.5-7B and OpenCUA-7B), are described in 4.1. Hyperparameters used across experiments (e.g., rank r, temperature τ , and trade-off coefficients) are reported in C. Additional ablations and profiling results, including breakdown of prefill-time overhead and sensitivity to the rank parameter, are included in the appendix tables referenced therein. All datasets we evaluate on are public and cited in the paper. Together, these details are intended to enable independent verification of the reported results. To further facilitate reproducibility, our implementation will be made publicly available as open-source code upon publication."
        },
        {
            "title": "REFERENCES",
            "content": "Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, and Wen Xiao. PyramidKV: Dynamic KV cache compression based on pyramidal information funneling. In Second Conference on Language Modeling, 2025. Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, and Tao Chen. MADTP: multimodal alignment-guided dynamic token pruning for accelerating vision-language transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1571015719. IEEE, 2024. doi: 10.1109/CVPR52733.2024.01487. URL https://doi.org/10.1109/CVPR52733.2024.01487. Gongwei Chen, Xurui Zhou, Rui Shao, Yibo Lyu, Kaiwen Zhou, Shuai Wang, Wentao Li, Yinchuan Li, Zhongang Qi, and Liqiang Nie. Less is more: Empowering GUI agent with context-aware simplification. CoRR, abs/2507.03730, 2025. doi: 10.48550/ARXIV.2507.03730. URL https: //doi.org/10.48550/arXiv.2507.03730. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, 10 September 29-October 4, 2024, Proceedings, Part LXXXI, volume 15139 of Lecture Notes in Computer Science, pp. 1935. Springer, 2024. doi: 10.1007/978-3-031-73004-7_2. URL https: //doi.org/10.1007/978-3-031-73004-7_2. Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, and Chunhua Shen. Mobilevlm V2: faster and stronger baseline for vision language model. CoRR, abs/2402.03766, 2024. doi: 10.48550/ARXIV.2402.03766. URL https://doi.org/10.48550/arXiv.2402.03766. Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=2dnO3LLiJ1. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan In AdSun, and Yu Su. Mind2web: Towards generalist agent vances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December URL http://papers.nips.cc/paper_files/paper/2023/hash/ 10 - 16, 2023, 2023. 5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_Benchmarks.html. the web. for Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum? id=kxnoqaisCT. Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. GEAR: an efficient KV cache compression recipe for near-lossless generative inference of LLM. CoRR, abs/2403.05527, 2024. doi: 10.48550/ARXIV.2403.05527. URL https://doi. org/10.48550/arXiv.2403.05527. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: GUI grounding for professional high-resolution computer use. CoRR, abs/2504.07981, 2025. doi: 10.48550/ARXIV.2504.07981. URL https://doi.org/10. 48550/arXiv.2504.07981. Wei Li, William E. Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on UI control agents. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024a. URL http://papers.nips.cc/paper_files/paper/2024/hash/ a79f3ef3b445fd4659f44648f7ea8ffd-Abstract-Datasets_and_Benchmarks_Track.html. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: LLM knows what you are looking for before generation. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024b. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 28ab418242603e0f7323e54185d19bde-Abstract-Conference.html. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-languageIn IEEE/CVF Conference on Computer Vision and action model for GUI visual agent. Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pp. 19498 19508. Computer Vision Foundation / IEEE, 2025. 10.1109/CVPR52734.2025. 01816. URL https://openaccess.thecvf.com/content/CVPR2025/html/Lin_ShowUI_One_ Vision-Language-Action_Model_for_GUI_Visual_Agent_CVPR_2025_paper.html. doi: Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. KIVI: tuning-free asymmetric 2bit quantization for KV cache. In Forty-first 11 International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=L057s2Rq8O. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering automated gui interaction with native agents. CoRR, abs/2501.02842, 2025. doi: 10.48550/ARXIV.2501.02842. URL https://doi.org/10.48550/arXiv.2501.02842. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=HPuSIXJaa9. Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. CoRR, abs/2403.15388, 2024. doi: 10.48550/ ARXIV.2403.15388. URL https://doi.org/10.48550/arXiv.2403.15388. Irwin Sobel. An isotropic 3x3 image gradient operator. Presented at talk at the Stanford Artificial Intelligence Project, 1968. Sometimes referred to as the Sobel-Feldman operator. Dezhan Tu, Danylo Vashchilenko, Yuzhe Lu, and Panpan Xu. Vl-cache: Sparsity and modality-aware In The Thirteenth KV cache compression for vision-language model inference acceleration. International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=HMrcv7Q4Ub. Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, and Mi Zhang. D2O: dynamic discriminative operations for efficient generative inference of large language models. CoRR, abs/2406.13035, 2024a. doi: 10.48550/ARXIV.2406.13035. URL https://doi.org/10.48550/arXiv.2406.13035. Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and Li Yuan. LOOK-M: look-once optimization in KV cache for efficient multimodal long-context inference. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pp. 40654078. Association for Computational Linguistics, 2024b. doi: 10.18653/V1/2024.FINDINGS-EMNLP.235. URL https://doi.org/10.18653/ v1/2024.findings-emnlp.235. Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, and Mi Zhang. MEDA: dynamic In Proceedings of the KV cache allocation for efficient multimodal long-context inference. 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pp. 24852497. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.NAACL-LONG.125. URL https://doi.org/10. 18653/v1/2025.naacl-long.125. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025. URL https://arxiv.org/abs/2508.09123. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. OS-ATLAS: foundation action model for generalist GUI agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview. net/forum?id=n9PDaFNi8t. 12 Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming In The Twelfth International Conference on Learning language models with attention sinks. Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: BenchIn Admarking multimodal agents for open-ended tasks in real computer environments. vances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December URL http://papers.nips.cc/paper_files/paper/2024/hash/ 10 - 15, 2024, 2024. 4f68d7d30b74f652b4a1a2c3f4de4d81-Abstract-Datasets_and_Benchmarks.html. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 1052410533. PMLR, 2020. URL http://proceedings.mlr.press/v119/xiong20b.html. Dongjie Yang, Xiaodong Han, Yan Gao, Yao Hu, Shilin Zhang, and Hai Zhao. Pyramidinfer: Pyramid KV cache compression for high-throughput LLM inference. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 32583270. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. FINDINGS-ACL.195. URL https://doi.org/10.18653/v1/2024.findings-acl.195. Yuxin Zhang, Yuxuan Du, Gen Luo, Yunshan Zhong, Zhenyu Zhang, Shiwei Liu, and Rongrong Ji. Cam: Cache merging for memory-efficient llms inference. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=LCTmppB165. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html. 13 DETAILS OF GUI-KV We provide detailed algorithmic description of our KV cache compression method in Algorithm 1. The algorithm combines spatial saliency guidance with temporal redundancy scoring to selectively retain the most informative tokens in the KV cache. The process consists of three main steps: (1) computing spatial saliency scores that combine attention weights with hidden state norms, (2) identifying temporally redundant tokens through subspace projection, and (3) combining both signals to select the final set of tokens for retention. else h=1. end for + αSi Text tokens if It then Si xi2 Ah ψh Ah ψh end if kept for each head h. 3.1: Compute Spatial Saliency Scores be the key matrix rows for indices in the current frame It. h=1, hidden states X, per-head attention scores {Ah}H Initialize spatial scores ψh Rn. for each token index do Visual tokens L2 norm of residual-stream hidden state Algorithm 1 GUI-KV: Spatio-Temporal KV Cache Compression for GUI Agents Require: Full context token indices C, partitioned into current visual It, past visual I<t, and text . Require: Per-head key matrices {Kh}H Require: Hyperparameters: saliency weight α, budget ratio γ, QR rank r. 1: Let be the total number of tokens. 2: Initialize sets of kept indices Ch 3: for each head = 1, . . . , do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: end for 32: Let γ 33: ˆψh indices of top-k tokens from based on scores ˆψh. Ch 34: 35: end for 36: return {Ch Let Kh It ) Qh (Kh It (Qh Qh Initialize redundancy scores ρh for tokens in I<t. for each past visual token I<t do i-th row of Kh kh kh ρh )2 end for (cid:0){ρh τ Percentile1γ Rank-r QR decomposition Projector onto current frames key subspace Redundancy threshold Combine Scores and Select Tokens Initialize final scores ˆψh Rn. for each token index do Keep spatial score for current visual and all text tokens 3.2: Compute Temporal Redundancy Scores Return the set of indices to keep for each head Number of tokens to keep based on budget Apply temporal filter to past visual tokens Residual norm (non-redundancy) ˆψh ψh end if : I<t}(cid:1) if I<t then ˆψh ψh (I Rh ) 1[ρh τ h] else ˆψh }H h=1 14 Table 3: Compression breakdown in GFLOPs (averaged across single instance). Component GFLOPs Attention Scoring QR Decomposition Projection Top-K Selection Memory Gathering 4.607 0.013 0.917 0.007 0.057 Table 4: Performance of UI-TARS-1.5-7B with GUI-KV on AgentNetBench under various budgets and rank r. Rank 8 16 32 64 128 Budget 1% 3% 5% 10% 15% 20% 40% 80% 0.2 0.2 0.2 0.2 0.2 0.9 0.5 1.0 0.7 0.9 1.9 1.8 1.8 1.8 1.9 6.3 6.8 6.1 6.4 6.3 9.9 10.9 11.9 11.5 9.9 14.6 14.3 16.3 16.6 14. 19.2 18.4 21.6 19.6 19.2 21.7 19.3 22.6 20.1 21."
        },
        {
            "title": "B FURTHER DISCUSSIONS",
            "content": "B.1 COMPRESSION OVERHEAD We profile the computation introduced by GUI-KV during the prefill phase and observe that it is negligible relative to the attention scoring computation. We measure the overhead using UITARS-1.5-7B on the AgentNetBench benchmark. As summarized in Table 3, attention scoring dominates the cost, while QR decomposition, top-k selection (0.007 GFLOPs; 0.15%), and memory gathering (0.057 GFLOPs; 1.22%) together contribute under 2% overhead. Consequently, the QRbased subspace projection in our proposed method adds vanishingly small fraction to prefill-time computation and does not hinder throughput. B.2 THE IMPACT OF IN TEMPORAL REDUNDANCY SCORING We aim to understand the impact of different values of r. We vary different values of and test UI-TARS-1.5-7B on AgentNetBench. Table 4 show the results. We found that various values of does not yield significantly different results as all settings outperforming the other baselines in Table 1. Among these values, = 32 results in the best overall performance. B.3 EFFECTIVENESS OF OTHER SPATIAL SALIENCY GUIDANCE In the early stage of our study, we experimented with other spatial saliency guidance. Essenatially, we can replace Si discussed in 3.1 with other methods that indicate spatial saliency. This includes: (1) Pixel Histogram Entropy: we transform the current screenshot into gray-scale image. Then, we compute histogram probability for each color bin b, {1, ..., 256}. Si for this method is defined as computing the entropy of each histogram probability for each image patch. (2) Sobel Filter: Sobel Filter (Sobel, 1968) is classic operator for detecting edges in an image. Si is defined as the output of 3 3 Sobel operator. (3) Center-Surround Contrast: For each patch, we compare its average color to the average color of larger surrounding region. large difference implies high saliency. Concretely, we convert the image from RGB to CIELAB, in which the Euclidean distance between two colors approximates the perceived difference to the human eye. The results are summarized in Table 5. We observe that different guidance approaches have their own strengths. For example, Center-Surround Contrast is most effective in the 10-20%, while Pixel Histogram Entropy performs the best at 1% budget. Overall, the best-performing method is the L2 Norm of the residual stream outlined in 3.1. 15 Table 5: Performance of UI-TARS-1.5-7B with GUI-KV on ScreenSpot-V2 using different spatial saliency guidance under various budgets. Spatial Saliency Guidance Pixel Histogram Entropy Sobel Filter Center-Surround Contrast Residual Stream L2 Norm (3.1) Budget 1% 3% 5% 10% 15% 20% 40% 80% 17.3 1.2 17.0 16.7 61.1 10.3 60.1 63. 76.2 21.5 76.7 79.1 83.5 33.7 85.6 83.5 85.5 43.2 86.1 85. 86.8 46.4 88.0 87.9 88.1 80.1 87.7 88.2 88.9 87.8 88.3 89. HYPER-PARAMETERS SELECTION In this section, we illustrate the values of the hyper-parameters used in our experiments. We set the QR rank to = 32 so that the projector in 3.2 spans compact representation of the latest frame. The temperature for normalizing residual-stream norms in Equation (4) is fixed at τ = 3.5, and the saliency-to-attention trade-off in the same scoring rule uses α = 2 (Equation (5)). Finally, we retain ω = 8 observation tokens when forming the queries that drive token scoring, as described in 2."
        },
        {
            "title": "D LARGE LANGUAGE MODELS USAGE STATEMENT",
            "content": "Large language models are only used for polishing the content of this paper."
        }
    ],
    "affiliations": [
        "Salesforce AI Research",
        "University of California, Los Angeles"
    ]
}