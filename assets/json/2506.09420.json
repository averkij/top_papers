{
    "paper_title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy",
    "authors": [
        "Henry Peng Zou",
        "Wei-Chieh Huang",
        "Yaozu Wu",
        "Chunyu Miao",
        "Dongyuan Li",
        "Aiwei Liu",
        "Yue Zhou",
        "Yankai Chen",
        "Weizhi Zhang",
        "Yangning Li",
        "Liancheng Fang",
        "Renhe Jiang",
        "Philip S. Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent improvements in large language models (LLMs) have led many researchers to focus on building fully autonomous AI agents. This position paper questions whether this approach is the right path forward, as these autonomous systems still have problems with reliability, transparency, and understanding the actual requirements of human. We suggest a different approach: LLM-based Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing them. By keeping human involved to provide guidance, answer questions, and maintain control, these systems can be more trustworthy and adaptable. Looking at examples from healthcare, finance, and software development, we show how human-AI teamwork can handle complex tasks better than AI working alone. We also discuss the challenges of building these collaborative systems and offer practical solutions. This paper argues that progress in AI should not be measured by how independent systems become, but by how well they can work with humans. The most promising future for AI is not in systems that take over human roles, but in those that enhance human capabilities through meaningful partnership."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 0 2 4 9 0 . 6 0 5 2 : r Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy Henry Peng Zou1*, Wei-Chieh Huang1*, Yaozu Wu2*, Chunyu Miao1, Dongyuan Li2, Aiwei Liu3, Yue Zhou1, Yankai Chen1, Weizhi Zhang1, Yangning Li3, Liancheng Fang1, Renhe Jiang2, Philip S. Yu1 1University of Illinois Chicago, 2University of Tokyo, 3Tsinghua University {pzou3, whuang80, psyu}@uic.edu, yaozuwu279@gmail.com, lidy@csis.u-tokyo.ac.jp, yankaichen@acm.org"
        },
        {
            "title": "Abstract",
            "content": "Recent improvements in large language models (LLMs) have led many researchers to focus on building fully autonomous AI agents. This position paper questions whether this approach is the right path forward, as these autonomous systems still have problems with reliability, transparency, and understanding the actual requirements of human. We suggest different approach: LLM-based HumanAgent Systems (LLM-HAS), where AI works with humans rather than replacing them. By keeping human involved to provide guidance, answer questions, and maintain control, these systems can be more trustworthy and adaptable. Looking at examples from healthcare, finance, and software development, we show how human-AI teamwork can handle complex tasks better than AI working alone. We also discuss the challenges of building these collaborative systems and offer practical solutions. This paper argues that progress in AI should not be measured by how independent systems become, but by how well they can work with humans. The most promising future for AI is not in systems that take over human roles, but in those that enhance human capabilities through meaningful partnership."
        },
        {
            "title": "Introduction",
            "content": "AI assistants capable of independent operation have long captivated imagination and scientific pursuit, spanning from speculative fiction to early research on autonomous problem-solving systems [17, 32]. Recent advancements in Large Language Models (LLMs) have rekindled this foundational vision with unprecedented success: LLM-based autonomous agents promise to achieve complex goals with the ability to perceive, plan, and act in dynamic environments, with minimal human intervention [83]. This rapid technological progress has naturally led many researchers and practitioners toward an autonomy-first mindset [15]. The prevailing assumption is that more autonomous agents are betterthat reducing human involvement is inherently desirable and that complete independence should be the ultimate goal. Industry leaders and academic institutions have invested heavily in pursuing systems that can operate with minimal human oversight, driven by visions of AI agents that can handle entire workflows from start to finish [28, 37]. However, this rush toward full autonomy, while understandable given recent breakthroughs, may be premature [18, 61, 108]. Our position is that deploying fully autonomous LLM-based agents in complex real-world scenarios at this stage of development poses significant risks and limitations that could undermine both safety and effectiveness. Rather than viewing autonomy as the primary measure of progress, Equal Contribution. Corresponding Authors. Preprint. Figure 1: From Autonomous Agent Systems to Human-Agent Systems. we argue for fundamental shift toward LLM-based Human-Agent Systems (LLM-HAS) [108], where AI agents function as active teammates rather than independent operators. We advocate for the focused development and deployment of collaborative, supportive, ethical, and adaptive AI-human partnerships that enhance human capabilities while maintaining essential human oversight and judgment. This approach does not represent retreat from AIs ambitious goals, but rather redefinition of what constitutes advanced AImeasuring progress not by isolation, but by collaborative intelligence. This paper advocates fundamental shift from pursuing autonomous LLM agents to prioritizing collaborative HAS. We begin by critically examining the current trajectory towards fully autonomous LLM-based agents, identifying limitations in reliability, complexity handling, and ethical concerns (Section 2). Building on these identified shortcomings, we present LLM-based HAS as paradigmatic alternative, establishing foundational principles that demonstrate how human-LLM collaboration directly addresses the core weaknesses of fully autonomous approaches (Section 3). To substantiate our position, we highlight the emerging significance of HAS across multiple domains, showcasing promising results (Section 4). We acknowledge that HAS approaches face their own challenges; Therefore, we list these key limitations and propose concrete research directions to address these issues (Sections 5 & 6). Finally, we present Alternative Views in Section 7."
        },
        {
            "title": "2 Existing Pursuit: Autonomous LLM-based Agents",
            "content": "Definition 2.1 (LLM-based Autonomous Agent). An LLM-based autonomous agent is system that operates independently in open-ended real-world environments by completing tasks through perception-reasoning-action loop without human intervention [83, 94]. Unlike human-in-the-loop systems, LLM-based Autonomous Agent interprets goals, plans actions, invokes tools, and adapts using language-based reasoning and memory all autonomously. To concretize the definition, we highlight real-world deployments that showcase the autonomous capabilities of such agents. (1) In software engineering, GitHub Copilot exemplifies how agents can autonomously generate, test, and refactor code with minimal developer input, significantly accelerating routine development workflows [55]. (2) In customer support, systems like Manus and Genspark perform complex itinerary planning, automate bookings, and resolve service issues without human oversight, demonstrating robust perception-action loops in dynamic environments. Overall, these applications highlight compelling vision: frictionless deployment, continuous 24/7 operation, and easy scalability will fundamentally reshape the possibilities of automation in industries, research, and daily life [73]. https://github.com/features/copilot https://manus.im/ https://www.genspark.ai/ 2 However, current LLM-based autonomous agents face significant challenges in real-world deployment: 1. Reliability, Trust, and Safety. LLMs generate hallucination outputs that appear plausible but are in fact completely fabricated [38]. The prevalence of hallucinations directly undermines trust in fully autonomous agents. If an autonomous system cannot consistently and reliably provide accurate information, its utility in high-risk environments (such as medical diagnosis [69], financial decision-making [101], or critical infrastructure control) will be severely compromised. 2. Handling Complex and Ambiguous Tasks. Agents struggle with tasks requiring deep reasoning, particularly when goals are ambiguous. Human instructions are often underspecified; without commonsense context, agents can misinterpret them and take incorrect actions. This makes them unreliable for complex domains like scientific research, where objectives are frequently open-ended. 3. Regulatory and Legal Challenges. Fully autonomous agents, despite their ability to act, are not formally accountable under existing law [65]. This ambiguity creates large accountability and transparency gap. When they cause harm or make an incorrect decision, it becomes extremely difficult to determine responsibility [36]. Is it the developer, the deployer, or the algorithm itself? As the capabilities of intelligent agents increase, the legal gap between capabilities and obligations grows wider."
        },
        {
            "title": "3 Towards LLM-Based Human-Agent Systems",
            "content": "The persistent challenges in the pursuit of fully autonomous LLM-based agentsspanning safety, ethics, reliability, and complexitynecessitate practical shift in paradigms. Instead of isolating human involvement, the LLM-HAS paradigm takes advantage of human strengths for creating more robust, effective, and trustworthy systems (as shown in Figure 1). 3.1 LLM-based Human-Agent Systems Definition 3.1 (LLM-based Human-Agent Systems). An LLM-based Human-Agent System is collaborative framework where humans and LLM-powered agents interact to accomplish tasks. Unlike fully autonomous agents, these systems maintain humans in the loop to provide critical information and clarifications [44, 62, 108], offer feedback by evaluating outputs and guiding adjustments [30, 24, 52], and assume control in high-stakes or sensitive scenarios [12, 63, 93]. This human involvement in LLM-HAS ensures enhanced performance, reliability, safety, and explicit accountability, particularly where human judgment remains indispensable. 3.2 Advantages of LLM-HAS The advantages and rationale for prioritizing HAS stems directly from its potential to address the critical limitations and risks associated with autonomous agent systems: Improved Trust and Reliability: The interactive nature of HAS allows humans to provide crucial feedback, correct potential LLM hallucinations in real-time [99], verify information, and guide the agent toward more accurate and reliable outputs [89]. This collaborative verification process is essential for building trust, especially where the cost of error is high. Managing Complexity and Ambiguity. Unlike autonomous agents that struggle with unclear instructions, LLM-HAS excels through continuous human clarification [31]. Humans provide essential context, domain expertise, and progressive refinement of ambiguous goalscritical capabilities for complex tasks. When faced with an underspecified objective, the system can request clarification rather than proceeding with potentially incorrect assumptions [60], making LLM-HAS particularly effective for open-ended research or creative endeavors where objectives evolve dynamically [22]. Clearer Lines of Accountability: With human involved in the decision-making process, especially in supervisory or interventional roles, establishing accountability becomes more straightforward. The human operator or supervisor can often be designated the responsible party, simplifying the legal and regulatory landscape compared to situations where an autonomous agent makes critical error [97]."
        },
        {
            "title": "4 Applications of LLM-powered Human-Agent Systems",
            "content": "LLM-HAS are increasingly used in domains that rely heavily on human input, contextual reasoning, and real-time interaction [33, 106]. Their core advantage lies in treating LLMs not as passive language 3 generators but as active partners that can understand user goals, plan actions, and adjust their behavior through ongoing dialogue. This iterative communication helps align agent behavior with human intent, making collaboration more flexible, transparent, and effective than traditional rule-based or end-to-end systems [84, 71, 102]. Specifically, in Embodied AI, LLM-HAS enables agents to follow complex instructions and coordinate physical tasks with humans using natural language [6, 78]. In Software Development, agentic assistants support multi-turn problem-solving, integrating human feedback to refine and adapt code generation [25, 86]. Conversational Systems benefit from agents that proactively ask clarifying questions, plan dialogue, and explain reasoning steps, improving controllability and user trust [72, 66]. In Gaming, LLM-HAS facilitates dynamic cooperation, allowing real-time collaboration with human players with uncertain or evolving objectives [54, 59]. In Finance, collaborative HAS like FinArena demonstrate how pairing LLM agents with experienced investors can enhance market prediction and portfolio performance [95]. In Healthcare, LLM-HAS support both patient-facing services and clinical workflows, facilitating diagnosis, treatment planning, and drug discovery through seamless integration of medical expertise and language-based reasoning [49, 67]. In Autonomous Driving, they enable intent-aware driving assistance, adaptive feedback loops, and shared control models [16, 90]. Across these domains, common methodological pattern emerges: LLM-HAS redefines humanAI interaction as collaborative process based on language, shaped by feedback, and driven by adaptive reasoning. This unified paradigm raises key questions about alignment, transparency, and co-adaptation, opening valuable space for discussion within the AI community. As these systems advance, they bring both significant promise and serious challenges, requiring careful study of their technical design and broader social impact [8, 58]."
        },
        {
            "title": "5 Key Challenges in Human-Agent Systems",
            "content": "While LLM-HAS represents promising direction, its successful implementation requires careful consideration of several inherent challenges throughout its development lifecycle. We discuss these limitations and propose potential solutions and future research directions [108]. [Initial Setup] Mostly Agent-Centered Work. Most current research on LLM-HAS adopts an agentcentered view, where humans primarily evaluate agent outputs and provide corrective feedback [108]. This unidirectional interaction dominates existing paradigms. However, there is compelling opportunity to reshape this dynamic. Enabling agents to actively monitor human performance, detect inefficiencies, and offer timely suggestions would allow their intelligence to be used effectively and reduce human workload [53]. When agents transition into an instructive roleproposing alternative strategies, highlighting potential risks, and reinforcing best practices in real-timeboth human and agent performance improves. We argue that shifting toward more human-centered or equitable LLM-HAS design is essential to fully realize true human-agent teamwork [46]. [Data] Human Flexibility and Variability. Human feedback in LLM-HAS varies significantly in role, timing, and style [27]. Since humans are subjective and influenced by their personalities, different individuals can lead to diverse outcomes when interacting with the same LLM-HAS. This highlights crucial need: first, for thorough investigations or benchmarks into how varied human feedback affects entire systems; and second, for flexible frameworks that can adopt to this diversity [108]. Moreover, humans are often under-evaluated in LLM-HAS, creating an imbalance that may obscure whether performance bottlenecks stem from the agent or the human [26]. Resolving this requires fair interaction protocols and shared evaluation standards. Another challenge lies in the widespread use of LLM-simulated human proxies, which often fail to reflect the variability of real human input, introducing performance gaps that undermine the validity of comparisons [86, 34]. [Model Engineering] Lacking Adaptivity and Continuous Improvement. core challenge in LLM-HAS development is building truly adaptive and continuously improving AI teammates. Previous approaches treat LLMs as fixed, pre-trained tools, thereby missing opportunities for dynamic evolution within collaborative settings [56]. This static view introduces three key challenges. First, most systems fail to adequately leverage human insights. Without advanced ways to incorporate diverse human guidance (e.g., preferences, critiques), LLMs struggle to become genuinely teachable and context-aware [98, 2]. Second, models lack robust capacity for continual learning and knowledge retention in dynamic environments. This prevents them from building long-term expertise and can lead to catastrophic forgetting, severely hindering their growth as collaborators [41, 50]. Third, 4 the absence of real-time optimizationsuch as adaptive prompting and self-correctionhampers efficiency, alignment, and resource use [75, 81]. Effectively addressing these challenges through integrated human feedback, lifelong learning, and dynamic optimization is key to unlocking the full potential of LLM-HAS in human-agent collaboration. [Development] Unresolved Safety Vulnerabilities. LLM-HAS face critical challenges in sustaining safety, robustness, and accountability post-deployment. While performance often takes precedence, crucial aspects like reliability, security, and user privacy in human interaction remain underexplored [68]. This leads to three key issues: First, without robust monitoring and continuous evaluation, systems risk undetected misaligned behaviors, unpredictable failures, or unintended data disclosures in real-time [103]. This lack of vigilance hinders the rapid iteration guided by humans. Second, inadequate alignment maintenance and human oversight make dynamic LLM-HAS vulnerable to unpredictable agent adaptation and behavioral drift, complicating liability for delegated actions without clear attribution [5]. Finally, overlooking long-term adaptation and responsible AI undermines safety and trust. Ensuring reliable human-agent collaboration requires ongoing monitoring, strong oversight, and integrated responsible AI practices [14]. [Evaluation] Inadequate Evaluation Methodologies. Existing evaluation frameworks for LLMHAS are fundamentally flawed. They primarily emphasize agent accuracy and static benchmarks, often entirely ignoring the real burden placed on human collaborators [57]. This oversight means crucial aspects remain unmeasured: First, standard metrics for human workload and efficiency are lacking. Humans invest varying time and effort to feedback, yet this critical cost remains unsystematically quantified. Evaluations must extend beyond mere output accuracy to cover factors like feedback time across all collaboration phases [7]. Second, as human expertise and LLM-based agent capabilities merge, uncertainty and variability grow. Current evaluations fail to capture nuances of interaction quality, the dynamics of trust, transparency, and explainability, or adherence to ethical alignment and safety beyond simple performance. Moreover, overall user experience and cognitive load are rarely assessed holistically. new evaluation approach or set of metrics comprehensively quantifying contributions and costs for both humans and agents across these critical dimensions is essential for truly efficient and responsible collaboration."
        },
        {
            "title": "6 Human-Agent Systems Implementation Guidelines",
            "content": "Implementing an effective LLM-HAS requires systematic framework in which every component is clearly defined and seamlessly integrated. This framework are partitioned five key domains, as shown in Figure 2: (1) Initial setup, where interaction paradigms, human feedback phases and interaction architecture for humanagent collaboration are specified; (2) Human Data, the proper data for LLM-HAS; (3) Model Engineering, an iterative process of fine-tuning, modular design, and continuous optimization that enhances flexibility, adaptability, and alignment with user needs; (4) Post-Deployment Evaluation and Monitoring, involving continuous performance assessment, human-in-the-loop feedback loops, and governance mechanisms to guarantee reliability, ethical compliance, and the long-term co-evolution of the human-agent partnership; and (5) Evaluation in different stages of the LLM-HAS. 6.1 Initial Setup: Architecture for Collaboration The initial setup phase for implementing LLM-HAS is crucial for the whole task and system. It requires careful definition of the environment, clear profiling of human and agent roles and capabilities, the design and architecture of interaction, and strategic configuration of the LLMs core functionalities, including knowledge grounding and tool integration. Environmental Settings. The environmental settings and profiling define where the interaction between the human and the agent occurs and the internal status of both the human and the agent. It involves defining the shared interaction space (physical or virtual) [108] and profiling human users (\"lazy\" vs. \"informative\") [85] alongside agent roles (assistant, specialist, or RACI (Responsible, Accountable, Consulted, Informed)) [77, 74, 70] and capabilities like planning and memory [104]. LLM-based agents possess degree of interpretive flexibility not found in deterministic model. Therefore, establishing clear environmental configurations and thorough profiling is essential to prevent inefficiencies in LLM-HAS that typically arise from ambiguous roles and system design. Figure 2: Implementation guidelines for the Human-Agent Systems. More details in Section 6. Interaction and Communication. Interaction types should be specified at fine-grained level. Rather than broadly characterizing collaboration, human must explicitly distinguish between supervision, delegation, cooperation, and coordination. Additionally, the orchestration strategy (one-by-one, simultaneous) and synchronization mode (synchronous, asynchronous) must be defined to clarify how humans and LLM-based agents interact [108]. Also, the human feedback phase (initial-setup, during task, post task) and granularity (holistic, segment-level) must also be well-defined. In terms of Interaction protocols, they should be grounded in communication theories (e.g., Gricean Maxims) [45] to foster cooperative interactions and situational awareness [13], enabling AI systems to build shared understanding with human collaborators. 6.2 Characterizing Human Data The foundation of well-aligned and effective LLM-HAS lies in the strategic acquisition, processing, and use of diverse high-quality data, especially human-generated data. This is crucial for equipping agents with nuanced understanding, enhancing their collaborative capabilities, and ensuring they align with human preferences and values. Diverse Datasets. Effective HAS implementation uses various human data, including interaction logs, to understand real-world user behavior [47]. It also incorporates both explicit feedback (e.g., corrections, ratings) and implicit feedback (e.g., inferred preferences) from humans to guide agent adaptation [108]. In terms of domain, task-specific corpora, such as those related to cybersecurity [80], help LLMs learn how to ensure security. Similarly, datasets like XtraQA [10] provide decent writing examples for enhancing collaborative academic writing. For multimodal agents, datasets such as LLaVA-RLHF [76] and VLFeedback [51], which align visual and textual reasoning, provide insight into enhancing collaboration with humans. Considering the variety of the dataset, human must specify it clearly. Alignment & Specialization. The increasing specialization of datasets, as demonstrated by BeaverTails [40], which specifically annotates helpfulness and harmlessness, reflects growing recognition that \"alignment\" is multifaceted concept. This complexity requires targeted data strategies that go beyond general preferences. As LLM-HAS becomes more sophisticated, the traditional \"one-sizefits-all\" data approach is proving to be less effective. This trend suggests that more detailed datasets will emerge, and frameworks must be developed to manage them effectively. Cost & Quality. The high cost of human annotation [100] is prompting innovation in hybrid data generation. This approach combines data collected by humans with synthetic data generated by LLMs [4]. While this method can improve the scalability and extendability of data collection, it also risks perpetuating biases present in flawed \"teacher\" LLMs. Therefore, it is crucial to implement 6 robust validation processes and maintain human oversight to prevent \"model inbreeding\" and ensure that the quality of the data aligns with real-world human experiences. 6.3 Model Engineering: Iterative Development for Adaptive Teammates The model engineering in LLM-HAS development is inherently iterative and adaptive, encompassing three core dimensions: (1) seamless integration of human feedback, (2) Lifelong Learning, and (3) dynamic optimization via continuous refinement and self-correction. Integration of Human Feedback. The iterative integration of human feedback through techniques such as reinforcement learning from human feedback (RLHF) [87, 23], reinforcement learning from AI feedback (RLAIF) [48], direct preference optimization (DPO) [39], and critique-guided improvement (CGI) [98, 2] can help LLM-based agents improve their performance. The LLM-HAS implementations shall apply methods to transform static, pre-trained LLMs into adaptive, teachable agents by leveraging human-ranked responses, LLM-based critiques, and natural-language guidance to drive continuous refinement. Lifelong Learning. LLM-based agents must be designed for lifelong learning, continuously adapting in dynamic environments by acquiring and retaining knowledge without catastrophic forgetting [105], supported by memory mechanisms like InfLLM for processing long sequences [92]. This pursuit aims to build agents that accumulate experience and develop form of \"expertise\" over time. Furthermore, teachable agents that learn through human instruction, constructed based on paradigms such as Learning by Teaching [41] and interactive human-in-the-loop frameworks like GradeHITL [50], can use human insights to offer more interpretable feedback. Dynamic Optimization. Model engineering involves employing dynamic refinement techniques to enhance agent performance, robustness, and alignment with human collaborators. Dynamic prompt engineering, involving iterative refinement by multi-stakeholder teams [75], and self-correction mechanisms, where agents identify and rectify their errors (e.g., ToolMaker [88], InSeC [81]). The system must adopt an optimization strategy to significantly reduce the redundant cost of resources and time to benefit the agents iteration. 6.4 Post-Development: Sustaining and Evolving Human-Agent Partnerships After the deployment, LLM-HAS demands continuous vigilance through robust monitoring systems, proactive strategies to maintain alignment and mitigate behavioral drift, clearly defined human oversight mechanisms, and long-term evaluation and updating protocols to ensure sustained performance, safety, and ethical operation. Continuous Monitoring & Evaluation. Post-deployment requires continuous vigilance through robust MLOps and AIOps systems that evaluate outputs in real time for reliability, hallucinations, user satisfaction, and task failures [103, 96], coupled with adaptive long-term evaluation [20, 91] that merges development, testing, and deployment to detect catastrophic or deceptive behaviors and enable rapid human-guided iteration. Alignment Maintenance & Oversight. LLM-HAS are dynamic systems that require continuous governance [79], as agents can adapt unpredictably. Human oversight is crucial, especially in sensitive situations. Addressing liability from delegated use requires principal-agent perspective [29], together with an agent infrastructure for attribution of actions and remediation of harm [5]. The call for Law-Following AIs [64] highlights societal expectations for compliance, necessitating compliance-by-design features. Long-Term Adaptation & Responsible AI. Ensuring sustained performance, safety, and ethical operation requires effective long-term adaptation and responsible AI practices. Maintenance strategies include optimizing task allocation, managing context and memory [35], and potentially using techniques like representation steering for post-deployment modifications [14], all within framework of responsible AI practices [82, 107]. 7 6.5 Evaluation Since LLM-HAS inherently requires human input and real-time adaptation, traditional static benchmark evaluation may be inadequate. We advocate that LLM-HAS should be evaluated in five domains: Task Effectiveness and Efficiency. The primary criterion for evaluating LLM-HAS is its effectiveness and efficiency in downstream applications. This includes factors such as system speed, output accuracy and quality, and resource utilization. Metrics that holistically capture these aspectssuch as pass@k and major@kare increasingly being adopted [9]. Human-Agent Interaction Quality. Interaction is the core process of LLM-HAS, and its quality directly influences the final outcome. This evaluation should encompass key aspects such as naturalness, coherence, seamlessness, and readability. Such criteria have been widely adopted in the evaluation of LLM-based agents in collaborative tasks, including MEGANno+ [43], tAIfa [1], and studies on AI awareness [13]. Trust, Transparency, and Explainability. LLM-HAS should be evaluated across the domains of trust, transparency, and explainability, as these factors directly influence human decision when system outputs are applied to real-world tasks. Key evaluation criteria within this domain include: accuracy and correctness, explainability and transparency, perceived competence, benevolence, integrity, system reliability, and user control. These dimensions have been extensively studied even prior to the rise of LLM-HAS [42]. More recently, research has focused on trust in LLMs in contexts such as LLM-based planning systems [11], question-answering tasks [21], and human-LLM collaboration scenarios [89]. Ethical Alignment and Safety. Given the powerful capabilities of LLM-HAS, it is essential to ensure they are directed toward beneficial and responsible use. Key evaluation aspects of this criteria include robustness, misuse prevention, operational safety, and protection of privacy and security. This domain has gained significant attention since the rise in popularity of LLMs [3]. User Experience and Cognitive Load. User Experience (UX) is particularly important and distinctive evaluation criterion for LLM-HAS. In these systems, UX involves the full spectrum of user perceptions, emotional responses, satisfaction, and overall impressions. [19] identified three primary strategies for evaluating UX in this context: Assessing the direct outputs generated by LLMs, evaluating co-created artifacts produced through human-agent collaboration, and analyzing user subjective experiences during their interaction with the system."
        },
        {
            "title": "7 Alternative Views",
            "content": "7.1 Framework for Evaluating Human-Agent Systems Before addressing specific critiques of LLM-HAS, we introduce an abstract framework to better understand the underlying trade-offs. Definition 7.1 (Utility of Agent Systems). The utility (U ) of an agent system can be represented as: = Ch Ce (1) where represents the value created per successful task, is the success rate (0-1), Ch denotes human costs, and Ce encompasses error costs (including financial, reputational, and societal costs). Definition 7.2 (Optimal Human Involvement). The optimal degree of human involvement is the level that maximizes the systems utility: = arg max (h) (2) This optimal point balances the increased success rate and reduced error costs against the additional human operational costs. 7.2 The Enduring Appeal of Fully Autonomous Agents View: The vision of Fully Autonomous Agents (FAA) remains compelling for several reasons. Proponents often highlight the potential for significant cost reductions through the automation of 8 human labor, substantial increases in speed and operational efficiency for various tasks, the ability to scale operations rapidly without commensurate increases in human resources, and the capacity for continuous 24/7 operation without issues like fatigue or breaks. Response: Within our framework from Definition 7.1, FAA systems typically minimize Ch but may significantly increase Ce due to higher error rates or more severe consequences when errors occur. We acknowledge that as AI capabilities advance, the optimal human involvement defined in Equation 2 will gradually decrease. However, current technological limitations mean that the optimal balance still requires significant human participation. 7.3 Human Involvement as Bottleneck - Low Quality and Unreliability of Human Feedback View: Humans feedback can often be noisy, biased, inconsistent, or even incorrect. Such imperfect feedback could degrade LLM agent performance or introduce new problems. Therefore, it might be preferable for agents to learn from more structured data sources or through self-play mechanisms. Response: It is true that human feedback is not infallible. However, core focus of LLM-HAS research is developing mechanisms to efficiently elicit and integrate high-quality feedback. Even imperfect human input often contains contextual knowledge and domain-specific insights that are currently absent in LLMs. In our conceptual framework from Equation (1), while suboptimal human feedback may temporarily decrease system success rate (S), the alternativean absence of human feedbackcan lead to agents that perpetuate their own biases without correction mechanisms, potentially leading to much higher error costs (Ce). The goal of LLM-HAS is to harness the strengths of human input while mitigating its weaknesses through careful system design, thereby approaching the optimal involvement level defined in Equation (2). 7.4 Human Involvement as Bottleneck - Responsiveness and Delay View: Requiring human intervention or feedback will inevitably slow down the systems response time, making it unsuitable for time-critical applications. The human in the loop can become the human as bottleneck, negating the speed advantages of AI. Response: This concern is valid for certain time-critical applications. However, within our utility model in Definition 7.1, response time is one component of the overall value equation. For many complex cognitive tasks, slight delay to incorporate human input is acceptable if it significantly improves the success rate (S). LLM-HAS designs can optimize when to solicit human help, performing many subtasks autonomously and involving humans only at critical decision points. This strategic integration of human expertise often yields higher total utility (U ) despite some speed trade-offs. As AI capabilities advance, the optimal human involvement will likely decrease, further improving response times while maintaining high success rates. 7.5 The Human Cost Factor in HAS View: Integrating humans into the operational loop (e.g., for providing feedback, oversight, or direct collaboration) is expensive in terms of human time, effort, and the need for specialized training. This can negate the economic benefits that AI automation is expected to deliver. Response: The human operational costs (Ch) must be weighed against the potentially far greater error costs (Ce) associated with failures in fully autonomous systems. In high-stakes domains, error costs can include accidents, reputational damage, legal liabilities, and loss of user trust. Effective LLMHAS design aims to optimize human effort, focusing it where it adds the most value. Furthermore, the value (V ) delivered by an LLM-HAS that successfully tackles complex problem can far outweigh the operational cost of human collaboration. As AI capabilities improve, we expect the optimal degree of human involvement from Definition 7.2 to decrease, reducing Ch while maintaining high success rates. This evolution represents responsible path toward increasingly autonomous systems that optimize the total utility function rather than simply minimizing human costs at the expense of other factors."
        },
        {
            "title": "8 Conclusion",
            "content": "This position paper calls for strategic shift from aggressively pursuing full autonomy to prioritizing LLM-based human-agent systems at this developmental stage. Despite significant advancements inspired by recent breakthroughs in LLM technology, the premature and widespread deployment of fully autonomous agents presents critical risks related to reliability, complexity, and legal issues across diverse application domains. We support this position by building on the concept of the human-agent system and detailing its practical development roadmap, design principles, and key challenges. In addition, we provide detailed implementation guidelines to help the AI research community effectively embrace, evaluate, and collaboratively advance this emerging paradigm."
        },
        {
            "title": "References",
            "content": "[1] Mohammed Almutairi, Charles Chiang, Yuxin Bai, and Diego Gomez-Zara. taifa: Enhancing team effectiveness and cohesion with ai-generated automated feedback. arXiv preprint arXiv:2504.14222, 2025. [2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2023. [3] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. [4] Conrad Borchers, Danielle Thomas, Jionghao Lin, Ralph Abboud, and Kenneth Koedinger. Augmenting human-annotated training data with large language model generation and distillation in open-response assessment. arXiv preprint arXiv:2501.09126, 2025. [5] Alan Chan, Kevin Wei, Sihao Huang, Nitarshan Rajkumar, Elija Perrier, Seth Lazar, Gillian Hadfield, and Markus Anderljung. Infrastructure for ai agents. arXiv preprint arXiv:2501.10114, 2025. [6] Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, et al. Partnr: benchmark for planning and reasoning in embodied multi-agent tasks. arXiv preprint arXiv:2411.00081, 2024. [7] Dian Chen, Han Jun Yoon, Zelin Wan, Nithin Alluru, Sang Won Lee, Richard He, Terrence J. Moore, Frederica F. Nelson, Sunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, and Jin-Hee Cho. Advancing human-machine teaming: Concepts, challenges, and applications, 2025. [8] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-end autonomous driving: Challenges and frontiers. IEEE TPAMI, 46(12):1016410183, 2024. [9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [10] Nuo Chen, Andre Lin HuiKai, Jiaying Wu, Junyi Hou, Zining Zhang, Qian Wang, Xidong Wang, and Bingsheng He. Xtragpt: Llms for human-ai collaboration on controllable academic paper revision. arXiv preprint arXiv:2505.11336, 2025. [11] Shenghui Chen, Yunhao Yang, Kayla Boggess, Seongkook Heo, Lu Feng, and Ufuk Topcu. Evaluating human trust in llm-based planners: preliminary study. arXiv preprint arXiv:2502.20284, 2025. [12] Ying-Jung Chen, Chi-Sheng Chen, and Ahmad Albarqawi. Reinforcing clinical decision support through multi-agent systems and ethical ai governance. arXiv preprint arXiv:2504.03699, 2025. 10 [13] Zhuoyi Cheng, Pei Chen, Wenzheng Song, Hongbo Zhang, Zhuoshu Li, and Lingyun Sun. An exploratory study on how ai awareness impacts human-ai design collaboration. In Proceedings of the 30th International Conference on Intelligent User Interfaces, pages 157172, 2025. [14] Jan Chojnacki. Interpretable risk mitigation in llm agent systems. arXiv preprint arXiv:2505.10670, 2025. [15] Peter Cihon, Merlin Stein, Gagan Bansal, Sam Manning, and Kevin Xu. Measuring ai agent autonomy: Towards scalable approach with code inspection. arXiv preprint arXiv:2502.15212, 2025. [16] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Drive as you speak: Enabling human-like interaction with large language models in autonomous vehicles. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 902909, 2024. [17] Kerstin Dautenhahn. The art of designing socially intelligent agents: Science, fiction, and the human in the loop. Applied artificial intelligence, 12(7-8):573617, 1998. [18] Chengyuan Deng, Yiqun Duan, Xin Jin, Heng Chang, Yijun Tian, Han Liu, Yichen Wang, Kuofeng Gao, Henry Peng Zou, Yiqiao Jin, et al. Deconstructing the ethics of large language models from long-standing issues to new-emerging dilemmas: survey. arXiv preprint arXiv:2406.05392, 2024. [19] Christine Dierk, Jennifer Healey, and Mustafa Doga Dogan. Evaluating llms in experiential context: Insights from survey of recent chi publications. 2025. [20] Laura Dietz, Oleg Zendel, Peter Bailey, Charles Clarke, Ellese Cotterill, Jeff Dalton, Faegheh Hasibi, Mark Sanderson, and Nick Craswell. Llm-evaluation tropes: Perspectives on the validity of llm-evaluations. arXiv preprint arXiv:2504.19076, 2025. [21] Yifan Ding, Matthew Facciani, Ellen Joyce, Amrit Poudel, Sanmitra Bhattacharya, Balaji Veeramani, Sal Aguinaga, and Tim Weninger. Citations and trust in llm generated responses. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2378723795, 2025. [22] Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Cheng Jiayang, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, and Wenpeng Yin. LLMs assist NLP researchers: Critique paper (meta-)reviewing. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 50815099, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [23] Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, and Liang He. survey on the optimization of large language model-based agents. arXiv preprint arXiv:2503.12434, 2025. [24] Subhabrata Dutta, Timo Kaufmann, Goran Glavaš, Ivan Habernal, Kristian Kersting, Frauke Kreuter, Mira Mezini, Iryna Gurevych, Eyke Hüllermeier, and Hinrich Schuetze. Problem solving through human-ai preference-based cooperation. arXiv preprint arXiv:2408.07461, 2024. [25] Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, and Ji-Rong Wen. Large language model-based human-agent collaboration for complex task solving. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 13361357, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [26] Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, and Ji-Rong Wen. Large language model-based human-agent collaboration for complex task solving, 2024. 11 [27] Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, and André F. T. Martins. Bridging the gap: survey on integrating (human) feedback for natural language generation. Transactions of the Association for Computational Linguistics, 11:16431668, 2023. [28] Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. From llm reasoning to autonomous ai agents: comprehensive review. arXiv preprint arXiv:2504.19678, 2025. [29] Garry Gabison and Patrick Xian. Inherent and emergent liability issues in llm-based agentic systems: principal-agent perspective. arXiv preprint arXiv:2504.03255, 2025. [30] Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, and Thomas Malone. taxonomy for human-llm interaction modes: An initial exploration. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 111, 2024. [31] Florian Geissler, Karsten Roscher, and Mario Trapp. Concept-guided llm agents for human-ai safety codesign. In Proceedings of the AAAI Symposium Series, volume 3, pages 100104, 2024. [32] Evana Gizzi, Lakshmi Nair, Sonia Chernova, and Jivko Sinapov. Creative problem solving in artificially intelligent agents: survey and framework. Journal of Artificial Intelligence Research, 75:857911, 2022. [33] Hojae Han, Seung-won Hwang, Rajhans Samdani, and Yuxiong He. Convcodeworld: Benchmarking conversational code generation in reproducible feedback environments. arXiv preprint arXiv:2502.19852, 2025. [34] Hojae Han, Seung won Hwang, Rajhans Samdani, and Yuxiong He. Convcodeworld: Benchmarking conversational code generation in reproducible feedback environments, 2025. [35] Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, and Chaoyang He. Llm multi-agent systems: Challenges and open problems. arXiv preprint arXiv:2402.03578, 2024. [36] Colin Holbrook, Daniel Holman, Joshua Clingo, and Alan Wagner. Overtrust in ai recommendations about whether or not to kill: Evidence from two human-robot interaction studies. Scientific reports, 14(1):19751, 2024. [37] Pengbo Hu and Xiang Ying. Unified mind model: Reimagining autonomous agents in the llm era. arXiv preprint arXiv:2503.03459, 2025. [38] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155, 2025. [39] Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah Smith, Yejin Choi, and Hanna Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. Advances in neural information processing systems, 37:3660236633, 2024. [40] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via human-preference dataset. Advances in Neural Information Processing Systems, 36:2467824704, 2023. [41] Lingxi Jin, Baicheng Lin, Mengze Hong, Kun Zhang, and Hyo-Jeong So. Exploring the impact of an llm-powered teachable agent on learning gains and cognitive load in music education. arXiv preprint arXiv:2504.00636, 2025. [42] Zahra Rezaei Khavas. review on trust in human-robot interaction. arXiv preprint arXiv:2105.10045, 2021. 12 [43] Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajjadur Rahman, and Dan Zhang. Meganno+: human-llm collaborative annotation system. arXiv preprint arXiv:2402.18050, 2024. [44] JiWoo Kim, Minsuk Chang, and JinYeong Bak. Beyond turn-taking: Introducing text-based overlap into human-llm interactions. arXiv preprint arXiv:2501.18103, 2025. [45] Yoonsu Kim, Brandon Chin, Kihoon Son, Seoyoung Kim, and Juho Kim. Applying the gricean maxims to human-llm interaction cycle: Design insights from participatory approach. arXiv preprint arXiv:2503.00858, 2025. [46] Benjamin Klieger, Charis Charitsis, Miroslav Suzara, Sierra Wang, Nick Haber, and John C. Mitchell. Chatcollab: Exploring collaboration between humans and ai agents in software teams, 2024. [47] Yara Kyrychenko, Jon Roozenbeek, Brandon Davidson, Sander van der Linden, and Ramit Debnath. Human preferences for constructive interactions in language model alignment. arXiv preprint arXiv:2503.16480, 2025. [48] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. 2023. [49] Binxu Li, Tiankai Yan, Yuanting Pan, Jie Luo, Ruiyang Ji, Jiayuan Ding, Zhe Xu, Shilong Liu, Haoyu Dong, Zihao Lin, et al. Mmedagent: Learning to use medical tools with multi-modal agent. arXiv preprint arXiv:2407.02483, 2024. [50] Hang Li, Yucheng Chu, Kaiqi Yang, Yasemin Copur-Gencturk, and Jiliang Tang. Llm-based automated grading with human-in-the-loop. arXiv preprint arXiv:2504.05239, 2025. [51] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, and Qi Liu. Vlfeedback: large-scale ai feedback dataset for large vision-language models alignment. arXiv preprint arXiv:2410.09421, 2024. [52] Youquan Li, Miao Zheng, Fan Yang, Guosheng Dong, Bin Cui, Weipeng Chen, Zenan Zhou, and Wentao Zhang. Fb-bench: fine-grained multi-task benchmark for evaluating llms responsiveness to human feedback. arXiv preprint arXiv:2410.09412, 2024. [53] Yancheng Liang, Daphne Chen, Abhishek Gupta, Simon S. Du, and Natasha Jaques. Learning to cooperate with humans using generative agents, 2024. [54] Haokun Liu, Yaonan Zhu, Kenji Kato, Izumi Kondo, Tadayoshi Aoyama, and Yasuhisa Hasegawa. Llm-based human-robot collaboration framework for manipulation tasks. arXiv preprint arXiv:2308.14972, 2023. [55] Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yiling Lou. Large language model-based agents for software engineering: survey. arXiv preprint arXiv:2409.02977, 2024. [56] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460, 2025. [57] Qianou Ma, Dora Zhao, Xinran Zhao, Chenglei Si, Chenyang Yang, Ryan Louie, Ehud Reiter, Diyi Yang, and Tongshuang Wu. Sphere: An evaluation card for human-ai systems. arXiv preprint arXiv:2504.07971, 2025. [58] Yunsheng Ma, Xu Cao, Wenqian Ye, Can Cui, Kai Mei, and Ziran Wang. Learning autonomous driving tasks via human feedbacks with large language models. In Proc. of EMNLP (Findings), pages 49854995, 2024. [59] Nikhil Mehta, Milagro Teruel, Xin Deng, Sergio Figueroa Sanz, Ahmed Awadallah, and Julia Kiseleva. Improving grounded language understanding in collaborative environment by interacting with agents through help feedback. In Yvette Graham and Matthew Purver, editors, Findings of the Association for Computational Linguistics: EACL 2024, pages 13061321, St. Julians, Malta, March 2024. Association for Computational Linguistics. 13 [60] Chunyu Miao, Yibo Wang, Langzhou He, Liancheng Fang, and Philip Yu. Clarigen: Bridging instruction gaps via interactive clarification in code generation. In AAAI 2025 Workshop on Preventing and Detecting LLM Misinformation (PDLM). [61] Margaret Mitchell, Avijit Ghosh, Alexandra Sasha Luccioni, and Giada Pistilli. Fully autonomous ai agents should not be developed. arXiv preprint arXiv:2502.02649, 2025. [62] Riya Naik, Ashwin Srinivasan, Estrid He, and Swati Agarwal. An empirical study of the role of incompleteness and ambiguity in interactions with large language models. arXiv preprint arXiv:2503.17936, 2025. [63] Sriraam Natarajan, Saurabh Mathur, Sahil Sidheekh, Wolfgang Stammer, and Kristian Kersting. Human-in-the-loop or ai-in-the-loop? automate or collaborate? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2859428600, 2025. [64] Cullen OKeefe, Ketan Ramakrishnan, Janna Tay, and Christoph Winter. Law-following ai: Designing ai agents to obey human laws. 2025. [65] Jasmine Chiat Ling Ong, Shelley Yin-Hsi Chang, Wasswa William, Atul Butte, Nigam Shah, Lita Sui Tjien Chew, Nan Liu, Finale Doshi-Velez, Wei Lu, Julian Savulescu, et al. Ethical and regulatory challenges of large language models in medicine. The Lancet Digital Health, 6(6):e428e432, 2024. [66] Bo Pan, Jiaying Lu, Ke Wang, Li Zheng, Zhen Wen, Yingchaojie Feng, Minfeng Zhu, and Wei Chen. Agentcoord: Visually exploring coordination strategy for llm-based multi-agent collaboration. arXiv preprint arXiv:2404.11943, 2024. [67] Dhavalkumar Patel, Ganesh Raut, Satya Narayan Cheetirala, Benjamin Glicksberg, Matthew Levin, Girish Nadkarni, Robert Freeman, Eyal Klang, and Prem Timsina. Ai agents in modern healthcare: From foundation to pioneera comprehensive review and implementation roadmap for impact and integration in clinical settings. 2025. [68] Jiahao Qiu, Yinghui He, Xinzhe Juan, Yiming Wang, Yuhan Liu, Zixin Yao, Yue Wu, Xun Jiang, Ling Yang, and Mengdi Wang. Emoagent: Assessing and safeguarding human-ai interaction for mental health safety. arXiv preprint arXiv:2504.09689, 2025. [69] Jianing Qiu, Kyle Lam, Guohao Li, Amish Acharya, Tien Yin Wong, Ara Darzi, Wu Yuan, and Eric Topol. Llm-based agentic systems in medicine and healthcare. Nature Machine Intelligence, 6(12):14181420, 2024. [70] Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, and Vishvak Murahari. Personagym: Evaluating persona agents and llms. arXiv preprint arXiv:2407.18416, 2024. [71] SeungWon Seo, SeongRae Noh, Junhyeok Lee, SooBin Lim, Won Hee Lee, and HyeongYeop Kang. Reveca: Adaptive planning and trajectory-based validation in cooperative language In Proceedings of the AAAI agents using information relevance and relative proximity. Conference on Artificial Intelligence, volume 39, pages 2329523303, 2025. [72] Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, and Diyi Yang. Collaborative gym: framework for enabling and evaluating human-agent collaboration. arXiv preprint arXiv:2412.15701, 2024. [73] Minjie Shen and Qikai Yang. From mind to machine: The rise of manus ai as fully autonomous digital agent. arXiv preprint arXiv:2505.02024, 2025. [74] Michael Smith, James Erwin, and Sandra Diaferio. Role & responsibility charting (raci). In Project Management Forum (PMForum), volume 5, page 12, 2005. [75] Hari Subramonyam, Divy Thakkar, Andrew Ku, Juergen Dieber, and Anoop Sinha. Prototyping with prompts: Emerging approaches and challenges in generative ai design for collaborative software teams. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 122, 2025. 14 [76] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. In ACL (Findings), 2024. [77] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023. [78] Daniel Tanneberg, Felix Ocker, Stephan Hasler, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Heiko Wersing, Bernhard Sendhoff, and Michael Gienger. To help or not to help: Llmbased attentive support for human-robot group interactions. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 91309137. IEEE, 2024. [79] Elizaveta Tennant, Stephen Hailes, and Mirco Musolesi. Moral alignment for llm agents. arXiv preprint arXiv:2410.01639, 2024. [80] Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, and Merouane Debbah. Cybermetric: benchmark dataset for evaluating large language models knowledge in cybersecurity. arXiv preprint arXiv:2402.07688, 2024. [81] Nishanth Upadhyaya and Raghavendra Sridharamurthy. Internalized self-correction for large language models. arXiv preprint arXiv:2412.16653, 2024. [82] Huandong Wang, Wenjie Fu, Yingzhou Tang, Zhilong Chen, Yuxi Huang, Jinghua Piao, Chen Gao, Fengli Xu, Tao Jiang, and Yong Li. survey on responsible llms: Inherent risk, malicious use, and mitigation strategy. arXiv preprint arXiv:2501.09431, 2025. [83] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [84] Wenxuan Wang, Zizhan Ma, Zheng Wang, Chenghan Wu, Wenting Chen, Xiang Li, and Yixuan Yuan. survey of llm-based agents in medicine: How far are we from baymax? arXiv preprint arXiv:2502.11211, 2025. [85] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023. [86] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback. In The Twelfth International Conference on Learning Representations, 2024. [87] Yuanhao Wang, Qinghua Liu, and Chi Jin. Is rlhf more difficult than standard rl? theoretical perspective. Advances in Neural Information Processing Systems, 36:7600676032, 2023. [88] Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelovic, and Jakob Nikolas Kather. Llm agents making agent tools. arXiv preprint arXiv:2502.11705, 2025. [89] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI 22, New York, NY, USA, 2022. Association for Computing Machinery. [90] Yaozu Wu, Dongyuan Li, Yankai Chen, Renhe Jiang, Henry Peng Zou, Liancheng Fang, Zhen Wang, and Philip Yu. Multi-agent autonomous driving systems with large language models: survey of recent advances. arXiv preprint arXiv:2502.16804, 2025. [91] Boming Xia, Qinghua Lu, Liming Zhu, Zhenchang Xing, Dehai Zhao, and Hao Zhang. An evaluation-driven approach to designing llm agents: Process and architecture. arXiv preprint arXiv:2411.13768, 2024. [92] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 15 [93] Hengjia Xiao and Peng Wang. Llm a*: Human in the loop large language models enabled a* search for robotics. arXiv preprint arXiv:2312.01797, 2023. [94] Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: survey. arXiv preprint arXiv:2402.15116, 2024. [95] Congluo Xu, Zhaobin Liu, and Ziyang Li. Finarena: human-agent collaboration framework for financial market analysis and forecasting. arXiv preprint arXiv:2503.02692, 2025. [96] Rongwu Xu, Xiaojian Li, Shuo Chen, and Wei Xu. Nuclear deployed: Analyzing catastrophic risks in decision-making of autonomous llm agents. arXiv preprint arXiv:2502.11355, 2025. [97] Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph Gonzalez, Bin Cui, and Shuicheng Yan. Supercorrect: Advancing small llm reasoning with thought template distillation and self-correction. arXiv preprint arXiv:2410.09008, 2024. [98] Ruihan Yang, Fanghua Ye, Jian Li, Siyu Yuan, Yikai Zhang, Zhaopeng Tu, Xiaolong Li, and Deqing Yang. The lighthouse of language: Enhancing llm agents via critique-guided improvement. arXiv preprint arXiv:2503.16024, 2025. [99] Xinyi Yang, Runzhe Zhan, Derek F. Wong, Junchao Wu, and Lidia S. Chao. Human-in-the-loop machine translation with large language model, 2023. [100] Tao Yu, Yi-Fan Zhang, Chaoyou Fu, Junkang Wu, Jinda Lu, Kun Wang, Xingyu Lu, Yunhang Shen, Guibin Zhang, Dingjie Song, et al. Aligning multimodal llm with human preference: survey. arXiv preprint arXiv:2503.14504, 2025. [101] Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng Cao, Zhi Chen, Jordan Suchow, Zhenyu Cui, Rong Liu, et al. Fincon: synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. Advances in Neural Information Processing Systems, 37:137010137045, 2024. [102] Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, et al. Leveraging dual process theory in language agent framework for real-time simultaneous human-ai collaboration. arXiv preprint arXiv:2502.11882, 2025. [103] Shuning Zhang, Jingruo Chen, Jiajing Gao, Zhiqi Gao, Xin Yi, and Hewu Li. Characterizing unintended consequences in human-gui agent collaboration for web browsing. arXiv preprint arXiv:2505.09875, 2025. [104] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024. [105] Junhao Zheng, Chengming Shi, Xidi Cai, Qiuke Li, Duzhen Zhang, Chenxing Li, Dong Yu, and Qianli Ma. Lifelong learning of large language model based agents: roadmap. arXiv preprint arXiv:2501.07278, 2025. [106] Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks, 2025. [107] Yue Zhou, Henry Zou, Barbara Di Eugenio, and Yang Zhang. Large language models are involuntary truth-tellers: Exploiting fallacy failure for jailbreak attacks. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1329313304, 2024. [108] Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, et al. survey on large language model based human-agent systems. arXiv preprint arXiv:2505.00753, 2025."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "University of Illinois Chicago",
        "University of Tokyo"
    ]
}