{
    "paper_title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning",
    "authors": [
        "Long Xing",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Jianze Liang",
        "Qidong Huang",
        "Jiaqi Wang",
        "Feng Wu",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \"good\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL."
        },
        {
            "title": "Start",
            "content": "CAPRL: STIMULATING DENSE IMAGE CAPTION CAPABILITIES VIA REINFORCEMENT LEARNING Long Xing1,2, Xiaoyi Dong2,3, Yuhang Zang2, Yuhang Cao2, Jianze Liang2, Qidong Huang5, Jiaqi Wang2,4, Feng Wu1, Dahua Lin2,3. 1University of Science and Technology of China, 2Shanghai AI Laboratory, 3The Chinese University of Hong Kong, 4Shanghai Innovation Institute, 5Alibaba Cloud xing_long@mail.ustc.edu.cn Model & Data: Code:"
        },
        {
            "title": "CapRL Github Repository",
            "content": "5 2 0 2 6 2 ] . [ 1 7 4 6 2 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Image captioning is fundamental task that bridges the visual and linguistic domains, playing critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes good caption. We introduce Captioning Reinforcement Learning (CapRL), novel training framework that redefines caption quality through its utility: high-quality caption should enable non-visual language model to accurately answer questions about the corresponding image. CapRL employs decoupled two-stage pipeline where an LVLM generates caption, and the objective reward is derived from the accuracy of separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Results validate that our CapRL effectively trains models to produce more general and accurate image descriptions, moving beyond the limitations of traditional SFT-based image captioning models. Code is available here: https://github.com/InternLM/CapRL."
        },
        {
            "title": "INTRODUCTION",
            "content": "The image captioning task (Karpathy & Fei-Fei, 2015; Vinyals et al., 2015), which generates natural language description for given image, bridges the gap between the visual and linguistic worlds. The captioning capability is fundamental to various applications, including vision-language models like CLIP (Radford et al., 2021), which learn shared embedding space for images and text. Furthermore, captions are often core component in the pre-training stage of Large Vision-Language Models (LVLMs) (Liu et al., 2023b), where the model learns to align visual information with linguistic descriptions on massive scale before being fine-tuned for other downstream tasks. Given the importance of image captioning, there is strong need for captioning models that can provide dense and accurate descriptions. Most modern captioning models Chen et al. (2024b); Rotstein et al. (2024); Vasu et al. (2025) are trained based on LVLMs using Supervised Fine-Tuning (SFT). While effective, SFT requires large datasets annotated by humans or proprietary models, which are expensive and not scalable. Furthermore, image captioning is an inherently open-ended"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: (a) Existing Reward Models: Current LVLM-as-a-judge/reward models suffer from limitations like rewarding verbosity or brevity, leading to low-quality captions and reward hacking. (b) Our CapRL: CapRL uses decoupled two-stage VQA approach to provide subjective rewards for captions. (c) CapRLs Advantage: CapRL outperforms previous subjective reward methods, as shown by training curves and higher performance in the Prism (Qiao et al., 2024) evaluation setting. problem, where single image can be accurately described by wide variety of captions. Since SFT models are trained to match single ground-truth description for each image, they tend to memorize specific answers rather than learning the underlying concepts. As result, the SFT models become less general and struggle to generate the diverse range of valid captions possible for single image. The limitations of SFT have led to recent paradigm shift in the post-training of LVLMs toward Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024). RLVR is the paradigm that trains models by providing clear and objective reward from the verifier, such as binary signal of correctness for mathematical reasoning (e.g., DeepSeek-R1 (Guo et al., 2025)). Unlike SFT, which teaches model to mimic single ground-truth response, RLVR encourages the model to generate more diverse and robust outputs that meet the verifiable criteria. Our objective is to design powerful and scalable RLVR training paradigm for the image captioning task to generate more creative and more general variety of accurate descriptions. However, applying RLVR to open-ended tasks like image captioning is challenging, primarily due to the difficulty of designing an objective reward function. good caption can be subjective, with multiple valid descriptions possible for the same image. As shown in Fig. 1 (a), early studies fail to provide accurate reward signals for RL training. Using reward models (Liu et al., 2025b; Su et al., 2025; Lu, 2025) or LLM-as-a-judge (Gunjal et al., 2025) to provide feedback is vulnerable to reward hacking. The captioning model learns to exploit weaknesses in the reward models (e.g., verbosity or brevity outputs) rather than producing high-quality response. Moreover, it is difficult to create effective rubrics or evaluation prompts for LVLM-as-a-judge methods because captions are free-form and encode substantial information. Using reference answer as rewards (Gurung & Lapata, 2025; Yu et al., 2025) like ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) is constrained when evaluating complex and long-form captions. Fig. 1 (c) further demonstrates the limitations of previous subjective caption rewards, showing reward hacking and unstable training curves. To design the objective RLVR reward function for the subjective image captioning task, we introduce novel perspective, where captions quality is proportional to its utility. When the image caption"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Illustration of the captioning capability improvement CapRL brings to Qwen2.5-VL-3B. is detailed and accurate, text-based LLM that cant directly see the image can still answer Visual Question Answering (VQA) questions about the image. For example, for the question What color is the frisbee?, the LLM finds the phrase red frisbee in the caption and correctly answers red. Driven by this motivation, we present an effective decoupled two-stage pipeline, dubbed as Captioning Reinforcement Learning (CapRL), as shown in Fig. 1 (b). Specifically, the reward of our CapRL framework is determined by how well caption generated by an LVLM enables separate non-visual LLM to answer Multiple-Choice Questions (MCQs) about the source image. The LLMs resulting accuracy serves as the objective reward for the RLVR training. To ensure the high-quality MCQs data that present enough knowledge required for VQA has been examined, we also developed specific QA curation pipeline. The images are sampled from various sources, including natural images, charts, and documents. The questions and answers are filtered to ensure the questions can only be answered by analyzing the image content itself. We conduct comprehensive evaluation of the significant benefits brought by CapRL. From qualitative perspective, as shown in Fig. 2, applying the CapRL framework to Qwen2.5-VL-3B makes its outputs more well-organized and accurate. Further illustrative cases for various charts, infographics, or natural images can be found in Section A. From quantitative perspective: (i) We employ CapRL-3B to annotate the CapRL-5M caption dataset, and LVLM pretraining on this dataset yields substantial improvements across 12 benchmarks. (ii) Furthermore, using the Prism Framework (Qiao et al., 2024) for caption quality evaluation, we observed that CapRL-3B remarkably achieves performance comparable to the 72B model, and outperforms the baseline by an average margin of 8.4%. These results demonstrate that our CapRL framework, by leveraging objective reward design as reliable optimization signal, effectively drives the model to produce dense and accurate captions. Our contributions are summarized as follows: 1) We contribute the first study of applying Reinforcement Learning with Verifiable Rewards for the open-ended and subjective image captioning task. Unlike traditional Supervised Fine-Tuning, which can lead to models memorizing limited set of annotated captions, our method allows the model to explore and generate broader range of creative and general descriptions. 2) We present CapRL, new training paradigm featuring decoupled two-stage pipeline. The initial stage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates caption quality by using vision-free LLM to perform the QA task. We also created specific QA curation pipeline to ensure the quality of the questions and answers used for the second stage. 3) We carry out extensive experiments to verify the effectiveness of CapRL. Notably, both in the LVLM Pretraining setting for modality alignment and the Prism setting for caption informativeness evaluation, CapRL consistently exhibits superior performance compared to the baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Image Captioning. Early Large-scale imagetext corpora (Schuhmann et al., 2022; Changpinyo et al., 2021; Thomee et al., 2016) have driven visionlanguage pretraining. To scale and improve"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Overview of CapRL. Unlike the standard single-stage RLVR, our CapRL performs decoupled two-stage process. Captions generated by the LVLM, paired with curated MCQs (b), are used to query an LLM, whose resulting accuracy becomes the objective reward for the LVLM (a). Our CapRL offers scalable framework for applying RLVR to the open-ended image captioning task. captions, researchers design advanced captioning pipelines: BLIP-LAION (Li et al., 2022) generates short synthetic captions, LaCLIP (Fan et al., 2023) uses ChatGPT to rewrite them, and CapsFusion (Yu et al., 2024) consolidates and refine information with fine-tuned models. Besides, there are many research projects which use GPT-4V and human-in-the-loop pipelines to produce richer, fine-grained annotations such as ShareGPT4V (Chen et al., 2024b) and ALLaVA (Chen et al., 2024a). Recent studies (Li et al., 2024b; Sun et al., 2024) have explored multi-expert approaches to compensate for LVLM limitations. In summary, some works rely on complex pipelines with multiple models, training-free but costly at inference, while others require lots of expensive labeled data for SFT. In contrast, our CapRL achieves strong performance with remarkable data efficiency through RLVR. Reinforcement Learning with Verifiable Rewards (RLVR). RLVR (Lambert et al., 2024) represents promising paradigm for training Large Language Models (LLMs) to tasks that have an objective, easily verifiable reward signal. For example, in mathematical problem-solving, the reward can be binary signal of correctness (Shao et al., 2024), and for code generation, it can be whether the code passes unit tests (Team et al., 2025). Compared to the traditional Supervised Fine-Tuning (SFT), RLVR offers more robust and scalable approach. While SFT trains models to imitate set of provided ground-truth answers, often leading to models that memorize specific phrasings (Chu et al., 2025), RLVR encourages the model to explore and discover optimal solutions. This is particularly beneficial for problems with multiple valid answers or reasoning paths."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "An overview of our CapRL is shown in Fig. 3. The CapRL framework consists of novel, decoupled two-stage process. In the first stage, an LVLM generates caption for an input image. In the second stage, this caption, along with series of MCQs, is provided as input to an LLM. In the following, we will describe how to apply RLVR on the image captioning task via our CapRL in Section 3.1. Then we use the model trained with CapRL to construct the CapRL-5M dataset in Section 3.2."
        },
        {
            "title": "3.1 CAPRL",
            "content": "The design of the reward function is pivotal factor in the success of RLVR-based approaches, since the reward function directly guides the optimization direction of the policy model. Although designing reward functions for objective tasks (Shao et al., 2024; Liu et al., 2025c; Luo et al., 2025) is straightforward, developing the reward function for the subjective image captioning task is challenging. While reward models (Liu et al., 2025b; Su et al., 2025; Lu, 2025) or the LLM-as-ajudge approach (Gunjal et al., 2025) have been explored for RL training on open-ended tasks, these"
        },
        {
            "title": "Preprint",
            "content": "models are still vulnerable to exploitation in captioning task, primarily owing to their intrinsic biases, which may unintentionally encourage the captioning model to produce verbose or brief results. To design reliable verifiable reward module, we leverage perception-reasoning decoupled VQA task as proxy to evaluate the quality of captions. The overall process of our proposed method CapRL, is illustrated in Fig. 3. During the GRPO training process, an image and an instruction are first provided as input to the policy model to sample set of candidate captions. Each caption is then paired with corresponding questions and fed to Large Language Model (LLM). We assign each caption reward score based on the accuracy of answers generated by the LLM. Subsequently, we calculate the mean and variance of rewards across the group to derive the advantage for each caption. To ensure training stability, and consistent with the original GRPO framework, we incorporate KL-divergence penalty. The policy model is then updated via policy gradient optimization. To prepare the data for GRPO training, we constructed VQA dataset composed exclusively of multiple-choice questions. This multiple-choice format facilitates the computation of verifiable rewards. Throughout this curation process, we utilized an LVLM to filter the data and prevent data leakage. Further details regarding our reward design and QA curation are provided below. Reward Design. Specifically, given an instruction and an image, the policy model MV generates set of captions {c1, c2, . . . , cG}. Each caption is then paired with questions related to the image and passed to large language model (LLM), denoted as ML, for answering. Since the ML does not have access to the image directly, its ability to answer the question correctly depends entirely on how comprehensive and accurate the caption is. Captions that include more relevant objects and detailed descriptions, are more likely to provide the necessary information for the LLM to answer question correctly. In contrast, less informative captions are more likely to lead to incorrect answers. Since LLMs exhibit high stability in answering multiple-choice questions, and the evaluation of their responses only requires exact matching, the accuracy of the LLMs responses can therefore serve as reliable indicator of caption quality. This question-answering process can be formulated as: am = ML(ci, qm), (1) where qm denotes the m-th question associated with current image I, and am is the LLMs answer to that question. Then the reward for single question is computed using simple exact-match criterion: r(am) = (cid:26)1, 0, if am = GTm, otherwise. (2) Here, GTm is the ground-truth answer to question qm. To eliminate potential bias in the LLMs preference for specific answer choices, we randomly shuffle the options each time question is presented. Additionally, relying on single answer to evaluate caption lacks robustness. To ensure the stability of caption scoring, we sample times from all the questions related to the image and let ML answer them independently. The final reward for caption is computed as the average accuracy over these sampled questions. Formally: Rci ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) k=1 (cid:16) ML (cid:0)ci, Shuffle(qmk )(cid:1)(cid:17) , mk {1, . . . , }. (3) Here, denotes the number of questions associated with the current image I. Since we compute the caption reward directly from the original caption, there is no need to perform intermediate reasoning steps as in DeepSeek-R1, which first carries out thinking process before formatting an answer. As result, our method avoids the need for any format-specific rewards and retains clean, flexible reward computation process that fully respects the free-form nature of the policy models output. It is important to note that, in our GRPO training setup, Qwen2.5-3B-Instruct is used as ML by default, which makes the overall training highly efficient. QA Curation. To train CapRL effectively, high-quality VQA dataset (q, a) with question and answer is required to provide reliable reward signals. We construct this VQA dataset using structured three-stage curation pipeline. (1) Image Collection. We begin by sourcing diverse images from the web and existing open-source datasets, including natural scenes, charts, and documents, to maximize variety. (2) QA Generation. For each image, we then use Qwen2.5-VL-72B (Bai et al., 2025) to automatically generate multiple question-answer pairs. (3) QA Filtering. Finally, we implement stringent QA filtering process to ensure the quality of the generated QA pairs. The QA filtering stage is to verify that all questions are strictly visually-grounded and answerable exclusively"
        },
        {
            "title": "Preprint",
            "content": "through analysis of the image content. The final QA filtering stage is crucial to prevent information leakage and guarantees that the model must perform true visual understanding, rather than relying on external knowledge or cues within the question itself to answer the generated questions. Specifically, the filtered set of QA pairs, denoted as Q, is then defined as: = {(q, a) MVf (q, I) = MVf (q) = a}, (4) where (q, a) is question-answer pair from the initial generated dataset D, is the corresponding input image, MVf is the LVLM used in QA Filtering, MVf (q, I) represents the answer generated when conditioned on both the question and the image I, and MVf (q) is the answer generated when the image is omitted. According to Eq. (4), the QA filtering step ensures that each selected QA pair requires the image context to be answered correctly. To manage computational costs effectively, the QA filtering step is performed using the Qwen2.5-VL-3B model (Bai et al., 2025) as MVf . After filtering, we retain approximately 75k images along with their corresponding QA pairs to train the CapRL captioning model. Please refer to Appendix and for the curation details."
        },
        {
            "title": "3.2 CAPRL-5M DATASET",
            "content": "By employing our carefully designed CapRL training scheme, we obtained CapRL-3B, and further used this powerful captioner to annotate 5M images, ultimately forming CapRL-5M. Image Collection and Processing. In collecting images, we primarily considered diversity, quality, and safety. Among the currently high-quality open-source image datasets, ShareGPT4V-1M (Chen et al., 2024b) and DenseFusion-1M (Li et al., 2024b) are relatively large in scale. Since both datasets have already undergone extensive filtering and clustering to ensure image quality, we directly incorporated all images from them. To further enhance dataset diversity, we also gathered large number of images from the web, spanning natural photographs, documents, charts, and user interfaces. However, the quality of web images is highly uneven, and they pose potential safety risks, which could severely impact both model training and deployment safety. To address this, we applied rigorous filtering and ultimately retained 3M high-quality images. Combined with the two open-source datasets, this yielded total of 5M images. The detailed filtering process is described in Appendix D. Caption Model selection. In typical multimodal pretraining scenarios, the pretraining dataset often requires massive number of image-text pairs, making annotation costs substantial. Considering practical applications, we decide to train highly lightweight yet powerful captioner to keep annotation costs more acceptable. Specifically, we initialize the policy model with Qwen2.5-VL-3B and employ our CapRL framework, resulting in the powerful CapRL-3B model as the captioner."
        },
        {
            "title": "4.1 PRETRAINING SETTING",
            "content": "To thoroughly evaluate the quality of the CapRL-5M dataset, we conduct comprehensive comparisons with widely used caption datasets from the open-source community. Implementation Details. In our setup, the language model is initialized with pretrained LLM, the visual encoder with pretrained ViT, and the MLP projector randomly, following standard multimodal pretraining scheme. We conduct experiments under three settings: Qwen2.5-3B + Qwen2.5-ViT, Qwen2.5-7B + Qwen2.5-ViT, and InternLM2.5-7B + CLIP-ViT-L. Training follows the ShareGPT4V paradigm in three stages: Initial Alignment with BLIP-558K dataset (Li et al., 2022); Further Pretraining with diverse high-quality image-caption datasets; and SFT with Open-LLaVANeXT-1M (Chen & Xing, 2024). For comparison, we adopt strong baselines including Vanilla, which skips Further Pretraining, ShareGPT4V-1M, DenseFusion-1M, and CapRL-1M (randomly sampled from CapRL-5M). Detailed training details are provided in Appendix F. Main Results. As shown in Table 1, when using CapRL-1M as the further pretraining dataset, performance on the vast majority of benchmarks surpasses both ShareGPT4V-1M and DenseFusion1M. Specifically, under the Qwen2.5-3B + Qwen2.5-ViT setting, it exceeds DenseFusion-1M by 6.8% on InfoVQA, and outperforms by 2.7% and 3.6% on DocVQA and ChartVQA. These remarkable"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparison using different pretraining datasets. CapRL-1M significantly outperforms other datasets across all 3 settings, and further improvements are observed when scaling the data to 5M. The best results are bold and the second-best results are underlined. Pretraining Dataset Info VQA Doc VQA Chart QA Real WorldQA Math Vista SEED2 Plus MME RW MMB MMStar MMVet AI2D GQA Average Vanilla 43.9 ShareGPT4V-1M 46.1 DenseFusion-1M 49.4 56.2 CapRL-1M 61.5 CapRL-5M Vanilla 47.6 ShareGPT4V-1M 49.8 DenseFusion-1M 53.5 59.9 CapRL-1M 63.4 CapRL-5M Vanilla 37.4 ShareGPT4V-1M 38.9 DenseFusion-1M 39.3 43.3 CapRL-1M 47.0 CapRL-5M 81.0 82.4 84.6 87.3 90.0 83.7 85.1 87.8 89.5 91.4 73.2 73.8 76.4 80.0 83.5 72.7 74.2 74.4 78.0 80.5 77.1 75.7 76.7 80.6 81. 68.7 69.8 70.8 75.8 77.7 55.1 55.0 54.1 55.1 57.6 55.9 56.8 58.6 58.9 61.4 56.9 56.3 59.7 58.0 59.7 Qwen2.5-3B + Qwen2.5-ViT 56.6 60.5 59.1 62.0 63.2 41.6 44.7 44.6 45.5 48. 30.5 29.8 30.7 30.3 30.9 Qwen2.5-7B + Qwen2.5-ViT 60.4 60.9 61.0 63.1 63.2 47.4 46.6 46.3 50.4 50.8 29.4 31.8 31.1 32.2 34.9 68.6 68.9 69.0 70.5 73.1 72.1 71.9 72.6 72.1 72. InternLM2.5-7B + CLIP-ViT-L 58.2 59.9 60.3 62.8 63.5 44.2 44.8 44.5 49.6 50.4 30.7 33.2 34.1 34.1 38.9 70.7 72.6 72.2 73.4 73.7 44.7 45.2 45.6 47.0 50.4 48.1 48.4 48.6 51.3 52. 47.0 46.2 47.9 50.2 53.3 41.0 42.4 40.2 50.0 52.6 47.1 45.9 49.7 50.5 52.6 43.1 43.3 44.0 46.6 54.3 68.3 70.1 70.4 72.9 74.7 72.4 72.2 72.5 75.3 76. 71.8 72.7 73.7 76.0 77.6 61.5 61.4 62.5 61.6 62.6 62.7 62.7 63.1 63.2 63.8 64.9 65.0 65.5 65.8 66.3 55.5 56.7 57.1 59.7 62.0 58.7 59.0 60.2 62.2 63. 55.6 56.4 57.4 59.6 62.2 Table 2: Ablation on image sources. We annotate the images in ShareGPT4V-1M and DenseFusion1M using CapRL-3B, and use them respectively as pretraining datasets for comparison. Pretraining Dataset Info VQA Doc VQA Chart QA Real WorldQA Math Vista SEED2 Plus MME RW MMB MMStar MMVet AI2D GQA Average Vanilla 43.9 81.0 72.7 Qwen2.5-3B + Qwen2.5-ViT 30.5 55.1 41.6 56. ShareGPT4V-1M 46.1 CapRL-ShareGPT4V-1M 52.1 DenseFusion-1M 49.4 CapRL-DenseFusion-1M 55.0 82.4 85.9 84.6 87.8 74.2 75.2 74.4 77. 55.0 56.3 54.1 56.2 44.7 45.6 44.6 44.7 60.5 60.0 59.1 62. 29.8 30.9 30.7 32.0 68.6 68.9 70.9 69.0 71.0 44. 45.2 46.7 45.6 46.6 41.0 42.4 47.5 40.2 49.9 68. 61.5 70.1 71.4 70.4 72.7 61.4 61.7 62.5 62.3 55. 56.7 58.7 57.1 59.9 results indicate that CapRL-3B is effective for domains such as documents, charts, and infographics, which demand fine-grained perception and structured description. The captions in CapRL-1M are highly detailed and accurate for such image types, enabling LVLMs to achieve better modality alignment and deeper understanding of the corresponding visual features. In addition, on natural image benchmarks such as MMStar and MMBench, CapRL-1M surpasses ShareGPT4V-1M by 1.6% and 1.8%, suggesting that training with CapRL-1M enables multimodal models to acquire richer world knowledge for interpreting objects and their attributes in natural images. CapRL-5M further demonstrates consistently superior performance across all 12 benchmarks. These results highlight the strong scaling properties of the CapRL-3B-annotated dataset: as the training data size expands from 1M to 5M, model performance continues to improve steadily. This phenomenon underscores the practical value of CapRL for multimodal pretraining, as it enables the construction of high-quality, scalable datasets at very low annotation cost. Ablations about Image Sources. In the previous comparisons, the images used in each dataset are not identical. To better control for this variable, we fix the set of images and instead compare the effect of caption quality of different datasets under the Qwen2.5-3B + Qwen2.5-ViT setting. As shown in Table 2, we compare CapRL with ShareGPT4V-1M and DenseFusion-1M. The results demonstrate that, when using the same set of images, further pretraining with the CapRL-3B-annotated dataset enables the LVLM to outperform the baselines by more than 2%. This finding indicates that the substantial advantage of the CapRL dataset over the baselines largely stems from the superior quality of its captions, rather than from differences in image diversity. Scaling Trend Comparison of Different Datasets. We further compare the scaling trend of CapRL and DenseFusion under Qwen2.5-3B + Qwen2.5-ViT setting. Specifically, we sample different numbers of image-caption pairs from each dataset for pretraining. As shown in Figure 4, the CapRL dataset consistently outperforms the corresponding DenseFusion dataset across various scales of pretraining data. Moreover, the overall trend indicates that this performance gap continues to widen as the data size increases. This phenomenon highlights the strong scaling properties of the CapRL dataset, thanks to its high-quality captions, LVLMs continue to benefit as the dataset size grows."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The scaling performance comparison between CapRL-1M and DenseFusion-1M. We use different amounts of pretraining data from the two datasets to observe the scaling trend. Table 3: Captioning ability comparison in Prism Framework. CapRL-3B achieves comparable performance to Qwen2.5-VL-72B, and significantly surpasses existing strategies that use LVLM-asa-Judge as the reward. The best results are bold and the second-best results are underlined. Caption Model GRPO Trained Chart QA ChartQA Pro Info VQA MMMU Pro Math Verse Char Xiv We Math Math Vision MMStar SEED MMMU Average Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-72B UnifiedRW-as-Judge-3B Qwen2.5VL-as-Judge-3B CapRL-3B 65.6 74.9 80.2 54.9 71.4 80.5 27.1 35.4 38.0 25.1 34.2 39.9 40.2 56.4 60.8 33.6 49.3 64. 28.6 30.1 34.1 28.1 29.1 30.7 32.8 36.4 39.9 34.6 33.8 36.4 21.8 54.4 24.8 57.0 30.7 60.2 20.4 58.2 22.9 54.3 32.4 60.1 22.6 23.3 24.5 24.5 24.1 23.4 46.4 50.7 55.0 45.4 47.7 55.0 64.1 67.1 69.3 61.2 64.5 70. 35.1 37.9 39.4 36.3 36.4 38.1 39.9 44.9 48.3 38.4 42.5 48.3 Figure 5: (Left) CapRL demonstrates strong generalization even when trained on images from single domain. CapRL-DocChart-20k refers to training conducted solely on document or chart images, while CapRL-Natural-20k is trained exclusively on natural images. Both models achieve significant improvements over the baseline on out-of-domain benchmarks, highlighting strong generalization capability. (Right) CapRL demonstrates promising scaling performance on QA training datasets."
        },
        {
            "title": "4.2 PRISM SETTING",
            "content": "In the previous section, we demonstrated from the pretraining perspective that captions generated by CapRL are highly beneficial for modality alignment. In this section, we directly evaluate the informativeness of the captions produced by CapRL-3B through the lens of the Decoupled VQA in Prism Framework (Qiao et al., 2024), and compare our CapRL-3B against other captioning models. Implementation details. Similar to our caption reward design, the Prism Framework decouples VQA into two stages. In Stage 1, the captioner generates captions about the input image. In Stage 2, an LLM answers questions based solely on the generated caption. We leverage the Prism framework primarily because it can evaluate caption quality in an objective and stable manner. In our setup, we fix Stage 2 with fine-tuned Qwen2.5-3B-Instruct as the answering LLM, ensuring that benchmark performance directly reflects the quality of captions produced by the captioner. To assess the effect of different reward designs in GRPO, we include two other baseline models: one trained with UnifiedReward-2.0-qwen-3b (Wang et al., 2025), and the other with Qwen2.5-VL-3B as the judge for caption quality evaluation. The corresponding prompts are provided in Appendix C. Comparison with Qwen2.5-VL series. As shown in Table 3, CapRL-3B significantly outperforms both the 3B and 7B models of the Qwen2.5-VL series, achieving performance comparable to that of the 72B model. In chart and infographic understanding, CapRL-3B surpasses Qwen2.5-VL-3B by 14.9%, 12.8%, and 24.6% on ChartQA, ChartQAPro, and InfoVQA, respectively. For natural image understanding, it leads Qwen2.5-VL-3B by 9.6% and 6.5% on MMStar and SEED. These results demonstrate that GRPO training has substantially unlocked the potential of Qwen2.5-VL-3B, enabling it to fully leverage its inherent knowledge to organize all objects and their attributes within an image into comprehensive and detailed captions. As result, its perception capability is pushed to the limit, reaching level comparable to that of the 72B model."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Analysis of the number of QA per image. Table 5: Ablations about Sampling Rounds N. Caption Model ChartQA Pro Info VQA MMMU MMStar WeMath Avg Sampling Rounds ChartQA Pro Info VQA MMMU MMStar WeMath Avg Qwen2.5-VL-3B CapRL-1QA-20k CapRL-2QA-20k CapRL-3QA-20k 27.1 35.5 36.8 36. 40.2 59.8 60.2 60.3 35.1 36.6 37.6 36.9 46.4 50.8 51.1 51.3 54.4 57.3 56.6 56.8 40.6 48.0 48.5 48.5 N=1 N=2 N=4 N= 35.4 36.2 36.7 36.9 58.1 59.1 59.9 59.6 36.5 36.3 37.1 36.5 50.2 49.3 50.9 50.8 56.1 56.9 57.3 57.7 47.3 47.6 48.4 48. Comparison with LVLM-as-a-Judge reward. In our comparison with other reward design methods, we observe that when using UnifiedReward-2.0-qwen-3b as the judge to evaluate caption quality, the models captioning ability actually deteriorates during GRPO training. We attribute this to the severe bias present in UnifiedReward-2.0-qwen-3b: during its training, it was exposed to lots of captions from text-to-image datasets, which are typically short and only describe the main objects. As result, the UnifiedReward model tends to favor shorter captions. As shown in Fig. 1, the average caption length during training continuously decreases and eventually collapses to producing only :description. Conversely, when using Qwen2.5-VL-3B as the judge, the bias is in the opposite direction: it prefers overly verbose captions. This makes the policy model prone to exploiting the bias by generating long passages of content irrelevant to the image, thereby satisfying the judge models preference. As shown in Table 3, the captioning ability under this reward shows significantly inferior to CapRL. Specific examples of such cases are illustrated in Fig. 9, Fig. 10, Fig. 11. All these observations highlight that LVLM-as-a-Judge reward is fundamentally unreliable. This further underscores the advantage of CapRL, which converts subjective evaluations into objective assessments."
        },
        {
            "title": "4.3 COMPREHENSIVE DISCUSSION ABOUT CAPRL",
            "content": "In this section, we provide comprehensive analysis and discussion of CapRL. These results further confirm CapRLs general applicability, robustness, and effectiveness. CapRL demonstrates strong generalization even when trained on images from single domain. We further investigate the effect of different image sources in the QA dataset used for GRPO training. To this end, we classify the images into two categories using Qwen2.5-VL-3B: (1) documents, charts, or infographics, and (2) natural images. From each category, we sample 20k images for comparison. As illustrated in Fig. 5 (Left), models trained exclusively on chart-type images via GRPO exhibit substantial gains over Qwen2.5-VL-3B, not only in document and chart understanding but also in general VQA tasks. This demonstrates the strong generalization of CapRL-induced captioning improvements beyond the domains encountered during training. CapRL demonstrates promising scaling performance on training data. We conduct training on different amounts of QA data to evaluate the scaling behavior. As shown in Fig. 5 (Right), the models performance improves steadily as the amount of QA data increases. These results indicate that our CapRL framework exhibits highly promising scaling potential. With the continued expansion of the training data, the captioning ability can be further enhanced, unlocking additional potential of Qwen2.5-VL-3B. Given its relatively small parameter size and excellent scaling properties, this approach holds strong promise for application in industrial-scale multimodal pretraining. Sparse QA supervision is sufficient for CapRL. We further examine the effect of varying the number of QA pairs per image. Specifically, we randomly sample 20k images that retain three QA pairs after filtering, obtain CapRL-3QA-20k after training. By controlling the number of QA pairs per image, we also construct CapRL-1QA-20k and CapRL-2QA-20k. The results, presented in Table 4, show that even with only single QA pair per image, Qwen2.5-VL-3B achieves substantial improvement in captioning performance, averaging 7.4% higher than the baseline and only 0.5% lower than CapRL-2QA-20k. This highlights the remarkable efficiency of CapRL: highly sparse QA supervision is sufficient to unlock significant gains in captioning ability. Ablations about sampling rounds N. Results are shown in Table 5, performance improves steadily when increases from 1 to 4, and reaches saturation at = 8. The relatively poor performance at = 1 can be explained by the fact that each question is answered by the LLM only once, without sufficient shuffling of the options. Due to inherent option biases in the LLM, the measured accuracy fails to serve as reliable proxy for reward, thereby misdirecting the optimization of the policy model."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce CapRL, novel framework that successfully applies RLVR to the subjective task of image captioning. By redefining caption quality based on its utility in enabling vision-free LLM to accurately answer questions, we create robust, objective reward signal. Our results show that CapRL effectively encourages models to generate dense and precise image descriptions, which in turn substantially promote modality alignment in LVLM pretraining. This work marks significant step away from the restrictive, data-hungry SFT paradigm for RLVR in open-ended tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Amro Abbas, Kushal Tirumala, DÃ¡niel Simig, Surya Ganguli, and Ari Morcos. Semdedup: Dataefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Bytheway: Boost your text-to-video generation model to higher quality in training-free way. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1299913008, 2025. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 35583568, 2021. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language models. arXiv preprint arXiv:2402.11684, 2024a. Lin Chen and Long Xing. Open-llava-next: An open-source implementation of llava-next series for facilitating the large multi-modal model community. https://github.com/xiaoachen98/ Open-LLaVA-NeXT, 2024. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. ShareGPT4V: Improving large multi-modal models with better captions. In ECCV, 2024b. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024c. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. SFT Memorizes, RL Generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36:3554435575, 2023. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025."
        },
        {
            "title": "Preprint",
            "content": "Alexander Gurung and Mirella Lapata. Learning to reason for long-form story generation. arXiv preprint arXiv:2503.22828, 2025. Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, and Dahua Lin. Beyond fixed: Training-free variable-length denoising for diffusion large language models. arXiv preprint arXiv:2508.00819, 2025. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 1288812900. PMLR, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, and Qi Liu. Temporal reasoning transfer from text to video. arXiv preprint arXiv:2410.06166, 2024a. Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024b. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In ACL, 2004. Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023b. Zihan Liu, Shuangrui Ding, Zhixiong Zhang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Songgen: single stage auto-regressive transformer for text-to-song generation. arXiv preprint arXiv:2502.13128, 2025a. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025b. Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Rar: Retrieving and ranking augmented mllms for visual recognition, 2024a. Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Mia-dpo: Multi-image augmented direct preference optimization for large vision-language models. arXiv preprint arXiv:2410.17637, 2024b. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025c. Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual agentic reinforcement fine-tuning, 2025d. URL https: //arxiv.org/abs/2505.14246. Xun Lu. Writing-zero: Bridge the gap between non-verifiable problems and verifiable rewards. arXiv preprint arXiv:2506.00103, 2025."
        },
        {
            "title": "Preprint",
            "content": "Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: method for automatic evaluation of machine translation. In ACL, 2002. Zhangyang Qi, Yunhan Yang, Mengchen Zhang, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, and Hengshuang Zhao. Tailor3d: Customized 3d assets editing and generation with dual-side images. arXiv preprint arXiv:2407.06191, 2024. Yuxuan Qiao, Haodong Duan, Xinyu Fang, Junming Yang, Lin Chen, Songyang Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Prism: framework for decoupling and assessing the capabilities of vlms. In NeurIPS, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1431314323, 2024. Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz, and Ron Kimmel. FuseCap: Leveraging large language models for enriched fused image captions. In WCAV, 2024. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. DeepseekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Kai Shu, Le Wu, Yuchang Zhao, Aiping Liu, Ruobing Qian, and Xun Chen. Data augmentation for seizure prediction with generative diffusion model. IEEE Transactions on Cognitive and Developmental Systems, 2024. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. Yanpeng Sun, Jing Hao, Ke Zhu, Jiang-Jiang Liu, Yuxiang Zhao, Xiaofan Li, Gang Zhang, Zechao Li, and Jingdong Wang. Descriptive caption enhancement with visual specialists for multimodal perception. arXiv preprint arXiv:2412.14233, 2024. Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, and Jiaqi Wang. Seagent: Self-evolving computer use agent with autonomous learning from experience. arXiv preprint arXiv:2508.04700, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi K1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016."
        },
        {
            "title": "Preprint",
            "content": "Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al. FastVLM: Efficient vision encoding for vision language models. In CVPR, 2025. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: neural image caption generator. In CVPR, 2015. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al. Videorope: What makes for good video rotary position embedding? In International Conference on Machine Learning, 2025. Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. Linli Yao, Weiying Wang, and Qin Jin. Image difference captioning with pre-training and contrastive learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 3108 3116, 2022. Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. Deco: Decoupling token compression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985, 2024. Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, et al. Timechat-online: 80% visual tokens are naturally redundant in streaming videos. arXiv preprint arXiv:2504.17343, 2025a. Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, and Junnan Li. Generative frame sampler for long video understanding. arXiv preprint arXiv:2503.09146, 2025b. Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. In CVPR, 2024. Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, et al. RLPR: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025. Beichen Zhang, Kun Zhou, Xilin Wei, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive math reasoning. Advances in Neural Information Processing Systems, 36:2357023589, 2023. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European conference on computer vision, pp. 310325. Springer, 2024a. Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Booststep: Boosting mathematical capability of large language models via improved single-step reasoning. arXiv preprint arXiv:2501.03226, 2025. Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. Internlm-xcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596, 2024b. Yujie Zhou, Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Qidong Huang, Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, et al. Light-a-video: Training-free video relighting via progressive light fusion. arXiv preprint arXiv:2502.08590, 2025."
        },
        {
            "title": "A CAPRL CASES",
            "content": "We provide further illustrative examples of CapRL-3B to highlight its surprising captioning capabilities in this section. Comparison with Qwen2.5-VL-3B. As illustrated in Fig. 6, CapRL-3B demonstrates remarkable capability in understanding infographics, providing information that is both comprehensive and accurate. In contrast, Qwen2.5-VL-3B, as shown in Fig. 7, makes numerous errors in identifying key information within infographics. Furthermore, Fig. 8 highlights that CapRL-3B achieves substantially higher accuracy in chart understanding compared to Qwen2.5-VL-3B. Similarly, in the case of physical image understanding in Fig. 12, CapRL-3B also demonstrates clear superiority. Comparison with UnifiedRW-as-Judge-3B and Qwen2.5VL-as-Judge-3B. To intuitively illustrate the issues introduced by LVLM-as-a-Judge rewards, we present examples of captions produced by models trained with different methods in Fig. 9, Fig. 11, and 10. The Qwen2.5VL-as-a-Judge-3B model tends to ignore key visual information in the image and outputs lengthy, irrelevant content, such as repeatedly asserting that its caption is of high quality in order to exploit reward hacking. In contrast, UnifiedRW-as-Judge-3B produces overly short captions that omit substantial amounts of critical chart information. More cases of CapRL-3B in understanding infographics and natural images. Fig. 13 and Fig. 14 provide additional evidence of the impressive perceptual capacity demonstrated by CapRL-3B."
        },
        {
            "title": "B MORE ANALYSIS EXPERIMENTS ABOUT CAPRL",
            "content": "Table 6: Comparison between training with data containing leakage issues and training with filtered data. Leaking data leads to an obvious performance drop. Leaking QA Data Leads to Performance Degradation. We randomly sample 20k instances and construct two training conditions: one using the retained QA and the other using the filtered QA. As shown in Table 6, the model trained on the leaking data performs on average 1.1% worse than the one trained on high-quality data. This indicates that leaking QA introduces spurious reward signals that mislead the optimization of the policy model. Even when captions are not closely aligned with the image content, the LLM may still achieve high answer accuracy, thereby preventing higher rewards from being correctly assigned to genuinely better captions. MMMU MMStar WeMath Avg Leaking20k Refined20k ChartQA Pro Training Data Info VQA 47.4 48.5 50.7 51.1 55.1 56.6 36.4 36.8 36.1 37. 58.9 60."
        },
        {
            "title": "C PROMPT USED",
            "content": "We provide all the prompts employed in our experiments in this section. Specifically, the prompt used in CapRL for guiding the LLM to answer questions conditioned on captions is illustrated in Figure 15; the prompt for utilizing the Unified Reward Model as the reward model is shown in Figure 16; and the prompt for adopting Qwen2.5-VL-3B as the reward model is presented in Figure 17."
        },
        {
            "title": "D DATA PROCESSING",
            "content": "To ensure both quality and safety, our data processing pipeline consists of three main stages. First, inspired by SemDeDup (Abbas et al., 2023), we construct clusters to identify and remove images with redundant semantics. During this step, we also discard low-resolution and overly simple images, while filtering out content that involves violence, pornography, or other safety concerns. Second, to avoid benchmark leakage, we integrate the images used in commonly referenced evaluation datasets and form clusters with them. Any images from our collection that are overly similar to benchmark samples are eliminated. Third, we conduct safety inspection through human verification. Annotators perform sample-based screening, and once the proportion of unsafe images falls to negligible level, we stop filtering. Following this process, we obtain the final dataset, CapRL-5M."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: An illustrative example of CapRL-3B applied to infographic understanding."
        },
        {
            "title": "E QA PROCESSING",
            "content": "In constructing the QA pairs, we employ the Qwen2.5-VL-72B model with prompts shown in Figure 18. For each image, we generate five questions and retain those without leakage issues. We do not deliberately control the number of QA pairs per image, prioritizing instead the overall dataset size and diversity. As revealed in later ablation studies 4, although even single QA per image proves highly effective, adding more QA pairs still brings marginal improvements. During QA filtering, since the models answers carry uncertainty due to temperature parameter, we do not filter questions solely based on the correctness of single response. Instead, we sample responses four times for each question, shuffling the answer options each time, and then measure the accuracy of the LVLMs answers both based on the image and based only on the question itself."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: An illustrative example of Qwen2.5-VL-3B applied to infographic understanding. We ultimately apply threshold to filter out questions with high image-based accuracy but low question-only accuracy. It is worth noting that, because our filtering criteria are quite strict, some discarded QA pairs contain only mild or even negligible leakage. This also explains why, as shown in Figure 6, training with the leaked data does not cause training collapse but merely led to degraded performance."
        },
        {
            "title": "F PRETRAINING DETAILS",
            "content": "Model Architecture. In our experimental setup, the language model component is initialized with pretrained LLM, the visual encoder is initialized with pretrained ViT, and the MLP projector is randomly initialized. This setup corresponds to commonly adopted starting point in multimodal pretraining. To ensure the robustness of our conclusions, we evaluate three groups of architectures: (1) Qwen2.5-3B + Qwen2.5-ViT, (2) Qwen2.5-7B + Qwen2.5-ViT, and (3) InternLM2.5-7B + CLIPViT-L. This selection jointly considers differences in parameter scale, LLM backbone, and visual encoder type. Training Setting. Following the training paradigm of ShareGPT4V, our training process consists of three stages: Initial Alignment Further Pretraining SFT. (1) In the Initial Alignment stage, we unfreeze the MLP and perform preliminary alignment using the BLIP-558K dataset. We adopt"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Chart understanding comparison between CapRL-3B and Qwen2.5-VL-3B. learning rate of 1e-3 and batch size of 256. (2) In the Further Pretraining stage, we unfreeze all parameters including the LLM, MLP, and ViT. This stage facilitates further alignment with various high-quality image-caption datasets, enabling the LLM to better understand visual features. We set the learning rate to 4e-5 and the batch size to 256. (3) In the SFT stage, we again unfreeze all"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Example of CapRL-3B chart understanding. parameters and train on the OpenLLaVA-Next dataset. We set the learning rate to 2e-5 and the batch size to 128. Baselines. We selected several strong baselines for comparison. (1) Vanilla, which skips the Further Pretraining stage and only goes through the first and third stages. Additionally, we constructed two more baselines by varying the dataset used in the Further Pretraining stage: (2) ShareGPT4V-1M, and (3) DenseFusion-1M. To ensure fair comparison by controlling the number of samples, we randomly sampled 1 million image-caption pairs from the 5M dataset to form CapRL-1M."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Example of Qwen2.5VL-as-Judge-3B chart understanding."
        },
        {
            "title": "MODELS",
            "content": "Multimodal learning has accelerated rapidly in the past two years, driven by systems that couple language with perception and action at scale (Liu et al., 2023a; 2025c; Bai et al., 2025; Dong et al., 2024; Liu et al., 2025d). These advances have produced broad empirical breakthroughs across benchmarks and real-world tasks (Zhang et al., 2024b; Liu et al., 2024a; Sun et al., 2025; Shu et al., 2024; Xing et al., 2024; Qi et al., 2024). Compared to earlier eras that focused mainly on images paired with text (Zhang et al., 2024a; Liu et al., 2024b; Yao et al., 2022), todays models are larger, trained on more diverse corpora, and natively support additional modalities such as video and audio. In video understanding and generation, in particular, we have seen surge of meaningful progress spanning long-horizon temporal modeling, spatialtemporal grounding, and instruction following (Chen et al., 2024c; Wei et al., 2025; Yao et al., 2025b; Li et al., 2024a). Complementary work expands audio modeling, bridging speech, music, and cross-modal alignment (Liu et al., 2025a). The community has also introduced interactive video agents that reason over time and act, further stressing the importance of temporal memory and tool use (Ren et al., 2024; Yao et al., 2025a). On the reasoning front, multimodal LLMs increasingly exhibit structured problem solving, calibrated self-reflection, and tool-augmented inference (Zhang et al., 2023; 2025). Architecturally, research explores how to best couple visual encoders and language backbonesranging from tightly integrated fusion layers to decoupled, composable adapterswhile preserving scalability and transfer (Li et al., 2023; Yao et al., 2024). Historically, image generation and understanding evolved as largely separate tracks; however, the field is now converging toward unified, bidirectional models that both parse and synthesize content under shared representation space (Ling et al., 2024; Zhou et al., 2025; Bu et al., 2025). In parallel, diffusion-based approaches continue to look especially promising for controllable, high-fidelity synthesis and cross-modal conditioning, offering principled path to training generativediscriminative hybrids (Nie et al., 2025; Li et al., 2025). Looking forward, we anticipate four themes to shape the landscape: (1) long-context multimodality, where models maintain persistent memory across hours-long video and streaming audio; (2) agentic behavior, combining perception with planning and tool execution in open environments; (3) unified pretraining objectives that align understanding and generation, reducing modality gaps; and (4) efficient adaptationvia lightweight finetuning and retrievalto safely deploy systems across"
        },
        {
            "title": "Preprint",
            "content": "domains and devices. Together, these directions suggest move from static perception to interactive, end-to-end multimodal intelligence."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Example of Qwen2.5VL-as-Judge-3B chart understanding."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Example of CapRL enhancing the captioning ability of Qwen2.5-VL-3B."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: An illustrative example of CapRL applied to infographic understanding."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: An illustrative example of CapRL applied to natural image understanding."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Prompt for LLM to answer questions based on Caption. Figure 16: Prompt for Unified Reward Model as Judge."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Prompt for Qwen2.5-VL-3B as Judge. Figure 18: Prompt Qwen2.5-VL-72B to generate QA"
        }
    ],
    "affiliations": [
        "Alibaba Cloud",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "The Chinese University of Hong Kong",
        "University of Science and Technology of China"
    ]
}