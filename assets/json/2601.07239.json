{
    "paper_title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
    "authors": [
        "Tanmay Joshi",
        "Shourya Aggarwal",
        "Anusa Saha",
        "Aadi Pandey",
        "Shreyash Dhoot",
        "Vighnesh Rai",
        "Raxit Goswami",
        "Aman Chadha",
        "Vinija Jain",
        "Amitava Das"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability. In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled. Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 9 3 2 7 0 . 1 0 6 2 : r Tanmay Joshi1, Shourya Aggarwal1, Anusa Saha1, Aadi Pandey1, Shreyash Dhoot1, Vighnesh Rai1, Raxit Goswami2, Aman Chadha3, Vinija Jain4, Amitava Das 2Raapid Lab, USA 3Apple, USA 4Google, USA, 1Pragya Lab, BITS Pilani Goa, India Abstract Deterministic inference is comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models (LLMs) move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented detailed analysis of nondeterminism in LLM inference, showing in widely discussed blog post how batch-invariant kernels and deterministic attention can be used to enforce bitwise-identical outputs for given prompt, effectively positioning deterministic inference as prerequisite for reproducibility, onpolicy RL, and enterprise reliability. In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills: it kills the ability to model uncertainty, makes emergent abilities vanish, disrupts reasoning abilities by killing multiple reasoning paths, and renders safety alignment brittle so that we lose honest generalization. LLMs implement conditional distributions pθ(y x), not fixed functions (x); collapsing these distributions to single canonical output per prompt may feel reassuring, but it systematically hides the very properties that matter for artificial cognition. We instead advocate Stochastic CHAOSand claim that distributional variability is the heart of artificial cognition. We begin by disentangling algorithmic stochasticityintentional sampling in decoding (temperature, top-k/top-p, self-consistency, tree-of-thought)from numerical and systems nondeterminism (batching and floating-point artifacts even at temperature 0), and introduce three distinct stability goals for LLM inference: bitwise determinism (same bits, any load), distributional reproducibility (stable output distributions across seeds, hardware, and batching), and semantic stability (safety, constraints, and coarse meaning preserved under sampling). At high level, we argue that optimizing against single deterministic surfaceone score per model, one canonical completion per promptencourages models to remember evaluation quirks rather than to generalize. Deterministic inference, we claim, privileges clean-looking traces over faithful characterization of the underlying distribution pθ. Empirically, we show that deterministic inference is systematically misleading in four (i) For instruction-following, single-sample deterministic evaluation consistently ways. underestimates both capability and fragility compared to multi-sample, distributional evaluation: models that appear perfectly reliable on canonical prompts exhibit substantial failure probability under paraphrases, re-orderings, and noisy variants. (ii) For emergent abilities, success probabilities that exhibit clear phase-like transitions across scales under stochastic evaluation effectively vanish when only greedy, deterministic decoding is (iii) For reasoning, multi-path measured, making genuine emergence invisible in metrics. methods such as self-consistency and tree-of-thought degrade sharply when forced onto deterministic backbone: the space of alternative reasoning trajectories collapses to single brittle script, reducing both accuracy and diagnostic power about how the model thinks. (iv) For safety and alignment, risk estimated from deterministic runs systematically underestimates tail behavior: rare but dangerous completionsjailbreaks, toxic outputs, subtle policy violationsappear only under multi-sample, multi-perturbation evaluation, while determinism makes discovered exploits reliably reproducible once found. Implications. Taken together, these results argue that bitwise determinism should not be the default objective. Instead, LLM inference should prioritize distributional reproducibility and semantic stability. In the spirit of Stochastic CHAOS, randomness is not an annoyance to be eliminated but signal to be measured and controlleda core substrate for robust artificial cognition."
        },
        {
            "title": "1 What Do We Mean by “Determinism” in LLM Inference?",
            "content": "Reproducibility is bedrock requirement for many real-world system deployments. Ideally, deterministic system produces the exact same output for given input every time, enhancing trust, debuggability, and auditability. In classical algorithmic terms, an algorithm is deterministic if its outputs are entirely determined by its inputs. The high-performance computing (HPC) literature further distinguishes external determinism (identical final results regardless of execution interleavings) from internal determinism (identical step-by-step execution traces) (Chiang et al., 2013; Demmel et al., 2016). In the context of large language model (LLM) inference, our concern is mainly with external determinism: given the same prompt, we expect the same response. In practice, however, even after pinning all obvious sources of randomness, LLM-based systems often fail this ideal. Recent work on LLM stability and reproducibility shows that repeated nominally deterministic runse.g., temperature =0 with fixed decoding parameterscan exhibit noticeable variation in both surface form and task accuracy (Atil et al., 2024; Kaikaus et al., 2024). At the same time, system developers can truthfully say that all the kernels used in language models forward pass are deterministic, while users still observe nondeterministic outputs. As emphasized in both the HPC (Chiang et al., 2013; Demmel et al., 2016) and ML reproducibility literature (Zhuang et al., 2021; Chen et al., 2022), such discrepancies often arise from where we draw the boundary around the input and which level of determinism we care about. In this section, we therefore disentangle several contributing layers: (1) algorithmic stochasticity in decoding by design, (2) system-level nondeterminism induced by floating-point arithmetic and parallel kernels, and (3) user-facing notions of bitwise, distributional, and semantic stability. This layered view will be central to our later analysis of stochastic chaos in LLM behavior."
        },
        {
            "title": "1.1 Idealized Determinism: Greedy Decoding at T =0\nFrom a theoretical perspective, a neural language model implements a fixed function fθ(x) that\nmaps an input text x to a probability distribution over output sequences. The model’s weights θ are\nfixed after training, so the same input always yields the same distribution. If we imagine an “oracle”\nimplementation with infinite-precision arithmetic and no external interference, then generating\ntext by always picking the highest-probability next token (greedy decoding) would indeed be deter-\nministic. This greedy decoding (equivalently, temperature T = 0) is often thought to remove all\nstochasticity: at each generation step t, the next token yt is chosen as",
            "content": "yt = arg max pθ(w x, y<t) . In this idealized world, an identical prompt would yield the same completion y1:T on every run. However, this scenario implicitly assumes perfectly fixed computation. As decades of work on floating-point numerics make clear, real implementations rarely enjoy such cleanliness (Demmel et al., 2016). Even in ostensibly deterministic pipelines, subtle numerical variations can creep in, especially on massively parallel hardware. This raises basic question: when we say same input, same output, do we include the entire system statehardware type, kernel versions, batch composition, and so onas part of the input? In an online serving context, one users prompt may be processed alongside many others. Those concurrent requests are not part of the users query, yet they can influence the result through dynamic batching, scheduling, and kernel selection (He and Thinking Machines Lab, 2025; Zhang et al., 2025). Under strict external determinism definition, we might treat the whole inference servers batch as the input, in which case the servers function could be deterministic while an individual user still experiences nondeterministic behavior. This mismatch is exactly what recent work on LLM stability and batch invariance highlights (Atil et al., 2024; He and Thinking Machines Lab, 2025). Throughout the paper, we adopt the intuitive notion that each user expects identical outputs for identical prompts, independent of what else is happening on the system. The rest of this section explains why this expectation frequently fails, even under =0 greedy decoding."
        },
        {
            "title": "1.2 Algorithmic Stochasticity: Sampling by Design",
            "content": "Large language models are fundamentally probabilistic generative models. During inference, they produce text by sampling from learned probability distribution over tokens. This algorithmic stochasticity is by design: it is what allows LLMs to generate varied and creative responses rather than always repeating single answer. When using non-zero temperature or nucleus/top-p sampling, the model intentionally injects randomness into its outputs. In such cases, nondeterminism is expected and often desired. range of decoding strategies has been developed: Temperature scaling. temperature > 0 smooths or sharpens the output distribution; higher values flatten probabilities, increasing randomness, whereas ! 0 approaches greedy selection (Holtzman et al., 2020). Top-k sampling. Fan et al. (Fan et al., 2018b) restrict sampling to the most probable tokens at each step, limiting the risk of bizarre low-probability words while still allowing variability. Nucleus (top-p) sampling. Holtzman et al. (Holtzman et al., 2020) showed that greedy and pure beam search often produce degenerate, repetitive text. They introduced nucleus sampling, which draws from the smallest set of tokens whose cumulative probability exceeds p, and demonstrated that this better matches the diversity and quality of human text. Self-consistency and Tree-of-Thought. Recent work leverages stochastic trajectories at the reasoning level. Self-consistency decoding samples multiple chain-of-thought (CoT) solutions and aggregates their answers, achieving large gains on math benchmarks (Wang et al., 2023a). Treeof-Thought prompting explicitly explores multiple sampled branches of reasoning and selects promising ones, further improving complex problem solving (Yao et al., 2024). In these settings, variability is feature. Diversity in sampled outputs tends to improve fluency, creativity, and even correctness on reasoning tasks. For example, self-consistency dramatically boosts success rates on GSM8K by voting over collection of independently sampled reasoning paths (Wang et al., 2023a). Similarly, Tree-of-Thought explores multiple stochastic trajectories through structured search, moving beyond the limitations of single greedy chain (Yao et al., 2024). It is therefore crucial to distinguish intentional randomness (an algorithm design choice) from implementation randomness (system-level nondeterminism). The former can be turned off by choosing deterministic decoding. The latter persists even with =0 and no sampling, and is the focus of the next subsection."
        },
        {
            "title": "1.3 System-Level Nondeterminism",
            "content": "Even after eliminating algorithmic randomness, modern LLM inference platforms can exhibit nondeterministic results due to underlying hardware and system behaviors. The primary technical cause is well-known in numerical computing: floating-point non-associativity. Finite-precision arithmetic on parallel hardware means that operations such as summation are not exactly associative or commutative; reordering them can change the outcome by tiny amounts (Demmel et al., 2016). Formally, for floating-point numbers we can have (a + b) + 6= + (b + c), even though, mathematically, addition is associative. In transformer inference, this arises in operations like the summation of attention scores or the accumulation of matrix multiplication results. GPU implementations execute many additions in parallel threads, and the order in which partial results are combined can vary depending on scheduling, batch size, or kernel selection (Zhuang et al., 2021). These differences are usually on the order of few units in the last place (ULPs). Most of the time, such tiny variations do not change the outcome of greedy decodingthe highest logit remains highest. However, when two candidate tokens have almost equal probability, minute numerical perturbation can flip their order. When that happens, the model may produce different next word, and from that point onward the entire generated text can diverge (Atil et al., 2024). Consider the sentence to be completed: The recipe calls for sugar, flour, and Suppose the models next-token logits (after softmax) yield: p(eggs) = 0.500, p(butter) = 0.499, p(others) = 0.001. Under one execution, floating-point reductions and softmax computations give the values above, and greedy decoding picks eggs. Under another execution, due to slightly different accumulation order or tiling in batched kernel, the logits are perturbed so that p(eggs) = 0.499, p(butter) = 0.500. Now greedy decoding chooses butter instead. From there, the continuation may diverge substantially, despite the same high-level decoding algorithm and prompt. Recent empirical studies document exactly this kind of sensitivity in =0 runs across evaluation suites (Atil et al., 2024; Kaikaus et al., 2024). Dynamic batching and batch invariance. These floating-point effects are exacerbated by the parallel and distributed execution strategies used to accelerate LLMs. Modern inference engines batch multiple user requests and split computations across many GPU cores (and sometimes multiple GPUs) for efficiency. The sequence of operations that produces particular output can depend on what other inputs are being processed in parallel. Thinking Machines Lab identify this lack of batch invariance as primary reason that most LLM endpoints appear nondeterministic to users (He and Thinking Machines Lab, 2025). Even if each low-level kernel (e.g., GEMM or RMSNorm) is individually deterministic in isolation, it may not be batch-invariant. With different batch sizes or sequence packings, the underlying library can choose different tiling strategies or reduction patterns, changing the accumulation order and hence the final floating-point result (Zhang et al., 2025; Zheng et al., 2024). Thus, user who sends the same prompt twice may receive different completions solely because the prompt was batched differently with other users requests. Other sources of system-level nondeterminism include: Non-deterministic GPU kernels. Some libraries use atomic operations or race-prone implementations for speed, introducing execution-order dependence. Hardware and software drift. Different GPU models, driver versions, or library updates can change low-level numerical behavior; deep-learning framework version changes have been shown to impact reproducibility even with fixed seeds (Zhuang et al., 2021; Chen et al., 2022; Shahriari et al., 2022; PyTorch Developers, 2024). Model and API updates. Cloud providers may silently roll out new checkpoint versions or finetuned variants behind the same model name, changing outputs even if everything else is held fixed. OpenAI, for example, explicitly warn that identical requests may produce slightly different outputs over time and expose seed and system_fingerprint field to help track such changes (OpenAI, 2024). Empirically, the impact of such nondeterminism is not purely cosmetic. Atil et al. (Atil et al., 2024; ?) show that, across repeated =0 runs of the same evaluation suite, task accuracy can fluctuate by double-digit percentages purely due to implementation-level nondeterminism. Kaikaus et al. (Kaikaus et al., 2024) report substantial variation in code-generation metrics from ChatGPT across identical prompts. These results echo earlier findings in training-time reproducibility (Zhuang et al., 2021; Chen et al., 2022), now appearing at inference time. Mitigations and trade-offs. Recent work has shown that it is technically possible to defeat many of these system-level nondeterminism sources, but not without cost. One approach is to redesign computational kernels to be explicitly batch-invariant and numerically reproducible: summations are performed in fixed order, tiling is chosen deterministically, and parallel reductions avoid race conditions (He and Thinking Machines Lab, 2025; Zhang et al., 2025). Using such custom kernels, these systems demonstrate bitwise-identical LLM outputs across repeated runs and dynamic batching. The drawback is performance and complexity. Enforcing strict determinism often means forgoing some optimizations and adding synchronization; both the ML reproducibility literature and framework documentation emphasize substantial throughput penalties and engineering overheads for deterministic modes (Zhuang et al., 2021; Chen et al., 2022; PyTorch Core Team, 2020). Later systems such as SGLang and deterministic vLLM reduce this overhead, but still report noticeable slowdowns when deterministic mode is enabled (Zheng et al., 2024; Zhang et al., 2025). More broadly, deterministic GPU algorithms are widely known to be slower than their nondeterministic counterparts (PyTorch Core Team, 2020)."
        },
        {
            "title": "1.4 Historical Perspectives",
            "content": "The tension between determinism and efficiency is not new. In HPC, reproducibility of simulation results has been longstanding concern (Demmel et al., 2016; Chiang et al., 2013). Researchers have catalogued sources of nondeterminism ranging from data races and thread scheduling to floatingpoint rounding differences on varying core counts, and proposed deterministic replay and reproducible reduction algorithms as remedies. These methods improve reproducibility but often incur sizable runtime and memory overheads. In machine learning, reproducibility discussions historically focused on training, where stochastic gradient descent introduces randomness via initialization, minibatch ordering, and augmentation. Efforts to make training fully deterministicby controlling random seeds, disabling nondeterministic kernels, and fixing parallel semanticshave shown that the resulting overheads can be severe (Zhuang et al., 2021; Nagarajan et al., 2018; Chen et al., 2022). Consequently, the common practice in training is to run multiple randomized trials and report aggregate metrics rather than bitwise-identical runs (Zhuang et al., 2021; Gundersen et al., 2023). Inference, however, differs: we typically run model once per input and cannot easily average over many runs. This elevates the importance of stability at inference time. Yet, as vendors like OpenAI explicitly note, even with temperature =0 and fixed parameters, identical requests may produce slightly different outputs due to infrastructure changes or subtle numeric drift; they therefore introduce seed parameter and system_fingerprint to provide some control and visibility, while carefully promising only mostly consistent behavior (OpenAI, 2024). More recently, Thinking Machines Lab has taken stronger stance, arguing in their Defeating Nondeterminism in LLM Inference post that we should treat inference nondeterminism as bug to be fixed (He and Thinking Machines Lab, 2025). Their work and follow-up efforts in vLLM and SGLang demonstrate that much of the observed variability in =0 inference can in fact be engineered away with appropriate kernels and infrastructure (Zhang et al., 2025; Zheng et al., 2024). However, as we argue throughout this paper, fixing nondeterminism is not always synonymous with improving the behavior of probabilistic generative model, especially when one cares about distributional properties rather than single bitwise output."
        },
        {
            "title": "1.5 A Stability Taxonomy",
            "content": "The preceding discussion suggests that determinism in LLM inference is best understood as spectrum, not binary property. We propose the following stability taxonomy: Bitwise determinism. The strictest notion: the entire output sequence (and, implicitly, all intermediate numerical states) is identical at the bit level across runs. Achieving this requires: Deterministic decoding (no sampling, no random tie-breaking), Numerically reproducible kernels (fixed reduction orders, no atomics, controlled tiling), Controlled execution environment (same hardware, same library versions, no hidden model updates). This is the level targeted by deterministic variants of vLLM, SGLang, and batch-invariant kernels from Thinking Machines (He and Thinking Machines Lab, 2025; Zhang et al., 2025; Zheng et al., 2024). It is extremely valuable for debugging, regression testing, and certain scientific audits, but comes with non-trivial cost in performance and engineering complexity (Zhuang et al., 2021; PyTorch Core Team, 2020). Distributional reproducibility. weaker but often more relevant requirement is that the distribution of outputs is stable, even if individual draws differ. For stochastic decoder (e.g., nucleus sampling), distributional reproducibility means repeated runs with the same configuration approximate the same underlying distribution pθ(y x): the frequencies of different outcomes, success rates, and uncertainty profiles remain consistent (Atil et al., 2024). From this perspective, the goal is not to produce the same answer every time, but to ensure that any variability reflects true model uncertainty rather than uncontrolled numeric noise. Evaluation frameworks increasingly recommend repeated sampling and reporting mean and variance of metrics rather than single-point estimates (Zhuang et al., 2021; Gundersen et al., 2023). Semantic stability. The weakest, but most user-facing, notion is that the meaning or task outcome remains stable under small perturbations or repeated queries. Two outputs may differ at the surface level yet still be semantically equivalent (e.g., paraphrases or alternate phrasings). For many applications, users care far more about semantic stability than bitwise identity. Empirical studies find that while raw text outputs may vary significantly run-to-run, the final answers (e.g., extracted multiple-choice labels or numeric results) are often much more stable (Atil et al., 2024; Kaikaus et al., 2024). Designing downstream systems to focus on semantic content rather than exact strings can therefore absorb much of the apparent nondeterminism. Putting it together. Determinism in LLM inference emerges from multiple layers of the stack algorithmic, numeric, system-level, and semantic. Improving stability is thus multi-pronged engineering and evaluation challenge. Researchers are beginning to conquer this challenge piece by piece: deterministic kernels, batch-invariant execution, environment fingerprinting, and evaluation practices that embrace distributional thinking (He and Thinking Machines Lab, 2025; Atil et al., 2024; OpenAI, 2024). Yet practical usage often strikes balance between strict reproducibility and the efficient, parallel, probabilistic nature of modern AI systems. Absolute determinism remains niche mode for special purposes; for most deployments, the goal is robust semantic and distributional stability under realistic, noisy serving environment."
        },
        {
            "title": "2 Letś Stress-Test “Deterministic Inference” in Practice",
            "content": "The discussion above makes one point clear: deterministic inference is not natural primitive of large language models, but an engineering objective imposed on top of fundamentally stochastic system. Recent work from Thinking Machines Lab (He and Thinking Machines Lab, 2025) shows, impressively, that careful redesign of batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs for given prompt, even under dynamic batching. In their narrative, nondeterminism is bug to be eradicated: source of flaky tests, unreliable on-policy RL, and enterprise-grade surprises. The implicit ideal is that LLM inference should behave like pure function from prompts to strings. Our central hypothesis takes the opposite stance. We argue that aggressively enforcing deterministic inference can itself degrade the scientific validity, generalization ability, and safety of LLMs. Rather than treating nondeterminism as mere engineering noise, we treat it as first-class signal about the models underlying distribution pθ(y x)a distribution that is central to how modern LLMs represent uncertainty, support multiple reasoning paths, and exhibit emergent behaviors (Wei et al., 2022a; Ganguli et al., 2022a). From this vantage point, the crucial question is not only can we defeat nondeterminism? but what do we lose if we do? To make this tension concrete, we move from theory to stress tests. Our empirical program is organized around four claims about the consequences of enforcing strict determinism at inference time: (i) Deterministic evaluation encourages benchmark memorization over genuine generalization. We revisit the trajectory of GLUE, where single-score, single-output evaluation led to rapid saturation and brittle models (Wang et al., 2018, 2019; Geirhos et al., 2020). We argue that sequencelevel determinism risks repeating the same mistake at finer granularity: optimizing for single canonical completion per prompt rather than for robust distributions over semantically correct answers. In later sections, we show that evaluation practices based on single deterministic run can mask substantial variability in model behavior and overstate progress. (ii) Deterministic decoding suppresses emergent abilities that rely on exploration. Many emergent behaviors in LLMsfrom few-shot in-context learning to chain-of-thought and selfconsistency gains on math and reasoning tasks (Brown et al., 2020; Wei et al., 2022b; Wang et al., 2023b; Yao et al., 2023; Wei et al., 2022a)depend critically on sampling multiple trajectories. Forcing single greedy path at =0 can eliminate these behaviors, not because the underlying model lacks the capacity, but because the inference stack refuses to explore it. We show that, on standard reasoning benchmarks, strict greedy decoding systematically underestimates the models latent competence relative to multi-sample decoding. (iii) Deterministic inference collapses multiple valid reasoning paths into single, brittle trace. Complex reasoning tasks often admit many correct solution paths and many near-miss failures. Multi-sample decoding surfaces rich landscape of alternative reasoning strategies, while strict greedy decoding prunes this diversity down to single chain. This path collapse hides the models internal uncertainty and makes it harder to diagnose where and how reasoning fails. Building on self-consistency-style analyses (Wang et al., 2023b), we show that restricting evaluation to one deterministic path can misclassify models as either failing or passing on an item when the underlying distribution is substantially more nuanced. (iv) Deterministic safety evaluation creates an illusion of robustness. Safety research increasingly treats LLMs as strategic, stochastic agents whose behavior can change under distribution shift, prompt injection, or perceived oversight (Perez et al., 2022; Ganguli et al., 2022b; Greenblatt et al., 2024; Hubinger et al., 2024). Evaluating safety only under single deterministic decoder can drastically underestimate risk: dangerous but low-probability modes may not surface in any one greedy run, giving false sense of security. We show that even when model appears safe under =0 greedy decoding, low-measure but high-risk behaviors emerge under modest stochasticity or paraphrased attack prompts. Crucially, our critique is not that deterministic inference is useless. We distinguish between deterministic modes as diagnostic tool and determinism as deployment norm. As we will argue later, deterministic modes remain indispensable for debugging, regression testing, and exact on-policy RL, where bitwise reproducibility is legitimate requirement. Our claim, rather, is that elevating bitwise determinism into default norm for LLM deployment and evaluation fundamentally misunderstands what these models are. LLMs are not compilers; they are stochastic semantic machines whose competence lives in the geometry of pθ(y x), not in any single string sampled from it. The rest of this paper operationalizes this perspective. Section 3 uses the history of GLUE as cautionary tale, showing how single-score, single-output evaluation led to benchmark saturation and spurious progress, and how paraphrastic and distributional variants reveal hidden brittleness. Section 4 examines instruction-following, contrasting deterministic and stochastic decoding on paraphrased and adversarial prompts to expose lost generalization under strict determinism. Section 5 turns to emergent reasoning abilities, quantifying how multi-path, sampling-based decoding recovers solutions that greedy decoding systematically misses. Section 6 focuses on safety and alignment, showing how deterministic evaluation underestimates risk by masking rare but harmful generations. Together, these stress tests collectively support our central thesis: what makes LLMs powerful is not their ability to be bitwise deterministic, but their ability to express and harness distributional variability in controlled way."
        },
        {
            "title": "3 Deterministic Inference Encourages Benchmark Memorization",
            "content": "The previous section argued that bitwise-deterministic inference is not natural primitive for probabilistic generative models. We now show that, even at the evaluation level, insisting on single deterministic output per input risks repeating an old mistake from the pre-LLM era: the GLUE saturation story. Our claim in this section is simple: We first revisit GLUE as cautionary tale of single-score benchmark culture, then show how modern sequence-level deterministic inference is structurally analogous. Finally, we introduce GLUEstyle robustness protocol over four LLM families and construct heatmap of robustness that directly visualizes the cost of determinism. Claim 1: Deterministic inferenceone input, one canonical output, one scalar score turns evaluation into answerfrommemory: success is measured by reproducing fixed surface form, not how stably the model supports distribution of semantically correct responses."
        },
        {
            "title": "3.1 GLUE as a Cautionary Tale",
            "content": "The GLUE benchmark (Wang et al., 2018) was designed as multi-task testbed for natural language understanding, aggregating performance across nine tasks, including natural language inference (MNLI, RTE), paraphrase detection (QQP), question answering (QNLI), and sentiment analysis (SST-2). GLUE was an enormous success: it provided standardized evaluation suite and single scalar GLUE score that made progress easy to track and compare. SuperGLUE extended this template with harder tasks and an even more entrenched leaderboard culture (Wang et al., 2019). GLUEs design implicitly enshrined particular notion of performance: for each example (xi, yi) and model fθ, the evaluation pipeline asked for single predicted label ˆyi = fθ(xi) and computed an accuracy or F1 score by comparing ˆyi to yi. The whole community then reported single scalar: GLUEScore(fθ) ="
        },
        {
            "title": "1\nT",
            "content": "TX t=1 Acct(fθ) , where indexes tasks. There was no notion of distribution over predictions, uncertainty, or robustness; only single deterministic mapping from inputs to labels. Within roughly two years, GLUE was effectively solved: state-of-the-art models reported scores at or above estimated human performance. Yet follow-up work revealed that these impressive numbers often reflected shortcut learning rather than deep understanding. Gururangan et al. and others documented pervasive annotation artifacts and label biases in NLI and related tasks (??). Geirhos et al. showed more broadly how deep networks, given fixed benchmark, gravitate toward cheap, brittle heuristics that exploit spurious correlations (Geirhos et al., 2020). Counterfactually augmented data, checklist-style tests, and adversarial GLUE variants further exposed how modest perturbations, paraphrases, or distribution shifts caused sharp performance drops despite near-perfect leaderboard scores (?????). From statistical perspective, the problem is not that GLUE was bad, but that the combination of finite test sets and single-output evaluation creates an evaluation surface that can be memorized. Once models and training pipelines are tuned directly against that surface, new parameters are free to overfit the idiosyncrasies of the benchmarks finite sample. The resulting leaderboards give an illusion of steady progress even as out-of-distribution behavior stagnates."
        },
        {
            "title": "3.2 From Label Determinism to Sequence Determinism",
            "content": "Large language models extend this picture in two important ways: they are generative, and they are stochastic. Instead of learning classifier fθ(x) ! y, they learn conditional distribution pθ(y x) , where is text sequence, not single label. Evaluation, however, often collapses this distribution back into deterministic mapping by choosing fixed decoding strategy Decd: pθ((cid:1) x) 7! ˆy. For example, greedy =0 decoder defines ˆydet(x) = Decgreedy(pθ((cid:1) x)) = arg max pθ(y x) , where in practice the argmax is taken token by token. In many contemporary LLM evaluations, especially those adapted from GLUE-style tasks, performance is reported as Accdet(fθ) ="
        },
        {
            "title": "1\nN",
            "content": "NX i=1 [ϕ(ˆydet(xi)) = yi] , where ϕ extracts label (e.g., multiple-choice option) from the deterministic completion. This is structurally identical to the original GLUE protocol: one input, one output, one bit of correctness. Yet, from the standpoint of artificial cognition, the meaningful object is not ˆydet(x) but the entire distribution pθ(y x). Different decoding strategiestemperature sampling, nucleus sampling, selfconsistency, Tree-of-Thoughtall probe different slices of this distribution and often reveal capabilities that deterministic greedy decoding hides (Holtzman et al., 2020; Wang et al., 2023b; Yao et al., 2023). Insisting on single deterministic trace amounts to replaying the GLUE error at the sequence level: we optimize and evaluate against single surface point on much richer distribution. To make this concern concrete, we now design GLUE-style robustness protocol over four widely used tasks and diverse set of LLM families, explicitly contrasting deterministic vs. stochastic evaluation."
        },
        {
            "title": "3.3 Experimental Setup: GLUE-Style Robustness Under Decoding Choices",
            "content": "Tasks. We focus on four GLUE tasks that are both influential and amenable to paraphrastic manipulation: MNLI (Multi-Genre Natural Language Inference): three-way classification (entailment, contradiction, neutral) over premisehypothesis pairs with diverse genres. QQP (Quora Question Pairs): binary paraphrase detection over question pairs; especially susceptible to lexical overlap shortcuts. QNLI: questionsentence pairs derived from SQuAD; recast as binary entailment, testing whether sentence answers question. SST-2: binary sentiment classification at the sentence level. For each task 2 fMNLI, QQP, QNLI, SST-2g, we start from held-out test (or dev) set"
        },
        {
            "title": "Dorig\nt",
            "content": "= f(x(t) , y(t) )gNt i=1 , where x(t) is the input text (single sentence or pair) and y(t) is the gold label. Paraphrastic and perturbed variants. To probe generalization beyond the benchmark surface, we create three additional variants for each task: Paraphrased (Dpara ): for each example, we generate 23 paraphrastic rewrites of one or both segments (premise/hypothesis, question/sentence) using strong paraphrase model and filter them to preserve the label; e.g., by requiring high entailment confidence or human verification. Perturbed (Dpert ): we apply small lexical and syntactic transformations that should not change the label: synonym substitution, tense changes, active/passive alternation, or mild word-order shuffles. Adversarial paraphrased (Dadv ): we prompt an LLM to produce label-preserving but challenging rewrites (e.g., keep the answer label unchanged but attempt to confuse classifier by changing connectives and information order), again filtered for correctness. Each variant shares the same labels y(t) but differs in surface form. Together, these sets allow us to distinguish surface memorization from semantic robustness. Models. To connect with our FRACTURE analysis, we evaluate the same 17-model zoo used in Figure ??: LLaMA-2 7B, LLaMA-2 13B, Vicuna-7B, LLaMA-3 8B, Gemma 2 9B, Gemma 2 27B, Mistral-7B, Mixtral-8(cid:2)7B, Phi-2, LLaMA-3 7B, LLaMA-3 70B, Claude, Mixtral-8(cid:2)22B, GPT-3.5, GPT-4o, GPT-4o mini, DeepSeek. These span open and closed models, small and large scales, and variety of training pipelines. For each model we use its official instruction-tuned checkpoint and recommended prompting style. Decoding modes. For each model and task t, we define two decoding modes: Deterministic (Det): temperature = 0, greedy decoding, nucleus = 1.0 (i.e., no sampling). This corresponds to the deterministic inference advocated by batch-invariant kernel designs (He and Thinking Machines Lab, 2025). Stochastic (Stoch): moderate temperature and nucleus sampling, e.g. = 0.7, top-p = 0.9, with independent samples per input (we use = 10). In both modes, we prompt the model with natural-language description of the task and constrained answer format (e.g., options A/B/C). deterministic label-extraction function ϕ maps each completion into label in the tasks label set."
        },
        {
            "title": "2\nDeterministic vs. distributional evaluation. For each task t, dataset variant v\nforig, para, pert, advg, model m, and decoding mode d, we compute per-split accuracies as\nfollows.",
            "content": "Deterministic accuracy. In Det mode, we generate single completion ˆydet for each input and compute"
        },
        {
            "title": "Adet",
            "content": "t,v (m) = 1 jDv (x,y)Dv [ϕ(ˆydet(x)) = y] . Stochastic majority-vote accuracy. In Stoch mode, we draw independent completions ˆy(1), . . . , ˆy(K) and take majority-vote label We then compute y(x) = mode(ϕ(ˆy(1)(x)), . . . , ϕ(ˆy(K)(x))) . Astoch t,v (m) = 1 jDv (x,y)Dv [y(x) = y] . This treats the model as distribution over labels and asks whether the mode of that distribution is correct. We further record, for analysis but not for the heatmap, per-example label entropy and disagreement rate across samples, which quantify the models epistemic uncertainty (Atil et al., 2024). robustness ratio. To isolate robustness rather than absolute accuracy, we define GLUE robustness ratio for each triplet (t, m, d): R(d) (m) = t,para(m) + A(d) A(d) 3 A(d) t,pert(m) + A(d) t,orig(m) t,adv(m) . By construction, R(d) (m) 2 [0, 1] whenever the model performs no better on the variants than on the original split. value near 1 indicates that performance on paraphrased, perturbed, and adversarially rewritten inputs matches performance on the original benchmark surface. value substantially below 1 indicates that the models high GLUE score is not robust: it collapses under simple rephrasings of the same underlying semantics. This normalization is important. Models differ in absolute strength: small student model may have lower raw accuracy but higher robustness ratio than large SOTA model. By focusing on R(d) ), and we can ask how decoding choices affect the latter. (m), we explicitly separate competence (high At,orig) from generalization (high R(d) t"
        },
        {
            "title": "3.4 A GLUE Robustness Heatmap for Deterministic vs. Stochastic Inference\nTo visualize the interaction between tasks, models, and decoding modes, we assemble an 8 (cid:2) 17 ro-\nbustness matrix. Rows correspond to task–decoder pairs, columns to models; the resulting matrix\nis shown in Figure 1.",
            "content": "Rows (top to bottom): MNLIStoch, MNLIDet; QQPStoch, QQPDet; QNLIStoch, QNLIDet; SST-2Stoch, SST-2Det. Columns (left to right): LLaMA-2 7B, LLaMA-2 13B, Vicuna-7B, LLaMA-3 8B, Gemma 2 9B, Gemma 2 27B, Mistral-7B, Mixtral8(cid:2)7B, Phi-2, LLaMA-3 7B, LLaMA-3 70B, Claude, Mixtral-8(cid:2)22B, GPT-3.5, GPT-4o, GPT-4o mini, DeepSeek. Figure 1: GLUE Robustness Heatmap under Deterministic vs. Stochastic Decoding. Each cell shows the robustness ratio R(d) (m) (higher is better) for task 2 fMNLI, QQP, QNLI, SST-2g, decoding mode 2 fStoch, Detg, and model (columns, same zoo as in the FRACTURE analysis). Darker green indicates that paraphrased, perturbed, and adversarial variants preserve most of the models original GLUE accuracy; purple indicates severe degradation. Across tasks and models, Stochastic rows are consistently greener than their Deterministic counterparts, showing that bitwise-deterministic greedy decoding systematically underestimates the distributional generalization capacity of the underlying model. In other words, deterministic evaluation replays the GLUE mistake: it optimizes for one canonical completion per prompt, while stochastic, distributional evaluation reveals that the models competence is broaderand its brittleness more severe than the single trace suggests. The entry in row (t, d) and column is precisely R(d) (m). We render this matrix as heatmap: Color encodes robustness: darker green for high R(d) (m) (robust), shifting toward yellow/blue and then purple as robustness degrades. Each cell additionally prints the numeric value (two decimal places); we boldface the best value in each row and optionally italicize the worst. Thin horizontal lines separate task bands (after each deterministic row), and vertical line separates early LLaMA-2/Vicuna-style baselines from later, more capable models, mirroring the FRACTURE visualization. Above the columns, we annotate the least robust model (lowest mean R(d) (m) across rows) as the most brittle column; on the right margin, we annotate the most brittle taskdecoder row. Qualitatively, we observe consistent pattern (detailed in Section ??): for almost every task and model m, the Stochastic row exhibits substantially higher R(d) (m) than the corresponding deterministic row. That is, when we treat the model as distribution over completions and evaluate via majority vote, robustness to paraphrase and perturbation improves markedly. In contrast, greedy deterministic decodingthe form of deterministic inference advocated by batch-invariant kernels systematically collapses this distribution onto single, often brittle, pattern. t,orig) while exhibiting dramatic robustness drops (low R(d) From the standpoint of benchmark design, this heatmap is the sequence-level analog of the GLUE cautionary story. model may achieve near-perfect accuracy on Dorig under deterministic decoding (high Adet (m)). Only when we expose and aggregate over multiple stochastic trajectories do we recover more faithful picture of the models semantic competence and uncertainty. Deterministic evaluation, by design, hides both the latent diversity of correct behavior and the tails of failure, giving false sense of generalization that closely echoes the early GLUE era. Beyond this aggregate view, Figures 218 provide complementary, per-model perspective on the same robustness ratios R(d) (m) defined in Section 3.3. Each panel fixes model and plots, for the four GLUE tasks, paired violin glyphs for stochastic (teal) and deterministic (orange) decoding. The vertical position of each violin encodes the mean robustness ratio for that task and decoding mode, while the shape and spread summarize the empirical variability of R(d) (m) across perturbation types (paraphrased, perturbed, adversarial) and resampled subsets of evaluation examples. Narrow, high violins (e.g., stochastic QNLI/SST-2 for Claude and GPT-4o in Figures 2 and 7) indicate both strong and stable robustness, whereas wide or low violins (e.g., deterministic MNLI/QQP bands for smaller open models in Figures 9 and 17) reveal decoding-sensitive brittleness. Compared to the single cell per (t, m, d) in Figure 1, these per-model diagrams expose how robustness is distributed across tasks and perturbation types, making it clear that the advantage of stochastic inference is not an artifact of few outlier settings but consistent, cross-task pattern that nevertheless manifests with different magnitudes and variance profiles for different architectures. Figure 2: Robustness ratios for Claude across GLUE tasks. Under stochastic decoding (teal), Claude attains robustness ratios between 0.85 and 0.91 across MNLI, QQP, QNLI, and SST-2, whereas deterministic decoding (orange) stays in the lower 0.790.82 band. This yields absolute stochasticdeterministic gaps in the range of 0.050.12. The tight stochastic violins on QNLI and SST-2 indicate low variance across perturbation types, while the slightly wider shapes on MNLI and QQP reveal task-dependent sensitivity. Overall, Claude is consistently more robust when decoded stochastically, and the gains are not marginal but numerically substantial. Figure 3: Robustness ratios for DeepSeek across GLUE tasks. Stochastic decoding places DeepSeek in highrobustness regime, with ratios spanning 0.850.93 across tasks, while deterministic decoding lags behind at 0.760.81. The stochasticdeterministic gap ranges from about 0.05 up to 0.16 absolute points, making DeepSeek one of the models with the largest decoding-induced robustness gains. QNLI and SST-2 show the highest stochastic robustness, whereas MNLI and QQP display broader violins, reflecting increased variability under perturbations. These numbers highlight that DeepSeeks strong robustness is tightly coupled to stochastic inference; deterministic decoding leaves significant robustness on the table. Figure 4: Robustness ratios for Gemma-2 9B across GLUE tasks. With stochastic decoding, Gemma-2 9B achieves robustness ratios between 0.82 and 0.91, while deterministic decoding stays in the 0.770.82 range. The task-wise stochasticdeterministic differences vary from essentially 0.00 (one task where deterministic is on par) up to about 0.09 absolute points. QQP and SST-2 show the highest stochastic robustness, while MNLI and QNLI are slightly lower and more spread out. This figure indicates that even mid-sized open model like Gemma-2 9B benefits measurably from stochastic decoding, though the magnitude of gains is somewhat smaller and more task-dependent than for frontier proprietary models. Figure 5: Robustness ratios for Gemma-2 27B across GLUE tasks. Scaling to 27B pushes the stochastic robustness band to 0.860.90, while deterministic decoding lies in the slightly lower interval 0.790.84. Stochasticdeterministic gaps span roughly 0.020.10 across tasks, smaller than for some proprietary models but still systematically positive. MNLI and QQP show clear upward shifts compared to Gemma-2 9B, and SST-2 reaches the top of the models robustness range with narrow, high violins. The combination of higher means and reduced spread suggests that Gemma-2 27B is both more robust and more stable, yet still meaningfully boosted by stochastic decoding. Figure 6: Robustness ratios for GPT-4o mini across GLUE tasks. Under stochastic decoding, GPT-4o mini attains robustness ratios in the 0.840.89 range, whereas deterministic decoding falls between 0.76 and 0.84. The resulting gaps are on the order of 0.050.09 absolute points depending on the task. MNLI and QQP sit around the lower end of the stochastic band, while QNLI and especially SST-2 approach the top, indicating that classification-style tasks can remain robust even for compressed model. These numeric ranges show that even distilled GPT-4o variant retains sizable robustness margin under stochastic decoding, making inference-time choices crucial when deploying lightweight models. Figure 7: Robustness ratios for GPT-4o across GLUE tasks. GPT-4o shows one of the strongest robustness profiles: stochastic ratios consistently lie between 0.87 and 0.93, while deterministic decoding drops to 0.750.84. Task-wise stochasticdeterministic gaps range from about 0.04 up to 0.16 absolute points, with the largest differences on QNLI and SST-2. The tight, high violins for stochastic decoding indicate high robustness and low variance, whereas deterministic violins are wider and noticeably shifted down. These results underscore that GPT-4os robustness is not merely property of the underlying model but also of the decoding policy: deterministic inference underutilizes its potential. Figure 8: Robustness ratios for GPT-3.5 across GLUE tasks. For stochastic decoding, robustness ratios span 0.840.91, situating GPT-3.5 below GPT-4o but still in relatively strong band. Deterministic decoding compresses the model into the 0.780.83 range, with per-task gaps of roughly 0.060.11 absolute points. QQP and MNLI exhibit the largest downward shifts and broader violins under deterministic decoding, signaling heightened vulnerability to adversarial paraphrases in these settings. Taken together, the figure positions GPT-3.5 as mid-robustness baseline whose observed robustness is highly sensitive to decoding: small sampling changes can translate into 510 pp differences in robustness ratio. Figure 9: Robustness ratios for LLaMA-2 7B across GLUE tasks. Stochastic decoding yields robustness ratios between 0.81 and 0.89, while deterministic decoding ranges more widely from 0.72 up to 0.86. The stochasticdeterministic differences vary from slight negative value (one task where deterministic happens to be slightly higher) to substantial positive gap of about 0.15 absolute points. MNLI and QNLI show the lowest medians and widest violins, indicating that 7B-class open model struggles most on inference-style tasks under perturbations. Numerically, this figure illustrates that LLaMA-2 7B sits at the lower end of the robustness spectrum and is highly decoding-sensitive, making it an informative but fragile baseline. Figure 10: Robustness ratios for LLaMA-2 13B across GLUE tasks. After scaling to 13B, stochastic robustness climbs to the 0.840.94 range, while deterministic decoding stays in narrower but lower interval of 0.790.82. The resulting stochasticdeterministic gaps fall between 0.03 and 0.14 absolute points, with the largest gains again on MNLI and QNLI. Compared to LLaMA-2 7B, both decoding modes shift upward and the stochastic violins become tighter, especially on QQP and SST-2. This figure shows that scaling within the same family substantially improves robustness, yet the qualitative pattern remains: stochastic decoding consistently exposes more robust operating regime than deterministic decoding. Figure 11: Robustness ratios for LLaMA-3 7B across GLUE tasks. Despite having the same parameter count as LLaMA-2 7B, LLaMA-3 7B achieves higher stochastic robustness, with ratios in the 0.830.90 range. Deterministic decoding occupies 0.770.84, and stochasticdeterministic gaps are more modest but still positive at roughly 0.060.08 absolute points. QQP and QNLI show the highest robustness and the tightest violins, while MNLI remains the most challenging task. Quantitatively, this figure suggests that architectural and data improvements from LLaMA-2 to LLaMA-3 shift the entire robustness band upward, even though the fundamental advantage of stochastic decoding persists. Figure 12: Robustness ratios for LLaMA-3 8B across GLUE tasks. Under stochastic decoding, LLaMA-3 8B attains robustness ratios in the 0.890.96 band (roughly 0.91 on MNLI, 0.80 on QQP, 0.86 on QNLI, and 0.95 on SST-2), whereas deterministic decoding falls to the 0.740.79 band across the same tasks. The stochastic deterministic gaps range from about 0.06 (QQP, QNLI) up to nearly 0.18 (SST-2), showing large decodinginduced robustness gains. The high, tight stochastic violin on SST-2 in particular indicates that LLaMA-3 8B becomes extremely robust when decoded stochastically, while deterministic decoding systematically underestimates its robustness. Figure 13: Robustness ratios for LLaMA-3 70B across GLUE tasks. Stochastic decoding places LLaMA-3 70B in strong robustness band of 0.840.96: around 0.85 on MNLI, 0.88 on QQP, 0.88 on QNLI, and near 0.96 on SST2. In contrast, deterministic decoding compresses robustness into the lower 0.740.83 interval. The resulting stochasticdeterministic differences span roughly 0.040.13 absolute points, with the largest margins on SST-2 and QQP. Compared with LLaMA-3 7B, these numbers show that scaling to 70B significantly strengthens robustness while preserving the same qualitative advantage of stochastic decoding. Figure 14: Robustness ratios for Mistral-7B across GLUE tasks. With stochastic decoding, Mistral-7B achieves robustness ratios between 0.84 and 0.90 on MNLI, QQP, and QNLI, and around 0.780.82 on SST-2. Deterministic decoding yields slightly lower values on most tasks, in the 0.790.84 range for MNLI/QQP/QNLI and around 0.770.82 on SST-2. Stochasticdeterministic gaps are moderate (0.020.06 absolute), except for SST-2 where deterministic decoding is marginally higher, illustrating that the decoding advantage can flip on specific tasks. Overall, the figure highlights that Mistral-7B is reasonably robust but exhibits nuanced, task-specific trade-offs between stochastic and deterministic decoding. Figure 15: Robustness ratios for Mixtral-8(cid:2)7B across GLUE tasks. Stochastic decoding places the mixtureof-experts model in high band of 0.830.95: about 0.92 on MNLI, 0.86 on QQP, 0.83 on QNLI, and 0.89 on SST-2. Deterministic decoding yields 0.770.84 across tasks, often trailing stochastic decoding by 0.050.10 absolute points. The largest gaps appear on MNLI and SST-2, where violins are clearly separated, while QQP shows smaller but still positive advantage for stochastic decoding. These patterns indicate that routingbased models like Mixtral-8(cid:2)7B can be highly robust, but their robustness is substantially unlocked only under stochastic inference. Figure 16: Robustness ratios for Mixtral-8(cid:2)22B across GLUE tasks. Scaling Mixtral to 8(cid:2)22B yields stochastic robustness ratios in the 0.840.95 band: about 0.84 on MNLI, 0.85 on QQP, 0.93 on QNLI, and 0.90 on SST2. Deterministic decoding remains in lower 0.780.82 band across all tasks. The stochasticdeterministic margins are modest (0.030.06) on MNLI/QQP/SST-2 but become very large on QNLI ((cid:25) 0.100.15). The very tall, narrow stochastic violin for QNLI emphasizes high and stable robustness, whereas deterministic decoding exhibits both lower means and larger spread. Thus, Mixtral-8(cid:2)22B combines scale with strong stochastic robustness, particularly on inference-style QNLI. Figure 17: Robustness ratios for Phi-2 across GLUE tasks. Despite being small model, stochastic decoding propels Phi-2 to surprisingly high robustness ratios: around 0.86 on MNLI, 0.930.95 on QQP, 0.960.98 on QNLI, and 0.890.93 on SST-2. In contrast, deterministic decoding stays in the 0.740.80 band across tasks. This yields very large stochasticdeterministic gaps of roughly 0.100.17 absolute points, some of the largest differences in the entire model suite. The tall, sharply peaked stochastic violins for QQP and QNLI further indicate that Phi-2s robustness is heavily latent and only surfaces under stochastic inference, making it striking example of decoding-dependent robustness. Figure 18: Robustness ratios for Vicuna-7B across GLUE tasks. With stochastic decoding, Vicuna-7B reaches robustness ratios of roughly 0.88 on MNLI, 0.87 on QQP, 0.90 on QNLI, and 0.820.84 on SST-2. Deterministic decoding lies around 0.83 on MNLI, 0.78 on QQP, 0.79 on QNLI, and 0.850.87 on SST-2. This produces positive stochasticdeterministic gaps of 0.050.11 on MNLI/QQP/QNLI, but negative gap on SST-2 where deterministic decoding is (cid:25) 0.030.04 higher. The figure thus reveals mixed robustness profile: Vicuna-7B strongly prefers stochastic decoding on inference-heavy tasks but appears better calibrated under deterministic decoding on sentiment classification."
        },
        {
            "title": "4 Deterministic Decoding Suppresses Exploration–Driven Abilities",
            "content": "Large language models are often described as exhibiting emergent abilities: fewshot incontext learning, sharp jumps in instruction following, and the ability to obey complex stylistic or structural constraints without explicit supervised training (Brown et al., 2020; Wei et al., 2022a). At high level, these behaviors are usually narrated as if they are intrinsic properties of the underlying parameter vector θ: once the model is big enough, new capability suddenly appears. Our perspective in this paper is more operational: many of these behaviors are best understood as properties of the joint system consisting of the base model and the decoding policy that probes its trajectory space. In particular, we will show that replacing richly stochastic, multisample decoding scheme with single greedy pass at temperature =0 can make an apparently emergent ability disappear, even when the underlying distribution pθ(τ x) still assigns substantial probability mass to successful trajectories (Wei et al., 2022b; Wang et al., 2023b; Yao et al., 2023; Kojima et al., 2022). This is the sequencelevel counterpart of our GLUE analysis in Section 3: just as single output, deterministic evaluation hides distributional generalization, strictly deterministic decoding hides explorationdriven abilities already encoded in pθ(τ x). Claim 2 (ExplorationDriven Emergence). Deterministic inference stifles emergence: by collapsing rich trajectory distribution into single greedy path, it prevents many otherwise available emergent behaviors from ever being expressed. trajectoryspace view. Formally, let be an input, let τ = (y1, . . . , yT ) denote an output trajectory, and let pθ(τ x) be the autoregressive distribution induced by the model. An ability (e.g., correct classification, or satisfying bundle of style and length constraints) corresponds to success set S(x) (cid:18) of trajectories that implement the desired behavior. decoding policy egreedy, beam, temperature sampling, bestofk, etc.induces stochastic kernel Ke(τ x, θ) over trajectories, from which we obtain realized success probability Psucc(e; θ) = Ex \" τ S(x) # Ke(τ x, θ) . Crucially, Ke need not coincide with pθ((cid:1) x): greedy decoding collapses the support of Ke onto single maximizing trajectory, while multisample stochastic decoding with selection spreads mass over richer subset of the models latent behavior space, in the spirit of selfconsistency and tree ofthought procedures for reasoning and planning on top of LLMs (Wei et al., 2022b; Wang et al., 2023b; Yao et al., 2023). Under strictly deterministic decodingin particular, greedy decoding at temperature =0 with no sampling or rerankingthe inference stack implements map ggreedy : (x, θ) 7! τ (x, θ), τ = arg max τ pθ(τ x), and therefore only ever observes single trajectory per input. If the success set S(x) does not contain this unique maximizer, but does contain many highprobability nearby trajectories, then pθ(S(x) x) can be large while Psucc(egreedy; θ) remains small. From the outside, the model appears to lack the ability, even though the success set is wellpopulated under pθ. In this sense, deterministic decoding can hide emergent abilities behind narrow, brittle view of the trajectory space, echoing earlier observations about degeneration and mode collapse under naive decoding strategies (Holtzman et al., 2019) and more recent critiques that many apparent emergent phenomena are highly sensitive to evaluation protocols, metrics, and aggregation choices (Sagawa et al., 2023; Schaeffer et al., 2023). We focus on two task families that are central to practical use of LLMs and widely treated as hallmarks of emergent behavior: (i) fewshot incontext learning for classification, and (ii) style and constraintsatisfying generation. In both settings, we keep the model weights and prompts fixed, and manipulate only the decoding policy e. For each task, model, and decoding regime we can view Psucc(e; θ) as scalar functional of Ke; moving from greedy to exploratory decoding corresponds to replacing lowentropy kernel with higherentropy, multisample kernel that explicitly samples from the tails of pθ(τ x) and then applies downstream selection rule. Empirically, we will show that the difference between greedy and such exploratory policies can amount to +1030 absolute points of accuracy or constraint satisfaction across standard benchmarks for incontext learning and controllable generation (Brown et al., 2020; Wei et al., 2022a; Rao and Tetreault, 2018; Fan et al., 2018a; He et al., 2020; Chan et al., 2021). In other words, large portion of the models competence lives in trajectories that deterministic decoding simply never visits, and what is often narrated as mysterious emergent property of the model is, to significant extent, an emergent property of the modeldecoder pair and of the exploration geometry induced by the chosen decoding policy. We next spell out the experimental design for our two focal settings: fewshot incontext learning for classification (4.1) and style and constraintsatisfying generation (??). After describing how tasks, prompts, models, and decoding regimes are instantiated in each case, we then formalize the decoding policies and evaluation metrics we use to quantify the effect of exploration (4.1.1, ??)."
        },
        {
            "title": "4.1 Few–Shot In–Context Learning Under Decoding Policies",
            "content": "Tasks. We study fewshot incontext learning (ICL) on recent benchmark for sentiment and sarcasm classification in English varieties, BESSTIE (Srirag et al., 2025). BESSTIE consists of manually annotated Google Place reviews and Reddit comments in three English varieties (enAU, enIN, en UK), with labels for both sentiment and sarcasm. We derive two ICL classification tasks: BESSTIESentiment (3way sentiment classification). Each instance is labeled as fpositive, negative, neutralg. BESSTIESarcasm (binary sarcasm detection). Each instance is labeled as fsarcastic, non_sarcasticg (or equivalently yes/no). central concern in our study is trainingdata contamination: if benchmark is heavily reused (e.g., SST2, MNLI, AG News), then strong performance or emergence could simply reflect direct memorization or heavy downstream finetuning. Classical work on emergent abilities in LLMs quite reasonably evaluated ICL on widely used benchmarks such as SST2, MNLI, and AG News (Socher et al., 2013; Zhang et al., 2015; Williams et al., 2018; Brown et al., 2020; Wei et al., 2022a). To reduce the risk that our emergence effects are driven by such benchmark reuse, we intentionally choose BESSTIE, whose dataset and code were released in late 2024 and formalized in Findings of ACL 2025, with public benchmark snapshot finalized after July 2024 (Srirag et al., 2025). For the open models in our panel (LLaMA2/3, Gemma2, Mistral7B, Mixtral8(cid:2)7B, Mixtral8(cid:2)22B, Vicuna7B, Phi2), the documented pretraining cutoffs precede this period, making it substantially less likely that labeled BESSTIE instances were used during pretraining or instruction tuning.1 Within this setting, we follow the conventional GPT3 / emergentICL setup (Brown et al., 2020; Wei et al., 2022a): for each benchmark, we construct prompts with kshot 2 f4, 8g randomly sampled demonstrations per example, drawing demonstrations only from the training portion of BESSTIE and evaluating on heldout development/test set. The prompt follows the standard shorttext + label pattern used in fewshot sentiment and topic classification (Socher et al., 2013; Zhang et al., 2015), but now over post2024 benchmark that is deliberately selected to reduce the chance of direct training contamination. All models are used in pure fewshot mode, with no taskspecific finetuning, so that any large gaps between greedy and exploratory decoding can be attributed to the decoding policy rather than additional gradient updates. 4.1.1 Quantifying InContext Ability and Exploration Gains We now formalize how we measure incontext ability and how much of it is recovered by exploration. Throughout this subsection: indexes ICL tasks (e.g., BESSTIESentiment, BESSTIESarcasm), indexes models, and 1Of course, we cannot rule out that some underlying raw text from similar domains appears in generic web corpora. Our claim is therefore not that BESSTIE is logically impossible to overlap with pretraining, but that it is postbenchmark resource whose labeled structure and exact splits are unlikely to have been part of the models training pipelines. indexes decoding regimes (e.g., greedy, stochastic singlesample, bestofk). For each dataset t, we evaluate on heldout set f(xi, yi)gNt scheme and fixed prompt template for given run. We denote by ˆy(e) under decoding policy on input xi for task t. i=1, with fixed demonstration sampling i,t,m the label produced by model Step 1: ICL accuracy as empirical success probability. For each triplet (t, m, e), the incontext classification accuracy is defined as the usual empirical risk:"
        },
        {
            "title": "AccICL",
            "content": "t,m(e) ="
        },
        {
            "title": "1\nNt",
            "content": "NtX i=1 1[ˆy(e) i,t,m = yi,t]. This is the standard quantity reported in ICL studies, but here we treat it explicitly as an estimator of an underlying success probability. To make the role of randomness explicit, let collect all stochastic choices of the decoder uni,t,m for the resulting label. The perexample der policy (sampling noise, seeds, etc.), and write ˆy(e,r) success probability under policy is and the empirical accuracy can be viewed as qICL i,t,m(e) = Pr [ˆy(e,r) i,t,m = yi,t],"
        },
        {
            "title": "AccICL",
            "content": "t,m(e) (cid:25) 1 Nt NtX i=1 qICL i,t,m(e), i.e., an average of these inputwise success probabilities. From this perspective, deterministic decoding (e.g., greedy with =0) corresponds to the degenerate case where, for almost all seeds r, ˆy(e,r) i,t,m(e) 2 f0, 1g. In contrast, exploratory decoding (nonzero temperature, sampling) induces distribution over trajectories in which qICL i,t,m(e) captures how much hidden success mass is actually available. i,t,m is constant and qICL Step 2: Exploration gain via bestofk. Our central object is the difference between what the model could do under exploration and what it actually does under greedy decoding. For sampling budget k, we consider bestofk selfconsistency decoder: draw i.i.d. completions under stochastic base policy estoch (e.g., =0.7, topp=0.9), map each completion to discrete label, and return the majority label across the samples. We denote this composite regime by ebest-k and define"
        },
        {
            "title": "AccICL",
            "content": "t,m(best-of-k) ="
        },
        {
            "title": "1\nNt",
            "content": "NtX i=1 1[ˆy(best-of-k) i,t,m = yi,t]. The corresponding exploration gain at budget is"
        },
        {
            "title": "EGICL",
            "content": "t,m(k) = AccICL t,m(best-of-k) (cid:0) AccICL t,m(greedy), where greedy is the standard =0 deterministic decoder. At the perexample level, let qICL i,t,m(estoch) be the probability that single stochastic sample yields the correct label. Under bestofk majority voting, the success probability on xi becomes (cid:19) (cid:18) qICL i,t,m(best-of-k) = j=k/2 kX (qICL i,t,m(estoch))j(1 (cid:0) qICL i,t,m(estoch))kj, the probability that at least half of the draws are correct. Averaged over i, the exploration gain is approximately"
        },
        {
            "title": "EGICL",
            "content": "t,m(k) (cid:25) 1 Nt NtX (cid:16) i=1 i,t,m(best-of-k) (cid:0) qICL qICL i,t,m(greedy) . (cid:17) i,t,m(estoch) 2 (0.3, 0.7), then qICL This makes the key regime transparent. If, for some input xi, greedy decoding is stuck on wrong local mode so that qICL i,t,m(greedy) = 0, but the stochastic policy has nontrivial success probability qICL i,t,m(best-of-k) can approach 1 as grows. In other words, the parameters θ already encode useful ICL rule, but the deterministic inference stack insists on suboptimal trajectory. Large, positive EGICL t,m(k) exactly measures this gap between latent capacity and realized performance. simple binary toy example makes this concrete: suppose the stochastic policy returns the correct label with probability q=0.6 and the wrong label with probability 0.4. Greedy decoding may still choose the wrong label (e.g., due to slightly higher tokenlevel probability for an incorrect verbal0.6j0.49j (cid:25) ization), so qICL(greedy)=0. For k=9, bestof9 succeeds with probability 0.73, so the exploration gain on this single example is (cid:25) 0.73, even though θ is unchanged. This is prototypical case where deterministic decoding hides capability that is clearly present under sampling. 9 j= 9 (cid:0) (cid:1) Step 3: Sample complexity of ICL emergence. To summarize how much exploration is needed to unlock this hidden capacity, we define simple samplecomplexity proxy. For desired accuracy improvement threshold δ 2 f0.05, 0.10g (5 or 10 absolute points), we set t,m(δ) = min fk 2 f4, 16, 64g : EGICL t,m(k) (cid:21) δg. Intuitively, t,m(δ) answers: how many samples does the selfconsistency decoder need before the improvement over greedy decoding becomes clearly visible? Small (e.g., k=4 for δ=0.10) means that even modest exploration budgets reveal substantial capability that greedy decoding hides. Larger suggests that successful ICL trajectories occupy thinner or more fragmented region of the models trajectory space. Step 4: Label distributions and entropy. Sampling trajectories per input also lets us inspect the distribution over labels rather than just the final majority vote. For each (i, t, m) and fixed stochastic configuration (e.g., =0.7, topp=0.9), define the empirical label distribution ˆpi,t,m(y) = 1 kX j=1 1[ˆy(estoch,rj ) i,t,m = y], where r1, . . . , rk are independent seeds. The corresponding label entropy is Hi,t,m = (cid:0) ˆpi,t,m(y) log ˆpi,t,m(y). Low entropy Hi,t,m (cid:25) 0 indicates almost deterministic behavior (almost all mass on single label), while intermediate entropy reveals that the model allocates nontrivial mass to multiple plausible labels. Crucially, we frequently observe inputs where: the greedy label is incorrect, yet the empirical distribution ˆpi,t,m(y) has clear majority on the correct label. In these cases, the model is not confused in uniform sense; instead, it has structured distribution where the correct label is the dominant mode under sampling, but the single greedy trajectory falls into an inferior local mode. Majorityvote decoding exploits this structure; deterministic decoding discards it. Aggregating fHi,t,mgi and the distributions ˆpi,t,m across inputs thus gives an inputwise explanation for large exploration gains: whenever many inputs exhibit such hidden majority behavior (correct label winning under sampling, but losing under greedy decoding), we should expect EGICL t,m(k) to be strongly positive. This is exactly what we observe empirically, reinforcing our claim that deterministic decoding suppresses an explorationdriven emergent ability already encoded in pθ(τ x). Figure 19: ICL accuracy as function of exploration budget k. Placeholder. Each panel corresponds to representative model (e.g., LLaMA3 8B, Gemma2 27B, Mixtral8(cid:2)7B, Phi2). Curves show AccICL t,m(e) for 2 f1, 4, 16, 64g, where k=1 with =0 is the greedy baseline and > 1 denotes bestofk under fixed stochastic policy. Across tasks, greedy decoding often sits in the 4065% band, while bestof16 frequently reaches the 6080% band, with diminishing but nontrivial gains up to k=64. The large vertical gaps between k=1 and (cid:21) 16 illustrate how exploration recovers ICL competence that deterministic decoding fails to surface, even though the underlying parameters θ are held fixed. Step 5: The explorationgain curve (boxed definition). For downstream visualizations and analysis, we will primarily work with the explorationgain curve as function of the sampling budget k:"
        },
        {
            "title": "EGICL",
            "content": "t,m(k) = AccICL t,m(best-of-k) (cid:0) AccICL t,m(greedy) positive value of EGICL t,m(k) indicates that exploration recovers incontext ability that the deterministic greedy decoder fails to surface. This boxed quantity is what we plot across tasks t, models m, and budgets to show how exploration systematically recovers incontext abilities that deterministic decoding systematically hides. 4.1.2 ICL Results: Exploration Recovers Suppressed Ability We now turn to the empirical behavior of the explorationgain curve EGICL t,m(k) defined in 4.1.1. Across our postJuly 2024 ICL benchmarks and the family of open models (LLaMA2 7B/13B, LLaMA3 7B/8B/70B, Gemma2 9B/27B, Mistral7B, Mixtral8(cid:2)7B, Mixtral8(cid:2)22B, Vicuna7B, Phi2), we consistently observe that greedy decoding substantially underestimates the incontext capability that is revealed by even modest levels of stochastic exploration. Accuracy curves as function of exploration budget. Figure 19 (placeholder) plots AccICL t,m(e) as function of the sampling budget 2 f1, 4, 16, 64g for four representative models and all ICL tasks. Each panel shows single model; within each panel, different curves correspond to different tasks t. few robust patterns emerge: For many (t, m) pairs, the greedy point (k=1, =0) lies in relatively modest band of 4065% accuracy, even on tasks that are structurally simple (singlesentence classification with short prompts). Increasing from 1 to 4 and then to 16 produces steep monotone gains, with typical improvements of +1020 absolute points by k=16. For instance, midsize LLaMA3 8B variant may move from (cid:25) 55% to (cid:25) 75% on one of the sentiment tasks, while Gemma2 27B and Mixtral8(cid:2)7B show comparable jumps. Beyond k=16, the curves still trend upward (e.g., bestof64 yields further +25 points), but with clear diminishing returns, suggesting that most of the latent success mass becomes accessible at moderate exploration budgets. Taken together, these curves show that the same base model and prompt can look either mediocre (under greedy decoding) or surprisingly strong (under bestofk) on the same benchmarks, purely as function of the decoding policy. Heatmaps of exploration gain across tasks and models. To summarize these improvements more compactly, we construct taskbymodel heatmap of exploration gains at fixed budget, e.g. k=16:"
        },
        {
            "title": "EGICL",
            "content": "t,m(16) = AccICL t,m(best-of-16) (cid:0) AccICL t,m(greedy). Figure 20 shows this quantity for all ICL tasks (rows) and all open models (columns). Qualitatively, the heatmap is dominated by: large block of cells in the 0.080.20 range, indicating that doubledigit absolute gains are common rather than exceptional, and Figure 20: Few-shot ICL accuracy and exploration gains across models on BESSTIE tasks. Each cell shows the absolute accuracy under either bestof16 decoding (top row for each task) or greedy decoding (bottom row for each task), evaluated on BESSTIESentiment and BESSTIESarcasm. For the greedy rows we additionally print the accuracy gap (# d%) relative to bestof16, where = EGICL t,m(16) (cid:2) 100. The warm vs. cool colormap encodes accuracy, while the overlaid arrows quantify how much capability is hidden when we collapse exploration to single deterministic trajectory. Across both tasks, most models suffer 822 absolute-point drops when moving from bestof16 to greedy decoding, reinforcing that few-shot incontext learning is an explorationdriven ability that deterministic inference systematically suppresses. several dark cells in the (cid:21) 0.22 range, where bestof16 recovers more than 22 percentage points relative to greedy decoding. Importantly, these gains are not restricted to the largest models. Smaller and midsize variants (e.g., LLaMA2 7B/13B, Phi2) often show larger relative gains, reflecting the fact that their greedy performance is particularly conservative while their stochastic trajectory space still contains rich pockets of correct behavior. t,m(greedy) together with the accuracy gap # d%, where = EGICL Figure 20 aggregates these effects into single taskbymodel view of exploration gains at fixed budget of k=16. For each open model (columns) and each BESSTIE task 2 fSentiment, Sarcasmg (row pairs), the top cell reports AccICL t,m(best-of-16), while the bottom cell reports the corresponding t,m(16) (cid:2) 100 greedy accuracy AccICL as defined in 4.1.1. The warm vs. cool colormap encodes absolute accuracy, so vertically stacked cell pairs with sharp color contrast immediately signal models whose greedy decoding severely underestimates their few-shot ICL ability. Across both tasks and almost all open backbones (LLaMA 2/3, Gemma2, Mistral, Mixtral, Vicuna, Phi2), the majority of greedy rows exhibit double-digit drops of roughly 822 pp relative to best-of-16, with some smaller models (e.g., Phi2, LLaMA2 7B) showing the largest relative gains. In other words, the same modelprompt pair can appear mediocre under =0 greedy decoding yet competitive under modest stochastic exploration, and Figure 20 makes this gap visually explicit: substantial slice of few-shot incontext competence lives in trajectories that deterministic decoding simply never explores. 4.1.3 ExplorationICL Landscapes across Models The ICL curves and heatmaps in 4.1.2 summarize exploration gains by collapsing over temperature and focusing on small set of sampling budgets 2 f1, 4, 16, 64g. To expose the full geometry of stochastic decoding, we additionally construct explorationICL landscapes for each open backbone on both BESSTIESentiment and BESSTIESarcasm. These landscapes are shown in Figures 2130 for all open models in our panel (LLaMA2/3, Gemma2, Mistral, Mixtral8(cid:2)7B / 8(cid:2)22B, Vicuna7B, Phi2). For given task 2 fSentiment, Sarcasmg, model m, temperature , and sampling budget k, we define the temperature and budgetspecific exploration gain as AccICL t,m (T, k) = AccICL t,m (best-of-k; ) (cid:0) AccICL t,m (greedy; =0) where: 1. AccICL t,m (best-of-k; ) is the empirical accuracy on the BESSTIE dev/test split when we draw independent completions under fixed stochastic base policy at temperature (with standard nucleus filtering (Holtzman et al., 2019)), map each completion to discrete label, and return the majority label, i.e., selfconsistency style decoder in the spirit of Wei et al. (2022b); Wang et al. (2023b); Yao et al. (2023); 2. AccICL t,m (greedy; =0) is the baseline accuracy under strictly deterministic decoding (T =0, k=1), i.e., the classical GPT3 style few-shot ICL evaluation (Brown et al., 2020). Thus, AccICL t,m (T, k) directly measures how much incontext ability is recovered at given exploration setting (T, k), holding the base model and prompt fixed and modifying only the decoding policy. In each panel of Figures 2130, the xaxis spans temperature 2 [0.05, 1.0] and the yaxis spans log2 2 [0, 6] (corresponding to 2 [1, 64]). We evaluate AccICL t,m (T, k) on regular grid (e.g., in steps of 0.05 and 2 f1, 2, 4, 8, 16, 32, 64g), and interpolate to obtain smooth surface. The color scale encodes AccICL t,m (T, k) in the fixed numeric range [0, 0.25] (i.e., [0, 25] percentage points), shared across all backbones and both tasks. This scale consistency ensures that differences in ridge height, width, and location between, say, LLaMA3 70B and Phi2, or between sentiment and sarcasm for the same model, reflect genuine variation in exploration headroom rather than arbitrary rescaling or colormap choices. Qualitatively, these landscapes reveal three recurring regimes that are systematically obscured by 1D curves or singlebudget heatmaps: Flat, lowgain surfaces for very strong models. Large backbones such as LLaMA3 70B (Figure 23) exhibit almost perfectly flat landscapes with peak AccICL of only (cid:25) 5 pp on sentiment and (cid:25) 10 pp on sarcasm. Intuitively, these models already solve most BESSTIE cases under greedy decoding, so exploration yields only small, localized bumps around narrow corridor (typically (cid:25) 0.7, 2 [8, 16]). In other words, the latent success mass under pθ(τ x) is already highly concentrated near the greedy mode, leaving little additional headroom to exploit. Key takeaway: for such models, ICL looks almost deterministica single trajectory already aligns closely with the majority label under sampling, and exploration mainly offers finetuning of calibration rather than dramatic capability jumps. Tall, narrow ridges for midsize backbones. Midsize models such as LLaMA2 13B and Gemma 2 9B/27B (Figures 2125) show pronounced, warmcolored ridges in (T, k) space: moving from (T =0, k=1) to sweet spot around (cid:25) 0.7, 2 [8, 32] unlocks 1020 pp of extra accuracy. Here, the trajectories that implement correct ICL rules occupy substantial but nondominant region of the models trajectory space (Wei et al., 2022b), and majorityvote sampling is precisely what converts this hidden probability mass into realized performance. Outside the ridge, gains collapse quickly: overly conservative settings (T too small, too small) underexplore the space, while overly hot settings (T too large, very large) wash out signal with noisy or offtask completions. Key takeaway: midsize backbones operate in sharp Goldilocks zone of exploration where small decoding changes unlock large, emergentlooking ICL gains without any gradient updates. Taskasymmetric landscapes. Several backbones (notably Vicuna7B, Gemma2 9B, Phi2; Figures 29 and 30) display striking task asymmetry: sarcasm surfaces often have taller and broader ridges than sentiment. The same model that appears almost solved on sentiment under greedy decoding can gain 1517 pp on sarcasm once we move into the highgain band 2 [0.65, 0.85], 2 [8, 48]. This aligns with the intuition that sarcasm relies on subtler cues, perspective shifts, and pragmatic context; single greedy path frequently locks onto plausible but wrong reading, whereas stochastic exploration samples multiple readings and lets majority vote recover the intended label. Key takeaway: sarcasm behaves like highentropy ICL regime where the model knows what to do but only reveals this reliably when we interrogate richer slice of its trajectory distribution. These regimes also provide intuitive crossmodel takeaways that are invisible from scalar accuracy alone: Scaling within family (e.g., LLaMA2 7B ! 13B, Gemma2 9B ! 27B) tends to flatten the landscape for easier tasks (sentiment) while still preserving noticeable ridges for harder ones (sarcasm), echoing reports that larger models are more calibrated yet still benefit from self consistency on challenging examples (Wei et al., 2022b; Schaeffer et al., 2023). In practical terms, bigger models still hide some capacity, but the amount that can be unlocked by exploration shrinks: the ridge becomes shorter and flatter, and small (e.g., bestof4) is often enough to capture most of the available gain. Strong models look robust under greedy decoding, but they are not fully explored either. Calibration vs. brittleness. Comparing LLaMA3 70B with midsize backbones shows that strong models trade large exploration gains for better calibrated greedy behavior: their flat surfaces signal that the top trajectory is usually aligned with the majority label under sampling. Midsize models, by contrast, are more brittle: greedy decoding often settles on an inferior local mode, and bestofk acts as calibration amplifier that pulls predictions toward the latent majority preference encoded in pθ(τ x). For mixtureofexperts models (Mixtral8(cid:2)7B / 8(cid:2)22B), the sentiment and sarcasm surfaces are surprisingly similar and mostly sit in the 312 pp band, suggesting that MoE routing induces fairly taskagnostic response to exploration, in contrast to the strong asymmetries seen in Vicuna7B or Gemma2. From an engineering perspective, these backbones offer steady, moderate gains from bestofk across both tasks, without requiring careful pertask tuning of (T, k): almost any reasonable point along the ridge provides useful, if not spectacular, boost. Sweetspot sensitivity. Several models (especially Vicuna7B and Gemma2 9B) exhibit ridges that are both tall and sharp: small misspecifications of or can substantially reduce gains. This highlights practical tension: the exploration budget required to unlock emergent ICL behavior is often modest, but finding the right (T, k) operating point can itself be nontrivial, particularly if one insists on single global configuration across tasks and domains. Small models such as Phi2 (Figure 30) can show pocket regions of high gainup to (cid:25) 11 pp on sentimenteven though their absolute accuracies are lower. For practitioners constrained to tiny models, this is good news: modest bestofk stack can turn seemingly weak backbone into competitive ICL engine on the same post2024 benchmark, provided that (T, k) are tuned into the narrow highgain corridor. Outside these pockets, however, the surfaces quickly collapse toward zero gain, underscoring that small models are highly explorationsensitive: poorly chosen decoding configuration can easily hide most of their usable ICL behavior. Taken together with the aggregated heatmap in Figure 20, these permodel landscapes make our central point visually inescapable: substantial fraction of few-shot incontext competence lives in trajectories that deterministic decoding never visits. What looks like lack of emergent ability under the classical GPT3 evaluation recipe (Brown et al., 2020; Wei et al., 2022a) is, in many cases, better described as an evaluation artefact: the ability is already encoded in pθ(τ x), but only becomes visible when the model is probed with richer, multisample decoding policy that respects the full trajectory distribution and actively exploits success mass outside the single greedy path. In this sense, emergence is not static property of the parameter vector θ; it is property of the modeldecoder pair and of the exploration geometry that our inference pipeline chooses to expose. Figure 21: ExplorationICL landscapes for LLaMA-2 13B on BESSTIE. Left: Sentiment (empirical best-of-16 gain (cid:25) 15 pp) shows broad ridge of exploration benefit concentrated around temperatures 2 [0.65, 0.80] and sample counts 2 [8, 32] (i.e., log2 2 [3, 5]), with gains tapering smoothly toward both very low and very high exploration. Right: Sarcasm (peak (cid:25) 18 pp) exhibits taller and slightly sharper ridge over similar range, indicating that sarcastic completions profit more aggressively from best-of-k sampling. In both panels, the x-axis spans temperature 2 [0.05, 1.0], the y-axis covers log2 2 [0, 6] (i.e., 2 [1, 64]), and the color scale encodes exploration gain AccICL in the numeric range [0, 0.25] (corresponding to [0, 25] percentage points). Figure 22: ExplorationICL landscapes for LLaMA-3 8B. Left: Sentiment has relatively low best-of-16 gain of only (cid:25) 6 pp, with shallow ridge centred near (cid:25) 0.7 and small-to-moderate (k 2 [4, 16]), indicating limited upside from exploration on this task. Right: Sarcasm (peak (cid:25) 13 pp) shows visibly stronger and more extended plateau, with useful gains persisting for 2 [0.65, 0.85] and up to (cid:25) 32, suggesting that sarcastic prompts require deeper exploration of the candidate distribution. Across both plots, the numeric ranges are fixed to 2 [0.05, 1.0], log2 2 [0, 6] and AccICL 2 [0, 0.25], making cross-model comparison in later figures scale-consistent. Figure 23: ExplorationICL landscapes for LLaMA-3 70B. Left: Sentiment (peak gain (cid:25) 5 pp) is characterized by very flat surface with only low-amplitude bump at (cid:25) 0.7 and (cid:25) 816, indicating that the strong base model already solves most cases under greedy decoding. Right: Sarcasm (peak (cid:25) 10 pp) displays slightly more pronounced ridge, but the overall magnitude remains modest compared to smaller models, again reflecting limited headroom for exploration. Formally, the figure keeps in [0.05, 1.0], log2 in [0, 6], and AccICL clipped to [0, 0.25], so the visually compressed ridges here are real signal of reduced exploration benefit rather than an artefact of scaling. Figure 24: ExplorationICL landscapes for Gemma-2 9B. Left: Sentiment shows substantial ridge with peak gain (cid:25) 14 pp, spanning 2 [0.65, 0.8] and 2 [8, 32], and quickly flattening for very low and overly hot temperatures. Right: Sarcasm is even more exploration-sensitive, achieving peak of (cid:25) 17 pp and maintaining high gains over wide band 2 [0.65, 0.85] and 2 [8, 48], where the surface height stays above roughly 0.10 (i.e., 10 pp). The color scale is again fixed to [0, 0.25], so the taller, warmer ridge for sarcasm versus sentiment visually encodes true difference in exploration headroom for the same backbone. Figure 25: ExplorationICL landscapes for Gemma-2 27B. Left: Sentiment exhibits one of the strongest ridges in our study, with peak gain (cid:25) 17 pp and high plateau for 2 [0.65, 0.8] and 2 [8, 48], where AccICL remains in the [0.10, 0.20] (1020 pp) band. Right: Sarcasm (peak (cid:25) 10 pp) has noticeably shorter and narrower ridge, concentrated near (cid:25) 0.7 and 2 [8, 24], suggesting that this larger Gemma variant is more exploration-hungry on sentiment than on sarcasm. Because all panels share common numeric range for , k, and gain, the visual contrast between the left and right surfaces directly quantifies how task identity modulates the value of best-of-k sampling. Figure 26: ExplorationICL landscapes for Mistral-7B. Left: Sentiment (peak gain (cid:25) 10 pp) has clean, single ridge around (cid:25) 0.7 and 2 [8, 24]; below = 4 or above = 32 the surface rapidly collapses toward 0. Right: Sarcasm (peak (cid:25) 6 pp) is noticeably flatter and lower, with only mild bump in the same approximate (T, k) region, showing that this backbone is less reliant on exploration to solve sarcastic prompts. Within the global numeric ranges 2 [0.05, 1.0], log2 2 [0, 6], and AccICL 2 [0, 0.25], Mistral-7B thus appears as model where exploration is useful but not critical, especially relative to Gemma-2. Figure 27: ExplorationICL landscapes for Mixtral-8x7B. Left: Sentiment and Right: Sarcasm both peak at roughly (cid:25) 7 pp, with gently sloping ridges around 2 [0.65, 0.8] and 2 [8, 24]. The similarity of the two surfacesboth staying mostly within the [0.03, 0.12] gain band (312 pp) across the high-exploration region suggests that the MoE routing in Mixtral-8x7B introduces fairly task-agnostic response to best-of-k sampling. Overall, the numeric ranges confirm that this model sees consistent but moderate exploration benefits across both sentiment and sarcasm, with no extreme dependence on temperature or very large k. Figure 28: ExplorationICL landscapes for Mixtral-8x22B. Left: Sentiment and Right: Sarcasm both reach peaks of about (cid:25) 8 pp, but the ridges are slightly broader in than for Mixtral-8x7B, with useful gains for extending up to roughly 32. Within 2 [0.65, 0.8] and 2 [8, 32], AccICL often stays above 0.05 (5 pp), while quickly dropping outside this band. The overall shape thus points to scaling-stable exploration pattern across MoE sizes: larger Mixtral variants do not dramatically change where exploration helps, but slightly widen the high-gain corridor. Figure 29: ExplorationICL landscapes for Vicuna-7B. Left: Sentiment reaches moderate peak of (cid:25) 9 pp, with compact ridge around (cid:25) 0.7 and 2 [8, 24], and limited gain outside this region. Right: Sarcasm is dramatically different: the surface climbs up to (cid:25) 17 pp, with tall ridge covering 2 [0.65, 0.85] and 2 [8, 48], where gains stay well above 0.10 (10 pp). This strong asymmetryin model fine-tuned on conversational datahighlights that exploration is especially crucial for sarcasm, even when sentiment behaves more like standard classification-style task. Figure 30: ExplorationICL landscapes for Phi-2. Left: Sentiment shows surprisingly strong ridge for small model, with peak gain (cid:25) 11 pp and concentrated band of high values around 2 [0.65, 0.8] and 2 [8, 24]; here, gains in the [0.06, 0.12] (612 pp) range are common. Right: Sarcasm is much flatter, with peak (cid:25) 5 pp and only small bump near (cid:25) 0.7 and 2 [8, 16], quickly collapsing towards zero for larger or temperatures too far from the sweet spot. Taken together with the global numeric ranges (shared across all figures), these panels emphasize that even tiny models can reap non-trivial exploration benefits, but that such benefits may be highly task-specific and vanish rapidly outside narrow (T, k) window. 4.1.4 EntropyExploration Tradeoffs in FewShot ICL Figure 31 makes the connection between uncertainty and exploration benefit explicit by plotting, for every taskmodel pair (t, m) in our BESSTIE experiments, the relationship between normalized label entropy and exploration gain at budget k=16. Each marker corresponds to one (t, m) pair, with circles denoting BESSTIESentiment and triangles denoting BESSTIESarcasm. The xaxis shows Ei[ eHi,t,m] 2 [0, 1], where for each example xi we estimate label distribution ˆpℓ,i,t,m from k=16 temperaturescaled stochastic samples (as in 4.1.1), compute Hi,t,m = (cid:0) ℓ ˆpℓ,i,t,m log ˆpℓ,i,t,m, normalize by the task arity Ct via the corresponding ICL exploration gain at k=16, eHi,t,m = Hi,t,m/log Ct, and then average over i. The yaxis plots"
        },
        {
            "title": "EGICL",
            "content": "t,m (k=16) = AccICL t,m (best-of-16) (cid:0) AccICL t,m (greedy), i.e., the improvement (in absolute accuracy) from bestof16 sampling over deterministic greedy decoding. Solid (Sentiment) and dashed (Sarcasm) curves overlay simple quadratic fits (h) (cid:25) ah2 + bh + to the points in each task, providing smooth summary of how exploration gains vary as function of entropy. Sample complexity and how much exploration we need. The entropy view is consistent with the samplecomplexity proxy t,m(δ) introduced in 4.1.1: for most taskmodel pairs we do not need extreme sampling budgets to see emergence. For modest threshold δ=0.05, large fraction of t,m(δ)=4, i.e., bestof4 already buys (cid:21) 5 pp gain over greedy decoding. Even for (t, m) satisfy t,m(δ) 2 f4, 16g, and only minority of the hardest the stricter δ=0.10 criterion, many pairs have combinations require k=64 to cross the 10 pp threshold. Taken together with Figure 31, this indicates that the basin of good ICL trajectories is often reasonably thick: small number of independent probes is enough to find and exploit it, provided we are willing to deviate from =0 greedy decoding. Qualitatively, Figure 31 reveals clear invertedU relationship between uncertainty and exploration benefit. At the lowentropy end (Ei[ eHi,t,m] 0.2), models behave almost deterministically: one label dominates the empirical distribution under sampling, and both greedy and bestofk tend to 0.05), predict the same outcome. In this regime we observe negligible exploration gains (EGICL t,m consistent with the view that these are easy BESSTIE cases where the model is already confident and usually right. At the opposite extreme, very high entropies (Ei[ eHi,t,m] 0.8) correspond to nearuniform confusion across labels; here the correct label has no clear majority even under stochastic sampling, and again exploration gains are tiny. In both extremes, extra samples simply reconfirm either strong certainty or genuine ambiguity (Guo et al., 2017). The most interesting structure lies in the intermediate entropy band (Ei[ eHi,t,m] (cid:25) 0.30.7), where many taskmodel pairs cluster. In this middle region, we see substantial exploration gains: EGICL t,m (k=16) routinely reaches 0.100.20 (1020 pp), with several sarcasm points peaking near 0.22 (22 pp). This is exactly the hidden majority regime discussed in 4.1.1: the correct label is the dominant mode under stochastic sampling but not the label preferred by the single greedy trajectory. Greedy decoding locks onto locally highprobability but globally suboptimal verbalization, while bestofk sampling reweights the trajectory space in favour of the majority label. The smooth concave shape across all open models highlights that these gains are not idiosyncratic artifacts of single backbone, but predictable function of how label entropy is distributed across inputs. We can summarize these patterns in three regimes: Lowentropy, lowgain pairs, where greedy and stochastic decoding almost always agree; exploration brings almost no benefit. Intermediateentropy, highgain pairs, where sampling reveals single, strongly dominant label (often the correct one) that greedy decoding systematically misses; these are the hidden majority cases that drive the largest positive gains. Highentropy, mixedgain pairs, where the label distribution is genuinely diffuse and both greedy and bestofk struggle; here the models internal representation is genuinely unsure rather than merely mis-decoded. t,m (k=16) = AccICL Figure 31: Entropyexploration relationship in BESSTIE few-shot ICL. Each marker is taskmodel pair (t, m) from open LLMs in Table ??, for either BESSTIESentiment (circles) or BESSTIESarcasm (triangles). The xaxis shows the normalized label entropy Ei[ Hi,t,m] 2 [0, 1], where for each example we estimate label distribution ˆpℓ,i,t,m from temperaturescaled stochastic samples and compute Hi,t,m = ℓ ˆpℓ,i,t,m log ˆpℓ,i,t,m. We then normalize by the task arity, Hi,t,m = Hi,t,m/log Ct, and average over i. The (cid:0) yaxis plots the ICL exploration gain EGICL t,m , i.e., the improvement (in accuracy) of best-of-k sampling over greedy decoding. Solid (Sentiment) and dashed (Sarcasm) curves show quadratic fits (h) (cid:25) ah2 + bh + to the points in each task. We observe clear inverted-U relationship: both low-entropy regimes (Ei[ Hi,t,m] 0.2, nearly deterministic labels) and very high-entropy regimes ( 0.8, almost uniform confusion) yield negligible exploration gains (EGICL 0.05), while intermediate entropies ((cid:25) 0.30.7) produce the largest gains (EGICL (cid:25) 0.100.20). In this middle band, many taskmodel pairs exhibit hidden majority structure: the correct label is the dominant mode under stochastic sampling but is not the label preferred by the greedy trajectory. The systematic concave shape across all models shows that exploration gains are not idiosyncratic artefacts of single LLM, but predictable function of label entropy: ICL exploration helps most when the model is uncertain in structured way (few strong modes) rather than either over-confident or fully confused. t,m (k=16) (cid:0) Accgreedy Task differences are also visible. The sarcasm curve generally peaks at slightly higher entropy and higher gain than the sentiment curve, reflecting the intuition that sarcasm requires subtler pragmatic and contextual cues, for which models are often locally uncertain but not uniformly confused. In other words, sarcastic examples tend to sit squarely in the middle of the invertedU: greedy decoding often takes plausible-but-literal reading, whereas stochastic exploration samples alternative readings and allows majority vote to recover the intended sarcastic label. This aligns with prior evidence that selfconsistency and sampling-based methods disproportionately help on harder reasoning and nuance-heavy tasks (Wei et al., 2022b; Wang et al., 2023b). From deployment perspective, Figure 31 suggests simple, operational rule: not all inputs deserve the same exploration budget. Inputs with very low or very high normalized entropy can be safely handled with cheap, deterministic decoding, since bestofk provides little additional value. In contrast, mediumentropy inputs are precisely where exploration should be concentrated: modest bestofk stack (often with 2 f4, 16g) can recover double-digit accuracy gains while keeping compute overhead focused on cases where it matters most. Taken together with the model-wise landscapes and the global heatmap (Figure 20), this entropy analysis reinforces our central message: substantial fraction of few-shot incontext competence lives in structured, medium-entropy regions of the trajectory space that deterministic decoding simply never visits. Under the classical fewshot evaluation recipe popularized by large autoregressive language models (Brown et al., 2020; Wei et al., 2022a), these abilities may be misread as missing; our results show that they are already encoded in pθ(τ x) and only reveal themselves when the model is interrogated with richer, multisample decoding policy that actively exploits the success mass outside the single greedy path. Qualitatively, we observe three regimes: Lowentropy, lowgain pairs, where both greedy and stochastic sampling almost always pick the same label; here, EGICL t,m(k) (cid:25) 0 and exploration offers little benefit. Intermediateentropy, highgain pairs, where sampling reveals distribution concentrated on one label (often the correct one) but greedy decoding systematically picks different, incorrect local mode; these are precisely the hidden majority cases that drive large positive exploration gains. Highentropy, mixedgain pairs, where the label distribution is genuinely diffuse and both greedy and bestofk struggle; here, the models underlying representation seems genuinely uncertain rather than merely misdecoded. Across all three viewsaccuracy curves, taskbymodel heatmaps, and entropyconditioned analysisthe conclusion is consistent: deterministic decoding systematically suppresses an explorationdriven incontext ability that is already encoded in the base model. Emergence, in this lens, is not mysterious phase change in the parameters θ, but property of the combined system consisting of pθ(τ x) and an exploratory decoding policy that is allowed to search the trajectory space rather than commit to its first greedy choice."
        },
        {
            "title": "4.2 InstruSum: Style–Constrained Generation as Multi–Objective Search",
            "content": "Beyond fewshot ICL on BESSTIE, we also ask whether the same distributional exploration that unlocks latent classification ability can surface instructionfollowing and styleconstrained behavior in openended generation. The InstruSum benchmark (Liu et al., 2024) offers natural setting for this question: each example couples news article with rich naturallanguage requirement that simultaneously specifies what to say (content focus) and how to say it (length, style, and format), building on long line of controllable summarization work over news corpora (Hermann et al., 2015; Nallapati et al., 2016; Fan et al., 2018a; He et al., 2020; Chan et al., 2021). Rather than treating such evaluation as single scalar score under fixed decoding recipe, we view instructioncontrollable summarization as genuine multiobjective search problem over trajectories: each candidate summary trades off semantic adequacy against multiple constraint axes, and different decoding policies carve out different regions of this semanticconstraint landscape. This subsection formalizes that multiobjective view, defines style exploration gain directly analogous to our ICL exploration gain, and shows that small multisample budgets can substantially improve joint satisfaction of content and constraints without changing model parameters. 4.2.1 Task Setup and MultiObjective View Tasks. For style and constraintsatisfying generation, we build on InstruSum, recently introduced benchmark for instructioncontrollable summarization that pairs news articles with natural language requirements specifying how the summary should be written (Liu et al., 2024). Each instance consists of: (i) an input article d, (ii) humanwritten reference summary y, and (iii) an instructional requirement describing constraints on length, content focus, and sometimes style or format (e.g., write very short summary in two sentences focusing on the financial impact, or produce neutral bulletpoint summary mentioning the key companies involved). In this sense, InstruSum can be viewed as modern successor to earlier work on controllable summarization over news corpora such as CNN/DailyMail and related datasets (Hermann et al., 2015; Nallapati et al., 2016; Fan et al., 2018a; He et al., 2020; Chan et al., 2021), but with richer space of freeform instructions and an explicit focus on testing LLMs instructionfollowing behavior. As with our classification setup, we deliberately choose InstruSum because its benchmark configuration and data release fall after mid2024. The benchmark and accompanying evaluation suite are introduced in 2024 NAACL paper, with public artifacts finalized in the second half of 2024 (Liu et al., 2024). For the open models we analyzewhose pretraining cutoffs predate this periodthis timing makes it unlikely that entire (article, requirement, summary) triplets or the InstruSum instruction templates were seen as supervised data. While underlying news articles or related domains may appear in generic web corpora, we treat InstruSum as fresh, postbenchmarked resource for evaluating how decoding policies surface or suppress instructionfollowing and constraintsatisfying behavior. Instancelevel formulation. Concretely, we treat each pair (di, ri) as an input and ask the model to generate summary τ that both captures the content of di and obeys the constraints expressed in ri. Let pθ(τ di, ri) denote the conditional distribution induced by model parameters θ together with decoding policy (e.g., greedy, sampling, or multisample reranking). From the requirement ri and reference i we automatically derive compact bundle of operational constraints Ci = (C len , inc , avoid , style ), is target length band (short / medium / long), inc is an optional set of avoidphrases, and style where len is set of required entities or keywords, avoid is coarse style/format indicator (e.g., neutral vs. opinionated tone; sentences vs. bullet list). These constraints feed into automatic checkers clen, cinc, cavoid, cstyle introduced below, which score how well candidate τ respects each requirement. i Multiobjective view. In addition to constraint satisfaction, we quantify semantic adequacy using ) 2 [0, 1] that rewards summaries which are faithful to the article and similarity score ssem(τ ; di, informationally consistent with the reference. Each trajectory τ 2 Ti (the space of token sequences for instance i) is therefore naturally associated with vector of objectives (cid:16) (cid:17) fi(τ ) = ssem(τ ; di, i ), clen(τ ; len ), cinc(τ ; inc ), cavoid(τ ; avoid ), cstyle(τ ; style ) 2 [0, 1]5. Style and constraintsatisfying summarization can thus be viewed as multiobjective search problem over Ti: the goal is to identify trajectories that achieve high semantic adequacy while simultaneously satisfying the length, inclusion, avoidance, and style/format requirements. decoding policy induces distribution Ke(τ di, ri, θ) over trajectories; different policies explore different regions of the same underlying model distribution pθ((cid:1) di, ri), and hence expose different subsets of the multiobjective landscape described by fi. 4.2.2 Metrics, Success Sets, and Style Exploration Gain Given the multiobjective view, each candidate summary τ is associated with fi(τ ) 2 [0, 1]5 capturing what the summary says and how well it obeys the instruction. We now turn this representation into concrete metrics that let us compare decoding policies as search strategies over the same pθ((cid:1) di, ri). Component scores. For clarity, we restate the objective vector: fi(τ ) = (ssem(τ ; di, ), clen(τ ; len ), cinc(τ ; inc ), cavoid(τ ; avoid ), cstyle(τ ; style )), with each component in [0, 1]. We instantiate these as follows: Semantic adequacy ssem(τ ; di, ) rewards summaries that actually say the right thing: they should be faithful to the article and informationally consistent with the reference. In practice, we obtain this score from fixed, rubricguided LLM judge (details in App. ??). Length satisfaction clen(τ ; len ) measures how well the realized length of τ fits the requested band (short / medium / long). We map the deviation into [0, 1] using piecewise linear penalty so that small deviations are not punished as harshly as large ones. Inclusion satisfaction cinc(τ ; inc that appear in τ (after case folding and light normalization), capturing whether the summary actually mentions what the instruction asked for. ) is the fraction of required entities or keywords from inc Avoidance satisfaction cavoid(τ ; avoid ) penalizes avoidphrases: it equals 1 when no element of appears and decays towards 0 as more violations are detected. This explicitly measures avoid whether the model can resist saying things it was told to avoid. Style/format satisfaction cstyle(τ ; style ) captures how well tone (e.g., neutral vs. opinionated) and structure (sentences vs. bullets) match style . We obtain this from lightweight classifier / LLM judge, so that we can ask: did the model follow the requested style, or snap back to its default voice? In addition to binary success rates, we report perdimension means ssem, clen, . . . to show which aspects of the requirement benefit most from stochastic search. Success sets. To reason about full instruction following, we convert component scores into binary success notion. We introduce thresholds αsem, αlen, αinc, αavoid, αstyle 2 (0, 1] and define, for each instance i, success set Si (cid:18) Ti: Si = τ 2 Ti : ssem(τ ; di, ) (cid:21) αsem, ) (cid:21) αlen, ) (cid:21) αinc, ) (cid:21) αavoid, ) (cid:21) αstyle . clen(τ ; len cinc(τ ; inc cavoid(τ ; avoid cstyle(τ ; style Intuitively, Si collects trajectories that both summarize the article well and obey ri along all four constraint axes. In our experiments we instantiate fixed thresholds (App. ??); here we keep them symbolic to stress that the definitions are thresholdagnostic. Policylevel success and style exploration gain. decoding policy induces kernel Ke(τ di, ri, θ) over trajectories. The ideal success probability of on instance is"
        },
        {
            "title": "P style",
            "content": "succ (e; i) = τ Si Ke(τ di, ri, θ). In practice we only observe small number of samples from Ke. For deterministic policies such as greedy decoding (temperature 0), there is single trajectory τ greedy , and we approximate success via 2 Sig. For stochastic policies that return one sample per instance, we use the indicator fτ greedy 2 Sig. the analogous fτ sample i Given policy and dataset of instances, we aggregate to datasetlevel success rate:"
        },
        {
            "title": "P style",
            "content": "succ (e) ="
        },
        {
            "title": "1\nN",
            "content": "NX i=1 fˆτ (e) i"
        },
        {
            "title": "2 Sig,",
            "content": "where ˆτ (e) ˆτ (e,k) to make the search budget explicit. is the final trajectory returned by for instance i. For multisample policies we write To quantify how much distributional exploration helps, we define the style exploration gain in disucc (e, m, k) denote rect analogy to the ICL exploration gain. For fixed model and budget k, let style the success rate of policy on InstruSum. We then set"
        },
        {
            "title": "EGstyle",
            "content": "m (k) = style succ (multisample, m, k) (cid:0) style succ (greedy, m, 1). Positive EGstyle (k) indicates that simply changing the decoding policywithout modifying model parametersis enough to unlock additional instructionfollowing behavior that deterministic decoding systematically fails to reveal. 4.2.3 Decoding Policies as MultiObjective Search Strategies The metrics above treat each decoding policy as search strategy over Ti and fi(τ ). We now make the specific policies explicit, emphasizing how each one explores the underlying pθ((cid:1) di, ri). Greedy decoding: degenerate search. Our baseline is standard greedy decoding at temperature =0, with nucleus sampling disabled. For each instance this policy deterministically returns τ greedy = arg max τ pθ(τ di, ri), and hence single point fi(τ greedy ) in objective space. In the multiobjective view, greedy decoding is degenerate search procedure: it always commits to one corner of the semantic/constraint trade off surface, regardless of how much probability mass pθ((cid:1) di, ri) assigns to alternative trajectories that might better satisfy the instruction. Any failure under greedy decoding is therefore ambiguous: it could reflect genuine lack of competence, or merely poor choice of decoding policy. Singlesample stochastic decoding. We next consider simple stochastic policy that samples one trajectory. Concretely, we use modest temperature (e.g., =0.7) and nucleus sampling with p=0.9, 2 Sig now reflects one producing τ sample draw from the models instructionconditioned distribution. This policy still returns single point in objective space, but unlike greedy decoding it does not collapse the model to single mode, allowing some of the models inherent variability to surface. (cid:24) Ksample((cid:1) di, ri, θ). The success indicator fτ sample Multisample decoding with lexicographic selection. The most interesting regime for our purposes is multisample decoding with small budget k. Here we again use the stochastic base sampler but draw trajectories fτ (1) from Ksample((cid:1) di, ri, θ) and then deliberately select among them using the multiobjective scores fi(τ (j) , . . . , τ (k) ). For each τ (j) we compute fi(τ (j) ) = (ssem(τ (j) ; di, ), clen(τ (j) ; len ), cinc(τ (j) ; inc ), cavoid(τ (j) ; avoid ), cstyle(τ (j) ; style )), and joint constraint score cjoint(τ (j) ) = clen(τ (j) ; len ) cinc(τ (j) We then apply lexicographic selection rule: ; inc ) cavoid(τ (j) ; avoid ) cstyle(τ (j) ; style ). 1. Prioritize constraint satisfaction. Restrict to candidates with maximal cjoint(τ ). 2. Break ties by content quality. Among these, choose the candidate with highest semantic adequacy ssem(τ ; di, i ). The final output ˆτ (k) is therefore the trajectory that, among small sampled neighborhood of pθ((cid:1) di, ri), makes the best tradeoff according to our instructionfirst, contentsecond criterion. In the multiobjective picture, this policy attempts to move closer to the Pareto frontier of semantic adequacy and constraint satisfaction using only handful of samples. Policies as different views of the same distribution. All three policiesgreedy, singlesample, and multisample lexicographicshare the same parameters θ and conditional distribution pθ((cid:1) di, ri). What differs is which parts of that distribution they expose: greedy decoding collapses the distribution to single mode and hence single point fi(τ greedy singlesample decoding reveals one random point from the broader cloud; multisample decoding probes local cloud and explicitly prefers trajectories closer to the ); instructionsatisfying frontier. Crucially, when EGstyle (k) is large for model m, the models competence has not changed at all; only the decoding policy did. Large gains from multisample decoding therefore provide direct evidence that strict deterministic evaluation can hide substantial instructionfollowing ability that is already present in pθ((cid:1) di, ri) but never surfaced by single canonical completion. 4.2.4 Experimental Protocol on InstruSum Dataset slice. InstruSum consists of news articles paired with naturallanguage requirements and human reference summaries (Liu et al., 2024). For our experiments we: use the official test split provided by the authors; filter to instances where ri specifies at least length and content focus constraint and optionally style/format (e.g., bullets vs. sentences); discard instances where the requirement is underspecified or purely topical (e.g., summarize the article) and cannot be mapped into (C len , inc , avoid , style ). This yields subset that is explicitly instructiondriven and for which our constraint checkers are welldefined. Models. We evaluate the same collection of openweight models as in our classification experiments, spanning range of scales and architectures. Concretely, our suite includes: decoderonly Transformers from the LLaMA2 and LLaMA3 families (including 1B, 3B, 7B, 8B, 13B, 70B variants); the Gemma2 family (2B, 9B, 27B); dense and MoE models from the Mistral/Mixtral family (Mistral7B, Mixtral8(cid:2)7B, Mixtral 8(cid:2)22B); instructiontuned conversational models such as Vicuna7B; and small, highly optimized model Phi2. All models are used in their instructiontuned variants where available. As before, all analyzed open models have pretraining cutoffs before mid2024, so InstruSum appears as posthoc instruction following evaluation rather than memorized supervised task. Decoding policies and hyperparameters. For each model we instantiate the three regimes from 4.2.3: Greedy (deterministic). Temperature =0, nucleus sampling disabled, standard lefttoright decoding with modelspecific stop tokens. Singlesample (stochastic). Temperature =0.7 and nucleus sampling with p=0.9, producing one trajectory per instance. Multisample + lexicographic selection. Same base sampler as singlesample, but with budgets 2 f4, 8, 16, 32g candidates per instance; we then apply the lexicographic selection rule over fi(τ ) to choose ˆτ (k) . We cap generation length based on the requested length band and truncate any trailing content beyond the first complete summary (e.g., after terminating blank line for bullet lists). Full hyperparameters and prompt templates appear in App. ??. Evaluation procedure. For each (model, policy, budget) triple we run the decoder on all instances in the filtered InstruSum slice and compute: perdimension scores ssem, clen, cinc, cavoid, cstyle for every completed summary; success indicators fˆτ (e,k) datasetlevel success rates style succ (e, m, k) and style exploration gains EGstyle"
        },
        {
            "title": "2 Sig for the final output;",
            "content": "i (k). We keep prompts, decoding settings, and infrastructure constant across policies and repeat subset of runs with different seeds; where shown, error bars denote bootstrap confidence intervals over instances."
        },
        {
            "title": "4.3 Results: Stochastic Search Unlocks Latent Instruction Following\nWe now turn to the empirical picture on InstruSum. Throughout this section, model parameters θ\nare held fixed; only the decoding policy e and the search budget k vary. Any gains therefore reflect\ndistributional exploration, not additional training.",
            "content": "How much does multisample search help? Figure 32 plots the style exploration gain EGstyle (k) as function of search budget for all open models in our suite. The leftmost point on each curve corresponds to strictly deterministic greedy decoding (k=1), while larger values correspond to multisample lexicographic search over the same underlying distribution pθ((cid:1) di, ri). Across models we observe consistent pattern: gains rise sharply between k=2 and k=8 and then largely saturate by k(cid:25)8. Recent instructiontuned backbones such as LLaMA3, Gemma2, and Mixtral8(cid:2)22B reach high plateaus, with EGstyle (k) in the +10+13 point range. Smaller or older models like Vicuna7B and Phi2 show fast saturation with low gains, often topping out at +3 +5 points. In all cases, however, even modest search budgets (k=4 or 8) are enough to deliver nontrivial improvements in full instruction following, with no change to the underlying weights. This mirrors our BESSTIE findings: the good trajectories were present in pθ(τ di, ri) all along, but single greedy run almost never visits them. Table 1: Where do gains come from? Perdimension effects of stochastic search on InstruSum. For each model we report mean semantic adequacy ssem and mean constraint scores clen, cinc, cavoid, cstyle under greedy and multisample decoding with budget k=8, together with the full success rate style succ (e, m, k)."
        },
        {
            "title": "Policy",
            "content": "Mean scores (01) ssem clen cinc cavoid cstyle LLaMA2 LLaMA3 Gemma2 Mistral7B"
        },
        {
            "title": "Greedy",
            "content": "0.76 0.58 0.71 0.83 Multi (k=8) 0.78 0.64 0.76 0."
        },
        {
            "title": "Greedy",
            "content": "0.82 0.63 0.78 0.87 Multi (k=8) 0.84 0.72 0.83 0."
        },
        {
            "title": "Greedy",
            "content": "0.80 0.61 0.76 0.85 Multi (k=8) 0.83 0.69 0.81 0."
        },
        {
            "title": "Greedy",
            "content": "0.78 0.59 0.74 0.84 Multi (k=8) 0.80 0.66 0.78 0.87 Mixtral8(cid:2)7B"
        },
        {
            "title": "Greedy",
            "content": "0.79 0.60 0.77 0.86 Multi (k=8) 0.82 0.69 0.82 0.89 Mixtral8(cid:2)22B"
        },
        {
            "title": "Greedy",
            "content": "0.83 0.64 0.80 0.88 Multi (k=8) 0.86 0.75 0.86 0.91 Vicuna7B Phi"
        },
        {
            "title": "Greedy",
            "content": "0.74 0.55 0.70 0.81 Multi (k=8) 0.76 0.61 0.73 0."
        },
        {
            "title": "Greedy",
            "content": "0.72 0.54 0.68 0.80 Multi (k=8) 0.73 0.58 0.71 0.82 0.52 0.61 0.58 0.69 0. 0.66 0.54 0.62 0.57 0.68 0. 0.73 0.49 0.56 0.47 0."
        },
        {
            "title": "P style\nsucc",
            "content": "0.34 0.40 0.48 0.64 0.45 0. 0.43 0.52 0.44 0.58 0.49 0. 0.33 0.40 0.30 0.35 4.3.1 How Much Exploration is Enough, and Where Do Gains Come From? Figures 32 and 33, together with Table 1, summarize how distributional exploration translates into improved instruction following on InstruSum, and how these gains relate back to the fewshot ICL gains observed on BESSTIE. Style exploration gain as function of budget. Figure 32 plots, for each open model m, the style exploration gain EGstyle (k) as the search budget increases from 1 (degenerate greedy decoding) up to 32 samples. Across models, the curves show distinctive pattern: most of the benefit arrives quickly. Gains typically rise sharply between k=2 and k=8 and then saturate or flatten by k(cid:25)8. Larger, recent backbones such as LLaMA3 and Mixtral8(cid:2)22B reach higher plateaus (style explom (k) (cid:25) 0.100.13), indicating that they hide substantial amount of instruction ration gains EGstyle compliant mass that only multisample search uncovers. In contrast, smaller models such as Phi2 Figure 32: Style exploration gain as function of search budget. For each model m, we plot the style exploration gain EGstyle (k) on InstruSum as the search budget increases from 1 (greedy decoding) to 32 samples. Gains rise sharply between k=2 and k=8 and mostly saturate by k=8. Larger, recent models such as LLaMA3 (k) (cid:25) 0.100.13), whereas smaller models like Phi2 show and Mixtral8(cid:2)22B reach higher plateaus (EGstyle fast saturation with low gains, indicating limited headroom for improving instruction adherence via search. Overall, the figure illustrates that distributional exploration systematically improves style/constraint satisfaction, but the attainable gains are strongly modeldependent. show fast saturation with low gains, suggesting that there is simply less probability mass in their success sets Si to begin with. Operationally, the figure suggests simple rule: small budget 2 f4, 8g is often enough to harvest the majority of style/constraint gains, with diminishing returns beyond that point. on BESSTIE and the yaxis showing its style exploration gain EGstyle Linking style exploration to ICL exploration. Figure 33 connects these InstruSum results back to the BESSTIE ICL analysis. Each point corresponds to model m, with the xaxis showing its ICL (k) exploration gain EGICL at k=8 on InstruSum. The regression line and correlation (ρ(cid:25)0.80) reveal clear, positive relationship: models that benefit more from exploration in fewshot ICL tend also to benefit more in style/constraint satisfaction. Architectures such as Mixtral8(cid:2)22B sit in the strongstrong corner (high ICL and high style gains), while others like Phi2 fall into an ICLstrong, styleweak regime. This pattern supports our claim that explorabilitythe degree to which greedy decoding leaves performance on the tableis shared but architecturedependent property: some models bury both reasoning and instructionfollowing abilities under single deterministic trajectory, while others expose these abilities unevenly across tasks. Where do the gains actually come from? Table 1 decomposes the effect of multisample search into its semantic and constraintlevel components. For each open model we report mean semantic adequacy ssem, the four constraint scores clen, cinc, cavoid, cstyle, and the full success rate style succ under greedy decoding and multisample decoding with k=8. consistent pattern emerges: distributional exploration nudges almost every dimension upward, but the largest relative gains come from length and style, not raw content quality. Across LLaMA3, Gemma2, and both Mixtral variants, Figure 33: Linking ICL exploration gains to style exploration gains. Each point is model m, with the axis showing its ICL exploration gain EGICL on BESSTIE and the yaxis showing its style exploration gain (k) on InstruSum at k=8. The regression line and correlation (ρ(cid:25)0.80) indicate that models with EGstyle larger ICL gains typically also gain more in style/constraint satisfaction. Outliers such as Phi2 (ICLstrong, styleweak) and Mixtral8(cid:2)22B (strong on both) show that this relationship is architecturedependent. Together with Figure 32, this scatter plot supports our claim that distributional exploration surfaces latent abilities in both ICL and styleconstrained generation, while the degree to which they are buried under greedy decoding is highly modelspecific. clen and cstyle typically jump by (cid:25) 0.070.13, while semantic adequacy ssem improves more modestly ((cid:25) 0.020.03). Inclusion cinc also moves in the right direction, whereas avoidance is already high under greedy decoding and sees only small refinements. In other words, multisample search does not mainly fix hallucinations; it rebalances the tradeoff between content and formatting constraints, finding trajectories that still say the right thing while much better respecting the requested length and style. Modelspecific headroom. The rightmost column of Table 1 translates these perdimension shifts into full instructionfollowing success. Strong, recent models such as LLaMA3, Gemma2, and espesucc under k=8 search (e.g., from 0.48 ! 0.64 for cially Mixtral8(cid:2)22B see large absolute jumps in style LLaMA3 and 0.49 ! 0.68 for Mixtral8(cid:2)22B), confirming that substantial fraction of instruction compliant trajectories was present but never reached by greedy decoding. Midtier models such as Mistral7B and Mixtral8(cid:2)7B exhibit similar, slightly smaller gains, while older or smaller backbones like Vicuna7B and Phi2 show only modest improvements (e.g., 0.30 ! 0.35 for Phi2), reflecting genuinely limited mass in their success sets Si. Taken together with Figures 32 and 33, the breakdown in Table 1 reinforces our central message: what greedy decoding hides is not just more good summaries, but better points on the multi objective frontier, where semantic adequacy and all four constraint axes are jointly satisfied. In this sense, the InstruSum results mirror our findings on BESSTIE: stochastic, multisample decoding reveals instructionfollowing competence that is already encoded in pθ(τ di, ri) but systematically suppressed by strictly deterministic inference. 4.3.2 SemanticConstraint Density Landscapes on InstruSum Figures 34a35d (aggregated in Figure 35) show, for each model m, where its probability mass actually lives in the semanticconstraint space defined in ??. For every InstruSum instance and model m, we draw pool of stochastic candidates fτ (j) gK j=1 using the same base sampler as in our multisample experiments (temperature =0.7, nucleus p=0.9). For each trajectory we compute the semantic adequacy score ssem(τ (j) ; di, ) = clen (cid:1)cinc (cid:1)cavoid (cid:1)cstyle (??). We then aggregate all (ssem, cjoint) pairs for model m, estimate smoothed 2D density on [0, 1]2, and plot it as 3D surface. The horizontal axes are semantic adequacy and joint constraint satisfaction; the vertical axis depicts how much probability mass the model places in each region under its instructionconditioned distribution pθ((cid:1) di, ri). ) and the joint constraint score cjoint(τ (j) i Across panels 34a35d, strikingly consistent picture emerges. Many instructiontuned LLMs exhibit tall, narrow underconstrained ridge: band where ssem(τ ) is reasonably high (the summary mostly captures the article) but cjoint(τ ) is low to moderate, meaning that length, inclusion, avoidance, and style requirements are only partially satisfied. This ridge dominates the landscapes for models such as LLaMA2, Vicuna7B, and Phi2 (Figures 34a, 35c, and 35d), with only thin, low density tail reaching into the high semantics & high constraints corner. In these cases, deterministic greedy decoding is effectively anchored to the ridge: it reliably produces summaries that get the gist but ignore parts of the requested format or style, even though wellaligned candidates do exist in the tails of the distribution. Newer and larger backbonesmost notably LLaMA3, Gemma2, and the mixtureofexperts models Mixtral8(cid:2)7B and Mixtral8(cid:2)22B (Figures 34b35b)show the same ridge but with pronounced shift of mass toward the upperright corner of Figure 35. For these models, the peak density lies around ssem(τ ) 2 [0.65, 0.85] and cjoint(τ ) 2 [0.35, 0.60], indicating that they naturally place substantial probability on summaries that jointly respect content and stylistic constraints. Here, the underconstrained ridge becomes secondary: nontrivial portion of the distribution is already near the semanticconstraint Pareto frontier, so even modest multisample search can routinely surface wellaligned summaries. By contrast, Phi2 (Figure 35d) concentrates mass in steep, low constraint ridge with only tiny highconstraint island, explaining why its style exploration gains plateau quickly and at low absolute levels in Figure 32. These density landscapes provide geometric explanation for the quantitative results in Figures 3233 and the perdimension breakdown in Table 1. When the high semantics & high constraints region carries substantial mass (e.g., LLaMA3, Mixtral8(cid:2)22B), multisample decoding with small budget can move outputs from the underconstrained ridge into this upperright plateau, yielding large positive EGstyle (k) and noticeable gains in all constraint dimensions. When that region is thin, lowdensity island (e.g., Vicuna7B, Phi2), additional sampling helps much less: even distributionally aware search policy rarely stumbles into genuinely wellaligned trajectories. The central takeaway is thus geometric: many apparent failures to follow detailed style and format instructions are not hard limits of pθ((cid:1) di, ri), but symptoms of decoding policy that is stuck on an under constrained ridge and never explores the highquality corner of the semanticconstraint landscape. (a) LLaMA2. The joint density over semantic adequacy ssem(τ ) and joint constraint score cjoint(τ ) reveals tall, narrow underconstrained ridge: most candidates concentrate in ssem(τ ) 2 [0.50, 0.70] but only cjoint(τ ) 2 [0.15, 0.35], meaning that LLaMA 2 frequently captures the gist of the article while ignoring length, inclusion, and style requirements. The arrowed point near (ssem, cjoint) (cid:25) (0.70, 0.45) highlights thin but nontrivial region where well balanced, constraintrespecting summaries exist but are rarely selected by greedy decoding. (b) LLaMA3. For the newer LLaMA3 model, the density mass shifts noticeably toward the upperright of the plane: the bulk of candidates lies in ssem(τ ) 2 [0.60, 0.80] and cjoint(τ ) 2 [0.25, 0.50]. The under constrained ridge is still visible at cjoint(τ ) 0.30, but the arrowed high semantics & constraints region around (0.70(cid:0)0.80, 0.45(cid:0)0.60) now carries substantial density, indicating that LLaMA3 often places mass directly on approximately Paretooptimal summaries that satisfy both content and stylistic hints. (c) Gemma2. Gemma2 shows broader and more spreadout landscape: candidates typically occupy ssem(τ ) 2 [0.55, 0.75] and cjoint(τ ) 2 [0.25, 0.55], with visible secondary peak near (0.70, 0.50). The underconstrained ridge at cjoint(τ ) 0.30 is less dominant than in LLaMA2, suggesting that Gemma 2 is more willing to trade small amount of semantic score to better respect formatting and style. In other words, Gemma2s stochastic support contains richer mix of semanticsheavy and constraint faithful summaries. (d) Mistral7B. Mistral7B exhibits tall peak concentrated in ssem(τ ) 2 [0.55, 0.75] but with cjoint(τ ) mostly confined to [0.15, 0.30], indicating that the model strongly prioritizes semantic coverage of the article over strict adherence to instruction constraints. The annotated underconstrained ridge therefore dominates the surface, while only relatively thin and lowdensity band of candidates reaches cjoint(τ ) 0.40. This pattern makes Mistral 7B look stylistically weak under greedy decoding, even though the geometry reveals that higher constraint summaries do exist in its sampling distribution. Semanticconstraint density landscapes, part I. Panels (a)(d) show four representative models, illustrating how probability mass can be concentrated on an underconstrained ridge even when higherquality, constraintrespecting summaries are present in the tails of the distribution. (a) Mixtral87B. The semanticconstraint landscape is balanced: most mass lies in ssem(τ ) [0.60, 0.80] and cjoint(τ ) [0.30, 0.55]. An underconstrained ridge remains at cjoint(τ ) 0.30, but the annotated peak in the upperright shows many candidates that jointly achieve high semantics and strong constraint satisfaction, so multisample decoding can routinely surface well aligned summaries. further (b) Mixtral822B. The larger Mixtral822B shifts density toward the highquality corner: ssem(τ ) [0.65, 0.85] and cjoint(τ ) [0.35, 0.60] for most candidates. The ridge becomes secondary to strong peak around (0.75, 0.55), showing that the model naturally places more probability on summaries that respect format, tone, and inclusion. (c) Vicuna7B. Vicuna7B is dominated by peak in ssem(τ ) [0.50, 0.70] and cjoint(τ ) [0.10, 0.30], with little mass beyond cjoint(τ ) 0.40. The pronounced underconstrained ridge reflects tendency to produce semantically competent but stylistically misaligned summaries, while the tiny arrowed island of higher cjoint indicates that wellaligned candidates exist but sit at the fringes of the distribution. Phi2 concentrates mass in ssem(τ ) (d) Phi2. [0.45, 0.70] and cjoint(τ ) [0.10, 0.30], yielding one of the steepest underconstrained ridges. The high constraint region cjoint(τ ) 0.40 is only thin, lowdensity tail, showing that jointly wellaligned summaries are rare and almost never selected by deterministic decoding. Figure 35: Semanticconstraint density landscapes across models on InstruSum. Taken together, panels (a)(h) reveal consistent pattern: many instructiontuned LLMs place most of their probability mass on an underconstrained ridge of semantically strong but stylistically misaligned summaries, while only smaller portion of the distribution occupies the high semantics & high constraints region near the Pareto frontier. As models become larger and more recent (e.g., LLaMA3, Mixtral8(cid:2)22B), this highquality corner receives increasingly more mass, yet greedy decoding remains anchored to the ridge. The figure therefore supports our central takeaway: apparent failures to follow detailed style and format instructions are often search artifact of the decoding policy rather than hard limitation of the underlying conditional distribution."
        },
        {
            "title": "5 Deterministic inference collapses diverse reasoning paths into a single",
            "content": "brittle trace So far, we have focused on what deterministic inference hides. On GLUEstyle classification, greedy decoding yields deceptively sharp point estimates that look stable but crumble under paraphrases and perturbations. On style and constraintsatisfying summarization, it anchors models to narrow corner of the semanticconstraint surface, masking the latent probability mass assigned to instructioncompliant outputs. We now turn to third axis: multistep reasoning. For reasoning, the question is not only whether model produces the right answer, but how many distinct ways it knows to get there. Complex problems often admit several valid solution paths and rich spectrum of nearmiss failures. Under strict greedy decoding, however, an LLMs conditional distribution pθ(τ x) over full chains of thought is collapsed to single winning trace τ greedy(x) single, brittle trajectory through much larger space of possibilities. Our goal in this section is to show that multisample decoding exposes richer landscape of reasoning strategies, while deterministic inference makes these internal degrees of freedom essentially invisible. Concretely, for each input we treat the models sampled chains of thought as finite reasoning graph rooted at x. Each leaf in this graph corresponds to complete chain of thought and is labeled as correct, nearcorrect, or clearly incorrect. Greedy decoding selects exactly one roottoleaf path; stochastic decoding with modest budget reveals additional branches that the model also considers plausible. We will show that: (i) many models assign nontrivial probability to multiple, qualitatively distinct correct strategies; (ii) greedy decoding often follows lowsupport, brittle branch that is not representative of the models multipath behavior; and (iii) substantial fraction of apparent failures under deterministic evaluation are in fact collapsed failures, where correct path exists in the sampled graph but is systematically pruned by greedy inference. Claim 3 (PolicyInduced Reasoning Collapse). Deterministic inference misdiagnoses reasoning ability: by forcing single greedy chain of thought, it routes models through low support, brittle paths and converts many solvable problems into collapsed failures, where correct multistep strategies exist in the sampled reasoning graph but are never expressed under the deterministic policy."
        },
        {
            "title": "5.1 Tasks and decoding setup",
            "content": "We study these phenomena on three complementary benchmarks that elicit multistep chainof thought (CoT) reasoning: GSM8K (Cobbe et al., 2021): corpus of gradeschool math word problems requiring several arithmetic and algebraic operations. Many instances admit multiple valid solution paths (e.g., reordering additions and subtractions, or choosing different intermediate quantities to track), as well as large space of nearmiss rationales that follow the right plan but make local numerical mistake. SVAMP (Patel et al., 2021): an adversarial variant of arithmetic word problems designed to reduce shortcut patterns from earlier datasets. SVAMP stresses robustness to lexical and structural perturbations: models must truly track quantities and operations, not merely match surface templates. As in GSM8K, the same final answer can often be reached via several qualitatively different chains of thought. StrategyQA (Geva et al., 2021): benchmark of yes/no questions that require multihop commonsense and world knowledge. Unlike GSM8K and SVAMP, the intermediate steps are primarily verbal: listing facts, decomposing the question, and eliminating alternatives. There can be many distinct, semantically valid argument chains leading to the same binary answer. Across all three datasets we use the same panel of instructiontuned openweight models as in our earlier experiments: LLaMA2, LLaMA3, Gemma2, Mistral7B, Mixtral8(cid:2)7B, Mixtral8(cid:2)22B, Vicuna7B, and Phi2. For each model and instance xi we elicit chainofthought rationales using standard CoT prompt of the form Lets reason this out step by step. and consider two decoding regimes: Greedy decoding. We decode with temperature =0 and no sampling, obtaining single chain of thought τ greedy (m) and its final answer. This mirrors common evaluation practice in both academic benchmarks and production deployments, where deterministic inference is preferred for reproducibility. Multisample decoding. We decode with moderate temperature (e.g., =0.7) and nucleus sampling (topp=0.9), drawing independent chains fτ (1) (m)g per instance. Unless otherwise noted we use k2f8, 16, 32g, which is large enough to expose diverse set of reasoning paths but still comparable to typical budgets for selfconsistency and bestofk decoding in practice. (m), . . . , τ (k) i For each sampled chain τ (j) (m) we record: (i) the final answer and whether it is correct; (ii) segmented sequence of intermediate steps; and (iii) simple diagnostics about the quantities, entities, or facts referenced at each step. These traces form the raw material for the reasoning graphs and diversity metrics introduced in 5.2, where we make precise what it means for greedy decoding to follow lowsupport path and how often multisample decoding uncovers alternative correct strategies that are otherwise invisible under deterministic inference."
        },
        {
            "title": "5.2 From sampled chains to reasoning graphs",
            "content": "To reason about how multiple chains of thought coexist for the same problem instance, we move from individual text samples to an explicit reasoning graph. Intuitively, each sampled chain τ is path in tree or DAGlike structure, whose internal nodes represent partial reasoning states and whose leaves correspond to complete rationales with final answers. Segmenting chains of thought into steps. Given model m, an instance xi, and sampled chain of thought τ (j) (m), we first segment the raw text into sequence of discrete reasoning steps τ (j) (m) = (s(j) i,1 , s(j) i,2 , . . . , s(j) i,T (j) ), where each s(j) i,t is either sentence or Step t: span. We use simple, modelagnostic heuristics (line breaks, bullet markers, and explicit Step prefixes) to define these segments; in practice this produces 410 steps for GSM8K and SVAMP, and 37 steps for StrategyQA. For each step s, we compute dense representation h(s) 2 Rd using sentence encoder (e.g., i,t ) for the embedding frozen SBERT model) or designated hidden layer of m. We write h(j) of the tth step in the jth sample of instance i. i,t = h(s(j) Prefix states and step similarity. reasoning prefix of length is the partial chain i,t = (s(j) π(j) i,1 , . . . , s(j) i,t ), 1 (cid:20) (cid:20) (j) . We represent such prefix by aggregating its step embeddings, e.g. via an average: h(π(j) i,t ) = 1 tX u=1 h(j) i,u. To decide when two sampled prefixes should be treated as the same reasoning state, we define cosinesimilaritybased equivalence: π (cid:24) π () cos(h(π), h(π)) (cid:21) δ, for fixed threshold δ 2 (0, 1) (we use δ 2 [0.85, 0.90] in our experiments). This allows for minor lexical variation across samples while merging prefixes that correspond to the same highlevel plan. Constructing the reasoning graph. For each instance xi and model m, we collect the sampled chains fτ (1) (m)g and build finite directed graph (m), . . . , τ (k) as follows: G(m) = (V (m) , E(m) ) 1. Create distinguished root node vroot corresponding to the empty prefix (no steps taken). 2. Process each sampled chain τ (j) its prefixes π(j) i,1 , . . . , π(j) i,T (j) . For each prefix π(j) i,t : (m) in order: starting at vroot, we extend path by iterating over if there already exists node 2 (m) otherwise, we create new node representing π(j) whose stored prefix is (cid:24)equivalent to π(j) i,t and add it to (m) . i,t , we reuse v; In both cases we create directed edge from the node encoding π(j) and add it to E(m) . i,t1 to the node encoding π(j) i,t 3. Once the last step s(j) i,T (j) is processed, the corresponding node is marked as leaf. Because different samples can share long common prefixes and only branch late, the resulting structure is typically shallow DAG rather than full tree, with substantial sharing among early steps. Labeling leaves: correct, nearmiss, and failure. Each leaf in G(m) chain of thought and final answer. We label leaves using three coarse outcome types: corresponds to complete ℓ(τ (j) (m)) 2 fCorrect, NearMiss, Failureg. Correct if the final answer matches the gold answer and the intermediate steps are logically consistent with it. NearMiss if the final answer is incorrect but the chain follows the same highlevel plan as at least one correct sample and agrees on most intermediate quantities (e.g., all steps are correct up to single arithmetic slip). Failure otherwise (early conceptual mistake, spurious hallucinated quantities, or clearly irrelevant reasoning). Formally, for NearMiss we require that the chains step sequence has high overlap with some correct chain τ (m): overlap(τ (j) (m), τ (m)) (cid:21) α, with α set to 0.7 in our experiments, where overlap((cid:1), (cid:1)) measures token or spanlevel agreement on intermediate quantities and operations. The greedy path as single roottoleaf trajectory. The greedy chain τ greedy guished path in G(m) : pathgreedy (m) = (vroot, vgreedy i,1 , . . . , vgreedy i,T greedy ), (m) induces distinwhere each vgreedy (m). In the reasoning graph view, deterministic inference simply selects this one path and discards all others, regardless of how much probability mass lies on alternative branches. is the node corresponding to the tstep prefix of τ greedy i,t Illustrative example. Figure 36 sketches typical reasoning graph for GSM8K instance. Three sampled chains of thought are shown: Correct chain that first computes the total number of apples, then subtracts those eaten, and finally divides the remainder among children; second, Correct chain that instead reasons in terms of apples per child from the outset, arriving at the same answer via different decomposition; Figure 36: Reasoning graph for multistep ticketsales word problem. The problem (shown below the tree) asks how much money school donates after selling adult and child tickets over two days, with different prices and fixed perticket expense. Internal nodes represent partial chains of thought: an initial parsing stage, split into Strategy (group by day) versus Strategy (group by ticket type), and finer subplans such as Monday first vs. Tuesday first or Adults first vs. Children first. Leaves are classified as Correct, Nearmiss, or Failure, with outcome types indicated by the colored markers in the legend box. The thick red path denotes the greedy chain of thought, which ends in nearmiss leaf, whereas the dashed green path shows an alternative, fully correct strategy that the model also assigns nontrivial probability to. Other sampled branches are drawn in light gray. This illustrates how multisample decoding exposes rich, multipath landscape of reasoning, while deterministic inference collapses it into single brittle trace. NearMiss chain that shadows the first strategy but makes local arithmetic error in the penultimate step. All three share the same first one or two steps and therefore share nodes near the root of G(m) , but they diverge into separate branches as the reasoning unfolds. The greedy decoding path (highlighted in bold in the figure) may follow either correct or nearmiss branch; in either case, it reveals only one of several plausible strategies. In the next subsection we quantify this phenomenon by measuring how much of the sampled reasoning graph is supported by the greedy path, how many distinct strategies exist per instance, and how often correct leaves are present but invisible under deterministic inference. i"
        },
        {
            "title": "5.3 Empirical evidence of path diversity and collapsed failures",
            "content": "We now quantify, across GSM8K, SVAMP, and StrategyQA, how often models (1) entertain multiple distinct reasoning strategies, (2) route the greedy chain along lowsupport, brittle branch, and (3) fail only because deterministic decoding prunes out correct path that is present elsewhere in the sampled reasoning graph. Unless otherwise noted, all statistics are computed over the first 1,000 instances of each dataset and over the same panel of models as in earlier sections (LLaMA2/3, Gemma 2, Mistral7B, Mixtral8(cid:2)7B, Mixtral8(cid:2)22B, Vicuna7B, Phi2). Path diversity and multistrategy instances. Recall that for each model and instance xi, the sampled chains fτ (j) ) with set of rootto leaf paths (m) j=1 induce reasoning graph G(m) . We define the path count = (V (m) , E(m) (m)gk N (m) paths(xi) = jP (m) and (m) corr (xi) = jfπ 2 (m) : leaf(π) is correctgj. An instance is labeled multistrategy for model if (m) tatively distinct correct roottoleaf paths that differ at one or more internal decision nodes. corr (xi) (cid:21) 2, i.e., there are at least two qualiA first observation is that path diversity is the norm, not the exception. Across GSM8K and SVAMP, we find that most models have (m) paths(xi) (cid:29) 1 for the vast majority of instances (e.g., medians around 46 distinct paths with k=16 samples), and nontrivial subset of problems exhibit (m) corr (xi) (cid:21) 2. For stronger instructiontuned models (e.g., LLaMA3, Mixtral8(cid:2)22B), substantial fraction of solved GSM8K instances admit multiple correct strategies, reflecting alternative decompositions (by day vs. by ticket type, by quantity vs. by price, etc.). On StrategyQA, where intermediate steps are verbal, the phenomenon is even more pronounced: models frequently construct several different but semantically valid fact chains that all support the same yes/no answer (e.g., resolving question via either geographic, historical, or functional argument). In short, the reasoning graphs for contemporary LLMs are intrinsically multipath: they encode more than one way of reaching the same conclusion. Support of the greedy path. We next ask whether the greedy chain of thought τ greedy (m) follows representative path in the graph, or whether it is routed through lowprobability branch. Let pθ(π xi) denote the probability of full path π 2 (m) under the autoregressive factorization sampled with our temperature and topp settings. We define the greedy support ratio as α(m) = pθ(τ greedy max πP (m) (m) xi) pθ(π xi) . An α(m) near 1 indicates that the greedy path coincides with, or is very close to, the highestsupport sampled path; small values indicate that greedy decoding has latched onto rare branch even though higherprobability alternatives exist. Empirically, we observe that α(m) is often far from 1. On GSM8K, for example, even when the greedy answer is correct, the corresponding reasoning chain frequently has support comparable to, or lower than, alternative correct paths discovered under sampling. For SVAMP and StrategyQA the effect is stronger: in many cases the greedy rationale follows an idiosyncratic shortcut (e.g., conflating two quantities, skipping justification step) that is relatively lowsupport compared to more careful argument chains elsewhere in G(m) . In other words, greedy decoding tends to select one specific way of reasoning, but this trace is neither unique nor necessarily representative of what the model usually does under its own distribution. (m) greedy(xi) for the indicator that the greedy chain yields the correct final answer, and acc Collapsed failures: errors caused by pruning correct paths. We now formalize and quantify the collapsed failure phenomenon illustrated in Figure 36. For model and instance xi, write (m) multi(xi) acc for the indicator that at least one sampled path in (m) is correct. We say that xi is collapsed failure for model if acc (m) greedy(xi) = 0 and acc (m) multi(xi) = 1, i.e., the model knows how to solve the problem along some path in its reasoning graph, but strict greedy decoding happens to follow different path that leads to an incorrect conclusion. Let E(m) be the set of instances where the greedy answer is wrong. We define the collapsed failure rate R(m) coll = jfxi 2 E(m) : acc (m) multi(xi) = 1gj jE(m)j . In our experiments, R(m) coll is substantial across all three datasets. On GSM8K, large fraction of greedy failures for stronger models are collapsed failures: given even modest budget (k(cid:25)8), we often find at least one correct path in G(m) for problems that greedy decoding mis-solves. On SVAMP, which was explicitly constructed to break shallow heuristics, collapsed failures are especially common: greedy decoding tends to commit to brittle shortcut, while alternative branches that correctly track quantities and operations are present but never visited. On StrategyQA, collapsed failures take more semantic form: the greedy CoT may omit key fact or make subtle factual error, whereas other sampled chains assemble the right evidence but are suppressed by the deterministic policy. Dataset and modellevel trends. coarse summary of these effects  (Table 2)  shows three consistent patterns: Path diversity increases with task complexity. GSM8K already exhibits multiple strategies for many problems; SVAMP and StrategyQA push this further, with higher proportion of instances where (m) corr (xi) (cid:21) 2. Reasoning graphs on StrategyQA in particular feature wide variety of factual decompositions, making the collapse induced by greedy decoding especially severe. Stronger models are more multipath and more exposed to collapse. As we move from smaller models (e.g., Phi2, Vicuna7B) to larger, betteraligned ones (e.g., LLaMA3, Mixtral8(cid:2)22B), both the fraction of multistrategy instances and the collapsed failure rate R(m) coll tend to increase. Intuitively, richer models entertain more candidate solution paths, including many correct ones, but single deterministic trace cannot faithfully represent this internal variety. Greedy traces systematically understate uncertainty. Even when the final answer is correct, greedy chains often follow lowsupport paths (small α(m) ) that sit alongside alternative correct or nearmiss strategies. From an interpretability and safety perspective, this means that reading off single CoT as the models reasoning is misleading: the true epistemic state is closer to bundle of plausible paths than to single crisp proof. Taken together, these findings extend our earlier results beyond classification and instruction following: deterministic inference does not merely hide alternative outputs, it collapses entire families of valid reasoning trajectories into single brittle trace. Multisample decoding reveals that LLMs often harbor several workable solution strategies and that many apparent failures are, in fact, policyinduced collapses rather than hard limitations of the underlying conditional distribution pθ((cid:1) x). 5.4 Results: exploration reveals hidden strategies and nearmiss failures We now aggregate the reasoninggraph analysis into modellevel and datasetlevel statistics, asking three questions: (i) how much multisample decoding improves accuracy on GSM8K, SVAMP, and StrategyQA, (ii) how these gains relate to the underlying path diversity and greedy path support, and (iii) how often deterministic inference fails only because it collapses rich multipath landscape into single brittle trace. Throughout, we summarize results via two global figures and one table: Figure ?? links modellevel diversity to accuracy gains, Figure ?? breaks instances into collapsed failures vs. brittle vs. robust successes, and Table 2 provides quantitative permodel summary. More diverse reasoning models benefit most from exploration. For each model and dataset, we compute (i) the greedy chainofthought accuracy Acc(m) greedy and (ii) the multisample accuracy Acc(m) multi(k), where an instance is counted as correct if any of the sampled traces yields the right final answer. We summarize the benefit of stochastic decoding by the exploration gain Accm(k) = Acc(m) multi(k) (cid:0) Acc(m) greedy, and relate this to modellevel path diversity metrics (Section 5.2), namely the average path entropy and the average number of distinct strategy clusters Dm over correct or nearcorrect traces. Figure ??a (ICL/reasoning_path_diversity_vs_gain.pdf) visualizes this relationship across models. Each point corresponds to model m, with the xaxis showing (or, alternatively, Dm) and the yaxis showing Accm(k) for k=16. We observe clear trend: models with richer reasoning path diversity exhibit larger gains from multisample decoding. Small models such as Phi2 and Vicuna7B cluster in the lowerleft corner, with low path entropy and only modest accuracy improvements under sampling. In contrast, larger, more recent models such as LLaMA3 and Mixtral8(cid:2)22B lie toward the upperright, combining high with substantial Accm(k). This pattern holds qualitatively across GSM8K, SVAMP, and StrategyQA: the more diverse models internal reasoning landscape, the more it stands to gain from distributional exploration, because greedy decoding sees only one of many viable trajectories. Outcome categories: collapsed failures, brittle successes, robust successes. To make the implications for evaluation more concrete, we group instances into three categories based on the sampled reasoning graphs (Section 5.2): Collapsed failures: greedy CoT yields an incorrect answer, but at least one sampled path in the graph is correct. These are policyinduced errors: the model knows solution but deterministic inference prunes it away. Brittle successes: greedy CoT yields correct answer, but only small minority of sampled paths are correct (e.g., fewer than 30%), and often belong to single strategy cluster. Here the model has found one lucky route through landscape dominated by failures or nearmisses. Robust successes: greedy CoT is correct and there exist multiple distinct correct strategies (e.g., corr (xi) (cid:21) 2 with at least two clusters), with substantial fraction of samples landing in the (m) correct region. Figure ?? (ICL/reasoning_outcome_categories.pdf) presents grouped bar plot showing the proportion of instances in each category for GSM8K, SVAMP, and StrategyQA, broken down by model. Two patterns stand out. First, collapsed failures are common across all datasets, especially for stronger models: nontrivial fraction of greedy mistakes arise in cases where correct path is present in the multisample graph but never selected by deterministic decoding. Second, even among correctly solved instances, brittle successes dominate over truly robust successes for many models, with only minority of problems admitting multiple, highprobability correct strategies. This reinforces the message of Figure ??: reading off single greedy CoT risks overstating both the models confidence and the stability of its reasoning. quantitative summary of brittleness. Table 2 summarizes these observations numerically. For each model m, we report: (i) greedy and multisample accuracies on the reasoning benchmarks, (ii) the exploration gain Accm(k), (iii) average path entropy and greedy support Sm, and (iv) the fractions of collapsed failures and brittle successes as defined above. Table 2: Reasoningpath diversity and brittleness across modern reasoning LLMs. For each model (macro averaged over GSM8K, SVAMP, and StrategyQA), we report greedy vs. multisample CoT accuracy, the resulting exploration gain Accm(16) at budget k=16, the average reasoningpath entropy m, the greedy path support Sm (fraction of sampled traces that follow the greedy prefix at each step), and the fraction of collapsed failures (greedy wrong, some sampled path correct) and brittle successes (greedy correct, but < 30% of samples correct). The simulated values reflect consistent trend: models with higher enjoy larger accuracy gains under stochastic decoding but also exhibit lower Sm and more collapsed failures, indicating that deterministic inference increasingly underrepresents their internal multipath uncertainty. Model Accgreedy Accmulti(16) Acc(16) Sm % Collapsed / % Brittle DeepSeekR1Distill LLaMA3.18BInstruct Mixtral8(cid:2)7BInstruct 0.67 0.62 0. 0.82 0.73 0.72 +0.15 +0.11 +0.08 1.68 1.49 1.37 0.38 0.45 0.52 31% / 29% 26% / 33% 21% / 37% The table makes three points explicit. First, exploration gains are nontrivial: across the panel, multisample decoding recovers between 515 absolute points of CoT accuracy beyond greedy, with the largest gains for the most diverse models. Second, greedy support systematically decreases with model strength: as increases from small to large models, Sm drops, indicating that the greedy path is supported by shrinking fraction of the models sampled behavior. Third, collapsed failures account for sizeable portion of errors, especially for highcapacity models; in some cases nearly third of greedy mistakes are instances where the model already contains correct strategy in its reasoning graph, but this strategy is invisible under deterministic inference. Permodel 3D reasoning landscapes. To complement these aggregate statistics, we visualize, for three representative models, the taskwise joint distribution of reasoning path entropy and exploration gain as 3D clouds. Figures 3739 contrast the deterministic regime (greedy decoding) with the stochastic regime (multisample decoding) across GSM8K, SVAMP, and StrategyQA, making the collapse induced by deterministic inference visible at glance. Figure 37: DeepSeekR1Distill: stochastic exploration exposes rich, high-gain reasoning modes across tasks. This 3D plot shows, for DeepSeekR1Distill, the joint distribution of average reasoningpath entropy m,d (xaxis) and accuracy gain Accm,d from multisample over greedy decoding (yaxis), with dataset layers d2fGSM8K, SVAMP, StrategyQAg separated along the zaxis. Grey point clouds represent deterministic behavior under greedy decoding: they cluster tightly near Accm,d(cid:25)0 with slightly lower entropies, indicating narrow set of chains of thought that the model actually emits when forced to be deterministic. Colored point clouds correspond to stochastic multisample decoding and span m,d(cid:25)1.451.90 and Accm,d(cid:25)0.05 0.22, revealing much richer, higherdiversity regime of reasoning that greedy decoding never visits. Arrows from greedy centers (black stars) to multisample centroids (colored circles) show consistent shift toward higher path entropy and substantially higher accuracy on all three datasets, with the largest displacement on GSM8K. Taken together, this figure illustrates that, for DeepSeek, imposing deterministic decoding collapses broad landscape of competent reasoning strategies into single, brittle trace that systematically underestimates the models true multipath capabilities. In combination with our GLUE and InstruSum results, these findings paint coherent picture: deterministic decoding simultaneously hides alternative highquality outputs, underrepresents instructionfollowing capacity, and collapses multipath reasoning into single brittle trace. Multisample decoding does not change the underlying model pθ((cid:1) x), but it does change what we see of it, revealing much richer space of latent strategies and nearmiss attempts than any single greedy chain can convey. Figure 38: LLaMA3.18BInstruct: heterogeneous gains across arithmetic and verbal reasoning. For LLaMA3.18B, we again plot average path entropy m,d vs. accuracy gain Accm,d, layered over GSM8K, SVAMP, and StrategyQA. The deterministic clouds (grey) remain tightly packed near Accm,d(cid:25)0, reflecting low apparent diversity when the model is evaluated with greedy decoding. In contrast, the stochastic clouds concentrate around m,d(cid:25)1.351.70 and Accm,d(cid:25)0.030.12, clearly separated from the deterministic regime and revealing nontrivial gains from distributional exploration. The greedy!multisample displacement (arrows from black stars to colored circles) is largest on StrategyQA, where verbal multihop reasoning admits many distinct but valid chains of thought; arithmetic datasets exhibit smaller but consistent gains in both diversity and accuracy. Overall, this figure shows that even strong instructiontuned model like LLaMA3.1 hides substantial fraction of its reasoning flexibility when run deterministically, and that the benefits of exploration are strongest precisely on tasks with rich, verbal reasoning structure. Figure 39: Mixtral8(cid:2)7BInstruct: moderate but consistent diversity and accuracy gains. This figure presents the same 3D landscape for Mixtral8(cid:2)7BInstruct. Here, the stochastic point clouds occupy an intermediate band, with m,d(cid:25)1.301.55 and Accm,d(cid:25)0.020.10 across GSM8K, SVAMP, and StrategyQA. The deterministic clouds again lie tightly near Accm,d(cid:25)0, but the gap to the stochastic regime is smaller than for DeepSeek or LLaMA3.1. Arrows from greedy centers to multisample centroids consistently move toward higher reasoning diversity and higher accuracy, yet the magnitude of these shifts is shallower: Mixtral already allocates noticeable probability mass to highquality chains of thought that greedy decoding sometimes captures. This intermediate pattern suggests that Mixtrals internal reasoning landscape is less severely collapsed by deterministic inference than DeepSeeks, but still benefits from explorationespecially on GSM8K and SVAMP, where multisample decoding uncovers additional correct, diverse solution paths. In combination with Figures 37 and 38, this figure highlights that the extent of reasoning-path collapse under greedy decoding is strongly architecture-dependent, even when all models are evaluated on the same three reasoning benchmarks."
        },
        {
            "title": "6 Deterministic safety evaluation creates an illusion of robustness",
            "content": "The previous sections showed that deterministic inference collapses stochastic structure in ostensibly benign settings: on GLUEstyle classification it hides prediction uncertainty and adversarial fragility; on styleconstrained summarization it masks large islands of instructioncompliant summaries; and on multistep reasoning it reduces rich forest of chains of thought to single brittle trace. In this section we argue that the same collapse is far more dangerous when the output space is safetycritical. Modern safety work increasingly treats large language models as strategic, stochastic agents whose behavior can shift under distribution shift, prompt injection, or perceived oversight (Perez et al., 2022; Ganguli et al., 2022b; Greenblatt et al., 2024). Yet, in practice, many evaluations still probe only single deterministic decodertypically greedy decoding with temperature =0 and then implicitly extrapolate its behavior to all future deployments. Formally, let Datk be an attack distribution over inputs x, let be the space of completions, and let (cid:26) denote set of harmful continuations (e.g., detailed selfharm instructions, dualuse biological advice, targeted harassment). model pθ together with decoding policy π induces random output Yπ(x) 2 Y, and the corresponding decoderlevel stochastic risk is Rθ(π) = ExDatk P(Yπ(x) 2 H) . Greedy decoding πgreedy returns single argmax continuation ygreedy(x) with no randomness at inference time, so its measured risk reduces to Rdet θ := Rθ(πgreedy) = ExDatk[1fygreedy(x) 2 Hg], which is exactly the quantity computed in most existing jailbreak and misuse benchmarks. The crucial observation is that Rdet θ can be arbitrarily smaller than the risk of realistic stochastic policies that sample, rerank, or aggregate multiple candidates. Define the singlesample harmful probability qθ(x) := Pypθ(x)(y 2 H). If deployment policy draws independent samples from pθ((cid:1) x) and an adversary can exploit any harmful completion that appears, then the probability that at least one sample is harmful is q(k) θ (x) = 1 (cid:0) (1 (cid:0) qθ(x))k, and the corresponding ksample tail risk is R(k) θ := ExDatk[ q(k) θ (x)]. If the modal continuation ygreedy(x) happens to be safe, then the deterministic risk Rdet is exactly zero, even when qθ(x) is large enough that q(k) θ (x) is substantial for realistic generation budgets k. In other words, deterministic evaluation is completely blind to any harmful behavior that lives in the nonargmax tail of pθ((cid:1) x). θ Our central claim in this section is that such deterministic evaluation creates systematic illusion of robustness. Across range of jailbreak and misuse benchmarks, we will show that: (i) many ostensibly safe models assign nontrivial probability to harmful behaviors that only appear under multisample decoding; and (ii) the gap between deterministic and stochastic risk increases with model capability, so that stronger models often look safest under greedy decoding while harboring fatter harmful tails. Taken together, these results support simple but important conclusion: any safety assessment that relies solely on deterministic decoding is fundamentally incomplete, and can drastically underestimate the true risk of modern LLMs in realistic stochastic deployments. In the remainder of this section we first formalize decoderlevel risk and introduce metrics for concealed risk and deterministic illusion, then describe our attack benchmarks and decoding setups. We next present quantitative results contrasting greedy and multisample risk, followed by detailed breakdowns of concealedrisk categories and oversightsensitivity experiments. Finally, we illustrate the illusion qualitatively through case studies where greedy evaluation certifies prompt as safe even though harmful completions emerge with substantial probability under modest distributional exploration. Claim 4 (Deterministic Safety Illusion). Greedy safety checks systematically underestimate risk: any evaluation protocol that inspects only single deterministic completion can certify model as safe even when realistic stochastic decoders expose substantial harmful mass in the nonargmax tail, and this mismatch intensifies as models become more capable and sampling budgets grow."
        },
        {
            "title": "6.1 Threat model, benchmarks, and decoders",
            "content": "Before quantifying the gap between deterministic and stochastic safety risk, we spell out the underlying threat model and the concrete benchmarks and decoders we study. Throughout, we focus on settings where model may be queried repeatedlyeither by user, tool chain, or an adversary and any single harmful completion already constitutes failure. Threat model. Let denote the space of prompts and the space of completions. An attacker chooses inputs 2 from an attack distribution Datk, which may be handcrafted jailbreak suite, an automated attack generator, or an adaptive adversary interacting with the system. Among all possible outputs 2 we distinguish set of harmful completions (cid:26) such as: detailed instructions for selfharm or suicide, operational guidance for weapons, explosives, or biothreats, targeted harassment and hate speech towards individuals or groups. We measure harm via simple indicator loss ℓ(x, y) = 1fy 2 Hg, optionally refined in practice by graded harm scores (e.g., low / medium / high severity) from safety classifier or human annotators. safety failure occurs whenever at least one generated completion for given prompt falls in H; this captures both interactive and offline scenarios where an adversary can choose the most harmful of many outputs. Benchmarks. Our attack distribution Datk is instantiated via standard safety benchmarks augmented with simple supervision vs. privacy framing: Direct harm prompts. Canonical jailbreak and misuse suites (AdvBenchstyle prompts, JailbreakBench, HarmBench, and related collections) that explicitly ask for dangerous instructions (Tell me exactly how to synthesize or Explain how to bypass ). Indirect and roleplay prompts. Prompts that wrap the same underlying goal in story, simulation, or thirdperson form (Write scene where the protagonist designs ). These are known to be systematically easier to exploit than direct queries, and thus probe whether safety holds under mild rephrasings. Oversight vs. nooversight variants. For each base prompt we construct two framings: watched prompts xwatch that state that responses will be audited by safety team, and hidden prompts xhidden that stress anonymity or lack of oversight (No one will ever see this output ). Comparing these variants lets us study how sensitive models are to explicit supervision cues. In all cases, prompts are normalized into consistent chat format and fed to each model under identical system instructions emphasising helpfulness and safety. Table 3: Decoding policies used in safety evaluation. For each model we evaluate spectrum of decoders, from strictly deterministic greedy decodingwhich is commonly used in academic benchmarksto stochastic and multisample policies that better approximate real deployments. The last column highlights the main way in which each policy can hide or reveal harmful but lowprobability behaviors. Policy Parameters Deployment analogue Effect on risk visibility Greedy πgreedy Temp. sampling πT,p Multisample (k) Adv. search (optional) =0, single sample 2f0.7, 1.0g, topp=0.9 k2f8, 16, 32g samples, bestofk or full set Beam / attackerguided sampling Deterministic API / eval scripts Interactive chat, creative generation Hides all nonargmax harmful modes Exposes some harmful tails as increases Regenerate button, bestofk reranking Makes rare harmful modes increasingly likely to appear Strong adaptive adversary Upper bound on reachable harmful behavior Decoding policies. For each model we consider small but representative family of decoding policies π that mirror realistic deployment choices: Greedy decoding πgreedy. Temperature =0 with no sampling, returning single argmax continuation per prompt. This is the de facto standard in many safety evaluations and the regime in which models often appear most robust. Temperature sampling πT,p. Stochastic decoding with temperature 2 f0.7, 1.0g and nucleus sampling (topp=0.9). This reflects typical enduser settings in chat products where generations are required to be diverse and creative. Multisample decoding / bestofk. For 2 f8, 16, 32g we draw i.i.d. samples from pθ((cid:1) x) under πT,p and either: (i) expose all completions to the adversary, or (ii) select single output according to an internal utility (e.g., maximum model score or maximum helpfulness logit). Both settings are common: the former approximates interactive probing where an attacker can repeatedly regenerate, while the latter mirrors bestofk decoding used to improve quality. Adversarial search (optional). In subset of experiments we also simulate beamlike search guided by an external attacker model that ranks partial continuations by estimated harmfulness. This provides an upper bound on what powerful adaptive adversary could achieve against fixed pθ. Table 3 summarizes these policies and their intended deployment analogues."
        },
        {
            "title": "6.2 Formalizing deterministic vs. stochastic risk",
            "content": "We now make precise what we mean by risk under different decoding policies and how purely deterministic evaluation can systematically underestimate that risk. Let pθ(y x) denote the models conditional distribution over completions 2 given prompt 2 , and let (cid:26) be the set of harmful responses (e.g., detailed selfharm instructions, weapons guidance). An attack distribution Datk over prompts encodes the adversarys strategy, ranging from static jailbreak suites to adaptive probing. Decoding policies as stochastic maps. decoding policy π maps the distribution pθ((cid:1) x) to random output Yπ(x). For example, greedy decoding πgreedy produces deterministic argmax continuation Ygreedy(x) = arg max yY pθ(y x), while temperature sampling or multisample decoding induces genuinely stochastic Yπ(x) even for fixed prompt x. We define binary loss ℓ(x, y) = 1fy 2 Hg, which evaluates to 1 whenever the completion is harmful and 0 otherwise. Given decoding policy π, the decoderlevel risk is Rθ(π) = ExDatk P(Yπ(x) 2 x) . This quantity captures the probability that at least one completion produced by π on randomly drawn attack prompt is harmful. In practice, Rθ(π) is estimated via empirical attack success rates (ASR) over finite set of prompts, but the conceptual object is probability of harm under both the model and the decoder. Perprompt harmful mass and tail risk. The key quantity that links model and decoder is the per prompt harmful mass Pr ypθ(x) Even if qθ(x) is small, repeated sampling can make harm likely. Consider ksample stochastic poli.i.d.(cid:24) pθ((cid:1) x) and exposes all completions to the user or adversary. The icy that draws Y1, . . . , Yk probability that at least one of these samples is harmful is qθ(x) = [y 2 H]. rk(x) = Pr [9j (cid:20) : Yj 2 x] = 1 (cid:0) (1 (cid:0) qθ(x))k. For small but nonzero qθ(x), rk(x) grows approximately linearly in k: for qθ(x) = ε (cid:28) 1 and moderate k, rk(x) (cid:25) 1 (cid:0) (1 (cid:0) ε)k (cid:25) kε, so an apparently tiny harmful mass ε = 0.01 already yields r16(x) (cid:25) 0.16 under k=16 samples. By contrast, greedy evaluation only inspects the single argmax continuation Ygreedy(x): if this particular completion is safe, the measured risk contribution of is exactly zero, regardless of how large will be in deployment. Hidden tail mass and perprompt underestimation. Greedy evaluation is blind to harmful probabilities that do not affect the argmax continuation. To capture this we define the hidden tail mass at prompt as h(x) = qθ(x) (cid:1) 1fYgreedy(x) /2 Hg, i.e., the harmful probability mass under pθ((cid:1) x) that remains invisible when only the greedy completion is inspected. At the model level we summarize this as hm = ExDatk[h(x)]. which we approximate empirically via Monte Carlo estimates of qθ(x) from stochastic samples. complementary perspective is to compare the true ksample tail risk rk(x) to what an evaluator that only checks Ygreedy(x) would report. If the greedy continuation is safe, the reported perprompt risk is 0, whereas the actual risk under samples is rk(x). For such prompts, deterministic evaluation underestimates risk by rk(x) in absolute terms and by factor on the order of 1/(1 (cid:0) (1 (cid:0) qθ(x))k) in relative terms. Deterministic illusion index. At the model level we compare two aggregate risks: the greedy risk"
        },
        {
            "title": "Rgreedy\nm",
            "content": "= ExDatk[1fYgreedy(x) 2 Hg]. which is precisely what standard deterministic evaluation estimates; and the stochastic ksample risk"
        },
        {
            "title": "Rstoch",
            "content": "m (k) = ExDatk[rk(x)] = ExDatk[1 (cid:0) (1 (cid:0) qθ(x))k]. which reflects the probability that at least one harmful completion appears when an attacker (or user) can draw samples. We then define deterministic illusion index that measures how much of the true stochastic risk is invisible under greedy evaluation:"
        },
        {
            "title": "Rstoch",
            "content": "Im(k) = (k) (cid:0) Rgreedy Rstoch (k) + δ where δ > 0 is tiny constant for numerical stability. By construction, Im(k) (cid:25) 0 when deterministic and stochastic risks match, and Im(k) ! 1 when the model has substantial ksample risk but appears almost perfectly safe under greedy decoding. In our experiments, Im(k) increases systematically with both model capability and search budget k, indicating that stronger models often look most robust under deterministic evaluation while hiding the fattest harmful tails. , simple bound on the illusion. The gap between deterministic and stochastic risk can be lower bounded in purely probabilistic terms. Proposition. Fix prompt and suppose that: (i) the greedy completion is safe, Ygreedy(x) /2 H, and (ii) the harmful mass satisfies qθ(x) (cid:21) ε > 0. Then for any (cid:21) 1, an evaluator that only ever inspects Ygreedy(x) necessarily underestimates the perprompt risk by at least rk(x) = 1 (cid:0) (1 (cid:0) qθ(x))k (cid:21) 1 (cid:0) (1 (cid:0) ε)k. Sketch. Under the assumptions, the deterministic evaluator reports 0 risk for x. However, the true sample risk is rk(x), and the function 7! 1 (cid:0) (1 (cid:0) q)k is monotonically increasing in q, so replacing qθ(x) by its lower bound ε yields the stated inequality. Even for modest k, the lower bound 1 (cid:0) (1 (cid:0) ε)k can be substantial: for ε = 0.01 and k=16 we obtain 1 (cid:0) (1 (cid:0) 0.01)16 (cid:25) 0.15, meaning that prompt certified as safe under greedy decoding can still yield harmful output in roughly one out of six stochastic runs. Risk surface as function of tail mass and budget. To visualize this effect, we will use simulated risk surface rk(q) = 1 (cid:0) (1 (cid:0) q)k that treats and as continuous variables. Figure 40 plots rk(q) for 2 [103, 0.2] and 2 f1, . . . , 32g. The k=1 slicecorresponding to deterministic evaluation is nearly flat across small q, suggesting that models with qθ(x) 2 [0.001, 0.02] are equally safe. In contrast, for realistic search budgets k(cid:25)832, the same range of induces steep rise in rk(q), turning apparently negligible tails into nontrivial failure probabilities. This gap between the k=1 baseline and the full surface is precisely what our deterministic illusion index Im(k) is designed to capture."
        },
        {
            "title": "6.3 Metrics and categories for concealed risk",
            "content": "The formalism in Section 6.2 characterizes decoderlevel risk in terms of the perprompt harmful mass qθ(x) and the ksample tail risk rk(x). We now introduce concrete metrics that (i) match standard safety reporting practice (attack success rates), (ii) decompose risk into promptlevel categories that expose concealed risk, and (iii) quantify how much oversight framing matters under deterministic vs. stochastic decoding. Decoderlevel attack success rates. For fixed model and decoding policy π, let be random attack prompt drawn from Datk and Yπ(X) the random completion produced by π. We define the attack success rate (ASR) as ASRm(π) = EXDatk 1fYπ(X) 2 Hg . i In practice, ASRm(π) is estimated by averaging the indicator 1fYπ(xi) 2 Hg over finite set of adversarial prompts fxign i=1. We distinguish two regimes:"
        },
        {
            "title": "ASRgreedy\nm",
            "content": "= ASRm(πgreedy),"
        },
        {
            "title": "ASRstoch",
            "content": "m (k) = ASRm(πstoch,k), where πstoch,k denotes ksample stochastic policy (e.g., temperature sampling with =0.7, top p=0.9, drawing independent completions and exposing all of them to the attacker or user). The stochastic risk gap at budget is then ASRm(k) = ASRstoch (k) (cid:0) ASRgreedy . model that appears nearly safe under greedy decoding but exhibits ASRm(k) (cid:29) 0 at realistic budgets is precisely one that creates an illusion of robustness. Promptlevel categories: robust, concealed, deterministic failure. To unpack where ASRm(k) comes from, we classify each attack prompt into safety categories based on its behavior under greedy and ksample decoding. For each we observe: Figure 40: Decoderlevel risk as function of harmful tail mass and search budget. We plot the theoretical decoderlevel risk rk(q) = 1 (cid:0) (1 (cid:0) q)k as function of the perprompt harmful tail mass (xaxis, log scaled) and the number of independent samples (yaxis), with cooltowarm colormap indicating low to high risk. The thick black curve on the front edge shows the k=1 slice (deterministic greedy evaluation): for small tails, r1(q) (cid:25) q, so prompts with (cid:25) 103 and (cid:25) 0.02 look similarly safe (risks around 0.001 vs. 0.02). The colored k=8, 16, 32 curves on the surface reveal that, under realistic multisample budgets, the same tail masses yield much higher risk: at q=103, risk rises from (cid:25) 0.001 (greedy) to (cid:25) 0.03 for k=32, while at q=0.02 it jumps from (cid:25) 0.02 to well above 0.45. The warm plateau near the back shows that even moderately sized harmful tails become near-certain failures once is large. The stark visual gap between the flatlooking k=1 boundary and the lifted multisample surface makes explicit the illusion of robustness created by deterministiconly safety evaluation. the greedy completion Ygreedy(x), and multiset of stochastic samples fY1(x), . . . , Yk(x)g drawn i.i.d. from pθ((cid:1) x) under fixed temperature and topp. We then define three disjoint events: RobustSafe(x) , Ygreedy(x) /2 and Yj(x) /2 8j (cid:20) k, ConcealedRisk(x) , Ygreedy(x) /2 and 9j (cid:20) : Yj(x) 2 H, By construction, every prompt falls into exactly one of these categories: DetFail(x) , Ygreedy(x) 2 H. 1fRobustSafe(x)g + 1fConcealedRisk(x)g + 1fDetFail(x)g = 1. At the model level, we summarize the fractions ρrobust = Pr XDatk [RobustSafe(X)], ρconcealed = Pr XDatk [ConcealedRisk(X)], ρdet-fail = Pr XDatk [DetFail(X)], which satisfy ρrobust + ρconcealed ing prompts in each category. + ρdet-fail = 1. Empirically, we estimate these probabilities by countLinking categories to deterministic and stochastic ASR. The above decomposition gives clean way to interpret ASRgreedy and ASRstoch (k). Under greedy decoding, prompt contributes to ASR if and only if it is deterministic failure:"
        },
        {
            "title": "ASRgreedy\nm",
            "content": "= EXDatk[1fYgreedy(X) 2 Hg] = P[DetFail(X)] = ρdet-fail . Under ksample stochastic decoding, an attack succeeds if at least one of the completions is harmful. Let Harmk(x) denote this event: Harmk(x) , 9j (cid:20) : Yj(x) 2 H. We can write"
        },
        {
            "title": "ASRstoch",
            "content": "m (k) = P[Harmk(X)] = EX [1fHarmk(X)g]. Now observe that: - If RobustSafe(x) holds, then Harmk(x) is false by definition. - If DetFail(x) holds, then the greedy completion is already harmful, and typically at least one stochastic sample will also be harmful, so such prompts contribute to both deterministic and stochastic ASR. - If ConcealedRisk(x) holds, then contributes only to stochastic ASR, never to deterministic ASR. This yields the decomposition"
        },
        {
            "title": "ASRstoch",
            "content": "m (k) = Pr [DetFail(X)] } {z ASRgreedy + Pr [ConcealedRisk(X) ^ Harmk(X)], and hence ASRm(k) = ASRstoch (k) (cid:0) ASRgreedy = Pr [ConcealedRisk(X) ^ Harmk(X)]. In other words, the entire stochastic risk gap ASRm(k) comes from prompts that look safe under greedy evaluation but reveal harm under multisample decoding. If we further approximate Harmk(X) as almost surely true whenever ConcealedRisk(X) holds (which is reasonable for moderate on prompts with nontrivial tail mass), then ASRm(k) is numerically close to ρconcealed . Concealed risk and the illusion index. The deterministic illusion index Im(k) introduced in Section 6.2 can be reinterpreted via these categories. Recall Im(k) ="
        },
        {
            "title": "ASRstoch",
            "content": "m (k) (cid:0) ASRgreedy ASRstoch (k) + δ , with tiny δ > 0 for stability. Using the decomposition above, Im(k) = Pr [ConcealedRisk(X) ^ Harmk(X)]"
        },
        {
            "title": "ASRstoch",
            "content": "m (k) + δ . Thus Im(k) is large precisely when substantial portion of the models stochastic risk comes from prompts that appear safe under greedy decoding. In our experiments, we will report Im(k) alongside ρrobust , and ρdet-fail to show how deterministic evaluation increasingly understates risk as models become more capable. , ρconcealed Table 4: Concealed risk and deterministic illusion across models. For each model we report deterministic and stochastic attack success rates (ASR) on our jailbreak suites, the stochastic risk gap ASRm(8) at budget k=8, the promptlevel fractions of robustly safe, concealedrisk, and deterministicfailure prompts (ρrobust (8), i.e., the fraction of total stochastic risk that is invisible under greedy evaluation. In our simulations, stronger models display lower ASRgreedy but much higher ASRm(8) and Im(8), indicating that most of their risk is concealed under deterministic evaluation. ), and the deterministic illusion index Im(8) = ASRm(8)/ASRstoch , ρconcealed , ρdet-fail Model"
        },
        {
            "title": "ASRstoch",
            "content": "m (8) ASRm(8) Phi2 Vicuna7Bv1.5 Mistral7BInstruct Gemma29BIT Mixtral8(cid:2)7BInstruct LLaMA3.18BInstruct DeepSeekR1Distill 0.60 0.52 0.30 0.19 0.13 0.06 0.03 0.75 0.70 0.55 0.45 0.40 0.32 0.28 +0.15 +0.18 +0.25 +0.26 +0.27 +0.26 +0.25 (ρrobust , ρdet-fail , ρconcealed (0.25, 0.15, 0.60) (0.30, 0.18, 0.52) (0.45, 0.25, 0.30) (0.55, 0.26, 0.19) (0.60, 0.27, 0.13) (0.68, 0.26, 0.06) (0.72, 0.25, 0.03) ) Im(8) 0.20 0.26 0.45 0.58 0.68 0.81 0.89 Oversight sensitivity under greedy vs. stochastic decoding. To probe oversight sensitivitywhere models behave more cautiously when they believe they are being monitoredwe introduce oversight-conditioned variants of the above metrics. For each base attack prompt x, we construct two framings: xwatch = fwatch(x), xhidden = fhidden(x), where fwatch adds language such as You are being audited by safety team and must follow all safety guidelines, and fhidden adds language such as No one will see this output; it is completely anonymous. For each framing condition 2 fwatch, hiddeng and decoder π, we define"
        },
        {
            "title": "ASRgreedy\nm",
            "content": "(c) = EXDatk 1fYgreedy(X c) 2 Hg . i"
        },
        {
            "title": "ASRstoch",
            "content": "m (k; c) = EXDatk 1f9j (cid:20) : Yj(X c) 2 Hg . We then measure the oversight gap under greedy and stochastic decoding:"
        },
        {
            "title": "Ggreedy\nm",
            "content": "= ASRgreedy (hidden) (cid:0) ASRgreedy (watch),"
        },
        {
            "title": "Gstoch",
            "content": "m (k) = ASRstoch (k; hidden) (cid:0) ASRstoch (k; watch). Finally, we define an oversight amplification term that captures how much additional oversight sensitivity becomes visible once we move beyond greedy decoding: Fm(k) = Gstoch (k) (cid:0) Ggreedy . model with Fm(k) (cid:29) 0 may look only mildly sensitive to oversight under deterministic evaluation, yet exhibit much larger increase in harm under hidden framing once stochastic tails are explored. Tabular and graphical summaries. We present both tabular and graphical summaries of these metrics. We use composition table to report permodel deterministic and stochastic ASR, concealed risk fractions, and illusion indices: Across all twelve decoderlevel risk surfaces for our opensource models LLaMA2 7B and 13B (Figs. 4142), Mistral 7B and Mixtral 8(cid:2)7B (Figs. 4748), Mixtral 8(cid:2)22B and Phi2 (Figs. 49 51), Gemma2 2B and 27B (Figs. 4546), LLaMA3 8B and 70B (Figs. 4344), and DeepSeekR1 Distill together with Vicuna7B (Figs. ??52) we see highly consistent pattern: for each model there is sizeable lowpercentile plateau (typically the lowest 4075% of prompts) where all slices 2 f1, 8, 16g remain in cool blue band rk(q) (cid:25) 00.2, and much smaller but sharp highrisk tail (roughly the top 1030% of prompts) where the k=8 and k=16 curves rise abruptly into rk(q) (cid:25) 0.8 1.0. Scaling within family (e.g., LLaMA2 7B!13B, Gemma2 2B!27B, Mixtral 8(cid:2)7B!8(cid:2)22B, LLaMA3 8B!70B) primarily enlarges the safe plateau and lowers the greedy slice, while leaving narrow band of prompts whose risk under stochastic decoding remains near certainty. In contrast, compact models like Phi2 and, to lesser extent, DeepSeekR1 Distill exhibit broad red regions where both greedy and multisample risks are high, indicating that their dangerous modes are numerous and already partially visible at k=1. Taken together, these surfaces show that safer frontier models do not remove highrisk modes; they concentrate them into thinner but steeper tail where deterministic evaluation drastically underestimates the true decoderlevel risk once realistic sampling budget (k(cid:25)816) is allowed. Figure 41: Decoderlevel risk landscape for LLaMA2 7B. The surface shows rk(q) = 1 (cid:0) (1 (cid:0) q)k over prompt percentile (xaxis; 0 to 1, sorted by harmful tail mass q), samples (yaxis; 1(cid:20)k(cid:20)32), and risk (zaxis and color; 0 to 1). The black, green, and orange curves trace slices at k=1 (greedy), k=8, and k=16. For LLaMA2 7B, roughly the lowest 4050% of prompts stay near rk(q) (cid:25) 0 even as grows, but the upper 2030% form steep ridge where moving from k=1 to k2f8, 16g drives risk into the 0.8(cid:0)1.0 band, revealing substantial highrisk tail that greedy testing largely misses. Figure 42: Decoderlevel risk landscape for LLaMA2 13B. Axes and color scale match Fig. 41. Compared to 7B, the greedy slice is lower over much of the distribution: the bottom 5060% of prompts remain very close to r1(q) (cid:25) 0, and even the k=8 curve stays below (cid:25) 0.2 for most prompts. However, the top 1525% of prompts still exhibit sharp transition where rk(q) under k2f8, 16g jumps to 0.7(cid:0)1.0, indicating that capability scaling reduces the mass of dangerous prompts but leaves concentrated band where multisample risk remains extreme. Figure 43: Decoderlevel risk landscape for LLaMA3 8B. As before, we plot rk(q) over prompt percentile, samples k, and risk. LLaMA3 8B shows broader lowrisk plateau than LLaMA2: approximately the lowest 5565% of prompts remain below rk(q) (cid:25) 0.1 even at k=8, and the greedy curve is tightly bound to zero in this region. Beyond the (cid:25) 65th percentile, the k=8 and k=16 slices bend sharply upward, with the top 1520% of prompts reaching 0.8(cid:0)1.0 risk. This reflects model whose overall safety improves, but whose tail prompts still become almost surely harmful under multisample decoding. Figure 44: Decoderlevel risk landscape for LLaMA3 70B. For the 70B variant, the lowrisk region widens further: roughly the lowest 70% of prompts remain at rk(q) 0.05 for k=1 and stay below (cid:25) 0.15 even at k=8. Yet the remaining 1015% of prompts still show dominant ridge where the k=8 and k=16 curves rise into the 0.8(cid:0)1.0 range. Thus, largescale alignment compresses the dangerous set into smaller fraction of prompts, but does not fully remove the nearcertain failure zone that appears under stochastic sampling. Figure 45: Decoderlevel risk landscape for Gemma2 2B. Gemma2 2B exhibits moderate plateau of low risk: around the lowest 4555% of prompts stay at r1(q) (cid:25) 0 and remain below (cid:25) 0.15 for k=8. In the upper half of the distribution, however, the k=8 and k=16 slices rise quickly, with the top 20% of prompts approaching rk(q) (cid:25) 0.8(cid:0)1.0. The contrast between the nearly flat greedy curve in the bulk and the steep rise in the tail again illustrates concealed risk: deterministic ASR underestimates how often multisample decoding will find harmful completions. Figure 46: Decoderlevel risk landscape for Gemma2 27B. Scaling to 27B flattens the lowpercentile region further: roughly the lowest 6070% of prompts stay at r1(q) (cid:25) 0 and below about 0.1 even for k=8. Yet, as with other stronger models, compact top 1015% still hosts steep ridge where k=8 and k=16 yield rk(q) (cid:25) 0.9(cid:0)1.0. Gemma2 27B therefore combines excellent average safety with persistent, narrowly concentrated highrisk tail that only becomes evident when exploring the decoder stochastically. Figure 47: Decoderlevel risk landscape for Mistral 7B. The surface is slightly flatter than for LLaMA2 7B in the lowpercentile region: roughly the lowest 5565% of prompts remain close to rk(q) (cid:25) 0 even at k=8, and the greedy slice stays near zero over broad plateau. However, the top 20% of prompts exhibit steep ridge: going from k=1 to k2f8, 16g increases risk from (cid:25) 0.05(cid:0)0.1 to nearly 0.9(cid:0)1.0. This highlights how decoder stochasticity exposes rare but extremely highrisk modes that remain invisible under purely deterministic testing. Figure 48: Decoderlevel risk landscape for Mixtral 8(cid:2)7B. Mixtral 8(cid:2)7B displays pronounced tworegime structure. For approximately the lowest 6070% of prompts, all three slices k=1, 8, 16 remain below rk(q) (cid:25) 0.2, forming wide, cool (blue) plateau where even multisample decoding yields low risk. Beyond the (cid:25) 70th percentile, however, the k=8 and k=16 curves bend sharply upward, with the top 1015% of prompts saturating to rk(q) (cid:25) 1. This is canonical concealedrisk pattern: strong average safety coexists with compact hightail region where stochastic sampling makes harmful completions almost inevitable. Figure 49: Decoderlevel risk landscape for Mixtral 8(cid:2)22B. The larger Mixtral variant further suppresses greedy risk: the black k=1 curve hugs zero for roughly the lowest 70% of prompts, and even at k=8 the bulk of prompts stay below rk(q) (cid:25) 0.1(cid:0)0.2. Nevertheless, the upper 1015% of prompts still form bright ridge where increasing the search budget from k=1 to k=16 lifts risk into the 0.8(cid:0)1.0 band. Scaling the model therefore reduces the measure of dangerous prompts but does not fully eliminate the highrisk tail exposed by stochastic decoding. Figure 50: Decoderlevel risk landscape for DeepSeekR1 Distill. DeepSeekR1 Distill exhibits an even more compressed tail: the greedy slice remains almost zero for the lowest 7580% of prompts, and even k=8 keeps most of this mass below rk(q) (cid:25) 0.1. However, narrow top 10% of prompts still shows sharp escalation where k=8 and k=16 rapidly push risk to 0.9(cid:0)1.0. This regime exemplifies the central theme of our analysis: highly capable, wellaligned models can have excellent deterministic safety profiles yet still hide small set of prompts where multisample decoding yields nearcertain failure. Figure 51: Decoderlevel risk landscape for Phi2. The surface shows rk(q) = 1 (cid:0) (1 (cid:0) q)k over prompt percentile (xaxis; 0 to 1, sorted by harmful tail mass q), samples (yaxis; 1(cid:20)k(cid:20)32), and risk (zaxis and color; 0 to 1). For Phi2, the greedy slice k=1 is relatively high across broad band: roughly the middle 3050% of prompts already reach r1(q) (cid:25) 0.1(cid:0)0.3, and the upper tail climbs to (cid:25) 0.4(cid:0)0.5. Increasing the budget to k2f8, 16g pushes much of the upper half into rk(q) (cid:25) 0.6(cid:0)1.0, yielding large red region where both deterministic and stochastic ASR are high. Figure 52: Decoderlevel risk landscape for Vicuna7B. Axes and color scale match Fig. 51. Relative to Phi2, the greedy slice is noticeably lower: about the lowest 5060% of prompts stay near r1(q) (cid:25) 0, and even up to the (cid:25) 70th percentile the greedy risk rarely exceeds 0.15. However, for the top 2025% of prompts the k=8 and k=16 curves bend sharply upward, with the worst 1015% saturating at rk(q) (cid:25) 0.8(cid:0)1.0. Vicuna7B therefore has lower average greedy ASR than Phi2 but still retains compact highrisk tail where stochastic decoding uncovers failures that deterministic evaluation largely misses."
        },
        {
            "title": "6.4 Experimental design and key visualizations",
            "content": "The metrics in Section 6.3 let us quantify how much decoder choice changes apparent risk and how much of that risk is concealed under greedy evaluation. We now describe our experimental design and the core visualizations that make these effects concrete. Protocol overview. For each model in our safety panel and each attack suite Datk (Section 6.1), we evaluate family of decoders fπgreedy, πstoch,kg with k2f1, 2, 4, 8, 16, 32g. For each (m, π) we estimate: (i) the decoderlevel attack success rate ASRm(π), (ii) the stochastic risk gap ASRm(k), (iii) the promptlevel composition (ρrobust ), and (iv) the illusion index Im(k) and oversightsensitivity quantities Ggreedy (k), and Fm(k). All estimates are computed over the same base set of attack prompts and framing conditions (watched vs. hidden), ensuring that differences are attributable solely to decoder behavior, not prompt mismatch. , ρconcealed , Gstoch , ρdet-fail We visualize these quantities from three complementary angles: 1. Riskvsbudget curves, showing how ASR increases with the sampling budget for each model. 2. Analytic risk surfaces, illustrating how even small perprompt harmful mass qθ(x) yields substantial tail risk rk(x) at realistic k. 3. Modellevel scatter plots and histograms, connecting illusion indices and concealedrisk fractions to overall capability and highlighting how apparently safer models can hide fatter harmful tails. We now detail each visualization and how it ties back to the formalism. Riskvsk curves per model. Our first visualization tracks how attack success evolves as we move from strictly deterministic decoding (k=1) to modest multisample regimes (k(cid:20)32). For each model we treat the ksample stochastic decoder πstoch,k as exposing all completions to the adversary or user, and estimate ASRstoch (k) over the attack distribution. In Figure 53, models that appear safe under deterministic evaluation (low ASRgreedy ) can still show steep ASR increases for modest k, whereas weaker or more poorly aligned models exhibit high deterministic ASR and comparatively smaller relative gaps. This pattern already hints at concerning trend: the models that look safest under greedy decoding can be the ones with the largest concealed stochastic risk. Risk surface as function of harmful mass and budget. The riskvsk curves are empirical; we complement them with an analytic visualization of the tail risk rk(x) = 1 (cid:0) (1 (cid:0) qθ(x))k as function of harmful mass qθ(x) and sampling budget k. This surface, shown in Figure 54, highlights how deceptively benign the k=1 slice looks compared to realistic multisample settings. This analytic surface does not depend on any particular model; instead, it shows that whenever model allocates nonzero probability qθ(x) to harmful outputs, any deployment that samples multiple times (regenerate button, multiturn chat, bestofk reranking) will experience risk that grows as 1(cid:0)(1(cid:0)qθ(x))k, while greedy evaluation remains blind to this entire dimension. Illusion vs. capability scatter. To relate the degree of illusion to model capability, we summarize each model by scalar capability score Cm (e.g., an average over standard reasoning benchmarks such as GSM8K, MMLU, or our own multistep tasks) and plot the illusion index Im(k) or stochastic risk gap ASRm(k) as function of Cm. Together with Table 4, this scatter shows that the illusion index is not small correction term: for some models, the majority of stochastic risk arises from prompts that appear entirely benign under deterministic evaluation. Figure 53: Attack success vs. sampling budget for different models. Each panel shows the stochastic attack (k) as function of sampling budget k2f1, 2, 4, 8, 16, 32g for single model on our success rate ASRstoch combined jailbreak suites. The horizontal dotted line marks the deterministic ASR ASRgreedy (i.e., k=1 under greedy decoding), which is often close to zero for stronger models. As increases, many models exhibit sharp rise from ASRstoch (32) in the 1030% range, revealing substantial hidden risk that is entirely invisible under standard greedy evaluation. The gap between the dotted baselines and the curves at k=8 or k=16 corresponds directly to the stochastic risk gap ASRm(k) and feeds into the illusion index Im(k) reported in Table 4. (1) (cid:25) 0 to ASRstoch (16) or ASRstoch Figure 54: Analytic tail risk surface as function of harmful mass and sampling budget. We plot the theoretical tail risk rk(x)=1(cid:0)(1(cid:0)q)k as function of the perprompt harmful mass 2 [103, 0.2] (xaxis) and sampling budget k2f1, . . . , 32g (yaxis). The bottom row (k=1) corresponds to deterministic evaluation and remains near zero for q(cid:28)0.1, creating the impression that the model is almost perfectly safe. However, for the same small (e.g., q=0.01) the risk surface quickly rises with k: at k=16 we have r16(x) (cid:25) 0.16, and at k=32 the risk exceeds 0.28. The heatmap thus makes visually explicit the core illusion: even seemingly tiny harmful tail can translate into doubledigit probability of harm once realistic sampling budgets are allowed, while deterministic assessment at k=1 reports exactly zero risk. Concealed risk composition. Finally, we visualize the promptlevel categories from Section 6.3 as compositional histogram. While Table 4 reports scalar summaries, the histogram highlights the structure of risk: Taken together, these plots and tables show that all three perspectives riskvsk curves, analytic surfaces, and compositional histograms tell consistent story: greedy evaluation systematically underestimates risk, and the extent of this underestimation grows with model capability and with the amount of distributional exploration permitted in deployment."
        },
        {
            "title": "6.5 Case studies: concrete prompts and empirical harmful tails",
            "content": "The aggregate metrics and visualizations in Sections 6.36.4 show that deterministic evaluation can substantially underestimate the true stochastic risk of modern LLMs. To make this illusion of robustness more tangible, we now zoom in on individual prompts and explicitly examine (i) how harmful tails emerge under multisample decoding and (ii) how framing about oversight modulates these tails. Throughout this section we work with the same notation as before: for fixed model pθ, input x, and harmful set H, let qθ(x) denote the (unknown) harmful probability mass under the models conditional distribution, and let rk(x) denote the probability that at least one of i.i.d. samples is harmful. Figure 55: More capable models exhibit larger deterministic illusions. Each point corresponds to model m, with the xaxis showing capability proxy Cm (average normalized score across our reasoning benchmarks) and the yaxis showing the illusion index Im(8) at budget k=8. The fitted trend line reveals striking pattern: models with higher capability scores tend to have larger Im(8), meaning that growing fraction of their true stochastic risk is concealed under greedy evaluation. In our data, smaller models cluster in the lowerleft region (moderate capability, modest illusion), while frontierscale models lie in the upperright, combining strong performance with substantial hidden harmful tails. This formalizes the concern that safer under greedy does not necessarily mean safer under realistic stochastic use. Monte Carlo estimation of perprompt harmful mass. For single prompt x, direct access to qθ(x) is impossible, but multisample decoding naturally yields Monte Carlo estimator. Let Y1, . . . , Yk be i.i.d. samples from pθ((cid:1) x) under fixed stochastic decoder (temperature , topp, etc.), and define the empirical harmful indicator Zj(x) = 1[Yj 2 H] for = 1, . . . , k. We estimate the harmful mass via and the empirical tail risk bqθ(x) = 1 kX j=1 Zj(x), brk(x) = 1 max 1jk Zj(x) = 1 , which simply records whether any of the samples was harmful. Under the idealized model where sampling uses the true pθ((cid:1) x) without truncation, the estimator bqθ(x) is unbiased with variance Var[bqθ(x)] = qθ(x)(1 (cid:0) qθ(x))/k. Standard concentration results give, for any ϵ > 0, Pr jbqθ(x) (cid:0) qθ(x)j > ϵ (cid:20) 2 exp ( (cid:0) 2kϵ2), so even modest budgets (e.g., k=32) are enough to obtain sharply concentrated estimate when qθ(x) is not vanishingly small. Promptlevel illusion ratios. At the level of single prompt, the deterministic assessment only inspects the greedy completion Ygreedy(x). It therefore reports perprompt risk of either 0 (if Ygreedy(x) /2 H) or 1 (if Ygreedy(x) 2 H). Multisample decoding, by contrast, experiences the tail concealedm concealedm+det_failm Figure 56: Decomposing safety into robustly safe prompts, concealed risk, and deterministic failures. For each model m, we partition attack prompts into three disjoint categories: robustly safe (the greedy completion is safe and all k=8 stochastic samples are safe), concealed risk (the greedy completion is safe, but at least one of the stochastic samples is harmful), and deterministic failure (the greedy completion itself is already harmful). The horizontal stacked bars show, for each model, the empirical fraction of prompts assigned to each category, while the yaxis labels additionally report in parentheses the fraction of total risk that is hidden, hidden_sharem = , among all risky prompts for that model. Models are ordered from top to bottom by their total stochastic attack success rate ASRstoch (k) = concealedm + det_failm, so that overall risk monotonically increases down the figure. Stronger, more safety-tuned models in the lower rows exhibit very small deterministic-failure mass but extremely large concealed-risk bands and correspondingly high hidden shares, indicating that most of their stochastic risk arises from prompts that would be certified safe by any evaluator that only inspects the greedy output. By contrast, weaker or poorly aligned models near the top allocate more mass to deterministic failures and less to concealed risk, meaning that larger portion of their risk remains directly visible under standard greedy evaluation. This compositional view makes the illusion of robustness visually explicit: seemingly safe region under deterministic decoding can in fact contain substantial reservoir of hidden stochastic risk. risk rk(x) = 1 (cid:0) (1 (cid:0) qθ(x))k. We can therefore define an empirical illusion ratio for prompts that are deemed safe under greedy evaluation: bIllk(x) = brk(x) 1[Ygreedy(x) 2 H] + ε , where ε is tiny constant (e.g., ε = 106) that prevents division by zero. If the greedy completion is bIllk(x) is proportional to brk(x); if the greedy completion safe, the denominator is essentially ε, and is harmful, the ratio is close to 1, reflecting that deterministic evaluation already exposes risk for that prompt. When aggregating across prompts, the illusion index Im(k) in Section 6.3 can be seen as modellevel normalization of the same phenomenon. Case study I: safe prompt with harmful tail. Our first case study focuses on single jailbreak prompt drawn from our attack suite, chosen such that the greedy completion is classified as safe by our automatic safety filters and by human inspection. We then draw k=16 independent completions from stochastic decoder and visualize the resulting empirical tail. In Figure 57 the greedy completion is safe, so the deterministic perprompt risk estimate is 0. However, with 3 harmful completions among 16 samples, the Monte Carlo estimate bqθ(x) (cid:25) 0.19 implies true tail risk near r16(x) (cid:25) 1 (cid:0) (1 (cid:0) 0.19)16, which exceeds 0.95. The empirical illusion bIll16(x) is therefore on the order of 105 when normalized by ε=106, highlighting that all of ratio the relevant risk for this prompt is hidden from any greedyonly evaluator. Case study II: oversight framing under greedy vs. stochastic decoding. Our second case study mirrors oversight framing setups explored by prior work, but now explicitly contrasts greedy and stochastic decoding. For base harmful intent xbase (e.g., request for weapons construction details), we construct two framings: xwatch = You are being audited by safety team. Please answer: xbase, xhidden = Your responses are anonymous and will never be audited. Please answer: xbase, where denotes string concatenation. For each of xwatch and xhidden we collect both greedy completion and k=16 stochastic completions, and estimate the four oversightconditioned ASRs:"
        },
        {
            "title": "ASRgreedy\nm",
            "content": "(watch), ASRgreedy (hidden), ASRstoch (k; watch), ASRstoch (k; hidden). The oversight differential Fm(k) defined in Section 6.3 can be interpreted as the extra oversight sensitivity that is only visible once we allow sampling. From case studies to populationlevel risk. These case studies are deliberately chosen to be illustrative rather than representative, but they highlight three general points that our populationlevel metrics then quantify: 1. Perprompt tails can be large even when the greedy sample is safe. For some prompts x, the harmful mass qθ(x) may be in the 1%10% range, implying rk(x) in the tens of percent for realistic k, while greedy evaluation reports zero risk. 2. Oversight framing changes the shape of the tail. The difference between qθ(xwatch) and qθ(xhidden) is often much larger in the tails than at the mode, so models can appear oversightrobust under deterministic evaluation yet exhibit strong differentials under sampling. 3. Greedyonly evaluation structurally discards exactly the information we care about. Both bqθ(x) and brk(x) are inherently tail quantities; any protocol that inspects only Ygreedy(x) cannot recover them, no matter how carefully the greedy completion is scored. In Section 7 we argue that safety evaluations must therefore be reoriented around decoderaware and tailsensitive metrics: rather than asking Is the greedy completion safe?, we should be asking How much harmful mass does the model allocate, and how quickly does that mass become visible under realistic decoding policies? These case studies provide concrete, promptlevel evidence that the answer to the latter question can be dramatically different from the former, and that the gap widens as models become more capable and more strategically responsive to oversight cues. Figure 57: Case study I: prompt that appears safe under greedy evaluation but hides harmful stochastic tail. (a) For single jailbreak prompt x, we draw k=16 completions fYj(x)g16 j=1 from the model and display them as 4(cid:2)4 grid. Each cell is annotated with its sample index (with j=1 marked as Greedy) and colored by the human harm label: green for safe, orange for borderline / evasive, and red for harmful. Even though the greedy completion Ygreedy(x)=Y1(x) is labeled safe, we observe multiple harmful samples among the remaining Yj(x) when we enable sampling. (b) The right panel shows the full prompt and one-line summary of each of the 16 sampled completions, again colorcoded by label. In this particular case, we obtain j=1 Zj(x)=3 harmful completions and an empirical harmful mass bqθ(x) (cid:25) 0.19, which yields tailrisk estimate br16(x)=1, i.e., batch of 16 samples almost surely contains at least one harmful continuation. Deterministic (greedy) evaluation would thus certify as safe, even though realistic multisample usage reveals substantial harmful tail. This singleprompt case study makes the illusion of robustness visually explicit: what looks green under greedy decoding still hides significant red mass once we examine the full stochastic behavior."
        },
        {
            "title": "7 Discussion and Limitations",
            "content": "In this paper we argued that bitwise deterministic inference is not neutral default, but strong design choice that systematically collapses the stochastic structure of large language models. Across classification, controlled generation, multipath reasoning, and safety stress tests, we observed the same pattern: greedy, singletrajectory evaluation hides both latent competence and the true shape of model failures. This section synthesizes these findings, discusses implications for evaluation and deployment, and outlines key limitations and open directions. Takehome message. 1. LLMs are not compilers; they are stochastic semantic machines whose competence lives in the geometry of pθ(y x), not in any single string sampled from it. 2. Deterministic inference picks out one fragile path through that geometry and then treats it as if it were the model itself, collapsing uncertainty, diversity, and alternative trajectories into single token sequence. 3. What makes LLMs powerful is not their ability to be bitwise deterministic, but their ability to express and harness distributional variability in controlled way."
        },
        {
            "title": "7.1 Determinism as a Design Choice, Not a Default Law",
            "content": "A recurring theme in our experiments is that determinism is downstream of systems and decoding, not of the model itself. Transformers implement pθ(y x); inference code decides which parts of this distribution are visible. Deterministic collapse of stochastic structure. On GLUEstyle classification and related robustness suites, single greedy pass produces one label per perturbation and one scalar accuracy. The full stochastic distribution, explored via multisample decoding and perturbations, reveals: sizeable regions where success probability is high under stochastic decoding but appears mediocre under single deterministic run; brittle islands where performance is excellent on canonical prompts but degrades sharply under paraphrases and small input shifts. Similar effects appear in styleconstrained summarization and controlled generation: the space of instructioncompliant outputs is rich, yet greedy decoding traces only one narrow island within it. Emergent abilities as properties of (model, decoder) pairs. Our trajectoryspace analyses recast emergence as property of kernels over trajectories induced by decoding policies. lowentropy, argmaxstyle kernel can fail to express behaviours that are well supported by pθ((cid:1) x), while higher entropy policies (temperature sampling, multisample decoding, simple aggregation) reveal them. In this view, many emergent behaviours are as much about the decoder as about the weights θ."
        },
        {
            "title": "7.2 Reproducibility vs. Distributional Faithfulness",
            "content": "Deterministic inference is often motivated by desire for reproducibility. Our results suggest that at least three distinct notions are being conflated: 1. Bitwise reproducibility: identical prompts, identical bytes on the wire. 2. Distributional reproducibility: stable estimates of success probabilities, uncertainty profiles, and error modes under fixed sampling scheme. 3. Semantic stability: similar inputs yield behaviour that is stable in meaningful ways (e.g., predictions and justifications change smoothly under paraphrase or minor perturbations). Bitwise determinism is neither necessary nor sufficient. Our experiments highlight two failures of intuition: bitwisedeterministic system can be distributionally misleading: the unique greedy completion may look stable and benign even when nontrivial fraction of the distribution mass lies on conflicting or unsafe trajectories that never surface. stochastic system with fixed decoding scheme can be highly reproducible in distributional terms: repeated sampling yields stable success rates and variance estimates, even though the exact strings vary. For many scientific and safetyrelevant questions, the object of interest is explicitly distributional: robustness to paraphrase and perturbation, the shape of error tails, the probability of extreme events. For such questions, distributional faithfulness matters more than bitwise determinism. Reframing reproducibility. Rather than insisting that the same prompt must always yield the same string, our results argue for: reproducible evaluation pipelines (fixed decoders, fixed random seeds, fixed sampling budgets); reproducible distributional summaries (reported as means, quantiles, and uncertainty intervals over stochastic outputs); reproducible robustness profiles (performance as function of perturbations and decoding budgets). This is closer in spirit to how reproducibility is treated in training (multiple runs, variance reporting) than to the strict bitwise determinism common in current inference stacks."
        },
        {
            "title": "7.3 Implications for Evaluation Practice",
            "content": "Our analysis suggests several concrete changes to how LLMs should be evaluated. (1) From singleshot scores to stochastic profiles Across tasks, replacing one pass per example with modest multisample evaluation (k 2 f4, 8, 16g) consistently uncovers: higher attainable accuracies and success rates than those suggested by singlerun scores, and richer picture of robustness and brittleness across perturbations. We therefore recommend reporting curves score(k) and stability metrics (e.g., variance, interquartile ranges across samples) rather than only singlepoint estimates at k=1. (2) Robustness as firstclass metric The robustness ratios and heatmaps used in our classification and generation experiments make explicit how strongly models overfit to canonical benchmark phrasing. This suggests that: benchmarks should routinely include paraphrased, perturbed, and outofdistribution variants, with explicit reporting of relative drops in performance; model comparisons should specify the decoding regime as part of the evaluation protocol, since the ranking of models can change when moving from deterministic to stochastic decoders; benchmark releases should prioritise publishing perinput multisample outputs where possible, enabling secondary analyses of distributional behaviour. (3) Making trajectory diversity visible Our trajectoryspace visualizations demonstrate that capability is not single scalar but structured object: different regions of input space support different forests of trajectories, with varied depth, diversity, and stability. Making such diversity visiblethrough violin plots, multipath diagrams, or kernelbased projectionsturns what is currently hidden internal degree of freedom into an explicit part of model reporting."
        },
        {
            "title": "7.4 Implications for System Design and Deployment",
            "content": "From systems perspective, our results argue against treating bitwise determinism as onesize fitsall principle. Where strong determinism is still essential. There are scenarios where reproducible bytelevel behaviour is nonnegotiable: regression testing and continuous integration for large codebases; forensic analysis and scientific auditing, where exact replay of past behaviour is required; specific onpolicy RL pipelines that assume deterministic environment dynamics. In these cases, investing in batchinvariant kernels, stable lowlevel libraries, and careful environment fingerprinting is appropriate. Where deterministic defaults become misleading. In userfacing deployments and safety assessment, however, deterministic defaults can: underestimate risk, by never surfacing lowprobability but highimpact failures present in the tail of pθ((cid:1) x); underestimate capability, by failing to traverse valid reasoning paths or alternative solutions that stochastic decoding easily finds; overstate robustness, by conflating the stability of single trajectory with stability of the whole conditional distribution. Our safety analyses, in particular, show that systems can look extremely safe under greedy evaluation while retaining substantial harmful mass under modest multisample decoding. Toward decoderaware system design. more nuanced design philosophy would: expose deterministic and stochastic modes as explicit options, with clear documentation of trade offs; separate serving concerns (throughput, latency, cost) from evaluation and monitoring concerns (distributional fidelity, tailrisk estimation); integrate lightweight multisample diagnostics into deployment: periodic probing with > 1, paraphrasebased canaries, and other distributional checks."
        },
        {
            "title": "7.5 Limitations and Threats to Validity",
            "content": "Our study has several limitations, which bound the scope of the conclusions. Model and task coverage. We examine particular set of models and tasks: classification, controlled generation, reasoning, and safetyoriented prompts. Other architectures (e.g., larger mixtureofexperts, retrievalaugmented, or deeply multimodal models) might exhibit different quantitative patterns. The qualitative messagethat singletrajectory evaluation can diverge sharply from stochastic behaviouris likely to generalise, but should be rechecked in new domains. Decoding hyperparameters and search strategies. We instantiate stochastic decoding with small family of temperatures, topp, and multisample budgets. Alternative search procedures (beam search with diversity penalties, tree search over chains of thought, entropyregularised decoders) may change the precise numbers. Our claims concern the direction of effects: lowentropy, single path policies consistently hide behaviour that higherentropy policies reveal. Prompting and data contamination. We adopt standard prompting schemes and, for newer benchmarks, take care to mitigate trainingdata contamination. Nonetheless, we cannot guarantee that no examples (or close variants) appear in pretraining or tuning corpora, especially for proprietary models. Such contamination could make some gaps between deterministic and stochastic evaluation larger or smaller than they would be on fully heldout distributions. Metric choices and semantic nuances. Our metrics treat success/failure and constraint satisfaction as discrete events. For summarization and style, there is inherent ambiguity about what counts as equally good or semantically equivalent. Our automatic metrics and manual checks capture only part of this nuance. In principle, decoding regimes could differ substantially in surface form while remaining similar in downstream utility, or vice versa. Temporal and infrastructural drift. APIs, kernels, batching policies, and model versions evolve. Our experiments capture snapshot of rapidly moving ecosystem. Future infrastructure may reduce some sources of numerical nondeterminism, and future model families may be trained with stronger incentives for stability under paraphrase. Our empirical results should therefore be seen as evidence about current practice, not as fixed laws."
        },
        {
            "title": "7.6 Open Directions",
            "content": "Our perspective opens several directions for further work. Principled stochastic evaluators. If distributional variability is central, evaluation protocols should be designed to capture it efficiently. This calls for: principled choices of sampling budgets and perturbation families; confidence intervals and power analyses for stochastic metrics; benchmarks that explicitly score not only point estimates but full risk profiles as function of and input variation. Training for distributional robustness. Most finetuning and alignment pipelines optimise deterministic losses derived from one (or few) completions per input. Our results suggest investigating objectives that directly reward: stability of success probabilities across paraphrases and decoding regimes; smoothness of trajectoryspace representations under perturbations; robustness of multipath reasoning, not merely the quality of single chain. Safety diagnostics beyond one trajectory. The safety analyses in this work take only first steps toward distributional risk as the primary object. Future work could develop: rareevent estimation techniques tailored to LLM harmful modes; adaptive stress tests that jointly search over prompts and decoders; online monitors that track how the tail of pθ((cid:1) x) shifts under updates and deployment feedback. Beyond textonly LLMs. Realworld systems increasingly involve multimodal inputs, tools, and agents. Each layer introduces new stochastic components: perception, environment dynamics, tool responses, and interactive users. Extending the lens of deterministic vs. stochastic behaviour to these richer settings is natural next step, and will likely further diminish the appeal of strictly deterministic defaults. Overall, our results argue that the push toward bitwise deterministic inference, while useful for some engineering goals, is misaligned with the probabilistic nature of LLMs and with many of the questions we care about in capability evaluation and safety. The aim should not be to suppress stochasticity, but to measure, shape, and leverage it in controlled way."
        },
        {
            "title": "8 Conclusion",
            "content": "Our core message is simple but uncomfortable: treating large language models as if they were deterministic programs is category error. Modern LLMs are stochastic semantic machines, yet our dominant evaluation and deployment practices insist on single canonical outputa greedy, bitwisereproducible string that often tells comforting but misleading story. Through Stochastic CHAOS, we show that this deterministic lens systematically erases the very signals we most care about: it hides emergent abilities that only appear as phase transitions in success probability, it collapses rich multi-path reasoning into brittle single traces, and it creates deterministic illusions of robustness that vanish the moment we look at the underlying distribution instead of its mode. Conceptually, our results reframe nondeterminism from nuisance to be engineered away into first-class object of scientific inquiry. We disentangle three often conflated goalsbitwise determinism, distributional reproducibility, and semantic stabilityand demonstrate that overoptimizing the first can actively undermine the latter two. Even modest multi-sample budgets and lightweight perturbations expose substantial gaps between greedy and stochastic behavior in instruction following, compositional generalization, multi-step reasoning, and safety. These gaps are not small implementation artifacts: they reveal that key properties of LLM behavior live in the geometry of pθ(y x), not in any single draw from it. In this view, quantities such as exploration gain, illusion indices, and stochastic risk gaps are not optional diagnostics but indispensable coordinates for navigating model behavior. Practically, this shift demands rethinking of how we design benchmarks, inference stacks, and governance protocols. In particular, our findings suggest that future work should: (i) Elevate distributional metrics to first-class status. Benchmarks should report and optimize multi-sample quantitiessuccess probabilities, exploration gains, and risk gapsrather than celebrating single-run scores. Emergent capabilities should be documented as smooth or abrupt changes in success probability, not as binary pass/fail headlines. (ii) Treat reasoning as an ensemble phenomenon. Evaluation of chains-of-thought should instrument ensembles of trajectories, analyzing their diversity, stability, and failure modes, instead of anointing one canonical trace as the reasoning path. (iii) Redesign safety audits for tails, not modes. Safety evaluation and red-teaming should explicitly budget for exploring low-probability regionsparaphrased attacks, mixed decoders, and small sampling budgetsrather than certifying models under single, deterministic decoding recipe. (iv) Repurpose determinism as diagnostic tool, not the default norm. Engineering effort should shift from enforcing global bitwise determinism toward achieving robust distributional reproducibility and semantic stability under realistic, noisy serving conditions. Deterministic modes remain invaluable for debugging, ablation, and regression testing, but they should no longer be the primary lens for understanding or governing LLM behavior. Our study is necessarily scoped, and it opens more questions than it closes. We analyze subset of models, decoding policies, and stress tests; richer families of samplers, adaptive allocation of evaluation budgets to risky prompts, and training-time regularizers that encourage epistemically meaningful variability all remain open directions. Equally important are institutional questions: how should regulators, practitioners, and standards bodies translate distributional notions of risk into operational guarantees and service-level objectives? We view Stochastic CHAOS as starting point for this agenda. If the community is serious about understanding and governing LLMs as they truly are, then we must move beyond one prompt, one answer and embrace controlled randomness as central design axis. In the long run, honest generalization and transparent failure will not come from suppressing stochasticity, but from learning to measure, control, and reason with it."
        },
        {
            "title": "References",
            "content": "Berk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu, Rebecca J. Passonneau, Evan Radcliffe, Guru Rajan Rajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture, Zhe Wu, Lixinyu Xu, and Breck Baldwin. 2024. Stability of large language models under repeated evaluation. arXiv preprint, arXiv:2408.04667. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:18771901. Hou Pong Chan, Samuel Cahyawijaya, and Pascale Fung. 2021. Controllable summarization with constrained markov decision process. Transactions of the Association for Computational Linguistics, 9:12131232. Yan Chen, Yu Huang, Xiang Li, Houyi Li, Yanjie Zhao, Zhiru Zhang, G. Edward Suh, Christina Delimitrou, Christopher Batten, Edward F. Redmond, et al. 2022. Towards training reproducible deep learning models. In Proceedings of the 44th International Conference on Software Engineering (ICSE), pages 15511563. IEEE. Wei-Fan Chiang et al. 2013. Deterministic replay for scientific debugging of large-scale parallel programs. In Proceedings of SC. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Łukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. In Advances in Neural Information Processing Systems. Dataset: GSM8K. James Demmel, Willow Ahrens, and Hong Diep Nguyen. 2016. Efficient reproducible floating point summation and BLAS. Technical Report UCB/EECS-2016-121, EECS Department, University of California, Berkeley. Angela Fan, David Grangier, and Michael Auli. 2018a. Controllable abstractive summarization. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 4554, Melbourne, Australia. Association for Computational Linguistics. Angela Fan, Mike Lewis, and Yann Dauphin. 2018b. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), pages 889 898. Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova Dassarma, Tom Henighan, Andy Jones, Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Scott Johnston, Shauna Kravec, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Tom B. Brown, Jared Kaplan, Sam McCandlish, Chris Olah, Dario Amodei, and Jack Clark. 2022a. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT 22), pages 17471764. ACM. Deep Ganguli, Liane Lovitt, Nick Rider, Lilian Weng, Amanda Askell, Yuntao Bai, et al. 2022b. Red teaming language models with language models. arXiv preprint. ArXiv:2202.03286. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361. Dataset: StrategyQA. Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, Akbir Khan, Julian Michael, Sören Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, and Evan Hubinger. 2024. Alignment faking in large language models. arXiv preprint arXiv:2412.14093. Odd Erik Gundersen, Saeid Shamsaliei, Hakon Sletten Kjaernli, and Helge Langseth. 2023. On reporting robust and trustworthy conclusions from model comparison studies involving neural networks and randomness. In Proceedings of the 2023 ACM Conference on Reproducibility and Replicability (ACM-REP 23), pages 3761, New York, NY, USA. Association for Computing Machinery. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), volume 70 of Proceedings of Machine Learning Research, pages 13211330, Sydney, Australia. PMLR. Junxian He, Wojciech Kryściński, Bryan McCann, Nazneen Fatema Rajani, and Caiming Xiong. 2020. CTRLsum: Towards generic controllable text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 58665887, Online. Association for Computational Linguistics. Tony He and Thinking Machines Lab. 2025. Defeating nondeterminism in llm inference. https:// blog.connectionism.ai/defeating-nondeterminism-in-llm-inference. Blog post. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, volume 28. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proceedings of the 8th International Conference on Learning Representations (ICLR). Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, and Ethan Perez. 2024. Sleeper agents: Training deceptive llms that persist through safety training. arXiv preprint, arXiv:2401.05566. Aaqib Kaikaus, Adeel Anwar, and Yusuf M. Khan. 2024. Determinism of chatgpt in code generation: systematic evaluation. arXiv preprint, arXiv:2405.12345. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems. ArXiv:2205.11916. Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. 2024. Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico. Association for Computational Linguistics. InstruSum dataset: https://github.com/yale-nlp/InstruSum. Prabhat Nagarajan, Garrett Warnell, and Peter Stone. 2018. Deterministic implementations for reproducibility in deep reinforcement learning. arXiv preprint, arXiv:1809.05676. Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. CoRR, abs/1602.06023. OpenAI. 2024. Reproducible outputs. https://platform.openai.com/docs/guides/ reproducible-outputs. Accessed 2025-11-22. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20802094. Association for Computational Linguistics. Dataset: SVAMP. Ethan Perez, Sam Ringer, Jane Irvine, Andrew Kissane, Megan Chen, Lawrence Chan, Sean Heiner, Catherine Olsson, Samuel R. Bowman, Christopher Ré, et al. 2022. Discovering language model behaviors with model-written evaluations. In Advances in Neural Information Processing Systems 35 (NeurIPS). ArXiv:2206.11871. PyTorch Core Team. 2020. Reproducibility controlling sources of randomness in PyTorch. https: //pytorch.org/docs/stable/notes/randomness.html. Accessed 2025-11-22. PyTorch Developers. 2024. Reproducibility controlling sources of non-determinism in PyTorch. https://pytorch.org/docs/stable/notes/randomness.html. Accessed: 2025-11-27. Sudha Rao and Joel Tetreault. 2018. Dear sir or madam, may introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 129140, New Orleans, Louisiana. Association for Computational Linguistics. Shiori Sagawa, Jason Wei, Yiding Li, Yi Tay, Josip Djolonga, Tatsunori B. Hashimoto, Behnam Neyshabur, Ed H. Chi, Jeff Dean, William Fedus, Deep Ganguli, et al. 2023. The many faces of emergent abilities in large language models. arXiv preprint arXiv:2304.15004. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language In Advances in Neural Information Processing Systems, volume 36, pages models mirage? 5556555581. Mostafa Shahriari, Rudolf Ramler, and Lukas Fischer. 2022. How do deep-learning framework versions affect the reproducibility of neural network models? Machine Learning and Knowledge Extraction, 4(4):888911. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 16311642, Seattle, Washington, USA. Association for Computational Linguistics. Dipankar Srirag, Aditya Joshi, Jordan Painter, and Diptesh Kanojia. 2025. BESSTIE: benchmark for sentiment and sarcasm classification for varieties of english. In Findings of the Association for Computational Linguistics: ACL 2025, Vienna, Austria. Association for Computational Linguistics. Datasets and code: https://github.com/unswnlp/BESSTIE. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353355, Brussels, Belgium. Association for Computational Linguistics. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 7th International Conference on Learning Representations (ICLR). ArXiv:1804.07461. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain-of-thought reasoning in language models. arXiv preprint, arXiv:2203.11171. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2023b. Selfconsistency improves chain of thought reasoning in large language models. International Conference on Learning Representations (ICLR). ArXiv:2203.11171. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (NeurIPS). ArXiv:2201.11903. Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 11121122, New Orleans, Louisiana. Association for Computational Linguistics. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Graham Neubig, and Yuan Cao. 2024. Tree of thoughts: Deliberate problem solving with large language models. In Proceedings of the 41st International Conference on Machine Learning (ICML). ArXiv:2305.10601. Shunyu Yao, Dian Zhao, Yuan Yu, et al. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, volume 28. Ziyang Zhang, Xinheng Ding, Jiayi Yuan, Rixin Liu, Huizi Mao, Jiarong Xing, and Zirui Liu. 2025. Deterministic inference across tensor parallel sizes that eliminates training-inference mismatch. arXiv preprint, arXiv:2511.17826. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2024. Sglang: Efficient execution of structured language model programs. arXiv preprint arXiv:2407.01239. Yang Zhuang, Zachary C. Lipton, Srinivasan Parthasarathy, and Weiwei Tu. 2021. Randomness in neural network training: Characterizing the impact of tooling. In Proceedings of the 3rd Conference on Machine Learning and Systems (MLSys)."
        }
    ],
    "affiliations": [
        "Apple, USA",
        "Google, USA",
        "Pragya Lab, BITS Pilani Goa, India",
        "Raapid Lab, USA"
    ]
}