{
    "paper_title": "MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios",
    "authors": [
        "Zhiheng Song",
        "Jingshuai Zhang",
        "Chuan Qin",
        "Chao Wang",
        "Chao Chen",
        "Longfei Xu",
        "Kaikui Liu",
        "Xiangxiang Chu",
        "Hengshu Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench ."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 8 3 6 2 2 . 2 0 6 2 : r MobilityBench: Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios Zhiheng Song Computer Network Information Center, Chinese Academy of Sciences AMAP, Alibaba Group Beijing, China songzhiheng2004@gmail.com Chao Wang University of Science and Technology of China Heifei, China wangchaoai@ustc.edu.cn Jingshuai Zhang AMAP, Alibaba Group Beijing, China zhangjingshuai0@gamil.com Chuan Qin Computer Network Information Center, Chinese Academy of Sciences Beijing, China chuanqin0426@gmail.com Chao Chen AMAP, Alibaba Group Beijing, China cc201598@alibaba-inc.com Longfei Xu AMAP, Alibaba Group Beijing, China longfei.xl@alibaba-inc.com Kaikui Liu AMAP, Alibaba Group Beijing, China damon@alibaba-inc.com Xiangxiang Chu AMAP, Alibaba Group Beijing, China cxxgtxy@gmail.com Hengshu Zhu Computer Network Information Center, Chinese Academy of Sciences Beijing, China zhuhengshu@gmail.com Abstract Route-planning agents powered by large language models (LLMs) have emerged as promising paradigm for supporting everyday human mobility through natural language interaction and toolmediated decision making. However, systematic evaluation in realworld mobility settings is hindered by diverse routing demands, nondeterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios Both are co-first authors and contribute equally to this work. Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/XXXXXXX.XXXXXXX and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAPML/MobilityBench. Keywords Large language models, route-planning agents, benchmarking ACM Reference Format: Zhiheng Song, Jingshuai Zhang, Chuan Qin, Chao Wang, Chao Chen, Longfei Xu, Kaikui Liu, Xiangxiang Chu, and Hengshu Zhu. 2026. MobilityBench: Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nThe advance of large language models (LLMs) has catalyzed the\nemergence of tool-augmented agents, which integrate natural lan-\nguage reasoning with executable actions via external APIs [18, 21].\nBy grounding user intent in programmatic interactions with real-\nworld services, such agents substantially broaden the range of tasks\nthey can support, from simple information retrieval to complex\ndecision-making workflows, such as web navigation [15, 22], com-\nputer interaction [9, 14], and route planning [4, 33].",
            "content": "Among these agents, route-planning agents constitute particularly challenging application domain, operating under diverse and dynamic real-world constraints that shape everyday human mobility [3, 5, 24]. Real-world mobility requests extend far beyond Conference acronym XX, Song & Zhang et al. Figure 1: Overview of MobilityBench, systematic benchmark for evaluating route-planning agents. simple point-to-point navigation [30], often involving multiple, interacting constraints, such as user preferences (e.g., avoiding highways or minimizing transfers), ordered waypoints, modalitydependent conditions, and time-sensitive requirements. Addressing such demands requires agents to accurately interpret nuanced user instructions, invoke appropriate travel-related APIs, and generate executable itineraries with reliable cost estimatesincluding travel time, distance, and transfer countscapabilities that remain difficult to evaluate systematically in realistic mobility settings. Recent benchmarks for evaluating the planning capabilities of LLMs and agents, such as TravelBench [5] and TravelPlanner [24], primarily focus on high-level itinerary generation and abstract constraint reasoning. As result, they fall short of capturing the complexity of route planning for everyday human mobility, which requires fine-grained reasoning over large-scale, map-based environments and dynamically changing conditions. Meanwhile, systematically evaluating route-planning agents in real-world mobility scenarios still faces several fundamental challenges: (1) scalable scenario coverage, as evaluation must span route-planning problems of varying difficulty and combinations of constraints, ranging from simple point-to-point queries to complex multi-constraint requests; (2) non-determinism of live mapping APIs, whose responses vary over time due to traffic dynamics, service availability, and backend updates [13, 26], thereby undermining reproducibility and fair comparison; (3) comprehensive and reliable evaluation, as effective assessment requires integrating multiple objective criteria beyond LLM-based subjective judging [34] to verify APIcall validity, constraint satisfaction, and factual grounding; and (4) extensible and reproducible evaluation toolkit, as rapid advances in LLM backbones and agent frameworks demand lightweight, modular toolkit that supports easy deployment, scalable data expansion, and consistent evaluation across settings. To address these challenges, we introduce MobilityBench, scalable benchmark for evaluating route-planning agents in realworld mobility scenarios. MobilityBench is constructed from largescale, anonymized real user queries collected from Amap, one of the largest map and navigation service providers in China, and is designed to reflect the diversity and complexity of everyday mobility needs while removing all personally identifiable information. It covers broad spectrum of real-world route-planning intents, including point-to-point routing, customized multi-waypoint itineraries, and multimodal route planning that integrates driving, walking, cycling, and public transit. In addition, MobilityBench supports preference-aware navigation, such as avoiding highways or minimizing transfers, as well as mobility-related information access, including bus station details, bus line information, and road congestion status. The benchmark spans queries from over 350 cities worldwide and is designed to be easily extensible, enabling continuous expansion to new regions, scenarios, and intent types. Given the inherent non-determinism and reproducibility challenges of live mapping services, MobilityBench is built around deterministic API-replay sandbox that enables reproducible, endto-end evaluation of route-planning agents. During dataset construction, responses fromrouting and points-of-interest APIs are captured and cached through standardized interface, effectively freezing traffic conditions and service states at the time of collection. During evaluation, all API calls issued by an agent are intercepted and resolved against the cached response store, ensuring that identical inputs consistently yield identical, verifiable outputs. By eliminating uncontrolled environmental variance introduced by live services, this sandbox-based design ensures that measured performance faithfully reflects an agents reasoning and tool-use capabilities rather than fluctuations in external systems. MobilityBench: Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios Conference acronym XX, We further propose multi-dimensional evaluation protocol that centers on outcome validity while providing complementary assessments of instruction understanding, planning, tool use, and efficiency. This protocol integrates multiple objective criteria to verify executable correctness, constraint satisfaction, and grounded API usage, enabling fine-grained and reliable assessment beyond surfacelevel plausibility. To facilitate reproducible research and rapid iteration, we publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench, supporting easy deployment, extensibility to new agent frameworks, and consistent comparison across models and settings."
        },
        {
            "title": "3.1 Benchmark Construction\n3.1.1 Episode-centric Formulation. To enable rigorous evaluation\nof route-planning agents in realistic mobility scenarios, Mobility-\nBench adopts an episode-centric formulation, in which each episode\nencapsulates a self-contained mobility request solvable via tool\naugmentation. Formally, an episode is represented as a four-tuple\nğ‘’ = (ğ‘¥, ğ‘§, S, ğ‘¦), where:",
            "content": "ğ‘¥ denotes an anonymized natural-language user query; ğ‘§ encodes contextual information associated with the request, such as user location, city, and other background variables relevant to mobility decision-making; denotes fixed and replayable snapshot of relevant API responses provided by the replay sandbox (Section 3.1.4), enabling consistent and deterministic evaluation across agent runs; and Conference acronym XX, Song & Zhang et al. Table 1: Overview of task scenarios in MobilityBench, grouped by intent family. Intent Family Task Scenario Example Query Basic Information Retrieval POI Query Geolocation Query Nearby Query Weather Query Traffic Info Query Where is the gas station? Where am I? Search for restaurants near Beijing Capital International Airport. What is the weather like in Wuhan tomorrow? Is there traffic jam on Chengdu Avenue right now? Route-Dependent Information Retrieval Route Property Query Arrival/Departure Time Query How far is it from Hefei to Huangshan? If drive from my home to Capital International Airport now, when will arrive? Basic Route Planning Point-to-Point Planning Multi-stop Planning Drive from Tiananmen Square to Capital International Airport. Route planning starting from No. 60 Qinhe Road, via Yinji Mall and Zhenghong City. Preference-Constrained Route Planning Option-Constrained Route Planning Route-Constrained Planning Plan driving route to Shanghai Disneyland that avoids tolls/highways. Route to Shanghai Disneyland via Peoples Square, avoid Inner Ring Elevated Road. ğ‘¦ denotes structured ground-truth annotation constructed (Section 3.1.3) and used exclusively to support the automated evaluation protocol (Section 3.2). It is never exposed to the agent and serves solely for evaluation and diagnostic analysis. Throughout this work, the route-planning agents are not permitted to ask users for clarification. Consequently, all episodes are designed to be fully solvable based solely on the initial user query ğ‘¥."
        },
        {
            "title": "3.1.2 Data Collection and Task Taxonomy Construction. Mobility-\nBenchis constructed from large-scale, anonymized mobility queries\ncollected from AMap over the past six months. In real-world on-the-\ngo scenarios such as driving or walking, safety and convenience\nconstraints limit usersâ€™ ability to interact with mobile devices, mak-\ning voice a natural and prevalent input modality for expressing\nmobility intent. As a result, voice queries provide direct and largely\nunconstrained expressions of real user intent, encompassing des-\ntination goals, situational information needs, and explicit prefer-\nence constraints. In our dataset, these voice queries are transcribed\ninto text and treated uniformly as query inputs for subsequent\nprocessing. From a large corpus of raw queries, we construct the\nbenchmark through a multi-stage filtering and curation pipeline,\nresulting in a substantial collection of high-quality episodes. Under\na strict no-clarification assumption, where each query must be self-\ncontained and solvable without follow-up interaction, we remove\nmalformed, underspecified, or ambiguous requests and deduplicate\nnear-identical queries to ensure diversity.",
            "content": "Following this approach, we leverage Qwen-4B to perform intent classification over the curated queries, identifying diverse real-world mobility scenarios that define the task taxonomy of our benchmark. Specifically, we initialize the process with two coarse-grained intent roots: information access (e.g., POI, traffic, and weather lookup) and route planning (e.g., navigation to destination). To identify long-tail and previously unobserved intents, we adopt an open-set labeling protocol, whereby queries that cannot be aligned with existing labels prompt the model to propose new candidate intents along with concise definitions. These candidate labels are subsequently iteratively consolidated, merged, and refined through multiple rounds of expert adjudication, ensuring semantic clarity, mutual exclusivity, and comprehensive coverage of the intent space. The resulting taxonomy comprises 11 task scenarios, which are further organized into four high-level Task families: Basic Information Retrieval, which encompasses fundamental information-seeking tasks, including POI Query, Geolocation Query, Nearby Query, Weather Query, and Traffic Info Query. Route-Dependent Information Retrieval, which targets information needs that require computing route as an intermediate step, including Route Property Query (e.g., distance or path characteristics) and Arrival/Departure Time Query. Basic Route Planning, which consists of two standard navigation tasks: Point-to-Point Planning, routing from single origin to single destination, and Multi-stop Planning, routing across multiple intermediate destinations. Preference-Constrained Route Planning, which covers route planning tasks involving explicit user-specified preferences or constraints beyond basic navigation. This family includes OptionConstrained Route Planning, which applies tool-native, standardized routing options such as minimizing tolls, preferring highways, optimizing for the fastest route, fewer transfers, or less walking; and Route-Constrained Planning, which enforces explicit path-level constraints specified by users, such as required waypoints or excluded roads. Table 1 presents representative examples for each task scenario, while detailed scenario definitions and additional examples are provided in Appendix A, Table S1."
        },
        {
            "title": "3.1.3 Ground-Truth Construction. To enable automated evalua-\ntion, we construct a structured ground-truth annotation ğ‘¦ for each\nepisode following scenario-specific standard operating procedures\n(SOPs) defined by domain experts, which specify the minimal se-\nquence of tool interactions required to correctly resolve a query.\nSpecifically, we construct a scenario-specific standard tool program\nthat defines the minimal sequence of tool calls required to answer\na query. The workflow operationalizes the corresponding SOP as\na structured and executable program, executes it within an exist-\ning agent framework to orchestrate tool invocations, validates the\nresulting outputs against historical data with reliability filtering,\nand consolidates the full execution trace together with key inter-\nmediate artifacts into a ground-truth archive. The standard tool",
            "content": "MobilityBench: Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios Conference acronym XX, program consists of three core steps: (i) extracting and normalizing query slots such as points of interest, temporal constraints, travel modes, and user preferences; (ii) resolving textual locations into structured entities or geographic coordinates via POI retrieval or geocoding tools; and (iii) after parameter validation, invoking downstream tools including routing, real-time traffic, and weather services while verifying constraint feasibility when applicable. The resulting tool evidence is then converted into structured reference ğ‘¦ for automated evaluation and diagnostic analysis."
        },
        {
            "title": "3.1.4 Deterministic Replay Sandbox. During ground-truth con-\nstruction, we rely on tools provided by the AMap Web Service API1\nto derive reference outputs. During evaluation, however, agents are\nprohibited from querying live API endpoints, as real-time updates\n(e.g., dynamic traffic and weather conditions) and external factors\n(e.g., API rate limits) would otherwise introduce non-determinism\nand compromise fair and reproducible comparisons. Instead, all\ntool interactions are routed through a deterministic replay sandbox\nthat serves pre-recorded, contextually consistent responses.",
            "content": "The replay sandbox returns responses captured during groundtruth execution and ensures deterministic behavior across agent runs. Each tool invocation is resolved from pre-recorded cache keyed by canonicalized arguments, such as normalized coordinates and standard time formats. When an exact cache hit is unavailable, the sandbox applies task-appropriate fallback strategies, including fuzzy matching for entity-based queries and nearest-neighbor spatial matching for coordinate-based queries, subject to maximum distance threshold. All tool invocations undergo strict schema validation, including required-field checks and type and range constraints. Calls that fail validation or cannot be resolved are treated as tool-use failures and are explicitly reflected in the evaluation metrics (Section 3.2), enabling fair and reproducible evaluation."
        },
        {
            "title": "3.2 Evaluation Protocol\nTo enable a comprehensive and in-depth evaluation of route-planning\nagents across diverse mobility scenarios, we introduce a multi-\ndimensional evaluation protocol. Existing evaluations predomi-\nnantly rely on end-to-end success rates, which treat agent behavior\nas a black box and obscure the intermediate failures along the\ndecision-making chain. Such coarse-grained metrics are insuffi-\ncient for diagnosing the complex reasoning processes required in\nrealistic route planning tasks. To address this limitation, our pro-\ntocol decomposes an agentâ€™s behavior into four core capabilities:",
            "content": "1https://lbs.amap.com/api/webservice/summary Figure 2: Global coverage of MobilityBench Data. Instruction Understanding, Planning, Tool Use, and Decision Making, corresponding to the key stages of route-planning reasoning. Each capability is further quantified using set of fine-grained indicators, enabling precise diagnosis of performance bottlenecks and failure modes that are invisible to end-to-end metrics. Instruction Understanding. Since accurate interpretation of 3.2.1 user requirements is prerequisite for route planning, we first evaluate the agents instruction understanding capability. Drawing on standard paradigms in natural language understanding [1, 11], this capability is assessed through two indicators, detailed as follows: Intent Detection (ID). We quantify the agents ability to understand the instructional intent embedded in user query. Specifically, the agent is explicitly instructed to output set of intent labels corresponding to the task scenario categories defined in Section 3.1.2. We measure intent detection by comparing the agents predicted intent label Ë†ğ‘¦ID (ğ‘¥) with the ground-truth intent label ğ‘¦ID (ğ‘¥) for each query ğ‘¥. prediction is considered correct if the similarity between the two labels exceeds predefined threshold ğ›¼threshold. The overall intent detection score is computed as: ID = 1 ğ‘¥ I(sim( Ë†ğ‘¦ID (ğ‘¥), ğ‘¦ID (ğ‘¥)) ğ›¼threshold) . (1) Information Extraction (IE). This indicator evaluates an agents ability to extract explicit and implicit constraints from user queries, including spatial attributes (e.g., origins and destinations), temporal parameters (e.g., departure windows and duration constraints), and preference-related signals (e.g., traffic avoidance or modality priorities). For query ğ‘¥, let Ë†ğ‘¦IE (ğ‘¥) and ğ‘¦IE (ğ‘¥) denote the predicted and ground-truth constraint sets, respectively. An extraction is considered correct only if the two sets exactly match. The overall IE score is computed as: IE = 1 ğ‘¥ I( Ë†ğ‘¦IE (ğ‘¥) = ğ‘¦IE (ğ‘¥)) . (2)"
        },
        {
            "title": "3.2.2 Planning. Effective planning is a core capability of LLM-\nbased agents, especially in real-world mobility scenarios where\nroute planning requires multi-step reasoning under uncertainty.\nThis dimension evaluates the agentâ€™s ability to generate a logically\ncoherent and sequential execution plan for complex routing tasks.\nTask Decomposition (DEC). This dimension evaluates an agentâ€™s\nability to decompose a high-level user goal into a coherent sequence\nof atomic actions, reflecting whether the agent produces the right\nsteps without omissions or redundancy. Given a predicted action",
            "content": "Conference acronym XX, Song & Zhang et al. sequence ğ‘‰ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥) = {ğ‘£1, ğ‘£2, ..., ğ‘£ğ‘› } and the corresponding groundtruth sequence ğ‘‰ğ‘”ğ‘œğ‘™ğ‘‘ (ğ‘¥), we assess task decomposition quality by jointly considering step coverage and step correctness, that is, DEC-P = DEC-R = 1 1 ğ‘¥ ğ‘¥ ğ‘‰ğ‘”ğ‘œğ‘™ğ‘‘ (ğ‘¥) ğ‘“ğ·ğ¸ğ¶ ğ‘‰ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥) ğ‘‰ğ‘”ğ‘œğ‘™ğ‘‘ ğ‘‰ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥) ğ‘“ğ·ğ¸ğ¶ ğ‘‰ğ‘”ğ‘œğ‘™ğ‘‘ (ğ‘¥) ğ‘‰ğ‘ğ‘Ÿğ‘’ğ‘‘ , , (3) where ğ´ ğ‘“ğ·ğ¸ğ¶ ğµ = {ğ‘ ğ´ ğ‘ ğµ, ğ‘“ğ·ğ¸ğ¶ (ğ‘, ğ‘) = True} and ğ‘“ğ·ğ¸ğ¶ (, ) is function that determines whether two atomic actions are considered match."
        },
        {
            "title": "3.2.3 Tool Use. Tool invocation serves as the interface between the\nagent and the sandbox environment. To comprehensively evaluate\nan agentâ€™s tool invocation capability, we define three evaluation\nindicators: tool selection, schema compliance, and parameter filling.\nTool Selection (TS). This metric evaluates whether an agent cor-\nrectly identifies the required tool(s) from a candidate tool set T\nbased on the inferred user intent. Let ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥) denote the set of\ntools selected by the agent, and ğ‘‡ğ‘”ğ‘œğ‘™ğ‘‘ (ğ‘¥) denote the ground-truth\nset of required tools. We measure tool selection quality from two\ncomplementary aspects: coverage and redundancy. Coverage re-\nflects whether all necessary tools are selected, while redundancy\npenalizes unnecessary tool calls, (for easier comparison, we report\nredundancy as its complement, 1 âˆ’ redundancy):",
            "content": "TS-P = TS-R = 1 1 1 ğ‘¥ ğ‘¥ ğ‘‡ğ‘”ğ‘œğ‘™ğ‘‘ (ğ‘¥) ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥) ğ‘‡ğ‘”ğ‘œğ‘™ğ‘‘ ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥) ğ‘‡ğ‘”ğ‘œğ‘™ğ‘‘ (ğ‘¥) ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ , . (4) Schema Compliance (SC). This metric evaluates whether an agents tool invocation conforms to predefined API specifications, requiring that all mandatory parameters are provided and that their values fall within valid formats and ranges. For each query ğ‘¥, let ğ‘†ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥) denote the sequence of tool invocations produced by the agent, and let ğ‘ƒ (ğ‘¡) denote the set of parameters associated with each tool call ğ‘¡ ğ‘†ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥). We define ğ‘“ğ‘†ğ¶ (ğ‘ƒ (ğ‘¡), ğ‘¡) as an indicator function that determines whether the parameters provided for tool ğ‘¡ conform to the predefined valid formats and ranges. Along this line, the overall SC score is calculated by: SC = 1 ğ‘¥ 1 (cid:12)ğ‘†ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥)(cid:12) (cid:12) (cid:12) ğ‘¡ ğ‘†ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¥ ) ğ‘“ğ‘†ğ¶ (ğ‘ƒ (ğ‘¡), ğ‘¡) . (5)"
        },
        {
            "title": "4.1.4 Experimental Details. To ensure reproducibility and fair com-\nparison, we applied a unified set of evaluation settings across all\nLLM backbones and agent frameworks.",
            "content": "MobilityBench: Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios Conference acronym XX, Table 2: Performance of models on MobilityBench. Abbreviations: Instr. Und. for Instruction Understanding; Dec. Mak. for Decision Making; ID for Intent Detection; IE for Information Extraction; DEC for Task Decomposition; TS for Tool Selection; SC for Schema Compliance; DR for Delivery Rate; FPR for Final Pass Rate; IT for Input Token; and OT for Output Token. Model ID IE DEC-P DEC-R TS-P TS-R SC DR FPR IT OT Instr. Und. Planning Tool Use Dec. Mak. Efficiency ReAct 85.86 GPT-4.1 82.16 GPT-5.2 88.70 Claude-Sonnet-4.5 Claude-Opus-4.5 85.99 Gemini-3-Flash-Preview 84.00 83.54 Gemini-3-Pro-Preview 78.18 DeepSeek-V3.2-Exp 77.89 Qwen3-4B 74.73 Qwen3-30B-A3B 80.87 Qwen3-32B 82.13 Qwen3-235B-A22B 94.40 GPT-4.1 89.58 GPT-5.2 97.21 Claude-Sonnet-4.5 Claude-Opus-4.5 76.82 Gemini-3-Flash-Preview 97.28 96.35 Gemini-3-Pro-Preview 96.93 DeepSeek-V3.2-Exp 95.98 Qwen3-4B 95.56 Qwen3-30B-A3B 96.03 Qwen3-32B 97.24 Qwen3-235B-A22B 90.07 89.65 93.06 91.23 88.16 88.75 90.78 86.75 91.23 88.46 90.51 94.79 96.58 95.69 95.81 94.41 95.32 95.92 94.53 94.35 94.63 94. 75.53 81.24 80.71 84.12 71.95 68.70 71.85 47.24 70.60 68.37 72.23 89.46 81.90 89.46 88.80 89.60 88.97 89.62 86.83 83.91 86.83 89.39 82.38 82.42 82.83 83.21 90.37 90.74 87.99 80.74 84.04 83.08 84.13 74.14 62.22 74.76 70.15 68.34 65.11 77.19 81.56 72.93 77.58 77.75 Plan and Execute 68.85 74.68 71.81 70.99 66.18 65.71 69.55 73.26 71.19 66.98 66.96 84.61 81.12 84.63 84.76 85.44 85.12 83.83 81.64 82.91 84.25 84.59 81.92 76.20 82.99 83.73 76.46 75.04 82.19 72.82 83.35 83.94 84. 73.26 75.94 78.17 76.53 68.13 64.38 75.28 69.20 68.97 69.80 73.49 97.00 95.49 97.42 97.52 98.31 98.70 98.23 94.46 97.06 96.76 97.24 97.02 95.94 96.89 97.22 97.86 97.35 97.23 96.88 97.48 97.17 97.01 79.23 79.09 80.62 80.20 85.18 84.38 84.95 63.80 84.57 83.16 85.95 80.70 77.26 81.96 83.53 80.50 78.64 80.73 78.06 78.81 80.24 81.22 61.66 61.90 63.17 62.22 67.90 69.09 68.88 53.80 66.65 65.68 66. 63.40 59.81 64.31 65.77 62.87 62.80 63.06 59.55 60.60 62.43 64.16 18680.81 18304.90 18856.68 19672.63 21072.79 20164.76 15427.89 26078.99 15013.79 15544.50 15391.23 13426.36 15312.45 13267.99 12643.41 14515.42 15936.49 12394.29 13612.71 14820.45 13658.79 12563.66 1166.27 1166.12 1311.01 1305.40 1232.76 1242.48 622.05 657.78 560.19 583.22 604.73 747.35 1644.18 863.81 808.83 784.06 815.26 706.14 673.03 667.69 703.31 703.60 Agent Inputs. Each agent instance received the user query along with spatial context signals, such as city and geographic location. When tool use was enabled, we additionally provided structured tool schemas or invocation patterns to standardize tool usage across different frameworks and backbones. Model Configuration. To further control evaluation variance, we set the sampling temperature to 0.1 for all evaluated LLM backbones and capped the maximum output length at 8, 192 tokens. Agent Configuration. To balance inference efficiency and robustness (e.g., preventing degenerate tool-calling loops), we limited the maximum number of inference steps to 10."
        },
        {
            "title": "4.2 Experimental Results\n4.2.1 Overall Performance.\nLLM performance. Under the Plan-and-Execute framework, Claude-\nOpus-4.5 stands out as the strongest performer, achieved a De-\nlivery Rate of 83.53% and a Final Pass Rate of 65.77%, both the\nhighest among all evaluated models in this setting. Within the Re-\nAct framework, Gemini-3-Pro-Preview attained the highest FPR of\n69.09%. This result highlights its exceptional ability to preserve task-\nrelevant context and maintain goal focus across extended iterative\ninference loops.\nClosed-Source vs. Open-Source Models. As shown in Table 2,\nClaude-Sonnet-4.5, Gemini-3-Pro-Preview still maintained a clear\nlead in instruction understanding dimensions, with average scores\nof 90.88% and 88.61% under the ReAct framework. However, the",
            "content": "gap is narrowing significantly. Among open-source models, Qwen3235B-A22B, MoE architecture activating only 22B parameters per forward pass, achieved DR of 85.95% and an FPR of 66.69% under the ReAct framework. Similarly, DeepSeek-V3.2-Exp demonstrated strong competitiveness, attaining an FPR of 68.88% while maintaining substantially lower inference costs due to its efficient architecture. This provides high-performance and cost-effective option for enterprise-level private deployments. Framework Comparison: ReAct vs. Plan-and-Execute. systematic comparison of the two execution architectures reveals fundamental trade-off between task success rate and computational efficiency. The final pass rate of the ReAct is generally better than that of Plan-and-Execute. This is mainly due to its closed-loop \"think-act-observe\" mechanism, which allows the agent to dynamically adjust its strategy based on real-time results returned by tools, while Plan-and-Executes static pre-planning shows significant lack of robustness when facing dynamic feedback in mobile scenarios. However, ReActs superior robustness comes at nontrivial computational cost. Due to the continuous accumulation of observation history within the inference context, the average number of input tokens (IT) consumed by ReAct is significantly higher than that of Plan-and-Execute. Across all models, ReActs average IT is approximately 35.38% higher than Plan-and-Executes. This increase translates directly into higher API costs and longer wall-clock inference times. Conference acronym XX, Song & Zhang et al. Figure 3: Performance across four high-level task families. Scenario Study. To further reveal the capabilities of the model 4.2.2 in different task scenarios, we created multi-dimensional indicator radar charts for four core categories in Figure 3, evaluating representative open-source and closed-source models under both ReAct and Plan-and-Execute frameworks. The scene from left to right represents significant increase in the depth of task logic and the complexity of constraints, and Preference-constrained Planning is the category where the model is the most likely to be error as we expected. In this type of tasks, Plan-and Execute framework performs best because it establishes clear strategy in advance, which makes handling structured tasks with logical order more predictable and efficient, thereby suppressing illusions and trajectory deviations."
        },
        {
            "title": "4.2.3 Model Study. We conduct a model-centric study to examine\nhow model scaling and reasoning mode (Thinking vs. Non-thinking)\ninfluence route-planning agent performance on MobilityBench.\nScaling effect. Experiments reveal a clear performance gap across\nmodel sizes (Table 2). Under the same dense architecture, scaling\nthe base model from 4B to 32B yields a consistent improvement in\naverage success rate, increasing by 0.91%. Under the MoE setting,\nQwen-30B-A3B further scales to Qwen-235B-A22B, bringing an\nadditional gain of 5.43% . Overall, these results align with the classic\nscaling law: increasing parameter scale leads to higher success rates\nin real-world mobility scenarios. By jointly examining DEC-P and\nDEC-R, we observe that, compared with smaller models, larger\nmodels tend to produce longer solution trajectories (i.e., more plans)\nto explore a broader space of possible outcomes. Although some\nof these steps can be redundant, this more exhaustive search-and-\nverification process ultimately improves the task success rate.\nThinking vs. Non-thinking. To examine the intrinsic potential\nof LLMs on complex route-planning tasks, we study the impact\nof reasoning mode (Thinking vs. Non-thinking) while accounting\nfor the extra cost and latency introduced by Thinking. We sample\n1,000 representative instances from MobilityBench for a controlled\ncomparison, and evaluate how different reasoning patterns affect\nfinal task success. Figure 4 reports the final pass rate of each model\nwith and without Thinking enabled.",
            "content": "We evaluate Qwen-4B, Qwen-32B, Qwen-30B-A3B, and Qwen235B-A22B under both settings, and additionally include DeepSeekR1 as strong reasoning-oriented baseline (Figure 4). DeepSeek-R1 Figure 4: Final pass rate comparison (Thinking vs. Nonthinking) under the Plan-and-Execute framework. achieves final pass rate of 70.46%, serving as competitive reference point. Across models, enabling thinking consistently improves performance, with the largest gain observed for Qwen-30B-A3B, final pass rate increased by 5.98% absolutely. Despite these gains, Thinking substantially increases the generated token volume, leading to markedly higher inference cost and latency. This overhead makes it challenging to deploy Thinking-enabled agents in realtime, production-grade online settings."
        },
        {
            "title": "5 Conclusion\nIn this work, we presented MobilityBench, a scalable benchmark for\nthe systematic evaluation of LLM-based route-planning agents in\nreal-world mobility scenarios. Built from large-scale, anonymized\nreal user queries, MobilityBench captured the diversity and com-\nplexity of everyday mobility demands while enabling reproducible,\nend-to-end evaluation through a deterministic API-replay sandbox.\nWe further introduced a multi-dimensional evaluation protocol cen-\ntered on outcome validity and complemented by assessments of\ninstruction understanding, planning, tool use, and efficiency. Using\nMobilityBench, we evaluated multiple LLM-based route-planning\nagents across diverse real-world mobility scenarios and conducted\nan in-depth analysis of their behaviors and performance, reveal-\ning both their strengths and limitations under realistic conditions.\nMobilityBench provides a robust and extensible foundation for ad-\nvancing research on route-planning agents and for enabling fair\nand reproducible comparison across LLMs and agent frameworks.",
            "content": "MobilityBench: Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios Conference acronym XX, References [1] ICMLT 2020: 2020 5th International Conference on Machine Learning Technologies. 2020. Proceedings of the 2020 5th International Conference on Machine Learning Technologies. [2] Palaash Agrawal, Shavak Vasania, and Cheston Tan. 2025. Can LLMs Perform Structured Graph Reasoning Tasks?. In International Conference on Pattern Recognition. Springer, 287308. [3] Soumyabrata Chaudhuri, Pranav Purkar, Ritwik Raghav, Shubhojit Mallick, Manish Gupta, Abhik Jana, and Shreya Ghosh. 2025. Tripcraft: benchmark for spatio-temporally fine grained travel planning. arXiv preprint arXiv:2502.20508 (2025). [4] Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, and Jiangjie Chen. 2024. Travelagent: An ai assistant for personalized travel planning. arXiv preprint arXiv:2409.08069 (2024). [5] Xiang Cheng, Yulan Hu, Xiangwen Zhang, Lu Xu, Zheng Pan, Xin Li, and Yong Liu. 2025. TravelBench: Real-World Benchmark for Multi-Turn and ToolAugmented Travel Planning. arXiv preprint arXiv:2512.22673 (2025). [6] Daniel Delling, Peter Sanders, Dominik Schultes, and Dorothea Wagner. 2009. Engineering route planning algorithms. In Algorithmics of large and complex networks: design, analysis, and simulation. Springer, 117139. [7] EW DlJKSTRA. 1959. Note on Two Problems in Connexion with Graphs. Numer. Math. 50 (1959), 269271. [8] Peter Hart, Nils Nilsson, and Bertram Raphael. 1968. formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics 4, 2 (1968), 100107. [9] Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. 2024. The dawn of gui agent: preliminary case study with claude 3.5 computer use. arXiv preprint arXiv:2411.10323 (2024). [10] Zhehui Huang, Guangyao Shi, and Gaurav Sukhatme. 2024. Can Large Language Models Solve Robot Routing? arXiv preprint arXiv:2403.10795 (2024). [11] Mourad Jbene, Abdellah Chehri, Rachid Saadane, Smail Tigani, and Gwanggil Jeon. 2025. Intent detection for task-oriented conversational agents: comparative study of recurrent neural networks and transformer models. Expert Systems 42, 2 (2025), e13712. [12] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael Mahoney, Kurt Keutzer, and Amir Gholami. 2024. An llm compiler for parallel function calling. In Forty-first International Conference on Machine Learning. [13] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench: Evaluating llms as agents. ICLR (2023). [14] Junting Lu, Zhiyang Zhang, Fangkai Yang, Jue Zhang, Lu Wang, Chao Du, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. 2025. Axis: Efficient human-agent-computer interaction with api-first llm-based agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 77117743. [15] Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Wenhao Yu, and Dong Yu. 2023. Laser: Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172 (2023). [16] Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng, and Kai-Wei Chang. 2024. Llm-a*: Large language model enhanced incremental heuristic search on path planning. arXiv preprint arXiv:2407.02511 (2024). [17] Yansong Ning, Rui Liu, Jun Wang, Kai Chen, Wei Li, Jun Fang, Kan Zheng, Naiqiang Tan, and Hao Liu. 2025. Deeptravel: An end-to-end agentic reinforcement learning framework for autonomous travel planning agents. arXiv preprint arXiv:2509.21842 (2025). [18] Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems 37 (2024), 126544126565. [19] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 (2023). [20] Yincen Qu, Huan Xiao, Feng Li, Gregory Li, Hui Zhou, Xiangying Dai, and Xiaoru Dai. 2025. TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation. arXiv preprint arXiv:2510.09011 (2025). [21] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems 36 (2023), 6853968551. [22] Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and Ameet Talwalkar. 2025. WorkflowAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data. In ICLR 2025 Workshop on Foundation Models in the Wild. [23] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chainof-thought reasoning by large language models. arXiv preprint arXiv:2305.04091 (2023). [24] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622 (2024). [25] Huimin Yan, Longfei Xu, Junjie Sun, Ni Ou, Wei Luo, Xing Tan, Ran Cheng, Kaikui Liu, and Xiangxiang Chu. 2025. Intsr: An integrated generative framework for search and recommendation. arXiv preprint arXiv:2509.21179 (2025). [26] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems 35 (2022), 2074420757. [27] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. ğœbench: Benchmark for Tool-Agent-User Interaction in Real-World Domains. arXiv preprint arXiv:2406.12045 (2024). [28] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems 36 (2023), 1180911822. [29] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. [30] Jiahao Yu, Yihai Duan, Longfei Xu, Chao Chen, Shuliang Liu, Kaikui Liu, Fan Yang, Xiangxiang Chu, and Ning Guo. 2025. DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario Route Ranking. In Companion Proceedings of the ACM on Web Conference 2025. 567576. [31] Liangqi Yuan, Dong-Jun Han, Christopher Brinton, and Sabine Brunswicker. 2025. LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences. arXiv preprint arXiv:2509.12273 (2025). [32] Junlin Zeng, Xin Zhang, Xiang Zhao, and Yan Pan. 2025. 1000 Faster LLMenhanced Algorithm For Path Planning in Large-scale Grid Maps. arXiv preprint arXiv:2510.02716 (2025). [33] Tao Zhe, Rui Liu, Fateme Memar, Xiao Luo, Wei Fan, Xinyue Ye, Zhongren Peng, and Dongjie Wang. 2025. Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents. arXiv preprint arXiv:2510.06078 (2025). [34] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems 36 (2023), 4659546623. [35] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406 (2023). Appendix A.1 MobilityBench Task Scenarios To facilitate thorough understanding of the benchmarks coverage and design rationale, we present detailed taxonomy of task scenarios in Table S1, including fine-grained subtypes and their definitions, and provide additional representative examples for each category, which are designed to reflect the diversity of natural language expressions that users may employ when issuing mobility-related instructions. A.2 Sandbox Tools core design principle of MobilityBench is to evaluate agents within realistic yet reproducible tool-use environment. To this end, we provide comprehensive tool specification table as shown in Table S2. It documents each tool used in the benchmark sandbox, including the tool name, input arguments and output fields. The sandbox tools are sourced from the AMap Open Platform. More detailed parameter definitions and response field descriptions are available in the official documentation2. 2https://lbs.amap.com/api/webservice/summary Conference acronym XX, Song & Zhang et al. Table S1: MobilityBench task scenarios. For each scenario, we provide concise definition and representative user queries. Introduction Query Examples Scenario POI Search Geolocation Query Retrieve point of interest (POI) by name or category and return key attributes (e.g., address, latitude/longitude). Reverse geocoding converts coordinates (or the current location) into an address, place name, and administrative region. Nearby Search Find POIs within specified radius of target location. Weather Query Query current weather and forecasts for target area to support travel decisions. Traffic Info Query Retrieve real-time traffic congestion information for roads or areas, including severity and affected segments. RouteProperty Query Query attributes of given route/itinerary (distance, duration, transfers, etc.). Arrival/Departure Time Query Plan routes with time constraints (depart-at/arrive-by) and infer feasible schedules. Point-to-Point Planning Plan route from an origin to destination under specified travel mode. Multi-stop Planning Plan an ordered multi-stop route that visits multiple waypoints sequentially. Option-Constrained Route Planning Plan routes based on standard user preferences supported by the routing API (e.g., avoid_tolls, avoid_highways, minimize_transfers). Customized Planning Plan routes under bespoke constraints that must be satisfied (e.g., designated line/stop/segment). Find Starbucks. Search for pharmacy in Nanshan District. Where is the shopping mall? Give me my current location. Tell me where am right now. Whats the latitude and longitude of Beijing Railway Station? Any parking lots within 500 meters of my location? Find the nearest EV charging station. Where is the nearest restroom nearby? Im arriving in Hangzhou tomorrowwhats the weather like there? Whats the temperature in Beijing tomorrow morning? Give 3-day forecast for Shenzhen. How is traffic on Yanan Elevated Road right now? Is there congestion near Guomao? How is the traffic flow on the way to the airport? How long from Lujiazui to Hongqiao by metro? How many transfers are there on this transit route? Whats the distance to Jiuzhaigou Valley? must arrive at the airport by 7:30; when should leave? My train departs at 9:00 PM tonightwhats the best time to leave for Nanchang Railway Station? If leave at 6 PM, can reach the concert by 7? How do get from Pudong Airport to The Bund by subway? Drive from Tsinghua University to Sanlitun now. Bike from my location to Zhongshan Park. Start from the Grand Hyatt Beijing, stop at Wangfujing Department Store, then proceed to Beijing South Railway Station. Travel from Guangzhou South Railway Station to Chimelong Tourist Resort via Tianhe Sports Center. Drive to the zoo but avoid highways. Take public transit with at most one transfer. Find the cheapest route to the airport. must take Metro Line 2; plan the route to the stadium. Route to the hospital via Peoples Square Station. Plan route to the airport with the fewest traffic lights. MobilityBench: Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios Conference acronym XX, Table S2: Overview of map-related tools and their tool-function I/O. Tool poi_query nearby_poi_query Function Input Output Search points of interest (POIs) using keywords, categories, city or city code. Retrieve nearby POIs within radius matching category/keyword. keyword(s), category, city, optional filters (e.g., limit). Candidate POIs: name, address, coordinates, category, brief metadata. center coordinate (lat/lon), radius, keyword/category, optional filters (e.g., limit/sort). Nearby POI list with distance (optional), name, address, coordinates, category. reverse_geocoding Convert geographic coordinates into human-readable address. coordinate (lat/lon). weather_query Query current weather or forecast for location. city name or coordinate (lat/lon), time range/type (current/forecast). traffic_info_query Retrieve real-time/recent traffic conditions for road segment/area. road segment/area identifier or polyline/bbox, optional time window. Address fields (province/city/district, street, number), nearby landmark/POI (optional), formatted address. Weather report: temperature, precipitation, wind, humidity, conditions; air quality (optional). Traffic status: congestion level, speed, incidents/events (optional), timestamp, suggested impact on ETA (optional). driving_planning Plan driving route between origin and destination. bus_planning Plan public-transit route between origin and destination. origin (lat/lon), destination (lat/lon), optional waypoints, route preferences (avoid highways/tolls), traffic-aware flag. origin (lat/lon), destination (lat/lon), departure time (optional), preferences (bus or subway, min transfers). bicycling_planning Plan cycling route between origin and destination. origin (lat/lon), destination (lat/lon), optional preferences (bike lanes). distance, Route: line/geometry, traffic-aware ETA (optional). polyturn-by-turn steps, ETA, plan: lines, transfers, Transit walking segments, total duration, fare/operating info (if available), step details. route: distance, ETA, Cycling polyline/geometry, step-by-step directions, elevation/road-type hints (optional). walking_planning Plan walking route between origin and destination. origin (lat/lon), destination (lat/lon). route: Walking polyline/geometry, directions. distance, ETA, step-by-step"
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Computer Network Information Center, Chinese Academy of Sciences",
        "University of Science and Technology of China"
    ]
}