{
    "paper_title": "Build the web for agents, not agents for the web",
    "authors": [
        "Xing Han Lù",
        "Gaurav Kamath",
        "Marius Mosbach",
        "Siva Reddy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 3 5 9 0 1 . 6 0 5 2 : r Build the web for agents, not agents for the web Xing Han Lù Gaurav Kamath Marius Mosbach Siva Reddy McGill University Mila Quebec AI Institute Equal Advising"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agentsAI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be collaborative effort involving the broader ML community."
        },
        {
            "title": "Introduction",
            "content": "Rapid advancements in the capabilities of large language models (LLMs), including multimodal models, have resulted in an increased interest in designing digital agents: artificial intelligence systems that, unlike conventional LLMs, can not only respond to user input, but also execute range of actions directly, from booking flight tickets to sending emails. For example, if user requests to book flight from Montreal to Boston, an LLM may suggest options and provide approximate durations, whereas digital agent can actually complete the booking on the users behalf, and have the booking details sent to the users inbox. To this end, recent works have focused on the problem of designing agents for different type of interfaces, including web APIs [Schick et al., 2023, Qin et al., 2023], mobile UIs [Rawles et al., 2023, 2024], desktop UIs [Xie et al., 2024, Qin et al., 2025], and web browsers [Liu et al., 2018, Nakano et al., 2022, Gur et al., 2023, He et al., 2024]. The focus of this position paper is on one specific type of digital agents that specializes on navigating websites through mediums like the web browser; we refer to them to as web agents. Acquiring this autonomy unlocks crucial capabilities in web agents that basic chat assistants lack, enabling users to complete web-related tasks that would otherwise not be possible with standard LLM assistants. One clear advantage is their ability to execute user-defined tasks independently, whether these involve making online purchases or sending messages. On top of these novel capabilities, web agents also offer more response-adaptability to user inputs than standard LLM-based assistants, due Preprint. Under review. to their ability to interact with online environments. For example, web agents can address user queries by directly accessing and interacting with websites, making them more suitable for situations where some desired knowledge is not present in text format, is frequently updated, or requires explicit online interaction (such as asking online vendors for price estimates). Current approaches to building web agents range from zero-shot prompting of LLMs through visual grounding [Zheng et al., 2024] to finetuning-based approaches that leverage planning [Gur et al., 2023, Sodhi et al., 2024], multimodality [Shaw et al., 2023, Furuta et al., 2023], and reinforcement learning [Liu et al., 2018, Qi et al., 2025]. Across all approaches, however, browser states are presented to the agent (i) as screenshots of the browser; (ii) via the underlying web page, as tree structure refined from the HTML elements present on the page; or (iii) through combination of the two. These means of presenting information to the web agent, however, face fundamental drawbacks. Screenshots do not directly provide agents with all comprehensive webpage information that may be required (for instance, elements in collapsed dropdown menu will not be visible), while providing web agents full DOM trees is often inefficient and computationally expensive, as these can include details that are irrelevant to the user, and used only to render the webpage. Similarly, web APIswhich may not face these representational issuesencounter limitations in other areas, due to their developer-oriented design. Additionally, in both cases, safety concerns arise, given that web agents may have access to sensitive personal information required to execute actions (e.g., payment and login information). While the approaches discussed above are valid approaches for building web agents, their limitations point to deeper way in which these approaches are ultimately misguided: they involve trying to build web agents that interact with the web the same way that humans do. More specifically, the tools that agents are trained to interact withe.g., the web browser, or web APIsare designed for human users and developers, and not for agentic AI systems. In this position paper, we argue that agents should not be forced to interact with the web in the same way as human users. Beyond designing web agents for human web interfaces, we argue that the research community should be deeply involved in the design of web interfaces for web agents. To this end, we present the concept of the Agentic Web Interface (AWI): new type of interface specifically designed to be used by web agents. This mirrors how user interfaces are designed to be used by humans, who do not have the same requirements as web agents. Given the nature of this work, we explicitly do not provide prototype or specific implementation details, as we believe designing AWIs will be collective and iterative effort involving broad range of stakeholders. We structure this paper as follows: in Section 2, we describe current approaches to designing web agents and highlight limitations and safety concerns. We then introduce AWIs in Section 3, discuss how they can be designed to solve the current issues faced by web agents, and offer guiding principles that should inform their design. Next, in Section 4, we argue that this effort is relevant to and requires contributions from the broader ML community. Finally, we discuss how AWIs differ from the model context protocol (MCP) in Section 5 and summarize our position in Section 6."
        },
        {
            "title": "2 How are web agents currently designed?",
            "content": "Web agents are designed to perform goal-directed tasks through programmatic interaction with web interfaces. To enable this, recent implementations utilize LLMs as core component of these agents [Gur et al., 2023, Zheng et al., 2024, Drouin et al., 2024, Boisvert et al., 2025b]; however, they are only part of bigger system, which interacts with websites through browser. Following Zhou et al. [2024], we define web agents within sequential decision-making framework where, given task described as natural language intent i, e.g., find all white shoes in size 10 and add them to my wishlist, the agent issues an action at at time step according to policy π(ati, ot, a1, o1). This policy conditions on the intent i, the current observation ot O, the action history a1, and the observation history o1. The execution of action at in the environment results in state transition to st+1 with corresponding observation ot+1 O. The action space of web agents includes operations analogous to human web interactions: element selection (clicking), text input, URL navigation, and browser tab management. These actions are executed via programmatic automation frameworks such as Playwright1, which return updated browser states after each agent intervention. The effectiveness of task completion can then be evaluated through reward function r(a1, s1) that assesses whether the final state satisfies the intent criteria. The reward function be an automatic 1https://playwright.dev 2 evaluator that programmatically determine success of final state based on human-defined rules [Zhou et al., 2024, Koh et al., 2024a, Drouin et al., 2024], or specialized reward model that can determine the success of trajectory without having access to the underlying environment or rules [Murty et al., 2024, Lù et al., 2025, Xue et al., 2025]. In this section, we provide an overview of web agents by examining two approaches: agents that solely rely on web browser UIs, (2.1), and agents that additionally incorporate web APIs (2.2). We briefly cover each category and highlight the limitations they each face as result of having to interact with interfaces designed for humans. 2.1 Browser-based agents Web agents that rely on the browsers UI may leverage visual representations from the screen [Shi et al., 2017, Shaw et al., 2023], the Document Object Model (DOM) tree [Deng et al., 2023], the accessibility tree [Zhou et al., 2024], or combination of them [e.g., Furuta et al., 2023, Lù et al., 2024, Boisvert et al., 2025b, Gou et al., 2025]. In addition to the raw visual and textual inputs, additional annotations like bounding boxes are sometimes added onto the screenshots [e.g., Yang et al., 2023, Koh et al., 2024a, Chezelles et al., 2025]. This multimodal representation allows holistic representation of the webpage, with the DOM showing the structure of the webpage and screenshots showing images that are not displayed inside the DOM. This paradigm enables the development of agents capable of predicting and executing browser actions, such as element selection, text input, URL navigation, and tab management operations. Representational limitations Browser-based web agents face limitations imposed by both DOMbased and screenshot-based representations. Screenshots provide token-efficient representations of browser state, but lack comprehensive DOM information that may be visually occluded i.e., not rendered or hidden inside dropdown. On the other hand, DOM-based representations, despite usually containing the most decision-relevant information, are extremely inefficient due to excessive structural tokens and supplementary attributes (such as server-side identifiers). This inefficiency in turn leads to higher computational and operational costs when using LLMs; for instance, with DOM trees potentially exceeding 1M tokens [Chezelles et al., 2025], deploying GPT-4.1-based agent for single 20-step task could cost roughly $402. While previous research has attempted various mitigation strategies [e.g., Deng et al., 2023, Tiwary et al., 2024, Lù et al., 2024], these solutions typically do not generalize well across websites and novel task scenarios. Resource challenges and defensive designs Beyond representational limitations, the proliferation of browser-based agents introduces additional resource challenges. Serving web pages can often be resource-intensive, and repeated rendering by browser automation tools (e.g., for web crawling) can lead to strain on web infrastructure, leading to performance degradation for end users. As web agents begin to constitute larger and larger proportion of web traffic, we can expect this resource consumption issue to increase in severity. In response to such challenges, website operators implement defense mechanisms like CAPTCHA; but as agent capabilities have become increasingly sophisticated, CAPTCHA systems have similarly become increasingly complex, creating accessibility barriers for legitimate human users. Although exclusion protocols like robots.txt could be enforced by regulatory bodies for major providers, they will penalize agents used as an assistive technology indistinguishably from agents built for web crawling. More generally, challenges arise in distinguishing between malicious automation and beneficial human-in-the-loop applications, leading to difficulties in web resource management. Safety and privacy concerns Since they are integrated within the browser, browser-based web agents introduce major safety and privacy concerns. If the web agent has access to the users personal accounts, as well as other sensitive information stored in the browsersuch as passwords and credit card informationagents that lack adequate safeguards may cause severe harm to users through their actions. web agent may, for example, use the users personal information to send harmful messages or make unauthorized online purchases [Tur et al., 2025, Boisvert et al., 2025a]. 2This estimate assumes $2 per million input tokens, and may increase for reasoning models. 2.2 API-based hybrid agents Agents that use tools like web APIs offer high potential benefits to users, particularly when considering tools like Deep Research [OpenAI, 2025], that can themselves inform agent behavior. Song et al. [2025] defines hybrid agent that extends the browser-based agent by giving it access to the underlying web API of the websites. Their agent alternates between API calls and browser interactions to complete web navigation task from the WebArena benchmark [Zhou et al., 2024]. While the incorporation of web APIs by hybrid agents opens new possibilities, it also raises questions about fundamental limitations of web APIs in agentic contexts. Most crucially, it begs the question: can web agents solely rely on the internal web API offered by websites? We argue that besides the previously discussed limitations that solely browser-based agents face, this approach will face limitations and safety concerns specific to web APIs used to power websites. Limitations of web APIs API-based agents are limited by the range of actions offered by the web APIs, which are far narrower than the range of actions offered by webpage UIs. Similarly, web APIs are typically not designed to directly manipulate stateful objects, such as sorting list of products on online shopping website. This limits the potential actions that an API-based agent could utilize. One could add state-centric actions, but such an endeavour would require substantial refactoring effort from the developers, which would solely benefit API-calling agents. Additionally, web APIs tend to be heavily controlled by the developers, and frequent usage of the APIs in short period can lead to request denials. These factors hold back the effectiveness of incorporating web APIs into web agents, as they have to fall back to using the UI when an action cannot be completed using the web API, which face the limitations discussed in Section 2.1. Safety concerns Internal web APIs are designed to strictly communicate with browsers, as opposed to external APIs that are designed as service. If used directly, internal APIs may pose security risks, as agents may lack essential safeguards to avoid unintended side effects [Levy et al., 2024, Boisvert et al., 2025a] and are prone to follow malicious and harmful instructions [Andriushchenko et al., 2025, Tur et al., 2025]. More concretely, guardrails like password prompts and two-factor authentication can be bypassed if the agent directly communicates with the web API through an elevated API key. This could lead to high API usage if uncontrolled, which could results in unexpectedly high expenses."
        },
        {
            "title": "3 Rethinking how agents interact with the web",
            "content": "As we have argued, the two primary entry points for agents to interact with the webUIs and tools (e.g., web APIs)are not optimized for use by agents, causing downstream issues with agent deployment. In order to address these limitations, we argue that the agent research community should rethink how agents interact with the web. To this end, we believe that ML researchers should work together to design unified interfaces for web agents to optimally access web content. We refer to these as Agentic Web Interfaces, and believe they are essential for web agent performance. Whereas APIs are targeted at developers and browser UIs are designed for users, AWIs should be built for agents to assist users in interactive setting or to complete user-defined tasks autonomously. In this section, we examine how AWIs approach the limitations faced by existing interfaces (3.1), offer guiding principles for designing AWIs (3.2), and then conclude by presenting more concrete suggestions for AWI design (3.3). 3.1 Addressing issues faced by agents through AWIs Existing work has primarily focused on designing agents around existing web interfaces like APIs and browser UIs, which are faced with the limitations discussed in Section 2. Such issues are unlikely to be directly solved by API and UI developers, as their primary focus is to build interfaces for humans. At the same time, designing workarounds for existing interfaces would require substantial and decentralized effort from individual groups of researchers, which may result in mutually exclusive solutions that may eventually be superseded after major updates to an API or UI. For instance, popular website may see its client-side source code completely rewritten from an older JavaScript framework to more recent one, which would completely change the observations received by the agent, while keeping the same UI. Similarly, website may go through complete visual redesign, 4 but the underlying architecture may remain the same. We therefore believe that the long-term solution is to design AWIs: web interfaces that are specifically designed for use by agents. Developing AWIs will directly fix many of the issues highlighted above that agents face. While the limited range of actions offered by most current web APIs means that tool use agents face constraints in how they can interact with online content, AWIs would allow for action spaces to be tailored to desired agent use. For example, an AWI may deliberately allow web agent to execute more actions than the web API allows for, but fewer actions than human user is allowed to execute. Similarly, AWIs would address the representational limitations and resource challenges faced by browser-based web agents. Designing dedicated web interfaces for agents would involve presenting them with tailored browser state representations that neither contain excessive, superfluous details (like the raw website DOM tree), nor lack comprehensive DOM information (like screenshots). This higher efficiency would significantly reduce the computational cost of web agents. On the other hand, dedicated AWIs would address concerns of web resource management, as they could involve agentic task queues (see Section 3.3) that handle the traffic of agents while consistently ensuring accessibility and adequate resources for human users. This effectively provides an incentive to website owners to implement AWIs, as they would avoid development effort towards managing resources due to higher traffic from agents. Additionally, the implementation of AWIs can be potentially abstracted through the use of coding agents [Yang et al., 2024], which could reduce the barrier for adopting AWIs. Finally, AWIs would address many of the safety concerns entailed by web agents. By controlling the information that agents have access to, and carefully limiting its range of executable actions, AWIs enable vital safeguards for web agent behavior. 3.2 Guiding principles for designing AWIs We believe the design of AWIs should be iterative and involve wide range of stakeholdersfrom ML researchers to website developers and end usersso that AWIs enable agents to navigate the web in ways that align with the interests of all stakeholders. Consequently, we do not present any single precise blueprint for designing AWIs, as optimal designs will likely require experimenting with several alternatives to identify what works best for broad range of websites and users. Instead, we provide list of guiding principles that we feel are crucial to design good AWIs: 1. Standardized: AWIs should follow standards that clearly define the structure of the interface and the action space supported. They would need to be designed by working group of experts across ML disciplines, who ensure that they are compatible with various agent designs. 2. Human-centric: AWIs should be designed to be used by agents that benefit human users. They should always preserve human agency, safety and privacy. For example, human should be able to pause an ongoing trajectory and steer the agent by requesting changes to the task, which should work seamlessly with any AWI. 3. Safe: AWIs should be designed to be safely used by agents, and should have defense mechanisms against malicious agents, malicious content, and catastrophic failures during trajectories; this can be accomplished through proper access controls, guardrails and privacypreserving methods, which will ensure the safety of the users and websites. 4. Optimal representations: AWIs should produce efficient representations when provided as observations to web agents. They should contain enough information for the agent to optimally solve the tasks and should not include information that the agent will not need. Moreover, agents should be able to specify exactly the information type (e.g., images) and parameters (e.g., resolution and compression level). 5. Efficient to host: AWIs should avoid increasing the total computational load of website. Since agents may be deployed in autonomous settings, or may attempt to navigate multiple websites at the same time, the total traffic will increase as web agents become more widespread. Thus, the efficiency will be crucial to ensure the scalability of web trajectories inside real web environments without increasing hosting costs for website owners. 6. Developer-friendly: Whereas researchers may be the ones designing AWIs, the developers of the websites will ultimately implement, deploy and maintain them. AWIs will need to work seamlessly with the architecture and infrastructure of the website, and ensure that it does not disrupt the hosting and reliability of the web services. 5 3.3 Concrete suggestions for AWIs Given the guiding principles mentioned above, we provide several suggestions that could be incorporated into future AWI designs. Our recommendations may be not be universally relevant to all use-cases, but we believe they will be helpful initial step towards more detailed blueprints. Unified higher-level actions In addition to more efficient representation of the trajectory state, the interface can further augment the action space by abstracting primitive elements, allowing for unified high-level actions. In existing action spaces for web tasks, such as the one defined by BrowserGym [Chezelles et al., 2025], we are already able to find actions like goto, which combines the actions of selecting the address bar, inserting an URL address, and pressing Enter to load the webpage. We can view them as high-level functions, which can take higher-level inputs and compose primitive functions (click, type, etc.) that can induce desired outcome like opening webpage by its URL address. By unifying the action space within the specifications of the agentic web interface, the same higher-level actions can be used across different websites with the same outcome, without having to worry about the internal implementation of the website. Compatibility with user interfaces To enable the use of web agents on UIs for humans, developers can decide to design AWIs to be compatible with traditional browsers. To accomplish this, actions on AWIs could be executable on the UIs, and the state of AWIs could match updates to the state of the UI. To this end, AWI developers can build bidirectional translation tool that allows converting actions between AWIs and UIs, which would allow the UI to match the state of the AWI; this could be accomplished through tools like Playwright or Selenium3. The resulting tool can be shared among the community, allowing faster developments of UI-compatible AWIs. Access control for agents Given agents would navigate the website through an AWI, we would need to define Access Control Lists [Shirey, 2007] specifically for agents. The agent should have restricted access to sensitive user information, such as their credit card information; to ensure such information can be used, they can use privacy-preserving password managers. Moreover, the agent could require the explicit confirmation of the user before performing potentially destructive actions, such as deleting an account. Changing the access of the agent would enable greater level of security, which is an area of research previously explored [Levy et al., 2024], and would allow the users to set their preferred permission levels for web agents that use their accounts. To differentiate the user from the agent, the website could request passkeys or biometric authentication, avoiding CAPTCHAs. Progressive information transfer Many web tasks require high amount of bandwidth and can be compute-intensive. For instance, gallery of high-resolution images could take over 100MB to be sent to the user, and would require expensive animations to display. However, the agent would not need the high-resolution images nor the animations to solve task. Instead, AWIs can constrain the information to be progressively sent to the agent in more efficient ways. In our example, the AWI could send an initial portion of the images resized to an optimal size or as embeddings. This would save substantial amount of bandwidth, thus leading to reduced costs and latency. Agentic task queues Designing task queues specifically for AWIs would allow the website developer to set maximum number of concurrent agents to access and navigate website, with subsequent agents having to wait until slot frees up before starting their navigation. The queue could distribute the access across the day to avoid usage spikes on certain times of the day, enabling greater number of agents to be included in queue for website. As result, human users can use the website as they usually would, without worrying about latency caused by usage spikes."
        },
        {
            "title": "4 Why must the broader ML community be involved?",
            "content": "Our position results not merely from an engineering convenience, but instead from strong belief that fundamental reorientation of web agent research is needed. By shifting focus from navigating interfaces built for humans to building agent-specific communication standards, we can accelerate progress toward genuinely intelligent web agents while also establishing more rigorous foundations 3https://selenium.dev 6 Table 1: In this table, we enumerate the guiding principles (3.2) alongside corresponding suggestions (3.3) and relevant ML disciplines (4). The guiding principles should inform any concrete implementation steps, and determine which other ML disciplines must be involved. Principles (3.2) Suggestions (3.3) ML Disciplines Involved (4)"
        },
        {
            "title": "Standardized",
            "content": "Unified higher-level actions Generalization, NLP Human-centric"
        },
        {
            "title": "AI Safety",
            "content": "Optimally-represented"
        },
        {
            "title": "Progressive information transfer",
            "content": "Generalization, NLP, RL"
        },
        {
            "title": "Progressive information transfer",
            "content": "Planning, RL Developer-friendly"
        },
        {
            "title": "All",
            "content": "for measuring and advancing agent capabilities. As result, we believe that implementing AWIs is of great relevance not only to those working on downstream applications of web agents, but rather the ML community at large. Below, we highlight the relevance of agentic web interfaces to various ML communities, in order to underscore both the importance of AWIs for their research, and the need for their involvement in designing AWIs. We show how each principle and suggestion corresponds to one or more disciplines in Table 1. Human-centric AI (HCAI) An AWI will enable further opportunity to design personalized agentsagents that are specifically tailored to users individual preferences [e.g., Cai et al., 2025]. Additionally, designing AWIs to allow both UI-based and chat-based web navigation will help improve the experience in scenarios where the user has access and knows how to use UI, as well as scenario where the user wishes to avoid using the UI, instead deferring the task to the web agent. Rather than building web agents that replace users, HCAI research would instead help guide the design of interoperability between AWI and the traditional UI in way that augments users, who may expect different levels of autonomy from agents based on their preferences [Ge et al., 2024]. Thus, having HCAI researchers partake in the design process would allow AWIs to be implemented in way that will benefit broader range of user goals and preferences. AI Safety Recent works on agent safety [Levy et al., 2024, Tur et al., 2025, Andriushchenko et al., 2025] highlight the substantial effort that will be required to design agents that are not only capable, but also safe. As the adoption of web agents increases, they will used in progressively more critical and sensitive scenarios. The AI safety research community will therefore play crucial role in guiding the design and development of AWIs, enabling safer out-of-the-box web environments for web agents to navigate. Notably, AI safety researchers can design AWIs to be robust to, prompt-level [Liao et al., 2025], HTML-level [Boisvert et al., 2025a] or vision-level attacks [Wu et al., 2024], and to carefully choose ACLs based on the nature of the task and the sensitivity level of the website. Natural Language Processing (NLP) Several topics within NLP can leverage AWIs to build more capable agents. For example, to achieve an optimal representation, AWIs could use iterative summarization [Wan et al., 2007, Yan et al., 2011, Zhang et al., 2023] to abstract unnecessarily verbose sections of the webpage, while keeping more relevant ones intact. Not only would this help AWIs produce concise representations of website to enhance the decision making of web agents, they will also provide salient representations that can be stored in collection, enabling them to be retrieved in future task as supporting artifacts. Thus, AWIs could be built with retrieval augmentation [Lee et al., 2019, Guu et al., 2020, Lewis et al., 2020] in mind, enabling the use of retrieval methods like dense encoders [Karpukhin et al., 2020], late interactions [Khattab and Zaharia, 2020], and LLM-based embeddings [BehnamGhader et al., 2024]. The retrieval-augmented web agents can leverage the retrieved knowledge or memory segments to complete contextual tasks, which would have otherwise required clarification from the user or resulted in refusal. Multimodality For researchers working on multimodality, AWIs hold great potential, as they represent an opportunity to boost the multimodal capabilities of agents. Current screen-based approaches often put the multimodal abilities of agents at disadvantage, as visual artifacts (such as product images in an online catalogue, or photos in gallery) tend to be passed to the agent as part of single snapshot [Shaw et al., 2023, Rawles et al., 2024, Zheng et al., 2024, Chezelles et al., 2025]. With AWIs, multimodal models could be provided purposefully processed media from webpage, in formats that benefit their capabilitiessuch as resized versions of images that take up too much or too little space in single screenshot, or truncated versions of long video or audio recordings. With the help of benchmarks focused on the multimodal capabilities of web agents [e.g., Koh et al., 2024a, Jang et al., 2024, Liu et al., 2024a,b], research on multimodality can help quantify the effectiveness of different representations of multimodal artifacts for web agents. In turn, these findings can be incorporated into AWI design, creating virtuous cycle that enables multimodal research to focus less on adapting models to human interfaces and more on how to represent multimodal inputs. Reinforcement Learning Designing AWIs will have strong consequences for reinforcement learning, as the limitations of current approaches to web agent development do not only extend to the safety and resource allocation concerns discussed already; instead, they cause inefficiencies in the learning process itself. Under current approaches, computing reward signal is unnecessarily complex, requiring elaborate engineering to determine task completion based on UI state [Liu et al., 2023]. Instead, standardized interface design would enable the development of reward functions that can produce consistent success or failure signals. Similarly problematic is the inconsistent action space that agents must navigate. Current approaches require web agents to learn low-level UI actions [Caccia et al., 2024, Patel et al., 2024, Murty et al., 2024, 2025, Qin et al., 2025], which can be inconsistent across agent frameworks. For instance, BrowserGyms action space [Chezelles et al., 2025] defines actions for sending messages to the user and for opening or closing tabs. On the other hand, the concurrent action space proposed by [Qin et al., 2025] does not include any tab-level action and does not allow agents to respond to user, instead defining an action specifically for requesting user intervention. unified high-level action standard would supersede action spaces defined by individual research works with consistent and extensible action space, enabling policy learning to be compatible across different frameworks. Planning AWIs will further enable research on search and exploration or, more broadly, planning in the context of web agents. By strictly interacting with AWIs, agents that leverage planning would be able to create search paths in the sandbox version of website without overloading it for human users, which has shown promising results in simulated paths [Gu et al., 2025]. Through the more efficient design and the isolated nature of the interface, we can substantially increase the number of episodes that an agent can complete within the same timeframe, while ensuring that the website remains functional for human users. Consequently, methods like Reflexion [Shinn et al., 2023], SkillWeaver [Zheng et al., 2025] and Tree Search [Koh et al., 2024b] can be scaled to achieve better test-time performance, whereas methods like ADaPT [Prasad et al., 2024] can be accelerated to enable real-time usage. Thus, designing AWIs to be efficient for planning-based agents would result in reduced website usage load and bandwidth costs, which could incentivize more website owners to adopt sandboxes for search-based agents. Generalization For researchers focused on task generalization, AWIs provide an opportunity to better study the generalization capabilities of agents. Many current approaches involve training agents on specific UI interaction patterns [Furuta et al., 2023, Lai et al., 2024, Trabucco et al., 2025], which ties agent capabilities to particular interface implementations. This may result in brittle systems that fail when encountering novel UI designs and therefore generalize poorly across websites [Li et al., 2024, Lù et al., 2024, Pan et al., 2024]. Training on standardized implementations, however, allows one to decouple task knowledge from interface knowledge, holding the potential for more effective transfer across websites. As result, ML researchers can focus on higher-level generalization problems (such as adapting agents to solve novel tasks) rather than the lower-level problems caused by differences in the DOM trees and UIs between websites. 5 Is an AWI another communication protocol for LLM agents? Tool use for LLMs is well researched topic [Qin et al., 2023, Schick et al., 2023], which paved the way for providers to integrate tool use into popular LLM assistants, such as Search for ChatGPT [OpenAI, 2024] and Function Calling for Gemini [Google, 2025]. As adoption grows, need to 8 standardize interactions between LLM agents (i.e., hosts) and tools (i.e., servers), led to the introduction of the Model Context Protocol (MCP) [Anthropic, 2024]. The protocol aims to standardize how LLM assistants, such as Claude [Anthropic, 2023], communicate with externally-hosted systems like PostgresSQL4 databases, Slack5 workspaces, and GitHub6 accounts. The protocol enables LLM assistants to use client to access MCP servers, where they can send query using the JSON-RPC 2.0 [JSON-RPC, 2013] format, and receive response in the same format. By standardizing how information is sent and received, the protocol enables MCP servers to be effortlessly compatible with any MCP-compatible LLM agent, avoiding the need for wrappers around API endpoints provided by web services. However, although MCP serves as major catalyst for developing tool use agents, it fundamentally differs from AWIs, which are interfacesnot protocoldesigned for web agents that navigates stateful representation of websites. Below, we highlight two key differences between MCP and AWIs, which further motivate the need for the latter. State tracking Since MCP uses JSON-RPC 2.0a stateless protocolit will not directly support client-side state tracking, thus limiting state-dependent actions. For instance, if an agent is tasked to purchase white shoes, it may first request the list of all shoes fitting certain size, which it can easily receive from an AWI or from MCP server; this becomes its latest state. Given this state, an AWI-based agent can request the list to be sorted by price. On the other hand, an MCP client would have to reformulate query that requests the same list again, this time sorted by price. This overhead would lead to substantially higher bandwidth cost (since the entire list needs to be sent again) and slowing down the task completion. Unlike MCP servers, AWIs are designed to universally track states, enabling more efficient ways of executing actions that depends on the state of website. Standardized interface vs protocol Although the communication protocol is standardized, the actual implementation may differ based on the server. For instance, an implementation for GitHub7 may have function get_file_contents that takes as inputs owner and repo and return the content of file; the same function for GitLab8 may instead require project_id as input. This difference stems from the flexibility of the protocol, which allows MCP servers to specify their own methods and parameters. On the other hand, AWIs will be standardized across implementations, similar to how FileReader object in JavaScript will behave the same across different browsers. Overall, Agentic Web Interfaces enable agents to navigate websites, whereas MCP provides unified approach for LLM assistants to communicate with wide range of web services. That being said, AWIs and MCPs need not be mutually exclusive; AWIs can be designed to communicate with web service through MCP, whereas an MCP server could access websites through AWIs and server-side agent, enabling MCP-compatible LLM assistants to navigate websites autonomously. Ultimately, we believe that MCP and AWIs will both be crucial for developing more capable tool use and web agents, though, once again, the two are fundamentally different."
        },
        {
            "title": "6 Conclusion",
            "content": "Web agents represent one of the most exciting current areas in AI research, with high potential for impact on the daily lives of everyday users. As we have argued, however, research in the field is currently being held back by limitations imposed by interfaces that are built for human users, and not web agents. These limitations not only act as bottleneck for research on web agents, but also obscure their potential capabilities, introduce safety risks, and will lead to major resource challenges. Consequently, we argue that it is imperative that the broader research community helps design agentic web interfaces (AWIs): interfaces designed specifically to be used by web agents. In this position paper, we highlighted the need for AWIs (2,3.1), and offered both high-level principles (3.2) and concrete design suggestions (3.3) that we believe should be held in mind when building them. We underscored the relevance of AWIs to the wider ML community, stressing that designing better interfaces for web agents is not concern only for web developers and web agent researchers (4), 4https://www.postgresql.org 5https://slack.com 6https://github.com 7https://github.com/modelcontextprotocol/servers/tree/2025.5.12/src/github 8https://github.com/modelcontextprotocol/servers/tree/2025.5.12/src/gitlab 9 but the research community at large. Finally, we highlight the difference between protocol like MCP and web interfaces dedicated for agents (5). Given the technical challenges discussed in Section 4, the development of web standards requires ML expertise from the outset. In our view, the machine learning community should be actively involved in the design of AWIs from their inceptionensuring that elements to facilitate testing, debugging, and safety are incorporated into the standard. Treating these capabilities as first-class considerations, rather than retrofitting them onto systems designed primarily for human interaction, should be one of the main priorities in the design of agentic web interface standards. Web agents hold the potential for major societal impact; it is important that we design interfaces that reflect this importance."
        },
        {
            "title": "Acknowledgement",
            "content": "Xing Han Lù acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) [funding reference no. 579403]. Siva Reddy is supported by Canada CIFAR AI Chair. We thank Alexandre Drouin, Alexandre Lacoste, Christopher Pal, Maxime Gasse, Peter Shaw, Tianbao Xie, Yu Su, and the McGill NLP group members for sharing their feedback and discussing their agreements and disagreements on the position of this paper."
        },
        {
            "title": "References",
            "content": "M. Andriushchenko, A. Souly, M. Dziemian, D. Duenas, M. Lin, J. Wang, D. Hendrycks, A. Zou, Z. Kolter, M. Fredrikson, E. Winsor, J. Wynne, Y. Gal, and X. Davies. Agentharm: benchmark for measuring harmfulness of llm agents, 2025. URL https://arxiv.org/abs/2410.09024. Anthropic. Introducing claude, Mar. 2023. URL https://www.anthropic.com/news/ introducing-claude. Anthropic. Introducing the model context protocol, Nov. 2024. URL https://www.anthropic. com/news/model-context-protocol. P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. L. Boisvert, M. Bansal, C. K. R. Evuru, G. Huang, A. Puri, A. Bose, M. Fazel, Q. Cappart, J. Stanley, A. Lacoste, A. Drouin, and K. Dvijotham. Doomarena: framework for testing ai agents against evolving security threats, 2025a. URL https://arxiv.org/abs/2504.14064. L. Boisvert, M. Thakkar, M. Gasse, M. Caccia, T. L. S. D. Chezelles, Q. Cappart, N. Chapados, A. Lacoste, and A. Drouin. Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks, 2025b. URL https://arxiv.org/abs/2407.05291. M. Caccia, M. Thakkar, L. Boisvert, T. L. S. de Chezelles, A. Piché, N. Chapados, A. Drouin, M. Gasse, and A. Lacoste. Fine-tuning web agents: It works, but its trickier than you think. In NeurIPS 2024 Workshop on Open-World Agents, 2024. URL https://openreview.net/ forum?id=SkwtxEkst2. H. Cai, Y. Li, W. Wang, F. Zhu, X. Shen, W. Li, and T.-S. Chua. Large language models empowered personalized web agents. In Proceedings of the ACM on Web Conference 2025, pages 198215, 2025. T. L. S. D. Chezelles, M. Gasse, A. Drouin, M. Caccia, L. Boisvert, M. Thakkar, T. Marty, R. Assouel, S. O. Shayegan, L. K. Jang, X. H. Lù, O. Yoran, D. Kong, F. F. Xu, S. Reddy, Q. Cappart, G. Neubig, R. Salakhutdinov, N. Chapados, and A. Lacoste. The browsergym ecosystem for web agent research, 2025. URL https://arxiv.org/abs/2412.05467. X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. 10 A. Drouin, M. Gasse, M. Caccia, I. H. Laradji, M. Del Verme, T. Marty, L. Boisvert, M. Thakkar, Q. Cappart, D. Vazquez, et al. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718, 2024. H. Furuta, K.-H. Lee, O. Nachum, Y. Matsuo, A. Faust, S. S. Gu, and I. Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023. X. Ge, C. Xu, D. Misaki, H. R. Markus, and J. L. Tsai. How culture shapes what people want from ai. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 24, page 115. ACM, May 2024. doi: 10.1145/3613904.3642660. URL http://dx.doi.org/10.1145/ 3613904.3642660. Google. Function calling with the gemini api, May 2025. URL https://ai.google.dev/ gemini-api/docs/function-calling. B. Gou, R. Wang, B. Zheng, Y. Xie, C. Chang, Y. Shu, H. Sun, and Y. Su. Navigating the digital world as humans do: Universal visual grounding for gui agents, 2025. URL https://arxiv. org/abs/2410.05243. Y. Gu, K. Zhang, Y. Ning, B. Zheng, B. Gou, T. Xue, C. Chang, S. Srivastava, Y. Xie, P. Qi, H. Sun, and Y. Su. Is your llm secretly world model of the internet? model-based planning for web agents, 2025. URL https://arxiv.org/abs/2411.06559. I. Gur, H. Furuta, A. Huang, M. Safdari, Y. Matsuo, D. Eck, and A. Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. Realm: Retrieval-augmented language model pre-training, 2020. URL https://arxiv.org/abs/2002.08909. H. He, W. Yao, K. Ma, W. Yu, Y. Dai, H. Zhang, Z. Lan, and D. Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. L. Jang, Y. Li, D. Zhao, C. Ding, J. Lin, P. P. Liang, R. Bonatti, and K. Koishida. Videowebarena: Evaluating long context multimodal agents with video understanding web tasks. arXiv preprint arXiv:2410.19100, 2024. W. G. JSON-RPC. Json-rpc 2.0 specification, Jan. 2013. URL https://www.jsonrpc.org/ specification. V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W. tau Yih. Dense passage retrieval for open-domain question answering, 2020. URL https://arxiv.org/abs/ 2004.04906. O. Khattab and M. Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert, 2020. URL https://arxiv.org/abs/2004.12832. J. Y. Koh, R. Lo, L. Jang, V. Duvvur, M. C. Lim, P.-Y. Huang, G. Neubig, S. Zhou, R. Salakhutdinov, and D. Fried. VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks, June 2024a. URL http://arxiv.org/abs/2401.13649. arXiv:2401.13649 [cs]. J. Y. Koh, S. McAleer, D. Fried, and R. Salakhutdinov. Tree search for language model agents, 2024b. URL https://arxiv.org/abs/2407.01476. H. Lai, X. Liu, I. L. Iong, S. Yao, Y. Chen, P. Shen, H. Yu, H. Zhang, X. Zhang, Y. Dong, and J. Tang. Autowebglm: large language model-based web navigating agent, 2024. URL https: //arxiv.org/abs/2404.03648. K. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question answering, 2019. URL https://arxiv.org/abs/1906.00300. I. Levy, B. Wiesel, S. Marreed, A. Oved, A. Yaeli, and S. Shlomov. St-webagentbench: benchmark for evaluating safety and trustworthiness in web agents, 2024. URL https://arxiv.org/abs/ 2410.06703. 11 P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. W. Li, W. Bishop, A. Li, C. Rawles, F. Campbell-Ajala, D. Tyamagundlu, and O. Riva. On the effects of data scale on ui control agents. In Neural Information Processing Systems, 2024. URL https://api.semanticscholar.org/CorpusID:270285816. Z. Liao, J. Jones, L. Jiang, E. Fosler-Lussier, Y. Su, Z. Lin, and H. Sun. Redteamcua: Realistic adversarial testing of computer-use agents in hybrid web-os environments, 2025. URL https: //arxiv.org/abs/2505.21936. E. Z. Liu, K. Guu, P. Pasupat, T. Shi, and P. Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1802.08802. J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024a. X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, M. Huang, Y. Dong, and J. Tang. Agentbench: Evaluating llms as agents, 2023. URL https://arxiv.org/abs/2308.03688. X. Liu, T. Zhang, Y. Gu, I. L. Iong, Y. Xu, X. Song, S. Zhang, H. Lai, X. Liu, H. Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. arXiv preprint arXiv:2408.06327, 2024b. X. H. Lù, Z. Kasner, and S. Reddy. Weblinx: Real-world website navigation with multi-turn dialogue. arXiv preprint arXiv:2402.05930, 2024. X. H. Lù, A. Kazemnejad, N. Meade, A. Patel, D. Shin, A. Zambrano, K. Stanczak, P. Shaw, C. J. Pal, and S. Reddy. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories, 2025. S. Murty, C. Manning, P. Shaw, M. Joshi, and K. Lee. Bagel: Bootstrapping agents by guiding exploration with language, 2024. URL https://arxiv.org/abs/2403.08140. S. Murty, H. Zhu, D. Bahdanau, and C. D. Manning. Nnetnav: Unsupervised learning of browser agents through environment interaction in the wild, 2025. URL https://arxiv.org/abs/2410. 02907. R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2022. URL https://arxiv. org/abs/2112.09332. OpenAI. Introducing chatgpt search, Oct. 2024. URL https://openai.com/index/ introducing-chatgpt-search/. OpenAI. Introducing deep research, Feb. 2025. URL https://openai.com/index/ introducing-deep-research/. Y. Pan, D. Kong, S. Zhou, C. Cui, Y. Leng, B. Jiang, H. Liu, Y. Shang, S. Zhou, T. Wu, and Z. Wu. Webcanvas: Benchmarking web agents in online environments, 2024. URL https: //arxiv.org/abs/2406.12373. A. Patel, M. Hofmarcher, C. Leoveanu-Condrei, M.-C. Dinu, C. Callison-Burch, and S. Hochreiter. Large language models can self-improve at web agent tasks, 2024. URL https://arxiv.org/ abs/2405.20309. A. Prasad, A. Koller, M. Hartmann, P. Clark, A. Sabharwal, M. Bansal, and T. Khot. Adapt: Asneeded decomposition and planning with language models, 2024. URL https://arxiv.org/ abs/2311.05772. 12 Z. Qi, X. Liu, I. L. Iong, H. Lai, X. Sun, W. Zhao, Y. Yang, X. Yang, J. Sun, S. Yao, T. Zhang, W. Xu, J. Tang, and Y. Dong. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning, 2025. URL https://arxiv.org/abs/2411.02337. Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, Y. Huang, C. Xiao, C. Han, Y. R. Fung, Y. Su, H. Wang, C. Qian, R. Tian, K. Zhu, S. Liang, X. Shen, B. Xu, Z. Zhang, Y. Ye, B. Li, Z. Tang, J. Yi, Y. Zhu, Z. Dai, L. Yan, X. Cong, Y. Lu, W. Zhao, Y. Huang, J. Yan, X. Han, X. Sun, D. Li, J. Phang, C. Yang, T. Wu, H. Ji, Z. Liu, and M. Sun. Tool learning with foundation models, 2023. Y. Qin, Y. Ye, J. Fang, H. Wang, S. Liang, S. Tian, J. Zhang, J. Li, Y. Li, S. Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. C. Rawles, A. Li, D. Rodriguez, O. Riva, and T. Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. C. Rawles, S. Clinckemaillie, Y. Chang, J. Waltz, G. Lau, M. Fair, A. Li, W. Bishop, W. Li, F. Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools, 2023. URL https://arxiv.org/abs/2302.04761. P. Shaw, M. Joshi, J. Cohan, J. Berant, P. Pasupat, H. Hu, U. Khandelwal, K. Lee, and K. N. Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. Advances in Neural Information Processing Systems, 36:3435434370, 2023. T. Shi, A. Karpathy, L. J. Fan, J. Z. Hernández, and P. Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, 2017. URL https://api.semanticscholar.org/CorpusID:34953552. N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. URL https://arxiv.org/abs/2303.11366. R. W. Shirey. Internet Security Glossary, Version 2. RFC 4949, Aug. 2007. URL https://www. rfc-editor.org/info/rfc4949. P. Sodhi, S. Branavan, Y. Artzi, and R. McDonald. Step: Stacked llm policies for web actions. arXiv preprint arXiv:2310.03720, 2024. Y. Song, F. Xu, S. Zhou, and G. Neubig. Beyond browsing: Api-based web agents, 2025. URL https://arxiv.org/abs/2410.16464. N. Tiwary, V. Dongre, S. A. Chawla, A. Lamani, and D. Hakkani-Tür. From context to action: Analysis of the impact of state representation and context on the generalization of multi-turn web navigation agents, 2024. URL https://arxiv.org/abs/2410.23555. B. Trabucco, G. A. Sigurdsson, R. Piramuthu, and R. Salakhutdinov. Towards internet-scale training for agents. In Will Synthetic Data Finally Solve the Data Access Problem?, 2025. URL https: //openreview.net/forum?id=6YFuaxXYxP. A. D. Tur, N. Meade, X. H. Lù, A. Zambrano, A. Patel, E. Durmus, S. Gella, K. Stanczak, and S. Reddy. Safearena: Evaluating the safety of autonomous web agents, 2025. URL https: //arxiv.org/abs/2503.04957. X. Wan, J. Yang, and J. Xiao. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of the 45th annual meeting of the association of computational linguistics, pages 552559, 2007. C. H. Wu, J. Y. Koh, R. Salakhutdinov, D. Fried, and A. Raghunathan. Adversarial attacks on multimodal agents. arXiv e-prints, pages arXiv2406, 2024. 13 T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. T. Xue, W. Qi, T. Shi, C. H. Song, B. Gou, D. Song, H. Sun, and Y. Su. An illusion of progress? assessing the current state of web agents, 2025. URL https://arxiv.org/abs/2504.01382. R. Yan, X. Wan, J. Otterbacher, L. Kong, X. Li, and Y. Zhang. Evolutionary timeline summarization: balanced optimization framework via iterative substitution. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 745754, 2011. J. Yang, H. Zhang, F. Li, X. Zou, C. Li, and J. Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press. Swe-agent: Agentcomputer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. H. Zhang, X. Liu, and J. Zhang. Summit: Iterative text summarization via chatgpt. arXiv preprint arXiv:2305.14835, 2023. B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. B. Zheng, M. Y. Fatemi, X. Jin, Z. Z. Wang, A. Gandhi, Y. Song, Y. Gu, J. Srinivasa, G. Liu, G. Neubig, and Y. Su. Skillweaver: Web agents can self-improve by discovering and honing skills. 2025. URL https://api.semanticscholar.org/CorpusID:277634081. S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, U. Alon, and G. Neubig. WebArena: Realistic Web Environment for Building Autonomous Agents, Apr. 2024. URL http://arxiv.org/abs/2307.13854. arXiv:2307.13854 [cs]."
        }
    ],
    "affiliations": [
        "Equal Advising",
        "McGill University",
        "Mila",
        "Quebec AI Institute"
    ]
}