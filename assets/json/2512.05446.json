{
    "paper_title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
    "authors": [
        "Cheng-Yuan Ho",
        "He-Bi Yang",
        "Jui-Chiu Chiang",
        "Yu-Lun Liu",
        "Wen-Hsiao Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 6 4 4 5 0 . 2 1 5 2 : r TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression Cheng-Yuan Ho1, He-Bi Yang1, Jui-Chiu Chiang2, Yu-Lun Liu1, Wen-Hsiao Peng1 1National Yang Ming Chiao Tung University, Taiwan 2National Chung Cheng University, Taiwan {kelvinhe0218.cs12, mrrrimge32.cs13}@nycu.edu.tw, rachel@ccu.edu.tw yulunliu@cs.nycu.edu.tw, wpeng@cs.nycu.edu.tw Figure 1. Overview of TED-4DGS. Left: Qualitative comparison on banana scene. Our TED-4DGS reconstructs the scene with superior rendering quality compared to ADC-GS. It achieves 26% file size reduction while closely matching the ground-truth view. Centre: Temporal duration map. Static background regions reuse long-duration Gaussian primitives, whereas occluded parts of the hand and banana are represented by short-duration primitives, demonstrating the effectiveness of temporal activation. Right-top: Rate-distortion comparison on the HyperNeRF [36] benchmark. Our TED-4DGS attains higher PSNR with smaller file sizes than prior methods. Right-bottom:Illustration of the learnable temporal-activation function, which activates Gaussian primitive from its appearance (as) to disappearance (af )."
        },
        {
            "title": "Abstract",
            "content": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenescommonly referred to as 4DGS or dynamic 3DGS has attracted increasing attention. However, designing more compact, efficient deformation schemes together with ratedistortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, temporally activated and embedding-based deformation scheme for rate-distortionoptimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on sparse anchor-based 3DGS representation. Each canonical anchor is assigned with learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while lightweight per-anchor temporal embedding queries shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with channelwise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves the state-of-the-art rate-distortion performance on several commonly used real-world datasets. To the best of our knowledge, 1 this work represents one of the first attempts to pursue ratedistortion-optimized compression framework for dynamic 3DGS representations. 1. Introduction Reconstructing dynamic scenes from multi-view videos has long been central challenge in 3D vision, with applications ranging from novel view synthesis and free-viewpoint rendering to dynamic scene understanding. Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenescommonly referred to as 4DGS or dynamic 3DGShas attracted increasing attention. Analogous to the evolution from 2D images to 2D videos, leveraging temporal redundancy for more efficient representation calls for principled design in both temporal modeling and compression strategies. This work represents one of the first attempts to pursue rate-distortion-optimized compression framework for dynamic 3DGS representations. Unlike previous methods that primarily aim to reduce memory footprint or accelerate rendering for dynamic 3DGS, our approach integrates entropy coding into the compression of Gaussian primitives for storage and transmission purposes. Existing works on dynamic 3DGS representations generally fall into two categories: the first represents the dynamic scene using explicit space-time 4DGS, while the second models deformation fields over canonical 3D Gaussian primitives (i.e. canonical 3DGS with deformation). Space-time 4DGS [8, 25, 39, 44] augments the spatial attributes of 3D Gaussian primitives with temporal parameters that specify their deformation and visibility across time. In contrast, canonical 3DGS with deformation [2, 15, 17, 27, 40] reconstructs dynamic scenes by learning deformation fields that warp canonical 3D Gaussian primitives over time. Since these methods define Gaussian primitives in canonical space shared across time, they typically require significantly fewer Gaussian primitives compared to space-time 4DGS. Consequently, canonical 3DGS with deformation is often adopted as the core representation in rate-distortion-optimized compression frameworks [15, 27]. However, canonical 3DGS with deformation methods face common challenge in handling occlusion and disocclusion. Since each Gaussian primitive persists throughout the entire sequence duration, the absence of temporal parameters that explicitly define its active duration may lead to peculiar deformation that is necessary, in some cases, to relocate non-contributing Gaussian primitives at specific time instances. The irregular deformation results in training instability and poses challenge in rate-distortion compression. To address this challenge, we propose TED-4DGS, temporally activated and embedding-based deformation framework for 4DGS compression. We build TED-4DGS upon sparse anchor-based 3DGS representation [8]. It features Table 1. Comparative analysis of dynamic 3DGS approaches. Model Space-time 4DGS 4DGS [44] STG [25] 4DScaffoldGS [8] FreeTimeGS [39] Canonical 3DGS + Deformation 4DGaussians [40] E-D3DGS [2] Light4GS [27] ADC-GS [15] Ours (TED-4DGS) Temporal Activation RD Comp. Motion Model Linear Polynomial Linear Linear Coord.-based Embedding-based Coord.-based Embedding-based Embedding-based per-anchor embedding-based deformation field, where each anchor is assigned learnable temporal feature that serves as query to shared global deformation bank. To model occlusion and temporal visibility, we extend canonical 3D anchors into 4D by introducing temporal activation parameters, which specify the appearance and disappearance transitions of each anchor over time. Furthermore, inspired by recent advances in 3DGS ratedistortion compression [6, 46], we incorporate an implicit neural representation (INR)based hyperprior to model anchor attribute distributions, along with channel-wise autoregressive model to capture intra-anchor correlations. In addition, we observe that cross-camera color inconsistencies across multi-view captures can severely destabilize training and degrade rendering quality. To mitigate this issue, we introduce lightweight Color Correction Module (CCM) that compensates for camera-specific color bias during training, leading to more stable optimization and color-consistent renderings across views. To sum up, our contributions include: We design per-anchor embedding-based deformation network that leverages temporal features to query shared global deformation bank, effectively capturing anchorspecific deformation. We extend static 3D anchors into 4D by introducing temporal activation parameters, promoting stable deformation and improved visibility modeling. We develop rate-distortion-optimized compression framework that incorporates an INR-based hyperprior and channel-wise autoregressive model for efficient attribute coding. With these novel elements, our scheme achieves the stateof-the-art rate-distortion performance on several commonly used real-world datasets. 2 2. Related Work 2.1. Dynamic 3DGS Representations Dynamic 3DGS representations can be broadly classified into two types: (1) space-time 4DGS and (2) canonical 3DGS with deformation, as illustrated in Tab. 1. Space-time 4DGS is natural extension of 3DGS. It augments the spatial attributes of 3D Gaussian primitives with temporal parameters that specify their motion and visibility across time. Methods in this category often feature simple parametric motion modeling, such as linear motion [8, 35, 39, 44] or polynomial motion [22, 25]. As such, recent methods in this category, e.g. 4DScaffoldGS [8] and FreeTimeGS [39], favor temporally short-lived Gaussian primitives, in order to model complex motion and enhance rendering fidelity. For instance, 4DScaffoldGS [8] introduces temporal coverage-aware anchor growing scheme to promote short-lived anchors, while FreeTimeGS [39] applies regularization strategy to penalize Gaussian primitives with extended lifespans across time. Although showing promising rendering fidelity, they significantly increase the number of Gaussian primitives or anchors, leading to higher storage demands. In contrast, the notion of canonical 3DGS with deformation [911, 13, 16, 1821, 28, 31, 37, 41, 42] aims to reconstruct dynamic scenes by deforming set of canonical 3D Gaussian primitives that persist throughout the entire scene duration. Deformation signaling can be either coordinate-based or embedding-based. Coordinate-based methods model the deformation field as space-time 4D function, which is typically cast as learning dense 4D grid. To reduce memory consumption, 4DGaussians [40] decomposes the 4D grid with hexplane representation [5], while Grid4D [17] employs 3D hash grids [34]. Despite these efforts, coordinate-based methods still incur considerable storage overhead. Instead of learning grid points, E-D3DGS [2], representative embedding-based method, learns per-Gaussian embedding for each Gaussian primitive, along with number of temporal embeddings, one per time instance. These embeddings are concatenated or multiplied to arrive at the deformation for given Gaussian primitive. When coupled with sparse canonical 3DGS representation, embedding-based methods usually have much reduced parameter count, as compared to coordinate-based methods. Although both strategies are capable of modeling complex motion, the absence of temporal parameters that explicitly define the active duration of each Gaussian primitive may lead to peculiar motion that is necessary, in some cases, to relocate non-contributing Gaussian primitives at specific time instances. Drawing inspiration from both types of dynamic 3DGS representations, this work integrates the temporal activation capability of space-time 4DGS with the strengths of embedding-based deformation in modeling complex motion. This enables the temporal duration of each canonical 3DGS Gaussian primitive to be defined explicitly, allowing non-contributing Gaussian primitives to be excluded from rendering without resorting to unnatural motion for their relocation. Consequently, our approach achieves memory and storage efficiency by modeling complex yet natural motion with reduced set of Gaussian primitives. 2.2. Rate-Distortion-Based 3DGS Compression Rate-distortion-optimized 3DGS compression is attracting growing interest due to increasing demands for efficient storage and transmission. The key challenge, which goes beyond merely reducing the parameter count of the 3DGS representation [1, 12, 23], lies in minimizing the entropy rate of these parameters to achieve compression efficiency. Recent work [6, 7, 26, 38, 46] in 3DGS compression Scafconverges on anchor-based representations, e.g. foldGS [8, 29], paired with joint rate-distortion optimization via the hyperprior [4] and context models [14, 32, 33]. For rate-distortion optimization, HAC [6] introduces hash gridbased hyperprior to capture spatial correlations among anchor attributes. To further exploit both inter-anchor and intraanchor correlations, CAT-3DGS [46] employs triplanebased hyperprior framework that features both spatial and channel-wise autoregressive modeling. Rate-distortion-optimized compression for dynamic 3DGS scenes remains largely underexplored. Most early attempts [15, 27] build on canonical 3DGS representations with deformation, with particular focus on how deformation is encoded. Light4GS [27] encodes hexplanes used for deformation modeling as images by adopting implicit neural representation (INR)-based image coding. ADC-GS [15] encodes embedding-based deformation with an autoregressive model applied along the channel dimension. 2.3. Comparison with Prior Work As shown in Table 1, our work differs from previous work on space-time 4DGS by incorporating rate-distortion optimized compression and embedding-based deformation. The capability to model complex motion via embedding-based deformation enables the representation of dynamic scenes using fewer Gaussian primitives, thereby facilitating ratedistortion optimized compression. In contrast to Light4GS [27] and ADC-GS [15], which rely on canonical 3DGS representations with deformation, our method introduces temporal activation to more effectively manage occlusion and disocclusion without introducing artificial deformation artifacts. Last but not least, we replace the grid-based hyperprior, e.g. [6, 46], with lightweight implicit neural representation (INR)-based hyperprior, which eliminates the signaling overhead associated with grid structures such as triplanes or hash 3 tables. 3. Preliminary: ScaffoldGS To construct an efficient 4DGS representation, our TED4DGS leverages ScaffoldGS [30]an anchor-based 3DGS representationas its core representation framework. ScaffoldGS [30] is compact and storage-efficient representation initially designed for static scenes. It introduces anchor points (referred hereafter to as anchors) located on predefined voxel grid. Each anchor encapsulates information for fixed number of Gaussian primitives. Their structural and color attributes, including the scale {si}K1 i=0 , roi=0 , and opacity {αi}K1 tation {ri}K1 i=0 , are decoded from the anchors feature . Moreover, the anchors position and those {µi}K1 i=0 of its associated Gaussian primitives are related by {µi}K1 i=0 , where {Oi}K1 i=0 are learned offsets, which are scaled by learned vector l. Due to its storage-friendly design, ScaffoldGS has been widely adopted in many RD-optimized compression frameworks [6, 26, 38, 46]. i=0 = x+l{Oi}K1 i=0 , color {ci}K1 4. Proposed Method: TED-4DGS Based on ScaffoldGS [30], this work introduces temporally activated and embedding-based deformation scheme for 4DGS compression. First, we adopt per-anchor embedding approach to modeling deformation. Anchors are classified into static and dynamic types via learnable, binary temporal mask Mt. Each dynamic anchor is equipped with temporal feature ϕ Rd that serves as soft query to global deformation bank 2 for frames, in order to retrieve its deformation. This design supports flexible motion modeling while exploiting anchor spatial sparsity to minimize the parameter count for deformation signaling. Second, we assign each canonical anchor an explicit temporal-activation parameter τ that defines its active duration. This facilitates more accurate modeling of occlusion and dis-occlusion, and prevents the generation of unnatural deformation caused by relocating non-contributing anchors during scene rendering at specific time instances Lastly, we adopt an INR-based hyperprior framework to model the coding distributions of anchor attributes, including the anchors feature , scaling l, offsets {Oi}K1 i=0 , temporal feature ϕ, and temporal activation τ . attributes encode deformation (; Section 4.2). To generate an anchor-specific deformation latent za for time instance t, the temporal feature ϕ of dynamic anchor is multiplied by global deformation vector z(t), which is obtained via temporal interpolation between deformation vectors corresponding to uniformly sampled time instances in the global deformation bank Z. The resulting latent za is fed into the deformation decoder Fdeform to predict an anchor displacement and feature residual . These are used to update and as the deformed anchor position and feature , respectively. Finally, is used to decode the scale {si}K1 i=0 for each Gaussian primitive, whereas the color {ci}K1 i=0 are treated as time-invariant and are decoded from the canonical feature . i=0 and rotation {ri}K1 i=0 and opacity {αi}K1 The rendering process utilizes both dynamic and static anchors. Dynamic anchors are temporally deformed by decoding their respective temporal attributes. With the deformed dynamic anchors and static anchors, the remaining procedure largely follows ScaffoldGS, except that each Gaussian primitives opacity is additionally modulated by the temporal activation of its associated anchor to account for temporal visibility. The entropy encoding (Section 4.4) of spatial and temporal features for each anchor leverages an INR-based hyperprior. It applies positional encoding to the anchors position, with the resulting representation used to derive quantization step sizes and Gaussian parameters (i.e. means and variances) for entropy encoding associated attributes. Notably, for coding the anchor feature , which carries much structural and color information, we further incorporate channel-wise autoregressive model to exploit intra correlations among its components. These attributes are jointly compressed in an rate-distortion-optimized manner. Additionally, an offset mask is learned to suppress less critical Gaussian primitives. The final compressed bitstream include (a) anchors positions and attributes, (b) the global deformation bank, (c) the network weights of the deformation decoder, Scaffold MLP decoder, hyperprior decoder, and channel-wise autoregressive model, and (d) the binary offset and temporal masks. The anchors positions are stored in 16-bit floating-point format (FP16), while the network weights and the global deformation bank are in FP32. Both masks are entropy encoded. 4.1. System Overview 4.2. Deformation Fields Fig. 2 illustrates our TED-4DGS framework. The representation of dynamic scene begins with the generation of anchor points, each characterized by position and set of spatial and temporal attributes. The spatial attributes, adopted from ScaffoldGS [30], capture the geometry and appearance of Gaussian primitives in canonical space, while the temporal To model dynamic motion, we introduce an embeddingbased deformation field that predicts the temporal deformation of each anchor (Fig. 2c. The deformation is conditioned on both the per-anchor temporal feature ϕ Rd and global deformation bank 2 for frames. In video language, functions analogously to set of optical flow maps (a) Overall pipeline. (b) INR-based anchor attribute compression framework. (c) Anchor-based deformation framework and Scaffold-GS rendering. Figure 2. System overview of our TED-4DGS framework. while ϕ serves as pixels location that determines where the relevant motion of that pixel should be extracted from these flow maps. The deformation inference for dynamic anchor at time starts by interpolating temporally between deformation vectors for various time instances in the global deformation bank Z. The outcome z(t) = interp(Z, t) is time-specific z(t) RD, which (similar to temporally-interpolated optical flow map in the context of video) encapsulates deformation information for all dynamic anchors at time and serves as shared query target. To perform the query, the temporal feature ϕ of dynamic anchor is projected by = Fproject(ϕ), which is multiplied with z(t) to yield an anchor-specific temporal latent za = z(t). This latent za further undergoes deformation decoder Fdeform to arrive at an positional displacement and feature residual : , = Fdeform (cid:0)Fproject(ϕ) interp(Z, t)(cid:1), (1) We then update the anchors position and feature as = + and = + . We note that the deformation vectors stored in are learnable parameters that must be explicitly signaled in the bitstream. To balance deformation accuracy and bitrate, we signal deformation vector for every other frame. 4.3. Progressive 3D-to-4D Anchor Transformation key challenge in deformation-based dynamic scene representations lies in handling occlusion and disocclusion, as canonical anchors persist throughout the entire sequence duration. Ideally, when an object is occluded by another, its corresponding anchors should remain behind those of the occluder. However, as shown in Fig. 6, trivial yet improper solution is to deform occluded anchors such that they fall outside the viewing frustum. This leads to highly irregular deformation field, hindering compression efficiency. To address this issue, we extend the static 3D anchor representation into 4D formulation by introducing learnable temporal activation parameter τ = [(as, bs), (af , bf )], which explicitly specifies each anchors active duration as τ (t) = (cid:104) exp 1, (cid:104) exp (cid:0)(t as)/bs (cid:1)2(cid:105) , < as, as af , (2) (cid:0)(t af )/bf (cid:1)2(cid:105) , > af , 5 where (as, bs) and (af , bf ) define the smooth transitions for appearance and disappearance, respectively. The specification of τ (t) allows each anchor and its associated Gaussian primitives to activate or deactivate gradually over time. During rendering, the time-aware opacity αt for each Gaussian primitive at time is computed by modulating its static opacity α with the temporal activation τ (t): αt = α τ (t), (3) Figure 3. Light4GS [27] and ADC-GS [15]. Rate-distortion comparison of our TED-4DGS, which enables dynamic visibility modeling over time. To ensure stable learning, we adopt progressive training strategy. During the initial 20,000 iterations, all anchors are treated as static 3D anchors without temporal activation modeling, encouraging them to remain consistently active across frames. Subsequently, the temporal activation parameters as, af are initialized for each anchor based on its earliest and latest time instances that an anchor is visible in the viewing frustum. These temporal parameters are then optimized jointly with other anchor attributes. As side note, we adopt the anchor pruning strategy of ScaffoldGS [30], removing anchors whose associated Gaussian primitives exhibit negligible collective opacity. By incorporating temporal activation, our time-aware opacity enables more effective removal of anchors that contribute minimally over time. 4.4. INR-based Compression for Anchor Attributes To encode anchors attributes in rate-distortion-optimized fashion, we propose an INR-based hyperprior to predict the distribution parameters for entropy coding and channelwise autoregressive model to further exploit intra-anchor correlations for its feature compression. The INR-based hyperprior is implemented as multilayer perceptron that learns prior distributions over anchor attributes, including the feature , offsets {Oi}K1 i=0 , scaling l, temporal feature ϕ and temporal activation τ . Given the positional embedding of the anchors position x, the network outputs the corresponding distribution parameters: means µh, variances σh, and quantization step sizes q. For each attribute type, distinct quantization step size is predicted and shared across its components. The probability of quantized attribute ˆa is evaluated as p(ˆa x(cid:1) = (cid:90) ˆa+ 2 ˆa 2 N(cid:0)µh, σh (cid:1) da, (4) which amounts to the likelihood of the quantized value under the learned Gaussian prior. Since an INR hyperprior is gridfree, we transmit only few network weights, eliminating the extra bits that triplanes or hash tables require and keeping the model lightweight. Following [46], we adopt channel-wise autoregressive model to exploit intra-feature dependencies for coding an6 chor features , which typically comprise substantial portion of the bitstream (Fig. 2b). 4.5. Training Objectives The training objective of TED-4DGS, as defined in Eq. (5), comprises several terms with distinct roles. The distortion loss Ldistortion and rate loss Lrate form the classical rate-distortion pair, where Ldistortion combines L1 and SSIM losses and Lrate denotes the average bitrate per anchor. To promote sparsity, we adopt the offset mask loss Loffset mask and temporal mask loss Ltemp mask from [23]. These losses suppress redundant spatial offsets and identify static anchors, respectively. The weight of Loffset mask is scaled proportionally to the rate-control factor λrate, so that lowering the target bitrate encourages more aggressive pruning of anchors and reduce the model size. Lastly, the regulization losses Lvol and Ltv encourage scale consistency and enforce temporal smoothness on the deformation vectors in the deformation bank Z. These are adopted from ScaffoldGS [30] and ED3DGS [2]. The complete objective is thus: = Ldistortion + λrate (cid:0)Lrate + λoffset mask Loffset mask + λtemp mask Ltemp mask + λvol Lvol + λtv Ltv. (cid:1) (5) 5. Experiments Implementation Details. Our method is implemented in PyTorch and trained on an NVIDIA RTX 3090 GPU. We evaluate range of rate-control factors λrate, choosing λrate = {0.001, 0.002, 0.004, 0.008} for the HyperNeRF dataset and λrate = {0.002, 0.004, 0.008, 0.016} for the Neu3D dataset. In our experiments, the low-rate configuration corresponds to the highest λrate, which yields the lowest bitrate, whereas the high-rate configuration uses the lowest λrate to achieve the maximum bitrate. The details of training process and the choice of hyperparameters are provided in the supplementary document. Baselines. We compare TED-4DGS with three groups of baseline methods: (1) rate-distortion-optimized compression approaches, including Light4GS [27] and ADC-GS [15], and non-rate-distortion-optimized approaches, including (2) deformation-based methodse.g. 4DGaussians [40] Table 2. Quantitative results on Neu3D. The best and 2nd best results are highlighted in red and yellow cells. Model 4DGS [44] STG [25] FreeTimeGS [39] 4DGaussians [40] E-D3DGS [2] Light4GS [27] (low rate) Light4GS [27] (high rate) ADC-GS [15] (low rate) ADC-GS [15] (high rate) Ours (low rate) Ours (high rate) PSNR 32.01 32.04 33.19 31.72 31.20 31.48 31.69 31.41 31.67 31.63 32.25 SSIM2 LPIPS FPS 114 110 34 42 40 37 126 110 78 0.055 0.044 0.036 0.049 0.030 0.064 0.053 0.066 0.061 0.061 0.051 0.986 0.974 0.984 0.974 0.972 0.981 0.969 0.972 Table 3. Quantitative results on HyperNeRF. Model 4DGaussians [40] E-D3DGS [2] Light4GS [27] (low rate) Light4GS [27] (high rate) ADC-GS [15] (low rate) ADC-GS [15] (high rate) Ours (low rate) Ours (high rate) PSNR 25.60 25.74 25.35 25.55 25.42 25.68 25.22 25.67 SSIM2 LPIPS FPS 0.848 0.816 0.777 0.825 0.800 0.808 0.281 0.231 0.315 0.252 0.311 0.287 22 26 28 27 135 101 111 96 Size (MB) 1000 175 125 38 40 3.77 5.46 4.04 6.57 1.73 2.26 Size (MB) 64 47 5.15 8.87 4.02 6.67 2.36 3.72 and E-D3DGS [2], and (3) space-time 4DGS methodse.g. 4DGS [44], STG [25] and FreeTimeGS [39]. Datasets. We follow the common test protocol [15] to evaluate our TED-4DGS on two real-world datasets: Neural 3D Video (Neu3D) [24] and HyperNeRF [36]. Specifically, on the Neu3D dataset, which includes multiple synchronized multi-view videos captured by 1821 cameras per scene, the results are reported for the cook spinach, cut roasted beef, flame salmon, flame steak, and sear steak sequences. On the HyperNeRF dataset, which includes videos captured using two phones rigidly mounted on handheld stereo rig. [36], we report results for four dynamic scenes, 3D printer, banana, broom, and chicken). We train (and test) on all frames at half resolution (536960) in width and height according to [15, 40, 43]. Metrics. To compare the rate-distortion (RD) performance of the competing methods, we report peak-signalto-noise ratio (PSNR), structural similarity index (SSIM2), and perceptual quality measure LPIPS. For the ratedistortion-optimized approaches, we compute the average PSNR/SSIM2/LPIPS and compressed file size across sequences for the low-rate and high-rate configurations. Likewise, we measure rendering speed in frames per second (FPS) under these configurations. 5.1. Rate-Distortion Comparison In Fig. 3, Table 2 and Table 3, our TED-4DGS consistently outperforms the competing methods across both datasets, achieving the state-of-the-art RD performance. Figs. 4 further presents the subjective quality comparison. On the sear steak scene (Neu3D), our TED-4DGS delivers similar or superior rendering quality while reducing the file (a) sear steak (Neu3D) (b) 3D printer (HyperNeRF) Figure 4. Subjective quality comparisons. (a) Deformation Field Variants (b) Compression Variants Figure 5. Rate-distortion comparisons on (a) deformation field variants and (b) compression variants. size by over 14x relative to E-D3DGS [2] and over 18x compared to 4DGaussians [40]. On the 3D printer scene (HyperNeRF), our TED-4DGS achieves 23.1dB PSNR with file size of 3.4MB, which amounts to 28% bitrate reduction compared to the ratedistortion-optimized ADC-GS [15] at comparable perceptual quality level. Compared to E-D3DGS [2], non-ratedistortion-optimized approach, the reduction in file size exceeds 14x. 5.2. Ablation Experiments We conduct ablation experiments on the Neu3D [24] dataset to validate (1) the query mechanism of the deformation bank, 7 (a) w/o Temporal Activation (b) w/ Temporal Activation Figure 6. Comparison of deformed point clouds in the cook spinach scene (Neu3D). Table 4. Ablation study of deformation field variants on Neu3D at highest rate point. Method Ours (i) Ours w/ Concatenation (ii) Ours w/o Temporal Activation (iii) Ours w/o Progressive Training (iv) Ours w/ INR-based Deformation PSNR 32.25 31.27 32.00 31.44 29.72 Size (MB) 2.11 2.10 2.23 1.80 5.09 Table 5. Temporal duration versus motion complexity. Scene Motion Complexity τ 0.2 Temporal Duration (τ = af as) 0.2 < τ < 0.8 0.8 τ Flame Steak Banana Broom Slow Medium Fast 3% 9% 18% 0% 6% 35% 97% 85% 47% (2) the temporal activation, (3) the progressive training strategy, and (4) the INR-based hyperprior. In addition, we include two baselines: (5) pure ScaffoldGS compression without temporal modeling and (6) naive INR-based deformation. Table 4 presents RD results at the highest rate point (λrate = 0.002) while Fig. 5 presents RD-optimized results in the form of RD curves. Deformation Query. When querying the global deformation bank (Fig. 2 (e)), there are two query mechanisms: multiplication (w z(t)) or concatenation (concat(ϕ, z(t))). Our approach adopts the multiplication strategy, which can be interpreted as retrieving relevant deformation from the global deformation vector z(t). From Fig. 5a, this approach outperforms the concatenation design ((i) w/ concatenation) in terms of RD performance. Temporal Activation. Fig. 5a shows that disabling temporal activation reduces our dynamic scene representation with deformation (Section 4.3). This consistently increases the bitrate across all rate points, while the rendering quality exhibits slight degradation. Recall that our temporal activation approach facilitates time-aware opacity pruning, which discards anchors with limited relevance across time. Fig. 6 further confirms that in the absence of our temporal activation mechanism, non-trivial yet improper solution may emerge, relocating non-contributing dynamic Gaussian primitives (red dots in Fig. 6) outside the viewing frustum. 8 Figure 7. Comparison of rendered images and rendered temporal duration maps with different motion dynamics. We also analyze the active time span of Gaussians (τ = af as). Table 5 and Fig. 7 show that slow-motion scenes have over 95% of Gaussians active throughout the sequence, while higher motion complexity produces more short-duration Gaussians with relatively uniform distribution. This demonstrates that temporal activation adaptively aligns anchor lifetimes with scene dynamics. Further results are provided in Supplementary Sec. E. Progressive Learning. Fig. 5a further shows that our progressive training strategy (Section 4.3) leads to superior ratedistortion performance. Across all rate points, we observe consistent reduction in bitrate, but this comes with noticeable drop in PSNR. The training process tends to prune anchors more aggressively when their deformation is not well learned in the early phase. This validates the effectiveness of using static 3D anchors to stabilize deformation learning prior to incorporating temporal activation modeling. INR-based Hyperprior. Fig. 5b presents the ratedistortion comparison with factorized prior [3] and gridbased vector-matrix hyperprior [45]. The former treats attribute components as independent and identically distributed random variables. Our approach achieves 20.0% BD-rate saving compared to FM and also provide superior RD performance than VM hyperprior. INR-based Deformation. In Fig. 5a, we also include INRbased deformation as baseline method, which extends Scaffold-GS [30] with INR-based deformation network from D3DGS [43] to model anchor deformation (x, ) as function of (x, y, z, t). The baseline performs notably worse than TED-4DGS in rate-distortion performance, highlighting the robustness of our deformation networks. 6. Conclusion This work presents novel rate-distortion-optimized compression framework for dynamic 3DGS. To efficiently represent anchor-wise temporal deformation, it features compact embedding-based deformation network that leverages per-anchor temporal embeddings and shared global deformation bank. Furthermore, progressive 4D anchor learning strategy is introduced to ensure smooth deformation and explicit temporal visibility control via learnable temporal activation parameters. Finally, to achieve effective attribute coding, we combine an INR-based hyperprior and channelwise autoregressive model to predict the distribution for entropy coding. These components collectively enable TED4DGS to achieve state-of-the-art rate-distortion performance across several real-world dynamic scene datasets. 7. Acknowledgement (NSTC), Taiwan, This work is supported by MediaTek Advanced Research Center and National Science and Technolunder Grants 113ogy Council and 2634-F-A49-007-, We thank to National 114-2221-E-A49-035-MY3. Center (NCHC) for High-performance Computing for providing computational and storage resources. 112-2221-E-A49-092-MY3,"
        },
        {
            "title": "References",
            "content": "[1] Muhammad Salman Ali, Maryam Qamar, Sung-Ho Bae, and Enzo Tartaglione. Trimming the fat: Efficient compression of 3d gaussian splats through pruning, 2024. 3 [2] Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, and Youngjung Uh. Per-gaussian embedding-based deformation for deformable 3d gaussian splatting, 2024. 2, 3, 6, 7 [3] Johannes Balle, Valero Laparra, and Eero P. Simoncelli. Endto-end optimized image compression, 2017. 8 [4] Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with scale hyperprior, 2018. 3 [5] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes, 2023. 3 [6] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Hac: Hash-grid assisted context for 3d gaussian splatting compression, 2024. 2, 3, 4 [7] Yihang Chen, Qianyi Wu, Mengyao Li, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Fast feedforward 3d gaussian splatting compression, 2025. 3 [8] Woong Oh Cho, In Cho, Seoha Kim, Jeongmin Bae, Youngjung Uh, and Seon Joo Kim. 4d scaffold gaussian splatting for memory efficient dynamic scene reconstruction, 2024. 2, 3 [9] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d-rotor gaussian splatting: Towards efficient novel view synthesis for dynamic scenes, 2024. 3 [10] Bardienus P. Duisterhof, Zhao Mandi, Yunchao Yao, Jia-Wei Liu, Jenny Seidenschwarz, Mike Zheng Shou, Deva Ramanan, Shuran Song, Stan Birchfield, Bowen Wen, and Jeffrey Ichnowski. Deformgs: Scene flow in highly deformable scenes for deformable object manipulation, 2024. [11] Cheng-De Fan, Chen-Wei Chang, Yi-Ruei Liu, Jie-Ying Lee, Jiun-Long Huang, Yu-Chee Tseng, and Yu-Lun Liu. Spectromotion: Dynamic 3d reconstruction of specular scenes, 2025. 3 [12] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps, 2024. 3 [13] Tobias Fischer, Jonas Kulhanek, Samuel Rota Bul`o, Lorenzo Porzi, Marc Pollefeys, and Peter Kontschieder. Dynamic 3d gaussian fields for urban areas, 2024. 3 [14] Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang. Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding, 2022. 3 [15] He Huang, Qi Yang, Mufan Liu, Yiling Xu, and Zhu Li. Adc-gs: Anchor-driven deformable and compressed gaussian splatting for dynamic scene reconstruction, 2025. 2, 3, 6, [16] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes, 2024. 3 [17] Xu Jiawei, Fan Zexin, Yang Jian, and Xie Jin. Grid4D: 4D decomposed hash encoding for high-fidelity dynamic gaussian splatting. The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 3 [18] Hanyang Kong, Xingyi Yang, and Xinchao Wang. Efficient gaussian splatting for monocular dynamic scene rendering via sparse time-variant attribute modeling, 2025. 3 [19] Agelos Kratimenos, Jiahui Lei, and Kostas Daniilidis. Dynmf: Neural motion factorization for real-time dynamic view synthesis with 3d gaussian splatting, 2024. [20] Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, and Munchurl Kim. Modec-gs: Globalto-local motion decomposition and temporal interval adjustment for compact dynamic 3d gaussian splatting, 2025. [21] Isaac Labe, Noam Issachar, Itai Lang, and Sagie Benaim. Dgd: Dynamic 3d gaussians distillation, 2024. 3 [22] Junoh Lee, Chang-Yeon Won, Hyunjun Jung, Inhwan Bae, and Hae-Gon Jeon. Fully explicit dynamic gaussian splatting, 2024. [23] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field, 2024. 3, 6 [24] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, and Zhaoyang Lv. Neural 3d video synthesis from multi-view video, 2022. 7 [25] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis, 2024. 2, 3, 7 [26] Lei Liu, Zhenghao Chen, Wei Jiang, Wei Wang, and Dong Xu. Hemgs: hybrid entropy model for 3d gaussian splatting data compression, 2025. 3, 4 [27] Mufan Liu, Qi Yang, He Huang, Wenjie Huang, Zhenlong Yuan, Zhu Li, and Yiling Xu. Light4gs: Lightweight compact 4d gaussian splatting generation via context model, 2025. 2, 3, 6, 7 [28] Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lyv, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from casually-captured monocular videos with depth priors, 2025. [42] Shuojue Yang, Qian Li, Daiyun Shen, Bingchen Gong, Qi Dou, and Yueming Jin. Deform3dgs: Flexible deformation for fast surgical scene reconstruction with gaussian splatting, 2024. 3 [43] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highfidelity monocular dynamic scene reconstruction, 2023. 7, 8 [44] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting, 2024. 2, 3, 7 [29] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. Compgs: Efficient 3d scene representation via compressed gaussian splatting, 2024. 3 [45] Yu-Ting Zhan, He bi Yang, Cheng-Yuan Ho, Jui-Chiu Chiang, and Wen-Hsiao Peng. Cat-3dgs pro: new benchmark for efficient 3dgs compression, 2025. 8 [46] Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, and Wen-Hsiao Peng. Cat3dgs: context-adaptive triplane approach to rate-distortionoptimized 3dgs compression, 2025. 2, 3, 4, [30] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering, 2023. 4, 6, 8 [31] Hidenobu Matsuki, Gwangbin Bae, and Andrew J. Davison. 4dtam: Non-rigid tracking and mapping via dynamic surface gaussians, 2025. 3 [32] David Minnen and Saurabh Singh. Channel-wise autoregressive entropy models for learned image compression, 2020. 3 [33] David Minnen, Johannes Balle, and George Toderici. Joint autoregressive and hierarchical priors for learned image compression, 2018. 3 [34] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics, 41 (4):115, 2022. 3 [35] Seungjun Oh, Younggeun Lee, Hyejin Jeon, and Eunbyung Park. Hybrid 3d-4d gaussian splatting for fast dynamic scene representation, 2025. [36] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinBrualla, and Steven M. Seitz. Hypernerf: higherdimensional representation for topologically varying neural radiance fields, 2021. 1, 7 [37] Richard Shaw, Michal Nazarczuk, Jifei Song, Arthur Moreau, Sibi Catley-Chandar, Helisa Dhamo, and Eduardo PerezPellitero. Swings: Sliding windows for dynamic 3d gaussian splatting, 2024. 3 [38] Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex C. Kot, and Bihan Wen. Contextgs: Compact 3d gaussian splatting with anchor level context model, 2024. 3, 4 [39] Yifan Wang, Peishan Yang, Zhen Xu, Jiaming Sun, Zhanhua Zhang, Yong Chen, Hujun Bao, Sida Peng, and Xiaowei Zhou. Freetimegs: Free gaussian primitives at anytime and anywhere for dynamic scene reconstruction, 2025. 2, 3, 7 [40] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering, 2024. 2, 3, 6, 7 [41] Kun Yang, Yuxiang Liu, Zeyu Cui, Yu Liu, Maojun Zhang, Shen Yan, and Qing Wang. Ntr-gaussian: Nighttime dynamic thermal reconstruction with 4d gaussian splatting based on thermodynamics, 2025."
        }
    ],
    "affiliations": [
        "National Chung Cheng University, Taiwan",
        "National Yang Ming Chiao Tung University, Taiwan"
    ]
}