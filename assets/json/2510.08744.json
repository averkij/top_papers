{
    "paper_title": "Graph Diffusion Transformers are In-Context Molecular Designers",
    "authors": [
        "Gang Liu",
        "Jie Chen",
        "Yihan Zhu",
        "Michael Sun",
        "Tengfei Luo",
        "Nitesh V Chawla",
        "Meng Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In-context learning allows large models to adapt to new tasks from a few demonstrations, but it has shown limited success in molecular design. Existing databases such as ChEMBL contain molecular properties spanning millions of biological assays, yet labeled data for each property remain scarce. To address this limitation, we introduce demonstration-conditioned diffusion models (DemoDiff), which define task contexts using a small set of molecule-score examples instead of text descriptions. These demonstrations guide a denoising Transformer to generate molecules aligned with target properties. For scalable pretraining, we develop a new molecular tokenizer with Node Pair Encoding that represents molecules at the motif level, requiring 5.5$\\times$ fewer nodes. We curate a dataset containing millions of context tasks from multiple sources covering both drugs and materials, and pretrain a 0.7-billion-parameter model on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses language models 100-1000$\\times$ larger and achieves an average rank of 3.63 compared to 5.25-10.20 for domain-specific approaches. These results position DemoDiff as a molecular foundation model for in-context molecular design. Our code is available at https://github.com/liugangcode/DemoDiff."
        },
        {
            "title": "Start",
            "content": "GRAPH DIFFUSION TRANSFORMERS ARE IN-CONTEXT MOLECULAR DESIGNERS Jie Chen2, Yihan Zhu1 Michael Sun3, Gang Liu1, Tengfei Luo1, Nitesh V. Chawla1, Meng Jiang1 1University of Notre Dame {gliu7, mjiang2}@nd.edu 2 MIT-IBM Watson AI Lab, IBM Research 3MIT CSAIL 5 2 0 O 9 ] . [ 1 4 4 7 8 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "In-context learning allows large models to adapt to new tasks from few demonstrations, but it has shown limited success in molecular design. Existing databases such as ChEMBL contain molecular properties spanning millions of biological assays, yet labeled data for each property remain scarce. To address this limitation, we introduce demonstration-conditioned diffusion models (DemoDiff), which define task contexts using small set of moleculescore examples instead of text descriptions. These demonstrations guide denoising Transformer to generate molecules aligned with target properties. For scalable pretraining, we develop new molecular tokenizer with Node Pair Encoding that represents molecules at the motif level, requiring 5.5 fewer nodes. We curate dataset containing millions of context tasks from multiple sources covering both drugs and materials, and pretrain 0.7-billion-parameter model on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses language models 1001000 larger and achieves an average rank of 3.63 compared to 5.2510.20 for domain-specific approaches. These results position DemoDiff as molecular foundation model for in-context molecular design."
        },
        {
            "title": "INTRODUCTION",
            "content": "In-context learning (ICL) is the emergent capability of large models to infer task-specific concepts from few demonstrations (Xie et al., 2021). ICL has been studied in large language models (LLMs), but was found less effective for molecular design than specialized methods (Liu et al., 2024b). These specialized models often depend on extensive Oracle calls (Gao et al., 2022) or large labeled datasets beyond what context examples provide. Molecular tasks, however, involve millions of types, many with only few labeled examples (Zdrazil et al., 2024). Such examples are enough to form task contexts but insufficient to train new model. This trade-off motivates our in-context molecular design, which combines the flexibility of ICL with the efficiency of molecular domain knowledge. Molecular structures and properties are discrete graphs and numbers with varying scales and units. Directly adapting the autoregressive framework from LLMs is infeasible (Brown et al., 2020) for in-context molecular designs, as the input and output of language data are text in sequential order. Diffusion models show promise for molecular structures (Vignac et al., 2022), and Graph Diffusion Transformers (Graph DiTs) are effective for modeling their joint distribution with properties (Liu et al., 2024c). However, Graph DiTs have been studied with at most five properties represented in single vector. In practice, molecular properties span millions of assays in biology, including functions, binding, ADME, and toxicity, as well as material properties such as gas permeability, thermal conductivity, and glass transition temperature (Figure 1). Representing all properties in one-hot vector with millions of dimensions is inefficient, produces sparse pretraining data since many assays have fewer than ten labels, and limits generalization to unseen properties in downstream tasks. Instead of property vector with large embedding table, we use demonstrations to define the task context for molecular design. As shown in Figure 1, the demonstrations consist of set of molecules with scores in [0, 1] and molecular design is framed as query for the target score of 1. Molecules in the context do not follow strict order, and their scores serve as relative positions to the target, functioning as replacement for position IDs in Transformers. (Xie et al., 2021) described ICL in"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: In-context molecular design with DemoDiff. Each demo is defined as scoremolecule pair, and set of them forms the task context as conditions. After pretraining on large and diverse tasks, DemoDiff serves as foundation model for designing molecules in new task contexts. Scores represent relative distances to the target and are converted from raw labels, as shown in Section 3.3. LLMs as implicit Bayesian inference over latent concepts expressed by examples in the prompt. Similarly, each task in Figure 1 shares the concepts defined by the joint distribution of molecules and their scores. The denoising Transformer in the Graph DiT attends to the context, implicitly extracts concepts, and uses them to guide the reverse process to refine the structure. An example generation trajectory is shown in Figure 7. simple way to represent the task concept is to use positive demonstrations, such as molecules close to the target or active in the assay. However, positive examples alone may be insufficient, as they can overlap across tasks due to factors such as task relatedness (e.g., activity in non-small-cell lung cancer but across different cell lines) or due to sampling bias when the set of positives is extremely sparse (e.g., only one positive example shared by two tasks). To address this, we form the task context using not only positive but also medium and negative examples, providing more complete representation of the task concepts. With these task contexts, we propose demonstration-conditioned diffusion models (DemoDiff) and pretrain 0.7B model with Graph DiT as the backbone, using over 140 H100 GPU days. To support efficient pretraining, we introduce molecular tokenizer trained with Node Pair Encoding (NPE) for motif-level representation. On average, it reduces the number of nodes by 5.5 compared to atom-level representations (Figures 9 and 10). The tokenizer iteratively merges neighboring nodes and selects frequent motifs to construct vocabularies. Motifs are connected by directed edges that preserve bond types and attachment rules, ensuring lossless reconstruction. Graph DiTs naturally use this motif-level representation and attend to motif semantics for denoising. For pretraining, we construct dataset of over 1.6 million tasks from 155K unique properties and one million molecules. It combines ChEMBL for drugs (Zdrazil et al., 2024) and multiple polymer data sources for materials (Otsuka et al., 2011; Thornton et al., 2012; Kuenneth et al., 2021). For ICL, we propose consistency score as confidence measure of whether generation aligns more closely with higher-scoring molecules in demonstrations, effectively filtering out false positives in generation (Section 4.3). We evaluate DemoDiff-0.7B on 33 design tasks across six categories. It matches or surpasses LLMs 1001000 larger in generating diverse, high-scoring molecules. Compared to ten specialized models (average ranks 5.2510.20), DemoDiff ranks 3.6, demonstrating its strength as molecular foundation model. The new molecular tokenizer further improves representation efficiency."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 IN-CONTEXT LEARNING WITH DEMONSTRATIONS In ICL, the context is set of demonstrations = {ei}L i=1, where each ei is an inputlabel pair. Following (Xie et al., 2021), we assume reflects latent concept θ Θ from family of concepts"
        },
        {
            "title": "Preprint",
            "content": "Θ. For example, for paragraph about Albert Einstein, the latent concept may be biography. Given query Q, foundation model with ICL generates an output by marginalizing over θ: p(X C, Q) = (cid:90) θ p(X θ, C, Q) p(θ C, Q) dθ. (1) Here (C, Q) form the prompt. If p(θC, Q) concentrates on the prompt concept with more demonstrations, then the model identifies and applies that concept through marginalization. ICL can thus be implicit Bayesian inference. All context, query, and outcomes are texts in language modeling. In inverse molecular design, we have molecule-score pairs as demonstrations. They capture the latent task concept, with semantics like the task descriptions in Figure 1. is the target score, and is the molecule to be designed. We focus on new molecular foundation model for ICL."
        },
        {
            "title": "2.2 MOLECULAR DESIGN WITH GRAPH DIFFUSION TRANSFORMERS",
            "content": "Molecules are discrete graphs = (A, B), where denotes the set of atoms and the set of bonds. These structures are commonly modeled using discrete diffusion processes (Vignac et al., 2022; Liu et al., 2024c). Graph DiTs concatenate atom and bond features to into the input format of standard Transformers. Given X, for each atom ai with di neighbors, Graph DiTs define token as = {ai, {bij}di j=1}, where bij encodes the bond type (single, double, triple, or none). Each token is represented by feature vector RF , formed by concatenating the one-hot encoding of the atom type and the connection types to all other atoms (either bond type or null type indicating no connection). Discrete diffusion has transition matrix Q, initialized based on the frequency of atoms and bonds in the training set. At step t, [Qt]ij = q(xt ) for i, [1, ]. xt1 The forward diffusion with is: q(xt xt1) = Cat(xt; = xt1Qt), where Cat(x; p) denotes two separate categorical sampling for atoms and bonds with probabilities from p. Starting from the original data = x0, we have q(xt x0) = Cat (cid:0)xt; = x0 Qt(cid:1), where Qt = (cid:81) it Qi. The forward diffusion gradually corrupts data points. When the total timestep is large enough, q(xT ) converges to stationary distribution. The reverse process samples from q(xT ) and gradually removes noise. The posterior distribution q(xt1 xt) is calculated as q(xt1xt, x0) xt(Qt) x0 Qt1. Given multiple properties {ci}n i=1) under property conditions. This model is trained by minimizing the negative log-likelihood for x0: (cid:2) log pϕ In Graph DiTs (Liu et al., 2024c), the constraints {ci}n In this work, we explore them as demonstrations for in-context learning. i=1, the denoising model approximates pϕ(xt1 xt, x0, {ci}n i=1 are numerical or categorical property values. (cid:0)x0 xt, c1, c2, . . . , cn LDM = Eq(x0)Eq(xtx0) (cid:1)(cid:3) , (2)"
        },
        {
            "title": "3 LEARNING DIFFUSION MODEL WITH DEMONSTRATIONS",
            "content": "Figure 2 shows the generation process of DemoDiff, combining motif-based representation (Section 3.1) with graph diffusion transformers for in-context molecular generation (Section 3.2)."
        },
        {
            "title": "3.1 MOLECULAR GRAPH TOKENIZATION WITH NODE PAIR ENCODING",
            "content": "More demonstrations help capture the latent task concept and are empirically useful (Bertsch et al., 2024). However, prior work (Liu et al., 2024c) uses atom-level molecular representations, similar to modeling text at the character level, which fundamentally limits the number of examples in context. For efficient representation, we merge frequent sub-molecular patterns as motif = ( A, B) X, where and define connected substructure. molecule becomes collection of Ai = A. Then, disjoint motifs = {mi}n each edge eij is directed from source motif mi to target motif mj, with two associated attributes: (1) bond type, and (2) attachment specification, indicating the atom within mi from which the bond originates. This abstraction induces tokenizer with two functions. They are tokenizer. encode : = (A, B) (cid:55) ˆX = (M, E) that compresses the atom-level graph into motif-level form and tokenizer. decode : ˆX = (M, E) (cid:55) = (A, B) for reconstruction. The tokenizer uses two vocabularies: for motif types and for edge types. It starts with the 118 atom i=1 such that: (1) Ai Aj = for all = j; (2) (cid:83)n i="
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Demonstration-conditioned diffusion generation. In the reverse process, DemoDiff starts from random noise and denoises molecules conditioned on set of moleculescore demonstration pairs at the motif level, with tokenizer bridging motif and atom representations. types from the periodic table and one * for the polymerization point, which form the initial motifs in M. This guarantees that, in the worst case, new molecule can still be represented at the atom level. In each iteration, the tokenizer merges the most frequent neighbors until no further merge is found in M, then proceeds with the corresponding connections between motifs. To construct M, existing methods, such as BRICS (Degen et al., 2008) or molecular grammars (Sun et al., 2025; 2024), rely on domain-specific heuristics based on chemical reactions or expert knowledge. The resulting vocabularies are independent of the pretraining data, often missing frequent motifs. To address this limitation, we propose Node Pair Encoding (NPE), frequency-based algorithm for molecular graphs inspired by BPE, as outlined in Algorithm 1. We initialize with elements from the periodic table and polymerization points *. NPE iteratively performs three steps: (1) Neighborhood merge: For each molecule in the dataset and current M, we identify candidate motifs by merging adjacent substructures that appear in M; (2) Frequency selection: The most frequent candidate motif is selected and added to M; (3) Graph update: Each is updated by replacing instances of merged motif pairs with the new motif. Constrained NPE: The standard NPE may produce multiple directed edges from motif mi to another mj when decomposing ring structures (e.g., aromatic rings). It leads to ambiguity during decoding since eij does not uniquely determine the attachment specification within mj. To address this, we introduce constraints such as rings into NPE at two stages. During initialization, we traverse each molecule to identify its set of maximal connected rings, denoted R, compute their frequencies, and include the top-Kring most frequent rings in the initial vocabulary M. During motif merging, any is merged with ring as complete unit, rather than merging individual atoms within r. This strategy integrates frequent rings into the vocabulary while preserving atom-level representations for rare rings, avoiding reconstruction ambiguity. Using NPE, molecule is represented as tokens {xi}n j=1}, with mi denoting motif and eij the associated edges. We set the motif vocabulary size to = 3000 (Kring = 300), with details and analysis provided in Section B.2. As shown in Figure 2, an example input with 38 atoms can be compactly expressed using four motifs. An empirical comparison of atomand motif-level representations over 1 million pretraining molecules is given in Figure 3b, with an average compression ratio of 5.446 2.569, reducing the median count from 30 atoms to 5 motifs. i=1, where xi = {mi, {eij}di 3.2 IN-CONTEXT LEARNING WITH GRAPH DIFFUSION TRANSFORMERS We construct the dataset = {(Ci, Qi, Xi)}Npretrain for pretraining, where each task consists of context of moleculescore pairs Ci, query score Qi, and target molecule Xi. To pretrain DemoDiff for in-context inverse molecular design, we replace the property conditions in Eq. (2) with and Q: i=1 Lpretrain = Eq(x0)Eq(xtx0) (cid:2) log pθ (cid:0)x0 xt, C, Q(cid:1)(cid:3) . (3)"
        },
        {
            "title": "Preprint",
            "content": "With large and diverse pretraining data and scalable Transformers (Peebles & Xie, 2023), DemoDiff learns to infer the latent task concept to generate the target molecule and can serve as foundation model for ICL. In task, it performs implicit Bayesian inference over diffusion trajectories. ICL with Context Consistency (Section A.3): Given query , demonstrations Ci are divided into positive, medium, and negative groups. For generated molecule X, we compare its fingerprint-based similarity with these groups to assess whether it follows the relation pos > med > neg. This yields consistency score that measures how well the generated molecule aligns with the relative relations in the context. In experiments (Section 4.1), we use this score to select high consistent generations before conducting the final evaluation with Oracles."
        },
        {
            "title": "3.3 MODEL DESIGN AND PRETRAINING",
            "content": "(a) Property rank vs. frequency in D. (b) Node counts (n 50; full in Figure 9). Figure 3: Pretraining data statistics for property rank-frequency and node count density. Model Designs: Figure 2 illustrates the model architecture. For the i-th task in the pretraining set (Ci, Xi, Yi) D, the property score Yi is scaled within [0, 1], providing positional signals for both demonstration molecules and the target. These scalar values are encoded using Rotary Position Embedding (RoPE) (Su et al., 2024). DemoDiff uses tokenizer to process the atom-level representation and defines maximum context length for the number of motif tokens. The context includes the target along with as many demonstration tokens as fit within the target length. Since molecules in the context are structurally disjoint, edge connectivity implicitly delineates context boundaries, removing the need for explicit delimiter tokens. Details are in Section A. Pretraining: To construct the pretraining dataset as illustrated in Section 3.2, we use the ChEMBL database (Zdrazil et al., 2024), the largest collection of biological assays, containing over 2.5 million molecules and 1.7 million assay records. To increase chemical diversity for materials discovery, we augment ChEMBL with polymer datasets from multiple sources (Liu et al., 2024b; Kuenneth et al., 2021), including properties such as thermal conductivity, free volume fraction, and glass transition temperature. For biological assays, we generate tasks by selecting molecule-assay pair as the target and treating other molecule-assay pairs as context. The target is assigned score of 1, and context scores are computed by normalizing differences in pChEMBL values (negative log of bioactivity measures such as IC50 and potency) to the interval [0, 1]. We restrict targets to bioactive molecules with pChEMBL > 6. For polymers, we apply the same strategy: each polymer is used as target, and its property value is normalized against those of other polymers to form context-target pairs. We partition context examples into three groups by normalized scores: positive [0.75, 1], medium (0.5, 0.75], and negative [0, 0.5], with up to 15 demonstrations from each. The final dataset comprises around 1 million molecules with 155K unique assays or properties, yielding 1.6 million tasks. As shown in Figure 3a, the frequency distribution of assays and properties follows Zipfs law, (Yrank) rank1.13, consistent with patterns in language corpora. These 1 million molecules are used to initialize the motif vocabulary via NPE, which provides more compact representation by reducing node counts (Figure 3b). We further extract edge connections to construct an edge vocabulary capturing motif-to-motif connectivity. Finally, we pretrain DemoDiff model with 0.7B parameter on Eq. (3), using 146 H100 GPU days. Details are provided in Section B."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Harmonic mean of oracle and diversity scores. We group 33 tasks into six categories and report the mean std within each category. The best results in each column are bolded. Task-specific results and additional metrics are provided in Section C.2. Task Category # Tasks Drug Rediscovery 7 Drug MPO 7 Structure Constrained Drug Design 4 Target Based 5 Material Design 5 Avg Rank 33 Total Sum 33 Molecular Optimization Methods with 100 Oracle Calls GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED 0.360.07 0.370.08 0.370.07 0.360.07 0.520.19 0.520.17 0.510.18 0.520.19 0.430.21 0.430.21 0.420.22 0.430.21 0.410.32 0.420.32 0.390.33 0.410. 0.760.04 0.760.03 0.760.03 0.760.04 0.580.11 0.000.00 0.600.21 NO SELFIES 5.25 6.70 5.63 6.42 Molecular Optimization Methods with 10000 Predictor Calls 0.370.09 0.300.13 0.330.10 0.330.10 0.500.18 0.230.23 0.450.22 0.400. 0.450.29 0.250.24 0.430.29 0.500.28 0.490.26 0.380.26 0.490.15 0.270.10 0.640.06 0.170.13 0.740.03 0.200.27 0.550.14 0.510.16 0.420.24 NO SELFIES 7.34 10.20 7.80 9.24 16.65 13.84 16.65 13. 16.30 9.82 15.35 9.66 Conditional Generation Models LSTM Graph-DiT 0.390.25 0.430.21 0.160.07 0.500.18 0.550.32 0.580. 0.330.35 0.480.37 0.720.04 0.710.04 0.160.11 0.550.17 9.31 6.91 12.30 17.64 Learning from In-Context Demonstrations DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) 0.450.18 0.470.21 0.150.21 0.440.21 0.510.20 0.530.20 0.170.15 0.540.23 0.490.24 0.520.30 0.320.32 0.560.33 0.650.18 0.480.40 0.290.29 0.790.11 0.640.06 0.730.05 0.190.26 0.780. 0.390.24 0.430.16 0.100.18 0.670.11 6.76 6.34 11.56 3.63 16.90 17.25 6.46 20."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "Setups: We curate 33 downstream tasks (see Table 1 and Section C.1) across six categories to evaluate DemoDiff against 13 baselines. These tasks are primarily curated by domain experts and are distinct from pretraining. We include eight molecular optimization methods and two conditional generation models (LSTMs and Graph DiT (Liu et al., 2024c)), and LLMs (DeepSeek-V3, GPT-4o, and QwenMax). We select the top four molecular optimization algorithms from the PMO benchmark (Gao et al., 2022) (out of 25 evaluated methods), under two settings: 100 oracle calls and 10,000 predictor calls. For evaluation, we generate 10 valid, unique, and novel molecules per task and score them with Oracles. We report the harmonic mean over two dimensions: (a) averaged oracle scores and (b) the diversity score Eq. (6). Each task has up to 450 moleculescore pairs, evenly divided into positive [0.75, 1], medium (0.5, 0.75], and negative [0, 0.5] groups. Each task has an Oracle function for evaluation, with limited budgets for Oracle calls. We use all molecules to train the task-specific predictor for predictor calls or to train conditional generation models directly. For different ICL methods, demonstrations are randomly sampled with similar budget for context."
        },
        {
            "title": "4.1 PERFORMANCE ON DIVERSE MOLECULAR DESIGNS TASKS",
            "content": "ICL achieves competitive performance with minimal supervision. Table 1 compares the harmonic mean of the top-10 generated molecules based on both task scores and structural diversity, while Table 5 (appendix) reports the top-1 scoring molecule per task. Under limited data and Oracle budgets, ICL methods perform comparably to, or better than, fully trained conditional generators and molecular optimization baselines. Excluding Qwen-Max, DemoDiff and other LLM-based ICL approaches consistently attain top-tier average ranks. These ICL methods rely on tens of demonstrations per task, significantly fewer than the training data or Oracle calls required by other models or algorithms. DemoDiff designs molecules with accurate scores and high diversity. Across six task categories, it performs best on property-driven tasks, including drug design with bioactivity targets, protein binding affinity, and material design for polymer gas separation. It achieves the lowest average rank of 3.62, outperforming the best baseline, GraphGA (rank 5.25). ICL methods with LLMs produce high-scoring top designs  (Table 5)  but often generate structurally similar molecules. These do not"
        },
        {
            "title": "Preprint",
            "content": "(a) Harmonic score vs. context length (b) Harmonic score vs. positive samples Figure 4: Ablation studies on Albuterol drug rediscovery. necessarily align better with the target score while reducing diversity. In contrast, DemoDiff designs molecules with scores closer to the query and better structural diversity. DemoDiff performs better on property-driven tasks than on structure-constrained ones. It scores 0.670.79 on drug and material design, but around 0.440.56 for rediscovery and structureconstrained tasks, where Oracle scoring is tied to the presence of specific structures. While DemoDiff still ranks highly in structure-constrained tasks, its stronger results on property-driven tasks highlight its advantage in exploring chemical spaces with broader solution ranges."
        },
        {
            "title": "4.2 ABLATION STUDIES AND PERFORMANCE ANALYSIS",
            "content": "Table 2: Performance across model sizes using harmonic mean scores from Top-10 generations DemoDiff 78M 311M 739M Drug Rediscovery 0.39 0.17 0.40 0.17 0.44 0.21 Drug MPO 0.46 0.24 0.46 0.23 0.54 0.23 Structure Constrained 0.59 0.06 0.63 0.06 0.56 0.33 Drug Design Target Based Material Design 0.57 0.31 0.53 0.27 0.79 0.11 0.73 0.03 0.75 0.04 0.78 0.05 0.62 0.13 0.62 0.14 0.67 0.11 Model Parameters: We pretrain DemoDiff with varying sizes: small (78.7M), medium (311M), and large (739M) parameters. Table 2 reports performance using the top-10 harmonic means of task score and diversity. We present averages with deviations across six categories. DemoDiff achieves reasonable scores even at small scale. At the medium scale, performance improves in most tasks except drug design, while the benefits of parameter scaling become more evident at the large scale. ICL with Demonstrations: Figure 4 studies two factors in demonstrations: (1) context length and (2) ratio of positive examples. In Figure 4a, longer context includes more molecular examples and supports better ICL performance. This aligns with the rationale of motif-level tokenization, which captures more examples within fixed context. Figure 4b shows that diverse demonstrations are important for ICL to represent the task accurately, while only positive examples are insufficient. This is because positive, medium, and negative examples together provide holistic view of the task context, and DemoDiff pretrained on such contexts is better able to infer latent concepts from diverse examples. In Figure 4b, we also observe that fewer positive examples may still yield reasonable results. We investigate this further in Section 4.3 to assess whether DemoDiff can infer positive examples (score > 0.5) using only negative examples with scores below 0.5. ICL with Consistency Scores: We ablate consistency scores and analyze their correlation with target scores in Figure 5. Using the consistency score as confidence filter improves performance across task categories, with gains from 0.8% to 27.5%. The second figure shows the correlation between the consistency and target scores. Moderate correlation appears in tasks with explicit structural constraints, such as drug rediscovery and structure-constrained design. For property-driven tasks (drug MPO and materials design), high fingerprint-based consistency with positive examples does not always correlate with high target scores. In these cases, latent concepts may rely on subtle substructures (e.g., methyl groups (Liu et al., 2022)) that standard fingerprints fail to capture."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Ablation studies on context consistency scores: (1) left shows improvements; (2) right shows the relationship between consistency score and oracle scores. Figure 6: Learning from negative demonstrations (score < 0.5) to infer target with score 1. All demonstrations are shown in Figures 14 to 16, with only three displayed here. Interestingly, context consistency still improves performance in these tasks. possible reason is that the score helps filter out false positive generations."
        },
        {
            "title": "4.3 CASE STUDIES",
            "content": "Figures 6 and 7 present two studies for DemoDiff. In extreme cases of inverse molecular design, demonstration sets may contain only negative examples, i.e., all scores < 0.5. In Figure 6, we study whether DemoDiff can still generate positive candidates when prompted solely with negative examples. Figure 6 presents the results for (a) structure-constrained design, (b) drug multi-objective optimization (MPO), and (c) target-based design. These findings suggest two insights: (1) negative demonstrations convey informative signals about the task concept, and (2) after pretraining, the posterior over the concept-to-structure mapping allows DemoDiff to generate desirable candidates that are aligned with the concept yet structurally distinct from the negative examples. Figure 7 is the generation trajectory from diffusion models. The task score, measured as structural similarity to Albuterol, rises from 0.22 at initial sampling to 0.74. This shows that the diffusion model refines the molecule toward the desired structure step by step with demonstrations."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Diffusion trajectory for Albuterol drug rediscovery: we sample five intermediate diffusion steps and score them with the Albuterol Oracle, which computes similarity to the ground truth."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Inverse Molecular Design: Molecular optimization uses diverse approaches, including genetic algorithms, Monte Carlo Tree Search (Jensen, 2019), and Bayesian optimization (Shahriari et al., 2015), applied to representations such as fingerprints, SMILES, graphs, and synthetic pathways (Gao et al., 2021). Gao et al. (2022) benchmarked 25 optimization methods and found that older models, such as genetic algorithms, remain competitive. However, existing benchmarks require on the order of 10,000 oracle calls, which is costly and limits applicability when single calls are expensive. Deep learning models offer an alternative by modeling the joint distribution of atoms and bonds without Oracle calls. GDSS applies noise and denoising in continuous space for graphs (Jo et al., 2022). DiGress (Vignac et al., 2022) introduces discrete noise through transition matrices based on marginal atom and bond distributions. Graph DiTs (Liu et al., 2024c) extend scalable diffusion transformers (Peebles & Xie, 2023) to discrete graphs. Yet, training diffusion models still requires hundreds of labeled molecules and is limited to specific tasks. Recent efforts explore chemical foundation models based on LLMs (Yu et al., 2024; Liu et al., 2024b), but their applications are either diverted to other molecular tasks, such as property prediction, or rely on fine-tuning within restricted scope of design tasks. In-Context Learning: ICL is an emergent ability observed in LLMs (Brown et al., 2020; Chan et al., 2022). Empirical and theoretical studies investigate this phenomenon from three perspectives: models, data, and learning mechanisms (Xie et al., 2021; Min et al., 2022). For the learning mechanism, ICL can be interpreted as implicit Bayesian inference (Xie et al., 2021), where pretraining data are generated from latent concepts and the posterior distribution marginalizes over them for inference. On the model side, Garg et al. (2022) trained Transformers from scratch on prompt-style inputlabel pairs of simple functions and found performance comparable to task-specific algorithms. Bhattamishra et al. (2023) compared Transformers with attention-free models and showed they do not match Transformer performance across tasks. On the data side, Chan et al. (2022) found that Transformers outperform recurrent models (e.g., LSTMs) on data with distributional properties resembling natural language, such as burstiness (words appearing in clusters) and query tasks with many rare classes. Singh et al. (2025) analyzed the strategy competition between ICL and in-weight learning, showing that the asymptotic strategy depends on in-weight information but is also context-constrained. This aligns with (Chan et al., 2022), suggesting that foundation model should support both capacities. skewed Zipfian distribution over tasks (e.g., Figure 3a) balances learning by storing common task information in weights while developing ICL ability from the long tail of rare tasks."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We presented DemoDiff, demonstration-conditioned diffusion Transformer model for in-context molecular design. We constructed large-scale pretraining dataset with over one million molecules and 155K unique biological assays and material properties, yielding millions of demonstrationtarget pairs. Using this dataset, we pretrained 0.7B-parameter model and showed that it matches or outperforms much larger LLMs and ranks higher than domainand task-specific methods. To support scalable pretraining, we introduce Node Pair Encoding, motif-level graph tokenizer that efficiently represents molecules with fewer nodes while preserving reconstruction. Experiments demonstrate that DemoDiff is promising molecular foundation model, highlighting its potential to scale further with larger models, broader datasets, and greater compute."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Amr Alhossary, Stephanus Daniel Handoko, Yuguang Mu, and Chee-Keong Kwoh. Fast, accurate, and reliable molecular docking with quickvina 2. Bioinformatics, 31(13):22142216, 2015. Amanda Bertsch, Maor Ivgi, Emily Xiao, Uri Alon, Jonathan Berant, Matthew Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024. Satwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade. Understanding in-context arXiv preprint learning in transformers and llms by learning to learn discrete functions. arXiv:2310.03016, 2023. Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3): 10961108, 2019. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in neural information processing systems, 35:1887818891, 2022. Jorg Degen, Christof Wegscheid-Gerlach, Andrea Zaliani, and Matthias Rarey. On the art of compiling and usingdrug-likechemical fragment spaces. ChemMedChem, 3(10):1503, 2008. Joseph Durant, Burton Leland, Douglas Henry, and James Nourse. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6): 12731280, 2002. Wenhao Gao, Rocío Mercado, and Connor Coley. Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design. arXiv preprint arXiv:2110.06389, 2021. Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: benchmark for practical molecular optimization. Advances in neural information processing systems, 35: 2134221357, 2022. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? case study of simple function classes. Advances in neural information processing systems, 35:3058330598, 2022. Jan Jensen. graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):35673572, 2019. Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International conference on machine learning, pp. 1036210383. PMLR, 2022. Xiangzhe Kong, Wenbing Huang, Zhixing Tan, and Yang Liu. Molecule generation by principal subgraph mining and assembling. Advances in Neural Information Processing Systems, 35: 25502563, 2022. Christopher Kuenneth, Arunkumar Chitteth Rajan, Huan Tran, Lihua Chen, Chiho Kim, and Rampi Ramprasad. Polymer informatics with multi-task learning. Patterns, 2(4), 2021. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a."
        },
        {
            "title": "Preprint",
            "content": "Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph rationalization with environment-based augmentations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 10691078, 2022. Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, and Jie Chen. Multimodal large language models for inverse molecular design with retrosynthetic planning. arXiv preprint arXiv:2410.04223, 2024b. Gang Liu, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph diffusion transformers for multi-conditional molecular generation. Advances in Neural Information Processing Systems, 37:80658092, 2024c. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. Shingo Otsuka, Isao Kuwajima, Junko Hosoya, Yibin Xu, and Masayoshi Yamazaki. Polyinfo: Polymer database for polymeric materials design. In 2011 International Conference on Emerging Intelligent Data and Web Technologies, pp. 2229. IEEE, 2011. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Lloyd Robeson. The upper bound revisited. Journal of membrane science, 320(1-2):390400, 2008. Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan Adams, and Nando De Freitas. Taking the human out of the loop: review of bayesian optimization. Proceedings of the IEEE, 104(1): 148175, 2015. Aaditya Singh, Ted Moskovitz, Sara Dragutinovic, Felix Hill, Stephanie C.Y. Chan, and Andrew Saxe. Strategy coopetition explains the emergence and transience of in-context learning. In Fortysecond International Conference on Machine Learning, 2025. URL https://openreview. net/forum?id=esBoQFmD7v. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Michael Sun, Minghao Guo, Weize Yuan, Veronika Thost, Crystal Elaine Owens, Aristotle Franklin Grosz, Sharvaa Selvan, Katelyn Zhou, Hassan Mohiuddin, Benjamin Pedretti, et al. Representing molecules as random walks over interpretable grammars. arXiv preprint arXiv:2403.08147, 2024. Michael Sun, Weize Yuan, Gang Liu, Wojciech Matusik, and Jie Chen. Foundation molecular grammar: Multi-modal foundation models induce interpretable molecular graph languages. arXiv preprint arXiv:2505.22948, 2025. Thornton, Robeson, Freeman, and Uhlmann. Polymer gas separation membrane database, 2012. URL https://research.csiro.au/virtualscreening/ membrane-database-polymer-gas-separation-membranes/. Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et al. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025. Botao Yu, Frazier Baker, Ziqi Chen, Xia Ning, and Huan Sun. Llasmol: Advancing large language models for chemistry with large-scale, comprehensive, high-quality instruction tuning dataset. arXiv preprint arXiv:2402.09391, 2024."
        },
        {
            "title": "Preprint",
            "content": "Barbara Zdrazil, Eloy Felix, Fiona Hunter, Emma Manners, James Blackshaw, Sybilla Corbett, Marleen De Veij, Harris Ioannidis, David Mendez Lopez, Juan Mosquera, et al. The chembl database in 2023: drug discovery platform spanning multiple bioactivity data types and time periods. Nucleic acids research, 52(D1):D1180D1192, 2024."
        },
        {
            "title": "A DETAILS ON DEMODIFF",
            "content": "A.1 GRAPH-LEVEL TOKENS AND PRETRAINING LOSS Graph DiTs define graph-level token that concatenates the node feature with all related edge features. In molecular generation, we use special type of edge, the null edge, to represent that there is no edge between two nodes. Thus, The feature vector (or x0) of graph-level token = {M, {ej}d j=1} consists of three components: Fmotif motif types, Fbond bond types, and Fattach attachment specifications. Here, Fmotif is the size of the motif vocabulary M, Fbond = 4 represents null, single, double, and triple bonds, and Fattach = arg maxM M is the maximum number of atoms in M. Eq. (3) can be decomposed as Lpretrain = Lmotif + Lbond + Lattach. Specifically, Lpretrain = Eq(x) Eq(xtx) (cid:34) log pmotif (cid:124) θ (cid:0)m xt, C, (cid:1) (cid:123)(cid:122) (cid:125) Lmotif (cid:88) j=1 (cid:88) j=1 (cid:124) (cid:124) log pbond θ (cid:0)bj xt, C, (cid:1) (cid:123)(cid:122) Lbond (cid:125) log pattach θ (cid:0)aj xt, C, (cid:1) (cid:35) . (cid:123)(cid:122) Lattach (cid:125) (4) To align feature dimensions across tokens, we use the dense edge representation by treating all nonconnections as null bonds. The resulting feature dimension is = Fmotif + Fbond + Fattach, where is the maximum number of nodes in the motif-represented dataset. For optimization with Eq. (4), we include the null bond type in Lbond but exclude attachment specifications of null edges in Lattach. A.2 TRANSITION MATRICES IN DIFFUSION MODELS We define the transition matrix that perturbs molecules at the motif level to pretrain DemoDiff. We model the joint distribution of nodes and edges with the transition matrix. It is constructed from four submatrices QV , QEV , QE, QV E, denoting transitions node node, edge node, edge edge, and node edge, respectively: QG = (cid:20) QV 1n QEV 1 QV 1nn QE (cid:21) , (5) where denotes the Kronecker product, and 1N , 1 n, and 1nn are the column vector, row vector, and all-ones matrix, respectively. Here is the number of nodes. For edges, diffusion is applied to bond types only, while attachment attributes are optimized and predicted directly by the denoising model as in Eq. (4). For categorical sampling, we separate the unnormalized logits of node and edge from the model outputs, compute probabilities for each motif and bond individually before sampling. To obtain the transition matrices, we use the prior from the pretraining data. The noisy distribution is defined as the marginal distributions of motif types mV and bond types mE. The transition matrices are defined as QV = αtI + (1 αt)1m E, where denotes the transpose and is the identity matrix. We compute co-occurrence frequencies of motif and bond types in training graphs to obtain the marginal distributions mEV and mV E. Each row in mEV and QE = αtI + (1 αt)1m"
        },
        {
            "title": "Preprint",
            "content": "gives the probability of co-occurring motifs for bond type, and mV is its transpose. The transition matrices are then defined as QEV = αtI + (1 αt)1m E, where αt is cumulative noise coefficient in diffusion. The cosine schedule is chosen as αt = cos(0.5π(t/T + s)/(1 + s))2. EV and QV = αtI + (1 αt)1m A.3 DETAILS ON CONSISTENCY SCORE Given query , demonstrations Ci = {(Xij, Yij)}L j=1 are divided into positive Cpos, medium Cmed, and negative Cneg examples to guide ICL. For generated molecule X, we use the Tanimoto similarity of fingerprints as the similarity measure. We compute the similarity between and all molecules in each group and average them to obtain group-wise similarity scores simpos, simmed, simneg [0, 1]. Difference-based score. We compute margin differences between groups: dpos,med = max(simpos simmed, 0), dmed,neg = max(simmed simneg, 0), The normalized difference-based score is dpos,neg = max(simpos simneg, 0). sdiff = min (cid:18) dpos,med + dmed,neg + dpos,neg (cid:19) , 1 . In experiments, the consistency score can be computed efficiently before applying Oracle functions. For example, we generate 1000 molecules and select the top 100 with the highest consistency scores. These molecules better follow the order of structural similarity across positive, medium, and negative examples. This removes poor generations that conflict with the demonstration semantics and increases confidence that selected molecules align with the query scores. Table 3 reports empirical improvements across task categories, each containing 47 tasks (Section C.1). Table 3: Improvement with the consistency score (average Top-10 harmonic scores)."
        },
        {
            "title": "Improvement",
            "content": "Drug Design Drug MPO Drug Rediscovery Structure Constrained Materials Design Target-Based 0.6230 0.4592 0.4110 0.5258 0.6407 0.7745 0.7943 0.5400 0.4407 0.5598 0.6724 0.7803 +27.5% +17.6% +7.2% +6.5% +4.9% +0.8%"
        },
        {
            "title": "B DETAILS ON PRETRAINING",
            "content": "The final pretraining dataset contains 1,084,566 molecules (polymers) and 155,150 unique assays or properties, yielding 1,639,515 tasks. These are constructed from ChEMBL (Zdrazil et al., 2024) and multiple polymer data sources (Otsuka et al., 2011; Thornton et al., 2012; Kuenneth et al., 2021). Each task has query moleculescore pair with the query score fixed at 1. Up to 45 molecules are grouped into positive, medium, and negative demonstrations based on their scores. The query molecule is the target, while the query score and demonstrations serve as inputs to DemoDiff during pretraining on Eq. (3). For pretraining with fixed maximum context window, we allocate half the window to positive demonstrations and one quarter each to medium and negative demonstrations, after excluding the target molecule."
        },
        {
            "title": "Preprint",
            "content": "B.1 PRETRAINING DATASET ChEMBL dataset We constructed molecular activity contexts from the ChEMBL database (version 35), which provides large collection of bioactivity measurements across diverse assays. ChEMBL standardizes published activity types, values, and units into unified variable, pChEMBL = log(molar IC50, XC50, EC50, AC50, Ki, Kd, or Potency). This value places different measures of half-maximal response, potency, or affinity on comparable negative logarithmic scale. For example, an IC50 of 1 nanomolar (1109 M) corresponds to pChEMBL value of 9. We extracted assay-level activity values (pChEMBL). For each assay, molecules were grouped according to their recorded activities. Within each group, we selected anchor molecules with strong activity (pChEMBL > 6) as targets for building demonstrations. Each anchor was compared against all other molecules in the same assay to compute normalized distances, defined as the relative difference between the anchors pChEMBL value and that of the candidate molecule, converted to the range [0, 1]. Specifically, for an anchor with value va and candidate with value vc, the normalized distance was given by = (v1 vc)/10. Based on this distance, we partitioned candidate molecules into three categories relative to the anchor. Molecules with distances in [0, 0.25) correspond to candidates with activity between 75% and 100% of the anchor and were assigned to the positive context. Molecules with distances in [0.25, 0.5) correspond to candidates with activity between 50% and 75% of the anchor and were assigned to the medium context. Molecules with distances [0.5, 1.0] correspond to candidates with activity below 50% of the anchor and were assigned to the negative context. From each category, we sampled up to 15 molecules to balance neighborhood size. Thus, there are up to 45 demonstration molecules for each task. Not all of them are used during pretraining due to the constraint of maximum context length. This procedure produced triplets of anchor molecules and their associated positive, medium, and negative contexts. Polymeric materials We have polymeric material datasets from different sources, including PolyInfo (Otsuka et al., 2011), MSA (Thornton et al., 2012), and from (Kuenneth et al., 2021). We considered wide range of polymer properties spanning several categories, including thermal properties (e.g., heat capacity, glass transition temperature, melting temperature, and thermal conductivity), electronic properties (e.g., ionization energy, electron affinity, and band gap), structural properties (e.g., density, crystallinity, and radius of gyration), and transport properties (e.g., gas diffusion, solubility, and permeability coefficients). For each property, raw values were normalized to the unit interval using minmax scaling, with logarithmic transformation applied when dynamic ranges exceeded 1000 and non-negative shifts applied when necessary. Each polymer with valid property values was treated as an anchor, and pairwise distances in normalized property space were computed against all other polymers. Candidate molecules were partitioned into positive [0, 0.25), medium [0.25, 0.5), and negative [0.5, 1] contexts, with up to 15 examples sampled per category based on smallest distances. B.2 TOKENIZER PREPARATION Figure 8: Change in node count with varying motif vocabulary size. We present NPE in Algorithm 1, inspired by both the classic BPE and (Kong et al., 2022). We build the tokenizer with NPE on the pretraining data. To choose the motif vocabulary size, we analyze the number of nodes in motif-represented molecular graphs as the vocabulary size varies (Figure 8)."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Node Pair Encoding (NPE) with Constraints Require: molecule list D, motif vocabulary = , max size K, ring count threshold Nring Ensure: motif vocabulary 1: Initialize each molecule with atom-level and ring-based motifs 2: Count frequencies of ring-based motifs across 3: Add all periodic-table elements, polymerization *, and top-Nring frequent rings to 4: while < do 5: 6: 7: 8: (1) Merge Neighbor: Initialize empty multiset for each molecule do for each motif from do for each adjacent motif in such that and are mergeable under structural constraints (e.g., rings treated as units) do form new motif add to multiset with frequency count end for end for 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end while 25: return end if end for end for (2) Frequency Selection: Find most frequent motif (3) Update Graph: for each molecule do for each pair of adjacent motifs (m, m) in do if their merged form equals then replace and with in end for Add to if not already in it We report mean, max, and median counts. We set Kring = K/10, except for = 6000, where Kring = 300. When 3000, the mean and max node counts no longer significantly decrease, and the median remains unchanged. Therefore, we select = 3000 with Kring = 300 for pretraining. Next, Figure 9 presents the tokenization results on the pretraining dataset. log-scale distributions of motifand atom-level node counts. Both representations exhibit heavy-tailed behavior, as shown by the rank-frequency plots and complementary cumulative distribution functions (CCDFs). Figure 10 shows the distribution of compression ratios, defined as Uncompressed Graph Size Compressed Graph Size , in both linear and logarithmic scale. The ratio ranges from 1 to 40, with median and mean around 5.5, indicating consistent reduction in graph size by approximately factor of five. Figure 11 provides detailed analysis of the relationship between atom-level and motif-level representations. We observe mild positive correlation: larger molecules tend to yield higher compression ratios. Notably, molecules with 150 to 200 atoms are reduced by up to factor of 15, demonstrating efficient compression at larger scales. B.3 MODEL PRETRAINING We pretrain 0.7B-parameter model (Transformer depth 24, hidden size 1280, 16 heads, MLP ratio 4) for 550 epochs, requiring 49 days on 24 H100 GPUs, or about 146 GPU days. We monitor training loss and reconstruction accuracy for each component in Eq. (4). As shown in Figure 12, loss decreases and accuracy increases throughout training. Near the end, values plateau but still show incremental gains. Pretraining was stopped once reconstruction accuracy exceeded 0.99 for all components due to resource limits. During training, we also generated 512 molecules at sampled steps using the validation set. In Figure 13, we report chemical validity and structural similarity to ground truth measured by MACCS fingerprints (Durant et al., 2002). Both metrics improve with training and reach about 0.83 validity and 0.69 similarity."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Comparison of the number of nodes in atomand motif-based representations for 1 million molecules from the pretraining set. Figure 10: Analysis on the compression ratio. These trends in loss, accuracy, validity, and similarity indicate that larger models, more data, and additional compute could further improve DemoDiff as molecular foundation model."
        },
        {
            "title": "C DETAILS ON EXPERIMENTS",
            "content": "C.1 DETAILS ON EXPERIMENTAL SET-UPS We curate 33 benchmark tasks across six categories to evaluate DemoDiff against 13 baselines, including eight molecular optimization methods, two conditional generation models, and three LLMs. Details are in Table 4. The benchmarks span seven drug rediscovery tasks, seven drug multi-objective optimization (MPO) tasks, five structure-constrained generation tasks, four drug design tasks, five target-based generation tasks, and five polymer property design tasks. Specifically, the benchmarks span the following tasks: Drug rediscovery (7 tasks): Celecoxib rediscovery, Mestranol similarity, Thiothixene rediscovery, Troglitazone rediscovery, Median 1 (median similarity between camphor and menthol), Median 2 (median similarity between tadalafil and sildenafil), and Albuterol"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Benchmark Task Statistics: Example Counts and Score Ranges [min, median, max] Task Name Total Pos Med Neg All Scores Pos Scores Med Scores Neg Scores Albuterol Similarity 450 150 150 Celecoxib Rediscovery 450 150 150 Median 1 Median 2 150 0 0 0 0 Mestranol Similarity 450 150 150 Thiothixene Rediscovery Troglitazone Rediscovery 383 36 83 150 150 150 150 150 150 150 Drug Rediscovery [0.075, 0.537, 1.000] [0.015, 0.548, 1.000] [0.000, 0.057, 0.419] [0.038, 0.122, 0.413] [0.004, 0.538, 1.000] [0.019, 0.510, 1.000] [0.018, 0.526, 1.000] [0.752, 0.807, 1.000] [0.753, 0.786, 1.000] [0.502, 0.537, 0.744] [0.505, 0.548, 0.735] [0.752, 0.824, 1.000] [0.753, 0.790, 1.000] [0.752, 0.792, 1.000] [0.500, 0.538, 0.713] [0.505, 0.560, 0.736] [0.504, 0.553, 0.727] Amlodipine Mpo 450 150 150 150 Fexofenadine Mpo 150 150 150 Osimertinib Mpo 450 150 150 Perindopril Mpo 306 6 150 Ranolazine Mpo 450 150 150 Sitagliptin Mpo Zaleplon Mpo 389 300 89 150 150 150 150 Drug MPO [0.000, 0.517, 0.871] [0.000, 0.570, 0.960] [0.000, 0.638, 0.908] [0.000, 0.502, 0.790] [0.000, 0.575, 0.867] [0.000, 0.610, 0.841] [0.000, 0.493, 0.637] [0.750, 0.784, 0.871] [0.750, 0.772, 0.960] [0.750, 0.761, 0.908] [0.766, 0.778, 0.790] [0.750, 0.764, 0.867] [0.750, 0.768, 0.841] Structure Constrained Design [0.075, 0.219, 0.476] [0.015, 0.153, 0.367] [0.000, 0.057, 0.419] [0.038, 0.122, 0.413] [0.004, 0.150, 0.379] [0.019, 0.155, 0.354] [0.018, 0.143, 0.250] [0.000, 0.145, 0.500] [0.000, 0.125, 0.498] [0.000, 0.047, 0.493] [0.000, 0.114, 0.415] [0.000, 0.052, 0.454] [0.000, 0.000, 0.213] [0.000, 0.003, 0.486] [0.252, 0.283, 0.500] [0.000, 0.000, 0.449] [0.000, 0.000, 0.386] [0.333, 0.380, 0.444] [0.000, 0.000, 0.000] [0.502, 0.517, 0.742] [0.501, 0.570, 0.720] [0.501, 0.638, 0.747] [0.502, 0.522, 0.680] [0.502, 0.575, 0.743] [0.500, 0.641, 0.748] [0.501, 0.528, 0.637] [0.502, 0.529, 0.677] [0.535, 0.592, 0.741] [0.503, 0.561, 0.720] [0.500, 0.510, 0.627] [0.512, 0.686, 0.739] [0.793, 0.842, 0.953] [0.799, 0.819, 1.000] [0.767, 0.779, 0.882] [0.754, 0.782, 0.828] [0.757, 0.806, 0.975] [0.760, 0.961, 1.000] [0.760, 0.880, 1.000] [0.760, 0.890, 1.000] [0.751, 0.819, 0.947] [0.507, 0.623, 0.742] [0.510, 0.615, 0.750] [0.510, 0.570, 0.750] [0.501, 0.644, 0.749] [0.000, 0.004, 0.344] [0.000, 0.030, 0.380] [0.000, 0.010, 0.340] [0.010, 0.348, 0.499] [0.757, 0.771, 0.879] [0.757, 0.771, 0.871] [0.764, 0.800, 1.000] [0.757, 0.771, 0.936] [0.757, 0.771, 0.864] [0.777, 0.777, 0.777] [0.752, 0.988, 1.000] [0.783, 0.974, 1.000] [0.507, 0.607, 0.729] [0.507, 0.600, 0.736] [0.507, 0.536, 0.629] [0.507, 0.586, 0.714] [0.507, 0.614, 0.750] [0.538, 0.603, 0.731] [0.506, 0.561, 0.631] [0.509, 0.615, 0.739] [0.514, 0.619, 0.731] [0.501, 0.579, 0.726] [0.000, 0.450, 0.500] [0.000, 0.464, 0.500] [0.000, 0.450, 0.500] [0.000, 0.464, 0.500] [0.000, 0.471, 0.500] [0.042, 0.198, 0.487] [0.000, 0.102, 0.498] [0.000, 0.050, 0.487] [0.005, 0.151, 0.495] [0.309, 0.346, 0.493] Deco Hop 450 150 150 150 Isomers C7H8N2O2 150 150 150 Isomers C9H10N2O2Pf2Cl 450 150 150 Scaffold Hop 450 150 150 Valsartan Smarts 188 23 15 DRD2 GSK3B JNK3 QED 450 150 150 450 150 150 450 150 150 150 450 150 150 [0.252, 0.529, 0.953] [0.000, 0.592, 1.000] [0.000, 0.561, 0.882] [0.333, 0.510, 0.828] [0.000, 0.000, 0.975] Drug Design [0.000, 0.623, 1.000] [0.000, 0.615, 1.000] [0.000, 0.570, 1.000] [0.010, 0.644, 0.947] Docking 5ht1b 450 150 150 Docking braf Docking fa7 Docking jak 450 150 150 305 5 450 150 150 Docking parp1 450 150 150 150 150 Target Based Design [0.000, 0.607, 0.879] [0.000, 0.600, 0.871] [0.000, 0.507, 1.000] [0.000, 0.586, 0.936] [0.000, 0.614, 0.864] 150 Polymer CO2 CH4 Polymer CO2 N2 Polymer H2 CH4 Polymer H2 N2 Polymer O2 160 158 177 175 173 0 13 7 0 9 14 18 23 Material Design 150 150 150 150 [0.042, 0.203, 0.777] [0.000, 0.110, 0.631] [0.000, 0.117, 1.000] [0.005, 0.178, 1.000] [0.309, 0.355, 0.726]"
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Analysis on the relationship between atomand motif-level representations. (a) Training losses over steps (b) Training accuracies over steps Figure 12: Training curves showing (a) losses and (b) accuracies. similarity. These tasks use Oracle scoring functions based on the similarity between the drug and the target using extended connectivity fingerprint (Brown et al., 2019). Drug MPO (7 tasks): Perindopril MPO, Ranolazine MPO, Osimertinib MPO, Zaleplon MPO, Sitagliptin MPO, Amlodipine MPO, and Fexofenadine MPO. These tasks use Oracle scoring functions based on drugtarget similarity with extended connectivity fingerprints, along with additional constraints such as logP, TPSA, and Bertz, computed using RDKit (Brown et al., 2019). Structure-constrained design (5 tasks): Isomers C7H8N2O2, Isomers C9H10N2O2PF2Cl, Decoration hop, Scaffold hop, and Valsartan SMARTS. These tasks use Oracle scoring functions primarily based on SMARTS patterns that evaluate whether particular structure is present or absent in the target, optionally combined with other computational constraints. Or whether the target is an isomer of the molecular formula."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Generation validity and structure similarity to the target during pretraining. Drug design (4 tasks): DRD2, JNK3, GSK3β, and QED. These tasks use Oracle scoring functions based on ML models. QED is based on RDKit. Target-based design (5 tasks): Docking BRAF, Docking PARP1, Docking JAK2, Docking FA7, and Docking 5-HT1B. These tasks use Oracle scoring functions based on the docking program QuickVina 2 (Alhossary et al., 2015). Docking scores are negative, with smaller values indicating better binding. To map them into [0, 1] (where larger values are better), we use = clip , where clip(x, a, b) = min(max(x, a), b). , 0, (cid:16) (cid:17) docking score 14 Material design (5 tasks): Polymer gas separation for different gas pairs: CO2/CH4, CO2/N2, H2/CH4, H2/N2, and O2/N2. Each task studies whether two gases can be separated based on the polymeric membrane materials. We evaluate their selectivity score for gas separation (Robeson, 2008), defined as the log-ratio of permeabilities relative to an empirical boundary, shifted and clipped into the range [0, 1]. Gas permeabilities are calculated using ML models trained on all available labeled data (a superset of the task-specific data), and the selectivity score is then computed based on gas permeabilities. Each task contains up to 450 moleculescore pairs, evenly split into positive, medium, and negative groups. Some tasks may have fewer pairs due to insufficient positive examples, as shown in Table 4. For instance, the Median 1 and Median 2 tasks in drug rediscovery have no positive or medium examples. These pairs are used to train predictors for molecular optimization methods, conditional generators, or to provide demonstrations for ICL methods. Task scores lie within [0, 1], with the objective of generating molecules with score 1. Each task also defines an oracle function, which is used only for evaluation, except by molecular optimization methods that actively query the oracle. For baselines, we compare against four molecular optimization methods from the PMO benchmark (Gao et al., 2022). They are the top four methods selected from 25 candidates: Graph Genetic Algorithm (GraphGA), REINVENT (SMILES-based), Gaussian Process Bayesian Optimization (GPBO), and Superfast Traversal, Optimization, Novelty, Exploration, and Discovery (STONED, based on SELFIES). SELFIES is unavailable for polymers and STONED cannot be applied to material design tasks. We have two evaluation settings: one with 100 oracle calls and one with 10,000 predictor calls. While PMO permits up to 10,000 Oracle calls, such budgets are impractical in real-world settings due to the cost and time associated with laboratory experiments, which may require days to months for single call. To address this issue, we examine whether molecular optimization methods can be paired with predictor calls. Following prior work (Gao et al., 2022; Liu et al., 2024c), we use random forest predictor trained on all 450 moleculescore pairs as the task-specific predictor. We include conditional generation models such as LSTM and Graph DiT (Liu et al., 2024c). They are trained on all available training data for each task. For ICL, we compare DemoDiff (739M parameters) with recent large-scale LLMs, including DeepSeek-V3 (Liu et al., 2024a), GPT-4o (Achiam et al., 2023), and Qwen-Max (Yang et al., 2025), each with up to hundreds of billions of parameters. For LLMs, we sample 12 positive, 6 medium, and 6 negative as demonstrations. For DemoDiff, we set the context size to 150 motif tokens. Excluding the target molecule, the context includes on average 23 demonstrations: half positive, one quarter medium, and one quarter negative. For evaluation, each method generates 100 valid, unique, and novel molecules per task, which are scored by oracle functions. We report the average of the top-10 oracle scores as the performance"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Top-1 performance across 33 tasks. Scores are reported with target of 1 as mean std by category. Best results in each column is bolded. Task Category # Tasks Drug Rediscovery 7 Drug MPO 7 Structure Constrained Drug Design 4 Target Based 5 Material Design 5 Avg Rank 33 Total Sum 33 Molecular Optimization Methods with 100 Oracle Calls GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED 0.280.06 0.310.07 0.280.06 0.280.06 0.490.18 0.470.16 0.460.18 0.490.18 0.460.14 0.450.14 0.470.14 0.460.14 0.450.33 0.560.38 0.410.35 0.440. 0.740.04 0.750.06 0.750.07 0.740.06 0.720.23 0.000.00 0.800.25 NO SELFIES Molecular Optimization Methods with 10000 Predictor Calls 0.330.14 0.360.29 0.370.31 0.280.17 0.560.21 0.380.30 0.500.25 0.430.25 0.540.38 0.420.43 0.470.32 0.510. 0.600.41 0.770.12 0.640.38 0.300.10 0.830.12 0.590.37 0.840.08 0.260.36 0.700.21 0.870.17 0.580.34 NO SELFIES 6.65 7.55 7.03 8.31 6.04 6.73 7.06 9.93 16.73 13.68 16.95 13. 19.03 17.67 18.13 10.07 Conditional Generation Models LSTM Graph-DiT 0.470.36 0.460.27 0.330.20 0.530.07 0.640.39 0.600. 0.360.34 0.510.44 0.730.12 0.700.07 0.480.29 0.780.23 8.76 7.61 16.30 19.44 Learning from In-Context Demonstrations DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) 0.660.37 0.530.31 0.180.28 0.540.33 0.600.26 0.560.20 0.190.17 0.540.19 0.540.25 0.520.32 0.530.34 0.590.37 0.740.14 0.540.41 0.510.45 0.910.07 0.710.12 0.690.09 0.210.29 0.770. 0.750.27 0.770.22 0.250.40 0.930.16 6.34 7.34 10.25 3.94 21.77 19.69 9.60 22.63 score and compute its harmonic mean with the diversity score. The diversity score is computed as IntDiv(G) = 1 1 G2 (cid:88) (m1, m2)2 m1,m2G m1=m2 1 2 , (6) where denotes the generated set of molecules for evaluation. For DemoDiff, we first generate 1000 candidates and select the top 100 with the highest consistency scores, prioritizing alignment with the context order of positive, medium, and negative examples. C.2 ADDITIONAL DISCUSSION OF EXPERIMENTAL RESULTS We include more results in Tables 6 to 9 and 12 to 15. Beyond the discussion of ICL methods and DemoDiff in Section 4.1, we have additional observations: Oracle quality critically affects molecular optimization. Comparing molecular optimization methods under varying numbers of function calls, we find that allowing more predictor queries does not consistently lead to better performance. This suggests that both the quantity and quality of function evaluations (oracle or predictor) are essential for guiding molecular optimization. While not the main focus of this study, this insight points to an important direction for future work. For instance, in the structure-constrained design task involving the Valsartan SMARTS pattern (CN(C=O)Cc1ccc(c2ccccc2)cc1), shown in Table 14, all molecular optimization methods receive score of zero. This failure is due to predictor trained on limited data, which cannot model the latent design constraints, such as satisfying multiple SMARTS patterns and physicochemical properties (e.g., logP, TPSA, and Bertz index (Brown et al., 2019)). In contrast, in target-based design tasks (e.g., Table 16), where training data are sufficient, more predictor calls improve performance by allowing finer structural optimization. Performance alignment across methods may indicate task difficulty. It is challenging to formally quantify task difficulty, as it depends on the oracle definition and data quality. However, we observe that tasks where molecular optimization performs wellsuch as target-based or material designare also more tractable for ICL methods, which can infer the underlying concept with few demonstrations. This alignment suggests that task difficulty may be partially reflected in cross-method consistency."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Harmonic mean of Top-10 performance and diversity scores on the Drug Design task category. Scores are reported with target of 1. Best results in each column is bolded. Task DRD2 JNK3 GSK3B QED Avg Rank Total Sum Molecular Optimization Methods with 100 Oracle Calls GraphGA REINVENT GPBO STONED 0.22 0.31 0.20 0.22 0.23 0.21 0.25 0.23 0.31 0.26 0.24 0.31 0.90 0.90 0.89 0. 6.25 7.00 7.75 7.25 Molecular Optimization Methods with 10000 Predictor Calls GraphGA REINVENT GPBO STONED 0.60 0.31 0.55 0.14 0.18 0.09 0.27 0.23 0.40 0.40 0.54 0. 0.79 0.71 0.60 0.32 7.00 8.75 6.00 9.50 1.65 1.68 1.57 1.65 1.97 1.51 1.97 1.07 Conditional Generation Models LSTM Graph-DiT 0.15 0.78 0.04 0.08 0.27 0.23 0.83 0.81 11.00 9.50 1.30 1. Learning from In-Context Demonstrations DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) 0.71 0.80 0.00 0.88 0.65 0.13 0.14 0.65 0.42 0.13 0.36 0.78 0.84 0.84 0.68 0. 3.75 8.25 10.75 2.25 2.62 1.90 1.18 3.18 Table 7: Top-1 performance on the Drug Design task category. Scores are reported with target of 1. Best results in each column is bolded. Task DRD2 JNK3 GSK3B QED Avg Rank Total Sum Molecular Optimization Methods with 100 Oracle Calls GraphGA REINVENT GPBO STONED 0.23 0.83 0.23 0.19 0.23 0.23 0.23 0. 0.41 0.24 0.23 0.41 0.94 0.94 0.94 0.94 6.50 6.00 8.25 8.25 Molecular Optimization Methods with 10000 Predictor Calls GraphGA REINVENT GPBO STONED 0.99 0.76 0.99 0. 0.14 0.61 0.19 0.21 0.38 0.80 0.47 0.44 0.90 0.91 0.93 0.30 8.00 5.25 6.50 9.75 1.82 2.25 1.64 1.77 2.41 3.08 2.58 1. Conditional Generation Models LSTM Graph-DiT 0.28 0.99 0.06 0.07 0.26 0.22 0.85 0. 11.00 10.25 1.45 2.06 Learning from In-Context Demonstrations DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) 0.77 0.94 0.00 1.00 0.75 0.21 0.26 0. 0.54 0.17 0.89 0.89 0.88 0.85 0.89 0.93 5.75 10.25 7.25 2.00 2.94 2.17 2.04 3.65 Nonetheless, exceptions exist. As shown in Table 7, for DRD2 and JNK3, molecular optimization underperforms under limited supervision, while ICL methods, including DeepSeek-V3, GPT-4o, and DemoDiff, achieve strong results."
        },
        {
            "title": "Preprint",
            "content": "Table 8: Harmonic mean of Top-10 performance and diversity scores on the Drug MPO task category. Scores are reported with target of 1. Best results in each column is bolded. Task Perindopril Ranolazine Osimertinib Zaleplon Sitagliptin Amlodipine MPO MPO MPO MPO MPO MPO Fexofenadine MPO Avg Rank Total Sum GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) 0.52 0.50 0.51 0. 0.41 0.11 0.23 0.53 0.06 0.59 0.49 0.63 0.26 0.52 Molecular Optimization Methods with 100 Oracle Calls 0.35 0.39 0.39 0.35 0.77 0.75 0.76 0. 0.47 0.48 0.45 0.47 0.24 0.24 0.23 0.24 0.60 0.59 0.59 0.60 Molecular Optimization Methods with 10000 Predictor Calls 0.60 0.24 0.36 0.26 0.14 0. 0.63 0.39 0.21 0.58 0.64 0.65 0.68 0.47 0.48 0.39 0.45 0.47 0.13 0.00 0.12 0.01 Conditional Generation Models 0.26 0. 0.09 0.43 0.13 0.31 Learning from In-Context Demonstrations 0.70 0.68 0.00 0.80 0.42 0.46 0.39 0.43 0.10 0.17 0.00 0. 0.59 0.09 0.62 0.60 0.21 0.55 0.63 0.69 0.25 0.62 0.70 0.66 0.68 0.70 0.62 0.10 0.67 0.45 0.21 0. 0.58 0.67 0.06 0.73 4.29 5.14 6.29 5.29 7.00 12.57 7.43 8.00 3.64 3.62 3.60 3.64 3.48 1.58 3.12 2.79 12.57 7. 1.10 3.49 7.14 4.71 12.86 4.71 3.55 3.70 1.17 3.78 Table 9: Top-1 performance on the Drug MPO task category. Scores are reported with target of 1. Best results in each column is bolded. Task Perindopril Ranolazine Osimertinib Zaleplon Sitagliptin Amlodipine MPO MPO MPO MPO MPO MPO Fexofenadine MPO Avg Rank Total Sum GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) 0.43 0.40 0.38 0.43 0.40 0.17 0.15 0.44 0.08 0.58 0.74 0.74 0.30 0. Molecular Optimization Methods with 100 Oracle Calls 0.36 0.41 0.36 0.36 0.78 0.74 0.78 0.78 0.40 0.43 0.37 0.40 0.23 0.23 0.23 0.23 0.56 0.48 0.48 0. Molecular Optimization Methods with 10000 Predictor Calls 0.67 0.34 0.41 0.26 0.11 0.51 0.77 0.47 0.21 0.65 0.83 0.82 0.78 0.77 0.41 0.40 0.40 0. 0.28 0.00 0.30 0.01 Conditional Generation Models 0.59 0.61 0.22 0.46 0.44 0.46 Learning from In-Context Demonstrations 0.78 0.71 0.00 0.77 0.39 0.43 0.49 0.39 0.10 0.19 0.00 0.24 0.53 0.22 0.64 0.52 0.42 0.48 0.77 0.71 0.22 0. 0.62 0.58 0.60 0.62 0.81 0.72 0.80 0.62 0.46 0.61 0.63 0.65 0.12 0.73 5.86 8.14 9.71 7.29 4.14 9.43 6.00 8. 3.40 3.26 3.20 3.40 3.93 2.68 3.48 3.03 11.71 6.00 2.31 3.71 5.57 5.57 11.29 5.71 4.17 3.90 1.33 3. C.3 DETAILS ON CASE STUDIES Figures 14 to 16 present case studies using negative demonstrations to generate molecules with positive scores. The tasks include structure-constrained design of an isomer with 17 demonstrations, drug MPO of Osimertinib with 23 demonstrations, and protein target design of PARP1 with 26 demonstrations. These molecules contain 460, 631, and 481 atoms, respectively, far exceeding the 150-token context window under atom-level representations. The motif-level representation efficiently encodes all demonstrations within the same window."
        },
        {
            "title": "Preprint",
            "content": "Table 10: Harmonic mean of Top-10 performance and diversity scores on the Drug Rediscovery task category. Scores are reported with target of 1. Best results in each column is bolded. Celecoxib Mestranol Rediscovery Thiothixene Similarity Rediscovery Rediscovery Troglitazone Median 1 Median 2 Albuterol Avg Similarity Rank Molecular Optimization Methods with 100 Oracle Calls 0.39 0.44 0.39 0.39 0.38 0.37 0.39 0.38 0.34 0.34 0.33 0.34 0.26 0.30 0.28 0.26 0.29 0.28 0.30 0.29 Molecular Optimization Methods with 10000 Predictor Calls 0.41 0.15 0.43 0.49 0.66 0.58 0.46 0.64 0.00 0.57 0.38 0.38 0.37 0.38 0.35 0.26 0.26 0.32 Conditional Generation Models 0.20 0.33 0.25 0.22 0.28 0.19 0.19 0.19 0.20 0.39 Learning from In-Context Demonstrations 0.47 0.61 0.00 0. 0.57 0.37 0.00 0.49 0.18 0.20 0.00 0.12 0.28 0.27 0.27 0.27 0.12 0.17 0.25 0.19 0.15 0.23 0.39 0.39 0.37 0. 0.38 0.31 0.34 0.39 0.61 0.60 0.54 0.54 0.48 0.50 0.49 0.50 0.51 0.49 0.55 0.53 0.46 0.26 0.70 0. 0.70 0.73 0.40 0.75 Table 11: Top-1 performance on the Drug Rediscovery task category. Scores are reported with target of 1. Best results in each column is bolded. Celecoxib Mestranol Rediscovery Thiothixene Similarity Rediscovery Rediscovery Troglitazone Median 1 Median 2 Albuterol Avg Similarity Rank Molecular Optimization Methods with 100 Oracle Calls 0.28 0.39 0.31 0.28 0.28 0.32 0.29 0.28 0.23 0.25 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0.23 0. Molecular Optimization Methods with 10000 Predictor Calls 0.38 0.28 0.57 0.64 0.82 0.47 1.00 0.68 0.00 0.64 0.28 0.28 0.26 0.28 0.25 0.27 0.20 0. Conditional Generation Models 0.18 0.51 0.33 0.18 0.21 0.13 0.13 0.13 0.20 0.40 Learning from In-Context Demonstrations 0.84 0.81 0.00 0.34 0.75 0.46 0.00 0.65 0.12 0.15 0.00 0.14 0.20 0.20 0.20 0.20 0.09 0.10 0.17 0.15 0.10 0. 0.27 0.34 0.28 0.27 0.35 0.36 0.25 0.27 1.00 0.78 0.77 0.53 0.72 0.84 0.40 0.41 0.41 0.40 0.62 1.00 1.00 0. 0.69 0.81 1.00 0.93 0.41 1.00 23 Task GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) Task GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) Total Sum 2.54 2.62 2.57 2.54 2.61 2.08 2.32 2.30 7.14 6.29 6.57 8.14 6.86 9.29 10.14 8. 7.71 6.57 2.74 3.02 5.29 4.29 12.57 5.43 3.18 3.28 1.03 3.09 Total Sum 1.93 2.18 1.99 1. 2.30 2.51 2.61 1.96 7.71 6.14 7.43 9.43 7.29 7.29 9.29 10.14 6.86 6.43 3.31 3.25 4.71 5.43 12.14 4. 4.64 3.70 1.23 3."
        },
        {
            "title": "Preprint",
            "content": "Table 12: Harmonic mean of Top-10 performance and diversity scores on the Material Design task category. Scores are reported with target of 1. Best results in each column is bolded. Task Polymer CO2/CH4 Polymer CO2/N2 Polymer H2/CH4 Polymer H2/N Polymer O2/N2 Avg Rank Total Sum GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) Molecular Optimization Methods with 100 Oracle Calls 0.59 0.00 0.46 NO SELFIES 0.42 0.00 0.31 NO SELFIES 0.71 0.00 0.76 NO SELFIES 0.52 0.00 0.79 NO SELFIES 0.64 0.00 0.68 NO SELFIES 4.60 11.00 3.40 NO SELFIES 2.88 0.00 3.00 NO SELFIES Molecular Optimization Methods with 10000 Predictor Calls 0.60 0.32 0.55 NO SELFIES 0.55 0.37 0.00 NO SELFIES 0.75 0.56 0.57 NO SELFIES 0.40 0.69 0.52 NO SELFIES 0.44 0.61 0.46 NO SELFIES 4.20 5.80 7.20 NO SELFIES 2.75 2.55 2.09 NO SELFIES 0.00 0. 0.13 0.37 0.00 0.63 Conditional Generation Models 0.29 0.55 0.16 0.74 0.12 0.57 Learning from In-Context Demonstrations 0.45 0.30 0.09 0.52 0.53 0.44 0.00 0.72 0.68 0.70 0.00 0.81 0.23 0.63 0.15 0.33 0.42 0.68 10.00 4. 7.40 7.00 10.80 2.00 0.80 2.76 1.94 2.14 0.51 3.36 Table 13: Top-1 performance on the Material Design task category. Scores are reported with target of 1. Best results in each column is bolded. Task Polymer CO2/CH Polymer CO2/N2 Polymer H2/CH4 Polymer H2/N2 Polymer O2/N2 Avg Rank Total Sum GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) Molecular Optimization Methods with 100 Oracle Calls 0.79 0.00 0.89 NO SELFIES 0.41 0.00 0.43 NO SELFIES 1.00 0.00 1.00 NO SELFIES 0.57 0.00 1.00 NO SELFIES 0.84 0.00 0.68 NO SELFIES 5.60 11.20 4.40 NO SELFIES 3.60 0.00 4.00 NO SELFIES Molecular Optimization Methods with 10000 Predictor Calls 0.69 0.71 0.67 NO SELFIES 0.82 0.67 0.00 NO SELFIES 1.00 1.00 0.80 NO SELFIES 0.51 1.00 0.85 NO SELFIES 0.50 0.97 0.57 NO SELFIES 6.20 3.60 8.40 NO SELFIES 3.52 4.35 2.90 NO SELFIES 0.00 0.44 0.35 0.67 0.00 1.00 Conditional Generation Models 0.77 1. 0.55 1.00 0.63 0.79 Learning from In-Context Demonstrations 1.00 1.00 0.02 1.00 0.60 0.52 0.34 0.64 0.95 1.00 0.00 1. 24 0.44 0.69 0.86 0.65 0.91 1.00 8.60 5.60 6.20 6.80 9.60 1.80 2.39 3. 3.76 3.85 1.27 4."
        },
        {
            "title": "Preprint",
            "content": "Table 14: Top-1 performance on the Structure Constrained Design task category. Scores are reported with target of 1. Best results in each column is bolded. Task Isomers c7h8n2o2 Isomers c9h10n2o2pf2cl Deco Hop Scaffold Valsartan Smarts Hop Avg Rank Total Sum GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) Molecular Optimization Methods with 100 Oracle Calls 0.55 0.55 0.55 0.55 0.47 0.44 0.50 0.47 0.58 0.58 0.59 0.58 0.45 0.45 0.47 0. 0.23 0.23 0.23 0.23 Molecular Optimization Methods with 10000 Predictor Calls 1.00 1.00 0.88 0.82 1.00 0.88 0.72 0.74 0.90 0.88 0.78 0.00 0.47 0. 0.54 0.62 0.55 0.57 Conditional Generation Models 0.87 0.82 0.54 0.53 0.39 0.47 0.45 0.47 0.78 0. Learning from In-Context Demonstrations 0.50 0.82 0.73 0.73 0.84 0.59 0.56 0.86 0.41 0.46 0.45 0.49 0.00 0.00 0.00 0.00 0.00 0. 0.21 0.00 0.01 0.00 7.60 8.20 6.80 9.40 8.00 6.60 10.20 8.40 2.29 2.26 2.34 2.29 2.72 2.09 2.36 2.57 6.20 7. 3.18 3.01 7.80 6.20 7.40 4.40 2.68 2.61 2.67 2.96 Table 15: Harmonic mean of Top-10 performance and diversity scores on the Structure Constrained Design task category. Scores are reported with target of 1. Best results in each column is bolded. Task Isomers c7h8n2o Isomers c9h10n2o2pf2cl Deco Hop Scaffold Valsartan Smarts Hop Avg Rank Total Sum GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) Molecular Optimization Methods with 100 Oracle Calls 0.30 0.36 0.27 0. 0.44 0.40 0.45 0.44 0.68 0.68 0.68 0.68 0.57 0.57 0.57 0.57 0.14 0.14 0.14 0.14 Molecular Optimization Methods with 10000 Predictor Calls 0.71 0.49 0.69 0. 0.80 0.85 0.64 0.61 0.70 0.82 0.69 0.00 0.29 0.65 0.42 0.46 0.65 0.68 Conditional Generation Models 0.75 0. 0.65 0.66 0.45 0.30 0.53 0.57 0.55 0.66 Learning from In-Context Demonstrations 0.47 0.78 0.55 0.77 0.70 0.67 0.00 0. 0.52 0.54 0.36 0.57 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00 0.01 0.00 6.40 7.00 6.00 7.40 8.60 12.00 10.00 7. 2.13 2.15 2.11 2.13 2.27 1.24 2.16 2.51 7.20 5.60 2.75 2.91 6.40 6.80 9.00 5.40 2.43 2.60 1.62 2."
        },
        {
            "title": "Preprint",
            "content": "Table 16: Top-1 performance on the Target Based Design task category. Scores are reported with target of 1. Best results in each column is bolded. Table 17: Harmonic mean of Top-10 performance and diversity scores on the Target Based Design task category. Scores are reported with target of 1. Best results in each column is bolded. Task GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT Docking Docking Docking Docking Docking 5HT1B Jak2 Parp1 Braf Fa7 Molecular Optimization Methods with 100 Oracle Calls 0.75 0.77 0.79 0. 0.76 0.80 0.83 0.77 0.74 0.69 0.72 0.77 0.67 0.66 0.64 0.64 0.76 0.80 0.78 0.76 Molecular Optimization Methods with 10000 Predictor Calls 0.84 0.68 0.84 0. 0.74 0.72 0.91 1.00 0.91 0.00 0.79 0.59 0.84 0.61 0.66 0.69 0.71 0.00 Conditional Generation Models 0.76 0. 0.63 0.66 0.61 0.59 Learning from In-Context Demonstrations DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) 0.80 0.77 0.54 0.81 0.72 0.72 0.00 0. 0.81 0.64 0.00 0.79 0.52 0.57 0.51 0.60 Task GraphGA REINVENT GPBO STONED GraphGA REINVENT GPBO STONED LSTM Graph-DiT Docking Docking Docking Docking Docking 5HT1B Jak2 Parp1 Braf Fa7 Molecular Optimization Methods with 100 Oracle Calls 0.78 0.77 0.77 0. 0.79 0.79 0.79 0.78 0.76 0.73 0.76 0.77 0.70 0.71 0.70 0.70 0.79 0.78 0.78 0.78 Molecular Optimization Methods with 10000 Predictor Calls 0.66 0.19 0.69 0. 0.73 0.71 0.70 0.27 0.78 0.00 0.60 0.31 0.76 0.52 0.57 0.09 0.73 0.00 Conditional Generation Models 0.73 0. 0.71 0.70 0.66 0.66 Learning from In-Context Demonstrations DeepSeek-V3 GPT-4o Qwen-Max DemoDiff (Ours) 0.63 0.77 0.48 0.81 0.64 0.74 0.00 0. 0.63 0.70 0.00 0.78 0.57 0.66 0.49 0.69 26 Avg Rank Total Sum 6.60 5.60 5.60 7. 2.60 8.20 2.00 12.80 3.69 3.73 3.76 3.69 4.15 2.96 4.21 1.32 8.20 9.60 3.66 3.49 8.00 9.80 13.80 5. 3.57 3.46 1.06 3.84 Avg Rank Total Sum 2.80 3.80 3.80 4.00 10.40 12.80 6.00 12.80 3.82 3.78 3.79 3. 3.22 0.86 3.70 1.00 7.40 8.20 3.61 3.55 10.60 7.00 13.40 2.00 3.19 3.63 0.96 3.90 0.95 0.00 0.91 0. 0.92 0.74 0.72 0.76 0.00 0.77 0.70 0.00 0.74 0.00 0.77 0.75 0.73 0.76 0.00 0."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Structure constrained generation for Isomer with all negative demonstrations."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Drug MPO for Osimertinib with all negative demonstrations."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Target-based design for PARP1 with all negative demonstrations."
        }
    ],
    "affiliations": [
        "MIT CSAIL",
        "MIT-IBM Watson AI Lab, IBM Research",
        "University of Notre Dame"
    ]
}