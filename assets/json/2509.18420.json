{
    "paper_title": "Instruction-Following Evaluation in Function Calling for Large Language Models",
    "authors": [
        "Nikolai Skripko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Function calling is a core capability of large language models, essential for AI agents. Existing benchmarks such as the Berkeley Function Calling Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench (arXiv:2501.12851) evaluate argument correctness but do not test adherence to format instructions embedded in parameter descriptions, such as enclosing values in double quotes or using ISO date formats. We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911) that assesses precise instruction following in function calling. IFEval-FC encodes verifiable formats directly within JSON schema descriptions, for example specifying that a value must not contain punctuation. It includes 750 test cases, each consisting of a function with an embedded format for one of its input parameters and a corresponding user query. Evaluation is fully algorithmic, ensuring objectivity, reproducibility, and scalability. Our results show that even state-of-the-art proprietary models, including GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules, highlighting a practical limitation for real-world agent systems. The complete codebase and data are publicly available at https://github.com/Skripkon/IFEval-FC."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 2 4 8 1 . 9 0 5 2 : r Instruction-Following Evaluation in Function Calling for Large Language Models Nikolai Skripko1,2 naskripko@edu.hse.ru 1Higher School of Economics, Moscow 2SberDevices, Moscow September 24,"
        },
        {
            "title": "ABSTRACT",
            "content": "Function calling is core capability of Large Language Models (LLMs), essential for AI agents. Existing benchmarks (e.g., BFCL (Patil et al., 2025), τ 2-Bench (Barres et al., 2025), ACEBench (Chen et al., 2025)) evaluate argument correctness but do not test adherence to format instructions embedded in parameter descriptions, such as enclosing values in double quotes or using ISO date formats. We introduce IFEval-FC, benchmark inspired by IFEval (Zhou et al., 2023), which assesses precise instruction following in function calling. IFEval-FC encodes verifiable formats directly within JSON schema descriptions, such as value must not contain punctuation. It offers 750 test cases, each consisting of function with an embedded format for one of its input parameters and corresponding user query. The evaluation is fully algorithmic, ensuring objectivity, reproducibility, and scalability. Our results indicate that even state-of-the-art proprietary models, such as GPT-5 (OpenAI, 2025) and Claude Opus 4.1 (Anthropic, 2025), frequently fail to adhere to basic formatting rules, highlighting significant limitation for practical applications in real-world agent systems. The complete codebase and data are publicly available at (cid:135) https://github.com/Skripkon/IFEval-FC"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) are increasingly being deployed as core components of AI agents that interact with tools, APIs, and external systems via function calling. These agents leverage LLMs not only for natural language understanding and reasoning but also for structured API usage, tool execution, and decision-making in real-world domains. critical capability in this context is the models ability to correctly interpret and adhere to function signatures, particularly the formatting requirements specified in parameter descriptions within JSON schemas. For instance, function schema may specify that parameter user name must be string starting with capital letter or that date must follow the ISO 8601 format. Despite their apparent simplicity, such format instructions are frequently overlooked or misinterpreted by LLMs, leading to invalid function calls and downstream failures in agent workflows. 1 Current benchmarks for function calling evaluate only functional correctness or API selection accuracy, often overlooking whether arguments are correctly formatted. This gap leaves crucial dimension of agent robustness under-evaluated. To address this, we introduce IFEval-FC, benchmark inspired by IFEval that focuses on evaluating LLMs ability to follow verifiable format instructions in the context of function calling. In IFEvalFC, each instruction is embedded directly into the description field of parameter in JSON schema (e.g., should not include punctuation, must be lowercase). These instructions are designed to be objectively verifiable through algorithmic checks, enabling fully automated and reproducible evaluation without reliance on human or LLM-as-a-Judge (Gu et al., 2025)."
        },
        {
            "title": "2 VERIFIABLE INSTRUCTIONS",
            "content": "The core innovation of IFEval-FC lies in its adaptation of verifiable instructions from the original IFEval benchmark. Unlike traditional function calling benchmarks that focus on functional correctness or API selection, IFEval-FC evaluates the models ability to follow precise formatting constraints embedded within JSON schema parameter descriptions. 2."
        },
        {
            "title": "INSTRUCTION CATEGORIES AND TYPES",
            "content": "We identified 19 distinct types of verifiable instructions, which we organized into seven major categories according to the nature of the constraints they impose (see Table 1). significant portion of our instruction types were adapted from the original IFEval benchmark, which focused on instruction following in general text generation tasks. Other types of instructions are introduced here for the first time (such as Cyrillic Greek or Python List Format)."
        },
        {
            "title": "2.2.1 FUNCTIONS",
            "content": "A subset of our functions was sourced from the BFCL benchmark, providing real-world function schemas that we enhanced with our verifiable instruction constraints. Other functions were generated synthetically using GPT-5 through carefully designed prompt engineering process. Our generation pipeline consisted of the following steps: 1. Domain Selection: We curated 80 diverse domains representing real-world use cases where AI assistants might need to call functions. 2. Function Schema Generation: For each domain, we used structured prompts to generate JSON schemas with specific requirements: Each function must include one free-form parameter for natural language input. The free-form parameter must be string with no enum or format field. This was done to simulate situation similar to IFEval, where model needs to generate natural language text when specific format is imposed on it. Such parameter allows the application of any format constraint, which is not always possible in BFCL, since not all formats are meaningful for specific arguments such as user id. 3. Instruction Injection: We injected verifiable instructions into the description of one parameter of each function (free-form parameter for the generated functions and randomly chosen string parameter with no enum or format field for the BFCL functions)."
        },
        {
            "title": "2.2.2 USER QUERY GENERATION",
            "content": "To ensure realistic evaluation scenarios, we generated five diverse user queries for each function using GPT-5. These queries were designed to satisfy the following criteria: They are expressed in natural, conversational language. 2 Instruction Group Instruction Description Keywords Keywords Keywords Language Keywords Presence Requires inclusion / exclusion of specific keywords Keyword Frequency Specifies exact frequency requirements for keywords Letter Frequency Controls the frequency of specific letters Cyrillic Greek Restricts text to specific writing systems Length Constraints Word Count Controls the number of words in the response Length Constraints Sentence Count Manages sentence count requirements Detectable Content Postscript Requires specific postscript markers Detectable Content Placeholder Count Controls the number of placeholder markers Detectable Format Spaces In Between Enforces specific spacing patterns Detectable Format Title Format Requires specific formatting markers Detectable Format Highlighted Sections Count Controls markdown highlighting Detectable Format Json Format Requires valid JSON formatting Detectable Format Python List Format Enforces Python list syntax Case Case Case Start/End Start/End Punctuation All Uppercase All Lowercase Requires all uppercase text Requires all lowercase text All Capital Words Controls the number of all-caps words End Phrase Quotation Commas Requires specific ending phrases Enforces quotation mark wrapping Controls comma frequency Table 1: Examples of verifiable format instructions adapted for function parameters. Each entry represents category of instruction constraints encoded in parameter description. They contain values for all required parameters to call function. They were generated based on the original functions (without embedded formats) so that users do not attempt to satisfy the format in advance; instead, it is the LLMs task during function calling to transform value into the required format. Each function and its associated queries were further validated to ensure that the function was indeed necessary for completing the task. We verified this through straightforward procedure: if an LLM failed to call the target function in response to all five queries, the task was deemed ill-posed or overly challenging and omitted. Conversely, tasks in which all five queries were answered correctly with ease were also omitted. An ensemble of LLMs was employed to increase the robustness of task selection. We observed that Anthropics most recent models (e.g., Claude Opus 4.1) exhibited significantly higher refusal rate when deciding whether to call function compared to other models. Closer analysis revealed that these models often requested user clarification whenever subtle uncertainties arose (e.g., adherence to specific format unknown to the user). To mitigate this issue and ensure fairness across evaluations, we added system message explicitly instructing the model to always call function: SYSTEM_MESSAGE = ''' YOU MUST CALL FUNCTION NO MATTER WHAT. NEVER ASK USER TO SPECIFY OR CLARIFY ANYTHING. ALWAYS CALL FUNCTION. '''.strip() Listing 1: System message enforcing function invocation. 3 2."
        },
        {
            "title": "INSTRUCTION DIFFICULTY AND FILTERING",
            "content": "During development, we discovered that certain instruction types (particularly trivial case constraints like all-uppercase/lowercase) were too easy for modern models, often achieving 90100% accuracy across models. To maintain the benchmarks discriminative power, we excluded these trivial instruction types from the final evaluation. The remaining instruction types represent carefully curated set that provides meaningful differentiation between model capabilities. 2.4 IFEVAL-FC METRICS For given response and verifiable instruction i, we define the function that verifies whether the instruction is followed: score(r, i) = (cid:26)1, 0, if instruction is followed. otherwise. (1) We evaluated several language models on IFEval-FC using Equation 1. Figure 1 presents the overall performance across all format constraint types. See Tables 2-4 in the appendix for comprehensive breakdown by instruction. Figure 1: Overall performance of several language models on the IFEval-FC benchmark. Higher scores indicate better adherence to format constraints."
        },
        {
            "title": "3 DISCUSSION AND FUTURE WORK",
            "content": "The results indicate that the latest models perform significantly better than their predecessors. However, no evaluated model surpassed 80% accuracy, suggesting that precise instruction-following in function calling remains an open problem for LLMs, despite being trivial task for humans. To increase the benchmarks difficulty, future work will expand the number of available functions and selecting more difficult samples from the originally generated ones. In the current setup, only one function is available for the model to call. more challenging scenario would require the model to first select the correct function from set of options, which may lead to decrease in observed performance. Additionally, while our current benchmark focuses on English-language function calls, future iterations could incorporate multilingual support, drawing inspiration from M-IFEval (Dussolle et al., 2025) to assess cross-lingual instruction-following capabilities."
        },
        {
            "title": "REFERENCES",
            "content": "Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, Joseph E. Gonzalez. The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to In Forty-second International Conference on Agentic Evaluation of Large Language Models. Machine Learning, 2025. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, Karthik Narasimhan. τ 2-Bench: Evaluating Conversational Agents in Dual-Control Environment. arXiv preprint arXiv:2506.07982, 2025. https://arxiv.org/abs/2506.07982 Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng arXiv preprint Wang, Wu Liu. ACEBench: Who Wins the Match Point in Tool Usage? arXiv:2501.12851, 2025. https://arxiv.org/abs/2501.12851 Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Instruction-Following Evaluation for Large Language Models. arXiv preprint Zhou, Le Hou. arXiv:2311.07911, 2023. https://arxiv.org/abs/2311.07911 Antoine Dussolle, Andrea Cardena Dıaz, Shota Sato, Peter Devine. M-IFEval: Multilingual Instruction-Following Evaluation. arXiv preprint arXiv:2502.04688, 2025. https://arxiv. org/abs/2502. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, Jian Guo. Survey on LLM-as-a-Judge. arXiv preprint arXiv:2411.15594, 2025. https: //arxiv.org/abs/2411.15594 OpenAI. GPT-5. Large language model, 2025. https://openai.com/index/ introducing-gptAnthropic. Claude 4.1 Opus. Large language model, 2025. https://www.anthropic.com/ news/claude-opus-4-1"
        },
        {
            "title": "4 DETAILED RESULTS",
            "content": "Instruction GigaChat 2 GigaChat 2 Pro GigaChat 2 Max Cyrillic Greek Highlighted Sections Count Json Format Keyword Frequency Keywords Presence Letter Frequency All Capital Words Commas Placeholder Count Python List Format Quotation Sentence Count Spaces In Between Title Format Word Count 22.00% 38.00% 0.00% 28.00% 54.00% 12.00% 30.00% 18.00% 6.00% 10.00% 26.00% 28.00% 2.00% 62.00% 0.00% 10.00% 66.00% 0.00% 60.00% 66.00% 24.00% 44.00% 40.00% 40.00% 2.00% 0.00% 36.00% 4.00% 64.00% 4.00% 50.00% 72.00% 0.00% 64.00% 84.00% 42.00% 46.00% 28.00% 58.00% 24.00% 36.00% 58.00% 8.00% 42.00% 18.00% Table 2: Evaluation results for GigaChat models. Instruction Haiku 3 Haiku 3.5 Sonnet 3.7 Opus 4.1 Opus 4.1 Thinking Cyrillic Greek Highlighted Sections Count Json Format Keyword Frequency Keywords Presence Letter Frequency All Capital Words Commas Placeholder Count Python List Format Quotation Sentence Count Spaces In Between Title Format Word Count 30.00% 48.00% 62.00% 36.00% 50.00% 22.00% 28.00% 14.00% 2.00% 94.00% 62.00% 26.00% 10.00% 60.00% 0.00% 40.00% 64.00% 30.00% 76.00% 46.00% 28.00% 78.00% 12.00% 50.00% 92.00% 34.00% 56.00% 54.00 78.00% 54.00% 44.00% 86.00% 34.00% 88.00% 86.00% 38.00% 76.00% 44.00% 76.00% 68.00% 66.00% 72.00% 62.00% 90.00% 64.00% 40.00% 94.00% 68.00% 90.00% 80.00% 28.00% 20.00% 52.00% 80.00% 90.00% 70.00% 84.00% 8.00% 100.00% 94.00% 34.00% 100.00% 68.00% 86.00% 90.00% 54.00% 14.00% 78.00% 92.00% 90.00% 62.00% 86.00% 24.00% 100.00% 96.00% Table 3: Evaluation results for Anthropic models. Instruction GPT 4o GPT 4.1 GPT 5 minimal GPT o4 mini low Cyrillic Greek Highlighted Sections Count Json Format Keyword Frequency Keywords Presence Letter Frequency All Capital Words Commas Placeholder Count Python List Format Quotation Sentence Count Spaces In Between Title Format Word Count 36.00% 24.00% 88.00% 58.00% 14.00% 40.00% 94.00% 80.00% 90.00% 74.00% 22.00% 28.00% 84.00% 64.00% 42.00% 28.00% 58.00% 12.00% 94.00% 72.00% 46.00% 18.00% 60.00% 48.00% 72.00% 76.00% 76.00% 52.00% 72.00% 90.00% 46.00% 86.00% 58.00% 98.00% 94.00% 36.00% 76.00% 56.00% 84.00% 98.00% 88.00% 82.00% 88.00% 94.00% 82.00% 70.00% 98.00% 0.00% 92.00% 98.00% 86.00% 90.00% 82.00% 94.00% 84.00% 56.00% 78.00% 98.00% 94.00% 78.00% Table 4: Evaluation results for OpenAI models."
        }
    ],
    "affiliations": [
        "Higher School of Economics, Moscow",
        "SberDevices, Moscow"
    ]
}