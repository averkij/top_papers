{
    "paper_title": "OpenBEATs: A Fully Open-Source General-Purpose Audio Encoder",
    "authors": [
        "Shikhar Bharadwaj",
        "Samuele Cornell",
        "Kwanghee Choi",
        "Satoru Fukayama",
        "Hye-jin Shim",
        "Soham Deshmukh",
        "Shinji Watanabe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Masked token prediction has emerged as a powerful pre-training objective across language, vision, and speech, offering the potential to unify these diverse modalities through a single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being the only notable example. BEATs has seen limited modifications due to the absence of open-source pre-training code. Furthermore, BEATs was trained only on AudioSet, restricting its broader downstream applicability. To address these gaps, we present OpenBEATs, an open-source framework that extends BEATs via multi-domain audio pre-training. We conduct comprehensive evaluations across six types of tasks, twenty five datasets, and three audio domains, including audio reasoning tasks such as audio question answering, entailment, and captioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics datasets, two environmental sound datasets and five reasoning datasets, performing better than models exceeding a billion parameters at one-fourth their parameter size. These results demonstrate the effectiveness of multi-domain datasets and masked token prediction task to learn general-purpose audio representations. To promote further research and reproducibility, we release all pre-training and evaluation code, pretrained and fine-tuned checkpoints, and training logs at https://shikhar-s.github.io/OpenBEATs"
        },
        {
            "title": "Start",
            "content": "OpenBEATs: Fully Open-Source General-Purpose Audio Encoder Shikhar Bharadwaj1, Samuele Cornell1, Kwanghee Choi1, Satoru Fukayama2, Hye-jin Shim1, Soham Deshmukh1, Shinji Watanabe1 1Carnegie Mellon University, USA 2National Institute of Advanced Industrial Science and Technology (AIST), Japan sbharad2@andrew.cmu.edu 5 2 0 2 8 ] . [ 1 9 2 1 4 1 . 7 0 5 2 : r AbstractMasked token prediction has emerged as powerful pretraining objective across language, vision, and speech, offering the potential to unify these diverse modalities through single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being the only notable example. BEATs has seen limited modifications due to the absence of open-source pre-training code. Furthermore, BEATs was trained only on AudioSet, restricting its broader downstream applicability. To address these gaps, we present OpenBEATs, an open-source framework that extends BEATs via multidomain audio pre-training. We conduct comprehensive evaluations across six types of tasks, twenty five datasets, and three audio domains, including audio reasoning tasks such as audio question answering, entailment, and captioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics datasets, two environmental sound datasets and five reasoning datasets, performing better than models exceeding billion parameters at one-fourth their parameter size. These results demonstrate the effectiveness of multi-domain datasets and masked token prediction task to learn general-purpose audio representations. To promote further research and reproducibility, we release all pre-training and evaluation code, pretrained and fine-tuned checkpoints, and training logs 1. 1. INTRODUCTION Self-supervised learning (SSL) has shown significant promise across wide range of audio processing tasks. It allows models to learn generalpurpose representations that transfer effectively to various downstream applications. Notable examples of SSL-based audio encoders (AEs) include BEATs [1], SS-AST [2], MAE-AST [3], and Audio-MAE [4]. Among these, BEATs stands out due to the wide adoption of its pretrained checkpoints, strong performance across DCASE challenges [5] [7] and the potential to unify vision, language and audio encoders through masked token prediction based learning. However, the pretraining pipeline for BEATs remains closed-source, limiting such broader research impact. In addition, BEATs uses multistage pretraining setup, combining teacher-student distillation and masked audio modeling, making the pre-training more complex than other AEs. In the speech community, open-sourcing models and their training pipelines has been crucial practice [8][11] that leads to enhanced reproducibility and accessibility. Motivated by these gaps, this work aims to completely open-source the BEATs pre-training using the ESPnet toolkit [12], [13] to foster further advancements. Despite advances in SSL-based audio modeling, the training and evaluation of audio encoders remain fragmented across distinct domains environmental sound, bioacoustics and music. As result, state-of-the-art (SOTA) performance is typically achieved by domainspecific models. For instance, BEATs excels in diverse environmental sound benchmarks, GPM-BT [14] leads in bioacoustics tasks, and MERT [15] sets the bar for music-related tasks. In contrast, the speech community has demonstrated that multitask and multilingual training [16][18] produces more generalizable and transferable representations. Similar trends are evident in Audio-Language Models 1https://shikhar-s.github.io/OpenBEATs (ALMs), which unify diverse audio tasks under shared sequence-tosequence framework. The performance of ALMs [19][21] is often strongly correlated with the quality of the underlying audio encoder [22], underscoring the need for robust, general-purpose encoders. Yet, the current literature lacks both universal audio encoder and standardized cross-domain benchmark for evaluating generalization. To address these limitations, we present unified encoder trained on diverse audio domains. Our results verify that multi-domain pre-training improves cross-domain generalization, achieving SOTA performance on multiple datasets. Evaluation of audio encoders has largely been limited to classification tasks on narrow set of general-purpose datasets, such as AudioSet [23] and ESC-50 [24]. Broader evaluations particularly in specialized domains like bioacoustics remain underexplored or are deferred to downstream users. However, with the advent of ALMs, audio encoders are increasingly being used for more complex semantic tasks, such as audio captioning, audio question answering, and audio reasoning, spanning diverse domains. In this setup, the audio encoder is expected to contain both audio information and necessary semantic information to steer the language model. This shift highlights the need for comprehensive evaluation of audio representations not only on traditional classification tasks, but also on open-ended and semantically rich tasks across bioacoustics, environmental sound, and music domains. To support this, we benchmark multiple models and offer streamlined interface, via single script, for testing new encoders using ESPnet. In this work, we address the aforementioned challenges, by releasing an open-source and reproducible training pipeline for BEATs, scaling it across diverse domains, and proposing unified benchmark for comprehensive ALM-focused evaluation. The main contributions, also depicted in Fig. 1, are: We open-source the training and evaluation framework for BEATs, SOTA SSL audio encoder. We scale the BEATs model to achieve general-purpose audio representations that can be used for variety of downstream tasks. Specifically, we increase the model size to 300M parameters and expand the pre-training data to 20k hours by adding environmental sound datasets and incorporating bioacoustics and music domains. Our model achieves strong cross-domain transfer and is comparable to models with over billion parameters [25]. We introduce an open-source comprehensive evaluation suite that spans multiple domains, including bioacoustics, music and environmental sound. Our benchmark is designed to streamline the evaluation of audio encoders and include tasks such as audio entailment, question answering, and captioning. Fig. 1: Our contributions in this work are three folds: 1) Open sourcing the pre-training and evaluation code for BEATs. 2) Scaling the training data and model to handle multiple sound domains like music (Y), environmental sound () and bioacoustics ((cid:10)). 3) Fine-grained multi-domain evaluations that go beyond standard tasks, probing reasoning ability via audio entailment and question-answering. 2. BACKGROUND 2.1. BEATs Architecture BEATs (Bidirectional Encoder representation from Audio Transformers) [1] is based on Vision Transformer [26] architecture adapted for audio representation learning. The model processes 16 kHz audio by converting it into 128-dimensional mel spectrograms, which are segmented into 1616 patches and then fed to transformer encoder. This patching mechanism enables BEATs to capture both local and global spectral features efficiently. The backbone of the model consists of stacked transformer encoder blocks, each employing multi-head self-attention to learn contextual representations of audio signals. 2.2. BEATs Pre-training BEATs undergoes two-stage SSL, involving encoder training and tokenizer training. These stages iteratively refine both the feature representations and the discrete tokenization of audio signals, improving the models ability to learn structured embeddings, with the process repeated up to three times. 2.2.1. Encoder Training: The pre-training of BEATs encoder follows Masked Language Modeling (MLM) paradigm applied to audio. In the first iteration, masked spectrogram patches are tokenized using randomly initialized tokenizer [27]. From the second iteration onward, the model employs trained tokenizer, leading to progressively refined learned representations. Given an input sequence of dimensional Mel-spectrogram patches with length , = {xn Rd}N n=1, subset of patches is randomly masked, producing sequence of unmasked patches X. The encoder produces sequence of h-dimensional embeddings e( X) = {on Rh}N n=1, and it is fed through the label predictor module pred is learned to predict the masked tokens by minimizing the cross-entropy loss between the predicted tokens and the ground truth token indices given the unmasked patches: pred. Then, LMLM = (cid:88) nM log p(zn X), (1) where is the set of masked positions and zn represents the ground truth token for position yielded from the tokenizer. Therefore, log p(zn X) stands for the predicted probability distribution of over possible tokens at position n, given the masked input X. At the implementation level, besides tracking the loss in (1), metrics like vocabulary coverage and masked accuracy are also tracked in our open-source implementation. 2.2.2. Tokenizer Training: BEATs adopts teacher-student framework for tokenizer training. The teacher model corresponds to the encoder above from the previous iteration, while the student model is the tokenizer being trained in the current iteration. The tokenizer encodes input into embedding sequence t(X) = n=1, which is quantized into q(E) = {vzn n=1. Here, the token zn is the nearest neighbor index in the = {en Rp}N Rp}N tokenizer codebook = {v1, v2, , vK } of size K: zn = arg min vt en2 2 , (2) where vt and en is the L2-normalized vt and en. Given the quantized embedding sequence q(E), the tokenizer learns to predict embeddings n=1 to align with the teacher embeddings through the knowledge distillation loss LKD: pred(q(E)) = {ˆon Rp}N LKD = (cid:88) cos(ˆon, on) + sg[en] vzn 2 2 + en sg[vzn ]2 2 , (3) where sg[] is the stop gradient operator. Equation (3) has three terms: cosine similarity loss encouraging alignment between the teacher and tokenizer, and two L2 loss terms improving stability and consistency of codebook representations. At the implementation level, the gradients of the quantized embedding sequence q(E) are copied to the embedding sequence to solve the non-differentiable nature of vector quantization [28], and the codebook is initialized using K-Means and optimized through exponential moving average. 3. OpenBEATs In this work, we extend the pre-training data beyond AudioSet [23] by incorporating additional large-scale public datasets such as FreeSound, FMA [29], and iNaturalist [30], which span diverse domains including music, environmental sound, and bioacoustics. In prior literature [14], [31] bioacoustics datasets have been used in isolation and not combined with music. However, to the best our knowledge, this work is the first to systematically unify these diverse domains and study the cross-domain impact from such multi-domain training via extensive evaluations. In addition, our model is trained with variable-length input sequences, making it more adaptable to the natural variability of real-wold audio data. Together, these changes enable our encoder to learn robust, general purpose representations for wide range of tasks and domains. detailed list of our pre-training datasets is provided in Table 2. We exclude speech corpora, as speech requires fine-grained temporal resolution to model phoneme durations, which differs from the coarser time resolution of 175 ms employed in BEATs patch. Table 1: Architecture, data, and training configuration for BEATs and OpenBEATs variants. Model Data (hr) Param Hidden size FFN size Layers # Heads # Codebook Batch (sec) Updates Warmup BEATs OpenBEATs -Base OpenBEATs -Large 5.8k 20k 20k 90M 90M 300M 768 768 1024 3072 3072 4096 12 12 24 12 12 16 1024 1024 1024 4.6k 10.7k 10.7k 400k 400k 400k 32k 40k 40k LR 5e-4 5e-4 1e-4 Opt. AdamW AdamW AdamW We first train 90M parameter OpenBEATs model (base) on this expanded dataset and observe saturation in downstream task performance. To address this, we scale up the model architecture and train 300M parameter variant (large), following the base/large design pattern established by HuBERT [11]. Table 1 lists the details of our architecture and pre-training hyper-parameters. To support reproducibility and future research, we open-source the full training pipeline, including the pre-training code, data preprocessing scripts, and training logs for all our models. Table 2: Overview of pre-training datasets covering music (Y), bioacoustics ((cid:10)) and environmental sound events (). *We use data filtered using the WavCaps [32] processing pipeline for these sources."
        },
        {
            "title": "Instances Domain",
            "content": "Table 4: Linear probing results on sound tasks from X-ARES benchmark [47]. and denotes Base and Large models. OpenBEATs achieves best performance on DESED and UrbanSound-8K datasets, and performs competitively with other models of similar scale across the remaining datasets."
        },
        {
            "title": "Model",
            "content": "Param DSD US8k F50k ESC FKgl Clotho Avg Dasheng [25] 90M 0.53 0.84 0.41 0.87 0.56 0.03 0.54 90M 0.14 0.44 0.08 0.25 0.20 0.01 0.19 Data2Vec [48] 74M 0.13 0.72 0.26 0.61 0.48 0.03 0.37 Whisper [49] EAT [50] 88M 0.36 0.79 0.36 0.66 0.38 0.04 0.43 90M 0.56 0.85 0.22 0.84 0.55 0.04 0.51 BEATs (iter3) [1] EAT [50] 309M 0.44 0.82 0.44 0.73 0.58 0.06 0.51 Dasheng-0.6B [25] 600M 0.54 0.85 0.45 0.88 0.58 0.04 0.56 Dasheng-1.2B [25] 1.2B 0.56 0.85 0.46 0.89 0.63 0.04 0."
        },
        {
            "title": "OpenBEATs L",
            "content": "300M 0.57 0.87 0.43 0.86 0.54 0.05 0.55 FMA [29] AudioSet [23] FreeSound* BBC Sound Effects* iNat Sounds [30] Others [33], [34]"
        },
        {
            "title": "Total",
            "content": "7.7k 5.6k 4.1k 1.0k 0.8k 0.3k 20k 2.8M 2.0M 1.7M 0.4M 0.3M 0.1M 7.3M (cid:10) Table 3: Evaluation tasks. Recipes for all of these tasks are also open-sourced as part of this work. LP = Linear Probing; FT = Full Fine-tuning; DFT = Decoder Fine-Tuning; CLS = Classification; SED = Sound Event Detection. We follow prior works for evaluation protocols where available (eg. BEANS). Higher values are better for all metrics. See details in Section 4.2. Dataset Abbr. Task Type Eval Metric Reference Environmental Sound DESED UrbanSound8K FSD-50K ESC-50 FSD2018-Kaggle Clotho (Retrieval) AudioSet-2M AudioSet-20K Bioacoustics DSD US8K F50K ESC FKgl Clotho AS-2M AS-20K SED CLS SED CLS SED Retrieval SED SED LP LP LP/FT LP/FT LP LP FT FT Score Score Score/mAP Score/Acc Score Score mAP mAP [35] [36] [37] [24] [38] [39] [23] [23] BEANS (10 tasks) BEANS CLS/SED FT Acc/mAP [40] Reasoning Audio Entailment AudioQA Entailment AQA Clotho (Captioning) Clotho AAC Entailment Question Answering Captioning FT FT DFT Acc Acc CIDEr [43] [41] [42] [39] Music GTZAN NSynth-Instr. NSynth-Pitch GTZAN NSynth-I NSynth-P CLS CLS CLS FT FT FT Acc Acc Acc [44], [45] [46] [46] 4.1. Baselines 4. EVALUATION SETUP Dasheng: Dasheng [25] surpasses the billion-parameter mark and is pre-trained on over 272k hours of data, covering multi-domain and speech-rich sources from YouTube. In contrast, our setup offers more controlled comparison by deliberately restricting to audioonly datasets. Dasheng adopts mean squared error loss, whereas OpenBEATs relies on masked modeling with discrete tokens. Speech models: To gauge the transferability of speech-only pretraining, we compare with Data2Vec [48] and Whisper-small [49]. Table 5: OpenBEATs achieves best performance on environmental sound detection tasks. denotes values reported by BEATs, all other values are from our open-source implementation. Please note that our copy of AudioSet (AS) downloaded from YouTube differs from BEATs. See Section 5 for details. AS-20K mAP FSD-50K mAP ESC-50 acc AS-2M mAP Param Model Audio-MAE [4] Audio-MAE BEATs iter 1 BEATs iter 2 BEATs iter 3 OpenBEATs iter 1 OpenBEATs iter 2 OpenBEATs iter 3 OpenBEATs iter 1 OpenBEATs iter 2 OpenBEATs iter 3 86M 304M 90M 90M 90M 90M 90M 90M 300M 300M 300M 94.1 - 93.0 95.1 95.6 / 94.8 93.9 94.8 95.0 95.3 95.7 95. - - 52.3 55.4 56.2 54.4 55.8 55.5 56.7 57.5 56.8 47.3 47.4 37.1 37.6 47.9 48.1 48.0 / 41.6 36.0 36.0 38.3 / 34.1 39.4 41.1 40.8 41.5 42.2 42.1 31.5 31.5 32.1 32.7 32.4 33. Bioacoustics models: We evaluate domain-specific pre-training via dedicated bioacoustics sound encoders like AVES [52], BioLingual [31], to test the claim that multi-domain SSL can outperform models trained on specific datasets. We also report GPM-BT [14] which has shown promising results on bioacoustics. While GPM-BT and AVES are SSL models, BioLingual is trained with text supervision following CLAP-like training methodology. AudioMAE: AudioMAE employs pure mask-and-reconstruct objective. This is an alternate SSL methodology to BEATs as it does not use any tokenization. Numbers are reported on standard audio classification tasks like ESC-50 and AudioSet. Efficient Audio Transformer (EAT): EAT [50] is another recently proposed SOTA audio encoder trained on AudioSet [23]. 4.2. Evaluation Tasks Table 3 lists all evaluation tasks. Linear probing measures the effectiveness of pretrained representations without any fine-tuning induced variations [53]. We benchmark linear probes on sound tasks using the X-ARES toolkit 2. This toolkit converts all individual metrics to be in the range of 0-1, with higher values being better. The sound reasoning tasks evaluate the utility of audio representations even before integrating with frozen language models to build ALMs. We consider two setups: (1) multi-modally trained text encoder, like CLAP [51], and (2) an independently trained text encoder, like BERT. 2https://github.com/jimbozhang/xares/tree/main Table 6: Bioacoustics: Full-finetuning evaluations on BEANS [40]. OpenBEATs achieves SOTA results performing competitively with models pre-trained with audio-text supervision [31], [51], in-domain SSL [14], [52], and SSL models with more parameters [25]. Model Pre-training Data Instances #Params Sound Event Classification (acc) Sound Event Detection (mAP) Watkins CBI HumBugDB Dogs Bats DCASE 21 Rfcx Gibbons Hiceas Enabirds Audio-text CL LAION-CLAP [51] BioLingual [31] Bioacoustics SSL AVES [52] GPM-BT [14] Dasheng [25] BEATs iter 1 iter 2 iter OpenBEATs Base iter 1 iter 2 iter 3 OpenBEATs Large iter 1 iter 2 iter 3 633k pairs 1.1M pairs 1.8M 1.2M 97M 31M 31M 89M 89M 90M 600M 1.2B 2M 90M 7.4M 90M 7.4M 300M 89.1 89.4 87.9 91.4 77.3 79.7 79.1 87.0 87.9 89. 85.0 86.4 87.0 88.8 88.8 88.2 62.2 74.4 59.8 54.3 63.1 67.8 66.7 61.9 64.4 64. 63.6 64.9 64.5 67.8 68.8 69.4 82.0 81.7 81.0 81.1 69.9 73.6 72.2 80.1 87.3 80. 84.2 87.3 84.8 86.6 87.7 87.4 96.4 97.1 95.0 94.2 95.7 95.7 97.1 97.1 92.1 90. 95.0 96.4 95.7 95.0 95.7 95.7 75.3 76.6 74.8 77.4 75.5 77.3 76.5 78.8 79.3 76. 78.4 77.3 76.9 79.4 79.7 79.5 47.0 47.5 39.2 45.4 45.6 45.5 46.4 45.0 45.2 43. 44.5 46.2 46.4 49.5 47.6 46.2 14.5 17.8 13.0 12.9 11.7 13.6 13.2 8.6 10.6 10. 10.1 13.2 12.4 13.0 13.0 14.9 29.6 37.6 28.4 34.5 41.8 47.3 36.8 45.8 44.0 44. 45.6 47.3 42.3 45.3 50.2 49.3 62.7 67.7 62.9 65.0 43.8 58.8 57.4 71.3 69.5 67. 68.2 68.2 68.1 69.1 68.3 66.0 66.4 68.8 55.5 62.4 43.9 45.6 45.0 48.8 51.1 49. 46.2 49.0 49.0 52.1 53.4 54.4 Table 7: Audio reasoning tasks. OpenBEATs outperforms other encoders on entailment and AQA and performs competitively with BEATs on captioning. AQA (BERT / CLAP)"
        },
        {
            "title": "Model",
            "content": "Metrics"
        },
        {
            "title": "Open",
            "content": "acc BEATs Dasheng-1.2B OpenBEATs Base OpenBEATs Large 10.5 / 9.7 10.4 / 10.1 12.2 / 11.1 12.7 / 11.5 Yes-No"
        },
        {
            "title": "CLAP",
            "content": "acc 56.7 / 57.5 58.5 / 56.7 59.4 / 59.4 59.9 / 58.7 acc 73.0 74.5 73.2 73.2 acc 71.5 73.0 75.2 74."
        },
        {
            "title": "Clotho\nAAC",
            "content": "CIDEr 39.0 47.7 35.3 37.5 Table 8: OpenBEATs improves performance on two of the three music tasks."
        },
        {
            "title": "GTZAN Genre",
            "content": "NSynth-I NSynth-P BEATs Dasheng Base OpenBEATs Base OpenBEATs Large 87.1 87.1 88.1 89.1 79.9 80.9 79.5 81.7 93.1 90.9 92.5 92. By comparing these approaches, we can understand how an audio encoder will behave with audio-informed text embeddings. Therefore, our benchmark bridges the gap between low-level feature extraction and high-level cognitive reasoning, offering structured framework to improve audio encoders for ALMs. For audio captioning, our setup consists of the frozen audio encoder and trainable transformer decoder, initialized randomly [54]. 5. RESULTS AND DISCUSSION Linear probing on X-ARES: As summarized in Table 4, OpenBEATs Large achieves the best linear-probe accuracy on DESED and US8k, and remains within few points of the 1.2 billion parameter Dasheng on FSD50K and ESC-50. Notably, it does so with four times fewer parameters than Dasheng-1.2B and considerably smaller pretraining data budget. These results show that token prediction based modeling approach scales more efficiently, delivering competitive performance with less data. Environmental Sound: OpenBEATs consistently delivers strong performance on the canonical ESC-50 and FSD-50K benchmarks in Table 5. The Large model attains 95.8 % accuracy on ESC-50 and 57.5 mAP on FSD-50K, confirming that parameter scaling pays off after multi-domain data saturates the 90 scale. Performance gaps in AudioSet originate primarily from differences in the evaluation dataset, since YouTube is not static source and our evaluation set differs by 5% from the one reported in BEATs. We verified the correctness of our training and inference code by replicating the numbers using the public BEATs inference codebase. We release both codebases and the YouTube IDs for our copy of the AudioSet publicly. The improved performance from OpenBEATs Large on AS-2M shows the benefits of multi-domain pre-training. These results demonstrate that the pre-training technique scales gracefully. Bioacoustics: Table 6 shows that the OpenBEATs attains SOTA results on 6 of the 10 BEANS datasets among all SSL models. Even against billion-parameter Dasheng variants, OpenBEATs Large remains better confirming that performance scales favourably with masked token prediction based modeling applied to multi-domain datasets. Crucially, OpenBEATs also outperforms the text-supervised audiotext contrastive baselines like LAION-CLAP and BioLingual, demonstrating that scaled-up SSL can surpass models that have access to paired textual information. These findings show that multi-domain pre-training yields general-purpose representations. Sound Reasoning and Music Tasks: OpenBEATs achieves best accuracies on both AQA and entailment tasks, underscoring the benefits of its larger capacity and broader pre-training corpus  (Table 7)  . Consistently, we observe that scaling the model size and expanding pre-training data yields systematic gains for both Dasheng and OpenBEATs, confirming that representation quality improves with greater model and data scale. Nevertheless, OpenBEATs lags behind on audio-captioning. One possible reason is that we applied the recipe tuned for BEATs by [54] to other models without sufficient hyperparameter exploration. OpenBEATs also improves performance on music tasks  (Table 8)  , showing that multiple domains in pre-training data produce general purpose representations. 6. CONCLUSION In this work, we open-source the complete pre-training pipeline for BEATs and demonstrate its effectiveness through strong performance across diverse benchmarks. Our comprehensive evaluation spans multiple domains and tasks, showcasing the robustness and versatility of the learned representations. We find that multi-domain pre-training enables the model to generalize effectively, achieving SOTA results on various datasets. These findings establish token-prediction based masked modeling on multi-domain datasets as powerful framework for general audio representation learning. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system at PSC and Delta and DeltaAI system at NCSA through allocations CIS210014 and IRI120008P from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, supported by National Science Foundation grants #2138259,#:2138286, #:2138307, #:2137603, and #:2138296."
        },
        {
            "title": "REFERENCES",
            "content": "[1] S. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, W. Che, X. Yu, and F. Wei, BEATs: audio pre-training with acoustic tokenizers, in ICML, 2023. [2] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. Glass, SSAST: Self-supervised audio spectrogram transformer, in AAAI, 2022. [3] A. Baade, P. Peng, and D. Harwath, MAE-AST: Masked autoencoding audio spectrogram transformer, Interspeech, 2022. [4] P.-Y. Huang, H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, and C. Feichtenhofer, Masked autoencoders that listen, NeurIPS, 2022. [5] S. Cornell, J. Ebbers, C. Douwes, I. Martın-Morato, M. Harju, A. Mesaros, and R. Serizel, DCASE 2024 task 4: Sound event detection with heterogeneous data and missing labels, DCASE Workshop, 2024. [6] S.-L. Wu, X. Chang, G. Wichern, J.-w. Jung, F. Germain, J. L. Roux, and S. Watanabe, BEATs-based audio captioning model with instructor embedding supervision and chatgpt mix-up, DCASE2023 Challenge, Tech. Rep., 2023. [7] F. Gelderblom, B. Cretois, P. Johnsen, F. Remonato, and T. A. Reinen, Few-shot bioacoustic event detection using beats, DCASE2023 Challenge, Tech. Rep., 2023. [8] R. Whetten, T. Parcollet, M. Dinarelli, and Y. Est`eve, Open implementation and study of best-rq for speech processing, ICASSP workshop on Self-supervision in Audio, Speech and Beyond, 2024. [9] Peng et al, Reproducing whisper-style training using an open-source toolkit and publicly available data, in ASRU, 2023. [10] S. Schneider, A. Baevski, R. Collobert, and M. Auli, wav2vec: Unsupervised pre-training for speech recognition, arXiv preprint arXiv:1904.05862, 2019. [11] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, HuBERT: Self-supervised speech representation learning by masked prediction of hidden units, IEEE/ACM transactions on audio, speech, and language processing, vol. 29, pp. 34513460, 2021. [12] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, ESPnet: End-to-end speech processing toolkit, in Interspeech, 2018. [13] Shi et al, VERSA: versatile evaluation toolkit for speech, audio, and music, arXiv preprint arXiv:2412.17667, 2024. [14] X. Deng, T. Wan, K. Xu, T. Gao, P. Qiao, D. Feng, and Y. Dou, Scaling bioacoustic signal pre-training with million samples via mask-modeling, in ICASSP, 2025. [15] Y. e. a. Li, Mert: Acoustic music understanding model with large-scale self-supervised training, in ICLR, 2024. [16] S. e. a. Chen, Wavlm: Large-scale self-supervised pre-training for full stack speech processing, IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 15051518, 2022. [17] Y. Zhang et al., Google USM: Scaling automatic speech recognition beyond 100 languages, arXiv preprint arXiv:2303.01037, 2023. [18] W. Chen, W. Zhang, Y. Peng, X. Li, J. Tian, J. Shi, X. Chang, S. Maiti, K. Livescu, and S. Watanabe, Towards robust speech representation learning for thousands of languages, in EMNLP, 2024. [19] S. Deshmukh, B. Elizalde, R. Singh, and H. Wang, Pengi: An audio language model for audio tasks, NeurIPS, 2023. [20] C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, M. Zejun, and C. Zhang, SALMONN: Towards generic hearing abilities for large language models, in ICLR, 2023. [21] S. Ghosh, S. Kumar, A. Seth, C. K. R. Evuru, U. Tyagi, S. Sakshi, O. Nieto, R. Duraiswami, and D. Manocha, GAMA: large audiolanguage model with advanced audio understanding and complex reasoning abilities, EMNLP, 2024. [22] S. Deshmukh, S. Dixit, R. Singh, and B. Raj, Mellow: small audio language model for reasoning, arXiv preprint arXiv:2503.08540, 2025. [23] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, Audio set: An ontology and human-labeled dataset for audio events, in ICASSP, 2017. [24] K. J. Piczak, ESC: Dataset for environmental sound classification, in Proceedings of ACM international conference on Multimedia, 2015. [25] H. Dinkel, Z. Yan, Y. Wang, J. Zhang, Y. Wang, and B. Wang, Scaling up masked audio encoder learning for general audio classification, Interspeech, 2024. [26] D. Alexey, An image is worth 16x16 words: Transformers for image recognition at scale, ICLR, 2020. [27] C.-C. Chiu, J. Qin, Y. Zhang, J. Yu, and Y. Wu, Self-supervised learning with random-projection quantizer for speech recognition, in ICML, 2022. [28] A. van den Oord, O. Vinyals, and k. kavukcuoglu, Neural discrete representation learning, in Advances in Neural Information Processing Systems, vol. 30, 2017. [29] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, FMA: dataset for music analysis, in International Society for Music Information Retrieval Conference, 2017. [30] V. H. et al., The INaturalist species classification and detection dataset, in CVPR, 2018. [31] D. Robinson, A. Robinson, and L. Akrapongpisak, Transferable models for bioacoustics with human language supervision, in ICASSP, 2024. [32] Mei et al, Wavcaps: chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. [33] I.-Y. Jeong and J. Park, CochlScene: Acquisition of acoustic scene data using crowdsourcing, in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 2022. [34] D. Damen, H. Doughty et al., The epic-kitchens dataset: Collection, challenges and baselines, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 41254141, 2020. [35] N. Turpault, R. Serizel, A. P. Shah, and J. Salamon, Sound event detection in domestic environments with weakly labeled data and soundscape synthesis, in DCASE Workshop, 2019. [36] J. Salamon, C. Jacoby, and J. P. Bello, dataset and taxonomy for urban sound research, in Proceedings of ACM international conference on Multimedia, 2014. [37] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, FSD50K: An open dataset of human-labeled sound events, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 829852, 2022. [38] E. Fonseca, M. Plakal, F. Font, D. P. Ellis, X. Favory, J. Pons, and X. Serra, General-purpose tagging of freesound audio with audioset labels: Task description, dataset, and baseline, in DCASE Workshop, 2023. [39] K. Drossos, S. Lipping, and T. Virtanen, Clotho: An audio captioning dataset, in ICASSP, 2020. [40] M. Hagiwara, B. Hoffman, J.-Y. Liu, M. Cusimano, F. Effenberger, and K. Zacarian, BEANS: The benchmark of animal sounds, in ICASSP, 2023. [41] S. Deshmukh, S. Han, H. Bukhari, B. Elizalde, H. Gamper, R. Singh, and B. Raj, Audio entailment: Assessing deductive reasoning for audio understanding, in AAAI, 2025. [42] S. Lipping, P. Sudarsanam, K. Drossos, and T. Virtanen, Clotho-aqa: crowdsourced dataset for audio question answering, in EUSIPCO, 2022. [43] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, CIDEr: Consensusbased image description evaluation, in CVPR, 2015, pp. 45664575. [44] J. Mehta, D. Gandhi, G. Thakur, and P. Kanani, Music genre classification using transfer learning on log-based mel spectrogram, in 2021 5th International Conference on Computing Methodologies and Communication (ICCMC), 2021. [45] B. L. Sturm, The GTZAN dataset: Its contents, its faults, their effects on evaluation, and its future use, arXiv preprint arXiv:1306.1461, 2013. [46] J. Engel, C. Resnick, A. Roberts, S. Dieleman, M. Norouzi, D. Eck, and K. Simonyan, Neural audio synthesis of musical notes with wavenet autoencoders, in ICML, 2017. [47] J. Zhang, H. Dinkel, Q. Song, H. Wang, Y. Niu, S. Cheng, X. Xin, K. Li, W. Wang, Y. Wang et al., The ICME 2025 audio encoder capability challenge, arXiv preprint arXiv:2501.15302, 2025. [48] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, Data2vec: general framework for self-supervised learning in speech, vision and language, in ICML, 2022. [49] A. Radford, J. W. Kim, and et al, Robust speech recognition via largescale weak supervision, in ICML, 2023. [50] W. Chen, Y. Liang, Z. Ma, Z. Zheng, and X. Chen, EAT: self-supervised pre-training with efficient audio transformer, in Proceedings of the International Joint Conference on Artificial Intelligence, 2024. [51] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation, in ICASSP, 2023. [52] M. Hagiwara, AVES: Animal vocalization encoder based on selfsupervision, in ICASSP, 2023. [53] J. Shor, A. Jansen, O. Lang, O. Tuval, F. de Chaumont Quitry, M. Tagliasacchi, and D. Emanuel, Towards learning universal nonsemantic representation of speech, in Proc. Interspeech, no. 2020, 2020. [54] J.-w. Jung, D. Zhang, H. C.-H. Yang, S.-L. Wu, D. M. Chan, Z. Kong, D. Ruifan, Z. Yaqian, V. Rafael, and S. Watanabe, Automatic audio captioning with encoder fusion, multi-layer aggregation, and large language model enriched summarization, DCASE2024 Challenge, Tech. Rep., 2024."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University, USA",
        "National Institute of Advanced Industrial Science and Technology (AIST), Japan"
    ]
}