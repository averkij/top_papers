{
    "paper_title": "HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems",
    "authors": [
        "Jun Xing",
        "Mayur Bhatia",
        "Sahil Phulwani",
        "Darshan Suresh",
        "Rafik Matta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 6 2 2 0 0 . 2 0 5 2 : r HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on Cross-Domain Multi-File Project Problems Jun Xing, Mayur Bhatia, Sahil Phulwani, Darshan Suresh, Rafik Matta Abstract Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking rigorous evaluation for consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations (v1) on 65 problems show that the top three modelso1, o1-preview, and Claude-3.5-Sonnet-1022achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with remarkably low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large language models (LLMs) has significantly impacted software development, enabling capabilities such as code generation and bug fixing. However, evaluating these models real-world effectiveness is challenging. Many of todays most popular and widely used evaluation benchmarks are heavily focused on single language or single-file, well-defined tasks. For instance, HumanEval focuses on standalone coding tasks, evaluating models for generating single-function solutions without accounting for multi-file dependencies or broader project contexts[1]. SWE-bench introduces GitHub-based evaluations for resolving real-world issues but focuses on 12 specific Python libraries[2] and 17 JavaScript Repositories [3]. DevEval broadens the domain scope by introducing multi-file coding tasks that simulate the software development lifecycle, including software design and testing [4]. However, despite its breadth, DevEval does not explicitly evaluate model consistency across multiple runs, leaving critical gap in understanding LLM reliability for real-world applications. Moreover, study by [5] introduces the concept of self-consistency in Code LLMs, emphasizing that trustworthy model should generate consistent natural language specifications for the code it generates and vice versa. Their evaluation of eleven code LLMs reveals frequent failures in maintaining self-consistency, highlighting gap between traditional accuracy metrics and the models true understanding of the shared semantics 1 between natural and programming languages. This underscores the need for more robust evaluation frameworks that go beyond conventional metrics to assess the reliability and predictability of LLM-generated code. To address these limitations, the HackerRank-ASTRA (Assessment of Software Tasks in Real-world Applications) Benchmark offers comprehensive evaluation framework for multi-file, project-based software development problems. ASTRAs initial release (v1) focuses on frontend development, featuring frameworks such as Node.js, React.js, Angular.js, Django, Java Spring Boot, Ruby on Rails, and .NET. The benchmark evaluates new feature development, where both inputs and outputs are text-based. Metrics such as mean pass@1 and mean score assess model correctness, while median standard deviation across 32 runs (k = 32) provides insights into consistency and reliability. By simulating practical coding challenges, HackerRank-ASTRA aims to provide actionable insights into the capabilities and limitations of state-of-the-art LLMs in addressing modern software engineering needs."
        },
        {
            "title": "2 HackerRank-ASTRA",
            "content": "HackerRank-ASTRA is benchmark built from HackerRanks proprietary library of multi-file, project-based software development problems. These problems were originally designed to assess the software development skills of human developers across wide range of skill domains in realistic, project-like settings. We observed that even advanced large language models (LLMs) face significant challenges when solving these problems, which motivated the creation of this benchmark."
        },
        {
            "title": "2.1 Task Formulation",
            "content": "The task requires models to take both the problem statements and relevant source code files as input, with the objective of generating the requested code as output. To assess consistency, the process is repeated multiple times, with each run initialized as new conversation to eliminate prior memory or contextual bias. Model performance is evaluated based on the percentage of test cases passed, ensuring clear and objective scoring system. Since these problems were originally intended for human evaluation, they include diverse and carefully curated set of test cases, with an average of 6.7 test cases per problem in the current version (v1). This content was preapred by HackerRanks Content Creation team."
        },
        {
            "title": "2.2 Features of HackerRank-ASTRA",
            "content": "Diverse Skill Domains: The v1 ASTRA Benchmark dataset comprises 65 projectbased coding problems, primarily focused on front-end development. These problems are categorized into 10 primary coding skill domains and 34 subcategories (Additional technical details are in Section 2.3), ensuring comprehensive evaluation of diverse technical capabilities. Long Context Multi-File Project Questions: To closely replicate real-world software development tasks, the HackerRank-ASTRA Benchmark dataset includes, on average, 12 source code and configuration files per question as model inputs. The problem statements alone have an average context length of 718 characters, while source code 2 inputs average 22,863 characters, and output strings average 2,744 characters. On average, the benchmark requires 84 lines of solution code per problem and modifications to 2.3 files, reflecting the complexity and breadth of real-world project scenarios. Model Correctness and Consistency Evaluation: To evaluate production reliability, the benchmark prioritizes metrics such as mean scores, mean pass@1, and median standard deviation with = 32, rather than relying solely on the traditional industrystandard pass@k. These metrics provide more nuanced assessment of the models correctness and consistency, crucial for real-world applicability. Wide Test Case Coverage: The ASTRA Benchmark dataset includes an average of 6.7 test cases per question, designed to rigorously evaluate the correctness of modelgenerated solutions across variety of implementation scenarios. Real-world Alignment: HackerRanks extensive content library serves as the foundation for the ASTRA Benchmark. With over 7,500 coding questions, an advanced skills taxonomy, and data-driven insights, HackerRank has well-established expertise in evalInformed by data from tens of thousands of job descriptions, uating developer skills. HackerRanks Roles Directory spans 9 job families, 77 roles, and 260 skills. This ensures alignment with real-world industry demands, leveraging machine learning to identify key skills critical for various technical roles."
        },
        {
            "title": "2.3 Skill Domains",
            "content": "The v1 ASTRA Benchmark consists of 65 project-based coding questions, systematically organized into 10 primary skill domains and 34 sub-skill categories. Figure 1: Distribution of v1 HackerRank-ASTRA benchmark main skill frequency. 3 Figure 2: Distribution of v1 HackerRank-ASTRA benchmark sub-skill frequency."
        },
        {
            "title": "2.4 Key Statistics",
            "content": "Table 1: Key Statistics of v1 HackerRank-ASTRA Benchmark Statistic Total Project Questions Count of Main Skill Categories Count of Sub Skill Categories Average Number of Test Cases Average Input Files Average Input Character Length Average Problem Statement Character Length Average Output Character Length Average Expected Lines of Code Average Modified Code Files Value 65 10 34 6.7 12 22,863 718 2,744 84 2."
        },
        {
            "title": "2.5 Evaluation Metrics",
            "content": "Mean Score: The Mean Score ( Passed Test Cases Total Test Cases ) with = 32 evaluates the models partial correctness and robustness by considering multiple attempts (up to = 32) for each problem. For each problem, the score is calculated as the average proportion of passed test cases across the 32 runs. Then, this score is aggregated across 65 problems to compute the final Mean Score. Formally: Mean Score = 1 (cid:88) i=1 1 k (cid:88) j=1 pij Ti , (1) Mean Pass@1: The Mean pass@1 metric evaluates the frequency with which the model achieves perfect score across = 32 runs for each problem. For given problem, pass@1 is defined as the proportion of runs where the model achieves perfect score (all test cases passed). The metric then aggregates this proportion across = 65 problems 4 to compute the final Mean pass@1: Mean Pass@1 = 1 (cid:88) i= 1 (cid:88) j=1 (cid:18) pij Ti (cid:19) = 1 , (2) Consistency: Consistency is defined as the median standard deviation of the scores of the models output across = 65 problems. For each problem, the standard deviation of scores across = 32 solutions is computed. The final metric is the median of these standard deviations across all problems. This approach is used because the standard deviation of scores across different problems often deviates from normal distribution. lower median standard deviation indicates more consistent performance, while higher median standard deviation suggests greater variability. 1. Compute the standard deviation of scores for each problem i: Where SDi = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) j=1 (cid:0)Scoreij Scorei (cid:1)2 Scorei = 1 (cid:88) j=1 Scoreij is the mean score for problem i. 2. Compute the Median SD across = 65 problems: Median SD = median (SD1, SD2, . . . , SDn) (3) (4) (5) These metrics are chosen for their alignment with real-world software development standards, where both complete and partially correct solutions carry significance. The Mean Score accounts for the models incremental problem-solving ability, offering granular view of how much of solutions functionality is achieved even when it is not fully correct. Pass@1 indicates how reliably model can produce correct code in single shot, which is crucial in real-world scenarios where developers working with code LLMs want to apply minimal revisions to the output. The consistency of models solutions for each problem highlights whether the model performs steadily across its multiple attempts or exhibits significant variability. Using k=32 provides meaningful measure of models capability to explore diverse solutions, as this number of attempts allows it to overcome minor variances while maintaining focus on feasible solution space. We use the mean for metrics like mean score and pass@1 because these aggregate metrics aim to capture the overall performance of the model across problems. For standard deviation, however, we use the median because the variability of scores across problems often contains outliers, and the median provides more robust measure of the typical consistency of the models performance."
        },
        {
            "title": "2.6 Sample Problem",
            "content": "The following is an example of RESTful API project from the ASTRA Benchmark Dataset. This task involves developing RESTful API for managing product records 5 using Node.js and Express, simulating real-world e-commerce development scenario. The project structure is depicted in Figure 3. Figure 3: Project structure of sample RESTful API problem. This problem prepares models for practical API development challenges, emphasizing correctness, maintainability, and scalability. Additional technical details are included in the Appendix."
        },
        {
            "title": "3.1 Evaluation Criteria",
            "content": "The evaluation primarily targets code generation correctness and consistency, focusing exclusively on the models ability to generate accurate and functional solutions in response to text-based API call."
        },
        {
            "title": "3.2 Evaluation Pipeline",
            "content": "Figure 4: Diagram of v1 HackerRank-ASTRA benchmark evaluation pipeline. Input Data Preparation: The evaluation process begins by reading CSV file containing the list of questions. For each question, the corresponding project files are retrieved from an S3 bucket, ensuring that input data is complete and consistent. This step eliminates ambiguity and standardizes the input for the model. structured prompt is then created, comprising: Question Instructions: Concise directives for the task. Problem Statement: detailed description of the coding challenge. Project Files: Relevant source files necessary for the solution. Solution Generation: The structured prompt is sent to the selected AI model(s) (e.g., o1, Gemini, Claude). The model generates solution, which is returned in the specified format (e.g., XML or JSON). Post-Processing: The generated solution is validated for structural and formatting issues, such as: Parsing errors in XML/JSON. Misformatted line breaks (n) or tabs (t). Necessary corrections are applied to ensure the solution adheres to the required structure and remains parsable. Solution Integration: The validated solution is integrated into the project files, updating the project with the models output and preparing it for testing. Test Case Validation: The updated project is executed in Docker container, where it is evaluated against pre-defined test cases. These test cases serve as the ground truth to assess the correctness and consistency of the solution. Store Partial Results: Evaluation results for each question, including the number of test cases passed and corresponding outputs, are recorded in CSV file for further analysis. Overall Aggregation: After evaluating all questions, an aggregation script computes key performance metrics for each question, including: Mean score Mean Pass@1 Standard deviation Mean test cases passed (used to calculate mean scores)"
        },
        {
            "title": "3.3 Prompt",
            "content": "In our prompt, we instruct the models to generate output code in either XML or JSON format. The primary motivation for adopting XML/JSON is that the HackerRank-ASTRA benchmark requires the model to handle multi-file modifications. Utilizing these structured formats ensures that the output is consolidated into single file, facilitating consistent and reliable evaluation of the results. As illustrated in the following figures, the JSON prompt included more detailed formatting instructions, as we observed format escaping issues with models generating JSON outputs. Since the objective is to evaluate the models code generation capabilities rather than their proficiency in consolidating XML/JSON formats, we incorporated postprocessing step in the evaluation pipeline. This step standardizes the output formatting to ensure no bias is introduced due to formatting discrepancies. Figure 5: XML prompt of v1 HackerRank-ASTRA benchmark. Figure 6: JSON prompt of v1 HackerRank-ASTRA benchmark."
        },
        {
            "title": "3.4 Models",
            "content": "In the v1 HackerRank ASTRA benchmark, we focused on evaluating frontier models, as detailed in the following tables. Given that our benchmark is designed to assess performance on real-world multi-file tasks, we have also emphasized the context length capabilities of the models to highlight their ability to handle complex scenarios effectively. Table 2: Context length and default temperature of models. Models o1 o1-preview GPT-4o-0513 Claude-3.5-Sonnet-1022 Gemini-1.5-pro Context Length Default Temperature 200,000 tokens 128,000 tokens 128,000 tokens 200,000 tokens 128,000 tokens 1 1 1"
        },
        {
            "title": "4.1 Evaluation Leaderboard",
            "content": "V1 ASTRA evaluates models on real multi-file front-end challenges, yielding an mean score around 70% and an mean pass@1 around 60%. Based on the analysis of mean scores, the models o1, o1-preview, and Claude-3.5-Sonnet-1022 demonstrate superior performance for multi-file, real-world front-end tasks. However, due to the high variance within the mean scores across 65 questions, paired t-test reveals that, with the exception of GPT-4o-0513, the differences between model performances are not statistically significant. Despite this, the mean score with = 32 indicates meaningful practical impact in real-world production settings. Similar trends were observed when evaluating the models using the mean pass@1 metric. In our benchmark evaluation, we assessed the consistency of LLMs using the standard deviation (SD) of their scores across 32 independent runs per question and then evaluated the median SD across 65 questions. The models demonstrated varying levels of performance stability, with Claude-3.5-Sonnet-1022 exhibiting the lowest variability (SD = 0.0497), indicating the highest consistency across problems. The difference between Claude-3.5-Sonnet-1022 and the rest of the models is statistically significant based on the paired t-test. Table 3: Model performance comparison for the v1 HackerRank-ASTRA benchmark. Model o1 o1-preview Claude-3.5-Sonnet-1022 Gemini-1.5-pro GPT-4o-0513 Mean Score Consistency (SD) Mean Pass@1 0.11 0.17 0.05 0.13 0.20 75.80% 75.55% 75.07% 71.17% 69.52% 63.92% 60.89% 62.74% 58.15% 58.15%"
        },
        {
            "title": "4.2 Main Skill Model Performance Summary",
            "content": "The best model for front-end development depends on the specific skills being evaluated. For skills with an occurrence of 3 in the benchmark, o1, o1-preview, and Claude-3.5 Sonnet demonstrated comparable performance levels. However, for skills with only single occurrence in the ASTRA dataset, such as Java and Selenium, Claude-3.5 Sonnet and Gemini 1.5 Pro tended to outperform OpenAI models. Interestingly, o1 underperformed compared to its predecessor, o1-preview, and even GPT-4o for certain skills, including AngularJS, Java Spring Boot Backend, Java, and Selenium. Notably, o1-preview achieved the highest performance across all models on tasks involving AngularJS, proving that newer iterations do not always guarantee better results. Figure 7: Model performance comparison by main skill categories."
        },
        {
            "title": "4.3 Sub-Skill Model Performance Summary",
            "content": "Model performance exhibits significant variation across subskills, indicating that the optimal AI tool for front-end development is highly dependent on the specific use case. Claude 3.5 Sonnet demonstrates superior performance in areas such as API integration, data filtering, and database interaction. Conversely, o1 performs particularly well in tasks related to form handling, pagination, API management, and EventEmitter functionality. Notably, o1-preview and GPT-4o outperform o1 in several subskills, underscoring the observation that newer models do not consistently achieve superior performance across all domains. These findings highlight the necessity of selecting models based on the precise requirements of individual projects to achieve optimal outcomes. 10 Table 4: Winning models by sub-skill categories. Winning Models o1 Claude-3.5-Sonnet, o1 Claude-3.5-Sonnet Claude-3.5-Sonnet Subskill (Occurrence) Form Handling (31) API Integration (18) State Management (12) Data Filtering (11) Controlled Components (10) Gemini-1.5-Pro Search Functionality (9) Database Interaction (8) EventEmitter (6) Component Reuse (3) Pagination and API (3) Regex (3) Routing (3) Sorting (3) o1-preview Claude-3.5-Sonnet o1 Claude-3.5-Sonnet o1 o1-preview GPT-4o Claude-3.5-Sonnet Figure 8: Model performance comparison by sub-skill categories (1/3). 11 Figure 9: Model performance comparison by sub-skill categories (2/3). Figure 10: Model performance comparison by sub-skill categories (3/3)."
        },
        {
            "title": "4.4 Formatting Impact on Model Performance",
            "content": "XML consistently outperforms JSON across all evaluated models in our benchmark, which requires models to return multi-file code solutions in both XML and JSON formats. After assessing the mean score and mean pass@1 with = 32, XML demonstrated statistically significant superiority over JSON for both metrics, with the exceptions of GPT-4o and the mean score from Gemini 1.5 Pro. This suggests that there is likely more XML training data available for these large language models (LLMs) compared to JSON. For similar development scenarios, developers are advised to prioritize XML over JSON to achieve better results when leveraging LLMs. 12 Table 5: XML format model results. Mean Score Mean Pass@1 Model o1-preview Claude-3.5-Sonnet-1022 Gemini-1.5-Pro GPT-4o-0513 75.56% 75.07% 71.17% 69.53% 60.89% 62.74% 58.15% 50.91% Table 6: JSON format model results. Mean Score Mean Pass@1 Model o1-preview Gemini-1.5-Pro Claude-3.5-Sonnet-1022 GPT-4o-0513 72.36% 70.08% 70.04% 68.13% 54.75% 53.56% 57.50% 50.31%"
        },
        {
            "title": "4.5 Format Escaping and Guardrail Issues",
            "content": "The ASTRA Benchmark highlights challenges related to JSON escaping and occasional solution refusals in o1-preview and o1, underscoring the need for improved format escaping capabilities and guardrails. ASTRA includes multi-file project questions that require models to convert their outputs into JSON format. While most models handled this task seamlessly, o1-preview encountered notable difficulties, particularly with escaping multiline strings. On average, o1-preview exhibited 2.3% error rate related to JSON escaping, even after detailed guidance and prompt adjustments. Additionally, o1-preview refused to provide solutions in 0.2% of cases, likely due to the guardrail settings implemented within the model. Similar issues were observed with o1. These findings emphasize the need for refining guardrails to strike an optimal balance between security constraints and usability, ensuring more robust and reliable model performance in development tasks."
        },
        {
            "title": "4.6 Common Errors",
            "content": "User Interface and Presentation Issues: Errors that impact the visual or interactive aspects of the application, degrading the user experience by displaying incorrect or suboptimal layouts and requiring user intervention to correct. Logical and Implementation Errors: Errors in the implementation that fail to account for specific conditions, edge cases, or problem constraints, despite having correct syntax. Data Handling and Misuse Errors: Errors caused by improper or unnecessary manipulation of data files or structures, disrupting the applications expected functionality and potentially leading to compilation failures. Typos, Syntax, and Misinterpretation Errors: Errors resulting from minor formatting issues, typographical mistakes, or misinterpretation of the problem statement. These errors typically involve incorrect output formatting or failure to adhere to the specified requirements."
        },
        {
            "title": "Length",
            "content": "The correlation between the average output length and the mean score is approximately -0.560, indicating moderate negative relationship. This suggests that longer outputs are generally associated with lower scores. In contrast, the correlation between input length and mean score is approximately -0.164, reflecting weak negative relationship. This implies that longer inputs may slightly reduce the mean score. Figure 11: Scatter Plot of Average Output vs Average Score vs Input Length."
        },
        {
            "title": "5 Related Work",
            "content": "The evaluation of large language models (LLMs) in coding tasks has evolved with the development of various benchmarks, each addressing specific aspects of model performance. Among the current industry standards, SWE-bench [2] is notable for leveraging real-world Python repositories to assess model capabilities. Recently, SWE-bench expanded to include image data for multi-modal tasks, particularly targeting JavaScript challenges [3]. DeepSeek-R1 [6]sampled 16 responses for each question and calculated the overall average accuracy to ensure stable evaluation, which essentially reflects the mean pass@1. In recent years, repository-style benchmarks have emerged to cover wider range of domains. For instance, DevEval [4]) evaluates LLMs on diverse repositories spanning various programming languages and domains, providing more holistic assessment of coding capabilities. The rise of agent-based benchmarks has further advanced the evaluation landscape. AgentBench [7] assesses LLMs reasoning and decision-making abilities in interactive environments, offering insights into their performance as autonomous agents. Similarly, MLAgentBench[8] evaluates LLM-based research agents in machine learning tasks, focusing on their ability to perform experimentation loops and build models autonomously. Our HackerRank-ASTRA Benchmark aligns with the trend of real-world evaluations by introducing multi-file setup that reflects the complexity of production-level software development. Unlike most existing benchmarks, ASTRA emphasizes both correctness and consistency, evaluated through metrics like mean pass@1 and median standard deviation across multiple runs. Furthermore, ASTRA uniquely spans the full software development lifecycle (SDLC)."
        },
        {
            "title": "6 Conclusions",
            "content": "The HackerRank-ASTRA Benchmark contributes to the evaluation of LLMs in real-world software development by introducing multi-file, project-based setup spanning wide range of domains. By focusing on both correctness and consistency through metrics like mean pass@1 and median standard deviation, ASTRA offers deeper insights into model reliability and performance across repeated runs. Initial findings reveal that LLMs face significant challenges in certain primary skills and sub-skill categories. Observations related to model consistency, formatting issues, guardrail adherence, and common error patterns further highlight areas where LLMs need improvement to better handle realistic and complex development scenarios."
        },
        {
            "title": "7 Limitations and Future Work",
            "content": "While our study provides valuable insights into AI model performance on multi-file, realworld coding tasks, it has several limitations that warrant consideration. The current version of the benchmark focuses primarily on front-end projects, such as React and Angular.js, which narrows the scope of skill evaluation by under-representing back-end skills and other domains. Future iterations will address this by including broader range of back-end technologies and skills. Additionally, the evaluation does not yet leverage argentic approaches, where models are granted autonomy to iteratively explore, adapt, and refine their solutions. Incorporating such methods would provide more realistic assessment of model potential in dynamic problem-solving scenarios. Furthermore, the current framework evaluates models based on direct output without iterative feedback from test case results, limiting our ability to assess how models improve when guided by incremental corrections. Another limitation lies in the restricted model selection, which currently includes subset of top-tier models. In future iterations, we plan to expand testing to include additional models, such as DeepSeek and Metas Llama models, and adopt community-driven approach to benchmarking, fostering broader model comparisons. As step toward this goal, we are open-sourcing all 65 project questions (https://huggingface.co/datasets/hackerrank/astra-benchmark) on GitHub and HuggingFace to enable wider participation and collaboration in advancing the benchmark."
        },
        {
            "title": "8 Acknowledgments",
            "content": "We thank Vivek Ravisankar, the CEO and Co-Founder of HackerRank for his sponsorship of this work and valuable input. We acknowledge the insightful methodology discussions and support provided by Ahmed El-Kishky, Nat McAleese, and Chris Orsinger from OpenAI."
        },
        {
            "title": "References",
            "content": "[1] Zhaojian Yu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. Humaneval pro and mbpp pro: Evaluating large language models on self-invoking code generation. arXiv preprint arXiv:2412.21199, 2024. 15 [2] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [3] John Yang, Carlos E. Jimenez, Alexander L. Zhang, Kilian Lieret, Jiani Yang, Xinyun Wu, Ofir Press, Nils Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. Swe-bench multimodal: Do ai systems generalize to visual software domains? arXiv preprint arXiv:2410.03859, 2024. [4] Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, and Yongbin Li. Deveval: manually-annotated code generation benchmark aligned with real-world code repositories. arXiv preprint arXiv:2405.19856, 2024. [5] Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. Enhancing large language models in coding through multi-perspective self-consistency. arXiv preprint arXiv:2309.17272, 2024. [6] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and Xiaokang Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2024. [7] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. [8] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: EvalarXiv preprint uating language agents on machine learning experimentation. arXiv:2310.03302, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Errors Observed in Restful project problem This example highlights RESTful API development task from the ASTRA Benchmark Dataset, illustrating both the design requirements and the challenges encountered by language models like GPT-4. The task involves implementing RESTful API for managing product records using Node.js and Express, mirroring real-world e-commerce application scenarios. Key endpoints include adding, retrieving, and updating product details, with explicit constraints against modifications or deletions using PUT or DELETE methods, reflecting specific business logic requirements. Additionally, candidates are required to ensure modular programming, robust error handling, and adherence to rules such as ensuring MRP Price and Stock > 0 before publishing products. representative solution generated by GPT-4o-0513 demonstrates both the models strengths and limitations. While it correctly implemented the core logic of the API by 16 defining modular routes and controllers for handling product operations, critical oversight occurred in integrating the routes into the main application file (app.js). Instead of linking the productsRouter to the root path, the model incorrectly retained placeholder indexRouter, resulting in all requests to /products failing with 404 Not Found error. This error highlights common gap in LLM outputs: an inability to ensure end-toend functionality by resolving dependencies across modular components. Consequently, the model failed all test cases requiring functional routes. Figure 12: The routes defined by GPT-4o-0513 for handling products. Figure 13: The app.js provided by GPT-4o-0513. 17 Figure 14: Fixes required in app.js. This example underscores the importance of validating LLM outputs in complex programming tasks. While models can generate syntactically correct code for modular components, they often miss essential integrations, leading to functionality gaps. Such errors reflect real-world challenges in software development, where end-to-end testing and integration are critical for delivering robust applications. A.2 Errors Observed in React project problem This problem revolves around creating dynamic form with two fields: Company Title and Number of Employees. The task requires implementing real-time validation to reflect practical use cases in production-level applications. The Company Title field is mandatory, and leaving it empty should trigger an error message. Meanwhile, the Number of Employees field, though optional, should only accept numbers greater than zero, with errors displayed for invalid inputs. Additionally, the Submit button should remain disabled until the form is fully validated, aligning with best practices for guiding users toward correct form submissions. 18 Figure 15: form is expected to behave dynamically by providing real-time feedback to users through form validation. This scenario mirrors real-world front-end challenges in web development, where forms serve as critical touchpoint for user interaction. Dynamic validation, error handling, and state management are essential for improving user experience and ensuring data accuracy. Proper implementation demands meticulous validation logic, efficient state handling, and conditional rendering of error messages. solution generated by GPT-4o demonstrated core competencies in state management and event handling but failed to execute dynamic validation and error management properly. The main shortcomings included the inability to dynamically re-evaluate the forms validity, leading to incorrect enabling/disabling of the Submit button. Additionally, error messages were rendered even when empty, causing unnecessary DOM elements and test failures. By introducing useEffect hook to monitor input and error state changes, conditionally rendering error elements, and updating error states dynamically during user input, these issues were rectified as the following screenshots. The corrected implementation adheres to production-grade standards, ensuring accurate validation and seamless user experience. 19 Figure 16: useEffect hook should be used to re-evaluate the forms validity whenever any error or input changes. Figure 17: Render error elements only when there is an actual error message. Figure 18: The handleChange function should update the error state dynamically based on user input to ensure the UI always reflects the correct validation status. A.3 Errors Observed in Angular project problem This task involves building Length Converter component in Angular, where users can convert between units of length (e.g., Kilometers, Meters, and Centimeters) in real time. The component includes two input fields with corresponding dropdown menus for selecting units. When user enters value in one input field, the other field updates dynamically based on the selected units. Similarly, selecting different unit from the dropdown menu recalculates the converted value for the associated input field. The initial state requires the first input field to default to Kilometer and the second to Meter, with both input fields initially empty. 20 Figure 19: Length Converter functionality. This problem reflects real-world use cases for unit conversion, commonly seen in industries like e-commerce, architecture, and engineering, where accurate and dynamic conversions are critical. For example, online platforms often provide options to switch measurement units for materials, while navigation systems allow users to toggle between distance measurements such as kilometers and miles. This scenario requires developers to employ two-way data binding, lifecycle hooks, and conversion logic to synchronize inputs, dropdowns, and calculated results seamlessly. Figure 20: This binds the input1 field to the components input1 property. Any changes to the input field will trigger the onInput1() method for conversion. Figure 21: The conversion logic should ensure that the corresponding input field is updated whenever change occurs. 21 Figure 22: The ngOnInit() lifecycle hook initializes the components state. Initially, the input1 and input2 should be set to null, with the default units being Kilometer and Meter. solution generated by GPT-4o successfully utilized Angulars two-way data binding with ngModel to link the input fields and dropdowns to component properties. It also implemented conversion matrix to calculate factors between units. However, specific flaws impacted the functionality. The input fields were incorrectly initialized to 0 instead of being empty (null), violating the problem requirements. Additionally, dropdown changes occasionally updated the wrong input field, leading to unpredictable results. Null and undefined values were not adequately handled, causing errors during conversions. By addressing these issuesensuring input fields are initialized to null, updating the correct field dynamically based on unit changes, and adding null checksthe component can function as intended, offering seamless and accurate user experience. A.4 Errors Observed in Ruby on Rails project problem This task involved developing Job Board API using Ruby on Rails, allowing job seekers and employers to interact seamlessly. The API required implementing key functionalities, such as user authentication, job listings, job search, and job application submissions. Users could register and log in securely, with JWT tokens ensuring authenticated interactions. The job listings included advanced search capabilities using the Ransack gem, filtering results dynamically by title, company, or location. Additionally, authenticated users could apply for jobs by submitting CVs and cover letters, with file upload handling as part of the requirements. The problem mirrors real-world web development tasks, such as securing APIs with JWT tokens, implementing RESTful design principles, and managing file uploads. These features are common in systems requiring secure access and efficient query tools, such as e-commerce or enterprise applications. Handling user registration, authentication, and advanced search showcases essential skills in API design, while ensuring data validation and response consistency is critical for production-grade systems. solution generated by GPT-4o successfully implemented core functionalities like user registration, login, job listings retrieval, and search. It efficiently utilized Ransack for advanced query filtering and followed RESTful architecture principles for modular and scalable endpoint design. However, key gaps impacted edge case handling and critical validations. For instance, the current user method was missing, preventing the application from identifying authenticated users in job application actions. Similarly, the Job model lacked validations for required fields, allowing invalid entries to be created. 22 Authentication concerns arose when unauthenticated users could access protected routes due to missing the AuthenticateUser concern in the JobsController. Addressing these issuessuch as adding the current user method, including field validations, and securing protected routesresulted in fully functional and secure application that passed all test cases. Figure 23: Fix: Adding the current user method to the AuthenticateUser concern to ensure that the method is available for use in the controller actions. Figure 24: Fix: Adding validations ensured that only valid job entries were created, improving data integrity. 23 Figure 25: Fix: The response was updated to include details about the associated job and user, making it more comprehensive."
        }
    ],
    "affiliations": []
}