{
    "paper_title": "Dynaword: From One-shot to Continuously Developed Datasets",
    "authors": [
        "Kenneth Enevoldsen",
        "Kristian Nørgaard Jensen",
        "Jan Kostkan",
        "Balázs Szabó",
        "Márton Kardos",
        "Kirten Vad",
        "Andrea Blasi Núñez",
        "Gianluca Barmina",
        "Jacob Nielsen",
        "Rasmus Larsen",
        "Peter Vahlstrup",
        "Per Møldrup Dalum",
        "Desmond Elliott",
        "Lukas Galke",
        "Peter Schneider-Kamp",
        "Kristoffer Nielbo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise. To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution."
        },
        {
            "title": "Start",
            "content": "Dynaword: From One-shot to Continuously Developed Datasets Kenneth Enevoldsen1*, Kristian Nørga ard Jensen2, Jan Kostkan1, Balázs Szabó1, Márton Kardos1, Kirten Vad1, Andrea Blasi Núñez4, Gianluca Barmina4, Jacob Nielsen4, Rasmus Larsen2, Peter Vahlstrup1, Per Møldrup Dalum1, Desmond Elliott3, Lukas Galke4, Peter Schneider-Kamp4, Kristoffer Nielbo1 1Aarhus University, 2The Alexandra Institute 3University of Copenhagen, 4University of Southern Denmark Correspondence: Kenneth.enevoldsen@cas.au.dk 5 2 0 2 ] . [ 1 1 7 2 2 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise. To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword1. The Dynaword approach is framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing sustainable framework for ongoing community contributions and dataset evolution."
        },
        {
            "title": "Introduction",
            "content": "Continuously developed open-source projects are instrumental for contemporary research and lay the foundation for the success of our fields. These projects range from foundational software such as NumPy (Harris et al., 2020), to datasets like the Universal Dependencies Treebank (Nivre et al., 2016), to recent contributions including Flash Attention (Dao et al., 2022) and LoRA (Hu et al., 1Available at: https://huggingface.co/datasets/ danish-foundation-models/danish-dynaword 1 Figure 1: Overview of the guiding principles for Dynaword corpora 2022). In open-source communities, it is understood that project is never complete but is continually enhanced and adjusted in response to advances in the field and the software ecosystem. It is similarly recognized that relying on proprietary technology may result in legal ramifications, which, in the worst case, can render projects unusable or lead to their removal, undermining community contributions. We have already seen this impact in several instances: Udio AI Music Generator was shut down citing legal concerns; state-of-the-art Danish encoder (Enevoldsen et al., 2023) was removed following threats of legal action; and the Nordic Pile corpus (Öhman et al., 2023) was never released, presumably due to copyright issues. However, the values of open source are notably lacking for (pre-)training datasets in the field. Although we have seen large-scale releases (Gao et al., 2020b; Xue et al., 2020), these datasets often adhere to the pattern of being released once without updates. Even when we see continual releases (Penedo et al., 2024; Abadji et al., 2022), they are frequently based on Common Crawl content. Baack et al. (2025) defines this category as open access and notes several legal risks associated with the use of both datasets and derived models. (Baack et al., 2025) also defines openly licensed data, which enables the resharing, reuse, and modification of data, thereby providing solid foundation for derivative works. There have been few initiatives (Langlais et al., 2025; Langlais, 2024); however, despite being step forward, these contributions still have notable shortcomings. These datasets (Langlais et al., 2025; Langlais, 2024) constitute single release without reproducible code for collection. This makes it difficult to reproduce the data collection process and difficult to improve2 or update3. Lastly, vague description of the underlying licenses makes it difficult to validate the license claims. For instance, stating that \"Alice in Wonderland\" is public domain differs from documenting that the author died in 1898, which renders it within the public domain; we refer to this as traceable license. Following these limitations, we propose the dynaword approach for curating corpus: Traceable and open licensing: All datasets within the corpus must be openly licensed and maintain traceable license. Reproducibility: It should be possible to derive substantially similar dataset4. Documented: The dataset should be welldocumented under best practices in the field (Gebru et al., 2021). Extensible: Extending and improving the corpus should be possible, and methods for doing so should be documented. These guidelines are intended to provide datasets that are compatible with the open-source AI definition5, FAIR (Wilkinson et al., 2016), and compliant under diverse set of legal frameworks, including the European Parliament Artificial Intelligence Act (European Union, 2024), and conducive 2For instance, the Common Corpus consists of multiple OCRd documents that could likely be improved with recent advancements. 3E.g. the number of tokens on Wikipedia is expected to grow linearly (Suh et al., 2009). 4Denoted as substantially equivalent system in the Open-Source AI definition: https://opensource.org/ai/ drafts/the-open-source-ai-definition-1-0-rc1 5https://opensource.org/ai/drafts/ the-open-source-ai-definition-1-0-rc1 to creating lasting resources for both research and industry. This approach draws upon successful datasets such as the Universal Dependency Treebanks (Nivre et al., 2016), which remain essential building blocks for linguistic analysis, model development (Honnibal et al., 2020), and recently, multilingual benchmarking of LLMs (Nielsen, 2023; Nielsen et al., 2024). These guidelines were developed jointly with the Danish Dynaword, which acts as practical implementation and testbed, showing that the guidelines are implementable and not simply ideals. We believe Danish provides an ideal case, being lowto-mid-resource6 language, which has enough contributors willing to participate, yet well-contained in scope. High-resource languages7 could likely sustain multiple dynaword projects targeting specific domains such as code, academia, or healthcare. The following sections outline how we developed framework around Danish Dynaword that actively ensures traceable and open licensing and reproducibility, while being documented and reproducible. Our final dataset represents more than fourfold increase in Danish tokens available compared to previous datasets and considerably improves reproducibility and documentation. We expect that this work will continue to expand with future contributions."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Continually released pre-training data Recently, dataset releases have been increasingly iterative releases with improvements to extraction, cleaning, and collection procedures. These include OSCAR (Abadji et al., 2022), HPLT (Aulamo et al., 2023; Arefyev et al., 2024), and fineweb (Penedo et al., 2024), all which are mostly based on Common Crawl8. Though Common Crawl and its derivatives represent significant artifacts, the license of the underlying data remains ambiguous, raising slew of ethical and legal concerns (Baack et al., 2025). Additionally, these sources typically work with web-derived content, overlooking sources such as APIs, television, or radio. 6Class 2-4 as defined by Joshi et al. (2021) 7Class 5 as defined by Joshi et al. (2021) 8https://commoncrawl.org 2 2.2 Openly licensed data Data licensing is often unclear, and it is not uncommon to find datasets on Hugging Face9 published under permissive licenses containing copyrighted content. In some cases (Abadji et al., 2022; Penedo et al., 2024) the license applies only to the packaging, metadata, and annotations, but not to the data itself. To avoid such confusion, we will, throughout this work, utilize the three tiers defined by Baack et al. (2025): 1) replicable - enables reproduction - 2) open access - enables download - and 3) openly licensed - enables resharing, reuse, and modification. Examples of openly licensed data sources intended for pre-training include YouTube Commons (Langlais, 2024) and Common Corpus (Langlais et al., 2025) as well as multiple gigaword projects (Graff and Cieri, 2003; Derczynski et al., 2021; Adewumi et al., 2020). These gigaword corpora consist of 1B (Giga) tokens available under permissive licenses. YouTube Commons compiles CC-BY licensed YouTube content, including 30B tokens of predominantly English (70%) transcripts. The most extensive openly licensed corpus is the 500B Common Corpus, which includes digitized newspapers and OCRd public domain books. While these collections solved many issues, they left much to be desired. The OCR quality is often questionable, and the lack of public processing workflows makes these datasets hard to reproduce, validate and improve."
        },
        {
            "title": "3 Dataset Collection",
            "content": "Danish Dynaword takes its outset in public segments of the Danish Gigaword (Derczynski et al., 2021), previously the largest publicly available data source for Danish. Danish Dynaword notably excludes social media data from Twitter ( 32M tokens), copyrighted samples from OpenSubtitles (<1M tokens), and common crawl segments ( 100M tokens) following the principles of traceable and open licensing. We also exclude DanAvis ( 30M tokens) due to lack of overall coherence caused by scrambling. For each source in this collection, datasheet (Gebru et al., 2021) was compiled, incorporating available information, including dataset description and license references. Furthermore, additional open datasets were added using process of identifying potentially 9https://huggingface.co openly licensed datasets, collecting them, conducting quality checks, and, lastly, reviewing their licenses in case of uncertainty. Quality checks were intentionally kept minimal to allow for downstream filtering and include verifying that the text is Danish, coherent, and readable. In the following, we will walk through each of these steps. Identification: The majority of openly licensed datasets were found through projects such as the Danish Foundation Models (Enevoldsen et al., 2023), on the HuggingFace Hub, or through Sprogteknologi.dk, website covering Danish language resources, curated by the Danish Ministry of Digitization. Additional resources were identified through social media outreach, personal communication, and issues in the Danish Dynaword repository. License and content review: After identification, maintainers review the data and filter out straightforward cases where the data is not Danish, is not openly licensed, or the redistributor lacks permission to license or re-license it. For complex cases, maintainers may request legal advice from faculty services. Collection and Quality Checks: After dataset is checked, it is collected and undergoes quality checks. The collection procedure is documented in the form of datasheets and reproducible scripts. These scripts enable dataset updates and critical examination. Some sources excluded at this stage include the Danish subsection of Common Corpus, where the OCR was deemed insufficient (alpha ratio generally below 0.7), with most text being unreadable. If rejected, the quality issues are documented, and the issue is closed. For all sources, we deduplicate and remove short documents. 3.1 Inclusion Policy In recent years, we have seen the rise of derived data in pre-training regimes. These data include synthetic (Li et al., 2023; Gunasekar et al., 2023), semi-synthetic (Chung et al., 2024), translations (Doshi et al., 2024), OCRd (Langlais et al., 2025), and transcribed data (Langlais, 2024). These sources require deliberation before inclusion, as they can also lead to model degradation (Bender et al., 2021; Shumailov et al., 2024). While it is clear that an inclusion policy is likely to change as technologies improve, the current dynaword does not, to our knowledge, contain synthetic, machine-translated, or automatically transcribed data, but does include human-annotated auDataset Size Tokens Contributions Replicable Open Access Enables the Enables Is there clear user to downthe user to process for load. reproduce. contributions Openly Licensed Enables the user to reuse, share, and modify. Tiers of Dataset Openness Tier 3: Openly Licensed Danish Dynaword (v1.2.7) Danish Gigaword (Derczynski et al., 2021) Common Corpus (dan) (Langlais et al., 2025) 0.3B Tier 2: Open Access SnakModel (Zhang et al., 2025) Fineweb (dan) (Penedo et al., 2024) 13.6B 26B 1B 4.8B Yes No No No (Yes)1 Table 1: Comparison of Danish Language Datasets. Danish Gigaword only includes segments that are currently publicly available. 1Has Discord channel and encourages involvement. dio transcriptions (e.g., FTSpeech) and translations by expert translators (e.g., Europarl) and OCRd documents (e.g., NCC books). For OCRd documents, we perform an additional quality check; these are described in the individual datasheets. 3.2 Evaluation data It is by no mean uncommon that large public dataset include segments of evaluation data (Gao et al., 2020a), this can lead overestimate of actual performance (Schaeffer, 2023; Deng et al., 2024) and thus leading to overestimates of the model capabilities. It is therefore encourages that model developers exclude evaluation data during the training process. To facilitate this we mark datasets contained in benchmarks. For the Danish Dynaword this includes, the Danish dependency treebank, though not the annotation, which is used to create the dataset ScaLA (Nielsen, 2023) and Nordjyllands News, which is used for evaluating summarization (Nielsen, 2023; Nielsen et al., 2024) as well as semantic similarity (Enevoldsen et al., 2024, 2025). 3.3 Contributions Danish Gigaword has already seen multiple contribution prior to its official release. We show this development in Figure 2. These contributions have come from companies, government institutions, individuals and universities and stemming from wide array of background ranging for cultural heritage to NLP."
        },
        {
            "title": "4 Dataset Comparison",
            "content": "In Table 1 we give conceptual comparison of Danish Dynaword to existing openly available datasets and in Table 2 we show the performance gap when training on Danish Dynaword instead of the Danish Figure 2: Number of tokens in Danish Dynaword over time. Gigaword, which is we eloborate on in the following section. We present an overview of the datasets in Appendix B. For individual datasheets, we refer to the dataset repository. 4.1 Training Experiments To study relative quality Danish Dynaword and Danish Gigaword (Strømberg-Derczynski et al., 2021) we perform set of training experiments to estimate the expected language modelling performance. The experiments were performed using the Gemma-1B model either continually pre-trained10 or trained from scratch. To ensure fair comparison we train two models on Dynaword, one matched in size to Danish Gigaword and one trained on the full dataset. We evaluated the perplexity performance on four datasets from Dynaword: DDT, JVJ, Synnejysk.dk, and Nordjyllands News which were held out during training. Additionally, we tested on contemporary sources not included in Dynaword: news articles from DR (dr.dk) and Danish Wikipedia articles published after January 1, 2025. All models were trained with maximum se10Using the gemma-3-1b-pt checkpoint. 4 quence length of 6144 tokens, an effective batch size of 32 (via gradient accumulation), and an initial learning rate of 105 for pretrained models and 103 for the models from scratch, in both cases we use cosine learning rate scheduler. Models were trained using the Danish Dynaword version 1.2.0 and with training code available on GitHub11. An overview of models is provided in Appendix C. Table 2 shows the perplexity across the six heldout datasets. Comparing Dynaword to Gigaword, we see an average relative improvement of 5.9% for continual pre-training starting from Gemma-31b-pt, and an improvement of 26% for Gemma-31b models trained from scratch. Even in the sizematched scenario, Dynaword yields improvements of 2.6% with continual pre-training and 18% when training from scratch. Results from downstream evaluation on Danish tasks from EuroEval (Nielsen, 2023) show that continual pre-training on Danish Dynaword yields improvements on 7 out of 9 tasks. Detailed results are provided in Appendix D."
        },
        {
            "title": "5 Conclusions",
            "content": "In this work, we argue for continuously developed datasets. We dub such datasets dynawords and outline four key principles which are required for continuous development; a) Open and traceable licensing, b) Reproducibility, c) Documentation, and d) Extensibility. As testbed and concrete implementation, we release Danish Dynaword, the largest openly licensed Danish corpus, which we expect to grow with future submissions. Danish Dynaword has already received contributions from multiple parties, including industry, private individuals, and research. For the datasets in the dynaword, we also provide reproducible scripts to update them and compliance review of the licensing. To enable future contributions, Danish Dynaword comes with lightweight tests to ensure formatting, documentation, and dataset quality. Danish Dynaword has already seen contributions from multiple parties, including industry, private individuals, and research. While Dynaword remains an order of magnitude smaller than non-openly licensed sources, it provides sustainable foundation for building models. Dynawords should be seen as complement to existing efforts, making high-quality and permissible data available. We hope that this dynaword can 11https://github.com/schneiderkamplab/ offpolicy_kd/, Commit 76b546e be blueprint for future dynawords targeting other languages or domains."
        },
        {
            "title": "Limitations",
            "content": "Size Danish Dynaword significantly expands available openly-licensed data, with future growth expected from initiatives like Dansk Sprogmodel Konsortium (DSK)12 and national AI programs13. However, it remains an order of magnitude smaller than the Common Crawl datasets (Penedo et al., 2024; Abadji et al., 2022). This gap may persist, but could be addressed through multilingual or multimodal sources. Coverage and bias: Danish Dynaword, given its requirements, is biased toward domains with clear licensing. Thus, the data set only contains limited amounts of social media data and disproportionate amount of legal documents. You can explore the coverage by domain in Figure 3. Figure 3: Content by domain. The inner circle shows the domain, while the outer layers are the source. Only Danish: Dynaword presents methodology for developing large-scale public language resources. However, in this work, we only present one such resource, namely, for Danish. We hope that Danish Dynaword can act as starting point for similar efforts for other languages. Review quality and dataset poisoning: Throughout the development process, it became clear that contributing minor changes, such as filtering out few bad examples, was difficult, both due to the limited support for reviewing large data changes. While previous projects (e.g. 12https://alexandra.dk/dsk/ 13https://digst.dk/strategier/ strategi-for-kunstig-intelligens/ Held out Datasets Contemporary (2025) Treebank DDT Fiction JVJ Dialect Synnejysk News Wikipedia Nordjylland Wiki (dan) News DR 16.0 33.9 62.8 9.8 9. 9.7 14.2 14.0 (+1.2%) 13.5 (+4.6%) 29.1 26.6 (+8.5%) 25.2 (+13%) 54.1 51.8 (+4.3%) 50.1 (+7.5%) 8.5 8.4 (+1.2%) 8.1 (+4.6%) 8.1 8.2 (-0.7%) 8.0 (+1.9%) 9.2 9.1 (+0.9%) 8.9 (+3.5%) Reference baseline Gemma-3-1b-pt Continual pre-training Gigaword* Dynaword* (matched) Dynaword* (full) Pre-training from scratch Gigaword* Dynaword* (matched) Dynaword* (full) 48.8 43.0 (+12%) 39.2 (+20%) 128 86.2 (+33%) 79.4 (+38%) 219 144 (+34%) 128 (+42%) 24.4 21.0 (+14%) 19.6 (+21%) 26.7 26.0 (+3.7%) 23.2 (+13%) 29.4 25.4 (+14%) 23.6 (+20%) Table 2: Perplexity for Gemma-3-1b models continually pre-trained (middle) and pre-trained from scratch (bottom) on Gigaword, Dynaword (size-matched to Gigaword), and the full Dynaword dataset. Relative performance is calculated with respect to the Gigaword baseline. *The four validation datasets were excluded from the training data. Nivre et al. (2016)) have tackled this issue using human-readable formats, this is likely inefficient at the current scale. This lack of clarity increased the likelihood of dataset attacks such as dataset poisoning (Goldblum et al., 2022). We expect to see both interface development and software development to detect and prevent such attacks and ensure review quality. Ethical consideration Despite our effort to prevent issues, large-scale dataset development often involves seemingly openly-licensed datasets that may contain copyrighted content. notable instance occurred with the initial release of OpenSubtitles, which was part of the Danish Gigaword (Derczynski et al., 2021). By providing versioned dataset along with changelog, we clearly indicate when sources are excluded, thereby promoting transparency."
        },
        {
            "title": "Acknowledgments",
            "content": "Part of the computation done for this project was performed on the UCloud interactive HPC system, which is managed by the eScience Center at the University of Southern Denmark. The Danish eInfrastructure Consortium (DeIC) granted the compute resources (DeiC-AU-N1-2025144 and DeiCAU-N1-2025118). This project was made possible by funding from the Danish Government to Danish Foundation Models (4378-00001B). special thanks goes to organizations that have publicly made their data available under openly licensed regimes and to individuals and organizations that have encouraged this development."
        },
        {
            "title": "References",
            "content": "Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. 2022. Towards Cleaner DocumentarXiv Oriented Multilingual Crawled Corpus. preprint. ArXiv:2201.06642 [cs]. Tosin Adewumi, Foteini Liwicki, and Marcus Liwicki. 2020. Corpora compared: The case of the swedish arXiv preprint gigaword & wikipedia corpora. arXiv:2011.03281. Nikolay Arefyev, Mikko Aulamo, Pinzhen Chen, Ona De Gibert Bonet, Barry Haddow, Jindˇrich Helcl, Bhavitvya Malik, Gema Ramírez-Sánchez, Pavel Stepachev, Jörg Tiedemann, Dušan Variš, and Jaume Zaragoza-Bernabeu. 2024. HPLTs first release of In Proceedings of the 25th Andata and models. nual Conference of the European Association for Machine Translation (Volume 2), pages 5354, Sheffield, UK. European Association for Machine Translation (EAMT). Mikko Aulamo, Nikolay Bogoychev, Shaoxiong Ji, Graeme Nail, Gema Ramírez-Sánchez, Jörg Tiedemann, Jelmer Van Der Linde, and Jaume Zaragoza. 2023. Hplt: High performance language technologies. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 517518. Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bommarito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl, Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang, Leandro von Werra, Mitchell Baker, Julie Belião, and 20 others. 2025. Towards Best Practices for Open Datasets for LLM Training. arXiv preprint. ArXiv:2501.08365 [cs]. Emily Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models 6 be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610623. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, and 1 others. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. 2024. Investigating data contamination in modern benchmarks for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 87068719, Mexico City, Mexico. Association for Computational Linguistics. Leon Derczynski, Manuel Ciosici, Rebekah Baglini, Morten Christiansen, Jacob Aarup Dalsgaard, Riccardo Fusaroli, Peter Juel Henrichsen, Rasmus Hvingelby, Andreas Kirkedal, Alex Speed Kjeldsen, and 1 others. 2021. The danish gigaword corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 413421. Meet Doshi, Raj Dabre, and Pushpak Bhattacharyya. 2024. Pretraining language models using translationese. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 58435862, Miami, Florida, USA. Association for Computational Linguistics. Kenneth Enevoldsen, Isaac Chung, Ashwin Mathur, Imene Kerboua, Márton Kardos, David Stap, Jay Gala, Wissam Siblini, Saba Sturua, Saiteja Utpala, Gabriel Sequeira, Marion Schaeffer, Mathieu Ciancone, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Orion Weller, Chenghao Xiao, Ömer Çag, and 44 others. 2025. MMTEB: Massive Multilingual Text Embedding Benchmark. Kenneth Enevoldsen, Lasse Hansen, Dan Nielsen, Rasmus AF Egebæk, Søren Holm, Martin Nielsen, Martin Bernstorff, Rasmus Larsen, Peter Jørgensen, Malte Højmark-Bertelsen, and 1 others. 2023. Danish foundation models. arXiv preprint arXiv:2311.07264. Kenneth Enevoldsen, Márton Kardos, Niklas Muennighoff, and Kristoffer Laigaard Nielbo. 2024. The scandinavian embedding benchmarks: Comprehensive assessment of multilingual and monolingual text embedding. Neurips. ArXiv: 2406.02396 [cs.CL]. European Union. 2024. Regulation (eu) 2024/1689 of the european parliament and of the council of 13 june 2024 laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending regulations (ec) no 300/2008, (eu) no 167/2013, (eu) no 168/2013, (eu) 2018/858, (eu) 2018/1139 and (eu) 2019/2144 and directives 2014/90/eu, (eu) 2016/797 and (eu) 2016/798. Entered into force: 1 August 2024. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020a. The pile: An 800gb dataset of diverse text for language modeling. Preprint, arXiv:2101.00027. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, and 1 others. 2020b. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets for datasets. Communications of the ACM, 64(12):86 92. Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander adry, Bo Li, and Tom Goldstein. 2022. Dataset security for machine learning: Data poisoning, IEEE Transacbackdoor attacks, and defenses. tions on Pattern Analysis and Machine Intelligence, 45(2):15631580. David Graff and Cieri. 2003. English gigaword, linguistic data consortium. Philadelphia, 4(1):34. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, and 1 others. 2023. arXiv preprint Textbooks are all you need. arXiv:2306.11644. Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, and 7 others. 2020. Array programming with NumPy. Nature, 585(7825):357 362. Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large In International conference on language models. learning representations. sen, Claus Ladefoged, Finn Årup Nielsen, Jens Madsen, Malte Lau Petersen, Jonathan Hvithamar Rystrøm, and Daniel Varab. 2021. The Danish GigaIn Proceedings of the 23rd Nordic word corpus. Conference on Computational Linguistics (NoDaLiDa), pages 413421, Reykjavik, Iceland (Online). Linköping University Electronic Press, Sweden. Bongwon Suh, Gregorio Convertino, Ed Chi, and Peter Pirolli. 2009. The singularity is not near: slowing growth of wikipedia. In Proceedings of the 5th international symposium on wikis and open collaboration, pages 110. Mark Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip Bourne, and 1 others. 2016. The fair guiding principles for scientific data management and stewardship. Scientific data, 3(1):19. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934. Mike Zhang, Max Müller-Eberstein, Elisa Bassignana, and Rob van der Goot. 2025. SnakModel: Lessons learned from training an open Danish large language In Proceedings of the Joint 25th Nordic model. Conference on Computational Linguistics and 11th Baltic Conference on Human Language Technologies (NoDaLiDa/Baltic-HLT 2025), pages 812825, Tallinn, Estonia. University of Tartu Library. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2021. The State and Fate of Linguistic Diversity and Inclusion in the NLP World. arXiv:2004.09095 [cs]. ArXiv: 2004.09095. Pierre-Carl Langlais. 2024. Releasing YoutubeCommons: massive open corpus for conversational and multimodal data. Pierre-Carl Langlais, Carlos Rosas Hinostroza, Mattia Nee, Catherine Arnett, Pavel Chizhov, Eliot Krzystof Jones, Irène Girard, David Mach, Anastasia Stasenko, and Ivan P. Yamshchikov. 2025. Common corpus: The largest collection of ethical data for LLM pretraining. Preprint, arXiv:2506.01732. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463. Dan Saattrup Nielsen. 2023. ScandEval: Benchmark for Scandinavian Natural Language Processing. In Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 185201. Dan Saattrup Nielsen, Kenneth Enevoldsen, and Peter Schneider-Kamp. 2024. Encoder vs decoder: Comparative analysis of encoder and decoder language models on multilingual NLU tasks. arXiv preprint arXiv:2406.13469. Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, and Natalia Silveira. 2016. Universal dependencies v1: multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 16591666. Joey Öhman, Severine Verlinden, Ariel Ekgren, Amaru Cuba Gyllensten, Tim Isbister, Evangelia Gogoulou, Fredrik Carlsson, and Magnus Sahlgren. 2023. The nordic pile: 1.2 tb nordic dataset for language modeling. arXiv preprint arXiv:2303.17183. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The FineWeb Datasets: Decanting the Web for arXiv preprint. the Finest Text Data at Scale. ArXiv:2406.17557 [cs]. Rylan Schaeffer. 2023. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. 2024. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759. Leon Strømberg-Derczynski, Manuel Ciosici, Rebekah Baglini, Morten H. Christiansen, Jacob Aarup Dalsgaard, Riccardo Fusaroli, Peter Juel Henrichsen, Rasmus Hvingelby, Andreas Kirkedal, Alex Speed Kjeld-"
        },
        {
            "title": "A Author Contributions",
            "content": "Table 3 shows authorship contributions across different categories. All authors have agreed to the final version of the print. Conceptualization Idea, narrative, planning Kenneth Enevoldsen Jan Kostkan Desmond Elliott Kristoffer Nielbo Kristian Nørgaard Jensen Data curation Download, select, process Kenneth Enevoldsen Kristian Nørgaard Jensen Jan Kostkan Balázs Szabó Peter Vahlstrup Dataset Comparison Model training, evaluation Andrea Blasi Núñez Gianluca Barmina Jacob Nielsen Rasmus Larsen Lukas Galke Peter Schneider-Kamp Writing Original draft Kenneth Enevoldsen Writing Review and editing Kenneth Enevoldsen Márton Kardos Kristoffer Nielbo Per Møldrum Dalum Data review Licensing Per Møldrum Dalum Software CI, testing Kenneth Enevoldsen Kristian Nørgaard Jensen Dataset Contributors Curation, annotation, documentation Kirseten Vad Supervision and Funding Ideation, planning Peter Schneider-Kamp Kristoffer Nielbo Table 3: Paper contributions."
        },
        {
            "title": "B Dataset Overview",
            "content": "Table 4 shows the overview of datasets within Danish Dynaword, including short description, license, and size. For an updated view, we recommend checking out the Huggingface repository"
        },
        {
            "title": "C Trained Models",
            "content": "Table 5 provides an overview of the trained models for the dataset comparison including access links. Despite models are limited in size to 1B, the results show that Danish Dynaword yields gains on 7 out of 9 downstream tasks."
        },
        {
            "title": "D Downstream Evaluation",
            "content": "Tables 6 and 7 report the scores of the continually pre-trained models on the Danish part of the EuroEval benchmark (Nielsen, 2023). Results show the expected gains 14https://huggingface.co/datasets/danish-foundation-models/danish-dynaword 9 Source Description Size License Legal Cellar retsinformation.dk Skat.dk Domsdatabasen.dk Retspraksis EUR-lex SUM Social Media Heste-nettet.dk Spoken Opensubtitles FT Danske Taler Spont NAAT Web EU legal documents and open data The legal information system of Denmark The Danish tax authority website Selected judgments from the Danish courts Case law or judicial practice in Denmark EU legislation with summaries 1.15B CC-BY-SA 4.0 818.25M Copyright Law 122.11M CC-0 86.35M Copyright Law 56.26M CC-0 31.37M CC-BY-SA 4.0 Danish Debate forum Movie Subtitles from OpenSubtitles Meeting records from the Danish parliament Speeches from dansketaler.dk Conversational samples from research project Danish speeches from 1930389.32M CC-0 271.60M CC-0 114.09M CC-0 CC-0 8.72M CC-0 1.56M 286.68K CC-0 The Danish subsection of Wikipedia The Danish subsection of Europarl The Danish subsection of Wikisource Environmental Reports from Miljøportalen Publication of the Ministry of Finance Danish content from Norwegian institutions Municipal Websites Municipality websites from AI-aktindsigt Miljoeportalen FM Udgivelser NCC Maalfrid Medical Health Hovedstaden Guidelines and info. documents for healthcare Encyclopedic Wikipedia Europarl WikiSource Books and Novels NCC Books MeMo ADL Grundtvig Gutenberg WikiBooks JVJ News Nordjylland News TV2R NCC Newspapers Dialect Botxt Synnejysk.dk Other NCC Parliament Nota DanNet Religious texts DDT OCRed Danish books Novels from the Modern Breakthrough Danish literature from 1700-2023 The complete works of N.F.S. Grundtvig Books from Project Gutenberg The Danish Subsection of Wikibooks The works of Johannes V. Jensen OCRed Danish from the Norwegian parliament The text segment from readaloud data Danish WordNet Religious text from the 1700-2022 The Danish Dependency Treebank Articles from Newspaper TV2 Nord Articles from TV2R OCRd Newwspapers from NCC Dictionary of the dialect Bornholmsk Dataset of the dialect Sønderjysk Total 139.23M Apache 2.0 127.38M CC-0 50.34M CC-BY-SA 4.0 29.26M NLOD 2.0 27.07M CC122.00M CC-0 100.84M CC-0 CC-0 5.34M 531.97M CC-0 113.74M CC-BY-SA 4.0 58.49M CC-0 10.53M CC-0 6.76M 6.24M 3.55M Gutenberg CC-0 CC-BY-SA 4.0 37.90M CC-0 21.67M CC-BY-SA 4.0 1.057M CC-0 847.97K CC-0 CC-0 52.02K 338.87M NLOD 2.0 CC-0 7.30M DanNet 1.0 1.48M 1.24M CC-0 185.45K CC-BY-SA 4. 4.80B Table 4: Overview of the dataset in Danish Dynaword (v1.2.7). Size in Llama 3 tokens. 10 Model name checkpoint trained on gemma-3-1b-cpt-gigaword-v1 gemma-3-1b-cpt-dynaword-matched-v1 gemma-3-1b-cpt-dynaword-full-v1 gemma-3-1b-scratch-gigaword-v1 gemma-3-1b-scratch-dynaword-matched-v1 gemma-3-1b-scratch-dynaword-full-v1 gemma-3-1b-pt Danish Gigaword* gemma-3-1b-pt Danish Dynaword* (matched) gemma-3-1b-pt Danish Dynaword* (full) random init. random init. random init. Danish Gigaword* Danish Dynaword* (matched) Danish Dynaword* (full) Table 5: Overview of trained models. *The four validation datasets (DDT, JVJ, Synnejysk, Nordjylland) were excluded from the training data. Task () Score () Reference baseline Gemma-3-1b-pt Continual Pre-training Gigaword* Dynaword* (matched) Dynaword* (full) angry-tweets Sentiment (MCC) dansk NER (Micro F1) scandiqa-da Reading comp. (F1) da-talemaader Knowledge (Accuracy) da-citizen-tests Knowledge (Accuracy) 37.27 2.28 14.55 0.86 51.80 3.95 23.28 4.58 40.78 3. 36.58 3.04 38.80 1.84 38.80 1.92 15.19 1.47 17.97 1.79 16.30 2.43 48.10 1.46 50.04 3.33 49.07 3.43 25.47 6.19 35.31 5.41 32.19 4.04 39.67 4.03 35.22 4.00 36.22 3.23 Table 6: Downstream results on the Danish subsection of EuroEval(NLU) of Gemma-3-1b models continually pre-trained (bottom) on Gigaword, Dynaword (size-matched to Gigaword), and the full Dynaword dataset. Scores are reported alongside standard error of the mean. Values marked in bold indicate an increase over the Gigaword baseline. *All the evaluation datasets were excluded from all training runs. Task () Score () Reference baseline Gemma-3-1b-pt Continual Pre-training Gigaword* Dynaword* (matched) Dynaword* (full) nordjylland-news Summarization (BertScore) (Rouge-L) hellaswag-da Common sense (Accuracy) scala-da Linguistic acceptability (MCC) 56.93 1.76 11.31 1.60 24.61 2. 0.84 4.43 47.36 3.97 46.40 3.00 47.06 2.86 7.77 0.72 7.59 0.74 7.84 0.69 24.77 2.10 26.84 2.23 24.92 2.28 1.27 6.79 0.74 3.85 1.86 3.66 Table 7: Downstream results on the Danish subsection of EuroEval(NLG) of Gemma-3-1b models continually pre-trained (bottom) on Gigaword, Dynaword (size-matched to Gigaword), and the full Dynaword dataset. Scores are reported alongside standard error of the mean. Values marked in bold indicate an increase over the Gigaword baseline. *All the evaluation datasets were excluded from all training runs."
        }
    ],
    "affiliations": [
        "Aarhus University",
        "The Alexandra Institute",
        "University of Copenhagen",
        "University of Southern Denmark"
    ]
}