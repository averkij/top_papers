{
    "paper_title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
    "authors": [
        "Mikołaj Zieliński",
        "Krzysztof Byrski",
        "Tomasz Szczepanik",
        "Przemysław Spurek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)"
        },
        {
            "title": "Start",
            "content": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing Mikołaj Zieli nski 1, Krzysztof Byrski 2, Tomasz Szczepanik 2, Przemysław Spurek 2,3, 1Poznan University of Technology, Institute of Robotics and Machine Intelligence, ul. Piotrowo 3A, Poznan 60-965, Poland 2Jagiellonian University, Faculty of Mathematics and Computer Science, Łojasiewicza 6, 30-348, Krakow, Poland 3 IDEAS Research Institute mikolaj.zielinski@doctorate.put.poznan.pl 5 2 0 2 4 ] . [ 1 1 3 8 2 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian trainable feature embedding. These embeddings are used to condition NeRF network based on the nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), fast nearest Gaussian search based on modified ray-tracing pipeline. We also integrate multiresolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, localityaware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometrybased editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie) Figure 1: GENIE capabilities. GENIE combines the editability of Gaussians with the neural rendering power of Neural Radiance Fields (NeRF). It enables fine-grained, onthe-fly editing through either manual interaction or meshdriven deformation. Introducion In recent years, we have seen significant development in the field of 3D graphics. It is primarily centered around two key tasks: the reconstruction of objects and scenes in 3D space, and the enhancement of user immersion in terms of manipulation and editing (Wang et al. 2023a; Huang, Yang, and Guibas 2024). Editing capabilities are essential, especially as applications in robotics, virtual environments, and content creation increasingly demand physically grounded simulation (Authors 2024). Tasks like object manipulation, deformable modeling, and physics-aware animation require 3D representations that support intuitive editing and tight integration with physics engines. In the context of scene reconstruction, neural rendering has emerged as prominent and rapidly advancing research. major breakthrough in this domain was the introduction of Neural Radiance Fields (NeRF) (Mildenhall et al. 2020), which transformed photogrammetry by enabling high-fidelity 3D scene reconstruction from sparse collections of 2D images and their associated camera poses. NeRFs combine neural networks with classical graphics techniques, to synthesize photorealistic views from novel perspectives. On the other hand, Gaussian Splatting (GS) (Kerbl et al. 2023) represents recent advancement in 3D scene representation, modeling scenes as collections of Gaussian primitives with associated colour, opacity, and spatial extent. Figure 2: Evolution of two physical simulations. From left to right: (1) rubber duck falling onto pillow and deforming it. (2) pirate flag waving under the influence of wind. Both simulations are performed on our own assets. GS employs discrete set of Gaussians that approximate surfaces through density accumulation. This approach enables extremely fast rendering, but introduces challenges in scenarios requiring view-dependent consistency and resolution scaling (Malarz et al. 2025). In particular, when applying super-resolution or scaling transformations, gaps may appear between Gaussian components due to the inherently discrete nature of the representation. In contrast, NeRFs avoid such artifacts, making them more suitable for applications that require seamless surface continuity, such as geometry merging or fine-scale detail preservation (Mildenhall et al. 2020). Furthermore, NeRFs are typically more robust in modeling complex lighting effects and maintaining photorealistic consistency across novel viewpoints, especially under limited training data (Martin-Brualla et al. 2021). Physics simulation enables object manipulation, collision detection, and realistic movement, which vanilla NeRF alone does not provide. Despite these needs, current NeRF representations offer limited editing capabilities. Recent works such as RIP-NeRF (Wang et al. 2023b), NeuralEditor (Chen, Lyu, and Wang 2023), and PAPR (Zhang et al. 2023) employ 3D point clouds for conditioning. Alternatively, methods like NeRF-Editing (Yuan et al. 2022b) and NeuMesh (Yang et al. 2022) use mesh faces to control NeRF representations. In (Monnier et al. 2023), the authors model primitives as textured superquadric meshes for physics-based simulations. While these approaches introduce forms of manual editing, they remain limited in scope and are typically constrained to coarse modifications. However, representing an object using primitives allows for direct manipulation in manner analogous to mesh vertices, enabling intuitive, fine-grained, and real-time editing. This representation has proven highly amenable to interactive modification (Guedon and Lepetit 2024; Waczynska et al. 2024; Gao et al. 2025; Huang et al. 2024), and its compatibility with physics engines (Xie et al. 2024; Borycki et al. 2024) opens the door to dynamic scene manipulation and physically grounded simulation. In this work, we explore the potential of combining NeRF with primitive-based representations to enable object manipulation and physical simulation. This means that we can use all universal simulation methods, including highly developed external tools such as Blender (Community 2018), to create simulations and easily assign the characteristics of given material (plasticity, material physics). To our knowledge, no previous NeRF-based approach has demonstrated this level of integration with physical simulation frameworks, especially for large scenes. We demonstrate that our method yields superior visual and numerical results compared to existing NeRF-based methods. In conclusion, the main contributions of this paper are as follows: GENIE hybrid architecture enabling the use of existing GS editing techniques for NeRF scene manipulation. We introduce Splash Grid Encoding, multi-resolution encoding that conditions NeRF on spatially-selected Gaussians. We propose an approximate algorithm for nearest neighbor search, referred to as Ray-Traced Gaussian Proximity Search (RT-GPS) for computational overhead reduction, which enables fast and scalable inference. Related Works Several approaches focus on modeling deformation or displacement fields at per-frame level (Park et al. 2021a,b; Tretschk et al. 2021; Weng et al. 2022), while others aim to capture continuous motion over time by learning timedependent 3D flow fields (Du et al. 2021; Gao et al. 2021; Guo et al. 2023; Cao and Johnson 2023). substantial body of research has also explored NeRFbased scene editing across different application domains. This includes methods driven by semantic segmentation or labels (Bao et al. 2023; Dong and Wang 2023; Haque et al. 2023; Mikaeili et al. 2023; Song et al. 2023; Wang et al. 2022), as well as techniques that enable relighting and texture modification through shading cues (Gong et al. 2023; Liu et al. 2021; Rudnev et al. 2022; Srinivasan et al. 2021). Other efforts support structural changes in the scene, such as inserting or removing objects (Kobayashi, Matsumoto, and Sitzmann 2022; Lazova et al. 2023; Weder et al. 2023), while some are tailored specifically for facial editing (Hwang et al. 2023; Jiang et al. 2022; Sun et al. 2022) or physics-based manipulation from video sequences (Hofherr et al. 2023; Qiao, Gao, and Lin 2022) Geometry editing within the NeRF framework has received considerable attention (Kania et al. 2022; Yuan et al. 2022a, 2023; Zheng, Lin, and Xu 2023). Our model uses geometry editing and physics simulations. Existing methods leverage various geometric primitives for conditioning NeRFs, most notably 3D point clouds. For instance, RIP-NeRF (Wang et al. 2023b) introduces rotation-invariant point-based representation that enables fine-grained editing and cross-scene compositing by decoupling the neural field from explicit geometry. NeuralEditor (Chen, Lyu, and Wang 2023) adopts point cloud as the structural backbone and proposes voxel-guided rendering scheme to facilitate precise shape deformation and scene morphing. Similarly, PAPR (Zhang et al. 2023) learns parsimonious set of scene-representative points enriched with learned features and influence scores, enabling geometry editing and appearance manipulation. Some approaches leverage explicit mesh representations to enable NeRF editing. NeRF-Editing (Yuan et al. 2022b) extracts mesh from the scene and allows users to apply traditional mesh deformations, which are then transferred to the implicit radiance field by bending camera rays through proxy tetrahedral mesh. Similarly, NeuMesh (Yang et al. 2022) encodes disentangled geometry and texture features at mesh vertices, enabling mesh-guided geometry editing and texture manipulation. To reduce computational complexity, some approaches rely on simplified geometry proxies, such as coarse meshes paired with cage-based deformation techniques (Jambon et al. 2023; Peng et al. 2022; Xu and Harada 2022). VolTeMorph (Garbin et al. 2024) introduces an explicit volume deformation technique that supports realistic extrapolation and can be edited using standard software, enabling applications such as physics-based object deformation and avatar animation. PIE-NeRF (Feng et al. 2024) integrates physics-based, meshless simulations directly with NeRF representations, enabling interactive and realistic animations. All of the aforementioned approaches support manual editing through explicit conditioning representations. In contrast, our method leverages GS-based representation, allowing seamless integration with existing GS editing tools to manipulate NeRF outputs. Preliminary Our method, GENIE, builds on two foundational models: Neural Radiance Fields (NeRF) and Gaussian Splatting (GS). We briefly review both in the following part. Neural Radiance Fields Vanilla NeRF (Mildenhall et al. 2020) represents 3D scene as continuous volumetric field by learning function that maps spatial location = (x, y, z) and viewing direction = (θ, ψ), to an emitted colour = (r, g, b) and volume density σ. Formally, the scene is approximated by multi-layer perceptron (MLP): FNeRF(x, d; Θ) = (c, σ), where Θ denotes the trainable network parameters. The model is trained using set of posed images by casting rays from each camera pixel into the scene and accumulating colour and opacity along each ray based on volumetric rendering principles. The goal is to minimize the difference between the rendered and ground-truth images, allowing the MLP to implicitly encode both the geometry and appearance of the 3D scene. To improve scalability and spatial precision, many NeRF variants adopt the Hash Grid Encoding (Muller et al. 2022), which captures high-frequency scene details by dividing space into multiple Levels of Detail (LoD), each with trainable parameters Φ and feature vectors . These levels vary in resolution, allowing the encoding to represent Figure 3: Examples of physical simulations. From top to bottom: (1) Rigid body simulation of falling leaves from the NeRF Synthetic Ficus plant. (2) Soft body simulation deforming the NeRF Synthetic Lego dozer. (3) Cloth simulation of fabric falling onto cup from our custom asset collection. The middle column shows the driving mesh deformations. both coarse and fine details. For query point x, the output feature vector is obtained by concatenating trilinearly interpolated features from all levels, based on xs position within the grid. Henc(x; Φ) = v(x). Gaussian Splatting The GS technique models 3D scene as set of three-dimensional Gaussian primitives. Each Gaussian is defined by centroid position, covariance matrix, an opacity scalar, and colour information encoded via spherical harmonics (SH) (Kerbl et al. 2023). This method builds radiance field by iteratively optimizing the Gaussian parameters: position, covariance, opacity, and SH colour coefficients. The efficiency of GS largely stems from its rendering process, which projects these Gaussian components onto the image plane. Formally, the scene is represented by dense collection of Gaussians: GGS = {(N (µi, Σi), σi, ci)}n where mi is the centroid location, Σi is the covariance matrix capturing anisotropic shape, σi denotes opacity, and ci contains the SH colour coefficients of the i-th Gaussian. i=1 , The optimization alternates between rendering images from the current Gaussian parameters and comparing them to the corresponding training views. Figure 4: Model overview. Top: During training, subset of Gaussians is selected using Ray-Traced Gaussian Proximity Search (RT-GPS), which also handles pruning based on Gaussian confidence. The selected Gaussians are passed to Splash Grid Encoding, which interpolates their features and drives the densification process by inserting new Gaussians as needed. The interpolated features are then processed by the neural network FGEN IE to predict colour and opacity σ, which are used for volumetric rendering. Bottom: At inference, the learned Gaussians serve as input and can undergo manual or physics-driven edits. The modified Gaussians are passed through the same rendering pipeline to produce the final image. Gaussian Splatting can be easily modified in meshbased fashion (Guedon and Lepetit 2024; Waczynska et al. 2024; Gao et al. 2025; Huang et al. 2024). In practice, this involves moving the Gaussian components directly in the 3D space. Proposed Method Our model, called GENIE, integrates Gaussian representation of shape and neural network-based rendering procedure into single system. Specifically, we use set of Gaussian components GGS, where we replace the original colour vector with trainable latent feature vector Rn, similar to the approach in (Govindarajan et al. 2024). We refer to this modified set of Gaussians as GGEN IE. To allow efficient training of anisotropic Gaussians, we adopt the standard factorization Σ = RSST RT , where is rotation matrix and is diagonal scale matrix. We use NeRF-based neural network FGEN IE to predict colour and opacity from the nearest Gaussian features. Formally, the model is defined as: GEN IE(x, d; GGEN IE, Θ, Φ) = = FGEN IE(Genc(RT-GPS(x, GGEN IE)), d) = (c, σ), where Θ and Φ denote the trainable network parameters. The model, alongside the standard NeRF input, takes set of trainable Gaussians GGEN IE and outputs colour and density σ at any query point, enabling neural rendering conditioned on nearby Gaussian features. Splash Grid Encoding The Hash Grid Encoding, although effective for encoding static scenes, does not support meaningful modifications. This is because altering the grid at lower LoD affects the resulting feature differently than modifying the higher-resolution levels. Consequently, editing the scene becomes inconsistent and unintuitive. To address this, we propose Splash Grid Encoding, novel encoding mechanism that decouples feature representation from grid vertices and instead ties it to set of Gaussians. Our method takes as input set of query points and set of Gaussians GGEN IE, and produces multi-LoD features. Formally, we define this encoding as: Genc (x, GGEN IE, Henc(µ; Φ)) = v(GGEN IE) Unlike the traditional Hash Grid Encoding, where the output depends directly on the query point x, here the features are derived from nearby Gaussians. This is achieved by selecting the closest Gaussians to using our RT-GPS algorithm (detailed in the following section). The final feature vector is computed as weighted interpolation of features assigned to these Gaussians, using modified Mahalanobis distance: (GGEN IE) = (cid:88) i=1 wi(GGEN IE) Henc(µi; Φ), wi(x) = (cid:26)exp (cid:0) 1 2 (x µi)Σ1 0, (x µi)(cid:1) , if otherwise , where wi(x) denotes the interpolation weight, is the maximum number of nearest neighbors considered, and Σi = diag(exp(ci)) R33 is the diagonal covariance matrix parameterized for numerical stability via the log-domain vector ci R3. The features Henc(µi; Φ) are generated from trainable hash-grid encoding and depend on the current Gaussian parameters. During training, both the hash grid parameters Φ and the Gaussian means µi are updated jointly, allowing the Gaussians to explore the multi-LoD feature field and shape the encoding. At inference time, the Gaussians positions are frozen but can be manipulated. Since the interpolation scheme remains unchanged, any modification to Gaussian parameters leads to modifications in the output renderings. Ray-Traced Gaussian Proximity Search Since nearest neighbor search is the bottleneck of our method, we employ an efficient approximation scheme. We observe that excluding certain Gaussians from the neighborhood set introduces only bounded error ε in the interpolated feature vector v(GGEN IE), which we formally derive in the Appendix A. RT-GPS method extends the RT-kNNS algorithm (Nagarajan, Mandarapu, and Kulkarni 2023), which finds neighbors within fixed radius by checking if query points lie inside expanded spheres. We adapt this by assigning each Gaussian an individual radius based on its covariance, ri = max {λ σ(Σi)} , where σ(Σi) is the set of eigenvalues of the Gaussians covariance matrix and is configurable quantile. This ensures we only consider Gaussians whose confidence ellipsoids are likely to influence the feature at x. Following (Nagarajan, Mandarapu, and Kulkarni 2023), we trace rays from and collect Gaussians whose confidence spheres intersect the ray exactly once (Figure 5). sorted buffer maintains the closest candidates based on mean distance, and in case of overflow, the set is refined by rerunning the traversal with retained neighbors. To limit traversal cost, we set the maximum ray distance to tmax = 2 max i= {Q max {λ σ(Σi)}} , which guarantees that no significant Gaussians are skipped. Pruning and Densification For densification, we adopt the strategy proposed in (Xu et al. 2022), defined as: αi = 1 exp(σii), = arg max αi, where αi is the opacity at sample along ray, is the spacing between samples, and is the index of the maximum-opacity point. new Gaussian is added at the shading location with the highest opacity only if its distance to existing closest Gaussians exceeds predefined spatial threshold τs, and its opacity value is above an opacity threshold τα. Unlike (Xu et al. 2022), we initialize features by sampling from the hash grid, rather than nearby shading information, ensuring better alignment with the feature field. For pruning, we maintain confidence vector = [c1, . . . , cn] with ci [0, 1]. At each iteration, all values decay by factor λd < 1, while Gaussians selected as neighbors are incremented by growth factor λg > 1: (cid:26)min(1, λg ci), if (µ, Σ) ci max(0, λd ci), otherwise . Figure 5: The RT-GPS working principle. light ray passing through the scene is illustrated, along with its intersections with the icosahedrons. The figure highlights which Gaussians are considered neighbors and which are excluded. This observation serves as the starting point for our approximated nearest neighbor finding method: Ray-Traced Gaussian Proximity Search (RT-GPS). RT-GPS restricts nearest neighbor candidates to Gaussians whose confidence ellipsoids (defined by quantile parameter Q) contain the query point x. This reduces neighbor search to pointin-sphere test, which we approximate using circumscribed icosahedrons for efficient computation. Gaussians with ci < τ are periodically. Editing Thanks to the feature encoding in Splash Grid Encoding, we regularize the models latent space around the spatial configuration of Gaussian primitives. This alignment allows edits to be performed directly in the coordinate space of Gaussians, effectively making spatial transformations equivalent to latent-space manipulations. In particular, modifying the means of the Gaussians enables localized scene edits that are instantly reflected in the rendered output. Gaussians can be manipulated either individually or indirectly through mesh parametrization. In the latter case, we export the Gaussians as triangle soup by projecting their two principal covariance directions onto triangle faces. Following the reparameterization strategy introduced in GaMeS (Waczynska et al. 2024), we associate these triangles with mesh surfaces, ensuring that Gaussian components move consistently with mesh deformations. All edits are applied in real time, with immediate visual feedback. Since the latent feature space is directly tied to Gaussian positions and attributes, the edits require no further fine-tuning or postprocessing, making them persistent and semantically meaningful. Experiments We design our experiments to demonstrate that GENIE maintains the reconstruction quality of state-of-the-art (SOTA) methods while enabling complex object modifications. Datasets Following prior work, we evaluate on the NeRFSynthetic dataset (Mildenhall et al. 2020), which contains eight synthetic scenes with diverse geometry, texture, and specular properties. Existing methods (Govindarajan et al. 2024; Xu et al. 2022; Wang et al. 2023b) typically operate in bounded regions and do not support unbounded scenes. In contrast, GENIE is the first editable NeRF model trained on the challenging Mip-NeRF 360 dataset (Barron et al. 2022), comprising five outdoor and four indoor real-world 360scenes. To further demonstrate editing capabilities, we include the fox scene from Instant-NGP (Muller et al. 2022), and introduce custom set of 3D assets with deformable and articulated objects, enabling dynamic scene editing and physical interaction. Baselines We compare GENIE against both static NeRFbased and editable point-based/Gaussian-based representations. For static radiance field models, we consider NeRF (Mildenhall et al. 2020), Nerfacto (Tancik et al. 2023), VolSDF (Yariv et al. 2021), ENVIDR (Liang et al. 2023), Plenoxels (Fridovich-Keil et al. 2022), GS, LagHash (Govindarajan et al. 2024), Mip-NeRF 360 (Barron et al. 2022), Instant-NGP (Muller et al. 2022), which are known for their high reconstruction quality, but lack support for scene editing. For editable models we compare ourselves with RIPNeRF (Wang et al. 2023b) and NeurlaEditor (Chen, Lyu, and Wang 2023). We select these baselines to demonstrate that GENIE not only achieves comparable or superior reconstruction quality to SOTA methods, but also introduces significantly more expressive and flexible editing capabilities. Implementation Details To reduce computation, we fix the rotation matrix to identity and restrict the covariance Σ to diagonal form, avoiding costly matrix inversions. The log-diagonal of Σ is initialized to 0.0001. For Splash Grid Encoding, we use quantiles [1, 3] and select 1632 nearest Gaussians per query. Densification runs periodically from early training until midpoint, adding up to 10,000 Gaussians per cycle. We use an opacity threshold τα = 0.5 and spatial threshold τs = 0.001. Pruning is performed every 1,000 steps. Confidence values decay via λd = 0.001 and increase via λg = 0.01. GausFigure 6: Example edits on real-world scenes. From top to bottom: (1) Manual edit of the fox scene from (Muller et al. 2022), where the head is rotated from left to right. (2) Physics-based simulation in the Garden scene from MipNeRF 360, showing an object falling onto tilted table and bouncing off. (3) Bottom row: physics simulation in the kitchen scene from Mip-NeRF 360, where force is applied to deform plasticine dozer. sians with confidence < τ = 0.1 are removed. Models are trained for 20,000 steps. For initialization on the NeRF-Synthetic dataset, we used Gaussians generated by the LagHash method. For the MipNeRF 360 dataset, we initialized GENIE using structurefrom-motion reconstructions from COLMAP (Schonberger and Frahm 2016), and further augmented the scene with an additional 1M points distributed along the scene boundaries to improve background reconstruction. For our custom assets, we initialized the Gaussians using the mesh vertices. For physics simulations, we used meshes generated with Permuto-SDF (Rosu and Behnke 2023) but also simple cage meshes for real scenes. All experiments were run on single NVIDIA RTX (24 GB) GPU. Quantitative Results We present quantitative results on the NeRF-Synthetic dataset in Table 1. As shown, GENIE achieves reconstruction quality comparable to SOTA noneditable methods. Among these, 3DGS performs best in terms of pure reconstruction fidelity. In the editable category, our method significantly outperforms RIP-NeRF in six out of eight scenes, and performs on par in the remaining two. For real-world scenes, we report PSNR on the Mip-NeRF 360 dataset in Table 2, where Mip-NeRF achieves the highest reconstruction quality. However, to the best of our knowledge, none of the existing methods allow editing in this setting. GENIE is the only approach that enables scene editing on these unbounded, complex real-world scenes. h u g c Static i a d s F S 33.00 25.01 32.54 32.91 29.62 28.65 36.18 30.13 NeRF 27.81 17.96 21.57 24.97 20.35 19.86 30.14 21.91 Nerfacto VolSDF 30.57 20.43 29.46 30.53 29.13 25.51 35.11 22.91 ENVIDR 31.22 22.99 29.55 32.17 29.52 21.57 31.44 26.60 Plenoxels 33.98 25.35 34.10 33.26 29.14 29.62 36.43 31.83 35.82 26.17 35.69 35.34 30.00 30.87 37.67 34.83 GS LagHash 35.66. 25.68 35.49 36.71 29.60 30.88 37.30 33.83 Editable RIP-NeRF 34.84 24.89 33.41 34.19 28.31 30.65 35.96 32.23 34.67 25.57 33.84 34.56 29.43 29.35 36.45 33.23 GENIE Table 1: Quantitative comparisons (PSNR) on NeRFSynthetic dataset showing that GENIE gives comparable results with other models. y Outdoor scenes e r fl u Static h t o Indoor scenes n n t a b 22.17 20.65 25.07 23.47 22.37 29.69 26.69 29.48 30.69 INGP 17.86 17.79 20.82 20.48 16.72 24.22 23.59 23.20 21.55 Nerfacto Mip-NeRF 24.37 21.73 26.98 26.40 22.87 31.63 29.55 32.23 33.46 25.25 21.52 27.41 26.55 22.49 30.63 28.70 30.32 31.98 GS Editable GENIE 19.47 18.14 22.29 20.04 16.34 28.57 24.98 25.69 25.94 Table 2: The quantitative comparisons of reconstruction capability (PSNR) on Mip-NeRF 360 dataset. GENIE is the only one editable representation to work on open scenes. Qualitative Results For the qualitative comparison, we utilized results reported by (Chen, Lyu, and Wang 2023), where objects from the NeRF-Synthetic dataset were modified to evaluate editing performance. The visual quality of the edits was assessed across different methods. We observe that GENIE outperforms other approaches in the task of zero-shot editing, producing visibly higher-quality results. In particular, it more accurately reconstructs lighting reflections in the Mic scene, handles stretching in Drums more naturally, and introduces fewer artifacts in shadowed regions of Hotdog and Lego. The comparison is shown in Figure 7. Point-NeRF appears in the comparison as it was adapted to support editing by the authors of the NeuralEditor method. Physic-based Editing We conducted range of physicsbased simulations in Blender (Community 2018) using the previously described mesh-driven editing mechanism. Our experiments span multiple datasets, including both synthetic and real-world scenes, and cover various physical phenomena such as rigid body dynamics, soft body deformation, and cloth simulation. For each simulation, the deformation of the underlying driving mesh was used to update the corresponding Gaussian components in real time, allowing seamless integration of physical interactions into the scene. The results of these simulations are illustrated in FigFigure 7: Qualitative comparison. Results shown on the NeRF-Synthetic dataset. Modified objects are in the top row. Each row compares reconstruction quality across different methods. Our results are added to those reported by (Chen, Lyu, and Wang 2023). ures 2, 3, and 6. These visualizations demonstrate that GENIE produces realistic and physically plausible edits across wide range of scenarios. Whether simulating leaves falling from plant, squashing soft object, or draping cloth over complex geometry, our method maintains high rendering fidelity while enabling expressive and controllable scene manipulation. This highlights the potential of GENIE as flexible framework for neural scene editing driven by physical interactions. Conclusions In this work, we introduced GENIE, Gaussian-based conditioning technique for NeRF systems that enables dynamic and physics-driven editing. Our method conditions NeRF network on jointly trained Gaussians that serve as spatial feature carriers. Editing can be performed either manually, through direct manipulation of Gaussians, or automatically, by coupling them with deformable meshes to enable physics-based interactions. We demonstrated the capabilities of our system across range of scenarios, highlighting its usability, versatility, and adaptability. GENIE can be seamlessly integrated into new simulation environments, offering promising path toward physically interactive neural scene representations. Limitations The detail reconstruction quality in our system depends on the spatial density of Gaussians. Sparse regions may lose fine details, posing challenges in large or open scenes where maintaining uniform density is difficult. Social Impact Our method lowers the barrier to editing realistic 3D content, enabling broader use in areas like VR/AR, education, and visualization. However, like other generative tools, it may be misused. We advocate for responsible use and content authentication. Acknowledgments The work of P. Spurek was supported by the National Centre of Science (Poland) Grant No. 2023/50/E/ST6/00068. The work of M. Zielinski was supported by the National Science Centre, Poland, under research project no UMO-2023/51/B/ST6/01646. References Authors, G. 2024. Genesis: Universal and Generative Physics Engine for Robotics and Beyond. Bao, C.; Zhang, Y.; Yang, B.; Fan, T.; Yang, Z.; Bao, H.; Zhang, G.; and Cui, Z. 2023. Sine: Semantic-driven imageIn Probased nerf editing with prior-guided editing field. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2091920929. Barron, J. T.; Mildenhall, B.; Verbin, D.; Srinivasan, P. P.; and Hedman, P. 2022. Mip-NeRF 360: Unbounded AntiAliased Neural Radiance Fields. CVPR. Borycki, P.; Smolak, W.; Waczynska, J.; Mazur, M.; Tadeja, S.; and Spurek, P. 2024. Gasp: Gaussian splatting for physicbased simulations. arXiv preprint arXiv:2409.05819. Cao, A.; and Johnson, J. 2023. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 130141. Chen, J.-K.; Lyu, J.; and Wang, Y.-X. 2023. Neuraleditor: Editing neural radiance fields via manipulating point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1243912448. Community, B. O. 2018. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam. Dong, J.; and Wang, Y.-X. 2023. Vica-nerf: Viewconsistency-aware 3d editing of neural radiance fields. Advances in Neural Information Processing Systems, 36: 6146661477. Du, Y.; Zhang, Y.; Yu, H.-X.; Tenenbaum, J. B.; and Wu, J. 2021. Neural radiance flow for 4d view synthesis and video In 2021 IEEE/CVF International Conference processing. on Computer Vision (ICCV), 1430414314. IEEE Computer Society. Feng, Y.; Shang, Y.; Li, X.; Shao, T.; Jiang, C.; and Yang, Y. 2024. Pie-nerf: Physics-based interactive elastodynamics with nerf. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 44504461. Fridovich-Keil, S.; Yu, A.; Tancik, M.; Chen, Q.; Recht, B.; and Kanazawa, A. 2022. Plenoxels: Radiance Fields without Neural Networks. In CVPR. Gao, C.; Saraf, A.; Kopf, J.; and Huang, J.-B. 2021. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 57125721. Gao, X.; Li, X.; Zhuang, Y.; Zhang, Q.; Hu, W.; Zhang, C.; Yao, Y.; Shan, Y.; and Quan, L. 2025. Mani-gs: Gaussian splatting manipulation with triangular mesh. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2139221402. Garbin, S. J.; Kowalski, M.; Estellers, V.; Szymanowicz, S.; Rezaeifar, S.; Shen, J.; Johnson, M. A.; and Valentin, J. 2024. VolTeMorph: Real-time, Controllable and Generalizable Animation of Volumetric Representations. In Computer Graphics Forum, volume 43, e15117. Wiley Online Library. Gong, B.; Wang, Y.; Han, X.; and Dou, Q. 2023. Recolornerf: Layer decomposed radiance fields for efficient color editing of 3d scenes. In Proceedings of the 31st ACM International Conference on Multimedia, 80048015. Govindarajan, S.; Sambugaro, Z.; Shabanov, A.; Takikawa, T.; Rebain, D.; Sun, W.; Conci, N.; Yi, K. M.; and Tagliasacchi, A. 2024. Lagrangian hashing for compressed neural field representations. In European Conference on Computer Vision, 183199. Springer. Guedon, A.; and Lepetit, V. 2024. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction In Proceedings of the and high-quality mesh rendering. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 53545363. Guo, X.; Sun, J.; Dai, Y.; Chen, G.; Ye, X.; Tan, X.; Ding, E.; Zhang, Y.; and Wang, J. 2023. Forward flow for novel In Proceedings of the view synthesis of dynamic scenes. IEEE/CVF International Conference on Computer Vision, 1602216033. Haque, A.; Tancik, M.; Efros, A. A.; Holynski, A.; and Kanazawa, A. 2023. Instruct-nerf2nerf: Editing 3d scenes with instructions. In Proceedings of the IEEE/CVF international conference on computer vision, 1974019750. Hofherr, F.; Koestler, L.; Bernard, F.; and Cremers, D. 2023. Neural implicit representations for physical parameter inference from single video. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 20932103. Huang, I.; Yang, G.; and Guibas, L. 2024. BlenderAlchemy: Editing 3D Graphics with Vision-Language Models. arXiv preprint arXiv:2404.17672. Huang, J.; Xu, S.; Yu, H.; and Lee, T.-Y. 2024. GSDeformer: Direct, Real-time and Extensible Cage-based arXiv preprint Deformation for 3D Gaussian Splatting. arXiv:2405.15491. Hwang, S.; Hyung, J.; Kim, D.; Kim, M.-J.; and Choo, J. 2023. Faceclipnerf: Text-driven 3d face manipulation usIn Proceedings of ing deformable neural radiance fields. the IEEE/CVF International Conference on Computer Vision, 34693479. Jambon, C.; Kerbl, B.; Kopanas, G.; Diolatzis, S.; Leimkuhler, T.; and Drettakis, G. 2023. Nerfshop: Interactive editing of neural radiance fields. Proceedings of the ACM on Computer Graphics and Interactive Techniques, 6(1). Jiang, K.; Chen, S.-Y.; Liu, F.-L.; Fu, H.; and Gao, L. 2022. Nerffaceediting: Disentangled face editing in neural radiance fields. In SIGGRAPH Asia 2022 Conference Papers, 19. Kania, K.; Yi, K. M.; Kowalski, M.; Trzcinski, T.; and Tagliasacchi, A. 2022. Conerf: Controllable neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1862318632. Kerbl, B.; Kopanas, G.; Leimkuhler, T.; and Drettakis, G. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics, 42(4). Kobayashi, S.; Matsumoto, E.; and Sitzmann, V. 2022. Decomposing nerf for editing via feature field distillation. Advances in neural information processing systems, 35: 2331123330. Lazova, V.; Guzov, V.; Olszewski, K.; Tulyakov, S.; and Pons-Moll, G. 2023. Control-nerf: Editable feature volumes for scene rendering and manipulation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 43404350. Liang, R.; Chen, H.; Li, C.; Chen, F.; Panneer, S.; and Vijaykumar, N. 2023. ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting. arXiv preprint arXiv:2303.13022. Liu, S.; Zhang, X.; Zhang, Z.; Zhang, R.; Zhu, J.-Y.; and In Russell, B. 2021. Editing conditional radiance fields. Proceedings of the IEEE/CVF international conference on computer vision, 57735783. Malarz, D.; Smolak-Dyzewska, W.; Tabor, J.; Tadeja, S.; and Spurek, P. 2025. Gaussian splatting with nerf-based color and opacity. Computer Vision and Image Understanding, 251: 104273. Martin-Brualla, R.; Radwan, N.; Sajjadi, M. S. M.; Barron, J. T.; Dosovitskiy, A.; and Duckworth, D. 2021. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In CVPR. Mikaeili, A.; Perel, O.; Safaee, M.; Cohen-Or, D.; and Mahdavi-Amiri, A. 2023. Sked: Sketch-guided text-based 3d editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1460714619. Mildenhall, B.; Srinivasan, P. P.; Tancik, M.; Barron, J. T.; Ramamoorthi, R.; and Ng, R. 2020. NeRF: Representing In Scenes as Neural Radiance Fields for View Synthesis. ECCV. Monnier, T.; Austin, J.; Kanazawa, A.; Efros, A.; and Aubry, M. 2023. Differentiable blocks world: Qualitative 3d decomposition by rendering primitives. Advances in Neural Information Processing Systems, 36: 57915807. Muller, T.; Evans, A.; Schied, C.; and Keller, A. 2022. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4): 1 15. Nagarajan, V.; Mandarapu, D.; and Kulkarni, M. 2023. RT-kNNS Unbound: Using RT Cores to Accelerate Unrestricted Neighbor Search. CoRR, abs/2305.18356. Accepted at the International Conference on Supercomputing 2023 (ICS23). Park, K.; Sinha, U.; Barron, J. T.; Bouaziz, S.; Goldman, D. B.; Seitz, S. M.; and Martin-Brualla, R. 2021a. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, 58655874. Park, K.; Sinha, U.; Hedman, P.; Barron, J. T.; Bouaziz, S.; Goldman, D. B.; Martin-Brualla, R.; and Seitz, S. M. 2021b. HyperNeRF: higher-dimensional representation for topologically varying neural radiance fields. ACM Transactions on Graphics (TOG), 40(6): 112. Peng, Y.; Yan, Y.; Liu, S.; Cheng, Y.; Guan, S.; Pan, B.; Zhai, G.; and Yang, X. 2022. Cagenerf: Cage-based neural radiance field for generalized 3d deformation and animation. Advances in Neural Information Processing Systems, 35: 3140231415. Qiao, Y.-L.; Gao, A.; and Lin, M. 2022. Neuphysics: Editable neural geometry and physics from monocular videos. Advances in Neural Information Processing Systems, 35: 1284112854. Rosu, R. A.; and Behnke, S. 2023. PermutoSDF: Fast MultiView Reconstruction with Implicit Surfaces using Permutohedral Lattices. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Rudnev, V.; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V.; and Theobalt, C. 2022. Nerf for outdoor scene relighting. In European Conference on Computer Vision, 615631. Springer. Schonberger, J. L.; and Frahm, J.-M. 2016. Structure-fromMotion Revisited. In Conference on Computer Vision and Pattern Recognition (CVPR). Song, H.; Choi, S.; Do, H.; Lee, C.; and Kim, T. 2023. Blending-nerf: Text-driven localized editing in neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, 1438314393. Srinivasan, P. P.; Deng, B.; Zhang, X.; Tancik, M.; Mildenhall, B.; and Barron, J. T. 2021. Nerv: Neural reflectance and visibility fields for relighting and view synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 74957504. Sun, J.; Wang, X.; Zhang, Y.; Li, X.; Zhang, Q.; Liu, Y.; and Wang, J. 2022. Fenerf: Face editing in neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 76727682. Tancik, M.; Weber, E.; Ng, E.; Li, R.; Yi, B.; Kerr, J.; Wang, T.; Kristoffersen, A.; Austin, J.; Salahi, K.; Ahuja, A.; McAllister, D.; and Kanazawa, A. 2023. Nerfstudio: Modular Framework for Neural Radiance Field Development. In ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH 23. Tretschk, E.; Tewari, A.; Golyanik, V.; Zollhofer, M.; Lassner, C.; and Theobalt, C. 2021. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of In Proceedings of dynamic scene from monocular video. the IEEE/CVF international conference on computer vision, 1295912970. Waczynska, J.; Borycki, P.; Tadeja, S.; Tabor, J.; and Spurek, P. 2024. Games: Mesh-based adapting and modification of gaussian splatting. arXiv preprint arXiv:2402.01459. Wang, C.; Chai, M.; He, M.; Chen, D.; and Liao, J. 2022. Clip-nerf: Text-and-image driven manipulation of neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 38353844. Wang, X.; Zhu, J.; Ye, Q.; Huo, Y.; Ran, Y.; Zhong, Z.; and Chen, J. 2023a. Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields. In ICCV, 1763717647. IEEE. ISBN 979-8-3503-0718-4. Wang, Y.; Wang, J.; Qu, Y.; and Qi, Y. 2023b. Rip-nerf: Learning rotation-invariant point-based neural radiance field In Proceedings for fine-grained editing and compositing. of the 2023 ACM international conference on multimedia retrieval, 125134. Weder, S.; Garcia-Hernando, G.; Monszpart, A.; Pollefeys, M.; Brostow, G. J.; Firman, M.; and Vicente, S. 2023. Removing objects from neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1652816538. Weng, C.-Y.; Curless, B.; Srinivasan, P. P.; Barron, J. T.; and Kemelmacher-Shlizerman, I. 2022. Humannerf: Freeviewpoint rendering of moving people from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition, 1621016220. Xie, T.; Zong, Z.; Qiu, Y.; Li, X.; Feng, Y.; Yang, Y.; and Jiang, C. 2024. Physgaussian: Physics-integrated 3d In Proceedings of the gaussians for generative dynamics. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 43894398. Xu, Q.; Xu, Z.; Philip, J.; Bi, S.; Shu, Z.; Sunkavalli, K.; and Neumann, U. 2022. Point-nerf: Point-based neural radiance fields. In CVPR, 54385448. Xu, T.; and Harada, T. 2022. Deforming radiance fields with cages. In European Conference on Computer Vision, 159 175. Springer. Yang, B.; Bao, C.; Zeng, J.; Bao, H.; Zhang, Y.; Cui, Z.; and Zhang, G. 2022. Neumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing. In European Conference on Computer Vision, 597614. Springer. Yariv, L.; Gu, J.; Kasten, Y.; and Lipman, Y. 2021. Volume rendering of neural implicit surfaces. In Thirty-Fifth Conference on Neural Information Processing Systems. Yuan, Y.-J.; Lai, Y.-K.; Huang, Y.-H.; Kobbelt, L.; and Gao, L. 2022a. Neural radiance fields from sparse rgb-d images for high-quality view synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(7): 87138728. Yuan, Y.-J.; Sun, Y.-T.; Lai, Y.-K.; Ma, Y.; Jia, R.; and Gao, L. 2022b. Nerf-editing: geometry editing of neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1835318364. Yuan, Y.-J.; Sun, Y.-T.; Lai, Y.-K.; Ma, Y.; Jia, R.; Kobbelt, L.; and Gao, L. 2023. Interactive nerf geometry editing with IEEE Transactions on Pattern Analysis and shape priors. Machine Intelligence, 45(12): 1482114837. Zhang, Y.; Peng, S.; Moazeni, A.; and Li, K. 2023. Papr: Proximity attention point rendering. Advances in Neural Information Processing Systems, 36: 6030760328. Zheng, C.; Lin, W.; and Xu, F. 2023. Editablenerf: Editing topologically varying neural radiance fields by key points. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 83178327. Appendix This appendix provides additional insights and supporting material for our method. We provide formal justification of the k-nearest neighbor approximation used in Ray-Traced Gaussian Proximity Search, showing that distant Gaussians can be safely ignored with bounded error. We then present an extensive ablation study to analyze the impact of key components in our system. Next, we include rendering speed comparisons with existing methods, highlighting the trade-off between editability and performance. We also provide extended qualitative results to showcase the generalization of our approach across various scenes. Finally, we present and discuss representative failure cases to inform future research directions and reveal current limitations of our method. Theoretical Motivation for Ray-Traced Gaussian Proximity Search Approximation To justify the motivation behind our Ray-Traced Gaussian Proximity Search, lets first recall the formula for the interpolated feature vector (GGEN IE). To begin, lets note that for the wi(x) appearing in the formula we have: 2 d2 (x, (µi, Σi))(cid:1) , (cid:26)exp (cid:0) 1 if otherwise, wi(x) = 0, where dM (x, (µi, Σi)) is the Mahalanobis distance of the point from the normal distribution (µi, Σi). Lets fix R3 and ε > 0. Lets consider the subset , such that for each we have: dM (x, (µi, Σi)) > (cid:118) (cid:117) (cid:117) (cid:117) (cid:116)2 ln (cid:80) iM ε v(x)i wi(x) v(x)i (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = Then: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) = (cid:88) iM (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) iM (cid:12) (cid:88) (cid:12)e 1 (cid:12) iM = 1 2 d2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 d2 (x,N (µi,Σi)) v(x)i 2 d2 (x,N (µi,Σi))(cid:12) (cid:12) (cid:12) v(x)i = (x,N (µi,Σi)) (cid:88) iM v(x)i < ε v(x)i (cid:88) iM < (cid:80) iM v(x)i = ε wi(x) v(x)i (cid:88) wi(x) v(x)i Thus: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) iN (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:88) iM wi(x) v(x)i iN (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) < ε (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = from which we conclude that removing the nearest neighbors from the set from the formula for (GGEN IE) can alter the interpolated feature vector coordinate by no more than ε. Ablation study To justify our design choices, we present an ablation study evaluating the impact of key components in our system. We analyze how performance is affected by the number of neighbors used in RT-GPS, using Gaussian scales as radii in RT-GPS (Σ in RT-GPS), the presence of Splash Grid Encoding, enabling densification and pruning, making Gaussian means learnable, and including an appearance embedding. Speed comparisons We compare the rendering performance of various methods in Table 4. For GENIE, we report results for different configurations based on the number of Gaussian components (in millions) and the number of nearest neighbors used to condition the NeRF. For example, GENIE 1.1M, 32 denotes model using approximately 1.1 million Gaussian components and = 32 neighbors. Extended results In this section, we extend the results presented in Tables 1 and 2 of the main paper by additionally reporting SSIM and LPIPS metrics for both synthetic and real-world datasets. Failure cases While our method performs well across variety of scenes and tasks, it is not without limitations. In this section, we present representative failure cases that highlight scenarios where our approach struggles. The first failure mode occurs when the mesh model contains discontinuities caused by editing or undergoes excessive stretching. This can lead to visible holes or rendering artifacts in the final output (see Figure 8). The second case arises when the number of Gaussians is insufficient during training and densification is disabled. In such situations, the network struggles to represent object boundaries accurately, leading to blurry or incomplete reconstructions (see Figure 9). Figure 8: Mesh discontinuity. Mesh discontinuity during the editing causes holes in the edited model especially visible on the left side of the water basin. Figure 9: Too few Gausses. Too few Gausses during initialization and no densification causes network to have problems with proper reconstruction. i n r a n a fi e i P b e # TR Σ GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 1.1 GENIE 2. 16 16 16 16 16 16 16 16 16 16 16 16 32 d b e c a a p A PSNR 32 32 10 5k 10k 10k r o c l e g o c i r C 33.95 24.80 32.80 33.44 28.51 28.81 35.48 32.57 29.27 23.47 25.15 28.72 21.94 OOM 30.19 27.92 33.92 24.84 32.42 33.26 28.26 OOM 34.98 31.87 28.67 23.43 24.95 28.58 21.85 OOM 29.07 27.53 30.59 23.69 27.84 29.03 24.30 OOM 31.52 27.97 33.11 24.89 32.67 32.57 28.43 OOM 35.26 32.11 34.10 24.94 33.02 33.82 21.16 28.91 35.71 32.07 34.12 24.97 33.00 OOM 28.61 OOM 35.66 32.11 32.92 25.08 32.98 22.74 28.15 28.23 35.54 32.43 34.20 25.15 33.09 33.88 29.03 29.43 35.89 32.53 34.29 25.20 33.24 33.93 29.19 29.32 36.12 32.53 34.34 25.24 33.38 34.03 29.18 29.45 36.18 32.54 34.67 25.57 33.84 34.56 29.43 29.35 36.45 33.23 Table 3: Ablation study (PSNR) comparisons on NeRF-Synthetic dataset showing that GENIE final system gives the best results. It can be observed that without Splash Grid Encoding system was sometimes giving the Our of Memory (OOM) errors. N a N S F S e I N N K 6 1 , 0 0 8 E 2 3 , 1 . 1 E FPS 0.023 0.860 0.909 3.330 0.020 0.042 0.815 10.66 0.301 0.089 Table 4: Rendering speed comparison on the NeRFSynthetic dataset. Despite its editability features, GENIE achieves competitive inference speeds. Performance varies with the number of Gaussian components and neighbors used. For instance, GENIE 1.1M, 32 refers to using approximately 1.1 million Gaussians and = 32 neighbors in the weighted conditioning. Chair Drums Lego Mic Materials Ship Hotdog Ficus PSNR NeRF Nerfacto VolSDF ENVIDR Plenoxels GS LagHash 33.00 27.81 30.57 31.22 33.98 35.82 35.66. 25.01 17.96 20.43 22.99 25.35 26.17 25.68 RIP-NeRF GENIE 34.84 34.67 24.89 25. NeRF Nerfacto VolSDF ENVIDR Plenoxels GS LagHash 0.967 0.951 0.949 0.976 0.977 0.987 0.984 0.925 0.835 0.893 0.930 0.933 0.954 0.934 RIP-NeRF GENIE 0.980 0.981 0.929 0. NeRF Nerfacto VolSDF ENVIDR Plenoxels GS LagHash 0.046 0.056 0.056 0.031 0.031 0.012 0.024 0.091 0.197 0.119 0.080 0.067 0.037 0.083 RIP-NeRF GENIE - 0.013 - 0. Static 32.54 32.91 21.57 24.97 29.46 30.53 29.55 32.17 34.10 33.26 35.69 35.34 35.49 36.71 Editable 33.41 34.19 33.84 34.56 SSIM Static 0.961 0.980 0.893 0.959 0.951 0.969 0.961 0.984 0.975 0.985 0.983 0.991 0.978 0.991 Editable 0.962 0.987 0.977 0.973 LPIPS Static 0.050 0.028 0.112 0.075 0.054 0.191 0.054 0.021 0.028 0.015 0.016 0.006 0.015 0.027 Editable - 0.005 - 0.016 29.62 20.35 29.13 29.52 29.14 30.00 29. 28.31 29.43 0.949 0.771 0.954 0.968 0.949 0.960 0.947 0.943 0.950 0.063 0.405 0.048 0.045 0.057 0.034 0.070 - 0.037 28.65 19.86 25.51 21.57 29.62 30.87 30. 30.65 29.35 0.856 0.797 0.842 0.855 0.890 0.907 0.892 0.916 0.880 36.18 30.14 35.11 31.44 36.43 37.67 37.30 30.13 21.91 22.91 26.60 31.83 34.83 33.83 35.96 36. 32.23 33.23 0.974 0.951 0.972 0.963 0.980 0.985 0.981 0.964 0.915 0.929 0.987 0.976 0.987 0.981 0.963 0.979 0.979 0.979 0.206 0.218 0.191 0.228 0.134 0.106 0. 0.121 0.029 0.043 0.072 0.037 0.020 0.036 0.044 0.112 0.068 0.010 0.026 0.012 0.049 - 0.110 - 0.022 - 0.015 Table 5: Quantitative comparisons (PSNR, SSIM, LPIPS) on NeRF-Synthetic dataset showing that GENIE gives comparable results with other models. PSNR Outdoor scenes bicycle flowers garden stump treehill Plenoxels INGP Nerfacto Mip-NeRF GS 21.91 22.17 17.86 24.37 25.25 20.10 20.65 17.79 21.73 21. Static 23.49 25.07 20.82 26.98 27.41 20.66 23.47 20.48 26.40 26.55 22.25 22.37 16.72 22.87 22.49 Editable Indoor scenes room counter kitchen bonsai 27.59 29.69 24.22 31.63 30.63 23.62 26.69 23.59 29.55 28.70 23.42 29.48 23.20 32.23 30.32 24.66 30.69 21.55 33.46 31.98 GENIE 19. 18.14 22.29 20.04 16.34 28.57 24. 25.69 25.94 SSIM Outdoor scenes bicycle flowers garden stump treehill Static Plenoxels INGP Nerfacto Mip-NeRF GS 0.496 0.512 0.548 0.685 0.771 0.431 0.486 0.495 0.583 0.605 0.606 0.701 0.559 0.813 0.868 0.523 0.594 0.657 0.744 0.775 0.509 0.542 0.591 0.632 0. Editable Indoor scenes room counter kitchen bonsai 0.842 0.871 0.815 0.913 0.914 0.759 0.817 0.773 0.894 0.905 0.648 0.858 0.692 0.920 0.922 0.814 0.906 0.728 0.941 0. GENIE 0.381 0.329 0.465 0.424 0. 0.834 0.718 0.655 0.736 LPIPS Outdoor scenes bicycle flowers garden stump treehill Plenoxels INGP Nerfacto Mip-NeRF GS 0.506 0.446 0.657 0.301 0.205 0.521 0.441 0.668 0.344 0.336 Static 0.386 0.257 0.595 0.170 0. 0.503 0.421 0.501 0.261 0.210 0.540 0.450 0.630 0.339 0.317 Editable Indoor scenes room counter kitchen bonsai 0.419 0.261 0.365 0.211 0.220 0.441 0.306 0.365 0.204 0. 0.447 0.195 0.239 0.127 0.129 0.398 0.205 0.299 0.176 0.205 GENIE 0.687 0.639 0. 0.605 0.641 0.232 0.314 0.315 0. Table 6: The quantitative comparisons of reconstruction capability (PSNR, SSIM, LPIPS) on Mip-NeRF 360 dataset."
        }
    ],
    "affiliations": [
        "IDEAS Research Institute",
        "Jagiellonian University, Faculty of Mathematics and Computer Science, Łojasiewicza 6, 30-348, Krakow, Poland",
        "Poznan University of Technology, Institute of Robotics and Machine Intelligence, ul. Piotrowo 3A, Poznan 60-965, Poland"
    ]
}