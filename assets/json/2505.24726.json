{
    "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
    "authors": [
        "Shelly Bensal",
        "Umar Jamil",
        "Christopher Bryant",
        "Melisa Russak",
        "Kiran Kamble",
        "Dmytro Mozolevskyi",
        "Muayad Ali",
        "Waseem AlShikh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 2 7 4 2 . 5 0 5 2 : r Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning Shelly Bensal Umar Jamil Christopher Bryant Melisa Russak Kiran Kamble Dmytro Mozolevskyi Muayad Ali Waseem AlShikh Writer, Inc. {shelly, ..., waseem}@writer.com"
        },
        {
            "title": "Abstract",
            "content": "We explore method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that models ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing given task, the model generates self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated impressive capabilities across wide range of natural language processing tasks (Zhao et al., 2025), as well as in mathematics (Ahn et al., 2024), coding (Jiang et al., 2024), and reasoning (Huang and Chang, 2023). Despite these advancements however, models still have blind spots, and there is no guarantee that model that succeeds at one task will succeed at another, even if the task is of similar type (Asher et al., 2023; Huckle and Williams, 2025). The most direct way to address this problem is to retrain or fine-tune model on data that represents the failed task, however this may not be possible if no such dataset exists. Furthermore, if the largest state-of-the-art models also struggle to complete the task, we similarly cannot use them to generate synthetic training data (Liu et al., 2024a). An alternative solution is to prompt the model to explain its reasoning or self-reflect on why it failed. For example, the popular Chain-of-Thought (CoT) paradigm (Wei et al., 2022) showed that models performed significantly better at arithmetic, commonsense, and reasoning tasks if they were prompted to show their reasoning in addition to simply providing response. Self-reflection operates on similar principle, in that if we can detect when LLM provides an incorrect response, we can prompt it to reflect on any flaws in its reasoning and perhaps try again (Ji et al., 2023; Renze and Guven, 2024). The main advantage of these approaches is that they do not require any additional training data, however their effectiveness is directly tied to the effectiveness of the reasoning/reflection prompt. Equal contribution Preprint. Under review. In this paper, we investigate the extent to which LLMs can learn to generate better self-reflections in order to self-improve on downstream tasks. More specifically, if model fails to complete task on its first attempt, it generates self-reflection which it uses to make second attempt. If the model then succeeds on its second attempt, we use reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO) (Shao et al., 2024), to reward the tokens in the self-reflection, such that future self-reflections will be more effective. In this way, models can learn how to improve upon all kinds of tasks without requiring any task-specific data; they instead just optimize how to reflect on mistakes. Our main contribution is thus novel methodology for training model to generate better selfreflections to improve on challenging tasks in task-agnostic way. Crucially, this method only requires binary success/failure signal from response verifier, which makes it well-suited to tasks where success can be easily verified. To demonstrate the efficacy of our approach, we carry out experiments on both the APIGen function calling dataset (Liu et al., 2024b) and the Countdown equation task introduced by Pan et al. (2025b)."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Self-Reflection Self-reflection in LLMs Self-reflection, also referred to as introspection, is metaprompting strategy in which language model analyzes its own reasoning in order to identify and correct potential mistakes. This paradigm has gained momentum in large language model (LLM) research as means to boost multi-step reasoning and problem-solving performance, especially in domains such as arithmetic, commonsense reasoning, and question answering (Wei et al., 2022; Madaan et al., 2023; Renze and Guven, 2024; Shinn et al., 2023). Typically, self-reflection involves generating an initial answer, producing natural language feedback to critique that answer, and then refining the response based on this critique. This process can be applied iteratively, often using the same model to both generate and evaluate solutions, and may include modules such as memory buffers or explicit meta-instruction guides (Liu et al., 2025; Wu et al., 2025). Approaches and Limitations The methodology for self-reflection in LLMs varies along several axes. Some methods apply self-correction only to failed or low-confidence queries, while others use it for every response; feedback can be provided in the form of scalar scores, external annotations, or natural language, and may be generated by humans, external models, or the LLM itself (Bai et al., 2022; Peng et al., 2023; Yang et al., 2022; Pan et al., 2025c). While prompting LLMs to self-reflect does improve accuracy in many settings, recent work has shown that the effectiveness depends strongly on the context: challenges include the inability to reliably identify self-errors without ground-truth oracles, diminishing returns from repeated reflection, and risks of performance deterioration for easier prompts or high-performing base models (Huang et al., 2024; Zhang et al., 2024; Kim et al., 2023). In particular, self-reflection is most effective when initial accuracy is low, question difficulty is high, and external verification is available. Conversely, LLMs may sometimes fail to recognize their own mistakes but can still benefit from external feedback when such supervision exists (Pan et al., 2025c; Shinn et al., 2023). Training-Based Methods Recent directions focus on incorporating self-improvement capabilities during model training, either by fine-tuning on self-correction trajectories or by formulating the process as multi-turn reinforcement learning problem (Kumar et al., 2024; Qu et al., 2024; Wu et al., 2025). These training-based methods suggest that leveraging the models own critiques during learning yields persistent improvementseven when no test-time self-reflection is performed. However, these approaches typically rely on larger teacher models for data generation or supervision, which can be seen as form of knowledge distillation (Hinton et al., 2015). Our Approach Building on insights from prior research, we propose correcting only failed cases identified by an external verifier, converting its binary feedback into self-reflective prompts, and training the model to use the self-reflection to succeed at the second attempt. This oracle-grounded conditional computation leverages training-time benefits to reduce test-time overhead and is guaranteed to improve or maintain performance, since corrections are applied only to initially incorrect examples. For training, we employ Group Relative Policy Optimization (GRPO), introduced in the next section. Notably, this approach bootstraps solely from the models own outputs, without relying on external LLMs. Figure 1: Reflect, Retry, Reward Mechanism The model is first prompted to complete task based on user query. If the initial response is correct, the process stops. If not, the model is prompted to generate self-reflection on how to improve. The model then retries the same task, this time with its self-reflection included, and the new answer is evaluated. If the second attempt succeeds, the model learns that it generated an effective self-reflection. 2.2 Reinforcement Learning for Language Models GRPO Group Relative Policy Optimization (GRPO) is an outcome-based reinforcement learning method proposed to address the unique challenges faced when fine-tuning LLMs, such as those encountered in complex mathematical reasoning tasks (Shao et al., 2024). Unlike conventional approaches like Proximal Policy Optimization (PPO) (Schulman et al., 2017), GRPO dispenses with separate value (critic) network and instead estimates advantages directly by comparing outcomes from group of sampled completions. This makes GRPO particularly well-suited to settings where supervision is sparse and only available at the conclusion of generationfor example, whether completed math solution is correct. In such environments, the model must generate an entire sequence before receiving any feedback, typically in the form of scalar reward reflecting the quality or correctness of the output. Our Approach In this work, we adopt GRPO as the sole mechanism for reinforcement learning, without involving additional supervised fine-tuning stages. Recent research has demonstrated that modifying GRPOs reward structure can effectively encourage models to persist through failure, for instance by rewarding retries after unsuccessful attempts, thereby promoting self-correction and robustness (Dao and Le, 2025). GRPO has further shown promise in related domains requiring complex, outcome-supervised behaviorsincluding tool use and advanced mathematical problem solvingoffering flexible and efficient optimization strategy in diverse LLM applications (Qian et al., 2025; Li et al., 2025)."
        },
        {
            "title": "3 Reflect, Retry, Reward",
            "content": "Our novel Reflect, Retry, Reward methodology operates as follows, and is illustrated in Figure 1. First, model is prompted to complete task. If it succeeds, we do nothing as the model already meets our needs. If it fails however, we prompt it to generate self-reflection on what might have gone wrong. Note that this presupposes validator that automatically evaluates whether response was success or failure (binary). While it is sometimes possible to define task-dependent validator that meets this criteria without ground-truth labels, such as in basic API function calling (Did the API call return valid response?), mathematical equations (Does the equation evaluate to the target answer?), or code (Does the generated code execute?), some task types may require gold-standard target answers. 3 Having generated self-reflection, the model then makes second attempt to complete the task, making use of the self-reflection in the conversation history. If it still fails, we do nothing; the self-reflection was insufficient to turn failure into success. If it succeeds however, we use GRPO to reward only the tokens that were generated in the self-reflection. This is possible by setting the advantage terms for all other generated tokens to zero. We do this because we want the model to learn how to self-reflect more generally rather than specialize for particular task. In other words, we do not reward the correct answer, we only reward the self-reflection."
        },
        {
            "title": "4 Experiments",
            "content": "We demonstrate the effectiveness of our approach through experiments on two different tasks: function calling and math equations. 4.1 Function Calling We use the APIGen dataset (Liu et al., 2024b) for our function calling experiments. APIGen is dataset of 60,000 high quality function calls that consist of user query (plain text), list of possible tools that can answer that query plus their parameters (JSON) and the correctly formatted function call with the correct parameters and values (JSON). There are total of 4,211 unique tools in the dataset, with an average of 2.3 parameters per tool, and each user query has an average of 2.8 tools to choose from (min 1, max 8). model is only considered to be correct if it not only selects the right tool, but also generates the correct parameters and values. sample datapoint with choice of two different tools is shown below (formatted to be more human readable). USER QUERY: Check if the Vimeo username john_doe_artist is available. TOOLS PROVIDED: [{ \"name\": \"vimeo\", \"description\": \"Checks if given Vimeo username is available using the (cid:44) Toolbench RapidAPI service.\", \"parameters\": {\"username\": {\"description\": \"The Vimeo username to check for (cid:44) availability.\", \"type\": \"str\", \"default\": \"username\"}} }, \"name\": \"get_user_pins\", \"description\": \"Retrieves the Pinterest pins of specified user.\", \"parameters\": {\"username\": {\"description\": \"The Pinterest username whose pins (cid:44) are to be fetched.\", \"type\": \"str\", \"default\": \"0869178429hau\"}} { }] CORRECT ANSWER: [{\"name\": \"vimeo\", \"arguments\": {\"username\": \"john_doe_artist\"}}] To preserve the integrity of our experiments, we only evaluate models that were released before the APIGen dataset was released (June 2024). This ensures it is impossible that any of these models could have been trained on the dataset to obtain an unfair advantage. Specifically, we report results for Qwen2 (1.5B/7B Instruct) (Yang et al., 2024), Llama3.1 (8B Instruct) (Grattafiori et al., 2024), and Phi3.5-mini Instruct (Abdin et al., 2024). We also report the vanilla performance of Qwen2-72B Instruct, Llama3.1-70B Instruct, and Writers Palmyra X4 (Writer.com, 2024) as baseline. Since different model families also have different suggested tool-calling approaches, we tested different templates for each model family and ultimately chose the prompt formats that provided the strongest baselines. For our function calling validator, we require the model output to exactly match the correct answer in the dataset (i.e. based on the ground-truth labels). We used the following prompt to generate self-reflections for failed function calling attempts: You tried performing the task, but failed in generating the correct tool call. (cid:44) Reflect on what went wrong and write short explanation that will help you (cid:44) do better next time. 4.2 Countdown Math Equations We use the Countdown dataset introduced by the TinyZero project for our math equation experiments (Pan et al., 2025a,b). The Countdown dataset consists of 450k lists of 3-4 numbers along with target number. The goal is to apply basic arithmetic operations to the numbers such that the equation evaluates to the target number. model is only considered to be correct if it uses all the numbers once (in any order) and if the final equation successfully evaluates to the target number. sample datapoint is shown below. Using the numbers [4, 73, 4, 23], create an equation that equals 76. You can use (cid:44) basic arithmetic operations (+, -, *, /) and each number can only be used (cid:44) once. As with function calling, to preserve the integrity of our experiments, we only evaluate models that were released or have knowledge cutoff before the Countdown dataset was made publicly available (January 2025). Specifically, we report results for Qwen2.5 (1.5B/3B/7B Instruct) (Yang et al., 2025), Llama3.1 (8B Instruct), Llama3.2 (3B Instruct), and Writers Palmyra 1.7B. We also report the vanilla performance of Qwen2.5-32B Instruct, Qwen2.5-72B Instruct, Llama3.1-70B Instruct, and Writers Palmyra X4 (Writer.com, 2024) as baseline. We once again tried several different prompt formats for each model family, and ultimately chose to use the format that provided the strongest baseline. For our math equation validator, we required the generated equation to match the target answer in the prompt (i.e. no need for ground-truth labels). We used the following prompt to generate self-reflections for failed Countdown math equations: You tried solving the problem and got the wrong answer. Reflect on what went wrong (cid:44) and write short explanation that will help you do better next time. 4.3 Dataset of Failures For reasons of efficiency, and to facilitate more intuitive analysis, we did not train our models on the full function calling and math equation training sets, but instead opted to first create dataset of failures for each task. More specifically, we prompted each model for each task to generate up to 64 responses (depending on model size) to each user query and preserved only those queries where the model failed (based on each task-dependent verifier). We typically generated more responses for larger models because they failed less frequently than smaller models, and so would otherwise yield fewer training samples. To accelerate the rejection sampling process, we used vLLM (Kwon et al., 2023) with prefix caching. This approach has several advantages. First and foremost, it saves time because there is no point training our self-reflection model on queries it already handles successfully and hence cannot learn from. Second, by generating several responses per query, we make the data more robust; for example, if base model generates correct response to the same query 80% of the time, we can still learn from the remaining 20% since responses are not deterministic. Finally, by only having failure cases in our dataset, we can precisely determine how many samples the model needed to train on before it converged on the optimum self-reflection. We must emphasize that we took this approach purely for reasons of efficiency and analysis, and it is otherwise functionally equivalent to learning from real-world scenario where we receive both successful and failed responses. 4.4 Multi-Step GRPO We used the TRL framework (von Werra et al., 2020) as starting base to implement our multi-step GRPO algorithm (i.e., to learn from the second attempt after self-reflection). In particular, we extend the GRPOTrainer and alter its _prepare_inputs function to call second_step function that, given the completions generated by the GRPOTrainer, will perform another step of completion generations, without affecting the mask already computed by the GRPOTrainer. As we operate on the dataset of failures, prompting the model to generate its self-reflection commentary, the mask corresponds to the tokens of the self-reflection text. This way, we can perform as many secondary steps on the initial completions as necessary and only reward the tokens (through the mask generated by the GRPOTrainer) corresponding to the initial completions. The second_step function also adds 5 APIGen Vanilla 1st Try + Reflection Trained 1st Try 2nd Try + Reflection 2nd Try Qwen-2-1.5B Instruct Qwen-2-7B Instruct Llama-3.1-8B Instruct Phi-3.5-mini Instruct (3.8B) Qwen-2-72B Instruct Llama-3.1-70B Instruct Palmyra-X4 (73B) 32.6% 66.4% 64.9% 47.5% 73.7% 66.8% 79.9% 34.8% 69.4% 70.9% 50.2% 76.6% 76.9% 83.5% 48.6% 72.2% 68.7% 52.9% - - - 52.9% 77.3% 74.9% 56.0% - - - Table 1: APIGen Results This table shows model performance in terms of accuracy on our APIGen test set (12,000 samples) both on the first and second attempt, and with/without our GRPO selfreflection training. data structure to the inputs sent to the reward function that helps in understanding the performance of the initial completion on the successive steps. This multi-step approach allows us to integrate any complex downstream reward mechanism instead of only rewarding the initial completions. We trained our models on the respective failure datasets for up to 1,750 steps with an effective batch size of 256 failures (though in practice most models converged significantly faster) and we evaluated them at their convergence point. For example, the function calling experiment on Llama-3.1-8B Instruct required only 100 training steps and utilized less than 2,000 unique queries. Only one function calling experiment saw the entire dataset of 48,000 queries; the average across all function calling experiments was less than 25,000 unique queries. The most any math equation writing experiments used was less than 25,000 unique problems; the average of all math equation writing experiments was around 15,000 unique problems. We used standard GRPO training parameters as described in the original DeepSeek implementation (Shao et al., 2024), and conducted some hyperparameter experimentation. In our final experiments, we set the KL divergence coefficient to 0.001, and used learning rate of 5e-7 with cosine annealing schedule and warmup ratio of 0.03. To train each model, we used between 4 and 8 H100 GPUs. We limit our experiments to models between 1.5 billion and 8 billion parameters due to known computational efficiency and scalability concerns with GRPO (Zhang and Zuo, 2025). In addition to the experimental results reported here, we also carried out experiments with some smaller models. We quickly discovered, however, that these models had very limited capacity to answer accurately and self-reflect; e.g. Qwen2/Qwen2.5 0.5B Instruct and Llama3.2-1B Instruct. Similarly, while Microsofts Phi 3.5 mini model was able to handle function calling, it struggled significantly with equation writing. We do not report results for these models."
        },
        {
            "title": "5 Experimental Results",
            "content": "Our main experimental results are shown in Table 1 and Table 2. Specifically, Table 1 shows model performance for each models first and second attempts on the APIGen test set (12,000 samples) both before and after our multi-step GRPO training, while Table 2 shows the same but for the Countdown test set (15,000 samples). In terms of APIGen, we first note that model size correlates perfectly with model performance after one attempt (as expected). We also note that performance increased by an average of 4.5% after second attempt using self-reflection, which again is in line with previous work. We see the biggest increase after our GRPO training however, where although we only reward self-reflection tokens, almost all models are able to outperform even the two-attempt vanilla models after just single attempt. We hypothesize this is because the self-reflection tokens help with model reasoning in general, so the model benefits even if it does not need to generate an explicit self-reflection. Nevertheless, self-reflection still helps after our training, and performance increases further 4.7% (on average) when models can self-reflect for their second attempt. Most strikingly, we observe that our Qwen-2-7B model after GRPO training is able to outperform vanilla Qwen-2-72B model when both models are given two attempts, even though the latter model is 10x bigger than the first. 6 Countdown Qwen-2.5-1.5B Instruct Qwen-2.5-3B Instruct Qwen-2.5-7B Instruct Llama-3.1-8B Instruct Llama-3.2-3B Instruct Palmyra 1.7B Qwen-2.5-32B Instruct Qwen-2.5-72B Instruct Llama-3.1-70B Instruct Palmyra-X4 (73B) Vanilla 1st Try + Reflection Trained 1st Try 2nd Try + Reflection 2nd Try 6.0% 18.8% 31.7% 2.2% 2.1% 26.8% 38.6% 45.2% 17.3% 46.8% 10.2% 29.0% 38.0% 4.6% 3.0% 31.8% 45.1% 49.9% 25.5% 51.6% 34.9% 33.9% 41.6% 8.8% 8.8% 33.3% - - - - 45.0% 47.3% 50.3% 17.8% 13.8% 38.6% - - - - Table 2: Countdown Results This table shows model performance in terms of accuracy on the Countdown test set (15,000 samples) both on the first and second attempt, and with/without our GRPO self-reflection training. Figure 2: Better Self-Reflections We observe that reflections generated by vanilla models tend to be long, confusing, and redundant, whereas GRPO fine-tuned models produce much shorter, clearer, and more generalisable reflections. In terms of Countdown, it is first worth noting that performance was lower across the board, and the vanilla Llama models in particular (both Llama-3.1 and Llama-3.2) really struggled to complete the task; for example, the Llama-3.1.70B model was outclassed by even the Qwen-2.5-3B model, which is more than 20x smaller. Otherwise, the pattern of improvement is similar to the APIGen experiments, albeit at slightly higher magnitude: self-reflection increased performance by an average of 5.3% and 8.6% respectively before and after our GRPO training. We hypothesize that these larger gains come from the fact the models started from lower baseline and hence had greater opportunity to learn. Ultimately, our findings not only reinforce previous work on the benefits of self-reflection, they also demonstrate how learning to optimize for self-reflection with GRPO can improve performance further still. 7 Model MMLU-Pro GSM8K HellaSwag MATH Vanilla Trained Vanilla Trained Vanilla Trained Vanilla Trained Function calling models Qwen2-1.5B Instruct Qwen2-7B Instruct Llama3.1-8B Instruct Phi3.5-mini 22.1% 43.4% 41.0% 44.6% 21.9% 59.6% 42.8% 77.6% 40.9% 78.0% 44.5% 79.2% 59.5% 66.0% 77.0% 80.7% 77.7% 79.3% 79.7% 76.8% 66.1% 21.2% 80.5% 34.0% 79.3% 31.1% 76.8% 31.6% Math equation writing models Qwen2.5-1.5B Instruct Qwen2.5-3B Instruct Qwen2.5-7B Instruct Llama3.1-8B Instruct Llama3.2-3B Instruct Palmyra 1.7B 30.9% 33.4% 44.8% 41.0% 31.5% 31.7% 31.5% 59.4% 32.7% 65.3% 44.2% 84.2% 41.1% 78.0% 31.7% 65.8% 31.7% 76.2% 58.0% 68.3% 65.1% 74.9% 82.1% 80.5% 77.3% 79.3% 65.0% 70.4% 76.3% 50.0% 68.3% 25.5% 74.9% 32.2% 80.5% 42.3% 79.6% 31.1% 71.0% 28.3% 49.9% 43.0% 20.9% 33.7% 31.5% 31.0% 26.3% 31.6% 40.3% 31.0% 28.2% 43.0% Table 3: Catastrophic Forgetting Analysis Comparison between vanilla and GRPO fine-tuned models on common LLM benchmarks shows that despite fine-tuning, we observe minimal catastrophic forgetting, with fine-tuned models maintaining strong performance on these standard benchmarks. 5.1 Better Self-Reflections To provide an insight into how self-reflections improve after self-reflection training, we present qualitative example of self-reflection generated by vanilla model alongside self-reflection generated by the same model after GRPO training in Figure 2. It is immediately obvious that vanilla self-reflections are much longer, more verbose, and repetitive compared to the more concise, optimized self-reflections after training. While this intuitively makes sense humans likewise prefer short, simple instructions this finding contrasts with chain-of-thought-style outputs, which are believed to perform better precisely because they are more verbose. We leave it as an open question as to when it may be more beneficial for model to generate concise vs. verbose output. 5.2 Low Catastrophic Forgetting common concern when fine-tuning models is catastrophic forgetting, i.e. when model learns to specialize on one task at the expense of others (Li and Hoiem, 2016; Lopez-Paz and Ranzato, 2017; Kotha et al., 2024). Since our self-reflection training is designed to improve performance in task agnostic way, we evaluate our models on several diverse benchmarks (MMLU-Pro (Wang et al., 2024), GSM8K (Cobbe et al., 2021), HellaSwag (Zellers et al., 2019), and MATH (Hendrycks et al., 2021)) in order to assess their capacity for language understanding, mathematical problem solving, and commonsense reasoning both before and after self-reflection training. We do this using the common evaluation benchmark framework lm-eval (Gao et al., 2024). Our hypothesis is that performance should remain relatively unchanged, since we never optimize for specific task, but instead optimize self-reflection reasoning in general. We present our results in Table 3, and find that performance does indeed remain stable after selfinflection training. In most cases, there is less than 1% degradation compared to the base model, and some models even improve; e.g. Qwen-2.5-1.5B performance increases by 0.6% and 0.8% respectively on MMLU-Pro and MATH after self-reflection training on the Countdown dataset. We treat this as evidence our approach is robust to catastrophic forgetting."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we have shown that it is possible to significantly improve LLM performance by training model to improve at self-reflection rather than at particular task. This indirect approach depends only on validator that can detect whether model response is correct or incorrect, and so is particularly well-suited to tasks where responses can be easily verified; e.g. whether JSON output is formatted correctly, whether generated code is actually executable, or whether all the constraints of an equation are satisfied. 8 We demonstrated the efficacy of our approach through experiments on the APIGen function calling and Countdown math equation solving datasets, and found that models trained for self-reflection using GRPO improved performance by an average of 9.0% on the function calling test set (12,000 samples) and 16.0% on the Countdown match equation dataset (15,000 samples). We furthermore found that smaller self-reflection trained models could outperform larger untrained models on both tasks, despite their size difference; e.g. Qwen-2-7B Instruct (trained) outperformed Qwen2-72B Instruct (untrained) on function calling, and Qwen2.5-7B Instruct (trained) outperformed Qwen2.572B Instruct (untrained) on Countdown math equations. Our models were also robust to catastrophic forgetting. Although we only trained models to improve at self-reflection, we found they also performed significantly better even when they did not need to self-reflect; i.e. they succeeded on the first attempt so there was no need to reflect and try again. We hypothesize that this is because by focusing on self-reflection rather than particular task, models may have improved their reasoning skills more generally. In future work, we hope to investigate whether self-reflection training generalizes across different tasks."
        },
        {
            "title": "7 Limitations",
            "content": "It may not always be straightforward to define binary success/fail validator for every task. We developed our method with the view that labeled training data may be scarce, but recognize that ground-truth labels could be used as validator if available. Alternatively, it may also be possible to use larger model as judge (Zheng et al., 2023). We also find that our approach does not work for all models and all tasks; the model must have some basic ability to perform the task, self-reflect, and learn in order for boosting self-correction ability to work. For example, Llama3.2-3B Instruct was unable to learn to self-correct on the function calling task."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language In Proceedings of the 18th models for mathematical reasoning: Progresses and challenges. Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 225237, St. Julians, Malta. Association for Computational Linguistics. Nicholas Asher, Swarnadeep Bhar, Akshay Chaturvedi, Julie Hunter, and Soumya Paul. 2023. Limits In Proceedings of the 12th Joint Conference on Lexical for learning with language models. and Computational Semantics (*SEM 2023), pages 236248, Toronto, Canada. Association for Computational Linguistics. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, and 12 others. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Alan Dao and Thinh Le. 2025. Rezero: Enhancing llm search ability by trying one-more-time. Preprint, arXiv:2504.11001. 9 Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. The language model evaluation harness. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in neural network. Preprint, arXiv:1503.02531. Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: survey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 10491065, Toronto, Canada. Association for Computational Linguistics. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations. James Huckle and Sean Williams. 2025. Easy Problems that LLMs Get Wrong. In Advances in Information and Communication, pages 313332, Cham. Springer Nature Switzerland. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating LLM hallucination via self reflection. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 18271843, Singapore. Association for Computational Linguistics. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. survey on large language models for code generation. Preprint, arXiv:2406.00515. Geunwoo Kim, Pierre Baldi, and Stephen Marcus McAleer. 2023. Language models can solve computer tasks. In Thirty-seventh Conference on Neural Information Processing Systems. Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. 2024. Understanding catastrophic forgetting in language models via implicit inference. Preprint, arXiv:2309.10105. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. 2024. Training language models to self-correct via reinforcement learning. Preprint, arXiv:2409.12917. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025. Torl: Scaling tool-integrated rl. Preprint, arXiv:2503.23383. Zhizhong Li and Derek Hoiem. 2016. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:29352947. Liping Liu, Chunhong Zhang, Likang Wu, Chuang Zhao, Zheng Hu, Ming He, and Jianping Fan. 2025. Instruct-of-reflection: Enhancing large language models iterative reflection capabilities via dynamic-meta instruction. Preprint, arXiv:2503.00902. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, and Andrew M. Dai. 2024a. Best practices and lessons learned on synthetic data. In First Conference on Language Modeling. 10 Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, and 1 others. 2024b. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482. David Lopez-Paz and MarcAurelio Ranzato. 2017. Gradient episodic memory for continual learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 64706479, Red Hook, NY, USA. Curran Associates Inc. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, and 1 others. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:46534 46594. Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. 2025a. Learning adaptive parallel reasoning with language models. Preprint, arXiv:2504.15466. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. 2025b. Tinyzero. https://github.com/Jiayi-Pan/TinyZero. Accessed: 2025-05-05. Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H. Vicky Zhao, Conghui He, and Lijun Wu. 2025c. Lemma: Learning from errors for mathematical advancement in llms. Preprint, arXiv:2503.17439. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. Preprint, arXiv:2302.12813. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025. Toolrl: Reward is all tool learning needs. Preprint, arXiv:2504.13958. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. 2024. Recursive introspection: Teaching language model agents how to self-improve. Preprint, arXiv:2407.18219. Matthew Renze and Erhan Guven. 2024. Self-reflection in llm agents: Effects on problem-solving performance. arXiv preprint arXiv:2405.06682. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. Preprint, arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Preprint, arXiv:2406.01574. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc. 11 Writer.com. 2024. Palmyra x4 tool calling llm. https://writer.com/llms/palmyra-x4/. Accessed: 2025-05-29. Zongqian Wu, Baoduo Xu, Ruochen Cui, Mengmeng Zhan, Xiaofeng Zhu, and Lei Feng. 2025. Rethinking chain-of-thought from the perspective of self-training. Preprint, arXiv:2412.10827. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and 43 others. 2024. Qwen2 technical report. Preprint, arXiv:2407.10671. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 23 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 43934479, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? Preprint, arXiv:1905.07830. Jixiao Zhang and Chunsheng Zuo. 2025. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models. Preprint, arXiv:2504.09696. Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. 2024. Self-contrast: Better reflection through inconsistent solving perspectives. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 36023622, Bangkok, Thailand. Association for Computational Linguistics. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, and 3 others. 2025. survey of large language models. Preprint, arXiv:2303.18223. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. In Proceedings of the 37th 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc."
        },
        {
            "title": "A Prompt Templates",
            "content": "For reproducibility and clarity, we provide details on the prompt templates used during training. To the best of our ability, we followed model provider recommendations for prompting. We iterated on prompts to achieve reasonable baselines for each model on each task. A.1 Function Calling Qwen 2 models Our prompting style for Qwen 2 models for function calling is as follows. First, we provide the following system prompt: You are helpful assistant that can answer questions and help with tasks. # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> {List of tools, each on new line} </tools> For each function call, return json object with function name and arguments within (cid:44) <tool_call></tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <args-json-object>} </tool_call> This is followed by the user query from the dataset, as role user. The model then replies with its first attempt at the task. If the attempt is incorrect, we prompt for self-reflection as follows: You tried performing the task, but failed in generating the correct tool call. (cid:44) Reflect on what went wrong and write short explanation that will help you (cid:44) do better next time. After the model generates self-reflection, we again prompt with the system prompt and user query to set up the model for its second attempt at the task. Llama 3.1 and Phi 3.5 models We follow the recommended Llama 3.1 tool calling format. We found that Phi performs better following the Llama tool-calling template than the Qwen 2 template. First, we provide the following system prompt: When you receive tool call response, use the output to format an answer to the (cid:44) original user question. You are helpful assistant with tool calling capabilities. Then, as role user, we provide the tools and user query from the dataset as follows: Given the following functions, please respond with JSON for function call with (cid:44) its proper arguments that best answers the given prompt. Respond in the format {\"name\": function name, \"parameters\": dictionary of (cid:44) argument name and its value}. Do not use variables. {List of tools, each on new line} Question: This is followed by the user query from the dataset, as role user. The model then replies with its first attempt at the task. We then prompt for self-reflection: You tried performing the task, but failed in generating the correct tool call. (cid:44) Reflect on what went wrong and write short explanation that will help you (cid:44) do better next time. 13 After the model generates self-reflection, we prompt with just the user query to set up the model for its second attempt at the task. A.2 Countdown Math Equations We provide the following system prompt: Please reason step by step, and put your final answer within boxed{}. Then, as role user, we provide the main problem as follows: Using the numbers {nums, in list format} create an equation that equals {target}. (cid:44) You can use basic arithmetic operations (+, -, *, /) and each number can (cid:44) only be used once. Please reason step by step, and put your final answer within boxed{}. The model then replies with its first attempt at the task. Given failure, we then prompt for self-reflection: You tried solving the problem and got the wrong answer. Reflect on what went wrong (cid:44) and write short explanation that will help you do better next time. After the model generates self-reflection, we repeat the user message from above to set the model up for its second attempt at the task: Using the numbers {nums, in list format} create an equation that equals {target}. (cid:44) You can use basic arithmetic operations (+, -, *, /) and each number can (cid:44) only be used once. Please reason step by step, and put your final answer within boxed{}. 14 APIGen Tool choice error Parameter error Format error Vanilla Qwen-2-1.5B Instruct Trained Qwen-2-1.5B Instruct Vanilla Qwen-2-7B Instruct Trained Qwen-2-7B Instruct Vanilla Llama-3.1-8B Instruct Trained Llama-3.1-8B Instruct Vanilla Phi-3.5-mini Instruct (3.8B) Trained Phi-3.5-mini Instruct (3.8B) 33.9% 21.3% 5.2% 3.5% 4.6% 3.7% 18.8% 17.6% 25.3% 25.3% 27.2% 22.7% 28.5% 25.6% 24.8% 23.9% 8.2% 4.8% 1.2% 1.6% 2.0% 2.0% 8.9% 5.7% Table 4: APIGen Error Analysis This table categorises model errors on the first attempt at the task with and without GRPO self-reflection training (12,000 sample test set). Countdown Invalid equation Wrong numbers Missed target Vanilla Qwen-2.5-1.5B Instruct Trained Qwen-2.5-1.5B Instruct Vanilla Qwen-2.5-3B Instruct Trained Qwen-2.5-3B Instruct Vanilla Qwen-2.5-7B Instruct Trained Qwen-2.5-7B Instruct Vanilla Llama-3.1-8B Instruct Trained Llama-3.1-8B Instruct Vanilla Llama-3.2-3B Instruct Trained Llama-3.2-3B Instruct Vanilla Palmyra 1.7B Trained Palmyra 1.7B 5.9% 0.6% 4.9% 2.1% 3.8% 0.1% 0.1% 0.0% 0.1% 0.0% 4.1% 3.0% 73.7% 34.3% 52.5% 33.6% 39.1% 55.6% 97.1% 90.4% 97.1% 90.4% 57.5% 44.6% 14.4% 30.2% 23.9% 30.4% 25.4% 2.7% 0.6% 0.7% 0.7% 0.7% 11.6% 19.1% Table 5: Countdown Error Analysis This table categorises model errors on the first attempt at the task with and without GRPO self-reflection training (15,000 sample test set)."
        },
        {
            "title": "B Error Analysis",
            "content": "We categorise the errors of our models before and after training in an attempt to better understand what types of errors models are prone to on these tasks, and what types of errors can be mitigated by self-reflection training. We look exclusively at errors made on the first attempt at the task (pass@1). B.1 Function Calling For function calling, we categorise errors into three types: errors in tool choice, errors in parameter names or values, and errors in format. We consider parameter choice to be much more difficult than tool choice. The two smallest models (Qwen-2-1.5B Instruct and Phi-3.5-mini Instruct) struggle significantly with tool choice without training, and dont improve much, if at all, on parameter values through training. Conversely, the larger models (7-8 billion parameters) are already quite good at tool choice without training, and training primarily seems to teach parameter selection. B.2 Math Countdown Equations For math countdown equations, we categorise errors into three types: an invalid equation (or one that uses disallowed characters), an equation that uses numbers outside of the provided ones (wrong numbers), and an equation that does not evaluate to the provided target (missed target). 15 All models struggled primarily with outputting equations that used only allowed numbers. Training significantly decreased this error for all models except for Qwen-2.5-7B Instruct. Put another way, all models except the largest Qwen model primarily learned to use the correct numbers in the equation through training, even if it resulted in missing the target, whereas Qwen-2.5-7B Instruct learned to hit the target even if it meant using the wrong numbers."
        }
    ],
    "affiliations": [
        "Writer, Inc."
    ]
}