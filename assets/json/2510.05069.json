{
    "paper_title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs",
    "authors": [
        "Dachuan Shi",
        "Abedelkadir Asi",
        "Keying Li",
        "Xiangchi Yuan",
        "Leyan Pan",
        "Wenke Lee",
        "Wen Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten."
        },
        {
            "title": "Start",
            "content": "SWIREASONING: SWITCH-THINKING IN LATENT AND EXPLICIT FOR PARETO-SUPERIOR REASONING LLMS Dachuan Shi1, Abedelkadir Asi2, Keying Li2, Xiangchi Yuan1, Leyan Pan1, Wenke Lee1, Wen Xiao2 1Georgia Tech 2Microsoft github.com/sdc17/SwiReasoning swireasoning.github.io 5 2 0 2 ] . [ 1 9 6 0 5 0 . 0 1 5 2 : r Figure 1: Pass@1 accuracy under unlimited token budgets. On mathematics and STEM reasoning benchmarks, SWIREASONING yields improvements of up to +2.8% and +2.0%, respectively. Figure 2: Token efficiency (accuracy per token compared to standard CoT), under limited token budgets. Across reasoning LLM families and sizes, SWIREASONING brings average efficiency improvements of up to +79%."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent work shows that, beyond discrete reasoning through explicit chain-ofthought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SWIREASONING, training-free framework for LLM reasoning which features two key innovations: 1) SWIREASONING dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SWIREASONING curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SWIREASONING consistently improves average accuracy by 1.5%2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SWIREASONING improves average token efficiency by 56%-79%, with larger gains as budgets tighten. Equal advising."
        },
        {
            "title": "SWIREASONING",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Reasoning is one of the central capabilities of large language models (LLMs) (Yang et al., 2025; Qwen Team, 2024; Meta, 2025a;b). It allows models to tackle complex tasks such as mathematics, science, and programming (Guo et al., 2025; OpenAI, 2025b; Jaech et al., 2024; Agarwal et al., 2025; Qwen Team, 2025; Abdin et al., 2025; Abouelenin et al., 2025; Anthropic, 2025; DeepMind, 2024a;b), far beyond simple question answering. key limitation of the dominant reasoning approach, explicit chain-of-thought (CoT) (Wolf et al., 2020; Wei et al., 2022; Yao et al., 2023a; Goyal et al., 2024; Pfau et al., 2024), lies in the reliance on discrete tokens during inference. In standard CoT decoding, the model commits to single token at each step, sampled from the predicted distribution. While effective and ensures readability by verbalizing intermediate steps, this discrete process collapses the full probability distribution into single trajectory, discarding uncertainty and eliminating many potentially useful reasoning paths. To overcome this bottleneck, recent work has explored an alternative reasoning technique, latent reasoning (Hao et al., 2024; Zhang et al., 2025; Cheng & Van Durme, 2024; Xu et al., 2025a;b; Tan et al., 2025), where the model operates directly in continuous hidden space instead of discrete text space. Latent reasoning offers two key advantages over CoT: 1) higher representational bandwidth per step, since hidden vectors can encode richer information than single tokens (Zhu et al., 2025b); and 2) the ability to preserve multiple reasoning hypotheses implicitly, rather than collapsing them prematurely into one tokenized path (Li et al., 2025b; Chen et al., 2025). Latent reasoning can be broadly categorized into training-required and training-free approaches. Training-required ones (Hao et al., 2024; Su et al., 2025; Liu et al., 2024; Shen et al., 2025; Tack et al., 2025) demand substantial retraining or fine-tuning (Yue et al., 2025; Li et al., 2025a; Wang et al., 2025a; Zhu et al., 2025a), making it excessively expensive to apply to large reasoning language models. In contrast, training-free approaches like Soft-Thinking (Zhang et al., 2025), which form probability-weighted mixture of token embeddings as inputs, operate directly at inference time without incurring additional training costs. Our work focuses on the latter category, which is costeffective and resource-friendly for deployment in large-scale reasoning models. Although training-free latent reasoning eliminates the need for costly retraining, operating purely in the latent space also presents significant challenges. First, the model is not explicitly trained to perform long-horizon reasoning with latent inputs. As result of distributional mismatches, when inference relies solely on latent trajectories, the process is less controlled and can easily drift off Instead of collapsing into single path, the model tends to spread course (Chen et al., 2025). probability mass across many implicit reasoning paths. While this preserves multiple hypotheses, it also introduces persistent noise, slows convergence, and ultimately harms reasoning accuracy (Li et al., 2025b). Second, the absence of explicit tokens does not necessarily ensure efficiency. In latent space, models may still suffer from repetitive or unnecessarily extended internal deliberations and continuation (Zhang et al., 2025), essentially overthinking. This prolongs inference and overconsumes tokens, undermining the very efficiency that latent reasoning is meant to improve. To address these issues, this paper introduces SWIREASONING (abbreviated as SWIR) as trainingfree framework for LLM reasoning that alternates between explicit and latent thinking, based on block-wise confidence inferred from entropy trends of next-token distributions, and suppresses overthinking by bounding the number of switches. More specifically, the framework first tracks reference entropy within each thinking block to reflect block-wise confidence. Rising confidence triggers an explicit switch to consolidate progress along single path, while sustained uncertainty triggers latent switch to re-explore in continuous space. Second, switch count controller caps the number of thinking block transitions and provides early-answer checkpoints, curbing unnecessary latent loops and improving token efficiency across difficulties. The proposed framework also benefits from reintroducing diversity by sampling in an explicit thinking block when compared to pure latent thinking. Even though motivated differently, SWIREASONING resonates with the concurrent observation of Liang et al. (2025) that introducing stochasticity benefits latent reasoning, but we achieve this via distinct mode switch mechanism rather than injecting distributions with randomness. Our contributions are summarized as follows:"
        },
        {
            "title": "SWIREASONING",
            "content": "We propose SWIREASONING, training-free reasoning framework that dynamically alternates between explicit and latent thinking based on confidence signals, thereby exploiting the expressivity of latent thinking without sacrificing the stability of explicit thinking. We introduce switch count control mechanism that caps the number of transitions, enabling early answering based on partial reasoning trajectories at switch boundaries. This effectively suppresses overthinking and improves token efficiency under limited budgets. We extensively validate the effectiveness of SWIREASONING on mathematics and STEM reasoning domains across multiple benchmarks, model families, and sizes, demonstrating consistent gains in both accuracy and token efficiency over training-free baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Explicit LLM Reasoning. Reasoning via explicit intermediate text has been extensively studied. Chain-of-thought (CoT) prompting elicits stepwise rationales that improve reasoning accuracy by decomposing problems into natural-language sub-steps (Kojima et al., 2022; Wei et al., 2022). Subsequent work increases robustness by aggregating multiple CoT trajectories through self-consistency (Wang et al., 2022). Searchand tool-augmented variants further expand the exploration space, such as Tree-of-thought that branches over partial rationales (Yao et al., 2023a) and interleaving reasoning and actions with external tools and environments (Yao et al., 2023b). Least-to-most prompting progressively solves subproblems to reduce reasoning load and mitigate error accumulation (Zhou et al., 2022). These approaches operate purely in the discrete token space and therefore commit to single token at each step. While readable, the discretizations in explicit reasoning discard alternative hypotheses early, and restrict the information bandwidth per step (Zhu et al., 2025b). Latent LLM Reasoning. Latent reasoning operates in the continuous representation space rather than discrete natural language space used by explicit reasoning. Prior work can be broadly grouped into two categories. 1) Training-required approaches modify pretraining (Tack et al., 2025; Zeng et al., 2025) or fine-tuning objectives (Tan et al., 2025; Wang et al., 2025a;b; Jiang et al., 2025; Wu et al., 2025; Yue et al., 2025; Li et al., 2025a; Shen et al., 2025; Xu et al., 2025a) to supervise hidden-state trajectories or to endow models with latent-planning skills. 2) Training-free approaches (Zhang et al., 2025; Liang et al., 2025) intervene only at inference time by manipulating hidden representations or probability distributions without updating model weights. Our work belongs to the training-free category but differs from prior single-mode methods. Instead of remaining purely latent, SWIREASONING dynamically switches between latent and explicit reasoning based on entropy-trend confidence, and further regulates the number of switches through count controller to suppress overthinking and improve efficiency."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 SWIREASONING OVERVIEW As shown in Fig. 3, SWIREASONING is training-free framework that dynamically alternates between explicit and latent reasoning. The number of switches is regulated to suppress overthinking and improve token efficiency. Sec. 3.2 presents the preliminaries of explicit and latent reasoning, Sec. 3.3 details the design of the dynamic switch, and Sec. 3.4 discusses the switch count control mechanism. Implementation details are provided in Appendix B.1. 3.2 PRELIMINARY: EXPLICIT AND TRAINING-FREE LATENT THINKING Explicit Thinking. Let be vocabulary and pθ(xt x<t) LLM over with parameters θ. Given question q, the model produces reasoning trace r1:T followed by final answer a1:U . We write the concatenated sequence as x1:(q+T +U ) = (cid:2)q, r1:T , a1:U (cid:3),"
        },
        {
            "title": "SWIREASONING",
            "content": "Figure 3: SWIREASONING framework. (a) Dynamic mode switching alternates between explicit and latent thinking based on block-wise confidence estimated from entropy trends. (b) switch count control mechanism limits the maximum number of thinking-block transitions, suppressing overthinking before the final answer. At inference, decoding proceeds by repeatedly choosing token xt from the predictive distribution pθ( x<t) according to policy πt(), e.g., xt πt() with πt = (cid:40) Greedy: arg maxvV pθ(v x<t), Sampling: Top-k/Top-p with temperature τ. The reasoning phase stops when termination condition is met, e.g., generating /think, after which the answer tokens a1:U are decoded in the same manner. While explicit reasoning improves reliability by externalizing intermediate steps, its hard policy πt() collapses the full distribution to single discrete decision at each step, i.e., discards information in pθ( x<t) beyond the chosen token. It replaces the hard policy πt() by continuous surrogate that Training-Free Latent Thinking. preserves distributional information. Let RV denote the token embedding matrix with rows e(v) Rd. At step t, the model yields logits ℓt RV and pt = softmax(ℓt). Given the next-token distribution pt := pθ( x<t) 1, it forms soft embedding et = (cid:88) vV pt[v] e(v) Rd, (1) and feeds et back to the model as the next input representation, rather than committing to an explicit token by πt(). Upon the thinking phase being complete, the policy reverts to πt() for answer generation. The convexity of Eq. 1 ensures et lies in the embedding hull of E, retaining all firstorder uncertainty in pt, which reduces information discards and increases robustness to local noise. 3.3 DYNAMIC SWITCH BETWEEN EXPLICIT AND LATENT THINKING Remaining in single mode throughout reasoning is inherently suboptimal: explicit thinking provides readability but may discard useful information beyond chosen tokens, while latent thinking preserves richer signals but can drift into noise and reduce accuracy. Our key insight is that reasoning should switch modes based on confidence. Latent reasoning enables exploration across multiple potential continuations when confidence is low, and explicit reasoning encourages convergence when confidence is high, striking balance that supports broad exploration while maintaining accuracy. Mode Switch Criterion. We refer to the reasoning content between two consecutive switches as thinking block and estimate its confidence by entropy Ht = (cid:80) pt[v] log pt[v]. Let denote the reference entropy of the current block, which is initialized at the first step of the block and refreshed when mode switch happens. We use criterion that converts local entropy trends into decisions: Latent Explicit : Explicit Latent : (cid:0)Ht < H(cid:1) (confidence rises), (cid:0)Ht > H(cid:1) (confidence drops), (2) (3)"
        },
        {
            "title": "SWIREASONING",
            "content": "Switch Window Size. To avoid oscillations, we impose dwell windows upon the mode switch criterion. Formally, with mode variable mt {Explicit, Latent} and dwell step counter t, we have mt+1 = Explicit, mt = Latent (Ht < H) (t WLE), Latent, mt = Explicit (Ht > H) (t WEL), mt, otherwise. We reset Ht, 0 upon any switch, i.e., mt+1 = mt. Otherwise, we update t+1. In practice, WLE = 0 while WEL is positive, i.e., LatentExplicit switch may occur immediately when Ht dips, whereas an ExplicitLatent switch requires staying for at least WEL steps. The key intuition behind the asymmetric design is that two modes play different roles in reasoning. Latent reasoning is inherently divergent, allowing for rich exploration. However, prolonging the latent phase after confidence has recovered is counterproductive. It increases the risks of introducing spurious signals that may mislead the model. Therefore, once confidence rises, an immediate switch back to explicit reasoning is necessary to consolidate progress onto single coherent trajectory. In contrast, explicit reasoning is convergent, gradually unfolding chain-of-thought where each token incrementally extends the current logical path. If the model were allowed to switch back to latent reasoning at the first sign of an entropy fluctuation, spurious short-term uncertainty could trigger oscillations. The dwell window WEL ensures that explicit reasoning is given sufficient opportunity to stabilize and accumulate meaningful structure. Thinking-Related Signal Mixing. To better align mode switches with the LLMs learned reasoning patterns, we blend the embeddings of thinking-related signal tokens, e.g., <think> and </think>, when switch occurs. Let ethink and e/think denote their embeddings. At the entrance to latent thinking block, we bias the first latent step toward begin thinking by et αt et + (1 αt )ethink, αt [0, 1], (4) and at the exit to an explicit thinking block, we bias the first explicit step toward end thinking (5) et βt et + (1 βt )e/think, βt [0, 1], which encourages the model to close the latent phase and move on to answer production. In practice, we schedule αt = α0 + (1 α0) , where Tmax is predefined maximum generation length, and apply Eq. 4 or Eq. 5 only at the steps when switches occur. and βt = β0 + (1 β0) Tmax Tmax 3.4 OVERTHINKING SUPPRESSION BY SWITCH COUNT CONTROL Even with confidence-aware switching, reasoning LLMs may still overthink. Therefore, we place bound on the total number of LatentExplicit switches. Our key insight is that each switch naturally marks the end of thinking block where partial reasoning trajectories have been consolidated, which may already contain sufficient evidence for arriving at reasonable solution. Under limited budgets, generating answers at these natural checkpoints can make use of partial reasoning trajectories, offering chance to obtain correct predictions earlier without consuming additional tokens. Counter and Triggers. Let Ct count completed Latent Explicit switches up to step t. Given user-specified budget Cmax, we define two triggers: Convergence trigger (at 2 Cmax Ct Cmax on Latent Explicit transitions): force the next token to be /think. The convergence trigger is to encourage rather than enforce the end of the thinking process and the start of converging to an answer based on partial reasoning trajectories. Termination trigger (at Ct > Cmax on subsequent Latent Explicit transition): inject concise answer prefix sfinal, /thinknn The final answer is, then allow at most additional tokens for the final answer. The termination trigger is to enforce an immediate answer generation. Triggers are implemented as short injection queues that overwrite future-generated tokens. Formally, let Qt be the per-sample injection queue. When convergence or termination trigger fires, we set Qt [ID(/think)] or [ID(sfinal)]. At the next step, if Qt = , we deterministically set xt Qt.pop(). For the termination trigger, we also start budget counter bt = and decrement it each step after the termination trigger fires. Decoding will be terminated once bt = 0."
        },
        {
            "title": "4.1 EXPERIMENTAL SETTINGS",
            "content": "Models. We evaluate SWIREASONING on three recent reasoning LLMs: Qwen3-8B (Yang et al., 2025), DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025), and Qwen3-1.7B (Yang et al., 2025). This selection helps us validate the effectiveness of SWIREASONING across different model families, model scales, and training paradigms. Domains and Benchmarks. We evaluate SWIREASONING thoroughly on five benchmarks, including GSM8K (Cobbe et al., 2021), Math500 (Hendrycks et al., 2021), AIME 2024 (HuggingFaceH4, 2024), AIME 2025 (Yentinglin, 2025) for mathematics reasoning, and GPQA Diamond (Rein et al., 2024) for STEM reasoning. More details are provided in Appendix B.2. Baselines. We compare SWIREASONING that dynamically switches between thinking modes against three baselines with single thinking mode, including 1) explicit thinking alone: standard CoT reasoning with sampling, standard CoT reasoning with greedy decoding, and 2) training-free latent thinking alone: Soft Thinking (Zhang et al., 2025). Metrics. We use the Pass@1 metric to evaluate reasoning accuracy, and token efficiency to assess the level of reasoning efficiency. Let Accm(ℓ) [0, 1] denote the accuracy of method when using ℓ generated tokens. Its plain token efficiency is the accuracy gained per token, Em(ℓ) = Accm(ℓ) ℓ . To express efficiency in units relative to the standard CoT, we normalize it by the CoTs plain token efficiency when the highest accuracy is achieved. Specifically, if CoT achieves its highest accuracy Acc . The token efficiency of is defined as CoT tokens, denote CoT using ℓ CoT ℓ CoT CoT = Acc Em(ℓ) CoT Em(ℓ) = = Accm(ℓ)/ℓ CoT/ℓ Acc CoT . And the average efficiency gain of method over CoT is E[Em] = (cid:82) (Em(ℓ) ECoT(ℓ)) dℓ (cid:82) ECoT(ℓ) dℓ . 4.2 REASONING ACCURACY UNDER UNLIMITED TOKEN BUDGETS We first evaluate SWIREASONING in the setting where token budgets are set large enough to ensure that each method is allowed to conduct sufficient thinking (refer to Appendix B.2 for detailed settings). Fig. 1 and Tab. 1 report the highest attainable accuracies across mathematics (GSM8K, MATH500, AIME24, AIME25) and STEM (GPQA Diamond) benchmarks under this setting. Across different model families of varying sizes, SWIREASONING consistently achieves higher Pass@1 accuracy than CoT with sampling, CoT with greedy decoding, and Soft Thinking. Our observation is that improvements are most pronounced on the more challenging benchmarks. For instance, on AIME24/AIME25, which require deep deductive reasoning and are widely regarded as more difficult, our method yields absolute gains of 3.34%/2.50% on Qwen3-8B, and 5.00%/5.00% on Qwen3-1.7B. These margins substantially exceed those observed on GSM8K or MATH500 with lower difficulty, suggesting that the proposed switching mechanism is particularly beneficial when problems involve long reasoning chains or higher uncertainty from the LLMs perspective. Overall, the accuracy results under unlimited token budgets highlight the strength of SWIREASONING in better addressing reasoning tasks compared to single-mode approaches. 4.3 TOKEN EFFICIENCY UNDER LIMITED TOKEN BUDGETS Across models and benchmarks, SWIREASONING consistently attains improved Pareto frontiers. As shown in Fig. 2, the peak efficiency gains range between 4.6 and 6.8 over CoT depending"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 1: Comparison of SWIREASONING and CoT with sampling, CoT with greedy decoding, and Soft Thinking on mathematics and STEM benchmarks. SWIREASONING improves accuracy by +2.17% on average. Method GSM8K MATH 500 GPQA Diamond AIME AIME 2025 Average Qwen3-8B (Yang et al., 2025) 96.00 CoT 95.60 96.40 CoT (Greedy) 95.68 96.00 Soft Thinking 95.38 SwiR (Ours) 96.06 +0.46 98.40 59.60 56.57 59.60 +2.40 61.11 75.83 70.00 67.92 +1.51 79. 67.50 60.00 68.33 +3.34 70.00 78.91 +0.00 75.73 3.18 77.45 1.46 80.94 +2.03 +2.50 Qwen3-1.7B (Yang et al., 2025) 39.39 CoT 90.44 31.82 CoT (Greedy) 89.61 Soft Thinking 90.30 34.34 SwiR (Ours) 90.83 +0.39 93.00 +1.00 41.41 92.00 91.00 90. 45.83 40.00 38.75 +2.02 50.83 33.33 33.33 36.67 +5.00 38.33 60.20 +0.00 57.15 3.05 58.13 2.07 +5.00 62.88 +2.68 DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025) 89.46 CoT CoT (Greedy) 85.82 Soft Thinking 85.90 SwiR (Ours) 90.07 +0.61 92.00 +0.60 91.40 84.80 83.80 46.46 31.81 33.33 47.98 43.75 30.00 34.17 +1.52 45.00 +1.25 26.25 30.00 20.42 31. 59.46 +0.00 52.49 6.97 51.52 7.94 +1.80 +5.00 61.26 on the model size. These improvements are not confined to single budget: the area-under-curve (AUC) advantage persists across broad range of small to moderate budgets. One observation from Fig. 4 is that the relatively large average efficiency gains occur on GSM8K, MATH500, and GPQA Diamond across three models (up to 213% AUC improvements in the perbenchmark panels). These tasks contain many instances with lower difficulty, which benefit most from our overthinking suppression design to obtain the correct answer with partial reasoning trajectories. In contrast, on AIME24/25, the efficiency gaps are smaller, while the accuracy gains with unlimited budgets are larger. This asymmetry is expected: the harder the problem is, the more difficult it is to predict correct answer with unfinished reasoning trajectories. Overall, token efficiency results under limited budgets substantiate the advantage of SWIREASONING in gaining accuracy more efficiently as budgets tighten compared to baseline methods. 4.4 EVALUATION WITH PASS@K ACCURACY In addition to Pass@1 accuracy, we also measure Pass@k accuracy, where [1, 64] on Qwen38B. Fig. 5 shows that SWIREASONING reaches its maximal accuracy with significantly smaller than baselines. Define as the smallest achieving the methods peak. On AIME24, we observe = 13 for SWIREASONING versus 46 for CoT (about 72% fewer samples), and on AIME25, = 16 versus 22 (about 27% fewer samples). In addition to the faster growth of the curve than CoT, SWIREASONING also exhibits 1) steeper initial slope at small (higher per-sample yield), and 2) higher eventual ceiling than Soft Thinking and greedy CoT, indicating better correctness and diversity simultaneously. Overall, Pass@k accuracy results indicate that SWIREASONING is particularly attractive for budgeted evaluation settings where cannot be large. 4.5 ABLATION STUDIES Switch Window Size. SWIREASONING uses dwell windows (Sec. 3.3) to enforce the model stays in thinking block for at least steps before switching to the other thinking mode. We conduct ablation studies on Qwen3-1.7B with representative setting consisting of WEL {64, 128, 256, 512, 1024} and report Pass@1 accuracy on five benchmarks. Results in Tab. 3 demonstrate that an intermediate window size of 512 consistently produces the best results. When window sizes are too small, LLMs are allowed to jump back to latent mode prematurely, before explicit reasoning has consolidated coherent trajectory. This increases exposure to noisy signals and harms final accuracy, especially on difficult tasks such as AIME24 and AIME25. On"
        },
        {
            "title": "SWIREASONING",
            "content": "Figure 4: Token efficiency comparisons. SWIREASONING achieves the highest token efficiency throughout all token budgets in 13 out of 15 evaluations, with an efficiency improvement of +84% over CoT on average. Figure 5: Pass@k accuracy (k [1, 64]) evaluation with Qwen3-8B on AIME 2024 and 2025 benchmarks. SWIREASONING achieves maximum reasoning accuracies +50% earlier compared to CoT on average. the contrary, when window sizes are too large, LLMs become sluggish to reenter latent exploration when confidence declines. promising improvement direction is to make adaptive to the models real-time density of effective reasoning. Thinking-Related Signal Mixing. SWIREASONING uses α0, β0 [0, 1] as the initial ratios for mixing thinking-related signals at switching instants (Sec. 3.3). We sweep α0 and β0 independently and report Pass@1 accuracies in Tab. 2."
        },
        {
            "title": "SWIREASONING",
            "content": "Table 2: Ablations on α0 and β0 for signal mixing. Greener indicates better performance within each column. α0 GSM8K MATH 500 GPQA Diamond AIME AIME 2025 Average β0 GSM8K MATH 500 GPQA Diamond AIME AIME 2025 Average 81.50% 67.20% 28.79% 8.33% 9.17% 39.00% 89.23% 89.80% 35.86% 46.67% 35.00% 59.31% 0.0 0.0 81.88% 70.20% 31.82% 11.67% 8.75% 40.86% 89.84% 91.00% 36.36% 46.25% 36.25% 59.94% 0.1 0.1 82.11% 70.60% 28.28% 14.17% 9.17% 40.87% 90.37% 91.60% 34.85% 46.25% 37.50% 60.11% 0.2 0.2 90.45% 91.60% 38.38% 47.08% 38.33% 61.17% 0.3 90.67% 92.00% 37.37% 45.42% 37.92% 60.68% 0.3 90.98% 91.40% 37.88% 47.92% 36.67% 60.97% 89.61% 92.80% 40.91% 48.33% 32.50% 60.83% 0.4 0.4 90.37% 91.20% 42.42% 47.92% 35.83% 61.55% 90.45% 93.00% 34.34% 50.83% 36.25% 60.97% 0.5 0.5 90.83% 92.00% 39.39% 44.58% 37.92% 60.94% 0.6 90.59% 90.40% 42.42% 42.50% 36.67% 60.52% 0.6 90.06% 91.60% 37.37% 45.00% 37.08% 60.22% 0.7 90.83% 93.00% 41.41% 50.83% 38.33% 62.88% 0.7 89.99% 92.20% 39.39% 49.17% 35.83% 61.32% 90.60% 92.00% 37.37% 48.33% 35.42% 60.74% 0.8 0.8 90.22% 92.20% 40.91% 48.75% 32.50% 60.52% 90.37% 90.80% 39.39% 50.42% 35.83% 61.36% 0.9 0.9 1.0 90.14% 90.60% 41.41% 49.17% 37.92% 61.85% 1.0 90.44% 91.00% 33.33% 46.67% 38.75% 60.04% For the exit bias β0, very small β0 implies excessive interference with when to conclude the thinking process and severely degrades accuracy (e.g., AIME24 drops to 8.33% at β0=0.0). Performance rises sharply and peaks near β0=0.7, which achieves the best average 62.88% and is either the best or the second-best on most datasets. promising improvement direction is to make β0 difficultyaware, so that it will be automatically adjusted based on problem difficulty. The situation for the entrance bias α0 is different. We observe broad performance plateau for α0 [0.4, 0.9], with the highest average at α0=1.0 (61.85%), however, only marginally higher than other values like α0=0.9 (61.36%). Task-wise, problems with different levels of difficulty tend to have various preferences over α0. We expose α0 to users for adjustment based on task difficulty. The more detailed hyperparameters we adopted for the experiments are provided in Appendix B.3. Maximum Switch Count. We suppress overthinking by bounding the number of mode switches with budget Cmax (Sec. 3.4), and reducing Cmax leads to earlier convergence. In Fig. 2 and Fig. 4, moving rightward on the xaxis corresponds to smaller token budgets, i.e., smaller Cmax. We collect data points in these figures by incrementing the value of Cmax from 1 until further increases in Cmax no longer alter generation results in most cases, i.e., maximum accuracy is reached at saturation. Detailed data points are provided in Appendix C.1. Table 3: Ablation on switch window size. Greener indicates better performance within each column. Window Size GSM8K MATH 500 GPQA Diamond AIME AIME 2025 Average 64 128 256 512 1024 89.69% 92.60% 40.91% 47.92% 34.17% 61.06% 90.45% 91.00% 38.89% 48.33% 36.25% 60.98% 89.76% 90.80% 39.90% 49.58% 36.25% 61.26% 90.83% 93.00% 41.41% 50.83% 38.33% 62.88% 90.83% 91.20% 40.40% 49.58% 36.67% 61.74% As analyzed in Sec. 4.3, decreasing Cmax yields significant improvement in token efficiency, which confirms the intended behavior of the switch count control design: it curbs prolonged latent exploration and commits to an answer path early, thereby mitigating overthinking. In summary, with switch count control, small number of confidence-aware blocks usually suffices for easy-tomoderate problems, while difficult instances benefit more from allowing few more switches before the final answer."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper presents SWIREASONING, training-free inference framework that integrates explicit chain-of-thought thinking with latent thinking through an entropy trendsbased controller. The framework is conceptually straightforward but empirically effective: when block-wise uncertainty decreases, we collapse to single explicit path to consolidate progress. When uncertainty rises and has persisted for minimal dwell window, we expand into latent space to explore more alternatives. Complementing this mode switch, switch count controller caps the number of transitions, thereby curbing overthinking while preserving prediction quality. Together, these two mechanisms yield consistently improved Pareto frontiers for reasoning LLMs, effectively enhancing both maximum accuracy under unlimited budgets and token efficiency under limited budgets. Looking ahead, integrating SWIREASONING with reinforcement learningbased training may unlock even stronger reasoning capabilities."
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This work focuses on enhancing the reasoning accuracy and token efficiency of LLMs, which does not raise safety concerns. This work involves no collection of sensitive data. All evaluations are conducted using publicly available models and benchmarks under their original licenses."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "We provide implementation details in Appendix B.1, details of the benchmark settings in Appendix B.2, and details of the hyperparameters in Appendix B.3 to facilitate reproducibility."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318, 2025. Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Anthropic. System card: Claude opus 4 & claude sonnet 4. 2025. URL https://www. anthropic.com/claude-4-system-card. Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: comprehensive survey on latent chain-of-thought reasoning. arXiv preprint arXiv:2505.16782, 2025. Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, robust, and hardware-aware speculative decoding. arXiv preprint arXiv:2402.12374, 2024. Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. DeepMind. Alphazero: Shedding new light on chess, https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/, 2024a. and go. shogi, URL DeepMind. Ai solves imo problems at silver medal level. URL https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/, 2024b."
        },
        {
            "title": "SWIREASONING",
            "content": "Yonggan Fu, Zhongzhi Yu, Junwei Li, Jiayi Qian, Yongan Zhang, Xiangchi Yuan, Dachuan Shi, Roman Yakunin, and Yingyan Celine Lin. Amoeballm: Constructing any-shape large language models for efficient and instant deployment. Advances in Neural Information Processing Systems, 37:7829978319, 2024. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lminfinite: Zero-shot extreme length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. HuggingFaceH4. Aime 2024 (american invitational mathematics examination 2024). Hugging Face dataset, 2024. URL https://huggingface.co/datasets/HuggingFaceH4/aime_ 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023. Nan Jiang, Ziming Wu, De-Chuan Zhan, Fuming Lai, and Shaobing Lian. Dart: Distilling autoregressive reasoning to silent thought, 2025. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36:3923639256, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, SongChun Zhu, Zixia Jia, Ying Nian Wu, et al. Seek in the dark: Reasoning via test-time instance-level policy gradient in latent space. arXiv preprint arXiv:2505.13308, 2025a. Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, and Rex Ying. Implicit reasoning in large language models: comprehensive survey. arXiv preprint arXiv:2509.02350, 2025b. Zeyi Liang, Anil K, Andrew H. Rodriguez, Dylan Freedman, Jordan Tilly, Illes Fegyo, Matan Plaut, Yunfan Lu, Shiori Sagawa, W. James Murdoch, James Zou, and Tatsunori B. Hashimoto. Llms are single-threaded reasoners. arXiv preprint arXiv:2507.06203, 2025."
        },
        {
            "title": "SWIREASONING",
            "content": "Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024. Tianqiao Liu, Zui Chen, Zitao Liu, Mi Tian, and Weiqi Luo. Expediting and elevating large language model reasoning via hidden chain-of-thought decoding. arXiv preprint arXiv:2409.08561, 2024. Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. Online speculative decoding. arXiv preprint arXiv:2310.07177, 2023. AI Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai. meta. com/blog/llama-4-multimodal-intelligence/, checked on, 4(7):2025, 2025a. AI Meta. Llama 3.3 model card. https://github.com/meta-llama/llama-models/ blob/main/models/llama3_3/MODEL_CARD.md, 2025b. OpenAI. Gpt-5 system card. System Card v2025-08-13, OpenAI, San Francisco, CA, August 2025a. URL https://cdn.openai.com/gpt-5-system-card.pdf. OpenAI. Openai o3-mini. URL https://openai.com/index/openai-o3-mini/, 2025b. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. arXiv preprint arXiv:2403.12968, 2024. Jacob Pfau, William Merrill, and Samuel Bowman. Lets think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024. Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, and Jiaqi Wang. Crossget: Cross-guided ensemble of tokens for accelerating vision-language transformers. In Forty-First International Conference on Machine Learning, 2024. Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, Xin Dong, Jan Kautz, Pavlo Molchanov, and Yingyan Celine Lin. Lacache: Ladder-shaped kv caching for efficient long-context modeling of large language models. In Forty-second International Conference on Machine Learning, 2025. DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning. arXiv preprint arXiv:2502.03275, 2025. Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36:3022230242, 2023."
        },
        {
            "title": "SWIREASONING",
            "content": "Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, and Xian Li. Llm pretraining with continuous concepts. arXiv preprint arXiv:2502.08524, 2025. Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, and Ruihua Song. Think silently, think fast: Dynamic latent compression of llm reasoning chains. arXiv preprint arXiv:2505.16552, 2025. Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. System-1.5 reasoning: Traversal in language and latent spaces with dynamic shortcuts. arXiv preprint arXiv:2505.18962, 2025a. Xinyuan Wang, Dongjie Wang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Sixun Dong, Kunpeng Liu, and Yanjie Fu. Efficient post-training refinement of latent reasoning in large language models, 2025b. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, and et al. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive arXiv preprint conditioning for controllability and toxicity reduction in language models. arXiv:2210.03162, 2022. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art In Proceedings of the 2020 conference on empirical methods in natural language processing. natural language processing: system demonstrations, pp. 3845, 2020. Haoyi Wu, Zhihao Teng, and Kewei Tu. Parallel continuous chain-of-thought with jacobi iteration, 2025. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134, 2025a. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot++: Test-time scaling with soft chainof-thought reasoning. arXiv preprint arXiv:2505.11484, 2025b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. In International Conference on React: Synergizing reasoning and acting in language models. Learning Representations (ICLR), 2023b. Yentinglin. Aime 2025 (american invitational mathematics examination 2025). Hugging Face dataset, 2025. URL https://huggingface.co/datasets/yentinglin/aime_ 2025. Xiangchi Yuan, Chunhui Zhang, Zheyuan Liu, Dachuan Shi, Soroush Vosoughi, and Wenke Lee. Superficial self-improved reasoners benefit from model merging. arXiv preprint arXiv:2503.02103, 2025."
        },
        {
            "title": "SWIREASONING",
            "content": "Zhenrui Yue, Bowen Jin, Huimin Zeng, Honglei Zhuang, Zhen Qin, Jinsung Yoon, Lanyu Shang, Jiawei Han, and Dong Wang. Hybrid latent reasoning via reinforcement learning. arXiv preprint arXiv:2505.18454, 2025. Boyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu Li, and Zhouhan Lin. Pretraining language models to ponder in continuous space, 2025. Bowen Zhang, Yanzhuo Li, Shuohang Wang, Yu Tu, Jason Wei, Ashish Vaswani, Shikun Liu, Chenlu Yu, Deli Chen, Hongyi Li, Zhihua Zhang, and Chen Liang. Soft thinking: Training-free latent reasoning for large language models. arXiv preprint arXiv:2505.15778, 2025. Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, and Yuandong Tian. Reasoning by superposition: theoretical perspective on chain of continuous thought. arXiv preprint arXiv:2505.12514, 2025a. Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, et al. survey on latent reasoning. arXiv preprint arXiv:2507.06203, 2025b."
        },
        {
            "title": "A USE OF LLMS DISCLOSURE",
            "content": "We employed GPT-5 (OpenAI, 2025a) from OpenAI to assist with language polishing in order to improve the readability of the paper. We affirm that large language models were not misused intentionally in any part of this work. All intellectual contributions are attributed to the human authors, and the results presented in this paper are entirely the product of human research efforts."
        },
        {
            "title": "B SUPPLEMENTARY DETAILS",
            "content": "B.1 SWIREASONING IMPLEMENTATION Algorithm 1 SWIREASONING Input: Question x1:n, model M, max steps Tmax, coefficient α0, coefficient β0, dwell window WEL, max switches Cmax, and answer budget Output: Answer y1:m mt Explicit; pt[v] log pt[v] continue if = 1 then H1; 0 if mt1 = Latent and Ht < then xt Q.pop() if = 0 then break if > 0 then 1 // Entropy // Token injection (convergence/termination prefix) ℓt M(x1:t1); pt softmax(ℓt); Ht (cid:80) if = then 1 Init: Mode m0 Latent, switch counter 0, injection queue , budget flag 1 2 for = 1 to Tmax do 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 else if mt1 = Explicit and Ht > and WEL then Ht; 0 et (cid:80) if mt = Latent and = 0 then // Switch count control (Sec. 3.4) // Convergence trigger [ ID[/thinknn The final answer is] ]; Ht; 0; + else if mt = Explicit and > Cmax then if mt = Explicit and > 0 then xt arg maxv pt[v] or Sampling 2 Cmax Cmax then // Thinking-related signal mixing mt mt1; t + if mt = Explicit and 1 // Mode switching (Sec. 3.3) // Termination trigger [ ID[/think] ] mt Latent; pt[v] E[v] t Tmax else else // Thinking-related signal mixing // Soft embeddings feed as inputs αt = α0 + (1 α0) et αt et + (1 αt) ethink if mt = Explicit and = 0 then βt = β0 + (1 β0) et βt et + (1 βt) e/think Tmax 30 31 32 xt et if xt = <EOS> then 33 34 35 36 Extract answer from xn+1:t 37 return break Alg. 1 provides detailed implementation of SWIREASONING, where the implementation for mode switching is written in black and switch count control for token efficiency is outlined in blue."
        },
        {
            "title": "SWIREASONING",
            "content": "B.2 BENCHMARKS DETAILS We conduct evaluation on five reasoning benchmarks, including GSM8K (Cobbe et al., 2021), Math500 (Hendrycks et al., 2021), AIME 2024 (HuggingFaceH4, 2024), AIME 2025 (Yentinglin, 2025) for mathematical domain reasoning, and GPQA Diamond (Rein et al., 2024) for STEM domain reasoning. GSM8K: We use the test set of 1,319 grade-school math word problems, designed to evalu- : https://huggingface. ate multi-step arithmetic reasoning in natural language. co/datasets/openai/gsm8k. MATH500: curated set of 500 problems from the MATH dataset, covering diverse highschool competition-level mathematics domains such as algebra, geometry, and number the- : https://huggingface.co/datasets/HuggingFaceH4/MATH-500. ory. AIME 2024: Contains 30 problems from the 2024 American Invitational Mathematics Examination, each requiring concise numeric answer and reflecting competition-level dif- : https://huggingface.co/datasets/HuggingFaceH4/aime_ ficulty. 2024. AIME 2025: Contains 30 problems from the 2025 American Invitational Mathematics Examination, continuing the focus on competition-style math reasoning with challeng- : https://huggingface.co/ ing questions that test symbolic and logical skills. datasets/yentinglin/aime_2025. GPQA Diamond: high-quality subset of about 198 carefully verified questions, focusing on STEM disciplines including mathematics, physics, chemistry, biology, and computer science, designed to evaluate expert-level factual knowledge and reasoning ability. : https://huggingface.co/datasets/hendrydong/gpqa_diamond_mc. To provide LLMs sufficient thinking space, following the same settings in Qwen3 (Yang et al., 2025), we set the maximum generation length to 32,768 tokens for GSM8K, Math500, and GPQA Diamond benchmarks, and 38,912 tokens for AIME 2024 and AIME 2025 benchmarks. We repeat the experiments eight times and report the average accuracy for both SWIREASONING and other baselines on the AIME 2024 and AIME 2025 benchmarks. B.3 BEST PRACTICE FOR HYPERPARAMETERS Table 4: Hyperparameters for mode switching across datasets and models. and β0 are fixed across all scenarios, while α0 provides users with flexibility for adjustment depending on the task. Hyperparameter Dataset Qwen3-8B Qwen3-1.7B DeepSeek-R1-Distill-Llama-8B Model (window size) α0 (user-exposed) β0 GSM8K MATH500 AIME2024 AIME2025 GPQA Diamond GSM8K MATH500 AIME2024 AIME2025 GPQA Diamond GSM8K MATH500 AIME2024 AIME2025 GPQA Diamond 512 (fixed for all) 0.5 1.0 0.9 0.9 1.0 0.6 0.5 0.5 0.3 1.0 0.1 0.5 0.65 0.7 0. 0.7 (fixed for all) In addition to Tab. 4, SWIREASONING operates as straightforward and instant substitution to model.generate() interface of Huggingfaces transformers (Wolf et al., 2020) package. There"
        },
        {
            "title": "SWIREASONING",
            "content": "are no model parameters or architecture changes, and no training was used in the experiments. For sampling-related hyperparameters and prompt templates, we use the ones recommended by Qwen3 and DeepSeek-R1s technical report (Yang et al., 2025; Guo et al., 2025) without modification. B.4 BROADER RELATED WORK Efficient LLM Reasoning. In terms of improving reasoning efficiency, there are broader techniques including but not limited to KV cache compression (Han et al., 2023; Xiao et al., 2023; Cai et al., 2024; Shi et al., 2025), prompt token compression (Wingate et al., 2022; Jiang et al., 2023; Pan et al., 2024; Shi et al., 2024), speculative decoding (Leviathan et al., 2023; Kim et al., 2023; Liu et al., 2023; Sun et al., 2023; Chen et al., 2024), traditional methods such as quantization, pruning, distillation (Lin et al., 2024; Fu et al., 2024; Yuan et al., 2025), and system-level optimizations such as FlashAttention (Dao et al., 2022; Dao, 2023; Shah et al., 2024). SWIREASONING, however, targets different axis of efficiency and is not aiming to surpass them. Instead, it saves tokens by dynamically alternating between latent steps and explicit steps and limiting the number of block switches. As such, it is plug-and-play during inference and can be layered on top of the aforementioned techniques for multiplicative gains."
        },
        {
            "title": "C SUPPLEMENTARY EXPERIMENTS",
            "content": "C.1 DETAILED EVALUATION RESULTS UNDER VARYING TOKEN BUDGETS We provide detailed evaluation results of Qwen3-8B in Tab. 5-9, Qwen3-1.7B in Tab. 10-14, and DeepSeek-R1-Distill-Llama-8B in Tab. 15-19. Table 5: Evaluation results of Qwen3-8B on the GSM8K benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method CoT (Greedy) SwiR (Ours) CoT (Greedy) CoT CoT CoT CoT (Greedy) Soft Thinking Soft Thinking CoT (Greedy) CoT SwiR (Ours) Soft Thinking SwiR (Ours) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 2240 2218 2199 2138 2136 2123 2115 2073 2033 1934 1926 1879 1865 1761 1585 1553 1540 1526 1297 990 988 988 844 512 512 512 301 256 256 256 95.68% 96.06% 95.75% 95.60% 95.60% 94.77% 95.15% 95.38% 95.07% 92.65% 91.81% 94.84% 92.12% 95.14% 94.39% 79.90% 79.68% 80.14% 94.47% 44.50% 45.79% 47.08% 93.70% 25.47% 25.93% 24.87% 92.19% 6.36% 6.07% 6.22%"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 6: Evaluation results of Qwen3-8B on the MATH500 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method CoT (Greedy) SwiR (Ours) CoT Soft Thinking Soft Thinking CoT CoT (Greedy) SwiR (Ours) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT Soft Thinking CoT (Greedy) SwiR (Ours) CoT Soft Thinking CoT (Greedy) Accuracy (%) Generation Length 96.40% 98.40% 96.00% 96.00% 95.40% 95.60% 94.00% 95.80% 93.80% 87.00% 87.00% 85.80% 93.00% 90.20% 72.40% 72.80% 70.20% 85.80% 46.20% 44.80% 43.00% 78.40% 24.20% 25.00% 22.20% 5311 5183 4985 4934 4733 4729 4565 4266 4057 3899 3819 3774 3635 3164 2940 2890 2865 2387 1922 1898 1873 1368 1024 1024 1023 Table 7: Evaluation results of Qwen3-8B on the GPQA Diamond benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method SwiR (Ours) Soft Thinking CoT CoT (Greedy) CoT CoT (Greedy) Soft Thinking SwiR (Ours) SwiR (Ours) SwiR (Ours) CoT (Greedy) CoT Soft Thinking SwiR (Ours) SwiR (Ours) CoT (Greedy) Soft Thinking CoT SwiR (Ours) Soft Thinking CoT CoT (Greedy) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 8359 8153 8123 7909 7570 7546 7433 7100 6338 5710 5086 4972 4961 4766 3603 3078 2959 2861 2117 1753 1743 1723 1527 1024 1024 1024 867 512 512 512 61.11% 59.59% 59.60% 56.57% 55.56% 55.05% 55.05% 58.08% 57.07% 58.08% 33.84% 33.33% 34.85% 55.05% 53.54% 12.12% 12.63% 10.61% 46.96% 2.53% 2.52% 2.02% 47.47% 0.00% 0.00% 0.00% 39.39% 0.00% 0.00% 0.00%"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 8: Evaluation results of Qwen3-8B on the AIME2024 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method SwiR (Ours) Soft Thinking CoT CoT (Greedy) SwiR (Ours) SwiR (Ours) CoT (Greedy) Soft Thinking CoT SwiR (Ours) SwiR (Ours) Soft Thinking CoT CoT (Greedy) SwiR (Ours) CoT Soft Thinking CoT (Greedy) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 12491 12271 12077 11680 10815 10349 10328 9846 9818 9275 8115 7343 7109 7033 6093 4096 4096 4056 3589 2048 2048 2048 1809 1024 1024 1024 818 512 512 79.17% 67.92% 75.83% 70.00% 69.58% 66.25% 66.67% 62.92% 63.75% 61.25% 57.08% 36.67% 38.75% 36.67% 45.42% 20.83% 23.33% 26.67% 25.42% 5.83% 10.00% 3.75% 12.08% 1.67% 3.33% 3.33% 6.67% 0.83% 0.00% 3.33%"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 9: Evaluation results of Qwen3-8B on the AIME2025 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method SwiR (Ours) Soft Thinking CoT (Greedy) CoT SwiR (Ours) SwiR (Ours) CoT Soft Thinking SwiR (Ours) CoT (Greedy) SwiR (Ours) CoT Soft Thinking CoT (Greedy) SwiR (Ours) CoT (Greedy) CoT Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 13911 13665 13292 12924 11482 10596 10215 9952 9791 9143 8220 6887 6772 6768 6243 4096 4091 4060 3608 2048 2048 2048 1999 1024 1024 1024 722 512 512 70.00% 68.33% 60.00% 67.50% 62.92% 58.33% 54.17% 51.25% 56.25% 43.33% 46.25% 34.58% 36.25% 33.33% 34.58% 13.33% 13.33% 14.17% 21.67% 7.50% 6.67% 6.25% 11.25% 1.67% 0.00% 3.33% 6.67% 2.50% 0.00% 3.33%"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 10: Evaluation results of Qwen3-1.7B on the GSM8K benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method CoT (Greedy) SwiR (Ours) CoT CoT (Greedy) Soft Thinking Soft Thinking CoT Soft Thinking CoT (Greedy) CoT CoT (Greedy) Soft Thinking SwiR (Ours) SwiR (Ours) SwiR (Ours) CoT Soft Thinking CoT (Greedy) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 89.61% 90.83% 90.45% 89.61% 90.30% 90.22% 89.23% 89.84% 89.31% 86.35% 87.64% 87.49% 89.23% 89.46% 87.95% 76.65% 78.92% 78.17% 88.32% 50.57% 48.52% 50.95% 86.80% 29.95% 30.02% 31.54% 82.26% 7.96% 8.34% 9.25% 2038 2010 1981 1968 1959 1946 1928 1896 1895 1753 1744 1736 1695 1621 1462 1420 1418 1407 1229 967 959 958 816 512 512 512 296 256 256 256 Table 11: Evaluation results of Qwen3-1.7B on the MATH500 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method SwiR (Ours) CoT (Greedy) CoT Soft Thinking CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT (Greedy) CoT Soft Thinking SwiR (Ours) SwiR (Ours) SwiR (Ours) CoT Soft Thinking CoT (Greedy) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 4924 4799 4780 4721 4435 4342 4288 3862 3681 3655 3605 3597 3280 2914 2761 2744 2738 2222 1857 1850 1830 1189 1022 1020 1020 93.00% 91.00% 92.00% 90.60% 90.80% 89.20% 89.00% 87.80% 83.60% 83.60% 83.60% 87.80% 86.80% 85.00% 68.40% 69.40% 69.20% 81.00% 46.40% 47.80% 46.20% 72.40% 25.60% 29.80% 27.20%"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 12: Evaluation results of Qwen3-1.7B on the GPQA Diamond benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method SwiR (Ours) CoT (Greedy) CoT (Greedy) Soft Thinking CoT CoT SwiR (Ours) Soft Thinking SwiR (Ours) SwiR (Ours) SwiR (Ours) CoT Soft Thinking CoT (Greedy) SwiR (Ours) Soft Thinking CoT CoT (Greedy) SwiR (Ours) CoT SwiR (Ours) CoT (Greedy) Soft Thinking CoT CoT (Greedy) Soft Thinking SwiR (Ours) Accuracy (%) Generation Length 41.41% 31.82% 30.30% 34.34% 39.39% 37.37% 36.87% 32.32% 35.35% 37.88% 36.87% 17.17% 16.67% 12.12% 37.37% 8.08% 8.59% 7.58% 31.31% 1.01% 27.27% 2.53% 0.51% 0.00% 0.00% 0.00% 29.80% 9517 9190 8751 8731 8625 8040 7773 7447 6792 5856 4766 4758 4463 3770 3497 2915 2843 2344 2112 1661 1573 1539 1378 1024 1024 1024 933 Table 13: Evaluation results of Qwen3-1.7B on the AIME2024 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method CoT (Greedy) SwiR (Ours) CoT Soft Thinking SwiR (Ours) Soft Thinking CoT CoT (Greedy) SwiR (Ours) SwiR (Ours) Soft Thinking CoT (Greedy) SwiR (Ours) CoT SwiR (Ours) CoT (Greedy) Soft Thinking CoT SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) Accuracy (%) Generation Length 12825 12702 11896 10788 10243 9841 9510 9377 9350 8654 7498 7302 7084 6978 5926 4096 4096 4063 3411 2048 2048 2048 1887 1024 1024 1024 812 40.00% 50.83% 45.83% 38.75% 42.08% 36.67% 38.33% 33.33% 38.75% 36.25% 30.00% 23.33% 29.17% 27.50% 25.42% 10.00% 16.67% 13.75% 14.17% 2.92% 0.00% 3.33% 7.50% 1.25% 0.00% 0.00% 5.83%"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 14: Evaluation results of Qwen3-1.7B on the AIME2025 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method CoT (Greedy) SwiR (Ours) CoT Soft Thinking CoT (Greedy) CoT SwiR (Ours) SwiR (Ours) Soft Thinking SwiR (Ours) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) Accuracy (%) Generation Length 33.33% 38.33% 33.33% 36.67% 26.67% 31.25% 34.58% 32.92% 33.33% 32.08% 30.83% 20.83% 20.00% 23.33% 25.42% 14.17% 13.33% 20.00% 17.50% 7.92% 10.00% 10.00% 8.75% 2.08% 3.33% 0.00% 4.58% 11408 9944 9733 8904 8890 8618 8543 8129 7630 7563 6761 6071 6008 5738 5145 4096 3927 3737 3311 2048 2048 2048 1865 1024 1024 1024 787 Table 15: Evaluation results of DeepSeek-R1-Distill-Llama-8B on the GSM8K benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method Soft Thinking Soft Thinking CoT (Greedy) Soft Thinking Soft Thinking CoT (Greedy) CoT SwiR (Ours) CoT CoT CoT (Greedy) CoT SwiR (Ours) SwiR (Ours) CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) SwiR (Ours) Soft Thinking CoT CoT (Greedy) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 85.90% 84.84% 85.82% 83.62% 77.41% 86.05% 89.46% 90.07% 88.10% 88.02% 85.82% 86.13% 88.55% 89.46% 85.44% 70.36% 87.26% 78.62% 79.53% 86.43% 49.58% 53.22% 57.39% 83.62% 27.37% 28.81% 28.35% 70.96% 5.69% 6.52% 5.46% 23 2953 2516 2393 2266 1741 1642 1588 1565 1554 1491 1421 1404 1349 1312 1307 1279 1217 1191 1092 1071 885 883 839 775 509 508 508 270 256"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 16: Evaluation results of DeepSeek-R1-Distill-Llama-8B on the Math500 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method Soft Thinking CoT (Greedy) Soft Thinking SwiR (Ours) CoT CoT Soft Thinking CoT (Greedy) SwiR (Ours) SwiR (Ours) CoT SwiR (Ours) CoT (Greedy) SwiR (Ours) Soft Thinking CoT CoT (Greedy) SwiR (Ours) Soft Thinking CoT CoT (Greedy) SwiR (Ours) CoT Soft Thinking CoT (Greedy) SwiR (Ours) Accuracy (%) Generation Length 83.80% 84.80% 82.60% 92.00% 91.40% 89.80% 75.40% 83.20% 89.80% 88.00% 85.20% 86.40% 79.60% 86.00% 64.40% 71.00% 71.20% 79.20% 42.60% 47.60% 52.60% 68.40% 12.20% 10.20% 11.80% 57.40% 4718 4110 4085 3837 3792 3572 3204 3203 3046 2931 2828 2722 2622 2462 2396 2133 2081 1953 1605 1539 1500 1116 512 511 511 453 Table 17: Evaluation results of DeepSeek-R1-Distill-Llama-8B on the GPQA Diamond benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method Soft Thinking CoT Soft Thinking SwiR (Ours) CoT SwiR (Ours) SwiR (Ours) CoT (Greedy) SwiR (Ours) CoT (Greedy) CoT Soft Thinking SwiR (Ours) CoT (Greedy) SwiR (Ours) CoT (Greedy) Soft Thinking CoT SwiR (Ours) CoT CoT (Greedy) Soft Thinking CoT (Greedy) CoT Soft Thinking SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 8593 7591 7507 7458 7236 6635 6038 5854 5406 5230 4943 4510 4388 4301 3292 2625 2350 2314 1840 1595 1469 1381 918 855 785 673 512 512 511 33.33% 46.46% 30.81% 47.98% 46.46% 45.45% 44.44% 31.82% 41.92% 27.78% 31.31% 16.67% 39.90% 23.73% 41.41% 11.62% 6.57% 9.60% 25.76% 2.53% 3.54% 3.03% 0.50% 1.01% 1.01% 29.80% 0.00% 0.00% 0.00%"
        },
        {
            "title": "SWIREASONING",
            "content": "Table 18: Evaluation results of DeepSeek-R1-Distill-Llama-8B on the AIME2024 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method SwiR (Ours) CoT CoT (Greedy) SwiR (Ours) CoT CoT (Greedy) Soft Thinking SwiR (Ours) SwiR (Ours) SwiR (Ours) Soft Thinking CoT CoT (Greedy) Soft Thinking SwiR (Ours) CoT (Greedy) Soft Thinking CoT SwiR (Ours) SwiR (Ours) CoT CoT (Greedy) Soft Thinking Accuracy (%) Generation Length 45.00% 43.75% 30.00% 44.17% 41.25% 26.66% 34.17% 40.42% 41.25% 39.58% 32.08% 28.33% 16.67% 27.50% 32.50% 6.67% 11.67% 14.58% 23.33% 8.75% 1.67% 3.33% 3.33% 8179 8145 7840 7555 7330 7107 6956 6803 6645 5876 5871 5086 4952 4860 4757 3795 3784 3515 3419 2103 2048 2048 2045 Table 19: Evaluation results of DeepSeek-R1-Distill-Llama-8B on the AIME2025 benchmark under varying token budgets. Rows are sorted by generation length in descending order. Method Soft Thinking SwiR (Ours) Soft Thinking CoT SwiR (Ours) CoT (Greedy) SwiR (Ours) CoT SwiR (Ours) SwiR (Ours) CoT (Greedy) CoT (Greedy) Soft Thinking CoT SwiR (Ours) CoT Soft Thinking SwiR (Ours) CoT (Greedy) CoT Soft Thinking CoT (Greedy) SwiR (Ours) Accuracy (%) Generation Length 8448 6827 6824 6583 6419 6293 6230 5724 5721 5229 4967 4370 4170 4085 4035 3197 3023 2970 2862 2048 2048 1904 1777 20.42% 31.25% 19.17% 26.25% 30.83% 30.00% 30.00% 25.00% 29.17% 26.67% 26.67% 23.33% 14.17% 19.58% 21.25% 14.17% 10.00% 16.67% 16.67% 3.33% 0.00% 3.33% 7.08%"
        }
    ],
    "affiliations": [
        "Georgia Tech",
        "Microsoft"
    ]
}