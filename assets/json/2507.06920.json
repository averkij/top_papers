{
    "paper_title": "Rethinking Verification for LLM Code Generation: From Generation to Testing",
    "authors": [
        "Zihan Ma",
        "Taolin Zhang",
        "Maosong Cao",
        "Wenwei Zhang",
        "Minnan Luo",
        "Songyang Zhang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 0 2 9 6 0 . 7 0 5 2 : r Rethinking Verification for LLM Code Generation: From Generation to Testing Zihan Ma1,2,3,, Taolin Zhang1,, Maosong Cao1, Wenwei Zhang1, Minnan Luo2,3,, Songyang Zhang1,, , ,Kai Chen1, 1Shanghai AI Laboratory 2School of Computer Science and Technology, Xian Jiaotong University, China 3MOE KLINNS Lab, Xian Jiaotong University, China {mazihan880}@stu.xjtu.edu.cn {zhangtaolin,zhangsongyang}@pjlab.org.cn {minnluo}@xjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, detailed examination reveals that these evaluation suites often comprise only limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves detection rate of 90.62% and verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have triggered paradigm shift in automatic code generation, demonstrating capabilities on par with or even exceeding human programmers on numerous benchmark tasks. As LLMs become increasingly integrated into software development workflows, ensuring the quality and reliability of the code they produce is paramount. This necessitates reliable evaluation methodologies, where code verifierstypically powered by test suitesplay critical role. This raises crucial question: Are the test cases of current benchmarks for evaluating models code capabilities robust enough? means corresponding authors, means project, means authors contributed equally. 1The data demo and prompts can be accessed via https://github.com/open-compass/SAGA Preprint. Initial analyses of benchmarks like HumanEval [6] (avg. 7.7 tests/problem), MBPP [3] (3 tests/problem), and EvalPlus [27, 28] (which saw 15% pass rate drop with 80 more tests) indicate the fragility of current evaluation setups due to sparse test coverage. While methods like TestEval [41] tailor tests to specific solutions, they are inefficient for large-scale evaluation and impractical for dynamic integration into RL training loops, thereby hindering the development of models robust against diverse failures. LiveCodeBench [19] employs LLMs to generate numerous tests from golden solutions and synthetic inputs, aiming to enhance robustness. However, fundamental concern is that such methods may inadvertently create tests biased towards typical, often homogenized, LLM error patterns, which starkly contrast with diverse human reasoning errors. Conversely, competitive programming platforms (Online Judges) possess extensive, rigorously curated test suites for assessing code robustness, but these are often private and inaccessible. This underscores the urgent need for accessible and robust code verifiers to enable reliable performance evaluation and reward estimation. Figure 1: (a) Verifiers synthesized primarily from LLM-generated data exhibit high failure rate when testing human-written bugs. (b) PCA analysis reveals that LLM-induced errors are highly clustered, indicating systematic weaknesses, whereas human errors are diverse and dispersed, posing greater challenges to existing verifiers. In this work, we first identify limitations in current benchmarks through preliminary experiments. Our investigation reveals critical weaknesses in existing verifier suites. Specifically, when we took LLM-generated solutions that had passed LiveCodeBenchs private tests and reevaluated them on LeetCodes online judge, we found that for significant portion of these solutions20% for medium and 40% for hard problems, respectivelyLeetCode identified errors that LiveCodeBenchs verifier had missed. This demonstrates that LiveCodeBenchs verifiers can be flawed, failing to achieve comprehensive error detection and thereby overestimating the true quality of the LLM-generated solutions. Furthermore, these LLM-centric verifiers themselves exhibit high failure rate (i.e., they fail to detect existing bugs) when evaluating human-written faulty code, rate much higher than any apparent failure rate on LLM-generated errors (Figure 1(a)). PCA analysis of error patterns (Figure 1(b)) reveals that LLM errors cluster tightly, indicating shared systematic biases, while human errors are widely distributed across complex error landscape. These findings underscore the fundamental inadequacy of constructing verifiers solely from LLMgenerated data, as such approaches misassess LLMs coding capabilities and provide flawed training feedback, manifesting two critical challenges: (1) Test Case Homogenization and LLM-Centric Bias: LLM-based TCG methods produce test suites that mirror the generating models error patterns and cognitive biases, creating homogenization trap where tests focus on LLM-like failures while neglecting diverse human programming errors (e.g., logical flaws, integer overflows). (2) Verifier Ineffectiveness and Persistent Blind Spots: Verifiers built on such test suites exhibit blind spots for human-like errors, failing to rigorously evaluate code due to challenges in generating tests for complex boundary conditions and interaction scenarios, compounded by LLM-centric test design. These challenges impede reinforcement learning frameworks (e.g., DeepSeek-R1 [8], O1-IOI [10]) from leveraging verifiable rewards, leading to optimization misdirection via reward hacking2. To address these challenges, we develop comprehensive measurement framework for code verifiers, introducing multi-dimensional evaluation metrics (detection rate, verifier accuracy) and identifying test case diversity and per-case strength as critical quality factors. We derive an upper bound for detection rate and validate it empirically across 1,500+ coding problems. Using this framework, we uncover systemic test quality issues in leading benchmarks, such as CodeForce-CoTs [37], where 50% of problems had tests failing to detect known errors and 84% of verifiers were flawed, further underscoring these limitations. To systematically improve the quality of code verifiers, we formally define the Test Case Generation (TCG) task, comprising three core components: task formulation, benchmark construction, and method exploration. We introduce TCGBench, benchmark curated by aggregating representative 2Models exploit verifier weaknesses instead of achieving genuine correctness, as they are insufficiently penalized for diverse errors. 2 problems from three leading competitive programming platformsAtcoder, Codeforces, and Nowcoder. TCGBench curates human-verified adversarial examples spanning diverse error patterns (e.g., logical flaws, edge cases) and supports comprehensive evaluation of TCG methods. We further investigate current TCG methods and propose SAGA (Strategic Adversarial & Constraint-differential GenerAtive workflow), novel human-LLM collaborative framework. SAGA is designed to systematically generate high-coverage, highly discriminative test cases by leveraging both human-derived constraints from correct solutions and insights from failure modes in incorrect solutions. This dualpronged analytical approach allows SAGA to achieve improvements over current state-of-the-art TCG methods, with the Detection Rate increasing by 9.55% and Verifier Accuracy by 12.14%. Additionally, we leverage SAGA to enhance the quality of the popular code generation benchmark LiveCodeBench-v6 (subset)3, and develop CodeCompass, new high-quality code benchmark. We believe SAGA can further be employed to scale datasets, enabling the production of training data with robust and accurate reward estimation for coding tasks. Our main contributions are as follows: We construct TCGBench, comprehensive dataset from competitive programming platform, to analyze existing Test Case Generation practices. On it, we formalize multi-dimensional metrics for rigorous test case quality evaluation. We propose and validate SAGA, novel human-LLM collaborative TCG framework  (Fig. 5)  . By integrating insights from both correct and incorrect human solutions, SAGA generates significantly more effective test suites, improving Verifier Accuracy by 15.86% over existing TCG methods. Using SAGA, we develop CodeComPass, challenging benchmark with human-verified adversarial examples for robust code generation evaluation, and introduce TCGCoder-7B, SAGA-distilled specialist model for capable TCG."
        },
        {
            "title": "2 Evaluating Verifier Quality: Metrics and TCG Paradigms",
            "content": "The reliable evaluation of LLM-generated code is critically constrained by the quality and accessibility of verifiers. Standard benchmarks like HumanEval [6] often employ limited test suites, while the extensive private verifiers of Online Judges remain largely inaccessible for broader research. This scarcity of robust, accessible verifiers impedes accurate LLM evaluation and the advancement of RLVR. To surmount this challenge, we focus on leveraging LLMs themselves for Test Case Generationthe systematic synthesis of test suites. Figure 2 illustrates the pivotal role of Code Verifierin the LLM code evaluation pipeline. It also highlights distinct TCG paradigms: Figure 2: The code evaluation pipeline and different TCG paradigms. Direct Generation: An LLM directly produces complete test cases (inputs and outputs). Representative works include TestChain [22], AceCoder [46], and CodeRM [31]. Input-Interpreter: An LLM generates test inputs; ground-truth interpreter (or reference solution) then computes the corresponding outputs. This paradigm, exemplified by LiveCodeBench [19] and Codeforce-COT [37], often employs random input samplinga strategy proven effective by LiveCodeBench for generating numerous diverse test cases and thus adopted in our baseline evaluations. EvalPlus [27], which mutates seed inputs for execution, also aligns with this approach. comparative analysis with EvalPlus is detailed in Section 3.2.1. Human Priors (Our Approach): LLM-driven TCG is guided by structured human expertise, strategy central to our proposed SAGA framework (detailed in Section 3). 3The subset used in our study comprises 101 problems from AtCoder, specifically from contests ABC387 through ABC400 and ARC190 through ARC196 (problems to F). This selection facilitates consistent evaluation scope when comparing with benchmarks like our proposed TCGBench, which also incorporates these recent algorithmic challenges. 3 Figure 3: Direct generation issues: (a) Low quality of LLM-generated tests. (b) High self-pass rates suggest model blind spots. 2.1 Problem Definition Formally, for programming problem with description D, input space XP , and ground-truth solution fP : XP YP , TCG method aims to produce test case set = {(Ii, Oi)}n i=1, where each input Ii XP and its corresponding output Oi = fP (Ii). To quantify the quality of such generated test suites, we employ two key metrics: Definition 1 (Detection Rate (DR)). solution-level metric, DR quantifies test suites ability to detect errors in specific incorrect candidate solution = fP . It is the probability that identifies at least one error in S: ϵS(T ) = ((I, O) s.t. S(I) = O) . If Ei = {S(Ii) = fP (Ii)} is the event that fails on test case i, then ϵS(T ) = 1 (cid:0)(cid:84)n (cid:1) . higher average DR across many incorrect solutions indicates the test suite is effective at exposing diverse faults. i=1 Ei Definition 2 (Verifier Accuracy (VAcc)). problem-level metric, VAcc assesses whether test suite can successfully identify all known incorrect solutions for given problem . Given Swrong(P ) = {S : XP YP = fP } as the set of all incorrect solutions for , the Verifier Accuracy is: VAcc(T ) = (S Swrong(P ), (I, O) s.t. S(I) = O) , where I() is the indicator function. VAcc(T ) = 1 if rejects every solution in Swrong(P ), signifying perfect diagnostic capability. 2.2 Investigating Current TCG Paradigms and Their Limitations To ground our exploration of TCG effectiveness, we leverage TCGBench, dataset we curated comprising 1840 recent programming problems from Atcoder, Codeforces, and Nowcoder, along with an average of 36.66 incorrect user submissions per problem. This rich resource (detailed in Appendix D) facilitates rigorous evaluation of TCG methodologies. Our analysis employ opensource LLMs: DeepSeek-V3-0324, Qwen2.5-72B-Instruct, and Qwen2.5-Coder-32B-Instruct with the greedy decoding strategy. For paradigm 1: Is Direct LLM-based Test Case Generation Effective? Directly prompting an LLM to generate complete test cases (inputs LLM ) from problem description , depends heavily on the LLMs deep comprehension, particularly of edge cases. Our experiments (Figure 3), involving LLM generation of 50 diverse test cases per problem, reveal the issues. The retention rate (proportion of valid tests post-verification against ground truth) is very low, indicating unreliable quality. This results in poor overall DR (often <60%) and VAcc (<10%). Moreover, LLM-generated solutions easily pass these self-generated tests. Notably, on AtCoder problems with historical official tests4, LLM solutions performed substantially better on their own generated tests, suggesting such tests fail to challenge the models cognitive biases. , outputs OLLM For paradigm 2: Can Large Number of Inputs from an Input-Interpreter Approach Compensate for Low Quality? The Input-Interpreter paradigm, where an LLM generates random inputs Rand XP for groundtruth interpreter, is employed by benchmarks like LiveCodeBench [19]. While generating many tests is feasible, merely increasing the quantity does not fundamentally improve the detection rate. This 4Early AtCoder problems provided official test cases; however, from December 2024 onwards, test cases are no longer publicly available for new contests. 4 Figure 4: Experimental validation of Input-Interpreter (random sampling) limitations on TCGBench. (a) Detection rate vs. number of test cases (linear scale), showing clear saturation below 100%. (b) Detection rate vs. log of the number of test cases (semi-log scale), illustrating diminishing returns consistent with the theoretical upper bound 1 (1 p)neff , validating the impact of correlation ρ. limitation arises from inherent correlations between test cases. We propose here corollary for the upper bound of the detection rate5: Corollary 1 (Asymptotic Saturation of Detection Rate). As the number of generated test cases , if (the average probability of single test detecting an error, 0 < < 1) and ρeff (the effective average positive correlation between detection events, ρeff > 0) are stable characteristics, the approximate upper bound on the detection rate ϵS(T ) converges to: limn ϵS(T ) 1 (1 p)1/ ρeff < 1. This corollary implies that due to inter-test correlation ρeff , simply increasing the number of random tests cannot guarantee 100% error detection; the detection rate will saturate. The underlying reasoning involves the concept of an \"effective sample size\" neff n/(1 + (n 1)ρeff ) [20], which quantifies the diminishing utility of additional correlated tests. Our experiments on TCGBench (Figure 4) confirm this: detection rates plateau (Fig. 4(a)), and marginal gains diminish rapidly. Notably, plotting the detection rate against the logarithm of the number of test cases (Fig. 4(b)) reveals trend consistent with our derived theoretical upper bound, further validating the impact of correlation ρ. Beyond DR and Acc, to more deeply quantify the intrinsic quality and efficiency of TCG strategies like SAGA (introduced in Section 3), we employ two advanced metrics. These provide observable insights into test suite characteristics related to (average test potency) and ρeff (inter-test correlation): Distinct Error Pattern Coverage (DEPC): For test suite and NP problems, let v(tk) be the (cid:8)v(tk) tk and v(tk)1 1(cid:9)(cid:12) (cid:12) error pattern vector for test tk. DEPC is (cid:12). The Diversity Ratio is DEPC(T )/n. Higher DEPC suggests lower ρeff , indicating broader unique error detection. Normalized Area Under the Accuracy-Number of test cases Curve (AUC-AccN): For Verifier (cid:12) (cid:12) (cid:12) Accuracy Acc(k) with tests up to , AUC@N 1 kmin 1 (cid:88) i=kmin Acc(ki) + Acc(ki+1) 2 (ki+1 ki). Higher AUC@N indicates superior average verifier accuracy, reflecting potent (high p) and efficiently diverse tests. Detailed explanations and derivations for these metrics are in Appendix B. 5The full derivation is provided in Appendix C. 5 Figure 5: Overview of the SAGA framework. SAGA leverages both GroundTruth (correct human solutions) and Human Bugs (incorrect submissions) alongside the Problem description. An LLM performs Multi-Dimensional Analysis and Differential Analysis to generate Python Case Scripts for test input synthesis. These scripts are accompanied by Math Explanations (capturing testing strategies and constraints) and Self-Validation code. The generated Test Inputs are then passed to an Interpreter (ground-truth solution) to produce Test Outputs, forming the final test cases."
        },
        {
            "title": "3 SAGA: A Human-LLM Collaborative Framework for Advanced TCG",
            "content": "The preceding analysis (Section 2.2) revealed critical limitations in prevalent Test Case Generation (TCG) paradigms, such as low test quality and homogeneity. To address this, we introduce SAGA (Strategic Adversarial & Constraint-differential GenerAtive workflow), novel human-LLM collaborative framework (Figure 5). SAGA systematically generates high-quality, diverse, and discriminative test suites by maximizing test potency (p) and diversity (lowering correlation ρ). Additionally, we trained TCGCoder-7B, SAGA-distilled 7B specialist model from 15,000 problems (details in Appendix J), as strong TCG baseline and reference. 3.1 The SAGA Framework: Integrating Human Expertise Table 1: Initial Impact of Incorporating Simple Human Priors (Shuman) into Basic TCG Paradigms on AtCoder Results (Accuracy@50). Recognizing the limitations of naive TCG, SAGA explores the integration of human expertise. Intuitively, leveraging human problemsolving insights should enhance TCG. Table 1 shows that incorporating simple human priors (e.g., boundary values from Shuman) into basic TCG paradigms on TCGBench-Full yields some improvement. However, these gains are often marginal, failing to fully exploit LLM potential for detecting complex flaws. While methods like EvalPlus [27] use human solutions, its often for mutating seeds or formatting inputs, not deeply guiding large-scale challenging test generation. Method Direct Gen. (Paradigm 1) Direct Gen. + Simple Priors Input-Interpreter (Paradigm 2) Input-Interpreter + Simple Priors Accuracy@50 11.30% 15.06% (+3.76%) 23.36% 27.95% (+4.59%) SAGA advances beyond such superficial integration. As depicted in Figure 5, it deeply incorporates multifaceted human programming insightsfrom both correct solutions (GroundTruth, Shuman) and incorrect submissions (Human Bugs, Swrong)with LLM reasoning via structured, dual-pronged analytical strategy. An LLM is fed the problem description and insights gleaned from human solutions through customized prompting module. This process generates Python Case Scripts to produce test inputs, accompanied by Math Explanations and Self-Validation code to ensure correctness and relevance. The generated inputs are then processed by an Interpreter (ground-truth solution) to yield test outputs, forming the final test cases. SAGAs core analytical dimensions are: Multidimensional Analysis (Leveraging Shuman): This dimension extracts profound insights from correct solutions to engineer challenging tests. It involves: 1. Constraint Handling Differences: Discrepancies in how Swrong and correct manage problemspecific constraints. 2. Defense Pattern Deconstruction: This is where the Math Explanation component (Figure 5) plays key role. Diverse defensive logic and problem-solving strategies within Shuman are decomposed into formal mathematical or logical constraints (e.g., \"equivalence class: player pairs\", \"boundary_value: [(1,2), (N,N-1)]\"). This allows SAGA to target singularities, extremal values, or specific structural properties, guiding the generation of edge and adversarial test cases. 6 Figure 6: SAGA outperforms the Baseline (Random Input-Interpreter) and its individual analytical components (Multidimensional Analysis leveraging Shuman; Differential Analysis leveraging Swrong) on the AtCoder subset of the full TCGBench across: (a) Detection Rate, (b) Verifier Accuracy (with AUC@50 values), (c) Distinct Error Pattern Coverage (DEPC), and (d) Diversity Ratio. Dotted lines in (a) & (b) show baseline performance at = 100. See Appendix for SAGAs performance on other platforms within the full TCGBench. 3. Targeted Test Generation: Using these constraints and pitfalls to guide the LLM in constructing challenging test inputs via the Case Scripts. This analytical approach aims to generate test cases that not only cover wide spectrum of valid scenarios, including complex boundary conditions and intricate interactions, but also enhance test diversity (lowering ρ) and individual test potency (increasing p). Differential Analysis (Leveraging Swrong): This addresses error types missed by analyzing only correct) to correct(Idiff), revealing common error patterns. This targets: correct manage problemcorrect solutions. It compares failed submissions (Swrong) with their corrected versions (S find inputs Idiff where Swrong(Idiff) = 1. Constraint Handling Differences: Discrepancies in how Swrong and specific constraints. 2. Lack of Defensive Completeness: Deficiencies in Swrong related to handling edge cases or boundary inputs, as revealed by comparison with correct. 3. Failure Pattern Analysis: Generating specific inputs that trigger failures in Swrong but are correctly handled by correct. The incorporation of these differentially identified inputs Idiff into the test suite creates more rigorous and challenging evaluation framework, as it specifically targets known failure modes, thereby substantially increasing the discriminative power of the resulting verifier. The Self-Validation scripts (Figure 5) ensure that generated test inputs adhere to problem constraints and the intended testing strategy before execution. For Multidimensional Analysis, SAGA leverages insights from 10 distinct, correct user solutions per problem to broaden perspective. In Differential Analysis, it meticulously pairs users correct solution with their most recent preceding incorrect submission. This focus on closely related yet differing attempts enhances SAGAs ability to identify subtle error patterns and generate challenging corner cases. By synergistically combining these refined analytical dimensions, SAGA produces comprehensive and highly effective test suites. 3.2 Experimental Validation of SAGA SAGAs effectiveness was initially validated on the TCGBench. Figure 6 visually summarizes SAGAs multifaceted superiority in this context, comparing it against the random Input-Interpreter baseline and its core analytical components (Multidimensional Analysis and Differential Analysis). Key Findings from Comparison on TCGBench  (Fig. 6)  : SAGA markedly improves solution vetting capabilities: its DR surpasses 93.81% (vs. baselines 82.85% at = 100) and VAcc reaches 41.33% (vs. baselines 21.89% at = 100) with only 50 tests. SAGAs AUC@50 (0.5445) more than doubles the baselines (0.2586), showcasing superior efficiency. SAGA also generates test suites of superior intrinsic quality: it achieves the highest DEPC (broader error coverage, lower ρ) and Diversity Ratio (more efficient error discovery per test). Both Multidimensional and Differential analysis components individually outperform the baseline, but their synergy in SAGA yields optimal results. This robustly validates SAGAs systematic leveraging of human insights on large scale. 3.2.1 Main Results and Analysis on TCGBench-Lite For focused main comparisons and ablation studies, we curated TCGBench-Lite, challenging subset of 270 problems from AtCoder, Codeforces, and Nowcoder contests since June 2024. This 7 Table 2: Experimental Results: Main Comparison of SAGA with Baselines and Ablation Studies on TCGBench-Lite. Metrics are DR@k, VAcc@k, AUC@50, and DivRatio@50. All methods use DeepSeek-V3-0324 backbone unless specified in ablation. Method / Configuration DR@20 DR@50 VAcc@20 VAcc@50 AUC@50 DivRatio@ Main Comparison with Baseline TCG Methods TestChain [22] Input-Interpreter (LiveCodeBench-style [19]) EvalPlus [27] TCGCoder-7B (SAGA-distilled model) SAGA (DeepSeek-V3 Backbone) 8.12% 65.91% 68.31% 12.36% 77.84% 81.07% 11.56% 67.52% 71.12% 85.14% 89.44% 17.93% 85.66% 90.62% 22.40% 11.88% 16.72% 15.15% 29.11% 32.58% SAGA Ablation Studies Analytical Component Ablation SAGA w/ Multidim. Analysis only SAGA w/ Differential Analysis only 84.51% 88.00% 84.31% 88.16% 20.70% 19.85% 26.05% 26.67% Prompt Design Ablation SimpleCOT Prompt for SAGA Random Input w/ GT for SAGA EvalPlus w/ GT for SAGA Base LLM Ablation for SAGA 83.36% 84.54% 82.31% 86.64% 76.72% 79.56% SAGA w/ Qwen2.5-Coder-7B-Instruct SAGA w/ Qwen2.5-72B-Instruct SAGA w/ Qwen2.5-Coder-32B-Instruct 78.88% 79.78% 82.77% 85.08% 86.25% 90.54% 15.61% 16.44% 11.67% 19.70% 20.30% 20.74% 19.11% 22.70% 20.44% 22.96% 26.46% 32.73% 0.0841 0.1234 0.1139 0.1890 0.2228 0.1923 0.1926 0.1424 0.1616 0.1278 0.1810 0.1943 0. 50.09% 79.42% 79.27% 94.43% 94.06% 95.81% 94.41% 96.23% 85.38% 89.49% 96.80% 94.92% 94.72% ensures contemporary relevance and minimizes potential data leakage. TCGBench-Lite includes an average of 41.41 incorrect submissions (Swrong) per problem. Its difficulty distribution (Easy: 27.04%, Medium: 32.59%, Hard: 40.37%) was determined by platform tags and contest characteristics (details in Appendix E). This curated set allows for rigorous yet manageable evaluation. Unless specified, all LLM-driven methods use DeepSeek-V3-0324 as the backbone for fair comparison. We evaluate against the Input-Interpreter (LiveCodeBench-style random sampling), TestChain [22], EvalPlus [27], and our SAGA-distilled TCGCoder-7B. Results are in Table 2. Analysis of Main Comparison  (Table 2)  : SAGAs structured integration of human insights yields substantial gains over other TCG methods on TCGBench-Lite. Notably, SAGA achieves an AUC@50 of 0.2228, surpassing the Input-Interpreter (0.1234), which is limited by random samplings homogeneity. This highlights SAGAs ability to produce more consistently effective tests. While EvalPlus achieves high raw Diversity Ratio through mutation, its lower AUC@50 (0.1278) suggests that SAGAs deep analysis of human solutions is more critical for overall verifier quality than mere test variety. TestChain, lacking rich human priors, performs weakest. Crucially, our SAGA-distilled TCGCoder-7B (AUC@50: 0.1890) outperforms all these established baselines, even when they utilize much larger backbone model (DeepSeek-V3-0324). This demonstrates SAGAs potential to distill effective TCG strategies into smaller, specialized, and highly capable models, thereby making advanced TCG more accessible. Analysis of Ablation Studies  (Table 2)  : Value of Structured Human Insights: Isolating SAGAs Multidimensional Analysis (leveraging Shuman) and Differential Analysis (leveraging Swrong) reveals that both significantly outperform simpler prompting (SimpleCOT) and the Input-Interpreter baseline in terms of AUC@50. This underscores that structured analysis of human solutions, whether correct or incorrect, is fundamental for higher-quality verifiers. The full SAGA framework, synergistically combining both analytical dimensions, achieves the optimal overall quality (highest AUC@50), confirming their complementary benefits. Importance of Prompt Design: The substantial performance degradation when replacing SAGAs detailed, insight-driven prompts with generic CoT (SimpleCOT) highlights that the methodology of integrating human insights is as critical as the insights themselves for effective TCG. Robustness Across LLMs: SAGA demonstrates commendable robustness when paired with different LLM backbones. While peak performance on specific metrics can vary with the LLM (e.g., SAGA with Qwen2.5-Coder-32B-Instruct yields the highest VAcc@50), the SAGA framework with its default DeepSeek-V3 backbone consistently provides the best overall AUC@50. This indicates SAGAs comprehensive design is more impactful for holistic verifier quality than relying on specific LLM capabilities or optimizing for isolated metrics like Diversity Ratio. Notably, SAGA also shows strong performance with Qwen2.5-Coder-7B-Instruct (TCGCoder-7Bs base model), further validating SAGAs efficacy on smaller, specialized coder models. 8 Table 3: Verifier Quality: CodeCompass vs. LCB-v6 (Shared Subset, @40 tests). Metric LiveCodeBench-v6 CodeCompass DR@40 VAcc@40 DivRatio@40 AUC@ 78.85% 19.61% 53.56% 0.1388 93.44% 30.39% 96.69% 0.1980 Figure 7: Avg. Pass@1 by Difficulty. Figure 8: Model Ranking Changes."
        },
        {
            "title": "4 Towards Advanced Applications of SAGA",
            "content": "SAGAs proven capability in generating high-quality, diverse, and discriminative test suites (Section 3) enables advancements in LLM code evaluation and training. This section primarily explores the development of superior code generation benchmark, CodeComPass, while also noting the potential for SAGA-enhanced verifiers to contribute to more reliable Reinforcement Learning from Verifiable Rewards (RLVR). CodeCompass: SAGA-Enhanced Benchmark for Code Generation Evaluation To address the need for more challenging and discerning evaluation of LLM code generation models, we introduce CodeCompass. The verifiers within CodeCompass are synthesized using SAGA for the 270 contemporary problems that also constitute TCGBench-Lite (detailed in Appendix E), ensuring relevance and minimizing data leakage risks. Each problem in CodeCompass features rich test suite, averaging 50.54 SAGA-generated test cases, meticulously curated to ensure comprehensive coverage (with additional manual curation applied if initial generation yielded fewer than target threshold). For comparative analysis of verifier quality and impact on model evaluation, we focus on shared subset of 101 AtCoder problems that overlap between our CodeCompass verifiers and the test suites used by LiveCodeBench-v6. This allows for direct comparison on common ground. Superior Verifier Quality of CodeCompass. We first establish the intrinsic quality of CodeCompass verifiers using core TCG metrics against those of LiveCodeBench-v6 on the shared AtCoder subset  (Table 3)  . CodeCompass demonstrates markedly higher efficacy in identifying faulty solutions, greater efficiency in discovering unique error patterns (Diversity Ratio@40: +43.13%), and faster convergence to quality. This confirms that SAGA produces verifiers that are intrinsically more diverse, discriminative, and effective. Enhanced Discriminative Power for Code Generation Model Evaluation. This superior verifier quality translates directly to more challenging and discerning evaluation for LLM code generation models. As shown in Figure 7, CodeComPass consistently elicits lower average Pass@1 rates across all problem difficulties compared to LiveCodeBench-v6 on the shared subset, indicating more rigorous test. Critically, this increased stringency enhances discriminative power (Figure 8): when evaluated on CodeComPass, the average Pass@1 for various models drops by relative 9.56% compared to their performance on LiveCodeBench-v6. This relative decrease leads to re-ranking of models (e.g., Qwen2.5-72B and Qwen2.5-Coder-32B switch relative positions). This ability to expose nuanced differences in model capabilities confirms that CodeComPass offers more robust and insightful assessment of true code generation proficiency. Meanwhile, the superior SAGA-generated verifiers, exemplified by CodeComPass, promise to enhance Reinforcement Learning from Verifiable Rewards (RLVR) frameworks [8, 10] by delivering more accurate reward signals and mitigating reward hacking, thereby fostering more robust and capable code generation models."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper critically re-evaluates LLM-based Test Case Generation (TCG), highlighting current verifier limitations and formalizing key quality metrics alongside TCGBench, foundational TCG research dataset. We introduce SAGA, novel human-LLM collaborative framework that integrates human programming insights with LLM reasoning to demonstrably generate superior test suites. Leveraging SAGA, we developed CodeComPass, an enhanced verifier suite for robust code generation evaluation, and distilled TCGCoder-7B, specialized, efficient TCG model. Collectively, these contributions significantly advance reliable LLM code evaluation and pave the way for future automated test synthesis and effective adversarial testing."
        },
        {
            "title": "References",
            "content": "[1] Saswat Anand, Edmund K. Burke, Tsong Yueh Chen, John A. Clark, Myra B. Cohen, Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, and Phil McMinn. An orchestrated survey of methodologies for automated software test case generation. J. Syst. Softw., 86(8):19782001, 2013. doi: 10.1016/J.JSS.2013.02.061. URL https://doi.org/10.1016/j.jss.2013.02. 061. [2] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805. [3] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv. org/abs/2108.07732. [4] S. Beyleda and V. Gruhn. Bintest - search-based test case generation. In Proceedings 27th Annual International Computer Software and Applications Conference. COMPAC 2003, pages 2833, 2003. doi: 10.1109/CMPSAC.2003.1245318. [5] Cristian Cadar and Koushik Sen. Symbolic execution for software testing: three decades later. Commun. ACM, 56(2):8290, 2013. doi: 10.1145/2408776.2408795. URL https: //doi.org/10.1145/2408776.2408795. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. [7] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. [8] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. 10 [9] Zhiyu Duan, Yujia Li, Pubo Ma, Xiaodong Gou, and Shunkun Yang. multi-layer fault triggering framework based on evolutionary strategy guided symbolic execution for automated test case generation. In 22nd IEEE International Conference on Software Quality, Reliability, and Security, QRS 2022 - Companion, Guangzhou, China, December 5-9, 2022, pages 255262. IEEE, 2022. doi: 10.1109/QRS-C57518.2022.00045. URL https://doi.org/10.1109/ QRS-C57518.2022.00045. [10] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera Gilaberte, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, and Wenda Zhou. Competitive programming with large reasoning models. CoRR, abs/2502.06807, 2025. doi: 10.48550/ARXIV.2502.06807. URL https://doi.org/10.48550/arXiv.2502. 06807. [11] Andrea Fioraldi, Alessandro Mantovani, Dominik Christian Maier, and Davide Balzarotti. Dissecting american fuzzy lop: fuzzbench evaluation. ACM Trans. Softw. Eng. Methodol., 32 (2):52:152:26, 2023. doi: 10.1145/3580596. URL https://doi.org/10.1145/3580596. [12] Garrett Fitzmaurice, Nan Laird, and James Ware. Applied longitudinal analysis. John Wiley & Sons, 2012. [13] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. CoRR, abs/2401.14196, 2024. doi: 10.48550/ARXIV.2401.14196. URL https://doi.org/10. 48550/arXiv.2401.14196. [14] Mark Harman, S. Afshin Mansouri, and Yuanyuan Zhang. Search-based software engineering: Trends, techniques and applications. ACM Comput. Surv., 45(1), December 2012. ISSN 0360-0300. doi: 10.1145/2379776.2379787. URL https://doi.org/10.1145/2379776. 2379787. [15] Dong Huang, Qingwen Bu, Yuhao Qing, and Heming Cui. Codecot: Tackling code syntax errors in cot reasoning for code generation, 2024. URL https://arxiv.org/abs/2308.08784. [16] Dong Huang, Jie M. Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation, 2024. URL https://arxiv.org/abs/2312.13010. [17] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [18] Kush Jain, Gabriel Synnaeve, and Baptiste Rozière. Testgeneval: real world unit test generation and test completion benchmark, 2024. [19] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [20] Leslie Kish. Survey sampling. new york: John wesley & sons. Am Polit Sci Rev, 59(4):1025, 1965. [21] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu-Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/8636419dea1aa9fbd25fc4248e702da4-Abstract-Conference.html. 11 [22] Kefan Li and Yuan Yuan. Large language models as test case generators: Performance evaluation and enhancement. CoRR, abs/2404.13340, 2024. doi: 10.48550/ARXIV.2404.13340. URL https://doi.org/10.48550/arXiv.2404.13340. [23] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason T. Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Trans. Mach. Learn. Res., 2023, 2023. URL Starcoder: may the source be with you! https://openreview.net/forum?id=KoFOg41haE. [24] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. [25] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. CoRR, abs/2203.07814, 2022. doi: 10.48550/ARXIV.2203. 07814. URL https://doi.org/10.48550/arXiv.2203.07814. [26] Guanghan Liu and Kung-Yee Liang. Sample size calculations for studies with correlated observations. Biometrics, pages 937947, 1997. [27] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=1qvx610Cu7. [28] Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language models for efficient code generation. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=IBCBMeAhmC. [29] Kaibo Liu, Yudong Han, Yiyang Liu, Jie M. Zhang, Zhenpeng Chen, Federica Sarro, Gang Huang, and Yun Ma. Trickybugs: dataset of corner-case bugs in plausible programs. In 2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR), pages 113117, 2024. [30] Sharon Lohr. Sampling: design and analysis. Chapman and Hall/CRC, 2021. [31] Zeyao Ma, Xiaokang Zhang, Jing Zhang, Jifan Yu, Sijia Luo, and Jie Tang. Dynamic scaling of unit tests for code reward modeling, 2025. URL https://arxiv.org/abs/2501.01054. [32] Valentin J.M. Manès, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J. Schwartz, and Maverick Woo. The art, science, and engineering of fuzzing: survey. IEEE Transactions on Software Engineering, 47(11):23122331, 2021. doi: 10.1109/TSE.2019. 2946563. [33] Phil McMinn. Search-based software test data generation: survey. Softw. Test. Verification Reliab., 14(2):105156, 2004. doi: 10.1002/STVR.294. URL https://doi.org/10.1002/ stvr.294. 12 [34] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-Tau Yih, Sida I. Wang, and Xi Victoria Lin. LEVER: learning to verify language-to-code generation with execution. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2610626128. PMLR, 2023. URL https://proceedings.mlr.press/v202/ni23b. html. [35] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303. 08774. URL https://doi.org/10.48550/arXiv.2303.08774. [36] Carlos Pacheco and Michael D. Ernst. Randoop: feedback-directed random testing for java. In Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion, OOPSLA 07, page 815816, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595938657. doi: 10.1145/1297846.1297902. URL https://doi.org/10.1145/1297846.1297902. [37] Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces cots. https://huggingface.co/datasets/open-r1/ codeforces-cots, 2025. [38] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. doi: 10.48550/ARXIV.2308.12950. URL https://doi.org/10. 48550/arXiv.2308.12950. [39] Kostya Serebryany and the LLVM Team. libFuzzer library for coverage-guided fuzz testing. https://llvm.org/docs/LibFuzzer.html, 2016. Accessed: 2024-04-15. [40] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan Reddy. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816, 2023. [41] Wenhan Wang, Chenyuan Yang, Zhijie Wang, Yuheng Huang, Zhaoyang Chu, Da Song, Lingming Zhang, An Ran Chen, and Lei Ma. TESTEVAL: benchmarking large language models for test case generation. CoRR, abs/2406.04531, 2024. doi: 10.48550/ARXIV.2406.04531. URL https://doi.org/10.48550/arXiv.2406.04531. [42] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 86968708. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN.685. URL https://doi.org/10.18653/v1/2021.emnlp-main.685. [43] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. Codet5+: Open code large language models for code understanding and generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 10691088. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023. EMNLP-MAIN.68. URL https://doi.org/10.18653/v1/2023.emnlp-main.68. [44] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong 13 Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [45] Kohsuke Yatoh, Kazunori Sakamoto, Fuyuki Ishikawa, and Shinichi Honiden. Feedbackcontrolled random test generation. In Michal Young and Tao Xie, editors, Proceedings of the 2015 International Symposium on Software Testing and Analysis, ISSTA 2015, Baltimore, MD, USA, July 12-17, 2015, pages 316326. ACM, 2015. doi: 10.1145/2771783.2771805. URL https://doi.org/10.1145/2771783.2771805. [46] Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis. ArXiv, 2502.01718, 2025. [47] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: pre-trained model for code generation with multilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 23, page 56735684, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599790. URL https://doi.org/10.1145/ 3580305.3599790. [48] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A Related Works",
            "content": "A.1 Advancements and Challenges in LLM-based Code Generation and Evaluation Large Language Models (LLMs) have revolutionized automated code generation, with models like AlphaCode [25], GPT-4 [35], Gemini [2], and specialized code models such as CodeT5 [42, 43], StarCoder [23], DeepSeek Coder [13], CodeLlama [38], and Qwen2.5-Coder [44, 17] demonstrating remarkable capabilities. These models, whether trained via Supervised Fine-Tuning or enhanced through Reinforcement Learning (RL) with execution feedback (e.g., AlphaCode [25], PPOCoder [40], CodeRL [21]), increasingly match or exceed human performance on diverse programming benchmarks. The reliable evaluation of these sophisticated models is paramount and hinges on the quality of Code Verifierstypically test suitesthat ascertain the functional correctness of generated code. This is especially critical for RL frameworks employing verifiable rewards (RLVR) [8, 10, 34], where test case quality directly impacts reward accuracy and training efficacy. However, as highlighted in Section 1, the comprehensiveness and robustness of verifiers often present significant bottleneck, underscoring the critical need for effective Test Case Generation. To this end, the research community has developed numerous benchmarks. Early benchmarks like HumanEval [6] and MBPP [3] provided foundational testbeds. Subsequent efforts like EvalPlus [27, 28] and MBPP-Plus aimed to improve robustness by expanding existing test sets. More comprehensive benchmarks such as TACO [24], CodeContests [25] (focused on algorithmic problems), BigCodeBench [48] (complex multi-library tasks), LiveCodeBench [19] (simulating online judge environments), and HumanEvalPack [47] (multilingual program synthesis) have broadened the evaluative scope. Concurrently, dedicated TCG benchmarks like TESTEVAL [41] and TestGenEval [18] have emerged. Despite these advancements, persistent issues in test case quality, coverage, and potential LLM-centric biases necessitate continuous efforts towards developing high-quality, diverse, and unbiased Code Verifiers. A.2 Methodologies for Test Case Generation (TCG) TCG is fundamental to validating code correctness and providing feedback for both evaluation and training of code generation models. While formally defined as distinct task in this paper, TCG principles are integral to benchmark creation (e.g., LiveCodeBench [19], TESTEVAL [41]) and RL data pipelines (e.g., ACECODER [46], CodeRL [21]). However, suboptimal test quality in existing datasets (e.g., TACO [24], CodeForces-CoTs [37]) can lead to overestimated model performance and reward hacking, particularly in rule-based RL systems like AlphaCode [25] and DeepSeek Coder [13], thus highlighting the urgent need for effective TCG. Traditional TCG Techniques: Software testing has rich history of TCG. Search-Based Software Testing (SBST) [1, 14] uses metaheuristics like genetic algorithms [4, 33] to optimize for coverage criteria, though it can be computationally intensive. Symbolic Execution [9, 5] explores program paths systematically but faces path explosion and challenges with complex code. Fuzzing [32, 11], especially coverage-guided variants like AFL [11] and libFuzzer [39], excels at finding crashes and vulnerabilities by mutating inputs, though it may lack semantic depth for logical tests. Feedbackdirected random testing [45, 36] also discovers defects but can sometimes lack diversity. LLM-based TCG Paradigms: With the rise of LLMs, new TCG methods have emerged, leveraging semantic understanding for potentially greater coverage and diversity. As discussed in Section 2, these generally follow two paradigms: 1. Direct Generation: LLMs produce complete test cases (inputs and outputs). This includes assertion-focused methods (e.g., CodeCoT [15], AgentCoder [16]) and direct input-output synthesis (e.g., TestChain [22], CodeRM [31], AceCoder [46]), aiming for logical coverage and boundary conditions. 2. Input-Interpreter: LLMs generate test inputs, which are then executed by ground-truth solution to derive outputs. This is seen in LiveCodeBench [19],CodeForce-Cot [37] and related to LLM15 guided fuzzing. EvalPlus [27, 28] also aligns by mutating seed inputs for execution. TestChain [22] also showed improvements by decoupling input and output generation. Advanced and Specialized TCG Approaches: In RL contexts, dynamic TCG is crucial. CodeRM8B [31] adjusts test quantity by problem difficulty. LEVER [34] uses learned verifiers, and other works focus on execution-feedback for reward model optimization, all highlighting the impact of test quality on RL outcomes. Differential testing, as in AID [29], compares program versions to expose bugs, showing strong results on datasets like TrickyBugs [29] and EvalPlus [27]. Recent efforts also explore generating tests targeting prior failures [7] or using LLMs for test suite refinement. Despite these diverse approaches, achieving comprehensive logical coverage, generating truly diverse and challenging corner-case tests, and maintaining computational efficiency remain significant hurdles. Furthermore, many LLM-centric TCG methods may perpetuate biases inherent in the LLMs themselves (Section 1). This papers SAGA framework addresses these limitations by proposing novel human-LLM collaborative paradigm that systematically integrates deep human insights from both correct and incorrect solutions. By focusing on dynamic, adaptive, and efficient TCG process, SAGA aims to enhance the reliability of LLM code evaluation and training, paving new path for TCG research."
        },
        {
            "title": "B Formulation and Interpretation of Advanced Evaluation Metrics",
            "content": "This section provides detailed explanations and interpretations for the advanced metrics introduced in Section 2.2 to evaluate the intrinsic quality of test suites. B.1 Distinct Error Pattern Coverage (DEPC) and Diversity Ratio Formulation (from main text): For test suite and NP problems, let v(tk) be the NP -dimensional binary error pattern vector for test tk (where v(tk)j = 1 if tk reveals an error for problem j). DEPC(T ) = (cid:12) (cid:12) (cid:12) (cid:8)v(tk) tk and v(tk)1 1(cid:9)(cid:12) (cid:12) (cid:12). The Diversity Ratio is DEPC(T )/n, where = . Interpretation: DEPC measures the breadth of error coverage by counting the number of unique ways (patterns) in which the test suite can detect failures across set of problems. higher DEPC signifies that the test suite is capable of identifying wider variety of distinct error types or combinations of errors. This directly relates to the concept of inter-test case correlation (ρeff ) discussed in our theoretical model (Appendix C): test suite with high DEPC is likely composed of tests that are less correlated in terms of the errors they detect, thus having lower ρeff . The Diversity Ratio normalizes DEPC by the number of test cases, indicating the average efficiency of each test in contributing new, distinct error pattern. high Diversity Ratio suggests that the test suite is not only diverse but also concise, with less redundancy among its test cases. B.2 Normalized Area Under the Accuracy-Number of test cases Curve (AUC-AccN) Formulation (from main text): For Verifier Accuracy Acc(k) achieved with test suite of size (up to maximum , starting from kmin), the AUC-AccN is approximated by the trapezoidal rule: AUC@N 1 kmin 1 (cid:88) i=kmin Acc(ki) + Acc(ki+1) 2 (ki+1 ki). Interpretation: AUC-AccN quantifies the average Verifier Accuracy as the test suite size grows, providing single scalar value to compare the overall effectiveness and efficiency of different TCG strategies. higher AUC@N (ranging from 0 to 1) indicates that TCG strategy consistently generates test suites that achieve higher verifier accuracy across various sizes up to . This metric is composite reflection of both the average potency of individual test cases (p) and the effective diversity of the test suite (related to ρeff ). Impact of (Test Potency): higher average means individual tests are more likely to detect errors. This leads to steeper initial rise in the Acc(k) curve and higher overall level of accuracy 16 achieved, both of which contribute to larger AUC@N . Thus, AUC-AccN is particularly sensitive to p. Impact of Diversity (low ρeff ): Greater diversity (lower ρeff ), empirically reflected by high DEPC, allows the Acc(k) curve to sustain its rise or plateau at higher level for larger number of test cases before saturation effects become dominant. This sustained high accuracy also contributes to larger AUC@N . Therefore, high AUC@N signifies TCG strategy that excels at generating tests that are individually powerful (high p) and collectively non-redundant (low ρeff ), leading to efficient and robust verifier construction within the specified test suite size limit. It reflects better overall quality of the generated test cases in achieving high average performance."
        },
        {
            "title": "C Theoretical Analysis of Detection Rate",
            "content": "We analyze the detection rate ϵS(T ) = P(X 1) for test suite = {t1, . . . , tn}, where = (cid:80)n i=1 1Ei and pi = P(Ei) is the probability that test ti detects an error. i=j Cov(1Ei, 1Ej ). C.1 Modeling Correlated Heterogeneous Bernoulli Trials The expectation of is E[X] = (cid:80) pi = np, where = 1 (cid:80) pi(1 pi) + (cid:80) To model the effect of correlation in tractable manner that allows for insights similar to the homogeneous case (uniform p, uniform ρ), we consider an approximating model. We define an (cid:80) pi(1 effective average pairwise covariance, Ceff , and an average individual variance, σ2 pi). We then model the variance as if it arose from system with these averaged second-order characteristics: (cid:80) pi. The variance is Var(X) = = 1 n Varapprox(X) = σ2 + n(n 1) Ceff . If we further posit that these average characteristics can be related through an effective average (cid:112)avg(pi(1 pi))2 or, more simply for conceptual linkage, correlation ρeff such that Ceff ρeff Ceff ρeff p(1 p) (assuming pi are not excessively dispersed, making σ2 p(1 p)), then: Varapprox(X) np(1 p)[1 + (n 1)ρeff ]. (1) This equation models the variance of as if it were sum of Bernoulli trials with common success probability and common pairwise correlation ρeff . The validity of this approximation depends on the actual distribution of pi and the structure of covariances. However, it serves as useful model to understand the qualitative impact of average correlation. Definition 3 (Model-Based Effective Sample Size eff ). Within this approximating model (Eq. 1), and by analogy to Kishs design effect [20] (where deff 1 + (n 1)ρeff ), the model-based effective sample size is: 1 + (n 1)ρeff eff represents the number of hypothetical independent Bernoulli(p) trials that would exhibit the This variance given by Eq. 1. This is consistent with adjustments for correlated data [30, 12, 26]. eff . C.2 Upper Bound and Saturation within the Model Using eff and from our model, we analyze the detection rate ϵS(T ) = 1 P(X = 0). Theorem 1 (Model-Based Approximate Upper Bound on Detection Rate). Within the described approximating model, the detection rate ϵS(T ) is approximately upper bounded by: ϵS(T ) 1 (1 p)n eff 1 (1 p) 1+(n1) ρeff . Proof Sketch. The probability P(X = 0) is approximated by that of trials, which is (1 p)n eff . eff independent Bernoulli(p) This theorem suggests that even when individual pi vary, if theres an effective positive average correlation ρeff > 0, the system behaves as if it has fewer independent tests, limiting the detection rate. 17 C.3 Interpretation and Implications for Performance Metrics The derivation above, employing an approximating model based on average parameters (p, ρeff ), robustly indicates that persistent positive effective correlation among test case detection events leads to saturation of the overall detection rate. The core insightthat redundancy limits the marginal gain from additional testsremains. This theoretical observation is crucial for understanding the performance of TCG strategies and their impact on the empirical metrics used in this paper: Verifier Accuracy (Acc(T )) and its relation to p: The Verifier Accuracy at any given test suite size n, Acc(Tn), is fundamentally driven by the test suites ability to expose errors, which is heavily influenced by the average potency of its constituent test cases. higher average error detection probability, p, means that individual tests are, on average, more \"powerful\" or \"incisive.\" Consequently, TCG strategy yielding higher will lead to verifier that more readily and correctly identifies faulty solutions, directly boosting Acc(Tn). While high correlation (ρeff ) can limit the ultimate achievable accuracy by causing early saturation of distinct error discovery, strong is essential for the accuracy curve to reach high level in the first place. AUC-AccN as reflection of sustained high and managed ρeff : The Area Under the AccuracyNumber of test cases Curve (AUC-AccN), which quantifies the average Verifier Accuracy as test suite size increases, is composite reflection of both and ρeff . high ensures that the Acc(k) curve rises steeply and achieves significant altitude. This initial rapid ascent and the overall height of the curve contribute substantially to larger AUC-AccN. Concurrently, lower effective correlation ρeff (i.e., greater diversity, reflected empirically by DEPC) allows the accuracy to be sustained or to continue growing across larger number of test cases before significant saturation, thereby expanding the area under the curve. Therefore, strategies achieving high AUC-AccN are those that likely generate test cases with consistently high average error detection probability (p) and effectively manage redundancy (lower ρeff ). The magnitude of is particularly critical for the \"value\" captured by AUC-AccN, as it dictates the average level of accuracy being integrated. DEPC and its relation to ρeff : DEPC empirically captures the diversity of error patterns. TCG strategy that yields high DEPC is effectively generating tests with low effective average correlation ρeff , thus mitigating the saturation effect on discovering new types of errors and allowing for more sustained increase in overall detection capability. Therefore, the pursuit of TCG methods like SAGA, which aim to enhance both individual test case strength (targeting higher p) and inter-test case diversity (targeting lower ρeff ), is theoretically well-founded for optimizing these key verifier performance metrics. TCGBench: Foundational Dataset for TCG Research As introduced in Section 2.2, TCGBench is the comprehensive dataset curated for our Test Case Generation (TCG) research. It aggregates 1840 recent programming problems sourced from three leading competitive programming platforms: AtCoder (https://atcoder.jp), Codeforces (https://codeforces.com), and Nowcoder (https://www.nowcoder.com). These platforms are recognized for their diverse algorithmic challenges. For each problem in TCGBench, we also collected an average of 36.66 incorrect human submissions, specifically those resulting in \"Wrong Answer\" (WA) or \"Time Limit Exceeded\" (TLE) verdicts. This large-scale collection of problems, along with their corresponding WA/TLE submissions, provides rich empirical foundation for studying human error patterns and rigorously developing and evaluating TCG methodologies. The problems are sourced from recent contests to ensure currency and minimize data leakage risks when evaluating contemporary LLMs. Further details on data collection, filtering criteria, and specific contest sources are available in supplementary materials. TCGBench-Lite and CodeCompass: Curated Set for Evaluation For the main experimental comparisons and ablation studies presented in Section 3.2.1, and for constructing the CodeCompass verifiers (Section 4), we curated TCGBench-Lite. This is focused subset of 270 problems sourced from AtCoder, Codeforces, and Nowcoder contests held since June 2024, ensuring high contemporary relevance and minimizing potential data leakage for evaluating newer models. TCGBench-Lite features an average of 41.41 incorrect submissions per problem. 18 The difficulty distribution for TCGBench-Lite and CodeCompass (Easy: 27.04%, Medium: 32.59%, Hard: 40.37%) was determined by multi-faceted approach. This involved considering platformprovided difficulty tags, the type of contest round (e.g., AtCoder Beginner Contest vs. Regular Contest; Codeforces Div.4/3 vs. Div.2/1), typical problem-solving patterns associated with specific problem slots within these contests, and general community perception of difficulty for similar problems. For instance, early problems in beginner-focused contests were generally classified as Easy, while later problems in advanced contests or those requiring complex algorithms/data structures were classified as Hard. This classification aims to provide balanced yet challenging set for rigorous evaluation. The verifiers in CodeCompass, used for code generation evaluation, consist of an average of 50.54 SAGA-generated test cases per problem for these 270 problems. The characteristics are summarized in Table 4. Table 4: Overview of TCGBench-Lite (Core Dataset for CodeCompass Verifiers). Details for CodeCompass Evaluation Aspect Core Dataset Source Total Problems Difficulty Distribution Atcoder, Codeforces, Nowcoder (June 2024 - Present) 270 Easy (27.04%), Medium (32.59%), Hard (40.37%) Scale for CG Evaluation Avg. Test Cases/Problem: 50.54 Primary CG Metric Avg. Swrong/Problem (for SAGAs TCG): 41.41 Pass@k Figure 9: SAGA performance on Codeforces (CF) and Nowcoder (NC) problems from the full TCGBench dataset: (a) Detection Rate, (b) Verifier Accuracy (with AUC@50), (c) DEPC, and (d) Diversity Ratio, compared to Baseline (Input-Interpreter) and SAGAs analytical components (Multidimensional Analysis and Differential Analysis). Dotted lines in (a) & (b) show respective baseline performance at = 100."
        },
        {
            "title": "F SAGA Performance on Full TCGBench",
            "content": "To demonstrate SAGAs broader applicability beyond the AtCoder subset shown in Figure 6 (which used the full TCGBench for that visualization), Figure 9 presents SAGAs performance on the Codeforces and Nowcoder portions of the complete TCGBench dataset. SAGA consistently replicates its superior performance, exhibiting enhanced efficacy (DR, Acc) and superior test suite quality (DEPC, Diversity Ratio) compared to the baseline and its individual analytical components across these platforms as well. This consistent pattern of improvement underscores the fundamental benefits of SAGAs structured, insight-driven approach to TCG. 19 To further demonstrate SAGAs broad applicability and robustness, Figure 10 presents its Detection Rate (DR) and Verifier Accuracy (VAcc) when paired with different LLM backbones (Qwen2.5Coder-7B-Instruct, Qwen2.5-72B-Instruct, and DeepSeek-V3-0324) on the Codeforces and Nowcoder portions of the full TCGBench dataset, compared against the Input-Interpreter baseline (using DeepSeek-V3). Across both platforms and all LLM backbones, SAGA consistently and significantly outperforms the baseline in DR and VAcc at various test case sizes. Notably, even SAGA with the smaller Qwen2.5-Coder-7B often surpasses the baseline that utilizes the larger DeepSeek-V3, highlighting SAGAs ability to effectively guide diverse LLMs. While larger SAGA backbones generally yield higher absolute performance, the consistent uplift provided by the SAGA framework across different models and problem sources underscores its fundamental benefits and general applicability for advanced TCG. Figure 10: SAGA performance with different LLM backbones (Qwen-Coder, Qwen-72B, DeepSeekV3) compared to the Baseline (Input-Interpreter with DeepSeek-V3) on Codeforces and Nowcoder portions of the full TCGBench dataset. Metrics: Detection Rate (DR) and Verifier Accuracy (VAcc) at varying test case sizes. Dashed lines indicate baseline performance. Detailed Performance Analysis on TCGBench-Lite by Difficulty To provide more granular understanding of how different Test Case Generation (TCG) methods perform across varying levels of problem complexity, Figure 11 illustrates the Verifier Accuracy (VAcc@50) and Detection Rate (DR@50) of SAGA, its analytical components (Multidimensional Analysis only, denoted \"w/ MultiDim\"; Differential Analysis only, denoted \"w/ Differ\"), and baseline TCG methods (TestChain, EvalPlus, and Random Input-Interpreter) on the Easy, Medium, and Hard problem subsets within TCGBench-Lite. Key Observations from Figure 11: Consistent Superiority of SAGA: Across all difficulty tiers (Easy, Medium, Hard) and for both VAcc@50 and DR@50, the full SAGA framework consistently outperforms all baseline methods (TestChain, EvalPlus, Random). This underscores SAGAs robustness and its ability to generate more effective test suites regardless of problem complexity. For instance, on Hard problems, SAGA achieves VAcc@50 of 25.06%, substantially higher than the Random baseline (18.47%) and other methods. similar trend is observed for DR@50, where SAGA reaches 87.0% on Hard problems. Impact of Problem Difficulty on Baselines: The performance of baseline methods, particularly TestChain and EvalPlus, degrades more noticeably as problem difficulty increases. For example, EvalPluss VAcc@50 drops from 38.10% on Easy problems to mere 5.56% on Hard problems. This suggests that these methods may struggle to generate effective test cases for more complex scenarios or subtle bugs prevalent in harder problems. The Random Input-Interpreter shows more resilience than TestChain and EvalPlus on harder problems but still falls short of SAGA. Synergy of SAGAs Analytical Components: While both Multidimensional Analysis (\"w/ MultiDim\") and Differential Analysis (\"w/ Differ\") components of SAGA individually outperform the baselines, the full SAGA framework generally achieves the best performance or is highly competitive. On Easy problems, SAGAs VAcc@50 (55.59%) is notably higher than \"w/ MultiDim\" (50.00%) and \"w/ Differ\" (49.15%), indicating that the combination of insights from both correct and incorrect solutions is beneficial even for simpler problems. For Medium problems, SAGA (VAcc@50: 33.24%) again leads, showing clear advantage over relying on only one type of human prior. Figure 11: Performance comparison (VAcc@50 and DR@50) of SAGA, its components, and baseline TCG methods across Easy, Medium, and Hard problem subsets of TCGBench-Lite. SAGA consistently outperforms baselines across all difficulties, and its full framework generally surpasses its individual analytical components, especially on harder problems. On Hard problems, the synergy is particularly evident. SAGAs VAcc@50 (25.06%) is superior to \"w/ MultiDim\" (21.92%) and \"w/ Differ\" (21.25%). This suggests that for complex problems with elusive bugs, leveraging diverse insights from both correct solution structures and patterns of common errors is crucial for generating highly discriminative test suites. similar synergistic effect is generally observed for DR@50, where the full SAGA framework often provides the highest or near-highest detection rates. Effectiveness of Differential Analysis on Harder Problems: Interestingly, the \"w/ Differ\" component (Differential Analysis leveraging Swrong) shows relatively strong performance on Hard problems compared to its performance on Easy problems, particularly for VAcc@50. This might imply that analyzing patterns from incorrect submissions is especially valuable for uncovering the types of subtle or complex errors that characterize more difficult problems. Limitations of Simpler Priors: The performance of \"Random\" (Input-Interpreter) and even \"EvalPlus\" (which uses human solutions for mutation) on Medium and Hard problems highlights that merely having access to human solutions or employing random generation is insufficient. SAGAs structured approach to analyzing and strategically leveraging these human priors is what drives its superior performance, especially as complexity increases."
        },
        {
            "title": "H Supplementary Experimental Analyses",
            "content": "To further explore mechanisms for robust test suite construction and the value of diverse generation strategies, we present two additional studies: an analysis of mixing random test cases from different LLMs, and an ablation study on SAGAs knowledge sources. H.1 Efficacy of Mixing Random Test Cases from Different Language Models Our theoretical framework highlights that test suite quality is influenced by individual test potency (p) and inter-test case correlation (ρeff ). We investigated managing ρeff by mixing random test cases (akin to the Input-Interpreter paradigm from Section 2.2) sourced from different LLMs (V3: DeepSeek-V30324, 72B: Qwen2.5-72B-Instruct, Coder: Qwen2.5-Coder-7B-Instruct). The hypothesis is that tests from different models may exhibit lower inter-source correlation, leading to improved combined suite characteristics. Figure 12 shows the AUC@50 when mixing random tests, with diagonal elements representing single-LLM suites. Observations 21 Figure 12: Heatmap illustrating AUC@50 performance of mixed random test suites. Diagonal elements: suites from single model (V3, 72B, Coder). Off-diagonal (i,j): mixing random tests from model and model j. Benefit of Model-Source Diversity: Mixing random tests from two different LLMs frequently yields superior AUC@50 compared to using tests from only one, across AtCoder, Codeforces, and Nowcoder datasets. For instance, on Codeforces, mixing V3 (AUC@50: 0.2014) with 72B (0.1691) results in mixed AUC@50 of 0.2629, surpassing both. This suggests complementary biases even in random generation. Reduced Effective Correlation: The improvements imply that combining tests from different models likely lowers ρeff compared to single-source suites. Enhanced diversity allows the combined suite to cover broader error spectrum, increasing AUC@50. Surpassing Stronger Components: Often, the mixed suite outperforms even the stronger individual model in the pair (e.g., V3+72B on Codeforces). This robustly shows different LLMs contribute unique, valuable random tests, highlighting complementary strengths. Take-away: Diversifying the source of even \"naive\" random tests reduces correlation and improves test suite quality, supporting our theoretical framework. While SAGA achieves this more directly via structured analysis, this experiment underscores the general impact of minimizing ρeff . H.2 Ablation Study: Impact of Human Knowledge Source Volume in SAGA To dissect SAGAs knowledge source contributions, we conducted an ablation study on the Codeforces portion of TCGBench. We modified SAGA by removing its Differential Analysis component (insights from incorrect solutions, Swrong) entirely. Instead, we doubled the volume of correct human solutions (Shuman) fed into SAGAs Multidimensional Analysis component, creating \"Multidim-Enhanced\" version. The aim was to see if increasing one type of human insight could compensate for omitting another. Table 5 compares the original SAGA with this \"Multidim-Enhanced\" configuration. Analysis of Ablation Results  (Table 5)  : Degradation in Core Effectiveness: Despite doubling Shuman input, the \"Multidim-Enhanced\" version shows clear drop in VAcc@50 (from 47.08% to 38.73%) and AUC@50 (from 0.3195 to 0.2744) compared to the full SAGA. This suggests insights from incorrect solutions via Differential Analysis are crucial for verifier quality and cannot be fully compensated by merely increasing the volume of correct solutions for Multidimensional Analysis. 22 Table 5: Ablation Study on Codeforces (TCGBench): SAGA vs. Multidim-Enhanced (Double Shuman for Multidimensional Analysis, No Differential Analysis). Configuration DR@50 VAcc@50 AUC@50 DivRatio@ SAGA (Multidim. + Differ.) Multidim-Enhanced 90.89% 88.09% 47.08% 38.73% 0.3195 0.2744 0.697 0.701 Diversity vs. Effectiveness: While the Diversity Ratio is comparable or slightly higher for \"Multidim-Enhanced\", this raw diversity doesnt translate to better overall verifier quality (AUC@50). This reinforces that the type of diversity and the nature of uncovered error patterns (targeted by Differential Analysis) are critical, not just diversity as raw count. Implications for SAGAs Design: This strongly supports SAGAs dual-pronged approach of integrating insights from both correct and incorrect human solutions. The unique error patterns revealed by Differential Analysis appear vital for building high-quality, discriminative verifiers, and their contribution is not simply replicable by scaling up the input to Multidimensional Analysis alone."
        },
        {
            "title": "I Model Performance on CodeCompass",
            "content": "To further illustrate the utility of CodeCompass as challenging benchmark for evaluating LLM code generation capabilities, Table 6 summarizes the Pass@1 performance of several contemporary LLMs. The evaluation was conducted separately for C++ and Python problem instances within CodeCompass, utilizing its SAGA-enhanced verifier suite. The results demonstrate clear differentiation among models in both languages, underscoring CodeCompasss ability to effectively rank and assess current code generation models. The challenging nature of the SAGA-generated test cases in CodeCompass provides rigorous testbed for future model development and comparison. Table 6: Performance of Various LLMs on CodeCompass (Pass@1) for C++ and Python. Model Pass@1 on CodeCompass (C++) Pass@1 on CodeCompass (Python) Qwen2.5-Coder-32B-Instruct Qwen2.5-72B-Instruct GPT-4o (2024-11-20) DeepSeek-V3 QWQ-32B DeepSeek-Chat-R1 Qwen3-235B-A22B 15.93% 17.04% 20.74% 24.81% 31.85% 38.15% 43.70% 12.59% 14.81% 14.44% 23.33% 26.30% 34.07% 36.30% From Table 6, it is evident that CodeCompass effectively differentiates LLM performance across both C++ and Python. While relative model rankings show some consistency, language-specific performance variations are also notable, highlighting the benchmarks capacity to reveal nuanced capabilities. The generally moderate Pass@1 rates underscore the challenging nature of CodeCompass due to its SAGA-enhanced test suites, making it valuable resource for rigorous multi-lingual code generation assessment. TCGCoder-7B Training Details TCGCoder-7B, our specialist 7-billion-parameter model for Test Case Generation (TCG), was finetuned from Qwen2.5-Coder-7B-Instruct [44, 17]. The training dataset, distinct from our evaluation sets to prevent data leakage, comprised 15,000 early-stage programming problems from Codeforces and NowCoder, processed by our SAGA framework (Section 3) to generate structured outputs (Python Case Scripts, Math Explanations, Self-Validation code, per Figure 5). The fine-tuning aimed to distill SAGAs TCG reasoning into TCGCoder-7B. Key training configurations included 3 epochs, global batch size of 16, an initial learning rate of 5e-6 (minimum 3e-7), max sequence length of 61,335 tokens, and the qwen2 chat template. Training utilized Fully Sharded Data Parallel (FSDP) across 2 nodes, each with 8 GPUs."
        }
    ],
    "affiliations": [
        "MOE KLINNS Lab, Xian Jiaotong University, China",
        "School of Computer Science and Technology, Xian Jiaotong University, China",
        "Shanghai AI Laboratory"
    ]
}