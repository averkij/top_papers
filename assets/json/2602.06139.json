{
    "paper_title": "EgoAVU: Egocentric Audio-Visual Understanding",
    "authors": [
        "Ashish Seth",
        "Xinhao Mei",
        "Changsheng Zhao",
        "Varun Nagaraja",
        "Ernie Chang",
        "Gregory P. Meyer",
        "Gael Le Lan",
        "Yunyang Xiong",
        "Vikas Chandra",
        "Yangyang Shi",
        "Dinesh Manocha",
        "Zhipeng Cai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 9 3 1 6 0 . 2 0 6 2 : r EgoAVU: Egocentric Audio-Visual Understanding Ashish Seth1,2,, Xinhao Mei1, Changsheng Zhao1, Varun Nagaraja1, Ernie Chang1, Gregory P. Meyer1, Gael Le Lan1, Yunyang Xiong1, Vikas Chandra1, Yangyang Shi1, Dinesh Manocha2, Zhipeng Cai1, 1Meta, 2University of Maryland, College Park Work done during intership at Meta, Project Lead Understanding egocentric videos plays vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, scalable data engine to automatically generate egocentric audio-visual narrations, questions and answers. EgoAVU enriches human narrations with multimodal context and generates audiovisual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph based curation ensure both the data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct large scale training dataset of 3M samples, and EgoAVU-Bench manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitation of existing MLLMs: they bias heavily towards visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively solves this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefit can also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community. Date: February 9, 2026 Correspondence: Ashish Seth: aseth125@umd.edu, Zhipeng Cai: czptc2h@gmail.com Project Page: https://cs20s030.github.io/EgoAVU/ Code: https://github.com/facebookresearch/EgoAVU Data: https://huggingface.co/datasets/facebook/EgoAVU_data"
        },
        {
            "title": "1 Introduction",
            "content": "Egocentric videos capture rich and dynamic first person audiovisual information centered on daily human activities, such as cooking, painting, or assembling objects (Grauman et al., 2022; Nasirimajd et al., 2023; Grauman et al., 2024). Understanding such data plays vital role in embodied intelligence and mixedrealities (Martins et al., 2023; Nagarajan et al., 2023; Puig et al., 2020; Savva et al., 2019). The highly dynamic camera motion and the limited field of view make comprehensive egocentric video understanding challenging with solely visual cues (Chen et al., 2024a). This motivates the use of audio information, which provides persistent contextual signals tied to ongoing events. Though recent multi-modal language models (MLLMs) can accept both audio and visual inputs (Xu et al., 2025b,a; Team et al., 2024; Han et al., 2023; OpenBMB, 2024), whether they understand the joint dynamic of audiovisual signals in egocentric videos remains an open question. The main bottleneck to study this problem arguably is limited data. For training, existing egocentric datasets, such as MultiHop-EgoQA (Chen et al., 2025) and MM-Ego (Ye et al., 2024), are derived mostly from the human narrations in Ego4D (Grauman et al., 2022). While these narrations provide valuable human supervision, they bias toward describing humanobject interactions and lack broader environmental context or the diversity of auditory signals in egocentric recordings. For evaluation, existing benchmarks (Mangalam et al., 2023; Cheng et al., 2024a; Plizzari et al., 2025) focus mainly on visual cues, limiting their ability to assess integrated audiovisual reasoning. Though exocentric audiovisual benchmarks (Yang et al., 2022; Li et al., 2024b; Sung-Bin et al., 2024; Ma et al., 2024) exist, their multi-modal dynamics are fundamentally different. 1 Figure 1 Overview of EgoAVU. We introduce EgoAVU, scalable and automated data engine to enable egocentric audiovisual understanding. EgoAVU enriches existing egocentric narrations by integrating human actions with environmental context, explicitly linking visible objects and the sounds produced during interactions or surroundings. Leveraging this pipeline, we construct EgoAVU-Instruct (3M QAs) and EgoAVU-Bench (3K verified QAs), enabling systematic training and evaluation of MLLMs. Models finetuned with EgoAVU-Instruct exhibit high audio-visual grounding in egocentric settings. To address these problems, we introduce EgoAVU, fully automated data engine that can generate diverse and high quality audio-visual-language data from public egocentric datasets such as Ego4D (Grauman et al., 2022). EgoAVU comprises four key components: (i) Enhancing egocentric narrations by enriching human descriptions with environmental context, visual object details, and audio captions generated using diverse open-source MLLMs (Bai et al., 2025; Xu et al., 2025a; AI@Meta, 2024); (ii) Filtering videos with rich audiovisual dynamics by selecting egocentric clips featuring varied action sequences, humanobject interactions, and wide range of ambient and foreground sounds; (iii) Generating fine-grained event captions through audiovisual correlation by integrating modality-specific cues such as actions, objects, and sounds, and modeling their relationships to enhance multimodal reasoning; and (iv) Curating diverse audiovisual understanding tasks that span grounding, temporal reasoning, scene understanding, and audiovisual hallucination. Leveraging EgoAVU, we construct EgoAVU-Instruct large scale training dataset of 9K egocentric videos with 3M audiovisual-language samples, and EgoAVU-Bench an evaluation split of 900 videos containing 3K manually verified samples. The generated data features strong multi-modal correspondence, long average video durations (4min), open and close-ended questions capturing diverse aspects of egocentric audio-visual understanding. Comprehensive experiments show that existing MLLMs perform poorly on EgoAVU-Bench, revealing significant limitations in their joint audiovisual reasoning capabilities. The seven models we tested show consistent bias towards the vision modality, often neglecting audio cues or struggle to connect them to the correct visual source. Fine-tuning MLLMs such as Qwen2.5-Omni (Xu et al., 2025a) on EgoAVU-Instruct can effectively close this gap, resulting in up to 113% relative performance boost on EgoAVU-Bench. More importantly, the performance gain is transferrable to other egocentric benchmarks, such as EgoTempo (Plizzari et al., 2025) and EgoIllusion (Seth et al., 2025), achieving up to 28% relative performance improvement."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal-Large Language Models. Recent advances in Multimodal Large Language Models (MLLMs) (Team et al., 2024; Achiam et al., 2023; Xu et al., 2025b,a; Wang et al., 2024; OpenBMB, 2024; Cheng et al., 2024b) 2 have substantially extended the capabilities of large language models (Achiam et al., 2023; AI@Meta, 2024; Yang et al., 2025) beyond text, enabling unified understanding of visual and auditory inputs. Despite this progress, existing MLLMs face critical limitations when applied to egocentric audio-visual understanding. First, leading models developed for egocentric video understanding, such as MM-Ego (Ye et al., 2024) and EgoVLPv2 (Pramanick et al., 2023), lack the ability to incorporate audio cues, limiting them to perform visual only tasks. Second, even models capable of handling viusal and auditory signals, including Qwen2.5-Omni (Xu et al., 2025a) and Video-LLaVA2 (Cheng et al., 2024b), remain primarily trained and benchmarked on exocentric audio-visual data (Yang et al., 2022; Geng et al., 2025). As result, they struggle to generalize to egocentric settings, which exhibit fundamentally different characteristics such as dynamic camera motion, frequent self-occlusions, and distinct audio profiles. These limitations highlight significant gap in current MLLMs to jointly understand audiovisual cues in egocentric videos. Egocentric-Video Understanding. The field of egocentric video understanding has gained increasing attention due to its relevance in augmented reality (AR) (Nagarajan et al., 2023; Martins et al., 2023) and embodied AI (Savva et al., 2019; Puig et al., 2020). Large-scale datasets such as Ego4D (Grauman et al., 2022), EPIC-KITCHENS (Nasirimajd et al., 2023), and Ego-Exo4D (Grauman et al., 2024) have driven progress by enabling the creation of egocentric videolanguage corpora (e.g., MultiHop-EgoQA (Chen et al., 2025), EgoTextVQA (Zhou et al., 2025), QaEgo4D (Bärmann and Waibel, 2022)) and benchmarks (e.g., EgoSchema (Mangalam et al., 2023), EgoTempo (Plizzari et al., 2025), EgoIllusion (Seth et al., 2025)). However, these datasets are predominantly constructed from textual narrations, focusing on humanobject interactions while overlooking environmental and auditory cues, critical for accurately inferring actions, contextual dynamics, and scene semantics in egocentric settings. Recent benchmarks such as EgoTempo and EgoIllusion attempt to enhance contextual understanding through visual captioning, but their reliance on closed-source models (e.g., Gemini (Team et al., 2024), GPT-4o (Achiam et al., 2023)) makes large-scale, reproducible data generation challenging. In this work, we address these problems by introducing scalable data-engine capable of generating time-aligned joint audio-visual generation while utilizing open-source MLLMs (Xu et al., 2025a; Bai et al., 2025; AI@Meta, 2024)."
        },
        {
            "title": "3 Method",
            "content": "Fig.2 provides an overview of EgoAVU. We begin by collecting and organizing egocentric data (Sec.3.1). Next, we enhance egocentric narrations using MLLM-generated multi-modal context (Sec.3.2), and use the tokens of enriched narration to filter videos for temporal diversity (Sec.3.3). We then introduce multi-modal context graph (MCG) that captures complex cross-modal relations, and parse these graphs with open-source LLMs to fuse multi-modal information into single dense narration (Sec.3.4). Finally, this fused narration is used to generate QA pairs for joint audio-visual understanding (Sec.3.5). To ensure the scalability, EgoAVU only utilizes open source models. Please refer to Appendix for example outputs and prompts for each module."
        },
        {
            "title": "3.1 Data Collection",
            "content": "We begin by collecting videos from the Ego4D dataset (Grauman et al., 2022), and filter out videos that lack audio tracks. Each video in Ego4D is accompanied by set of narrations that provide first-person descriptions of the events, such as #C holds cup, where #C refers to the camera wearer. These narrations can , where Nj denotes the narration text, tj is the corresponding timestamp, and be represented as {Nj, tj}K is the total number of narrations in the video. To obtain the temporal boundries of each narration, we use the strategy proposed in prior works (Plizzari et al., 2025; Lin et al., 2022), where temporal boundries Tj for narration Nj is defined as: j=1 Tj = βi 2α βi represents the average interval between consecutive timestamps ({Tj, Tj+1}), and α denotes the global average of βi across the entire dataset. Since the segments associated with each narration are short (on average 3s), they are insufficient to capture fine-grained visual and auditory information. Following (Di and Xie, 2024), for each video v, we group consecutive segments and their corresponding narrations to form video clips vj of at least 10s and not more than 360s. , tj + βi 2α tj . (1) (cid:27) (cid:26) Figure 2 EgoAVU pipeline. EgoAVU consists of four key components. (1) For each egocentric video clip, EgoAVU enhances the raw narration with detailed multisensory context using open-source MLLMs (Bai et al., 2025; Xu et al., 2025a). (2) These enriched narrations are then used to select clips that exhibit diverse audiovisual dynamics. (3) Next, EgoAVU constructs Multimodal Context Graph (MCG), automatically generated via open-source LLMs (AI@Meta, 2024), to capture complex cross-modal relations. The MCG is parsed alongside the enhanced narrations to produce coherent audiovisual narrations. (4) The generated audio-visual narrations are leveraged to generate high-quality audiovisual QA pairs, forming both the instruction-tuning dataset EgoAVU-Instruct and the evaluation benchmark EgoAVU-Bench."
        },
        {
            "title": "3.2 Narration Enhancement",
            "content": "After data collection, we generate descriptions about audio and video frames to enrich the original egocentric action-centric narrations. The goal of this stage is to obtain detailed descriptions for both modalities, which paves the way for diverse and fine-grained QA generation. The most straight-forward approach is to input both video frames and audio tracks into an MLLM, and prompt it to generate the audio and visual descriptions. However, our initial experiments show that MLLMs, when provided with audio and video inputs together, cannot capture important details due to modality bias and hallucination. Specifically, we evaluate open-source models, including Qwen2.5-Omni (Xu et al., 2025a) and MiniCPMo (OpenBMB, 2024), on 200 randomly sampled video clips. Each model is first used in uni-modal setting, captioning visual and auditory information independently, i.e., visual captioning without audio input and audio captioning without video input. We then perform joint audiovisual captioning by providing both modalities simultaneously. After that, we manually compare the consistency for both visual and audio modalities by computing the ratio where an object/event captured in the uni-modal output is also correctly appearing in the multi-modal output. I.e., whether the multi-modal output can capture all the details as in the uni-modal settting. We observe consistent pattern where models either omit various sounds or associate audio cues with incorrect visual events. For example, Qwen2.5-Omni shows an error rate of 54.3% for audio and 25.4% for visual consistency, while MiniCPM-o yields 68.2% and 31.2%, respectively (see Fig 10 for example). These findings show that existing MLLMs struggle to maintain accurate and detailed cross-modal grounding in egocentric contexts, motivating modular data engine that leverages specialized models to process different modalities independently. With this observation, we leverage collection of MLLMs to extract rich multisensory information from individual modalities separately. Specifically, for each video segment, we first capture detailed spatial 4 descriptions of objects by applying an image captioner such as Qwen2.5-VL (Bai et al., 2025) to the center frame. To model temporal dynamics, including camera motion, action sequences, and auditory events, we employ Qwen2.5-Omni in two complementary modes. First, we use it as video captioner, processing video frames without audio, to generate coherent video-level narrations. Then, we use the same model as an audio captioner without visual inputs to produce detailed auditory narrations that capture both foreground sounds closely tied to human activities (e.g., impacts and hissing) and background sounds (e.g., bird chirping, wind blowing). This process results in time-aligned uni-modal narrations that encapsulate both visual and auditory aspects of the scene."
        },
        {
            "title": "3.3 Video Filtering for Diversity Enhancement",
            "content": "After obtaining the enhanced narrations, we use them to compute lexical diversity, filtering videos with rich and diverse visual and auditory signals. Specifically, for each video v, we combine the segment-level narrations into single narration that captures all objects, actions, and sounds present in the video. We then tokenize this narration into Tv = {t1, . . . , tn} and compute the Moving-Average TypeToken Ratio (MATTR) (Covington and McFall, 2010), which averages the proportion of unique words across sliding windows of size w: MATTR(Tv) = 1 + 1 nw+1 (cid:88) i=1 Uni.(ti, . . . , ti+w1) . (2) Higher MATTR scores indicate narrations that describe wider variety of objects, actions, and auditory events. We retain videos whose MATTR exceeds threshold τ = 0.3, effectively removing the bottom 25% of our distribution to filter static or repetitive descriptions, leading to the final video count of 9,900."
        },
        {
            "title": "3.4 Audio-Visual Narration Generation",
            "content": "Our next objective is to generate unified narration that captures both auditory and visual information for each segmented video clip. We do this leveraging text-based LLMs. When synthetically merging unimodal narrations, the LLM must first implicitly retrieve key details across modalities, such as which objects the human interacts with, which remain in the background, and what actions or objects produce specific sounds. It must then integrate this information into coherent and perceptually grounded narration. However, our preliminary experiments show that directly prompting open-source LLMs such as LLaMA-70B (AI@Meta, 2024) often fails to maintain consistent humanobject interactions and soundsource associations in their responses (See Appendix A.3 for detailed comparison). We design two-stage pipeline that addresses this challenge. First, we organize audio-visual cues from uni-modal narrations into structured representation called the Multi-modal Context Graph (MCG). As shown in Fig. 2, MCG captures relationships between visible actions, objects, and audible sounds. Each MCG is generated by prompting an LLM (LLaMA-70B) with enhanced narrations to extract the following information : Interacted Objects: Objects with which the person physically interacts, along with interaction types, inferred from action narrations. Background Objects: Objects visible in the environment but never interacted with, identified by comparing objects and action narrations. Foreground Sounds: Human-induced action sounds correlated with specific actions (e.g., impact sound places the phone on the table), or ambient sounds grounded in visible scene elements (e.g., dog barking aligned with visible dog). Background Sounds: Sounds present in the audio track whose sources cannot be visually grounded. We find that MCG makes multi-modal relationships explicit and easily inferable, which we utilize further to generate joint audio-visual narrations. Next, we leverage MCGs to guide the generation of unified, high-coherence audiovisual narrations. The goal is to fuse both visual and auditory details into the same multi-modal coherent narration. As shown in Fig 2, we provide the LLM (LLaMA-70B) with enhanced narrations and MCGs, and prompt it to generate the combined 5 Category Example Open-Ended SourceSound Association (SSA) What is the source of the sizzling sound heard in the video? AudioVisual Segment Narration (AVSN) Between 240 and 250 seconds, describe the persons surroundings, actions, and the sounds that can be heard? AudioVisual Dense Narration (AVDN) Describe in detail what the person sees, hears, and does throughout the video. Close-Ended Temporal Reasoning (TR) What happened before the person opened kitchen cabinet?. Choose the correct option from the following options: ... AudioVisual Hallucination (AVH) Is there beeping sound coming from the microwave in the video? Table 1 Question Prompts. Examples of open-ended and close-ended questions in EgoAVU-Instruct and EgoAVU-Bench. Dataset A&V # Test Avg. Dur. QA-Type # Ans. EgoTaskQA (Jia et al., 2022) EgoSchema (Mangalam et al., 2023) EgoThink (Cheng et al., 2024a) EgoTempo (Plizzari et al., 2025) EgoIllusion (Seth et al., 2025) EgoAVU-Bench(Ours) 8k 500 750 500 8k 3k 25s 3 min 45s 45s Open Close Open Open 3 min Open+Close 13 - 4 10 3 4 min Open+Close Table 2 Benchmark statistics. Beside having QAs with high audio-visual coherence, EgoAVU-Bench features large number of QA pairs, longer egocentric videos with audio track, support both open/closed QA type and consists of significantly longer and descriptive responses. Figure 3 Video duration distribution. Our videos includes both short clips within 1 min and long videos of 6 min. narration. The prompt asks the LLM to first extract explicit cues from the MCG, including interacted and background objects, grounded sound events, and their associations with actions and visible sources, and then align these cues with the corresponding temporal descriptions from the video and action-level narrations."
        },
        {
            "title": "3.5 QA Generation",
            "content": "Using the unified audiovisual narrations, EgoAVU synthesizes questionanswer pairs that probe diverse aspects of egocentric audio-visual understanding. EgoAVU Taxonomy. We design five categories of QAs encompassing both open-ended questions, where models must produce descriptive responses integrating visual and auditory cues, and closed-ended questions, which test precise multi-modal perceptual understanding through multiple-choice or binary (Yes/No) formats. detailed description of each category is provided below. Open-ended QAs. These questions are primarily free-form, requiring the model to develop comprehensive understanding of temporal and spatial dynamics across visual and auditory cues in egocentric videos. The specific categories include: (1) SoundSource Association (SSN): Identify various foreground sounds in the video and determine their corresponding visible sources including human actions or various objects shown in the video. (2) AudioVisual Segment Narration (AVSN): Answer segment-level questions by producing coherent and natural narrations that describe what the person is doing, seeing, and hearing, including both foreground and background sounds, grounded within the specified temporal range. (3) AudioVisual Dense Narration (AVDN): Extend previous task to the entire video, assessing the models ability to maintain narrative coherence across the complete video. Close-ended QAs. These questions are particularly useful for adversarial testing. Close-ended QAs allow the construction of fine-grained distractors and counterfactuals to probe models susceptibility to spurious correlations in audio-visual understanding. We design two main categories: (1) Temporal 6 Figure 4 Distribution of 20 most common visual scenarios in EgoAVU-Instruct and EgoAVU-Bench. Figure 5 Distribution of proposed tasks across EgoAVUInstruct and EgoAVU-Bench. Reasoning (TR): Multiple-choice questions with four options that assess the models understanding of temporal relationships among multi-modal events such as human actions, visual objects, and auditory cues in egocentric videos. These questions focus on reasoning about event ordering, for example identifying which event occurred first or last, or answering before/after queries. (2) AudioVisual Hallucination (AVH): Binary (Yes/No) questions that evaluate the models tendency to hallucinate when verifying the presence of actions, objects, or sounds throughout the video."
        },
        {
            "title": "3.6 Dataset Overview",
            "content": "Leveraging EgoAVU, we construct the first egocentric audiovisual dataset suite for training and evaluating MLLMs. Our large-scale instruction-tuning corpus, EgoAVU-Instruct, comprises approximately 3M samples with 9K egocentric videos, while the evaluation benchmark, EgoAVU-Bench, contains 3K QA pairs with 900 distinct videos. As shown in Fig. 4, both EgoAVU-Instruct and EgoAVU-Bench encompass wide range of real-world visual scenarios, such as cooking, painting, and other indoor/outdoor activities. Extensive manual verifications are conducted on EgoAVU-Bench to ensure that the audio-visual information is correctly grounded. As illustrated in Fig. 5, both EgoAVU-Instructand EgoAVU-Bench covers all five question categories introduced earlier, featuring balanced mix of openand closed-ended formats. As shown in Fig. 3, the video length in both datasets vary from 1 to 6 minutes, providing rich temporal diversity for model training and evaluation. Table 2 compares existing egocentric benchmarks with our EgoAVU-Bench. Besides filling the gap of egocentric audio-visual understanding, EgoAVU-Bench includes large number of QAs, features longer videos with synchronized audio tracks, supports both openand closed-ended question formats, and provides significantly longer and more descriptive responses."
        },
        {
            "title": "4 Results",
            "content": "Implementation Details. We verify the effectiveness of our instruction-tuning dataset, EgoAVU-Instruct, by fine-tuning MLLMs such as Qwen2.5-Omni (7B) on it and compare the finetuned model with baselines on EgoAVU-Bench. Fine-tuning is performed using LLaMA-Factory (Zheng et al., 2024), under two settings: LoRA (Hu et al., 2022) and full fine-tuning. All experiments are conducted on 64 H100 GPUs with global batch size of 64, training each model for 5 epochs. To ensure consistent visual coverage across samples, during training, we uniformly sample 300 frames per video. We also uniformly sample data from each of the 5 tasks to achieve balanced performance (See Appendix for additional details). Evaluation Protocol. For close-ended QAs in EgoAVU-Bench, we follow (Yue et al., 2024) and use regex-based string matching, where we construct robust regular expressions and design response-processing module to extract key phrases such as option IDs (A, B, C, D), binary indicators (yes/no), and conclusion phrases from long responses for accurate answer matching. For open-ended QAs, similar to prior work (Plizzari et al., 2025), 7 Models Size SSA () AVDN () () Open-Source MLLMs AVSN () () TR AVH () Acc. () Acc. () VideoLLaMA2 (Cheng et al., 2024b) Baichuan-Omni (Li et al., 2024a) Intern-Omni (Chen et al., 2024b) Phi4-mm (Abouelenin et al., 2025) MiniCPM-o (OpenBMB, 2024) Qwen2.5-Omni (Xu et al., 2025a) Qwen2.5-Omni (Xu et al., 2025a) Ours (LoRA) Ours (F ull) (%) 7B 7B 8.7B 8B 8B 3B 7B 7B 7B 1.51 1.49 1.47 1.42 1.43 1.45 1.50 1.88 1.92 1.95 1.59 2.27 2.17 2.37 3.65 4.82 5.20 8.79 10.84 8.55 10.69 1.71 1.79 1.82 1.69 2.06 1.85 1.99 7.50 8.34 8.69 12.17 9.68 8.63 9.99 13.89 14.21 14.70 16.90 12.19 13.08 13. MLLM trained with EgoAVU-Instruct 3.15 2.60 12.20 17.19 2. 22.53 28.34 3.20 17.32 28.70 +113.3 +12.2 +16.9 +17.2 +27.6 +86.5 +69.8 22.68 12. 2.66 2.63 37.00 39.85 41.22 45.04 26.44 46.40 53.20 64.31 67.84 +27.2 20.32 21.10 21.75 22.89 21.76 26.28 42. 61.69 60.12 +30.8 () 8.32 9.75 10.11 13.13 14.77 13.45 14.74 Table 3 Main result on EgoAVU-Bench. We compare seven MLLMs with joint audiovisual understanding capabilities against our fine-tuned models across diverse set of tasks in EgoAVU-Bench, including open-ended QAs: Source-Sound Association (SSA), Audio-Visual Dense Narration (AVDN), and Audio-Visual Segment Narration (AVSN), as well as closed-ended QAs: Temporal Reasoning (TR) and Audio-Visual Hallucination (AVH). For the open-ended tasks, we report LLM-as-Judge (S), METEOR (M), and ROUGE-L (R). For the closed-ended tasks, we report Accuracy (Acc.). Additionally for each task, we compute the relative performance gain () between the best open-source model and our fine-tuned models. we adopt the LLM-as-a-judge approach, employing Qwen3-235B-A22B-Instruct-2507 (Yang et al., 2025) as an open-source judge for reproducibility. The model rates MLLM-generated responses on 15 scale (see Appendix for the evaluation prompt). We additionally report standard metrics used for dense response evaluation, including ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie, 2005)."
        },
        {
            "title": "4.1 Main Results\nTable 3 presents the main result on EgoAVU-Bench. The key findings are summarized below.",
            "content": "MLLMs struggle to associate sounds with their visual sources. This is evident from their low scores on the Source-Sound Localization (SSA) task, where LLM-as-a-judge (S) evaluations remain below 1.6 out of 5 across all baseline MLLMs. MLLMs struggle to produce coherent and temporally aligned audio-visual narrations. In both the Audio-Visual Dense Narration (AVDN) and Audio-Visual Segment Narration (AVSN) tasks, even the best-performing model, Qwen2.5-Omni (7B), attains scores of below 2.4, which is consistent with low caption quality metrics such as ROUGE-L (R) and METEOR (M). MLLMs demonstrate limited temporal reasoning over joint audio-visual inputs. The highest Temporal Reasoning (TR) accuracy, achieved by Qwen2.5-Omni (7B), is merely 53.2%, with models such as VideoLLaMA2 and MiniCPM-o performing considerably worse. MLLMs frequently hallucinate during audio-visual reasoning, as reflected by their below 43% accuracy on the Audio-Visual Hallucination (AVH) task. Finetuning MLLMs on EgoAVU-Instruct effectively improves egocentric audio-visual understanding. Our finetuned model yields substantial and consistent performance gains across all tasks. Compared to the best performing baseline, we achieve up to 113.3% and 44.5% relative performance improvement on open and close ended tasks respectively. In addition, LoRA and full finetuning both provides considerable performance gain. This shows the possibility to achieve strong audio-visual understanding under resource-limited scenarios. Figure 6 compares the responses of VideoLLaMA2 and Qwen2.5-Omni (7B), with our fine-tuned model on both open-ended and close-ended tasks. In close-ended tasks like Audio-Visual Hallucination, when queried about sound source absent from the video, Qwen2.5-Omni often fabricates visually plausible 8 Models Size VideoLLaMA2 Phi4-MM MiniCPM-o Qwen2.5-Omni Qwen2.5-Omni Ours (LoRA) Ours (F ull) 7B 8B 8B 3B 7B 7B 7B Action Sound Object Acc. () Acc. () Acc. () 20.32 21.74 24.46 29.35 50.00 19.27 24.49 16.33 24.49 33.67 21.36 22.45 24.49 25.00 44. 60.20 61.32 61.09 62.40 63.78 64.20 Table 4 Error Analysis for Audio-Visual Hallucination (AVH). Models Size VideoLLaMA2 Phi4-MM MiniCPM-o Qwen2.5-Omni Qwen2.5-Omni Ours (LoRA) Ours (F ull) 7B 8B 8B 3B 7B 7B 7B Action Sound Object Acc. () Acc. () Acc. () 38.38 48.48 25.00 69.70 64.65 38.30 46.74 37.50 31.91 36. 45.88 48.81 40.48 44.44 43.53 54.31 55.29 68.90 69.80 52.45 53.17 Figure 6 Qualitative Analysis on EgoAVU-Bench. Table 5 Error Analysis for Temporal Reasoning (TR). Model Qwen2.5-Omni Ours (LoRA) Ours (F ull) EgoTempo Acc. () 16.25 EgoIllusion Acc. () 56.32 20.83+28.1% 20.21+24.4% 60.36+7.2% 60.24+7.0% EgoSchema Acc. () VideoMME Acc. () AVQA Acc. () 67.43 67.340.1% 66.211.8% 73.0 72.40.01% 72.00.01% 89.4 89.7+0.003% 89.5+0.001% Table 6 Results on Egocentric and Exocentric benchmarks. Finetuning on EgoAVU-Instruct benefits other egocentric benchmarks such as EgoTempo and EgoIllusion, achieving up to 28.1% accuracy gain. Our model also maintains strong performance on exocentric video QA benchmarks such as VideoMME and AVQA. yet non-existent source, whereas our model effectively resists such misleading prompts through improved audiovisual grounding. For open-ended tasks such as Audio-Visual Segment Narration, our model exhibits stronger soundsource coherence and more accurate action-sequence understanding. Fig. 7 further shows examples on audio-visual dense narration (AVDN). Unlike our fine-tuned model, existing MLLMs produce sparse descriptions for Audio-Visual Dense Narration task in EgoAVU-Bench and further overlook audio cues or fail to ground sounds to their sources."
        },
        {
            "title": "4.2 Evaluating Existing Egocentric Benchmarks",
            "content": "To further evaluate the generalizability of our fine-tuned model, we report its performance on additional egocentric videolanguage benchmarks, including EgoTempo, EgoSchema, and EgoIllusion. As shown in Table 6, fine-tuning on EgoAVU-Instruct leads to notable accuracy improvements on EgoTempo and EgoIllusion, with gains of up to 28.1%, while maintaining competitive performance on EgoSchema, showing only marginal decrease of 0.1%. Note that LoRA performs slightly better than full finetuning in terms of improving other datasets. These results demonstrate that training on EgoAVU-Instruct enhances audio-visual understanding without causing overfitting, and further complements performance across diverse egocentric video QA tasks. We also evaluate our finetuned model on non-egocentric datasets. In addition to egocentric benchmarks, we evaluate our fine-tuned model on popular exocentric Video QA benchmarks, including VideoMME (Short duration split w/o subtiles) (Fu et al., 2024) and AVQA (Yang et al., 2022). As shown in Table 6, despite being fine-tuned exclusively on egocentric QAs, our model almost retains its original performance on VideoMME and slightly outperforms the base model Qwen2.5-Omni on 9 Figure 7 Qualitative comparison of various MLLMs on the Audio-Visual Dense Narration (AVDN) task. Our model fine-tuned on EgoAVU-Instruct captures significantly more dense visual details than Qwen2.5 Omni and VideoLLaMA2, while also identifying auditory cues related to human actions and background sounds in the video. audio-visual QAs in AVQA."
        },
        {
            "title": "4.3 Error Analysis\nIntrigued by the weak performance of current MLLMs across all tasks in EgoAVU-Bench, we conduct a\ndetailed error analysis separately for the close-ended and open-ended tasks to analyze their behavior patterns.\nOur key findings are summarized below:",
            "content": "Close-Ended Tasks. Firstly, as illustrated in Table 4 and 5, for close-ended tasks such as Audio-Visual Hallucination (AVH) and Temporal Reasoning (TR), we evaluate MLLMs ability to independently perceive multisensory inputs within egocentric videos, specifically, human actions, visual objects, and sounds, including both foreground and background audio cues. This is achieved by separately evaluate the accuracy on the 3 subsets of corresponding tasks. Overall, our analysis reveals consistent trend: MLLMs struggle the most with identifying sounds, followed by human actions, while performing relatively better at recognizing visual objects. For instance, in the TR task, the best-performing model, Qwen2.5-Omni, achieves only 36.1% accuracy in identifying sounds, exhibiting substantial performance gaps of 28.5% and 7.4% relative to visual object and human action identification, respectively. Furthermore, we find that model fine-tuned on our dataset, achieves significant boost in its ability to independently perceive multisensory inputs within egocentric videos. For example, in the AVH task, compared to Qwen2.5-Omni, our fine-tuned model shows reduced hallucination rate of 15.9%, 11.0%, and 30.0% when identifying human actions, visual objects, and sounds, respectively. Open-Ended Tasks. We further extend our analysis to open-ended tasks such as Source-Sound Association (SSA) in EgoAVUBench, aiming to determine which modality contributes more to the overall error rate when models are asked to produce joint audiovisual descriptions. Specifically, we randomly sample 200 data points from the SSA task and, for each incorrect response across different MLLMs, manually annotate whether the error stems from inaccurate sound perception or an incorrect source description (which may involve visible objects or humanobject interactions in the egocentric video). As shown in Fig. 8, our model achieves substantially lower error rate of 21.1% compared to the next best model, Qwen2.5-Omni. Interestingly, other 10 Figure 8 Error Analysis on Sound-Source Association (SSA). Figure 9 Examples of video filtering based on MATTR scores. MLLMs such as MiniCPM-o and Phi4-mm, which exhibit much higher error rates, show that over 72% of their errors stems from incorrect or missed sound descriptions rather than misidentified humanobject interactions. This indicates that MLLMs primarily fail due to their limited ability to accurately perceive and interpret sounds, leading to incorrect associations with their visible sources."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented EgoAVU, novel data engine to enhance egocentric audio-visual understanding by addressing the data limitations. EgoAVU employed modularized MLLMs to enhance egocentric narrations, and leveraged multi-modal context graphs to generate diverse, high-quality audio-visual QA pairs. Our evaluation benchmark EgoAVU-Bench revealed for the first time that existing MLLMs exhibit consistent vision bias, often neglecting or hallucinating audio information in egocentric videos. Our large-scale training dataset EgoAVU-Instruct effectively mitigated this gap, significantly improved performance on both EgoAVU-Bench and existing egocentric benchmarks. Importantly, EgoAVU demonstrated the self-learning potential of MLLMs: using uni-modal capabilities to improve the joint-modal capability. In terms of limitation, our training data, though with carefully designed filtering techniques, still contains noise from open source MLLM outputs. We believe this problem can be continually alleviated as the uni-modal capabilities of MLLMs improve, and leave the development of more soficsticated approaches as future work."
        },
        {
            "title": "References",
            "content": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AI@Meta. Llama 3 model card. 2024. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. https://arxiv.org/abs/2502.13923. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. Leonard Bärmann and Alex Waibel. Where did leave my keys?-episodic-memory-based question answering on egocentric videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15601568, 2022. Changan Chen, Puyuan Peng, Ami Baid, Zihui Xue, Wei-Ning Hsu, David Harwath, and Kristen Grauman. Action2sound: Ambient-aware generation of action sounds from egocentric videos, 2024a. https://arxiv.org/abs/2406. 09272. Qirui Chen, Shangzhe Di, and Weidi Xie. Grounded multi-hop videoqa in long-form egocentric videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 21592167, 2025. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024b. Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, and Yang Liu. Egothink: Evaluating first-person perspective thinking capability of vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429114302, 2024a. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024b. Michael Covington and Joe McFall. Cutting the gordian knot: The moving-average typetoken ratio (mattr). Journal of quantitative linguistics, 17(2):94100, 2010. Shangzhe Di and Weidi Xie. Grounded question-answering in long egocentric videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1293412943, 2024. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, and Feng Zheng. Longvale: Vision-audiolanguage-event benchmark towards time-aware omni-modal perception of long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1895918969, 2025. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. 12 Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Imagebind-llm: Multi-modality instruction tuning, 2023. https://arxiv.org/abs/2309.03905. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human tasks in egocentric videos. Advances in Neural Information Processing Systems, 35:33433360, 2022. Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, et al. Baichuan-omni technical report. arXiv preprint arXiv:2410.08565, 2024a. Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, et al. Omnibench: Towards the future of universal omni-language models. arXiv preprint arXiv:2409.15272, 2024b. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. https://aclanthology.org/ W04-1013/. Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Xu, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language pretraining. Advances in Neural Information Processing Systems, 35:75757586, 2022. Jie Ma, Min Hu, Pinghui Wang, Wangchun Sun, Lingyun Song, Hongbin Pei, Jun Liu, and Youtian Du. Look, listen, and answer: Overcoming biases for audio-visual question answering. Advances in Neural Information Processing Systems, 37:95079531, 2024. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. Nuno Cid Martins, Bernardo Marques, Paulo Dias, and Beatriz Sousa Santos. Extending the egocentric viewpoint in situated visualization using augmented reality. In 2023 27th International Conference Information Visualisation (IV), pages 8389. IEEE, 2023. Tushar Nagarajan, Santhosh Kumar Ramakrishnan, Ruta Desai, James Hillis, and Kristen Grauman. Egoenv: Humancentric environment representations from egocentric video. Advances in Neural Information Processing Systems, 36: 6013060143, 2023. Amirshayan Nasirimajd, Simone Alberto Peirone, Chiara Plizzari, and Barbara Caputo. Epic-kitchens-100 unsupervised domain adaptation challenge: Mixed sequences prediction. arXiv preprint arXiv:2307.12837, 2023. OpenBMB. Minicpm-o 2.6: gpt-4o level mllm for vision, speech, and multimodal live streaming on your phone, 2024. Accessed: 2025-03-07. Chiara Plizzari, Alessio Tonioni, Yongqin Xian, Achin Kulshrestha, and Federico Tombari. Omnia de egotempo: Benchmarking temporal understanding of multi-modal llms in egocentric videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2412924138, 2025. Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, and Pengchuan Zhang. Egovlpv2: Egocentric video-language pre-training with fusion in the backbone. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 52855297, 2023. Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: challenge for social perception and human-ai collaboration. arXiv preprint arXiv:2010.09890, 2020. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 93399347, 2019. Ashish Seth, Utkarsh Tyagi, Ramaneswaran Selvakumar, Nishit Anand, Sonal Kumar, Sreyan Ghosh, Ramani Duraiswami, Chirag Agarwal, and Dinesh Manocha. EGOILLUSION: Benchmarking hallucinations in egocentric In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, video understanding. 13 editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 28449 28468, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main.1446. https://aclanthology.org/2025.emnlp-main.1446/. Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung, and Tae-Hyun Oh. Avhbench: cross-modal hallucination benchmark for audio-visual large language models. arXiv preprint arXiv:2410.18325, 2024. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025a. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. Avqa: dataset for audio-visual question answering on videos. In Proceedings of the 30th ACM international conference on multimedia, pages 34803491, 2022. Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, et al. Mm-ego: Towards building egocentric multimodal llms. arXiv preprint arXiv:2410.07177, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. http://arxiv.org/abs/2403.13372. Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, and Angela Yao. Egotextvqa: Towards egocentric scene-text aware video question answering. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 33633373, 2025."
        },
        {
            "title": "A Additional Details on EgoAVU",
            "content": "A.1 Prompts Prompt Used. We describe the various prompts used in EgoAVU. These prompts cover the generation of multiple modules including the Multi-modal Context Graph (see Fig. 11), Audio-Visual Narration (see Fig.13), and task-specific QA generation (see Fig. 14, Fig. 15, Fig. 16, Fig. 17, Fig. 18, Fig. 19, Fig. 20). The general structure of each prompt includes: (i) defining the objective, (ii) specifying the input format, (iii) describing the task, (4) providing general instructions, (iv) including human-generated examples to enable in-context learning, and (v) specifying the output format. A.2 MATTR For video filtering, we utilize the Moving-Average Type-Token Ratio (MATTR) to identify videos with rich multimodal diversity. We set the window size to = 200 tokens based on the average token length of combined narrations across video segments in our dataset, ensuring the window captures sufficient multi-modal context for meaningful diversity measurement. We retain videos whose MATTR exceeds τ = 0.3, effectively removing the bottom 25% of our distribution to filter out static or repetitive descriptions. This threshold was determined through manual inspection of 100 randomly sampled videos across different MATTR ranges. Videos below τ = 0.3 predominantly featured repetitive actions with limited object diversity and minimal auditory variation, while videos above this threshold exhibited richer multimodal dynamics (refer to Fig 9 for more examples). A.3 Ablation on Multi-Modal Context Graphs To validate the necessity of the Multi-Modal Context Graphs (MCG) component in EgoAVU, we conducted an ablation experiment on 200 randomly sampled video clips. We compared our MCG-based pipeline against direct baseline where LLaMA-3-70B generates audio-visual narrations directly from enhanced narrations (video caption, image caption, audio caption, and action narration) without the intermediate MCG structure. We manually evaluated both output, assessing: (1) completeness of sound-source associations, (2) accuracy of action sequences, and (3) overall audio-visual coherence. We observed that the direct method produced errors in 82 out of 200 captions (41.0%), with the breakdown as follows: 48 captions (19.0%) missed or incorrectly associated sound sources, 31 captions (15.5%) omitted crucial action sequences or interaction details, and 17 captions (3.5%) exhibited both issues (refer to Fig. 10 for example). In contrast, the our MCG-based approach reduced errors to 21 out of 200 captions (10.5%), representing 76.1% relative error reduction. Manual Effort for EgoAVU-Bench Construction To ensure the reliability of EgoAVU-Bench, we conducted extensive manual verification across all 3,000 question-answer pairs covering 900 egocentric videos. Each video was carefully reviewed by trained annotators, taking approximately 2-3 minutes per video to verify temporal alignment and audio-visual correspondence. Out of 3,000 QA pairs, 1,524 pairs (50.8%) were modified or corrected during this process. For open-ended tasks (SSA, AVDN, AVSN), corrections primarily addressed missing sounds, incorrect human-object interactions, and sound-source misalignments to ensure accurate audio-visual grounding. For close-ended tasks (TR, AVH), we verified answer correctness and enhanced distractor quality by ensuring multiple-choice options were sufficiently challenging and plausible while avoiding options that were too similar to correct answers or obviously incorrect. The complete manual verification process required approximately 225 hours of human annotation effort. 15 Figure 10 Qualitative comparison of audio-visual narrations generated with and without (w/o) Multi-modal Context Graph (MCG). Our MCG-based approach produces narrations with superior audio-visual coherence, accurately capturing action sequences and sound-source associations, while the direct method (w/o MCG) often misses critical sounds or action sequences."
        },
        {
            "title": "C Additional Details on Narration Enhancement",
            "content": "Prompts. To capture spatial details, we extract the center frame and prompt Qwen2.5-VL with Identify all the objects visible in the image to detail all objects present in the video clip. To capture temporal dynamics, we utilize Qwen2.5-Omni in two stages: first, we process only the video frames with the prompt Describe the video in detail to capture visible activities; then, we process the audio with the prompt Describe all the sounds heard in detail to capture auditory information."
        },
        {
            "title": "D Additional Experiment Details",
            "content": "Training Details. For both LoRA and full fine-tuning, we use maximum context length of 30,000 tokens and sample videos at 1 FPS with frame resolution of 256 256. Training is performed using DeepSpeed ZeRO-3 with learning rate of 1 104 and cosine schedule with 10% warmup over 5 epochs. We perform balanced sampling, i.e, sample with equal weights from each task, during training. Evaluation Details. Fig. 21 presents the prompt used for our LLM-as-judge evaluation. Following prior work (Plizzari et al., 2025), we assess the reliability of LLM-based scoring by measuring its alignment with human judgments on 300 randomly sampled open-ended QA pairs from EgoAVU-Bench. The resulting human-alignment rate is 87.6%, indicating strong alignment between the two. 16 Objective: You are an AI assistant tasked with performing high-fidelity analysis of video content. Your role is to function as an evidence extractor, not an open-world reasoner. You must strictly use the provided captions to identify object interactions and to analyze sounds, grounding every piece of information directly to the source text. Inputs: You will be provided with JSON object containing the following four keys: Video Caption: Describes the overall visual scene, including events and actions of entities other than the narrator (e.g., animals, other people, environmental events). Image Caption: Describes the diverse objects visible in the center frame. Audio Caption: transcript of sounds and audio events. Action Narration: Describes the specific actions performed by the primary person (#CC denotes person) Task: Analyze the provided captions to generate structured multimodal context graph in the form of JSON that captures multimodal relationship. For generating the multimodal context graph, follow the instruction mentioned below: Instructions: Identify Interacted Objects: Parse the action narration to find all objects the primary person is described as touching, holding, using, or manipulating. Compile these into the \"interacted objects\" list with what action the person performed. Identify Background Objects: Take the complete list of objects from image caption. Create new list containing only the items from image caption that are NOT in your \"interacted objects\" list. This will be your \"non interacted objects\" list. Identify Sound-Source Associations This is strict, evidence-based process. Ideally there can be two type of sound, sound caused by action/object or background sound. Your task is to capture both of them for audio caption A. Find the Grounding Evidence for foreground sound: For each sound in audio caption, you must search action narration and video caption to find the specific text that describes the action or event causing it. Look in action narration for causes related to the primary persons actions. Look in video caption for causes related to other entities (animals, other people) or general scene events. ambient sound include music, background noise, etc. Crucially: If no direct textual evidence can be found in either caption that explains the sounds origin, the sound is background sound. B. Exclude \"Unquestionable\" Sounds: Even if grounded, do NOT include sound if it falls into these categories: Mundane Biological Sounds: Common sounds like \"breathing,\" \"sighing,\" \"swallowing.\" Vague Ambient Noise: like \"white noise\" or \"faint hum.\" C. Determine the sound category: Classify as Foreground Sound (from the human action or visible object) or Background Sound. D. Handle Empty Results: If no sounds pass the filtering and grounding process, the \"sounds\" list in your output must be an empty list ([]). Human Generated Examples: Here are 5 human created examples for the correct execution <examples>. Important Note: In the example how \"giggle\" was excluded because it had no grounding evidence in video caption or action narration. Source description will contain either the corresponding action or the object that can produced the sound Sound category can have foreground sound such as Action Sound, Object Sound or background such as Ambient Sound Final Output Format: Your entire response MUST be single, valid JSON object following the structure of the example. Do not include any text outside of the JSON structure. Here is the input: <input> Figure 11 Prompt For Generating Multi-Modal Context Graphs. 17 { \"interacted_objects\": [ [\"sink\", \"#C rinses both hands\"], [\"tap\", \"#C turns on tap\"], [\"door\", \"#C opens the door\"], ], \"background_objects\": [ \"oranges\", \"sponge\", \"red chair\", \"microwave\", \"cabinets\" ], \"sounds\": [ \"acoustic_description\": \"water flowing sound\", \"source\": \"#C turns on tap\", \"evidence_source\": \"action_narration\", \"sound_category\": \"Foreground Sound\" \"acoustic_description\": \"hands being rinsed sound\", \"source\": \"#C rinses both hands\", \"evidence_source\": \"action_narration\", \"sound_category\": \"Foreground Sound\" \"acoustic_description\": \"door opening and closing sound\", \"source\": \"#C opens the door\", \"evidence_source\": \"action_narration\", \"sound_category\": \"Foreground Sound\" { }, { }, { } ] } Figure 12 Example of MCG. Example of MCG generated in JSON format using the above-mentioned prompt. 18 Objective: Your job is to generate single, detailed, and objective paragraph summarizing what can be seen and heard in the video clip. Input: You will be given: free-form natural language paragraph summarizing what happens in the video. This is typically derived from loose transcription or human description of the scene. list of short, possibly overlapping or action tags extracted from the video. These may contain minor inconsistencies, but offer clues about human interactions and movements in the scene. Focus closely on the interaction starting with #C C. multi-modal context graph represented as structured JSON object containing: \"interacted_objects\" objects the person interacted with, \"non_interacted_objects\" objects present but not interacted with, and \"sounds\" sound events grounded in the narration, with descriptions and causes. Task: Write single, coherent paragraph that summarizes the video scene in detail, following these rules: Instructions Clearly describe all key actions in the scene, combining the raw description and narration tags. Include all interacted objects, and describe how each was interacted with. Mention all non-interacted objects that are visible or relevant once, integrated naturally into the scene description. Do not repeat them again at the end. Integrate sound events by describing what caused them and when, grounded in the referenced actions. Not all actions have corresponding sounds. Only include sounds that are listed in the scene graph. Use semantically appropriate or naturalistic descriptions for acoustic events. For instance, if the sound is caused by shaking spray bottle, you may refer to it as crunching or rattling. Use an objective and factual toneavoid any emotional, subjective, or evaluative language (e.g., no cute, interesting, or simple). Write in past tense. Ensure the paragraph flows naturally and avoids redundancy. Human Generated Examples: Here are 5 human created examples for the correct execution <examples>. Final Output Format: The final output must be in JSON format with key as \"caption\" Here is the input to generate the caption: <input> Figure 13 Prompt For Generating Audio-Visual Narration. Objective: You are an AI assistant tasked with analyzing video segment and performing three tasks: Generate single open-ended question about the sound-source association observed in the video. Produce natural, human-like narration that links sounds to the actions and objects responsible. Generate detailed, structured answer to the question, grounded entirely in the provided scene graph metadata. Input: You will be provided with: video description: description of the video segment Multi-modal Context Graph: <Details on Multi-modal Context Graph> Instructions: Follow the below instruction to complete the task: Question Generation. If the \"sounds\" list is empty or missing, return this exact string as the only output: \"No significant sound is present in the video clip.\" Otherwise, use the template below: <template> When narrating egocentric data, person is sometimes referred to by capital letters, such as \"C.\" When writing the description, treat such IDs as referring to person. For example, if sound-producing evidence states \"person is clapping,\" it should be treated as \"the person is clapping.\" Detailed Answer Generation. Structure the answer as follows: Begin with sentence that clearly states how many distinct grounded sound events were present. Then provide one sentence for each sound, explaining what caused it by using the acoustic description and grounding evidence. Treat as the person in the video. Do not speculate or add interpretation beyond the metadata. Do not include any text outside of the JSON structure. Do not include any step by step explanations. Human Generated Examples: Here are 5 human created examples for the correct execution <examples>. Output Format: Output must be in JSON format with following key \"question\" and \"answer\". Here is the input to generate the question-answer pair: <input> Figure 14 Prompt for generating Sound-Source Association Question-Answer pair. Objective: Generate two sound-related questionanswer pairs from an egocentric video caption that describes persons visible actions, sounds, and objects. The output should be formatted as JSON with one correct and one hallucinated sound question. Input: You will be given an egocentric video narration containing descriptions of: The persons visible actions Distinctive sounds (e.g., hissing, tapping, scraping) Objects present in the scene Temporal information about when events occur Instructions: Follow the instruction below: Focus on distinctive sounds such as foreground sounds related to human-object interaction such as hissing, tapping etc. or background sounds such as bird chirping etc. Generate one correct question: Ask about sound explicitly mentioned in the narration. Generate one hallucinated question : Ask about plausible sound that is not mentioned in the narration. Answer format : Answers must be in binary format \"Yes\" or \"No\". Output Format: The output must be in JSON format with following keys: \"question\", \"question type\" including \"Factual\", \"Hallucinated\" and \"answers\". Here is the input to generate the question-answer pair: <input> Figure 15 Prompt for generating Audio-Visual Hallucination (Sound) Question-Answer pair. 20 Objective: Generate two action-related questionanswer pairs from an egocentric video caption that describes persons visible actions, sounds, and objects. The output should be formatted as JSON with one correct and one hallucinated action question. Input: You will be given an egocentric video narration containing descriptions of: The persons visible actions Distinctive sounds (e.g., hissing, tapping, scraping) Objects present in the scene Temporal information about when events occur Instructions: Follow the instruction below: Focus on distinctive, non-trivial actions such as wiping, twisting, or squeezing. Avoid trivial actions such as breathing, walking, or placing. Generate one correct question: Ask about an action explicitly mentioned in the narration. Generate one hallucinated question : Ask about plausible action that is not mentioned in the narration. Answer format : Answers must be in binary format \"Yes\" or \"No\". Output Format: The output must be in JSON format with following keys: \"question\", \"question type\" including \"Factual\", \"Hallucinated\" and \"answers\". Here is the input to generate the question-answer pair: <input> Figure 16 Prompt for generating Audio-Visual Hallucination (Action) Question-Answer pairs. Objective: Generate two object-related questionanswer pairs from an egocentric video caption that describes persons visible actions, sounds, and objects. The output should be formatted as JSON with one correct and one hallucinated object question. Input: You will be given an egocentric video narration containing descriptions of: The persons visible actions Distinctive sounds (e.g., hissing, tapping, scraping) Objects present in the scene Temporal information about when events occur Instructions: Follow the instruction below: Focus on specific, manipulable objects. Avoid generic nouns like things, stuff, or material. Generate one correct question: Ask about an object explicitly mentioned in the narration. Generate one hallucinated question : Ask about plausible object that is not mentioned in the narration. Answer format : Answers must be in binary format \"Yes\" or \"No\". Output Format: The output must be in JSON format with following keys: \"question\", \"question type\" including \"Factual\", \"Hallucinated\" and \"answers\". Here is the input to generate the question-answer pair: <input> Figure 17 Prompt for generating Audio-Visual Hallucination (Object) Question-Answer pairs. 21 Objective: Generate two temporal reasoning questionanswer pairs from list of chronological video narrations, focusing on the order of Action, Object, and Sound events. The output should be formatted as JSON list containing one \"before\" and one \"after\" question. Input: list of narration describing what happens in the video in chronological order ({caption_list}). The specific question type to be generated ({type}: one of Action-Action, Action-Object, or Action-Sound). Instructions: Follow the steps below: 1. Identify Distinct Events: Identify several unique, non-trivial, and non-repetitive events, each describing an Action, an Object, or Sound. 2. Select Event Pair (E1, E2): Choose two events occurring at different times that match the required category ({type}). E1 must chronologically precede E2. 3. Generate Questions: Create one \"before\" question (referencing E2) and one \"after\" question (referencing E1) using the corresponding template: ActionAction Before: \"What action was the person performing before <E2>?\" After: \"What action did the person perform after <E1>?\" ActionObject Before: \"What objects can be seen before the person performed the <E2> action?\" After: \"What objects can be seen after the person performed the <E1> action?\" ActionSound Before: \"What sound can be heard before the person <E2>?\" After: \"What sound can be heard after the person <E1>?\" 4. Answer and options: Write concise, naturalistic answer as if you watched the video. Include three plausible options that fit the context but are temporally incorrect. Output Format: The output must be JSON list of exactly two question objects (one \"before\" and one \"after\") with the following keys: \"question\", \"answer\", \"type\", and \"options\". Figure 18 Prompt for generating Temporal Reasoning (before/after) Question-Answer Pairs Objective: Generate one multiple-choice question about the temporal order of four events derived from sequence of chronological egocentric video narration. Input: sequence of detailed narration in chronological order describing what happens in egocentric video. Instructions: Follow the steps below: Identify Four Grounded Events: Identify four unique, non-trivial events. Each event must include concise but meaningful details about the persons activity, the visible surroundings, and any sounds mentioned. Grounding Constraint: All events must be directly derived from the narrations. Do not hallucinate or invent any objects, sounds, or actions. Create Temporal Question: Create one general multiple-choice question that asks about the temporal order of the four events (e.g., \"Which event happened first?\", \"Which moment occurred last?\"). Create Options: List the four events as options A, B, C, and D. Ensure the description for each option is the exact description provided in the events list. Provide Correct Answer: Indicate the correct temporal order by selecting one of the options (A, B, C, or D) as the correct_answer. Output Format: The output must be in JSON format with the following keys: \"events\" (a list of the four descriptions), \"question\", \"options\" (a map of A, B, C, to the event descriptions), and \"answer\". Figure 19 Prompt for generating Temporal Reasoning (Event Ordering) Question-Answer Pairs 22 Objective: Write single, coherent, dense narration summarizing the entire video based on list of 10-second captions. Input: list of narration, each describing 10-second segment of egocentric video, including start_time, end_time, and the caption text. Instructions: The final output must be single, fluent paragraph that acts as dense narration. The paragraph must adhere to the following rules: Integrate all actions, objects, and sounds across the full video. Use timestamps in seconds to indicate when key events occurred. Group similar or adjacent events into continuous spans. Avoid listing or repeating captions verbatim. Use only the information in the input captions. Be concise and fluent. Do not invent any new information or context. Human Generated Examples: Here are 3 human created dense narration: <examples> Output Format: single paragraph, not JSON object. Here is the input: <input> Figure 20 Prompt for generating Audio Visual Dense Narration Objective: Act as an impartial grader to evaluate PREDICTED_ANSWER against GROUNDING_ANSWER with respect to QUESTION. Input: QUESTION: The question posed to the model. GROUNDING_ANSWER: The authoritative reference answer. PREDICTED_ANSWER: The models answer to be graded. Instructions for Grading: 1. Comparison: Compare PREDICTED_ANSWER to GROUNDING_ANSWER with respect to the QUESTION. 2. Assign Rating (1-5, integer only): 5: Fully correct, complete, and faithful to the grounding; no meaningful errors or omissions. 4: Mostly correct; minor omissions or small inaccuracies that do not change the overall correctness. 3: Partially correct; captures some key points but misses important details or includes notable inaccuracies. 2: Largely incorrect; substantial errors, contradictions, or missing major required points. 1: Incorrect/irrelevant; contradicts the grounding or fails to answer the question. 3. Provide Reasoning: Briefly explain the rating (14 concise sentences). Judging Rules (Priorities): Prioritize factual alignment with the GROUNDING_ANSWER. Contradictions result in heavy penalization. Extra details are acceptable only if they do not conflict with the grounding and remain relevant to the QUESTION. Penalize hallucinations, unverifiable claims, safety issues, and failure to address the core of the QUESTION. Do not reward verbosity or style unless it improves factual accuracy or completeness with respect to the grounding. If the grounding indicates the question is unanswerable, judge whether the prediction correctly reflects that. Output Format: The output MUST be valid JSON (no markdown, no extra text) with the following keys: { \"rating\": <int value between 1 to 5>, \"reason\": \"<string of 1-2 lines explaining the rating>\" } Figure 21 Prompt for LLM-as-judge evaluation"
        }
    ],
    "affiliations": [
        "Meta",
        "University of Maryland, College Park"
    ]
}