{
    "paper_title": "Efficient Data Selection at Scale via Influence Distillation",
    "authors": [
        "Mahdi Nikdan",
        "Vincent Cohen-Addad",
        "Dan Alistarh",
        "Vahab Mirrokni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each sample's influence on a target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose a $\\textit{landmark-based approximation}$: influence is precisely computed for a small subset of \"landmark\" samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to $3.5\\times$ faster selection."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 1 5 0 9 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Efficient Data Selection at Scale\nvia Influence Distillation",
            "content": "Mahdi Nikdan ISTA & Google Research Vincent Cohen-Addad Google Research Dan Alistarh ISTA & Red Hat AI Vahab Mirrokni Google Research"
        },
        {
            "title": "Abstract",
            "content": "Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each samples influence on target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose landmark-based approximation: influence is precisely computed for small subset of landmark samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to 3.5 faster selection."
        },
        {
            "title": "Introduction",
            "content": "The rise of Large Language Models (LLMs) has driven significant advances in natural language processing; yet, training and fine-tuning these models requires massive computational resources and carefully-curated datasets. One key direction towards improved training efficiency has been via data selection and data weighting methods [Xia et al., 2024, Yin and Rush, 2024, Antonello et al., 2020, Marion et al., 2023, Ankner et al., 2024, Li et al., 2023a, Ivison et al., 2025, Axiotis et al., 2024, Xie et al., 2023a, Engstrom et al., Huang et al., 2024], which aim to curate training subsets that maximize models effectiveness, often with respect to particular target data distribution or downstream task. However, existing approaches typically rely on heuristicssuch as perplexitybased filteringor require expensive proxy model training or expensive embedding functions to generate data representations. More precisely, existing methods face several limitations. First, many existing methods utilize fixed, model-agnostic features or representations (e.g., static embeddings) that may not capture the full relationship between training samples and the target distribution [Yin and Rush, 2024, Antonello et al., 2020, Marion et al., 2023, Ankner et al., 2024]. Second, methods that update weights during training lack theoretical justification and can be unstable [Xie et al., 2023a, Huang et al., 2024]. Lastly, approaches that rely on reference model training or costly embeddings are computationally Work done while an intern at Google Research. Correspondence to mnikdan@ista.ac.at and cohenaddad@google.com. Preprint. Under review. intensive and often challenging to scale [Li et al., 2023a, Xia et al., 2024, Ivison et al., 2025]. Thus, there remains clear need for mathematically-grounded, efficient, and scalable framework for data selection that directly optimizes for performance on specific target distribution. Contribution. We introduce Influence Distillation, novel framework for data selection that addresses these challenges. Given pre-trained model and target task (represented by small target dataset), Influence Distillation formulates the influence of training samples on the target distributions loss via second-order approximation. Influence Distillation directly optimizes sample weights by analyzing how each training sample, if included in gradient step, is expected to affect model performance on the target data. This formulation leads to quadratic optimization objective for the sample weights, which we demonstrate can be solved efficiently. We provide derivations for these optimal weights under both standard Gradient Descent and the adaptive Adam optimizer, backed by theoretical justifications. Figure 1: Average improvement over uniform sampling across six tasks vs. runtime. The model used is Llama2-7B [Touvron et al., 2023], and the training dataset is Tulu V2 [Ivison et al., 2023]. The annotation M/N indicates that the method selected samples from pool of size N. Further details are provided in Section 5. To ensure scalability to large datasets, we further introduce an efficient landmark-based approximation. This approach first involves selecting small subset of landmark samples and precisely computing their influence. The influence for all other samples is then efficiently approximated by transferring the computed influence from these landmarks. This transfer mechanism is guided by novel and computationally inexpensive embedding space derived from Jacobian-vector Products. This significantly reduces the computational overhead of gradient computations for the entire dataset. We validate Influence Distillation via comprehensive instruction tuning experiments using standard open LLMs (from the Llama [Touvron et al., 2023, Grattafiori et al., 2024] and Qwen [Team, 2024] families) on the Tulu V2 [Ivison et al., 2023] training dataset, while targeting advanced downstream tasks like MMLU [Hendrycks et al., 2021a,b], mathematics and code. Our results demonstrate that Influence Distillation not only substantially outperforms uniform random selection but also in most cases matches or exceeds the performance of state-of-the-art data selection methods, while offering significant computational speedups on the same selection problemup to 3.5 in embedding + selection runtime. This positions Influence Distillation as strong method on the Pareto frontier of overall embedding, selection and training cost versus downstream task accuracy (see Figure 1)."
        },
        {
            "title": "2 Related Work",
            "content": "Data selection (pruning) and weighting methods have become increasingly important in the context of efficient LLM training. In celebrated paper, Sorscher et al. [2022] et al. showed that (modelagnostic) data pruning, and in particular deduplication, helps go beyond scaling laws for LLMs. This was later further improved by Abbas et al. [2023]. Early work on model-dependent data pruning focused on heuristics like perplexity-based filtering and confidence-based selection: Marion et al. [2023] found that selecting examples with moderate perplexity scores often outperforms training on the full dataset or examples selected by other metrics. Do and Gaspers [2019] introduced DSIR, which uses importance resampling based on n-gram features to select relevant training examples, with promising results on mathematical reasoning and clinical text summarization. Similarly, Xie et al. [2023b] proposed clustering loss trajectories to identify representative training examples, though their approach focused more on general domain adaptation rather than specific target distributions. Another approach, so-called Classifier, was introduced by Brown et al. [2020] and has been employed in subsequent work (Gao et al. [2020], Chowdhery et al. [2023], Du et al. [2022]. Other strategies include selecting examples that maximize the loss difference between LMs trained on candidate and reference datasets (Moore and Lewis [2010], Axelrod [2017], Feng et al. [2022]). Simpler, yet common, techniques involve filtering documents 2 based on length or the presence of excessive special characters (Raffel et al. [2020], Xie et al. [2023b]). related, though distinct, task in the LM domain is optimizing the weights for sampling from mixed data sources (Chen et al. [2024], Albalak et al. [2023]). Recently, Ivison et al. [2025] proposed RDS+, which uses similarity between model-dependent embeddings computed by position-weighted mean pool of the last hidden layer states. Recent work has also highlighted the importance of considering the training dynamics when selecting data. Zhou et al. [2023a] proposed measuring learnability based on loss changes during training, while Swayamdipta et al. [2020] introduced dataset cartography to analyze training dynamics across examples. These methods provide useful signals about which examples are most valuable for training; at the same time, they require training reference models which can be computationally expensive. For large-scale applications, Bhatt et al. [2024] evaluated various data selection approaches for LLM fine-tuning, and found that facility-location selection based on hidden representations was particularly effective. However, Tirumala et al. [2023] observed that generating these representations for large datasets remains computationally challenging. More recently, Engstrom et al. [2024] framed the data selection problem as an optimization problem: Given the learning algorithm, find the subset of the data that maximizes the performance of the trained model. To obtain an efficient solution, they design model that given subset of the training data and target example t, predicts the loss of the model trained on on t. Axiotis et al. [2024] recently use coreset-related ideas to propose computationally efficient way of sampling an unbiased estimator of the model loss from the training data so as to train on smaller input. While previous methods like DSIR and facility location selection rely on fixed features or representations, our method directly optimizes sample weights based on their influence on the target distribution through second-order approximation. Importantly, this does not require training proxy model to predict the value of the elements and is computed directly from the input, model and learning algorithm. Unlike curriculum learning or confidence-based approaches that update weights during training, we derive optimal weights analytically for both SGD and Adam optimizers. In contrast to methods that require training reference models, our landmark-based approximation allows efficient weight computation without extensive pre-training. There is large body of work on data selection methods for other learning tasks and mode, and it is beyond the scope of this paper to provide detailed overview. We refer the reader to Kaushal et al. [2019], Killamsetty et al. [2021], Wei et al. [2015], Chen et al. [2023], Cao et al. [2023], Sener and Savarese [2017] and references therein."
        },
        {
            "title": "3 Method",
            "content": "3.1 Problem and Notation Let θ Rd be the model parameters. For any dataset of size and any vector of sample weights (cid:80)n = [w1, w2, ..., wn]T , denote L(θ; D, w) = 1 i=1 wi ℓ(θ; Di) as the weighted average of the model loss ℓ on the samples of dataset at point θ. Additionally, define M(θ; D, w) as training mechanism that returns the parameters after being trained on dataset weighted by w. Unless otherwise stated, we will assume is simply one step of (full) gradient descent. Let and represent the training (source) and downstream (target) distributions, respectively. Assume we have access to dataset sampled from and small representative dataset from . Our high-level goal will be to determine sample weights such that: = arg min L(M(θ; S, w); T, 1) (1) where 1 RT represents the all-ones vector. In words, we wish to find sample weights for instances within the source dataset S, such that training on using these weights results in minimal loss on the target dataset . Notably, this notation also allows for the special case of = , where our method would find weights that maximize in-distribution loss improvement. 3.2 Running Example Throughout this section, we utilize toy training setting to illustrate variants of our method. Specifically, we consider linear regression model parameterized by θ with the loss function 3 Figure 2: (Left) Distribution of unconstrained weights, (Middle) Distribution of robust weights for λ = 0.02, and (Right) validation loss during training with different variants in the running experiment setting. Robust weights are found by minimizing Objective 7 using the SLSQP algorithm [Kraft, 1988] implemented in the SciPy library [Virtanen et al., 2020]. ℓ(θ; x, y) = (θT y)2 for any θ, Rd, {0, 1}. For the source dataset, we sample 256 random instances from the first two classes of the CIFAR-10 dataset [Krizhevsky, 2009] and combine them with 256 synthetic samples generated from Gaussian distribution with the same mean and standard deviation as the real samples. The target dataset consists of another set of 256 samples from CIFAR-10. We use gradient descent with learning rate of 103 as the optimizer. Finally, the loss values are reported on validation dataset of size 256, also sampled from CIFAR-10. 3.3 Influence Distillation Case 1: Unconstrained Weights. Let gT (θ) = θL(θ; T, 1) and HT (θ) = 2 θL(θ; T, 1) denote the gradient vector and Hessian matrix of the loss with respect to the model parameters on the target dataset. Construct GS(θ) RSd by stacking the gradients of the loss with respect to θ across samples of S. As mentioned before, assume is one step of gradient descent, i.e., M(θ; D, w) = θ ηθL(θ; D, w) = θ η GT (θ)w, where η denotes the learning rate. We estimate Objective 1 by: = arg min L(M(θ; S, w); T, 1) = arg min L(θ η GT (θ)w; T, 1) arg min [L(θ; T, 1) (θ)w + η (θ)GT gT η 2 η2 2 S2 wT GS(θ)HT (θ)GT (θ)w] (θ)w] (2) [gT (θ)GT (θ)w + wT GS(θ)HT (θ)GT = arg min where the approximation comes from second-order Taylor expansion, i.e., L(θ + δ; T, 1) L(θ; T, 1) + gT GT Next, we define two key objects, RS and RSS, as follows: 2 δT HT (θ)δ where δ is replaced with η (θ)δ + 1 (θ)w. p(θ; S, ) = GS(θ)gT (θ), Q(θ; S, ) = 1 GS(θ)HT (θ)GT (θ), For brevity, unless stated otherwise, we will omit and from the arguments of and Q. Let (w; θ) = p(θ)T + η 2 wT Q(θ)w. Then, the objective in Equation 2 becomes = arg min (w; θ). (3) (4) (5) (6) In words, represents scaled approximation of the change in loss on when the model at point θ is trained on with weights w. It is quadratic function in w, as and do not depend on w. This objective can be minimized in closed form as = 1 η Q(θ)1p(θ). 4 Discussion. While simple, the proposed solution has several crucial limitations: (a) it may produce negative or highly irregular sample weights, such as excessively large values, which lack intuitive interpretation, (b) the weights may overfit to the current set of parameters θ, and (c) the weights may also overfit to the target dataset. The first two issues can be easily observed in our running experiment. The irregularity of the weights is illustrated in Figure 2 (left). Furthermore, Figure 2 (right) demonstrates that unconstrained weights become invalidated after just one step of training, suggesting that the weights overfit to the current model parameters θ. We note that, since the model in our running example is linear, the second-order approximation is exactthus, the first update step reaches the optimum on . However, this behavior does not generalize to non-linear models. Case 2: Robust Weights. We modify Objective 6 to address the above limitations. First, we restrict the weights to non-negative values, i.e., 1 S: wi 0. Second, we require the weights to sum to the size of the source dataset, wT 1 = S. This prevents weights from becoming excessively large and ensures that rescaling the weights does not change the effective step size: using αw with learning rate η is equivalent to using with learning rate αη. To mitigate overfitting, standard approach is to add regularization term. Indeed, Appendix derives such term for linear models. In the general case, we employ simple L2 regularization term. The Robust Influence Objective. Hence, we define the robust Influence Distillation objective: = arg min (w; θ) + w2 2, s.t. Refer to Section 4.4 for discussion on how we tune λ in practice. λ 2 (cid:26) 0 wT 1 = (7) We compute the robust weights with λ {0.01, 0.02, 0.03} in the context of our running example. Figure 2 (right) highlights the effectiveness of these robust weights, showing that all three configurations outperform the default weights while remaining stable throughout training. Additionally, Figure 2 (middle) depicts the distribution of weights for λ = 0.02. Adam Optimizer. The Adam optimizer [Kingma, 2014] is the default choice for fine-tuning LLMs. Therefore, we tailor our method for Adam optimizers. To this end, we employ greedy approach, where we assume the firstand second-order momentums (m and v, respectively) are fixed after warm-up. In this case, the QAdam and pAdam objects are calculated as follows: pAdam(θ) = GAdam (θ)(gT (θ) η HT (θ)b), (8) QAdam(θ) = GAdam (θ)HT (θ)GAdam 1 (θ) is constructed by element-wise multiplying every row (θ)T , (9) . Additionally, is the number of warm-up steps, and (β1, β2, ϵ) (1βs where = , and GAdam β1m 1 )((cid:113) 1βs 2 1β1 1 )((cid:113) 1βs 2 are Adam hyperparameters. See Appendix for more details. of GS(θ) by = (1βs +ϵ) +ϵ) Handling Variable Lebel Lengths. common practice in data selection is to normalize the gradients prior to measuring similarities [Xia et al., 2024]. This is motivated by the observation that the norm of samples gradient tends to correlate negatively with the number of label tokens, thereby biasing unnormalized gradient-based methods toward shorter samples. Normalizing the gradients mitigates this issue and shifts the similarity measure from dot product to cosine similarity. In our approach, we adopt this normalization as well. See Appendix for study on this correlation. Per-target Importance. The formulation above assigns weights to training samples based on their average influence over the target set. However, recent work [Xia et al., 2024, Ivison et al., 2025] has shown that selecting training samples based on per-target influence can yield better performance; that is, iterating over individual target samples repeatedly and selecting one top-scoring training sample each time. We adopt this approach, noting that influence scores for each target can be computed by running Influence Distillation timesonce per target sample."
        },
        {
            "title": "4 Efficient Influence Distillation",
            "content": "In this section, we tackle several challenges regarding the implementation of Influence Distillation. 5 Cost of Hessian. While Q(θ) can be calculated exactly using Hessian-Vector Products (HVPs), these HVPs require storing the backward graph, which can incur extra memory costs in practice. Cost of GS. Constructing the matrix GS requires computing the gradient of the model with respect to each individual sample in the training set. This process is computationally expensive, as it incurs cost similar to one full epoch of training on the entire dataset S. Furthermore, storing the matrix GS requires memory proportional to times the size of the model, which is intractable in practice. Regularization Coefficient The solution to Equation 7 is sensitive to the choice of regularization strength λ. key challenge, therefore, is determining how to select λ in practical and effective way. 4.1 First-order Influence Distillation Recall the definition of (θ; w) from Equation 5, where the second-order term is scaled by the learning rate η. In Appendix H, we observe that in our settings, η is small enough that the secondorder term becomes negligible. As result, computing can be avoided with little to no loss in performance. This first-order approximation aligns with prior work on gradient-based influence estimation, such as the methods proposed by Xia et al. [2024]. 4.2 Gradient Projection To reduce the cost of storing the gradients, we take similar approach to Xia et al. [2024] and project each gradient vector upon calculation into k-dimensional space, where d. As opposed to the mentioned work, which uses random projections sampled from the Rademacher distribution (1 with equal probability), we find that projection using Randomized Hadamard Transform is faster in practice. For more details on the projections, see Appendix I. 4.3 Landmark-Based Gradient Approximation To circumvent the need to compute gradients for every training sample, we introduce landmarkbased approach. At high level, this method provides an efficient low-rank approximation of the gradient matrix GS, given by ˆGS = CGL, where GL Rℓd contains the gradients of ℓ selected landmark samples. The matrix RSℓ holds the coefficients that express each samples gradient as linear combination of the landmark gradients. Specifically, let denote set of ℓ landmark samples (e.g., selected at random), and suppose we have access to low-cost per-sample embeddings, represented by ES RSe. As before, we assume that all embedding and gradient vectors are normalized. To compute the coefficient matrix C, we minimize the objective minCRnℓES CEL2 2, where EL Rℓe contains the embeddings of the landmark samples. In words, this procedure approximates each samples embedding as linear combination of landmark embeddings. We then estimate the i-th row of the gradient matrix, gi, by ˆgi = GT Lci, where ci is the i-th row of C. This approximation implicitly assumes that the linear relationships learned in the embedding space transfer to the gradient space. Theoretical Justification. Although this approximation is not expected to recover the true gradients with high accuracy, the key intuition is that, as long as it is unbiased, even weak recovery can yield similar per-sample weights in high-dimensional spaces. Theorem 4.1 demonstrates this property for the specific case of the first-order variant of Influence Distillation. Theorem 4.1. (Informal version of Theorem D.3 and Corollary D.4 tailored to landmark-based approximation see Appendix D) Consider the special case of first-order Influence Distillation. Let gi and ˆgi denote the true and the landmark-based approximated gradients for sample i, and assume Influence Distillation with GS and ˆGS results in sample weights of and ˆw. Further assume: Unbiased: {1, 2, . . . , n} : E[ˆgi] = gi, i.e., the approximation is unbiased. Bounded Low-rank MSE: Let δ2 = E[ˆgi gi2], and for some 2 0: 1 (cid:80)n i=1 δ2 2. Then E[w ˆw)2] S2 λ2d , with λ being the regularization coefficient in Influence Distillation. 6 This theorem relates the accuracy of the weights to the low-rank approximation error 2, given the training set size S, dimension d, and regularization parameter λ. If the approximations are unbiased, in high dimension d, it suffices to reasonably control 2 in order to recover the correct weights. Integration with Influence Distillation. Given low-rank approximation of the gradient matrix GS ˆGS = CGL, one can define approximations to objects and as below: ˆp(θ; S, T, L) = CGL(θ)gT (θ) = p(θ; L, ), ˆQ(θ; S, T, L) = 1 CGLHT (θ)GT LCT (θ) = S Q(θ; L, ) CT , (10) (11) where = minCRnℓES CEL2 2 is the coefficient matrix, defined above. As shown in Equations 10 and 11, the landmark-based Influence Distillation computes and only for the landmark points, and then propagates them to the remaining samples. JVP Embeddings. We observe that existing embedding methods perform poorly in this setting, exhibiting weak correlation with the true gradients (see Appendix for detailed empirical analysis). To address this issue, we introduce Jacobian-vector Product (JVP) Embeddings. Given sample S, we define its JVP embedding as: hJV (x; , ℓ, ) = 1 (cid:88) vV Nℓ(x) θℓ (12) where is the model being trained, Nℓ() represents the logits of the next predicted token after processing through the first ℓ layers (or transformer blocks, in case of LLMs), and θℓ are the parameters of these ℓ layers. The set contains random Gaussian vectors matching the shape of θℓ, and the term Nℓ(x) is the Jacobian of Nℓ(x) with respect to θℓ. In words, JVP embeddings project the Jacobian of an intermediate model output onto set of random directions in parameter space. θℓ 4.4 Tuning the Regularization Coefficient. Finally, we describe our approach for selecting the regularization coefficient λ in Equation 7. As detailed in Appendix G, when η is small enough for the second-order term to be negligible and λ = 0, the solution assigns all the weight to single sample. As λ increases, the solution becomes progressively less sparse, distributing weight across more samples. In the limit λ , the solution becomes fully dense, assigning equal weight to all samples. In practice, given budget of samples to select for training, we tune λ via binary search to achieve target sparsity level of Sk , thereby ensuring that exactly samples receive non-zero weight, which we will pick for training."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate Influence Distillation across several challenging tasks. We start by detailing the datasets, models, and hyperparameters used in our experiments. Then we present our main results and ablations. Further studies are included in Appendix. 5.1 Setting We largely follow the experimental setup of Ivison et al. [2025] and reuse their code. Training Dataset. We use Tulu V2 [Ivison et al., 2023], combination of 9 instruction-tuning datasets containing approximately 5.8 million samples. Detailed descriptions of each component dataset are provided in Appendix E. Unless stated otherwise, we randomly sample 200k examples from Tulu V2, and then use sampling methods to pick subset of 10k samples from this pool. Target Datasets. We evaluate on six target datasets: MMLU [Hendrycks et al., 2021a,b], GSM8k [Cobbe et al., 2021], BBH [Suzgun et al., 2022], TyDIQA Clark et al. [2020], Codex [Chen et al., 2021], and SQuAD [Rajpurkar et al., 2016]. For each, we assume access to 8500 examples from their train, dev, or eval splits 3. Details are in Appendix E. 3Following Ivison et al. [2025], except we omit AlpacaEval [Li et al., 2023b] as it requires paid API access. 7 Table 1: Accuracy ( standard deviation) and estimated embedding and selection cost (Embd+Sel, in TF, TeraFLOPs) of various methods across tasks and models. For each modeldataset pair, 10k training samples are selected from pool of size 200k from the Tulu V2 dataset [Ivison et al., 2023]. We additionally report average improvement over the Uniform baseline (Avg. w/ Uniform). Top performing selection methods, as well as Full training numbers are in bold. Model Method MMLU GSM8k BBH TyDIQA CODEX SQuAD Avg. w/ Uniform Embd+Sel Cost Llama2-7B Llama3.2-3B Qwen2.5-1.5B Qwen2.5-3B Uniform 45.6 0.43 Mid-PPL 45.6 0.86 46.3 0.33 RDS+ 48.3 0.21 InfDist 48.8 0.08 Full Uniform 53.9 0.52 Mid-PPL 54.0 0.27 53.1 0.58 RDS+ 54.0 0.94 InfDist 52.9 0.87 Full Uniform 58.8 0.11 Mid-PPL 58.9 0.17 58.3 0.07 RDS+ 59.4 0.12 InfDist 59.4 0.13 Full Uniform 63.7 0.27 Mid-PPL 63.7 0.18 63.6 0.19 RDS+ 64.6 0.19 InfDist 63.8 0.06 Full 17.5 1.08 15.0 0.54 20.2 2.77 20.3 1.65 21.2 0.85 34.6 1.22 29.5 0.12 38.4 0.58 35.7 1.28 37.0 0.33 63.3 1.67 63.2 0.65 60.1 0.13 62.0 0.46 60.3 0.38 68.7 1.87 70.8 0.84 67.4 0.68 67.8 0.60 71.0 1.78 41.8 0.20 40.9 0.23 42.7 0.61 43.2 0.67 43.9 0. 48.9 0.67 48.3 0.44 49.6 0.45 48.6 0.27 48.9 0.14 44.1 0.38 44.3 0.31 44.2 0.24 44.1 0.35 44.0 0.22 54.9 0.24 55.1 0.21 54.0 0.63 53.9 0.36 53.8 0.32 51.6 0.38 52.1 0.44 50.5 0.84 53.6 0.34 51.3 0.18 63.1 0.36 65.9 0.66 61.0 0.35 64.6 1.29 62.5 1.50 55.0 0.32 54.3 0.29 53.0 0.38 57.6 0.32 50.3 1. 65.6 0.16 65.3 0.41 65.1 0.33 66.9 0.23 64.5 0.42 27.0 0.60 26.1 1.56 30.4 0.96 29.5 3.14 29.3 3.72 56.1 1.35 55.9 4.40 60.6 1.77 55.4 1.10 57.7 2.81 70.5 2.06 70.6 1.61 72.3 0.00 69.8 1.15 73.0 1.79 83.1 1.35 83.3 3.14 82.4 1.46 82.4 0.55 82.0 1.41 80.8 1.05 80.7 0.73 85.3 0.22 83.2 1.02 83.6 0. 80.4 0.51 80.9 0.18 84.2 0.47 83.3 1.97 83.9 1.15 16.5 4.65 22.3 3.39 46.0 3.12 54.4 13.13 63.2 6.05 84.5 0.54 79.9 2.61 86.3 0.33 86.0 0.22 85.4 0.42 0.00 -0.65 +1.85 +2.30 +2.30 0.00 -0.42 +1.65 +0.77 +0.98 0.00 +0.90 +4.28 +6.52 +7. 0.00 -0.40 -0.28 +0.18 0.00 0 2800 TF 2800 TF 872 TF 0 1200 TF 1200 TF 417 TF 0 600 TF 600 TF 208 TF 0 1200 TF 1200 TF 340 TF Model. We mainly consider fine-tuneing the LlaMA-2 7B model [Touvron et al., 2023], consistent with the Tulu V2 paper [Ivison et al., 2023] and the experiments of Ivison et al. [2025]. We also experiment with Llama-3.2 3B [Grattafiori et al., 2024] and Qwen 2.5 1.5/3B [Team, 2024]. Baselines. We consider four baselines: (1) Random Uniform selection, which picks samples uniformly at random, (2) The state-of-the-art RDS+ [Ivison et al., 2025] embedding-based method, where the embeddings are computed by position-weighted mean pool of the last hidden layer states, (3) Mid-PPL [Yin and Rush, 2024], where samples are sorted by their perplexity, and the middle ones are selected, and (4) Full, where we do not perform any sampling and train on the full dataset. Additionally, in Appendix F, we evaluate Influence Distillation using the true gradients as embeddings, which we show corresponds to the expensive LESS method [Xia et al., 2024]. Hyperparameters. We use the AdamW optimizer with learning rate of 2 105 and linear schedule for 2 epochs. The sequence length is fixed at 2048, and we use microbatch size of 1 with gradient accumulation over 128 steps. All experiments are conducted on single H100 GPU, and each are repeated with 3 random seeds, including the selection of 200k samples from Tulu V2. By default, we use first-order Influence Distillation with 4096 landmarks. We select the landmarks uniformly at random, as we find this performs comparably to more complex methods such as leverage score sampling or clustering. Linear coefficients are computed via Kernel Ridge Regression (KRR) with an RBF kernel and dampening of 0.01. JVP embeddings are obtained from the first four transformer blocks using two random vectors (ℓ = 4, = 2), following brief warm-up on 10k random samples. The model is then reset and trained on the selected subset. This warm-up is needed to stabilize gradients (see Appendix A). Gradients are projected to 131072 dimensions via Hadamard projections; we use the largest dimension that fits in GPU memory, as projection cost does not depend on the dimension (Appendix I). After selection, we do not incorporate the sample weights during training, as experiments in Appendix suggest this does not improve performance. 5.2 Results Main Experiments. Table 1 summarizes our main experimental results. In each case, subset of size 10k is selected from pool of 200k Tulu V2 [Ivison et al., 2023] samples. On average, Influence Distillation achieves higher performance compared to other more expensive selection baselines in three out of four models and remains competitive in the fourth, while enabling 2.93.5 faster selection. Notably, for two models, it matches or surpasses training on the full dataset. These results clearly showcase the effectiveness and efficiency of Influence Distillation. Selection Runtime Estimation. The table also reports the approximate FLOPs required for sample selection. Following the estimation from Kaplan et al. [2020], each forward pass is assumed to cost Figure 3: (Left) Effect of the number of landmarks on the performance of Influence Distillation across six tasks using Llama2-7B. (Right) MMLU accuracy of Influence Distillation on Llama2-7B across different pool sizes and number of selected samples. 2d FLOPs and each backward pass 4d FLOPs, where is the number of model parameters. Mid-PPL and RDS+ require one forward pass per sample. Influence Distillation requires computing JVP embedding for each sample, along with full forward and backward passes for the 4096 selected landmarks. We estimate the cost of JVP as 2 that of partial forward pass over the same number of blocks, following Cobb et al. [2024]. Pareto Superiority. We repeat the above experiments on the Llama2-7B model using pool sizes 50k, 100k, 150k, and 200k. For each pool size, we select 2048, 4096, 6144, and 8192 landmarks, respectively, maintaining fixed pool-to-landmark ratio. As shown in Figure 1, all points corresponding to Influence Distillation lie on the Pareto frontier, matching or surpassing the performance of RDS+ while requiring lower overall cost of embedding, sampling, and training. Effect of Number of Landmarks. To evaluate how the number of landmarks impacts performance, we repeat the experiments on Llama2-7B and report average improvement over the Uniform baseline across the six tasks in Figure 3 (Left). As shown, Influence Distillation improves with more landmarks, surpassing RDS+ beyond 2048 landmarks. Effect of Pool Size and Number of Selected Samples. Figure 3 (Right) presents heatmap of MMLU accuracy on Llama2-7B across different combinations of pool size (up to 200k) and number of selected samples. For each pool size, we use the same number of landmarks as the Pareto experiment. As expected, accuracy improves with larger pools and more selected samples, highlighting the scalability and robustness of Influence Distillation."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "Below, we outline three main limitations of our work, along with corresponding directions for future research. No Target Distribution. While we demonstrate that Influence Distillation is highly effective for targeted instruction tuning across range of models and tasks, it does not directly extend to general data selection scenarios where no target dataset is available. In such cases, one could define the target distribution as small set of high-quality examples or representative subset of the training corpus. Investigating how to construct and utilize such proxy targets is an important direction for future work. Pre-training. Extending Influence Distillation to the pre-training setting presents unique challenges. In particular, the considerably longer duration of pre-training implies that gradients may shift substantially over time, likely making single static selection insufficient. This suggests the need for multi-phase selection strategy, such as periodic re-sampling. We leave the exploration of such dynamic approaches to future work. Warm-up Cost. We exclude the cost of the warm-up phase from our runtime measurements for two reasons: (1) as the training pool grows, the cost of brief warm-up on small random subset becomes negligible compared to embedding the full dataset; and (2) the warm-up can be shortened 9 (as shown in Appendix A) or compressedfor example, via Low-Rank Adaptation [Hu et al., 2022], as in Xia et al. [2024]. We leave rigorous investigation of warm-up optimization to future work."
        },
        {
            "title": "References",
            "content": "Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024. Junjie Oscar Yin and Alexander Rush. Compute-constrained data selection. arXiv preprint arXiv:2410.16208, 2024. Richard Antonello, Nicole Beckage, Javier Turek, and Alexander Huth. Selecting informative contexts improves language model finetuning. arXiv preprint arXiv:2005.00175, 2020. Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. When less is more: Investigating data pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564, 2023. Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew Leavitt, and Mansheej Paul. Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprint arXiv:2405.20541, 2024. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. arXiv preprint arXiv:2308.12032, 2023a. Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, and Pradeep Dasigi. Large-scale data selection for instruction tuning. arXiv preprint arXiv:2503.01807, 2025. Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, David P. Woodruff, and Michael Wunder. Data-efficient learning via clustering-based sensitivity sampling: Foundation models and beyond. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=WUQ4YzIQt2. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36: 3420134227, 2023a. Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels, 2024. URL https://arxiv. org/abs/2401.12926. Wei Huang, Yunxiao Zhang, Shangmin Guo, Yuming Shang, and Xiangling Fu. Dynimpt: dynamic data selection method for improving model training efficiency. IEEE Transactions on Knowledge and Data Engineering, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah Smith, Iz Beltagy, et al. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021a. 10 Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR), 2021b. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:1952319536, 2022. Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari Morcos. Semdedup: Dataefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023. Quynh Do and Judith Gaspers. Cross-lingual transfer learning with data selection for large-scale spoken language understanding. EMNLP, 2019. Sang Michael Xie et al. Smalltolarge (s2l): Scalable data selection for fine-tuning large language models by summarizing training loss trajectories of small models. arXiv preprint, 2023b. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Voss, and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 55475569. PMLR, 2022. Robert Moore and William Lewis. Intelligent selection of language model training data. In Proceedings of the ACL 2010 conference short papers, pages 220224, 2010. Amittai Axelrod. Cynical selection of language model training data. arXiv preprint arXiv:1709.02279, 2017. Yukun Feng, Patrick Xia, Benjamin Van Durme, and João Sedoc. Automatic document selection for efficient encoder pretraining. arXiv preprint arXiv:2210.10951, 2022. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher Ré. Skill-it! data-driven skills framework for understanding and training language models. Advances in Neural Information Processing Systems, 36, 2024. Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. In R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models, 2023. Haotian Zhou et al. Lobass: Gauging learnability in supervised fine-tuning data. arXiv preprint arXiv:2310.13008, 2023a. Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. EMNLP, 2020. 11 Gantavya Bhatt et al. An experimental design framework for label-efficient supervised finetuning of large language models. arXiv preprint arXiv:2401.06692, 2024. Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. improving LLM pretraining via document de-duplication and diversification. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ a8f8cbd7f7a5fb2c837e578c75e5b615-Abstract-Datasets_and_Benchmarks.html. D4: Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= GC8HkKeH8s. Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan Mahadev, Khoshrav Doctor, and Ganesh Ramakrishnan. Learning from less data: unified data subset selection and active learning framework for computer vision. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 12891299. IEEE, 2019. Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Coreset selection for efficient and robust semi-supervised learning. Advances in neural information processing systems, 34:1448814501, 2021. Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In International conference on machine learning, pages 19541963. PMLR, 2015. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023. Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. Instruction mining: Instruction data selection for tuning large language models. arXiv preprint arXiv:2307.06290, 2023. Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: core-set approach. arXiv preprint arXiv:1708.00489, 2017. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. Dieter Kraft. software package for sequential quadratic programming. ForschungsberichtDeutsche Forschungsund Versuchsanstalt fur Luftund Raumfahrt, 1988. Virtanen, Gommers, TE Oliphant, Haberland, Reddy, Cournapeau, Burovski, Peterson, Weckesser, Bright, et al. Fundamental algorithms for scientific computing in python and scipy 1.0 contributors. scipy 1.0. Nat. Methods, 17:261272, 2020. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. Tydi qa: benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 2020. 12 Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/ D16-1264. URL https://aclanthology.org/D16-1264. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023b. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Adam Cobb, Atılım Günes Baydin, Barak Pearlmutter, and Susmit Jha. Second-order forwardmode automatic differentiation for optimization. arXiv preprint arXiv:2408.10419, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. arXiv preprint arXiv:2303.14186, 2023. Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020. Alexandra Peste, Adrian Vladu, Eldar Kurtic, Christoph Lampert, and Dan Alistarh. Cram: compression-aware minimizer. arXiv preprint arXiv:2207.14200, 2022. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Andreas Köpf, Yannic Kilcher, Dimitri Von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36:4766947681, 2023. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the worlds first truly open instruction-tuned llm, 2023. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023b. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and Teknium. Openorca: An open dataset of gpt augmented flan reasoning traces, 2023. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, et al. Sciriff: resource to enhance language model instruction-following over scientific literature. arXiv preprint arXiv:2406.07835, 2024. Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021. Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024. Krish Agarwal, Rishi Astra, Adnan Hoque, Mudhakar Srivatsa, Raghu Ganti, Less Wright, and Sijia Chen. Hadacore: Tensor core accelerated hadamard transform kernel. arXiv preprint arXiv:2412.08832, 2024. Tri Dao. Fast hadamard transform in cuda, with pytorch interface, 2023. URL https://github. com/Dao-AILab/fast-hadamard-transform."
        },
        {
            "title": "A Gradient Analysis During Training",
            "content": "In this section, we analyze the behavior of gradients throughout training. We fine-tune LLaMA2-7B model [Touvron et al., 2023] on 10000 randomly selected samples from Tulu V2 [Ivison et al., 2023] for 2 epochs, saving model checkpoints every 10 steps. For each checkpointincluding the initial and final modelswe compute the gradients of 1000 held-out samples from Tulu V2, as well as samples from the target dataset BBH [Suzgun et al., 2022], and project them into an 8192-dimensional space using random Rademacher matrices, following the efficient GPU implementation of Park et al. [2023], also adopted in Xia et al. [2024]. For each dataset, we compute the average gradient cosine similarity across checkpoints. As shown in Figure 4, while the gradient directions can change substantially in the early steps, they stabilize quickly during training. This observation justifies the use of short warm-up phase as both necessary and sufficient. Similar plots for GSM8k [Cobbe et al., 2021] and SQuAD [Rajpurkar et al., 2016] are provided later in the Appendix (Figure 9). Additionally, for each dataset and checkpoint, we measure the Pearson product-moment correlation between gradient norms and the number of label tokens per sample. As shown in Figure 5, we observe consistent negative correlation, which supports our decision to normalize gradients prior to distillation."
        },
        {
            "title": "B Linear Model Study",
            "content": "In this section, we show that regularization term can be effective in robustifying Objective 6 to small changes in the model parameters θ, when the model is linear and the loss is quadratic. For fixed ϵ > 0, define new objective as below: = arg min max δϵ (w; θ + δ), (13) minimizing the maximum value of around point θ in neighborhood of radius ϵ. This ensures the weights are stable as long as θ is in this neighborhood. To solve for w, we employ Lemma B.1 below: = (cid:80)D i=1 wi(xD = D)} is dataset. For datasets and , let HT = 2 θL(θ; T, 1), θL(θ; S, w), and gw = θL(θ; S, w). Define aw and Bw as )2, where , θ yD 1 , yD 1 ), (xD Lemma B.1. Assume L(θ; D, w) {(xD 2 , yD D, yD gT = θL(θ; T, 1), Hw = 2 below: 2 ), ..., (xD aw = HwgT HT gw + ηHwHT gw Bw = HT Hw + η 2 HwHT Hw (14) (15) In the setting above (linear model with quadratic loss), the function has the property that θ, δ Rd, Rn: (w; θ + δ) = (w; θ) + aT wδ + δT Bwδ. (16) Proof. First notice that, for simplicity, the loss here is defined as the sum (as opposed to the average) of per-sample losses, which drops the 1 terms in the loss, gradient, Hessian, and objects. Recalling the definition of from 5, we can write (w; θ + δ) = p(θ + δ)T + η 2 wT Q(θ + δ)w. Since the loss is quadratic in θ, the Hessian is independent of θ, and the derivatives above the second order are zero. Hence, defining gD as the gradient and Hessian of the sample in D, we can write: (θ) and HD (17) for any δ with the same dimension as θ. Setting = and summing across samples, we can write: (θ + δ) = gD gD (θ) + HD δ gT (θ + δ) = gT (θ) + HT δ Additionally, setting = and taking weighted sum we can write: GS(θ + δ)w = GS(θ)w + Hwδ (18) (19) 15 Figure 4: Average gradient cosine similarity on unseen samples from Tulu V2 (top) and BBH (bottom) across checkpoints. Next, we see that, p(θ + δ)T = gT (θ + δ)GS(θ + δ)w = (gT (θ)T + δT HT )(GS(θ)w + Hwδ) = p(θ)T + (gT (θ)T Hw + gw(θ)T HT )δ + δT HT Hwδ (20) And, wT Q(θ + δ)T = wT GS(θ + δ)T HT GS(θ + δ)w = (wT GS(θ)T + δT Hw)HT (GS(θ)w + Hwδ) = wT Q(θ)w + 2gw(θ)T HT Hwδ + δT HwHT Hwδ (21) 16 Figure 5: Correlation between gradient norm and number of label tokens, across checkpoints on four datasets. Putting them together: (w; θ + δ) = p(θ + δ)T + η 2 wT Q(θ + δ)w = (w; θ) ((gT (θ)T Hw + gw(θ)T HT )δ + δT HT Hwδ) + η 2 (2gw(θ)T HT Hwδ + δT HwHT Hwδ) = (w; θ) + (gT (θ)T Hw gw(θ)T HT + ηgw(θ)T HT Hw)δ + δT (HT Hw + η 2 wδ + δT Bwδ = (w; θ) + aT HwHT Hw)δ which concludes the proof. Substituting the result of the B.1 into Objective 13, we can write (cid:104) (w; θ) + aT wδ + δT Bwδ (cid:105) = arg min = arg min max δϵ (cid:104) (w; θ) + max δϵ (aT wδ + δT Bwδ) (cid:105) (22) wδ + δT Bwδ in the sphere with radius ϵ approximately by taking single We maximize r(δ) = aT r(0) step of size ϵ in the gradient direction, i.e., δ ϵ r(0) . This approximation is standard in the sharpness-aware optimization literature [Foret et al., 2020, Peste et al., 2022], which addresses similar min-max objective to search for flat minima. Note that r(δ) = aw + (Bw + BT w)δ, hence r(0) = aw and max δϵ (aT wδ + δT Bwδ) ϵ aw+ϵ2 Substituting into Equation 22, we get the following objective: aT wBwaw aw2 . arg min (cid:104) (w; θ) + ϵ aw+ϵ2 aT wBwaw aw2 (cid:105) . This suggests that the robustness of the weights can be controlled via the hyperparameter ϵ, which determines the strength of the regularization. We apply this regularization to the running example introduced in Section 3.2. As shown in Figure 6, using the tuned value ϵ = 104 yields better performance than the default weights. However, due to the high computational cost of this regularization term, we use standard L2 regularization for general non-linear models. 17 (23) (24)"
        },
        {
            "title": "C Adam Optimizer",
            "content": "Here we derive Equations 8 and 9, which adapt the vector and the matrix to the case of the Adam optimizer. Assume that after warm-up phase, the firstand second-moment estimates of Adam are and v, respectively. For new gradient g, the Adam update rule can be written as: θ = η v + ϵ (25) 1βs and = β2v+(1β2)g2 where η is the learning rate, and = β1m+(1β1)g are the updated moment estimates, with (β1, β2) being the Adam beta values for firstand second-order estimate updates, and being the number steps the optimizer has already been trained for. For single update, we note that β2v + (1 β2)g2 v. That is because (1) the value β2 is typically very close to 1, e.g., 0.995 or 0.999, and (2) due to the warm-up, is stabilized and is not expected to change much. This allows us to ignore the dependence of on g, i.e., simplifying the computations. 1βs 2 1βs 2 Enabled by this, we revisit the Taylor expansion in Equation 2: = arg min L(MAdam(θ; S, w); T, 1) = arg min L(θ η β1m+(1β1)GS (θ)w 1βs 1 (cid:113) 1βs 2 + ϵ ) = arg min L(θ η [ β1m (cid:113) 1βs (1 βs 1)( + + ϵ) (1 β1)GS(θ)w (cid:113) (1 βs 1)( 1βs 2 + ϵ) ]) (26) Let = (1βs (1β1) 1 )((cid:113) 1βs 2 +ϵ) and = (1βs β1m 1 )((cid:113) 1βs +ϵ) . Construct GAdam (θ) by element-wise multiplying each column of GS(θ) by a. We can now continue Equation 26 by: η w = arg min (b + GAdam L(θ (θ)w) arg min [L(θ; T, 1) η gT (θ)(b + GAdam (θ)w)+ (θ)T )HT (θ)(b + GAdam (θ)w)] = arg min η2 2 S2 (bT + wT GAdam (θ) (gT + η 2 wT GAdam bT HT (θ))GAdam η (θ)T HT (θ)GAdam (θ)w (θ)w = arg min pAdam(θ)T + η 2 wT QAdam(θ)T (27) Where pAdam and QAdam are defined in Equations 8 and 9, respectively. Proof of Theorem 4.1 We begin by noting property of the landmark-based approximation introduced in Section 4.3: it exhibits rotational equivariance. That is, if all source and target gradients are rotated by an orthonormal matrix, the resulting landmark-based gradient approximations will also be simply rotated by the same matrix. In the remainder of this section, we prove two useful lemmasLemma D.1 and Lemma D.2. We then state and prove Theorem D.3, which bounds the error in the vector for any unbiased approximation that satisfies rotational equivariance. Finally, Corollary D.4 bounds the difference in the resulting sample weights, thereby completing the proof of Theorem 4.1. 18 Figure 6: (Left) Distribution of theoretical robust weights for the linear case with ϵ = 104, and (Right) validation loss during training with different variants in the running experiment setting. Lemma D.1. Given unit vectors g, tRd, assume ˆg=g+e is noisy approximation to g. Additionally, assume Rd is zero-mean, isotropic random vector, i.e., E[e] = 0 and Cov(e) = σ2Id for some σ > 0. Let = ˆg, t. Then E[S] =g, t, and Var(S) = E[ˆgg2 2] . Proof. The expectation of follows directly from zero-mean property of e. To bound its variance, let Σ denote the covariance matrix of e. Since is isotropic, Σ = σ2Id for some σ. We can write: Var(S) = Varˆg, = Var(g, + e, t) = Var(e, t) = tT Σt = σ2t2 = σ E[ˆg g2] = E[e2] = tr(Σ) = dσ2 (28) (29) Also, Hence σ2 = E[ˆgg2] , which concludes the proof. Lemma D.2. Assume Rd is random vector from an arbitrary distribution. For any random orthonormal matrix of the form = PD, where is random permutation matrix is diagonal matrix with i.i.d. Rademacher signs (1) the random vector = Rx is isotropic, i.e., Cov(y) = σ2Id for some real value σ. Proof. We can write: Cov(Rx) = EP,D,x[RxxT RT ] = EP,D,x[PDxxT DPT ] = Ex[EP[ED[PDxxT DPT P, ] ]] = Ex[EP[PED[DxxT ]PT ]] (30) 19 Now note that ED[DxxT ] = diag(x2). Substituting into the expectation over P, we need to compute EP[Pdiag(x2)PT ]. However, since is random permutation, off-diagonal elements are zero and for the diagonal elements, any element of x2 can be picked with equal probability. Hence, the expectation over equals x2Id. Putting it all together in Equation 30, we get Cov(Rx) = E[x2] Id, (31) which concludes the proof. Theorem D.3. Let Rnd and Rd, with and each row of having unit lengths. Let gi denote the ith row in G. Additionally, assume access to (randomized) mapping function : {g1, g2, . . . , gn} Rd, and let {1, 2, . . . , n} : ˆgi = h(gi; G). Additionally, assume h(.) satisfies: 1. Unbiased: {1, 2, . . . , n} : E[ˆgi] = gi, i.e., h(.) is unbiased. 2. Bounded Average Mean Squared Error: Let δ2 = E[ˆgi gi2]. Then: 1 n (cid:88) i=1 2 δ2 for some 2 0. 3. Rotation Equivariance: For any orthonormal rotation matrix Rdd and {1, 2, . . . , n} : h(Rgi; GR) = Rˆgi. Construct the vector p=[p1, p2, . . . , pn]T such that pi=gi, t. Similarly define ˆp=[ˆpi, ˆp2, . . . , ˆpn]T , where ˆpi = ˆgi, t. Then E[p ˆp2] n2 (32) Proof. For all i, let ei = ˆgi gi denote the error. By the Unibased assumption, E[ei] = 0. Without loss of generality, we can assume that for all i, the vector ei is isotropic, i.e., Cov(ei) is scalar multiple of the identity matrix. If this is not the case, we take advantage of Lemma D.2 and apply change of variables: GR and Rt, where = PD, is permutation matrix, and is diagonal matrix with entries chosen uniformly at random from {1}. Note that by the Rotation Equivariance assumption, this transformation implies ˆgi Rˆgi. Under this transformation, the error vectors ei are mapped into space where they become isotropic, and the pairwise dot products and distances remain unchanged as is orthonormal. Now we can directly apply Lemma D.1 for each coordinate i: E[ˆpi] =pi and Var(ˆpi) = E[ˆgg2] This means: . E[p ˆp2] = = = E[(pi ˆpi)2] Var(ˆpi) E[ˆg g2] (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 n2 where the last inequality comes from the Bounded Average Mean Squared Error assumption. Corollary D.4. In the setting of Theorem D.3, if we define: w(p) = arg min pT + λ 2 x2 2, s.t. (cid:26) 0 xT 1 = (33) then E[w(p) w(ˆp)2] n2 λ2d . (34) Proof. Let Fp(x) = pT + λ 2 and = {x Rd : 0, xT 1 = n}. Note that the objective above has unique solution since Fp is λ-strongly convex and is convex set independent of p. 2 x2 By strong convexity, x, Rd: Fp(y) Fp(x) + xFp(x)T (y x) + λ 2 x2 (35) Set = := w(p) and = ˆw := w(ˆp). Since minimizes Fp over C, xFp(x)T (y w) 0. Hence: Fp( ˆw) Fp(w) + ˆw2 λ 2 Swapping and ˆw, Adding the two equations above: ˆp(w) ˆp( ˆw) + λ 2 ˆw2 (p ˆp)T (w ˆw) λw ˆw2 Applying Cauchy-Schwarz on the left hand side, we get Hence ˆpw ˆw λw ˆw2 ˆw 1 λ ˆp Combining with the result of Theorem D.3: E[w ˆw2] n2 λ2d . (36) (37) (38) (39) (40) (41)"
        },
        {
            "title": "E Dataset and Model Details",
            "content": "This section provides details on the datasets and models used throughout the paper. E.1 Datasets For the datasets, we largely follow the setup of Ivison et al. [2025]. Tulu V2 (ODC-BY License). The Tulu V2 dataset [Ivison et al., 2023], also known as the Tulu V2 SFT Mixture, is comprehensive instruction-tuning dataset. Following Ivison et al. [2025], we consider the unfiltered version with 5.8M samples, consisting of 961,322 samples from FLAN v2 [Chung et al., 2024], 398,439 samples from FLAN CoT [Chung et al., 2024], 7,707 samples from Open Assistant [Köpf et al., 2023], 15,007 from Dolly [Conover et al., 2023], 52,002 from GPT-4 Alpaca [Peng et al., 2023], 20,022 from Code Alpaca [Chaudhary, 2023], 100,054 from ShareGPT, 1,030 from LIMA [Zhou et al., 2023b], 142,802 from Wizard Evol-Instruct V2 [Xu et al., 2023], 4,111,858 from Open Orca [Lian et al., 2023], 7,535 from SciRIFF [Wadden et al., 2024], and 14 from Hardcoded. For more information, we refer the reader to Ivison et al. [2025]. MMLU (MIT License). The Massive Multitask Language Understanding (MMLU) dataset [Hendrycks et al., 2021a,b] consists of challenging multiple-choice questions from 57 topics, such as abstract algebra, astronomy, machine learning, and more. It includes 5 development samples per category and total of 14,042 test samples. We use the development samples as our target set and evaluate the final model zero-shot on the test set. GSM8K (MIT License). This dataset comprises grade school math questions, with 7.47k training and 1.32k test samples [Cobbe et al., 2021]. We evaluate the models on the test set using 8 examples 21 in the context (8-shot evaluation) and use the same 8 individual samples as the target set. As is standard, only the final answer to each question is considered. Big-Bench-Hard (MIT License). This dataset includes questions from 27 challenging tasks, such as causal judgment, multi-step arithmetic, and logic. Following Suzgun et al. [2022], we perform 3-shot evaluations using the same 3 samples per category (a total of 81) as the target set. TyDIQA (Apache-2.0 License). TyDIQA is dataset of 204k question-answering samples across 11 languages [Clark et al., 2020]. For evaluation, we follow Ivison et al. [2025], which in turn follows Anil et al. [2023], using 1-shot prompting. We select 9 samples per language for the target set. Codex (MIT License). This dataset contains 164 Python programming questions [Chen et al., 2021], of which 16 are used as the target set and the remaining as the test set. See Ivison et al. [2025] for additional evaluation details. SQuAD (CC BY-SA 4.0 License). The Stanford Question Answering Dataset (SQuAD) [Rajpurkar et al., 2016] contains reading comprehension questions based on Wikipedia articles. We use 500 random samples from the training split as the target set. We perform 3-shot evaluations with three samples randomly selected from the training set. E.2 Model Licenses In this paper, we utilize LLaMA 2 [Touvron et al., 2023], LLaMA 3.2 3B [Grattafiori et al., 2024], Qwen 2.5 1.5B [Team, 2024], and Qwen 2.5 3B [Team, 2024] models. These models are distributed under the LLaMA 2 Community License, LLaMA 3.2 Community License, Apache-2.0 License, and Qwen Research License, respectively."
        },
        {
            "title": "F Embeddings Study",
            "content": "In Section 4.3, we noted that existing embedding functions are insufficient for our landmark-based gradient approximations and introduced the JVP embeddings as an alternative. In this section, we compare different embedding functions in two settings. In all the experiments, the model we consider is Llama-2 7B [Touvron et al., 2023]. Gradient Recovery. First, we randomly take 200k samples from Tulu V2 [Ivison et al., 2023] and embed them using various embedding functions. We then use small number of landmark gradient samples (selected uniformly at random) to approximate the gradients for all data points, following the method described in Section 4.3. This process is repeated for different numbers of landmarks to evaluate how performance varies with landmark count. We report the average cosine similarity between the approximated gradients and the true gradients (projected into 8192-dimensional space using Rademacher-based projections [Ivison et al., 2025, Park et al., 2023]) for each case. We evaluate several embedding functions: the RDS+ embeddings from Ivison et al. [2025], NVIDIAs NV-Embed-v2 [Lee et al., 2024], GTR-base [Ni et al., 2021], and our proposed JVP-based approach using two random vectors and four transformer blocks. As lower bound, we also include Trivial embedding: here, we assume that the gradients for the landmark samples are perfectly recovered, while the gradients for all other samples are treated as completely random. Figure 7 (Left) presents comparison of these embedding functions. Our JVP embeddings outperform all other methods, including the more computationally intensive RDS+ and NV-Embed-v2. Finally, we compute an upper bound by using the true projected gradients as the embedding function and repeating the same experiment. As shown in Figure 7 (Right), this idealized setting quickly achieves high accuracy in gradient approximationsurpassing 0.9 cosine similarity with just over 4096 landmarks. This suggests that the gradients are approximately low-rank, known phenomenon in LLMs [Hu et al., 2022, Zhao et al., 2024]. End-to-end Selection and Training. We repeat the selection and fine-tuning experiments from Table 1, this time replacing the JVP embeddings with either GTR-base or true gradient embeddings. Table 2 reports the resulting accuracy for each task. Due to the high computational cost of obtaining true gradients, we include only single random seed for this setting. 22 Figure 7: (Left) Gradient direction recovery vs number of landmarks, when different proxy embdeding functions are used, and (Right) gradient direction recovery when the actual gradients are used as an ideal embedding. Table 2: Accuracy ( standard deviation) of Llama2-7B across six tasks when using Influence Distillation with different embeddings to select 10k samples from pool of 200k in the Tulu V2 dataset [Ivison et al., 2023]. The number of landmarks is fixed at 4096. Model Embedding MMLU GSM8k BBH TyDIQA CODEX SQuAD Avg. w/ Uniform Llama2-7B GTR-base JVP Grad 46.7 0.17 48.3 0.21 48.3 18.7 0.27 20.3 1.65 20.2 42.8 0.34 43.2 0.67 43. 52.2 0.56 53.6 0.34 51.7 29.3 0.84 29.5 3.14 27.7 82.1 0.30 83.2 1.02 84.5 45.3 46.4 46.0 We fix the number of landmarks to 4096 across all experiments. The results show that while GTR-base consistently underperforms, the JVP and true gradient embeddings yield comparable accuracyfalling within each others standard deviation in most cases. This indicates that the gradient approximations provided by JVP embeddings are sufficiently accurate for end-to-end training. Finally, we note that since Figure 7 (Right) demonstrates near-perfect gradient recovery using the Grad embedding, the corresponding row in Table 2 closely mirrors the performance of the LESS method [Xia et al., 2024]. An Active-Set Solution In this appendix we derive the solution to the Influence Distillation objective under the assumption that ηQ + λI is positive definite (PD). This setting includes the special first-order case used in the main body of the paper, where η 0. Concretely, we solve = arg min pT + η wT Qw + λ 2 wT w, s.t. (cid:26) 0 wT 1 = (42) where denotes the dimension of and ηQ + λI 0. Introduce the Lagrange multipliers τ for the equality constraint and α Rn negativity constraints. The Lagrangian is 0 for the nonL(w, τ, α) = pw + η 2 wQw + λ 2 ww τ (1w n) αw. (43) Differentiating with respect to and setting it equal to zero yields Let := ηQ + λI 0. Then ηQw + λw τ 1 α = 0. Rw τ 1 α = 0. (44) (45) By complementary slackness, : wiαi = 0. Let = {i wi = 0} be the active set and its complement. Restricting (45) to the free indices gives Because RBB is principal sub-matrix of the PD matrix R, it is itself PD. Hence RBBwB = pB + τ 1B. w(τ ; B) = R1 BB(pB + τ 1B). 23 (46) (47) Enforcing 1T = determines τ : BR1 1T BB(pB + τ 1B) = n, and therefore τ = 1T BR1 1T BR1 BB1B BBpB . Substituting τ back into w(τ ; B) gives us the weights on B: = R1 BB (cid:16) pB + ( 1T BR1 1T BR1 BB1B BBpB (48) (49) (50) (cid:17) . )1B For indices in the active set we have B). Optimality requires that the remaining KarushKuhnTucker (KKT) conditions hold, namely B, wi 0 (primal feasibility) and A, αj 0 (dual feasibility). Because the objective is convex (R 0), any partition A, satisfying these conditions is the global optimum. = 0, giving the final candidate solution = (w A, Examining the coordinates in in (45) gives αA = (RABw B)A pA τ 1A. (51) Problems of this type are typically solved with primaldual active-set algorithm. We start from the feasible point = 1 (so = , = {1, . . . , n}) and repeat: 1. Solve for via (50). 2. If any component of B is negative, move its index to A. 3. Compute αA; if any component is negative, move its index back to B. Each move strictly decreases the objective, and with only finitely many index sets the algorithm terminates once all components of wB and αA are non-negative. The Special Case of η 0. This setting corresponds to the first-order Influence Distillation variant used throughout the main body of the paper. In this case, we demonstrate that as λ increases, the solution becomes denserthat is, it contains more non-zero elements. This observation is leveraged in Section 4.4 for tuning the parameter λ. When η 0, we can write = λI, which implies R1 into Equations 49, 50, and 51, we obtain: λ and RAB = 0. Substituting these BB = τ = BpB nλ 1T w = 1 λ (p + τ 1)B αA = (p + τ 1)A (52) (53) (54) Since both wB and αA must be non-negative, the last two equations imply that the active set must satisfy = {i : pi τ }, i.e., is necessarily set of top-k elements from for some k. Consider two values λ1 < λ2, and let B1 and B2 denote their optimal supports with sizes k1 and k2, and w(1) and w(2) their respective optimal weight vectors; similarly, let α(1) and α(2) denote their associated dual variables. Suppose for contradiction that k2 < k1. Note that B1 consists of the indices of the top k1 elements in p, while B2 B1 includes the top k2 elements of p. Let sk1 and sk2 represent the sums of the top k1 and k2 elements in p, respectively. Define as the index of the k1-th largest element in p. Since B1, we have w(1) 0, and since / B2, it follows that 24 α(2) 0. Therefore, and w(1) pj + nλ1 sk1 k1 0 nλ1 sk1 k1pj = α(2) pj + nλ2 sk2 k2 0 nλ2 sk2 k2pj = (cid:88) iB (pi pj) (cid:88) iB2 (pi pj) Observe that (cid:80) nλ2 nλ1, which contradicts our initial assumption that λ1 < λ2. (pi pj) (cid:80) iB2 iB1 (pi pj) by definition of pj, leading to the inequality This contradiction confirms that as the regularization parameter λ increases, the solution becomes progressively denser. Specifically, at λ = 0, the solution concentrates all weight on the largest element of to minimize the objective, whereas in the limit as λ , the regularization dominates, resulting in = 1. Firstvs Second-Order Influence Distillation Recall the robust Objective = arg min (w; θ) + λ 2 w2 2, s.t. (cid:26) 0 wT 1 = where, (w; θ) = p(θ)T + η 2 wT Q(θ)w (55) (56) In this section, we compare the first-order term T1 = p(θ)T with the second-order term T2 = η 2 wT Q(θ)w. To do so, we sample 128 random examples from the Tulu V2 dataset [Ivison et al., 2023] as the source dataset, and 4 examples from either GSM8k [Cobbe et al., 2021] or MMLU [Hendrycks et al., 2021a,b] as the target dataset. We compute the vectors and the matrices exactly for the Qwen-2.5 1.5B model [Team, 2024], using Hessian-vector products to obtain Q. We then evaluate both T1 and T2 using default weights = 1 and range of learning rates. To measure the relative contribution of the second-order term, we report the ratio (cid:12) (cid:12) (cid:12) T2 (cid:12) (cid:12) (cid:12). As shown in Figure 8, the second-order term is generally negligible for practical learning rates (η 104), indicating that the first-order approximation is sufficient in this setting."
        },
        {
            "title": "I Projection Details",
            "content": "While in some of our lower-cost experiments we employ Rademacher-based projectionsincluding projecting JVP embeddings to 4096-dimensional space using this method, as supported on GPUs by Park et al. [2023]we find that projecting the landmark gradients with Rademacher projections becomes computational bottleneck. To address this, we instead use combination of pre-masking and Randomized Hadamard Transform-based projections, as described below. Hadamard-based Projection. Given high-dimensional gradient vector g, we first pad it with zeros to the nearest power of two, 2k. Then, we apply random sign (1) to each element. The 2 . We then apply signed vector is reshaped into matrix of dimensions = 2 2 and = 2 25 Figure 8: Ratio of secondto first-order terms for Qwen-2.5 1.5B across learning rates on two target datasets. Table 3: Accuracy ( standard deviation) of Llama2-7B across six tasks when using Influence Distillation to select 10k samples from pool of 200k in the Tulu V2 dataset [Ivison et al., 2023], with and without loss weighting during training. The number of landmarks is fixed at 8192. Model Embedding MMLU GSM8k BBH TyDIQA CODEX SQuAD Avg. w/ Uniform Llama2-7B Weighted Not Weighted 47.8 0.16 48.2 0. 19.5 0.06 19.6 0.79 42.3 0.26 42.4 0.14 52.2 1.38 52.7 1.67 27.0 2.53 29.3 1.27 84.4 0.48 83.4 0.86 +1.48 +1. Hadamard transforms from both sides: HT subset of its entries is selected as the projected vector. mXHn. The resulting matrix is flattened, and random Importantly, both the random sign patterns and the final index subset are generated once and reused across all projected vectors. This ensures consistency and enables meaningful comparison. The left and right Hadamard transforms are highly efficient and provide strong mixing across rows and columns. Pre-masking. Although efficient GPU implementations of the Hadamard transform exist [Agarwal et al., 2024, Dao, 2023], they support transforms up to dimension 215 = 32,768. This allows us to efficiently project vectors of up to 230 = 1,073,741,824 elementsjust over one billion. However, the full gradients of large language models (LLMs) can exceed this size. To address this, we apply pre-masking: we randomly select one billion elements from the gradient vector before projection. For LLaMA-2 7B [Touvron et al., 2023], we select these elements from the down_proj matrices, which we find to represent the overall gradients well. For smaller models, we randomly sample one billion elements from the entire gradient vector."
        },
        {
            "title": "J Weighted Training Loss",
            "content": "In this section, we investigate the effect of incorporating the weights derived by Influence Distillation into the training loss. Specifically, we conduct an experiment using LLaMA-2 7B [Touvron et al., 2023], with pool size of 200k and 8192 landmarks sampled from Tulu V2 [Ivison et al., 2023]. During training, we scale the loss of each selected sample by its corresponding weight. Table 3 compares this weighted training setup with baseline where the weights of the selected samples are ignored. The results show that incorporating weights during training does not improve performanceand in some cases, it may even degrade it. This may be due to some samples having near-zero weights, effectively pruning them from the training process."
        },
        {
            "title": "K Differed Figures",
            "content": "26 Figure 9: Average gradient cosine similarity on unseen samples from GSM8k (top) and SQuAD (bottom) across checkpoints."
        }
    ],
    "affiliations": [
        "Google Research",
        "ISTA",
        "Red Hat AI"
    ]
}