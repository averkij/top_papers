{
    "paper_title": "SeqPE: Transformer with Sequential Position Encoding",
    "authors": [
        "Huyang Li",
        "Yahui Liu",
        "Hongyu Sun",
        "Deng Cai",
        "Leyang Cui",
        "Wei Bi",
        "Peilin Zhao",
        "Taro Watanabe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each $n$-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe."
        },
        {
            "title": "Start",
            "content": "PREPRINT 1 SeqPE: Transformer with Sequential Position Encoding Huyang Li, Yahui Liu, Hongyu Sun, Deng Cai, Leyang Cui, Wei Bi, Peilin Zhao, Taro Watanabe AbstractSince self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALIBI and ROPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SEQPE, unified and fully learnable position encoding framework that represents each n-dimensional position index as symbolic sequence and employs lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SEQPEs embedding space, we introduce two complementary objectives: contrastive objective that aligns embedding distances with predefined position-distance function, and knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracyparticularly under context length extrapolationbut also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe. Index TermsTransformers, Position Encoding, Position Embeddings, Contrastive Learning, Multi-modality"
        },
        {
            "title": "1 INSTRUCTION",
            "content": "T He Transformer model [46] has become the dominant framework for AI tasks across data with varied modalities, including text [3, 35], image [9, 43], and video [13]. However, the self-attention mechanism in Transformers is inherently permutation-invariant, lacking an intrinsic understanding of input token order, an aspect crucial for capturing the semantics of sequential data. To address this, position encoding is introduced to inject order-sensitive information into the models [12, 46], enabling Transformers to distinguish tokens based not only on their content but also on their position within the context. Recently, there has been extensive research exploration on position encoding, which can be broadly categorized into two main directions. In the first direction, fixed or learnable position embeddings (PEs) are provided through the use of fixed-size lookup tables [8, 9, 35, 36]. Such PEs are typically added to the input text or vision embeddings as the input to transformer models. However, due to training constraints imposed by fixed maximum sequence lengths, such PEs usually exhibit poor extrapolation capacity to sequence lengths that exceed the length encountered during training. In the second direction, positional information is injected by modifying attention scores with expert-designed weights or mapping functions [1, 19, 33, 42, 42, 44, 49]. key advantage of such methods is their ability to establish Huayang Li, Hongyu Sun and Taro Watanabe are with Nara Institute of Science and Technology (NAIST), Nara, Japan. Yahui Liu is with Kuaishou Technology, Beijing, China. Deng Cai, Leyang Cui and Wei Bi are independent researchers in China. Peilin Zhao is with Tencent, Shenzhen, China. Taro Watanabe and Yahui Liu are co-corresponding authors. E-mail: {li.huayang.lh6, taro}@is.naist.jp, yahui.cvrs@gmail.com consistent, predetermined positional relationship patterns, enabling models to extrapolate to previously unseen sequence positions. However, they also come with two crucial limitations: (1) They primarily model distance decay or local emphasis but may fail to capture more subtle, nonmonotonic or directional positional relationships that learnable embeddings could represent. (2) They use fixed, nonlearned functional form for the bias, and lack adaptability for different datasets, tasks [27, 53] or even different layers within the same model [16]. In this work, we propose an alternative scheme for learnable position encoding, called SEQPE, which employs sequential position encoder to directly map sequential representationderived from an n-dimensional position into hidden vector representing that position. Concretely, we represent the n-dimensional position using three types of embeddings: sequence tokens, sequence positions, and data dimensions. The embedded sequence is then encoded using lightweight encoder . An example of the position representation for 2D data is illustrated on the left-hand side of Figure 1. By adopting this sequential representation as the new interface, our learnable position encoding method can easily encode unseen positions, thereby overcoming the limitation of standard learnable position encoding methods, which can only handle fixed number of positions. Additionally, compared to PEs with expert-designed weights or mapping functions, our method offers superior adaptability to diverse tasks and enhanced extensibility to data across varying dimensionality. To guarantee the stability of performance, there is critical challenge needs to be addressed: sequential models often struggle to distinguish semantic differences between lexically 5 2 0 2 6 1 ] . [ 1 7 7 2 3 1 . 6 0 5 2 : r PREPRINT 2 Fig. 1: Overall architecture and objectives for SEQPE. We use the representation for the (2, 3) position in an image with 4 4 patches as an example, where SeqPE converts the (2, 3) position to left-padded sequence (0, 2, 0, 3) and learn its representation by sequential position encoder. The main purpose of the left-side padded 0 for each dimension is to ensure that digits with the same place value (e.g., units, tens) always occupy the same position in the sequence, resulting in more consistent representation and improved training efficiency. With the sequential interface of positions, SEQPE can be easily extended to positions of longer context and varied dimensions. Moreover, two regularization objectives, agnostic to data dimensionality, are introduced to regularize the position representation space of SEQPE. similar sequences [2, 23]. For example, while one might intuitively expect that closer positions would yield more similar representations, we find that, in the case of one-dimensional data, the embedding of 100 learned by SEQPE is closer to that of 1000 than to 123, because 100 and 1000 share higher lexical overlaps. To tackle this challenge, we introduce contrastive-learning-based objective to regularize the representation space of encoded position embeddings. Specifically, given predefined distance function δ between positions, we train SEQPE to ensure that pairs of positions and that are closer under δ produce more similar embeddings, where and may have arbitrary dimensionality. In addition, although the contrastive-learning strategy has the potential to generalize to unseen positions, its performance on the main task may degrade when evaluated on out-of-distribution (OOD) positions. This is because the embeddings of such OOD positions may shift into regions of the state space that were under-optimized for the main task objective. To mitigate this issue, under the assumption that the main task model is largely optimized on positions within the training context of n-dimensional data, we introduce knowledge distillation objective. This objective leverages embeddings from the training context to guide the learning of embeddings for n-dimensional OOD positions. We conducted extensive experiments to validate the effectiveness of SEQPE in three key aspects: learnability and adaptability on various tasks, context extrapolation capability, and extensibility to higher-dimensional data. We evaluate our method in three tasks (i.e., language modeling, question answering, and image classification) under the context extrapolation setting. We first evaluate on the language modeling task using the GPT-2 model [35]. Our SEQPE outperforms other baselines by at least 0.6 perplexity points on the Wikitext-103 test set. To assess adaptability, we fine-tuned the pre-trained language models on longcontext question answering task [20]. Results show that, compared to other baselines, our method achieves at least 24.2 and 2.6 points of average performance improvement in perplexity and exact match (EM) metrics on the answer span, respectively. Finally, we extend our method to 2D image data and demonstrate that the ViT model [9, 43] trained on ImageNet-1K with SEQPE surpasses other baselines by minimum of 2.1 percentage points in classification accuracy. In summary, our contributions are three-fold: We propose novel scheme for learnable position encoding, called SEQPE, which replaces conventional index-based lookup schemes with sequential position encoder for position representation. We introduce two training objectives to regularize the representation space of SEQPE: distance alignment objective that matches embedding distances and positional distances, and an objective for generalization of out-ofdistribution positions. Both objectives are agnostic to the dimensionality of data. We perform comprehensive experiments across language modeling, long-context question answering, and 2D image classification tasks, demonstrating enhanced adaptability, superior context extrapolation capabilities, and effective extensibility to higher-dimensional data."
        },
        {
            "title": "2 PRELIMINARY: POSITION ENCODING IN TRANS-\nFORMERS",
            "content": "Given an input data, e.g., sequence of sub-words [8, 35] or image patches [9], the Transformer model [46] first embeds it into sequence of content embeddings = RLd, which has no position information (x1, x2, . . . , xL) yet. Then the Transformer model processes the information in using multiple layers of self-attention-based neural networks. The core of the attention mechanism1 for the i-th 1. For the simplicity of notions, we omit the multi-head part of the Attention function [46]. PREPRINT position in each layer is as follows: 3.1 Overview Q, K, = HW{q,k,v}, Attention(A, V)i = Ai,j = kj (cid:80)L j=1 exp(Ai,j/d)vj (cid:80)L j=1 exp(Ai,j/d) (1) (2) (3) , Rd3d is the parameter for the linear where W{q,k,v} RLd is the input of this layer, and transformation, RLL contains the non-normalized attention scores between any pair of positions. Notably, = for the first layer input if no position information is used (e.g., NOPE method [24, 50]). To equip the model with sense of order at the input level, the Absolute Position Embedding (APE) method [12, 46] is proposed by adding PEs to the input content embeddings: = (x1 + e1, x2 + e2, . . . , xL + eL), (4) where ei is the position embedding for the i-th position, which could be sinusoidal-based fixed weights [46] or learnable embeddings [8, 35]. Afterwards, = is used as the first layer input for the APE method [12]. Differently, the ALIBI method [33] does not add the position information into (i.e., = for the first layer input of the Transformer model), but injects the position information into the attention mechanism:"
        },
        {
            "title": "AALiBi",
            "content": "i,j = kj + Mi,j (5) where AALiBi i,j will be used for the attention function in Equation 3. The Mi,j is the expert-designed bias for text data: Mi,j = (cid:40) , j), (i j, if otherwise, (6) where is head-specific scale value. Alternatively, the core of ROPE [42] is the rotary matrix RΘ, which is designed based on Eulers formula, where Θ represents pre-defined frequencies vector. The RΘ is directly applied to the query qi and key kj as follows:"
        },
        {
            "title": "ARoPE",
            "content": "i,j = R Θ,iRΘ,jkj (7) where property of the rotary matrix is that RΘ,ji = Θ,iRΘ,j, i.e., modeling the relative relationship between positions. To achieve these goals, we propose an alternative framework for the position encoding, which employs sequential position encoder (SEQPE) that directly maps an n-dimensional position into hidden vector representing that position via sequential modeling process. As illustrated in Fig. 1, when representing the position (2, 3) in an image with 4 4 patches, SEQPE first converts it into left-padded sequence (0, 2, 0, 3) and then uses multi-layer sequential encoder [46] to transform the sequence into the hidden representation e(2,3) of the position. Details of sequential representation and position encoder are provided in Section 3.2. This sequential modeling approach offers several advantages. First, SEQPE can leverage fixed learning parameters to model dynamic number of input positions. In contrast, standard learnable position embeddings typically rely on lookup table whose size grows linearly with the training context length, consuming extensive memory when training on long-context data. Moreover, the size of the lookup table is fixed, making it difficult to extend to unseen positions in longer contexts. Second, the new input interface in SEQPE, sequence of position tokens, is unified for data of any dimensionality, including hybrid-dimensional data [27, 53] such as interleaved text and images. As result, SEQPE can be readily extended to data of varying dimensions without the need for manual adjustments, unlike many expert-crafted position encoding methods [33, 42]2. However, simply integrating the position embeddings learned by SEQPE into the main task model, e.g., for language modeling or image classification, does not always yield satisfactory results in our preliminary experiments. One limitation is that the representation space learned by SEQPE struggles to distinguish distance differences between lexically similar position sequences. For example, the embedding of 100 learned by non-regularized SEQPE is closer to that of 1000 than to 123, which introduces non-beneficial bias to the main task model. Additionally, although SEQPE has the potential to generalize to unseen positions, its performance on the main task may degrade when evaluated on out-ofdistribution (OOD) positions, because embeddings of such OOD positions may shift into regions of the state space that were under-optimized for the main task objective. In the rest of this section, we introduce two position objectives to regularize the representation space of SEQPE, solving the two aforementioned challenges: embedding distances (Section 3.3) and out-of-distribution generalization (Section 3.4). It is worth noting that all the proposed regularization objectives are agnostic to the data dimensionality, although we frequently use the one-dimensional case for explanation."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we introduce our general position encoding method that involves three key properties within single model: the learnability and adaptability across different tasks; context extrapolation (i.e., generalization capability to sequences longer than those encountered during training); and extensibility to data of arbitrary dimensions with minimal additional manual efforts. 3.2 Sequential Position Encoding Our proposed SEQPE aims to maps an dimensional position into dimensional hidden vector using sequential Rd. As show in model parameterized by θ: fθ : Zn Fig. 1, the encoding process fθ includes: 1) representing the position using sequence of hidden vectors; 2) encoding the sequence into ep using sequential model. 2. For instance, extending the ROPE method from 1D to 2D requires careful expert design [19]. PREPRINT 4 ; θθθ) refers to our proposed sequential position en- ( is the concatenation where coder with trainable parameters θθθ, and operation between vectors. 3.3 Regularization for Embedding Distances Previous research reveals that sequential models often struggle to distinguish semantic differences between lexically similar sequences [2, 23]. Similarly, the representation space of the position encoder (i.e., Eq. (8)) may become biased toward positions with similar input sequences. To evaluate this limitation of sequential modeling, we visualize the dot product between position embeddings (from 0 to 2048) generated by non-regularized SEQPE, trained jointly with GPT-2 language model [35]. As shown in Fig. 2(a), in the case of 1-dimensional text data, positions separated by intervals of 100 or 1000 are often closer in the representation space than adjacent positions. This unnatural bias can negatively impact both training and extrapolation performance. Further implementation details for visualization are provided in Appendix B.2. To address this issue, we propose an objective to regularize the embedding distances of positions. Specifically, for given distance function on n-dimensional positions δ(p, p) : Zn R0, we assume that any positions and that are closer under δ are expected to yield more similar embeddings under the encoding function fθ. Inspired by contrastive learning [7, 11], our objective Lδ is designed as follows: Zn (cid:80) (9) log ep) Lδ = exp(ep ep+) pC exp(ep where Cδ is set of randomly collected positions and p+ = arg minp . We simply choose Euclidean distance for δ in this work. However, exploration for more distance functions, e.g., customized distance for interleaved data with varied dimensions, will be interesting for future works. In Fig. 2(b), we demonstrate that training Lδ get more reasonable GPT-2 language model [35] with representation space. δ(p, p) { Cδ} 3.4 Regularization for Out-of-Distribution Positions [0, L0), i=0 [0, Li) [0, L1) . } We first define the in-distribution training context as Dtrain = (cid:81)n1 Zn, which is the Cartesian product of sets of position ranges, where [0, Li) denotes the indistribution range along the i-th dimension. For example, the training positions for text data are drawn from [0, L0), while for 2D image data, they are drawn from [0, L0) [0, L1), i.e., (x, y) the set of positions { Although our regularization for embedding distances (Section 3.3) has the potential to generalize to unseen positions, the representation space for out-of-distribution (OOD) positions may be under-optimized for the main task objective. Specifically, the positions within the training context Dtrain can be optimized for both the main task (e.g., text perplexity) Lδ objectives, but positions outside the training and the context Dtrain are generally only optimized with respect to Lδ, lacking the regularization for main task performance. Here, we propose knowledge-distillation-based objecLOOD to gently bridge the gap between positions within tive and beyond the training context. Concretely, our idea is (a) SEQPE w/o Lδ (b) SEQPE w/ Lδ Fig. 2: Heatmap for the values of dot product between position embeddings from 0 to 2048 for text data. Brighter area indicate higher similarity between two position embeddings. The regularization objective Lδ makes SEQPE learn smoother representations. Implementation details are in Appendix B.2. 0, . . . , s0 , . . . sn1 k1, . . . , sn1 Position Representation Given an dimensional position p, e.g., (2, 3)-th patch of 2D image, we first convert it to concatenated left-padded sequence = (s0 k1 ), e.g., =(0, 2, 0, 3), where is the maximum number of digits for each dimension. The sub-sequence for each dimension with less than digits will be padded with 0 from the left-hand side to ensure that digits with the same place value (e.g., units, tens) always occupy the same position in the sequence, resulting in more consistent representation and improved training efficiency. For each token si in the sequence, we represent it using the addition of three embeddings: Ui = Tsi + Oj + Di, (b+1)d is the embedding matrix for all the where possible sequence tokens from positions3 and the special kd is the embedding for each tokens token [CLS], nd is the embedding position in the sequence, and for data dimensions. Notably, our system can represent (bk)n positions using + + embeddings. Thus, the number of embeddings required by our system grows logarithmically with the number of positions, whereas it grows linearly in nkd is the lookup-table-based methods [8, 35]. The hidden representation of the sequence s. As illustrated in Fig. 1, we set = 2, = 2, and = 10 to represent the positions of patches in 2D images from (0, 0) to (99, 99). The use case for 1-dimensional data is shown in Appendix A.1. Encoder Architecture To effectively represent positional information, we introduce sequential modelling network. Any sequential modeling network can be used for this task. In our work, we use lightweight Transformer model [46] with layers (e.g., = 2) and causal attention4. We add an additional [CLS] token at the end of each position sequence, and use its encoder output as the final representation for the position p: ep = (U T[CLS]; θθθ), (8) 3. The value of is determined by the base of the number system we choose; for example, is 10 in the decimal system and 16 in the hexadecimal system. We use the decimal system by default. 4. In our preliminary study, the bi-directional and causal attention (i.e., decoder-only) achieves almost the same performance PREPRINT 5 TABLE 1: Perplexity ( ) on the test set of Wikitext-103 with varied context lengths. We train language model from scratch on sequences with = 512 tokens. The best and second best results are represented by bold text and underline, respectively."
        },
        {
            "title": "Model",
            "content": "512 1K 2K 4K 8K 16K AVG. NOPE APE-SIN APE-LEARN ROPE-104 ROPE-106 ROPE-108 ALIBI 20.40 23.37 20.30 19.44 19.46 19.42 20.06 34.98 77.86 21.36 35.25 30.18 42.92 19.06 116.11 628.68 37.70 113.17 92.01 144.71 18.65 289.73 2.3e3 86.90 252.15 201.57 341.40 18. 536.29 5.0e3 152.53 477.89 332.04 541.17 19.36 881.56 9.4e3 230.63 745.52 539.20 786.08 21.39 313.17 2.9e3 91.57 273.90 202.41 371.25 19."
        },
        {
            "title": "SEQPE",
            "content": "19.65 18.74 18.51 18.58 18.86 19. 18.95 i=0 [zi, Li + zi) Dz = (cid:81)n1 to use the representations of positions in training context Dtrain to teach the positions in shifted region of OOD context , Dtrain = (z0, . . . , zn1), and zi is random shift for the start position at the i-th dimension. More concretely, we randomly p0, p1, . . . pm1} sample set of teacher positions { without replacement, where pj Dtrain, and calculate Rmm for teacher and student similarity matrices P, positions respectively: COOD = Zn, where Dz = , (cid:80)m Pi,j = exp(epi epj ) k=1 exp(epi exp(epi+z k=1 exp(epi+z where pj + = (pj,1 + z1, . . . , pj,n1 + zn1). Thereafter, we optimize our position encoder by the knowledge distillation objective: epk ) epj +z) epk+z) Si,j = (cid:80)m , LOOD = DKL(SG(P) S), (10) where DKL is the KullbackLeibler (KL) divergence and the SG( ) is the stop-gradient operation [45]. In addition, we find that training the main task model on shifted positions Dtrain for small portion of data can Dz rather than further minimize the LOOD and stabilize the extrapolation performance 5. Discussion We expect the LOOD objective to transfer the relative patterns of in-distribution PEs to out-of-distribution PEs, thereby enhancing extrapolation performance on longer contexts. However, trivial solution exists that minimizes LOOD by collapsing the embeddings, i.e., learning position embeddings ep to be identical to their shifted counterparts ep+z. This behavior undermines extrapolation, as it destroys meaningful relative positional differences. Fortunately, the embedding distance regularization term, Lδ in Equation 9, discourages such collapse. That is, for positions satisfying δ(p, + z) < δ(p, + z), i.e., is closer to + than to + z, the objective Lδ encourages exp(ep ep+z) > exp(ep As result, making ep+z identical to ep, which would imply ep+z), is penalized. Moreover, our exp(ep exp(ep ep+z). ep+z) 5. We do not train on larger context size, but only random shift of start position. Moreover, many popular relative PE methods are invariant to the shift [19, 33, 42], as explained in Appendix A.5, ensuring fair comparison. ablation study in Section 4.4 confirms that jointly optimizing Lδ leads to better extrapolation performance than LOOD and LOOD alone. optimizing 3.5 Training & Inference Training Here we explain how to train the SEQPE along with the main task model. Most methods that integrate position embeddings (PE) into the main task model are designed specific for their own PEs such as rotary operation in ROPE [42]. To verify the universality of our proposed scheme, we experimentally confirm that our method is not constrained by any specific integration approach. We abstract three types of general integration methods, i.e., ATTNSUM, ATTNMUL, and ATTNBIAS. For different tasks, we can flexibly select the most appropriate one based on the performance on the validation dataset. Specifically, given consecutive positions (e.g., from 0 to RLd L), we first encode them into embedding matrix using SEQPE, and then map to query and key position embeddings, respectively: Eq, Ek = EW{q,k} , (11) where W{q,k} Rd2d can be applied in each layer of the main task model, following the implementation in the Transformer model [46]. We leverage the Eq and Ek to implement three types of non-normalized attention scores between the i-th and j-th positions as follows: = (qi + eq )(kj + ek ) eq )(kj ek = (qi ) ek kj + eq = AAttnSum i,j AAttnMul i,j AAttnBias i,j (12) (13) (14) where is the Hadamard product. The un-normalized attention score matrices can be integrated into the Attention(A, V) in Equation 3. In addition, the ultimate objective for training with SeqPE is as follows: = Lmain + α Lδ + β L"
        },
        {
            "title": "LOOD",
            "content": "(15) where α and β are two hyper-parameters, and Lmain is the main task objective, e.g., cross entropy of the next-token prediction distribution [35] or image classification [9]. Inference Our SEQPE method is inference-friendly because the embeddings for identical positions can be pre-encoded PREPRINT 6 by the encoder only once. In other words, we can construct an efficient lookup table, as in standard methods, by pre-computing the embeddings for all positions. This not only reduces inference latency but also makes our method compatible with most existing inference engines for large language models [26, 52]."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conducted extensive experiments to evaluate the three motivations of our SEQPE, i.e., learnability and adaptability, extrapolation, and extension to data with varied dimensions. Most SEQPE setups remain consistent across experiments. When representing positions, we use decimal system (i.e., = 10). We represent positions for 1-dimensional text data using = 5 digits, and for 2-dimensional image data using = 2 digits per dimension. We use = 2 for the number of layers in architecture of the Transformer-based [46] position with weight decay value setting to 0. The weights encoder α and β in Equation 15 are both set to 0.1 based on validation performance. Both the sizes of sampled positions for Cδ COOD are set to 32. We set the batch sizes for the two and regularization objectives LOOD to 32 (distinct from Lδ and the main task). More implementation details are shown in Appendix A. Additionally, 10% of the main task data is trained with shifted start positions to reduce the knowledge distillation loss in Equation 10. This shift affects only the start position without changing context length, making it no-op for strong baselines like ROPE [19, 42] and ALIBI [33] (See Appendix A.5). 4.1 1D Task: Language Modeling Setup We first evaluate the performance of SEQPE on the language modeling task, where Lmain is the cross entropy of the next-token prediction distribution. We follow the setup in [33] to train language model for 100K steps with sequences of = 512 tokens on the training data of Wikitext-103, and evaluate it on the test set with extended sequences with [1024, 16384] tokens. We use huggingface/transformers6 as our code-base and GPT-2 model [35] as architecture for the main task. The best integration method of SEQPE is ATTNSCALAR on the validation set. In this task, we share the parameters of Wq,k in Equation 11 for all layers of the GPT-2 model. We compare SEQPE with strong position encoding baselines under the same setting: RoPE: Rotary positional encoding [42] is widely used in language models [44]. Previous studies find that its base value significantly impacts extrapolation [30, 48]. Therefore, we train three variants with base values 104, 106, and 108. ALiBi: ALIBI introduces hand-crafted scalar biases to attention weights [1, 33]. Despite its simplicity, it performs well in language modeling and extrapolation. APE: Absolute position encoding sums token and position embeddings. We compare two variants: APE-SIN with fixed sinusoidal embeddings [46], and APE-LEARN with learned embeddings [8, 35]. 6. https://github.com/huggingface/transformers NOPE: Recent work shows that causal attention can be effective without PEs, referred to as NOPE [24, 50]. When testing on longer contexts, most methods, except APE-LEARN that uses linear interpolation, generate new embeddings for unseen positions while keeping existing ones unchanged. More explanation for implementation details are in Appendix B.1. Results As shown in Table 1, our SEQPE demonstrates significant advantage in the average score, indicating stronger generalization capability on Wikitext-103 test set. The base value of ROPE affects its final performance, but its extrapolation performance is still limited compared with SEQPE and ALIBI. 4.2 1D Task: Fine-tuning for Long-context Question Answering Setup To assess the adaptability of various position encoding methods, we fine-tune the models from Section 4.1 on long-context QA task. Using the question answering (QA) prompt template in Fig. 4 (See Appendix) and the data pipeline from [20], we generate 86,820 SQuAD-based QA pairs [37], each with multi-document context of approximately 1024 tokens. Models are fine-tuned for 10K steps using cross-entropy loss on the answer span, and we select the best model based on validation perplexity. We evaluate on the long-context SQuAD dataset from the RULER benchmark [20] using two metrics. First, we compute perplexity of the gold answer span given the context and question. Then, for contexts ranging from 1024 to 8192 tokens, each model generates 16 tokens via greedy search, and we report the lower-cased Exact Match score against the gold answer. Additional implementation details are presented in Appendix B.3. Results As shown in Table 2, SEQPE achieves better average performance than both the ALIBI and ROPE baselines in terms of both perplexity and EM scores. Moreover, after switching to the QA downstream task, the extrapolation perplexity of ALIBI increases sharply when tested on sequences with 8K tokens, in contrast to its consistent performance on the pretraining task (See Section 4.1). In comparison, the perplexity of SEQPE remains relatively stable, demonstrating its strong adaptability. 4.3 2D Task: Image Classification Setup One intriguing property of SEQPE is that it can be easily extended to data with higher dimensions using the sequential representation as unified interface. Therefore, we further evaluate our method on the ImageNet-1K [40], which contains 1.28M training images and 50K validation images of 1,000 classes. We follow official settings in [43] to train VIT-S models using different position encoding Lmain is the cross entropy on the image methods. The classification distribution. The best integration method of SEQPE is ATTNMUL on the validation set. We compare SEQPE mainly with three groups of baselines: APE: The APE-LEARN with learnable parameters is the PE strategy in VIT [9]. We additionally evaluate the APE-SIN with expert-designed sinusoidal embeddings. PREPRINT 7 TABLE 2: Perplexity and Exact-Match (EM) score on the validation set of long-context RULER-SQuAD dataset [20]. We fine-tuned corresponding model in Section 4.1 on the RULER-SQuAD training dataset with 1024 context length. The perplexity is calculated only based on the golden-truth answer span after the input prompt. The EM score is based on the tokens generated by greedy search. The best and second best results are represented by bold text and underline, respectively."
        },
        {
            "title": "Model",
            "content": "1K 2K 4K 8K AVG. Perplexity / EM NOPE APE-SIN APE-LEARN ROPE-106 ALIBI 6.07 / 29.2 7.66 / 26.2 6.79 / 25.0 5.62 / 28.0 5.49 / 23.8 236.53 / 3.6 577.48 / 1.8 15.66 / 11.8 23.00 / 9.0 8.07 / 11.4 967.89 / 0.6 1.5e4 / 0.0 41.30 / 6.2 314.77 / 3.0 9.52 / 8.0 1.5e4 / 0.0 1.5e6 / 0.0 365.49 / 0.2 963.48 / 0.8 121.14 / 2.0 698.94 / 8.3 4.3e4 / 7.0 107.31 / 10.8 326.72 / 10.2 36.06 / 11."
        },
        {
            "title": "SEQPE",
            "content": "5.56 / 26.8 7.64 / 15.0 7.58 / 11.4 28.60 / 2.6 12.34 / 13.9 ) on ImageNet-1k. Results of APE-LEARN, ROPE2D, ROPE2D-MIX are reported from the model TABLE 3: Accuracy ( pre-trained by [19]. The training image resolution is 224 16, while the testing resolutions are 224 with patch size 16 672. The best and second best results are represented by bold text and underline, respectively. from 224 to"
        },
        {
            "title": "Model",
            "content": "NOPE APE-SIN APE-LEARN ROPE2D ROPE2D-MIX"
        },
        {
            "title": "SEQPE",
            "content": "224 78.8 80.9 80.4 80.9 80.9 81.2 320 79.2 50.3 80.6 81.5 82.2 81. 384 78.5 33.3 79.4 80.0 81.8 81.7 448 77.3 32.4 77.6 78.2 80.9 81. 512 75.8 15.2 75.4 76.1 79.1 80.5 640 71.7 8.8 70.3 67.8 71.6 77. 672 AVG. 70.5 8.5 69.1 65.0 68.3 76.7 75.9 32.7 76.1 75.6 77.8 80. RoPE2D: Since ROPE position encoding [42] is proposed for 1-dim data, Heo et al. [19] design ROPE2D position encoding method for image data by considering the rotary operation for x-axis and y-axis separately. They also propose variant ROPE2D-MIX with learnable frequencies. NOPE: Following the experiments in Section 4.1, we also train VIT without position encoding. We evaluate the extrapolation performance mainly following the setting in previous studies [10, 19]. In other words, 224 image resolution and we train the model with the 224 testing on varied resolutions from 224 672. 224 to 672 For all the image resolutions, each embedded patch represent 16 16 pixels. Results As shown in Table 3, SEQPE significantly outperform all baselines in terms of averaged accuracy for different resolutions. It is also interesting to find that the NOPE baseline achieves relatively stable performance when testing on larger resolutions. We note that ROPE2D-MIX exhibits superior performance with slight increases in input resolutions (e.g., 320 and 384), yet it fails to maintain these advantages at larger resolutions. As comparison, our SEQPE achieves consistent performance across various resolutions, demonstrating the superior generalization capability. 4.4 Ablation Study We first demonstrate the effect of the two regularization objectives. As shown in Table 4, enabling the distance Lδ alone (i.e., α = 0.1, β = 0.0) regularization objective noticeably improves extrapolation performance on both the TABLE 4: Ablation study for regularization objectives. We vary the values of α and β for LOOD, respectively, Lδ and while keeping the rest settings as same as those in Section 4.1 and Section 4.3 for text and image tasks. Language Modeling (Perplexity ) 512 19.57 19.62 19.57 19.65 1K 23.57 18.78 33.81 18.74 2K 60.51 18.56 104.07 18.51 4K AVG. 57.24 18.89 90.69 18.86 125.32 18.62 205.33 18. Image Classification (Accuracy ) 224 81.1 81.0 81.3 81.2 320 81.2 81.2 81.3 81.8 384 79.7 80.7 79.3 81."
        },
        {
            "title": "448 AVG.\n79.7\n77.1\n80.7\n80.0\n79.5\n76.1\n81.3\n80.5",
            "content": "β 0.0 0.0 0.1 0.1 0.0 0.0 0.1 0.1 α 0.0 0.1 0.0 0.1 0.0 0.1 0.0 0.1 language modeling and image classification tasks. When the out-of-distribution objective Lδ (i.e., α = 0.1, β = 0.1), the overall performance is further enhanced across both tasks. Interestingly, however, enabling LOOD alone (i.e., α = 0.0, β = 0.1) significantly degrades extrapolation performance, likely due to the potential embedding collapse discussed in Section 3.4."
        },
        {
            "title": "LOOD is used together with",
            "content": "In Table 5, we compare different integration methods under the image classification setting described in Section 4.3. We find that both ATTNMUL and ATTNBIAS achieve strong PREPRINT TABLE 5: Ablation study for integration methods on the image classification task. We only vary the integration methods while keeping rest settings the same as those in ) is reported. Section 4.3. Accuracy ("
        },
        {
            "title": "ATTNSUM\nATTNBIAS\nATTNMUL",
            "content": "224 80.9 80.8 81.2 320 80.8 80.9 81.8 384 79.8 80.1 81. 448 AVG. 78.5 79.2 81.2 80.0 80.2 81.4 extrapolation performance, with ATTNMUL performing slightly better. In contrast, ATTNSUM underperforms relative to the other methods, likely due to redundant correlation modeling between content and position embeddings [25, 42]. Additionally, in Fig. 3, we visualize the relationship between position embeddings (PEs) obtained from learnable position encoding methods, i.e., SEQPE and APELEARN, trained under the 2D positional setting described in Section 4.3. Specifically, we select five positions, four at the corners and one at the center, and compute the dot product between each of these selected positions and all other positions in the 2D image. Compared to the patterns produced by the APE method, we observe that SEQPE better captures the relative relationships between positions."
        },
        {
            "title": "5 RELATED WORK\n5.1 Position Encoding Schemes",
            "content": "In Transformer-based methods, the foundational work in position encoding includes Sinusoidal Encoding [46], which uses trigonometric functions to encode absolute positions while capturing relative relationships. Subsequent research has explored various variants of encoding positional information. From the perspective of input, it is common that the positional information is parameterized by employing lookup table. These methods can be broadly categorized into absolute position embeddings (APE) [8, 9, 38, 46, 47] (i.e., assigning unique embeddings to each absolute position) and relative position embeddings (RPE) [22, 36] (i.e., explicitly modeling relative distances between tokens). Specifically, for some methods [46], the parameters of PEs are fixed during model training, while others [8, 29] are learnable. However, most these methods are either trained on fixed-length sequences or lack inherent inductive biases for positional relationships (e.g., translation invariance), leading to poor extrapolation to longer sequences during inference. From the perspective of attention mechanism, injecting positional information by modifying attention scores with predefined learnable biases is also promising direction. For example, Shaw et al. [41] first introduce learnable embeddings for relative positions that are used to modify key and value vectors, or directly added to attention logits. T5 [36] bias method learns scalar bias for each relative distance bin, added to attention scores before softmax. ALIBI [1, 33] adds linear penalty to attention scores based on the relative distance between tokens. While more complex, DeBERTa [18] uses disentangled matrices to separately represent content and position, and the relative position 8 information also influences the attention scores in biaslike manner. Subsequent, Swin Transformer [29] creates learnable bias parameter table where the table size is determined by the possible range of relative coordinates within local windows. Notably, Rotary Position Embedding (RoPE) [42] emerges as popular alternatives, with RoPE integrating relative positional information through rotational transformation on the query and key vectors. LiePE [31] generalizes RoPE to high-dimensional rotation matrices by leveraging Lie group theory, which replaces RoPEs rotation matrices with learned, dense rotations derived from Lie group generators. Although many of those PEs offer significantly advantages in terms of extrapolation, efficiency and capturing relative positional information, they also come with several limitations: (1) They primarily model distance decay or local emphasis but may fail to capture more subtle, non-monotonic or directional positional relationships that learnable embeddings could represent. (2) Methods like ALIBI use fixed, non-learned functional form for the bias lack adaptability for different datasets, tasks [27, 53] or even different layers within the same model [16]. We have proved that SeqPE can significantly improve on these limitations. Among these aforementioned methods, FLOATER [28] is the most relevant work to ours, which used the Neural ODEs [5] to generate position embeddings that are integrated into each Transformer block. In comparison, our proposed SeqPE has three distinct advantages: (1) Comparing to the O(n) time complexity of FLOATER, our position encoder is lightweight module that barely increases the training and inference complexities. (2) We use explicit position rules to constrain PEs, offering better interpretability than the blackbox ODEs. (3) Unlike the carefully tuned layer-specific initial values in FLOATER, our method is insensitive to the initialization values. Moreover, we demonstrate that the proposed SeqPE is unified framework that can be applied to both the input embeddings and the attention mechanism. 5.2 Advances in Long-Context Modeling Recently, employing the large language model (LLM) to process text beyond their training sequence length or image beyond their training resolution, i.e., extrapolation, has become prominent research topic. Many research works attribute the poor extrapolate of LLMs to the position encoding [33, 51]. For example, Fourier Position Embedding (FoPE) [21] enhances the frequency-domain properties of attention to increase the models robustness against the spectrum damage, helping in better handling of long-range dependencies in sequence. Recently, CLEX [4] introduces continuous scaling framework based on ODEs to generalize positional embedding adjustments dynamically for ROPE, allowing LLMs to extrapolate context windows 48 beyond their training lengths. However, since these methods [4, 6, 32] rely on pre-specified context length to determine scaling factors, they may not generalize well to fractional or arbitrarily stretched context lengths, impairing the models understanding of token order. Moreover, in generative models [15, 44, 49], where the output sequence length is unknown in advance, such dependence can introduce challenges during inference, especially when using key-value caching in Transformer models [26]. In contrast to prior PREPRINT 9 SEQPE (0, 0) SEQPE (0, 13) SEQPE (7, 7) SEQPE (13, 0) SEQPE (13, 13) APE-LEARN (0, 0) APE-LEARN (0, 13) APE-LEARN (7, 7) APE-LEARN (13, 0) APE-LEARN (13, 13) Fig. 3: Heatmap for position embeddings of 2D images with 14 based on the 5 selected positions. Brighter area indices larger dot-product values. 14 patches. For each sub-figure, we plot the heatmap approaches, we propose knowledge distillation method that smoothly bridges the gap between positions within and beyond the training context length, preserving consistent positional embeddings during extrapolation across variablelength sequences. Some methods [10, 39, 54] improve the generalization of model on unseen positions by introducing randomness to PEs during training, preventing overfitting to specific context length or image resolitions. Similarly, we also introduce random shift for the starting position for better generalization in our proposed method. 5.3 Contrastive Learning in PEs. Contrastive learning is powerful technique used to learn robust and meaningful embeddings of data. The goal is to map data into an embedding space where similar items are close and dissimilar ones are far apart, which has achieved remarkable performance on various tasks [7, 11, 14, 17, 34]. As far as we know, we are the first to propose that when learning PEs, the sequential or spatial relationships between PEs are reinforced in self-supervised manner by introducing contrastive learning."
        },
        {
            "title": "6 CONCLUSION\nIn this work, we introduce a lightweight sequential position\nencoder to learn position embeddings in Transformer-based\nmodels, referred to as sequential position encoding (SEQPE).\nTo regularize the representation space of SEQPE, we propose\ntwo complementary objectives that aim to align embedding\ndistances with a predefined position-distance function and\nenhance the generalization for out-of-distribution positions.\nOn tasks involving one-dimensional data, i.e., language\njointly\nmodeling and question answering, our SEQPE,\ntrained with the main task model in an end-to-end manner,",
            "content": "demonstrates greater adaptability on downstream tasks and achieves stronger extrapolation performance. Furthermore, when transforming SEQPE from 1-dimensional tasks to the 2dimensional image classification, SEQPE outperforms strong baselines such as ROPE2D by large margin, with almost no manual architectural redesign. These experimental results validate that our method demonstrates superior learnability and adaptability across diverse tasks, exhibits robust context extrapolation capabilities that enable effective generalization to sequences significantly longer than training examples, and offers seamless extensibility to arbitrary-dimensional data with minimal manual intervention."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023. [2] Elron Bandel, Yoav Goldberg, and Yanai Elazar. Lexical generalization improves with larger models and longer training. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP, 2022. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 2020. [4] Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. CLEX: Continuous length PREPRINT extrapolation for large language models. In International Conference on Learning Representations (ICLR), 2024. [5] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. Advances in Neural Information Processing Systems (NeurIPS), 2018. [6] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [8] [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive In International learning of visual representations. Conference on Machine Learning (ICML), 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image In International Conference on recognition at scale. Learning Representations (ICLR), 2021. [10] Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, and Hongxia Yang. Vitar: Vision transformer with any resolution. arXiv preprint arXiv:2403.18361, 2024. [11] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. [12] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning (ICML), pages 12431252, 2017. [13] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [14] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 2020. [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional arXiv encodings still learn positional information. preprint arXiv:2203.16634, 2022. [17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised In Proceedings of the visual representation learning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. [18] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations (ICLR), 2020. [19] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transIn European Conference on Computer Vision former. (ECCV), 2024. [20] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the real context size of your longcontext language models? In First Conference on Language Modeling, 2024. [21] Ermo Hua, Che Jiang, Xingtai Lv, Kaiyan Zhang, Ning Ding, Youbang Sun, Biqing Qi, Yuchen Fan, Xuekai Zhu, and Bowen Zhou. Fourier position embedding: Enhancing attentions periodic extension for length generalization. arXiv preprint arXiv:2412.17739, 2024. [22] Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better relative position embeddings. arXiv preprint arXiv:2009.13658, 2020. [23] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017. [24] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems (NeurIPS), 2023. [25] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International Conference on Learning Representations. [26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [27] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, et al. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. [28] Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and ChoJui Hsieh. Learning to encode position for transformer In International with continuous dynamical model. Conference on Machine Learning (ICML), 2020. [29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using In Proceedings of the IEEE/CVF Inshifted windows. ternational Conference on Computer Vision (ICCV), 2021. PREPRINT 11 [30] Xin Men, Mingyu Xu, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng Chen. Base arXiv preprint of arXiv:2405.14591, 2024. rope bounds context length. [31] Sophie Ostmeier, Brian Axelrod, Michael Moseley, Akshay Chaudhari, and Curtis Langlotz. Liere: GenarXiv preprint eralizing rotary position encodings. arXiv:2406.10322, 2024. [32] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations. [33] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length In International Conference on Learning extrapolation. Representations (ICLR), 2021. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. [35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21:167, 2020. [37] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine In Jian Su, Kevin Duh, and comprehension of text. Xavier Carreras, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016. [38] Bin Ren, Yahui Liu, Yue Song, Wei Bi, Rita Cucchiara, Nicu Sebe, and Wei Wang. Masked jigsaw puzzle: versatile position embedding for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [39] Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2023. [40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015. [41] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Selfattention with relative position representations. arXiv preprint arXiv:1803.02155, 2018. [42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [43] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning (ICML), 2021. [44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in Neural Information Processing Systems (NeurIPS), 2017. [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems (NeurIPS), 2017. [47] Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. Encoding word order in complex embeddings. In International Conference on Learning Representations (ICLR), 2019. [48] Chen Wu and Yin Song. Scaling context, not parameters: Training compact 7b language model for efficient long-context processing. arXiv preprint arXiv:2505.08651, 2025. [49] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [50] Bowen Yang, Bharat Venkitesh, Dwarak Talupuru, Hangyu Lin, David Cairuz, Phil Blunsom, and Acyr Locatelli. Rope to nope and back again: new hybrid attention strategy. arXiv preprint arXiv:2501.18795, 2025. [51] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bin Qin, and Ting Liu. Length extrapolation of transformers: survey from the perspective of position encoding. arXiv preprint arXiv:2312.17044, 2023. [52] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 6255762583. Curran Associates, Inc., 2024. [53] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [54] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. PREPRINT"
        },
        {
            "title": "APPENDIX A\nIMPLEMENTATION DETAILS OF SEQPE",
            "content": "A.1 Example of 1D Position Representation 0 = 1, s0 1 = 2, . . . , s0 In Figure 1, we demonstrate how to represent 2-dimensional positions using our framework. Here, we further explain the use case for 1-dimensional positions. For instance, when representing the position 123 using our framework, we first convert it into sequence = (s0 = 3), where the maximum number of digits is set to 3, and the data dimension is 1. The embedding for the position R10d, which is the same as in the tokens is given by 2-dimensional case. The embedding for positions of tokens R3d, corresponding in the sequence is represented by to = 3. Finally, since = 1 in this case, we have R1d for data dimensions. However, the embedding because is the same for all positions in the 1-dimensional case, we simply fix the = 0 in practice. A.2 Regularization for Embedding Distances When implementing the contrastive-learning-based objective Lδ in Equation 9, the method used to construct the position Cδ with positions is critical. We implement two set methods for constructing Cδ and randomly select one of them for each training example. Suppose that we have Lδ, denoted as Lmax, which maximum position for training can be specified by the user. We first sample pivot position [0, Lmax). Global Random Sampling. We randomly sample positions that are lexically similar to p. This is achieved by first converting to string and then randomly applying one of the following operations: swapping two digits, removing additional digit, or adding digit. Next, we sample positions from [0, Lmax) to construct the set Cδ, ensuring that p+ sampled positions. Cδ is selected from the Local Random Sampling. We observe that the global sampling method may make it difficult for the encoder to distinguish positions that are far apart. For example, the encoder should learn that the embedding of position 16384 should be closer to 123 than to 122. To address this, we propose an alternative method: we first randomly sample small range (pleft, pleft + max(256, m)), then sample all positions from this local range. These sampling strategies can be applied in similar manner to data of arbitrary dimensionality. A.3 Regularization for Out-of-Distribution Positions p, . . . , en = SplitHeads(ep), where ei Lδ in Equation 10. We implement multi-head version of Following the multi-head operation in Transformer model Rd of position into [46], we split the embedding ep Rd/n. We heads: e1 construct the and similarity matrices for teacher and student positions individually for each head. The final loss of Equation 10 is averaged over all heads. Different from the objective Lδ in Equation 9 that we only want the overall embedding are close in the representation space, we hope the embeddings of student positions could learn similar patterns as those of teachers at fine-grained level. A.4 Balancing Position Training Frequencies Lδ and The training of LOOD relies on data randomly sampled from [0, Lmax). The choices of batch size for optimizing the two objectives will affect the training frequencies of each position given the fixed number of training steps. In our preliminary experiments, we find the batch sizes for Lδ and LOOD should be at least 16Lmax 10,000 . In the text and image tasks, 14, and the hyperthe training context length are 512 and 14 parameter Lmax for the two objectives are set to 20, 000 and 100 100, respectively. For simplicity, we set the batch sizes of the two objectives to 32 for both text and image tasks. A.5 Random Shift In our work, we randomly shift the start positions of small portion of the training data to stabilize the optimization of the position distillation objective Lδ in Equation 10. For example, we randomly change 10% of the training positions from (0, L) to [z, + z) for 1-dimensional data, where L) is is the training context length and randomly shifted start position. [0, Lmax Allowing this random shift is actually feature of our SEQPE. Since most learnable position embeddings are implemented via lookup tables, training on shifted positions is not feasible. For strong baselines such as ALIBI and ROPE, they are in fact relative position encoding methods and are invariant to such shifts. Specifically, the core of ROPE is the Euler-formula-based rotary matrix RΘ, under which Θ,iRΘ,j = RΘ,ji, as discussed in Section 2. Therefore, the position representations before and after shifting are identical for ROPE: Θ,iRΘ,j = RΘ,ji = RΘ,(j+z)(i+z) = Θ,i+zRΘ,j+z. Similarly, for ALIBI, according to Equation 6, we have: Mi,j = m(i j) = m(i + z) = Mi+z,j+z. Therefore, shifting is no-op for both ROPE and ALIBI. Here, we are not arguing that relative position encoding is inadequate, but rather aim for our model to learn everything directly from the data."
        },
        {
            "title": "APPENDIX B\nIMPLEMENTATION DETAILS OF TASKS",
            "content": "B.1 Language Modeling The batch size per device is set to 32 for training the language model [35] on sequences with 512 tokens. We train the model on 4 A6000 GPUs for 100K steps. The learning rate is 105, with 4000 warmup steps. We use linear learning 5 rate scheduler provided by huggingface/transformers. The dropout rate is set to 0.1. We employ mixed-precision training using the BF16 format. For evaluation, we compute perplexity on nonoverlapping chunks with varied numbers of tokens on the validation and test sets, following the setup of [33]. PREPRINT Answer the question based on the given documents. Only give me the answer and do not output any other words. The following are given documents. {MULTI_DOCS} Answer the question based on the given documents. Only give me the answer and do not output any other words. Question: {QUESTION} Answer: {ANSWER} Fig. 4: The placeholder MULTI_DOCS represents the long context with multiple Wikipedia documents, QUESTION is the user question, and ANSWER is the target answer. We evaluate performance by measuring the perplexity of the gold answer or generating hypothesis using the fine-tuned language model at the position of ANSWER. B.2 Visualization for Effect of Lδ We follow most of the setups in the language modeling task (i.e., Appendix B.1) to visualize the effect of Lδ. When Lδ, we set the α and β in Equation 15 to 0.1 and 0.0, using respectively. In contrast, both of them are set to 0.0 when Lδ is disabled. B.3 Long-context Question Answering We retain most of the training settings from Appendix B.1. However, for our QA task, we extend the training context length from 512 to 1024 to evaluate the adaptability of the PE methods. The pretrained model is fine-tuned for 10K steps, with 100 warmup steps. We select the best model based on the perplexity of answer spans on the validation dataset. B.4 Image Classification We follow the standard training recipe from [43] to train the VIT-S model, except that we extend the training to 400 epochs, as in [19]. Training is conducted on machines equipped with 4 A100 or A6000 GPUs, with batch size of 256."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Nara Institute of Science and Technology (NAIST)",
        "Tencent"
    ]
}