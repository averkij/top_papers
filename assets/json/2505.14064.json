{
    "paper_title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI",
    "authors": [
        "Cosmin I. Bercea",
        "Jun Li",
        "Philipp Raffler",
        "Evamaria O. Riedel",
        "Lena Schmitzer",
        "Angela Kurz",
        "Felix Bitzer",
        "Paula Roßmüller",
        "Julian Canisius",
        "Mirjam L. Beyrle",
        "Che Liu",
        "Wenjia Bai",
        "Bernhard Kainz",
        "Julia A. Schnabel",
        "Benedikt Wiestler"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Out-of-distribution detection identifies whether an input stems from an unseen distribution, while open-world recognition flags such inputs to ensure the system remains robust as ever-emerging, previously $unknown$ categories appear and must be addressed without retraining. Foundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging. However, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use. We therefore present $NOVA$, a challenging, real-life $evaluation-only$ benchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. Because NOVA is never used for training, it serves as an $extreme$ stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space. Baseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops across all tasks, establishing NOVA as a rigorous testbed for advancing models that can detect, localize, and reason about truly unknown anomalies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . e [ 1 4 6 0 4 1 . 5 0 5 2 : r NOVA: Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI Jun Li1 Cosmin I. Bercea1,3 Angela Kurz2 Felix Bitzer2 Che Liu5 Wenjia Bai5 Bernhard Kainz4,5 Philipp Raffler2 Evamaria O. Riedel2 Lena Schmitzer2 Julian Canisius2 Mirjam L. Beyrle2 Paula Roßmüller2 Julia A. Schnabel1,3,6 Benedikt Wiestler1,2 1Technical University of Munich 4FAU Erlangen-Nürnberg 2Klinikum Rechts der Isar 5Imperial College London cosmin.bercea@tum.de 3Helmholtz Center Munich 6Kings College London"
        },
        {
            "title": "Abstract",
            "content": "In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Out-of-distribution detection identifies whether an input stems from an unseen distribution, while open-world recognition flags such inputs to ensure the system remains robust as ever-emerging, previously unknown categories appear and must be addressed without retraining. Foundation and visionlanguage models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging. However, benchmarking these models on test sets with only few common outlier types silently collapses the evaluation back to closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use. We therefore present NOVA, challenging, real-life evaluation-only benchmark of 900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. Because NOVA is never used for training, it serves as an extreme stress-test of out-of-distribution generalisation: models must bridge distribution gap both in sample appearance and in semantic space. Baseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops across all tasks, establishing NOVA as rigorous testbed for advancing models that can detect, localize, and reason about truly unknown anomalies."
        },
        {
            "title": "Introduction",
            "content": "Generalization under distribution shift remains central unsolved challenge in machine learning [11, 47]. Despite advances in large-scale pretraining and transfer learning [10, 30], most models fail to reliably detect or reason about previously unseen categories or domains at test time. Anomaly detection, the task of identifying deviations from given normative distribution, e.g., samples exclusively from healthy patients, represents an extreme stress-test of out-of-distribution (OOD) generalization due to the open-ended and unpredictable nature of anomalies. While OOD generalization has been extensively studied in natural image classification [14, 31, 19], it remains underexplored in healthcare. Medical data presents extreme heterogeneity, rare event frequencies, and non-standardized acquisition protocols, making it worst-case scenario for evaluating model robustness to distribution shift. Detecting anomaliespotential pathologiesin imaging is often the first and most challenging step of the diagnostic process. Providing effective assistance to physicians at this stage has the potential to substantially improve clinical outcomes. Preprint. Under review. Figure 1: Overview of the NOVA benchmark. Task 1: Anomaly localization: models predict bounding boxes identifying abnormal regions in brain MRI; ground truth annotations from two independent radiologists are shown. Task 2: Image captioning: models generate brief diagnostic description from the MRI image. Task 3: Diagnostic reasoning: models predict the final diagnosis by integrating clinical history and image findings. NOVA establishes the first benchmark designed to systematically evaluate vision-language models (VLMs) and large language models (LLMs) for rare anomaly localization, clinical description, and multimodal diagnostic reasoning in brain MRI. In medical imaging, unsupervised anomaly detection (UAD) methods [34, 5] are trained exclusively on healthy anatomy and identify deviations from this learned distribution as potential pathologies. While recent methods demonstrate strong performance on curated benchmarks [49, 35, 9, 40, 36, 4], they remain insufficiently reliable in the wild, particularly in high-stakes settings like clinical triage and health screening, where specificity and robustness to rare clinical presentations are essential [3, 20]. This challenge is particularly acute in magnetic resonance imaging (MRI) of the brain, where radiologists must detect subtle and diverse abnormalities across patient populations and heterogeneous imaging protocols. The fundamental bottleneck lies in the datasets used for validation. Most existing benchmarks define anomalies through fixed categories, inducing implicit data leakage: although models are trained on healthy data, test sets remain constrained to known abnormality types. This narrows the evaluation to familiar distributions and undermines open-set detection. Datasets such as BraTS [26], ATLAS [23], and ISLES [15] were designed for segmentation and primarily capture canonical disease patterns, causing model development to converge on closed-set optimization rather than true discovery of unknown conditions. The medical out-of-distribution analysis challenge (MOOD) [48] introduced synthetic anomalies to simulate unknown deviations. However, real anomalies from rare or previously unobserved diseases remain essential for clinical relevance. fastMRI+ [45] provided incremental pathology variability through bounding box annotations of brain and knee MRI scans [44], yet lacked the pathology heterogeneity and structured clinical metadata necessary to reflect clinical variability. Detecting an abnormality alone does not satisfy clinical requirements. Radiologists must localize pathologically suspicious regions, assess severity, distinguish them from imaging artefacts, and formulate differential diagnosis based on patient history and imaging findings. No existing dataset reflects this full diagnostic workflow, limiting prior benchmarks to binary detection and systematically failing to capture clinically meaningful information in generated text or diagnostic predictions [24]. Data sharing restrictions and poor standardization have further constrained the development of vision-language models (VLMs) in medicine. Therefore, NOVA establishes new benchmark for evaluating, detecting, and reasoning on unexpected abnormalities in clinical brain MRI, as illustrated in Figure 1. The dataset comprises 906 brain MRI scans spanning 281 rare and diagnostically diverse pathologies from Eurorad, enriched with detailed clinical narratives. Each case is independently annotated by at least two radiologists with bounding boxes identifying suspected abnormalities. NOVA uniquely enables joint evaluation of anomaly localization, visual captioning, and diagnostic reasoning under real-world clinical heterogeneity. It 2 is explicitly designed as an evaluation-only benchmark to serve as an extreme stress-test of OOD generalization, requiring models to bridge distribution shifts in both visual and semantic space. We benchmark state-of-the-art vision-language models, including GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B, on NOVA. Results reveal substantial performance degradation across all tasks, underscoring the urgent need for benchmarks that reflect the demands of open-world clinical reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Anomaly detection, OOD detection, and novelty detection have received sustained focus in computer vision and machine learning, with advances across tasks from natural image understanding to industrial inspection [29, 22, 46, 42, 12, 33, 39]. Despite this progress, transferring these methods to medical imaging remains challenging. The concept of normality in medicine is inherently ambiguous, varying across individuals, imaging protocols, and institutions. Clinical anomalies are often rare and highly heterogeneous, making them ill-suited for evaluation protocols that treat selected set of predefined categories as representative out-of-distribution cases. The distinction between healthy and abnormal tissue is frequently subtle and localized, with considerable overlap between in-distribution and out-of-distribution regions within the same image. As result, approaches that excel on constrained datasets such as MVTec-AD [6] fail systematically under the extreme clinical variability of real-world neuroimaging [16]. In medical imaging, unsupervised anomaly detection models learn the normative distribution of healthy anatomy to identify deviations as candidate pathologies [5]. Large healthy population datasets, including IXI [1], CamCAN [38], and UK Biobank [37] are valuable for normative modeling and population studies, but they are ill-suited for evaluating anomaly detection, as they lack pathological cases and localized anomaly annotations. Datasets such as ADNI [28] and OASIS [25] focus exclusively on Alzheimers disease and neurodegeneration, providing only narrow coverage of pathologies. Similarly, condition-specific datasets, including MSLUB [21] for multiple sclerosis lesions, ATLAS [23] and ISLES [15] for Stroke lesions, and BraTS [26] for brain tumors, support segmentation of predefined abnormalities but offer no framework for open-set detection or evaluation of vision-language reasoning in clinical contexts. In parallel, large-scale vision-language datasets in medical imaging focus exclusively on chest radiographs. MIMIC-CXR [17] and PadChest [7, 8] integrate images with radiology reports for multimodal learning but are entirely disconnected from brain MRI. Hamamci et al. [13] introduced the CLM3D dataset and corresponding VLM3D challenge for developing generalist vision-language models in 3D medical imaging. However, CLM3D targets thoracic CT and focuses on common abnormality classification, report generation, and text-conditioned image synthesis, without addressing rare disease detection, anomaly localization, or open-world clinical reasoning. Despite the critical technical and clinical need, comprehensive neuroimaging benchmark remains absent. Brain MRI analysis presents significant technical challenges due to the wide spectrum of pathologies and their diverse appearances, ranging from localized lesions to diffuse structural alterations, coupled with inherent technical variability. Clinically, clear need exists as most rare diseases are neurological or have neurological manifestations [32], positioning brain MRI centrally in patient care. NOVA establishes the first rigorous benchmark for systematically evaluating these capabilities under the real-world variability and diagnostic uncertainty of clinical brain MRI."
        },
        {
            "title": "3 Dataset Description",
            "content": "We curated the NOVA dataset to establish an evaluation benchmark for vision-language model generalization under extreme clinical variability in brain MRI. We sourced cases from Eurorad, peerreviewed educational platform operating under Creative Commons Attribution-NonCommercialShareAlike 4.0 International License1. To comply with licensing requirements, we included only cases published after July 6, 2015. We filtered the dataset to include all cases from the Neuroradiology category and manually excluded non-relevant content such as CT, spine MRI, clinical photographs, and other non-MRI data. This ensured consistent imaging modality and anatomical focus. 1https://www.eurorad.org/node/38655 3 Figure 2: Representative brain MRI scans from the NOVA dataset illustrating the diversity of anatomical planes, MRI sequences, and pathological conditions. Radiologist-provided bounding box annotations are overlaid. The examples include rare congenital malformations, toxic and metabolic encephalopathies, and inflammatory or neoplastic lesionscapturing the broad radiological spectrum. We collected total of 906 brain MRI scans spanning 281 unique diagnoses. We retained all images in their original form without preprocessing, cropping, or normalization to preserve the full clinical variability essential for evaluation. We preserved the naturally imbalanced long-tailed distribution of rare diseases to reflect real-world case frequencies. Representative examples illustrating the diversity of imaging planes, sequences, and pathologies are shown in Figure 2. 3.1 Dataset Composition NOVA captures the diagnostic heterogeneity of clinical brain MRI. We included axial, sagittal, and coronal planes across standard sequences, including T1-weighted, T2-weighted, and FLAIR imaging. We manually grouped cases into six diagnostic categories: neoplastic, neurodegenerative, inflammatory, congenital, metabolic, and vascular pathologies. Figure 3a) shows the long-tailed distribution of diseases and the rarity of many conditions, which present unique challenges for model evaluation. The datasets 281 unique diagnosis labels exceed the diversity of existing brain MRI benchmarks by an order of magnitude. We summarize additional statistics on patient demographics, sequence types, and imaging parameters in Appendix. 3.2 Annotation Process and Quality Control We implemented rigorous multi-stage protocol to obtain high-quality anomaly localization annotations. Eight neuroradiology residents annotated the dataset using custom web-based platform (Appendix). Each case was independently labeled by two readers, who reviewed the full original Eurorad clinical description and associated metadata to inform their annotations. Inter-rater agreement was computed using greedy matching algorithm that maximized intersection over union (IoU) between boxes. Annotations with IoU > 0.3 were merged into consensus labels. For 247 cases with persistent disagreement, senior board-certified neuroradiologist (15+ years experience) adjudicated the final ground truth. Figure 3b shows the distribution of inter-reader agreement, illustrating the inherent variability in clinical anomaly localization. Figure 3c presents the overall IoU distribution across annotations, highlighting the diagnostic ambiguity and challenges of consistently identifying anomalies in realworld clinical imaging. Figure 3: Dataset composition and annotation quality in NOVA. (a) Distribution of cases across six diagnostic categories. (b) Inter-rater agreement as mean intersection over union (IoU) between radiologist pairs. (c) Histogram of IoU scores across all scans. 3.3 Data Format and Benchmark Design We release all brain MRI scans as uniformly sized 480480 grayscale PNG slices. We provide accompanying clinical metadata, including clinical history, patient demographics, imaging information, radiologist image captions, and bounding boxes for detected abnormalities in csv files. We explicitly designed NOVA as an evaluation-only dataset. Each case represents unique diagnosis, and we do not provide predefined train, validation, or test splits. This enforces zero-shot evaluation setting for all models, requiring them to generalize to previously unseen cases. We publicly release NOVA on Hugging Face Datasets2 under the same Creative Commons AttributionNonCommercial-ShareAlike 4.0 license as the Eurorad source. The dataset is distributed solely for non-commercial research to enable reproducible evaluation of vision-language models under realistic clinical conditions."
        },
        {
            "title": "4 Benchmark Tasks",
            "content": "NOVA defines comprehensive evaluation suite to assess the capabilities of vision-language models under realistic and clinically relevant conditions. These three tasks defined here reflect the sequential decision-making process of radiologists, progressing from anomaly localization to image description and diagnostic interpretation, enabling realistic benchmarking. 4.1 Task 1: Anomaly Localization This task requires models to detect and localize abnormalities within brain MRI scans, regardless of the patients eventual diagnosis. Clinically, this is relevant task, as most medical errors actually stem from not seeing pathology at all [18]. Models must predict one or more bounding boxes per image corresponding to abnormal regions, using radiologist-annotated ground truth as reference. Performance is measured using mean average precision at intersection over union thresholds of 0.3 (mAP@30), 0.5 (mAP@50), and the COCO-style averaged mAP across thresholds from 0.50 to 0.95 in 0.05 increments (mAP@[50:95]). The benchmark also reports the number of correctly detected versus missed pathologies per case to reflect the clinical priority of minimizing false negatives. The 2hhttps://huggingface.co/datasets/Ano-2090/Nova 5 dataset includes cases with multiple annotated abnormalities, providing uniquely difficult evaluation setting for object detection under open-world conditions and rare disease variability. 4.2 Task 2: Image Description This task measures the ability of models to generate clinically meaningful descriptions of brain MRI scans, an important prerequisite for making the correct diagnosis and in clinical communication. Each image is paired with an expert-generated caption describing the imaging findings. Evaluation uses case-insensitive exact keyword matching to compute precision, recall, and F1-score across the full keyword set. Modality-specific terms (such as flair, axial, sagittal, t1, t2, coronal, dwi, t1w, t2w, weighted) are evaluated separately from non-modality keywords capturing clinical content. Binary classification accuracy and F1-score for normal versus abnormal classification are also reported. Sentence-level generation quality is evaluated using BLEU [27] and METEOR [2]. 4.3 Task 3: Diagnostic Reasoning This task tests whether models can integrate clinical context and imaging observations to predict diagnosis, arguably the \"supreme discipline\" in medical decision-making. Each case provides brief clinical history and corresponding image caption as input, and the model must generate free-text diagnostic label. Performance is reported as Top-1 accuracy (exact match with ground truth) and Top-5 accuracy (ground truth among the five most likely predictions). As model outputs are unconstrained free text, GPT-4o is used to perform semantic matching between predictions and ground truth labels. The task demands multimodal reasoning and open-ended prediction and is performed in zero-shot setting, closely mirroring real-world clinical decision-making workflows."
        },
        {
            "title": "5 Experiments and Results",
            "content": "We benchmarked large vision-language models on NOVA to systematically test their ability to generalize under extreme clinical heterogeneity. All experiments were conducted in inference-only mode. We report results for Gemini 2.0 Flash (Google DeepMind), Qwen2-VL-72B (Alibaba DAMO Academy), and Qwen2.5-VL-72B for abnormality grounding; and GPT-4o (OpenAI), Gemini 2.0 Flash, and Qwen2.5-VL-72B-Instruct for image captioning and diagnostic reasoning. As these models are proprietary and their training data is undisclosed, Eurorad cases may have been partially included. Results should thus be interpreted as an upper bound on zero-shot generalization. We encourage future evaluation of open models for more conservative assessment. We designed NOVA to expose generalization failures in models confronted with previously unseen, rare clinical cases. To do so, we evaluate along three critical axes of clinical reasoning: localization, description, and diagnosis. 5.1 Stress Test 1: Localization under Clinical Heterogeneity The anomaly localization task revealed strong performance degradation. While large visionlanguage models achieve detection scores of 73%92% [41] on natural image benchmarks such as RefCOCO [43], performance on NOVA dropped sharply to 8.3%28.5%. Models were evaluated using standard object detection metrics (mAP@30, mAP@50, and mAP@[50:95]), as summarized in Table 1. Despite occasional correct detections, all models exhibited poor calibration under clinical distribution shift, frequently producing incomplete, misplaced, or spurious bounding boxes. Clinical inspection of representative cases (Figure 4) revealed typical failure patterns such as false-positive detection of normal anatomical structures (e.g., orbital cavity misinterpreted as lesion by Qwen2.5VL) and failure to localize true abnormalities even under relaxed overlap thresholds. Quantitatively, even at 30% IoU criteria, fewer than half of ground truth abnormalities were detected, and over 600 false-positive boxes were recorded. In clinical practice, missed anomalies risk delayed or missed diagnoses, while high false-positive rates drive unnecessary specialist referrals, patient anxiety, and increased healthcare costs. NOVA sets the first extreme benchmark designed to systematically expose these critical failure modes. 6 Table 1: Localization performance on NOVA. We evaluate the models with standard object detection metrics (mAP at multiple thresholds), detection accuracy (ACC50), number of true positives (TP30), and number of false positives (FP30)."
        },
        {
            "title": "Model",
            "content": "mAP30 mAP50 mAP50-95 ACC50 TP30 Gemini 2.0 Flash Qwen2-VL-72B Qwen2.5-VL-72B 20.16 25.02 37.66 7.37 15.09 24.49 1.99 6.44 11. 8.83 25.50 28.48 227/1068 338/1068 406/1068 FP30 899 1163 672 Figure 4: Examples of model predictions for anomaly grounding on NOVA. Ground truth and modelpredicted bounding boxes are shown for Gemini 2.0 Flash, Qwen2.0-VL-72B, and Qwen2.5-VL-72B. 5.2 Stress Test 2: Description under Semantic Shift The second test probes whether models can generate clinically meaningful image descriptions under severe semantic shifts. Table 2 summarizes performance across Clinical Term F1, Modality Term F1, BLEU, METEOR, and binary abnormality classification accuracy. Gemini 2.0 Flash achieved the highest Clinical Term F1 (19.8%), Modality Term F1 (59.8%), and BLEU (1.83), reflecting comparatively stronger recognition of structured imaging attributes. GPT-4o outperforms in Binary F1 (11.3%) and METEOR (17.5), indicating slightly better fluency and sentence-level description. Qwen2.5-VL-72B-Instruct underperformed across all metrics. To further investigate the semantic limitations observed in captioning  (Table 2)  , we analyzed language behavior across models in Figure 5. Example cases illustrate that all models tend to produce longer but vaguer descriptions under uncertainty. The ground truth showed the highest vocabulary size (1527 unique words), reflecting expert use of precise diagnostic terminology. VLMs exhibited drastic vocabulary compression, with GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL using only 647, 437, and 595 unique words, respectively. Interestingly, models showed comparable or slightly higher sentence diversity (895, 785, and 854 unique captions vs. 729 in the ground truth), likely due to paraphrasing and verbose redundancy. Caption length distribution revealed distinct patterns: Gemini produced captions with similar lengths to ground truth, while GPT-4o and Qwen2.5-VL consistently generated longer outputs. This analysis highlights consistent pattern of low lexical precision and repetitive verbosity, particularly for GPT-4o and Qwen2.5-VL, confirming that current models exhibit limitations in expressing clinically meaningful descriptive detail in image captioning tasks under real-world clinical distribution shifts. 7 Table 2: Image Description on NOVA. Captioning quality is evaluated by Clinical Term F1, Modality Term F1, BLEU, and METEOR. Binary F1 measures binary abnormality classification performance. Model Clinical F1 (%) Modality F1 (%) BLEU METEOR Binary F1 (%) Gemini 2.0 Flash GPT-4o Qwen2.5-VL-72B 19.8 15.7 13.6 59.8 49.3 45.3 1.83 0.92 1.08 15.2 17.5 17.1 5.3 11.3 2.4 Figure 5: Task2: Image Captioning. (Top) Example image-caption pairs showing model predictions and reference ground truth. Model outputs tend toward verbose, redundant phrasing with fewer specialized terms. (Bottom) Quantitative analysis. Left: Vocabulary size (unique words). Center: Sentence diversity (unique captions). Right: Caption length distribution (number of words per caption). Ground truth radiology reports exhibit the highest vocabulary richness but shorter, information-dense sentences. All models display severe vocabulary collapse and compensate with longer and more varied sentence constructions. 5.3 Stress Test 3: Diagnostic Reasoning under Distributional Shift The final test evaluates models ability to assign diagnostic labels based on combined image captions and clinical history. Performance was assessed via Top-1 and Top-5 classification accuracy  (Table 3)  . GPT-4o achieved the highest scores (24.2% Top-1, 38.4% Top-5), with Gemini 2.0 Flash and Qwen2.5-VL-72B performing lower. To further probe diagnostic behavior, we analyzed prediction distributions against ground truth (Figure 6). All models followed the expected Zipfian-like scaling of disease frequencies (right), demonstrating comparable rank-frequency slopes to ground truth. However, this occurred over substantially compressed label space, with model predictions collapsing onto smaller vocabularies covering only 30% of the ground truth  (Table 3)  . This truncation effect was also visible in the cumulative frequency curves (left), where model predictions saturated rapidly relative to ground truth. Ground truth labels exhibited Shannon entropy of 8.68 bits, reflecting the high uncertainty and diversity of rare disease distributions. In contrast, model outputs showed marked entropy reduction of approximately 1 bit  (Table 3)  , consistent with over-reliance on dominant classes and poor exploration of the long tailan effect akin to premature entropy collapse under distributional shift. NOVA provides the first large-scale benchmark to systematically diagnose and quantify failure modes, and to encourage the development of more robust vision-language reasoning under clinical distribution shift. Table 3: Diagnostic reasoning results on NOVA. Diagnostic accuracy is captured by the Top-1 and Top-5 accuracy. Coverage and entropy are extracted from diagnostic reasoning distributions. Model Top-1 Top-5 Cov. Ent. 22.1 Gemini 2.0 Flash 24.2 GPT-4o Qwen2.5-VL-72B 22.4 37.4 29.4 7.71 38.4 31.9 7.64 35.2 26.1 7.26 Figure 6: Distribution of diagnostic label frequencies for ground truth vs model predictions."
        },
        {
            "title": "6 Discussion",
            "content": "NOVA introduces new benchmark paradigm for evaluating anomaly detection and multimodal reasoning in clinical brain MRI. Its design offers several key advantages. First, NOVA provides one of the largest and most diverse expert-annotated collections of brain MRI scans available, covering approximately 900 scans with over 280 distinct diagnoses and rare pathological conditions. Second, the dataset uniquely integrates multimodal annotations, combining radiologist-drawn bounding boxes, expert-generated image captions, and detailed clinical histories. This comprehensive structure enables systematic evaluation of detection, description, and diagnostic reasoning within single resource. Third, the dataset reflects real-world clinical variability by using actual patient cases rather than synthetic perturbations, creating challenging and realistic testbed. Finally, the structured multireader annotation protocol with adjudication by senior neuroradiologist ensures high level of annotation quality and reliability. Despite these advantages, NOVA has limitations that are important to acknowledge. The dataset is sourced from European radiology teaching repository, which may introduce geographic or demographic biases that could affect model generalization in other healthcare systems. Additionally, NOVA provides only 2D image slices rather than full 3D volumes. While this choice may constrain certain volumetric analyses, the decision to release data in 2D format was deliberate: most standard machine learning and computer vision tools and libraries offer limited support for 3D medical imaging, which can significantly slow down experimentation and accessibility for the broader research community. Finally, NOVA is released as an evaluation-only benchmark, not intended for supervised model training. This design reflects the realities of rare disease imaging, where collecting sufficient labeled data for training is often infeasible and where true generalization must be tested without model adaptation. Looking forward, we plan to maintain NOVA as dynamic benchmark and to open public leaderboard to encourage continuous community participation and advancement of the state of the art. Given the datasets focus on rare diseases and its intended role as an inference benchmark, we do not envision extensions to fine-tuning tasks or inclusion of 3D imaging data. Instead, we anticipate that NOVA will catalyze the development of next-generation foundation models and vision-language systems capable of performing robust anomaly detection and clinical reasoning under realistic open-set clinical conditions."
        },
        {
            "title": "7 Conclusion",
            "content": "We present NOVA, the first large-scale, expert-annotated benchmark dataset for anomaly localization, clinical captioning, and diagnostic reasoning in brain MRI. NOVA provides uniquely challenging and clinically grounded resource, combining real-world imaging variability with high-quality multimodal annotations. By releasing NOVA to the community, we aim to establish new standard for evaluating the robustness and generalization of models for clinical anomaly detection and multimodal medical reasoning. We invite the research community to engage with NOVA and drive the development of next-generation models capable of detecting the unknown in clinical imaging."
        },
        {
            "title": "References",
            "content": "[1] Ixi dataset. https://brain-development.org/ixi-dataset/. Accessed: 2023-02-15. [2] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. [3] Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert, and Julia Schnabel. Generalizing unsupervised anomaly detection: Towards unbiased pathology screening. In Medical Imaging with Deep Learning, 2023. [4] Cosmin Bercea, Benedikt Wiestler, Daniel Rueckert, and Julia Schnabel. Diffusion models with implicit guidance for medical anomaly detection. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 211220. Springer, 2024. [5] Cosmin Bercea, Benedikt Wiestler, Daniel Rueckert, and Julia Schnabel. Evaluating normative representation learning in generative ai for robust anomaly detection in brain imaging. Nature Communications, 16(1):1624, 2025. [6] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad comprehensive real-world dataset for unsupervised anomaly detection. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95849592, 2019. [7] Agustín Bustos, Antonio Pertusa, Jose M. Salinas, and María de la Iglesia-Vayá. Padchest: large chest x-ray image dataset with multi-label annotated reports. Medical Image Analysis, 66:101797, 2020. [8] Daniel Castro, Aurelia Bustos, Shruthi Bannur, Stephanie Hyland, Kenza Bouzid, Maria Teodora Wetscherek, Maria Dolores Sánchez-Valverde, Lara Jaques-Pérez, Lourdes Pérez-Rodríguez, Kenji Takeda, et al. Padchest-gr: bilingual chest x-ray dataset for grounded radiology report generation. arXiv preprint arXiv:2411.05085, 2024. [9] Xiaoran Chen, Suhang You, Kerem Can Tezcan, and Ender Konukoglu. Unsupervised lesion detection via image restoration with normative prior. Medical Image Analysis, 64:101713, 2020. [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [11] Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin Wei, Weidi Xie, and Lin Ma. Promptdet: Towards open-vocabulary detection using uncurated images. In European conference on computer vision, pages 701717. Springer, 2022. [12] Jia Guo, Shuai Lu, Lize Jia, Weihang Zhang, and Huiqi Li. Recontrast: Domain-specific anomaly detection via contrastive reconstruction. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1072110740, 2023. [13] Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, et al. Developing generalist foundation models from multimodal dataset for 3d computed tomography. arXiv preprint arXiv:2403.17834, 2024. [14] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018. [15] Moritz Hernandez Petzsche, Ezequiel de la Rosa, Uta Hanning, Roland Wiest, Waldo Valenzuela, Mauricio Reyes, Maria Meyer, Sook-Lei Liew, Florian Kofler, Ivan Ezhov, et al. Isles 2022: multi-center magnetic resonance imaging stroke lesion segmentation dataset. Scientific Data, 9(1):762, 2022. 10 [16] Zesheng Hong, Yubiao Yue, Yubin Chen, Lele Cong, Huanjie Lin, Yuanmei Luo, Mini Han Wang, Weidong Wang, Jialong Xu, Xiaoqi Yang, et al. Out-of-distribution detection in medical image analysis: survey. arXiv preprint arXiv:2404.18279, 2024. [17] Alistair E. W. Johnson, Tom J. Pollard, Seth J. Berkowitz, et al. MIMIC-CXR, de-identified publicly available database of chest radiographs with free-text reports. Scientific Data, 6:317, 2019. [18] Y. W. Kim and L. T. Mansfield. Fool me twice: Delayed diagnoses in radiology with emphasis on perpetuated errors. AJR. American Journal of Roentgenology, 202(3):465470, 2014. [19] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. In Marina Meila and Tong Zhang, Wilds: benchmark of in-the-wild distribution shifts. editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 56375664. PMLR, 1824 Jul 2021. [20] Seungjun Lee, Boryeong Jeong, Minjee Kim, Ryoungwoo Jang, Wooyul Paik, Jiseon Kang, Won Chung, Gil-Sun Hong, and Namkug Kim. Emergency triage of brain computed tomography via anomaly detection with deep generative model. Nature Communications, 13:4251, 07 2022. [21] Žiga Lesjak, Alina Galimzianova, Andrej Koren, et al. novel public MR image dataset of multiple sclerosis patients with lesion segmentations based on multi-rater consensus. Neuroinformatics, 16(1):5163, 2018. [22] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. Cutpaste: Self-supervised learning for anomaly detection and localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96649674, 2021. [23] Sook-Lei Liew, Bethany P. Lo, ., and et al. Miarnda R. Donnelly. large, curated, open-source stroke neuroimaging dataset to improve lesion segmentation algorithms. Scientific Data, 9, 2022. [24] Faisal Mahmood. benchmarking crisis in biomedical machine learning. Nature Medicine, 31(4):10601060, 2025. [25] Daniel S. Marcus, Aditya F. Fotenos, John G. Csernansky, John C. Morris, and Randy L. Buckner. Open access series of imaging studies: longitudinal MRI data in nondemented and demented older adults. Journal of Cognitive Neuroscience, 22(12):26772684, 2010. [26] Bjoern H. Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, Levente Lanczi, Elizabeth Gerstner, Marc-André Weber, Tal Arbel, Brian B. Avants, Nicholas Ayache, Patricia Buendia, D. Louis Collins, Nicolas Cordier, Jason J. Corso, Antonio Criminisi, Tilak Das, Hervé Delingette, Çagatay Demiralp, Christopher R. Durst, Michel Dojat, Senan Doyle, Joana Festa, Florence Forbes, Ezequiel Geremia, Ben Glocker, Polina Golland, Xiaotao Guo, Andac Hamamci, Khan M. Iftekharuddin, Raj Jena, Nigel M. John, Ender Konukoglu, Danial Lashkari, José António Mariz, Raphael Meier, Sérgio Pereira, Doina Precup, Stephen J. Price, Tammy Riklin Raviv, Syed M. S. Reza, Michael Ryan, Duygu Sarikaya, Lawrence Schwartz, Hoo-Chang Shin, Jamie Shotton, Carlos A. Silva, Nuno Sousa, Nagesh K. Subbanna, Gabor Szekely, Thomas J. Taylor, Owen M. Thomas, Nicholas J. Tustison, Gozde Unal, Flor Vasseur, Max Wintermark, Dong Hye Ye, Liang Zhao, Binsheng Zhao, Darko Zikic, Marcel Prastawa, Mauricio Reyes, and Koen Van Leemput. The multimodal brain tumor image segmentation benchmark (brats). IEEE Transactions on Medical Imaging, 34(10):19932024, 2015. [27] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. 11 [28] Ronald C. Petersen, Paul S. Aisen, Laurel A. Beckett, Michael C. Donohue, Anthony C. Gamst, Danielle J. Harvey, Clifford R. Jr. Jack, William J. Jagust, Leslie M. Shaw, Arthur W. Toga, John Q. Trojanowski, and Michael W. Weiner. Alzheimers disease neuroimaging initiative (adni): clinical characterization. Neurology, 74(3):201209, January 2010. [29] Stanislav Pidhorskyi, Ranya Almohsen, and Gianfranco Doretto. Generative probabilistic novelty detection with adversarial autoencoders. Advances in neural information processing systems, 31, 2018. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PmLR, 2021. [31] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In Proceedings of the International Conference on Machine Learning, pages 53895400. PMLR, 2019. [32] Carola Reinhard, Anne-Catherine Bachoud-Lévi, Tobias Bäumer, Enrico Bertini, Alicia Brunelle, Annemieke I. Buizer, Antonio Federico, Thomas Gasser, Samuel Groeschel, Sanja Hermanns, Thomas Klockgether, Ingeborg Krägeloh-Mann, G. Bernhard Landwehrmeyer, Isabelle Leber, Alfons Macaya, Caterina Mariotti, Wassilios G. Meissner, Maria Judit Molnar, Jorik Nonnekes, Juan Dario Ortigoza Escobar, Belen Pérez Dueñas, Lori Renna Linton, Ludger Schöls, Rebecca Schuele, Marina A. J. Tijssen, Rik Vandenberghe, Anna Volkmer, Nicole I. Wolf, and Holm Graessner. The european reference network for rare neurological diseases. Frontiers in Neurology, Volume 11 - 2020, 2021. [33] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1431814328, 2022. [34] Lukas Ruff, Jacob Kauffmann, Robert Vandermeulen, Grégoire Montavon, Wojciech Samek, Marius Kloft, Thomas Dietterich, and Klaus-Robert Müller. unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756795, 2021. [35] Thomas Schlegl, Philipp Seeböck, Sebastian Waldstein, Georg Langs, and Ursula SchmidtErfurth. f-anogan: Fast unsupervised anomaly detection with generative adversarial networks. Medical Image Analysis, 54:3044, 2019. [36] Hannah Schlüter, Jeremy Tan, Benjamin Hou, and Bernhard Kainz. Natural synthetic anomalies for self-supervised anomaly detection and localization. In European Conference on Computer Vision, pages 474489. Springer, 2022. [37] Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, et al. Uk biobank: An open access resource for identifying the causes of wide range of complex diseases of middle and old age. PLoS Medicine, 12(3):e1001779, 2015. [38] Jason R. Taylor, Nitin Williams, Rhodri Cusack, Tibor Auer, Meredith A. Shafto, Marie Dixon, Lorraine K. Tyler, Cam-CAN, and Richard N. Henson. The cambridge centre for ageing and neuroscience (cam-can) data repository: Structural and functional mri, meg, and cognitive data from cross-sectional adult lifespan sample. NeuroImage, 144:262269, 2017. Data Sharing Part II. [39] Han Wang and Yixuan Li. Bridging ood detection and generalization: graph-theoretic view. Advances in Neural Information Processing Systems, 2024. [40] Julia Wolleb, Florentin Bieder, Robin Sandkühler, and Philippe Cattin. Diffusion models for medical anomaly detection. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 3545. Springer, 2022. [41] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 12 [42] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, WENXUAN PENG, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan Hendrycks, Yixuan Li, and Ziwei Liu. Openood: Benchmarking generalized out-ofdistribution detection. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3259832611, 2022. [43] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European Conference on Computer Vision, pages 6985. Springer, 2016. [44] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, et al. fastmri: An open dataset and benchmarks for accelerated mri. arXiv preprint arXiv:1811.08839, 2018. [45] Ruiyang Zhao, Burhaneddin Yaman, Yuxin Zhang, Russell Stewart, Austin Dixon, Florian Knoll, Zhengnan Huang, Yvonne Lui, Michael Hansen, and Matthew Lungren. fastmri+, clinical pathology annotations for knee and brain fully sampled magnetic resonance imaging data. Scientific Data, 9(1):152, 2022. [46] Haotian Zheng, Qizhou Wang, Zhen Fang, Xiaobo Xia, Feng Liu, Tongliang Liu, and Bo Han. Out-of-distribution detection learning with unreliable out-of-distribution sources. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 7211072123, 2023. [47] Chaoyang Zhu and Long Chen. survey on open-vocabulary detection and segmentation: Past, present, and future. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [48] David Zimmerer, Peter Full, Fabian Isensee, Paul Jäger, Tim Adler, Jens Petersen, Gregor Köhler, Tobias Ross, Annika Reinke, Antanas Kascenas, et al. Mood 2020: public benchmark for out-of-distribution detection and localization on medical images. IEEE Transactions on Medical Imaging, 41(10):27282738, 2022. [49] David Zimmerer, Fabian Isensee, Jens Petersen, Simon Kohl, and Klaus Maier-Hein. Unsupervised anomaly localization using variational auto-encoders. In Medical Image Computing and Computer Assisted InterventionMICCAI 2019: 22nd International Conference, Shenzhen, China, October 1317, 2019, Proceedings, Part IV 22, pages 289297. Springer, 2019."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "C.I.B. is funded via the EVUK program (Next-generation AI for Integrated Diagnostics) of the Free State of Bavaria."
        }
    ],
    "affiliations": [
        "FAU Erlangen-Nürnberg",
        "Helmholtz Center Munich",
        "Imperial College London",
        "Kings College London",
        "Klinikum Rechts der Isar",
        "Technical University of Munich"
    ]
}