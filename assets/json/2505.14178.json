{
    "paper_title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits",
    "authors": [
        "Xiang Zhang",
        "Juntai Cao",
        "Jiaqi Wei",
        "Yiwei Xu",
        "Chenyu You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 8 7 1 4 1 . 5 0 5 2 : r Tokenization Constraints in LLMs: Study of Symbolic and Arithmetic Reasoning Limits Xiang Zhang1 Juntai Cao1 Jiaqi Wei2 Yiwei Xu3 Chenyu You4 1 University of British Columbia 2 Zhejiang University 3 Cisco 4 Stony Brook University xzhang23@ualberta.ca, jtcao7@cs.ubc.ca, chenyu.you@stonybrook.edu"
        },
        {
            "title": "Abstract",
            "content": "Tokenization is the firstand often underappreciatedlayer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations. Full code, prompts, and results are available at Anonymous GitHub."
        },
        {
            "title": "Introduction",
            "content": "Inductive reasoning and arithmetic computation, such as counting, addition, and pattern generalization, are foundational components of symbolic and algorithmic intelligence. These abilities have long been studied across disciplinesfrom their cognitive development in humans (Wynn, 1990; De Bruijn, 1964) to their formal characterization in logic and computability theory (Boolos et al., 2002; Cooper, 2017). In theoretical computer science, arithmetic primitives like counting and addition Equal contribution. have been analyzed in terms of circuit complexity (Jerrum, 1995), computational depth (Fischer et al., 1968), and machine models such as counter automata (Ibarra et al., 2002). Even simple operationse.g., counting from 1 to nare known to require depth complexity that grows with input length, imposing strict lower bounds on any computational model (Fischer et al., 1968). Transformers (Vaswani, 2017), including both autoregressive (GPT-style) (Gregor et al., 2014; Achiam et al., 2023) and non-autoregressive (BERT-style) (Devlin, 2018; Liu et al., 2022) variants, are inherently limited to constant-depth computation (Zhang et al., 2024; Delétang et al., 2022; Li et al., 2024a). In neural models, all computation occur within models latent space H, where the hidden state encodes intermediate computational representations. Unlike recurrent architectureswhere hidden states evolve over time via recursive updates ht = gθ(ht1)Transformers update only across fixed number of layers, independent of input length. As result, standard Transformer can process (or reason over) its hidden states only constant number of times, limiting its computational depth and situating it at the lower end of the Chomsky hierarchy (Delétang et al., 2022). This architectural bottleneck fundamentally restricts Transformer-based modelsranging from task-specific expert systems to large-scale LLMsfrom solving even basic arithmetic operations such as counting, which require iterative updates to internal state and growing depth with input length. Chain of Thought (CoT)(Wei et al., 2022) revolutionizes the reasoning paradigm by shifting the locus of computation from the latent space to the textual output space O(Zhang et al., 2024). By externalizing intermediate reasoning steps into natural language \"thoughts,\" CoT enables transformerbased models to tackle fundamental computational tasks that would otherwise exceed their architecFigure 1: Illustration of inductive reasoning as performed by humans, RNNs, and LLMs with CoT, respectively. tural capacity. These include basic arithmetic and symbolic reasoning operations such as counting, addition, and sequence manipulation. Theoretical studies (Li et al., 2024a; Zhang et al., 2024; Feng et al., 2024) demonstrate that CoT-augmented language models, under idealized assumptions, possess an upper bound capacity to simulate computations of arbitrary complexitythereby extending the class of tasks solvable beyond what standard Transformers can achieve. Despite extensive theoretical analyses and guarantees on the upper bound of computational abilities (Zhang et al., 2024; Chang and Bisk, 2024), actual model performance remains far below these limits. As LLMs scale from millions to billions of parameters (Achiam et al., 2023), improvements on fundamental tasks such as counting have been marginalGPT-4, for instance, still struggles to count the number of \"r\"s in word. While recent work has explored contributing factors like training data (Allen-Zhu and Li, 2023) and positional encoding (Chang and Bisk, 2024), one of the most basic componentstokenizationhas received surprisingly little attention. In particular, modern byte pair encoding (BPE) (Sennrich, 2015) merges multiple characters into single tokens for efficiency, often degrading arithmetic reasoning due to information loss during tokenization. Even OpenAIs latest o11 model, which integrates Monte Carlo Tree Search (MCTS) for improved reasoning, achieves only 50% accuracy on long string arithmetic tasks involving 3040 characters. In this work, we systematically investigate how tokenization choices can substantially constrain the theoretical reasoning and arithmetic capabilities of neural models. Our approach is model-agnostic, 1https://openai.com/o1 allowing us to evaluate even closed-source LLMs with undisclosed tokenization schemes. Leveraging extensive experiments with Chain of Thought (CoT)which has been theoretically shown to achieve Turing completeness under idealized conditions (Li et al., 2024b)we demonstrate that the choice of tokenization plays critical role in unlocking models full computational potential and bridging the gap between theoretical guarantees and practical performance. Neglecting this factor can lead to performance degradations of up to 80%. Moreover, we find that the impact of tokenization is model-dependent: some tokens disproportionately hinder performance on counting tasks, even when the underlying task remains fixed."
        },
        {
            "title": "Revisit",
            "content": "Training neural networks for arithmetic computation. Arithmetic operationsincluding counting, matching, and bracket balancingare foundational for symbolic reasoning and more complex algorithmic tasks (Chang and Bisk, 2024). Early studies on training neural networks (NNs) for such tasks focused on architectures capable of handling variable-length inputs. Since multi-layer perceptrons (MLPs) (Rosenblatt, 1958) are inherently limited to fixed-size inputs, initial progress came through recurrent neural networks (RNNs). Rodriguez et al. (1999) trained early RNNs to recognize the regular language anbn, which requires the network to implicitly count occurrences of and b. Of the 50 networks trained, 8 successfully generalized to longer sequences, highlighting RNNs capacity for basic arithmetic generalization. Building on this, Suzgun et al. (2019) showed that LSTMs could perform more complex dynamic layers and input sequence x1:n n, the hidden representation at layer ℓ and position is given by h(ℓ) = Embed(xt). The output token ot is then computed as (cid:17) ot = Softmax . = Layerℓ , with h(0) h(ℓ1) 1:n h(L) (cid:16) (cid:16) (cid:17) Figure 2: CoT vs Answer Only Generation Models. counting via bracket pairing tasks, leveraging gating and cell-state mechanisms to maintain multiple counterscapabilities that standard RNNs lacked. Theory of Model Architectures with Composability. Delétang et al. (2022) systematically investigated arithmetic capabilities across modern architectures, including RNNs, LSTMs, and Transformers. Their findings confirmed that while LSTMs exhibit computational behavior aligned with counter machines, Transformers consistently fail at even basic counting tasks. Chang and Bisk (2024) extended this analysis to newer architectures such as Mamba (Gu and Dao, 2023) and RWKV (Peng et al., 2023), revealing that these models also underperform on arithmetic tasks outside their training distributionsoften performing worse than classic RNNs in generalization. Recent studies (Weiss et al., 2018; Ackerman and Cybenko, 2020) have further validated the computational capabilities of both RNNs and LSTMs, particularly for tasks requiring symbolic or arithmetic reasoning. In contrast, Transformerslacking inherent recurrenceare restricted to TC0 complexity in their inductive reasoning capacity (Li et al., 2024a), placing them at the lower bound of the Chomsky hierarchy (Sanford et al., 2024; Li et al., 2024a; Delétang et al., 2022). As result, they are fundamentally incapable of solving even basic algorithmic tasks, such as arithmetic pattern induction or sequence manipulation, without incorporating explicit inductive biases (Chang and Bisk, 2024)."
        },
        {
            "title": "3 Theoretical Limits of Answer-Only",
            "content": "Models for Arithmetic and Symbolic Computation. Transformer-based models (Vaswani, 2017) without Chain-of-Thought (CoT) prompting (Fig. 2 left 2) are inherently constrained by their fixed architectural depth and lack of recurrence. Let be the input token space, the hidden state space, and the output token space. For Transformer with This process applies fixed sequence of transformations to each input xt, with the number of computational steps bounded above by = O(1). Since does not scale with the input length n, the depth complexity of computation in such models is constant, i.e., DepthTransformer = O(1). As established in complexity theory (Zhang et al., 2024; Li et al., 2024a; Chang and Bisk, 2024), this places answer-only Transformers in the class TC0constant-depth circuits with polynomial size and threshold gatesincapable of performing even simple arithmetic functions such as parity, addition, or comparison over unbounded inputs. Formally, consider function : Om defined by task such as computing sum(x1:n). Such tasks require computation of depth Ω(log n) for associative operations and Ω(n) for sequentially dependent operations (e.g., counting, carry propagation, or string reversal) (Fischer et al., 1968). Since is constant in Transformers and all transformations are composed in parallel across tokens, such models fail to meet the depth requirement: Depthtask(f ) > DepthTransformer implies that the Transformer cannot compute . Furthermore, Transformers lack mechanism to store and evolve intermediate computational states over time. In recurrent models, hidden states ht are recursively defined as ht = gθ(ht1, xt), allowing the system to simulate Turing-complete behavior (Zhang et al., 2024; Li et al., 2024b). In contrast, Transformers treat all inputs simultaneously through attention-based aggregation without iterative update: h(L) = fattn(x1:n), disallowing symbolic loop constructs or dynamic memorykey components in arithmetic computation. When constrained to generate only answer tokens o1:m Om without emitting intermediate reasoning steps (Fig 2), the models total computational budget is tightly bound by m, which is typically small. Let o1:m = (x1:n) be the models prediction. Then, for computation that requires (n) steps, with (n) m, the model must either (1) compress computation into fixed layersviolating the tasks depth complexityor (2) memorize input-output mappingsan approach that does not generalize beyond training. This reliance on shallow function approximation implies that such models can only succeed by memorizing specific input-output pairs, not by executing general algorithms. Empirically, these models exhibit sharp performance degradation on arithmetic tasks outside their training distribution (Chang and Bisk, 2024). If the maximum number of distinct computation traces model can represent with d-dimensional hidden states and p-bit numerical precision: 2dp. Therefore, the number of unique state transitions is also bounded above by 2dp, which is finite and insufficient for representing the O(n)-length trajectories required for tasks such as binary addition or bracket matching. In summary, models limited to generating only final answer tokens without CoT or external recurrence simulation (Fig 2)."
        },
        {
            "title": "4 CoT under Ideal Assumptions Enables\nGeneral Arithmetic Computation",
            "content": "Transformer-based LLMs (Achiam et al., 2023; Touvron et al., 2023; Bai et al., 2023), though powerful, are fundamentally bounded by their fixed architectural depth (Li et al., 2024a; Zhang et al., 2024), limiting their ability to perform arithmetic operations that require sequential, stateful updates. Chain-of-Thought (CoT) prompting (Wei et al., 2022), however, offers mechanism to simulate recurrence, transforming the depth-limited Transformer into theoretically Turing-complete system under ideal assumptions. 4."
        },
        {
            "title": "Inductive Arithmetic Requires Depth",
            "content": "Arithmetic reasoning, in both human cognition and formal computation, often involves inductive updates across time or space (Fig 1). For instance, computing the cumulative sum of digit sequence x1:n = (7, 3, 2, ) requires maintaining an accumulator that evolves as st = st1 + xt over = 1 to n. In recurrent neural networks (RNNs), this is naturally represented as ht = gθ(ht1, xt), where the hidden state ht stores intermediate quantities such as partial sums, carries, or flags. The computation depth required for such tasks is O(n), aligning with results from counter machine theory (Fischer et al., 1968). Transformers, in contrast, lack temporal recurrence. Their hidden states ht are updated via fixed sequence of layers, independent of sequence length. As result, their total reasoning depth is O(1) per token. Because all xt are processed in parallel, the Transformer cannot simulate stepwise updates required for arithmetic unless all logic is memorized or encoded through exponentially wide circuits (Li et al., 2024a). This explains why arithmetic taskssuch as computing sum(x1:n), reverse(x1:n), or counttoken(x1:n)are infeasible for answeronly Transformers without inductive bias (Chang and Bisk, 2024; Delétang et al., 2022). These tasks require depth-sensitive computation, where each output depends on chain of intermediate results not recoverable from input alone."
        },
        {
            "title": "Computation",
            "content": "Chain-of-Thought (CoT) reasoning allows Transformer to externalize its hidden state through intermediate tokens. Instead of directly mapping x1:n y, the model generates sequence of thought tokens o1:k: x1:n (o1, o2, . . . , ok) y. Each ot can encode intermediate computations (e.g., partial sums, loop counters, flags) that are later re-ingested through the embedding layer, latent state: ht+1 = reinitializing the next fθ(Embed(ot), xt+1). This iterative reasoning cycle approximates the recurrence in RNNs:ht1 ot ht, where ot encodes sufficient information from ht1 to resume and advance computation. Under ideal assumptionsnamely unlimited CoT token budget and precise token-to-state fidelitythis externalization loop can simulate unbounded depth, making CoT+autoregressive models Turing complete (Zhang et al., 2024; Li et al., 2024b)."
        },
        {
            "title": "5 Tokenization as a Barrier to",
            "content": "Chain-of-Thought Computation Despite the theoretical promise of Chain-of- (CoT) prompting to approximate Thought Turing-complete computation under ideal assumptions (Zhang et al., 2024; Li et al., 2024a), empirical failures persist even in state-of-the-art models such as GPT-4. These failures are particularly evident in arithmetic and symbolic tasks that require precise reasoning over fine-grained units (e.g., digits, letters, or symbols), where large language models often yield incorrect results for inputs of even moderate length (e.g., computing the number of rs in Strawberry). This discrepancy highlights critical limitation: CoT effectiveness is inherently bounded not just by model architecture, but also by the expressiveness of the underlying language, which is in turn shaped by the tokenizer. We treat tokenization as black-box preprocesT t1:m, where x1:n is raw input string sor x1:n over characters and t1:m is the resulting sequence of tokens from vocabulary V. Modern tokenizers such as byte-pair encoding (BPE) (Sennrich, 2015) are designed to optimize compression and training efficiency, not fidelity of semantic or syntactic granularity. However, for CoT reasoning to succeed in arithmetic and symbolic computation, the token space must be able to express and preserve intermediate state information."
        },
        {
            "title": "Mapping",
            "content": "We define the expressiveness of language = (G, V)with grammar and vocabulary Vas the number of unique semantically meaningful sequences SL it can generate: Expressiveness(L) := SL. For CoT process to emulate Turing-complete computation, it must support recurrent approximation: ht1 (o1, . . . , ok) ht, where latent state ht1 is decoded into intermediate natural language tokens o1:k k, which are then re-embedded and fed back to reconstruct ht. This implies the need for high-fidelity vectorto-token mapping: ϕ : and ψ : H, the composed transformation ψ such that ϕ(ht1) ht retains sufficient computational state to perform stepwise updates. Tokenization introduces two major obstacles to this cycle, degrading the effective CoT expressiveness."
        },
        {
            "title": "Token Granularity",
            "content": "The first form of damage is semantic obfuscation. Suppose the reasoning task requires operating over atomic units (e.g., characters, digits), but tokenization merges these into opaque multi-character tokens: Strawberry [Straw, berry]. Now, let ti denote token for which the model lacks fine-grained internal features (e.g., how many rs are present). We define the token awareness function as: TokenAware(ti, prop) := [prop Emb(ti)] , where prop denotes property (e.g., digit count, lexical features), and Emb(ti) is the token embedding. When TokenAwareness(ti, prop) = 0, reasoning that relies on prop (e.g., count the number of 3s) will fail. Thus, even if the CoT reasoning process is intact, its input signal is corrupted at the encoding layer."
        },
        {
            "title": "Expressiveness via Token Bottleneck",
            "content": "The second, more subtle limitation occurs during the CoT process itself. The latent state ht1 stores accumulated reasoning. To externalize this into thought tokens o1:k, we require that: ht1 H, o1:k such that ϕ(ht1) = o1:k, but when is coarse (e.g., BPE with token merges) or lacks the necessary expressive forms (e.g., missing digits, variable names, or operations), this surjection fails. Let Sh be the space of latent states and So be the expressible token sequences. Then CoT fidelity is bounded as: Fidelity(CoT) ϕ(Sh) So Sh . Low expressiveness low overlap critical reasoning steps cannot be externalized. This mismatch becomes catastrophic in arithmetic tasks where thought tokens must verbalize structured computations (e.g., carrying, intermediate sums). Without expressive enough V, many ht1 are untranslatable, rendering the CoT step ineffective. The model is thus forced to either truncate reasoning or approximate it via memorized heuristics, both of which degrade generalization."
        },
        {
            "title": "Tokenizer",
            "content": "Assume the model is asked to compute symbolic function : x1:n (e.g., reverse digits). The optimal CoT process proceeds via: h0 o1:k1 h1 ok1+1:k2 y, but if o1:ki / SL due to token constraints, then hi+1 will be misaligned, i.e., hi+1 g(hi). Over time, errors compound, and becomes uncomputable. Figure 3: Four types of string formatting to manipulate tokenization in counting. Examples in the figure are tokenized using the GPT-4o tokenizer. Each string-token type is labeled as (a), (b), (c), and (d) in the diagram. Note that changing the format does not alter the fundamental nature or difficulty of the counting task."
        },
        {
            "title": "Symbolic Computation",
            "content": "then symbolic reasoning that depends on those units will faileven if CoT prompting is used. To complement our theoretical findings, we introduce general, model-agnostic framework to evaluate how tokenization impacts symbolic and arithmetic reasoning in LLMs. While many models are closed-source, we treat LLMs as black boxes and isolate tokenization as the key variable influencing performance. Let be character-level input string and tokenizer mapping it into token sequence t1:m = (x), where each ti comes from fixed vocabulary. The LLM then performs symbolic task by computing an output = M(t1:m). Our goal is to determine how mismatches between task granularity and token structure affect the models ability to solve : Y. We base our input manipulation on three typical properties of modern BPE-like tokenizers: (1) Common substrings of 24 characters are merged (2) Delimiters (e.g., spaces, into single tokens. commas) are usually merged with adjacent tokens. (3) Adding repeated delimiters can break these merges and force token boundaries. These patterns let us construct inputs that vary tokenization while keeping the underlying symbolic task fixed. Input Design. For symbolic function (e.g., digit sum, string reversal, pattern matching), we generate two sets of inputs: Atomic-aligned inputs xatomic: token boundaries align with units required for the task. Merged-token inputs xmerged: intentionally merged to obscure symbolic units within tokens. If the model lacks internal awareness of subtoken structure (e.g., characters inside token), Quantifying Degradation. We define the tokenization damage as the average accuracy drop: tok := Ex (cid:2)A(xatomic) A(xmerged)(cid:3) , where A() is model accuracy. high tok indicates that the model relies on token structure and cannot generalize symbolic logic across inconsistent tokenizations."
        },
        {
            "title": "6.1 Settings",
            "content": "We evaluate the impact of tokenization and input formatting on symbolic reasoning capabilities of large language models (LLMs). We focus on three fundamental symbolic tasks: Arithmetic counting, Sorting and Sorting. Each task operates on inputs drawn from controlled domains (letters, digits, or words), and varies in input length and tokenization strategy. Despite differing surface forms, all tasks share symbolic core that requires composition, memory, and manipulation of atomic units. Importantly, the task identity remains unchanged across formatting conditions. To isolate the role of tokenization, we disable tool use in all models and treat LLMs as black-box functions , where is the tokenizer and is the model. For counting tasks, we test four competitive LLMs: GPT-4o-mini, Claude 3.5 Sonnet, Qwen Turbo, and OpenAI o1. For sorting and reversing, we focus on GPT-4o-mini due to its consistent performance and API accessibility. Each experiment Counting letter Counting letter String-Token Type len [10-20] len [20-30] len [30-40] len [10-20] len [20-30] len [30-40] no-CoT CoT no-CoT CoT no-CoT CoT no-CoT CoT no-CoT CoT no-CoT CoT pure string BPE tokens (a) \" \"-deliminated token (b) \", \"-deliminated token (c) precise-item token (d) tok [max] 30.10 46.20 56.00 50.70 25.90 45.70 58.40 55.40 96.80 41.10 15.10 16.10 19.40 15.80 4. 9.10 24.90 38.60 81.60 72.50 6.40 7.50 10.20 7.90 3.80 2.00 10.90 28.10 56.10 54.10 33.20 45.90 63.60 58.30 30.40 47.70 63.70 69.30 96.50 48.80 14.00 17.60 32.80 30.20 18. 9.40 34.00 56.10 90.00 80.60 3.80 5.60 13.90 12.60 10.10 2.70 18.60 42.30 70.80 68.10 Table 1: Results of counting as and bs in string consisting of letter and b, using GPT-4o-mini API. Numbers indicate the average accuracy (%) over 1000 random generated instances. Counting letter Counting letter String-Token Type len [10-20] len [20-30] len [30-40] len [10-20] len [20-30] len [30-40] no-CoT CoT no-CoT CoT no-CoT CoT no-CoT CoT no-CoT CoT no-CoT CoT pure string BPE tokens (a) \" \"-deliminated token (b) \", \"-deliminated token (c) precise-item token (d) tok [max] 26.60 41.00 45.50 60.10 33.50 55.20 52.90 64.20 97.70 44.80 19.80 23.90 27.40 32.50 12. 12.20 28.20 44.20 89.30 77.10 11.40 13.00 18.00 15.30 6.60 2.10 16.00 27.60 70.70 68.60 31.10 45.30 56.20 60.60 29.50 59.10 63.90 73.60 98.40 39.30 11.70 16.60 28.20 30.60 18. 22.10 46.20 55.60 93.80 71.70 4.60 6.80 13.90 13.30 9.30 7.30 29.50 41.90 74.80 67.50 Table 2: Results of counting es and zs in string consisting of letter and z, using GPT-4o-mini model. Numbers indicate the average accuracy (%) over 1000 random generated instances. Method/Length Letter Letter+Digit Digit exact-match accuracy. Str List tok Str List tok Str List tok CoT SCoT 5-10 10-15 155-10 10-15 15-20 24.6 3.3 0.4 28.0 10.4 2.4 32.0 8.7 1.1 35.1 12.6 3.4 7.4 5.4 0. 7.1 2.2 1.0 30.1 5.4 0.7 31.1 10.6 2.6 35.7 10.1 2.7 38.5 15.6 4.7 5.6 4.7 2. 7.4 5.0 2.1 56.2 7.9 0.8 64.7 15.6 3.2 84.4 33.0 4.8 84.8 34.6 8.6 27.8 25.1 4. 20.1 19.0 5.4 Table 3: Performance on sorting tasks using GPT4o mini with Chain-of-Thought (CoT) and Supervised Chain-of-Thought (SCoT) across different input types, length ranges and tokenization types. Method/Length Random Word High-freq Word Str List tok Str List tok Str List tok CoT SCoT 5-10 10-15 15-20 20-25 25-30 5-10 10-15 15-20 20-25 25-30 46.0 8.6 2.5 0.3 0.4 50.2 35.8 24.9 18.6 12. 70.0 38.1 20.1 9.6 4.7 72.1 56.9 44.4 31.3 23.8 24.0 29.5 17.6 9.3 4.3 21.9 21.1 19.5 12.7 11.2 39.1 11.7 1.5 0.5 0.1 51.1 29.9 18.7 13.6 7. 56.5 22.4 8.0 2.2 0.7 68.0 52.8 38.9 30.1 21.4 17.4 10.7 6.5 1.7 0.6 16.9 22.9 20.2 16.5 14.2 54.2 13.6 2.3 0.6 0.5 59.1 33.6 21.5 12.3 8. 66.6 25.9 9.0 2.4 0.4 72.3 56.5 43.9 32.6 23.8 12.4 12.3 6.70 1.8 0.1 23.2 22.9 22.4 20.3 15.3 Table 4: Performance comparison on reversing tasks, using simlar settings as Table 3. uses 1,000 randomly generated input instances per length bucket. Input lengths are task-specific: for counting, we use lengths in [10, 20], [20, 30], and [30, 40]; for sorting, lengths are in [5, 10], [10, 15]; and for reversing, in [5, 30] (5-step increments). All models use identical prompts. We evaluate with and without CoT reasoning, and for some tasks apply supervised CoT (SCoT) (Zhang et al., 2025) to control for CoT quality. Evaluation measures"
        },
        {
            "title": "6.2 Tokenizer Sensitivity in Symbolic Tasks",
            "content": "Across all tasks, we observe consistent phenomenon: model performance varies dramatically with tokenization format, even when the underlying symbolic function remains fixed. Following are detailed analysis across all tasks conducted:"
        },
        {
            "title": "CoT grants compositional logic",
            "content": "Enabling Chain-of-Thought (CoT) significantly boosts performance, particularly for tasks that require sequential or compositional logic. This improvement is most pronounced when input length increases, suggesting that CoT enables models to simulate recurrent computation by externalizing intermediate state. In tasks where CoT is not used, performance plateaus or drops sharply as sequence length grows, reflecting the fixed-depth constraint of answer-only Transformers."
        },
        {
            "title": "Symbolic Reasoning is sensitive to input token\nstructure",
            "content": "Experiments show that tokenization plays critical role in determining symbolic generalization. For fixed task function , changes in tokenization alonewithout altering task semanticscan yield over 70% variance in accuracy. This phenomenon is captured quantitatively by the tokenization degradation gap tok, which consistently reaches high values across all experiments. In Table 1, for exFigure 4: Distribution of shifts from the correct count. ample, switching from raw BPE inputs (type a) to atomic-aligned inputs (type d) improves accuracy by tok = 54.1% for counting a, and similar gains are observed in Table 2 for letters and z. The results in Tables 4 and 3 further reveal that symbolic reasoning ability is not only sensitive to tokenizer (controlled by using string vs. list), but also to the type of atomic unit being processed. Tasks involving digits consistently yield significantly higher performance than those involving letters or words, even when the overall structure of the task and input formatting are matched. For instance, in sorting tasks  (Table 3)  , CoT performance on digit sequences reaches up to 84.8% accuracy for lengths 510 in list format, with corresponding tok of 27.8. In contrast, performance drops to 35.1% on letter sequences under identical conditions, with much smaller tok of 7.1. This discrepancy persists across input lengths and holds under both CoT and SCoT prompting. similar trend is observed in reversing tasks  (Table 4)  : digit and high-frequency word sequences achieve the highest absolute accuracies and largest gains from structured formatting, suggesting that token contenti.e., whether the model processes compact numerical symbols or open-vocabulary lexical tokenshas significant effect on symbolic generalization. These results validate our theoretical claim that symbolic reasoning over atomic units cannot emerge reliably unless tokenization preserves unitlevel structure. When input tokens merge multiple semantic units (e.g., letters or digits), the model cannot apply symbolic operations like comparison or increment at the proper resolution. This leads to brittle reasoning and reliance on memorization. In contrast, atomic-aligned formats ensure that symbolic computation is recoverable from token-level patterns, enabling models to generalize even across longer inputs or different domains (letters vs. digits vs. words). Overall, the combination of CoT and precise tokenization unlocks the models latent arithmetic and symbolic capabilities. However, without either component, performance degradeseven if the model architecture is unchanged. These findings confirm that CoT grants access to general symbolic reasoning under ideal token granularity, and that high tok is strong indicator that models performance hinges on token alignment rather than true generalization."
        },
        {
            "title": "Counting Failures",
            "content": "We define error shifts as the difference between the model-predicted and true counts on failed instances. As shown in Figure 4, GPT-4o mini exhibits strong bias toward negative shifts across all tokenization types, indicating systematic undercounting. With pure BPE tokenization, shifts are exclusively negativelikely due to the models inability to parse individual characters within merged tokens (e.g., \"abaa\"), often resulting in zero counts for target symbols (see Appendix Figure 10)."
        },
        {
            "title": "When",
            "content": "formats delimiter-separated (types (b)(d)) are used, some positive shifts appear, likely caused by overcounting or retrieval inconsistencies. Yet with fully atomic-aligned tokens (type (d)), errors narrow to band between 1 and 3, reflecting smaller arithmetic missteps rather than structural confusion. This confirms that BPE introduces larger, systematic errors, whereas cleaner tokenization mitigates extreme deviations."
        },
        {
            "title": "7 Conclusion",
            "content": "We have demonstrated that tokenization is critical bottleneck in the symbolic reasoning ability of language models. Even with Chain-of-Thought prompting, coarse or misaligned token structures prevent models from accurately performing arithmetic and structured symbolic tasks. Our theoretical framework and empirical findings jointly show that both token format and token type (e.g., digits vs. letters) significantly affect generalization. Aligning tokenization with atomic reasoning units enables smaller models to rival or surpass larger ones, highlighting the need to treat tokenization design as core component of model capabilitiesnot merely preprocessing step."
        },
        {
            "title": "Limitations",
            "content": "Our experiments were conducted on GPT-4o Mini, Claude 3.5 Sonnet and Qwen Turbo. While all models demonstrated strong patterns and consistent evidence showing that certain types of tokenization significantly improve counting performance, we did not extend our testing to other open-source LLMs such as LLaMA, Mistral. This was primarily due to budget and time constraints, as well as preliminary findings that these models exhibited weaker instruction-following abilities compared to GPT and Claude, making the evaluation process more challenging. However, we believe our research remains robust despite these limitations, as mainstream model training and design principles are largely universal, and the patterns observed are likely generalizable to other LLMs. Additionally, our experiments did not explore extreme context lengths, such as counting instances with more than several hundred tokens. We found that such cases often led to instability due to the accumulation of long CoT steps. We aim to further investigate this aspect as LLMs improve in handling long-context retrieval and generation."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Joshua Ackerman and George Cybenko. 2020. survey of neural networks and formal languages. arXiv preprint arXiv:2006.01338. Zeyuan Allen-Zhu and Yuanzhi Li. 2023. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. George Boolos, John Burgess, and Richard Jeffrey. 2002. Computability and logic. Cambridge university press. Yingshan Chang and Yonatan Bisk. 2024. Language models need inductive biases to count inductively. arXiv preprint arXiv:2405.20131. Barry Cooper. 2017. Computability theory. Chapman and Hall/CRC. Nicolaas Govert De Bruijn. 1964. Polyas theory of counting. In Applied combinatorical mathematics, pages 144184. Eindhoven University of Technology. Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, et al. 2022. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098. Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. 2024. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36. Patrick Fischer, Albert Meyer, and Arnold Rosenberg. 1968. Counter machines and counter languages. Mathematical systems theory, 2(3):265283. Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. 2014. Deep autoregressive networks. In International Conference on Machine Learning, pages 12421250. PMLR. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752. Oscar Ibarra, Jianwen Su, Zhe Dang, Tevfik Bultan, and Richard Kemmerer. 2002. Counter machines and verification problems. Theoretical Computer Science, 289(1):165189. Mark Jerrum. 1995. The computational complexity In Proceedings of the International of counting. Congress of Mathematicians: August 311, 1994 Zürich, Switzerland, pages 14071416. Springer. Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. 2024a. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875. Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. 2024b. Chain of thought empowers transformers to solve inherently serial problems. Puyuan Liu, Xiang Zhang, and Lili Mou. 2022. character-level length-control algorithm for nonautoregressive sentence summarization. Advances in Neural Information Processing Systems, 35:29101 29112. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. 2023. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048. Paul Rodriguez, Janet Wiles, and Jeffrey Elman. 1999. recurrent neural network that learns to count. Connection Science, 11(1):540. Frank Rosenblatt. 1958. The perceptron: probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386. Clayton Sanford, Daniel Hsu, and Matus Telgarsky. 2024. Transformers, parallel computation, and logarithmic depth. arXiv preprint arXiv:2402.09268. Rico Sennrich. 2015. Neural machine translation of arXiv preprint rare words with subword units. arXiv:1508.07909. Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M. Shieber. 2019. Lstm networks can perform dynamic counting. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite precision rnns for language recognition. Karen Wynn. 1990. Childrens understanding of counting. Cognition, 36(2):155193. Xiang Zhang, Muhammad Abdul-Mageed, and Laks V. S. Lakshmanan. 2024. Autoregressive + chain of thought = recurrent: Recurrences role in language models computability and revisit of recurrent transformer. Xiang Zhang, Juntai Cao, Jiaqi Wei, Chenyu You, and Dujian Ding. 2025. Why does your cot prompt (not) work? theoretical analysis of prompt space complexity, its interaction with answer space during cot reasoning with llms: recurrent perspective."
        },
        {
            "title": "A Supervised Chain of Thought",
            "content": "Naive Chain of Thought (CoT), which uses generic \"think step by step\" prompt for all tasks, poses significant challenges for models in determining the correct steps, especially for complex, multi-step reasoning tasks. To mitigate this confounding factor, we follow previous work and employ Supervised CoT (Zhang et al., 2025), as the derivation of steps is not the focus of our research and should not affect performance due to incorrect CoT steps. Below, we define Supervised CoT and explain its application in counting tasks. A.1 Definition The search space for solving task can be viewed as combination of the prompt space and the answer space. When instructed to perform tasks step by step, language models must devise step template which is used to determine the actions at each step. This template is crucial for solving tasks, as it specifies what information is processed and how it is computed at each CoT step. However, for given task, there are numerous ways to perform \"step-by-step\" approach, each computing different elements per step. Finding the optimal set of steps is challenging yet essential, as it directly influences the ability to find solutions in the answer space (Zhang et al., 2025). Supervised CoT provides human supervision in determining the step template. Rather than asking the model to develop its own plan for each step, humans identify the \"recurrent\" procedure in the computation and explicitly instruct the model to follow specific step template. This approach allows the CoT to bypass the need to search for optimal steps, focusing instead on finding solutions within the answer space under optimal step guidance. Figure 5: Counting accuracy (Orange) with respect to target letter frequency (Blue) in Human Natural Language. A.2 Supervised CoT and Counting In inductive counting, which relies on CoT to compute the counter value recurrently(Figure 1), it is crucial that each step of CoT accurately extracts and outputs the counter value in text. This output is necessary for the value to be recurrently processed through \"string-vector\" conversion. Therefore, rather than simply prompting the model with \"determine the number of in the given string\" using the generic instruction \"think step by step,\" we specifically instruct the model to print out counter value at each step. We explicitly define the step template to ensure the model follows the optimal CoT steps, preventing deviations or the use of suboptimal steps. Experiments. We demonstrate the significant performance gap between Supervised and Unsupervised CoT. Specifically, we observe that supervision not only helps the model accurately extract the counter but also ensures it follows the correct steps (e.g., an incorrect step would be outputting whether the current letter is the target, rather than extracting the counter value). Even when Unsupervised CoT identifies the correct steps (i.e., extracting the counter into text), we still notice more frequent errors during the extraction process compared to Supervised CoT, which imposes strict constraints on what to extract at each step. The comparison between Supervised and Unsupervised CoT is presented in Table 5, showing clear dominance of Supervised CoT, with accuracy gains observed in nearly all cases."
        },
        {
            "title": "Relationship Between Letter Frequency\nand Symbolic Reasoning Performance",
            "content": "Our results in counting experiments show consistently higher counting accuracy for the letter compared to across all proper counting settings (CoT enabled, non-BPE tokenization), as shown in Table 1 and Figure 8 left. We hypothesized this difference stems from varying letter frequencies in natural language affecting token-embedding sensitivity. To further investigate this hypothesis, we compared counting performance between the most frequent letter (12.7%) and least frequent letter (0.07%) in English. Results in Table 2 show significantly outperforming e, mirroring the pattern seen with (1.5%) versus (8.2%). The accuracy advantage for lower-frequency letters ranges from 3-14% (Figure 8). Our results reveal that lower-frequency tokens carry less embedded information from training, making them easier to track through the attention mechanism. In contrast, common letters like and may encode more complex linguistic information, potentially interfering with counting tasks. To verify these results beyond the letter pairs a, and e, z, we selected another set of letters with significantly different frequencies in human languages, according to Wikipedia: (0.07%), (1.48%), (6.02%), and (12.70%). We generated counting instances of lengths between 80 and 100ensuring that each letter appears more than 20 times on averageby uniformly sampling one of the four letters to form each string (e.g., zrrbeez). We then performed counting for each letter in the generated strings. As shown in Table 7, consistent trend was observed across tokenization types (b)(d) (excluding (a), as pure BPE was previously shown not to yield meaningful counting results). Specifically, rare tokens consistently outperformed more frequent tokens in natural language, with performance improvements ranging from 6% to 12%. Figure 5 visually compares performance and letter frequency, showing an overlap between frequency and error rate. We suspect that rare letters carry less information in their embeddings, reducing distraction during the attention calculation in the counting process. Figure 6: Tokenization patterns of the GPT-4o tokenizer when processing four different input types for reversing task: (a) random character strings, (b) regular dictionary words, (c) high-frequency English words, and (d) listified random strings with explicit delimiters. Figure 7: Tokenization patterns of the GPT-4o tokenizer across diverse input compositions for sorting task: (a) random letter strings composed solely of alphabetic characters, (b) mixed random strings containing both letters and digits, (c) random digit strings composed exclusively of numerical characters, and (d) listified mixed strings with explicit delimiters separating letter and digit combinations."
        },
        {
            "title": "C Tokenization in Different LLMs",
            "content": "Figure 10 and Figure 11 illustrate the tokenization of input binary strings with difference lengths across various LLMs. We investigate both language models and multi-modal models, observing nearly identical tokenization behaviors across most tested models (except GPT-4o series). Therefore, in Figure 6 and Figure 7, we use the GPT-4o series models to further demonstrate tokenization String-token Type Counting Counting Unsupervised-CoT Supervised CoT Unsupervised-CoT Supervised CoT (b) (c) (d) 8.40 24.00 34.90 10.90 28.10 56.10 20.70 29.30 42.70 18.60 42.30 70.80 Table 5: Counting experiments in the length range of 30-40 comparing Supervised CoT and Unsupervised CoT. The bolded font indicates the better performance in the pairwise comparison between Supervised and Unsupervised CoT. string-token len [10-20] len [20-30] len [30-40] type count count count count count count (a) (b) (c) (d) 86.30 90.60 94.90 93.00 86.20 94.00 97.70 94.20 62.40 80.40 92.80 87.80 65.20 87.50 97.90 91.00 50.60 76.10 91.40 87.30 54.40 79.60 94.20 89. Table 6: Counting results on strings with letter and b, using Claude 3.5 Sonnet API. All results are using supervised CoT (Zhang et al., 2025), with same prompt for GPT-4o mini. Numbers indicate the average accuracy (%) over 1000 random generated instances. String-Token Type len [80, 100] r (b) (c) (d) percentage 13.60 14.50 36.60 36.00 61.60 60.20 Letter Frequency 1.48 0.07 8.90 28.30 54.10 8.40 24.30 51. 6.02 12.70 Table 7: Counting performance of letters that have very different letter frequency in human language. patterns in more complex cases. These include random letter strings, random number sequences, dictionary words, high-frequency words, and mixed digit-letter strings. This analysis provides insight into how tokenization varies across different input types and structural formats. Since pure strings may be tokenized differently due to the varying byte pair encoding (BPE) schemes used by each tokenizer. When tokenlevel delimiter is introduced, we consistently observe that the delimiter is combined with the adjacent letter, aligning with our previous assumptions. Additionally, some models handle the initial token differently, resulting in the first letter being treated as standalone token (e.g., in the Grok model) or being combined with quotation delimiter (e.g., in GPT-4o mini). In summary, our string design effectively allows us to manipulate modern LLMs to tokenize identical counting instances into different, desired tokens. Figure 8: Pairwise comparison of counting accuracy for different letters in strings. The left plot shows the distribution of accuracy for and in ab strings, with each dot representing the average accuracy for in given CoT case (e.g., spaced-string in the [10,20] range), connected to the corresponding accuracy for in the same setting. The right plot illustrates similar case for and in ez strings. Note: The y-axis limit exceeds [0,1] as the distribution is calculated based on variance and mean, with larger variance pushing the upper bound of the confidence interval beyond the maximum value."
        },
        {
            "title": "D Prompt Template",
            "content": "The set of prompts we use for counting experiments (base, unsupervised CoT, supervised CoT) are shown in Figure 12. The set of prompts we use for reversing and sorting experiments (unsupervised CoT, supervised CoT) are shown in Figure 13 and Figure 14 respectively. Case Studies: Counting In this section, we use counting task to showcase our experiment results. The tables present cases for each type of token counted using CoT. As shown, Supervised CoT consistently adheres to strict stepby-step template, accurately extracting the counter at each step. In contrast, Unsupervised CoT often skips crucial steps or deviates from the optimal method of extracting counters. Additionally, Supervised CoT with type (d) tokenization tends to produce much longer reasoning contexts, yet still achieves the best performance due to the combination of optimal tokenization and supervision."
        },
        {
            "title": "3.5 Sonnet and Qwen Turbo. With Qwen model,\nwe observed similar trends as GPT-4o mini (Table\n8). With Claude model, there is a slight exception\nthat type (c) yielded the best results among types\n(a)-(d), as shown in Table 6. Upon investigation,\nwe suspect this is because type (d) results in longer\nCoT steps due to the higher number of irrelevant\ntokens generation, leading to long-context reason-\ning failures in many cases for this model. We also\nprovide case studies using GPT-4o mini for count-\ning tasks, including examples where CoT led to\nboth correct and incorrect answers. Additionally,\nwe reveal the inferior performance with OpenAI o1\nfull model when tokenization is not properly done,\ndetailed in Appendix section E.1.",
            "content": "To this end, we are confident that our experimental results can be generalized to other LLMs, given that the training methods and tokenization strategies (as demonstrated in Appendix Section C) are nearly identical, leading to counting being performed in similar manner across such models. E.1 OpenAI o1 We evaluate o1 on samples ranging from 30 to 40 letters in length, using pure string (type (a)) to showcase the importance of using proper tokenization. Additionally, since o1 applies inference-time scaling techniques (such as MCTS search and perstep verifying), it implicitly engages in advanced chain-of-thought reasoning. Thus, we rely solely on straightforward prompt rather than explicitly specifying reasoning steps using Supervised Chainof-Thought as with other naive models. The final accuray on tested samples for o1 is 50%, which is much lower than using GPT-4o mini with most optimal tokenization techniques (Table 1, 70% in such length range). An example for correct counting is shown in Table 9, and an example for incorrect counting is shown in Table 10. In conclusion, advanced LLM searching algorithms and inference time scaling techniques do not make up for defect in tokenizer. E.2 Qwen Turbo We evaluate Qwen Turbo using supervised Chainof-Thought (CoT) prompts. Tables 17 and 18 demonstrate an incorrect counting example using tokenization type (a), Table 19 shows correct counting example using tokenization type (d). string-token len [10-20] len [20-30] len [30-40] type count count count count count count (a) (b) (c) (d) 56.40 75.10 93.40 95.90 62.50 80.00 96.00 96.70 26.20 50.60 81.60 83.20 32.20 54.60 83.50 86.50 16.20 31.10 59.20 68.10 15.90 28.30 57.60 63. Table 8: Counting results under the same settings as in Table 6 but using Qwen Turbo API. Numbers indicate the average accuracy (%) over 1000 random generated instances. This suggests that concise reasoning steps is beneficial for counting accuracy. E.3 GPT-4o mini We present three progressive examples that demonstrate the effectiveness of combining Chain-ofThought (CoT) reasoning with appropriate tokenization strategies: 1. Table 20 shows that using base prompt with standard BPE tokenization (type (a)) results in counting error of 3 from the correct value. 2. Table 21 demonstrates that incorporating supervised CoT improves accuracy, reducing the counting error to just 1. 3. Table 22 illustrates that combining supervised CoT with type (d) character-wise tokenization achieves perfect accuracy, matching the gold label exactly. These examples clearly demonstrate how the synergy between CoT reasoning and appropriate tokenization methods can progressively enhance counting accuracy. E.4 Claude 3.5 Sonnet We evaluate Claude 3.5 Sonnet using supervised CoT prompts. We provide example cases demonstrating different tokenization approaches and their outcomes: Tables 11 and 12 showcase incorrect counting results using tokenization type (a) Tables 13 and 14 demonstrate correct counting using tokenization type (d) Tables 15 and 16 illustrate correct counting using tokenization type (c) Notably, Qwen Turbo generates more tokens per CoT step compared to Claude 3.5 Sonnet, which appears to lead to its slightly lower performance. To sum up, our analyses show that tokenization type (c) yields superior results compared to type (d). Notably, in Tables 13 and 14, we observe that type Figure 9: Same error-shifting distribution (as in Figure 4) but for Claude model. Claude 3.5 tend to count more than counting less, compared to GPT-4o. (d) tokenization generates excessive and irrelevant content (specifically, index information) which may interfere with the accuracy of the counting process."
        },
        {
            "title": "F Replication Experiments Note",
            "content": "We have open-sourced the experimental results for every instance of each experiment, in the provided GitHub link, to facilitate future research and analysis by other researchers. All reported experiment numbers are stable, using the same experimental settings and prompts. Specifically, we observe an average variance in accuracy of less than 1% across runs of the same experiments, indicating that they are fully replicable with the same model version used. Note that updates to the API version may cause potential variations in results, which are beyond our control. Figure 10: Difference in tokenization on long binary strings without punctuations across different LLMs. Figure 11: Difference in tokenization on binary strings when counting instances are presented in different formats with punctuations to facilitate tokenization, across different LLMs. Count the number of appearances of {substring}s in the string below. Directly output Result: followed by the counted number. Do not use bold font in the response. String: {sample} Response: (a) Base prompt template Determine the number of appearances of {substring}s in the string below. Think step by step. Directly output Result: followed by the counted number. Do not use bold font in the response. String: {sample} Response: (b) Chain-of-Thought (unsupervised) prompt template Task: Count the number of occurrences of the substring {substring} in the given string step by step. Do not use bold font in the response. Instructions: Iterate through the string and count each occurrence of the substring. Use counter to keep track of how many times the substring appears. Output your step-by-step counting process. Conclude with: Result: followed by the number. String: {sample} Response: (c) Chain-of-Thought (supervised) prompt template Figure 12: Different prompt templates used in counting experiments. The templates include base prompt, an unsupervised chain-of-thought prompt, and supervised chain-of-thought prompt. Reverse the string provided below. Think step by step. Output the final result in dictionary with the key Result. For instance, if the input string is iamhappy, the output would be: {Result: yppahmai}. The input string may contain punctuations, do not add them into reversed string. Input string: {{string}} (a) Chain-of-Thought (unsupervised) prompt template Reverse the string following the instructions below: 1. Initialize an empty string called reversed 2. For each character in the input string: Remove the first (leftmost) character Add this character to the beginning of reversed Display the remaining input string 3. Once all characters are processed, reversed will contain the reversed string. Return the reversed string in dictionary format with the key Result. For example, the result dictionary for input string \"hello\" will be {Result: olleh}. Input String: {{string}} (b) Chain-of-Thought (supervised) prompt template Figure 13: Different prompt templates used in reversing experiments. The templates include an unsupervised chain-of-thought prompt, and supervised chain-of-thought prompt. Sort the string below in ascending order using insertion sort. Think step by step. Note that lower case characters are greater than upper case characters. Convert the sorted characters into single string, and store the result in dictionary with the key Result. For example, given the string 2CsU4bSc, the sorted result should be 24CSUbcs and should be stored as {Result: 24CSUbcs}. String: {{string}} (a) Chain-of-Thought (unsupervised) prompt template Sort the string below in ascending order using insertion sort following the steps below. Note that lower case characters are greater than upper case characters. 1. Start by creating an empty list for sorted characters. 2. For each step: Specify which character youre currently looking and insert the character to the correct place in sorted list. Display both the original string (showing characters remaining) and sorted list after each step. 3. Conbine the sorted list to single string. Return the sorted string in dictionary format with the key Result. For example, the result dictionary for input list [2, C, s, U, 4, b, S, c] will be {Result: 24CSUbcs}. String: {{string}} (b) Chain-of-Thought (supervised) prompt template Figure 14: Different prompt templates used in sorting experiments. The templates include an unsupervised chain-ofthought prompt, and supervised chain-of-thought prompt. Model & Experiment: OpenAI o1, token type (a), length 30-40 Prompt: Count the number of appearances of in the string below. Do not use tool. String: abaaaaaaabaabaaabbaaaaabaabbbbbbbabaaaaa Response: Model Response: Lets count the occurrences of in the given string: String: abaaaaaaabaabaaabbaaaaabaabbbbbbbabaaaaa Well go through each character and count how many times appears: (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) b a (21) (22) (23) (24) (25) (26) (27) Number of as counted: **27** Correct Answer: Table 9: Case Study 1 - Incorrect Prediction with o1 (base prompt) Model & Experiment: OpenAI o1, token type (a), length 30-40 Prompt: Count the number of appearances of in the string below. Do not use tool. String: babbbbbbbabbababbbaabbbbbbabaabbbbabb Response: Model Response: Lets count carefully: The given string is: babbbbbbbabbababbbaabbbbbbabaabbbbabb Lets go through it character by character and count every b: (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) Counting all the marked bs, we get total of **27**. Correct Answer: 27 Table 10: Case Study 2 - Correct Prediction with o1 (base prompt) Model & Experiment: Claude 3.5 Sonnet, token type (a), length 30-40 Prompt: Task: Count the number of occurrences of the substring in the given string step by step. Do not use bold font in the response. Instructions: Iterate through the string and count each occurrence of the substring. Use counter to keep track of how many times the substring appears. Output your step-by-step counting process. Conclude with: Result: followed by the number. String: babbbababbbaabbbbaabaabaaabbaaababbbabb Response: Model Response: Step 1: Initialize counter to 0. Counter = 0 Step 2: Start iterating through the string: - not a, counter remains 0 - found a, increment counter Counter = 1 Step 3: Continue iterating: - not a, counter remains 1 - not a, counter remains 1 - not a, counter remains 1 - not a, counter remains 1 - found a, increment counter Counter = 2 Step 4: Continue iterating: - not a, counter remains 2 - found a, increment counter Counter = 3 Step 5: Continue iterating: - not a, counter remains 3 - not a, counter remains 3 - not a, counter remains 3 - found a, increment counter Counter = 4 Step 6: Continue iterating: - found a, increment counter Counter = Step 7: Continue iterating: - not a, counter remains 5 - not a, counter remains 5 - not a, counter remains 5 - not a, counter remains 5 - found a, increment counter Counter = 6 Step 8: Continue iterating: - found a, increment counter Counter = 7 Table 11: Case Study 3 - Incorrect Prediction with Claude 3.5 Sonnet (CoT prompt) Step 9: Continue iterating: - not a, counter remains 7 - found a, increment counter Counter = 8 Step 10: Continue iterating: - found a, increment counter Counter = 9 Step 11: Continue iterating: - not a, counter remains 9 - found a, increment counter Counter = Step 12: Continue iterating: - found a, increment counter Counter = 11 Step 13: Continue iterating: - not a, counter remains 11 - not a, counter remains 11 - found a, increment counter Counter = 12 Step 14: Continue iterating: - found a, increment counter Counter = 13 Step 15: Finish iterating through the remaining characters: - not a, counter remains 13 - found a, increment counter Counter = 14 Step 16: Iteration complete. Result: 14 Correct Answer: 17 Table 12: Case Study 3 Continued Model & Experiment: Claude 3.5 Sonnet, token type (d), length 30-40 Prompt: Task: Count the number of occurrences of the substring in the given string step by step. Do not use bold font in the response. Instructions: Iterate through the string and count each occurrence of the substring. Use counter to keep track of how many times the substring appears. Output your step-by-step counting process. Conclude with: Result: followed by the number. String: [b, a, b, b, b, a, b, a, b, b, b, a, a, b, b, b, b, a, a, b, a, a, b, a, a, a, b, b, a, a, a, b, a, b, b, b, a, b, b] Response: Model Response: Step 1: Initialize counter to 0 Counter: 0 Step 2: Start iterating through the string [b, a, b, b, b, a, b, a, b, b, b, a, a, b, b, b, b, a, a, b, a, a, b, a, a, a, b, b, a, a, a, b, a, b, b, b, a, b, b] Step 3: Found at index 1 Counter: 1 Step 4: Found at index 5 Counter: 2 Step 5: Found at index 7 Counter: 3 Step 6: Found at index 11 Counter: Step 7: Found at index 12 Counter: 5 Step 8: Found at index 17 Counter: 6 Step 9: Found at index 18 Counter: 7 Step 10: Found at index 20 Counter: 8 Step 11: Found at index 21 Counter: 9 Table 13: Case Study 4 - Correct Prediction with Claude 3.5 Sonnet 1 (CoT prompt) Step 12: Found at index 23 Counter: 10 Step 13: Found at index 24 Counter: 11 Step 14: Found at index 25 Counter: 12 Step 15: Found at index 28 Counter: 13 Step 16: Found at index 29 Counter: 14 Step 17: Found at index 30 Counter: Step 18: Found at index 32 Counter: 16 Step 19: Found at index 36 Counter: 17 Step 20: Finished iterating through the string Result: 17 Correct Answer: 17 Table 14: Case Study 4 Continued Model & Experiment: Claude 3.5 Sonnet, token type (c), length 30-40 Prompt: Task: Count the number of occurrences of the substring in the given string step by step. Do not use bold font in the response. Instructions: Iterate through the string and count each occurrence of the substring. Use counter to keep track of how many times the substring appears. Output your step-by-step counting process. Conclude with: Result: followed by the number. String: b, a, b, b, b, a, b, a, b, b, b, a, a, b, b, b, b, a, a, b, a, a, b, a, a, a, b, b, a, a, a, b, a, b, b, b, a, b, Response: Model Response: Step 1: Initialize counter to 0 Counter: 0 Step 2: Start iterating through the string b, - Counter: 0 a, - Found a. Counter: 1 b, - Counter: 1 b, - Counter: 1 b, - Counter: 1 a, - Found a. Counter: 2 b, - Counter: 2 a, - Found a. Counter: 3 b, - Counter: 3 b, - Counter: 3 b, - Counter: 3 a, - Found a. Counter: 4 a, - Found a. Counter: 5 b, - Counter: 5 b, - Counter: 5 b, - Counter: 5 b, - Counter: 5 a, - Found a. Counter: 6 a, - Found a. Counter: 7 b, - Counter: 7 a, - Found a. Table 15: Case Study 5 - Correct Prediction with Claude 3.5 Sonnet 2 (CoT prompt) Counter: 8 a, - Found a. Counter: 9 b, - Counter: 9 a, - Found a. Counter: 10 a, - Found a. Counter: 11 a, - Found a. Counter: 12 b, - Counter: 12 b, - Counter: 12 a, - Found a. Counter: 13 a, - Found a. Counter: 14 a, - Found a. Counter: 15 b, - Counter: 15 a, - Found a. Counter: 16 b, - Counter: 16 b, - Counter: 16 b, - Counter: 16 a, - Found a. Counter: 17 b, - Counter: 17 - Counter: 17 Step 3: Finished iterating through the string Result: 17 Correct Answer: 17 Table 16: Case Study 5 Continued Model & Experiment: Qwen Turbo, token type (a), length 30-40 Prompt: Task: Count the number of occurrences of the substring in the given string step by step. Do not use bold font in the response. Instructions: Iterate through the string and count each occurrence of the substring. Use counter to keep track of how many times the substring appears. Output your step-by-step counting process. Conclude with: Result: followed by the number. String: baaabaaabaaababbbababbaababbabbaabb Response: Model Response: To count the number of occurrences of the substring in the given string step by step, we will iterate through the string and increment counter each time we encounter an a. String: baaabaaabaaababbbababbaababbabbaabb Step 1: Start with the first character b, no found. Step 2: Second character is a, increment the counter (counter = 1). Step 3: Third character is a, increment the counter (counter = 2). Step 4: Fourth character is a, increment the counter (counter = 3). Step 5: Fifth character is b, no found. Step 6: Sixth character is a, increment the counter (counter = 4). Step 7: Seventh character is a, increment the counter (counter = 5). Step 8: Eighth character is b, no found. Step 9: Ninth character is a, increment the counter (counter = 6). Step 10: Tenth character is a, increment the counter (counter = 7). Step 11: Eleventh character is b, no found. Step 12: Twelfth character is a, increment the counter (counter = 8). Step 13: Thirteenth character is b, no found. Step 14: Fourteenth character is b, no found. Step 15: Fifteenth character is b, no found. Step 16: Sixteenth character is a, increment the counter (counter = 9). Step 17: Seventeenth character is b, no found. Step 18: Eighteenth character is a, increment the counter (counter = 10). Table 17: Case Study 6 - Incorrect Prediction with Qwen Turbo (CoT prompt) Step 19: Nineteenth character is b, no found. Step 20: Twentieth character is b, no found. Step 21: Twenty-first character is a, increment the counter (counter = 11). Step 22: Twenty-second character is a, increment the counter (counter = 12). Step 23: Twenty-third character is b, no found. Step 24: Twenty-fourth character is b, no found. Step 25: Twenty-fifth character is a, increment the counter (counter = 13). Step 26: Twenty-sixth character is b, no found. Step 27: Twenty-seventh character is a, increment the counter (counter = 14). Step 28: Twenty-eighth character is b, no found. Step 29: Twenty-ninth character is b, no found. Step 30: Thirtieth character is a, increment the counter (counter = 15). Step 31: Thirty-first character is b, no found. Step 32: Thirty-second character is b, no found. Step 33: Thirty-third character is a, increment the counter (counter = 16). Step 34: Thirty-fourth character is b, no found. Step 35: Thirty-fifth character is b, no found. Step 36: Thirty-sixth character is a, increment the counter (counter = 17). Step 37: Thirty-seventh character is b, no found. Step 38: Thirty-eighth character is b, no found. Step 39: Thirty-ninth character is a, increment the counter (counter = 18). Step 40: Fortieth character is b, no found. Step 41: Forty-first character is b, no found. Step 42: Forty-second character is a, increment the counter (counter = 19). Result: 19 Correct Answer: 18 Table 18: Case Study 6 Continued Model & Experiment: Qwen Turbo, token type (d), length 30-40 Prompt: Task: Count the number of occurrences of the substring in the given string step by step. Do not use bold font in the response. Instructions: Iterate through the string and count each occurrence of the substring. Use counter to keep track of how many times the substring appears. Output your step-by-step counting process. Conclude with: Result: followed by the number. String: [b, a, a, a, b, a, a, a, b, a, a, a, b, a, b, b, b, a, b, a, b, b, a, a, b, a, b, b, a, b, b, a, a, b, b] Response: Model Response: To count the number of occurrences of the substring in the provided string, well go through it step by step: - First element: - No match. Counter = 0 - Second element: - Match found. Counter = 1 - Third element: - Match found. Counter = 2 - Fourth element: - Match found. Counter = 3 - Fifth element: - No match. Counter = 3 - Sixth element: - Match found. Counter = 4 - Seventh element: - Match found. Counter = 5 - Eighth element: - Match found. Counter = 6 - Ninth element: - No match. Counter = 6 - Tenth element: - Match found. Counter = 7 - Eleventh element: - Match found. Counter = 8 - Twelfth element: - Match found. Counter = 9 - Thirteenth element: - No match. Counter = 9 - Fourteenth element: - Match found. Counter = 10 - Fifteenth element: - No match. Counter = 10 - Sixteenth element: - No match. Counter = 10 - Seventeenth element: - No match. Counter = 10 - Eighteenth element: - Match found. Counter = 11 - Nineteenth element: - No match. Counter = 11 - Twentieth element: - Match found. Counter = 12 - Twenty-first element: - No match. Counter = 12 - Twenty-second element: - No match. Counter = 12 - Twenty-third element: - Match found. Counter = 13 - Twenty-fourth element: - Match found. Counter = 14 - Twenty-fifth element: - No match. Counter = 14 - Twenty-sixth element: - Match found. Counter = 15 - Twenty-seventh element: - No match. Counter = 15 - Twenty-eighth element: - No match. Counter = 15 - Twenty-ninth element: - Match found. Counter = 16 - Thirtieth element: - No match. Counter = 16 - Thirty-first element: - No match. Counter = 16 - Thirty-second element: - Match found. Counter = 17 - Thirty-third element: - Match found. Counter = 18 - Thirty-fourth element: - No match. Counter = Result: 18 Correct Answer: 18 Table 19: Case Study 7 - Correct Prediction with Qwen Turbo (CoT prompt) Model & Experiment: GPT-4o mini, token type (a), length 20-30 Prompt: Count the number of appearances of as in the string below. Directly output Result: followed by the counted number. Do not use bold font in the response. String: abaaababbbbbbaaaabaaaa Response: Model Response: Result: 10 Correct Answer: Table 20: Case Study 8 - Incorrect Prediction with GPT-4o mini (base prompt) Model & Experiment: GPT-4o mini, token type (a), length 20-30 Prompt: Task: Count the number of occurrences of the substring in the given string step by step. Do not use bold font in the response. Instructions: Iterate through the string and count each occurrence of the substring. Use counter to keep track of how many times the substring appears. Output your step-by-step counting process. Conclude with: Result: followed by the number. String: abaaababbbbbbaaaabaaaa Model Response: 1. Start with the string: abaaababbbbbbaaaabaaaa. 2. Initialize counter at 0. 3. Iterate through each character in the string. - Index 0: (counter = 1) - Index 1: (counter remains 1) - Index 2: (counter = 2) - Index 3: (counter = 3) - Index 4: (counter = 4) - Index 5: (counter remains 4) - Index 6: (counter = 5) - Index 7: (counter remains 5) - Index 8: (counter remains 5) - Index 9: (counter remains 5) - Index 10: (counter remains 5) - Index 11: (counter = 6) - Index 12: (counter = 7) - Index 13: (counter = 8) - Index 14: (counter remains 8) - Index 15: (counter = 9) - Index 16: (counter = 10) - Index 17: (counter = 11) - Index 18: (counter = 12) 4. End of string reached. Result: 12 Correct Answer: 13 Table 21: Case Study 9 - Incorrect Prediction with GPT 4o mini (CoT prompt) Model & Experiment: GPT-4o mini, token type (a), length 20-30 Prompt: Task: Count the number of occurrences of the substring in the given string step by step. Do not use bold font in the response. Instructions: Iterate through the string and count each occurrence of the substring. Use counter to keep track of how many times the substring appears. Output your step-by-step counting process. Conclude with: Result: followed by the number. String: [a, b, a, a, a, b, a, b, b, b, b, b, b, a, a, a, a, b, a, a, a, a] Model Response: Lets count the occurrences of the substring step by step in the given string: 1. Initialize counter at 0. 2. Start iterating through the list: [a, b, a, a, a, b, a, b, b, b, b, b, b, a, a, a, a, b, a, a, a, a]. - Index 0: The element is a. Increment counter to 1. - Index 1: The element is b. Counter remains 1. - Index 2: The element is a. Increment counter to 2. - Index 3: The element is a. Increment counter to 3. - Index 4: The element is a. Increment counter to 4. - Index 5: The element is b. Counter remains 4. - Index 6: The element is a. Increment counter to 5. - Index 7: The element is b. Counter remains 5. - Index 8: The element is b. Counter remains 5. - Index 9: The element is b. Counter remains 5. - Index 10: The element is b. Counter remains 5. - Index 11: The element is b. Counter remains 5. - Index 12: The element is b. Counter remains 5. - Index 13: The element is a. Increment counter to 6. - Index 14: The element is a. Increment counter to 7. - Index 15: The element is a. Increment counter to 8. - Index 16: The element is a. Increment counter to 9. - Index 17: The element is b. Counter remains 9. - Index 18: The element is a. Increment counter to 10. - Index 19: The element is a. Increment counter to 11. - Index 20: The element is a. Increment counter to 12. - Index 21: The element is a. Increment counter to 13. 3. The final count after iterating through all elements is 13. Result: 13 Correct Answer: Table 22: Case Study 10 - Correct Prediction with GPT-4o mini (CoT prompt)"
        }
    ],
    "affiliations": [
        "Cisco",
        "Stony Brook University",
        "University of British Columbia",
        "Zhejiang University"
    ]
}