{
    "paper_title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
    "authors": [
        "John Nguyen",
        "Marton Havasi",
        "Tariq Berrada",
        "Luke Zettlemoyer",
        "Ricky T. Q. Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 0 5 3 0 . 0 1 5 2 : r OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows John Nguyen1, Marton Havasi1, Tariq Berrada1,2, Luke Zettlemoyer1, Ricky T. Q. Chen1 1FAIR at Meta, 2Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, France We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation. Website: johnlnguyen.com/oneflow Figure 1 OneFlow is variable-length non-autoregressive model that can concurrently generate interleaved text and variable number of images using insertions as primitive operation."
        },
        {
            "title": "1 Introduction",
            "content": "Native Multimodal Models models capable of handling both multimodal understanding and generation within single backbone have advanced considerably in visual understanding and generation. These models typically employ unified transformer architecture with next-token prediction to handle both discrete and continuous generation (Team, 2024; Wu et al., 2025; Ma et al., 2025; Deng et al., 2025; Zhou et al., 2025). Recent work like Transfusion (Zhou et al., 2025) and Show-O (Xie et al., 2024) demonstrates that leveraging modality-specific training objectives within shared architectures can significantly improve performance, particularly on continuous modalities such as vision. However, both autoregressive (AR) and diffusion-based multimodal approaches face fundamental architectural constraints. Autoregressive models can handle interleaved data but require strict sequential generation each image must be fully completed before text generation can continue, preventing simultaneous cross1 Figure 2 Text-to-image generation. Generated images at 512512 resolution from OneFlow. Prompts are in Figure 12. Figure 3 Visual question answering. OneFlow generates by simply inserting tokens based on confidence, resulting in natural hierarchical sampling and implicit reasoning where the most difficult answer tokens are generated later. Generated tokens: = 0 = 1 want to paint flowers but need photo for inspiration. Can you show me two pictures of flowers and describe them? In this first image, beautiful arrangement of pink flowers is on display, with total of six flowers spread out in visually appealing manner.<image>The roses are delicate and have soft, pink hue, which adds to the overall charm of the scene. The bouquet seems to be the main focus of the image, with the flowers captured in close-up shot, showcasing their intricate details. The display creates an elegant and serene atmosphere, evoking sense of romance and appreciation for the beauty of nature. This image features close-up view of beautiful pink flower sitting in glass vase on table.<image>The flower appears to be gerbera, with vibrant pink color and prominent center, possibly bud. The glass vase is positioned at the base of the flower, enhancing the visual appeal of the scene. The table beneath the vase provides simple yet elegant backdrop, allowing the flower to be the focal point of the image. Figure 4 Concurrent interleaved text & image generation. OneFlow can insert variable number of images in the generated sequence, which are concurrently denoised alongside the text. This allows the text and images to depend on each other during the generation process. modal refinement. Conversely, diffusion-based multimodal models such as MMaDA (Yang et al., 2025), FUDOKI (Wang et al., 2025), and Unidisc (Swerdlow et al., 2025) enable simultaneous mixed-modal generation but only for predetermined single text-image pairs where modality assignments must be known priori and rely on independent time schedules for each modality. Neither paradigm supports the simultaneous generation of variable-length interleaved sequences. We present OneFlow, the first model to achieve simultaneous generation of interleaved data. Unlike autoregressive models that enforce sequential completion of each modality, and unlike diffusion models restricted to fixed length generation, OneFlow combines an insertion-based discrete text generation using Edit Flows with Flow Matching for image generation. This enables concurrent refinement of both text and images with per-image time schedules, using novel interleaved time schedule. Through controlled experiments across various model sizes and compute regimes, we demonstrate that OneFlow outperforms both autoregressive (AR) and diffusion baselines on generation and understanding tasks while requiring 50% fewer training FLOPs. Moreover, we find that concurrent mixed modal pretraining yields 4% relative improvement on VQA and 1.5% on image generation over sequential pretraining. We summarize our contributions below. Contributions: 1. We introduce OneFlow, non-autoregressive multimodal model that unifies image and text generation under simultanous Edit Flow and Flow Matching framework. 2. OneFlow enables new capabilities such as concurrent mixed-modal generation, which helps boost performance over uni-modal generation on wide range of benchmarks. 3. Through controlled experiences from 1B to 8B, we find that OneFlow scales better than autoregressive multimodal models, more so with mixed-modal training. 4. OneFlow outperforms or is competitive with existing AR and diffusion-based models, on diverse range of image generation and image understanding benchmarks."
        },
        {
            "title": "2 OneFlow: Mixed-Modal Generation through Flow Matching",
            "content": "n=0 n. OneFlow handles multiple modalities through sequence model, where elements in the sequence can be discrete tokens or continuous embeddings, e.g., of images. Concretely, let denote the space of single element of the sequence, which can take either discrete value, up to some fixed vocabulary size , or continuous value, i.e., = [M ] R. Then our state space is defined as the set of all possible sequences up to some maximum length , i.e., = (cid:83)N During generation, our model transforms noisy sequences into clean sequences. We do this by combining discrete-valued and continuous-valued generative processes. Specifically, we make use of the Edit Flows (Havasi et al., 2025) framework which enables variable-length sequence generation through the use of edit operations. It starts with noisy sequence and iteratively applies edits until it is denoised into generation. We focus on the insertion capabilities of Edit Flows, which is conceptually simple yet extremely powerful, as it allows inserting arbitrary number of tokensand imagesinto the generated sequence. When images are inserted, we initialize them with noise and then use Flow Matching (Lipman et al., 2024) to generate the image. Since the same model predicts both the text edits and the image denoising, OneFlow achieves variable-length, non-autoregressive joint image and text generation. In the following, we state equations with only intuitive justifications and explanations. Full mathematical details and derivations can be found in Appendix D."
        },
        {
            "title": "2.1 Discrete Text Generation via Edit Flows",
            "content": "Edit Flows uses continuous-time Markov chain (CTMC) to iteratively refine variable-length discrete sequences. We start with an empty sequence X0 = at time = 0, and transform the sequence through insertion operations. Let ins(x, i, a), , {1, . . . , n}, , be the sequence resulting from inserting the token value to the right of position of the sequence x, resulting in ins(x, i, a) = (x1, . . . , xi, a, xi+1, . . . , xn). (1) 3 This forms the primitive operation that we use during generation. During training, we take data sequence X1 and randomly delete tokens with equal probability to obtain Xt. This defines the process X[0,1] that we will fit to. The probability of each token being deleted is set by monotonic scheduler κt with κ0 = 0, κ1 = 1. P(xi in Xt) = κt, for each xi X1. (2) In preliminary experiments, we tested different κt but found that the linear schedule κt = works most consistently across our diverse benchmarks. Deleted tokens are removed from the sequence. Noting that on average we retain Et[κt] fraction of the original tokens, with the linear schedule we retain 50% of the data sequence. This can lead to significant FLOPs savings during training, and tuning the scheduler can save even more if desired. 2.1.1 Parameterization. The parameterization of an Edit Flow model for insertions naturally decomposes into two predictions: (i) how many tokens are missing at the right of position i, and (ii) which tokens are missing. Thus, at each position of the sequence, our model outputs two quantities λi : R+ is scalar that predicts the number of missing tokens between and + 1. Qi : [M ] is normalized distribution that predicts what tokens are missing. These two predictions form the CTMC rate, so for sampling we would use (up to o(h) error), (Xt+h = ins(Xt, i, a) Xt) = κt 1κt λi(Xt)Qi(aXt). (3) κt 1κt . Note that unlike prior work (Havasi et al., 2025), we factor out this ratio The ratio dictates the distribution of insertion times according to the schedule κt imposed during training (2), where κt = dκt from dt the rate predictions and use simplified model that is independent of t. In our practical implementation, we do not feed time values into the network for predicting insertions. While not theoretically justified, we found this t-independence assumption to work better in practice, likely because Xt already contains sufficient information for predicting the insertions. κt 1κt 2.1.2 Insertion prediction (λi). The main component that determines whether insertions occur is the prediction head λi, which is trained by regressing onto the number of missing tokens. Each position of the noisy sequence Xt has corresponding number of missing tokens ki, which is the number of deleted tokens between . The original Edit Flows loss was constructed through choice of Bregman divergence (Holderrieth et al., 2024) which results in and i+1 ℓPoisson(λi) = (cid:80) λi(Xt) ki log λi(Xt). (4) Alternatively, (4) can be interpreted as the negative log-likelihood of Poisson distribution, so λi is trained to fit Poisson distribution to model missing token counts ki. However, the distribution of has very high concentration for ki = 0. Furthermore, during sampling, the key prediction is whether the missing token count is either zero or nonzero. As such, we explicitly model the probability of inserting zero tokens. P(k = 0) = π, and P(k) = (1 π)Pois(k; λnonzero > 0) for > 0 (5) where π (0, 1) is the probability of observing zero, and λnonzero > 0 is the rate parameter but restricted to only modeling the distribution of nonzero counts. We train π by using binary cross entropy (BCE) loss to detect if the missing count is zero, and we train λnonzero using the original loss ((4)) on nonzero counts. For sampling (3), we can use the expectation λi(Xt) = (1 πi(Xt))λi nonzero(Xt). However, we found that consistently better sampling strategy is to first sample whether there are zero insertions using π, then simply nonzero(Xt) if there are nonzero insertions. Pseudocode for sampling can be found in Algorithm 2. use the rate λi 4 2.1.3 Bag-of-tokens prediction (Qi). To determine what token to insert at each position, we make use of the output head which is softmax over the discrete vocabulary [M ]. We use the same Edit Flows loss, which is sum of cross-entropy loss. Let and i+1 Ai denote the set of deleted tokens between , then for each position i, the loss is ℓtokens(Qi) = (cid:80) aAi log Qi(aXt). (6) 2.1.4 Combined loss. At each training iteration, we randomly delete tokens from the data sequence, and learn to predict the set of missing tokens at each position, resulting in the total insertion loss: Ltext = Et,XtX (cid:34) 1 (cid:88) i=1 ℓtokens(Qi) + ℓPoisson(λi (cid:35) nonzero)1[ki>0] + ℓBCE(πi) (7) where is the length of the noisy sequence Xt. Note that this differs from the original training objective in Edit Flows (Havasi et al., 2025) which additionally weights the loss by the factor , not affecting the optimal solution. We found that not using this factor produces better results. κt 1κt"
        },
        {
            "title": "2.2 Continuous Image Generation via Flow Matching\nFollowing standard practice, we generate images starting from a Gaussian noise of fixed dimension Nimg,\napplying a deterministic generation procedure that follows an ordinary differential equation. Let Yt ∈ RNimg\ndenote the noisy image, then the generative process is",
            "content": "d dt Yt = v(Yt, t), Y0 (0, I), (8) where : RNimg RNimg is velocity field that determines the direction to transform Yt into clean sample by = 1. During training, we sample noise Y0 and obtain Yt with linear schedule Yt = tY1 +(1t)Y0. The Flow Matching loss can then be written as Limage = Et,Y0,Y1 v(Yt, t) (Y1 Y0)2 . (9) In OneFlow, we use pretrained autoencoder to map images into latent space. We then design the velocity network v() to use shared Transformer backbone as text but with additional U-Nets to downsample and upsample between the backbone and autoencoder embedding spaces, making use of the same architectural design as Transfusion (Zhou et al., 2025). See illustration in Figure 19."
        },
        {
            "title": "2.3 Concurrent Mixed-modal Generation",
            "content": "To generate multiple modalities, we simply concatenate them into single sequence. We now present two multimodal time schedules, an independent schedule that can be used when the number of images is known, and an interleaved schedule that needs to be used when the number of images is arbitrary. OneFlow is designed to work with variable-length text and variable number of images. 2.3.1 Independent mixed-modal generation. We can consider the simple case with fixed number of imagestypically one. In such case, we can generate both the text and image simultaneously by using two time values ttext and timg, where ttext determines the state of the insertion generation process and timg determines the image generation process. Following prior work, we simply set independent time schedules, one for the text and one for each image. This allows the modalities to be concurrently generated and be dependent on each other during the generation process. However, this naïve process does not allow us to insert images. 5 2.3.2 Interleaved mixed-modal generation. much more complicated setting arises when the number of images is variable and images are being inserted as part of the generation process. Similar to the text-only setting, we start generating from the empty sequence. We then model image insertion as special token value <image>, which is added to the token prediction output Q. During generation, when the model predicts an image insertion, we insert noise embeddings of dimension Nimg into the sequence to represent an inserted image initialized at timg = 0. ins(x, i, <image>) = (cid:0)x1, . . . , xi, y1, . . . , yNimg, xi+1, . . . , xn(cid:1), yi (0, I). (10) Subsequent steps during generation would then simultaneously generate the image embeddings while also performing more insertions into the sequence. However, since the image is generated at later time, this implies there is delay between the image time and the text time, i.e. timg ttext, which needs to be taken into account during training. During training, we need to ensure that the text and image noise levels are consistent with with the ones seen during generation. Based on the schedule in (2), the time at which an insertion happens is random variable that has κ as its cumulative density function, so the time difference between the inserted image time timg and the initial text time ttext is given by timg = ttext κ1(u), where Unif(0, 1). (11) We call this the interleaved time schedule, which imposes distributional dependency between the time values timg for each image and the text time ttext. In order for the model to learn to fully generate all images, during training we sample from an extended time interval, τtext from [0, 2], since the <image>token can be inserted at τtext = 1.0 at the latest, and fully denoised by τtext = 2.0. The probability for each token being in Xt is then determined by κ(min{1, τtext}) in place of (2). We also sample for each image an extended time value τimg = τtext κ1(u). Finally, we determine if an image is deleted from the sequence by checking τimg < 0, and if so, the insertion loss ((7)) will include the <image> token which the model would learn to insert. Otherwise if τimg 0, the image is in the sequence and we set timg = min{1, τimg}; using the Flow Matching loss (9) to train the velocity. detailed derivation and in-depth explanation can be found in Appendix D.1."
        },
        {
            "title": "3 Findings and Unlocking New Capabilities in Unified Multimodal",
            "content": "To study the performance of OneFlow against AR, we design series of controlled experiments. We establish strong baselines by comparing both our approach and AR against other unified multimodal models in the literature. Finally, we explore the novel capabilities that OneFlow enables beyond existing methods. We present our experimental results through five research questions: 3.1 3.2 3.3 3.4,3.5 3.6 How does OneFlow scale compared to AR? What is the impact of mixed-modal vs. sequential pretraining? What emergent behaviors does OneFlow exhibit during generation? How does OneFlow compare to other unified multimodal models? What new capabilities does OneFlow enable beyond existing methods? 3.0.1 Training stages. Our training consists of two main stages: multimodal pretraining and instruction finetuning. During the pretraining stage, we use mixture of image understanding and image generation data to learn representations for both image and text. We trained with sequence length of 512 and global batch size of 4096. We set the mixed generation probability (the likelihood of concurrently generating clean text and the image) to be either 0 or 0.2. For finetuning, we use mixture of VQA, text, and interleaved data to give the model the ability to respond to visual question answering problems. We also fine-tune on image generation data at higher resolution of 512512 to improve the models image generation capabilities. We study the models behavior at the 1B scale for our ablations and controlled experiments, and the scaling trend up to 8B is detailed in Section 3.1. 6 3.0.2 Datasets. For multimodal pretraining, we use image-text pairs from filtered version of the Conceptual Captions dataset (CC12M (Sharma et al., 2018)), the YFCC dataset (Thomee et al., 2016), and licensed data, for total of 400M examples. During instruction finetuning, we use filtered image portion of the PerceptionLM dataset (Cho et al., 2025), interleaving data from Chameleon (Team, 2024), and filtered portion of the Cambrian-7M (Tong et al., 2024a) dataset. 3.0.3 Baselines. To evaluate our models performance against existing methods, we compare against two baselines: (1) an autoregressive (AR) + Flow Matching (FM) multimodal model based on Transfusion (Zhou et al., 2025), where text tokens are generated autoregressively and image tokens via FM, and (2) masked diffusion model based on LLaDA (Nie et al., 2025). For the masked diffusion baseline, we tested two sampling variants: low-confidence and random remasking, with random remasking performing better across all experiments. Unlike Transfusion, we follow Janus-Flow (Ma et al., 2025) and adopt dual-encoder setup. For image encoders, we use pretrained SigLIP2 ViT-SO400M-16@512 (Tschannen et al., 2025) for understanding and an SD3 VAE (Esser et al., 2024) for generation. Following Transfusion, we use U-Net adapters. Figure 19 illustrates how OneFlow consumes mixed-modal sequence. To improve performance of the AR model, we follow Chameleon (Team, 2024) and sweep over several generation lengths for optimizing CIDEr score. For image generation, we use first order Euler solver with entropy rectifying guidance (Ifriqi et al., 2025), we set the guidance scale to 5.0 across 50 sampling steps. 3.0.4 Model Configurations. To investigate scaling trends, we train models at three different sizes 1B, 3B, and 8B parameters, and token counts. For each benchmark, we plot all results over log-FLOPs curve and regress linear trendlines. Following Transfusion, we also estimate relative compute efficiency by measuring the parity FLOP ratio: the ratio between the number of FLOPs required by our AR and OneFlow to reach the same level of performance. 3.0.5 FLOPS Estimation. To estimate the FLOPS during training, we track the number of tokens throughout training for all models. During multimodal pretraining, we do not delete images and OneFlow uses on average 50% fewer generated text tokens compared to AR. We use Flash Attention and follow their FLOP estimation (Dao et al., 2022). 3.0.6 Evaluation setup. Following Cambrian (Tong et al., 2024a) and PLM (Cho et al., 2025), we group VQA tasks into five groups: General, Knowledge, OCR & Chart, Hard Perception, and Hallucination. We evaluate image generation quality using the FID metric (Heusel et al., 2017) on the COCO-2014 (Lin et al., 2014) validation set, using our base model at 256256 resolution. To assess prompt alignment, we report CLIPScore (Hessel et al., 2021) and DPG-Bench (Hu et al., 2024). Additionally, we include WISE (Niu et al., 2025) cultural to better understand knowledge-based generation."
        },
        {
            "title": "3.1 OneFlow Scales Better Than AR",
            "content": "In this experiment, we study the performance of OneFlow and AR in controlled settings at various model sizes and token counts. To ensure OneFlow has no advantage in data-constrained settings, we trained both models on 2B image-text pairs over 500k iterations using batch size of 4096. Both models were initialized from Llama 3.2 1B (AI@Meta, 2024). For AR, the number of tokens predicted during training equals the sequence length, whereas for OneFlow, the number of predicted tokens corresponds to the number of deleted tokens, which on average is 50% of the data sequence. We measure compute efficiency using the parity FLOP ratio  (Table 1)  , which quantifies the relative FLOPs required by OneFlow to match ARs performance. Our results show that OneFlow achieves the largest compute savings on dense prompt alignment and captioning tasks, requiring less than half the FLOPs of AR to reach the same level. 7 Figure 5 Performance of OneFlow vs. AR +FM baseline models at different model scales, data and compute. For text-to-image generation, we report DPG-Bench and FID. For image-to-text caption quality, we report CIDEr and ROUGE. In every benchmark, OneFlow consistently exhibits better scaling laws than AR +FM. Model sizes include 1B, 3B, and 8B. Parity FLOP Ratio DPG 0.49 FID CIDEr ROUGE 0. 0.32 0.42 Table 1 Parity FLOP ratio in controlled experiment. Both models were trained on 2B image-text pairs. Parity FLOP Ratio represents the relative amount of OneFlow FLOPs needed to match the final AR + FM performance. We find that OneFlow scales better than AR on every benchmark. This scaling advantage is especially pronounced on DPG Bench, where OneFlow scales significantly better than AR. Conversely, for image captioning, OneFlow shows notable performance gap relative to AR. Figure 5 visualizes the scaling trend, and the final metrics after training are shown in Table 2, along with comparison against other state-of-the-art models."
        },
        {
            "title": "3.2 Mixed Modal Pre-training Enables Better Generation and Understanding",
            "content": "In this section, we study the impact of mixed modal pretraining. We investigate whether concurrent mixed modal pretraining and sequential pretraining affect downstream understanding tasks. We train two 1B variants: one using sequential pretraining (T2I or I2T), and another with the same data but where 20% of examples use concurrent generation. Figure 7 Mixed modal concurrent training. Figure 6 indicates that when starting from concurrent model, downstream image understanding improves by 5% relatively. These results demonstrate the impact of concurrent mixed modal pre-training dense prompt alignment. We also finetune the pretrained model from Section 3.1 on mixture of interleaving data, filtered PLM data, and text data to evaluate downstream performance."
        },
        {
            "title": "3.3 Hierarchical Generation Enables Reasoning",
            "content": "In Figure 3, we present OneFlows sampling process when prompted with visual question. In response to prompts such as \"{question} Explain why.\", OneFlow generates reasoning chain before arriving at the final Figure 6 Mixed modal vs Sequential pretraining. Mixed modal pretraining vs sequential pretraining. Mixed pretraining achieves 4% relative improvement on VQA tasks and slight improvements on image generation as well. 8 Model Unified MLM MetaMorph (Tong et al., 2024b) LMFusion (Shi et al., 2024b) Transfusion (Zhou et al., 2025) Janus-Pro (Chen et al., 2025) Janus-Flow (Ma et al., 2025) Bagel (Deng et al., 2025) Multimodal Diffusion UniDisc (Swerdlow et al., 2025) D-DiT (Li et al., 2025) Muddit (Shi et al., 2025) MMaDA (Yang et al., 2025) FUDOKI (Wang et al., 2025) Controlled Comparisons AR + FM Ablation Mask + FM Ablation OneFlow OneFlow Mixed OneFlow OneFlow Mixed Text AR AR AR AR AR AR Image AR Diff Diff AR FM FM Size 7B 7B 7B 1.5B 1.5B 7B 1.4B Mask Mask Mask 2B Diff Mask Mask 1B Mask Mask 8B DFM DFM 1.5B 1B 1B 1B 1B 8B 8B AR Mask EF EF EF EF FM FM FM FM FM FM Image Generation Captioning FID CLIP DPG WISE (c.) CIDEr ROUGE BLEU4 11.8 14.0 16.0 15.2 12.4 27.7 23.9 33.2 12.2 11.3 12.1 9. 10.7 9.5 26.6 24.4 26.5 26.0 26.1 26.2 25.1 26.5 26.5 26.6 26.6 26.7 26.6 77.8 82.0 80.1 84.7 74.2 83.6 73.4 75.5 79.1 80.3 79.3 80.4 0.20 0.13 0.44 0.67 0.61 0.64 0.62 0. 0.65 0.68 38.4 33.7 56.2 59.7 123.9 128.4 138.1 139.8 141.1 142.1 57.2 58.6 60.8 60.9 61.1 61.1 0.39 0.39 0.41 0. 0.42 0.43 Table 2 Image generation and captioning benchmarks after multimodal pretraining. We find that mixed-modal training consistently improves performance. Evaluated using official open-source model weights. Highlighting denotes best results across all models. answer, without any Chain-of-Thought (CoT) (Wei et al., 2022) prompting or RL post-training. For example, when asked \"Is there snowboard in the image? Explain why.\", the model first implicitly performs visual search by examining the image and searching through likely locations for the snowboard. Similarly, for the math puzzle in (Figure 3 bottom), the model first identifies objects in the image that match the prompt descriptionthe green sphere and the large shiny cylinderbefore arriving at the final answer. Our results align with findings in Physics of LLMs (Ye et al., 2024) and MetaMorph (Tong et al., 2024b), where the authors suggest that LLMs precompute reasoning graphs before generating tokens. However, our findings demonstrate that the model can perform the same reasoning chain without autoregressive decoding. This suggests that reasoning capabilities can emerge in non-autoregressive architectures and transfer effectively to OneFlow. We show more example VQA generations compared to the AR baseline in Figure 9."
        },
        {
            "title": "3.4 OneFlow Outperforms AR in both Understanding and Generation.",
            "content": "In this section, we present the image generation results of our pretrained model compared to controlled baselines. We train both models on mixture of image-text paired data. To ensure we do not give the OneFlow model any advantage in data-constrained settings, we train both models on 500M unique samples (roughly 256B tokens) and epoch over the data 4 times. We expect AR and OneFlow to behave similarly for image generation, since both use continuous flow matching (FM) for this task. Indeed, we observe similar FID scores across all three methods, and AR and OneFlow exhibit comparable scaling trends. The most substantial improvement comes from dense prompt alignment (DPG Bench), where OneFlow tends to capture subtle details more accurately than the AR model. We show qualitative examples in Figure 8. In this section, we present the results of the image understanding in the VQA and captioning tasks compared to the controlled baselines. We finetune both models on large instruction tuning dataset totaling 40M samples. On average, OneFlow outperforms AR across all VQA tasks. The greatest improvement appears in vision-centric tasks such as RealWorldQA, where the bidirectional models (Mask and OneFlow) achieve 10% higher performance than AR."
        },
        {
            "title": "3.5 Comparison with State-of-The-Art Unified Models",
            "content": "We compare OneFlow with other autoregressive and diffusion multimodal models and summarize the results in Table 3. OneFlow achieves competitive performance on understanding and generation benchmarks, matching 9 a a o n polar bear balancing on blue barrel. table with some oranges and some cups. very cute cat sitting on piece of luggage. building with door sitting behind sign. Figure 8 Qualitative comparison of OneFlow and SOTA models. We notice that OneFlow gets the details of the prompt correctly, for instance the polar bear is balancing on blue barrel. The visual details of our generation are also better compared to MMaDA possibly due to using continuous image tokens rather than discrete. In the last column, the figure shows OneFlow handles common semantic challenges more effectively, as it was able to generate building with door sitting behind sign. Figure 9 (left) OneFlow first locates the object in the prompt before calculation. (right) OneFlow locates the target (stool), then analyzes color contrast. red arrow is added for clarity and is not in the original image. state-of-the-art models despite differences in training procedures. For example, models such as MMaDA underwent extensive post-training, while OneFlow did not. Similarly, FUDOKI was initialized from pretrained multimodal model, whereas ours was trained from scratch. We evaluate FID using the open-source weights from these models on COCO-2014. We find that MMaDA and Bagel underwent extensive aesthetic finetuning that produces images stylistically distant from real images, resulting in higher FID scores. For fair comparison, we evaluate using the open-source weights of MMaDA Base and Bagel without thinking mode for image generation. 10 General Knowledge OCR & Chart Vision Halluc. Model Multimodal LM Show-O (Xie et al., 2024) MetaMorph (Tong et al., 2024b) Janus-Flow (Ma et al., 2025) Janus-Pro (Chen et al., 2025) Janus-Pro (Chen et al., 2025) Mask Diffusion Muddit (Shi et al., 2025) D-DiT (Li et al., 2025) MMADA (Yang et al., 2025) Discrete Flow Params 1.3B 7B 1.5B 1.5B 7B 1B 2B 8B 75.2 74.9 73.4 76.9 68. 2 V 79.8 67.9 74.1 67.7 60.1 76.7 61.0 60.3 59.3 62.0 57.1 59.2 61. M 1232.9 1333.1 1443.0 1531.0 1104.6 1124.7 M 27.4 41.8 29.3 33.4 38.2 30. FUDOKI (Wang et al., 2025) 1.5B 73.9 57.6 1485. 34.3 c 64.6 21.2 24.3 r C 37.1 55.5 35.8 t 60.5 53.9 57.2 2 62.8 68.1 Controlled Comparisons AR + FM Ablation Mask + FM Ablation OneFlow OneFlow 1B 1B 1B 8B 64.7 66.0 69. 66.0 64.4 67.7 55.0 55.6 57.8 1394.8 1462.2 1497.1 28.9 28.4 29.8 59.1 55.1 58.5 22.7 18.7 23. 35.5 34.7 35.0 48.3 44.6 50.4 72.5 73.7 61.9 1542. 33.1 63.4 37.1 42.1 58.6 o R 58.3 53.5 56.4 50.5 50.6 50.6 54. O 84.5 88.0 84.8 85.2 84.0 86.1 86.1 85.6 84.6 85.7 86. Table 3 VQA performance comparison. OneFlow outperforms AR and Mask models across all benchmarks in controlled experiments using identical finetuning data. Highlighting shows best results in the 1B controlled comparisons. Our results are also competitive with existing autoregressive and discrete diffusion models. Evaluated using official open source weights. [CFG 0.0:] dog sitting in the drivers seat of truck. [CFG 1.0:] dog sitting in the drivers seat of red and gray truck. [CFG 2.0:] red and grey suv truck with dog sitting in the drivers seat. CFG Scale 0.0 1.0 1.4 2.0 AR The truck is parked on grassy field with tree on the left side and clear GPT4o Score 5.36 5.61 5.53 5.44 5.60 blue sky in the background. Input image Figure 10 Edit Flows with classifier-free guidance produces longer and more detailed answers, improving metrics that involve VLM as judge (Cheng et al., 2025). Additional examples of classifier-free guidance effects on text generation are shown in Figure 16."
        },
        {
            "title": "3.6 OneFlow Unlocks New Capabilities",
            "content": "3.6.1 Classifier-free guidance improves text detailedness. The use of continuous-time Markov chains allows us to apply classifier-free guidance (CFG) to our models insertion rates. Specifically, given an unconditional prediction λ(Xt)Q(Xt) and conditional prediction λ(Xtc)Q(Xtc), where is the prompt and is the guidance weight, the modified insertion rate is constructed as: λcfg(Xtc) = λ(Xtc)wλ(Xt)1w and Qcfg(Xtc) Q(Xtc)wQ(Xt)1w. (12) As shown in Figures 10 and 16, higher CFG values consistently increase the length and detail of generated text. We quantitatively evaluated caption quality and hallucination using CapArena (Cheng et al., 2025) by prompting GPT4-o. Our findings show that increasing CFG leads to more detailed captions, with OneFlow matching ARs level of detail at guidance scale of 1. However, this increased detail comes at the expense of hallucinations at very high CFG values."
        },
        {
            "title": "Text Generation",
            "content": "Autoregressive Mask diffusion Edit Flow"
        },
        {
            "title": "Interleaved Text and Image Generation",
            "content": "Transfusion (autoreg. + diffusion) OneFlow Figure 11 Illustration of the model input and targets during training for (top) text generation and (bottom) interleaved generation. To train autoregressive with diffusion denoising, the images are typically duplicated so that both the clean and the noisy images are in the sequence. On the other hand, OneFlow deletes tokens and images during training which reduces the sequence length. Generative Model Framework Text Image Training Properties Attention Tokens / Iter Generation Capabilities Understanding Single image gen Variable length Interleaved gen Examples AR AR Causal Seq Len AR Diffusion / FM Block Causal up to 2x Seq Len Mask Diffusion Mask Diffusion Bidirectional Seq Len Discrete FM Discrete FM Bidirectional Seq Len Edit Flow FM Bidirectional 50% Seq Len Chameleon (Team, 2024) JanusPro (Chen et al., 2025) Transfusion (Zhou et al., 2025) Bagel (Deng et al., 2025) MMaDA (Yang et al., 2025) FUDOKI (Wang et al., 2025) OneFlow Table 4 High-level comparison between different frameworks for building unified models of text and image generation. 3.6.2 Simultaneous generation of interleaved text and images. When autoregressive multi-modal models insert an image, they append it at the end of the current generation, fully denoise it, then continue the generation process. However, OneFlow is able to simultaneously denoise images and the text. When the model deems it necessary, it is able to insert an new image in the existing text and denoise it along with the text, as proposed in Section 2.3. To train this model, we took OneFlow 1B Mixed and finetuned it on the interleaved subset of the Chameleon dataset (Team, 2024) for 20000 steps. This subset contains 17000 examples that interleave both text and image data. Figure 4 shows the generation order of the tokens where two images were generated as part of the answer, with more detailed examples in Appendix and animated versions in the supplementary material."
        },
        {
            "title": "4 Related Work",
            "content": "Unified Multimodal Models. Current approaches for unified multimodal models fall into three main paradigms: fully autoregressive (Team, 2024; Wang et al., 2024; Wu et al., 2025), hybrid (Zhou et al., 2025; Deng et al., 2025; Xie et al., 2024; Ma et al., 2025), and fully diffusion-based (Yang et al., 2025; Swerdlow et al., 2025; Li et al., 2025; Wang et al., 2025). While these models are limited by fixed generation order or fixed-length 12 output, our approach fundamentally differs by being able to simultaneously generate interleaved content and variable number of images. We summarize the differences between each framework in Table 4. Iterative refinement models, including diffusion (SohlDiscrete Diffusion and Discrete Flow Matching. Dickstein et al., 2015; Ho et al., 2020) and flow models (Liu et al., 2022; Albergo et al., 2023; Lipman et al., 2024), have been adapted for discrete token spaces. Discrete diffusion models typically learn to reverse corruption process (Austin et al., 2021; Lou et al., 2024), while discrete flow models transport between two distributions with an interpolating scheme (Campbell et al., 2024b; Gat et al., 2024). Although these frameworks offer large design space (Shaul et al., 2024; Wang et al., 2025), recent works have predominantly focused on simplified mask construction (Sahoo et al., 2024; Shi et al., 2024a; Ou et al., 2024; Zheng et al., 2024). This masking framework, however, cannot be easily applied to variable-length and especially simultaneous interleaved generation. Edit-based Non-autoregressive Language Models. Early non-autoregressive models for variable-length generation (Gu et al., 2019a,b; Stern et al., 2019; Reid et al., 2022) often relied on multiple models and evaluations to handle edit operations. While later work like Edit Flows (Havasi et al., 2025) improved on this by using continuous-time framework and using only single evaluation per step. Campbell et al. (2024a) also proposed modeling insertions with diffusion model for denoising, but did not consider sequential data and requires separate network due to using single time during inference. In contrast, our approach considers mixed-modal sequence data, denoising images concurrently with per-image time values, allows for parallel token insertions, and uses unified backbone architecture."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "We introduced OneFlow, novel non-autoregressive multimodal model that overcomes the fixed-length generation limitations of diffusion models and has better scaling than autoregressive multimodal models. We introduced mixed-modal generation approaches, which through extensive controlled experiments, improve on benchmarks for both image understanding and image generation. We also propose novel approach to interleaved generation that simultaneously denoises images and inserts text tokens, with promising qualitative results. Interleaved generation is still in its infancy and we expect to see more incoming research efforts in constructing large-scale data sets (Awadalla et al., 2024; Laurençon et al., 2023; Zhu et al., 2023) and designing comprehensive benchmarks. limitation of requiring bidirectional attention is the lack of key-value caching, which increases inference cost. We do find that OneFlow can already obtain good captioning performance with very few model evaluationsoutperforming AR with only 6 sampling steps (Figure 18). Therefore, further reducing inference costs, with semi-autoregressive models (Arriola et al., 2025; Gat et al., 2025) or more sophisticated methods, would be an exciting research direction."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Maha Elbayad, Emily Dinan, Xiaochuang Han, Lili Yu, Chunting Zhou, and Melissa Hall for building the Transfusion code base which we built our code upon. We thank Peter Tong and David Fan for pointers on VQA and feedback on the paper. We thank Koustuv Sinha, Sharut Gupta, Marjan Ghazvininejad, Jakob Verbeek, and Amir Bar for feedback on the paper draft and fruitful discussions throughout the project."
        },
        {
            "title": "References",
            "content": "AI@Meta. Llama 3 model card. 2024. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md. Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 13 Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Anas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Guha, Sheng Shen, Mohamed Awadalla, Silvio Savarese, Caiming Xiong, et al. Mint-1t: Scaling open-source multimodal data by 10x: multimodal dataset with one trillion tokens. Advances in Neural Information Processing Systems, 37:3680536828, 2024. Andrew Campbell, William Harvey, Christian Weilbach, Valentin De Bortoli, Thomas Rainforth, and Arnaud Doucet. Trans-dimensional generative modeling via jump diffusion models. Advances in Neural Information Processing Systems, 36, 2024a. Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024b. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. Kanzhi Cheng, Wenpo Song, Jiaxin Fan, Zheng Ma, Qiushi Sun, Fangzhi Xu, Chenyang Yan, Nuo Chen, Jianbing Zhang, and Jiajun Chen. Caparena: Benchmarking and analyzing detailed image captioning in the llm era. arXiv preprint arXiv:2503.12329, 2025. Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. Advances in Neural Information Processing Systems, 37:133345133385, 2024. Itai Gat, Heli Ben-Hamu, Marton Havasi, Daniel Haziza, Jeremy Reizenstein, Gabriel Synnaeve, David Lopez-Paz, Brian Karrer, and Yaron Lipman. Set block decoding is language model inference accelerator. arXiv preprint arXiv:2509.04185, 2025. Jiatao Gu, Qi Liu, and Kyunghyun Cho. Insertion-based decoding with automatically inferred generation order. Transactions of the Association for Computational Linguistics, 7:661676, 2019a. Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in neural information processing systems, 32, 2019b. Marton Havasi, Brian Karrer, Itai Gat, and Ricky T. Q. Chen. Edit flows: Flow matching with edit operations. arXiv preprint arXiv:2506.09018, 2025. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Peter Holderrieth, Marton Havasi, Jason Yim, Neta Shaul, Itai Gat, Tommi Jaakkola, Brian Karrer, Ricky T. Q. Chen, and Yaron Lipman. Generator matching: Generative modeling with arbitrary markov processes. arXiv preprint arXiv:2410.20587, 2024. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Tariq Berrada Ifriqi, Adriana Romero-Soriano, Michal Drozdzal, Jakob Verbeek, and Karteek Alahari. Entropy rectifying guidance for diffusion and flow models, 2025. https://arxiv.org/abs/2504.13987. Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36:7168371702, 2023. Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 27792790, 2025. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft COCO: Common objects in context. 2014. Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, and Itai Gat. Flow matching guide and code. arXiv preprint arXiv:2412.06264, 2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Proceedings of the 41st International Conference on Machine Learning, pages 3281932848, 2024. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77397751, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, JUN ZHOU, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. In ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, 2025. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Machel Reid, Vincent Hellendoorn, and Graham Neubig. Diffuser: Discrete diffusion via edit-based reconstruction. arXiv preprint arXiv:2210.16886, 2022. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. Neta Shaul, Itai Gat, Marton Havasi, Daniel Severo, Anuroop Sriram, Peter Holderrieth, Brian Karrer, Yaron Lipman, and Ricky T. Q. Chen. Flow matching with general discrete paths: kinetic-optimal perspective. arXiv preprint arXiv:2412.03487, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024a. Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024b. 15 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible sequence generation via insertion operations. In International Conference on Machine Learning, pages 59765985. PMLR, 2019. Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024a. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024b. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, and Ping Luo. Fudoki: Discrete flow-based unified understanding and generation via kinetic-optimal velocities. arXiv preprint arXiv:2505.20147, 2025. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1296612977, 2025. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems. arXiv preprint arXiv:2408.16293, 2024. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/ forum?id=SI2hI0frk6. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. Advances in Neural Information Processing Systems, 36:89588974, 2023."
        },
        {
            "title": "A Additional Generation Examples",
            "content": "A man standing in the middle of nebula filled with stars. dog wearing hat and scarf. cute animal holding an ice cream cone. Lovely adventurer and explorer in tropical forest with mountains. shake next to cake. blue jay standing on large basket of rainbow macarons. An angel in the middle of the road. Cute and fluffy Labrador puppy in hat. Watercolor painting. Space scene. Magician standing on landscape silhouette with tree and fractal colorful nebula. Fantasy portal in forest landscape surrounded by tree and magic. Green nature with magical architecture concept art. Portrait of hipster wearing sunglasses. Stylized drawing of fashionista on colored background. majestic castle illuminated by lights. The ground is covered in blanket of snow, with body of water in the foreground. People walking with umbrellas with balloons in the sky painting of city street at night. blue cat. robot made out of wood. Figure 12 Generated images from the OneFlow model at 512512 resolution. Generated Text Prompt: Im looking for new hobby thats relaxing and will allow me to be outdoors to get some fresh air. Offer me suggestion and show me an image of the type of place can do it = 0.2: ? Its that. senseness tranqu promotes. Generated Images No image = 0.4: Why try? Its that you wide of, lake trail. in can create senseness tranqu promotes stress and. No image = 0.6: Why not try nature? Its that you wide variety of natural, like this lake trail,. in can create sense ofness and tranqu promotes stress and. No image = 0.8: Why not try nature? Its relaxing that allows you to in wide variety of natural environments, like this lake trail, for instance.<image>Walking in can create sense of calmness and tranqu that promotes stress and relaxation. = 1.0: Why not try nature walking? Its relaxing hobby that allows you to immerse yourself in wide variety of natural environments, like this lake trail, for instance.<image>Walking in nature can create sense of calmness and tranquility that promotes stress reduction and relaxation. = 2.0: Why not try nature walking? Its relaxing hobby that allows you to immerse yourself in wide variety of natural environments, like this lake trail, for instance.<image>Walking in nature can create sense of calmness and tranquility that promotes stress reduction and relaxation. Figure 13 Example interleaved generation process showing outdoor exercise. In this example, the image is inserted late in the generation process when the text is almost fully denoised. Generated Text Prompt: want to paint flowers but need photo for inspiration. Can you show me two pictures of flowers and describe them? = 0.2: first pink of out in appealing roses and overall the. seems be the, appreciation for. pink flower pink with possibly vase is base the flower appeal simple backdrop the point of the image Generated Images No image = 0.4: In first, arrangement pink flowers is, of six flowers out in appealing manner.<image> roses are delicate and pink the overall of the. The bouquet seems be the captured,oking and appreciation for the nature. This image view pink sitting table flower be ger with pink with prominent center possibly a. vase is base the flower visual appeal table simple backdrop, allowing flower be the point of the image. = 0.6: In first, arrangement of pink flowers is, of six flowers out in appealing manner.<image> roses are delicate and have a, pink, the overall of the. The bouquet seems be focus the image captured, showcasing their The display and atmosphere,oking of and appreciation for the nature. This image view of pink sitting table.<image> flower appears be ger with pink with prominent center possibly bud. vase is the base the flower, enhancing visual appeal of the. The table vase simple backdrop, allowing flower be the point of the image. = 0.8: In this first image, beautiful arrangement of pink flowers is on display, with total of six flowers spread out in appealing manner.<image>The roses are delicate and have soft, pink hue, which adds the overall charm of the scene. The bouquet seems be the main focus of the image flowers captured close shot, showcasing their intricate details. The display and atmosphere, evoking sense of romance and appreciation for the beauty nature. This image view of pink flower sitting glass table.<image>The flower appears be ger with pink color with prominent center, possibly bud. vase is the base of the flower, enhancing visual appeal of the. The table the vase provides simple backdrop, allowing flower to be the focal point of the image. = 1.0: In this first image, beautiful arrangement of pink flowers is on display, with total of six flowers spread out in visually appealing manner.<image>The roses are delicate and have soft, pink hue, which adds to the overall charm of the scene. The bouquet seems to be the main focus of the image, with the flowers captured in close-up shot, showcasing their intricate details. The display creates an elegant and serene atmosphere, evoking sense of romance and appreciation for the beauty of nature. This image features close-up view of beautiful pink flower sitting in glass vase on table.<image>The flower appears to be gerbera, with vibrant pink color, with vibrant pink color and prominent center, possibly bud. The glass vase is positioned at the base of the flower, enhancing the visual appeal of the scene. The table beneath the vase provides simple yet elegant backdrop, allowing the flower to be the focal point of the image. = 2.0: In this first image, beautiful arrangement of pink flowers is on display, with total of six flowers spread out in visually appealing manner.<image>The roses are delicate and have soft, pink hue, which adds to the overall charm of the scene. The bouquet seems to be the main focus of the image, with the flowers captured in close-up shot, showcasing their intricate details. The display creates an elegant and serene atmosphere, evoking sense of romance and appreciation for the beauty of nature. This image features close-up view of beautiful pink flower sitting in glass vase on table.<image>The flower appears to be gerbera, with vibrant pink color, with vibrant pink color and prominent center, possibly bud. The glass vase is positioned at the base of the flower, enhancing the visual appeal of the scene. The table beneath the vase provides simple yet elegant backdrop, allowing the flower to be the focal point of the image. Figure 14 Example interleaved generation process showing flowers. 19 Generated Images Generated Text Prompt: am teaching an ESL lesson on home types. Please show me images of bungalow, two-story home and townhouse that can show my students, and briefly explain about each of them. = 0.2: good. **B is typically for with a-p simple including suburban towns. is of a.Two** type residential building levels with the typically bedrooms on the. example-story. **Town town unit town. They vary typically than homes image example.<image> = 0.4: which good show to them about different types1. **Bungalow** ungalow is of is typically cosy designed for. Bungal are single-story, with a-p simple can including suburban neighbourhood towns and areas. Below is of a.Two-** type residential building levels, with the typically on bedrooms on the. below example of-story. **Town town multi residential unit that town walls. They can vary size are typically smaller than single homes image is example of.<image> = 0.6: images which would good to show students to them about different types: 1. **Bungalow** ungalow is type of is typically small, cosy and designed for comfortable. Bungal are often single-story, with a-p simple can found various locations including suburban neighbourhood towns and areas. Below is an image of a.Two-** two type residential building has levels, with the typically on the first floor and bedrooms and additional on the. image below an example of-story. **Town town multi-level residential unit that attached town shared walls. They can vary size, are typically smaller than single homes. The image is example of of.<image> = 0.8: are images which would be good to show your ESL students to them about different types: 1. **Bungalow** bungalow is type of that is typically small, cosy and designed for comfortable living. Bungalows are often single-story, with a-pitched and simple floor can be found various locations, including suburban neighbourhoods, beach towns and areas. Below is an image of b2.Two-Story home** two home type residential building has levels or floors, with the typically on the first floor and bedrooms and additional spaces on the second. image below is an example of two-story home.3. **Townhouse** town multi-level residential unit that attached one more other town shared walls. They can vary size, but are typically smaller than single-family homes. The image below is example of ofhouses.<image> = 1.0: Here are three images which would be good to show your ESL students to teach them about different home types: 1. **Bungalow** bungalow is type of house that is typically small, cosy, and designed for comfortable living. Bungalows are often single-story homes, with low-pitched roof and simple, open floor plan. They can be found in various locations, including suburban neighbourhoods, beach towns, and rural areas. Below is an image of bungalow:<image>2. **Two-Story home** two-story home is type of residential building that has two levels or floors, with the main living areas typically on the first floor and bedrooms and additional living spaces on the second floor. The image below is an example of two-story home.<image>3. **Townhouse** townhouse is multi-level residential unit that is attached to one or more other townhouses via shared walls. They can vary in size, but are typically smaller than single-family homes. The image below is an example of row of townhouses.<image> = 2.0: Here are three images which would be good to show your ESL students to teach them about different home types: 1. **Bungalow** bungalow is type of house that is typically small, cosy, and designed for comfortable living. Bungalows are often single-story homes, with low-pitched roof and simple, open floor plan. They can be found in various locations, including suburban neighbourhoods, beach towns, and rural areas. Below is an image of bungalow:<image>2. **Two-Story home** two-story home is type of residential building that has two levels or floors, with the main living areas typically on the first floor and bedrooms and additional living spaces on the second floor. The image below is an example of two-story home.<image>3. **Townhouse** townhouse is multi-level residential unit that is attached to one or more other townhouses via shared walls. They can vary in size, but are typically smaller than single-family homes. The image below is an example of row of townhouses.<image> Figure 15 Example interleaved generation process showing home types. An animated version is included in the supplementary material. 20 Input image Captions with varying classifier-free guidance weights [CFG 0.0:] llama and horse standing in field. [CFG 2.0:] white llama and white horse standing in field. [CFG 0.0:] group of toy animals sitting on table. [CFG 1.0:] table topped with variety of toy animals, including giraffe , cow , and bird , as well as toy duck . In the background, there is frame attached to the wall. [CFG 1.5:] green table topped with toy animals, including giraffe, cow, yellow bird, and toy duck, next to red box . In the background, there is photo frame attached to the wall. [CFG 0.0:] bathroom with large tub and sink. [CFG 1.0:] bathroom with claw foot tub and three windows . [CFG 2.5:] brown bathroom or master bathroom with classic claw foot tub and three windows. [CFG 0.0:] cat sitting on wooden deck looking up. [CFG 1.0:] cat sitting on wooden deck looking at its reflection in window. [CFG 2.5:] Two fluffy ginger and white cats sit and gaze at their reflection in glass window on green wooden deck in Japan . [CFG 0.0:] white plate topped with cake and spoon. [CFG 1.0:] plate with dessert and two spoons on it. [CFG 2.0:] white plate topped with ice cream , accompanied by two spoons, bottle , glass , and tissue paper on the table . Through the glass window in the background, we can see the water and the sky. [CFG 0.0:] glass bowl filled with colorful paper cranes. [CFG 1.0:] Colorful origami cranes in glass bowl shaped like heart . [CFG 2.0:] table with heart-shaped bowl filled with colorful origami cranes in various colors . The background is slightly blurred , giving the focus to the vibrant colors of the cranes. Figure 16 Text generation examples from OneFlow, which allows the use of classifier-free guidance (CFG). We observe that CFG produces longer and more detailed captions and also increased chance of hallucinations . Highlighted text show increased levels of detail when using higher CFG weights."
        },
        {
            "title": "B Additional Experiment Results",
            "content": "B.1 Performance Between AR and OneFlow During Pretraining Figure 17 Training curve for OneFlow vs. AR for multimodal pretraining. OneFlow initially starts out lower than AR however it quickly catches up and exceeds AR, most notably on DPG and CIDEr. B.2 Pretraining from Scratch vs LLama Init Image Generation VQA Model Initialization DPG FID CLIP CIDEr Avg VQA OneFlow OneFlow OneFlow Mixed OneFlow Mixed AR + FM AR + FM Random LLaMA Random LLaMA Random LLaMA 73.17 75.41 (+2.24) 74.86 75.08 (+0.22) 7.96 7.79 (0.17) 7.69 7.44 (0.25) 25.7 26.0 (+0.3) 25.8 25.8 (+0.0) 139.4 138.2 (1.2) 140.0 139.1 (0.9) 71.90 73.40 (+1.50) 7.83 7.91 (0.08) 25.8 25.7 (0.1) 122.9 123.9 (+1.0) 51.2 52.2 (+1.0) 51.6 52.8 (+1.2) 46.6 49.0 (+2.4) Table 5 Ablation study comparing LLaMA initialization vs. random initialization. Except for CIDEr, using LLaMA as initialization generally offers benefits, especially for dense prompt image generation (DPG) and for VQA performance. Image generation metrics use CFG=3, and VQA results are averaged across benchmarks. B.3 Sampling Steps on Caption Performance Figure 18 Performance vs. sampling steps compared to AR. OneFlow achieves parity with the AR model using only 6 sampling steps."
        },
        {
            "title": "C OneFlow Architecture",
            "content": "Figure 19 Architecture. With multimodal prompt, OneFlow can produce variable length generations with interleaved text & images in unified non-autoregressive sequence model, simultaneously generating all modalities with an interleaved time schedule for each generated image and text."
        },
        {
            "title": "D Full Derivations",
            "content": "We provide the derivations of the model here. We briefly summarize the Edit Flow (Havasi et al., 2025) formulation and derivation, and then derive the interleaved time schedule when insertions and image denoising are performed simultaneously. Setup. We make use of blank token ε to denote empty spaces within sequence. This token is only used for tracking token deletions during training and is not part of the vocabulary. Let = (cid:83)N n=0(T {ε})n be an extended space of aligned sequences. Furthermore, define frm-blanks : as the function that removes all blank tokens from the sequence. Lastly, we define the delta function over sequences δz1(z2) = (cid:81) (zi 2) which is one if all tokens are the same otherwise zero (i.e. Kroneckers delta function). δzi 1 Continuous-time Markov chain (CTMC). CTMC is continuous-time discrete-space process which iteratively jumps between discrete values, with transitions P(Xt+hXt) = δXt(Xt+h) + hut(xXt) + o(h), (13) where ut can be interpreted as first-order characterization of the transition kernel. Since with insertions, the sequence lengths of Xt can change over time. To simplify notation, Havasi et al. (2025) used an augmented space of (Xt, Zt), where it is basically always enforced that Xt = frm-blanks(Zt). The role of Zt is only for training, to keep track of which tokens are deleted and to compute the loss, and it is neither seen by the model nor used during sampling. To briefly summarize the construction below, the Flow Matching recipe makes use of prescribed conditional CTMC that generates single data sequences, which is then marginalized over the data distribution. The resulting marginal CTMC will then sample from the data distribution. 23 Figure 20 During training we construct Zt by replacing tokens with the blank token (ε), with the original tokens used to construct the target bag-of-tokens Ai. Conditional probability path. Given data sequence X1 pdata, we prescribe conditional probability path over Zt of the same sequence length which interpolates between the empty sequence and this data sequence. We then obtain Xt by applying the frm-blanks function. Concretely, we can express the conditional probability path as pt(Xt, ZtX1) = pt(XtZt, X1) pt(ZtX1) = pt(XtZt) pt(ZtZ1) = δfrm-blanks(Zt)(Xt) (cid:32) (cid:89) (1 κt)δε(Z ) + κtδX 1 (cid:33) (Z ) , (14) (15) (16) i=1 where κt is scheduler where κ0 = 0, κ1 = 1, and is the sequence length of X1. In English, (16) is mixture can either be equal to ε with probability 1 κt or equal to data value distribution where each token 1 with probability κt. Conditional CTMC rate. As discussed in Havasi et al. (2025), conditional CTMC that samples from this conditional probability path can be constructed as ut(x, zXt, Zt, X1) = (cid:32) (cid:88) i=1 κt 1 κt (cid:33) (δX 1 (zi) δZi (zi)) δfrm-blanks(z)(x), where = ins(Xt, i, a) for some [n] and [M ] (17) which denotes the infinitesimal change in probability of going from the state (Xt, Zt) (x, z), constrained to next sequences that are one token insertion difference from Xt. In English, (17) assigns rate of if ; otherwise, it is zero. This ensures that sample starting with all blanks Z0 = [ε, ...ε] is not yet equal to 1 at = 0 will eventually turn into X1 at = 1. This ratio is the infinitesimal rate that each token changes its value, matching the distribution imposed by the scheduler κt, and conditioned on that it is still the ε token at time t. κt 1κt κt 1κt Training loss. In order to train model thats transport sequences via insertions, uθ (xXt), where = ins(Xt, i, a) for some and (18) 24 we would need to marginalize out the auxiliary process Zt and the data X1. Havasi et al. (2025) showed this can be done by using loss based on any Bregman divergence while summing up over all possible sequences such that = frm-blanks(z). Concretely, given convex function ϕ that defines Bregman divergence Dϕ(a, b) = ϕ(a) ϕ(b) b, db ϕ(b), we can use the loss EXt,Ztpt(Xt,ZtX1),X1pdataDϕ (cid:16) (cid:80) ut(, zXt, Zt, X1), uθ (Xt) (cid:17) Plugging in the entropy ϕ(u) = u, log u, this results in the Edit Flow loss Et,pt(Xt,ZtX1),X1pdata (cid:88) x=Xt uθ (xXt) (cid:88) i= 1[Zi =ε] κt 1 κt log uθ (ins(Xt, j, 1)Xt) , . (19) (20) where is the position in Xt that corresponds to the first non-ε token on the left of inserting at the i-th position corresponds to changing the value of t from ε to 1 . . This ensures that Loss simplification. We deviate from Havasi et al. (2025) and use t-independent parameterization. In particular, for sequences that are one token insertion of Xt, i.e., = ins(Xt, i, a), we use uθ (cid:0)ins(Xt, i, a)Xt (cid:1) = κt 1 κt λi(Xt)Qi(aXt), (21) where the neural network parameterizes λ and Q. Using this parameterization, letting Aj be the set of missing tokens to the right of position of Xt, the training loss (20) can be decomposed into (cid:18) κt (cid:19) (cid:32) n(Xt) (cid:88) E(... ) =E(... ) 1 κt (cid:18) κt 1 κt j=1 (cid:32) (cid:19) n(Xt) (cid:88) λj(Xt) n(Xt) (cid:88) (cid:88) j= aAj log (cid:0)λj(Xt)Qj(aXt)(cid:1) (cid:33) (cid:33) λj(Xt) Ai log λj(Xt) + j=1 (cid:124) (cid:123)(cid:122) (4) (cid:125) (cid:88) aAj (cid:124) log Qj(aXt) + const. (cid:123)(cid:122) (6) (cid:125) (22) (23) which recovers the losses for λ and in (4) and (6) respectively, after removing the coefficient . While keeping this coefficient relates the loss the to an evidence lower bound (Havasi et al., 2025), we found that removing this coefficient in the loss gave better results in practice. κt 1κt D.1 Interleaved time schedule In order to model image insertions, we would make choice. We can either (i) fully denoise images at the time of insertion, or (ii) insert only noise and denoise later. We choose the latter approach, as this allows simultaneous generation across images and text, and provides the best parallelism as only single model forward at each step is needed for both modalities. Without loss of generality, assume there is only single image. Generation starts by advancing the sequence time, denoted ttext = 0. When the image is inserted, we associate the image with its own time timg. The main difficulty is that we can not simply set timg and ttext independently during training, as evidently we always have ttext timg. In fact, an independent scheduler induces the wrong distribution for our insertion prediction, and it will not insert the correct distribution at generation time. Instead, we need to ensure that training and generation see the same distribution of time values. To achieve this, we first note that the image exists in the sequence according to the scheduler κt, which means that the insertion times are distributed according to p(tinsert) = κt, (24) 25 where tinsert is the time at which an image is inserted, i.e., κt is the cumulative distribution function (CDF) of the insertion times. Equivalently, to sample the insertion time, we can apply the inverse CDF sampling, tinsert = κ1(u), Unif(0, 1). (25) If we set timg = 0 when an image is inserted, then the difference between ttext and timg is distributed according to the insertion time. This gives us the relation ttext timg = tinsert (26) when 0 ttext, timg, tinsert 1. Since ttext will reach 1 before timg, and we want to train for the entire process until timg = 1, we can construct an extended time interval τtext [0, 2], ttext = clip(τtext), where clip(τ ) = min{1, max{0, τ }} clips the time values back into the interval [0, 1]. During training, we first sample τtext, then sample τimg = τtext κ1(u), Unif(0, 1). (27) (28) This will sample an extended time for the image in the internal [-1, 2]. If τimg < 0, then it has not yet been inserted, hence it is deleted from the sequence. Otherwise, it is clip, and we proceed to use the Flow Matching loss (9) to train the image denoising. timg = clip(τimg), (29)"
        },
        {
            "title": "E Sampling and Training Algorithms",
            "content": "Algorithm 1 OneFlow interleaved textimage generation. 1: function OneFlowGeneration(step size t, schedule κ) 2: empty sequence ttext 0 while ttext < 1 or : timg(Y ) < 1 do X, I, ttext, timg OneFlowStep(X, I, ttext, timg, t, κ) end while return and {VAEDec(Y ) : I} 3: 4: 5: 6: 7: 8: 9: end function Text tokens (initially empty set) Set of image latents with per-image times Decode VAE latents into image space Algorithm 2 OneFlow step function. is the token sequence, is the set of image latents each with time timg(Y ). 1: function OneFlowStep(X, I, ttext, timg, t, κ) 2: ({π, λnonzero, Q}, {v(Y, )}Y I) OneFlowModel(X, I, timg) for all with timg(Y ) < 1 do timg min{1 timg(Y ), t} + timg v(Y, timg(Y )) timg(Y ) timg(Y ) + timg end for ttext min{1 ttext, t} if ttext > 0 then for all positions {1, . . . , n(X)} do 1 πi pπ κ(ttext) pλ 1κ(ttext) λi ttext do-insert Bernoulli(pπ if do-insert then Qi( X) ins(X, i, a) if = <image> then nonzero ) and Bernoulli(pλ ) (0, I), timg(Y ) 0, {Y } Image: Flow matching step on images Text: parallel insertions If using (4) without π, then skip this step If = <image>, this will insert Nimg tokens 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end if end if end for end if ttext ttext + ttext return X, I, ttext, timg 24: 25: end function 27 Algorithm 3 OneFlow training loss with interleaved schedule 1: function OneFlowTrainingStep(data sequence X, image latents I, schedule κ) 2: 3: 4: if < κ(ttext) where Unif(0, 1) then Keep each ground-truth token with prob κ(ttext) to get noisy Xt τtext Unif[0, 2] ttext min{1, τtext} 0 Xt [] for all xi do Xt Xt + [xi] + 1 Aj {} else Aj Aj {X i} end if end for It {} for all images do Y1 VAEEnc(img) Unif(0, 1) τimg(Y ) τtext κ1(u) if τimg < 0 then add <image> to the appropriate Ai else timg(Y ) min{1, τimg(Y )} Y0 (0, I) Yt timg(Y )Y1 + (1 timg(Y ))Y0 It It {Yt} end if end for Record the deleted tokens at each position in Aj Image is deleted at this snapshot {π, λnonzero, Q} OneFlowModel(Xt, It) Forward pass Ltext + Limage Compute text and image losses (7) and (9) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: Θ optimizer_step(L; Θ) 31: 32: end function Compute gradients and update model"
        }
    ],
    "affiliations": [
        "CNRS",
        "FAIR at Meta",
        "Grenoble INP",
        "Inria",
        "LJK, France",
        "Univ. Grenoble Alpes"
    ]
}