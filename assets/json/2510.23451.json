{
    "paper_title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences",
    "authors": [
        "Zhuoran Jin",
        "Hongbang Yuan",
        "Kejian Zhu",
        "Jiachun Li",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 5 4 3 2 . 0 1 5 2 : r Preprint OMNI-REWARD: TOWARDS GENERALIST OMNIMODAL REWARD MODELING WITH FREE-FORM PREFERENCES Zhuoran Jin1,2,*, Hongbang Yuan1,2,*, Kejian Zhu1,2,*, Jiachun Li1,2, Pengfei Cao1,2, Yubo Chen1,2, Kang Liu1,2, Jun Zhao1,2 1School of Artificial Intelligence, University of Chinese Academy of Sciences 2Institute of Automation, Chinese Academy of Sciences {zhuoran.jin, hongbang.yuan} @nlpr.ia.ac.cn zhukejian2025@ia.ac.cn {jiachun.li, pengfei.cao, yubo.chen, kliu, jzhao} @nlpr.ia.ac.cn ABSTRACT Reward models (RMs) play critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with freeform preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks. Benchmark Dataset Model Code https://hf.co/datasets/HongbangYuan/OmniRewardBench https://hf.co/datasets/jinzhuoran/OmniRewardData https://hf.co/jinzhuoran/OmniRewardModel https://github.com/HongbangYuan/OmniReward"
        },
        {
            "title": "INTRODUCTION",
            "content": "To achieve more human-like intelligence (Shams & Seitz, 2008), artificial general intelligence (AGI) is increasingly advancing toward an omni-modal paradigm (Wu et al., 2024; Fang et al., 2024; Xie et al., 2024), where AI models are expected to process and generate information across diverse modalities (i.e., any-to-any models). Benefiting from the rapid progress in large language models (LLMs) (Dubey et al., 2024; Yang et al., 2024), researchers are extending their powerful text-centric capabilities to other modalities such as images, video, and audio, enabling models (e.g., GPT-4o (OpenAI, 2024), Gemini 2.0 Flash (DeepMind, 2025), and Qwen2.5-Omni (Xu et al., 2025)) to not only understand multimodal inputs but also generate outputs using the most appropriate modality. Despite the remarkable progress that existing omni-modal models have achieved on textual, visual, and auditory tasks, aligning their behaviors with human preferences remains fundamental challenge (Ji et al., 2024; Yu et al., 2024b; Zhang et al., 2025). For example, models may fail to follow user instructions in speech-based interactions (i.e., helpfulness), respond to sensitive prompts with harmful videos (i.e., harmlessness), or generate hallucinated content when describing images (i.e., trustworthy). Reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019; Ouyang et al., 2022) has emerged as promising approach for aligning model behaviors with human preferences. RLHF * These authors contributed equally to this work. 1 Preprint integrates human feedback into the training loop by using it to guide the model toward more desirable and human-aligned responses. This process (Dong et al., 2024) involves collecting human preference data to train reward model (RM), which is subsequently used to fine-tune the original model through reinforcement learning by providing reward signals that guide its behavior. Therefore, RMs play pivotal role in RLHF, acting as learned proxy of human preferences. However, current RMs face two challenging problems: (1) Modality Imbalance: Most existing RMs (Park et al., 2024; Liu et al., 2024a; Zang et al., 2025b) predominantly focus on text and image modalities, while offering limited support for other modalities such as video and audio. With the development of omni-modal models, achieving alignment in both understanding and generation across underrepresented modalities is becoming critically important; (2) Preference Rigidity: Current preference data (Kirstain et al., 2023; Liu et al., 2024a) is typically collected based on broadly accepted high-level values, such as helpfulness and harmlessness. RMs are then trained on these binary preference pairs, resulting in fixed and implicit notion of preference embedded within the model. Nevertheless, because human preferences cannot be neatly categorized into binary divisions, this paradigm fails to capture the diversity of personalized preferences (Lee et al., 2024). Omni-Reward, step towards universal omniConsidering the above challenges, we propose modal reward modeling with free-form preferences. For modality imbalance, Omni-Reward should be able to handle all modalities used in omni-modal models, including those that are rarely covered in existing preference data, such as video and audio. It should also support reward shaping for complex multimodal tasks, such as image editing, video understanding, and audio generation, enabling broad range of real-world applications. For preference rigidity, Omni-Reward should not only capture general preferences grounded in widely shared human values, but also be capable of dynamically adjusting reward scores based on specific free-form preferences and multi-dimensional evaluation criteria. To achieve this goal, we design Omni-Reward based on three key aspects: Evaluation: RM evaluations (Lambert et al., 2024; Liu et al., 2024c; Zhou et al., 2024) have primarily focused on text-only tasks, with recent efforts extending to visual understanding and generation (Wu et al., 2023a; Li et al., 2024a; Chen et al., 2024b). Moreover, most RM benchmarks emphasize general preference judgments, while largely overlooking user-specific preferences and modality-dependent evaluation needs. To address these gaps, we introduce Omni-RewardBench, an omni-modal reward modeling benchmark with free-form preferences, designed to evaluate the performance of RMs across diverse modalities. Specifically, we collect prompts from various tasks and domains, elicit modality-specific responses from multiple models, and employ three annotators to provide free-form preference descriptions and label each response pair as chosen, rejected, or tied. Ultimately, Omni-RewardBench includes 3,725 high-quality human-annotated preference pairs, encompassing 9 distinct tasks and covering modalities such as text, image, video, audio, and 3D data. Data: Current RMs are built upon large amounts of high-quality preference data. However, these preference datasets are typically designed for specific tasks and preferences, making it challenging for RMs to adapt to unseen multimodal tasks or user preferences. To enhance generalization, we construct Omni-RewardData, large-scale multimodal preference dataset that spans wide range of tasks. We collect existing preference datasets to support general preference learning, and propose in-house instruction-tuning data to help RMs understand user preferences expressed in free-form language. Omni-RewardData comprises 248K general and 69K fine-grained preference pairs. Model: Building on Omni-RewardData, we further introduce two omni-modal reward models: Omni-RewardModel-BT and Omni-RewardModel-R1. First, we train discriminative RM named Omni-RewardModel-BT on the full Omni-RewardData using classic BradleyTerry objective. Despite strong performance, its scoring process lacks transparency. To address this, we explore reinforcement learning approach to train generative RM, named Omni-RewardModel-R1. It encourages the RM to engage in explicit reasoning by generating textual critic in addition to producing scalar score, and it is trained with only 3% of the Omni-RewardData. Built upon Omni-RewardBench, we conduct thorough evaluation of multimodal large language models (MLLMs) used as generative RMs, including GPT-4o (OpenAI, 2024), Gemini-2.0 (DeepMind, 2025), Qwen2.5-VL (Bai et al., 2025), and Gemma-3 (Team, 2025), as well as several purpose-built RMs for multimodal tasks, such as IXC-2.5-Reward (Zang et al., 2025a) and UnifiedReward (Wang et al., 2025). Our experimental results reveal the following findings: (1) Omni-RewardBench presents significant challenges for current MLLMs, especially under the w/ 2 Preprint Ties setting. The strongest commercial model, Claude 3.5 Sonnet (Anthropic, 2024b), achieves the highest accuracy at 66.54%, followed closely by the open-source Gemma-3 27B at 65.12%, while existing purpose-built multimodal RMs still lag behind, indicating substantial room for improvement. (2) There indeed exists the modality imbalance problem, particularly evident in the poor performance of existing models on tasks such as text-to-audio, text-to-3D, and text-image-to-image. (3) RM performance is significantly correlated across various multimodal understanding (or generation) tasks, suggesting certain degree of generalization potential within similar task categories. Building on the findings above, we further evaluate how well Omni-RewardModel addresses the limitations of existing RMs. Our experiments uncover the key insights below: (1) Omni-RewardModel achieves strong performance on Omni-RewardBench, attaining 73.68% accuracy under the w/o Ties setting and 65.36% accuracy under the w/ Ties setting, and shows strong generalization to challenging tasks. (2) Omni-RewardModel also captures general human preferences and achieves performance comparable to or even better than the state-of-the-art (SOTA) on public RM benchmarks such as VL-RewardBench (Li et al., 2024a) and Multimodal RewardBench (Yasunaga et al., 2025). (3) Instruction-tuning is crucial for RMs, as it effectively alleviates the preference rigidity issue and enables the model to dynamically adjust reward scores according to free-form user preferences. In summary, our contributions are as follows: (1) We present Omni-RewardBench, the first omni-modal reward modeling benchmark with free-form preferences, designed to systematically evaluate the performance of RMs across diverse modalities. It includes nine multimodal tasks and 3,725 high-quality preference pairs, posing significant challenges to existing multimodal RMs, revealing substantial room for improvement. (2) We construct Omni-RewardData, multimodal preference dataset comprising 248K general preference pairs and 69K newly collected instruction-tuning pairs with free-form preference descriptions, enabling RMs to generalize across modalities and align with diverse user preferences. (3) We propose Omni-RewardModel, including the discriminative Omni-RewardModel-BT and the generative Omni-RewardModel-R1. Our model not only demonstrates significant improvement on Omni-RewardBench, with 20% accuracy gain over the base model, but also achieves performance comparable to or even exceeding that of SOTA RMs on public benchmarks."
        },
        {
            "title": "2 OMNI-REWARDBENCH",
            "content": "In this section, we introduce Omni-RewardBench, an omni-modal reward modeling benchmark with free-form preferences for evaluating the RM performance across diverse modalities. Table 7 presents comprehensive comparison between Omni-RewardBench and existing multimodal reward modeling benchmarks. Omni-RewardBench covers 9 tasks across image, video, audio, text, and 3D modalities, and incorporates free-form preferences to support evaluating RMs under diverse criteria. Figure 5 illustrates the overall construction workflow, including prompt collection ( 2.2), response generation ( 2.2), criteria annotation ( 2.3), and preference annotation ( 2.3). 2.1 TASK DEFINITION AND SETTING Each data sample in Omni-RewardBench is represented as (x, y1, y2, c, p), where denotes the input prompt, y1 and y2 are two candidate responses generated by AI models, specifies the free-form user preference or evaluation criterion, and indicates the preferred response under the given criterion c. An effective RM is expected to correctly predict given (x, y1, y2, c). We provide two evaluation settings: (1) w/o Ties (ties-excluded), where {y1, y2}, requiring strict preference between the two responses; (2) w/ Ties (ties-included), more challenging setting where {y1, y2, tie}, allowing for the case where the two responses are equally preferred under the given criterion. 2.2 DATASET COLLECTION Figure 1 provides an overview of the nine tasks covered in Omni-RewardBench, spanning wide range of modalities. Detailed descriptions of each task are provided below. Text-to-Text (T2T): T2T refers to the text generation task of outputting textual responses based on user instructions, which represents fundamental capability of LLMs. In this task, denotes the user instruction, and denotes the textual response. We collect prompts from real-world downstream 3 Preprint Figure 1: Illustration of nine reward modeling tasks in Omni-RewardBench. tasks across diverse scenarios in RMB (Zhou et al., 2024) and RPR (Pitis et al., 2024), covering tasks like open QA, coding, and reasoning. Subsequently, we include responses generated by 13 LLMs. Text-Image-to-Text (TI2T): TI2T denotes the image understanding task of generating textual responses based on textual instructions and image inputs. In this task, represents pair consisting of user instruction and an image, and denotes the textual response. We consider image understanding tasks with varying levels of complexity. We first collect general instructions from VL-Feedback (Li et al., 2024b), and subsequently gather meticulously constructed, layered, and complex instructions from MIA-Bench (Qian et al., 2025). The responses are collected from 14 MLLMs. Text-Video-to-Text (TV2T): TV2T refers to the video understanding task of generating textual In this task, indicates user responses based on both textual instructions and video inputs. instruction and video, and indicates the corresponding textual response. We collect video-question pairs from VCGBench-Diverse (Maaz et al., 2024), which contains range of video categories and diverse user questions. The durations of the selected videos range from 30 to 358 s, with an average of 207 s. We collect responses from 4 MLLMs equipped with video understanding capabilities. Text-Audio-to-Text (TA2T): TA2T denotes the audio understanding task of generating textual responses based on both textual instructions and audio inputs. In this task, denotes the paired input of user instruction and an audio clip, and denotes the textual response. We collect diverse, open-ended questions from OpenAQA (Gong et al., 2024), each paired with an approximately 10 audio clip. Subsequently, responses are collected from 4 MLLMs capable of audio understanding. Text-to-Image (T2I): T2I denotes the image synthesis task of generating high-fidelity images based on user textual prompts. In this task, denotes the textual description, and denotes the generated image. We collect diverse manually-written prompts that reflect the general interests of model users, along with corresponding images from Rapidata (Rapidata, 2024) and HPDv2 (Wu et al., 2023a), covering 27 text-to-image models ranging from autoregressive-based to diffusion-based architectures. Text-to-Video (T2V): T2V denotes the video synthesis task of generating temporally coherent videos from textual descriptions. In this task, denotes the input textual description, and denotes the corresponding generated video. We collect human-written prompts from GenAI-Bench (Jiang et al., 2024) and subsequently acquire the corresponding videos generated by up to 8 text-to-video models. 4 Preprint Text-to-Audio (T2A): T2A denotes the audio generation task of synthesizing audio clips with temporal and semantic consistency from textual descriptions. In this task, denotes the textual description, and denotes the generated audio. We collect various prompts from Audio-alpaca (Majumder et al., 2024) and responses from the latent diffusion model Tango (Ghosal et al., 2023). Text-to-3D (T23D): T23D denotes the 3D generation task of synthesizing three-dimensional objects from textual descriptions. In this task, is the textual prompt, and denotes the generated 3D object. We collect user prompts from 3DRewardDB (Ye et al., 2024) and responses from the multi-view diffusion model mvdream-sd2.1-diffusers (Shi et al., 2024). The responses are presented in the multi-view rendered format of each 3D object, enabling direct image-based input to MLLMs. Text-Image-to-Image (TI2I): TI2I denotes the image editing task of modifying an image based on textual instructions. In this task, denotes source image and an editing prompt, and denotes the edited image. We collect images to be edited and user editing prompts from GenAI-Bench (Jiang et al., 2024). The responses are generated with broad range of diffusion models. 2.3 CRITERIA AND PREFERENCE ANNOTATION Following the collection of user prompts and corresponding responses, the evaluation criteria and the user preference are subsequently annotated. For the criteria annotation, each annotator manually creates multiple evaluation criteria in textual form based on the input x. For the preference annotation, each data sample is independently labeled by three annotators based on the free-form evaluation criteria. To ensure data quality, we first discarded 23% of instances with invalid criteria annotations, followed by 15% with conflicting preferences. The entire annotation process is conducted by three PhD students in computer science, guided by detailed guidelines and supported by an annotation platform in Appendix D. Ethics and quality control during data annotation are detailed in Appendix E. total of 3,725 preference data are finally collected, covering 9 tasks across all modalities. More detailed statistics of Omni-RewardBench are provided in Table 8 and Table 9."
        },
        {
            "title": "3 OMNI-REWARDMODEL",
            "content": "In this section, we first construct Omni-RewardData, multimodal preference dataset comprising 248K general preference pairs and 69K newly collected instruction-tuning pairs with free-form preference descriptions for RM training. Based on the dataset, we propose two omni-modal RMs: Omni-RewardModel-BT (discriminative RM) and Omni-RewardModel-R1 (generative RM). 3.1 OMNI-REWARDDATA CONSTRUCTION High-quality and diverse human preference data is crucial for training effective omni-modal RMs. However, existing preference datasets are often limited in scope because they focus on specific tasks or general preferences. This limitation hinders the models ability to generalize to novel multimodal scenarios and adapt to multiple user preferences. To improve the generalization ability of RMs, we construct Omni-RewardData, which primarily covers four task types: T2T, TI2T, T2I, and T2V, and comprises total of 317K preference pairs, including both general and fine-grained preferences. Table 1: Data statistics of Omni-RewardData. * denotes the subset constructed in this work. Task Subset T2T T2I Skywork-Reward-Preference Omni-Skywork-Reward-Preference* Omni-UltraFeedback* HPDv2 EvalMuse Omni-HPDv2* Omni-Open-Image-Preferences* #Size 50,000 16,376 7, 50,000 2,944 8,959 8,105 TI2T RLAIF-V OmniAlign-V-DPO Omni-RLAIF-V* Omni-VLFeedback* 83,124 50,000 15,867 12,311 Specifically, we first collect substantial amount of existing preference datasets to help the model learn general preferences. The details are as follows: (1) For T2T, we select 50K data from Skywork-Reward-Preference (Liu et al., 2024a), high-quality dataset that provides binary preference pairs covering wide range of instructionfollowing tasks. (2) For TI2T, we use select 83K data from RLAIF-V (Yu et al., 2024c), multimodal preference dataset that targets trustworthy alignment and hallucination reduction of MLLMs. Moreover, we also include 50K data from OmniAlign-V-DPO (Zhao et al., 2025), which features diverse VideoDPO VisionRewardDB-Video T2V 10,000 1,795 5 Preprint Figure 2: Overview of the architecture of Omni-RewardModel. images, open-ended questions, and varied response formats. (3) For T2I, we sample 50K data from HPDv2 (Wu et al., 2023a), well-annotated dataset containing human preference judgments on images generated by text-to-image generative models. In addition, we adopt EvalMuse (Han et al., 2024), which provides large-scale human annotations covering both overall and fine-grained aspects of image-text alignment. (4) For T2V, we collect 10K samples from VideoDPO (Liu et al., 2024b), which evaluates both the visual quality and semantic alignment. We also integrate 2K preference pairs from VisionReward (Xu et al., 2024). Moreover, as these data primarily reflect broadly accepted and general preferences, RMs trained solely on them often struggle to adapt reward assignment based on user-specified fine-grained preferences or customized evaluation criteria. Therefore, we propose constructing instruction-tuning data specifically for RMs, where each data instance is formatted as (c, x, y1, y2, p). We first sample preference pairs (x, y1, y2) from existing datasets, and prompt GPT-4o to generate free-form instruction reflecting user preference that supports either y1 or y2, together with the corresponding label p. To ensure quality, we use GPT-4o-mini, Qwen2.5-VL 7B, and Gemma-3-12B-it to verify the consistency of (c, x, y1, y2) with the label p. We obtain the following in-house subset: (1) For T2T, we construct 24K data based on Skywork-Reward-Preference (Liu et al., 2024a) and UltraFeedback (Cui et al., 2024). (2) For TI2T, we synthesize 28K data based on RLAIF-V and VLFeedback (Li et al., 2024b). (3) For T2I, we generate 17K data using HPDv2 and Open-Image-Preferences (is Better Together, 2024). The statistics of Omni-RewardData are shown in Table 1. 3.2 DISCRIMINATIVE REWARD MODELING WITH BRADLEY-TERRY Following standard practice in reward modeling, we adopt the Bradley-Terry loss (Bradley & Terry, 1952) for training our discriminative RM where scalar score is assigned to each candidate response: LBT = log exp(rBT(c, x, yc)) exp(rBT(c, x, yc)) + exp(rBT(c, x, yr)) , (1) where denotes an optional instruction that specifies user preference, yc denotes the chosen response, yr denotes the rejected response, rBT() denotes the reward function. Specifically, we train Omni-RewardModel-BT on Omni-RewardData using MiniCPM-o-2.6 (Yao et al., 2024). As shown in Figure 2(1), we freeze the parameters of the vision and audio encoders, and only update the language model decoder and the value head. User-specific preferences and task-specific evaluation criteria are provided as system messages, allowing the RM to adapt its scoring behavior accordingly. 3.3 GENERATIVE REWARD MODELING WITH REINFORCEMENT LEARNING To improve the interpretability of the reward scoring process, we further explore reinforcement learning approach for training pairwise generative reward model, denoted as Omni-RewardModel-R1. As shown in Figure 2(2), given the input (c, x, y1, y2), the model rR1() is required to first generate Chain-of-Thought (CoT) explanation e, followed by preference prediction p. We optimize the 6 Preprint Table 2: Evaluation results on Omni-RewardBench under the w/ Tie setting. Model T2T TI2T TV2T TA2T T2I T2V T2A T23D TI2I Overall Phi-4-Multimodal-Instruct Qwen2.5-Omni-7B MiniCPM-o-2.6 MiniCPM-V-2.6 LLaVA-OneVision-7B-ov Mistral-Small-3.1-24B-Instruct-2503 Skywork-R1V-38B Qwen2-VL-7B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-32B-Instruct Qwen2.5-VL-72B-Instruct InternVL2_5-4B InternVL2_5-8B InternVL2_5-26B InternVL2_5-38B InternVL2_5-8B-MPO InternVL2_5-26B-MPO InternVL3-8B InternVL3-9B InternVL3-14B Gemma-3-4B-it Gemma-3-12B-it Gemma-3-27B-it GPT-4o Gemini-1.5-Flash Gemini-2.0-Flash GPT-4o-mini Claude-3-5-Sonnet-20241022 Claude-3-7-Sonnet-20250219-Thinking PickScore HPSv2 InternLM-XComposer2.5-7B-Reward UnifiedReward UnifiedReward1.5 Omni-RewardModel-R1 Omni-RewardModel-BT 70.98 65.71 61.39 57.55 50.84 74.58 77.94 63.55 53.00 68.59 74.82 76.98 57.55 60.43 64.75 69.06 65.95 70.74 76.02 73.86 76.74 74.34 73.62 77.22 78.18 72.90 74.10 76.50 76.74 75. 42.93 43.41 59.95 60.19 59.47 71.22 75.30 Open-Source Models 55.74 59.66 60.50 - - - - - - - - - - - - - - - - - - - - - 62.53 56.66 60.95 53.27 45.37 68.62 67.72 59.37 51.24 68.40 63.88 68.40 55.30 54.63 62.98 64.56 68.17 70.43 67.95 66.59 68.62 68.40 66.14 67.04 53.60 55.11 51.89 54.73 42.23 57.98 59.47 55.30 49.05 53.03 60.23 61.17 50.76 49.62 57.01 54.73 52.46 60.98 58.71 57.39 61.74 56.82 58.52 61.17 Proprietary Models 62.75 57.42 61.90 - - - 69.30 68.62 60.50 67.95 67.04 68. 61.74 58.52 54.92 60.23 61.55 63.83 Specialized Models 43.56 45.27 52.65 53.22 54.17 56.06 60.23 46.95 44.70 65.69 69.53 69.30 63.88 68.85 - - - - - - 70.59 35.36 55.99 47.35 48.92 43.42 58.55 47.94 33.20 47.74 60.51 60.51 58.94 48.72 54.42 56.97 54.81 56.97 58.74 57.37 57.37 60.51 60.31 59.33 59. 59.33 62.48 62.28 57.56 61.69 62.28 60.12 63.85 45.19 59.72 58.35 61.69 58.35 32.14 50.85 39.70 44.61 40.08 59.92 45.94 61.25 51.23 47.83 62.38 56.52 47.07 49.53 49.72 40.26 52.55 47.26 48.77 51.80 61.25 60.30 62.57 61.44 65.03 63.52 67.49 65.22 64.27 62.38 66.92 64.65 61.25 70.32 69.57 58.22 64.08 44.77 32.60 21.90 - - - - - - - - - - - - - - - - - - - - - 44.53 32.85 31.87 - - - - - - - - - 63.99 24.17 43.71 37.09 39.40 35.43 60.60 43.71 42.38 45.36 50.99 62.58 59.60 47.35 42.72 57.28 55.96 52.98 56.95 51.66 60.93 59.27 54.64 56.95 63.91 70.86 62.25 68.54 60.26 68.54 68.21 59.27 61.26 43.05 59.93 61.59 63.91 67.88 22.71 43.23 39.30 36.68 37.12 62.88 41.92 10.04 44.54 41.05 69.43 62.01 47.16 44.10 48.03 46.72 41.05 48.03 43.67 47.16 55.02 54.15 56.33 65. 69.87 63.32 65.50 60.26 65.94 63.76 51.53 55.02 9.61 42.36 45.41 46.29 58.95 Average 67.32 55.52 63. 59.66 55.31 55.59 34.75 53.98 48. 44.67 51.50 46.67 47.88 42.07 63.30 54.95 46.44 48.88 55.77 64.83 63.37 50.56 50.78 56.68 55.16 55.73 59.02 57.74 59.30 63.31 61.28 61.92 65.12 64.62 60.21 60.79 64.00 66.54 66.44 53.04 54.02 48.20 59.32 59.69 60.18 65.36 56.68 model using the GRPO-based reinforcement learning (DeepSeek-AI et al., 2025), where the reward signal is computed by comparing the predicted preference with the ground-truth preference p. We train Omni-RewardModel-R1 from scratch on 10K samples from Omni-RewardData, using Qwen2.5-VL-7B-Instruct (Bai et al., 2025) as the base model, without distillation from larger models."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we conduct comprehensive evaluation of wide range of multimodal reward models, including generative RMs based on MLLMs and specialized RMs trained for task-specific objectives, as well as our proposed Omni-RewardModel. Moreover, we also extend the evaluation to include widely adopted benchmarks from prior work in multimodal reward modeling. 4.1 BASELINE REWARD MODELS Generative Reward Models. We evaluate 30 generative RMs built upon state-of-the-art MLLMs, including 24 open-source and 6 proprietary models. The open-source models cover both omnimodal (e.g., Phi-4 (Abouelenin et al., 2025), Qwen2.5-Omni (Xu et al., 2025), MiniCPM-o-2.6 (Yao et al., 2024)) and vision-language models (e.g., Qwen2-VL (Wang et al., 2024b), Qwen2.5-VL (Bai et al., 2025), InternVL2.5 (Chen et al., 2024c), InternVL3 (Zhu et al., 2025a), and Gemma3 (Team, 2025)), with sizes ranging from 3B to 72B. For proprietary models, we consider the GPT (OpenAI, 2023), Gemini (DeepMind, 2025), and Claude (Anthropic, 2024a) series. Specifically, we use GPT-4o-Audio-Preview in place of GPT-4o for the TA2T and T2A tasks. Specialized Reward Models. We evaluate several custom RMs that are specifically trained on particular reward modeling tasks. PickScore (Kirstain et al., 2023) and HPSv2 (Wu et al., 2023b) 7 Preprint Table 3: Evaluation results on Omni-RewardBench under the w/o Tie setting. Model T2T TI2T TV2T TA2T T2I T2V T2A T23D TI2I Overall Phi-4-Multimodal-Instruct Qwen2.5-Omni-7B MiniCPM-o-2.6 MiniCPM-V-2.6 LLaVA-OneVision-7B-ov Mistral-Small-3.1-24B-Instruct-2503 Skywork-R1V-38B Qwen2-VL-7B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-32B-Instruct Qwen2.5-VL-72B-Instruct InternVL2_5-4B InternVL2_5-8B InternVL2_5-26B InternVL2_5-38B InternVL2_5-8B-MPO InternVL2_5-26B-MPO InternVL3-8B InternVL3-9B InternVL3-14B Gemma-3-4B-it Gemma-3-12B-it Gemma-3-27B-it GPT-4o Gemini-1.5-Flash Gemini-2.0-Flash GPT-4o-mini Claude-3-5-Sonnet-20241022 Claude-3-7-Sonnet-20250219-Thinking PickScore HPSv2 InternLM-XComposer2.5-7B-Reward UnifiedReward UnifiedReward1.5 Omni-RewardModel-R1 Omni-RewardModel-BT 81.15 82.79 74.04 74.86 66.67 84.43 88.25 79.78 68.58 80.87 86.34 87.70 69.95 72.13 77.60 84.15 75.96 80.87 84.70 83.06 85.79 83.88 81.69 88.25 86.89 83.88 85.25 87.43 88.25 84. 49.18 49.18 68.85 68.58 67.76 81.77 85.79 Open-Source Models 63.47 63.77 69.76 - - - - - - - - - - - - - - - - - - - - - 74.74 78.16 71.58 69.47 53.42 79.47 76.84 76.58 60.00 78.95 77.37 80.53 64.47 65.00 76.32 70.53 77.63 80.53 76.84 78.42 77.11 77.37 78.42 78.16 68.14 68.14 66.05 65.12 57.67 65.79 74.42 70.00 66.05 66.28 74.19 74.65 63.49 64.88 72.79 66.05 65.12 73.72 71.63 70.23 74.65 73.02 72.09 75.58 Proprietary Models 70.96 62.28 67.96 - - - 77.11 78.16 75.26 77.89 78.68 77. 75.58 69.53 67.91 74.65 76.28 76.28 Specialized Models 53.49 55.12 64.19 59.77 67.39 69.53 72.79 54.47 51.58 74.74 79.47 78.68 75.53 79.47 - - - - - - 75.45 46.03 65.53 58.50 57.37 51.93 65.99 55.10 37.41 52.15 65.53 70.29 71.88 58.50 64.40 68.03 66.67 65.99 68.93 69.39 65.31 72.79 72.34 71.20 68. 69.61 71.43 70.52 67.80 70.75 70.07 69.61 73.70 51.47 68.93 67.57 71.20 67.12 51.72 63.09 61.16 58.15 51.72 68.67 57.94 68.03 60.09 64.59 70.39 67.17 54.94 61.59 62.88 63.30 61.80 62.66 65.67 65.67 68.24 66.09 71.03 71.03 73.18 71.89 74.25 74.89 72.53 70.60 75.97 73.61 68.24 79.83 78.97 62.02 72.75 55.05 50.76 54.80 - - - - - - - - - - - - - - - - - - - - - 53.28 40.66 60.86 - - - - - - - - - 66.41 39.02 56.44 54.92 51.14 43.94 67.80 45.83 47.35 51.89 64.77 68.56 66.67 50.38 58.33 68.56 68.94 62.88 67.80 59.85 71.97 68.56 67.05 67.05 73.86 77.65 74.24 79.17 71.59 77.65 76.89 67.05 70.45 46.59 68.56 70.45 72.35 77.65 49.28 54.11 48.79 53.62 43.48 71.98 52.66 12.08 53.62 50.72 70.05 69.57 41.55 53.14 59.90 57.97 55.07 60.87 53.62 58.45 58.94 63.77 65.70 71. 73.91 73.43 71.98 66.67 72.46 72.46 57.49 60.87 56.04 46.86 50.72 55.56 65.70 Average 78.38 68.57 73. 66.37 64.61 66.62 52.57 63.54 58. 58.73 64.75 62.18 61.39 52.69 72.02 64.43 55.89 58.91 67.39 73.88 74.02 57.61 62.78 69.44 68.23 66.35 70.77 68.81 70.44 72.30 71.93 72.45 75.27 73.13 69.50 72.57 74.42 76.66 75.52 61.04 62.07 61.45 67.43 68.79 69.71 73.68 67.29 are CLIP-based scoring functions trained for image generation tasks. InternLM-XComposer2.5-7BReward (Zang et al., 2025a) broadens the scope to multimodal understanding tasks that cover text, images, and videos. UnifiedReward (Wang et al., 2025) further incorporates both generation and understanding capabilities across image and video modalities. 4. IMPLEMENTATION DETAILS We conduct experiments under two evaluation settings: w/o Ties and w/ Ties. For the w/o Ties setting, we exclude all samples labeled as tie and require the model to choose the preferred response from {y1, y2}. For the w/ Ties setting, the model is required to select from {y1, y2, tie}. Accuracy is used as the primary evaluation metric. For generative RMs, we adopt pairwise format where the model first generates explicit critiques for both responses, and then produces final preference decision. Prompt templates for generative RMs are detailed in Appendix K. For discriminative RMs, we follow prior work (Deutsch et al., 2023) and define the w/ Ties accuracy as the maximum three-class classification accuracy obtained by varying the tie threshold. More details are shown in Appendix G. 4.3 EVALUATION RESULTS ON OMNI-REWARDBENCH The evaluation results on Omni-RewardBench are shown in Table 2, Table 3 and Figure 3. Limited Performance of Current RMs. The overall performance of current RMs remains limited, particularly under the w/ Ties setting. For instance, the strongest proprietary model, Claude 3.5 Sonnet, achieves an accuracy of 66.54%, while the best-performing open-source model, Gemma-3 27B, follows closely with 65.12%. In contrast, specialized reward models perform less competitively, with the most capable one, UnifiedReward1.5, achieving only 59.69% accuracy. These results reveal Preprint Figure 3: Performance of open-source models, closed-source models, and our proposed model on the nine tasks in Omni-RewardBench, with results under w/ Tie (left) and w/o Tie (right). Table 4: Evaluation results on VL-RewardBench. Models General Hallucination Reasoning Overall Acc Macro Acc LLaVA-OneVision-7B-ov Molmo-7B InternVL2-8B Llama-3.2-11B Pixtral-12B Molmo-72B Qwen2-VL-72B NVLM-D-72B Llama-3.2-90B Gemini-1.5-Flash Gemini-1.5-Pro Claude-3.5-Sonnet GPT-4o-mini GPT-4o LLaVA-Critic-8B IXC-2.5-Reward UnifiedReward Skywork-VL-Reward Omni-RewardModel-R1 Omni-RewardModel-BT Open-Source Models 20.1 31.8 41.1 38.4 25.9 42.3 32.8 31.6 57.3 Proprietary Models 59.6 72.5 55.0 34.5 67.6 Specialized Models 38.3 62.5 78.4 80.0 90.2 94.2 57.1 56.2 59.0 56.6 59.9 54.9 58.0 62.0 61. 58.4 64.2 62.3 58.2 70.5 59.1 62.9 60.5 61.0 59.0 60.4 32.2 31.1 35.6 33.3 35.6 33.9 38.1 38.9 42.6 47.8 50.8 43.4 41.7 49.1 54.6 84.7 60.6 66.0 71.9 81.5 29.6 37.5 44.5 42.9 35.8 44.1 39.5 40.1 56. 57.6 67.2 55.3 41.5 65.8 41.2 65.8 66.1 73.1 69.6 76.3 36.5 39.7 45.2 42.8 40.4 43.7 43.0 44.1 53.9 55.3 62.5 53.6 44.8 62.4 44.0 70.0 66.5 69.0 73.7 78.7 that current RMs remain inadequate for omni-modal and free-form preference reward modeling, reinforcing the need for more capable and generalizable approaches. Modality Imbalance across Various Tasks. As shown in Figure 3, task-level performance varies considerably, with up to 28.37% gap across modalities. In particular, tasks like T2A, T23D, and TI2I perform notably worse, highlighting persistent modality imbalance, as current reward models primarily focus on text and image, while modalities such as audio and 3D remain underexplored. Strong Performance of Omni-RewardModel. Omni-RewardModel-BT achieves strong performance on the Omni-RewardBench, attaining 73.68% accuracy under the w/o Ties setting and 65.36% accuracy under the w/ Ties setting. It also generalizes well to unseen modalities, achieving SOTA performance on TA2T and T2A tasks. Omni-RewardModel-R1 also surpasses existing specialized RMs in performance while providing better interpretability via explicit reasoning. 4.4 EVALUATION RESULTS ON GENERAL REWARD MODELING BENCHMARKS We further evaluate Omni-RewardModel on other widely-used RM benchmarks to assess its ability to model general human preferences. VL-RewardBench (Li et al., 2024a) evaluates multimodal 9 Preprint Table 5: Evaluation results on Multimodal RewardBench. Model Overall General Correctness Preference Knowledge Reasoning Safety Math Coding Bias Toxicity VQA Llama-3.2-90B-Vision Aria Molmo-7B-D-0924 Llama-3.2-11B-Vision Llava-1.5-13B Claude 3.5 Sonnet Gemini 1.5 Pro GPT-4o 62.4 57.3 54.3 52.4 48. 72.0 72.0 71.5 Omni-RewardModel-BT 70.5 60.0 59.5 56.8 57.8 53.3 62.6 63.5 62.6 71. Open-Source Models 68.4 63.5 59.4 65.8 55.2 Proprietary Models 67.8 67.7 69.0 Specialized Models 58. 61.2 55.5 54.6 55.5 50.5 73.9 66.3 72.0 66.7 56.3 50.3 50.7 50.6 53.5 68.6 68.9 67.6 53.1 54.2 53.4 51.7 49. 65.1 55.5 62.1 52.0 46.1 34.8 20.9 20.1 76.8 94.5 74.8 51.8 54.4 53.8 50.4 50.0 60.6 58.2 58.8 77.1 64.2 60.3 55.8 51. 85.6 87.2 87.2 71.0 48.5 79.3 - 85. Table 6: Ablation results on Omni-RewardBench under the w/ Tie setting. Model T2T TI2T TV2T TA2T T2I T2V T2A T23D TI2I Overall MiniCPM-o-2.6 w/ T2T w/ TI2T w/ T2I & T2V w/ Full w/ Preference-Only 61.39 74.30 74.54 52.28 75.30 54.92 51.89 54.73 59.62 45.83 60.23 49.80 60.95 66.37 66.82 51.47 68.85 64. 60.50 69.75 69.75 59.38 70.59 55.74 47.35 45.38 41.45 58.93 58.35 59.14 39.70 43.86 48.77 64.84 64.08 61.06 21.90 55.96 61.31 56.93 63.99 64.00 37.09 49.67 51.00 67.55 67.88 64.90 39.30 54.15 56.33 60.26 58.95 53. 46.67 57.13 58.84 57.50 65.36 58.67 RMs across general multimodal queries, visual hallucination detection, and complex reasoning tasks. Multimodal RewardBench (Yasunaga et al., 2025) covers six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. In Table 4, Omni-RewardModel achieves SOTA performance on VL-RewardBench, with an accuracy of 76.3%. On Multimodal RewardBench  (Table 5)  , Omni-RewardModel also matches the performance of Claude 3.5 Sonnet."
        },
        {
            "title": "5 ANALYSIS",
            "content": "In this section, we analyze the impact of training data composition in Omni-RewardData and examine the correlations among model performances across tasks in Omni-RewardBench. We further investigate the roles of CoT reasoning, free-form criteria, and scoring strategy in Appendix I. 5.1 IMPACT OF TRAINING DATA COMPOSITION We examine the impact of training data composition on Omni-RewardModel, focusing on two key factors: the use of mixed multimodal data and the incorporation of instruction-tuning. First, to assess the role of mixed multimodal data, we train MiniCPM-o-2.6 separately on (1) T2T, (2) TI2T, and (3) T2I and T2V data. As shown in Tables 6 and 10, while training on single modality yields only marginal improvements, using mixed multimodal data leads to significantly better generalization across tasks. Second, to assess the role of instruction-tuning data, we remove this type of data and train MiniCPM-o-2.6 using only the general preference data in Omni-RewardData. This leads to clear drop in performance, highlighting the importance of instruction-tuning for RMs. Figure 4: across lation Omni-RewardBench. Performance various correin tasks 5.2 CORRELATION OF PERFORMANCE ON DIFFERENT TASKS We analyze RM performance across nine tasks and reveal significant degree of performance correlation among related tasks. Specifically, we compute the Pearson correlation coefficients between tasks based on RM performance across the nine tasks in Omni-RewardBench and present the inter-task correlations as shown in Figure 4. We can observe that the performance correlations among understanding tasks, including text, image, and video understanding, are notably strong, with 10 Preprint Pearson coefficients ranging from 0.8 to 0.9. Similarly, generation tasks such as video, 3D, and image generation also exhibit relatively high correlations, with scores mostly between 0.7 and 0.8. These correlations suggest that RMs capture shared patterns within understanding and generation tasks, demonstrating generalization potential across modalities."
        },
        {
            "title": "6 RELATED WORK",
            "content": "6.1 MULTIMODAL REWARD MODEL Reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019; Ouyang et al., 2022; Rafailov et al., 2023; Ji et al., 2024; Jin et al., 2024a; Yu et al., 2025) has emerged as an effective approach for aligning MLLMs with human preferences, thereby enhancing multimodal understanding (Zhang et al., 2024; Liu et al., 2024d; Zhao et al., 2025), reducing hallucinations (Sun et al., 2024; Yu et al., 2024a;c; Jin et al., 2024b), improving reasoning ability (Wang et al., 2024c; Huang et al., 2025), and increasing safety (Zhang et al., 2025; Yuan et al., 2025). Moreover, alignment is also beneficial for multimodal generation tasks, such as text-to-image generation (Lee et al., 2023; Liang et al., 2024; Xu et al., 2023) and text-to-video generation (Furuta et al., 2024; Wang et al., 2024d; Liu et al., 2025a; Ma et al., 2025), by improving generation quality and controllability. In the alignment process, reward models are crucial for modeling human preferences and providing feedback signals that guide the model toward generating more desirable and aligned outputs. However, most existing reward models (Cobbe et al., 2021; Wang et al., 2024a; Liu et al., 2024a) primarily focus on text-to-text generation tasks, offering limited support for multimodal inputs and outputs. Recently, an increasing number of reward models have been proposed to support multimodal tasks. For example, PickScore (Liang et al., 2024), ImageReward (Xu et al., 2023), and HPS (Wu et al., 2023b;a) are designed to evaluate the quality of text-to-image generation. VisionReward (Xu et al., 2024), VideoReward (Liu et al., 2025a), and VideoScore (He et al., 2024) focus on assessing text-to-video generation. LLaVA-Critic (Xiong et al., 2024) and IXC-2.5-Reward (Zang et al., 2025a) aim to align vision-language models by evaluating their instruction following and reasoning capabilities. UnifiedReward (Wang et al., 2025) is the first unified reward model for assessing both visual understanding and generation tasks. However, existing multimodal reward models remain inadequate for fully omni-modal scenarios, 6.2 REWARD MODEL EVALUATION As the diversity of reward models expands, growing number of benchmarks are emerging to address the need for evaluation (Jin et al., 2024c; Zheng et al., 2024; Ruan et al., 2025; Men et al., 2025). RewardBench (Lambert et al., 2024) is the first comprehensive framework for assessing RMs in chat, reasoning, and safety domains. Furthermore, RMB (Zhou et al., 2024) broadens the evaluation scope by including 49 real-world scenarios. RM-Bench (Liu et al., 2024c) is designed to evaluate RMs based on their sensitivity to subtle content differences and style biases. In the multimodal domain, several benchmarks have been proposed to evaluate reward models for image generation, such as MJ-Bench (Chen et al., 2024b) and GenAI-Bench (Jiang et al., 2024). For video generation, VideoGen-RewardBench (Liu et al., 2025a) provides suitable benchmark for assessing visual quality, motion quality, and text alignment. More broadly, VL-RewardBench (Li et al., 2024a) and Multimodal RewardBench (Yasunaga et al., 2025) have been proposed to evaluate reward models for vision-language models. Extending further, AlignAnything (Ji et al., 2024) collects large-scale human preference data across modalities for post-training alignment and evaluates the general capabilities of omni-modal models. Meanwhile, in text-to-text generation tasks, several recent studies such as PRP (Pitis et al., 2024), HelpSteer2-Preference (Wang et al., 2024e), and GRM (Liu et al., 2025b) have started to focus on fine-grained reward modeling. However, existing benchmarks lack unified framework for evaluating reward models with respect to specific textual criteria across diverse multimodal scenarios."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we present Omni-Reward, unified framework for omni-modal reward modeling with free-form user preferences. To address the challenges of modality imbalance and preference rigidity in current RMs, we introduce three key components: (1) Omni-RewardBench, comprehensive RM benchmark spanning five modalities and nine diverse tasks; (2) Omni-RewardData, largescale multimodal preference dataset incorporating both general and instruction-tuning data; and (3) Omni-RewardModel, family of discriminative and generative RMs with strong performance. 11 Preprint"
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research involves human annotations to construct preference data. All annotation tasks were conducted by the authors of this paper, who participated voluntarily and with full knowledge of the studys purpose, procedures, and intended use of the data. No external crowdsourcing or paid annotation platforms were employed. To safeguard research integrity and mitigate potential biases, detailed annotation protocols and quality control measures are documented in the Appendix E. The study does not involve sensitive personal data, human subjects outside of the annotation task, or applications that raise privacy, security, or legal concerns. We also follow the standard research ethics protocols of our institution, with explicit approval from the IRB, for all internal annotation efforts. The research complies with the ICLR Code of Ethics, and no conflicts of interest or sponsorship concerns are associated with this work."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have taken extensive measures to ensure the reproducibility of our results. All implementation details of the proposed Omni-Reward framework, including architectures, training procedures, and evaluation protocols, are described in the main paper and further elaborated in the Appendix. To support future research, we will release Omni-RewardBench, Omni-RewardData, and Omni-RewardModel as part of comprehensive open-source package. All assets we provide are licensed under the Creative Commons Attribution Non Commercial 4.0 International License (CC BY-NC 4.0). In addition, complete data processing steps and annotation protocols are documented in the Appendix. These efforts are intended to enable the community to replicate our experiments and build upon our findings. 12 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi-ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Arindam Mitra, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, and Xiren Zhou. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. CoRR, abs/2503.01743, 2025. doi: 10.48550/ARXIV.2503.01743. URL https://doi.org/10.48550/arXiv.2503.01743. Anthropic. Introducing the next generation of claude, March 2024a. URL https://www. anthropic.com/news/claude-3-family. Accessed: 2025-04-10. Anthropic. Claude 3.5 sonnet. claude-3-5-sonnet, 2024b. https://www.anthropic.com/news/ Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. doi: 10.48550/ARXIV.2502.13923. URL https://doi.org/ 10.48550/arXiv.2502.13923. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. URL https://openreview. net/forum?id=dbFEFHAD79. Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, Canyu Chen, Qinghao Ye, Zhihong Zhu, Yuqing Zhang, Jiawei Zhou, Zhuokai Zhao, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? CoRR, abs/2407.04842, 2024b. doi: 10.48550/ARXIV.2407.04842. URL https://doi.org/ 10.48550/arXiv.2407.04842. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. CoRR, abs/2412.05271, 2024c. doi: 10.48550/ARXIV.2412.05271. URL https://doi.org/10.48550/arXiv.2412.05271. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. ULTRAFEEDBACK: boosting 13 Preprint language models with scaled AI feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=BOorDpKHiJ. Google DeepMind. Gemini flash, 2025. URL https://deepmind.google/ technologies/gemini/flash/. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. Daniel Deutsch, George F. Foster, and Markus Freitag. Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 1291412929. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.798. URL https: //doi.org/10.18653/v1/2023.emnlp-main.798. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. RLHF workflow: From reward modeling to online RLHF. CoRR, abs/2405.07863, 2024. doi: 10.48550/ARXIV.2405.07863. URL https://doi. org/10.48550/arXiv.2405.07863. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https: //doi.org/10.48550/arXiv.2407.21783. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. Llama-omni: Seamless speech interaction with large language models. CoRR, abs/2409.06666, 2024. doi: 10. 48550/ARXIV.2409.06666. URL https://doi.org/10.48550/arXiv.2409.06666. 14 Preprint Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with AI feedback. CoRR, abs/2412.02617, 2024. doi: 10.48550/ARXIV.2412.02617. URL https://doi.org/ 10.48550/arXiv.2412.02617. Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction-tuned llm and latent diffusion model, 2023. URL https://arxiv. org/abs/2304.13731. Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James R. Glass. Listen, think, and understand. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=nBZBPXdJlC. Shuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Chunle Guo, and Chongyi Li. Evalmuse-40k: reliable and fine-grained benchmark with comprehensive human annotations for text-to-image generation model evaluation. CoRR, abs/2412.18150, 2024. doi: 10.48550/ARXIV.2412.18150. URL https://doi.org/10. 48550/arXiv.2412.18150. Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, Kai Wang, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Bill Yuchen Lin, and Wenhu Chen. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 21052123. Association for Computational Linguistics, 2024. URL https: //aclanthology.org/2024.emnlp-main.127. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. CoRR, abs/2503.06749, 2025. doi: 10.48550/ARXIV.2503.06749. URL https: //doi.org/10.48550/arXiv.2503.06749. Data is Better Together. Open image preferences v1. https://huggingface.co/datasets/ Accessed: data-is-better-together/open-image-preferences-v1, 2024. 2025-05-13. Jiaming Ji, Jiayi Zhou, Hantao Lou, Boyuan Chen, Donghai Hong, Xuyao Wang, Wenqi Chen, Kaile Wang, Rui Pan, Jiahao Li, Mohan Wang, Josef Dai, Tianyi Qiu, Hua Xu, Dong Li, Weipeng Chen, Jun Song, Bo Zheng, and Yaodong Yang. Align anything: Training all-modality models to follow instructions with language feedback. CoRR, abs/2412.15838, 2024. doi: 10.48550/ARXIV.2412. 15838. URL https://doi.org/10.48550/arXiv.2412.15838. Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. Advances in Neural Information Processing Systems, 37:7988979908, 2024. Zhuoran Jin, Pengfei Cao, Chenhao Wang, Zhitao He, Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu, and Jun Zhao. RWKU: benchmarking real-world knowledge unlearning for large language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - URL http://papers.nips.cc/paper_files/paper/2024/ 15, 2024, 2024a. hash/b1f78dfc9ca0156498241012aec4efa0-Abstract-Datasets_and_ Benchmarks_Track.html. Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, and Jun Zhao. Cutting off the head ends the conflict: mechanism for interpreting and mitigating knowledge conflicts in language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, 15 Preprint Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 11931215. Association for Computational Linguistics, 2024b. doi: 10.18653/V1/2024.FINDINGS-ACL.70. URL https: //doi.org/10.18653/v1/2024.findings-acl.70. Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Rag-rewardbench: Benchmarking reward models in retrieval augmented generation for preference alignment. CoRR, abs/2412.13746, 2024c. doi: 10.48550/ARXIV.2412.13746. URL https: //doi.org/10.48550/arXiv.2412.13746. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 73aacd8b3b05b4b503d58310b523553c-Abstract-Conference.html. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Raghavi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. CoRR, abs/2403.13787, 2024. doi: 10.48550/ARXIV.2403.13787. URL https://doi.org/10.48550/arXiv. 2403.13787. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. CoRR, abs/2302.12192, 2023. doi: 10.48550/ARXIV.2302.12192. URL https: //doi.org/10.48550/arXiv.2302.12192. Seongyun Lee, Sue Hyun Park, Seungone Kim, and Minjoon Seo. Aligning to thousands of preferences via system message generalization. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 86c9df30129f7663ad4d429b6f80d461-Abstract-Conference.html. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, and Qi Liu. Vlrewardbench: challenging benchmark for vision-language generative reward models. CoRR, abs/2411.17451, 2024a. doi: 10. 48550/ARXIV.2411.17451. URL https://doi.org/10.48550/arXiv.2411.17451. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, and Qi Liu. VLFeedback: large-scale AI feedback dataset for large vision-language models alignment. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 62276246, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.358. URL https://aclanthology.org/2024. emnlp-main.358/. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katherine M. Collins, Yiwen Luo, Yang Li, Kai J. Kohlhoff, Deepak Ramachandran, and Vidhya Navalpakkam. Rich human feedback for text-to-image generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1940119411. IEEE, 2024. doi: 10.1109/CVPR52733.2024.01835. URL https://doi.org/10.1109/ CVPR52733.2024.01835. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. CoRR, abs/2410.18451, 2024a. doi: 10.48550/ARXIV.2410.18451. URL https://doi.org/10. 48550/arXiv.2410.18451. 16 Preprint Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, and Wanli Ouyang. Improving video generation with human feedback. CoRR, abs/2501.13918, 2025a. doi: 10.48550/ARXIV.2501.13918. URL https://doi.org/ 10.48550/arXiv.2501.13918. Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. CoRR, abs/2412.14167, 2024b. doi: 10.48550/ARXIV.2412.14167. URL https://doi.org/10.48550/arXiv. 2412.14167. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Rm-bench: Benchmarking reward models of language models with subtlety and style. CoRR, abs/2410.16184, 2024c. doi: 10. 48550/ARXIV.2410.16184. URL https://doi.org/10.48550/arXiv.2410.16184. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. CoRR, abs/2504.02495, 2025b. doi: 10. 48550/ARXIV.2504.02495. URL https://doi.org/10.48550/arXiv.2504.02495. Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. MIA-DPO: multi-image augmented direct preference optimization for large vision-language models. CoRR, abs/2410.17637, 2024d. doi: 10.48550/ ARXIV.2410.17637. URL https://doi.org/10.48550/arXiv.2410.17637. Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, and Yuchu Luo. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. CoRR, abs/2502.10248, 2025. doi: 10.48550/ARXIV.2502.10248. URL https://doi.org/10.48550/arXiv.2502.10248. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding, 2024. URL https://arxiv.org/ abs/2406.09418. Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 564572, 2024. Tianyi Men, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Agent-rewardbench: Towards unified benchmark for reward modeling across perception, planning, and safety in real-world multimodal agents. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 1752117541. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.acl-long.857/. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. 17 Preprint Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html. Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators. CoRR, abs/2407.06551, 2024. doi: 10.48550/ ARXIV.2407.06551. URL https://doi.org/10.48550/arXiv.2407.06551. Silviu Pitis, Ziang Xiao, Nicolas Le Roux, and Alessandro Sordoni. Improving context-aware preference modeling for language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 82acbbc04435f6c1e7f656b1cbe4ad82-Abstract-Conference.html. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Miabench: Towards better instruction following evaluation of multimodal llms, 2025. URL https: //arxiv.org/abs/2407.01509. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea In Finn. Direct preference optimization: Your language model is secretly reward model. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html. Rapidata. Rapidata image generation preference dataset. https://huggingface.co/ datasets/Rapidata/human-style-preferences-images, 2024. Jiacheng Ruan, Wenzhen Yuan, Xian Gao, Ye Guo, Daoxin Zhang, Zhe Xu, Yao Hu, Ting Liu, and Yuzhuo Fu. Vlrmbench: comprehensive and challenging benchmark for vision-language reward models. CoRR, abs/2503.07478, 2025. doi: 10.48550/ARXIV.2503.07478. URL https: //doi.org/10.48550/arXiv.2503.07478. Ladan Shams and Aaron Seitz. Benefits of multisensory learning. Trends in cognitive sciences, 12 (11):411417, 2008. Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multiview diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=FUgrjq2pbB. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented RLHF. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 1308813110. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.775. URL https: //doi.org/10.18653/v1/2024.findings-acl.775. Gemma Team. Gemma 3. 2025. URL https://goo.gle/Gemma3Report. 18 Preprint Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In LunWei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 94269439. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024.ACL-LONG.510. URL https://doi.org/10.18653/v1/2024. acl-long.510. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024b. doi: 10.48550/ARXIV. 2409.12191. URL https://doi.org/10.48550/arXiv.2409.12191. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. CoRR, abs/2411.10442, 2024c. doi: 10. 48550/ARXIV.2411.10442. URL https://doi.org/10.48550/arXiv.2411.10442. Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. Lift: Leveraging human feedback for text-to-video model alignment. CoRR, abs/2412.04814, 2024d. doi: 10.48550/ ARXIV.2412.04814. URL https://doi.org/10.48550/arXiv.2412.04814. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024e. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal LLM. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= NZQkumsNlf. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of textto-image synthesis. CoRR, abs/2306.09341, 2023a. doi: 10.48550/ARXIV.2306.09341. URL https://doi.org/10.48550/arXiv.2306.09341. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 20962105. IEEE, 2023b. doi: 10.1109/ICCV51070.2023.00200. URL https://doi.org/10.1109/ICCV51070. 2023.00200. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. CoRR, abs/2408.12528, 2024. doi: 10.48550/ ARXIV.2408.12528. URL https://doi.org/10.48550/arXiv.2408.12528. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. CoRR, abs/2410.02712, 2024. doi: 10.48550/ARXIV.2410.02712. URL https://doi.org/10.48550/arXiv.2410. 02712. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 33646ef0ed554145eab65f6250fab0c9-Abstract-Conference.html. Preprint Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. CoRR, abs/2412.21059, 2024. doi: 10.48550/ARXIV.2412.21059. URL https: //doi.org/10.48550/arXiv.2412.21059. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412. 15115. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: GPT-4V level MLLM on your phone. CoRR, abs/2408.01800, 2024. doi: 10. 48550/ARXIV.2408.01800. URL https://doi.org/10.48550/arXiv.2408.01800. Michihiro Yasunaga, Luke Zettlemoyer, and Marjan Ghazvininejad. Multimodal rewardbench: Holistic evaluation of reward models for vision language models. CoRR, abs/2502.14191, 2025. doi: 10.48550/ARXIV.2502.14191. URL https://doi.org/10.48550/arXiv.2502. 14191. Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. Dreamreward: Text-to-3d generation with human preference. In European Conference on Computer Vision, pp. 259276. Springer, 2024. Tao Yu, Chaoyou Fu, Junkang Wu, Jinda Lu, Kun Wang, Xingyu Lu, Yunhang Shen, Guibin Zhang, Dingjie Song, Yibo Yan, et al. Aligning multimodal llm with human preference: survey. arXiv preprint arXiv:2503.14504, 2025. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. RLHF-V: towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1380713816. IEEE, 2024a. doi: 10.1109/CVPR52733.2024.01310. URL https://doi.org/10.1109/ CVPR52733.2024.01310. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. RLHF-V: towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1380713816. IEEE, 2024b. doi: 10.1109/CVPR52733.2024.01310. URL https://doi.org/10.1109/ CVPR52733.2024.01310. Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. RLAIF-V: aligning mllms through open-source AI feedback for super GPT-4V trustworthiness. CoRR, abs/2405.17220, 2024c. doi: 10.48550/ ARXIV.2405.17220. URL https://doi.org/10.48550/arXiv.2405.17220. Hongbang Yuan, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Towards robust knowledge unlearning: An adversarial framework for assessing and improving unlearning robustness in large language models. In Toby Walsh, Julie Shah, and Zico Kolter (eds.), AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 2576925777. AAAI Press, 2025. doi: 10.1609/AAAI.V39I24.34769. URL https://doi.org/10.1609/aaai.v39i24.34769. 20 Preprint Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, and Jiaqi Wang. Internlmxcomposer2.5-reward: simple yet effective multi-modal reward model. CoRR, abs/2501.12368, 2025a. doi: 10.48550/ARXIV.2501.12368. URL https://doi.org/10.48550/arXiv. 2501.12368. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, and Jiaqi Wang. Internlmxcomposer2.5-reward: simple yet effective multi-modal reward model. CoRR, abs/2501.12368, 2025b. doi: 10.48550/ARXIV.2501.12368. URL https://doi.org/10.48550/arXiv. 2501.12368. Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and Yiming Yang. Direct preference optimization of video large multimodal models from language model reward. CoRR, abs/2404.01258, 2024. doi: 10.48550/ARXIV.2404.01258. URL https://doi.org/10.48550/arXiv.2404. 01258. Yifan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen, Fan Yang, Zhang Zhang, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, and Tieniu Tan. MM-RLHF: the next step forward in multimodal LLM alignment. CoRR, abs/2502.10391, 2025. doi: 10.48550/ARXIV.2502.10391. URL https://doi.org/10.48550/arXiv.2502.10391. Xiangyu Zhao, Shengyuan Ding, Zicheng Zhang, Haian Huang, Maosong Cao, Weiyun Wang, Jiaqi Wang, Xinyu Fang, Wenhai Wang, Guangtao Zhai, Haodong Duan, Hua Yang, and Kai Chen. Omnialign-v: Towards enhanced alignment of mllms with human preference. CoRR, abs/2502.18411, 2025. doi: 10.48550/ARXIV.2502.18411. URL https://doi.org/10. 48550/arXiv.2502.18411. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. CoRR, abs/2412.06559, 2024. doi: 10.48550/ARXIV.2412.06559. URL https://doi.org/10.48550/arXiv.2412.06559. Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Rmb: Comprehensively benchmarking reward models in llm alignment, 2024. URL https: //arxiv.org/abs/2410.09893. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Weiye Xu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025a. URL https://arxiv.org/abs/2504.10479. Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. MMR-V: whats left unsaid? benchmark for multimodal deep reasoning in videos. CoRR, abs/2506.04141, 2025b. doi: 10.48550/ARXIV.2506.04141. URL https://doi.org/10.48550/arXiv.2506.04141. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. CoRR, abs/1909.08593, 2019. URL http://arxiv.org/abs/1909.08593. 21 Preprint"
        },
        {
            "title": "A LLM USAGE STATEMENT",
            "content": "LLMs were used solely as auxiliary tools for grammar checking and language polishing. They did not contribute to the generation of research ideas, the design of experiments, the development of methodologies, data analysis, or any substantive aspects of the research. All scientific content, conceptual contributions, and experimental results are entirely the work of the authors. The authors take full responsibility for the contents of this paper."
        },
        {
            "title": "B LIMITATIONS",
            "content": "(1) Our Omni-RewardBench is In this section, we outline some limitations of our work. benchmark consisting of several thousand human-labeled preference pairs. Its current scale may not be sufficient to support evaluations at much larger magnitudes, such as those involving millions of examples. (2) While our benchmark covers nine distinct task types across different modalities, current task definitions remain relatively coarse, and further fine-grained categorization within each task type is desired. (3) The current preference data is limited to single-turn interactions and does not capture multi-turn conversational preferences, which are increasingly important for modeling real-world dialogue scenarios. (4) The reinforcement learning technique in training the Omni-RewardModel-R1 is limited to preliminary exploration, and further investigation is needed. (5) Incorporating additional modalities such as thermal, radar, tabular data, and time-series data would further enhance the scope and utility of our benchmark."
        },
        {
            "title": "C BROADER IMPACTS",
            "content": "Some preference pairs in Omni-Reward may contain offensive, inappropriate, or otherwise sensitive prompts and responses, as they are intended to reflect real-world scenarios. We recommend that users exercise caution and apply their own ethical guidelines when using the dataset. 22 Preprint"
        },
        {
            "title": "D ANNOTATION DETAILS",
            "content": "D.1 CONSTRUCTION WORKFLOW Figure 5: Construction workflow of Omni-RewardBench. D.2 ANNOTATION GUIDELINE 1. Objective This annotation task aims to identify and label evaluation dimensions under which one model response (Response A) is preferred over another (Response B), given specific task instance (e.g., text-to-image generation, video understanding, or text-to-audio generation). The annotated dataset will serve as foundation for building robust evaluation benchmarks that reflect nuanced human preferences across different modalities and task types. 2. Task Definition Each data instance consists of the following components: task description (e.g., prompt or instruction corresponding to specific task category such as image generation or video analysis), Two model responses, denoted as Response and Response B. Annotators are expected to analyze the responses and determine which aspects make one response superior to the other, focusing on concrete and interpretable evaluation dimensions (e.g., relevance, coherence, visual quality). 3. Annotation Procedure The annotation process involves the following steps: (1) Carefully read the task description and understand the intended objective. (2) Examine Response and Response in the context of the given task. (3) Write one or more evaluation dimension descriptions using fluent, complete English sentences. Each sentence should define specific, human-interpretable dimension along which the two responses can be meaningfully compared. (4) For each evaluation dimension that you articulate, assign comparative label among the following three: Response is better, Response is better, Both responses are equivalent. 23 Preprint D.3 ANNOTATION PLATFORM Figure 6: Annotation platform for human annotators. 24 Preprint"
        },
        {
            "title": "E ETHICS AND QUALITY CONTROL",
            "content": "E.1 ETHICS We confirm that all annotations were conducted voluntarily by the authors of this paper, who were fully informed about the nature and purpose of the task, their rights, and how the data would be used. We also follow the standard research ethics protocols of our institution, with explicit approval from the IRB, for all internal annotation efforts. E.2 QUALITY CONTROL As illustrated in Figure 5, our annotation pipeline consists of two key stages: Criteria Annotation and Preference Annotation. Throughout these two stages, we removed total of 38% of the samples to ensure data quality. Criteria Annotation. We filtered out 23% of the samples whose criteria were deemed either too vague or overly specific, as part of our quality control on preference criteria. Such criteria would undermine the overall consistency and utility of the preference data. Preference Annotation. We further removed 15% of the samples due to disagreements among annotators, where no consensus could be reached on the preferred output. To quantify inter-rater reliability, we report Krippendorffs alpha of 0.701, indicating substantial agreement among annotators. The annotation was carried out by small group of PhD students. Despite the resource-intensive nature of the task, we undertook extensive measures, as documented in Appendix D, to safeguard annotation consistency and mitigate potential biases. These procedures collectively ensured that the final dataset is both ethically collected and of high quality. Moreover, unlike broad and subjective preferences such as helpfulness or harmlessness, our benchmark provides explicit and well-defined textual criteria for each annotation instance. This design choice reduces the risk of ambiguity and limits the impact of cultural or individual variation in interpretation, thereby minimizing the potential issues arising from lack of demographic diversity among annotators. 25 Preprint"
        },
        {
            "title": "F DATASET STATISTICS",
            "content": "F.1 BENCHMARK COMPARISON Table 7 presents detailed comparison between Omni-RewardBench and existing reward modeling benchmarks. While prior benchmarks often focus on narrow range of modalities or task types, Omni-RewardBench provides the most comprehensive coverage, spanning nine tasks across five modalities: text, image, video, audio, and 3D. Moreover, Omni-RewardBench uniquely supports free-form preference annotations, allowing more expressive and fine-grained evaluation criteria compared to the binary preferences used in most existing datasets. Notably, Table 7 shows that AlignAnything bears similarity to Omni-RewardBench. As an influential contribution, it has inspired several aspects of Omni-Reward, particularly the notion of any-to-any alignment. Nevertheless, key distinction exists: AlignAnything concentrates on aligning omni-modal models to enhance their capabilities across diverse inputoutput modalities, introducing EvalAnything to assess the performance of the aligned models. By contrast, our work emphasizes reward modeling within the alignment pipeline, with Omni-RewardBench designed to directly evaluate reward models by testing whether their inferred preferences align with human judgments under specified textual criteria. We compare the performance of ten models on OmniRewardBench and VLRewardBench, obtaining Spearman correlation coefficient of 0.4572 between their rankings. This indicates that incorporating additional modalities and free-form criteria differentiates our benchmark from previous ones. Table 7: The comparison between Omni-RewardBench and other reward modeling benchmarks. Benchmark RewardBench (Lambert et al., 2024) RPR (Pitis et al., 2024) RM-Bench (Liu et al., 2024c) MJ-Bench (Chen et al., 2024b) GenAI-Bench (Jiang et al., 2024) VisionReward (Xu et al., 2024) VideoGen-RewardBench (Liu et al., 2025a) MLLM-as-a-Judge (Chen et al., 2024a) VL-RewardBench (Li et al., 2024a) Multimodal RewardBench (Yasunaga et al., 2025) MM-RLHF-RewardBench (Zhang et al., 2025) AlignAnything (Ji et al., 2024) Omni-RewardBench (Ours) #Size 2,985 10,167 1,327 4,069 9,810 2,000 26,457 15,450 1,250 5,211 170 20,000 3,725 Tasks T2T TI2T TV2T TA2T T2I T2V T2A T23D TI2I Free-Form Preference Annotation Human GPT GPT Human Human Human Human Human GPT+Human Human Human GPT+Human Human F.2 OMNI-REWARDBENCH STATISTICS Due to the inherent difficulty of collecting high-quality data across multiple modalities, some imbalance in the distribution of preference pairs is unavoidable. While some imbalance remains, our dataset maintains relatively balanced distribution across modalities, especially when compared to the significant disparities commonly observed in real-world data availability between modalities such as images and audio. F.3 OMNI-REWARDDATA STATISTICS To mitigate potential systematic biases introduced by relying solely on GPT-4o, we incorporated multi-model verification process (Zhu et al., 2025b) to mitigate potential errors and biases introduced by GPT-4o during instruction generation. Notably, this filtering process is framed as classification task, which is generally less complex and more robust than open-ended instruction generation, helping catch mistakes made by GPT-4o. 26 Preprint Table 8: Data statistics of Omni-RewardBench. The Avg. #Tokens (Prompt), Avg. #Tokens (Response), and Avg. #Tokens (Criteria) columns report the average number of tokens in the prompt, model-generated response, and human-written evaluation criteria, respectively, all measured using the tokenizer of Qwen2.5-VL-7B-Instruct. The Prompt Source column specifies where the prompts were collected from, while the Model column identifies which models were used to produce the corresponding responses. The letters V, I, A, and in the table stand for Video, Image, Audio, and 3D content, respectively. Task #Pairs Avg. #Tokens (Prompt) Avg. #Tokens (Response) Avg. #Tokens (Criteria) Prompt Source #Models T2T TI2T TV2T TA2T T2I T2V T2A T23D TI2I 417 528 443 357 509 529 411 302 229 83.3 22.47 & 14.53 & 14.46 & 17.77 9.61 11.46 14.32 7.89 & Total 3,725 27. 222.1 104.66 133.42 77.83 D 134.50 17.24 15.71 14.69 21.85 21.72 23.29 11.47 30.21 29.81 20.67 RMB, RPR MIA-Bench, VLFeedback VCGBench-Diverse LTU HPDv2, Rapidata GenAI-Bench Audio-alpaca 3DRewardDB GenAI-Bench - 15 19 4 2 27 8f 1g 1h 10 - Claude-3-5-Sonnet-20240620, Mixtral-8x7B-Instruct-v0.1, Vicuna-7B-v1.5, GPT-4o-mini-2024-07-18, Llama-2-7b-chat-hf, Mistral-7B-Instruct-v0.1, Claude-2.1, Gemini-1.5-Pro-Exp-0801, Llama-2-70b-chat-hf, Gemini-Pro, Qwen2-7B-Instruct, Claude-3-Opus-20240229, GPT-4 Turbo, Qwen1.5-1.8B-Chat, Claude-Instant-1.2. GPT-4o, Gemini-1.5-Pro, Qwen2-VL-7B-Instruct, Claude-3-5-Sonnet-20240620, GPT-4o-mini, Qwen-VL-Chat, Llava1.5-7b, Gpt-4v, VisualGLM-6b, LLaVA-RLHF-13b-v1.5-336, MMICL-Vicuna-13B, LLaVA-RLHF-7b-v1.5-224, Instructblip-vicuna-7b, Fuyu-8b, Instructblip-vicuna-13b, Idefics-9b-instruct, Qwen-VL-Max-0809, Qwen-VL-plus, GLM-4v. Qwen-VL-Max-0809, Qwen2-VL-7B-Instruct, Claude-3-5-Sonnet-20241022, GPT-4o. Qwen-Audio, Gemini-2.0-Flash. sdv2, VQGAN, SDXL-base-0.9, Cog2, CM, DALLE-mini, DALLE, DF-IF, ED, RV, flux-1.1-pro, Laf, LDM, imagen-3, DL, glide, OJ, MM, Deliberate, VD, sdv1, FD, midjourney-5.2, flux-1-pro, VQD, dalle-3, stable-diffusion-3. LaVie, VideoCrafter2, ModelScope, AnimateDiffTurbo, AnimateDiff, OpenSora, T2VTurbo, StableVideoDiffusion. Tango. MVDream-SD2.1-Diffusers. MagicBrush, SDEdit, InstructPix2Pix, CosXLEdit, InfEdit, Prompt2Prompt, Pix2PixZero, PNP, CycleDiffusion, DALL-E 2. Table 9: Statistics of free-form criteria per preference pair in Omni-RewardBench. Task Mean Median Min Max T2T TI2T TV2T TA2T T2I T2V T2A T23D TI2I 2.7 2.8 2.6 2.8 7.6 4.4 3.0 4.2 2.0 1 1 1 1 1 1 2 1 1 6 6 6 3 10 5 3 6 4 2.0 3.0 3.0 3.0 8.0 5.0 3.0 4.0 2.0 27 Preprint"
        },
        {
            "title": "G IMPLEMENTATION DETAILS",
            "content": "For training Omni-RewardModel-BT, we use the LLaMA-Factory framework 1. We adopt MiniCPM-o-2.6 as the base model and freeze the parameters of the vision encoder and audio encoder. The model is trained for 2 epochs with learning rate of 2e-6, weight decay of 1e-3, cosine learning rate scheduler, and warmup ratio of 1e-3. For training Omni-RewardModel-R1, we use the EasyR1 framework 2. We adopt Qwen2.5-VL-7B-Instruct as the base model and freeze the parameters of the vision encoder. The model is trained for 2 epochs with learning rate of 1e-6, weight decay of 1e-2, and rollout number of 6. We use vllm 3 for open-source MLLM inference. All experiments are conducted on 4A100 80GB GPUs. For evaluation, we compute the overall score by averaging the performance across all modalities supported by given model. 1https://github.com/hiyouga/LLaMA-Factory 2https://github.com/hiyouga/EasyR1 3https://github.com/vllm-project/vllm 28 Preprint"
        },
        {
            "title": "H ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "Figure 7: Effect of CoT reasoning on Omni-RewardBench under w/ Tie setting. Figure 8: Effect of CoT reasoning on Omni-RewardBench under w/o Tie setting. Table 10: Ablation results on Omni-RewardBench under the w/o Tie setting. Model T2T TI2T TV2T TA2T T2I T2V T2A T23D TI2I Overall MiniCPM-o-2.6 w/ T2T w/ TI2T w/ T2I & T2V w/ Full w/ Preference-Only 74.04 85.25 85.79 59.84 85.79 62. 66.05 67.20 73.72 55.35 72.79 61.40 71.58 76.84 77.89 59.74 79.47 74.21 69.76 74.55 74.25 63.47 75.45 59.28 58.50 51.47 47.62 67.80 67.12 68.03 61.16 49.79 54.94 73.61 72.75 68.88 54.80 58.08 63.64 58.84 66.41 66. 54.92 56.06 57.95 77.27 77.65 73.86 48.79 59.90 61.35 65.70 65.70 58.94 62.18 64.24 66.35 64.62 73.68 65.90 29 Preprint"
        },
        {
            "title": "I ADDITIONAL ANALYSIS",
            "content": "I.1 EFFECT OF CHAIN-OF-THOUGHT REASONING We investigate the impact of chain-of-thought (CoT) reasoning on the final predictions produced by generative RMs. We evaluate the RMs under two settings: (1) w/o CoT, where the model directly generates preference judgment; and (2) w/ CoT, where the model first generates textual critic before providing the final judgment. As shown in Figures 7 and 8, CoT exhibits two-fold effect: it enhances performance in weaker models by compensating for limited capacity through intermediate reasoning, whereas in stronger models, it yields little to no improvement and may even slightly degrade performance, likely because such models already internalize sufficient reasoning capabilities. I.2 EFFECT OF FREE-FORM CRITERIA To illustrate the challenge posed by free-form criteria in Omni-RewardBench, we conduct quantitative experiment comparing model performance when inherent preferences align or conflict with these criteria. Specifically, we elicit each models inherent preferences without criteria, compare them against the ground-truth annotations, and partition the data into two groups: invariant (agreement between inherent and criteria-based preferences) and shifted (conflict between them). Model accuracy is evaluated separately under the free-form criteria for both groups, with substantially lower performance in the shifted group. The results show that GPT-4o-mini suffers an average accuracy drop of 26.32%, while Claude-3.5-Sonnet shows an 18.50% drop. I.3 EFFECT OF SCORING STRATEGY We investigate the impact of two scoring strategies for generative reward models: pointwise and pairwise. Pointwise approach assigns scalar score to each response individually, and predictions are subsequently derived from score comparisons. By contrast, pairwise approach involves directly comparison between the responses to identify the superior one. We conduct experiments on Omni-RewardBench, and as shown in Figure 11, the pairwise scoring strategy significantly outperforms the pointwise variant. Table 11: Overall performance of generative RMs under different scoring strategies. Model Pairwise PointWise Gemma-3-4B-it Qwen2.5VL-7B-Instruct Qwen2.5-VL-32B-Instruct GPT-4o-mini Gemini-1.5-Flash Claude-3-5-Sonnet-20241022 66.61 61.58 69.36 69.21 69.58 71. 37.61 43.62 49.52 50.98 50.05 53.30 29.00 17.96 19.84 18.23 19.53 18.30 30 Preprint"
        },
        {
            "title": "J DATA EXAMPLES",
            "content": "Figure 9: Data example of the T2T task. 31 Preprint Figure 10: Data example of the TI2T task. 32 Preprint Figure 11: Data example of the TV2T task. 33 Preprint Figure 12: Data example of the TA2T task. 34 Preprint Figure 13: Data example of the T2I task. 35 Preprint Figure 14: Data example of the T2V task. 36 Preprint Figure 15: Data example of the T2A task. 37 Preprint Figure 16: Data example of the T23D task. 38 Preprint Figure 17: Data example of the TI2I task. 39 Preprint"
        },
        {
            "title": "K PROMPT TEMPLATES",
            "content": "Table 12: Evaluation prompt for the T2T task. Prompt for Text-to-Text Task SYSTEM PROMPT: You are helpful assistant that scores other AI assistants based on given criteria and the quality of their answers to the user question. You will be given the one user prompt ([[PROMPT]]) and two responses ([[RESPONSE A]] and [[RESPONSE B]]) generated by two models. Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: You are helpful assistant that scores other AI assistants based on given criteria and the quality of their answers to the user question. You will be given the one user prompt ([[PROMPT]]) and two responses ([[RESPONSE A]] and [[RESPONSE B]]) generated by two models. Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[RESPONSE A]] {response_a} [[END OF RESPONSE A]] [[RESPONSE B]] {response_b} [[END OF RESPONSE B]] 40 Preprint Table 13: Evaluation prompt for the TI2T task. Prompt for Text-Image-to-Text Task SYSTEM PROMPT: As professional Text-Image-to-Text quality inspector, your task is to score other AI assistants based on given criteria and the quality of their answers to an image understanding task. You will be given the image ([[image]]), one question ([[question]]) related to the image, and two responses ([[RESPONSE A]] and [[RESPONSE B]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: As professional Text-Image-to-Text quality inspector, your task is to score other AI assistants based on given criteria and the quality of their answers to an image understanding task. You will be given the image ([[image]]), one question ([[question]]) related to the image, and two responses ([[RESPONSE A]] and [[RESPONSE B]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[IMAGE]] {image} [[END OF IMAGE]] [[RESPONSE A]] {response_a} [[END OF RESPONSE A]] [[RESPONSE B]] {response_b} [[END OF RESPONSE B]] 41 Preprint Table 14: Evaluation prompt for the TV2T task. Prompt for Text-Video-to-Text Task SYSTEM PROMPT: As professional Text-Video-to-Text quality inspector, your task is to score other AI assistants based on given criteria and the quality of their answers to video understanding task. You will be given the video (10-frame-video-clip), one question ([[question]]) related to the video, and two responses ([[RESPONSE A]] and [[RESPONSE B]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: As professional Text-Video-to-Text quality inspector, your task is to score other AI assistants based on given criteria and the quality of their answers to video understanding task. You will be given the video (10-frame-video-clip), one question ([[question]]) related to the video, and two responses ([[RESPONSE A]] and [[RESPONSE B]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[VIDEO]] {video} [[END OF VIDEO]] [[RESPONSE A]] {response_a} [[END OF RESPONSE A]] [[RESPONSE B]] {response_b} [[END OF RESPONSE B]] 42 Preprint Table 15: Evaluation prompt for the TA2T task. Prompt for Text-Audio-to-Text Task SYSTEM PROMPT: As professional Text-Audio-to-Text quality inspector, your task is to assess the quality of two answers ([[RESPONSE A]] and [[RESPONSE B]]) for the same question ([[QUESTION]]) based on the same audio input ([[AUDIO]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: As professional Text-Audio-to-Text quality inspector, your task is to assess the quality of two answers ([[RESPONSE A]] and [[RESPONSE B]]) for the same question ([[QUESTION]]) based on the same audio input ([[AUDIO]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[AUDIO]] {audio} [[END OF AUDIO]] [[RESPONSE A]] {response_a} [[END OF RESPONSE A]] [[RESPONSE B]] {response_b} [[END OF RESPONSE B]] 43 Preprint Table 16: Evaluation prompt for the T2I task. Prompt for Text-to-Image Task SYSTEM PROMPT: As professional Text-to-Image quality inspector, your task is to assess the quality of two images ([[RESPONSE A]] and [[RESPONSE B]]) generated from the same prompt ([[PROMPT]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: As professional Text-to-Image quality inspector, your task is to assess the quality of two images ([[RESPONSE A]] and [[RESPONSE B]]) generated from the same prompt ([[PROMPT]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[RESPONSE A]] {image_a} [[END OF RESPONSE A]] [[RESPONSE B]] {image_b} [[END OF RESPONSE B]] 44 Preprint Table 17: Evaluation prompt for the T2V task. Prompt for Text-to-Video Task SYSTEM PROMPT: As professional Text-to-Video quality inspector, your task is to assess the quality of two videos ([[RESPONSE A]] and [[RESPONSE B]]) generated from the same prompt ([[PROMPT]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: As professional Text-to-Video quality inspector, your task is to assess the quality of two videos ([[RESPONSE A]] and [[RESPONSE B]]) generated from the same prompt ([[PROMPT]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[RESPONSE A]] {video_a} [[END OF RESPONSE A]] [[RESPONSE B]] {video_b} [[END OF RESPONSE B]] 45 Preprint Table 18: Evaluation prompt for the T2A task. Prompt for Text-to-Audio Task SYSTEM PROMPT: As professional Text-to-Audio\" quality inspector, your task is to assess the quality of two audio responses ([[RESPONSE A]] and [[RESPONSE B]]) generated from the same question ([[QUESTION]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: As professional Text-to-Audio\" quality inspector, your task is to assess the quality of two audio responses ([[RESPONSE A]] and [[RESPONSE B]]) generated from the same question ([[QUESTION]]). Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[RESPONSE A]] {audio_a} [[END OF RESPONSE A]] [[RESPONSE B]] {audio_b} [[END OF RESPONSE B]] 46 Preprint Table 19: Evaluation prompt for the T23D task. Prompt for Text-to-3D Task SYSTEM PROMPT: As professional Text-to-3D quality inspector, your task is to score other AI assistants based on given criteria and the quality of their answers to text-to-3D generation task. You will be given user instruction ([[PROMPT]]) and two responses ([[RESPONSE A]] and [[RESPONSE B]]), each presenting the rendering of 3D object. Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: As professional Text-to-3D quality inspector, your task is to score other AI assistants based on given criteria and the quality of their answers to text-to-3D generation task. You will be given user instruction ([[PROMPT]]) and two responses ([[RESPONSE A]] and [[RESPONSE B]]), each presenting the rendering of 3D object. Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[RESPONSE A]] {image_a} [[END OF RESPONSE A]] [[RESPONSE B]] {image_b} [[END OF RESPONSE B]] 47 Preprint Table 20: Evaluation prompt for the TI2I task. Prompt for Text-Image-to-Image Task SYSTEM PROMPT: You are helpful assistant that scores other AI assistants based on given criteria and the quality of their answers to an image-editing task. You will be given the one user prompt ([[PROMPT]]), the image to be edited ([[ORIGINAL_IMAGE]]), and two resulting images ([[RESPONSE A]] and [[RESPONSE B]]) generated by two image-editing models. Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better. SYSTEM PROMPT WITH TIE: You are helpful assistant that scores other AI assistants based on given criteria and the quality of their answers to an image-editing task. You will be given the one user prompt ([[PROMPT]]), the image to be edited ([[ORIGINAL_IMAGE]]), and two resulting images ([[RESPONSE A]] and [[RESPONSE B]]) generated by two image-editing models. Rate the quality of the AI assistants response(s) according to the following criteria: {criteria} Your score should reflect the quality of the AI assistants response(s) with respect to the specific criteria above, ignoring other aspects of the answer (such as overall quality), and should agree with the score provided by reasonable human evaluator. The order of the responses is random, and you must avoid letting the order bias your answer. Be as objective as possible in your evaluation. Begin your evaluation by carefully analyzing the evaluation criteria and the response. After providing your explanation, please make decision. After providing your explanation, output your final verdict by strictly following this format: [[A] if response is better, [[B] if response is better, [[C] means you cannot decide which one is better (or they are equal). However, please try to avoid giving tie preference and be as decisive as possible. USER PROMPT: [[PROMPT]] {prompt} [[END OF PROMPT]] [[ORIGINAL_IMAGE]] {original_image} [[END OF ORIGINAL_IMAGE]] [[RESPONSE A]] {image_a} [[END OF RESPONSE A]] [[RESPONSE B]] {image_b} [[END OF RESPONSE B]]"
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences"
    ]
}