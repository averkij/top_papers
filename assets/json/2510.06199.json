{
    "paper_title": "DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation",
    "authors": [
        "Chengyang Zhao",
        "Uksang Yoo",
        "Arkadeep Narayan Chaudhury",
        "Giljoo Nam",
        "Jonathan Francis",
        "Jeffrey Ichnowski",
        "Jean Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hair care is an essential daily activity, yet it remains inaccessible to individuals with limited mobility and challenging for autonomous robot systems due to the fine-grained physical structure and complex dynamics of hair. In this work, we present DYMO-Hair, a model-based robot hair care system. We introduce a novel dynamics learning paradigm that is suited for volumetric quantities such as hair, relying on an action-conditioned latent state editing mechanism, coupled with a compact 3D latent space of diverse hairstyles to improve generalizability. This latent space is pre-trained at scale using a novel hair physics simulator, enabling generalization across previously unseen hairstyles. Using the dynamics model with a Model Predictive Path Integral (MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair styling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model outperforms baselines on capturing local deformation for diverse, unseen hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling tasks on unseen hairstyles, with an average of 22% lower final geometric error and 42% higher success rate than the state-of-the-art system. Real-world experiments exhibit zero-shot transferability of our system to wigs, achieving consistent success on challenging unseen hairstyles where the state-of-the-art system fails. Together, these results introduce a foundation for model-based robot hair care, advancing toward more generalizable, flexible, and accessible robot hair styling in unconstrained physical environments. More details are available on our project page: https://chengyzhao.github.io/DYMOHair-web/."
        },
        {
            "title": "Start",
            "content": "DYMO-Hair: Generalizable Volumetric DYnamics MOdeling for Robot Hair Manipulation Chengyang Zhao1, Uksang Yoo1, Arkadeep Narayan Chaudhury2, Giljoo Nam3, Jonathan Francis1,4, Jeffrey Ichnowski1, Jean Oh1 5 2 0 2 7 ] . [ 1 9 9 1 6 0 . 0 1 5 2 : r Abstract Hair care is an essential daily activity, yet it remains inaccessible to individuals with limited mobility and challenging for autonomous robot systems due to the finegrained physical structure and complex dynamics of hair. In this work, we present DYMO-HAIR, model-based robot hair care system. We introduce novel dynamics learning paradigm that is suited for volumetric quantities such as hair, relying on an action-conditioned latent state editing mechanism, coupled with compact 3D latent space of diverse hairstyles to improve generalizability. This latent space is pre-trained at scale using novel hair physics simulator, enabling generalization across previously unseen hairstyles. Using the dynamics model with Model Predictive Path Integral (MPPI) planner, DYMO-HAIR is able to perform visual goal-conditioned hair styling. Experiments in simulation demonstrate that DYMO-Hairs dynamics model outperforms baselines on capturing local deformation for diverse, unseen hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling tasks on unseen hairstyles, with an average of 22% lower final geometric error and 42% higher success rate than the state-of-the-art system. Real-world experiments exhibit zero-shot transferability of our system to wigs, achieving consistent success on challenging unseen hairstyles where the state-of-the-art system fails. Together, these results introduce foundation for model-based robot hair care, advancing toward more generalizable, flexible, and accessible robot hair styling in unconstrained physical environments. More details are available on our project page: https://chengyzhao.github.io/DYMOHair-web/. I. INTRODUCTION Hair is central to personal identity and self-esteem [1], [2], yet routine care is difficult for individuals with limited mobility due to reduced coordination, strength, and flexibility [3]. To improve accessibility and autonomy, robot hair care systems have been explored [4][7], but existing approaches rely on either handcrafted trajectories or rulebased controllers, restricting generalization across diverse hairstyles and goals. To address these limitations, we propose DYMO-Hair, model-based robot hair care system. Our system is capable of generalizable and flexible visual goal-conditioned hair manipulation, across diverse hairstyles and objectives in unconstrained physical environments. At the core of our system 1 Chengyang Zhao, Uksang Yoo, Jonathan Francis (by courtesy), Jeffrey Ichnowski, and Jean Oh are with Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA. {chengyaz, uyoo, jmf1, jichnows, hyaejino}@andrew.cmu.edu 2 Arkadeep Narayan Chaudhury is with Epic Games, Inc., Pittsburgh, Pennsylvania, USA. arkadeep.chaudhury@epicgames.com 3 Giljoo Nam is with Meta Codec Avatars Lab, Pittsburgh, Pennsylvania, USA. giljoonam@meta.com 4 Jonathan Francis is with Bosch Center for Artificial Intelligence, Pittsburgh, Pennsylvania, USA. Fig. 1. DYMO-HAIR Overview. We introduce DYMO-Hair, unified, model-based robot hair care system. We propose the first 3D volumetric haircombing dynamics model, featuring novel learning paradigm. It uses an action-conditioned latent state editing mechanism, coupled with compact 3D latent space of diverse hairstyles, enabled by our novel hair-combing simulator, for generalizable dynamics modeling. Building on this model, we develop DYMO-Hair with MPPI-based planner for closed-loop visual goal-conditioned hair styling. is dynamics model that captures diverse hair deformations across various hairstyles and combing motions. For deformable objects like hair, complex structures and unobservable properties make accurate dynamics modeling difficult. While analytical physics-based models exist, they are computationally expensive and impractical for realtime control, motivating the use of learning-based neural dynamics as proxies [8][10]. However, hair poses unique challenges: 1) Representation. Low-resolution point clouds cannot capture strand-level geometry. 2) Structure. Graphbased methods scale poorly as point counts increase for higher resolution. 3) Supervision. Global metrics miss finescale deformations, while point-wise correspondence is impractical for strands. 4) Data. Hair entanglement makes realworld data collection slow and difficult to reset across styles. To address these challenges, we introduce novel paradigm for generalizable volumetric hair dynamics modeling. We present the first 3D hair-combing dynamics model that leverages large-scale diverse synthetic data for hair dynamics learning and generalizes across various hairstyles. We represent hair as high-resolution volumetric occupancy grid with 3D orientation field to capture both hair position and local strand flow, which offers more geometric details and structural information of hair than sparse point clouds. It also allows dense supervision on both occupancy and orientation, providing sufficient local signals for the model to capture fine-grained deformations during learning. Our key innovation, inspired by ControlNet [11], is to pre-train compact 3D latent space for diverse hair states and to introduce control branch that models dynamics as actionconditioned state editing, enabling significantly improved generalizability through large-scale pre-training. To avoid time-consuming real-world data collection required by the pre-training, we further develop hair-combing simulator based on Genesis [12]. It leverages novel formulation of the position-based dynamics (PBD) method for strandlevel, contact-rich hair simulation, enabling efficient largescale generation of visually-realistic and physically-plausible synthetic dynamics data across diverse hairstyles. Experiments in simulation demonstrate that our model outperforms baselines on generalizable hair dynamics modeling for local hair deformation across diverse unseen hairstyles. Building on our dynamics model, we introduce DYMOHair, unified, model-based robot hair care system for visual goal-conditioned hair styling. We adopt Model Predictive Control (MPC) framework, using Model Predictive Path Integral (MPPI)-based planner to optimize an action trajectory that minimizes the geometric distance between predicted hair states and the objective [13], [14]. Simulation experiments on diverse unseen hairstyles show that DYMOHair achieves superior effectiveness and generalizability for closed-loop hair styling compared to all system baselines, with an average of 22% lower final geometric error and 42% higher absolute success rate than the state-of-the-art system. Real-world demonstrations further exhibit zero-shot transferability of DYMO-Hair to physical wigs, achieving consistent success on challenging unseen hairstyles where the state-of-the-art system fails. To summarize, our contributions are: study of model-based approaches for robot hair manipulation. DYMO-Hair, unified, model-based robot system for visual goal-conditioned hair styling, evaluated across diverse hairstyles in simulation and real-world settings. 3D generalizable volumetric dynamics model for hair combing. hair simulator with novel PBD method for strandlevel, contact-rich hair-combing simulation. II. RELATED WORKS A. Dynamics Modeling for Deformable Object Manipulation Physics-based modeling offers first-principles dynamics formulation [15], [16], but is often impractical for deformable objects due to complex internal structures and unobservable material properties, limiting its real-time use in manipulations. Learning-based methods have gained attention as alternatives [10], with graph-based modeling proving effective in capturing spatial relationships in deformable dynamics [17], [18] for various object types like plasticine [8], [13], cloth [19], rope [20], and plush toys [14]. However, these approaches often assume easily observable deformations, use low-resolution state representations, and face scalability issues, which are unsuitable for hair modeling. Recent works [9], [10] explore particle-grid hybrid or diffusionbased modeling to improve scalability, but rely on point-wise correspondence for dense supervision, which is impractical for hair. We propose new dynamics learning paradigm for hair that preserves geometric and structural details, avoids the scalability limits of graph-based methods and the pointwise correspondence requirement for dense supervision, and enables more generalizable dynamics modeling. B. Robot System for Hair Manipulation Robot hair manipulation integrates multiple research areas and remains underexplored due to its inherent complexity. One line of work [6] focuses on mechanical and human-robot interaction aspects, introducing soft robot manipulator for safer and more user-friendly system design. Another line of work approaches it algorithmically, often relying on 2D observations and rule-based strategies. Some studies [5], [21] tackle detangling with sensorized brushes using visual and force feedback, but rely on constrained settings and specific initial conditions. Another method [4] plans combing trajectories aligned with hair flow, but lacks goal-driven flexibility. Recent work [7] introduces rule-based, goal-conditioned planning based on 2D orientation differences, but is restricted to particular hairstyles and goals. We advance robot hair manipulation by capturing 3D hair states, explicitly modeling their 3D dynamics, and developing model-based system for generalizable, flexible, goal-conditioned hair manipulation. C. Hair Dynamics Simulation Hair dynamics simulation is challenging due to hairs fine structure, complex inter-strand interactions, and large strand count. Physics-based methods have been explored for both clump-level and strand-level modeling. Clumplevel methods [22], [23] represent hair as large bundles, achieving computational efficiency suitable for real-time applications but failing to capture strand-level hair behavior. Strand-based methods, on the other hand, use physically more accurate representations such as mass-spring models [24], [25] and Kirchhoff rod models [26], [27], enabling more accurate modeling of strand-level dynamics but remaining computationally expensive and inefficient. Recently, learning-based methods have been explored to accelerate simulation [28], [29], reduce the need for detailed Fig. 2. Comparison of Hair State Representations. (a) Colors distinguish individual hair strands. (b) We show resolution of 2K points, the maximum used for point cloudbased methods in our experiments (see Sec. VII-B for more details). (c) We show 64 64 128 grids with voxel size of about 5 mm. Colors denote local strand orientations. Red dashed box: zoomed-in region. Brown dashed box: the corresponding local strand segments. physical modeling [30], and improve generalizability [31]. While efficient and visually-realistic, they still fall short of achieving strand-level, physically-accurate modeling. We propose novel PBD method for strand-level hair simulation to enable efficient, both visually-realistic and physicallyplausible simulation of contact-rich hair-combing dynamics, supporting synthetic data generation and closed-loop hair manipulation experiments. III. PROBLEM FORMULATION We develop model-based robot hair manipulation system for hair styling using preset combing tool. Given userspecified visual goal G, at each timestep the system receives the current observation ot from the environment and estimates the underlying hair state st. The system then selects the next-step action at that best advances toward G, guided by dynamics model fdyn capturing the hair deformation i.e., ˆst+1 = fdyn(ˆst, ˆat). The behavior under combing, action space consists of 3D combing motions, represented as sequences of tool positions and orientations. After executing at, the system receives the updated observation ot+1 and iterates this process until is reached. IV. HAIR COMBING DYNAMICS MODELING A. State Representation Designing an effective state representation is fundamental to dynamics model, as it should not only capture taskrelevant information but also ensure robustness and efficiency for estimation. In the context of hair, strand is the fundamental physical structure from which hair is formed, serving as key geometric cue for describing hair state. While humans can perceive strand geometry almost instantly, even the most advanced methods for full 3D strand reconstruction require several minutes [32], [33], making them unsuitable for real-time robot manipulation system. To achieve both fidelity and efficiency, we draw inspiration from prior computer vision work [34] and represent the hair as set of dense 3D points, each with position and unit direction vector describing the local strand orientation. This captures both the spatial distribution and the flow direction of hair, and can be estimated robustly and efficiently from multi-view RGB-D data. Compared with widely used DYMO-Hairs Dynamics Model Overview. Left: State latent Fig. 3. space pre-training. 3D volumetric hierarchical model with vector quantization enables compact compression while preserving detailed representation capability. Right: Dynamics learning. The pre-trained model is adapted to capture hair dynamics in ControlNet-style framework, formulating dynamics as action-conditioned editing in the pre-trained state latent space. zero: zero-convolution; copy: weight copying for initialization; : elementwise addition; : 3D attention-based feature fusion. In this phase, only the motion encoding path is trainable, with all pre-trained components frozen. standard point clouds for dynamics modeling, it encodes richer geometric and structural information while avoiding the computational cost of strand-level reconstruction. For more structured processing, following [35], we further discretize it into high-resolution volumetric occupancy grid with 3D orientation field as our final state representation: st = (occt, orit), occt {0, 1}V0 , orit [0, 1]V03, where V0 is the spatial resolution of the grid. This voxelized form allows us to balance computational efficiency with geometric fidelity: it enables the use of highly optimized neural operators and dense voxel-level supervision in the learning process, while maintaining acceptable accuracy when resolution is sufficiently high. An illustration of our state representation is shown in Fig. 2. B. State Latent Space Pre-training Inspired by recent advances in conditioned image and video generation [11], [36], [37], we propose novel ControlNet [11]-style two-phase paradigm for hair dynamics learning that is compatible with our high-resolution volumetric state representation and enables more generalizable modeling across diverse hairstyles. In Phase 1, we use pre-training to encode diverse hairstyles with various deformations during combing into unified, compact 3D latent space. As shown in Fig. 3, we adopt hierarchical model structure, analogous to VQ-VAE-2 [38], in the 3D volumetric setting to achieve both compact state compression and detailed representation capability. The model is pre-trained for state reconstruction. Given volumetric state representation of any hairstyle with deformation, the model concatenates the occupancy and orientation components together and hierarchically encode it into lower resolutions with two 3D encoders, producing latent embeddings hbottom RV1D1 and htop RV2D2 , where V0 > V1 > V2. The top-level codebook [38], [39] quantizes htop into ˆhtop by replacing each entry with its nearest codebook vector, which is further decoded into resolution V1 to serve as prior for quantizing hbottom. decoder decodes the quantized ˆhbottom and the up-sampled ˆhtop into the final reconstruction s. This hierarchical design allows the two quantized latent spaces, i.e., the bottomand top-level codebooks, to capture complementary local and global information respectively, enabling better state modeling and reconstruction than singlelevel quantization [39]. We use exponential moving averages (EMA) to update codebooks progressively during training. C. Dynamics: Action-conditioned State Editing In Phase 2, we formulate the dynamics as an actionconditioned state editing process, and adapt the pre-trained model with ControlNet [11]-style framework to leverage the pre-trained state latent space for capturing hair dynamics. The pre-trained state encoding path takes the initial state st as input and encodes it progressively, as in Phase 1. For dynamics modeling, we introduce an additional trainable motion encoding path that processes the combing motion at as control signal in parallel with the state, while keeping all other pre-trained components, including the codebooks, frozen. This path consists of three cascaded encoders: the first preserves resolution, while the other two, sharing the state encoders architecture, progressively compress the signal to lower resolutions, producing control embeddings cbottom RV1D1 and ctop RV2D2 . Following ControlNet [11], the path employs weight copying, zeroconvolution, and cross-path feature fusion mechanisms to integrate state and motion features at fine-grained voxel level while stabilizing early training. The control embeddings are then fused with state embeddings hbottom and htop to perform edits in the pre-trained state latent space for the dynamics behavior. The edited embeddings are finally quantized and decoded into the end state st+1. To enforce fine-grained spatial alignment between the state and the motion, we convert the motion into volumetric grid matching the state resolution before feed it into the first motion encoder. Specifically, we sample the motion into up to uniformly spaced key tool poses and, for each tool pose, compute the shortest distance from every voxel center in the grid to the tools center line. Voxels outside central cylindrical region, defined by the center line and preset contact radius, are discarded to form prior of the local contact region. This yields time-indexed volumetric distance map in RV0K2, with distance and validity in the last dimension. The map is then fed into the motion encoding path for dynamics modeling. D. Supervision We use the same composite loss for both phases. For occupancy, we combine the focal loss [40] and the soft Dice loss to address class imbalance and encourage volumetric overlap with ground truth, as occupied regions typically form thin Fig. 4. Constraints for PBD-based Strand-level, Contact-rich Hair Combing Simulation. Top: Each constraints formulation and intended effect. Bottom: Simulation results under progressive constraint addition. Starting from the initial state (far left), combing motion is applied along the white dashed arrow. The red dashed box marks the contact-rich region, with its simulation results shown on the right. With all three constraints, the hair maintains realistic shape; the twist constraint, in particular, preserves curvature and prevents gravity-induced oversmoothing. shell within the grid. For orientation, we use the L1 loss on normalized orientation vectors for each occupied voxel, with symmetry handling for directional equivalence, i.e., and represent the same local strand orientation. For codebook matching, we adopt the EMA commitment loss from VQ-VAE [39], keeping latent entries close to their matched codebook vectors. The total loss is weighted sum of these terms, jointly enforcing geometric accuracy, directional consistency, and latent representation quality. V. HAIR DYNAMICS SIMULATION Existing neural dynamics models are often trained directly on real-world data for each object. However, for hair, finegrained strand entanglement and deformation can create messy, hard-to-reset states, making real-world data collection highly time-consuming. Moreover, our approach requires large-scale data covering diverse hairstyles and deformations to build strong, representative state latent space during pre-training, which further increases the impracticality of collecting such data in the real world. In this paper, we use fully synthetic data for dynamics learning. We develop novel GPU-accelerated simulator based on Genesis [12] that enables efficient, both visually-realistic and physicallyplausible strand-level, contact-rich hair-combing simulation. its core, our simulator uses novel PBD method for strand-level hair simulation. We model each hair strand as 3D particle sequence [p0, p1, , pn], where p0 is the root connected to the scalp and fixed. Thousands of strands are simulated in parallel. We use three physicsinformed constraints to model inner-strand physical properties. Let d(pi, pj) = pi pj denote the Euclidean distance between particles pi and pj, and θ(pi, pj, pk) = denote the angle at pj formed bearccos tween the vectors pi pj and pk pj. The corresponding rest-state values are denoted d0(pi, pj) and θ0(pi, pj, pk). Stretch: Neighboring particles preserve their rest distances: Cstretch(pi, pi+1) = d(pi, pi+1) d0(pi, pi+1), [0, 1]. (cid:16) (pipj )(pkpj ) pipj pkpj At (cid:17) Bending: Local in-plane bending is regulated by constraining consecutive particle angles: Cbending(pi1, pi, pi+1) = θ(pi1, pi, pi+1) θ0(pi1, pi, pi+1), [1, 1]. Twist: The 3D twist constraint of the strand is approximated by multiple skip-connected 2D bending constraints with fixed index gap > 1: Ctwist(pik, pi, pi+k) = θ(pik, pi, pi+k) θ0(pik, pi, pi+k), [k, k]. They collectively form multiple 3D-intersecting 2D constraint planes to approximate the full 3D twist behavior. The constraints with their effects are illustrated in Fig. 4. During simulation, the particle positions are iteratively updated toward satisfying all = 0. Note that our twist model is heuristic approximation, as true hair twist arises from the strands internal physical microstructure. While more complex techniques like Kirchhoff rod models may offer more physically-accurate twist simulation, they typically remain computationally expensive and inefficient. Here we use the heuristics to balance accuracy and computation efficiency. We model inter-strand and hair-tool contacts at the particle level with standard PBD collision and friction handling. Our simulator enables efficient simulation of visuallyrealistic and physically-plausible hair-combing dynamics across diverse hairstyles, supplying abundant data for model learning and serving as testbed for closed-loop experiments with our hair styling system (see Sec. VII-C). VI. MODEL-BASED ROBOT HAIR STYLING SYSTEM Building on our hair-combing dynamics model, we develop DYMO-Hair, model-based robot hair care system for hair styling, capable of handling diverse hairstyles and goal configurations. As illustrated in Fig. 1, the system takes multi-view RGB-D observations from the environment and estimates the current volumetric hair state, following [34]. Given the current state and the visual goal configuration, the MPPI-based planner samples candidate actions, rolls out the dynamics model to predict possible future outcomes, and optimizes an action trajectory that minimizes the geometric distance between the predicted states and the goal, following prior neural dynamics-based planners [13], [14], [20]. chunk of actions is executed, after which the system updates its observation and repeats the loop until the goal is reached. VII. EXPERIMENTS A. Experiment Setup Hairstyles Used. Fig. 5 shows the 10 hairstyles we use in simulation to train the hair dynamics model. For dynamics model and closed-loop system evaluation, we use 7 unseen hairstyles in simulation, and 2 physical wigs with different hairstyles for real-world test. The hairstyles and the mannequin head in simulation are from USC-HairSalon [41]. Simulation Setup. The simulation workspace consists of mannequin head and hair model with realistic hairstyle, as shown in Fig. 5. The tool for manipulation is thin cylinder. Experiment Setup. Left: Synthetic hair used for training and Fig. 5. evaluation; simulation environment. Right: Real-world setup for evaluation. For simulation stability, we preprocess each strand for an appropriate linear density while preserving its curvature. Real-world Setup. Fig. 5 also shows our real-world workspace, consisting of 25 cm19 cm30 cm mannequin head with physical wig. The head is painted with calibration marks. We use UFactory 850 6-axis robot arm with 3D-printed tool of length 16 cm and tip radius 4 mm, designed to resemble the cylindrical tool used in simulation to reduce the sim-to-real gap. We add TPUprinted deformable finger tip on it to ensure better contact between the tool and the head. For perception, we use wristmounted RealSense D405 RGB-D camera to capture multiview observations of the hair by varying the robot pose. Both the head and the camera are calibrated to the robot frame. B. Hair Combing Dynamics Learning Data. We construct large-scale synthetic hair-combing dynamics dataset using our simulator for generalizable dynamics learning. For each of 10 training hairstyles, we randomly sample over 1K diverse tool motions, rolling them out in simulation to either mess or clean the hair, and recording intermediate states and sub-motions. After filtering lowquality samples, we obtain 10K training dataset. For latent space pre-training, we combine these synthetic hair states with deformed variants from our dataset and an existing large-scale neat hairstyle database [42], [43], yielding 47K hair states to build strong and representative latent space. Dynamics learning is then trained on our 10K combing dataset. For evaluation, we generate separate dynamics dataset from 7 unseen hairstyles, producing 800 filtered transitions for testing the models generalizability. Baselines. We compare our method against three baselines: PC-GNN: represents hair as point clouds with per-point orientation, applies GNN-based structure without pretraining, following the most common paradigm for neural deformable object dynamics [13]. V-UNet: represents hair as volumetric grids as ours does, but uses UNet-based architecture with pyramid finegrained state-action fusion and no pre-training; serves as an TABLE HAIR COMBING DYNAMICS MODEL EVALUATION ON UNSEEN HAIR Method PC-GNN [13] V-UNet V-FiLM Ours CDpoint 90th 0.1359 0.1345 0.1334 0.1240 mean 0.0814 0.0792 0.0807 0. Errori 90th 30.16 31.00 29.59 26.12 mean 13.34 14.25 13.60 12.03 CDstrand 90th 0.1983 0.1966 0.2011 0.1878 mean 0.1052 0.1047 0.1065 0.1005 ablation to test the benefit of our pre-trained latent space. V-FiLM: represents hair as volumetric grids as ours does, uses FiLM [44]-style state-action fusion mechanism, and is trained on the same pre-trained latent space as ours; serves as an ablation to test the effectiveness of our ControlNet-style design for fine-grained state-action fusion. Evaluation Metrics. We convert the outputs of all volumetric methods into point clouds with per-point orientation and down-sample them to the same resolution as PC-GNN for fair comparison. Three metrics are used: 1) CDpoint: Chamfer Distance (CD) between predicted and ground-truth point clouds, ignoring orientation. 2) Errori: point-level orientation error, computed as the average angular difference between predicted orientations and those of the closest ground-truth points, with symmetry tolerated. Predicted points farther than 2 cm from the ground truth are discarded to avoid invalid matches. 3) CDstrand: strand-level CD, where predicted point clouds with orientations are reconstructed into strand segments to evaluate the strand structure they convey, measuring on average how well each predicted segment aligns with its closest ground-truth segment and vice versa. Implementation Details. For PC-GNN, we use 2K-point resolution, the highest feasible while maintaining enough message-passing capability under the same computation budget as other methods. For all volumetric methods, we use 64 64 128 grids with 5 mm voxel size. All models are trained to convergence on 2 NVIDIA RTX 4090 GPUs. Results. We evaluate all methods on 7 unseen hairstyles to test generalizability, with results shown in Tab. I. Since in hair-combing scenarios deformations are highly localized, occurring mainly near the comb while most of the hair remains remains stable and unchanged, evaluating the entire hairstyle can obscure local effects. We therefore focus on the near-motion region and report both the mean, reflecting overall performance, and the 90th percentile, which captures sparse but significant deformations, e.g., deformations that occur in only few strands, and reduces averaging bias. The results show that our method outperforms all baselines in capturing local hair deformation behavior in the near-motion region for unseen hairstyles. Our model surpasses the widely used PC-GNN paradigm, underscoring its suitability for generalizable hair dynamics. Compared with other voxel-based methods, our model leverages latent space pre-training for stronger generalization than V-UNet, and finer state-action fusion for more accurate future state prediction than V-FiLM based on the same pre-trained state latent space. Overall, the results highlight the effectiveness and generalizability of Fig. 6. Quantitative Results for Closed-loop Hair Styling in Simulation. Experiments are conducted on 7 unseen hairstyles, each with 3 cases repeated 5 times. (a) Error curves over planning steps, where solid lines and shaded regions denote mean and standard deviation across hairstyles. Faster error reduction and lower final error indicate higher effectiveness. (b) Success rate curves w.r.t. error thresholds used to determine success, reflecting the distribution of final errors. Curves closer to the top-left correspond to more low-error outcomes and better performance. In both (a) and (b), error is defined as relative error: the ratio of the current strandlevel distance to the initial strand-level distance, providing unified metric across hairstyles with varying absolute error magnitudes. our model on hair-combing dynamics, validating our design choices of pre-training and fine-grained state-action fusion. C. Closed-loop Goal-conditioned Hair Styling Baselines. We compare DYMO-Hair against four baselines. Rule-based System: the only existing method for visual goal-conditioned robot hair styling [7], which uses single front-view 2D observation and handcrafted rules to derive actions from 2D orientation differences between current and goal states. PC-GNN System, V-UNet System, and V-FiLM System: three model-based systems that replace DYMOHairs dynamics model with the baseline models described in Sec. VII-B, while keeping all other parts unchanged. Implementation Details. All systems for visual goalconditioned hair styling assume geometrically grounded goal configurations that are well-aligned with the states, consistent with existing neural dynamics-based deformable object manipulation methods [13], [14], [20]. For all model-based systems, the MPPI planning cost is defined as the strandlevel distance between strand segments reconstructed from the predicted future states and the goal state, incorporating both point and orientation predictions in unified manner. Simulation Experiments. We first evaluate DYMO-Hairs effectiveness and generalizability for visual goal-conditioned closed-loop hair styling thoroughly in simulation, using diFig. 7. Qualitative Results for Closed-loop Hair Styling in Simulation. Three hard cases of different unseen hairstyles are shown, with columns (left to right) illustrating the initial state, the intermediate planning steps, the end state with relative error, and the goal. verse unseen hairstyles and varying initial states. Specifically, we test on 7 unseen hairstyles with 3 cases per hairstyle levels of messiness, and repeat each case 5 at different times with different random seeds to reduce randomness. All experiments are conducted with maximum budget of 15 steps. Quantitative results in Fig. 6 show that all modelbased methods outperform Rule-based System. Among them, DYMO-Hair achieves the best performance, with an average of 22% lower final geometric error and 42% higher absolute success rate across three cases (using thresholds of 0.90, 0.70, and 0.70, chosen as reasonable success criteria based on experiments and visualization), demonstrating both the advantage of incorporating dynamics model for improving system capability and generalizability, and the superior effectiveness of our advanced dynamics model in boosting closed-loop manipulation at the system level. Qualitative results are shown in Fig. 7. Real-world Experiments. We further evaluate the zero-shot transferability of DYMO-Hair from simulation to unconstrained real-world settings. Experiments on 2 physical wigs with different hairstyles, using the setup in Fig. 5, compare our system against Rule-based System, the state-of-the-art robot hair styling system. As shown in Fig. 8, DYMOHair consistently outperforms the baseline. While the Rulebased System fails in both cases, DYMO-Hair achieves rapid, effective progress toward the target style even in the challenging long-length wig case, where success requires pushing hair behind the ear to achieve long-horizon spatial transformation. Rule-based System fails mainly due to two weaknesses: 1) its handcrafted rules rely on strand-level correspondence for geometric difference computation, requiring accurate 2D orientation maps and strand tracking. These dependencies are brittle as errors accumulate with longer hair and performance degrades in unconstrained settings with variations in lighting, resolution, etc.; and 2) it relies solely on single front-view observation, which is inadequate for Fig. 8. Results for Closed-loop Hair Styling in the Real World. For each case, the visual goal is shown on the left, with key observations and actions for each method displayed alongside the curve. Error is defined as relative error: the ratio of the current strand-level distance to the initial distance. styles demanding long-horizon changes across both front and side views, as in the long-length wig case in Fig. 8. In contrast, DYMO-Hair leverages multi-view 3D hair state estimations and evaluates geometric differences without the need for strand-level correspondence. This design is more robust to environmental variations, generalizes across hair lengths, and effectively captures long-horizon goals. The dynamics model further provides predictive capability, improving both action efficiency and manipulation effectiveness. VIII. CONCLUSION This paper presents the first study on model-based robot hair manipulation. We introduce the first 3D hair-combing dynamics model with novel volumetric learning paradigm for generalizable dynamics modeling. We also develop simulator with novel PBD method for strand-level, contact-rich hair-combing simulation to support data requirements for large-scale pre-training. Together, these contributions yield the first unified model-based robot hair care system, DYMOHair, for visual goal-conditioned hair styling, generalizable to novel hairstyles and evaluated in both simulation and the real world. Several limitations remain for future work. First, we focus on model-based planning with limited consideration of human-robot interaction aspects, such as enforcing actionspace constraints to avoid safety-critical regions like the eyes to enhance real-world usability. Second, the system assumes privileged mannequin head information and perfect calibration; incorporating online head estimation would improve practicality. Finally, we use simple 3D-printed combing tool; replacing it with advanced designs like soft robot fingers [6], [45] could further enhance usability. ACKNOWLEDGMENTS We would like to thank Feiyu Zhu and John Z. Zhang for their valuable feedback and discussions. This work was supported by NSF IIS-2112633 and NSF Graduate Research Fellowship under Grant No. DGE2140739."
        },
        {
            "title": "REFERENCES",
            "content": "[1] C. M. McFarquhar and M. J. Lowis, The effect of hairdressing on the self-esteem of men and women, Mankind quarterly, vol. 41, no. 2, p. 181, 2000. 1 [2] U. Yoo, N. Dennler, S. Patil, J. Oh, and J. Ichnowski, Inclusion in assistive haircare robotics: Practical and ethical considerations in hair manipulation, arXiv preprint arXiv:2411.05137, 2024. 1 [3] A. Waugh, Personal care, sensory impairment and unconsciousness. Foundations of Nursing Practice: Fundamentals of Holistic Care, p. 363, 2013. 1 [4] N. Dennler, E. Shin, M. Mataric, and S. Nikolaidis, Design and evaluation of hair combing system using general-purpose robotic arm, in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 37393746. 1, 2 [5] J. Hughes, T. Plumb-Reyes, N. Charles, L. Mahadevan, and D. Rus, Detangling hair using feedback-driven robotic brushing, in 2021 IEEE 4th International Conference on Soft Robotics (RoboSoft), 2021, pp. 487494. 1, 2 [6] U. Yoo, N. Dennler, E. Xing, M. Mataric, S. Nikolaidis, J. Ichnowski, and J. Oh, Soft and compliant contact-rich hair manipulation and care, in 2025 20th ACM/IEEE International Conference on HumanRobot Interaction (HRI), 2025, pp. 610619. 1, 2, 8, [7] S. Kim, N. Kanazawa, S. Hasegawa, K. Kawaharazuka, and K. Okada, Front hair styling robot system using path planning for root-centric strand adjustment, in 2025 IEEE/SICE International Symposium on System Integration (SII), 2025, pp. 544549. 1, 2, 6, 12 [8] H. Shi, H. Xu, S. Clarke, Y. Li, and J. Wu, Robocook: Long-horizon elasto-plastic object manipulation with diverse tools, arXiv preprint arXiv:2306.14447, 2023. 1, 2 [9] K. Zhang, B. Li, K. Hauser, and Y. Li, Particle-grid neural dynamics for learning deformable object models from rgb-d videos, arXiv preprint arXiv:2506.15680, 2025. 1, 2 [10] T. Tian, H. Li, B. Ai, X. Yuan, Z. Huang, and H. Su, Diffusion dynamics models with generative state estimation for cloth manipulation, arXiv preprint arXiv:2503.11999, 2025. 1, 2 [11] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, 2023. 2, 3, [12] G. Authors, Genesis: generative and universal physics engine [Online]. Available: for robotics and beyond, December 2024. https://github.com/Genesis-Embodied-AI/Genesis 2, 4 [13] H. Shi, H. Xu, Z. Huang, Y. Li, and J. Wu, Robocraft: Learning to see, simulate, and shape elasto-plastic objects with graph networks, arXiv preprint arXiv:2205.02909, 2022. 2, 5, 6, 10, 13 [14] M. Zhang, K. Zhang, and Y. Li, Dynamic 3d gaussian tracking for graph-based neural dynamics modeling, arXiv preprint arXiv:2410.18912, 2024. 2, 5, 6 [15] T. Tang, C. Wang, and M. Tomizuka, framework for manipulating deformable linear objects by coherent point drift, IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 34263433, 2018. 2 [16] S. Tiburzio, T. Coleman, and C. Della Santina, Model-based manipulation of deformable objects with non-negligible dynamics as shape regulation, arXiv preprint arXiv:2402.16114, 2024. [17] T. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W. Battaglia, Learning mesh-based simulation with graph networks, 2021. [Online]. Available: https://arxiv.org/abs/2010.03409 2 [18] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. Battaglia, Learning to simulate complex physics with graph networks, in International conference on machine learning. PMLR, 2020, pp. 84598468. 2 [19] X. Lin, Y. Wang, Z. Huang, and D. Held, Learning visible connectivity dynamics for cloth smoothing, in Conference on Robot Learning. PMLR, 2022, pp. 256266. 2 [20] K. Zhang, B. Li, K. Hauser, and Y. Li, Adaptigraph: Material-adaptive graph-based neural dynamics for robotic manipulation, arXiv preprint arXiv:2407.07889, 2024. 2, 5, 6 [21] T. B. Plumb-Reyes, N. Charles, and L. Mahadevan, Combing double helix, 2021. [Online]. Available: https://arxiv.org/abs/2103.05211 [22] C. K. Koh and Z. Huang, simple physics model to animate human hair modeled in 2d strips in real time, in Proceedings of the Eurographic Workshop on Computer Animation and Simulation. Berlin, Heidelberg: Springer-Verlag, 2001, p. 127138. 2 [23] K. Wu and C. Yuksel, Real-time hair mesh simulation, in Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, ser. I3D 16. New York, NY, USA: Association for Computing Machinery, 2016, p. 5964. [Online]. Available: https://doi.org/10.1145/2856400.2856412 2 [24] R. E. Rosenblum, W. E. Carlson, and E. Tripp III, Simulating the structure and dynamics of human hair: Modelling, rendering and animation, The Journal of Visualization and Computer Animation, vol. 2, no. 4, pp. 141148, 1991. 2 [25] A. Selle, M. Lentine, and R. Fedkiw, mass spring model for hair simulation, ACM Trans. Graph., vol. 27, no. 3, p. 111, Aug. 2008. [Online]. Available: https://doi.org/10.1145/1360612.1360663 2 [26] M. Bergou, M. Wardetzky, S. Robinson, B. Audoly, and in ACM SIGGRAPH Discrete E. Grinspun, 2008 Papers, ser. SIGGRAPH 08. New York, NY, USA: Association for Computing Machinery, 2008. [Online]. Available: https://doi.org/10.1145/1399504.1360662 elastic rods, [27] T. Kugelstadt and E. Schömer, Position and orientation based cosserat rods, in Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, ser. SCA 16. Goslar, DEU: Eurographics Association, 2016, p. 169178. 2 [28] Q. Lyu, M. Chai, X. Chen, and K. Zhou, Real-time hair simulation with neural interpolation, IEEE Transactions on Visualization and Computer Graphics, vol. 28, no. 4, pp. 18941905, 2020. 2 [29] T. Stuyck, G. W.-C. Lin, E. Larionov, H. yu Chen, A. Bozic, N. Sarafianos, and D. Roble, Quaffure: Real-time quasi-static neural hair simulation, 2025. [Online]. Available: https://arxiv.org/abs/2412. 10061 2 [30] Z. Wang, G. Nam, T. Stuyck, S. Lombardi, C. Cao, J. Saragih, M. Zollhöfer, J. Hodgins, and C. Lassner, Neuwigs: neural dynamic model for volumetric hair capture and animation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023, pp. 86418651. [31] J. X. Zhang, J. Zhu, H. Chen, and S. Marschner, Hairformer: Transformer-based dynamic neural hair simulation, 2025. [Online]. Available: https://arxiv.org/abs/2507.12600 3 [32] T. Sun, G. Nam, C. Aliaga, C. Hery, and R. Ramamoorthi, Human hair inverse rendering using multi-view photometric data, in Eurographics Symposium on Rendering, 2021. [Online]. Available: https://api.semanticscholar.org/CorpusID:235681666 3 [33] R. A. Rosu, S. Saito, Z. Wang, C. Wu, S. Behnke, and G. Nam, Neural strands: Learning hair geometry and appearance from multiview images, ECCV, 2022. 3 [34] G. Nam, C. Wu, M. H. Kim, and Y. Sheikh, Strand-accurate multiview hair capture, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 3, 5, 12 [55] , Real-time physically guided hair Transactions on Graphics vol. 43, no. 4, pp. 95:195:11, 07 2024. https://doi.org/10.1145/3658176 14 interpolation, ACM (Proceedings of SIGGRAPH 2024), [Online]. Available: [56] J. A. A. Herrera, Y. Zhou, X. Sun, Z. Shu, C. He, S. Pirk, real- [Online]. Available: https: and D. L. Michels, Augmented mass-spring model time dense hair simulation, 2024. //arxiv.org/abs/2412.17144 14 for [57] J. Hsu, T. Wang, K. Wu, and C. Yuksel, Stable cosserat rods, ser. SIGGRAPH 25. New York, NY, USA: Association for Computing Machinery, 2025. [Online]. Available: https://doi.org/10. 1145/3721238.3730618 [58] C. He, J. A. Amador Herrera, Z. Shu, X. Sun, Y. Feng, S. Pirk, D. L. Michels, M. Zhang, T. Y. Wang, J. Dorsey, H. Rushmeier, and Y. Zhou, Digital salon: An ai and physics-driven tool for 3d hair grooming and simulation, arXiv preprint arXiv:2507.07387, 2025. 14 [59] C. He, J. A. Amador Herrera, Y. Zhou, Z. Shu, X. Sun, Y. Feng, S. Pirk, D. L. Michels, M. Zhang, T. Y. Wang, and H. Rushmeier, Digital salon: An ai and physics-driven tool for 3d hair grooming and simulation, in SIGGRAPH Asia 2024 Real-Time Live!, ser. SA Real-Time Live! 24, 2024. 14 [60] Y. Shen, S. Saito, Z. Wang, O. Maury, C. Wu, J. Hodgins, Y. Zheng, and G. Nam, Ct2hair: High-fidelity 3d hair modeling using computed tomography, ACM Transactions on Graphics, vol. 42, no. 4, pp. 113, 2023. 16 [35] S. Saito, L. Hu, C. Ma, H. Ibayashi, L. Luo, and H. Li, 3d hair synthesis using volumetric variational autoencoders, ACM Trans. Graph., vol. 37, no. 6, Dec. 2018. [Online]. Available: https://doi.org/10.1145/3272127.3275019 3 [36] D. Geng, C. Herrmann, J. Hur, F. Cole, S. Zhang, T. Pfaff, T. LopezGuevara, C. Doersch, Y. Aytar, M. Rubinstein, C. Sun, O. Wang, A. Owens, and D. Sun, Motion prompting: Controlling video generation with motion trajectories, arXiv preprint arXiv:2412.02700, 2024. 3 [37] Z. Liu, H. Zhu, R. Chen, J. Francis, S. Hwang, J. Zhang, and J. Oh, Mosaic: Generating consistent, privacy-preserving scenes from multiple depth views in multi-room environments, arXiv preprint arXiv:2503.13816, 2025. [38] A. Razavi, A. Van den Oord, and O. Vinyals, Generating diverse high-fidelity images with vq-vae-2, Advances in neural information processing systems, vol. 32, 2019. 3, 4, 12 [39] A. Van Den Oord, O. Vinyals, et al., Neural discrete representation learning, Advances in neural information processing systems, vol. 30, 2017. 4 [40] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, Focal loss for dense object detection, 2018. [Online]. Available: https: //arxiv.org/abs/1708.02002 4 [41] L. Hu, C. Ma, L. Luo, and H. Li, Single-view hair modeling using hairstyle database, ACM Transactions on Graphics (Proceedings SIGGRAPH 2015), vol. 34, no. 4, July 2015. 5, 10 [42] C. He, X. Sun, Z. Shu, F. Luan, S. Pirk, J. A. A. Herrera, D. L. Michels, T. Y. Wang, M. Zhang, H. Rushmeier, and Y. Zhou, Perm: parametric representation for multi-style 3d hair modeling, in International Conference on Learning Representations, 2025. 5 [43] C. H. Yi Zhou, Xin Sun, Hair20k: large 3d hairstyle [Online]. Available: https: database for hair modeling, 2024. //zhouyisjtu.github.io/project_hair/hair20k.html [44] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, Film: Visual reasoning with general conditioning layer, in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018. 6, 11 [45] U. Yoo, J. Francis, J. Oh, and J. Ichnowski, Kinesoft: Learning proprioceptive manipulation policies with soft robot hands, in Proceedings of the 9th Conference on Robot Learning (CoRL), 2025. 8, 14 [46] B. Wen, M. Trepte, J. Aribido, J. Kautz, O. Gallo, and S. Birchfield, Foundationstereo: Zero-shot stereo matching, CVPR, 2025. 12 [47] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. Dollár, and C. Feichtenhofer, Sam 2: Segment anything in images and videos, 2024. [Online]. Available: https://arxiv.org/abs/2408.00714 12 [48] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, et al., Grounding dino: Marrying dino with grounded pre-training for open-set object detection, arXiv preprint arXiv:2303.05499, 2023. 12 [49] T. Ren, Q. Jiang, S. Liu, Z. Zeng, W. Liu, H. Gao, H. Huang, Z. Ma, X. Jiang, Y. Chen, Y. Xiong, H. Zhang, F. Li, P. Tang, K. Yu, and L. Zhang, Grounding dino 1.5: Advance the \"edge\" of open-set object detection, 2024. 12 [50] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan, Z. Zeng, H. Zhang, F. Li, J. Yang, H. Li, Q. Jiang, and L. Zhang, Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 12 [51] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollár, and R. Girshick, Segment anything, arXiv:2304.02643, 2023. [52] Q. Jiang, F. Li, Z. Zeng, T. Ren, S. Liu, and L. Zhang, T-rex2: Towards generic object detection via text-visual prompt synergy, 2024. 12 [53] J. Hsu, N. Truong, C. Yuksel, and K. Wu, general stage initialization for Transactions on Graphics vol. 41, no. 4, pp. 64:164:13, 07 2022. https://doi.org/10.1145/3528223.3530165 14 twosag-free deformable simulations, ACM (Proceedings of SIGGRAPH 2022), [Online]. Available: [54] J. Hsu, T. Wang, Z. Pan, X. Gao, C. Yuksel, and K. Wu, Sag-free initialization for strand-based hybrid hair simulation, ACM Transactions on Graphics (Proceedings of SIGGRAPH 2023), vol. 42, no. 4, 07 2023. [Online]. Available: https://doi.org/10.1145/3592143 14 IX. APPENDIX A. Table of Contents Simulation Setup: More Details (Sec. IX-B): more technical details on hair-combing simulation. Dynamics Learning: More Details (Sec. IX-C): more technical details on hair-combing dynamics learning, including method implementation, data processing, and formal definitions of the evaluation metrics. Dynamics Learning: Additional Results (Sec. IXD): additional experiments and results on hair-combing dynamics learning, including qualitative evaluation, performance comparison on seen and unseen hairstyles for effectiveness and generalization test, and an additional ablation study on our design choice. Closed-loop Hair Styling: More Details (Sec. IX-E): more technical details on the system implementation for closed-loop, goal-conditioned hair styling. Closed-loop Hair Styling: Additional Results (Sec. IX-F): gallery of DYMO-Hairs qualitative results on various messy cases of diverse unseen hairstyles in simulation, further demonstrating our systems generalizability and effectiveness. Failure Mode Analysis (Sec. IX-G): analysis on typical failure modes of our simulation, dynamics modeling, and hair styling system, outlining directions for future improvement. System Time Cost Analysis (Sec. IX-H): analysis on the overall identifying directions for future optimization on system efficiency. time cost of our system, B. More Details on Simulation Each hairstyle from USC-HairSalon [41] contains 10K strands, with each strand represented by 100 consecutive 3D points. For efficient simulation, we downsample each hairstyle to 2K strands while preserving its overall geometry. To improve simulation stability, we further preprocess each strand to ensure appropriate linear density while maintaining its curvature. Directly importing these hairstyles into our simulator can lead to physical incompatibilities, since they were originally designed for visual realism rather than physical accuracy. In particular, they typically do not account for physical properties such as the need for inner or external forces to preserve hairstyle in reality. As result, hair may fail to maintain its original shape and instead sag under gravity at the start of simulation. To mitigate this issue, we introduce gravity scheduling mechanism in our hair-combing simulation. Specifically, gravity is enabled only for strands that exhibit sufficient positional movement, typically caused by contact with the combing tool or inter-strand interactions, and disabled for all other strands. This design reflects the observation that, during combing, only strands subject to external contact forces should move, while others should remain stable. With this scheduling, strands influenced by external contact forces exhibit realistic motion under gravity in our simulation, while the rest remain stable. Despite TABLE II KEY SIMULATION PARAMETERS FOR HAIR COMBING Parameter Time step Gravity Particle size Stretch solver iterations Twist solver iterations Stretch relaxation Bending relaxation Twist relaxation Value 0.01 [0, 0, -5.0] 0.01 20 20 1.0 1.0 1.0 Parameter Substeps Damping Linear Density Bending solver iterations Stretch compliance Bending compliance Twist compliance Index gap (twist) Value 5 0.99 1.0 20 0.0 1 105 1 105 10 30 its simplicity, this mechanism effectively reduces gravityinduced sagging and enables visually-realistic, physicallyplausible hair-combing simulations with our lightweight simulator, supporting our large-scale synthetic data generation for dynamics learning and closed-loop experiments for system evaluation. The key parameters used in our simulation are listed in Tab. II. Note that the index gap in the twist constraints is tuned individually for each hairstyle. C. More Details on Dynamics Learning 1) Method Implementation Our Method. In the state encoding path, the model takes volumetric initial state of resolution V0 = 64 64 128 as input, concatenates the occupancy and orientation channels into V0 4 tensor, and hierarchically encodes it into lower resolutions using two 3D convolutional encoders with down-sampling. This produces latent embeddings hbottom RV1D1 and htop RV2D2 , where V1 = 16 16 32, V2 = 8 8 16, D1 = 256, and D2 = 512. In the motion encoding path, we sample the combing motion into up to = 16 uniformly spaced key tool poses and convert them into volumetric grid spatially aligned with the state, as introduced in Sec. IV-C. contact radius of 3 voxels is used to define the cutoff, forming the local contact region prior. The resulting grid is encoded into control embeddings cbottom RV1D1 and ctop RV2D2 , which are then fused with hbottom and htop using 3D attentionbased feature fusion mechanism for latent state editing. The fused embeddings are projected to = 32 and quantized with codebooks of size 2048 entries at both levels. The decoders progressively up-sample and decode the quantized embeddings into the original resolution V0 for end state prediction. To better recover geometric details from the latent embeddings, we use transposed convolutions with residual connections instead of direct interpolation for up-sampling. PC-GNN Baseline. Our implementation is built on the original RoboCraft framework [13]. In this paradigm, computational cost and model capacity are jointly determined by the point cloud resolution, the maximum number of neighbors used to construct edges in the graph, and the number of message passing steps per iteration. To balance these factors while ensuring sufficient message passing capability for capturing hair deformation, which requires dense propagation, we set the maximum number of neighbors to 10, the number of message passing steps to 10, and the point cloud resolution to 2000, which is the highest feasible within the computational budget. The cylindrical tool is represented by 10 points, which are included in the graph together with the hair points. To enable the model to predict per-point local strand orientation for hair modeling, we extend the original prediction head to output both per-point orientations in the end state and point positions. Supervision is applied to both position and orientation predictions. For positions, we use Chamfer Distance between the predicted and groundtruth point clouds. For orientations, we use the bidirectional cosine distance between the predicted and groundtruth orientations, accounting for orientation symmetry, with point-level assignment determined by nearest neighbors. We do not use Earth Movers Distance for supervision, as it is computationally expensive and infeasible for training at the point cloud resolution required in our hair scenario. Following the standard setting of this paradigm, we do not use pre-training for PC-GNN. V-UNet Baseline. We use the same state representation and volumetric motion representation as in our method for V-UNet. The model takes the initial state and motion representation as input and employs 4-level UNet architecture to progressively encode and decode them for end state prediction. The resolution is down-sampled from 6464128 to 448 in the UNet. In each down-sampling and encoding layer (except the first, where the resolution is too high and may cause out-of-memory issues), the motion embedding is fused with the state embedding using finegrained, 3D attention-based mechanism, enabling motion information propagation within the grid. During decoding, both state and motion embeddings from the encoder are passed through skip connections. As in our method, we use transposed convolutions with residual connections for upsampling, and apply the same loss functions for supervision. We do not use pre-training for V-UNet. V-FiLM Baseline. V-FiLM builds on the same pre-trained components as our method, including the state encoders, latent codebooks for quantization, and decoders. The key difference is that V-FiLM employs FiLM [44]-based approach to adapt the pre-trained model for dynamics modeling. Specifically, it encodes the volumetric motion representation into 1-D latent embedding, then injects it into the two hierarchical state encoders. This conditions the state encoding on the motion signal through learnable, featurewise affine transformation applied per channel. After fusion, the embeddings are quantized and decoded to predict the end state, following the same procedure as in our method. Unlike our method, which performs fine-grained, spatially aligned state-action fusion, V-FiLM adopts more straightforward fusion mechanism to incorporate motion signals. We apply the same loss functions as our method for supervision. 2) Data Processing To prepare the synthetic hair-combing dynamics data for training, we first convert the 2K strands in each state into dense point clouds with per-point orientations, computed from the ground-truth strand geometry. The point cloud is then down-sampled to 50K points and voxelized into 64 64 128 volumetric grid for all voxel-based methods, and to 2K points for PC-GNN. Considering the hair scale in both simulation and the real world, each voxel in the grid corresponds to approximately 5 mm in the real-world physical space. 3) Formal Definitions of the Evaluation Metrics For fair comparison, we convert the outputs of all volumetric methods into point clouds with per-point orientations and down-sample them to the same resolution as PC-GNN. Specifically, voxel centers are used as points, with their corresponding voxel orientations assigned as point orientations. The resulting point cloud is then down-sampled to 2K-point resolution. The ground-truth data is also down-sampled to the same resolution. We use three metrics for evaluation: a) Point-level Chamfer Distance CDpoint. To evaluate positional accuracy, we compute the symmetric Chamfer Distance between the predicted point cloud and the ground truth Q, ignoring orientation: CDpoint(P, Q) = 1 2 (cid:32) 1 (cid:88) qQ min pP p2 + 1 (cid:88) pP (cid:33) min qQ q2 . Note that, in addition to the mean values, we report percentile statistics to better capture the error distributions. This highlights sparse but significant deformations (e.g., affecting only few strands) that may be obscured by averaging. Specifically, for each predicted and ground-truth point, we record the nearest distance to the opposite set, yielding two error distributions. Percentiles are computed for each, and the final percentile values are obtained by averaging over both directions. b) Point-level Orientation Error Errori. To evaluate orientation accuracy, we compute the angular difference between predicted and ground-truth orientations. Each predicted point is matched to its nearest ground-truth neighbor within positional error threshold of 2 cm, and their orientation vectors are compared. The per-point error is defined as the absolute angular deviation (in degrees) between the two orientations, computed from cosine similarity while accounting for orientation symmetry. Errori is given by the average error across all predicted points with valid groundtruth assignments within the threshold. As with CDpoint, we report both mean errors and percentile statistics to capture the distribution of orientation deviations, highlighting sparse but significant deformations. c) Strand-level Chamfer Distance CDstrand. As complement to point-level evaluation, we reconstruct strand segments from the point clouds with orientations and assess prediction quality at the strand level. This metric evaluates on average how well each predicted strand segment aligns with its closest ground-truth strand segment and vice versa, considering the piecewise-linear curve structure of hair strands. Formally, for two strand segments = {a1, . . . , aM } and = {b1, . . . , bN }, the directed distance from to is defined as D(A, B) ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) k=1 min 1l<N min x[bl,b(l+1)] ak x2, where [bl, b(l+1)] denotes the line segment between consecutive points of strand B. The strand-level Chamfer Distance is then defined by averaging over both directions: CDstrand(P, G) = 1 2 1 (cid:88) min GG D(P, G) + 1 (cid:88) GG min , D(G, ) where and denote the sets of predicted and ground-truth strand segments, respectively. As with CDpoint and Errori, we report both mean errors and percentile statistics to capture the distribution of strand-level distances, highlighting sparse but significant deformations. D. Additional Results on Dynamics Learning 1) Qualitative Evaluation We provide qualitative comparison on unseen hairstyles across all methods for hair-combing dynamics learning in Fig. 9. The results show that, among all methods, ours best captures local hair deformation in the near-motion region. 2) Comparison on Seen and Unseen Hairstyles As complement to the results in Sec. VII-B, in Tab. III, we compare the performance of all methods on 10 seen hairstyles used in training and 7 novel unseen hairstyles, reporting more detailed percentile metrics to better capture the error distributions. The results show that V-UNet fits the training distribution best but fails to generalize well to unseen hairstyles. In contrast, our method achieves the second-best performance on seen hairstyles, with only small gap compared to V-UNet, while outperforming all other methods on unseen hairstyles with larger margin. This demonstrates both the effectiveness and the generalizability of our approach. We note that the performance gap between V-UNet and our method on training hairstyles arises because V-UNet leverages skip-connections to directly preserve input details during decoding. In contrast, our method separates the encoders and decoders to pre-train latent space, and the subsequent quantization operation may introduce more detail loss. To alleviate this, we adopt hierarchical model structure in our design following VQ-VAE-2 [38], as validated in Sec. IX-D.3. Nevertheless, some fine-grained detail loss may remain, and future work may explore improved latentspace pre-training designs that better balance generalizability with geometric detail preservation. 3) Ablation Study on Hierarchical Model Structure In Sec. VII-B and Tab. III, we use V-UNet and V-FiLM to ablate the effects of the pre-trained latent space design and the ControlNet-style fine-grained state-action fusion design, respectively. Here, we further ablate the hierarchical model structure in our method to evaluate its benefit. We introduce V-1layer, which is identical to our method in all aspects (e.g., pre-training and the ControlNet-style framework) except that it uses single-layer architecture instead of the hierarchical two-layer structure. Specifically, V-1layer removes the bottom-level codebook and directly encodes the initial state input at 6464128 resolution into an 8816 embedding, quantizes it with 4096-entry codebook in = 32, and then decodes the quantized embedding for prediction. The action-conditioned latent state editing is also performed at the 8 8 16 resolution. We compare V-1layer with our method in terms of both latent space pre-training quality and dynamics learning performance. For latent space pretraining, we evaluate reconstruction capability using 1,000 cases with various deformations covering 10 seen and 7 unseen hairstyles respectively, measuring overall reconstruction accuracy at the same 2K-point resolution as above. Higher reconstruction accuracy indicates stronger representation ability of the latent space. For dynamics learning, we evaluate both models end state prediction capability under the same evaluation protocol as in Sec. VII-B and Tab. III. As shown in Tab. IV, the hierarchical structure allows the model to learn stronger latent space, achieving higher reconstruction accuracy on both seen and unseen hairstyles, and leading to improved dynamics learning performance. We visualize reconstruction examples of both seen and unseen hairstyles in Fig. 10. The results demonstrate that the pretrained latent space generalizes well to unseen hairstyles with deformations. Compared to the single-layer variant, the hierarchical design allows our method to construct richer latent space that preserves finer geometric details, providing stronger foundation for downstream dynamics learning. E. More Details on Closed-loop Hair Styling Perception. In simulation, we use oracle strand information for all systems to ensure fair comparison of their goal-conditioned planning capabilities. Specifically, Rulebased System receives front-view 2D oracle strand orientation map from the simulator, while all other methods are provided with 3D oracle dense point clouds with per-point orientations. In real-world experiments, Rule-based System captures front-view RGB-D image at 1280720 resolution using the wrist-mounted camera and estimates 2D orientation map from it. Our system performs 3D state estimation by capturing RGB-D observations at 1280720 resolution from 56 viewpoints around the hair, estimating 2D orientation maps for each, and reconstructing the 3D hair state following [34]. For both systems, we use long exposure time to obtain clearer images for more accurate orientation estimation. For both systems, we estimate depth maps using an external module based on FoundationStereo [46], which computes disparity maps from stereo infrared images and estimates depths, instead of relying on the on-board depth estimation as it typically provides poor performance in our unconstrained hair scenarios. For both systems, we obtain hair segmentation masks using Grounded-SAM-2 [47][52]. Planning. For all model-based systems, at each step, the planner samples 48 action candidates, i.e., 3D tool motions, and optimizes an action trajectory using MPPI to minimize the cost. The planning cost is defined as the strand-level distance between strand segments reconstructed from the predicted future states and the goal state. The planner optimizes over two-step prediction horizon and executes the first step. For Rule-based System, we reimplemented the planner ourselves since the original code is not publicly available. As only limited technical details are provided in their paper [7], Fig. 9. Qualitative Evaluation for Hair-combing Dynamics Learning on Unseen Hairstyles. From left to right: the initial state, the ground-truth end state, and predicted end states from different methods. Key regions are highlighted with red boxes. color map is applied to local strand segments based on their orientations; however, due to discontinuities in the mapping, strands with similar orientations may sometimes appear in different colors. For best understanding of local orientations, please refer to the strand segment geometry by zooming in. TABLE III HAIR COMBING DYNAMICS MODEL EVALUATION ON SEEN VS. UNSEEN HAIR Split Seen Unseen CDpoint 75th Method 90th mean V-UNet V-FiLM Ours 50th PC-GNN [13] 0.0796 0.0717 0.0930 0.1273 0.1636 0.0732 0.0712 0.0871 0.1182 0.1456 0.0780 0.0717 0.0915 0.1273 0.1580 0.1456 0.0883 0.0750 0.0714 PC-GNN [13] 0.0814 0.0713 0.0943 0.1359 0.1733 0.1345 0.1696 0.1708 0.1578 0.0792 0.0717 0.0917 0.0807 0.0715 0.0928 0.1334 0.0775 0.0711 0.0885 0.1240 95th mean 11.47 8.98 10.14 9.55 13.34 14.25 13.60 12. V-UNet V-FiLM Ours 0.1183 TABLE IV HIERARCHICAL STRUCTURE ABLATION IN DYNAMICS LEARNING Split Method CDpoint 90th mean Errori 90th mean CDstrand 90th mean Seen Unseen Seen Unseen Latent Space Pre-training - Reconstruction V-1layer Ours V-1layer Ours 0.0694 0.0612 0.0683 0. 0.1015 0.0959 0.0992 0.0936 8.33 6.83 9.51 7.49 18.11 14.79 19.61 15.89 0.0873 0.0844 0.0824 0.0787 Dynamics Learning - End State Prediction V-1layer Ours V-1layer Ours 0.0791 0.0750 0.0818 0.0775 0.1273 0.1183 0.1371 0.1240 9.75 9.55 12.84 12.03 20.54 20.63 27.06 26.12 0.0981 0.0975 0.1022 0.1005 0.1485 0.1467 0.1398 0. 0.1808 0.1802 0.1909 0.1878 we reproduced their design to the best of our understanding, following their high-level methodology. Specifically, at each step, the planner computes 2D orientation difference map between the estimated orientations of the current and goal states, samples 300 2D points based on the differences for backward strand extraction and root-centric target strand extraction, derives the 2D tool motion, and converts it back to 3D for execution. Errori 90th 75th 25.98 14.70 19.55 11.02 22.66 12.48 20.63 11.74 30.16 17.68 18.28 31.00 17.52 29.59 26.12 15.40 50th 7.45 5.76 6.43 6.23 8.98 9.91 9.43 8.23 95th mean 50th 0.1042 35.38 0.0844 0.0960 27.66 0.0784 0.1038 31.95 0.0833 0.0975 29.00 0.0793 0.0857 0.1052 39.67 0.1047 0.0857 42.29 0.0863 0.1065 39.63 0.0824 0.1005 35.32 CDstrand 75th 0.1298 0.1197 0.1293 0.1210 0.1338 0.1333 0.1350 0. 90th 0.1958 0.1778 0.1960 0.1802 0.1983 0.1966 0.2011 0.1878 95th 0.2465 0.2244 0.2481 0.2290 0.2428 0.2415 0.2478 0.2319 F. Additional Results on Closed-loop Hair Styling In Fig. 11, we show more qualitative results of DYMOHair on diverse unseen hairstyles under various messy initial conditions in simulation. These results complement Fig. 7 and further demonstrate the generalizability and effectiveness of our system on visual goal-conditioned hair styling. In some cases, the system does not fully complete the styling within the given 15-step budget, leaving slight gap between the end state and the goal state, which would require additional planning steps to close. We discuss this failure mode in detail in Sec. IX-G.3. G. Failure Mode Analysis 1) Simulation Our simulation leverages novel PBD-based method for strand-level, contact-rich hair-combing simulation, providing lightweight yet effective solution that achieves visuallyrealistic and physically-plausible results, supporting both large-scale dynamics data generation and closed-loop manipulation experiments. While the lightweight design offers effectiveness and ease of implementation, it can also lead to certain simulation artifacts in some cases. One issue arises from the coupling between the twist constraint and the gravity scheduling mechanism (introduced small gaps or discontinuities (e.g., row 2, column 6 in Fig. 9). We attribute this issue to the purely voxel-level supervision for training, which lacks regularization to enforce geometric continuity. Future work may improve this by introducing geometric regularization terms that leverage strand-structure priors, or by incorporating such structural priors directly into the representation and the model architecture design. Second, as shown in Fig. 10 and discussed in Sec. IX-D.2, although our hierarchical architecture significantly enhances detail preservation for richer latent space, some fine-grained details may still be lost. Future work could explore improved latent-space pre-training strategies that better balance generalizability and geometric detail fidelity. Finally, scaling up the data with more diverse hairstyles together with increasing the model and dynamics cases, capacity, may further improve the models representation capability, generalizability, and robustness across wider range of hairstyles. 3) Hair Styling System Besides the limitations of the underlying dynamics model discussed above, failures in closed-loop manipulation may also arise from other system-level factors. First, our system uses the strand-level distance as the MPPI planning cost to jointly capture position and orientation. While effective in many cases, the complex physical structure and high degrees of freedom of hair can sometimes make this cost insufficient to fully represent state differences, leading to incomplete styling within the step budget. More broadly, representing the difference between two hair states with single scalar value remains inherently challenging. Future work may explore combining multiple complementary cost terms that better reflect the structural characteristics of hair, however, balancing their weights could introduce additional complexity. Second, unlike prior neural dynamics-based systems that often operate on tabletop tasks, our system naturally works in 3D space, where gravity can reduce real-world styling efficiency. Motions that fail to create sufficient occlusion or friction between the hair and head may result in hair falling back due to gravity. Currently, the system relies on closedloop re-planning and repeated retries to gradually approach the goal, which are effective but not optimal. Future work could incorporate gravity effects directly into the dynamics modeling to make it predictable, or design planners that explicitly consider head geometry during action sampling to favor motions that may lead to gravity-stable hair states. Third, the contact among the combing tool, the head, and the hair remains challenging. Although the 3D-printed deformable finger tip in our system improves contact and has shown effectiveness, it can still be overly stiff, requiring millimeter-level calibration accuracy and potentially generating excessive contact force to the head. Inaccurate calibration may also result in insufficient contact, allowing fine strands to slip through small gaps between the tool and the head, thereby causing styling failures. Future work may explore softer, compliant tool designs [6], [45] that introduce tolerance margins and reduce calibration sensitivity, though Fig. 10. Qualitative Evaluation of the Hierarchical Structure Ablation in Dynamics Learning. From left to right: the ground-truth state and the reconstructed states from different models. color map is applied to local strand segments based on their orientations; however, due to discontinuities in the mapping, strands with similar orientations may sometimes appear in different colors. For best understanding of local orientations, please refer to the strand segment geometry by zooming in. in Sec. IX-B). For example, as shown in row 4, column 7 of Fig. 11, some strands appear to float rather than fall naturally (in the bottom-right region of the image). This happens because the twist constraint preserves the strands initial curvature, while gravity is disabled to prevent sagging, resulting in unrealistic suspended strands. Such artifacts may occur particularly for long strands with complex initial curvature. Another limitation is from the PBD-based collision handling, where each strand is represented as collection of particles. When the initial gap between strands is smaller than the particle size, the hair may be inflated when the simulation starts due to collision responses, preventing accurate preservation of the original hairstyle. This is acceptable in our case, as our primary focus is on ensuring realistic dynamic behavior of hair during the combing process. However, it does constrain the applicability of the simulator to broader hair-related scenarios. Future work may incorporate advances from recent research in computer graphics [53][59] to improve structural and physical modeling of hair for more physically-accurate and stable hair simulation. 2) Dynamics Modeling Our method demonstrates strong effectiveness and generalizability for hair-combing dynamics modeling, enabling model-based system for successful closed-loop, goalconditioned hair styling in both simulation and the real world. However, several limitations remain in the modeling for future improvement. First, as shown in Tab. III, there is still noticeable performance gap between seen and unseen hairstyles. Qualitatively, although our method captures the deformation trends of unseen hairs dynamics better than all baselines, it may still fail to predict complete end-state geometries, leaving Fig. 11. More Qualitative Results of DYMO-Hair on Diverse Unseen Hairstyles with Various Messy Initial States in Simulation. Each row shows one unseen hairstyle with two different cases (A and B). From left to right: the goal, and for each case, the initial state, the intermediate planning steps, and the end state. such modifications could alter the contact dynamics among the tool, the head, and the hair, complicating the dynamics modeling. H. System Time Cost Analysis Our current focus is on closing the loop to enable effective model-based hair manipulation, where the current system demonstrates strong closed-loop hair styling capability. Beyond further enhancing its effectiveness, future efforts could also focus on improving system efficiency. Below, we analyze the overall time cost of our system and discuss potential directions for future optimization. Current Time Cost. In the real world, 5-step manipulation experiment takes approximately 60 minutes in total. It consists of five cycles of multi-view observation capturing, action planning, and action execution, followed by an additional observation step at the end to record the final state for evaluation. For each multi-view observation, the system perform dense capturing: the robot takes 3 seconds to move and capture each view, resulting in 56 views collected in about 3 minutes. The subsequent state estimation takes around 4 minutes. For each action step, the system computes the action planning in approximately 2.5 minutes and executes it in 30 seconds. Potential Future Optimizations. We identify three directions to improve system efficiency: Reducing redundant views. Currently, we thoroughly capture 56 dense views to ensure accurate state estimation. Future work may explore reducing the number of views to achieve better balance between efficiency and estimation accuracy. Faster state estimation. Additional parallelization and GPU-accelerated computation (e.g., following [60]) could substantially speed up state estimation. While our method is already much faster than full strand reconstruction, which typically requires tens of minutes, there still remains room for further acceleration. Faster planning computation. The large-batch action sampling required for planning and strand-level distance computation for optimization remain time-consuming. While large batches are often necessary to ensure sufincorporating parallelism and GPU ficient sampling, acceleration into the strand-based distance computation during planning could further enhance efficiency. Overall, while our system achieves effective closed-loop, model-based manipulation, the time cost analysis reveals several opportunities to further accelerate the system through targeted engineering improvements."
        }
    ],
    "affiliations": [
        "Bosch Center for Artificial Intelligence, Pittsburgh, Pennsylvania, USA",
        "Epic Games, Inc., Pittsburgh, Pennsylvania, USA",
        "Meta Codec Avatars Lab, Pittsburgh, Pennsylvania, USA",
        "Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA"
    ]
}