{
    "paper_title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation",
    "authors": [
        "Wang Zhao",
        "Yan-Pei Cao",
        "Jiale Xu",
        "Yuejiang Dong",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 0 0 2 5 1 . 2 1 4 2 : r DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation Wang Zhao1 Yan-Pei Cao Jiale Xu1 Yuejiang Dong1,3 Ying Shan1 1 ARC Lab, Tencent PCG 2 VAST 3 Tsinghua University https://thuzhaowang.github.io/projects/DI-PCG Figure 1. Given condition images, DI-PCG can accurately estimate suitable parameters of procedural generators, resulting high fidelity 3D asset creation. Textures and materials are randomly assigned by the procedural generators for visualizations."
        },
        {
            "title": "Abstract",
            "content": "Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous In this work, sample iterations or limited controllability. we present DI-PCG, novel and efficient method for Inverse PCG from general image conditions. At its core is lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers promising approach for efficient inverse PCG and represents valuable exploration step towards 3D generation path that models how to construct 3D asset using parametric models. 1. Introduction Procedural Content Generation (PCG) is powerful mean to design and generate high-quality 3D contents, via algorithmic programs and rules, and has wide application in the gaming and movie industry. Over decades, number of 1 works were proposed to automatically generate various 3D contents such as tree [44, 57, 74], terrain [16, 17], building [47], material [22, 26], city [51, 92], or even the whole natural world [59], through different domain-specific language grammars like L-system [37, 55, 56], shape and split program [77], Blender geometry nodes [12], etc. However, even exhibited with explicit parameter definitions, creating desired 3D asset using PCG is highly non-trivial and requires cumbersome parameter tuning, hindering broader applications such as text or image to 3D generation. This controlling difficulty in PCG leads to Inverse Procedural Content Generation (I-PCG), which aims to inverse the PCG task, i.e. automatically estimate the best-fit parameters from the given observations. The observations could be image, 3D, or other constraints. Similar to other non-linear and non-differential inverse problems, probabilistic sampling-based method is the golden rule for inverse PCG, where set of samples are conducted and scored to approximate the posterior distribution given the observation. Markov chain Monte Carlo (MCMC) [24, 45] is one of the most representative methods. Many variants of MCMC [20, 64, 79] and different likelihood evaluation metrics [75, 81] are explored to improve sampling efficiency and approximation accuracy. Unfortunately, most of the sampling-based methods still entail hundreds or thousands of iterations, with procedural generator forward and evaluation in each iteration, resulting long time to finish the inverse. The key reason is that sampling-based methods do not have any data priors about the target distributions, thus need to approximate it from scratch with numerous samples. Motivated by this, several works [21, 29, 49, 52, 65, 95, 96] aim to utilize deep neural networks to learn the distribution correspondence between PCG parameters and input observations. Despite impressive inverse performance on certain input conditions (e.g. sketch) or categories, these methods often suffer from limited condition ability, poor generalization on real-world data, and specific designs for certain object categories, preventing their usage as general way for inverse PCG and 3D generation. In this work, we present DI-PCG, an innovative diffusion model based method for efficient inverse PCG from general image conditions. At its core is light-weight diffusion transformer model, where the PCG parameters are directly treated as the denoising target and the observed image serves as the condition to control the parameter generation. Through iterative denoising score-matching training, the diffusion model learns to fit the parameter space of the current procedural generator, and can perform efficient sampling on the target posterior distribution of PCG parameters within several seconds, controlled by the condition image. The sampled parameters are then fed into PCG, resulting in high-quality 3D asset generation from images. Our proposed DI-PCG is efficient and effective. It requires only 7.6M network parameters, 30 GPU hours to train, and several seconds to draw sample from, thus suitable for resource-constraint scenarios. Besides efficiency, DI-PCG could effectively fit the procedural generators parameter space, recover the corresponding parameters accurately, and generalize well to in-the-wild images, thanks to the adoption of visual foundation model features for image condition. Moreover, DI-PCG is self-contained, which only relies on current procedural generator to generate data for training, without any external data collection efforts, yet generalizes well to real-world unseen data. Both quantitative and qualitative experiments clearly verify the effectiveness of DI-PCG on inverse PCG and image-to-3D generation tasks. Figure 1 shows some examples. The generated 3D assets are in high-quality, consistent with their condition images, and ready to use for downstream applications. DI-PCG demonstrates promising way of utilizing the diffusion model to learn distribution priors for efficient inverse PCG. Compared to previous sampling-based or feedforward neural network-based methods, DI-PCG features significant speed-ups and nice generalization ability. From another perspective, DI-PCG leverages procedural generator and its parameters as an explicit 3D representation, and designs diffusion model to model its distribution, enabling high-quality, ready-to-use, and editable image-to-3D asset generation. DI-PCG represents valuable exploration step towards an encouraging 3D generation path, where how to construct 3D asset is modeled, with parametric model, instead of modeling 3D object itself, and the parametric model can be inversely determined given input conditions. 2. Related Works 2.1. Procedural Content Generation and Inverse Procedural Content Generation (PCG) is long-standing research problem in computer graphics and vision community. L-systems [37] were firstly proposed for biological modeling, and later extended to model geometry of plants [44, 57]. To concisely describe different object categories, many domain-specific languages were introduced such as shape grammar [38, 77], split grammar [47, 84] for generating trees [57, 74], man-made facades and buildings [66]. Beyond shapes, PCG is also widely used in generating textures [13] and materials [27, 70]. Utilizing powerful node graph grammar in modern commercial software like Blender [12], Infinigen [59] and Infinigen Indoors [60] developed broad collection of diverse procedural generators including objects, natural assets, and compositional scenes, greatly facilitating the synthetic data generation. Recently, inspired from the success of Large Language Models (LLMs), many works [2, 28, 30, 32, 34, 73, 78, 88, 94] proposed to leverage the LLM reasoning capability to automatically design or edit procedural generators 2 for 3D creation and interaction. While still limited in certain constrained scenarios, these works demonstrate promising attempts to employ general LLM agents with contexts to produce usable domain-specific languages for PCG. Despite the ability of generating high-quality 3D assets with diversity, one of the major drawback of PCG is its difficulty to control. While easy to tune one or two specific parameters, it would be annoyingly complicated to find the appropriate combinations for tens of parameters to produce the desired shape. Inverse PCG is then introduced to inversely find the best fit parameters from the observations. Many works [3, 5, 75, 81, 89] used Markov chain Monte Carlo (MCMC) methods to search the parameters. To better deal with multiple groups of parameters, Talton et al. [79] adopt Reversible Jump MCMC, with same spirit in [62, 63]. Ritchie et al. [64] further proposed stochasticallyordered sequential Monte Carlo to reduce the total numbers of PCG forward. Other optimization algorithms such as genetic [25] was also studied for inverse PCG. PICO [33] designed procedural model with constraint optimizer for interactive controlling. To enable the continuous optimization, some works [18, 19, 76] tried to make the PCG process differentiable and then optimize it using gradients. With the tremendous success of neural networks for solving vision problems, number of works have explored using neural network to directly map input conditions to the PCG parameters. Ritchie et al. [65] built neuralguided procedural model, where certain ramdom parameters are predicted by the trained network. CSGNet [68] and InverseCSG [15] focus on inferring parameters of Constructive Solid Geometry (CSG), which can be viewed as special class of procedural modeling used in CAD. In [29, 49, 52], procedural models are controlled via sketches, with convolutional neural networks (CNNs) learned to extract sketch features and regress parameters. Guo et al. [21] and DeepTree [95] focus on branching structures like tree and introduce specific designs to handle it. Different from these methods, our DI-PCG enables general image besides sketch as the input condition, and supports any procedural generator with nearly zero modifications of code. By leveraging the best practices from recent diffusion models, such as using pre-trained visual foundation model features of input image as condition, and transformer-based diffusion denoising architecture, DI-PCG achieves accurate and generalizable inverse results for different generators. 2.2. Diffusion Models for 3D Generation Diffusion models have achieved remarkable progress in generative modeling, with increasing popularity in 3D generation. Due to the scarcity of 3D data, early works attempted to utilize 2D diffusion priors through score distillation sampling [54] and its enhancements [9, 36, 46, 67, 83]. This distillation inherently lacks view consistency and 3D priors, often leading to blurry textures and multi-head Janus problem. To mitigate this issue, Zero-1-to-3 [39] proposed to generate novel view images under required camera viewpoints, and reconstruct 3D representation using generated multiview images. Following this line of research, number of works [40, 42, 58, 71, 72, 82] explored fine-tuning 2D diffusion models to directly generate multiview images via carefully designed view interaction, which greatly improve the view consistency and thus benefit 3D generation. With the advent of large-scale 3D datasets such as Objaverse [14], training 3D native diffusion models is made possible. Different kinds of 3D shape representations are explored such as point cloud, voxel, mesh, implicit functions, etc. Point-E [48] pioneered the denoising diffusion on point cloud. LION [80] and SLIDE [43] further introduced latent point diffusion model with point cloud VAE to enhance the compactness. To directly model 3D surfaces, PolyDiff [1] represented meshes as quantized triangle soups and applied diffusion model on triangle vertex coordinates. MeshDiffusion [41], in another way, utilized deformable marching tetrahedra [69] representation for meshes and trained diffusion model upon it. By exploiting sparse voxel hierarchy or Octree-base latent voxel representations, XCube [61] and OctFusion [86] managed to relief the memory-resolution trade-off of 3D voxels and train diffusion models over latent voxels, achieving detailed 3D generation results. Different from above explicit 3D representations, many works focus on implicit representations which features higher compression ratio, infinite decoding resolution, and intrinsic smoothness. SDFusion [10] employed 3D VAE to decode SDF fields from denoised latent variables. 3DGen [23] and Direct3D [85] selected triplane [6] as the representation, while Michelangelo [93], 3DShape2VecSet [90] and CLAY [91] adopt pure 3D shape latent vectors with VAEs to fully unleash the scaling ability. Image conditioned inverse PCG can be viewed as image to 3D generation. From this view, DI-PCG essentially takes the procedural generator and its parameters as powerful, highly compact, editable 3D representation and trains diffusion model on top of it for high-quality 3D generation. 3. Methods 3.1. Preliminaries Procedural Generator. Procedural generator defines algorithmic rules with set of parameters to create an asset. generator usually handles one specific category of objects, such as chair, vase, tree, etc. For example, chair is procedurally constructed by the selected parameters which describe the back type of the chair, the leg height, the numbers of bars, the existence of arms, etc. Theoretically, it can generate infinitely many variants of objects by randomly sampling parameters. In practice, the capability of generator 3 Figure 2. Overview of DI-PCG. (Left) The procedural generator consists of programs and parameters, and can be randomly sampled to produce various shapes. (Right) To control it with images, DI-PCG trains denoising diffusion model directly upon canonicalized generator parameters, using DINOv2 to extract condition image features and inject them via cross attention. The resulting parameters are projected back to original ranges and then fed into the generator, delivering high-quality 3D generation with neat geometry and meshing. to provide diverse instances is determined by the generality and granularity of its rules. Diffusion Model. diffusion model consists of forward noising and reverse denoising process. The forward process gradually corrupts clean data x0 into Gaussian distribuαtx0, (1 αt)I), tion (0, I) by: q(xtx0) = (xt; where x0 is the input data, is the timestep and αt are constant hyperparameters. With the reparameterization trick, we can sample xt = 1 αtϵt, where ϵt αtx0 + (0, I). The reverse process is then defined through Markov chain: pθ(xt1xt) = (µθ(xt), Σθ(xt)). By parameterizing µθ as noise prediction network ϵθ, the reverse process is trained via the variational lower bound, with the objective reduced to the mean square error (MSE) between the predicted noise and the ground truth noise: Lθ = Ex0,t,ϵtϵθ(xt, t) ϵt2 2. (1) After training, the diffusion model can sample directly on the data distribution of from Gaussian distribution noise. 3.2. Diffusion Model for Inverse PCG Our proposed DI-PCG considers the procedural generator with its parameters as controllable 3D shape representation, and carefully designs and trains diffusion model for the parameters, enabling to efficiently sample the target parameters under condition, as illustrated in Figure 2. Next, we will describe in detail the representation, architecture, condition scheme and the data preparation process. Representation. We directly treat the parameters of the procedural generator as the parametric representation of the 3D models, and learn to sample it with diffusion models. 4 Specifically, we assume that the given procedural generator provides list of its controllable random parameters = {p0, p1, p2, ..., pN }, and each parameter has its own sampling range, e.g. minimum and maximum values for continuous parameters, and all available choices for discrete parameters. If not provided, we manually derive them from the procedural generators code. Since the procedural generator has both continuous and discrete parameters, which is difficult for the diffusion model to jointly model, we first make the discrete parameters continuous. We uniformly cut [1, 1] into pieces where each piece corresponds to discrete choice. To facilitate training, the continuous parameters are also normalized to [1, 1] according to the minimum and maximum values. We denote these canonicalization operations together as reversible projection ϕ from the original parameter set to the normalized continuous representation = ϕ(p). These normalized parameters [1, 1]N 1 are then used in the diffusion noising and denoising process. During inference, the sampled normalized parameters are projected back to the original generator parameters using = ϕ1(x), and the 3D asset is then generated via the procedural generator with p. Model architecture. Following recent successful practices [4, 8, 91] in both 2D and 3D generative modeling, we employ the Diffusion Transformer (DiT) [53] model. The DiT model, which served as ϵθ in Eq. 1, predicts the noise at each timestep via cross and self attentions: ϵθ(xt, t, c) = {CrossAttn(SelfAttn(xt), c)}L (2) where xt is the noisy version of x0, and represents the condition features. denotes the number of attention layers. Since our procedural parameter representation is Chair images Results Table images Results Vase images Results Figure 3. Qualitative results for chair, table, and vase generations. Input images are collected from the internet. fairly expressive and compact, the denoising variable usually only contains dozens of tokens. Thus, we can use lightweight transformer model to process it. We build the DiT with 12 attention layers with 6 heads, and the hidden feature dimensions set to 192, resulting in an efficient model with 7.6M parameters. Compared to large-scale 3D generative models [35, 85, 91] with hundreds of millions or billions of parameters for learning general objects, DI-PCG takes different path, where tiny, generator-specific model is responsible for creating category-specific 3D objects in high quality. With the increasing number of available procedural generators, DI-PCG can be potentially extended to diffusion model collection, and different model combinations for various categories can be deployed to fulfill the application demands in flexible way. Condition scheme. DI-PCG takes single image as the observed data, and injects it into diffusion model as conditions. To facilitate the generalization ability, we utilize pre-trained visual foundation model to provide general and compact latent representations for images. Specifically, given condition image I, we use pre-trained DINOv2 [50] model to extract spatial patch features as tokens RM C, where is the token length and is the feature channel number. MLP projector is applied to map the feature tokens to the hidden dimension of DiT. We adopt cross attention to integrate conditions for better spatial alignment, as formulated in Eq. 2. Data preparation. DI-PCG is trained with imageparameter pairs generated from the corresponding procedural generator. This self-contained training characteristic is natural and necessary, since the desired objects may be at the end of the long-tail data distribution and hard to collect. To train the diffusion model, we randomly sample parameters, use the procedural generator to build 3D models, and then render RGB images of the model. To improve the generalization ability, multi-view rendering and data augmentation are employed. Specifically, we render the image from the combinations of three azimuths [0, 30, 60], two elevations [30, 60], and two camera distances [1.8, 2.0]. Random color augmentation, flipping and cropping are adopted. In addition, we occasionally drop the RGB values and use binary mask as condition, and also sometimes use edge maps from Canny Detector, to enhance the model robustness to texture variations and make it focus on the shapes. Implementation details. To demonstrate the effectiveness of the proposed DI-PCG, we select six procedural generators from Infinigen [59] and Infinigen Indoors [60], namely Chair, Table, Vase, Basket, Flower and Dandelion. For 5 Input image Shap-E [31] Michelangelo [93] InstantMesh [87] CraftsMan [35] DI-PCG Figure 4. Qualitative comparisons of DI-PCG with baselines. each procedural generator, we generate 20000 data pairs following the above mentioned data preparation process, with 18000 for training and 2000 for validation. We train diffusion model for each procedural generator, resulting in total six diffusion models. These diffusion models have the same model configurations, only except for the input token length, which is determined by the parameter numbers of the generator. Note that DI-PCG is general method suitable for any procedural generator, without proceduralspecific priors in design. Condition images are resized into 256 256 resolution and processed by the DINOv2 ViTB/14 model. Each diffusion model is trained on single NVIDIA V100 GPU for around 30 hours. 4. Experiments To validate the effectiveness of DI-PCG, we conduct detailed experimental evaluations both qualitatively and quantitatively. For baselines, we select representative stateof-the-art 3D reconstruction and generation methods, including 3D native diffusion methods Shap-E [31], SDFusion [10], Michelangelo [93], CraftsMan [35] and large reconstruction model based method InstantMesh [87]. 4.1. Qualitative Results Image condition. We collect diverse images from internet for all six categories. These images are in multiple styles with different object orientations, delicate geometries, various textures and materials, forming an extensive and challenging test for image-to-3D generation methods. In Figure 3, we show qualitative results on chair, table, and vase categories. Our method can reliably recover appropriate procedural generator parameters, thus deliver high fidelity 3D generated models of neat geometry, standard meshing and precise alignments with condition images. We recInput sketches Results Input sketches Results Input sketches Results Figure 5. Sketch-conditioned generation results. Textures and materials are randomly picked by the procedural generators. Test Split of DI-PCG"
        },
        {
            "title": "ShapeNet Chairs",
            "content": "CD EMD F-Score CD EMD F-Score Shap-E SDFusion Michelangelo CraftsMan InstantMesh DI-PCG 0.261 0.252 0.181 0.253 0.098 0.033 0.235 0.234 0.171 0.231 0.097 0.028 0.208 0.167 0.289 0.189 0.416 0.896 0.227 0.255 0.111 0.177 0.095 0.093 0.201 0.244 0.125 0.168 0.112 0. 0.285 0.178 0.407 0.280 0.473 0.452 Table 1. Quantitative comparisons on the test split of DI-PCG and the selected ShapeNet chair subset. ommend readers to supplementary materials for more results. We also conduct comparisons with above mentioned strong baselines. As shown in Figure 4, thanks to its parametric representation upon procedural generators, DI-PCG achieves much improved generation results, being able to preserve intricate details such as holes in basket, dandelion petals, etc. As comparison, Shap-E [31] produces noisy surfaces and fails to handle natural objects like flower and dandelion. Michelangelo [93] tends to output smooth geometries thanks to its latent representation design, yet lacks sufficient details or misaligned with the image. InstantMesh [87] and CraftsMan [35] both rely on multi-view diffusion model to dream about the inputs. While more generalizable than direct 3D methods, they suffer from the inconsistency and errors of the generated multi-view images, and also can not recover complex 3D details. Sketch condition. Thanks to the generalization ability of visual foundation model features and our data augmentation strategy, DI-PCG can directly process sketch image conditions and outputs decent 3D generations, as illustrated in Figure 5. This functionality greatly facilitate the object designs and edits, offering simple yet effective way to create high-quality 3D assets. More results are included in supplementary materials. Comparison with MCMC. As the representative sampling method for inverse PCG, MCMC can effectively approximate the parameter distribution, with the presence of powerful scoring metrics and sufficient iterations. We impleInput image 100 iters / 2 mins 500 iters / 8 mins 1k iters / 17 mins 2k iters / 33 mins 5k iters / 83 mins 10k iters / 167 mins DI-PCG / 5 secs Figure 6. Example of comparison with MCMC method. ment vanilla MCMC method with Metropolis-Hasting algorithm [45], and employ DINOv2 [50] as the scorer. The DINOv2 scores each sample by calculating the feature distance between input condition image and the rendered image from generated 3D models. As shown in Figure 6, the MCMC method outputs gradually closer results as the sampling continues, yet often requires thousands of iterations to complete. Due to the costly forward process of Infinigen generators, it can take several hours. The final results may still contain errors due to the limited ability of the scorer and sensitive hyperparameters. Compared to MCMC, DI-PCG learns the target distribution priors with diffusion models, thus can directly sample the desired parameters with high precision in only several seconds. 7 CD EMD F-Score w/o MV & Aug DI-CLIP DI-Small (1.6M) DI-Large (39M) 0.139 0.161 0.108 0.094 0.140 0.163 0.121 0.110 DI-PCG 0.093 0. 0.321 0.288 0.423 0.452 0.452 Table 2. Ablation studies on ShapeNet chairs subset. 4.2. Quantitative Comparison For quantitative evaluations, we use the chair category to demonstrate since it is commonly used and widely available in existing datasets. In addition to the evaluation on test split of our generated data, we also test on the ShapeNet [7] chair models to verify its generalization ability. Specifically, we follow the split of 3D-R2N2 [11], and manually filter the test chair models to exclude totally out-of-domain samples such as sofa-like or artistic-designed chairs which are currently impossible for Infinigen [59] chair generator to model. The resulting ShapeNet chairs contain 218 models for testing. We adopt commonly used 3D metrics Chamfer Distance (CD), Earth Moving Distance (EMD), and F-Score. Table 1 summarizes the results. It clearly shows that DI-PCG can reliably fit the procedural generator and inversely estimate the parameters with high accuracy. Moreover, it generalizes beyond the procedurally generated chairs and achieves comparable or even better results than previous SOTA methods on ShapeNet chairs subset. 4.3. Ablation Study We conduct ablation studies for different components of DI-PCG. The results are obtained on the above mentioned ShapeNet chair subset, summarized in Table 2. w/o MV & Aug indicates generating training data with single view image and no augmentations, thus the performance is degraded. DI-CLIP denotes using CLIP instead of DINOv2 as the feature for condition. It clearly verifies the effectiveness of DINOv2 features on capturing rich shape features. To study the effect of model size, we train another two diffusion models with small (1.6M parameters) and large (39M parameters) network configurations. As shown in the table, larger model with more parameters is not necessary and provides no improvements. While small model indeed causes some performance degradation, the trade-off between model size and performance is reasonable and provides more options for different scenarios. 4.4. Editing Application Thanks to the explicit and semantically meaningful characteristic of the procedural generator parameters, we can easily adjust specific parameter values to edit the 3D model. Some simple editing examples are shown in Figure 7, where Input image Original Edit - No arm Edit - Short arm Edit - Thick legs Edit - Taller Edit - Whole back Edit - Wider Figure 7. DI-PCG supports easy editing by simply adjusting corresponding parameters. the geometric attributes of the given chair, such as leg height, back types, are easily changed. We argue that this handy editing functionality is not in conflict with the controlling difficulty of PCG. It would be painful to find suitable combinations of tens of parameters from scratch, but it is easy and natural to adjust one or two specific parameters to edit existing 3D models. In this way, DI-PCG, as an efficient and effective inverse PCG method, unleashes the controlling advantage of procedural generation. 4.5. Limitations and Future Works As an early attempt to explore diffusion-based inverse PCG for 3D generation, DI-PCG has limitations. First, since DI-PCG relies on off-the-shelf procedural generators, the generation scope is strictly bounded by these generators, i.e. DI-PCG cannot generate out-of-domain objects beyond current generators. Some failure examples in the supplementary materials illustrate this shortage. Second, current DI-PCG only supports image as conditions, while text conditions are widely used in 3D AIGC. Finally, DI-PCG is demonstrated on the object generators, and its applicability on scene-level procedural generation is not verified. Future works include extension to scene generation, more conditions, and automatic generation of procedural generators. 5. Conclusion In this paper, we present DI-PCG, an innovative diffusionbased efficient inverse procedural content generation method for creating high-quality 3D assets. By directly modeling procedural generator parameters as diffusion denoising variables, the posterior distribution of parameters given condition images can be efficiently determined by the learned diffusion model. DI-PCG solves the inverse PCG problem with high efficiency and accuracy, validated by both quantitative and qualitative evaluations. It represents valuable exploration towards promising path for 3D content generation, where parametric models and algorithmic rules together play the roles."
        },
        {
            "title": "References",
            "content": "[1] Antonio Alliegro, Yawar Siddiqui, Tatiana Tommasi, and Matthias Nießner. Polydiff: Generating 3d polygonal meshes with diffusion models. arXiv preprint arXiv:2312.11417, 2023. 3 [2] Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, et al. Scenescript: Reconstructing scenes with an autoregressive structured language model. arXiv preprint arXiv:2403.13064, 2024. 2 [3] Bedrich Beneˇs, Ondrej ˇStava, Radomir Mˇech, and Gavin Miller. Guided procedural modeling. In Computer graphics forum, pages 325334. Wiley Online Library, 2011. 3 [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 4 [5] Cagan and WJ Mitchell. Optimally directed shape generation by shape annealing. Environment and Planning B: Planning and Design, 20(1):512, 1993. 3 [6] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1612316133, 2022. 3 [7] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 8 [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [9] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF international conference on computer vision, pages 2224622256, 2023. 3 [10] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44564465, 2023. 3, 6 [11] Christopher Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: unified approach for single and multi-view 3d object reconstruction. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 628644. Springer, 2016. 8 [12] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 2 [13] Ziqiang Dang, Wenqi Dong, Zesong Yang, Bangbang Yang, Liang Li, Yuewen Ma, and Zhaopeng Cui. Texpro: Textguided pbr texturing with procedural material modeling. arXiv preprint arXiv:2410.15891, 2024. 2 [14] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 3 [15] Tao Du, Jeevana Priya Inala, Yewen Pu, Andrew Spielberg, Adriana Schulz, Daniela Rus, Armando Solar-Lezama, and Wojciech Matusik. Inversecsg: Automatic conversion of 3d models to csg trees. ACM Transactions on Graphics (TOG), 37(6):116, 2018. 3 [16] Alain Fournier, Don Fussell, and Loren Carpenter. Computer rendering of stochastic models. Communications of the ACM, 25(6):371384, 1982. [17] Eric Galin, Eric Guerin, Adrien Peytavie, Guillaume Cordonnier, Marie-Paule Cani, Bedrich Benes, and James Gain. review of digital terrain modeling. In Computer Graphics Forum, pages 553577. Wiley Online Library, 2019. 2 [18] Daoyi Gao, David Rozenberszki, Stefan Leutenegger, and Angela Dai. Diffcad: Weakly-supervised probabilistic cad model retrieval and alignment from an rgb image. ACM Transactions on Graphics (TOG), 43(4):115, 2024. 3 [19] Albert Garifullin, Nikolay Maiorov, and Vladimir Frolov. Single-view 3d reconstruction via inverse procedural modeling. arXiv preprint arXiv:2310.13373, 2023. 3 [20] Peter Green. Reversible jump markov chain monte carlo computation and bayesian model determination. Biometrika, 82(4):711732, 1995. 2 [21] Jianwei Guo, Haiyong Jiang, Bedrich Benes, Oliver Deussen, Xiaopeng Zhang, Dani Lischinski, and Hui Huang. Inverse procedural modeling of branching structures by inferring l-systems. ACM Transactions on Graphics (TOG), 39(5):113, 2020. 2, 3 [22] Yu Guo, Miloˇs Haˇsan, Lingqi Yan, and Shuang Zhao. bayesian inference framework for procedural material paIn Computer Graphics Forum, pages rameter estimation. 255266. Wiley Online Library, 2020. 2 [23] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023. 3 [24] Keith Hastings. Monte carlo sampling methods using markov chains and their applications. 1970. [25] Karl Haubenwallner, Hans-Peter Seidel, and Markus Steinberger. Shapegenetics: Using genetic algorithms for procedural modeling. In Computer Graphics Forum, pages 213 223. Wiley Online Library, 2017. 3 [26] Yiwei Hu, Chengan He, Valentin Deschaintre, Julie Dorsey, and Holly Rushmeier. An inverse procedural modeling pipeline for svbrdf maps. ACM Transactions on Graphics (TOG), 41(2):117, 2022. 2 [27] Yiwei Hu, Paul Guerrero, Milos Hasan, Holly Rushmeier, and Valentin Deschaintre. Generating procedural materials from text or image prompts. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. 2 9 [28] Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David Ross, Cordelia Schmid, and Alireza Fathi. Scenecraft: An llm agent for synthesizing 3d scenes as blender code. In Forty-first International Conference on Machine Learning, 2024. 2 [29] Haibin Huang, Evangelos Kalogerakis, Ersin Yumer, and Radomir Mech. Shape synthesis from sketches via procedural models and convolutional networks. IEEE transactions on visualization and computer graphics, 23(8):20032013, 2016. 2, [30] Ian Huang, Guandao Yang, and Leonidas Guibas. Blenderalchemy: Editing 3d graphics with vision-language models. arXiv preprint arXiv:2404.17672, 2024. 2 [31] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. 6, 7 Shap-e: GeneratarXiv preprint [32] Milin Kodnongbua, Benjamin Jones, Maaz Bin Safeer Ahmad, Vladimir Kim, and Adriana Schulz. Reparamcad: Zero-shot cad program re-parameterization for interactive manipulation. 2023. 2 [33] Vojtˇech Krs, Radomır Mˇech, Mathieu Gaillard, Nathan Carr, and Bedrich Benes. Pico: procedural iterative constrained IEEE Transactions on optimizer for geometric modeling. Visualization and Computer Graphics, 27(10):39683981, 2020. [34] Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Abrevaya, and Michael Black. Re-thinking inverse graphics with large language models. arXiv preprint arXiv:2404.15228, 2024. 2 [35] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 5, 6, 7 [36] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 [37] Aristid Lindenmayer. Mathematical models for cellular interactions in development i. filaments with one-sided inputs. Journal of theoretical biology, 18(3):280299, 1968. 2 [38] Markus Lipp, Peter Wonka, and Michael Wimmer. Interactive visual editing of grammars for procedural architecture. In ACM SIGGRAPH 2008 papers, pages 110. 2008. 2 [39] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 3 [40] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. In The Twelfth International Conference on Learning Representations, 2024. 3 [41] Zhen Liu, Yao Feng, Michael Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdif10 fusion: Score-based generative 3d mesh modeling. arXiv preprint arXiv:2303.08133, 2023. 3 [42] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. 3 [43] Zhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua Lin, and Bo Dai. Controllable mesh generation through In Proceedings of sparse latent point diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 271280, 2023. 3 [44] Radomır Mˇech and Przemyslaw Prusinkiewicz. Visual models of plants interacting with their environment. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 397410, 1996. 2 [45] Nicholas Metropolis, Arianna Rosenbluth, Marshall Rosenbluth, Augusta Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):10871092, 1953. 2, [46] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1266312673, 2023. 3 [47] Pascal Muller, Peter Wonka, Simon Haegler, Andreas Ulmer, In and Luc Van Gool. Procedural modeling of buildings. ACM SIGGRAPH 2006 Papers, pages 614623. 2006. 2 [48] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 3 [49] Gen Nishida, Ignacio Garcia-Dorado, Daniel Aliaga, Bedrich Benes, and Adrien Bousseau. Interactive sketching of urban procedural models. ACM Transactions on Graphics (TOG), 35(4):111, 2016. 2, 3 [50] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5, 7 [51] Yoav IH Parish and Pascal Muller. Procedural modeling of cities. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 301308, 2001. 2 [52] Ofek Pearl, Itai Lang, Yuhua Hu, Raymond Yeh, and Rana Hanocka. Geocode: Interpretable shape programs. arXiv preprint arXiv:2212.11715, 2022. 2, [53] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 4 [54] Ben Poole, Ajay Jain, Jonathan Barron, and Ben MildenIn The hall. Dreamfusion: Text-to-3d using 2d diffusion. Eleventh International Conference on Learning Representations, 2023. 3 [55] Przemyslaw Prusinkiewicz. Graphical applications of lsystems. In Proceedings of graphics interface, pages 247 253, 1986. 2 [56] Przemyslaw Prusinkiewicz and Aristid Lindenmayer. The algorithmic beauty of plants. Springer Science & Business Media, 2012. 2 [57] Przemyslaw Prusinkiewicz, Mark James, and Radomır In Proceedings of the 21st anMˇech. Synthetic topiary. nual conference on Computer graphics and interactive techniques, pages 351358, 1994. 2 [58] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99149925, 2024. [59] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Infinite photorealistic Beining Han, Yihan Wang, et al. In Proceedings of worlds using procedural generation. the IEEE/CVF conference on computer vision and pattern recognition, pages 1263012641, 2023. 2, 5, 8 [60] Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. Infinigen indoors: Photorealistic indoor scenes using procedural generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21783 21794, 2024. 2, 5 [61] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42094219, 2024. 3 [62] Nora Ripperda and Claus Brenner. Reconstruction of facade In Joint structures using formal grammar and rjmcmc. Pattern Recognition Symposium, pages 750759. Springer, 2006. 3 [63] Nora Ripperda and Claus Brenner. Evaluation of structure In Joint Pattern recognition using labelled facade images. Recognition Symposium, pages 532541. Springer, 2009. 3 [64] Daniel Ritchie, Ben Mildenhall, Noah Goodman, and Pat Hanrahan. Controlling procedural modeling programs with stochastically-ordered sequential monte carlo. ACM Transactions on Graphics (TOG), 34(4):111, 2015. 2, 3 [65] Daniel Ritchie, Anna Thomas, Pat Hanrahan, and Noah Goodman. Neurally-guided procedural models: Amortized inference for procedural graphics programs using neural networks. Advances in neural information processing systems, 29, 2016. 2, [66] Michael Schwarz and Pascal Muller. Advanced procedural modeling of architecture. ACM Transactions on Graphics (TOG), 34(4):112, 2015. 2 [67] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Hyeonsu Kim, Jaehoon Ko, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim. Let 2d diffusion model know In The 3d-consistency for robust text-to-3d generation. Twelfth International Conference on Learning Representations, 2024. 3 [68] Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, and Subhransu Maji. Csgnet: Neural shape parser for constructive solid geometry. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 55155523, 2018. 3 [69] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:60876101, 2021. 3 [70] Liang Shi, Beichen Li, Miloˇs Haˇsan, Kalyan Sunkavalli, Tamy Boubekeur, Radomir Mech, and Wojciech Matusik. Match: Differentiable material graphs for procedural material capture. ACM Transactions on Graphics (TOG), 39(6): 115, 2020. [71] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 3 [72] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2024. 3 [73] Habib Slim and Mohamed Elhoseiny. Shapewalk: Compositional shape editing through language-guided chains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2257422583, 2024. 2 [74] Alvy Ray Smith. Plants, fractals, and formal languages. ACM SIGGRAPH Computer Graphics, 18(3):110, 1984. 2 [75] Ondrej Stava, Soren Pirk, Julian Kratt, Baoquan Chen, Radomır Mˇech, Oliver Deussen, and Bedrich Benes. Inverse procedural modelling of trees. In Computer Graphics Forum, pages 118131. Wiley Online Library, 2014. 2, 3 [76] Sinisa Stekovic, Stefan Ainetter, Mattia DUrso, Friedrich Fraundorfer, and Vincent Lepetit. Pytorchgeonodes: Enabling differentiable shape programs for 3d shape reconstruction. arxiv, 2024. 3 [77] George Stiny and James Gips. Shape grammars and the In IFIP generative specification of painting and sculpture. congress (2), pages 125135. Citeseer, 1971. [78] Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, 3d-gpt: Procedural 3d arXiv preprint Zishan Qin, and Stephen Gould. modeling with large language models. arXiv:2310.12945, 2023. 2 [79] Jerry Talton, Yu Lou, Steve Lesser, Jared Duke, Radomır Mech, and Vladlen Koltun. Metropolis procedural modeling. ACM Trans. Graph., 30(2):111, 2011. 2, 3 [80] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. Lion: Latent point diffusion models for 3d shape generation. Advances in Neural Information Processing Systems, 35:1002110039, 2022. 3 [81] Carlos Vanegas, Ignacio Garcia-Dorado, Daniel Aliaga, Bedrich Benes, and Paul Waddell. Inverse design of urban 11 [94] Mengqi Zhou, Yuxi Wang, Jun Hou, Chuanchen Luo, Zhaoxiang Zhang, and Junran Peng. Scenex: Procedural controllable large-scale scene generation via large-language models. arXiv preprint arXiv:2403.15698, 2024. [95] Xiaochen Zhou, Bosheng Li, Bedrich Benes, Songlin Fei, and Soren Pirk. Deeptree: Modeling trees with situated latents. arXiv preprint arXiv:2305.05153, 2023. 2, 3 [96] Silvia Zuffi and Michael Black. Awol: Analysis without synthesis using language. arXiv preprint arXiv:2404.03042, 2024. 2 procedural models. ACM Transactions on Graphics (TOG), 31(6):111, 2012. 2, 3 [82] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2025. 3 [83] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. 3 [84] Peter Wonka, Michael Wimmer, Francois Sillion, and William Ribarsky. Instant architecture. ACM Transactions on Graphics (TOG), 22(3):669677, 2003. [85] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. 3, 5 [86] Bojun Xiong, Si-Tong Wei, Xin-Yang Zheng, Yan-Pei Cao, Zhouhui Lian, and Peng-Shuai Wang. Octfusion: OctreearXiv based diffusion models for 3d shape generation. preprint arXiv:2408.14732, 2024. 3 [87] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 6, 7 [88] Yutaro Yamada, Khyathi Chandu, Yuchen Lin, Jack Hessel, Ilker Yildirim, and Yejin Choi. L3go: Language agents with chain-of-3d-thoughts for generating unconventional objects. arXiv preprint arXiv:2402.09052, 2024. 2 [89] Lap Fai Yu, Sai Kit Yeung, Chi Keung Tang, Demetri Terzopoulos, Tony Chan, and Stanley Osher. Make it home: automatic optimization of furniture arrangement. ACM Transactions on Graphics (TOG)-Proceedings of ACM SIGGRAPH 2011, v. 30,(4), July 2011, article no. 86, 30(4), 2011. 3 [90] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. [91] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3, 4, 5 [92] Shougao Zhang, Mengqi Zhou, Yuxi Wang, Chuanchen Luo, Rongyu Wang, Yiwei Li, Xucheng Yin, Zhaoxiang Zhang, and Junran Peng. Cityx: Controllable procedural conarXiv preprint tent generation for unbounded 3d cities. arXiv:2407.17572, 2024. 2 [93] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. 3, 6, 7 12 DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 8. Some failure cases. any existing procedural generator, facilitating their usage in 3D content creation. As proceudral generators are getting increasing attention and become mature to develop thanks to the modern design softwares, the available number and cover range of existing procedural generators are rapidly growing, which can further benefit DI-PCG. DI-PCG can be applied for any procedural generator, to greatly enhance its controllability. Moreover, in the future, utilizing AI techniques, such as Large Language Model (LLM), to generate procedural generation programs could be possible and exciting. AI-generated procedural generators and DI-PCG can naturally work together, to form new paradigm of 3D content generation. 6. More Implementation Details We use six procedural generators from Infinigen and Infinigen Indoors, namely chair, table, vase, basket, flower and dandelion generators. They contain 48, 19, 12, 14, 9, 15 controllable parameters, respectively. These are also the input token lengths of each diffusion models, as the procedural parameters directly serve as the denoising variables. Our code will be released once the paper is public. 7. More Qualitative Results Here we show more qualitative results of DI-PCG. The generation results for the chair, table, and vase categories are shown in Figure 9. DI-PCG can handle complex shape variations and details, generating high-quality 3D models from input single images. The results for the basket, flower, and dandelion are shown in Figure 10. These categories intrinsically have bit fewer variations due to the somewhat limited generality of these three procedural generators from Infinigen. Despite that, our method can capture the geometric details and recover the appropriate parameters for the input images, generating fine 3D geometries. DI-PCG can effectively handle sketch input as conditions. We show qualitative examples in Figure 11. In our experiments, we observe that DI-PCG works just as well on sketch inputs as on RGB image inputs. This provides DI-PCG more flexibility and less burden to cooperate with artists. We also provide some visual examples of our quantitative evaluations on DI-PCGs test split and ShapeNet chair subset. As shown in Figure 12 and 13, compared to existing SOTA reconstruction and generation methods, DI-PCG delivers much better 3D models with neat geometry. 8. Discussions and Failure Cases As discussed in the main paper, DI-PCG is limited by the generality and granularity of the given procedural generators. Although the adopted generator from Infinigen can cover wide range of common variations of the corresponding category, it still has obvious boundaries. Figure 8 shows some failure cases. The input chair images are outof-domain samples for Infinigen chair generators, thus DIPCG can not generate precisely aligned 3D models. Instead, it outputs the closest parameter sets to approximate the images. Although bounded by the procedural generator, DI-PCG focus on the efficient inverse ability of PCG, and represents general tool to easily and effectively control 1 Figure 9. More qualitative results for chair, table, and vase generations. Input images are collected from the internet. DI-PCG can handle diverse input images with various styles, views and textures. It accurately captures geometric details in the input images and generates high fidelity 3D models, facilitating downstream applications. 2 Figure 10. More qualitative results for basket, flower and dandelion generations. Figure 11. More qualitative results for sketch inputs. DI-PCG can effectively process sketch inputs, offering convenient way to design and edit objects. 3 Figure 12. Example comparisons on DI-PCGs test split of chairs. Only DI-PCG generates aligned and clean 3D models. Figure 13. Example comparisons on ShapeNet chair subset. DI-PCG generalizes well and produce high quality 3D meshes."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Tsinghua University",
        "VAST"
    ]
}