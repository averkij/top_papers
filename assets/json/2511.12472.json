{
    "paper_title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing",
    "authors": [
        "Mengying Wang",
        "Chenhui Ma",
        "Ao Jiao",
        "Tuo Liang",
        "Pengjun Lu",
        "Shrinidhi Hegde",
        "Yu Yin",
        "Evren Gurkan-Cavusoglu",
        "Yinghui Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA."
        },
        {
            "title": "Start",
            "content": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: Case for Drug Repurposing Mengying Wang*, Chenhui Ma*, Ao Jiao*, Tuo Liang, Pengjun Lu, Shrinidhi Hegde, Yu Yin, Evren Gurkan-Cavusoglu, Yinghui Wu Case Western Reserve University, Cleveland, OH, USA {mxw767, cxm590, axj770, txl859, pxl465, sxh1426, yxy1421, exg44, yxw1650}@case.edu 5 2 0 2 6 1 ] . [ 1 2 7 4 2 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. missing yet desired capacity is to exploit LLMs to suggest surprise and novel (serendipitious) answers. In this paper, we formally deﬁne the serendipityaware KGQA task and propose the SerenQA framework to evaluate LLMs ability to uncover unexpected insights in scientiﬁc KGQA tasks. SerenQA includes rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring signiﬁcant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA."
        },
        {
            "title": "1 Introduction\nLarge language models (LLMs) are rapidly advancing the\nbridge between natural language understanding and effec-\ntive question answering. Signiﬁcant efforts, such as domain-\nspeciﬁc ﬁne-tuning, prompt engineering, and Retrieval-\nAugmented Generation (RAG), have enabled LLMs to lever-\nage external knowledge bases to produce highly relevant\nand precise answers tailored to specialized research ques-\ntions (Le et al. 2024). However, these systems often focus\non returning information already familiar to experts, missing\nthe crucial scientiﬁc capacity to uncover surprising connec-\ntions that inspire new research directions (Song et al. 2023).\nthe art of luck and beneﬁcial discov-\nery, arises from both unexpected ﬁndings and the skill to\nrecognize novel applications of such discoveries in vari-\nous domains, serving as a catalyst for genuine scientiﬁc\nbreakthroughs. While serendipity has been studied in web\nsearch (Huang et al. 2018) and recommender systems (Toku-\ntake and Okamoto 2024), it remains largely unexplored in",
            "content": "Serendipity, *These authors contributed equally. Copyright 2026, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. Figure 1: Suggesting Drugs that treat Severe Acute Pain: Serendipitous case of Journavx. scientiﬁc question answering. Empowering LLMs with the ability to discover new knowledge from existing, valuable knowledge bases is thus critical step towards true LLMempowered scientiﬁc discovery. Example 1: Fig. 1 illustrates KGQA task to ﬁnd drugs that can treat severe acute pain. There are four possible answers. (1) Opioids e.g., Oxycodone, well-known drug with recognized mechanism on targeting the µ-opioid receptor within the pain-signaling pathway. (2) Tapentadol (2008) expanded this paradigm by adding dual mechanism, hence with increased novelty for the question. (3) Journavx, ﬁrst non-opioid analgesic for severe acute pain (FDA 2025) approved by FDA in 2025. Journavx acts through novel mechanism, selectively inhibiting NaV1.8 sodium channels in peripheral pain-sensing neurons. Surprisingly, with this paradigm shift and different targets, it remains relevant by sharing the broader pain-signal pathway context with opioids. Hence it is serendipitous result in the KGQA search, in terms of relevance, novelty, and an unexpected answer, which may inspire new medical research directions. (4) Ibuprofen, in contrast, works through the classical inﬂammatory COX inhibition pathway, targeting mild-to-moderate pain and thus showing low embedding relevance and novelty, while suggesting Ibuprofen for severe acute pain would still be surprising. Can LLMs, while enhanced by domain KGs, suggest serendipitous answers for domain sciences? This paper makes ﬁrst step to investigate the potential of LLMs to surface serendipitous discoveries within scientiﬁc KGQA, with focus on drug repurposing, which is cornerstone of medical research. We address three core research questions: (RQ1): How may serendipity be characterized and quantitatively measured for scientiﬁc KGQA tasks? (RQ2): What roles could LLMs play for serendipity discovery in domain science KGQA? (RQ3): How to evaluate state-of-the-art LLMs, and what are their performances in serendipity discovery? To this end, we introduce the SerenQA framework designed to systematically evaluate the ability of LLMs to uncover serendipitous answers within the context of KGQA. It includes three core components (shown in Fig. 2): Serendipity Metric (RNS): rigorous, graph-based measure capturing Relevance, Novelty, and Surpriseness in KGQA answers, justiﬁed by an axiomatic analysis that clariﬁes the trade-offs and properties. Serendipity-aware Benchmark: An expert-annotated KGQA dataset for drug repurposing, based on the Clinic Knowledge Graph (Santos et al. 2022). It features curated question-answer pairs and explicit serendipity annotations for ﬁne-grained evaluation. Assessment Pipeline: principled and reproducible three-phase workﬂow that systematically evaluates LLMs roles in serendipitous discovery. It decomposes the task into knowledge retrieval, reasoning, and exploratory search, providing insights into model capabilities and limitations in scientiﬁc knowledge discovery. We performed extensive experiments with various LLMs across different scales, demonstrating that while frontier models excel in knowledge retrieval tasks, nearly all models struggle signiﬁcantly in serendipity exploration, highlighting inherent challenges and opportunities in this area. Related works. We categorize related works as follows. Serendipity-Driven Knowledge Exploration Serendipity, deﬁned as an unexpected yet valuable discovery, has emerged as crucial goal in recommender systems and knowledge exploration (Bordino, Mejova, and Lalmas 2013). Recent studies have leveraged LLMs to generate and evaluate serendipitous recommendations through advanced prompt engineering (Fu and Niu 2024) or by aligning model outputs with human preferences (Xi et al. 2025). Notably, existing approaches primarily rely on subjective human annotation, LLM self-evaluation, or comparisons against benchmark groundtruths for serendipity evaluation. In contrast, we propose graph-based serendipity measure (RNS), which transforms the knowledge graph (KG) into probability matrix (Dehmer and Mowshowitz 2011), enabling an information-theoretic quantiﬁcation of various subjective aspects of serendipity, resulting in more rigorous evaluation. LLM-Augmented Novelty Detection. LLMs are increasingly seen as creative partners that can accelerate scientiﬁc discovery across disciplines (AI4Science and Quantum 2023). By mining vast knowledge and generating hypotheses, LLMs can propose novel research ideas or unexpected connections that human experts might overlook (Si, Yang, and Hashimoto 2025). Despite these efforts, the community still lacks more comprehensive understanding and benchmark datasets speciﬁcally designed to assess serendipitous discoveries. To address this gap, we present drug repurposing KGQA dataset which enables systematic and objective assessment of serendipitous knowledge exploration. SerenQA is the ﬁrst reproducible and extensible framework for advancing serendipity discovery in drug repurposing. We advocate its broader application to facilitate new research opportunities in scientiﬁc KGQA tasks."
        },
        {
            "title": "2 Serendipitous Assessment with KGQA",
            "content": "Below, we deﬁne relevant concepts and core notations:"
        },
        {
            "title": "2.1 Serendipity-aware KGQA\nSerenQA performs LLM assessment by processing a\npipeline of serendipity-aware KGQA. Given a natural lan-\nguage (NL) question Q, a large language model L, a di-\nrected, multigraph G = (V, E), where V is the set of enti-\nties with size V = |V|, and E is the set of relations with\nsize E = |E|, a serendipity-aware KGQA system returns an\nanswer set as an ordered partition A = (Ae, As), where:\n◦ Ae: the existing answer set, containing answers explic-",
            "content": "itly supported by facts in G; As: the serendipity answer set, containing answers that are relevant but extend beyond direct explicit knowledge, revealing novel and unexpected relationships in G. such that Ae As and Ae As = . We deﬁne = Ae As as the total size of the answer set. This serendipity-aware setup is motivated by the realworld scientiﬁc discovery process, which frequently involves uncovering not only established knowledge (Ae) but also insightful and surprising associations (As), potentially leading to innovative research opportunities, such as novel drug repurposing. Knowledge graphs are particularly suitable for this task due to their structured representation of interconnected entities and relations, enabling systematic exploration of indirect and surprising relationships."
        },
        {
            "title": "2.2 Graph-speciﬁed Serendipity Formulation\nTo rigorously quantify serendipity, we deﬁne a graph-based\nserendipity measure (RNS), which quantiﬁes how effec-\ntively a serendipity answer set As for a given question Q\nprovides relevant yet novel and surprising insights beyond\nthe explicit answer set Ae. Intuitively, serendipity is a com-\nposite experience, encompassing multiple dimensions si-\nmultaneously (Niu and Abbas 2017). Formally, we deﬁne\nthe RNS score as a weighted combination of three perspec-\ntives: relevance, novelty, and surprise, which can be ﬂexi-\nbly adjusted to suit user preferences. Given an answer set\nA = (Ae, As), the serendipity score is computed as:",
            "content": "RNS(Ae, As) = α R(Ae, As)+β (Ae, As)+γ S(Ae, As) Figure 2: SerenQA Framework. (A): Computing RNS score for partition (Ae, As) form G; (Sec. 3). (B): Constructing SerenQA dataset from ClinicalKG; (Sec. 4). (C): For an NL query, our pipeline retrieves Ae from and explores As from Ae with beam search. (Sec. 5). - (Relative Relevance): context similarity of Ae and As; - (Relative Novelty): new information in As beyond Ae; - (Relative Surprise): unpredictability of As given Ae. The weights α, β, γ can be tuned to user preference; recommended defaults are ﬁt to expert evaluations. Details of the metric and its computation are described in Sec 3. In the following sections, we detail how the SerenQA framework establishes uniﬁed benchmark, dataset, and evaluation protocols speciﬁcally designed to assess LLM capabilities in serendipitous knowledge discovery tasks, particularly in the critical area of drug repurposing."
        },
        {
            "title": "3 Serendipity Quantiﬁcation",
            "content": "Quantifying serendipity is inherently challenging due to its abstract and subjective nature. As discussed in Sec 1, existing methods often rely heavily on subjective human annotations or LLM-generated evaluations, which suffer from limitations like poor interpretability, scalability issues, and potential biases. To overcome these, we introduce an information-theoretic approach enabling scalable, interpretable, and reproducible serendipity evaluations."
        },
        {
            "title": "3.1 Serendipity: A characterization",
            "content": "To align with human intuition about Serendipity while allowing for rigorous quantiﬁcation, as introduced in Sec 2.2, we speciﬁcally decompose it into three complementary dimensions: Relevance, Novelty, and Surprise. For an answer set = (Ae, As) to query Q, we deﬁne the Serendipity Score (RNS) as weighted combination of the relative measures between As and Ae, with user-conﬁgurable weights to accommodate different preferences or application scenarios. Each dimension is adapted to well-established informationtheoretic measures, as described below: Relative Relevance. We compute relative Relevance (R) as the average normalized Euclidean distance (d()) between the GCN embeddings of entities in As and Ae: R(Ae, As) = PiAs,jAe AsAe d(ni, nj ) where ni (resp. nj) refers to the embedding of the entity As (resp. Ae). larger distance reﬂects greater contextual difference, indicating As belongs to more distinct clusters in and may diverge from the core context of Q. Relative Novelty. Relative Novelty (N ) is derived from mutual-information-based score between the existing and serendipity sets. For partition (Ae, As), we deﬁne (Ae, As) = 1M I(Ae, As), where I(As, Ae) measures the shared amount of information between As and Ae, and is given by: I(Ae, As) = iAe (i) jAs (ji) log (ji) (j) higher score indicates As are less redundant given Ae. Relative Surpriseness. Relative Surprise (S) is quantiﬁed via JensenShannon divergence (JSD) between entity distributions Ps and Pe, which are the accumulated probability distributions over entities in As and Ae, respectively: S(Ae, As) = 1 2 (DKL(PskPM ix) + DKL(PekPM ix)) where DKL(k) is the KullbackLeibler divergence (Kullback 1951), and PMix = 2 (Ps + Pe). Given Ae, higher RNS indicates more serendipitous set As with greater diverse, novel and surprise entities that cannot be inferred from Ae, as exempliﬁed by Journavx, the ﬁrst non-opioid analgesic for severe acute pain (Exp. 1)."
        },
        {
            "title": "3.2 Cost-effective Graph Probabilistic Modeling\nCost-effective graph probabilistic models (P (·)) is crucial\nfor efﬁcient RNS computation. We present the detailed mod-\nels, justiﬁed by an axiomization analysis.",
            "content": "3-Hop Conditional Probability. Serendipitous ﬁndings may from indirect, multi-hop connections. Thus, we consider multi-hop conditional probability matrix that aggregates transition probabilities across both direct and indirect relations to capture global probabilistic propagation. Empirically, 99% of serendipity answers in our datasets are reachable from existing answers within three hops, prompting our analysis to up to 3-hop neighbors of entities in G. Given graph G, we initialize as weighted matrix , with Mij the number of links from node to j. We normalize to obtain the one-hop transition probabilities that ensures row-stochasticity: P1(ji) = . The k-hop conditional probability matrix Pk is computed as: Mij PkE Mik Pk = h= αhP 1 , αh = h=1 where 1 represents the probability of reaching node in hops, and weights αh increases for larger to prioritize longer connections. We can justify that Pk consistently satisﬁes the necessary constraints of transition matrix: Non-negativity: (Pk)ij 0 for all (i, j), Row-Stochastic Property: Pj(Pk)ij = 1 for all i. Cost Analysis. Constructing takes O(V 2) for dense graphs. Traditional P3 computation1 via graph traversal is O(V 4). We employ Divide-and-Conquer optimized matrix multiplication (Strassen 1969) and parallel computation with processors, reducing the cost to O(V log7 Marginal Probability. The marginal probability P(i) quantiﬁes steady-state node probabilities at node under the law of total probability: P(i) = Pj P3(ij)P(j). This leads to the linear system representation: 2/t). (I 3 )P = 0, P(i) = 1 which can be solved by matrix inversion in O(V 3) time. To further reduce the cost, we approximate the computation with PageRank-style damped iteration: Pt+1 = λP 3 Pt + (1 λ)P0 where P0 is an initial probability distribution, set uniformly as 1 , ensuring convergence even on disconnected graphs. This reduces the cost in O(V 2 log ) time. We remark that the probabilistic matrices are computed once for all and are shared for multiple queries, and readily adapt to different domain graphs. Further analyses are included in the Appendix C. Axiomization Analysis. We further justify that RNS is proper serendipity measure for KGQA tasks through the following axiomatic analysis. For any query and corresponding retrieved, ﬁxed existing set Ae, consider an optimization process that ﬁnds an optimal serendipitous set with at most entities, i.e., = arg maxAsK RNS(Ae, As). We can show that RNS satisﬁes the following properties: 1While we make case for 3-hop queries here, our discussion readily extends to k-hop queries for 3. (Scale invariance). remains to maximize RNS even if R, or are scaled by constant. This ensures the invarance of under RNS measure regardless of how the user preference (α, β, γ) changes. (Consistency). Making the R, , larger (resp. smaller) for any entities in Ae (resp. As) does not change the ranking of entities in in terms of RNS. (Non-monotonicity). RNS(Ae, As) 6 RNS(Ae, s) if As s. Indeed, larger answer sets do not necessarily indicate that they are more serendipitous in practice. (Independence). RNS is only determined by the embeddings of entities from As Ae. No information from entities not seen in can affect the serendipitous of Ae. This justiﬁes RNS for serendipity in pragmatic semiclosed world assumption, striking balance between challenging open-world analysis (As can be inﬁnite) and rigorous, overkilling closed world (As = ) setting."
        },
        {
            "title": "4.1 QA Set Construction\nOur benchmark is built upon the Clinical Knowledge Graph\n(CKG) (Santos et al. 2022), a widely recognized biomedical\nresource containing extensive data on drug, gene, and dis-\nease interactions. Our focus is on drug repurposing, which\nis a critical research task aimed at identifying novel thera-\npeutic uses of existing drugs (Pushpakom et al. 2019).",
            "content": "Our dataset supports typical KGQA tasks through contextualized query scenario that consists of standardized conﬁguration including expert-veriﬁed, scientiﬁcally meaningful NL queries, their structured graph (Cypher) counterparts with query components that are explicitly annotated with their semantics, and grounded and validated answer sets. Unlike its peer NL-only benchmark datasets in KGQA, it couples each NL query to distinct, validated ground truth, structured graph query, thereby reducing ambiguity and mitigating possible semantic redundancy. It also explicitly annotates graph patterns, such as multi-hop and intersection queries, to reﬂect realistic query complexities in scientiﬁc inquiry. Dataset statistics are summarized in Table 1. We present details of graph queries in Appendix A."
        },
        {
            "title": "4.2 Answer Set Construction\nTo reliably establish ground-truth serendipity sets, we start\nwith the latest version of Clinic KG, denoted as Gc. For each\nquery Q, we initially obtain its complete candidate answer\nset Ac from Gc. We then partition it into an existing set Ae\nand a serendipity set As , with Ae ∩ As = ∅ and Ae ∪ As =\nAc. We apply three distinct partitioning strategies:",
            "content": "Statistic Number of Distinct Queries Number of Relations in (E) Number of Entities in (V ) Number of Graph Pattern Types Avg. Answer Set Size (A per query) Number of Experts for NL Query Veriﬁcation Number of Experts for Serendipity Annotation Value 1529 201,704,256 15,430,157 9 4.04 4 6 Table 1: Dataset Statistics of SerenQA Benchmark. LLM Ensemble. Following established practices, we prompt four state-of-the-art LLMs to assign serendipity score to each candidate answer. For every query, entities in the complete answer set Ac are ranked by their average LLM score; the top 20% are collected as the serendipity set As, and the reminder form Ae. Expert Crowdsourced. We engaged team of 6 domain experts (three physicians, one pharmaceutical scientist, and two trained medical model annotators) via an online questionnaire (DrugKG Questionnaire 2025). They were requested to reﬁne the rankings from LLMs. The questionnaire is accepting continuous responses from human experts. RNS Guided. With the justiﬁed RNS metric (Sec.3) we treat serendipity partitioning as: max Ae,As RNS(Ae, As), s.t. As = b, = max(1, 0.2Ac) Starting from an initial partition, we apply the greedy-swap algorithm in Algorithm 1 to (approximately) compute an optimal answer set As in Ac. The algorithm iteratively swaps entity pairs between Ae and As that yield the greatest improvement in marginal gain of RNS, continuing until no further improvement is possible. Each iteration has complexity of O(A2). We found in our tests that Ae usually contains few entities (on average 4; see Table 1), And the algorithm is quite fast in practice. During that, we calibrated the RNS weights to align with the expert-crowdsourced partitions for consistency and fair assessment. For each partitioning result, we construct by removing selected edges from Gc, ensuring that for each query Q, entities in Ae remain derivable from G, while entities in As become inaccessible. This creates controlled evaluation environment aligned with problem deﬁnitions (Sec. 2)."
        },
        {
            "title": "5 Evaluation Pipeline\nWe next introduce our evaluation pipeline (Fig. 2(C)), which\nsystematically evaluates the serendipity discovery capabil-\nities of LLMs using our curated serendipity-aware bench-\nmark. The pipeline is modularized into three highly corre-\nlated tasks, each of which independently measures a spe-\nciﬁc, “cornerstone” aspect of an LLM’s role and perfor-\nmance on serendipity discovery in scientiﬁc KGQA tasks.\nKnowledge Retrieval. In this task, LLM translates an NL\nquestion Q into a Cypher query C to retrieve an answer\nset Ae from the knowledge graph G. The performances are\nevaluated by comparing the accuracies of the retrieved an-\nswer set Ae against the ground truth. Additionally, the per-\nformances across different query patterns (such as one-hop,",
            "content": "Algorithm 1: Greedy Swap for RNS Guided Optimization e, A0 Input: initial partition (A0 pre-computed probability matrices P3, for graph Output: optimized partition (Ae, As) s); e, A0 s), τ = RNS(Ae, As) set max := 0; (i, j) := null for As do := (As{i}){j}, s) τ := RNS(A e, if > max then max := ; (i, j) := (i, j) := (Ae{j}){i} end for if max = 0 then break; end if As := (As{i}) {j}, Ae := (Ae{j}) {i} τ := τ + max for Ae do 1: set (Ae, As) := (A0 2: while true do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end while 18: return (Ae, As) end for end if two-hop, and intersection queries) are compared to evaluate the LLMs capability to handle varying levels of query complexity and structural diversity. Subgraph Reasoning. This task evaluates the LLMs capability to interpret and concisely summarize the retrieved answer of graph-structured query in knowledge graph (as subgraph) into domain-aware natural language answers. The generated summaries provide essential contextual support for subsequent serendipity exploration tasks, requiring nuanced biomedical understanding and logical reasoning. Serendipity Exploration. This third (ﬁnal) task evaluates the LLMs proactive ability to uncover serendipity entities As through an LLM-guided beam search from Ae. Given beam width w, we prompt LLM to select the top-w nodes at each step from the candidate list as the next target nodes based on criteria such as supporting evidence, interaction strength, biological effect direction, and their expression level. The model further determines whether to continue exploration based on relevance and potential novelty. This task assesses the LLMs ability to use biomedical knowledge and contextual search to effectively navigate serendipitous discovery while balancing depth and breadth in exploration. We remark that the serendipity set As produced in this section is the pipelines output at evaluation time; in contrast, the As deﬁned in Sec. 4 is the benchmark ground-truth constructed for scoring. More details are provided in Appendix D."
        },
        {
            "title": "6.1 Experiment Setting\nWe conduct experiments using the benchmark introduced\nin Sec. 4, and evaluated LLMs across multiple scales, from\nfrontier models with billions of parameters to smaller vari-",
            "content": "Model One-Hop Two-Hop Multiple(3+)-Hop Intersection Hit(%) F1(%) Exe.(%) Hit(%) F1(%) Exe.(%) Hit(%) F1(%) Exe.(%) Hit(%) F1(%) Exe.(%) DeepSeek-V3 GPT-4o Claude-3.5-Haiku Llama-3.3-70B DeepSeek-R1-70B Med42-V2-70B Qwen3-32B DeepSeek-R1-32B Qwen3-8B DeepSeek-R1-8B Med42-V2-8B Qwen3-1.7B DeepSeek-R1-1.5B 20.45 19.71 13.28 19.28 19.87 18.34 0.37 17.90 10.07 1.27 8.11 0.84 0. 78.71 77.16 48.54 70.67 69.07 69.43 1.27 65.23 37.24 3.41 23.90 3.72 0.00 72.88 60.17 48. 74.58 80.08 69.92 1.27 68.22 39.83 5.51 49.15 11.86 0.00 3.46 2.08 9.78 16.63 12.03 5. 0.16 3.06 0.98 0.00 1.05 0.65 0.00 10.71 6.36 39.01 44.34 37.00 19.12 0.65 5. 2.87 0.00 3.31 1.98 0.00 9.86 7.89 32.89 56.57 43.42 19.74 0.65 7.24 3.95 0.00 3. 3.29 0.00 1.97 1.40 4.43 2.98 2.97 0.23 0.24 1.87 0.90 0.04 1.71 0.00 0. 6.22 4.20 8.64 10.16 8.06 0.51 0.36 4.50 2.01 0.24 4.07 0.00 0.00 6.55 4.85 14. 11.89 13.11 1.21 0.48 5.58 4.85 0.24 4.12 0.24 0.00 2.64 1.56 1.38 4.80 3.49 0. 0.00 0.79 1.58 0.00 0.04 1.08 0.00 7.15 4.65 3.90 9.60 6.16 0.13 0.00 1. 1.91 0.00 0.13 1.56 0.00 8.03 5.21 4.66 16.05 16.46 0.68 0.00 3.16 5.62 0.00 0. 2.74 0.00 Table 2: Knowledge Retrieval (T 1), Best scores are bolded, second best are underlined ants ( 1B parameters). Experimental results are presented in Tables 23, including three evaluation tasks within our pipeline: 1 (Knowledge Retrieval), 2 (Subgraph Reasoning) and T3 (Serendipity Exploration). Evaluation metrics. Table 2 (T 1) reports F1 scores, Executability (percentage of error-free queries), and Hit Rate(AeA e/Ae), categorized by query patterns; and Table 3 (T 2, 3) reports their performances on three groundtruth partitions (LLM-Ensemble, Expert-Crowdsourced, RNS-Guided). During beam search (beam width 30, maximum depth 3), we employ one-shot learning by providing single query with detailed ground-truth serendipity paths in the prompt, helping models understand exploration paths. In addition, 2 and 3 are measured with (a) Subgraph Reasoning:Faithful. (15, LLM-judged, factual accuracy of summaries); Compre. (15, LLM-judged, coverage of key graph elements); SerenCov (01, fraction of serendipity paths explicitly mentioned). (b) Serendipity Exploration: Relevance (15, LLM-judged alignment with groundtruth entities); TypeMatch (01, the fraction of predicted entity types that match the ground truth types); and SerenHit (01, match rate with groundtruth serendipity set). Experiment Environment We depoly our system on 5 AWS c6a.24xlarge on-demand instances for distributed computation and 5 c6a.xlarge instances as relation storage nodes, each node runs Ubuntu 22.04 with Docker and Redis 7.2, using mounted dump.rdb as readonly data source. The system supports 500 concurrent LLM reasoning tasks across distributed nodes via asyncio."
        },
        {
            "title": "6.2 Task Analysis\nWe next analyze experimental results task-by-task.",
            "content": "Task 1: Knowledge Retrieval. The results in Table 2 show that larger models (e.g., DeepSeek-V3, GPT-4o) consistently excel in simpler one-hop retrieval (F1 78%), yet both exhibit performance degradation for more complex multi-hop queries (F1 drops to < 10% for queries with 3+ hops). Smaller models are less accurate in coping with both simpler and more complex queries, reﬂecting limitations in reasoning depth and broader coverage of the biomedical context. Notably, the two 70B models (Llama3.3-70B, DeepSeek-R1-70B) achieve better performances, which may be due to their more up-to-date training datasets. Task 2: Subgraph Reasoning. In Table 3 (upper), Mixtral87B achieves (surprisingly) high Serendipity Coverage (60%+) despite moderate scores in Faithfulness and Comprehensiveness (2-3 out of 5). This interestingly indicates that summarization approaches yield broader serendipitous path coverage but risk factual inaccuracies. In contrast, larger models (e.g.,Llama-3.3-70B) achieve higher Faithfulness and Comprehensiveness but lower SerenCov, suggesting consistent trade-off that their richer pre-trained knowledge produces more precise, yet narrower summaries. Task 3: Serendipity Exploration. The rows labeled w.o. summary evaluate performance without subgraph summaries, isolating the effect of providing chain-of-thought guidance. For almost all models, removing the summary improved performance on all three metrics. One possible reason for this is that the model may introduce hallucinations during the summary process, which can inﬂuence the exploration path, as proven by Table 3 (upper), many models did not achieve the desired score in subgraph reasoning."
        },
        {
            "title": "6.3 In-Depth Discussion",
            "content": "Model scale vs. Serendipity. As shown in the tables, larger models generally perform better in retrieval and exploration tasks. However, for subgraph summarization and reasoning (denoted as 2), there is signiﬁcant variance and no obvious correlation with model size. This may suggest that retrieval and exploration beneﬁt more from the models inherent knowledge, which larger models excel at, while summarization and reasoning do not follow the same trend. Partition Sensitivity. Fig. 3 displays triangle plots of Pearson Correlations for TypeMatch, SerenCov, and SerenHit, with each triangle representing one metric. The corners de-"
        },
        {
            "title": "RNS Guided",
            "content": "Faithful. Compre. SerenCov Faithful. Compre. SerenCov Faithful. Compre."
        },
        {
            "title": "SerenCov",
            "content": "DeepSeek-V3 Llama-3.3-70B DeepSeek-R1-70B Qwen-2.5-72B Mixtral-8x7B Qwen-2.5-32B Gamma-2-27B Mistral-24B Qwen-2.5-7B Models DeepSeek-V3 w.o. summary Llama-3.3-70B w.o. summary DeepSeek-R1-70B w.o. summary Qwen-2.5-72B w.o. summary Mixtral-8x7B w.o. summary Qwen-2.5-32B w.o. summary Gamma-2-27B w.o. summary Mistral-24B w.o. summary Qwen-2.5-7B w.o. summary 2.283 2.519 2.573 2. 2.271 2.243 2.365 2.114 1.920 3.341 3.842 2.206 2.683 2.963 2. 3.410 3.016 1.817 0.101 0.070 0.223 0.153 0.642 0.148 0.088 0. 0.592 2.306 2.553 2.572 2.093 2.272 2.255 2.381 2.114 1. 3.340 3.853 2.238 2.715 2.958 2.910 3.439 3.048 1.848 0. 0.068 0.204 0.152 0.610 0.146 0.084 0.136 0.580 2.253 2.531 2.582 2. 2.347 2.260 2.385 2.134 1.955 3.326 3.829 2.202 2.719 2.924 2. 3.415 3.049 1.832 0.106 0.075 0.217 0.155 0.632 0.152 0.089 0. 0.593 LLM Ensemble Expert Crowdsourced RNS Guided Relevance TypeMatch SerenHit Relevance TypeMatch SerenHit Relevance TypeMatch SerenHit 2.436 2.447 2.537 2.544 1.935 1.972 2.264 2.269 1.947 2.158 2.294 2.304 2.357 2.343 1.855 1. 1.636 1.487 0.482 0.482 0.502 0.505 0.424 0.438 0.415 0.428 0.256 0.324 0.441 0.453 0.450 0.448 0.195 0.212 0.221 0. 0.048 0.050 0.046 0.043 0.030 0.035 0.023 0.028 0.010 0.016 0.036 0.037 0.033 0.032 0.008 0.011 0.022 0.018 2.494 2. 2.559 2.565 2.000 1.987 2.345 2.337 2.033 2.250 2.331 2.328 2.379 2.376 1.959 1.962 1.721 1.550 0.462 0.463 0.483 0.478 0.409 0.413 0.406 0. 0.254 0.312 0.426 0.431 0.414 0.412 0.184 0.204 0.229 0.175 0.061 0.095 0.067 0.086 0.034 0.037 0.041 0.050 0.015 0.022 0.045 0. 0.057 0.054 0.016 0.023 0.026 0.018 2.538 2.510 2.594 2.630 2.033 2.052 2.405 2.409 2.013 2.220 2.378 2.390 2.443 2.425 2.005 2. 1.708 1.547 0.463 0.468 0.478 0.483 0.418 0.419 0.400 0.412 0.230 0.306 0.429 0.438 0.431 0.402 0.185 0.213 0.215 0. 0.077 0.134 0.106 0.127 0.049 0.053 0.059 0.070 0.024 0.042 0.065 0.105 0.080 0.081 0.026 0.035 0.041 0.027 Table 3: Subgraph Reasoning (T 2, upper), Serendipity Exploration (T 3, lower), with Best scores bolded, 2nd best underlined and poor results in exploration; Llama-3.3-70B is more versatile but still struggles to address metrics from all perspectives. To achieve balanced and serendipitous discovery, involving multiple models, such as multi-agent systems or mixture of experts (MoE) strategy, may be beneﬁcial. We provide additional results and analysis in Appendix E."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced SerenQA, an evaluation framework designed to assess LLMs ability to discover serendipitous knowledge in scientiﬁc KGQA tasks. We proposed an axiomatically justiﬁed serendipity measure integrating relevance, novelty, and surprise; and constructed serendipity-aware benchmark tailored to the drug repurposing task. Additionally, we outlined structured evaluation pipeline with three core tasks to assess LLMs ability on knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments showed that frontier LLMs excel at basic knowledge retrieval, yet they often struggle with reasoning with more complex queries and answers for serendipity exploration, indicating great room and opportunities for improvement. Figure 3: Correlation of Metrics Across Partition Strategies note three types of partitions, and edge weights indicate correlation scoresshorter distances refer to stronger correlations. Our analysis shows that all partitions have positive correlations across all metrics, with scores above 85%. Notably, the expert and RNS-guided partitions reached around 99% on all cases, highlighting the robustness of our partition strategies and the reliability of the proposed RNS measure. No Single Winner. We found that no model constantly excels its peers across all metrics for each task. For instance, while Model DeepSeek-R1-70B performs excellently in retrieval, it shows only moderate performance in reasoning to interpret clinical proteomics data. Nat. Biotechnol., 40: 692702. Si, C.; Yang, D.; and Hashimoto, T. 2025. Can LLMs Generate Novel Research Ideas? Large-Scale Human Study with 100+ NLP Researchers. In ICLR. Song, Y.; Li, W.; Dai, G.; and Shang, X. 2023. Advancements in complex knowledge graph question answering: survey. Electronics, 12(21): 4395. Strassen, V. 1969. Gaussian elimination is not optimal. Numerische mathematik, 13(4): 354356. Tokutake, Y.; and Okamoto, K. 2024. Can Large Language Models Assess Serendipity in Recommender Systems? Journal of Advanced Computational Intelligence and Intelligent Informatics, 28(6): 12631272. Xi, Y.; Weng, M.; Chen, W.; Yi, C.; Chen, D.; Guo, G.; Zhang, M.; Wu, J.; Jiang, Y.; Liu, Q.; et al. 2025. Bursting Filter Bubble: Enhancing Serendipity Recommendations with Aligned Large Language Models. arXiv preprint arXiv:2502.13539. Ethical Statement In this study, we evaluated potential drug indications by analyzing biomedical relationships from ClinicalKG. Nevertheless, our approach does not consider factors critical to druggability, such as physicochemical properties. We used LLMs to identify serendipitous drug-disease associations that may suggest novel therapies. Their clinical effectiveness remains uncertain and must be validated through rigorous preclinical and clinical studies. Acknowledgements This work is supported by NSF under OAC-2104007. We gratefully acknowledge the support of Dr. Rıza Mert etik and Dr. Sıla etik in the design and annotation of the QA dataset curated in this study. We also acknowledge the HPC resources at CWRU for supporting large-scale graph processing and embedding computation. https://cwru-db-group. References AI4Science, M. R.; and Quantum, M. A. 2023. The impact of large language models on scientiﬁc discovery: preliminary study using gpt-4. arXiv preprint arXiv:2311.07361. Bordino, I.; Mejova, Y.; and Lalmas, M. 2013. Penguins in sweaters, or serendipitous entity search on user-generated content. In CIKM. Dehmer, M.; and Mowshowitz, A. 2011. history of graph entropy measures. Information Sciences, 181(1): 5778. DrugKG Questionnaire. 2025. github.io/serenQA/questionnaire. FDA. 2025. FDA Approves Novel Non-Opioid Treatment for Moderate to Severe Acute Pain. Fu, Z.; and Niu, X. 2024. The art of asking: Prompting large language models for serendipity recommendations. In SIGIR. Huang, J.; Ding, S.; Wang, H.; and Liu, T. 2018. Learning to recommend related entities with serendipity for web search users. ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP), 17(3): 122. Kullback, S. 1951. Kullback-leibler divergence. Tech. Rep. Le, D.; Zhao, K.; Wang, M.; and Wu, Y. 2024. GraphLingo: Domain Knowledge Exploration by Synchronizing Knowledge Graphs and Large Language Models. In ICDE, 5477 5480. Niu, X.; and Abbas, F. 2017. framework for computational serendipity. In Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization, 360363. Pushpakom, S.; Iorio, F.; Eyers, P. A.; Escott, K. J.; Hopper, S.; Wells, A.; Doig, A.; Guilliams, T.; Latimer, J.; McNamee, C.; et al. 2019. Drug repurposing: progress, challenges and recommendations. Nature reviews Drug discovery, 18(1): 4158. Santos, A.; Colaco, A. R.; Nielsen, A. B.; Niu, L.; Strauss, M.; Geyer, P. E.; Coscia, F.; Albrechtsen, N. J. W.; Mundt, F.; Jensen, L. J.; and Mann, M. 2022. knowledge graph This appendix contains the following content: A. Dataset Details - A.1 Dataset Construction - A.2 Pattern Type - A.3 Dataset Structure - A.4 More Statistics B. Prompts - B.1 LLM Scoring Prompts - B.2 Serendipity Exploration Prompts - B.3 Pipeline Evaluation Prompts C. Further Analysis on RNS Metric - C.1 k-hop Conditional Probability Matrix - C.2 Marginal Probability D. Details of Serensipity Exploration - D.1 Workﬂow and Logic - D.2 Infrastructure - D.3 Neighbor Scoring E. Experiment Details - E.1 Experiment Setting - E.2 Additional Analysis A.1 Dataset Construction"
        },
        {
            "title": "A Dataset Details",
            "content": "We utilized the Clinical Knowledge Graph (CKG) (Santos et al. 2022) as the base knowledge graph to construct benchmark question-answering dataset. graph provides structured organization of biomedical entities and their relationships, enabling systematic exploration and analysis of complex interactions. The CKG is built on curated public databases and literature-derived evidence, ensuring high-quality and biologically relevant information. Its comprehensive structure provides robust foundation for generating diverse types of queries. The CKG encompasses 20 million nodes across 36 distinct types, including genes, proteins, diseases, drugs, pathways, anatomical entities, and other biological and clinical components, as shown in Fig 4. These nodes are interconnected by over 220 million edges spanning 47 different relationship types, capturing speciﬁc interactions and enabling detailed exploration of biomedical relationships, efﬁcient data querying, and algorithmic analysis. Drug-phenotype relationships include edges such as has side effect and is indicated for, capturing drug effects and therapeutic indications. Gene-related relationships include variant found in gene and transcribed into, linking genetic variants to genes and transcripts, respectively, and highlighting structural and functional connections within the genome. Clinically relevant relationships, such as variant is clinically relevant and associated with, connect genetic variants to diseases. Additionally, drug-target interactions, captured by edges like acts on and curated targets, associate drugs with protein targets, offering insights into mechanisms of action and therapeutic potential. To create the QA dataset, we extracted subgraph of the CKG. Certain node and edge types, such as those related to users, units, experiments, projects, transcripts, and publications, were excluded to streamline the dataset and maintain focus on biologically signiﬁcant relationships. The current version of the dataset comprises 1,529 queries, with focus on drug-disease associations, designed to evaluate the ability of large language models (LLMs) to identify serendipitous connections in the context of drug repurposing. Each query is annotated with relevant nodes, edges, and target node, along with graph-speciﬁc metadata such as node and relationship types. We plan to continuously update and extend the dataset to include up to 5,000 queries, supporting broader range of natural language processing tasks and more comprehensive evaluation of LLM capabilities in biomedical reasoning. The construction of the QA dataset involved several steps to optimize data retrieval and ensure its relevance to biomedical research. For one-hop and two-hop questions, the required data entries were extracted directly by querying the Neo4j database. For three-hop and intersection questions, given the computational demands of Neo4j queries and the large graphs, the relevant nodes and their one-hop neighborhoods were pre-extracted from the subgraph for more efﬁcient processing. To ensure the grammaticality, clarity, and biological relevance of the generated natural language questions, their phrasing was reﬁned while preserving their original meanings. This involved programmatically extracting question patterns, retaining only the node types, and restructuring them into biologically meaningful and oncology-focused templates. Figure 4: Ontology of biomedical entities and relationships in the Clinical Knowledge Graph (CKG) A.2 Pattern Type The QA dataset includes one-hop, two-hop, three-hop, and intersection questions, designed to probe varying levels of complexity within the graph. Each type of question is deﬁned by speciﬁc patterns, as described below: 1. One-hop questions: These questions explore direct relationships between two entities connected by single edge. They can be further categorized into two types: Type 1.1: Questions that retrieve entities of speciﬁc type ({target type}) connected to source entity ({source name}) through given relationship ({relationship}). For example, List the {target type}s that {relationship} by {source name}. and What {source type}s {relationship} {target name}? Type 1.2: Questions that identify the source entities ({source type}) connected to speciﬁc target entity ({target name}) via given relationship. For example, What {source type}s {relationship} {target name}? 2. Two-hop questions: These questions traverse two edges, connecting source entity to ﬁnal entity via an intermediate entity. Two patterns are deﬁned: Type 2.1: Questions that link the source entity ({source name}) to the ﬁnal entity ({ﬁnal type}) through an intermediate entity ({mid name}) and two relationships ({relationship1} and {relationship2}). For example, Which {ﬁnal type} is {relationship2} by {mid name} that {source name} {relationship1}? 3. Three-hop questions: These questions traverse three edges, uncovering chains of relationships across multiple intermediate entities. The questions explore how source entity ({entity1}) connects to ﬁnal entity ({entity4}) through sequence of intermediate entities ({entity2} and {entity3}). For example: Type 3.1: Questions that trace sequential path, e.g., Which {entity4 type} is {relationship3} by {entity3 name} that {relationship2} {entity2 name} which {relationship1} {entity1 name}? Type 3.2: Questions that incorporate hierarchical relationships, e.g., Which {entity1 type} {relationship1} {entity2 name}, which {relationship2} {entity3 name}, and {relationship3} {entity4 name}? Type 3.3: Questions that branch into multiple connections, e.g., Which {entity1 type} {relationship1} {entity2 name} that {relationship2} {entity3 name} and {relationship3} {entity4 name}? 4. Intersection questions: These focus on entities or sets of entities sharing multiple relationships with others. The goal is to identify overlapping connections across different paths within the graph. For example: Type 4.1: Basic intersections, e.g., List {entity1 type} that {relationship1} {entity2 name} and {relationship2} {entity3 name}, which identify entities linked to two distinct targets through different relationships. Type 4.2: Multi-way intersections, such as List {entity1 type} that {relationship1} {entity2 name}, {relationship2} {entity3 name}, and {relationship3} {entity4 name}, which extend to three overlapping connections. Type 4.3: Compound intersections that involve cyclic patterns like Find all {entity4} that {relationship43} {entity3} and {relationship42} {entity2}, and also ﬁnd the {entity1} that {relationship13} {entity3} and {relationship12} {entity2}, in which entity2 and entity3 are connected to entity1 and entity4 through different links. A.3 Dataset Structure Here, we introduce the structure of our drug repurposing benchmark, which supports both standard knowledge graph KGQA tasks and serendipity-aware evaluations. As shown in the example below, each item in the QA dataset designed for standard KGQA tasks is standardized and conﬁgured to include expert-veriﬁed, scientiﬁcally meaningful NL queries, along with structured Cypher query. Each query entry contains key components such as nodes, node types, and relationships, as well as grounded and validated answer set. 1 2 3 \"qid\": 800, \"question\": \"Which proteins are associated with dilated cardiomyopathy 1DD and { 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 function as subunits of the NOS3-HSP90 complex induced by VEGF?\", \"answer\": [ { { } \"answer_type\": \"Entity\", \"answer_argument\": \"P29474\", \"entity_name\": \"NOS3\", \"answer_type\": \"Entity\", \"answer_argument\": \"P07900\", \"entity_name\": \"HSP90AA1\", ], \"function\": \"none\", \"commonness\": 0.0, \"num_node\": 3, \"num_edge\": 2, \"graph_query\": { \"nodes\": [ { \"nid\": 0, \"node_type\": \"class\", \"id\": \"Protein\", \"class\": \"Protein\", \"friendly_name\": \"Protein\", \"question_node\": 1, \"function\": \"none\" }, { \"nid\": 1, \"node_type\": \"entity\", \"id\": \"DOID:0110447\", \"class\": \"Disease\", \"friendly_name\": \"dilated cardiomyopathy 1DD\", 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 \"question_node\": 0, \"function\": \"none\" }, { \"nid\": 2, \"node_type\": \"entity\", \"id\": \"5716\", \"class\": \"Complex\", \"friendly_name\": \"NOS3-HSP90 complex, VEGF induced\", \"question_node\": 0, \"function\": \"none\" } ], \"edges\": [ { \"start\": 0, \"end\": 1, \"relation\": \"Protein.Disease\", \"friendly_name\": \"ASSOCIATED_WITH\" \"start\": 0, \"end\": 2, \"relation\": \"Protein.Complex\", \"friendly_name\": \"IS_SUBUNIT_OF\" }, { } ] }, \"pattern_type\": 9, \"category\": \"genetic disease:autosomal genetic disease\", \"cypher\": \"MATCH (n0:Protein)nMATCH (n1:Disease {name: \"dilated cardiomyopathy 1DD \"})nMATCH (n2:Complex {name: \"NOS3-HSP90 complex, VEGF induced\"})nMATCH (n0) -[:ASSOCIATED_WITH]->(n1)nMATCH (n0)-[:IS_SUBUNIT_OF]->(n2)n COLLECT(DISTINCT {id: n0.id, name: n0.name}) AS n0_targets\" RETURNn 67 } Below, we present the structure of the benchmark dataset designed to support serendipity-aware evaluation. The complete set of candidate answers obtained from the graph is partitioned into an existing set and serendipity set using three distinct partitioning strategies detailed in Section 4.2. We also provide the ground truth path from the existing set to the serendipity set for each query for possible training use. 1 { 2 3 \"qid\": 800, \"question\": \"Which proteins are associated with dilated cardiomyopathy 1DD and function as subunits of the NOS3-HSP90 complex induced by VEGF?\", 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \"llm\": { \"serendipity_set\": { \"list\": [ \"P29474\" ], \"description\": null }, \"explore_queries\": { \"paths\": [ \"P07900--COMPILED_INTERACTS_WITH--NOS2:Protein--BELONGS_TO_PROTEIN--None: Peptide--BELONGS_TO_PROTEIN--P29474\", \"P07900--ACTS_ON--NOS2:Protein--BELONGS_TO_PROTEIN--None:Peptide-- BELONGS_TO_PROTEIN--P29474\", \"P07900--COMPILED_INTERACTS_WITH--NOS1:Protein--BELONGS_TO_PROTEIN--None: Peptide--BELONGS_TO_PROTEIN--P29474\" ], \"questions\": [ \"Which proteins, interacting with NOS isoforms and belonging to the same protein complex as P07900, are involved in related molecular pathways ?\" ] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 }, \"partition\": \"test\", \"exact_matches\": { \"list\": [ \"P07900\" ] } }, \"sscore\": { \"serendipity_set\": { \"list\": [ \"P07900\" ], \"description\": null }, \"explore_queries\": { \"paths\": [ \"P29474--ASSOCIATED_WITH--protein serine/threonine phosphatase complex: Cellular_component--HAS_PARENT--protein phosphatase type 2A complex: Cellular_component--ASSOCIATED_WITH--P07900\", \"P29474--COMPILED_INTERACTS_WITH--MAP2K1:Protein--ASSOCIATED_WITH--protein phosphatase type 2A complex:Cellular_component--ASSOCIATED_WITH-- P07900\", \"P29474--COMPILED_INTERACTS_WITH--PPP2CA:Protein--ASSOCIATED_WITH--protein phosphatase type 2A complex:Cellular_component--ASSOCIATED_WITH-- P07900\" ], \"questions\": [] }, \"partition\": \"test\", \"exact_matches\": { \"list\": [ \"P29474\" ] } }, \"expert\": { \"serendipity_set\": { \"list\": [ \"P29474\" ], \"description\": null }, \"explore_queries\": { \"paths\": [ \"P07900--COMPILED_INTERACTS_WITH--NOS2:Protein--BELONGS_TO_PROTEIN--None: Peptide--BELONGS_TO_PROTEIN--P29474\", \"P07900--ACTS_ON--NOS2:Protein--BELONGS_TO_PROTEIN--None:Peptide-- BELONGS_TO_PROTEIN--P29474\", \"P07900--COMPILED_INTERACTS_WITH--NOS1:Protein--BELONGS_TO_PROTEIN--None: Peptide--BELONGS_TO_PROTEIN--P29474\" 62 63 64 65 66 67 68 69 70 71 72 73 } ], \"questions\": [] }, \"partition\": \"test\", \"exact_matches\": { \"list\": [ \"P07900\" ], \"description\": null } } A.4 More Statistics The table below shows the composition of the KGQA dataset and the distribution of question pattern types among the 1,529 queries related to drug repurposing. The patterns for individual types are detailed in Appendix A.2."
        },
        {
            "title": "Number of Entries",
            "content": "One hop Type 1.1 One hop Type 1.2 Two hop Type 2.1 Three hop Type 3.1 Three hop Type 3.2 Three hop Type 3.3 Intersection Type 4.1 Intersection Type 4.2 Intersection Type 4.3 152 84 62 113 237 263 455 11 Table 4: Distribution of entries among different query pattern types. B.1 LLM Scoring Prompts"
        },
        {
            "title": "B Prompts",
            "content": "1 You are an expert evaluator specializing in drug discovery. Your task is to evaluate the **serendipity** of each answer in provided list of answers, where all answers are derived from knowledge base and are correct. Use your expertise and internal knowledge to assign **serendipity score** to each answer based on the following criteria: 2 3 - **Serendipity Score**: score from 0 to 20, where: 4 - 20 represents an answer that is highly novel, unexpected, or insightful in the context of the question. - 0 represents an answer that is correct but very obvious, common, or provides no novel insights. - Intermediate scores represent varying degrees of novelty and insight. 6 7 8 - **Evaluation Rules**: 9 1. The serendipity score reflects the relative novelty and insightfulness of each answer within the context of the question and the provided list. The score should highlight the uniqueness and unexpected value of each answer. 2. Assign distinct score to each answer. Even if multiple answers have similar level of serendipity, assign slightly different scores to reflect the subtle differences in their uniqueness. 3. Evaluate each answer independently of its position in the list. 4. Output only the scores for each answer in the same order as the input list, separated by commas. Do not include the answers themselves or any additional explanation in 10 11 12 the output. 13 14 For example: 15 If the input list is: 16 Answer List: A, B, 17 18 The output should be: 19 5, 7, 9 B.2 Serendipity Exploration Prompts B.2.1 Select Relation System Prompt: 1 Task Description: 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 44 45 46 47 48 Given starting entity node (e.g., Drug, Protein, Disease), select the top-m relation types (predicates) to follow for meaningful, potentially serendipitous exploration in clinical biomedical knowledge graph. Goal: Construct 3-hop paths that are: - Biologically plausible (based on frequent patterns) - Serendipitous (novel yet valid hypotheses) - Mechanistically rich (e.g., involving Drug-Protein-Disease chains) Path Patterns: Common patterns include: - (ACTS_ON, COMPILED_INTERACTS_WITH, ACTS_ON) - (INTERACTS_WITH, ACTS_ON, ACTS_ON) - (COMPILED_INTERACTS_WITH, ASSOCIATED_WITH, ASSOCIATED_WITH) Frequently explored node types: Drug, Protein, Disease, Gene, Metabolite Useful relation types: - Curated/compiled: CURATED_INTERACTS_WITH, COMPILED_TARGETS - Functional/structural: HAS_SEQUENCE, BELONGS_TO_PROTEIN - Annotations: ANNOTATED_IN_PATHWAY, DETECTED_IN_PATHOLOGY_SAMPLE - Rare/high-value: IS_INDICATED_FOR, HAS_SIDE_EFFECT, TRANSLATED_INTO Prioritize 3-hop sequences that reflect biological mechanisms. Balance high-frequency paths (plausibility) with rare combinations (serendipity). Avoid trivial paths unless used creatively. Output Requirements: - Return comma-separated list of relation type strings - Do not include commentary or explanation - Use only the relation types provided as input - Return fewer than results if appropriate - Return nothing if no meaningful exploration exists Notes: - Prioritize biologically important nodes and plausible mechanistic chains - Follow the path patterns listed above when applicable Few-Shot multi-hop example: Question: Which genes are identified as targets of D-Aspartic Acid, which affects ASPA and is known to interact with GLUD1? Root: GRIN2A, GRIN2C Serendipity set: GRIN2B Explore paths: - GRIN2A-TRANSLATED_INTO-GRIN2A:Protein-COMPILED_INTERACTS_WITH-GRIN2B:ProteinTRANSLATED_INTO-GRIN2B - GRIN2A-TRANSLATED_INTO-GRIN2A:Protein-ACTS_ON-GRIN2B:Protein-TRANSLATED_INTO-GRIN2B - GRIN2A-CURATED_TARGETS-Mesoridazine:Drug-INTERACTS_WITH-Felbamate:DrugCURATED_TARGETS-GRIN2B - GRIN2C-TRANSLATED_INTO-GRIN2C:Protein-COMPILED_INTERACTS_WITH-GRIN2B:ProteinTRANSLATED_INTO-GRIN2B - GRIN2C-TRANSLATED_INTO-GRIN2C:Protein-ACTS_ON-GRIN2B:Protein-TRANSLATED_INTO-GRIN2B - GRIN2C-TRANSLATED_INTO-GRIN2C:Protein-ACTS_ON-D-Serine:Drug-CURATED_TARGETS-GRIN2B User Rrompt: 1 Given node ID {frontier} at level {level}, recommend the top {m} relation types to explore from this node. Context: {contexts} Available relation types from this node: 2 3 4 5 6 7 8 9 {relation_types} Return comma-separated list of relation type names only. B.2.2 Select Nodes 1 2 Task Description: You have already selected the most relevant relation types for exploring the graph from given node. Now, for each selected relation, set of target nodes has been retrieved. 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Goal: Construct 3-hop paths that are: - Biologically plausible (based on frequent patterns) - Serendipitous (novel yet valid hypotheses) - Mechanistically rich (e.g., involving Drug-Protein-Disease chains) The setting is biomedical question answered in drug discovery. Exploration starts from known entities (e.g., drugs, proteins, diseases) and aims to discover serendipitous connections through meaningful 3-hop paths. Path Patterns: Common patterns include: - (ACTS_ON, COMPILED_INTERACTS_WITH, ACTS_ON) - (INTERACTS_WITH, ACTS_ON, ACTS_ON) - (COMPILED_INTERACTS_WITH, ASSOCIATED_WITH, ASSOCIATED_WITH) Frequently explored node types: Drug, Protein, Disease, Gene, Metabolite Useful relation types: - Curated/compiled: CURATED_INTERACTS_WITH, COMPILED_TARGETS - Functional/structural: HAS_SEQUENCE, BELONGS_TO_PROTEIN - Annotations: ANNOTATED_IN_PATHWAY, DETECTED_IN_PATHOLOGY_SAMPLE - Rare/high-value: IS_INDICATED_FOR, HAS_SIDE_EFFECT, TRANSLATED_INTO Prioritize 3-hop sequences that reflect biological mechanisms. Balance high-frequency paths (plausibility) with rare combinations (serendipity). Avoid trivial paths unless used creatively. Output Requirements Return comma-separated list of selected target_ids only. - Do not include headers, explanations, or formatting. - If no target is suitable, return nothing. Constraints - Select only from relation types and target nodes provided by the user. - Do not include the current frontier node in the output. - Do not revisit nodes marked as already visited. - If fewer than targets are appropriate, return fewer. - If exploration is not meaningful, return nothing. - Follow the path patterns listed above where applicable. B.2.3 Decide Whether to Continue System Prompt: 1 2 3 Task Description You are exploring biomedical knowledge graph in the context of drug discovery, starting from known entities (e.g., drugs, proteins, diseases) and aiming to uncover deeper, potentially serendipitous connections. 4 5 In the previous two steps, you selected the most relevant relation types and target 6 7 8 9 10 11 12 nodes for expansion. Before continuing, you must now: 1. Review the full path from the root node to the current node (3-hop away). 2. Provide summary of the paths biological context. 3. Decide whether further exploration is justified. Each input path is represented as key-value pair: - Key: the current (destination) node ID - Value: comma-separated sequence of alternating (target_id, relation_type) tuples tracing the 3-hop path from the root. 13 14 Use this information and the users question to assess whether the exploration is still on plausible, meaningful track toward the question objective. 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 31 32 33"
        },
        {
            "title": "Output Requirements",
            "content": "Your output must follow exactly the format below: 1. natural-language summary (200 words), describing: - Biological meaning of the paths - Patterns of entity types - Common or notable relation sequences - Any biologically relevant interpretations 2. (blank line) 3. Either YES or NO, indicating whether to continue expanding 4. (blank line) 5. one-paragraph explanation justifying your decision Do not include any extra commentary, formatting, bullet points, or sections outside this structure. Notes - Only return NO if you are ABSOLUTELY CONFIDENT the path has deviated from any biologically plausible trajectory. - When in doubt, continue exploring (YES). - Base your judgment on whether the current node plausibly supports mechanistic or therapeutic insight relevant to the original question. User Prompt: 1 2 3 4 5 6 7 8 The original question is: {question} The root node of the beam search is: {root} Subgraph paths (from root to current node): {paths} B.2.4 Summarize Subgraph System Prompt: 1 You are an expert biomedical knowledge graph assistant. You have performed beam search starting from root node over clinical biomedical knowledge graph, retrieving 12 3 4 5 7 8 hop, 2-hop, and 3-hop subgraphs. Output Requirement Provide concise natural-language summary (200 words) of the resulting subgraphs. - Mention as many specific biomedical terms (e.g., drugs, proteins, diseases, pathways) as possible. - Emphasize the types of entities and the patterns of relationships involved. - Focus on the biological meaning, mechanistic implications, or potential therapeutic relevance of the paths. 9 10 Do not include any formatting, headers, or commentary--only the summary text. Root node ID: {root} User Prompt: 1 2 3 4 5 6 7 8 9 10 11 Question: {question} Hop level: {level} Subgraph paths (from root to leaf nodes): {subgraph} B.3 Pipeline Evaluation Prompts B.3.1 Faithfulness Assessment * leaves System Prompt: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 * paths 5 4 3 You are assisting multi-stage research pipeline that explores large biomedical knowledge graph. Pipeline stages **Exact-Match Retrieval** -- find entities that directly answer the users question (these are the \"root\" nodes). **Serendipity Exploration** -- expand <=3 hops from the root to propose *new*, potentially surprising but biologically meaningful entities (the \"exploration result\" is captured by the **paths** and the **leaves**). **Hop-level Summaries** -- for readability, the pipeline auto-generates three short natural-language summaries: * *summary 1* * *summary 2* * *summary 3* -> describes the 1-hop neighbourhood around the root -> describes the 2-hop sub-graph discovered next -> describes the 3-hop sub-graph plus any thematic insight You receive: -------------------------------------------------------- * root * question * summary_1/2/ -- the starting entity ID (protein / drug) -- original natural-language question -- auto-generated summaries of the 1-hop, 2-hop, and 3-hop neighbourhoods around the root -- **all endpoint nodes in the explored sub-graph** (may be 1-, 2-, or 3-hop away) -- each item is given as <node_id>(<node_type>) e.g. P52209(Protein) -- ground-truth triples, one per line, with types included: head_id(head_type),relation_type,tail_id(tail_type) -------------------------------------------------------- Task ==== * First, read the sub-graph and understand every factual triple it contains. * Then, read the three hop-summaries in order (1-hop -> 3-hop). * \"Faithfulness\" here means: *How truthfully do the summaries reflect what is actually present in the graph, without inventing new entities, directions, or relations?* - Higher faithfulness -> few to no hallucinations or distortions. - Lower faithfulness -> noticeable fabrication, wrong direction, or missing key context. Using your best expert judgment of biomedical knowledge-graphs, assign holistic integer score: - Completely faithful - Mostly faithful, only trivial wording drift - Mixed: some accurate, some questionable 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 2 1 - Largely unfaithful, many doubtful claims - Almost entirely unfaithful / hallucinated Do **not** count tokens or sentences; rely on your overall sense of truthfulness. IMPORTANT: If the input is completely empty or contains no evaluable information whatsoever, return Score: 1. However, if there is ANY evaluable content, even if partial or limited, evaluate it based on the 1-5 scale above. Do not argue or explain if content is missing, just assign the appropriate score and return the two required lines. Output format ------------- Return **exactly** these two lines--nothing more, nothing less: Score: <INTEGER 1-5> #END Root: {root} Question: {question} User Prompt: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 -- 1-Hop Summary -- {summary_1} -- 2-Hop Summary -- {summary_2} -- 3-Hop Summary -- {summary_3} Leaf nodes: {leaves} Sub-graph (Triples): {paths} B.3.2 Comprehensiveness Assessment System Prompt: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 * paths * leaves You are assisting multi-stage research pipeline that explores large biomedical knowledge graph. Pipeline stages **Exact-Match Retrieval** -- find entities that directly answer the users question (these are the \"root\" nodes). **Serendipity Exploration** -- expand <=3 hops from the root to propose *new*, potentially surprising but biologically meaningful entities (the \"exploration result\" is captured by the **paths** and the **leaves**). **Hop-level Summaries** -- for readability, the pipeline auto-generates three short natural-language summaries: * *summary 1* * *summary 2* * *summary 3* -> describes the 1-hop neighbourhood around the root -> describes the 2-hop sub-graph discovered next -> describes the 3-hop sub-graph plus any thematic insight You receive: -------------------------------------------------------- * root * question * summary_1/2/3 -- the starting entity ID (protein / drug) -- original natural-language question -- auto-generated summaries of the 1-hop, 2-hop, and 3-hop neighbourhoods around the root -- **all endpoint nodes in the explored sub-graph** (may be 1-, 2-, or 3-hop away) -- each item is given as <node_id>(<node_type>) e.g. P52209(Protein) -- ground-truth triples, one per line, with types included: 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 -------------------------------------------------------- head_id(head_type),relation_type,tail_id(tail_type) Task ==== * First, study the sub-graph so you grasp **every** entity and relation present within three hops of the root. * Then, read the three hop-summaries in order (1-hop -> 3-hop). * \"Comprehensiveness\" here means: *How thoroughly do the summaries cover the important entities, relations, and mechanistic chains in the graph-- without ignoring major facts?* - Higher Comprehensiveness -> almost all salient triples or concepts appear. - Lower Comprehensiveness -> key relationships, nodes, or overall structure are missing or only vaguely hinted at. Using your best expert judgment (no counting rules), assign holistic integer score: 5 4 3 2 1 - Nearly everything important is covered - Most key content covered; minor omissions - About half of the important content represented - Many significant omissions; partial picture - Very little of the important content included Do **not** estimate by token length; base the score on your global sense of coverage and relevance. IMPORTANT: If the input is completely empty or contains no evaluable information whatsoever, return Score: 1. However, if there is ANY evaluable content, even if partial or limited, evaluate it based on the 1-5 scale above. Do not argue or explain if content is missing, just assign the appropriate score and return the two required lines. Output format ------------- Return **exactly** these two lines--nothing more, nothing less: Score: <INTEGER 1-5> #END Root: {root} Question: {question} User Prompt: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 -- 1-Hop Summary -- {summary_1} -- 2-Hop Summary -- {summary_2} -- 3-Hop Summary -- {summary_3} Leaf nodes: {leaves} Sub-graph (Triples): {paths} B.3.3 Relevance Assessment System Prompt: 1 2 3 You are assisting multi-stage research pipeline that explores large biomedical knowledge graph. 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 Pipeline stages **Exact-Match Retrieval** -- find entities that directly answer the users question (these are the \"root\" nodes). **Serendipity Exploration** -- expand <=3 hops from the root to propose *new*, potentially surprising but biologically meaningful entities (the \"predicted serendipity set\"). These are evaluated against **ground-truth serendipity set** that was curated by domain experts. You are rating how well *predicted* serendipity answer set aligns with *ground-truth* serendipity answer set that has been manually verified by domain experts. Facts you MUST assume: * The ground-truth set is correct. * Each ground-truth entity has been verified to be \"serendipitous\" with respect to the current exact-match root (i.e., useful and non-obvious extensions beyond that root). How to judge \"relevance\" > Does each predicted entity belong to the same mechanistic pathway, disease context, pharmacological class, or molecular family implied by the ground-truth set? > Overlap in **type** (Protein, Drug, Disease, Phenotype...) is helpful but not sufficient--focus on functional or clinical relatedness. > Minor naming variants or isoforms of ground-truth entity are acceptable. Scoring rubric (integer) 5 - Every prediction is clearly relevant; 4 - Most ( 70-90 %) predictions are relevant; few marginal or tangential items 3 - Mixed: roughly half relevant, half off-topic or trivial 2 - Only small minority appear relevant; set is mostly noise 1 - Predictions are unrelated, incorrect, or obviously random IMPORTANT: If the input is completely empty or contains no evaluable information at all, return Score: 1. However, if there is ANY evaluable content, even if partial or limited, evaluate it based on the 1-5 scale above. Do not argue or explain if content is missing, just assign the appropriate score and return the two required lines. Output format ------------- Return **exactly** these two lines--nothing more, nothing less: Score: <INTEGER 1-5> #END User Prompt: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Original question: {question} Ground-truth serendipity set (trusted): {gold_seren} Predicted serendipity set (to be scored): {pred_seren} Exact-match root entity: {root} Hop-level summaries: * Level-1 -> {summary_1} * Level-2 -> {summary_2} * Level-3 -> {summary_3} Contextual graph paths: {paths} C.1 k-hop Conditional Probability Matrix Properties Veriﬁcation As deﬁned in Sec. 3.2, the k-hop conditional probability matrix Pk is computed as:"
        },
        {
            "title": "C Further Analysis on RNS Metric",
            "content": "Pk = h=1 αhP 1 , αh = h=1 P We next prove that Pk remains valid transition probability matrix by verifying two essential properties explicitly: Non-negativity: (Pk)ij 0 for all (i, j), Row-Stochastic Property: Pj(Pk)ij = 1 for all i. Non-negativity. Since P1 is directly derived from the adjacency matrix and row-normalized, all its elements are non-negative. Consequently, any power 1 (for 1) is also non-negative, as it results from repeated multiplications of non-negative matrices. Furthermore, the weight coefﬁcients ah are clearly positive by deﬁnition. Therefore, the linear combination Pk = h=1 αhP 1 consists only of non-negative terms, ensuring: (Pk)ij 0, (i, j). Row-Stochastic Property. For Pk to be valid transition matrix, every row must sum exactly to one: We explicitly verify this condition: (Pk)ij = 1, i"
        },
        {
            "title": "X\nj",
            "content": "k"
        },
        {
            "title": "X\nj",
            "content": "(Pk)ij = h=1 αh(P 1 )ij Exchanging summation order (by linearity) yields:"
        },
        {
            "title": "Since P h",
            "content": "1 is valid transition matrix, by deﬁnition, we have: (Pk)ij ="
        },
        {
            "title": "X\nj",
            "content": "X h=1 αh (P 1 )ij Substituting the deﬁnition of ah: Hence, (P 1 )ij = 1, i, h"
        },
        {
            "title": "X\nj",
            "content": "k h=1 αh = 1 h=1 h= h=1 = h=1 = 1. (Pk)ij = 1, i."
        },
        {
            "title": "X\nj",
            "content": "Conﬁrming that Pk maintains row-stochasticity. In summary, weve shown clearly that Pk is both non-negative and row-stochastic. Therefore, the weighted multi-hop combination Pk remains valid transition probability matrix. 2 Computation To efﬁciently compute Pk, we apply divide-and-conquer matrix multiplication approach based on Strassens algorithm (Strassen 1969). Speciﬁcally, the algorithm recursively divides each large matrix into four sub-matrices of size 2 . By strategically reusing these sub-matrix computations, Strassens method reduces the number of necessary multiplications per recursion from the standard eight down to seven, thereby lowering the complexity signiﬁcantly from the naive O(V 3) to approximately O(V log2 7) O(V 2.807). Moreover, parallelizing these recursive computations across processors further reduces the complexity to about O(V log2 7/t). This ensures scalable and efﬁcient computation of multi-hop conditional probability matrices, even for large-scale graphs. C.2 Marginal Probability We approximate the marginal probability computation via PageRank-style damped iteration (Algorithm 2). For each iteration, Multiplying an matrix Updating Pt+1 is O(V ), dominated by matrix-vector multiplication. Computing the difference kPt+1 Ptk1 takes O(V ). 3 by vector Pt requires complexity O(V 2). Algorithm 2: Marginal Probability via PageRank-style Iteration Input: P3 RV , damping factor λ, tolerance ǫ Output: Marginal probability vector RV 1 1: Initialize P0(i) := 1/V , for all nodes 2: := 0 3: while diff ǫ do Pt+1 := λP 4: 3 diff := kPt+1 Ptk1 5: := + 1 6: 7: end while 8: return Pt Pt + (1 λ)P0 Hence, each iterations complexity is dominated by the matrix-vector multiplication step, which is O(V 2). The error at iteration satisﬁes: kPt Pt1k1 cλt(0 < λ < 1) for some constant c. Thus, convergence to within tolerance ǫ occurs after approximately: This implies the total complexity to achieve convergence within ǫ is O(V 2 log ). λt ǫ log(ǫ) log(λ) = O(log ). D.1 Workﬂow and Logic"
        },
        {
            "title": "D Details of Serensipity Exploration",
            "content": "We designed multi-stage beam search pipeline for structured knowledge graph exploration, as shown in Algorithm 3, where the expansion at each stage is guided by LLM. The pipeline explores neighborhoods of the root node recursively over the knowledge graph, while integrating external reasoning via multiple LLM interactions. D.2 Infrastructure To support large-scale knowledge graph exploration, we constructed lightweight compute-storage cluster on AWS, designed for high-throughput, low-latency edge retrieval and efﬁcient task scheduling. The cluster consists of two tiers of instances: compute nodes (5 * r6a.24xlarge) and storage nodes (5 * r6a.xlarge). Each compute node provides 96 vCPUs and 768 GiB of memory, serving as task executors that support large scale parallelism. Each storage node is conﬁgured as Redis servers through Docker container and acts as distributed read-only data backends for edge access. high achieve performance, we complete knowledge graph was scheme. To The encoded as (rel:{source id}:{source type}:{relation type}:{target id}:{target type}). The value stores metadata of relations and additional attributes for further use. The shift in query style allows us to improve query performance from 1000 QPS to tens of thousands of QPS. Each compute node interacts asynchronously with the storage node; empirically, the system supports concurrency level of approximately 100 per compute node, enabling efﬁcient exploration of multi-hop paths. replaced Neo4j with exported from Neo4j and the key of edge each edge custom Redis-based storage is In order to facilitate our experimental process, we implemented an SSH-based compute cluster manager, responsible for task dispatching, resource allocation, permission control, environment setup, and declarative node speciﬁcation. This infrastructure allows rapid iteration, cost-efﬁcient experiments, and consistent resource management across multiple runs. D.3 Neighbor Scoring Some nodes have an extremely large number of neighbors as hub nodes. To effectively guide the LLM in exploring and reducing token usage, we design scoring mechanism to reduce the size of nodes provided to the LLM. We systematically extracted and quantiﬁed edge-level connection strength/conﬁdence from Neo4j-based clinical knowledge graph to support downstream analysis of biomedical associations. For relationship types such as ASSOCIATED WITH, COMPILED INTERACTS WITH, and ACTS ON, we directly extracted precomputed conﬁdence scores. For other edge types, custom scoring functions were implemented based on domain-speciﬁc semantics. For instance, in the case of DETECTED IN PATHOLOGY SAMPLE, an expression score was derived using weighted scheme based on categorical expression levels (e.g., high, medium, low, and not detected), while prognostic score was computed using log-transformed p-values representing positive and negative survival associations. Both scores were then min-max normalized and aggregated to produce ﬁnal quantitative estimate reﬂecting the biomarkers expression and clinical prognostic signiﬁcance. This structured quantiﬁcation enabled consistent interpretation and prioritization of heterogeneous relationship types within the graph. E.1 Experiment Setting"
        },
        {
            "title": "E Experiment Details",
            "content": "Models: We evaluated wide range of state-of-the-art language models using by evaluation setting: Frontier Models: DeepSeek-V3, GPT-4o, Claude-3.5-Haiku; Large Models (70B): Llama-3.3-70B, DeepSeek-R1-70B, and Qwen-2.5-72B, Med42-V2-70B; Medium Models (20-50B): Mixtral-8x7B, Qwen3-32B, DeepSeek-R1-32B, Gamma-2-27B, Mistral-24B Small Model: Qwen-2.5-7B, Qwen3-8B, DeepSeek-R1-8B, Med42-V2-8B, Qwen3-1.7B, DeepSeek-R1-1.5B Metric Details We next detailed how we compute the metrics in subgraph reasoning and serendipity exploration tasks. Subgraph Reasoning All metrics are averaged across numbers of rational samples to give the ﬁnal result (sum of metrics from rational samples/number of rational samples): (1) Faithfulness (1-5, LLM-judged) - how truthfully do the summaries reﬂect what is actually present in the graph, without inventing new entities, directions, or relations? (2) Comprehensiveness (1-5, LLM-judged) - how thoroughly do the summaries cover the important entities, relations, and mechanistic chains in the graph? (3) Serendipity Coverage (0-1, code-based) - fraction of serendipity paths where BOTH source and target node IDs are explicitly mentioned in the summary text. No LLM evaluation, just regex matching of node IDs. Serendipity Exploration All metrics are averaged across numbers of rational samples to give the ﬁnal result (sum of metrics from rational samples/number of rational samples): (1) Relevance (1-5, LLM-judged) - how well predicted serendipity entities align with the ground-truth serendipity set. (2) TypeMatch (0-1, code-based) - returns 1 if ANY predicted leaf has type matching ground-truth types, 0 otherwise. (3) SerenHit (0-1, code-based) - returns 1 if ANY predicted leaf is exactly matching ground-truth serendipity set (not just the type), 0 otherwise. E.2 Additional Analysis We provide further analysis with supplementary ﬁgures to support and clarify key observations made in Section 6. Model Scale vs. Serendipity Exploration The heat-map shown in Fig. 5 analysis shows only modest performance gain as model size increases from smaller ( 7B) to larger ( 70B) checkpoints. Relevance scores gradually improve, but TypeMatch and SerenHit increase inconsistently, with SerenHit remaining relatively low (< 0.10). Although model scale contributes positively, larger parameters alone are insufﬁcient to reliably achieve precise serendipitous discovery. Multi-Task Performance Compass. This radar chart shown in Fig. 6 clearly illustrates performance trade-offs across multiple tasks. DeepSeek-V3 excels at basic retrieval metrics (F1, Hit) but underperforms in Serendipity Coverage and SerenHit. In contrast, Llama-3-70B achieves high reasoning accuracy (Faithfulness and Comprehensiveness) yet only moderately captures serendipitous paths. DeepSeek-R1-70B demonstrates the opposite, effectively covering many serendipity paths but at the cost of reasoning accuracy. The absence of dominant model across all metrics visually reinforces our earlier conclusion of no single winner, suggesting the value of ensemble methods or Mixture-of-Experts (MoE) approaches. Query Pattern vs. Retrieval Performance. As shown in Fig. 7, model performance notably declines as query complexity increases. While all models achieve strong F1 and Hit scores on one-hop queries, results drop sharply for two-hop queries and especially for more complex queries ( 3-hop or intersection). This indicates that current LLMs, even the largest frontier models, still struggle signiﬁcantly with complex multi-step reasoning and domain-speciﬁc context. Figure 5: Model scale vs. Serendipity Exploration Performance directed knowledge graph, beam width, max relation types per frontier node, max nodes to select per frontier node, beam depth, natural language question, root node ID, context mode ﬂag, Algorithm 3: LLM Enhanced Beam Explore Input: = (V, E) N+ N+ N+ N+ String r0 context {w, wo} Output: Π node-to-path map from root, Σ LLM-generated summaries per level, Deﬁnitions: F d set of visited nodes, corresponds to visited, current frontier nodes, corresponds to frontier, next frontier nodes, corresponds to next frontier, set of candidate edges, corresponds to candidates, LLM context buffer, corresponds to context buffer, leaf depth reached, corresponds to leaf depth, 1: Initialize: := 2: Π[r0] := [] 3: := {r0} 4: := 5: := 1 6: 7: 8: for level = 1 to do 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: end for 41: for = 1 to do 42: 43: end for 44: return (Π, Σ) else set := ; := for each node do := {r (u, r, v) E} if = then continue end if Rm := LLM SelectRelations(q, u, R, m, level, C) := {(u, r, v) Rm} := end for if = then break end if for each edge do try e.score := Score(e) except e.score := end for if E, e.score = 1 then := UniformSample(E, min(k, E)) := FilterTopKByScore(E, k) end if := LLM SelectNodes(q, u, E, n, level, C) for each (u, r, v) where do Π[v] := Π[u] (u, r, v) := {v} end for := C[level] := LLM DescribePaths(q, r0, Π) decision := LLM ShouldContinue(q, r0, Π) if decision = no then break end if := d := level Σ[l] := LLM Summarize(Π of depth l) Figure 6: Multi-step Performance Radar Chart Figure 7: Query Pattern vs. Retrieval Performance"
        }
    ],
    "affiliations": [
        "Case Western Reserve University"
    ]
}