{
    "paper_title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters",
    "authors": [
        "Zonghang Li",
        "Tao Li",
        "Wenjiao Feng",
        "Mohsen Guizani",
        "Hongfang Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 1 9 7 8 0 . 4 0 5 2 : r PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters Zonghang Li1 Tao Li2 Wenjiao Feng2 Mohsen Guizani1 Hongfang Yu2 1Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE 2University of Electronic Science and Technology of China, Chengdu, China"
        },
        {
            "title": "Abstract",
            "content": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what common home cluster can handle. This paper introduces prima.cpp, distributed inference system that runs 70B-scale models on everyday home devices using mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each devices CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp."
        },
        {
            "title": "Introduction",
            "content": "Current large language models (LLMs) are mostly cloud-based, but DeepSeek [DeepSeek-AI, 2025] Its R1 series, spanning from 1.5B to 70B, bring small models closer to frontier changes this. performance. Its 70B version even surpasses cloud-based models like GPT-4o and Claude 3.5 Sonnet. This has sparked interest in deploying LLMs locally on users own devices, especially with the explosion of open-source LLMs. However, limited by weak chips and small RAM, user devices struggle to run anything beyond 10B, even with 4-bit quantization. For example, running Qwen 2.5-14B (Q4K) on Mac M1 with 8 GiB RAM takes staggering 10 seconds per token. As result, most end-side LLM systems prefer smaller models, e.g., run 7B models on phones and browsers [MLC, 2023, Lugaresi et al., 2019] and 3.8B models on an Android device [Ghorbani, 2024]. Thats not too bad, users have many devices, e.g., laptops, desktops, phones, and tablets. Some PCs have high-end GPUs like the NVIDIA 20/30/40 series, and Mac M-series have Apple Metal GPUs. By pooling the computing power and memory of home devices, its possible to run larger models. In pipeline parallelism, the model is split into segments and assigned to devices. Exo [Exo, 2024] distributes model layers based on memory size, devices with more RAM/VRAM handle more layers. While this prevents OOM, it can slow speed if high-memory devices have weak CPUs/GPUs. This raises question: How should model layers be assigned? simple alternative is to assign model layers based on computing power, with stronger devices handling more layers, but also with high OOM risk. Ye et al. [2024] offers an idea: first, assign model layers based on computing power, Preprint. Under review. then migrate layers from OOM devices to those with free memory. However, this also slows down inference, as weaker CPUs handle more layers. So new questions arise: Can stronger devices handle these overloaded layers instead? If we require the cluster memory to meet the models needs [Ye et al., 2024, Exo, 2024, Tadych, 2024, Zhang et al., 2025, Lee et al., 2024, Zhao et al., 2023, Zhang et al., 2024], the answer is no, as stronger devices have reached their OOM limits. But what if we relax this requirement? For example, we can free up processed layers on stronger devices and load the extra layers to continue computation. With fast SSD, this could be more efficient than using weak-CPU devices. This adds complexity: whose disks are faster than weak CPUs? And how many extra layers can be migrated to maximize inference speed? This is tricky because disk I/O introduces new bottleneck: disk loading latency, which depends on data size to be (re-)load and disk read speed. These two factors vary with hardware and OS differences. For example, macOS with Metal reclaims memory more aggressively than Linux, causing more reloads, while Linux optimizes sequential reads, making reloading faster than on macOS. This heterogeneity makes disk loading latency hard to estimate. This paper introduces device profiler to capture system heterogeneity across computation, memory, disk, communication, and OS. We mathematically model token latency from these factors and propose Halda, an efficient algorithm to solve this layer-to-device assignment (LDA) problem. Halda provides an optimal workload distribution across CPU and GPU (if available) for each device. It determines how many model layers each device should handle and how to allocate them between CPU and GPU. To hide disk loading, we design piped-ring parallelism, where devices form ring and pass their results to the next for subsequent processing. Unlike existing systems, in our design, devices can complete token with multiple rounds, and with prefetching, we overlap disk loading with other devices to hide latency. We call it prima.cpp, as it uses piped-ring parallelism and builts on llama.cpp [Gerganov, 2024]. The main contributions are summarized as follows: We propose prima.cpp, distributed inference system designed for low-resource home It uses mmap to lazily load model weights, and piped-ring parallelism with clusters. prefetching to hide disk latency. It prevents OOM and reduces token latency. We model the LDA problem and develop device profiler to capture the system heterogeneity. It minimizes token latency by modeling delays in computation, memory access, disk loading, and communication, while optimizing the use of RAM and VRAM. We propose the Halda algorithm to solve the LDA problem. Halda breaks the NP-hard problem into set of simple ILPs, so we can find the optimal solution in polynomial time. We implement prima.cpp with 20K LoC modifications. Evaluation on real home cluster shows that prima.cpp is 15 faster than llama.cpp on 70B models, with memory pressure below 6% per device. It also surpasses distributed alternatives like exo [Exo, 2024] and dllama [Tadych, 2024] in both speed and memory efficiency across all 7B-72B models. In our experiments, small, heterogeneous, and budget-friendly home cluster (2 laptops, 1 desktop, 1 phone) was used. Our prima.cpp achieves 600 milliseconds per token and time-to-first-token (TTFT) below 2 seconds for 70B model, making it accessible for voice chat apps like home Siri. For 45B-70B models, the speed matches those of audiobook apps. It now supports hot models including Llama 3, Qwen 2.5, QwQ, and DeepSeek R1 (distilled versions)."
        },
        {
            "title": "2 Related Work",
            "content": "On-device LLM systems. Most of these systems are limited to small models. MLC-LLM [MLC, 2023] and MediaPipe [Lugaresi et al., 2019] bring 7B models to mobile phones and browsers. PocketPal AI [Ghorbani, 2024], which runs on Android using llama.cpp [Gerganov, 2024], supports models up to 3.8B. Some efforts push for larger models, like AirLLM [Li, 2023], which loads only needed layers to save memory but at the cost of speed. Others turn to high-end hardware, such as the Apple M2 Ultra (192 GiB RAM) for 65B model or kTransformers [kvcache ai, 2025] (382 GiB RAM) for 671B model (75 GiB for 70B). These setups (large RAM, advanced CPUs with specific instruction sets) go far beyond common home devices and are inaccessible to most users. Distributed LLM systems at the user edge. Distributed systems break the limits of single device. We focus on edge LLM inference and categorize recent efforts into tensor and pipeline parallelism. 2 Table 1: Comparison of distributed LLM inference systems at the edge. Mem3 Quantization Mem Pressure4 Speed Heterogeneity dllama [Tadych, 2024] Zhang et al. [2025] Hepti [Lee et al., 2024] Galaxy [Ye et al., 2024] TPI-LLM [Li et al., 2024] Type1 TP TP TP TP+SP TP exo [Exo, 2024] LinguaLinked [Zhao et al., 2023] EdgeShard [Zhang et al., 2024] PP PP PP Backends CPU CPU CPU CPU / GPU CPU CPU / GPU CPU CPU / GPU RAM RAM RAM RAM / VRAM RAM RAM / VRAM RAM RAM / VRAM Q4 FP32 FP32 FP32 FP32 Q4+FP32 Q8 / FP32 FP prima.cpp (ours) PRP CPU & GPU RAM & VRAM Q4 / IQ1 Critical Critical Critical Critical Medium Critical Critical Critical Low Slow Slow Slow Slow Slow Slow Slow Fast Fast 1 TP\" is tensor parallelism, PP\" is pipeline parallelism, SP\" is sequential parallelism, PRP\" is piped-ring parallelism. 2 Backends simultaneously used on one device. CPU / GPU\" indicates only one backend is used at time on device, and if GPU is available, only the GPU will be used. CPU & GPU\" indicates that both CPU and GPU are used if GPU is available. 3 Memory simultaneously used on one device. The definitions of RAM / VRAM\" and RAM & VRAM\" are similar to the above. 4 Memory pressure when the cluster is just enough to run the model. Low: memory can be reclaimed normally. Medium: Potential for page swapping. Critical: High pressure, likely to freeze or be killed. 5 Whether be optimized for heterogeneous devices. - Tensor Parallelism. Tensor parallelism splits weight tensors (e.g., attention heads and FFNs) across devices to share the load [Shoeybi et al., 2019]. To reduce all-reduce costs, dllama [Tadych, 2024] uses USB4 and Thunderbolt 5 for fast connections, and [Zhang et al., 2025] uses wireless analog superposition to perform all-reduce over the air. Due to device heterogeneity, Hepti [Lee et al., 2024] optimizes workload partitioning with three slicing strategies for different memory budgets, and Galaxy [Ye et al., 2024] prioritizes compute power first, then memory, to maximize speed and avoid OOM. These systems load the full model into cluster memory. With limited total memory, only small models can run. TPI-LLM [Li et al., 2024] loads model layers on demand and hides disk loading with prefetching. This allows poor devices with only 4 GiB RAM to run 70B model, but its still slow. - Pipeline Parallelism. Due to Wi-Fis high latency, pipeline parallelism is better for home clusters as its fewer P2P communication. Exo [2024], Zhao et al. [2023], Zhang et al. [2024] split the model into segments and assign them to devices based on memory, compute, and network conditions. Each device computes its segment and passes the result to the next, until the last device outputs the next token. Exo [2024] partitions model segments based on memory ratio; LinguaLinked [Zhao et al., 2023] uses linear optimization to solve the device assignment problem; and EdgeShard [Zhang et al., 2024] uses dynamic programming. Some of them are efficient, but rely on specialized devices like Jetson AGX/Nano, which are not common in household. These distributed systems, except TPI-LLM, have severe limitation: they require cluster memory to meet the models needs. When cluster memory is limited, only small models can run. However, users have few devices with less mem_available (could be much less than mem_total), and allocating too much for LLM app can freeze devices and degrade user experience. Additionally, as shown in Table 1, some systems lack GPU support, while others dont use CPU offloading. This restricts devices to either RAM or VRAM and further limiting cluster memory. Most also skip quantization and reside model weights in memory, making OOM highly likely for larger models. Prima.cpp addresses these issues with four key features: (a) support disk offloading to allow cluster memory to be less than the models needs. While this causes disk latency, prima.cpp minimizes it through optimal workload distribution and piped-ring parallelism with prefetching; (b) support GPU & CPU offloading to combine RAM and VRAM into cluster memory, with optimal workload assignment to alleviate CPU slowdowns; (c) support quantization like Q4K (used by default) and IQ1 to cut memory needs; and (d) use mmap to cache model weights, so the OS can free them as needed."
        },
        {
            "title": "3 Prima.cpp: Parallel Architecture and Scheduler Design",
            "content": "We first introduce the piped-ring parallelism with prefetching adopted by prima.cpp in Section 3.1. Section 3.2 then mathematically models the layer-to-device assignment problem and Section 3.3 proposes polynomial-time algorithm to solve it, so that the inference latency is minimized. 3 3.1 Piped-ring Parallelism with Prefetching Pipeline parallelism is common in distributed inference, where the model is split into layer-based segments and assigned to chain of devices. Each device computes its segment and passes the result to the next, this process continues until the last device outputs the next token. Exo [2024], Zhang et al. [2024], Zhao et al. [2023] take this step further by linking the last device back to the first, forming ring. This allows input and output to be processed on the head device, providing enhanced privacy. Pipeline parallelism works well for batch inference when resources are abundant, but its not good fit for resource-limited home clusters due to: (a) home users have few devices with low specs (see Table 2); (b) LLM apps should use mem_available, not mem_total, or other apps will slow down, degrading user experience; and (c) batch inference demands more RAM. These limitations make it hard to gather enough devices to meet the memory needs of larger models. In this study, we focus on single-request inference to tackle the immediate challenge of fast inferencing 70B models in resource-scarce clusters. Batch inference can be developed once this foundation is solid. Given this setup, the drawback of pipeline becomes apparent: when device is running, it loads required layers (due to limited mem_available, we swap layers from disk instead of keeping full model) and computes them, meanwhile, others sit idle. To improve this, we use prefetching. in advance, Pipeline parallelism with prefetching. Prefetching loads layers ensuring that when computing starts, the required layers are already (or partially) in place. This overlaps disk loading with ongoing operations on other devices, so part of the latency can be hidden. We use mmap advice to achieve this: once device finishes computing, we mark upcoming layers as WILLNEED to advise the OS to prefetch them in the background when the load permits, so it does not block the main process. It continues even after computing starts until page faults occur. Figure 1: Piped-ring parallelism. In this case, 6 devices handle 36-layer model. With layer window size of 2, the model is splitted into 18 segments, which are assigned to 6 devices in ring order, so each device needs 3 rounds to predict one token. This vanilla approach has flaw: if the current device has high disk speed, or other devices run for long time, prefetching may load too many layers, exceeding its available memory. As result, later-loaded layers will overwrite those prefetched earlier. We call this effect prefetch-release\" (see Appendix A.1), which results in all layers being loaded twice, adding unnecessary disk overhead without any benefit from prefetching. To address this, we propose the piped-ring parallelism. Piped-ring parallelism with prefetching. In piped-ring parallelism, devices are connected in ring structure, but unlike Zhang et al. [2024], Exo [2024], Zhao et al. [2023], we can run multiple rounds to predict one token. We define layer window size as the number of model layers device should handle in each round. For example, in Figure 1, each device takes 3 rounds to predict one token. In each round, device 1 should handle 2 model layers, so its window size is 2. Layer window sizes vary by device capabilitystronger devices have larger windows. However, finding the optimal window size setup is complex. In section 3.2, we mathematically model this problem, followed by an efficient algorithm in section 3.3 to solve it. By setting the layer window size small, we ensure the model layers stays within memory limits, avoiding prefetch-release\" during prefetching. We visualize this magic and explain how disk loading is overlapped in Appendix A.2. 3.2 Layer-to-device Assignment Problem An implicit assumption was made in section 3.1: all devices, even heterogeneous ones, use the same layer window size, which is poor practice. High-end devices should handle more layers than 4 low-end ones. As shown in Figure 6(f), if we offload some layers from devices 2, 3 to devices 1, 4, bubbles\" (device idle time) can be smaller, and more rounds can be executed within the same time. So here comes the question: how should the model layers be distributed among resource-limited and heterogeneous devices, and how many layers should run on the GPU (if any)? To answer this question, we define the layer-to-device assignment (LDA) problem as follows. Definition 1 (Layer-to-Device Assignment Problem, LDA). Assume there are devices, wm is the layer window size on device dm and nm is the number of GPU layers within wm. Let the decision variables be wT = [w1, w2, , wM ] and nT = [n1, n2, , nM ]. Our objective is to find the optimal and that minimizes the token latency: min w,n s.t. aT + bT + eT eT + κ, wm Z>0, nm Z0, nm wm L, k(eT w) = 0, Z>0, Pw + Pn + eT < 0, zgpu eT + gpu gpu 0. (1) (2) (3) (4) (5) where is the number of model layers; a, b, are constant vectors determined by computation latency, memory access latency, disk loading latency, and communication latency on each device; κ is constant coefficient; is the number of rounds to predict one token; and are the extended vectors of and n; and zgpu are constraint vectors for RAM and VRAM; Pw, Pn, gpu are diagonal matricies that activate or deactivate the decision variables. For the derivation of this definition, please see Appendix A.3. Simply put, constraint (2) ensures that each device handles at least one layer, with the GPU layers not exceeding the total. Constraint (3) enforces all devices are assigned an equal number of windows and all the windows are filled. This is not mandatory in our implementation, but can simplify the problem model. Constraints (4) and (5) ensure that RAM and VRAM usage stay within limits. Table 5 summarizes the key symbols. To construct a, b, c, κ, w, n, z, Pw, Pn, we should categorize devices into one of the following cases: Case 1 (set M1): macOS with Metal disabled and insufficient RAM; Case 2 (set M2): macOS with Metal enabled and insufficient RAM; Case 3 (set M3): Linux and Android with insufficient RAM; Case 4 (set M4): OS with sufficient RAM or low disk speed. For example, for cases 13 where devices should overload, RAM usage must stay above lower bound, but for case 4 where overloading is not allowed, RAM usage must stay below upper bound. Whether device is overloaded depends on and n, e.g., large wm nm will overload RAM. However, we cannot determine devices case before solving the problem, and without knowing the case, we cannot solve the problem. This traps us in circular dependency. Besides, since = eT appears in both the objective and constraints, this LDA model is not standard integer linear fractional programming (ILFP) problem, making the problem more challenging. 3.3 Our Halda Scheduler To solve the LDA problem, our core ideas include: (i) transform the NP-hard original problem into set of standard integer linear programming (ILP) problems by enumerating over all possible k, and (ii) search for optimal set assignments M1 M4 by iterative optimization. Transform into standard ILP problems. Given that the number of layers in typical LLMs is less than 100, the integer has limited range of values at most 11 valid factors for any 100. By enumerating over these factors, we can treat and as constants, then the problem becomes: min w,n s.t. k(aT + bT + eT c) + κ, wm Z>0, nm Z0, nm wm L, eT = W, Pw + Pn + < 0, zgpu 0. gpu P gpu 5 (6) (7) (8) (9) (10) Algorithm 1: Heterogeneity-Aware Layer-to-Device Allocation (HALDA) 1 Initialize layer windows proportionally to devices memory budgets and the number of GPU layers 0; 2 Calculate platform-specific coefficients αm, βm, ξm for each device m; 3 Calculate valid factors KL of (excluding L); 4 while true do 5 Calculate = eT and = L/W ; Re-assign devices to sets M1, M2, M3, M4 based on the latest w, n, and Mforce if the assignment sets M1, M2, M3, M4 remain unchanged then ; break; Calculate the objective coefficients a, b, c, κ, the RAM upper bound z, and the VRAM/shared memory upper bound zgpu according to the updated assignment sets; foreach KL do Solve the ILP problem (1-5) with fixing using ILP solver; Update best solution (w, n) if the current objective is smaller; if any device has free VRAM but another device is overloaded then Force the device ms from M1 M2 M3 with lowest disk read speed into Mforce continue; ; 6 7 8 9 11 12 13 14 15 Update and n; 16 17 return w, n; Hence, for each fixed k, the objective and constraints boil down to linear functions/inequalities, and the problem becomes an ILP. Then, we can run standard ILP solver (e.g., HiGHS [Huangfu and Hall, 2018]) to obtain the optimum w, n. Iterative optimization on set assignments. The problem remains unsolvable because the set assignments M1 M4 are unknown. To resolve the circular dependency, we adopt an iterative optimization approach. Initially, is set proportionally based on the memory budget (davail for macOS without Metal and Linux, davail for Android), and is initialized to 0. This gives an initial division of devices into sets M1 M4. Then, we solve the ILP problems to update and n, re-assign the devices to their new sets, and repeat this process until the set assignment remain unchanged. m,metal for macOS with Metal, and davail + dswapout This approach still has defects. The ILP model only constrains the upper bound on the number of layers that can be allocated to the GPU n. In certain cases, such as when the total number of layers is small and when the initial set assignment falls into local optimum, the number of layers assigned to the GPU may be insufficient. For example, if device is assigned wm = 30 layers and its GPU can handle up to 20 layers, but due to memory constraints in M1 M3, only 10 layers go to the GPU, leading to underutilization of GPU resources. This issue is caused by the initial set assignment, which allocates too many devices to M1 M3, leaving an insufficient number of layers for GPUs. Therefore, in Algorithm 1, we apply calibration step: if GPU is underutilized (its VRAM is not full) while another device is overloaded (i.e., its VRAM is full and has offloaded some layers to the CPU, or it has exceeded the RAM limit), we force the device M1 M2 M3 with lagging disk into M4, re-construct the sets M1 M4, and re-solve the problem. This way, we can converge to the optimal set assignment and then determine the optimal setup for w, and k. Complexity Analysis. The main loop has two key tasks: set assignment and solving the LDA problem. It runs for iterations, which in the worst case is = O(M ). Set assignment also takes O(M ) time. For LDA, we solve an ILP for each valid factor of L, with = O(log L) factors in total. Our ILP is usually tiny and sparse, taking O((2M )3.5) to solve it. Thus, the total time complexity is O(T (M + K(2M )3.5)), which can be solved in polynomial time."
        },
        {
            "title": "4 Experiments",
            "content": "We built the experimental platform on common home devices detailed in Table 2. On Mate40Pro (HarmonyOS) and Honor Pad (Android), we run prima.cpp inside Termux-simulated Linux. These 6 Table 2: Configurations on experimental device D1 D2 D4 Device OS CPU CPU Cores RAM (available) Disk Read Speed GPU Type VRAM (available) Mac M1 MacOS (UMA) Apple M1 8 2.4 GiB 0.72 GB/s Apple Metal - Laptop Linux Intel i9 8 4.1 GiB 2.98 GB/s 3070 8 GiB Desktop Linux Intel i9 16 9.7 GiB 3.17 GB/s 2080TI 11 GiB Mate40Pro Linux (on HarmonyOS) Kirin 9000 8 1.9 GiB 1.37 GB/s - - D5 Honor Pad D6 Mac Air Linux (on Android) MacOS (NUMA) Dimensity 8100 8 5.1 GiB 2.00 GB/s - - Intel i5 4 6.8 GiB 0.39 GB/s - - devices were connected to local Wi-Fi router. By default, we used 4 devices (D1D4) with total available RAM+VRAM of 37 GiB (not enough for Q4K-quantized 70B model). We evaluated Llama models from 8B to 70B (in Q4K format) in terms of token latency, TTFT, and memory pressure, and compared with llama.cpp [Gerganov, 2024], exo [Exo, 2024], and dllama [Tadych, 2024]. The results showed the significant advantages of prima.cpp in both speed and memory pressure. 4.1 Faster Inference on Larger Models As llama.cpp is an on-device system, we ran it on the most powerful desktop. Meanwhile, we ran exo on devices D1-D3, as they are equipped with Apple/NVIDIA GPUs, and running exo on D4 require root access, which is not permitted. Instead, dllama and prima.cpp used devices D1-D4. Table 3 presents the token latency and TTFT across different model sizes. As of now, exo and dllama do not support Llama models from 14B to 65B, so we put -\" in the table. As illustrated in Figure 9a (in Appendix A.6), for smaller models (14B), the 11 GiB VRAM on the desktop is sufficient for full GPU inference, at this time, llama.cpp is the best choice. However, at 30B, VRAM runs out, forcing layers onto the CPU, thus inference speed becomes limited. As the model size increases to 45B, even the 9.7 GiB of available RAM is exhausted. Since llama.cpp uses mmap to lazily load model weights, the OS frees inactive mmap-ed pages and reloads them as needed, which introduces disk loading latency. At this stage, only few pages are released, so efficiency loss is small. However, at 60B, more active mmap-ed pages are labeled as inactive earlier and then released, leading to sharp increase in token latency and TTFT. This indicates that llama.cpp is not well-suited for running larger models on home devices. For exo and dllama, despite only data for the 8B model is available, their limitations are already evident. Llama 3-8B (Q4K) requires just 5.3 GiB of VRAM, so all 32 layers could fit entirely on D3-GPU. However, exo allocates model layers proportional to each devices memory. D1 (8 GiB RAM), D2 (8 GiB VRAM), and D3 (11 GiB VRAM) are assigned 9, 10, 13 layers, respectively. While D1 has an Apple Metal GPU, its efficiency is much lower than D3-GPU, making it an efficiency bottleneck. For dllama, inference is performed using tensor parallelism, and the workload is evenly distributed across devices. This forces highand low-performance devices to handle the same load and causes blocking during all-reduce, leading to limited efficiency. Prima.cpp addresses these limitations effectively, which uses piped-ring parallelism with prefetching to collaborate multiple devices. Unlike existing systems using solely CPU or GPU, prima.cpp uses both CPU and GPU on each device. It also adapts better to heterogeneous home devices by analyzing CPU/GPU power, available RAM/VRAM, memory management behavior, and disk speed to assign workloads, with the goal to minimize token latency. As shown in Table 3, prima.cpp has significantly lower token latency and TTFT than exo and dllama across all model sizes and outperforms llama.cpp for models larger than 30B. Specifically, compared to llama.cpp, prima.cpp improves token latency by up to 17 and TTFT by up to 8. Against exo and dllama, it speeds up token latency by 58 and TTFT by 1224. It also effectively prevents OOM errors. Appendix A.4 shows the results on more hot models, including Qwen 2.5, QwQ and DeepSeek R1. In current implementation, each device is assigned at least one model layer, for example, leading to 1:1:29:1 split for Llama 3-8B. This restriction is unnecessary and we will remove it in future updates. Then, we will have 0:0:32:0 split and idle devices removed, making llama.cpp special case of prima.cpp when serving small models. Appendix A.5 explains why we use D1-D4 instead of D1-D6: 7 Table 3: Token latency and TTFT (in millisecond/token) for llama.cpp, exo, dllama, and prima.cpp. Model Llama 3-8B Llama 3-14B Llama 1-30B Llama 3-45B Llama 3-60B Llama 1-65B Llama 3-70B llama.cpp exo dllama prima.cpp (w/o halda) prima.cpp (w/o prefetch) prima.cpp Latency TTFT Latency TTFT Latency TTFT Latency Latency Latency TTFT 15 202 328 7965 8807 10120 18 25 611 712 8350 9662 10806 263 - - - - - OOM 960 - - - - - OOM 459 - - - - - OOM 1845 - - - - - OOM 78 258 409 7053 12253 20848 53 62 79 263 532 688 755 54 65 72 233 468 569 674 78 214 440 990 1770 1793 more is not always better, and the cluster memory does not need to match the models needs. It also shows how prima.cpp can pick subset of devices to build best-performing cluster. 4.2 Ablation Study on Prefetching, Halda and Piped-ring Parallelism Table 3 also includes an ablation study on Halda and prefetching. For fair comparison, we set prima.cpp (w/o halda) to use exos strategy - assigning model layers proportional to device RAM/VRAM, but improves it by using available RAM/VRAM instead of total and offloading overloaded layers from GPU to CPU to prevent OOM. Instead, prima.cpp uses our Halda strategy. Prefetching. To test prefetching, we disable it in prima.cpp (w/o prefetch) and enable it in prima.cpp. Prefetching has no effect on small models, as they fit entirely in RAM/VRAM without triggering memory reclamation. However, for larger models, memory reclamation triggers frequent page faults as early layers have been released by the OS, but computation starts with them. To mitigate this, prima.cpp prefetches released layers immediately after computation, ensuring they are already in memory before the next round of computation begins to reduce page faults. This overlaps disk loading with other devices operations, thus lowering token latency by 9%17%. Halda. proper layer assignment is crucial for fast inference. For small models, RAM/VRAM on D1-D4 is sufficient, so prima.cpp (w/o halda) and prima.cpp perform similarly. However, prima.cpp (w/o halda) assigns more layers to weaker CPUs/GPUs, while prima.cpp prioritizes powerful GPUs, the latter achieves lower latency. For larger models, prima.cpp (w/o halda) overloads non-GPU devices, triggering memory reclamation and reloading, causing latency spike. In contrast, prima.cpp avoids memory overload on slow-disk devices through optimized layer assignment, achieving up to 30 speedup for 70B-scale models. Figure 2: Normalized token latency over k. Piped-ring Parallelism. To evaluate it in isolation, we built CPU cluster with 4 Linux devices, each having 8 cores, 8 GiB RAM, and SSD of 2 GB/s. We tested models from 8B to 72B, and assigned model layers evenly across devices. For example, for 65B-72B models with 80 layers, at = 1 (k denotes the rounds to predict one token), layers were split 20:20:20:20; at = 2, 10:10:10:10, and so on. The results in Figure 2 show that when total available RAM is insufficient (>60B), piped-ring parallelism reduces latency by nearly half. However, at = 1, it offers no benefit due to the prefetch-release\" effect. Thus, the layer window size should be set smaller, and piped-ring parallelism becomes essential (see Appendices A.1 and A.2). However, when memory is sufficient (<45B), no disk loading occurs, but increasing slightly raises latency due to overhead from model fragmentation and reduced computational parallelism. To sum up, piped-ring parallelism excels under high disk-loading conditions, but = 1 is better when memory is sufficient. 8 Model Llama 3-8B Llama 3-14B Llama 1-30B Llama 3-45B Llama 3-60B Llama 1-65B Llama 3-70B Table 4: Memory pressure for llama.cpp, exo, dllama, and prima.cpp on each device. llama.cpp D3 D1 exo D2 D3 D1 D3 D4 D1 dllama prima.cpp D3 D4 2.0% 2.5% 8.0% 3.9% 5.5% 15.6% 6.0% 1.0% 5.4% 20.0% 51.3% 42.5% 13.5% 12.8% 55.8% 12.8% 5.3% 1.0% 4.3% 5.3% 1.0% 3.0% 5.7% 1.0% 4.9% 1.0% 1.0% 6.3% 4.7% 3.9% 1.0% 1.0% 1.0% 1.0% OOM OOM OOM OOM OOM OOM OOM 4.7% 2.7% 2.2% 2.9% 6.0% 4.7% 4.8% 4.8% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 4.3 Low Memory Pressure for Better Experience Memory pressure is crucial for user experience because high pressure can slow down apps or even crash the device. Imagine if an LLM app will cause home devices to freeze, would you use it? Clearly, no. However, this is ignored by existing systems. Appendix A.6 shows devices memory footprint to illustrate the workload distribution and explain why prima.cpp is faster. However, higher value does not indicate higher pressure, as reclaimable memory like page cache is included but can be freed instantly by the OS. To better measure memory pressure, we define it as the reduction in mem_available during runtime relative to mem_total. For example, 2 GiB decrease in mem_available on an 8 GiB device results in 25% pressure. As mem_available includes free and reclaimable pages, its reduction contains only non-reclaimable pages, so it is memory pressure. Table 4 shows the memory pressure caused by llama.cpp, exo, dllama, and prima.cpp on each device. Prima.cpp and llama.cpp have low memory pressure, using only small amount of mem_used for key-value cache and compute buffer. Since they load model weights via mmap, the majority of resident memory is held in the page cache and can be instantly released by the OS without affecting other apps. In contrast, exo and dllama keep model weights in mem_used, causing high memory pressure. This forces the OS to free inactive pages, compress memory, swap data to disk, potentially slowing down apps, leading to system lag, or OOM for larger models. Overall, exo and dllama prioritize resources for the LLM app at the cost of other apps; prima.cpp and llama.cpp prioritize user experience, keeping low memory pressure and thus better suited for user devices. Therefore, prima.cpp is the best choice for running larger models on home devices (>30B in our case)."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces prima.cpp, distributed inference system to run 70B-scale LLMs on everyday home devices. The core design is piped-ring parallelism with prefetching and scheduler to split model layers to heterogeneous devices. Prima.cpp uses mmap to manage model weights, preventing OOM for any model size but introducing disk latency. To hide this latency, prima.cpp employs prefetching and piped-ring parallelism, which overlaps disk loading and avoiding the prefetchrelease\" effect. To further speed up inference, we mathematically model token latency by considering the heterogeneous nature of home devices. We propose Halda to solve this NP-hard LDA problem, which assigns model layers to device CPUs and GPUs and minimizes token latency. On small, heterogeneous, and budget-friendly home cluster, prima.cpp outperforms llama.cpp, exo, and dllama in speed, model size, and memory pressure. This work requires lower-end hardwares and has better support for cross-platform deployment. With prima.cpp, we are able to collaborate phones, tablets, laptops, desktops etc. to bring 70B LLMs (Llama 3, DeepSeek R1, Qwen 2.5, QwQ) to home Siri. Limitations: (a) Limited device types and quantity restrict our exploration of diverse home clusters. Readers are welcome to share their results on our Github repo. (b) Though much faster than other on-device systems, 70B models remain much slower than those on the cloud. Future work will integrate IQ1/Q4K for higher efficiency. (c) In low-RAM clusters without SSDs or GPUs, larger models will be extremely slow. (d) Token latency is heavily affected by memory competition. If there are other processes, prima.cpp slows down to free RAM for them and speeds up when they stop, so we use stable value from multiple runs instead of an error bound. (e) Prima.cpp unlocks larger-scale open-source LLMs on user devices, where malicious content may not be filtered. The open-source community should enhance oversight of their models to prevent misuse."
        },
        {
            "title": "References",
            "content": "DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Exo. exo: Run your own ai cluster at home with everyday devices. https://github.com/ exo-explore/exo, 2024. Georgi Gerganov. llama.cpp: Llm inference in c/c++. https://github.com/ggerganov/llama. cpp, 2024. Asghar Ghorbani. Pocketpal ai: An app that brings language models directly to your phone. https: //github.com/a-ghorbani/pocketpal-ai, 2024. Qi Huangfu and JA Julian Hall. Parallelizing the dual revised simplex method. Mathematical Programming Computation, 10(1):119142, 2018. kvcache ai. ktransformers: flexible framework for experiencing cutting-edge llm inference optimizations, 2025. URL https://github.com/kvcache-ai/ktransformers. Juhyeon Lee, Insung Bahk, Hoseung Kim, Sinjin Jeong, Suyeon Lee, and Donghyun Min. An autonomous parallelization of transformer model inference on heterogeneous edge devices. In Proceedings of the 38th ACM International Conference on Supercomputing, pages 5061, 2024. Gavin Li. Airllm: scaling large language models on low-end commodity computers, 2023. URL https://github.com/lyogavin/airllm/. Zonghang Li, Wenjiao Feng, Mohsen Guizani, and Hongfang Yu. Tpi-llm: Serving 70b-scale llms efficiently on low-resource edge devices. arXiv preprint arXiv:2410.00531, 2024. Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, et al. Mediapipe: framework for perceiving and processing reality. In 3rd Workshop on Computer Vision for AR/VR at CVPR, 2019. MLC. Mlc-llm: Universal llm deployment engine with ml compilation. https://github.com/ mlc-ai/mlc-llm, 2023. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, et al. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Bartłomiej Tadych. Distributed llama. https://github.com/b4rtaz/distributed-llama, 2024. Shengyuan Ye, Jiangsu Du, Liekang Zeng, Wenzhong Ou, Xiaowen Chu, Yutong Lu, and Xu Chen. Galaxy: resource-efficient collaborative edge ai system for in-situ transformer inference. In IEEE INFOCOM 2024-IEEE Conference on Computer Communications, pages 10011010. IEEE, 2024. Kai Zhang, Hengtao He, Shenghui Song, Jun Zhang, and Khaled Letaief. Distributed on-device llm inference with over-the-air computation. arXiv preprint arXiv:2502.12559, 2025. Mingjin Zhang, Jiannong Cao, Xiaoming Shen, et al. Edgeshard: Efficient llm inference via collaborative edge computing. arXiv preprint arXiv:2405.14371, 2024. Junchen Zhao, Yurun Song, Simeng Liu, Ian Harris, and Sangeetha Abdu Jyothi. Lingualinked: arXiv preprint distributed large language model inference system for mobile devices. arXiv:2312.00388, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Prefetch-release Effect As illustrated in Figure 3. Before computation starts, the OS prefetches 3 model layers to the available memory limit. However, it does not stop and continues to load the 4th layer, causing the 1st layer to be released. This prefetch-release\" cycle repeats, so by the end, the last 3 layers are in memory, while the first 3 are not. Then, when computation begins, the 1st layer, which is not in memory, triggers page fault, prompting the OS to reload it and the 4th layer to be released. Finally, all layers are loaded twice, adding unnecessary disk overhead without any benefit from prefetching. Figure 3: Illustration of model layers loaded into memory in pipeline parallelism with prefetching. In this case, the device handles 6 model layers but its available memory can only hold 3. The green blocks show the layers loaded into memory, while white blocks indicate those not yet loaded. A.2 How Piped-ring Parallelism Solve the Prefetch-release Effect? To illustrate, Figure 4 considers fast-disk device where prefetching is fast enough to complete before computation begins. In this case, with fast disk and layer window size of 2, 1 prefetching is fast enough to load 2 layers before computation begins, then 2 computation runs without page faults. Then, 3 the next round of 2 layers are prefetched, replacing the used ones. Steps 27 repeat until inference is complete. Prefetching overlaps with other devices operations, so its latency does not contribute to inference time. Here, with no page faults, total latency comes only from computation. In other words, disk loading latency is fully overlapped. Figure 4: Illustration of model layers loaded into memory in piped-ring parallelism with fast disk. Figure 5 shows common case with slow disk. In this case, 1 prefetching loads only one layer then 2 computation begins, 3 page fault triggers when reaching the 2nd layer, blocking until it loads. After computation, 4 the device prefetches the next round of 2 layers, but only one layer loads due to the slow disk and releasing the oldest layer. Then, 5 the next round of computation begins, and 6 at the 6th layer, another page fault occurs. This cycle of loading (prefetch) - computing - loading (page fault) - computing\" repeats until inference completes. While page fault-induced loading blocks computation, prefetching helps overlap some latency. 11 Figure 5: Illustration of model layers loaded into memory in piped-ring parallelism with slow disk. We use timeline to visualize this overlap. In Figure 6, green blocks show prefetching that is overlapped, and orange blocks show page fault-induced loading that is not overlapped. In Figure 6(a), with fast disk, disk loading is fully overlapped. In Figures 6(b) and 6(c), with device has slow disk, only part of the disk loading is overlapped, while others are fully overlapped. In Figures 6(c), 6(d) and 6(e), while disk loading is not fully hidden, piped-ring parallelism significantly reduces token latency compared to vanilla pipeline parallelism. In Figure 6(e), while prefetching is used, it exceeds memory limits and triggers prefetch-release\", where the OS releases the earlier prefetched layers as new ones load, adding disk cost with no benefit. This underscores the need to combine piped-ring parallelism with prefetching for higher efficiency. A.3 Layer-to-Device Assignment: From Latency Analysis to Vectorized Model Assume there are devices, where the layer window size for device dm is wm. On device dm, the number of GPU layers nm is defined as: within layer window of size wm, nm layers run on the GPU, while the remaining wm nm layers run on the CPU (wm and nm can vary across devices). Our objective is to find vector = {w1, , wM } and vector = {n1, , nM } to minimize the token latency , which is the sum of latencies from computation comp , memory access mem , disk loading disk , and communication comm on each device. = (cid:88) m= (cid:0)T comp + mem + disk + comm (cid:1) . (11) Here, we minimize = (cid:80)M m=1 Tm instead of = max{Tm} because Figure 6(f) is an idealized visualization. In practice, the OS does not start prefetching immediately after computation, and the timing is unknown. As result, device 4 experiences more bubbles\" and higher page fault-induced latency than expected. This uncertainty prevents solving = max{Tm} before deployment (it is also hard to measure), and historical data is useless due to fluctuating device conditions. Thus, we take worst-case approach, assuming the OS hasnt started prefetching when computation begins, leading to our objective = (cid:80)M Estimation of computation latency comp . The computation latency on device dm is defined as the time taken to process lm model layers and the output layer (if dm is the head device), where lgpu layers and output layer run on the CPU. Here, we have lm = (cid:4) = (cid:4) m=1 wm, and = mod . Since the input layer adopts look-up table, it does not contribute to computation latency. (cid:5) wm + min(wm, max(0, (cid:80)m1 j=1 min(wj, R))), where nm wm, = (cid:80)M layers run on the GPU, and the remaining lm lgpu m=1 Tm. Next, we analyze these latencies in detail. (cid:5) nm + min(nm, max(0, (cid:80)m j=1 min(wj, R))), lgpu 12 Figure 6: Timeline of (a,b) piped-ring parallelism on homogeneous devices with fast and slow disks; (c,e) piped-ring parallelism on heterogeneous devices with same and different window sizes; and (d,e) vanilla pipeline parallelism on heterogeneous devices with and without prefetching. To estimate the computation time, we develop model profiler to count the number of floating-point operations (FLOPs) for each model layer and device profiler to measure the floating-point throughput (FLOPS) of each device. Taking Q4K as an example, the model weights are primarily quantized in the Q4K format, though some weights use other formats. Specifically, we consider ={Q4_K, Q5_K, Q6_K, Q8_0, F16, F32}, as well as three types of backends CPU, CUDA, and Metal. The FLOPs for , q5k each layer Fm and output layer out }, m,out, q80 out m,out}, with each represents the FLOPs under specific quantization format. The FLOPS Sm consists of 3 sets {S cpu , cuda }, with each set consists of , scpu,q80 6 values (e.g., for CPU, cpu }) representing the floating-point throughput under specific device and quantization format. Fm, out and Sm can be easily extended. consists of 6 values Fm = {f q4k , metal , scpu,f16 = {scpu,q4k = {f q4k m,out, q6k m,out, q5k m,out, m,out, f16 , q80 , q6k , scpu,q5k , scpu,q6k , scpu,f32 m , f32 , f16 With these profilers, we can estimate the computation time as follows: = (lm lgpu comp ) scpu,q (cid:88) qQ + lgpu m sgpu,q (cid:88) qQ + Im=1 m,out scpu,q , (cid:88) qQ (12) 13 . If it runs on an Apple device with Metal enabled, sgpu,q refers to GPU FLOPS. If prima.cpp is compiled with CUDA support, sgpu,q Here, sgpu,q to scuda,q current implementation, the output layer is executed only on CPUs by the master node = 1. Estimation of memory access latency mem copy time kv_cpy specific device; b) device copy time dev_cpy and the GPU (CUDA or Metal); c) device loading time dev_load RAM or VRAM into the processing cores of the CPU or GPU. . This latency consists of three components: a) kvcache : the time taken to copy the new token cache to the key-value cache storage within : the time taken to copy hidden states between the CPU : the time taken to load data from corresponds . In our corresponds to smetal,q For the kvcache copy time, in each token step, new key and value caches are generated with dimensions (hkek, 1) and (hvev, 1), respectively. Here, hk and hv are the number of attention heads for the key and value caches, ek and ev are the embedding size per head for the key and value vectors. Thus, for generating one token, each layer needs to copy hkek + hvev values to the key-value cache storage. If the values are stored in the F16 format, each value takes 2 bytes, so the total number of bytes to be copied is 2(hkek + hvev). In the device profiler module, we measure the time of copying 2(hkek + hvev) bytes within CPU, CUDA, and Metal to obtain tkv_cpy,cpu . Then, the + lgpu kvcache copy time kv_cpy and tkv_cpy,gpu tkv_cpy,gpu can be estimated by (lm lgpu )tkv_cpy,cpu . For the device copy time, this latency arises when the GPU is enabled, as it involves copying the input from RAM to VRAM and then copying the output from VRAM back to RAM. Both the input and output have dimensions (e, 1), where represents the embedding size. These values are typically stored in the F32 format. In the device profiler module, we measured the latency for two operations: the time taken to copy 4e bytes of data from RAM to VRAM, denoted as tram-vram , and the time taken to copy 4e bytes of data from VRAM to RAM, denoted as tvram-ram . For sequence of layers within window, one RAM-to-VRAM copy and one VRAM-to-RAM copy are needed, so the device copy time for one window is tram-vram . For device dm, it was assigned Wm = (cid:4) (cid:5) + min(1, max(0, (cid:80)m1 j=1 min(wj, R))) windows. Thus, the device copy time for device dm is dev_cpy + tvram-ram = 1 indicates that device dm uses unified memory architecture (UMA, e.g., Apple with M1 chip) and the CPU and GPU use the shared memory, so no explicit RAM-VRAM copy is needed. ), where IUMA = Wm(tram-vram + tvram-ram )(1 IUMA m For the device loading time, processing cores must load data from RAM/VRAM into registers before executing instructions, which incurs latency. However, the theoretical bandwidth of memory cards cannot estimate this latency because apps often fail to fully utilize the bandwidth, and multi-level caching also has significant influence. To capture these effects, our device profiler implements an operator to read data from RAM/VRAM into registers. By measuring its latency with data volumes similar to the tensor sizes, we have the practical throughput, denoted as {T cpu }. Next, we count the data volume that needs to be loaded into registers during each token step, which typically consists of the weight data and the key-value cache. In the model profiler, we record the total bytes of weight data for the input and output layer as bi, bo, and for each layer as b. Additionally, the key-value cache size for each layer is 2(hkek + hvev)nkv, where nkv is the number of tokens for which the cache is stored. Then, the device loading time dev_load can be expressed as = ( lmlgpu )(b + 2(hkek + hvev)nkv) + bi/V +bo dev_load depends on the cpu cpu hardware: it equals to metal if the GPU uses Metal, or cuda for CUDA, and is the vocabulary size. Im=1, where gpu , metal , cuda + lgpu gpu m Now we can combine the three latency components and give the formal definition of the memory access latency mem : =(lm lgpu mem + Wm(tram-vram )(1 IUMA ) tkv_cpy,gpu + lgpu (13) )tkv_cpy,cpu lgpu lm lgpu gpu cpu + + ( + tvram-ram bi/V + bo cpu )(b + 2(hkek + hvev)nkv) + Im=1. (14) Estimation of disk loading latency disk . Prima.cpp is designed to run on memory-constrained home devices, so it cannot load the entire model weights into physical RAM. To address this, prima.cpp uses mmap to manage model weights. By using mmap, model weights are loaded into memory from disk only when needed for computation, and the OS will release inactive mmap-ed pages when memory pressure is high. This prevents OOM issues but incurs significant disk I/O latency because mmap must reload the model weights when they are accessed again after being released. To estimate this disk loading latency, it is necessary to determine the data volume that mmap needs to load in each 14 sdisk token step. This is challenging task because different OSs have very different memory management behaviors and great dynamics. On macOS (without Metal) and Linux, the OS gradually reclaims memory. When memory pressure is moderate, i.e., when blio + 2(hkek + hvev)nkv(lm lgpu , mmap-ed pages are released incrementally until the pressure is alleviated. As result, some weight data remain in the page cache, and the amount of data that mmap needs to reload is max(blio + 2(hkek + hvev)nkv(lm lgpu ) + )b + (bi/V + bo) Im=1, 2(hkek + hvev)nkv(lm lgpu ccpu davail ) is the key-value cache size, and ccpu is the total computing buffer size. If CUDA is enabled on Linux, the model weights in private VRAM are locked by the CUDA driver, keeping them resident so no disk I/O occurs. Therefore, the disk loading latency for macOS (without Metal) and Linux can be estimated as , 0), where blio = (lm lgpu ) + ccpu > davail m,macOS(no Metal) = disk disk m,Linux = (cid:16) max blio + 2(hkek + hvev)nkv(lm lgpu ) + ccpu davail , bi/V (cid:17) . m,macOS(no Metal), lgpu For disk for sequential access, so sdisk = 0 and sdisk is now the sequential read throughput. (15) is the random read throughput. On Linux, mmap is configured However, when Metal is enabled on macOS, the behavior changes. Metal loads mmap pages into the shared memory, and the OS prioritizes retaining these pages. That is, the OS is more inclined to swap out or compress active pages while keeping mmap-ed model weight pages in shared memory intact. However, when memory is exhausted (with free and inactive pages exhausted, the compression pool nearing saturation, and heavy swap usage), macOS will release these mmap-ed pages in more aggressive manner. This may cause the entire model weights to be repeatedly loaded and released. As result, when the required memory exceeds the total available memory, i.e., when lmb + (bi/V + bo) Im=1 + 2(hkek + hvev)nkvlm + ccpu + cgpu > davail m,metal, device dm needs to reload lmb + (bi/V + bo) Im=1 bytes in each token step. Here, davail m,metal denotes the maximum working set size recommended by Metal. By measuring the random read throughput sdisk of disk, we can calculate the disk loading latency for macOS (with Metal) as: disk m,macOS (with Metal) = max (cid:16) lmb + (bi/V + bo) Im=1 sdisk (cid:16) lmb + (bi/V + bo) Im=1 + 2(hkek + hvev)nkvlm + ccpu + cgpu davail m,metal (cid:17) , bi/V (cid:17) . (16) When running on Android devices, the OS prioritizes swapping out inactive pages to disk, such as memory used by background apps, to ensure that the active app runs smoothly. As result, the available RAM for prima.cpp can be higher than initially expected, as the OS will swap out cold pages to disk, freeing up additional space in memory. Thus, on Android, the number of bytes that mmap needs to reload is max(bbio + 2(hkek + hvev)nkv(lm lgpu ) + ccpu davail = min(max(0, bbio + 2(hkek + hvev)nkv(lm lgpu ) + ccpu davail )) represents the data bytes that are swapped out to disk, dbytes_can_swap is the data bytes of currently used memory that can be swapped out, and dswap_avail is the total available swap space on the device. Then we have: dswapout ), min(dbytes_can_swap , 0), where dswapout , dswap_avail m disk m,Android = max(bbio + 2(hkek + hvev)nkv(lm lgpu ) + ccpu davail dswapout sdisk , bi/V ) . (17) By aggregating them, we obtain unified expression compatible for cross-platform devices: =T disk disk m,Linux ILinux + disk m,macOS (with Metal) ImacOS (with Metal) m,macOS (no Metal) ImacOS (no Metal) + disk m,Android IAndroid, + disk (19) where ImacOS (no Metal), ImacOS (with Metal), ILinux, IAndroid are indicator functions. This expression can be easily extended to include new OSs, e.g., Windows will be added in future updates. Estimation of network communication latency comm . In prima.cpp, devices are logically interconnected in ring structure, where each device receives input from its predecessor, processes layer window, and sends the output to its successor. After device completes the computation (18) 15 for one layer window, it transmits the result (e values in F32 format, totaling 4e bytes) to the next device for further computation on the next layer window. Therefore, during token step, the number of network communications on device dm equals the number of layer windows, which is Wm = (cid:4) for transmitting 4e bytes between adjacent devices, we can estimate the network communication latency on device dm as: j=1 min(wj, R))). By measuring the latency tcomm (cid:5) + min(1, max(0, (cid:80)m comm = (cid:23) (cid:16) (cid:22) + min(1, max(0, m1 (cid:88) j=1 min(wj, R))) (cid:17) tcomm . (20) By aggregating all these latencies, we have the objective as: (cid:34) (cid:88) = (lm lgpu ) m=1 + (lm lgpu (cid:16) lm lgpu cpu + )tkv_cpy,cpu + lgpu lgpu (cid:17) gpu + scpu,q (cid:88) qQ (cid:88) + lgpu m sgpu,q tkv_cpy,gpu qQ + Wm(tram-vram + Im=1 (cid:88) m,out scpu,q (b + 2(hkek + hvev)nkv) + )(1 IUMA ) qQ + tvram-ram bi/V + bo cpu Im= + max(lmb + (bi/V + bo) Im=1 + 2(hkek + hvev)nkvlm + ccpu davail , bi/V ) sdisk ImacOS (no Metal) + max (cid:16) lmb + (bi/V + bo) Im=1 sdisk (cid:16) lmb + (bi/V + bo) Im=1 + 2(hkek + hvev)nkvlm + ccpu + cgpu davail m,metal (cid:17) , (cid:17) bi sdisk max((lm lgpu ImacOS (with Metal) )b + (bi/V + bo) Im=1 + 2(hkek + hvev)nkv(lm lgpu ) + ccpu davail , bi/V ) sdisk ILinux max((lm lgpu )b + (bi/V + bo) Im=1 + 2(hkek + hvev)nkv(lm lgpu ) + ccpu davail dswapout (cid:23) (cid:16) (cid:22) + min(1, max(0, sdisk m1 (cid:88) j=1 min(wj, R))) (cid:17) tcomm (cid:35) , bi/V ) IAndroid (cid:34) (cid:88) (cid:0) (cid:88) m= qQ m,out scpu,q + bi/V + bo cpu (cid:1) Im=1 + (lm lgpu ) (cid:16) (cid:88) qQ scpu,q + tkv_cpy,cpu + + 2(hkek + hvev)nkv cpu (cid:17) + lgpu (cid:16) (cid:88) qQ sgpu,q + tkv_cpy,gpu + + 2(hkek + hvev)nkv gpu (cid:17) (cid:16) + Wm (tram-vram + tvram-ram )(1 IUMA ) + tcomm (cid:17) max(lmb + (bi/V + bo) Im=1 + 2(hkek + hvev)nkvlm + ccpu davail , 0) ImacOS (no Metal) + + + = + lmb + (bi/V + bo) Im=1 + 2(hkek + hvev)nkvlm + ccpu + cgpu davail (cid:17) , m,metal sdisk (cid:16) (cid:16) lmb + (bi/V + bo) Im=1 sdisk (cid:17) bi/V ImacOS (with Metal) + max + max (cid:16) (lm lgpu ) (cid:104) + 2(hkek + hvev)nkv (cid:105) sdisk (cid:35) (ILinux + IAndroid) (cid:17) bi sdisk + (bi/V + bo) Im=1 + ccpu davail dswapout sdisk IAndroid , To remove the max operator, we decompose the disk loading latency into multiple terms, separately accounting for cases where memory is sufficient or insufficient. Let be the set of all devices, and M1, M2, M3, M4 be the subsets of devices that satisfy the respective conditions in Cases 1-4, where M1 M2 M3 M4 = and M1 M2 M3 M4 = M. Case 1 (macOS with Metal disabled and insufficient RAM): Im=1 + 2(hkek + hvev)nkvlm + ccpu > davail lmb+(bi/V +bo)Im=1+2(hkek+hvev)nkvlm+ccpudavail sdisk and sdisk , M1. If > sdisk lmb + (bi/V + bo) = threshold, then disk m,metal and sdisk Case 2 (macOS with Metal enabled and insufficient RAM): If lmb + (bi/V + bo) Im=1 + 2(hkek + hvev)nkvlm + ccpu + cgpu > davail > sdisk , M2. (cid:104) Case 3 (Linux and Android with insufficient RAM): If (lm lgpu ) + dswapout bo) Im=1 + ccpu > davail 2(hkek + hvev)nkv] + (bi/V + bo) Im=1 + ccpu davail + 2(hkek + hvev)nkv = 1 sdisk (cid:17) threshold, then disk IAndroid = lmb+(bi/V +bo)Im= threshold, then disk IAndroid and sdisk > sdisk (lm lgpu , M3. + (bi/V + dswapout )[b + sdisk (cid:16) (cid:105) Case 4 (OS with sufficient RAM or low disk speed): In these cases, the physical RAM is large enough to hold the model weights or the disk speed is too slow (i.e., sdisk threshold). As result, no disk loading is expected, except for the latency incurred during lookup table access, thus = bi disk , M4. sdisk < sdisk With these cases, we can rewrite the objective function as follows: (cid:88) qQ (cid:88) = + 1,out scpu,q 1 + bi/V + bo cpu + bi/V sdisk 1 + bo sdisk 1 I1 /M4 (21) (cid:104) (lm lgpu ) (cid:16) (cid:88) scpu,q + tkv_cpy,cpu + + 2(hkek + hvev)nkv cpu (cid:17) mM qQ + lgpu (cid:16) (cid:88) qQ m sgpu,q + tkv_cpy,gpu + + 2(hkek + hvev)nkv gpu (cid:17) (cid:16) + Wm (tram-vram + tvram-ram )(1 IUMA ) + tcomm (cid:17)(cid:105) (cid:88) + mM lmb sdisk + (cid:88) (cid:104) (lm lgpu )[b + 2(hkek + hvev)nkv] mM1M sdisk + ccpu davail dswapout sdisk IAndroid (cid:105) To further simplify the objective function, we make the following assumption. Assumption 1. Let an equal number of windows and all the windows are filled. be an integer, i.e., = 0, where = (cid:80) mM wm, all devices are assigned qQ m scpu,q +tkv_cpy,cpu = nmL , βm = (cid:80) , lgpu Now, we have lm = wmL (cid:80) + cpu )(1 IUMA + tvram-ram IAndroid dswapout ccpudavail sdisk . Let = + 2(hkek + hvev)nkv, αm = + , cpu gpu I1 /M4 + , where αm, βm, ξm are platform-specific constants and κ is global constant. Then, we add the first general term to the three platform-specific terms and obtain: , Wm = (cid:80) sgpu,q , κ = (cid:80) ξm = (tram-vram (cid:80) +tkv_cpy,gpu + bi/V +bo cpu 1 tkv_cpy,cpu + bi/V sdisk 1 ) + tcomm scpu,q 1,out scpu,q 1 + bo sdisk mM1M3 qQ qQ qQ T = + W (cid:88) (cid:104) (αm + mM1 (cid:88) (cid:104) mM3 (αm + sdisk sdisk )wm + ξm (cid:105) + (cid:88) (cid:104) (αm + mM2 sdisk )wm + βmnm + ξm (cid:105) )wm + (βm sdisk )nm + ξm (cid:105) + (cid:88) (cid:104) mM4 αmwm + βmnm + ξm (cid:105) + κ. This objective is sum over three sets, M1, M2, M3. Each summand involves expressions linear in wm and nm, plus constant terms specific to the platform. To clarify the form, we define linear function (a, b, c) = awm + bnm + c, where the platform-specific constants a, b, are independent of the decision variables wm and nm. Consequently, the objective can be rearranged to combination 17 of linear functions: ="
        },
        {
            "title": "L\nW",
            "content": "(cid:104) (cid:88) (αm + mM1 (cid:88) (αm + + sdisk sdisk , 0, ξm) + (cid:88) mM2 (αm + sdisk , βm, ξm) , βm sdisk , ξm) + (cid:88) (αm, βm, ξm) (cid:105) + κ mM3 mM4 Noted that the objective function is not linear because = (cid:80) variables, and the term 1 optimization problem. mM wm is the sum of decision introduces nonlinear dependency, which makes the problem nonlinear (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (34) (35) (36) (37) Now, we put it all together to present final model: min wm,nm s.t. wm Z>0, nm Z0, nm wm L, = kW, Z>0, (cid:88) = wm, mM (a, b, c) = awm + bnm + c, 4 (cid:91) 4 (cid:92) Mi = , Mi = M, i= i=1 + dswapout IAndroid bcio ), M3, ), M4, m,metal bcio cgpu), M4, ), M1, cgpu), M2, wm > wm > bcio Lb (davail m,metal bcio Lb (davail Lb (davail wm ImacOS (no Metal) < wm nm > bcio wm ImacOS (with Metal) < Lb (davail Lb (davail (wm nm)(ILinux + IAndroid) < Lb (davail = (bi/V + bo) Im=1 + ccpu, bcio Lb (davail nm Imetal Lb (davail nm = 0, if Icuda = 0 and Imetal = 0. m,cuda cgpu) Icuda, nm Icuda m,metal cgpu bo Im=1) Imetal, + dswapout IAndroid bcio ), M4, (33) Constraint (23) requires that the window size wn must be positive integer, the number of GPU layers nm must be non-negative integer, and nm cannot exceed wm. Constraint (24) requires that all devices are assigned an equal number of windows and all the windows are filled. This is not mandatory in our implementation, but can simplify the problem model. Constraint (28-30) ensure that devices categorized into sets M1, M2, M3 meet the memory condition outlined in Cases 1-3. Similarly, Constraints (31-33) ensure that devices assigned to set M4 meet the memory condition outlined in Case 4. bcio in eq. (34) is platform-independent constant. Constraints (35-36) ensure that the VRAM used by CUDA or the shared memory used by Metal does not exceed the available capacity. Here, davail m,metal denotes the maximum working set size recommended by Metal. Note that the output layer weights (of bo bytes) are kept in Metals shared memory but run on the CPU by default. m,cuda denotes the available GPU private memory for CUDA, and davail This is an integer linear fractional programming (ILFP) problem because the numerator is linear function of the decision variables wm, nw and the denominator is also linear function of wm. Moreover, the constraints are linear inequalities. Even when indicator variables 18 ImacOS, ILinux, IAndroid, Icuda, Imetal, Im=1 appear, each devices platform is known in advance, so these indicators are fixed, they just activate or deactivate certain linear constraints for each device. Next, we transform the above model into its vectorized form. Let the decision variables be wT = [w1, w2, , wM ], nT = [n1, n2, , nM ], and the coefficients a, b, be: = αm + sdisk αm + sdisk αm + sdisk M1 M2 αm M4 , = 0 M1 βm M2 βm sdisk , = ξm M1 ξm M2 ξm M3 βm M4 ξm M4 . M4 M4 M4 M4 M4 M4 M4 , 02 , 3 , 2 To apply constraints to the subset of and corresponding to M1, M2, M3, M4, we define diagonal matricies Pw = diag(IM1, IM2, IM3 , 1 ), Pn = diag(0M1, 0M2 , IM3, 01 , P3 ), where IM1, IM2, IM3 are identity matrices and are zero matrices corresponding to the subsets M1, M2, M3, 0M1 , 0M2, 0M3 and 1 , 3 , 2 are diagonal binary matricies (i.e., selection matricies) corresponding to the M4 three constraints (31-33) within the subset M4. To construct 1 , we define binary vector pmacOS, where value of 1 indicates that the current device is running on macOS and value of 0 indicates otherwise. The number of elements in pmacOS matches the number of devices in the set M4. Similarly, we define binary vectors pLinux, pAndroid, pmetal. Thus, we have = diag(pmacOS (1 pmetal)), 2 1 To handle constraints (35-36), we define gpu to one for devices with CUDA or Metal support. Specifically, we let gpu , cuda cuda n + metal = cuda = diag(0M1, IM2, 0M3, metal ). M4 as similar diagonal binary matrix, with elements set , where = diag(0M1, 0M2 , cuda M3 = diag(pmacOS pmetal), 3 = pLinux + pAndroid. ) and metal , 2 , 3 M4 M4 M4 M4 M4 Let the decision variables be wT M4 ], nT = [nT, nT [wT, wT , wT M4 M4 M4 = [wm M4], nT , nT ], the RAM upper bound be bcio davail M4 M4 = [nm M4], wT = = 1 Lb m,metal bcio davail cgpu M2 + dswapout davail IAndroid bcio M3 davail + bcio M4 davail m,metal + bcio + cgpu M4 davail dswapout IAndroid + bcio , and the VRAM/shared memory upper bound be zgpu = [zgpu 0, m,cuda cgpu, davail m,metal cgpu, davail m,metal cgpu do, davail 1 Lb zgpu = if Icuda = 0 and Imetal = 0, if Icuda = 1, if Imetal = 1 and = 1, if Imetal = 1 and = 1. 1 , , zgpu ], where The problem model can then be reformated as: min w,n s.t. aT + bT + eT eT + κ, wm Z>0, nm Z0, nm wm L, k(eT w) = 0, Z>0, Pw + Pn + eT < 0, zgpu eT + gpu gpu 0. 19 (38) (39) (40) (41) (42) Table 5: Summary of key symbols and their explanations. Symbol wm nm lm lgpu hk, hv ek, ev b, bi, bo nkv davail ccpu, cgpu sdisk sdisk threshold M1, M2, M3, M4 a, b, Pw, Pn gpu w, z, zgpu m=1 wm). Explanation Number of devices. Layer window size on device dm. Number of GPU layers on device dm. Token latency. Total model layers processed by device dm. Total GPU layers processed by device dm. Total number of model layers. Total layer window size across all devices (W = (cid:80)M Number of attention heads for keys and values. Embedding size per attention head. Embedding size. Bytes of weight data for each layer, input, and output. Number of tokens stored in key-value cache. Vocabulary size. Available memory on device dm. Buffer sizes for CPU/GPU computations. Disk read throughput for device dm. threshold for disk speed. considered too slow. Set assignments, corresponding to cases 1-4. Coefficient vectors for the objective function. Constraint coefficients for wm and nm, should be diagonal matrices. diagonal binary matrix that indicates whether device uses GPU. Extended vectors for and n. Vectors of RAM/VRAM upper bounds for constraints. If the disk speed is below this threshold, it is Table 5 summarizes the key symbols used in this paper. A.4 Run Prima.cpp with More Hot Models: Llama, Qwen, QwQ and DeepSeek Figure 7 provides clear comparison, showing that for models larger than 30B, our prima.cpp always has the lowest token latency and TTFT. We also support Qwen-2.5, QwQ, and DeepSeek-R1 in prima.cpp. The results are given in Table 6. Figure 7: Comparison of token latency and TTFT for llama.cpp, exo, dllama, and prima.cpp. A.5 Select Devices to Build the Most Powerful Cluster Existing systems require the cluster with enough RAM/VRAM, users have to gather more devices to run larger models. However, collecting enough devices is challenging. Here we raise two questions: 20 Table 6: Token latency (in millisecond/token) for Qwen, QwQ and distilled DeepSeek R1 models. Model llama.cpp exo1 dllama prima.cpp Qwen-2.5-7B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B Qwen-2.5-14B DeepSeek-R1-Distill-Qwen-14B 14 14 14 23 24 86 682 772 317103 234753 - - 435 - - 44 52 59 65 76 89 Qwen-2.5-32B and QwQ-32B 93 DeepSeek-R1-Distill-Qwen-32B 724 DeepSeek-R1-Distill-Llama-70B 867 Qwen-2.5-72B 1 Exo supports only the MLX backend for these models, so only D1 works. 2 Latency decreases because exo provides full-precision Qwen models but 3-bit 224 232 10978 OOM OOM OOM OOM - - - - quantized DeepSeek models. 3 Latency spikes because exo is swapping data in/out from disk. (a) should we collect enough devices to meet the models needs? (b) Is more devices always better? The answer is no. Figure 8 shows how layer assignment adjusts and token latency varies on Llama 3-70B as devices reduce from 6 to 1. Figure 8: Layer assignment and token latency over different number of devices. For question (a), with only 3 devices (D2, D3, and D5), the total available RAM+VRAM is 38 GiB, which is insufficient to hold the 40 GiB Llama 3-70B (Q4K) model. However, thanks to the fast SSDs on D2 and D3, mmap can swap model layers in flash, and prima.cpp achieves the lowest latency. Thus, prima.cpp does not require enough memory to hold the entire model. For question (b), when we increase the number of devices to 6, the total available RAM+VRAM reaches 50 GiB, enough to hold the entire model. However, token latency is lower with just 3 devices, as the additional devices (D4 and D6) have weak CPUs and slow disks, creating bottlenecks. This shows that more devices do not always result in faster inference. This raises new question: (c) If user has device with weak CPU or slow disk, should it be added to the cluster? Intuitively, such weak device would be bottleneck. However, in cases of severe memory shortage, it actually help. For example, D2 and D3 are devices with GPU, strong CPU, and fast disk, while D5 is weak device. As shown in Figure 8, adding D5 reduced token latency by half, as the disk loading latency on D3 (which was heavily overloaded) became more problematic than D5s computing. This raises more complex questions: if users have some weak devices, which ones should be added? More generally, (d) given set of heterogeneous devices, how can we select subset to build the best-performing cluster? This is challenging due to the uncertain number of devices to be selected, the highly heterogeneous cluster, and the various factors like CPU, GPU, 21 RAM, VRAM, disk, network, and even OS that significantly affect inference speed. Fortunately, prima.cpp offers an easy solution: start by including all devices in the cluster, then remove those with only one assigned layer or fewer than set threshold, as Halda identifies them as drags. Future updates will automate this process for easier to use. A.6 Efficient Workload Distribution and Memory Usage Figure 9 shows each devices RAM and VRAM usage to illustrate why prima.cpp achieves faster speed and prevents OOM. As exo and dllama dont support Llama 14B-65B and encounter OOM at 70B, the memory usage for 14B-70B in Figures 9b and 9c is estimated based on system behavior and memory load at 8B. Figure 9a has been discussed in Section 4.1. Figures 9b and 9c show that exo and dllama consume high memory. Exo mixes multiple backends, using MLX on macOS for 4-bit computation and Tinygrad on Linux, where model weights load in 16-bit on the CPU and decoded to 32-bit on the GPU. In our case, D1(8 GiB UMA RAM) and D2 (8 GiB VRAM) get the same number of model layers, yet D2-CPU uses 4 more RAM and D2-GPU 8 more VRAM than D1-GPU. This results in high memory usage on Linux devices, increasing the risk of OOM. For dllama, it uses tensor parallelism and Q40 quantization to distribute and compress memory usage but lacks GPU support, so all memory load is on RAM and inference speed is limited. It has similar memory usage across devices due to its uniform tensor splitting, which causes problems on low-memory devices. In Figure 9c, when running 30B model, D2/D3 have more available RAM, while D1/D4 have less. To allocate enough memory, D1/D4 must free more active pages or swap out app data, slowing user apps or even the system. In such cases, OOM might be the better outcome. Additionally, D3 (the head device) loads the entire model before slicing and distributing it, taking significant RAM and making it more prone to OOM. In contrast, prima.cpp optimizes workload distribution with Halda and prevents memory waste with mmap. Though the solution to (1-5) is unobvious, we can observe Haldas preference from Figure 9d: powerful GPUs > weak GPUs > powerful CPUs > fast disks. For example, at 8B30B, Halda first fills D2-GPU and D3-GPU. At 45-65B, it fills D1-CPU to D4-CPU. Lastly, the remaining layers are placed on D2-CPU and D3-CPU because they have fast disks. This assignment prevents weak CPUs and slow disks from being used. Finally, only D2-CPU and D3-CPU experience RAM overload, but this does not cause OOM because the OS will free inactive mmap-ed pages instantly and prefetch model layers in advance. With fast disk reads, disk loading latency stays low, ensuring minimal token latency, which is exactly the result of our optimization goal (1). Beyond the advanced workload distribution, prima.cpp also prevents memory waste. With mmap, it loads only required model layers instead of the full model, eliminating the need for model slicing. Additionally, it supports model inference in Q4K format across heterogeneous platforms, eliminating the need to decode back to 16/32-bit, so RAM/VRAM usage is further reduced. 22 (a) llama.cpp (b) exo (c) dllama Figure 9: Comparison of memory usage on each device for llama.cpp, exo, dllama, and prima.cpp. (d) prima.cpp"
        }
    ],
    "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",
        "University of Electronic Science and Technology of China, Chengdu, China"
    ]
}