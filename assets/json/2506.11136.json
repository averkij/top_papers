{
    "paper_title": "JAFAR: Jack up Any Feature at Any Resolution",
    "authors": [
        "Paul Couairon",
        "Loick Chambon",
        "Louis Serrano",
        "Jean-Emmanuel Haugeard",
        "Matthieu Cord",
        "Nicolas Thome"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 6 3 1 1 1 . 6 0 5 2 : r JAFAR: Jack up Any Feature at Any Resolution Paul Couairon1,2 Loïck Chambon1,3 Louis Serrano1 Jean-Emmanuel Haugeard2 Matthieu Cord1,3 Nicolas Thome1 1Sorbonne Université, CNRS, ISIR, F-75005 Paris, France 2Thales, TSGF, cortAIx Labs, France 3Valeo.ai Figure 1: JAFAR upsamples features from any foundation vision encoder to any image resolution, using the input image as high-resolution guidance. It generates sharp, boundary-aligned feature maps and serves as versatile drop-in module for variety of downstream tasksincluding semantic segmentation, open-vocabulary segmentation, depth estimation, CAM evaluation, and birds-eye-view segmentationconsistently enhancing performance."
        },
        {
            "title": "Abstract",
            "content": "Foundation Vision Encoders have become essential for wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFARa lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attentionbased module designed to promote semantic alignment between high-resolution queriesderived from low-level image featuresand semantically enriched lowresolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across diverse set of downstream tasks. Project page: https://jafar-upsampler.github.io Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Foundation vision encoderswhether trained with language supervision [1, 2, 3, 4, 5] or purely on visual data [6, 7, 8]have become core components of modern computer vision pipelines. Vision-language models excel at tasks requiring generalization, such as zero-shot classification and open-vocabulary segmentation [9, 10]. In contrast, image-only models, which focus on visual structure, often outperform in dense prediction tasks that demand fine-grained spatial reasoning, including semantic segmentation, depth estimation, object discovery, and point tracking [11, 12, 13]. To handle high-resolution inputs and large-scale training, foundation vision encoders typically downsample spatial information aggressivelyby factor of 14 to 16yielding semantically rich but spatially coarse feature maps. This compression introduces bottleneck for downstream tasks that require pixel-level accuracy. As result, downstream pipelines [14, 15, 11, 16, 17] often rely on interpolation or dedicated modules [18, 19] designed to produce high-resolution outputs. Several strategies have been explored to overcome this bottleneck, but each comes with trade-offs in efficiency and output quality. straightforward solution is to apply training-free interpolation methods, such as bilinear upsampling. While computationally efficient, these direct interpolationsrelying solely on low-resolution feature mapsfail to leverage information from the original high-resolution image, often resulting in blurry outputs. Alternatively, one can upsample the input image prior to encoding to increase feature resolution. However, this approach significantly increases computational cost due to the quadratic complexity of self-attentioncommon in foundation modelsand may introduce artifacts in the feature maps, ultimately degrading performance [20, 21]. Focusing specifically on target downstream task, [22, 23, 24, 25, 26] learn feature upsamplers using high-resolution supervision from task-specific labels. While generally lightweight, these upsamplers depend on labeled data tied to the end application, which limits their generalization and may bias the learned features toward optimizing task-specific losses. To address this, recent methods such as LiFT [27] and FeatUp [28] adopt task-agnostic training objectives. LiFT is trained to perform 2 upsampling by regressing feature maps extracted from images at twice the input resolution. However, its convolution-based architecture is limited to fixed 2 scaling, restricting its flexibility for arbitrary output resolutions. FeatUp, in contrast, uses augmented views and self-reconstruction to support higher upsampling ratios. Yet, its Joint Bilateral Upsampling (JBU) variant suffers from over-smoothed outputs, while its implicit variant requires training the upsampler for each image, making it impractical in real-world scenarios. In this paper, we introduce feature upsampler designed to satisfy the following criteria: (i) task-agnostic training objective, (ii) support for arbitrary output resolutions, (iii) compatibility with any vision encoder, and (iv) minimal computational overhead at inference time. To enable upsampling to arbitrary target resolutions, we formulate our approach as global interpolation mechanism using cross-attention block. The success of this attention-based method depends critically on achieving strong semantic alignment between the queries and keys. In JAFAR, we construct these representations asymmetrically (see Fig. 2): the queries retain high-resolution, low-level details such as color and texture, while the keys are hybrid features that combine high-level semantics with spatial cues. We find that enriching the keys with low-level information significantly improves query-key alignment and enhances generalization to unseen output resolutions. Additionally, we propose simple training objective similar to [27], but without being constrained to fixed upsampling factor. Notably, we find that training on low upsampling factors at low resolutions (e.g., 8 8 32 32) is sufficient to generalize effectively to much larger scales (e.g., 32 32 448 448) while keeping memory requirements low during training, unlike training directly at higher resolutions and factors. Our contributions can be summarized as follows: We introduce JAFAR, novel lightweight attention-based feature upsampler that naturally supports upsampling to arbitrary resolutions. It explicitly promotes spatial alignment between high-resolution queries extracted from low-level image features and semantically enriched low-resolution keys. We enforce this alignment by computing both queries and keys from the same input features, and injecting semantic information from the encoders deep features via spatial feature modulation. This design enables precise fusion of spatial detail and semantic context without reliance on external supervision. 2 We propose highly efficient, task-agnostic training objective that requires no highresolution supervision signal. Remarkably, we show that training at low resolutions and low upsampling ratios generalizes robustly to significantly higher output scales. We demonstrate that the combination of our architecture and training objective yields substantial performance gains across variety of downstream tasks. When used as drop-in module, JAFAR consistently outperforms existing upsampling methods by wide margin."
        },
        {
            "title": "2 Related Work",
            "content": "Feature Upsampling Feature upsampling aims to increase the spatial resolution of intermediate feature maps within deep networksanalogous to image upsampling, but performed in latent space. This process is essential for dense prediction tasks such as segmentation and depth estimation, where fine spatial detail is critical. Traditional interpolation techniques, such as bilinear, spline, or Lanczos [29, 30, 31, 32], provide simple and efficient baselines but do not adapt to the underlying content. Recent neural methods improve on static approaches by learning to reconstruct high-resolution features from data. These methods fall into two categories: task-dependent, trained with downstream labels supervision, and task-agnostic, trained independently of the end task. For example, CARAFE [22] and DySample [24] predict content-aware kernels or dynamic sampling positions. SAPA [23] and ReSFU [25] exploit similarity based approach to refine spatial semantics. However, task-specific reliance on labels limits generalization. Recent task-agnostic methods like LiFT [27] and FeatUp [28] remove this dependency. LiFT introduces CNN module trained with simple fixed scale training objective, while FeatUp relies on complex multi-loss objective which makes training difficult to tune in practice. Moreover, it requires training both an upsampler and downsampler, adding unnecessary computational overhead. Notably, its best performance is achieved through per-image optimization, further limiting its practicality. In contrast, JAFAR provides scalable task-agnostic framework that generalizes across resolutions without complex pipelines or per-image optimization, showing strong performance even when trained on small upsampling factors at low resolution. Architectural Design for Upsampling Modules Upsampling modules architectures vary from fixed-scale decoders to continuous resolution predictors. LiFT [27] relies on lightweight CNN module trained to upsample by fixed factor, making further scaling dependent on iterative use which leads to performance degradation or additional interpolation steps. FeatUp [28] introduces two architectural variants: fast Joint Bilateral Upsampler (JBU) and more accurate implicit network allowing continuous querying. While the implicit model yields superior results, it suffers from significant inference latency due to per-image optimization. JBU, on the other hand, trades expressivity for scalability, stacking multiple 2 stages to achieve higher upsampling ratios. Attentionbased designs, as in SAPA [23] and ReSFU [25], offer increased flexibility by modeling affinities between features across scales. These methods exploit spatial similarities to reconstruct highresolution maps. JAFAR innovates by unifying lowand high-resolution streams: it aligns highresolution queries and low-resolution keys using shared low-level features while enriching the representation with additional semantic cues. This design maintains spatial alignment and expressivity even at large upsampling ratios, offering robust and scalable architecture for feature reconstruction. Semantic Guidance and Feature Modulation Feature modulation techniques modulate features using conditioning information, thereby enabling spatially or semantically guided transformations. Early forms such as Conditional BatchNorm [33], AdaIN [34], and FiLM [35] apply learned scale (γ) and shift (β) parameters per channel, derived from global conditioning signals. These methods are effective for tasks involving global transformations like style transfer or classification. However, their spatial invariance limits expressiveness in tasks requiring spatial sensitivity. SPADE [36] and SFT [37] address this limitation by computing γ and β as full-resolution maps conditioned on dense inputs like segmentation masks. This spatial adaptation improves expressivity by allowing each feature location to be modulated uniquely. Moreover, this form of modulation can be interpreted as parameterized, learned recombination of feature channels, similar to 1 1 convolutions but more powerful due to the spatial specificity. In JAFAR, modulation is employed not merely to shift feature distributions but to enable semantically enriched reconstruction by injecting high-resolution semantics directly into the upsampling pipeline. This allows richer linear combination of features, improving generalization and spatial expressivity without relying on per-pixel optimization at test time [28]."
        },
        {
            "title": "3 JAFAR",
            "content": "JAFAR is feature upsampler that uses the input image as high-resolution guidance to reconstruct dense feature maps. To support upsampling to arbitrary target resolutions, we formulate the method as global interpolation mechanism based on cross-attention. The effectiveness of this attentionbased approach hinges on achieving strong semantic alignment between the queries and the keys K. In JAFAR, we construct the query and key representations asymmetrically. The queries retain high-resolution, low-level details such as color and texture, while the keys are designed as hybrid representations that combine high-level semantics with low-level spatial cues. We find that enriching the keys with low-level information significantly improves query-key alignment and enhances generalization to unseen output resolutions. 3.1 Architecture The overall flow of our architecture is illustrated in Fig. 2. JAFAR takes as input high-resolution image R3HW and low-resolution feature map Flr = (I) RChkwk , extracted from frozen vision encoder . The image is first projected into higher-dimensional space and processed by lightweight encoder Eθ to obtain an intermediate representation IE = Eθ(I) RdHW , further enriched with RoPE positional embeddings [38]. Figure 2: Overview of JAFAR. To construct the upsampling kernel, queries and keys are derived from shared image representation. Queries are downsampled to match the target output resolution, while keys are downsampled to align with the spatial resolution of the vision encoders features. Keys are then semantically enriched via SFT modulation to promote semantic alignment between queries and keys. The resulting kernel is then used to interpolate features from the foundation vision encoder. Query features Rdhqwq are derived by passing the image representation IE through small query encoder, producing IQ, followed by adaptive average pooling to reach the target resolution (hq wq). Key features Rdhkwk are similarly obtained by encoding IE to IK and downsampling it to match the spatial resolution of the semantic features Flr. These semantic features provide modulation parameters that inject high-level information into the keys. cross-attention mechanism then enables the queries to attend to the keys by computing an attention map: = Softmax (cid:18) (cid:19) , (1) which is then used to interpolate the low-resolution feature map Flr and produce the upsampled output features ˆFHR = Flr RChqwq . The resulting representation preserves fine-grained spatial details while remaining semantically consistent with the input image. We provide detailed description of each of the main components of the architecture below. Query Branch Directly aligning high-resolution, low-level queries with high-level semantic keys often results in weak or noisy attention, as the disparity in abstraction levels limits meaningful interactions. To overcome this challenge, we apply adaptive average pooling to downsample the intermediate representation IQ and generate the query features Q. This operation, performed exclusively during 4 training, reduces the spatial resolution of the queries while aggregating local context into region-level descriptors. As result, the downsampled queries are more semantically aligned with the keys, less susceptible to pixel-level noise, and computationally more efficient due to the reduced number of tokens. These effects collectively make query downsampling an effective strategy for bridging the gap between fine-grained visual details and abstract semantic representations, promoting more stable and scalable cross-scale attention. Importantly, because downsampling is only applied during training, the model maintains its capacity to generate high-resolution outputs during inference. Key Branch Relying exclusively on low-resolution features from the vision encoder to construct keys leads to poor generalization and noticeable artifacts, primarily due to an abstraction gap between these coarse features and the fine-grained queries. As demonstrated in Sec. 4, this mismatch results in inconsistent alignment across resolutions. To address this issue, we construct hybrid key representations that retain structural alignment with the queries while incorporating the semantic richness of the vision encoder. This is achieved by encoding the intermediate representation IE to produce IK, which is then downsampled to match the spatial resolution of the encoders feature map to produce preliminary keys K. These are further modulated using the encoder feature map Flr RChkwk through spatial semantic feature modulation inspired by [36, 37]: = γF + βF , (2) where γF , βF Rdhkwk are spatially varying parameters obtained via linear projections from Flr. This adaptive, feature-wise modulation enriches the keys with localized semantic context, enhancing both spatial and semantic alignment and supporting more faithful and generalizable upsampling across resolutions. Similarity Based Upsampling To perform upsampling, we use simplified attention mechanism where attention weights are computed via scaled dot product between queries and semantically modulated keys. Crucially, both queries and keys have been been enriched with relative positional embeddings using RoPE [38], which introduces an inductive bias that captures spatial relationships between queries and keys. This positional encoding allows us to entirely bypass the arbitrary selection of neighboring keys for each query, common heuristic in prior similarity-based methods such as [23, 25]. Without this positional grounding, the attention mechanism lacks spatial awareness and generalizes poorly to unseen resolutions. In practice, we use multiple attention heads to increase expressivity and average the resulting attention weights across heads after applying softmax. The resulting attention map is then used to interpolate the low-resolution encoder features Flr via simple matrix product: ˆFHR = Flr. By avoiding learned value projection, we preserve the original feature content and enable resolution-agnostic design that generalizes reliably across scales. 3.2 Training Pipeline Learning to upsample high-resolution features without access to ground-truth supervision poses natural challenge: how can model learn to produce sharp high-resolution features (e.g., 448 448) when only low-resolution features are available (e.g., 32 32)? Thanks to JAFARs architectural design, the model can be trained with simple objective at low target resolution without requiring supervision at the original image size, yet it still generalizes effectively to much higher upsampling ratios during inference. Training with Multi-Resolution Views To enable this, we introduce fully annotation-free training scheme that relies only on multi-resolution views of the same image, easily obtained through standard downsampling. Given high-resolution image IHR R3HW , we generate downsampled version ILR R3 δ using randomly sampled factor δ [2, 4]. Both images are passed through the frozen vision encoder , producing two feature maps: Fhr = (IHR) RChw and Flr = (ILR) RC δ , respectively. JAFAR then takes IHR and Flr as input to predict an upsampled feature map ˆFhr. The predicted output is aligned with the target Fhr using simple alignment loss, which combines cosine similarity and L2 distance [20]: δ δ L( ˆFhr, Fhr) = 1 cos( ˆFhr, Fhr) + ˆFhr Fhr2. (3) Notably, during training, JAFAR is only exposed to moderate upsampling factors (up to 4), yet it generalizes remarkably well to much higher resolutions at test timewithout access to any groundtruth high-resolution features. 5 How is it different from LiFT? While our training objective is similar to that of LiFT, our approach demonstrates significantly greater capability, as shown in Tabs. 1 and 2. LiFT relies on CNN-based architecture and is trained for fixed 2 upsampling at two predefined resolutions. As result, it struggles to extrapolate beyond that setting without additional heuristics such as iterative upsampling or bilinear fallback. In contrast, JAFAR maintains resolution-agnostic design which generalizes to much higher upsampling factors using this similar simple training setup."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup In our experiments, we train JAFAR on single NVIDIA A100 on ImageNet training set for 100K steps using AdamW optimizer [39], with learning rate of 2e4 and batch size of 4. The input images fed into the foundation vision encoder are resized to 448 448, producing high-resolution target feature maps Fhr of size 32 32 or 28 28, depending on the encoders patch size (14 or 16). For improved training efficiency, the guidance image input to JAFAR is downsampled to 224 224. 4.2 Qualitative Comparisons Figure 3: PCA Feature Visualization. DINOv2 ViT-S/14 features at 32 32 resolution from the ImageNet validation set are upsampled to 448 448. Baseline methodswhether training-free, task-dependent, or task-agnosticintroduce varying levels of blurriness and artifacts. Besides being task-agnostic, JAFAR produces sharp, content-aware feature maps with fewer artifacts. To qualitatively evaluate the upsampled feature maps produced by various baselines, we project all features onto shared 3-dimensional PCA basis, mapping them into common RGB space. As shown in Figs. 3 and 5, the low-resolution featuresdue to the spatial compression imposed by the vision encoders patch sizereveal large, blocky regions that capture semantic content but fail to preserve fine image geometry, object boundaries, or shape details. Bilinear upsampling, which interpolates features without considering image content, yields blurry output feature maps that preserve positional embeddings artifacts without adding meaningful detail. While methods like Large-Image and Strided preserve sharpness, their outputs are noisier and less coherent than JAFARs. Furthermore, they are more computationally demanding, as they require the vision encoder to process larger number of patches (see Tab. 10). JAFAR shows clear qualitative advantage over all baselines, consistently producing sharp features that accurately capture image structure. It is also the only task-agnostic method that effectively suppresses artifacts from positional embeddings in the low-resolution features. 4.3 Transfer on Downstream Tasks Since upsampled features are expected to provide richer signal for downstream tasks, we evaluate their effectiveness on two benchmarks: linear-probing semantic segmentation and depth estimation, using DINOv2 ViT-S/14 as the foundation vision encoder. For the Large-Image and Strided baselines, upsampling is performed during the encoders forward pass and followed by bilinear interpolation to reach the target output resolution. For task-agnostic upsamplers such as LiFT, FeatUp, and JAFAR, we pre-train the upsampling module on the corresponding backbone, then freeze it and apply it after feature extraction. The linear probe is trained independently of the upsampler. For task-dependent 6 methodsincluding CARAFE, SAPA, ReSFu, and DySamplewe jointly train both the upsampler and the linear probe on each dataset and task. All experiments (except Large-Image) use input images of resolution 448 448, with target labels at the same resolution. 4.3.1 Semantic Segmentation For semantic segmentation, we train linear projection head to predict coarse class labels using cross-entropy loss across several benchmark datasets: COCO-Stuff [40] (27 classes), ADE20K [41] (150 classes), Pascal VOC [42] (21 classes including background), and Cityscapes [43] (27 classes). The linear layer is trained for 5 epochs on COCO-Stuff and 20 epochs on the remaining datasets, using batch size of 4. Performance is evaluated on the respective validation sets using mean Intersection-over-Union (mIoU) and pixel-wise accuracy. Table 1: Linear Probing on Downstream Tasks. JAFAR consistently outperforms other baselines across all segmentation benchmarks while reaching competitive depth metrics without being optimized on specific downstream task. DINOv2-ViT-S/14 Training-free Nearest Bilinear Large Image (x8) Strided Task-Dependent CARAFE [22] SAPA [23] DySample [24] ReSFU [25] Task-Agnostic FeatUp [28] LIFT [27] JAFAR Semantic Segmentation Depth Estimation COCO VOC ADE20K Cityscapes COCO mIoU () Acc () mIoU () Acc () mIoU () Acc () mIoU () Acc () δ1 () RMSE () 56.17 59.03 55.93 59.73 57.77 59.50 60.08 60.10 58.18 60. 76.97 79.07 77.40 79.65 78.28 79.42 79.84 79.95 78.95 80.47 76.41 80.70 56.94 75.88 80.26 77.02 81.62 80. 81.08 78.06 84.44 93.80 95.17 88.60 93.94 95.14 94.07 95.48 95.05 95.32 94.62 96. 37.27 39.23 26.42 36.15 38.30 35.87 38.99 38.91 38.82 38.73 40.49 71.91 73.69 66.39 72.08 73.42 71.85 73.62 73. 73.74 73.69 74.92 54.05 59.37 47.72 59.26 56.05 50.12 59.71 55.53 56.06 58.75 61. 90.36 92.47 92.49 92.57 91.83 90.02 92.69 91.62 91.86 92.60 93.42 58.08 59.92 56.98 61.42 60.34 61.25 66. 61.69 57.04 62.18 0.70 0.66 0.70 0.64 0.67 0.64 0.56 0.64 0.70 0. As shown in Tab. 1, JAFAR consistently achieves the highest performance across all four semantic segmentation benchmarks, in both mIoU and accuracy. On average, JAFAR delivers +1.63 mIoU improvement over the next-best method across all datasets. Compared to FeatUp, JAFAR achieves an average gain of +2.78 mIoU corresponding to +4.8% gain, with peak improvement of +5.41 mIoU (+9.7%) on Cityscapes. Fig. 4 shows linear probe segmentation result. 4.3.2 Depth Estimation For depth estimation, we follow the approach in [28] and train on pseudo-labels generated by the state-of-the-art Depth Anything V2 network [16]. We report two standard metrics from the monocular depth estimation literature: root mean square error (RMSE) and δ1 < 1.25. The δ1 metric measures the percentage of pixels where the predicted depth is within 25% of the ground-truth y, formally < 1.25. We train the linear probe for 5 epochs on the COCO training defined as δ1 = max set, using batch size of 4. Although JAFAR was not trained on this specific task, we observe that it reaches competitive scores, ranking second among the baselines. Notably, JAFAR outperforms both FeatUp and LiFT while also surpassing all task-dependent methods but ReSFU. Fig. 4 shows linear probe depth estimation result. (cid:16) , (cid:17) 4.3.3 Class Activation Maps Faithfulness Following the approach in [28], our method can be seamlessly integrated into explainability tools such as Class Activation Maps (CAMs). Despite recent advances, CAMs are still fundamentally limited by the low-resolution feature maps produced by standard vision encoders, which hinders their ability to localize fine-grained details. By upsampling the features, our method yields sharper and more informative explanations. To assess the quality of the resulting CAMs, we adopt standard evaluation metrics from the literature: Average Drop (A.D), Average Increase (A.I), Average Gain (A.G), Coherency (Coh.), and Complexity (Cplx.). 7 Figure 4: Visual Comparison of Upsampler Outputs in Downstream Tasks. JAFAR-upsampled features produce sharper outputs that align more accurately with object boundaries across various downstream tasks respectively class activations maps, semantic segmentation and depth estimation. A.D () A.I () A.G () Coh. () Cplx. () ADCC () Training-free Bilinear Large Image (x8) Strided Table 2: Grad-CAM Evaluation. Integrating JAFAR into Grad-CAM analysis yields significantly more faithful explanations compared to baseline methods. Top three methods are highlighted as first, second, third according to ADCC. In particular, A.D, A.I, and A.G measure how sensitive the classifiers output is to the most salient regions of the inputan effective CAM should highlight areas that, when masked, lead to notable change in classification confidence. Since each of these metrics captures only single aspect of CAM quality, we also report the ADCC scorean aggregate metric proposed in [44] that provides more holistic evaluation. Additional details are provided in Supp. B. As illustrated qualitatively in Fig. 4, JAFAR generates sharper and more semantically accurate CAMs compared to all baselines. While training-free methods dont help to recover important regions, task-dependent approaches typically produce blurrier and less precise maps. Quantitative results in Tab. 2 further support this, with JAFAR achieving the highest score on the aggregate ADCC metricoutperforming the second-best method by 8 points, relative improvement of 12.5%. Task-Dependent CARAFE [22] SAPA [23] Dysample [24] ReSFU [25] Task-Agnostic FeatUp [28] LiFT [27] JAFAR 59.7 (-13.6) 55.4 (-17.9) 61.6 (-11.7) 59.4 (-13.9) 61.7 (-11.6) 59.5 (-13.8) 65.3 (-8.0) 64.3 (-9.0) 53.0 (-20.3) 73.3 49.9 8.5 17.8 14.5 34.9 69.6 60.7 64.2 4.8 32.7 20.0 24.0 66.9 96.5 90.2 92.1 19.0 48.8 19. 15.3 66.9 17.4 88.8 67.9 85.9 91.6 65.2 91.4 18.5 12.8 15.1 59.9 38.0 54.2 58.2 9.4 44. 24.0 8.7 30.9 1.0 4.1 3.8 3.7 3.4 2.5 3.5 4.3 2.3 6.5 4.3.4 Zero-Shot Open-Vocabulary Segmentation We further evaluate our method on zero-shot open-vocabulary segmentation task, following the setup from [9], where class labels from the dataset serve as textual inputs and predictions are made by selecting the class with the highest similarity score (argmax). Using CLIP-ViT-B/16 backbone, this approach is entirely training-free, as it does not require learned probing head. Results show that JAFAR significantly outperforms all baselines, with particularly strong improvements on Pascal VOC. Despite the increased difficulty of ADE20K, which includes 150 classes, our method still achieves the highest performance in both mIoU and accuracy. We report only FeatUp among the task-agnostic baselines, as it is the second-best performing method. Table 3: Zero-Shot Open-Vocabulary Evaluation. Using MaskCLIP [9] for zeroshot open-vocabulary segmentation, JAFAR consistently improves performance, indicating strong alignment with the original features. Upsampling Nearest Bilinear Large Image (x2) FeatUp [28] JAFAR VOC ADE20K Cityscapes mIoU () Acc () mIoU () Acc () mIoU () Acc () 24.13 27.87 23.24 32.27 35.70 30.80 35.27 32.16 39.78 44.93 9.33 11.03 8.08 13.03 13.61 24.65 27.78 24.94 33.28 33.28 19.66 21.56 21.91 24.76 25. 50.27 53.21 52.22 60.11 61.73 8 4.3.5 Birds-Eye View Segmentation Table 4: BeV Vehicle Segmentation. JAFAR consistently improves vehicle-IoU in complex BeV architectures, outperforming all other baselines. Finally, we studied the impact of our upsampler in complex training pipeline. The task takes several images taken from cameras as input and consists on outputting the birds-eye view (BeV) segmentation map. In our setup, we used frozen DINOv2 [7] backbone and trained the rest of the architecturenamely, the upsampler, the BeV encoder, and the segmentation head. This task is particularly challenging, as the model must learn to map features from the image plane to the BeV plane. To ensure fair comparison, we also trained the architecture without an upsampler, using lower-resolution input images (496224). We adopted the optimization hyperparameters from PointBeV [46], adjusting the batch size to 1 and training for 100 epochs. Our results show that using an upsampler consistently improves predictions, regardless of the architecture employedSimpleBev [45], PointBeV [46], or BevFormer [47]. Notably, performance improves significantly when using JAFAR as the upsampler, with mIoU gains up to +5 points. Low-Res Bilinear FeatUp JAFAR PointBeV [46] BeVFormer [47] 33.72 34.18 34.01 36.54 34.89 36.01 35.38 37.20 31.75 33.67 33.95 36.59 SimpleBeV [45] Upsampling 4.4 Ablations To evaluate the benefit of deriving both queries and keys from shared image encoding, we compare in Tab. 5 several key generation strategies. In the Linear Projection baseline, keys are obtained by applying simple linear layer to the low-resolution features Flr from the vision encoder, without using the image encoding. In the Concatenation baseline, we replace the modulation block with direct concatenation of Flr and the preliminary keys K. Injecting semantic information into the Table 5: Attention mechanism ablations with respect to key strategy and number of attention heads. Best scores per dataset are in bold and selected choices are highlighted in blue. Ablation Type / Setting Semantic Segmentation VOC ADE20K Cityscapes mIoU () Acc () mIoU () Acc () mIoU () Acc () Keys Strategy Linear Projection Concatenation SFT Modulation Attention Heads = 1 = 2 = 4 = 80.02 (-4.42) 83.13 (-1.27) 84.44 94.87 (-1.41) 95.94 (-0.34) 96.28 37.87 (-2.62) 40.06 (-0.43) 40.49 73.22 (-1.70) 74.56 (-0.36) 74.92 52.45 (-9.02) 58.70 (-2.77) 61.47 90.80 (-2.62) 92.65 (-0.77) 93. 84.13 (-0.31) 84.27 (-0.17) 84.44 83.82 (-0.62) 96.21 (-0.07) 96.27 (-0.01) 96.28 96.13 (-0.15) 40.15 (-0.34) 40.42 (-0.07) 40.49 40.07 (-0.42) 74.79 (-0.13) 74.95 (+0.03) 74.92 74.20 (-0.72) 60.94 (-0.53) 61.19 (-0.28) 61.47 60.56 (-0.91) 93.32 (-0.10) 93.42 (-0.00) 93.42 93.33 (-0.09) keys through feature modulation is crucial for generating high-quality features and achieving strong query-key alignment. In comparison, the linear projection baseline shows significant performance drop, and SFT consistently outperforms the concatenation approach. Increasing the number of attention heads up to 4 further enhances performance by producing more robust upsampling kernels through averaged post-softmax scores. Beyond this point, however, the benefits reverse: the perhead dimensionality becomes too low to support effective alignment, while the computational cost increases, ultimately degrading output quality."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce JAFAR, lightweight, attention-based feature upsampler designed with simple training objective. It can upscale features from any foundation vision encoder to arbitrary output resolutionswithout requiring supervision at the original image size or annotations from downstream tasks. Although task-agnostic, JAFAR outperforms prior state-of-the-art upsamplers across variety of downstream tasks, despite not being trained specifically for them. This work lays the groundwork for unified feature upsampler that could enable significantly more efficient architectures for dense vision tasks. Currently, the method requires training separate upsampler for each backbone. Future work will focus on making JAFAR backbone-independent at inference time and on further reducing feature-level artifacts to produce sharper outputs."
        },
        {
            "title": "References",
            "content": "[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [2] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [3] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [4] Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam Yala, Alane Suhr, Trevor Darrell, and David Chan. Tulip: Towards unified language-image pretraining. arXiv preprint arXiv:2503.15485, 2025. [5] Greg Heinrich, Mike Ranzinger, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, Pavlo Molchanov, et al. Radio amplified: Improved baselines for agglomerative vision foundation models. arXiv preprint arXiv:2412.07679, 2024. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [7] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. [8] Timothée Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Cluster and predict latents patches for improved masked image modeling. arXiv preprint arXiv:2502.08769, 2025. [9] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European Conference on Computer Vision, pages 696712. Springer, 2022. [10] Feng Wang, Jieru Mei, and Alan Yuille. Sclip: Rethinking self-attention for dense visionlanguage inference. In European Conference on Computer Vision, pages 315332. Springer, 2024. [11] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [12] Cijo Jose, Théo Moutakanni, Dahyun Kang, Federico Baldassarre, Timothée Darcet, Hu Xu, Shang-Wen Li, Marc Szafraniec, Michael Ramamonjisoa, Maxime Oquab, Oriane Simeoni, Huy V. Vo, Patrick Labatut, and Piotr Bojanowski. Dinov2 meets text: unified framework for imageand pixel-level vision-language alignment. ArXiv, abs/2412.16334, 2024. [13] Narek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel. Dino-tracker: Taming dino for self-supervised point tracking in single video. In European Conference on Computer Vision, pages 367385. Springer, 2024. [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 10 [15] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. [16] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:21875 21911, 2024. [17] Paul Couairon, Mustafa Shukor, Jean-Emmanuel HAUGEARD, Matthieu Cord, and Nicolas THOME. Diffcut: Catalyzing zero-shot semantic segmentation with diffusion features and recursive normalized cut. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [18] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1215912168, 2021. [19] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12801289, 2021. [20] Jiawei Yang, Katie Luo, Jiefeng Li, Congyue Deng, Leonidas Guibas, Dilip Krishnan, Kilian Weinberger, Yonglong Tian, and Yue Wang. Denoising vision transformers. In European Conference on Computer Vision, pages 453469. Springer, 2024. [21] Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, and Hongxia Yang. Vitar: Vision transformer with any resolution, 2024. [22] Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, and Dahua Lin. Carafe: Contentaware reassembly of features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 30073016, 2019. [23] Hao Lu, Wenze Liu, Zixuan Ye, Hongtao Fu, Yuliang Liu, and Zhiguo Cao. Sapa: Similarityaware point affiliation for feature upsampling. Advances in Neural Information Processing Systems, 35:2088920901, 2022. [24] Wenze Liu, Hao Lu, Hongtao Fu, and Zhiguo Cao. Learning to upsample by learning to sample. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 60046014, 2023. [25] Minghao Zhou, Hong Wang, Yefeng Zheng, and Deyu Meng. refreshed similarity-based upsampler for direct high-ratio feature upsampling. CoRR, 2024. [26] Hao Lu, Wenze Liu, Hongtao Fu, and Zhiguo Cao. Fade: Fusing the assets of decoder and encoder for task-agnostic upsampling. In Proc. European Conference on Computer Vision (ECCV), 2022. [27] Saksham Suri, Matthew Walmer, Kamal Gupta, and Abhinav Shrivastava. Lift: surprisingly simple lightweight feature transform for dense vit descriptors. In European Conference on Computer Vision, pages 110128. Springer, 2024. [28] Stephanie Fu, Mark Hamilton, Laura E. Brandt, Axel Feldmann, Zhoutong Zhang, and William T. Freeman. Featup: model-agnostic framework for features at any resolution. In The Twelfth International Conference on Learning Representations, 2024. [29] Einar Maeland. On the comparison of interpolation methods. IEEE transactions on medical imaging, 1988. [30] Claude Duchon. Lanczos filtering in one and two dimensions. Journal of Applied Meteorology, 1979. [31] Isaac Schoenberg. Cardinal spline interpolation. SIAM, 1973. [32] Sky McKinley and Megan Levine. Cubic spline interpolation. College of the Redwoods, 1998. 11 [33] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. learned representation for artistic style. In International Conference on Learning Representations, 2017. [34] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. [35] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In AAAI Conference on Artificial Intelligence, 2018. [36] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 23372346, 2019. [37] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 606615, 2018. [38] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [41] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302321, 2019. [42] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: retrospective. International Journal of Computer Vision, 111(1):98136, January 2015. [43] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. [44] Samuele Poppi, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Revisiting the evaluation of class activation mapping for explainability: novel metric and experimental analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22992304, 2021. [45] Adam W. Harley, Zhaoyuan Fang, Jie Li, Rares Ambrus, and Katerina Fragkiadaki. SimpleBEV: What really matters for multi-sensor bev perception? In IEEE International Conference on Robotics and Automation (ICRA), 2023. [46] Loïck Chambon, Éloi Zablocki, Mickaël Chen, Florent Bartoccioni, Patrick Pérez, and Matthieu Cord. Pointbev: sparse approach to bev predictions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, 2024. [47] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers. In Computer Vision - ECCV 2022, 2022. [48] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12091218, 2016. 12 [49] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. [50] Hanwei Zhang, Felipe Torres, Ronan Sicre, Yannis Avrithis, and Stephane Ayache. Opti-cam: Optimizing saliency maps for interpretability. Computer Vision and Image Understanding, 248:104101, 2024. [51] MMCV Contributors. MMCV: OpenMMLab computer vision foundation. https://github. com/open-mmlab/mmcv, 2018. [52] Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. [53] Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang. Loftup: Learning coordinate-based feature upsampler for vision foundation models, 2025. [54] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 13 JAFAR: Jack up Any Feature at Any Resolution Supplementary Material"
        },
        {
            "title": "A Class Activation Maps Evaluation",
            "content": "To evaluate Class Activation Maps (CAMs), we employ frozen pre-trained ViT-B/16 model as the backbone and extract Grad-CAMs. We randomly sample 2,000 images from the ImageNet validation set for which the model produces correct predictions. For each image, we compute the gradients with respect to the predicted class, average them, and use the result to weight the corresponding activation maps. The weighted activations are then summed to produce the final CAM. These activation maps are upsampled from 14 14 to 224 224, resulting in high-resolution CAMs. For each CAM, we generate masked version of the input image by applying binary mask that highlights regions positively associated with the models prediction. Formally, masked image is obtained as xmasked = 1CAMc(x)>0. These masked images are then used to compute the evaluation metrics. Average Drop Average Drop (A.D) quantifies how much the models confidence in the predicted class decreases when it is presented with the masked image instead of the full image. For single image, the metric is defined as: A.D = 1 (cid:88) i= Oc ) max(0, c 100, (4) where denotes the models output score for class when using the full image, and Oc denotes the score when using the masked version derived from the explanation map. The final A.D value is computed by averaging over set of images. Average Increase Average Increase (A.I) measures how often the models confidence in the predicted class is higher when using the masked image than when using the full image. It is defined as: A.I = 100 , (5) (cid:88) i=1 1Y <Oc is the models output score for class when using the full image, and Oc where is the score when using the masked image based on the explanation map. The metric reflects the percentage of images where the explanation-based input yields higher confidence score than the original image. Average Gain [50] Average Gain (A.G) quantifies the improvement in predictive confidence for the target class when using the masked image instead of the full image. It is defined as: A.G = 1 (cid:88) max(0, Oc ) 1 100, (6) i=1 is the models output score for class on the full image, and Oc where is the score when using the masked version derived from the explanation map. This metric captures how much the explanation enhances the models confidence, normalized by the room for improvement (1 ). Coherency [44] Class Activation Map should highlight all the relevant features that contribute to models prediction, while suppressing irrelevant ones in coherent and consistent manner. Consequently, for given input image and target class c, the CAM should remain unchanged when the image is conditioned on the CAM itself. This self-consistency can be expressed as: CAMc(x CAMc(x)) = CAMc(x) (7) where denotes element-wise multiplication. This condition implies that the CAM produced from the masked image should be identical to the original CAM, ensuring that the explanation is stable. Following the approach in [44], we use the Pearson Correlation Coefficient between the original CAM and the CAM obtained after masking: Coherency(x) = Cov(CAMc(x CAMc(x)), CAMc(x)) σCAMc(xCAMc(x))σCAMc(x)) (8) where Cov denotes the covariance and σ the standard deviation of each CAM. Since the Pearson Correlation Coefficient ranges from 1 to 1, we normalize it to the range [0, 1] and express it as percentage for interpretability. coherency score of 100% indicates that the attribution method is fully invariant to input perturbations guided by its own explanations. Complexity In addition to ensuring that CAM is coherentpreserving predictive features while discarding irrelevant onesit is also desirable for the CAM to be as simple as possible. That is, it should highlight the minimal subset of pixels necessary to explain the models prediction. To quantify this notion of simplicity, we use the ℓ0 norm as proxy for the Complexity of CAM: Complexity(x) = CAMc(x)0, (9) where 0 counts the number of non-zero (i.e., activated) pixels. lower Complexity score indicates that the attribution method focuses on fewer, more relevant regions, thereby producing more concise and interpretable explanations. ADCC [44] Since each individual metric captures distinct aspect of CAM quality, we compute an aggregated evaluation metricAverage DCC (ADCC)which combines Coherency, Complexity, and Average Drop into single score using the harmonic mean: ADCC(x) = 3 (cid:18) 1 Coherency(x) + 1 1 Complexity(x) + 1 1 A.D(x) (cid:19) (10) ADCC offers unified, single-valued measure that enables direct and consistent comparison. By balancing coherency, sparsity (via low complexity), and confidence preservation (via low Average Drop), it provides more comprehensive assessment of attribution quality."
        },
        {
            "title": "B Additional Details on Baselines",
            "content": "Large Image: For the Large Image baseline, we upsampled the original image via bilinear upsampling and use it as input to the foundation vision encoder. During evaluation on downstream tasks (see Tabs. 1 and 2), we upsample the input to the maximum ratio that fits in memory (i.e., 8), and subsequently apply bilinear upsampling to the resulting feature map to match the target output resolution. Due to the high computational cost and training time, we omit results on the COCO dataset. For efficiency, on Open-Vocabulary segmentation we limit upsampling to 2 ratio in Tab. 3. Strided: To obtain higher-resolution feature maps, we modify the stride of the ViT backbone to produce more patches. While the stride typically equals the patch size (e.g., 14 in DINOv2), we reduce the former to 6 in our experiments corresponding to 2.3 upsampling. We then apply bilinear upsampling to the resulting feature map to reach the desired output resolution. CARAFE [22]: For CARAFE, we use the CARAFEPack module from MMCV [51], stacking four upsampling stages with 2 ratio each, resulting in final feature map upsampled by factor of 16. SAPA [23]: We adopt the default implementation from the official SAPA repository, stacking four SAPA upsampling modules, each with an upsampling factor of 2. This results in final feature map with total upsampling factor of 16. DySample [24]: We adopt the default implementation from the official Dysample repository, stacking four Dysample upsampling modules, each with an upsampling factor of 2. This results in final feature map with total upsampling factor of 16. ReSFU [25]: We use the default implementation from the official ReSFU repository, performing direct upsampling to the target output resolution. FeatUp [28]: For FeatUp, we use the scalable JBU variant from the official FeatUp respository stacking 4 upsampling modules to achieve total upsampling factor of 16. We re-train FeatUp using the provided training scripts. While the original paper trains FeatUp on the COCO dataset for 2,000 steps with batch size of 4 and evaluates on the same dataset, we train it on the ImageNet training set for 50,000 steps (25 more) using the same batch size, ensuring fair comparison across methods. LiFT [27]: For LiFT, we slightly adapt the official implementation by resizing the intermediate representations after the downsampling module to ensure compatibility with backbones using patch size of 14 (i.e., downsampling factor of 14). In the official code, LiFT performs 2 upsampling and then relies on bilinear interpolation to upsample the features to the target output resolution. We summarize in Tab. 6 the differences between upsamplers. Table 6: Comparison of feature upsampling methods. Method Task-Agnostic Direct Upsampling Lightweight Inference CARAFE - SAPA - DySample ReSFU LiFT FeatUp (JBU) FeatUp (Implicit) JAFAR"
        },
        {
            "title": "C Additional Visualizations",
            "content": "We provide in the following subsections additional comparison visualizations for upsampled feature maps Supp. C.1, class activation maps predictions Supp. C.2, depth estimation Supp. C.3 and semantic segmentation Supp. C.4. C.1 Feature Visualization Figure 5: PCA Feature Visualization. DINOv2 ViT-S/14 features at 32 32 resolution from the ImageNet validation set are upsampled to 448 448. 16 Fig. 5 shows additional PCA visualizations of upsampled feature maps. Starting from 32 32 feature maps extracted using DINOv2-S/14 backbone, each is upsampled to 448 448 using different baseline methods. These baselineswhether training-free, task-dependent, or task-agnostictend to introduce varying degrees of blurriness and visual artifacts. In contrast, JAFAR, while remaining task-agnostic, produces sharp, content-aware features with minimal artifacts. C.2 Class Activation Maps We present additional Grad-CAM visualizations based on ViT-B/16 features from the ImageNet validation set in Fig. 6. Except for the Low-Res column, where features remain at their original 1414 resolution, all feature maps are upsampled to 224224 before Grad-CAM extraction. The explainability maps generated by our upsampling approach are noticeably sharper and more accurate, exhibiting fewer artifacts compared to those from alternative methods. Notably, CARAFE and LiFT fail to produce meaningful explanations in this setting, suggesting that the training of these methods does not transfer effectively to these ViT-based features. Figure 6: Class Activation Maps comparison. C.3 Depth Estimation Fig. 7 presents additional examples of linear probe transfer learning for depth estimation on the COCO-Stuff dataset. Feature maps of size 32 32, extracted from DINOv2-S/14 backbone, are upsampled to 448 448 using the various baseline methods. linear probe is then trained on these features to predict depth, using supervision from Depth-AnythingV2 model. The results demonstrate that both FeatUp variants produce high-quality features well-suited for transfer learning in depth estimation tasks. Figure 7: Depth Estimation Visualization. C.4 Semantic Segmentation Fig. 8 presents examples of linear probe transfer learning for semantic segmentation on the COCOStuff dataset. Feature maps of size 32 32, extracted from DINOv2-S/14 backbone, are upsampled to 448 448 using the various baseline methods. JAFAR produces more coherent segmentation results, offering improved delineation of both object boundaries and background regions. 17 Figure 8: Semantic Segmentation Visualization. Additionnal Comparison With Task-Agnostic Baselines D.1 FeatUp & LiFT In Tab. 1, we reported results obtained by training FeatUp and LiFT within our own codebase. To complement this evaluation, we present an additional comparison in Tabs. 7 and 8 using the official released checkpointsFeatUp on DINOv2 ViT-S/14 and LiFT on DINO ViT-S/16, respectively. Table 7: Semantic Segmentation Comparison between JAFAR and FeatUp. JAFAR is evaluated using FeatUps original feature extractor and compared against the official JBU checkpoint on the DINOv2 ViT-S/14 backbone. Model 224 448 ADE20K Cityscapes VOC ADE20K Cityscapes VOC Bilinear FeatUp JAFAR 24.50 29.26 30.04 38.18 42.84 48.52 64.10 71.90 75.36 28.19 32.87 32.29 49.35 53.17 56.45 70.64 77.57 79. Table 8: Semantic Segmentation Comparison between JAFAR and LiFT. JAFAR is evaluated using LiFTs original feature extractor and compared against the checkpoint on the DINO ViT-S/16 backbone. Model 224 448 ADE20K Cityscapes VOC ADE20K Cityscapes VOC Bilinear LiFT LiFT-iterative JAFAR 15.89 16.97 15.62 21.08 32.52 36.31 37.68 39.93 32.15 35.30 31.25 47.36 16.44 16.98 14.96 21.44 36.56 40.01 39.97 43.93 33.13 35.07 29.43 48. Across all datasets and output resolutionswith the exception of ADE20K at 448 for FeatUpJAFAR consistently delivers significant improvements in semantic segmentation performance. We also evaluate LiFT-iterative baseline, which stacks two LiFT 2 upsamplers as described in the original LiFT paper. However, this iterative approach does not outperform the simpler method of applying single LiFT 2 upsampler followed by bilinear interpolation. D.2 LoftUp We present an additional comparison with LoftUp [53], recent baseline which relies on segmentation masks from SAM [54] during training and employs two-stage pipeline that includes self-distillation phase. For fair evaluation, we tested JAFAR within LoftUps official codebase using DINOv2 ViT-S/14 backbone. 18 Table 9: Semantic Segmentation Comparison between JAFAR and LiFT. JAFAR is evaluated using Loftups original feature extractor and compared against the checkpoint on the DINOv2 ViT-S/14 backbone. Resolution Cityscapes COCO mIoU () Acc () mIoU () Acc () 56 112 448 LoFTUp JAFAR JAFAR + distillation LoFTUp JAFAR JAFAR + distillation LoFTUp JAFAR JAFAR + distillation LoFTUp JAFAR JAFAR + distillation 15.30 19.09 18. 32.02 34.56 33.63 50.83 51.45 51.84 62.49 61.49 62.30 75.50 79.34 79.05 86.28 87.19 87.47 91.55 91.25 91. 93.69 93.46 93.76 25.33 28.79 28.60 48.89 50.67 50.76 59.79 59.76 59.90 62.25 62.02 62.36 54.06 57.86 57. 73.09 74.56 74.60 80.04 79.93 80.04 81.43 81.30 81.45 As shown in Tab. 9, JAFAR outperforms LoftUp at lower upsampling resolutions (56 and 112) and delivers comparable performance at higher resolutions (224 and 448). Different from LoftUp, JAFAR uses simpler and more efficient single-stage strategy: it operates entirely at low resolution and does not rely on external annotations. Nevertheless, the self-distillation mechanism introduced in LoftUp is complementary to our approach and can be seamlessly integrated into JAFARs pipeline. To demonstrate this, we implemented similar distillation objective and report the results as JAFAR + distillation in Tab. 9. While the gains are minimal at lower resolutions, this enhancement provides clear boost at higher resolutions (224 and 448). Lastly, JAFAR is considerably more lightweight, with just 0.7 million parameters compared to 4.3 million in LoftUp."
        },
        {
            "title": "E Inference Time",
            "content": "In Tab. 10, we compare the inference times of various methods using batch size of 1 and input images of resolution 448, across multiple output resolutions. The experiments are conducted on single A100 GPU. Table 10: Parameter Count and Inference Time Comparison (ms). Model Upsamplers FeatUp JAFAR LiFT LoftUp Other configurations Large Image (x8) Strided (1) # Params (M) 562 1122 2242 4482 0.2 0.7 1.2 4.3 - - 5.7 4.0 0.9 3.8 8.0 5.7 0.9 8.9 14.9 16.6 1.0 24.5 64.9 94.0 1.5 145.1 6.2 11.4 34.7 136. 348 2 482 5 558 48 090 Additionally, Tab. 11 reports the memory usage of each model during both forward and backward passes, evaluated across multiple output resolutions using fixed batch size of 1 and input resolution of 448. 19 Table 11: Memory usage (in GB) for different models and resolutions. 4482 # Params (M) Model 2242 1122 Pass 562 Forward Backward Bilinear FeatUP JAFAR LoftUp FeatUP JAFAR LoftUp 0.0 0.2 0.7 4.3 0.2 0.7 4.3 0.4 0.6 0.6 0. 0.7 0.7 0.7 0.5 0.8 0.6 0.7 0.9 1.1 1.3 0.5 1.6 1.1 1.8 2.1 3.5 4.6 0.8 4.8 7.7 12. 7.4 26.0 26."
        }
    ],
    "affiliations": [
        "Sorbonne Université, CNRS, ISIR, F-75005 Paris, France",
        "Thales, TSGF, cortAIx Labs, France",
        "Valeo.ai"
    ]
}