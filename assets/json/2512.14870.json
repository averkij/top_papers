{
    "paper_title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
    "authors": [
        "Dan Ben-Ami",
        "Gabriele Serussi",
        "Kobi Cohen",
        "Chaim Baskin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. 5 2 0 2 6 1 ] . [ 1 0 7 8 4 1 . 2 1 5 2 : r HERBench: Benchmark for Multi-Evidence Integration in Video Question Answering Dan Ben-Ami1, Gabriele Serussi1, Kobi Cohen2 Chaim Baskin1 1INSIGHT Lab, Ben-Gurion University of the Negev, Israel 2Ben-Gurion University of the Negev, Israel Equal contribution. Abstract Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. In this direction, we present HERBench, VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question is constructed to require aggregating at least three non-overlapping evidential cues across distinct video segments (so neither language priors nor single snapshot can suffice). HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS)-the smallest number of frames model must fuse to answer correctly-and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 3142% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) retrieval deficit, where frame selectors overlook key evidence, and (2) fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes principled target for advancing robust, compositional video understanding."
        },
        {
            "title": "1 Introduction",
            "content": "As Video Large Language Models [24] achieve strong scores on established VideoQA benchmarks [511], their video understanding capabilities appear to be rapidly emerging. However, recent audits reveal these high scores often stem from language priors or single-cue shortcuts rather than grounded temporal reasoning [12, 13], causing models to fail tasks that explicitly require multi-hop inference [1416]. In contrast, tasks like Referring Video Object Segmentation (RVOS) demonstrate that robust, multi-frame aggregation is achievable, as models successfully link instances across occlusions and appearance changes [1720]. We advocate centering evaluation on evidential requirement, because single-cue questions fail to measure multievidence integration. We define the Evidential Requirement (ER) as the minimum number of distinct, non-redundant visual evidence needed for an answer. High-ER items make compositional reasoning, such as temporal binding and clue combination, unavoidable [7, 15, 21]. Controlling ER therefore distinguishes models that integrate information from those that rely on isolated cues [12]. This approach makes aggregation measurable, aligns VideoQA with real-world reasoning, and offers principled path for progress beyond single-cue success [7, 15, 21]. We introduce HERBench (High Evidential Requirement Benchmark), where questions across twelve compositional subtasks (e.g., entity binding, temporal ordering) are constructed to structurally enforce 3 distinct pieces of evidence, as presented in Figure 1. To measure this, we present the Minimum Required Frame-Set (MRFS) metric, defined as the minimum number of frames needed for correct answer. Cross-benchmark validation confirms our high-ER design: HERBenchs mean MRFS of 5.5 far exceeds the 2.6 to 4.2 observed in existing benchmarks and enables principled, ER-focused diagnostics. Our evaluation of state-of-the-art Video-LLMs exposes two critical bottlenecks. Finding 1: Frame selection is major bottleneck. While adaptive selectors [22, 23] outperform uniform sampling, they still lag significantly behind Preprint. Under review. Figure 1: From Single-Cue to Multi-Evidence Integration. While existing benchmarks like MVBench [1] (top) often focus on short-term attributes solvable via single salient frames or language priors, HERBench (bottom) enforces high Evidential Requirement (ER). In this Temporal Shot Ordering example, the model must identify and temporally bind four distinct, non-overlapping visual evidence dispersed across the video to reconstruct the correct sequence. This design ensures that successful answering requires genuine multi-evidence integration rather than reliance on static shortcuts. ground-truth keyframes. Finding 2: Multi-evidence reasoning is also bottleneck. Even with ground-truth frames, models achieve only modest accuracy because they fail to assign proper importance to all critical frames and struggle to integrate them. Progress therefore requires advances in both frame selection and multi-evidence reasoning. Our main contributions are summarized as follows. We introduce HERBench: benchmark with 26,806 questions that are constructed to structurally enforce 3 distinct, non-redundant visual cues. We propose the Minimum Required Frame-Set (MRFS) metric: measure of the smallest number of frames model must aggregate to answer question correctly, thereby enabling apples-to-apples comparison across benchmarks (existing benchmarks range from 2.6 to 4.2, whereas HERBench averages 5.5) and powering ER-focused diagnostics. We identify two critical bottlenecks in current Video-LLMs: By disentangling frame selection from multi-evidence reasoning, we reveal two systemic failures. (i) Frame selection: Adaptive selectors, though an improvement over uniform sampling, still overlook key evidence and do not yet match the performance of oracle key-frames. (ii) Multievidence reasoning: Even with oracle frames, models fail to integrate complementary information and systematically underweight necessary evidence. Progress requires advances in both selection and reasoning. Preprint. Under review."
        },
        {
            "title": "2 Related Work",
            "content": "Video Large Language Models. Video-LLMs architectures have evolved from simple feature pooling [24, 25] to sophisticated systems employing advanced alignment modules (e.g., Q-Formers) and large-scale instruction tuning [2628] to bridge the modality gap. Despite massive token capacities in proprietary models like Gemini 2.5 [29] and GPT-4o [30], recent audits [12, 31] reveal persistent failure in robust temporal aggregation. Instead of performing multi-hop inference, these models frequently default to language priors or single-frame shortcuts to solve tasks. Video Question Answering Benchmarks. VideoQA evaluation has progressed from short-clip recognition [32, 33] to long-form reasoning. While MVBench [1] introduced diverse temporal tasks, its short duration limits long-horizon assessment. Successors like EgoSchema [9], LongVideoBench [11], and Video-MME [10] address this by targeting minute-to-hour scale videos and diverse modalities. However, long context does not equate to high evidential requirement. Critiques [9, 31] indicate these benchmarks often remain solvable via single salient keyframes, failing to measure genuine multi-step reasoning. HERBench distinguishes itself by explicitly controlling the Evidential Requirement (ER), constructing questions that structurally enforce aggregating multiple (k 3) distinct, temporally separated visual cues to assess compositional reasoning rather than memory capacity."
        },
        {
            "title": "3 HERBench: High Evidential Requirement Benchmark",
            "content": "In this section, we present HERBench, VideoQA benchmark explicitly designed to evaluate multi-evidence integration. The section is organized as follows: Sec. 3.1 introduces the twelve tasks grouped into four reasoning families, together with the specific capabilities they target; Sec. 3.2 describes the construction pipeline that enforces high Evidential Requirement (ER) while suppressing shortcuts; Sec. 3.3 summarizes the dataset scale and corpus statistics; and Sec. 3.4 formalizes ER through the Minimum Required Frame-Set (MRFS) metric and presents the standardized evaluation protocol."
        },
        {
            "title": "3.1 Task Taxonomy",
            "content": "To evaluate whether models truly use evidence rather than guess from the most prominent cue, we organize twelve tasks into four families (Figure 2). Each task is constructed so that correct answer requires aggregating at least 3 distinct observations (ER), enforced via identity binding, set-level aggregation, temporal ordering, or video-wide verification. Temporal Reasoning & Chronology [TR&C]. These tasks require understanding event order, co-occurrence, and durations, compiling distributed cues into linear chronology. The three tasks are: 1) [TSO] Temporal Shot Ordering: Arrange four shot descriptions from trailer into the correct chronological order, using only content cues to reconstruct high-level scene transitions. 2) [MPDR] Multi-Person Duration Reasoning: Compare interval statistics for appearancedescribed people (e.g., who stayed in view the longest, or who entered/exited first), focusing on fine-grained time-span contrasts across individuals. 3) [ASII] Action Sequence Integrity & Identification: Select the correct ordering of five narrated actions among plausible permutations, stressing micro-level task sequencing rather than scene-level ordering. The ER is driven by ordering and interval comparisons across at least three temporally separated observations, but each task probes distinct temporal structure. Referring & Tracking [R&T]. This family tests binding uniquely appearance-described target across time to reason about trajectory-dependent properties. Models must maintain stable reference as the target interacts with the scene. The tasks are: 1) [AGBI] Appearance-Grounded Behavior Interactions: Identify who accompanies or interacts with the target during traversal, emphasizing social and relational cues. 2) [AGAR] Appearance-Grounded Attribute Recognition: Track the target to read out attributes anchored to their immediate local context (e.g., passerbys jacket color), focusing on moment-specific attribute extraction. 3) [AGLT] Appearance-Grounded Localization Trajectory: Recover path endpoints and coarse trajectory (e.g., exit method), highlighting global, path-level motion reasoning. This enforces 3 through identity maintenance across separated glimpses, each task centering on different aspect of target evolution. Preprint. Under review. Figure 2: Task taxonomy of HERBench. We organize 12 fine-grained compositional tasks into four essential reasoning families: (1) Temporal Reasoning & Chronology, (2) Referring & Tracking, (3) Global Consistency & Verification, and (4) Multi-Entity Aggregation & Numeracy. Unlike existing benchmarks that may allow for single-frame shortcuts, every task in HERBench is constructed to enforce High Evidential Requirement, requiring models to aggregate at least three distinct, temporally separated visual cues (k 3) to derive the correct answer. Global Consistency & Verification [GC&V]. Next, we test exhaustive video-wide verification and absence detection, sweeps that must confirm what occurred and surface plausible but missing elements. The three tasks are: 1) [FAM] False Action Memory: Among several plausible actions, select the one that never occurs while verifying the others do, requiring action-level absence detection. 2) [SVA] Scene Verification Arrangement: Given 2-4 shot descriptions where some may be fabricated, first identify the faithful ones, then arrange the correct shots in temporal order, or return calibrated abstention when too many descriptions are false; this combines shot-level fidelity checking with chronology. 3) [FOM] False Object Memory: Among plausible objects, identify the one the camera wearer does not interact with while verifying the rest, stressing object-level absence tied to first-person interactions. Here 3 arises from multi-moment sweeps needed to validate presence and detect absence across the video. Multi-Entity Aggregation & Numeracy [MEA&N]. Finally, this family stresses many-way binding, spatial partitioning, and precise counting across multiple people or events. Models must deduplicate identities across time and fuse evidence spread over the video. The three tasks are: 1) [MEGL] Multi-Entities Grounding & Localization: Given 2-3 detailed appearance descriptions, decide which individuals actually appear in the video (exact-match verification among plausible distractors), focusing on set membership and identity deduplication. 2) [AC] Action Counting: Count Preprint. Under review. Figure 3: HERBench Data Construction Pipeline. We employ tripartite pipeline. (Left) Videos are processed through three parallel streams: 1) Object Tracking and Trajectory Analysis (via RF-DETR and DeepSORT) to produce targets to generate disentangled Appearance (A) and Behavior (B) cards; 2) Shot Segmentation using shot detection with an MLLM description for producing scene descriptions; and 3) Ground Truth Integration refining human verified raw event logs. (Middle) These refined data input are controlled via Manual Review and then input into an Oriented Task Programming module that programmatically compiles the 12 compositional tasks. (Right) The pipeline enforces rigorous quality control through expert Manual Review and Text-Only Filtering stage to eliminate language priors, ensuring all final Multiple Choice Questions (MCQs) enforce multi-evidence integration. the occurrences of specified action-object pair distributed across the timeline, emphasizing event-accumulation across dispersed moments. 3) [RLPC] Region-Localized People Counting: Count unique individuals subject to spatial constraints (e.g., entries through the top edge), with answers reported as binned ranges, requiring region-conditioned identity aggregation. Here 3 is enforced by set-level aggregation and cardinality constraints over multiple moments, with each task stressing complementary aggregation mode."
        },
        {
            "title": "3.2 Benchmark Construction",
            "content": "We construct HERBench through the tripartite data construction pipeline shown in Figure 3. The core of this process is the creation of rich spatiotemporal scaffold by processing each video through three complementary streams. The first stream, Object Tracking & Trajectory Analysis, focuses on continuous, micro-level object dynamics. Complementing this, the second stream, Shot Segmentation, provides macroscopic view by discretizing the video into semantic units. Finally, the Ground Truth Integration stream anchors the analysis in human-verified facts. Together, these streams produce diverse set of refined data (such as A/B cards, scene cards, and event labels). Pipeline I: Object Tracking & Trajectory Analysis. This first stream anchors tasks in continuous object dynamics. We employ RF-DETR [34] and DeepSORT [35] to obtain entity tracks, retaining top entities via TrackRank score, composite score favoring appearance rarity (HSV/LBP), trajectory length and frame coverage (see Supplementary). For each track, we generate strictly non-overlapping A-cards (appearance) and B-cards (behavior/trajectory). This decorrelation intentionally separates the identifying appearance from the queried behavior, often placing them in temporally distant frames. This scaffold supports tasks requiring fine-grained interaction and motion analysis: [AGBI], [AGAR], [AGLT]: We generate questions strictly from B-cards while referring to entities via A-cards, separating appearance from behavior. [AGBI] queries behavioral interactions with other entities; [AGAR] queries attributes; and [AGLT] queries path integration and motion topology. [MPDR]: We compute per-entity visible-time intervals to generate queries comparing durations (e.g., longest presence) or checking for temporal overlaps. The correct answer is, by definition, property of the relationship between multiple, ordered cues. Preprint. Under review. Figure 4: Left: Wordcloud of frequent terms in HERBench queries. Center: Distribution of samples across source datasets. Right: Number of questions per task category. [RLPC]: We execute spatial programs to count unique track IDs traversing predefined regions of interest or entry/exit gates, testing spatiotemporal aggregation capabilities. [MEGL]: We form sets of appearance descriptors and inject plausible distractors, forcing models to verify the exact set of present individuals throughout the video. Pipeline II: Shot Segmentation. Where the first pipeline focuses on continuous entity-level detail, this second stream discretizes the video into larger semantic units. It uses shot boundary detection, employing an MLLM to summarize each segment into concise scene card. This macroscopic view supports tasks dependent on global temporal coherence: [TSO]: We query the chronological arrangement of the generated scene cards, requiring the model to reorder shuffled scenes. [SVA]: We mix faithful scene cards with plausibly perturbed variants, altering 2-5 atomic details (e.g. actions, attributes), to test resistance to gist cues or partially correct descriptions. Pipeline III: Ground Truth Integration. Finally, this stream moves beyond automated analysis to leverage human verified narrated events [36]: [FAM], [FOM]: We introduce corpus-plausible distractors, entities or actions common in similar videos but verified as absent, requiring multi-timestamp scanning rather than single-frame spot checks. [ASII]: We establish ground-truth chronology from narrated events and present proposed sequences (faithful vs. perturbed) for careful verification. [AC]: Ground-truth counts are derived directly from verified event logs to test long-horizon aggregation. Synthesis & Quality Control. We employ multi-stage verification protocol governing both the refined data input and the final output. First, the structured cards undergo component-level verification. To ensure referring tasks rely on tracking rather than description matching, we enforce strict disentanglement between A-cards and B-cards via token-level Jaccard similarity checks and manual leakage review. The validated components are processed by the Oriented Task Programming module to instantiate the tasks. Next, to explicitly suppress language priors, we apply Text-Only Filtering stage: we discard any question correctly answered by 3 of 4 blind LLMs (Qwen2-7B [37], Qwen2.5-7B [38], Llama-3-8B [39], and Vicuna-7B v1.5 [40]). Finally, experts conduct verification on stratified 15% sample to audit constraint satisfaction. This stage specifically targets the minimum multi-frame requirement (k 3), rejecting items solvable by single frames ( 18% rejection rate) or those lacking unique, objective ground-truth answers."
        },
        {
            "title": "3.3 Benchmark Statistics",
            "content": "Scale and Scope. HERBench contains 26,806 multiple-choice questions sampled from 336 unique videos. These items are organized into 12 compositional tasks across four categories (Sec. 3.1). Frequent query terms are visualized in Figure 4 (left). Preprint. Under review. Benchmark Questions Video length (s) MRFS NExT-QA MVBench LongVideoBench HERBench 4,996 4,000 6,678 26,806 44 16 473 395 2.61 3.52 4.07 5. Table 1: Benchmark comparison. HERBench is 4 larger than existing benchmarks with the highest MRFS (indicating high evidential requirement). Video Corpus Characteristics. The corpus is curated to test reasoning over substantial durations (avg. 395s, range 602100s), ensuring that required evidential cues are temporally dispersed. It covers balanced environments (46% indoor, 28% outdoor, 26% mixed) and diverse perspectives, including egocentric, surveillance, and cinematic views. Videos are sourced from HD-EPIC [36], WildTrack [41], PersonPath22 [42], and publicly available movie trailers on YouTube; source distribution appears in Figure 4 (center). Question Format and Density. Questions utilize five-way multiple-choice format, establishing uniform 20% random-guess baseline. Crucially, correct answers are statistically balanced across options (A-E) to eliminate positional bias. Figure 4 (right) details the question distribution per task."
        },
        {
            "title": "3.4 Evidential Requirement & the MRFS Metric",
            "content": "Motivation. Answering VideoQA item may require fusing evidence from multiple, temporally separated moments, or it may be solvable from single salient frame. To make this requirement measurable, we introduce the Minimum Required Frame-Set (MRFS): the smallest number of frames that must be fused for model to answer given question correctly. Aggregating MRFS over items yields benchmark-level statistic that quantifies evidential demand, higher mean MRFS indicates that questions cannot be solved by single-cue shortcuts and instead require multi-moment integration. Definition. Let denote the video, the question, and the ground-truth answer. Let be fixed MLLM, question-conditioned frame selector, and frame budget. The selector produces ranking π = r(v, q) over frames and we denote Fk = {π1, . . . , πk} as the top-k subset. With evaluator E(ˆy, y) = 1{ˆy = y}, we define MRFSx(q; f, r) = min(cid:8) {1, . . . , x} : E(cid:0)f (q, Fk), y(cid:1) = 1 (cid:9), (1) subject to the precondition E(f (q, ), y) = 0 so that text-only solvable items are excluded from MRFS computation. Intuitively, MRFSx is the least amount of visual evidence (in frames) that suffices for to be correct when frames are supplied in an r-determined, question-aware order. Computation. We search for the smallest success index using an adaptive bisection over [1, x], requiring O(log x) model calls per item. Each question is categorized as: (i) text-only (correct with no frames, (q, )), (ii) visual-required (correct for some 1 x), or (iii) undefined (incorrect even at = x). MRFS measures across benchmarks. Because MRFS is defined with respect to (f, r, x), cross-benchmark comparability requires fixing these components. We standardize on = Qwen2.5-VL [43], = AKS adaptive keyframe sampling [22], and = 16 frames. This protocol isolates the datasets evidential requirement from model or selector variability. As seen in Table 1, HERBench achieves mean MRFS of 5.49, 35% increase over the next-highest benchmark, LongVideoBench (4.07), and far exceeds MVBench (3.52) and NExT-QA (2.61). Notably, this high ER is achieved with shorter average video duration (395s) than LongVideoBench (473s), indicating the difficulty arises from evidential density, not just length. These results confirm that HERBench makes multi-evidence integration unavoidable. MRFS turns an otherwise qualitative notion like how much evidence does question require? into quantitative, modeland selector-standardized measurement. Reporting mean MRFS alongside accuracy thus separates success via single-cue shortcuts from genuine multi-evidence reasoning and makes benchmark evidential demand explicit. Preprint. Under review. Model TR&C R&T GC&V ME&N Overall Avg. TSO MPDR ASII Avg. AGBI AGAR AGLT Avg. FAM SVA FOM Avg. MEGL AC RLPC Avg. GPT-4.1 [44] Gemini-2.5-Flash [29] 18.9 28.6 Qwen2.5-VL-72B [43] 10.4 Gemma-3-27B [45] 38.4 LLaMA-4-Scout-17B [46] 6.2 43.9 InternVL3.5-14B [47] Ovis-2.5-9B [4] 0.1 InternVL3.5-8B [47] 41.3 LLaVA-OneVision1.5-8B [3] 26.6 Qwen3-VL-8B [2] 2.2 19.1 MiniCPM-V4.5-8B [48] 14.6 Qwen2.5-VL-7B [43] 33.3 LLaVA-OneVision-7B [28] 29.7 35. 42.6 42.0 30.0 38.8 30.6 31.3 28.7 28.7 26.3 28.0 24.9 27.7 25.4 24.8 29.7 27.8 26.9 15.7 32.0 20.1 18.8 30.3 37.7 26.0 18.9 28.1 33.6 23.0 26.1 26.0 19.0 26.0 23.8 22.9 21.8 23.7 27.3 78.0 75.2 74.4 69.0 64.7 75.9 79.7 77.6 76.8 74.6 77.9 69.7 67.1 59.1 71. 76.1 50.5 51.6 69.4 76.2 71.6 67.5 69.6 72.3 59.3 58.0 61.0 63.1 62.2 55.6 55.6 62.6 64.7 61.4 58.8 61.9 63.2 52.9 52.3 66.0 30.4 38.9 41.9 37.1 69.9 29.2 31.3 44.2 34.9 70.9 25.6 50.6 33.5 36.6 58.4 21.8 14.3 28.4 21.5 57.3 19.3 36.5 20.7 25.5 69.3 26.8 22.8 43.8 31.1 73.5 33.6 57.2 49.6 46.8 70.2 26.3 21.2 41.5 29.7 67.7 29.8 33.9 37.1 33.6 68.7 30.0 51.3 40.4 40.6 71.1 30.2 43.7 45.2 39.7 60.6 33.0 36.0 47.1 38.7 59.1 28.9 22.4 38.9 30.1 25.5 22. 18.1 15.7 17.2 25.3 27.5 33.1 25.2 18.8 24.1 21.1 22.8 24.3 26.6 23.0 29.0 26.1 20.8 23.4 21.2 17.6 21.8 22.9 20.3 22.4 37.3 31.2 32.0 25.7 29.4 37.3 36.7 38.1 31.9 34.9 27.9 26.3 32.8 29.0 26. 24.4 23.5 24.2 27.8 29.2 30.8 24.9 25.2 24.9 22.6 26.0 Avg. 23.1 31.9 25.5 26.8 74. 66.3 59.7 66.8 28.4 35.2 40.9 34.8 23.2 23.0 32. 26.3 39.4 40.3 39.7 33.8 31.4 41.5 42.1 41.1 38.1 38.3 39.9 35.9 35.6 38.2 Table 2: Main results on the HERBench. We report per-task accuracy (%) for 13 leading MLLMs. The highest performance in each task column is marked in bold. The 4 largest-size models (first rows) were run on representative 10% subset (2.6K questions), while the remaining models were evaluated on the full benchmark."
        },
        {
            "title": "4 Experiments",
            "content": "To validate the challenges posed by HERBench, we conduct comprehensive evaluation of current state-of-the-art Multimodal Large Language Models. Our experiments are designed to quantify their performance on tasks explicitly requiring the integration of multiple, temporally dispersed visual cues. Setup. We evaluate 13 prominent MLLMs, spanning closed-source models (GPT-4.1 [44], Gemini-2.5-Flash [29]) and diverse range of open-source systems, detailed in Table 2. This selection allows for broad assessment of state-of-the-art capabilities across scales and design paradigms. To isolate reasoning ability from evidence retrieval, we standardize the visual input. All models receive an identical budget of 16 frames, sampled uniformly across the video. This fixed input ensures that performance differences are attributable to the models multi-evidence integration capabilities, not to varied frame selection. We report top-1 accuracy. Results. As shown in Table 2, performance is systematically poor. The mean accuracy across all 13 state-of-the-art models is 38.2%, with the best model (Ovis-2.5-9B [4]) reaching only 42.1% and the lowest (LLaMA-4-Scout-17B [46]) at 31.4%. This narrow performance band, just 1122 percentage above the 20% random baseline, reveals that failure to integrate dispersed evidence is pervasive limitation across all current architectures. The performance breakdown by task is telling. Models show relative competence on single-entity tracking tasks like [AGBI] and [AGAR] (Ovis-2.5-9B: 79.7%, 76.2%). This suggests they can track single described entity. However, performance collapses on all tasks strictly requiring multi-cue aggregation. For [AC] and [MEGL], mean accuracies are 23.0% and 23.2% respectively, barely above chance. Similarly, models fail at temporal ordering ([TSO]), with scores as low as 0.1%, demonstrating clear inability to compose dispersed information. In summary, these results demonstrate that while state-of-the-art MLLMs can track single entities, they fundamentally fail at the core challenge of multi-evidence compositional reasoning. Our controlled-frame evaluation confirms this deficit stems from failure to integrate information, not merely failure to access it."
        },
        {
            "title": "5 Analysis",
            "content": "This section analyzes the two major challenges highlighted by HERBench: (Q1) how frame selection strategies affect performance through evidence retrieval, and (Q2) whether models can effectively aggregate evidence across the correct frames once retrieval uncertainty is removed. Preprint. Under review. Frame Selection InternVL3.5-14B Qwen3-VL-8B Ovis-2.5-9B Uniform Vanilla-BLIP BOLT-ITS AKS Oracle Frames (OF) 42.7 42.1 41.1 42.7 47. 37.7 37.9 38.4 36.2 41.0 43.1 41.6 42.1 42.6 47.9 Table 3: Mean accuracy by frame selection method and model. Rows list frame selection methods. Each cell shows the mean accuracy over 1200 questions (100 from each task). 5."
        },
        {
            "title": "Isolating the Evidence Retrieval Bottleneck",
            "content": "Frame Selection methods. To address (Q1), i.e. the impact of evidence retrieval, we compare five strategies (all operating in the same BLIP [49] embedding space for fairness): AKS [22] learns keyframe policy that balances relevance and temporal coverage; BOLT-ITS [23] using inverse transform sampling to select query-relevant frames; Uniform takes evenly spaced frames; Vanilla-BLIP retrieves frames with highest cosine similarity to the question; Oracle Frames (OF) use frame indices gathered from our benchmarks construction pipeline along with few complementary non-evidence frames. They are applied only in tasks where relevant evidence is scarce or confined to very short portions of the video (notably [TSO], [FAM], [SVA]). 1 Performance across frame selection strategies. Table 3 presents the accuracy, averaged across all tasks, for each selection method applied to three representative models. Learned selectors such as AKS and BOLT-ITS outperform simple uniform sampling on many tasks, yet still trail behind the Oracle Frames (OF) configuration, performance gap that is even more pronounced in the per-task breakdown (see Supplementary), reinforcing the fact that evidence retrieval remains major performance bottleneck. More importantly, even when the model is provided with the correct evidence frames (using OF), performance gains are limited, with accuracy remaining below 50%, indicating that access to the right information alone is insufficient for successful multi-evidence reasoning. This finding aligns with our broader observation that current models underweight or fail to integrate critical cues, even when ground-truth evidence is fully available."
        },
        {
            "title": "5.2 Evidence Aggregation with Oracle-Only Frames",
            "content": "Having established that evidence retrieval is significant bottleneck (Sec. 5.1), we now turn to (Q2): can models effectively aggregate evidence even when retrieval uncertainty is removed? To isolate the fusion capability from the retrieval challenge, we conduct targeted study on subset of HERBench supplying models with only the manually curated ground-truth frames (the oracle set). Measuring Frame-Level Contribution. For each item, we compute per-frame deltas and shares that quantify how much each frame contributes to the models own predicted option: 1. Full prediction. Run the model on all oracle frames and compute log pfull, where is the post-softmax probability of the chosen letter token (AE), with the softmax taken only over these candidate tokens. 2. Leave-one-out re-run. For each frame i, re-run with that frame excluded (the context contains the remaining 1 frames) to obtain log pminus[i]. 3. Delta. = log pfull log pminus[i]; positive means frame supports the models chosen option. / (cid:80) + 4. Share. si = + 1We use OF to denote the curated set of evidence frame indices plus few additional non-evidence frames. The purpose of the additional frames is to fill the fixed frame budget (e.g., 16 frames), making OF best-case selection strategy that is comparable to AKS and BOLT-ITS. This setup tests if model can identify the correct evidence when it is guaranteed to be retrieved, and is distinct from the oracle-only analysis in Sec. 5.2, which tests pure fusion. Curation exists only for subset of tasks, so its effect is not uniform across the benchmark. , yielding normalized importance distribution across frames. Preprint. Under review. Figure 5: Top-1 frame share under oracle-only frames. Violin/box plots show the distribution of the maximum normalized frame-importance share across oracle-only frames for three models (InternVL3.5-14B, Ovis-2.5, Qwen3VL-8B), split by Correct vs. Incorrect predictions. For each item, we compute leave-one-out deltas of the log-probability of the models predicted option and normalize them to per-frame shares; the plotted statistic is the largest share (Top-1). Correct predictions allocate credit more evenly across frames (typically 0.5), whereas errors over-concentrate on single frame (often 0.8), indicating insufficient multi-evidence fusion even when only evidence-bearing frames are provided. Diagnosing Fusion Failures via Importance Distribution. We analyze per-frame importance shares to understand why models succeed or fail under oracle-only inputs, summarizing each item using the Top-1 Share (maxi si) as shown in Figure 5. This statistic captures how strongly model concentrates its decision on single frame. The distributions reveal consistent pattern: correct predictions (green) exhibit substantially more balanced allocations, with mean Top-1 shares near 0.5. Incorrect predictions (red), in contrast, show pronounced over-concentration, with Top-1 shares frequently approaching 0.8. This separation indicates that errors arise not merely from insufficient signal, but from misallocation of attention-models place disproportionate weight on one frame while failing to assign sufficient importance to the multiple, distributed evidential cues present across the oracle set. Because HERBench questions structurally require multi-frame reasoning, this behavior shows that the fusion module itself-independent of retrievalremains primary source of failure."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce HERBench, VideoQA benchmark comprising 26,806 questions across 12 tasks, each constructed to structurally enforce aggregating 3 distinct, temporally separated visual cues. To quantify evidential demand, we propose the Minimum Required Frame-Set (MRFS) metric. Cross-benchmark validation confirms HERBench imposes substantially higher evidential requirements (mean MRFS 5.5 vs. 2.6-4.2 on prior datasets). Evaluating 13 state-of-the-art MLLMs reveals limited accuracy range (31.442.1%), only slightly above the 20% chance-level baseline (corresponding to randomly selecting one of five answer options), exposing pervasive limitations. We identify two core bottlenecks: (i) retrieval deficit, as frame selectors fail to access all necessary cues, and (ii) fusion deficit, where models fail to integrate evidence even when provided. This fusion failure manifests as over-concentration on single frames. Our analysis thus isolates specific, actionable deficits in aggregation and selection, providing clear target for improving future Video-LLMs. This benchmark will thus serve as critical tool to guide the development of next-generation models capable of genuine compositional reasoning. By making multi-evidence aggregation both unavoidable and quantifiable, HERBench establishes principled target for advancing compositional video understanding and exposes clear headroom for progress beyond single-cue success."
        },
        {
            "title": "References",
            "content": "[1] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo, L. Wang, and Y. Qiao, MVBench: comprehensive multi-modal video understanding benchmark, in CVPR, 2024. [Online]. Preprint. Under review. Available: https://openaccess.thecvf.com/content/CVPR2024/papers/Li MVBench Comprehensive Multi-modal Video Understanding Benchmark CVPR 2024 paper.pdf [2] A. Yang et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025, covers the Qwen3 family; Qwen3-VL repo indicates paper is coming. [Online]. Available: https://arxiv.org/abs/2505. [3] X. An et al., Llava-onevision-1.5: Fully open framework for democratized multimodal training, arXiv preprint arXiv:2509.23661, 2025. [Online]. Available: https://arxiv.org/abs/2509.23661 [4] S. Lu et al., Ovis2.5 technical report, arXiv preprint arXiv:2508.11737, 2025. [Online]. Available: https: //arxiv.org/abs/2508.11737 [5] J. Lei, L. Yu, M. Bansal, and T. L. Berg, Tvqa: Localized, compositional video question answering, in EMNLP, 2018, arXiv:1809.01696. [6] Y. Jang, Y. Song, C. D. Kim, Y. Yu, and G. Kim, Tgif-qa: Toward spatio-temporal reasoning in visual question answering, in CVPR, 2017. [Online]. Available: https://openaccess.thecvf.com/content cvpr 2017/papers/ Jang TGIF-QA Toward Spatio-Temporal CVPR 2017 paper.pdf [7] J. Xiao, X. Shang, A. Yao, and T. Chua, NExT-QA: Next phase of question-answering to explaining temporal actions, in CVPR, 2021. [Online]. Available: https://arxiv.org/abs/2105. [8] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urtasun, and S. Fidler, Movieqa: Understanding stories in movies through question-answering, in CVPR, 2016. [Online]. Available: https://openaccess.thecvf.com/content cvpr 2016/papers/ Tapaswi MovieQA Understanding Stories CVPR 2016 paper.pdf [9] K. Mangalam, R. Akshkulakov, and J. Malik, Egoschema: diagnostic benchmark for very long-form video language understanding, in NeurIPS, 2023. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2023/file/ 90ce332aff156b910b002ce4e6880dec-Paper-Datasets and Benchmarks.pdf [10] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, P. Chen, Y. Li, S. Lin, S. Zhao, K. Li, T. Xu, X. Zheng, E. Chen, C. Shan, R. He, and X. Sun, Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025, pp. 24 10824 118. [11] H. Wu, D. Li, B. Chen, and J. Li, Longvideobench: benchmark for long-context interleaved video-language understanding, in NeurIPS, 2024, arXiv:2407.15754. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2024/hash/ 329ad516cf7a6ac306f29882e9c77558-Abstract-Datasets and Benchmarks Track.html [12] Y. Liu, S. Li, Y. Liu, Y. Wang, S. Ren, L. Li, S. Chen, X. Sun, and L. Hou, Tempcompass: Do video llms really understand videos? in Findings of ACL, 2024, arXiv:2403.00476. [Online]. Available: https://aclanthology.org/2024.findings-acl.517/ [13] J. Xiao, A. Yao, Y. Li, and T. Chua, Can trust your answer? visually grounded video question answering, in CVPR, 2024. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao Can Trust Your Answer Visually Grounded Video Question Answering CVPR 2024 paper.pdf [14] R. Girdhar and D. Ramanan, Cater: diagnostic dataset for compositional actions & temporal reasoning, in ICLR, 2020. [Online]. Available: https://arxiv.org/abs/1910.04744 [15] K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum, Clevrer: Collision events for video representation and reasoning, in ICLR, 2020. [Online]. Available: https://arxiv.org/abs/1910.01442 [16] M. Grunde-McLaughlin, R. Krishna, spatiotemporal reasoning, in CVPR, 2021. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2021/papers/ Grunde-McLaughlin AGQA Benchmark for Compositional Spatio-Temporal Reasoning CVPR 2021 paper.pdf benchmark for compositional and M. Agrawala, Agqa: [17] K. Gavrilyuk, A. Ghodrati, Z. Li, and C. G. M. Snoek, Actor and action video segmentation from sentence, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [18] S. Seo, J.-Y. Lee, and B. Han, Urvos: Unified referring video object segmentation network with large-scale benchmark, in European Conference on Computer Vision (ECCV), 2020. [19] A. Botach, E. Zheltonozhskii, and C. Baskin, End-to-end referring video object segmentation with multimodal transformers, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Preprint. Under review. [20] J. Wu, Y. Jiang, P. Sun, Z. Yuan, and P. Luo, Language as queries for referring video object segmentation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [21] B. Wu, S. Yu, Z. Chen, J. B. Tenenbaum, and C. Gan, STAR: benchmark for situated reasoning in real-world videos, in NeurIPS, 2021, datasets & Benchmarks Track. [Online]. Available: https://arxiv.org/abs/2405.09711 [22] X. Tang, J. Qiu, L. Xie, Y. Tian, J. Jiao, and Q. Ye, Adaptive keyframe sampling for long video understanding, in CVPR, 2025, arXiv:2502.21271. [23] S. Liu, C. Zhao, T. Xu, and B. Ghanem, Bolt: Boost large vision-language model without training for long-form video understanding, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025, pp. 33183327. [24] H. Zhang, X. Li, and L. Bing, Video-llama: An instruction-tuned audio-visual language model for video understanding, arXiv preprint arXiv:2306.02858, 2023. [Online]. Available: https://arxiv.org/abs/2306.02858 [25] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, Video-chatgpt: Towards detailed video understanding via large vision and language models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. [26] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: bootstrapping language-image pre-training with frozen image encoders and large JMLR.org, language models, in Proceedings of the 40th International Conference on Machine Learning, ser. ICML23. 2023. [27] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, Instructblip: towards general-purpose vision-language models with instruction tuning, in Proceedings of the 37th International Conference on Neural Information Processing Systems, ser. NIPS 23. Red Hook, NY, USA: Curran Associates Inc., 2023. [28] B. Li et al., Llava-onevision: Easy visual task transfer, arXiv preprint arXiv:2408.03326, 2024. [Online]. Available: https://arxiv.org/abs/2408.03326 [29] Gemini Team, Gemini 2.5: Unlock multimodal intelligence, arXiv preprint arXiv:2507.06261, 2025. [Online]. Available: https://arxiv.org/abs/2507.06261 [30] OpenAI, Gpt-4o, 2024, model card and system card. [31] B. Feng, Z. Lai*, S. Li, Z. Wang, S. Wang, P. Huang, and M. Cao, Breaking down video llm benchmarks: in NeurIPS Workshop, 2025. [Online]. Available: Knowledge, spatial perception, or true temporal understanding? https://arxiv.org/abs/2505.14321 [32] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang, Video question answering via gradually refined attention over appearance and motion, in ACM Multimedia, 2017. [33] J. Xu, T. Mei, T. Yao, and Y. Rui, Msr-vtt: large video description dataset for bridging video and language, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 52885296. [34] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, Deformable detr: Deformable transformers for end-to-end object detection, in ICLR, 2021. [35] N. Wojke, A. Bewley, and D. Paulus, Simple online and realtime tracking with deep association metric, in 2017 IEEE International Conference on Image Processing (ICIP), 2017. [36] T. Perrett, A. Darkhalil, S. Sinha, O. Emara, S. Pollard, K. K. Parida, K. Liu, P. Gatti, S. Bansal, K. Flanagan, J. Chalk, Z. Zhu, R. Guerrier, F. Abdelazim, B. Zhu, D. Moltisanti, M. Wray, H. Doughty, and D. Damen, Hd-epic: highly-detailed egocentric video dataset, in 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 23 90123 913. [37] Qwen Team, Qwen2: family of open large language models, 2024, alibaba Cloud. [38] , Qwen2.5 technical report, 2024, alibaba Cloud. [39] Meta AI, The llama 3 herd of models, 2024, model release report. [40] W.-L. Chiang, Z. Li et al., Vicuna v1.5: An open-source chatbot, 2023, fastChat project report. Preprint. Under review. [41] T. Chavdarova, P. Baque, S. Bouquet, A. Maksai, C. Jose, T. Bagautdinov, L. Lettry, P. Fua, L. Van Gool, and F. Fleuret, Wildtrack: multi-camera hd dataset for dense unscripted pedestrian detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 50305039. [42] B. Shuai, A. Bergamo, U. Buechler, A. Berneshawi, A. Boden, and J. Tighe, Large scale real-world multi-person tracking, in European Conference on Computer Vision. Springer, 2022, pp. 504521. [43] S. Bai et al., Qwen2.5-vl https://arxiv.org/abs/2502.13923 technical report, arXiv preprint arXiv:2502.13923, 2025. [Online]. Available: [44] OpenAI, Introducing gpt-4.1 in the api, https://openai.com/index/gpt-4-1/, 2025. [45] Gemma Team, Gemma 3 technical report, arXiv preprint arXiv:2503.19786, 2025. [Online]. Available: https: //arxiv.org/abs/2503.19786 [46] Meta AI, Llama 4 model card (scout models), https://github.com/meta-llama/llama-models/blob/main/models/llama4/ MODEL CARD.md, 2025, model card; no public technical report for Llama 4 Scout as of Nov 2025. [47] W. Wang et al., Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency, arXiv preprint arXiv:2508.18265, 2025. [Online]. Available: https://arxiv.org/abs/2508. [48] T. Yu et al., Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe, arXiv preprint arXiv:2509.18154, 2025. [Online]. Available: https://arxiv.org/abs/2509.18154 [49] J. Li, D. Li, C. Xiong, and S. Hoi, Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in International conference on machine learning. PMLR, 2022, pp. 12 88812 900. Preprint. Under review. HERBench: Benchmark for Multi-Evidence Integration in Video Question Answering"
        },
        {
            "title": "7 Implementation Details",
            "content": "This section provides comprehensive implementation details for the HERBench construction pipeline, which employs tripartite structure to enforce high evidential requirements (ER). We detail the algorithms, mathematical formulations, thresholds, and quality control procedures used to transform raw videos into the final dataset."
        },
        {
            "title": "7.1 Track Ranking and Selection",
            "content": "Tracking and Trajectory Refinement. We utilize detection-tracking stack where RF-DETR detectors feed into the DeepSORT multi-object tracker. We apply high-recall detector with confidence threshold of 0.3 and per-frame cap of 300 detections. Association uses two-stage IoU matching: high-confidence detections (score > 0.5) are matched with an IoU threshold of 0.7, followed by lower-confidence detections with relaxed IoU threshold of 0.35. To enforce physical plausibility, we apply an outlier removal step that explicitly discards per-frame boxes implying implausible motion (velocity > 50 pixels/frame) to eliminate spurious detections. To ensure continuity, we apply gap interpolation for missing detections up to 30 frames (1s at 30 fps) and trajectory smoothing via Gaussian filtering (window size 5). We specifically address track fragmentation by detecting merge candidates (Ti, Tj) that are temporally ordered with gap 30 frames and spatially compatible. We minimize the following merge cost: Cmerge = tgap + last cj irst2 ci IoU(boxi last, boxj irst) (2) where denotes the bounding box centroid. The overall tracking, post-processing, and ranking pipeline is visualized in Figure 6. TrackRank scoring function. To select the top [6, 10] salient entities per video, we compute composite TrackRank score Si that aggregates metrics for each track (all computed per video and normalized by the maximum over tracks). Unlike simple duration-based ranking, we use the following weighted formulation: Si = (cid:80) wk Mi,k (cid:80) wk (3) The specific components and their empirically tuned weights are: Duration (w = 2.0) & Size (w = 1.0): Favors tracks with sustained presence and higher average bounding box area. Associated Objects (w = 2.0): Normalized count of distinct non-person object classes overlapping the persons box (IoU > 0.2). Center Distance (w = 2.4) & Motion (w = 1.0): Euclidean distance between first and last centroids, favoring traversals over stationary behavior. Appearance Exceptionality (w = 2.2): We quantify rarity as the normalized L1 distance from the datasets average appearance in feature space (HSV and LBP histograms). Scene Coverage (w = 1.5): Area of the Convex Hull enclosing the tracks boxes. Quality Metrics: Aggregates Average Confidence (w = 0.8, mean detection score), Smoothness (w = 0.7, computed as 1 minus normalized acceleration magnitude to penalize jitter), and Aspect-Ratio Stability (w = 0.5, defined as 1 minus the standard deviation of width/height ratios to penalize shape fluctuations). Preprint. Under review. Figure 6: Tracking, post-processing, and ranking pipeline. RF-DETR detections are linked with DeepSORT into raw person tracks, followed by outlier removal, gap interpolation, and Gaussian smoothing. TrackRanker then scores and selects salient trajectories, which are passed to an MLLM descriptor module to generate temporally decoupled appearance (A) and behavior (B) cards that serve as the scaffold for downstream HERBench tasks. Hard Filter Cascade. Prior to ranking, we enforce hard filter: we keep only the COCO person class, require length 20 frames, average area 5, 500 pixels, and require the track center to fall within the central safe region (frame cropped by 10% margins) in at least 5 frames. Diversity Sampling Strategy. To ensure diversity among the selected tracks, we employ round-robin selection across rankings generated from multiple perturbed weight configurations (γ (0.5, 1.5)). This prevents redundancy (e.g., selecting visually identical pedestrians) and ensures broad coverage of high-quality entities, which are subsequently manually validated to exclude phantom detections or identity switches."
        },
        {
            "title": "7.2 Decoupled Descriptor Generation",
            "content": "A-card and B-card generation. For each selected track, we generate disentangled descriptions using GPT-4o. We sample 10-11 crops, reserving the first and last 20% of the trajectory for Appearance (A-cards) and the middle 60% for Behavior (B-cards). This ensures temporal gap of at least 30 frames between appearance and behavior cues. An example of the resulting disentangled Aand B-cards for single track is shown in Figure 7. We use the following prompt structure: System prompt. For the following tasks, use only your vision capabilities. When referring to directions, use the cameras point of view. 1. Person Description. All images depict the same individual. In 24 sentences, describe their appearance in detail: clothing types and colors, accessories, hair, body build, and any distinctive features that make them easy to pick out. Do not mention position in the frame or any actions. 2. Path Description. In 37 sentences, describe the persons path and behavior over time. Mention the overall path shape, entry and exit edges, stops, and interactions. Do not repeat any appearance details from the first description. To visualize the output of this pipeline, Figure 7 presents qualitative examples of the generated Appearance (A) and Behavior (B) cards alongside their corresponding tracked image crops. These examples highlights the effectiveness of the temporal split: the tracked visual crops from the start and end of the trajectory inform the static attribute descriptions in the A-card, while the central frames drive the dynamic action summaries in the B-card. This separation ensures that the descriptors remain disentangled. Leakage prevention. To strictly enforce the Look & Separate principle, we calculate the token-level Jaccard similarity between the generated A-card and B-card. We set the Jaccard threshold to 0.15 based on manual inspection: above this, descriptors often share explicit appearance/behavior leakage."
        },
        {
            "title": "7.3 Spatial Operations and Region Definitions",
            "content": "Entry/exit edge labeling. For tasks like Region-Localized People Counting (RLPC), we define entry and exit edges based on the position of tracks centroid in its first and last frames. Let ct = (xt, yt) be the centroid at frame of track with start frame tstart and end frame tend, and let W, denote the frame width and height. We say that track enters through edge if ctstart lies in the corresponding edge band, and exits through edge if ctend lies in the band of e. Preprint. Under review. Figure 7: Example of disentangled Aand B-cards. For single tracked individual (highlighted trajectory in the top-left strip), we show the sampled frames and the corresponding appearance (A-card) and behavior (B-card) descriptions. The A-card captures only static visual attributes (clothing, colors, accessories, physique), while the B-card describes the persons path, timing, and interactions over time without repeating appearance cues, enforcing the Look & Separate principle. The top edge band is defined as < 0.3H, the bottom as > 0.85H, and the left/right edges as the outer 15% of the width (x < 0.15W and > 0.85W , respectively). Region-of-interest (ROI) membership. For [RLPC], we also define rectangular ROIs (e.g., frame halves or specific zones). track is counted as visiting an ROI if, at any frame, at least 50% of its bounding box area lies within the region (Intersection-Over-Box 0.5). We count the unique track IDs that satisfy this predicate to derive people counts under spatial constraints. Duration computation (MPDR). We compute visible-time intervals (tstart, tend) for every track. Using interval algebra, we determine ground truth for questions such as Who stayed longest? or Who entered first? by comparing duration scalars (tend tstart) and timestamps."
        },
        {
            "title": "7.4 Scene Card Perturbations",
            "content": "Shot Segmentation and Description. We use TransNetV2 for shot boundary detection. For the Scene Verification Arrangement (SVA) task, faithful scene cards are generated via an MLLM using the following prompt: Describe concisely the scene in one sentence without reference to the scene, refer (if relevant) to the entities, genders and appearance (type and colors of hair/clothing/accessories) of each entity, occurrence, actions, background, and location. Perturbation Engine. To generate negative samples for SVA, we prompt the model to modify faithful descriptions by altering 2-5 atomic details. The prompt constraints ensure: Modifications: Change existing details (color, count, attributes). Additions: Insert plausible but absent elements (extra objects, background items). Plausibility: Changes must be false but highly plausible within the context of the video. An example of faithful scene card and its perturbed counterpart used for the SVA task is shown in Figure 8."
        },
        {
            "title": "7.5 Corpus-Plausible Foil Generation",
            "content": "Ground Truth Integration. For tasks requiring verification of absence, we leverage human-verified event logs. Preprint. Under review. Figure 8: Faithful and perturbed scene cards for SVA. The top card provides faithful one-sentence description of shot, mentioning the main actor, appearance, background, and motion. The bottom card is perturbed variant where 2-5 atomic details (e.g., clothing pattern, background appearance, additional objects) are modified or added while remaining globally plausible. These pairs form positive and negative options in the Scene Verification & Arrangement task, probing fine-grained scene-level sensitivity to small but visually significant details. False Action Memory (FAM): We sample false action by pairing an object present in the video with an action from the corpus that does not occur in the current video. False Object Memory (FOM): We select an absent object from the corpus-wide index that is compatible with actions present in the video (e.g., if cutting occurs, carrot is valid distractor if absent). Action Counting (AC): Distractor counts are generated such that the correct counts rank varies uniformly across options. Action Sequence Integrity (ASII): We sample 5-event ground-truth timeline. Distractors are generated using two perturbation functions: swap mid (swapping two non-adjacent events) and rotate (shifting the sequence). Crucially, we verify against the event log that the perturbed timeline does not accidentally exist in the video."
        },
        {
            "title": "7.6 Text-Only Bias Filtering Details",
            "content": "Filtering procedure. To suppress language priors, we apply rigorous Text-Only Filtering stage. We discard any question correctly answered by 3 of 4 blind LLMs (Qwen2-7B, Qwen2.5-7B, Llama-3-8B, and Vicuna-7B v1.5). This step rejects approximately 10% of candidates (e.g., questions answerable via object-color co-occurrence priors)."
        },
        {
            "title": "7.7 Human Verification Protocol",
            "content": "Verification checklist. Experts conduct verification on stratified 15% sample. The checklist includes: Minimum Frame-Set: Confirming the question requires 3 distinct frames. Uniqueness: Ensuring unique, objective ground-truth answer exists. Disentanglement: Verifying A/B cards do not leak information. This process resulted in an 17.8% rejection rate."
        },
        {
            "title": "7.8 Dataset Statistics",
            "content": "Scale and Video Characteristics. HERBench comprises 26,806 questions derived from 336 unique videos. The videos feature substantial duration (avg. 395s, range 60-2100s) to ensure temporal dispersion of evidence. Sources include HD-EPIC, WildTrack, PersonPath22, and movie trailers. Preprint. Under review. Question Properties. The average question length is 65.5 tokens with vocabulary of 7.3k unique word types. Questions are strictly balanced across 5 multiple-choice options. The mean temporal span of evidence required per question is 101.1 seconds."
        },
        {
            "title": "8 Extended Experimental Results & Analysis",
            "content": "We provide deeper quantitative analysis of the challenges posed by HERBench, expanding on the MRFS metrics and frame selection ablation."
        },
        {
            "title": "8.1 Extended MRFS Analysis",
            "content": "Per-Task MRFS. Table 4 details the Minimum Required Frame-Set statistics. We observe distinct correlation between the reasoning scope of task and its evidential requirement. Tasks requiring global chronology and the integration of multiple semantic units, specifically [TSO] (Temporal Shot Ordering, MRFS 9.05), [FAM] (False Action Memory, MRFS 6.77), and [SVA] (Scene Verification, MRFS 6.74), naturally exhibit the highest MRFS. To answer these questions correctly, model must aggregate evidence from widely dispersed video segments or perform an exhaustive search to verify absence, effectively precluding single-frame shortcuts. In contrast, tasks focused on local attributes or spatially constrained counting, such as [RLPC] (Region-Localized People Counting, MRFS 3.11) and [AGAR] (Attribute Recognition, MRFS 3.85), require fewer distinct frames. However, even these lower MRFS values demonstrate that reliance on single frame is insufficient, confirming that HERBench successfully enforces multi-evidence integration even for localized tasks. The overall weighted mean MRFS of 5.49 validates the benchmarks design goal: forcing models to look at multiple snapshots to derive correct answers. Table 4: Per-task MRFS statistics Computed with = 16 using Qwen2.5-VL and AKS. Task [AC] [ASII] [AGAR] [AGBI] [AGLT] [FAM] [FOM] [RLPC] [MEGL] [MPDR] [SVA] [TSO] Total Mean MRFS 1623 2127 876 1226 2362 1962 2022 2138 3061 2717 4569 2123 5.26 6.00 3.85 3.81 4.45 6.77 5.14 3.11 6.33 4.30 6.74 9.05 5.49 Total / Weighted Mean 26,806 MRFS vs Accuracy As illustrated in Figure 9, there appears to be an inverse relationship between the evidential demand of benchmarkquantified by the Mean MRFSand the performance of state-of-the-art Video-LLMs. Existing benchmarks such as NeXT-QA exhibit lower evidential requirement (2.61 MRFS), where Qwen 2.5 VL 7B achieves relatively high accuracy (76.3%), possibly due to the feasibility of single-frame shortcuts or language priors. In contrast, HERBench presents higher burden (5.49 MRFS), designed to require the integration of non-redundant, temporally separated cues. This increased demand coincides with lower accuracy of 35.9%, pattern that is consistent with the hypothesized fusion deficit in current architectures. These results suggest that while models may be effective at retrieving isolated frames, their capacity for compositional reasoning appears to be increasingly challenged as the number of required evidence pieces grows. Preprint. Under review. Figure 9: Impact of Evidential Requirement on Model Accuracy. We plot the Mean Minimum Required Frame-Set (MRFS) against Full-context Accuracy (k = 16), measured using Qwen 2.5 VL 7B, across four video QA benchmarks. The data suggests an inverse trend: as the necessity to aggregate distinct visual cues increases (higher MRFS), model performance tends to decrease. HERBench (green) imposes higher evidential burden (MRFS 5.49), highlighting the potential challenges current Video-LLMs face in multi-evidence integration relative to benchmarks with lower requirements like NeXT-QA. Table 5: Frame Selection Ablation. Accuracy (%) on random subsample of questions. GT Frames (OF) represents the upper bound with manually curated evidence. Model Selector AC AGAR AGBI AGLT ASII FAM FOM MEGL MPDR RLPC SVA TSO Mean InternVL3. Qwen3-VL Ovis-2.5 23.0 Uniform Vanilla-BLIP 26.0 22.0 BOLT-ITS 27.0 AKS GT Frames 24.0 Uniform 26.0 Vanilla-BLIP 27.0 25.0 BOLT-ITS 24.0 AKS GT Frames 24.0 Uniform 25.0 Vanilla-BLIP 32.0 35.0 BOLT-ITS 25.0 AKS GT Frames 30.0 75.0 74.0 72.0 66.0 81. 67.0 71.0 68.0 65.0 69.0 79.0 77.0 78.0 76.0 85.0 77.0 76.0 74.0 77.0 81.0 78.0 76.0 75.0 73.0 73.0 81.0 83.0 82.0 80.0 84.0 70.0 71.0 71.0 74.0 79. 68.0 66.0 66.0 69.0 71.0 71.0 69.0 70.0 74.0 80.0 32.0 27.0 20.0 36.0 20.0 34.0 26.0 27.0 29.0 35.0 34.0 17.0 17.0 31.0 23.0 30.0 27.0 27.0 29.0 50. 30.0 24.0 21.0 22.0 50.0 35.0 29.0 28.0 39.0 60.0 30.0 29.0 33.0 30.0 39.0 24.0 23.0 33.0 27.0 25.0 34.0 33.0 33.0 39.0 39.0 34.0 28.0 30.0 35.0 27. 16.0 30.0 30.0 20.0 24.0 35.0 37.0 38.0 30.0 40.0 48.0 46.0 48.0 54.0 52.0 36.0 37.0 38.0 35.0 36.0 38.0 44.0 46.0 49.0 41.0 27.0 33.0 33.0 33.0 32. 23.0 19.0 21.0 22.0 21.0 21.0 19.0 18.0 17.0 21.0 23.0 41.0 41.0 43.0 27.0 36.0 33.0 17.0 37.0 53.0 50.0 56.0 57.0 49.0 61.0 65.0 58.0 60.0 51.0 68.0 0.0 0.0 0.0 0.0 3. 0.0 0.0 0.0 0.0 4.0 42.7 42.1 41.1 42.7 47.8 37.7 37.9 38.4 36.2 41.0 43.1 41.6 42.1 42.6 47.9 Preprint. Under review."
        },
        {
            "title": "8.2 Full Frame-Selection Ablation",
            "content": "To more precisely disentangle the role of evidence retrieval from that of multi-evidence fusion, we perform an extensive ablation over five frame selection strategiesUniform, Vanilla-BLIP, BOLT-ITS, AKS, and Oracle Frames (OF)and evaluate their effect across all twelve HERBench tasks  (Table 5)  . Overall, learned strategies such as BOLT-ITS and AKS provide moderate gains over Uniform sampling, reflecting their ability to prioritize query-relevant frames while maintaining broader temporal coverage. However, their improvements are uneven across tasks: both methods show the largest benefits in sparse-evidence settings such as [TSO] and [FAM], where the critical evidence may appear only briefly within long videos. The oracle-based setting establishes an upper bound by supplying the manually curated evidence frames used during dataset construction. As shown in the rightmost column of Table 5, all three representative models experience non-trivial but still limited performance improvements in the OF regime (typically +3-6 absolute accuracy points relative to the best learned selector). Importantly, the OF results highlight two key phenomena. First, even perfect access to the relevant frames does not resolve the majority of model failures: fusion-bound tasks such as [AC], [RLPC], and [MEGL] remain bottlenecks with accuracies barely above chance, indicating that retrieval is not the sole limiting factor. Second, improvements under OF are disproportionately large for temporally global tasks such as [TSO] and [SVA], where correct reasoning requires coordinating multiple distant, non-overlapping visual clues. Here retrieval quality is dominant factor, and learned selectors struggle to consistently surface all required frames. However, the inability of models to capitalize fully on oracle-quality evidence emphasizes that multi-frame integration itself remains major unresolved challenge. Taken together, these results reinforce two-stage deficit: (i) an evidence retrieval bottleneck, where existing selectors fail to reliably surface all critical cues, and (ii) more fundamental fusion bottleneck, where models fail to combine available cues even when retrieval uncertainty is eliminated. HERBenchs high evidential density and stringent cue separation make both deficits sharply visible, underscoring the need for future MLLMs to improve not only frame selection but also the downstream mechanisms for multi-cue aggregation."
        },
        {
            "title": "9 Illustrative Examples for All Tasks",
            "content": "This section provides qualitative examples for all twelve HERBench tasks, each figure displays one representative structured question for the corresponding task. However, each task in HERBench contains many distinct question structures and evidential templates, and the examples below illustrate only single instance of the broader variability present in the dataset. Temporal Reasoning & Chronology. Figure 10 presents an example of the Temporal Shot Ordering (TSO) task, which requires reconstructing the chronological order of four non-overlapping shots. Figure 11 shows the Multi-Person Duration Reasoning (MPDR) task, where models must compare visible-time intervals across multiple individuals. Figure 12 illustrates the Action Sequence Integrity & Identification (ASII) task, requiring identification of the correct sequence among plausible permutations of narrated events. Referring & Tracking. Figure 13 shows the Appearance-Grounded Behavior Interactions (AGBI) task, where models must track target described only by appearance and determine who interacts with them. Figure 14 provides an example of the Appearance-Grounded Attribute Recognition (AGAR) task, requiring attribute extraction anchored to the tracked target. Figure 15 illustrates the Appearance-Grounded Localization Trajectory (AGLT) task, where the model must infer how the target enters or exits the scene. Global Consistency & Verification. Figure 16 presents the False Action Memory (FAM) task, requiring verification of which plausible action did not occur in the video. Figure 17 shows the Scene Verification & Arrangement (SVA) task, combining faithful and perturbed shot descriptions to assess fine-grained scene-level verification and ordering. Figure 18 depicts the False Object Memory (FOM) task, requiring identification of plausible but absent object interaction. Multi-Entity Aggregation & Numeracy. Figure 19 provides an example of the Multi-Entities Grounding & Localization (MEGL) task, where models must verify which appearance-described individuals actually appear in the video. Figure 20 illustrates the Action Counting (AC) task, requiring enumeration of all instances of specified actionobject Preprint. Under review. Figure 10 pair across the entire video. Finally, Figure 21 shows the Region-Localized People Counting (RLPC) task, where the model must count unique individuals entering through specific spatial regions. Preprint. Under review. Figure 11 Preprint. Under review. Figure Figure 13 Preprint. Under review. Figure 14 Figure 15 Preprint. Under review. Figure Figure 17 Preprint. Under review. Figure 18 Figure 19 Preprint. Under review. Figure Figure"
        }
    ],
    "affiliations": [
        "Ben-Gurion University of the Negev, Israel",
        "INSIGHT Lab, Ben-Gurion University of the Negev, Israel"
    ]
}