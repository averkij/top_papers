{
    "paper_title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation",
    "authors": [
        "Wenhao Wang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io."
        },
        {
            "title": "Start",
            "content": "TIP-I2V: Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation Wenhao Wang University of Technology Sydney wangwenhao0716@gmail.com Yi Yang* Zhejiang University yangyics@zju.edu.cn 4 2 0 2 5 ] . [ 1 9 0 7 4 0 . 1 1 4 2 : r Figure 1. TIP-I2V is the first dataset comprising over 1.70 million unique user-provided text and image prompts. Besides the prompts, TIPI2V also includes videos generated by five state-of-the-art image-to-video models (Pika [5], Stable Video Diffusion [8], Open-Sora [73], I2VGen-XL [71], and CogVideoX-5B [69]). The TIP-I2V contributes to the development of better and safer image-to-video models."
        },
        {
            "title": "Abstract",
            "content": "Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-ofthe-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this largescale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and Diffu- *Corresponding author. sionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multidimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io. 1. Introduction Image-to-video diffusion models transform static images into dynamic videos, with wide-ranging applications in animation, content creation, and visual storytelling [21, 39, 67, 71, 72]. As video generation becomes more commercialized, image-to-video methods are increasingly preferred over text-to-video for several reasons: (1) they offer users more control, allowing for precise direction of objects to perform specific actions; (2) they provide greater consistency, enabling narratives focused on central subject; and (3) they are more practical, particularly on social media where image-driven videos tend to gain higher engagement. Despite their popularity and importance, there currently lacks dataset from the users perspective one that features user-provided text and image prompts alongside the corresponding generated videos. Such dataset could help improve the alignment of image-to-video models with realworld user needs, while also enhancing safety. Therefore, this paper conducts the first study of its kind in the imageto-video community. Specifically, we focus on curating the first image-to-video prompt-gallery dataset, analyzing the differences between the proposed dataset and similar ones, and exploring the new research directions inspired by us. The first Text and Image Prompt dataset for Imageto-Video generation (TIP-I2V). As shown in Fig. 1, our TIP-I2V dataset includes over 1.70 million unique userprovided text and image prompts for image-to-video diffusion models, along with the corresponding generated videos, sourced from Pika Discord channels [5]. It is important to note that: (1) We intend to include videos generated by other state-of-the-art image-to-video diffusion models, including Stable Video Diffusion [8], Open-Sora [73], I2VGen-XL [71], and CogVideoX-5B [69]; however, due to limited computing resources, we only use 100, 000 randomly selected prompts to generate videos for each imageto-video model. Researchers are free to extend our TIP-I2V by generating more videos with these methods (or other state-of-the-arts, such as Open-Sora-Plan [31]) and our prompts; (2) we acknowledge that the currently generated videos are not perfect, and in the future, researchers are encouraged to use newly released image-to-video models (such as Sora [4] and Movie Gen [3]) and our prompts to further extend our TIP-I2V. Besides prompts and generated videos, our TIP-I2V also includes Universally Unique Identifier (UUIDs), timestamps, embeddings, subjects, and notsafe-for-work (NSFW) scores for these data points. Differences between TIP-I2V and other similar datasets in basic and semantic information. We notice that there are two popular prompt-gallery datasets in the visual generation community, i.e., VidProM [58] for text-tovideo and DiffusionDB [64] for text-to-image. Our TIP-I2V mainly differs from them in: (1) Basic information: Both VidProM and DiffusionDB begin the generation with text, while our TIP-I2V starts with text and an image. (2) Semantics: Each text in our TIP-I2V focuses on how to bring static elements in the corresponding image to life through motion. For instance, some text prompts are the hair and body movement, dancing, the flowers falling, the woman move, and make the statue broke down. In contrast, the prompts in VidProM and DiffusionDB are more descriptive, i.e., they directly specify the content to be generated without referencing specific object, such as dragon flying over medieval city, serene beach at sunset, and an astronaut walking on Mars. The differences in basic and semantic information highlight need for specialized image-to-video prompt dataset. Exciting new research areas inspired by TIP-I2V. Our TIP-I2V helps researchers develop better and safer imageto-video diffusion models. For better models: (1) Enhancing user experience. Before TIP-I2V, researchers do not know which subjects users prefer to transform into videos and what directions they expect. However, with our dataset, researchers can cater to more users in the real world with limited training sources. This avoids the problem researchers previously faced, where blindly expanding the training set led to wasted resources and low user sat- (2) Improving evaluation practicality. Existisfaction. ing image-to-video benchmarks, such as VBench-I2V [25], I2V-Bench [49], and AIGCBench [18], suffer from limited number of topics and prompts designed by experts, which may not accurately reflect real-world user needs. With the help of TIP-I2V, researchers can build more comprehensive and practical benchmark for evaluating image-to-video models. For safer models: major safety concern of image-to-video generation is misinformation, i.e., they can make objects or humans in images to perform actions they never did. For instance, given an image of Elon Musk and Donald Trump together, image-to-video models could generate video of them fighting, misleading the public. To address this issue, our TIP-I2V allows researchers to train model to (1) distinguish between generated videos from images and real videos, and (2) trace the source image from any given frame in generated video. Beyond these areas, we also encourage researchers to explore additional directions inspired by the TIP-I2V. In conclusion, our key contributions are as follows: 1. We present TIP-I2V, the first dataset of text and image prompts specifically for image-to-video generation. This dataset contains over 1.70 million prompts from real users, along with corresponding videos generated by five state-of-the-art image-to-video diffusion models. 2. We compare TIP-I2V with two popular prompt datasets, i.e., VidProM (text-to-video) and DiffusionDB (text-toimage), highlighting their differences in both basic and semantic information. This emphasizes the need for specialized image-to-video prompt dataset. 3. We demonstrate how TIP-I2V contributes to building better and safer image-to-video diffusion models. Specifically, it can help enhance user experience, improve the practicality of evaluation, distinguish generated videos from real ones, and trace the source image. 2 Figure 2. data point in our TIP-I2V includes UUID, timestamp, text and image prompt, subject, NSFW status of text and image, text and image embedding, and the corresponding generated videos. 2. Related Works ity of this task, our paper aims to fill this gap. Video Generation. With the introduction of Sora [4], there has been increasing interest in video generation, leading to many real-world applications. Previous researchers [9, 10, 29, 56] have primarily focused on text-to-video generation, exploring methods for synthesizing realistic videos directly from textual descriptions. However, main drawback of text-to-video is uncontrollability and inconsistency. For instance, the short film Air Head produced by Sora [4] takes professional film crews long time to control that the man has consistent yellow balloon head. promising solution is image-to-video generation, where video is created from an image, and text is used to control the objects within the image. This makes the generated videos more consistent. The videos created from images by recent works such as [1, 2, 8, 71] are rapidly gaining popularity on social media platforms. This paper also focuses on image-to-video generation, but from different perspective, i.e., researching user-provided text and image prompts. Text-Video Datasets. text-video dataset is collection of video clips with corresponding textual descriptions or captions. There are many popular text-video datasets, such as HDVILA-100M [68], WebVid-10M [7], Panda-70M [12], InternVid [62], OpenVid-1M [38], and MiraData [26], offering high-resolution and diverse content. Unlike other datasets that consist of caption-(real)-video pairs, our TIP-I2V dataset contains real user-provided text and image prompts, along with the corresponding generated videos. To further highlight the difference, in the Supplementary (Section 7), we also provide WIZMAP visualization comparing the texts in our TIP-I2V with those in Panda-70M [12]. Prompt Datasets. With the rapid spread of large AI models (such as large language models and diffusion models), research on prompts has become particularly important, as they form the foundation of efficient user interactions with AI systems. Based on this background, in the text-to-text community, [74] aggregates 43 existing datasets from various domains and tasks to create prompt dataset, while [6] develops PromptSource, system for creating, sharing, and managing prompts for natural language processing (NLP) tasks. In the visual generation domain, VidProM [58] and DiffusionDB [64] collect prompts for textto-video and text-to-image tasks, respectively. However, to the best of our knowledge, there are no prompt datasets for the image-to-video task. Given the importance and popular3. Curating TIP-I2V Fig. 2 is an example data point in the proposed TIP-I2V. It includes UUID, timestamp, text and image prompts, subject, NSFW status for text and images, embeddings for both text and images, as well as generated videos from five state-of-the-art image-to-video diffusion models. The following explains how we collect these pieces of information. Collecting source HTML files. We collect chat messages from Pikas official Discord channels between July 2023 and October 2024 by using DiscordChatExporter [20] to save them as HTML files. According to Pikas terms of service (see exact words in the Supplementary (Section 8)), these chat messages are publicly available under the CC BY-NC 4.0 License. As result, we comply with this license and release our TIP-I2V under the same terms. This collection process is similar to that of the well-established datasets VidProM [58] and DiffusionDB [64]. Extracting text prompts, scraping Pika videos, and parsing image prompts. We use regular expressions to extract text prompts and their corresponding video links from HTML files. After deduplicating the text prompts and validating the video links, we obtained 1, 701, 935 unique text prompts along with their corresponding 3s-length Pika videos. Since the original image prompts are not accessible, and the image-to-video models developed by Pika utilize these user-inputted prompts as the first frames of generated videos, the image prompts in our TIP-I2V are parsed from the scraped videos. We have checked the quality of these image prompts and find that they are of high quality. Assigning UUIDs, timestamps, subjects, embeddings and NSFW status. To facilitate subsequent research utilizing our TIP-I2V, we (1) calculate UUIDs based on unique prompts, (2) extract timestamps from Pika videos, (3) infer subjects using GPT-4o [42], (4) embed text and image prompts using text-embedding-3-large [41] and CLIP [47], respectively, and (5) assign NSFW status to text and image prompts using Detoxify [54] and nsfw image detection [17], respectively. Generating videos using other image-to-video models. To diversify the proposed TIP-I2V, we also include videos generated by other state-of-the-art diffusion models, i.e., Stable Video Diffusion [8], Open-Sora [73], I2VGen-XL [71], and CogVideoX-5B [69]. See the Supple3 Table 1. Comparison of our TIP-I2V (image-to-video) with popular VidProM (text-to-video) and DiffusionDB (text-to-image) in terms of basic information. Our TIP-I2V is comparable in scale to these datasets but focuses on different aspects of visual generation. Details TIP-I2V (Ours) VidProM [58] DiffusionDB [64] Domain No. of unique text prompts No. of unique image prompts Embedding of text prompts Embedding of image prompts Maximum length of text prompts Time span No. of generation sources Image-to-Video 1, 701, 935 1, 701, 935 text-embedding-3-large CLIP 8192 tokens Jul 2023 Oct 2024 5 Text-to-Video 1, 672, 243 - text-embedding-3-large - 8192 tokens Jul 2023 Feb 2024 4 Collection method Web scraping + Local generation Web scraping + Local generation Text-to-Image 1, 819, 808 - CLIP - 77 tokens Aug 2022 1 Web scraping mentary (Section 9) for details on how we adopt these models. Due to the time constraint (for instance, generating single video with CogVideoX-5B [69] requires 294 seconds on standard A100 GPU), we limit the number of generated videos for each diffusion model to 100, 000. This number of generated videos is likely to be sufficient for drawing conclusions in subsequent research. Extension. The large-scale collection of user-provided prompts enables future researchers to extend the TIP-I2V. First, if they currently need more generated videos, they can generate additional videos using the diffusion models we employed or other state-of-the-art models. Additionally, as more advanced image-to-video models become available in the future, researchers can leverage these models with our prompts to generate higher-quality videos. 4. Comparing TIP-I2V with Similar Datasets In this section, we compare the proposed TIP-I2V with similar datasets, i.e., VidProM [58] and DiffusionDB [64], in terms of basic information and prompt semantics. The differences highlight the necessity of introducing specialized prompt dataset for image-to-video generation. Basic information. As shown in Table 1, we compare the basic information of TIP-I2V with two existing promptgallery datasets, i.e., VidProM [58] (text-to-video) and DiffusionDB [64] (text-to-image). It is observed that: (1) All three datasets have reached million-scale, providing support for the research of large-scale machine learning. (2) Unlike these datasets, which only contain text prompts, our TIP-I2V also includes image prompts. (3) Our dataset spans longer collection period and includes more generation sources compared to these datasets, which suggests greater diversity of prompts and generated outputs. Takeaway: The TIP-I2V and popular datasets are all large-scale, but we focus on different domain, which additionally needs image prompts. Prompt semantics. As shown in Fig. 3, we present comparison of the prompts used in our TIP-I2V, VidProM [58], and DiffusionDB [64]. Additionally, we compare the semantic distributions of the prompts across these datasets. From the comparisons, we conclude that: (1) The text prompts in our TIP-I2V serve as instructions to animate the specified object(s) described in the corresponding image prompts. For instance, on the left side of Fig. 3 (1), the user wants to see the bearded guy lower his hands. In contrast, the prompts in VidProM and DiffusionDB primarily describe the scenes that users expect to visualize. The drawback is that no matter how detailed users specify, the generation may always has visual differences from the images/videos they have in mind. (2) The WIZMAP [65] visualization shows that the text prompts in TIP-I2V have distinct distribution compared to those in VidProM and DiffusionDB. This further validates the semantic differences between our text prompts and those in existing datasets. Takeaway: The semantics of the text prompts in our TIP-I2V differs from those in popular datasets. 5. New Research based on TIP-I2V In this section, we first elaborate on four new research directions inspired by our TIP-I2V, followed by brief discussion of other potential ones. These directions collaboratively contribute to improving the quality and safety of image-to-video generation. 5.1. Catering Users Better Analysis of users preferred subjects and generation directions. In Fig. 4, we visualize the top 25 subjects and directions preferred by users. Calculation details and examples are provided in the Supplementary (Sections 10 and 11). Furthermore, in Fig. 5, we show the proportion of the sum of top subjects/directions relative to the total frequencies. We observe that: (1) The users preferences are unbalanced. For instance, the top-3 subjects, i.e., person, astronaut, and portrait painting, are all humanrelated. Beyond the general action move, people are more likely to generate specific movements such as zoom, walk, and blink. (2) To cater to the preferences of the general audience, image-to-video model designers only 4 Figure 3. Our TIP-I2V (image-to-video) differs from popular VidProM (text-to-video) and DiffusionDB (text-to-image) in terms of semantics. Top: Example prompts from the three datasets. Bottom: The WIZMAP [65] visualization of our TIP-I2V compared to VidProM/DiffusionDB. Please (cid:252) zoom in to see the detailed semantic focus of text prompts across the three datasets. need to focus on relative small number of subjects and directions. Specifically, to cover 80%/90% users preferences, researchers only need to focus on 2, 721/6, 586 subjects and 309/929 directions, respectively. These two observations imply that, to create successful commercial image-to-video model, researchers may only need to focus on these specific subjects and directions, rather than wasting resources and time to expand datasets blindly. Therefore, future research may focus on: User-oriented training datasets for video generation. In the future, researchers may search for top subjects on free-to-use video platforms and scrape the resulting videos. Unlike previous large-scale datasets, which were randomly collected from websites, datasets curated through this procedure will better align with users expectations, and models trained on them are likely to gain more popularity. More precise semantic segmentation for subjects. Although the subjects in our TIP-I2V are inferred by the powerful GPT-4o [42], some semantic overlap is inevitable. For instance, both person, people, and man appear as subjects. This may negatively impact the construction of user-oriented video datasets, and therefore researchers may design methods to minimize semantic overlap within subjects before scraping. 5.2. More Comprehensive and Practical Evaluation Based on TIP-I2V, we can conduct more comprehensive and practical evaluation of image-to-video models. As shown in Table 2, the current benchmarks for imageto-video generation face two main issues. Issue 1: comprehensiveness. Their benchmarks cover only limited range of subjects, which may result in the omission of many topics of interest. Issue 2: practicality. Their image prompts are directly extracted from frames in the public videos, and 5 Table 2. comparison of the proposed benchmark with existing ones. Our TIP-Eval is more comprehensive and practical. Benchmarks VBench-I2V [25] I2V-Bench [49] AIGCBench [18] TIP-Eval (Ours) Number Subjects Prompts 11 16 - 1, 355 2, 951 1, 000 10, 000 Prompt source Text Image Gen. Pexels YouTube Gen. Gen. WebVid Real users Figure 4. The top 25 subjects (top) and directions (bottom) preferred by users when generating videos from images. Figure 6. Benchmarking results using 10, 000 prompts in TIPEval and 10 dimensions from [18, 25, 49]. Similar to VBench [25], results are normalized per dimension for clearer comparisons. ity of dimensions (8 out of 10). This may somewhat contrast with the evaluation based on expert-designed prompts (as shown in the CogVideoX-5B paper), which emphasizes (2) No the practicality and importance of our TIP-Eval. image-to-video model outperforms across all dimensions, indicating the complexity of balancing different evaluation dimensions, such as consistency, dynamic, and alignment. (3) The performance of all models on the video-text alignment dimension is suboptimal, with the highest score being only 0.26. This indicates that current image-to-video models still struggle to accurately adhere to human control. Beyond the current analysis, in the future, researchers can use our TIP-Eval and TIP-I2V to explore: The targeted training for poorly performing subjects. The existing benchmarks, due to their limited subject scope, fail to inform researchers about the areas where their models perform well and where they fall short. With the TIP-Eval, researchers can identify underperforming subjects: for instance, in the case of the latest CogVideoX-5B [69], while it achieves an average aesthetic quality of 0.74 on the cottage subject, it only reaches an average aesthetic quality of 0.40 on the calligraphy subject. After identifying, researchers may gather targeted training videos and fineFigure 5. The ratio of the sum of the top subjects (top) or directions (bottom) to the total frequencies. their text prompts are generated by multimodal models. This may not accurately reflect the needs of real-world users and differs from their actual usage habits. To solve these problems, we propose TIP-Eval, benchmark consisting of 1, 000 of the most popular subjects, each paired with 10 text and image prompts provided by real users. Using these comprehensive and practical prompts along with the evaluation dimensions from [18, 25, 49], we benchmark the videos generated by five state-of-the-art image-to-video diffusion models. The visualization results are shown in Fig. 6; for the full experiments, please refer to the Supplementary (Section 12). We observe that: (1) From the users perspective, even the early-stage commercial image-to-video model (Pika [5]) outperforms the latest open-source one (CogVideoX-5B [69]) on the major6 Figure 7. case illustrating the misuse of image-to-video models, resulting in misinformation: given friendly image of Elon Musk and Donald Trump shaking hands, an image-to-video model can easily generate video of them fighting, which fuels political rumors. Table 3. The generalization experiments of existing fake image detection methods to identify generated videos from images. Accuracy (%) Blind Guess CNNSpot [57] FreDect [19] Fusing [27] LGrad [50] LNP [33] DIRE [63] UnivFD [40] mAP (%) Blind Guess CNNSpot [57] FreDect [19] Fusing [27] LGrad [50] LNP [33] DIRE [63] UnivFD [40] Pika 50. 50.7 47.8 50.0 54.7 58.2 50.2 48.5 Pika 50.0 49.0 44.7 47.7 56.8 82.1 49.8 40.1 SVD 50. 50.3 59.7 50.0 44.5 41.3 49.8 52.0 SVD 50.0 48.7 59.2 47.6 43.0 38.8 49.4 56.2 OpS 50. 50.7 47.2 50.5 44.4 43.1 50.3 53.4 OpS 50.0 54.0 50.2 60.1 42.2 37.3 47.9 60.1 IXL 50. 50.3 48.7 50.1 44.8 53.3 50.1 60.9 IXL 50.0 49.0 46.5 58.7 43.2 72.3 49.0 72.6 Cog 50. 50.3 59.2 49.9 46.5 42.5 50.6 50.7 Cog 50.0 44.7 59.8 44.3 45.2 41.9 51.5 48.6 Avg. 50. 50.5 52.5 50.1 47.0 47.7 50.2 53.1 Avg. 50.0 49.1 52.1 51.7 46.1 54.5 49.5 55.5 tune their image-to-video models accordingly. Evaluating models performance from direction perspective. While the existing benchmarks and our TIPEval evaluate image-to-video models across various subjects, it is equally important to assess whether the spatial transformations in the generated videos well-align with users expected directions. As illustrated at the bottom of Fig. 4 and 5, our TIP-I2V encompasses wide range of directions, such as zoom, run, and wave, which are desired by users. Therefore, using our TIP-I2V, future researchers may develop benchmark specifically focused on directional control to complement existing ones. 5.3. Identifying Generated Videos from Images Researchers should not overlook the safety issues in image-to-video generation while improving video quality. key safety concern is misinformation, because imageto-video models can easily manipulate people or objects in images to make them perform actions they never actually did, as exemplified in Fig. 7. In this section, we show how the proposed TIP-I2V helps combat such misinformation from the perspective of identifying generated videos from images. Specifically, the generated videos from five stateof-the-art models are split into separate sets to form TIP-ID dataset for training and testing the detectors. The details are shown in the Supplementary (Section 13). Table 4. Our trained strong detectors performance in classifying videos as real, text-generated, or image-generated. Same/Cross Domain refers to training and testing on the same or different diffusion models, respectively. Accuracy (%) Blind Guess Same Domain Cross Domain mAP (%) Blind Guess Same Domain Cross Domain Pika 33.3 93.2 84. Pika 33.3 98.7 94.4 SVD 33.3 97.3 92. SVD 33.3 99.7 97.3 OpS 33.3 96.9 93. OpS 33.3 99.6 98.2 IXL 33.3 97.9 73. IXL 33.3 99.8 86.5 Cog 33.3 96.2 92. Cog 33.3 99.4 97.5 Avg. 33.3 96.3 87. Avg. 33.3 99.4 94.8 unique challenge in detecting generated videos from images. As shown in Table 3, current fake image detection algorithms struggle to generalize when identifying such videos (note that because none of the state-of-theart models can process entire videos directly, we use the middle frame from each video as the input image for each model.). This is because each frame in these videos can be considered variant of the input real image, leading the detection algorithms to mistakenly classify these frames as real images. This unique characteristic of videos generated from images invalidates existing methods and calls for new efforts to address this challenge. surprising and strong detector built by us. To address this challenge and establish baseline for future research, we fine-tune VideoMAE [53] (see details in the Supplementary (Section 14)) to classify videos into three categories: (1) real, (2) text-generated, and (3) imagegenerated. The experiments are conducted in two settings: (1) Same domain: we train and test the model on videos generated by the same diffusion model; for instance, both the training and testing videos are from Pika. (2) Cross domain: we train and test the model on videos generated by different diffusion models; for example, training videos are from Stable Video Diffusion, Open-Sora, I2VGen-XL, and CogVideoX-5B, while test videos are from Pika. From the experimental results in Table 4, we conclude that: (1) simple classification model already achieves relative high performance in both same and cross domain settings. This result is somewhat surprising, as previous research, such as DIRE [63] and UnivFD [40], has shown that this naıve ap- (2) proach typically suffers from limited generalization. There still remains performance gap between the two settings (with 9.1% difference in accuracy and 4.6% in mAP). Therefore, future studies may focus on enhancing Table 5. The performance of publicly available pre-trained models and our trained models for tracing source images. µAP (%) Method Pika SVD Ops IXL Cog Avg. i p d a - s o s e - S n L d - s e g s o o a o e s o Swin-B [34] ResNet-50 [22] ConvNeXt [35] EfficientNet [51] ViT-B [15] SimSiam [13] MoCov3 [23] DINOv2 [43] MAE [24] SimCLR [11] CLIP [47] SLIP [37] ZeroVL [14] BLIP [32] ASL [60] CNNCL [70] BoT [59] SSCD [45] AnyPattern [61] 74.4 65.3 25.9 21.6 67.7 51.0 80.7 66.9 23.4 16.3 72.4 51.9 77.1 67.6 26.1 22.0 69.4 52.4 90.5 79.5 29.9 24.0 82.3 61.2 86.4 74.6 27.8 21.7 78.4 57.8 8.71 5.13 1.28 0.74 7.75 4.72 32.0 17.1 1.97 1.19 26.3 15.7 73.1 63.4 25.6 25.0 66.4 50.7 37.2 29.8 2.02 0.23 28.6 19.6 93.3 81.4 21.2 22.1 86.2 60. 41.2 28.1 3.42 1.85 33.5 21.6 82.0 71.6 26.3 25.2 75.8 56.2 68.4 39.0 5.41 7.38 51.8 34.4 77.0 68.9 26.9 23.7 68.0 52.9 43.3 31.4 9.12 5.91 37.7 25.5 93.0 78.0 27.6 15.2 83.5 59.5 98.3 94.6 47.1 43.2 94.0 75.4 98.5 95.5 49.9 47.9 95.9 77.5 94.2 89.5 44.0 48.3 89.5 73.1 O Same Domain Cross Domain 99.1 97.0 61.5 90.3 97.6 89.1 99.1 96.9 57.0 73.3 97.1 84.7 the models generalizability to unseen diffusion models. 5.4. Tracing the Source Images In addition to identifying videos generated from images, in this section, we explore another approach to combat misinformation: given any frame from generated video, we aim to retrieve its source image from large database. For example, given frame depicting Donald Trump throwing punch at Elon Musk (Fig. 7, last frame), could we retrieve its source image showing the two shaking hands friendly (Fig. 7, left)? If we achieve this, whenever malicious users attempt to mislead the public with any frame from generated videos, we can reveal the original source image to debunk the misinformation. Based on TIP-I2V, we construct large database, TIP-Trace, with 4, 590, 000 training, 90, 000 query, and 1, 010, 000 reference images to conduct this study. For detailed dataset settings, please refer to the Supplementary (Section 15). The performance of existing pre-trained models is unsatisfactory for this task. We benchmark existing methods on the TIP-Trace test set, including supervised pretrained models, self-supervised learning models, visionlanguage models, and image copy detection models. From the Table 5, we observe that the top-performing model, SSCD [45], achieves only 77.5% µAP. This underscores the necessity of developing specialized model for this task. strong baseline proposed by us. We train deep metric learning baseline aimed at creating space where generated frames are close to their source images (see details in the Supplementary (Section 16)). During testing, we compare the cosine similarity between the query feature and each reference feature to identify the source image. Experiments in Table 5 show that, under the Cross Domain setting, the proposed baseline achieves +7.2% superiority in µAP compared to its nearest competitor. However, we also observe that, compared to training and testing within the same diffusion model (Same Domain), there remains significant performance gap for some diffusion models, such as I2VGen-XL [71], with decrease of 17.0% in µAP. Therefore, future research could focus on improving the generalizability of our baseline, making it more practical. 5.5. Other Promising Research Beyond the research areas detailed above, we also introduce the following additional promising directions briefly. Meaning-preserving prompt refinement. Some userprovided text prompts in TIP-I2V may be ambiguous and challenging for image-to-video models to interpret accurately. Based on our TIP-I2V, future research could focus on designing prompt refiners to clarify these prompts while preserving the users original intent. Retrieval-augmented generation. Enhancing imageto-video models with retrieval can improve generation quality and relevance. By retrieving similar past prompts or generated videos in TIP-I2V, models may use this additional context to produce more accurate and coherent videos. Unsafe-prompt blocking. Some prompts in TIP-I2V may contain unsafe content, such as two people fighting or asking girl to undress. Developing classifiers to detect and block such prompts before they are processed by imageto-video models is crucial for responsible AI deployment. Copyright-respecting cartoon generation. About 11.0% of image prompts in TIP-I2V contain cartoon or animation elements, which may be subject to copyright. Future researchers may need to check whether the corresponding generated cartoon videos are similar to copyrighted real videos and try to prevent generating such content. 6. Conclusion In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided text and image prompts for image-to-video diffusion models. We compare TIP-I2V with existing prompt datasets, emphasizing the need for specialized image-to-video prompt dataset due to differences in both basic and semantic content. Our dataset enables new research directions, such as better accommodating user preferences, developing more comprehensive and practical evaluation benchmarks, and addressing safety concerns like misinformation by identifying generated videos and tracing their source images. We encourage the research community to utilize and build upon our dataset to further advance the field."
        },
        {
            "title": "Acknowledgment",
            "content": "We sincerely thank OpenAI for their support through the Researcher Access Program. Without their generous contribution, this work would not have been possible."
        },
        {
            "title": "References",
            "content": "[1] Hailuo ai video generator - reimagine video creation - imageto-video. https://hailuoai.video. 3 [2] Kling - kuaishou - image-to-video. https://kling. kuaishou.com/en. 3 [3] Movie gen: Ai video generation tool. https://ai. meta.com/research/movie-gen/, 2024. [4] Sora: Ai text-to-video model. https://openai.com/ index/sora/, 2024. 2, 3 [5] Pika - creative video editing platform. https://pika. art/, 2024. 1, 2, 6 [6] Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal Nayak, Abheesht Sharma, Taewoon Kim, Saiful Bari, Thibault Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. Annual Meeting of the Association for Computational Linguistics, 2022. 3 [7] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. IEEE/CVF International Conference on Computer Vision, 2021. [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2, 3 [9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 3 [10] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3, 2 [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning International conference on maof visual representations. chine learning, 2020. 8 [12] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple IEEE/CVF Conference on Comcross-modality teachers. puter Vision and Pattern Recognition, 2024. 3, 1 [13] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. IEEE/CVF conference on computer vision and pattern recognition, 2021. 8 [14] Quan Cui, Boyan Zhou, Yu Guo, Weidong Yin, Hao Wu, Osamu Yoshie, and Yubo Chen. Contrastive vision-language pre-training with limited resources. European Conference on Computer Vision, 2022. 8 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2021. 8 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2021. 6 [17] Falconsai. Falconsai: nsfw image detection. https: / / huggingface . co / Falconsai / nsfw _ image _ detection, 2024. 3 [18] Fanda Fan, Chunjie Luo, Wanling Gao, and Jianfeng Zhan. Aigcbench: Comprehensive evaluation of image-to-video content generated by ai. BenchCouncil Transactions on Benchmarks, Standards and Evaluations, 2024. 2, 6 [19] Joel Frank, Thorsten Eisenhofer, Lea Schonherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz. Leveraging freInternaquency analysis for deep fake image recognition. tional conference on machine learning, 2020. [20] Alexey Golub. Discordchatexporter. https://github. com/Tyrrrz/DiscordChatExporter, 2024. 3 [21] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, et al. I2v-adapter: general image-to-video adapter for diffusion models. ACM SIGGRAPH 2024 Conference Papers, 2024. 1 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. IEEE conference on computer vision and pattern recognition, 2016. 8 [23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. IEEE/CVF conference on computer vision and pattern recognition, 2020. 8 [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable IEEE/CVF conference on computer vision vision learners. and pattern recognition, 2022. 8 [25] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 6 [26] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Thirty-eighth Conference on Neural Information Processing Systems, 2024. 9 [27] Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano, and Siwei Lyu. Fusing global and local features for generIEEE International alized ai-synthesized image detection. Conference on Image Processing, 2022. 7 [28] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. IEEE/CVF International Conference on Computer Vision, 2023. 2 [29] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. IEEE/CVF International Conference on Computer Vision, 2023. 3 [30] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 2020. 6 [31] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 2 [32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. International Conference on Machine Learning, 2022. [33] Bo Liu, Fan Yang, Xiuli Bi, Bin Xiao, Weisheng Li, and Xinbo Gao. Detecting generated images by real images. European Conference on Computer Vision, 2022. 7 [34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. IEEE/CVF international conference on computer vision, 2021. 8 [35] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. IEEE/CVF conference on computer vision and pattern recognition, 2022. 8 [36] Leland McInnes, John Healy, and Steve Astels. Hdbscan: Hierarchical density based clustering. The Journal of Open Source Software, 2017. 1 [37] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pretraining. European Conference on Computer Vision, 2022. 8 [38] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. [39] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. IEEE/CVF conference on computer vision and pattern recognition, 2023. 1 [40] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 7 [41] OpenAI. New embedding models and api updates. https://openai.com/index/newembeddingmodels-and-api-updates/, 2024. 3 [42] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 3, 5, 1, 2 [43] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023. 8 [44] Zoe Papakipos, Giorgos Tolias, Tomas Jenicek, Ed Pizzi, Shuhei Yokoo, Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang, Sanjay Addicam, et al. Results and findings of the 2021 image similarity challenge. In NeurIPS 2021 Competitions and Demonstrations Track, pages 112. PMLR, 2022. 6 [45] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. self-supervised descriptor for image copy detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 8 [46] Ed Pizzi, Giorgos Kordopatis-Zilos, Hiral Patel, Gheorghe Postelnicu, Sugosh Nagavara Ravindra, Akshay Gupta, Symeon Papadopoulos, Giorgos Tolias, and Matthijs Douze. The 2023 video similarity dataset and challenge. Computer Vision and Image Understanding, 2024. 2 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. International conference on machine learning, 2021. 3, [48] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, 2019. 1 [49] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. Transactions on Machine Learning Research. 2, 6 [50] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei. Learning on gradients: Generalized artifacts representation for gan-generated images detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 7 [51] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. International conference on machine learning, 2019. 8 [52] Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and [67] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 2023. 1 [68] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 3 [69] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 3, 4, 6 [70] Shuhei Yokoo. Contrastive learning with large memory bank and negative embedding subtraction for accurate copy detection. arXiv preprint arXiv:2112.04323, 2021. 8 [71] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. arXiv preprint synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 1, 2, 3, 8 [72] Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, and Tao Mei. Trip: Temporal residual learning with image noise prior for image-to-video diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1 [73] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 1, 2, [74] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by metatuning on dataset and prompt collections. Empirical Methods in Natural Language Processing, 2021. 3 Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016. 2, 6 [53] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 2022. 7, 5 [54] Unitary team. Detoxify. https : / / github . com / unitaryai/detoxify, 2020. 3 [55] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52655274, 2018. [56] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3, 2 [57] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. IEEE/CVF conference on computer vision and pattern recognition, 2020. 7 [58] Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. Thirty-eighth Conference on Neural Information Processing Systems, 2024. 2, 3, 4, 6 [59] Wenhao Wang, Weipu Zhang, Yifan Sun, and Yi Yang. Bag of tricks and strong baseline for image copy detection. arXiv preprint arXiv:2111.08004, 2021. 8 [60] Wenhao Wang, Yifan Sun, and Yi Yang. benchmark and asymmetrical-similarity learning for practical image copy In Proceedings of the AAAI Conference on Ardetection. tificial Intelligence, pages 26722679, 2023. 8 [61] Wenhao Wang, Yifan Sun, Zhentao Tan, and Yi Yang. Anypattern: Towards in-context image copy detection. In arXiv preprint arXiv:2404.13788, 2024. [62] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. The Twelfth International Conference on Learning Representations, 2024. 3 [63] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire IEEE/CVF Interfor diffusion-generated image detection. national Conference on Computer Vision, 2023. 7 [64] Zijie Wang, Evan Montoya, David Munechka, Haoyang Yang, Benjamin Hoover, and Polo Chau. Diffusiondb: large-scale prompt gallery dataset for text-to-image generative models. Annual Meeting of the Association for Computational Linguistics, 2023. 2, 3, 4, 6 [65] Zijie Wang, Fred Hohman, and Duen Horng Chau. Wizmap: Scalable interactive visualization for exploring large machine learning embeddings. Annual Meeting Of The Association For Computational Linguistics, 2023. 4, 5, 1 [66] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. International Conference on Computer Vision, 2023. 2 11 TIP-I2V: Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "Table 6. The generated video specifications in our TIP-I2V, including frame per second (FPS), duration, and resolution. Image-to-Video Models FPS Duration Resolution Pika [5] Stable Video Diffusion [8] Open-Sora [73] I2VGen-XL [71] CogVideoX-5B [69] 24 7 24 7 8 3s 3.57s 4.25s 2s 6.13s Varied 1024 576 640 360 1280 704 720 480 tent. Users can sign up using Discord account to access the platforms services. Currently, the service is free to use; however, generated videos include Pika Labs watermark and are intended for non-commercial purposes. Additionally, all created clips are publicly shared. Stable Video Diffusion [8] is an open-source generative AI model developed by Stability AI that transforms static images into short video clips (without text guidance). It is available in two versions: one generating 14 frames and another producing 25 frames, both supporting frame rates between 3 and 30 frames per second. Open-Sora [73] is an open-source project developed by HPCAI Tech to democratize efficient video production. In Version 1.2, it supports image-to-video generation for durations from 2s to 15s, resolutions from 144p to 720p, and any aspect ratio, effectively bringing the image to life. I2VGen-XL [71] is an advanced image-to-video synthesis model that generates high-quality videos from static images using two-stage cascaded diffusion approach. To improve diversity, I2VGen-XL was trained on approximately 35 million single-shot text-video pairs and 6 billion textimage pairs. It addresses challenges in video synthesis like semantic accuracy, clarity, and spatio-temporal continuity. CogVideoX-5B Image-to-Video [69] is the latest AI model designed to generate dynamic videos from static images, guided by textual prompts. It is developed by the Knowledge Engineering Group at Tsinghua University and has 5 billion parameters. 10. Details of Calculating User Preference Calculate the most popular subjects: (1) for each data point, embed the subject using SentenceTransformer [48] to obtain 384-dimensional vector; (2) cluster the resulting 1, 701, 935 vectors using HDBSCAN [36], which automatically generates 21, 247 clusters; and (3) for each cluster, use the most frequent subject as the representative and then rank the obtained subjects by frequency. Note that we adopt this approach because GPT-4o [42] may use slightly Figure 8. The WIZMAP [65] visualization of our TIP-I2V compared to Panda-70M [12]. Please (cid:252) zoom in to see the details. 7. Comparing TIP-I2V with Panda-70M As shown in Fig. 8, we compare the text semantics of our TIP-I2V and Panda-70M [12] using WIZMAP to highlight their differences. 8. Exact Words from Pikas Terms of Service Your Inputs and Outputs. You own all Outputs you create with the Service (Your Outputs). Notwithstanding the foregoing, nothing herein prevents Mellis or the Service from providing any Outputs to third party that are the same as, or similar to, Your Outputs, and you hereby agree that such third party is free to use and exploit such Outputs without restriction from or obligation to you. You hereby grant Mellis and other users license to any of your Inputs and Outputs that you make available to other users on the Service under the Creative Commons Noncommercial 4.0 Attribution International License (as accessible here: https://creativecommons.org/licenses/bync/4.0/legalcode). Excerpt from Pikas regulations 9. Details of Adopted Image-to-Video Models This section details the image-to-video models utilized in our TIP-I2V and the specifications we choose for each model, as shown in Table 6. Pika Image-to-Video [5] is commercial AI-driven platform that transforms static images into dynamic video conFigure 9. An extension of Fig. 4: the top 50 subjects (top) and directions (bottom) preferred by users when generating videos from images. different variations for the same subject. For example, for the subject Dragon, GPT-4o [42] may output Dragon, Dragons, Dragon, creature or Dragon creature. Calculate the most popular directions: (1) use GPT-4o [42] to extract each verb from the text prompts; (2) gather all extracted verbs; and (3) rank them by frequency. The used prompt for GPT-4o [42] is: Extract verbs in given sentence, return their base form, separated by commas, and do not return anything else. If there is no verb, please return . 11. Examples for Top Subjects and Directions As shown in Fig. 10 and Fig. 11, for each of the top 25 most popular subjects and directions, we select one text and one image prompt for illustration. Beyond this, in Fig. 9, we extend Fig. 4 to show the top 50 users preferred subjects (top) and directions (bottom). 12. Full Experiments for Benchmarking Table 7 provides the full experiments for generating the 6. For the selected 10 diradar chart shown in Fig. mensions, subject consistency, background consistency, motion smoothness, dynamic degree, aesthetic quality, and imaging quality are derived from VBench-I2V [25] and I2V-Bench [49], while temporal consistency, videotext alignment, video-image alignment, and disentangled objective video quality evaluator (DOVER) [66] are from AIGCBench [18]. 13. Details of TIP-ID Dataset Unlike previous fake image detection datasets, which classify images into two classes real and fake our TIPTable 7. The full experimental results for drawing Fig. 6. Similar to VBench [25], when drawing the radar chart, results are normalized per dimension to common scale between 0.3 and 0.8 linearly. Dimension Pika SVD OpS IXL Cog Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality Temporal Consistency Video-text Alignment Video-image Alignment DOVER [66] 0.976 0.981 0.995 0.058 0.659 0.627 0.997 0.254 0.974 0.713 0.950 0.959 0.984 0.667 0.585 0.586 0.984 0.252 0.932 0.607 0.826 0.909 0.992 0.326 0.512 0.514 0.987 0.258 0.767 0.460 0.816 0.893 0.948 0.775 0.555 0.551 0.953 0.260 0.791 0.478 0.949 0.962 0.983 0.253 0.611 0.617 0.989 0.255 0.946 0.652 ID dataset emphasizes three classes: real videos, videos generated from texts, and videos generated from images. Sources. (1) Real videos. The real videos are sourced from the VSC22 dataset [46], which comprises approximately 100, 000 videos derived from the YFCC100M dataset [52], ensuring diversity and comprehensiveness. To match the lengths of generated videos, we split the real videos into 3-second segments. This results in 354, 486 real videos totally. (2) Videos generated from texts. We randomly select 400, 000 text-generated videos from VidProM [58], with 100, 000 videos from each text-to-video diffusion model: Pika [5], VideoCraft2 [10], Text2Video-Zero [28], and ModelScope [56]. (3) Videos generated from images. We use 500, 000 image-generated videos in our TIP-I2V, with 100, 000 videos from each image-to-video diffusion model. With these sources, the constructed TIPID dataset is relatively balanced across each class. Split. We split the TIP-ID dataset into 9:1 ratio for training and testing, respectively. It is important to note that: (1) When benchmarking existing fake image detection 2 Figure 10. For each top-ranked subject, we select one text and one image prompt as examples for illustration. methods, we exclude the class of text-generated videos, as these methods can only classify videos (images) as real or fake. (2) For the training and test sets of image-generated videos, UUIDs do not overlap. This restriction is intended to prevent potential data leakage, as for the same UUID, diffusion models generate videos from the same image. (3) Although we split real videos into segments, the segments of any given real video are assigned to either the training set or the test set, but not both. This is also for preventing potential data leakage. Settings. We consider two settings for evaluating detectors on our TIP-ID dataset. (1) Same domain. Both the training and testing image-generated videos are generated by the same diffusion model. For instance, we train and test detector on videos generated by Open-Sora. This setting aims to test whether detector can achieve high per3 Figure 11. For each top-ranked direction, we select one text and one image prompt as examples for illustration. formance when it has already encountered videos generated by one diffusion model, which can be considered the upper bound for the next setting. (2) Cross domain. The training and testing image-generated videos are generated by different diffusion models. For example, we train detector on videos generated by Pika, Stable Video Diffusion, I2VGen-XL, and CogVideoX-5B, but test it on Open-Sora. This approach is more practical, as trained detector will likely encounter newly-developed image-to-video models that it has not previously seen. Evaluation metrics. Following the fake image detection task, we use Accuracy and Mean Average Precision (mAP) to evaluate the performance of models on the proposed TIPID dataset. Specifically, Accuracy measures the proportion of correct predictions among all predictions; whereas mAP evaluates performance for each class, which is useful for 4 Figure 12. The illustration of the TIP-Trace, which is designed to train model to identify the source image of any given generated frame. handling class imbalances. 14. Details of Fine-tuning VideoMAE We fine-tune the Video Masked Autoencoder (VideoMAE) [53] on the TIP-ID dataset for video classification task. Specifically, our preprocessing pipeline includes (1) temporal subsampling, (2) spatial transformations, and (3) normalization. During training, the spatial transformations consist of random short-side scaling, random cropping to 224 224, and random horizontal flipping; for testing, we only resize frames to 224 224. The pre-trained model is downloaded from https://huggingface.co/MCGNJU/videomaebase, and we adjust it to match the number of classes in our dataset, i.e., 3, by updating the classification head. Training is distributed across server with 8 A100 GPUs. The mini-batch size is 8, the learning rate is 5 105, and the number of iterations is 20, 000. 15. Details of TIP-Trace Dataset As shown in Fig. 12, this section provides detailed description of the proposed TIP-Trace dataset. It includes training, query, and reference sets: Training set. Recall that we randomly selected 100, 000 text and image prompts from TIP-I2V to generate videos using state-of-the-art image-to-video models. We use 90, 000 of these text and image prompts to construct the training set. Specifically, for the videos generated by each image-to-video model, we uniformly select 10 frames from each, resulting in total of 5 90, 000 10 = 4, 500, 000 training images. Including the image prompts (source im5 ages), we have total of 90, 000+4, 500, 000 = 4, 590, 000 training images, as shown in Fig. 12 (top). Query set. As shown in Fig. 12 (middle), the query set consists of two parts: (1) Queries generated from remainInstead of uniformly selecting 10 ing 10,000 prompts. frames from each video, we randomly pick one frame from each video for testing. This results in 50, 000 query images with true matches. (2) Distractor queries. The distractor images, i.e., images not extracted from image-to-video generation, serve to replicate real-world scenarios where there is an abundance of authentic images rather than artificially generated ones. We randomly select 40, 000 from Open Images Dataset [30] as the distractor queries. Reference set. We design the reference set to mimic needle-in-a-haystack scenario in the real world, where the majority of images do not have corresponding queries. Specifically, as shown in Fig. 12 (bottom), we incorporate the 10, 000 source images into set of 1, 000, 000 reference images from DISC21 [44], which is derived from the realworld multimedia dataset YFCC100M [52]. Beyond the split sets, we also introduce two evaluation settings and one evaluation metric to assess model performance on the proposed dataset: Two evaluation settings. We consider two settings for evaluating model performance on the TIP-Trace dataset. (1) Same domain. In this setting, models can be trained and tested on data from all 5 image-to-video models. This setting is used to evaluate whether model can learn discriminative information after training. (2) Cross domain. We observe that, in the real world, new image-to-video models continually emerge. Therefore, in this setting, we assess whether trained model can generalize to unseen models. Specifically, we exclude one of the five models from the training set and conduct testing on the excluded model. This setting is more challenging and practical than the first. An evaluation metric. This task uses µAP (micro Average Precision) as the evaluation metric. µAP considers the overall performance across all queries by aggregating true positives, false positives, and false negatives over the entire dataset before calculating precision and recall. This evaluation metric is particularly suitable for this task as it provides more holistic measure of models effectiveness in distinguishing between matching and non-matching images in large-scale datasets. 16. Details of Deep Metric Learning Baseline We first treat each source image and its generated frames together as single class, then train CosFace [55] on the resulting 90, 000 classes as strong deep metric learning baseline. The hyperparameters are set as follows: the model architecture is ViT-Base [16], with CosFace loss margin of 0.35 and scale parameter of 64. The training process is with batch size of 512, using 4 instances per class. We use cosine learning rate schedule with maximum learning rate of 0.00035 and warmup period of 5 epochs. The model is trained for 25 epochs, with 2, 000 iterations per epoch, distributed across 8 A100 GPUs. Input images are resized to height and width of 224 224. When testing, we remove the classification layer and use the trained ViTBase to extract features from queries and references. 17. Potential Social Impact The TIP-I2V dataset has potential for positive social impact by enhancing digital creativity and fostering responsible AI use. Specifically, by helping the creation of more advanced and user-responsive image-to-video models, TIPI2V enables artists, content creators, and educators to create engaging and customized videos. Additionally, TIP-I2V contributes to the development of detection models that help verify authenticity, trace image sources, and prevent harmful content misuse. Nevertheless, the TIP-I2V dataset may also have potential negative social impacts if misused. Below, we outline several potential negative social impacts and provide corresponding solutions: NSFW content. Although limited in quantity, our dataset includes some NSFW text and image prompts, which may be sensitive or potentially discomforting for certain individuals. Similar to VidProM [58] and DiffusionDB [64], we choose not to remove these NSFW prompts, as they may provide valuable data for AI safety researchers to analyze and develop content-blocking solutions. Nevertheless, we provide NSFW scores for text and image prompts, allowing regular researchers to easily identify and remove these prompts if they find the content uncomfortable. Privacy. Although, per Pikas regulations, users agree to make their input and output publicly available, some may still feel uncomfortable with the inclusion of their prompts in TIP-I2V. To enhance user privacy, we implement the following measures: (1) each prompt is assigned new UUID instead of user-identifiable original IDs; and (2) users have the right to request that their contributions be removed from TIP-I2V. They can simply email us to make this request. Copyright. According to Pika Labs Terms of Service, users are responsible for ensuring that their content does not violate any copyright laws or third-party rights. However, we have noticed that Pika Labs lacks preventive measures, leading some users to upload images, such as Mickey Mouse, which may be subject to copyright restrictions. Nevertheless, including these images in our TIPI2V does not constitute copyright infringement, as our usage falls under fair use. While our dataset is open-sourced under non-commercial license (CC BY-NC 4.0), some malicious users may still use this dataset for commercial purposes, potentially infringing copyright. Therefore, we strongly recommend that users of TIP-I2V comply with our license to avoid any legal risks."
        }
    ],
    "affiliations": [
        "University of Technology Sydney",
        "Zhejiang University"
    ]
}