{
    "paper_title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
    "authors": [
        "Hung-Yueh Chiang",
        "Chi-Chih Chang",
        "Natalia Frumkin",
        "Kai-Chiang Wu",
        "Mohamed S. Abdelfattah",
        "Diana Marculescu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 7 8 2 2 . 3 0 5 2 : r Quamba2: Robust and Scalable Post-training Quantization Framework for Selective State Space Models Hung-Yueh Chiang1, Chi-Chih Chang2, Natalia Frumkin1, Kai-Chiang Wu3, Mohamed S. Abdelfattah2, and Diana Marculescu1 1 Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin 2Department of Electrical and Computer Engineering, Cornell University 3Department of Computer Science, National Yang Ming Chiao Tung University {hungyueh.chiang, nfrumkin, dianam}@utexas.edu, {cc2869, mohamed}@cornell.edu, kcw@cs.nycu.edu.tw Abstract State Space Models (SSMs) are emerging as compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of linear recurrence in 8-bit by sorting and clustering for input ùë•, combined with per-state-group quantization for input-dependent parameters ùêµ and ùê∂. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3 and 3 speed-ups in the pre-filling and generation stages, respectively, while offering 4 memory reduction with only 1.6% average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba."
        },
        {
            "title": "1 Introduction",
            "content": "State Space Models (SSMs) (Dao and Gu 2024; Gu and Dao 2024; Gu et al. 2020; Smith, Warrington, and Linderman 2023), offering constant memory complexity, are emerging as efficient alternatives to Transformers (Vaswani 2017) in various areas such as language modeling (Waleffe et al. 2024; Wang et al. 2024), vision (Li et al. 2025; Liu et al. 2024a; Zhu et al. 2024a), and audio (Goel et al. 2022; Saon, Gupta, and Cui 2023). Some studies expand the size of the models and demonstrate their performance on par with Transformers of the same scale (Lieber et al. 2024; Team et al. 2024; Waleffe et al. 2024). However, the large size of SSMs limits the hardware options and increases the deployment costs. Post-training quantization (PTQ) offers an attractive solution to efficient deployment by eliminating the needs of fine-tuning large models. PTQ reduces the bit-width of pre-trained weights and activations to lower-bit formats (such as 8-bit), cutting down memory use for weight storage and leveraging advanced hardware units. Recent studies (Chiang et al. 2025; Xu et al. 2025) reveal that quantization techniques that are effective in Transformers struggle with SSMs due to the sensitivity of linear recurrence to quantization-induced errors. This prior work introduces PTQ algorithms tailored for SSMs to bridge 1 Table 1: (Supported models.) Our framework supports W8A8, W4A8, and W4A16 for both Mamba1 (Gu and Dao 2024) and Mamba2 (Dao and Gu 2024). Table 2: (Supported bit-widths.) Quamba2 supports headto-toe (H2T) 4/8-bit from the embedding layer, SSM blocks, to the final output layer (i.e., lm head). Methods Models Bitwidth Mamba1 Mamba2 W8A8 W4A8 W4A16 MambaQuant (Xu et al.) Quamba (Chiang et al.) Quamba2 (Ours) - - - - - Methods Embed. SSM blocks lm head H2T MambaQuant (Xu et al.) 16-bit 4/8-bit Quamba (Chiang et al.) 16-bit 8-bit 16-bit 16-bit Quamba2 (Ours) 4/8-bit 4/8-bit 4/8-bit the performance gap between low and half-precision models. However, they either do not explore diverse bit-widths (Chiang et al. 2025) or fail to achieve satisfactory performance at lower bit-widths (Xu et al. 2025), such as W4A8. Specific bit-width setups are crucial for certain scenarios. For instance, W4A8 enhances cloud service throughput with large-batch inputs (Lin et al. 2024b), whereas W4A16 improves the efficiency of short prompt applications (Lin et al. 2024a). As result, current SSM-based quantization methods (Chiang et al. 2025; Xu et al. 2025) may underperform on edge devices or fail to maximize throughput on cloud services. Moreover, recent study (Gong et al. 2024; Kumar et al. 2025; Zhao et al. 2024a) reveals that heavy quantization of model weights and activations (e.g., W4A4) impairs model generalization on multi-step reasoning tasks. Previous SSM-based studies overlook the generalizability of quantized models. To address these issues, we present Quamba2, robust and scalable post-training quantization framework for selective SSMs. As shown in Table 1 and 2, our framework supports head-to-toe W8A8, W4A8, and W4A16 for both Mamba1 (Gu and Dao 2024) and Mamba2 (Dao and Gu 2024), meeting the demands for SSM deployment on cloud and edge platforms. Based on channel order preserving and activation persistence of the SSM computation, as shown in Figure 2 and 3, we employ an offline cluster-aware weight reordering approach to group SSM heads and channels with similar value ranges, allowing them to share quantization scaling factor and boost quantization precision. For selective SSM input-dependent parameters (ùêµ, ùê∂), we identify state persistence in activations and apply quantization per state group. Our sort-and-cluster and per-state-group quantization methods improve quantization accuracy, closing the accuracy gap in half-precision models. In Figure 1 and the rest of our experiments, we show that Quamba2-8B surpasses several leading SSM quantization methods, achieving up to 1.3 and 3 higher speeds in prefilling and generation, respectively, and offering 4 memory reduction, with only 1.6% accuracy loss across six zero-shot tasks. Additionally, we tested Quamba2 on MMLU (Hendrycks et al. 2020), large multitasking dataset, demonstrating the generalizability and robustness of our framework."
        },
        {
            "title": "2 Related Work",
            "content": "Model quantization. Representing model weights and activations with low bit-width data types reduces the cost of storing and loading parameters and benefits from advanced low bit-width computing units (i.e., Tensor Cores). Quantization methods are generally divided into two categories: Quantization-aware training (QAT) (Dettmers et al. 2024; Liu et al. 2024b; Tang et al. 2024; Yu et al. 2025) and post-training quantization (PTQ) (Zhou et al. 2024; Zhu et al. 2024b). QAT requires additional GPU resources and training efforts to adapt models to low bit-width. PTQ is an attractive option for large language models (LLMs) since it eliminates the need for training. Our work falls under PTQ and minimizes GPU requirements. Our framework provides bit-width configurations of W8A8, W4A8, and W4A16 for SSM-based language models, delivering generic memory and latency reduction on all target platforms. Figure 1: (Quamba2-8B memory and throughput.) The head-totoe (H2T) quantization enables the deployment of Mamba2-8B on edge platforms. Quamba2 delivers 3 throughput on Nvidia A5000 and 13 tokens-per-second (TPS) on Nvidia Nano 8G. 2 Figure 2: (SSD flows with sorted heads and the activation persistence.) We sort the head channels prior to applying quantization scaling factors. The orange blocks on the right indicate the activated channels with higher values in the input and output SSD heads. The SSD performs channel-wise calculation thereby retaining the channel order between input ùë• and output ùë¶, which we call channel order preserving. The blue and green blocks represent the activated states of input-dependent parameters ùêµ and ùê∂. Our study shows that activated channels and states remain consistent across time steps and input samples, property we denote as channel persistence and state persistence. Figure 3: (Channel order preserving and activation persistence.) We show the activations in the last block of Mamba28B. For an input with ùë° tokens, we demonstrate that the ùë• remains sorted by the maximum of the calibrated channel (a). The SSD calculation is channel-wise, so the output channel order ùë¶ matches the input order ùë• (b). For ùêµ and ùê∂, the activated states remain consistent over time steps ùë° (c-d) and input samples (e-f). We leverage the observations and design our techniques, sort-and-cluster and per-state-group quantization, to increase the quantization precisions for ùë• (a), ùêµ, and ùê∂ (c-f). PTQ and weight reordering for Transformers. Post-training quantization (PTQ) techniques are generally classified into two categories: weight-only quantization (e.g., W4A16) and weight-activation quantization (e.g., W8A8) (Zhu et al. 2024b). Weight-only quantization (Frantar et al. 2023; Lin et al. 2024a) minimizes weight storage, while weight-activation quantization (Ashkboos et al. 2024b; Zhao et al. 2024b) optimizes throughput with low bit-width operations. Reordering weights is frequently used to enhance quantization precision (Yuan et al. 2024; Zhao et al. 2024b) or efficiency (Lin et al. 2024b) of Transformers, but its use and its subsequent effectiveness in SSMs is unclear. Our study shows that the selective State Space Duality (SSD) computing (Dao and Gu 2024) preserves channel order between input and output, with activated channels and states consistent over time. PTQ for SSMs. Xu et al. (2025) and Chiang et al. (2025) highlight that standard quantization techniques for Transformers are not effective for SSMs and propose PTQ algorithms tailored for SSMs. Despite this, these strategies do not offer variety of bit-width configurations (Chiang et al. 2025) and struggle to perform well at reduced bit-widths such as W4A8 (Xu et al. 2025). Our framework provides W8A8, W4A8, and W4A16 for both Mamba1 (Gu and Dao 2024) and Mamba2 (Dao and Gu 2024) with practical speed-up and memory reduction, addressing the growing demand for the deployment of SSMs both in the cloud and on the edge. Moreover, Zhao et al. (2024a) show that 4-bit models lose generalizability, and Kumar et al. (2025) indicate the best performance under memory constraints for bit-width of 6-8, with worse results for bit-width of 4. We further evaluate Quamba2 on large and challenging multitasking dataset, MMLU (Hendrycks et al. 2020), to show the robustness of our framework. 3 Figure 4: (Sort-and-cluster.) We leverage the channel-persistent property in SSMs to sort the channel with the calibrated maximum (a-c). The sorted heads disentangle the embedding, as shown in (c-1) and (c-2), enabling the clustering on the heads. We cluster the sorted heads into ùëö groups (ùëö = 8 in (d)), and reorder the weights offline to match the clustering results. Then, we apply the clustering again in each head group to cluster the channels into ùëõ groups (ùëõ = 4 in (e)). For each group, scaling factor is calculated, resulting in ùëö ùëõ factors used to quantize ùë•ùë° to 8-bit."
        },
        {
            "title": "3.1 Model Quantization",
            "content": "Notations. We follow the notation in Chiang et al. (2025). We use ùëã to represent the floating-point matrices, and ùëã to represent their quantized matrices with their floating-point scaling factors ùë†ùë• . For operators, we use ùëì () to represent the quantized version of the function ùëì () (i.e., the weights are quantized in the function ùëì ). Quantization. We focus on symmetric uniform quantization to approximate floating-point weights and activations with discrete ùëÅ -bit signed integers (i.e., INT8 or INT4) due to its hardware compatibility. The general symmetric uniform quantization function is defined as ùëã = Clamp (cid:16) (cid:22) ùëã ùë† (cid:25) , 2ùëÅ 1, 2ùëÅ 1 1(cid:17) , (1) where ùë† = Max(cid:0)ùëã (cid:1)/(2ùëÅ 1 1). ùëã represents the quantized weights or activations, ùëã is the input matrix in floating point, and ùë† is the scaling factor (i.e., quantization step) that is determined by the target bit-width ùëÅ (ùëÅ = {4, 8} in our setting). The static scaling factor ùë† is pre-calibrated and fixed during inference."
        },
        {
            "title": "3.2 Selective State Space Models\nThe selective SSM (Dao and Gu 2024; Gu and Dao 2024) transforms the time-invariant SSM (Gu et al. 2020) to a time-varying\nsystem. The system dynamics is defined by",
            "content": "‚Ñéùë° = (cid:164)ùê¥ùë°‚Ñéùë° 1 + (cid:164)ùêµùë°ùë•ùë°, ùë¶ùë° = ùê∂ùë°‚Ñéùë° + ùê∑ùë•ùë° (2) where ( (cid:164)ùê¥ùë°, (cid:164)ùêµùë°, ùê∂ùë° ) are input-dependent. (cid:164)ùê¥ùë° and (cid:164)ùêµùë° are discrete parameters of ùê¥ and ùêµ. The discretization function for (cid:164)ùê¥ùë° and (cid:164)ùêµùë° with given input-dependent Œîùë° is defined as (cid:164)ùê¥ùë° = exp(Œîùë°ùê¥), (cid:164)ùêµùë° = (Œîùë°ùê¥) 1(exp(Œîùë°ùê¥) ùêº ) Œîùë° ùêµùë° Œîùë° ùêµùë° . (ùê¥, ùê∑) are trainable parameters, and ùê∑ is an optional residual parameter. An optional residual branch ùëßùë° is applied to the SSM 4 output such that ùë¶ùë° SiLU(ùëßùë° ) before the output projection. We follow Dao and Gu (2024) and abstract the selective SSM computation at the time step ùë° with the function Optional ùëßùë° and ùê∑ are omitted in the function. We omit the subscript ùë° to represent the computation for the entire sequence. The abstract SSM block is shown in Figure 5. ùë¶ùë° = SSM( (cid:164)ùê¥ùë°, (cid:164)ùêµùë°, ùê∂ùë° )(ùë•ùë° ). (3) Mamba1. Gu and Dao (2024) presents selective SSMs in which the parameters ùêµ, ùê∂, and Œî vary with input (i.e., timevarying), allowing the model to selectively prioritize or ignore inputs based on their content. The interaction with the input ùë•ùë° is specified as ùêµùë° = Fùêµ (ùë•ùë° ), ùê∂ùë° = Fùê∂ (ùë•ùë° ), Œîùë° = softplus(FŒî (ùë•ùë° )), where Fùêµ and Fùê∂ are linear transformations mapping ùë•ùë° to ùêµùë° and ùê∂ùë° . The function FŒî involves two sequential projection layers, formulated as FŒî = Proj(Proj(ùë•ùë° )) + bias. The ùë•ùë° is calculated from the input of the block ùë¢ùë° with projection layer at the time step ùë°. Mamba2. Dao and Gu (2024) establish theoretical link, Structured State Space Duality (SSD), between selective SSMs and self-attention. They also introduce an efficient algorithm that utilizes matrix multiplication units on contemporary hardware to perform linear recurrence calculations. Mamba2 simplifies block design by removing sequential linears where ùë•ùë°, ùêµùë°, ùê∂ùë° , and Œîùë° are produced in parallel with single projection layer such that (ùë•ùë°, ùêµùë°, ùê∂ùë°, Œîùë° ) = F(ùë¢ùë° ), where ùë¢ùë° is the block input at the time step ùë°. The modified block design is better suited to tensor parallelism (Shoeybi et al. 2019) in the context of larger models."
        },
        {
            "title": "3.3 Quantizing Selective SSMs\nSSM input parameters. The SSM defined in Equation\n3 receives an input in the form of ( (cid:164)ùê¥ùë°, (cid:164)ùêµùë°, ùê∂ùë° ; ùë•ùë° ). Recent\nefforts (Chiang et al. 2025; Xu et al. 2025) show that the\nSSM block is extremely sensitive to quantization-induced\nerrors in ùë•ùë° due to the linear recurrence mechanism in\nMamba1 (Gu and Dao 2024). Our work indicates that the\nphenomenon persists in Mamba2 (Dao and Gu 2024). To\naddress this issue, we propose sort-and-cluster to quantize\nthe input ùë•ùë° with 8-bit. Our method groups the channels\nacross the heads with the same value range to create a\nsmoother landscape in the group, and therefore increases\nthe quantization precision.",
            "content": "SSM outliers. Prior studies on Transformers (Dettmers et al. 2022; Xiao et al. 2023) have detected channel-persistent outliers. common method for outlier elimination is applying the Hadamard transform (Ashkboos et al. 2024b; Liu et al. 2024c). In SSM quantization (Chiang et al. 2025; Xu et al. 2025), online Hadamard matrices transform the input of output projection into smoother space, enhancing the quantization precision. Although the fast WalshHadamard transform (FWHT) can be executed in parallel with ùëõlogùëõ complexity (Dao 2024b; Sloane 1999), we adhere to Xu et al. (2025) and Chiang et al. (2025) to quantize the output projection input, with the aim of minimizing online Hadamard transform overheads. Figure 5: (Quamba2 precision.) The detailed precision mapping of W4A8 and W8A8 Quamba2. We reorder the weights offline to match the sorting and clustering indices of ùë•ùë† ùë° , and apply per-state-group quantization on ùêµùëî ùë° and ùê∂ùëî ùë° . 5 Table 3: (SSD latency.) We profile SSD latency of Mamba28B in milliseconds (ms) across sequence lengths with different input bit-width. We set batch size to eight. Table 4: (Quamba2 model size in GB.) We profile the model size in GB of different bit-width configurations for Mamba1 and Mamba2 in our framework."
        },
        {
            "title": "Inputs",
            "content": "ùêø = 256 FP16 Int8 (Ours) 0.82 0.76 512 1.61 1.47 2048 3.51 2.97 7.22 6."
        },
        {
            "title": "Speedup",
            "content": "1.08 1.10 1.18 1.19 Models Mamba Mamba2 Size 2.8B 2.7B FP16 W8A8 W4A8 W4A 5.3 GB 2.8 GB 1.5 GB 1.5 GB 5.2 GB 2.7 GB 1.4 GB 1.4 GB 8B 15.7 GB 7.9 GB 4.0 GB 4.0 GB"
        },
        {
            "title": "4.1 Quantizing SSM Parameters\nOur method is based on two findings in SSM activations: channel persistence and state persistence, together with a\ncomputational property of SSM: channel order preserving. The notation follows the definition from Equation 3.",
            "content": "Sort-and-cluster. We observe the persistence of the channel magnitude and the preservation of channel order in the SSM input ùë• and output ùë¶, as shown in Figure 2. Although ùë• is sensitive to quantization-induced errors in Mamba2 (Dao and Gu 2024), with the findings of Chiang et al. (2025) still applicable, Chiang et al. (2025) overlook the persistence characteristic and order-preserving of the SSM channel. In contrast, we leverage these two properties to first sort the head channels and group both heads and channels. Specifically, we first obtain the channel maximum from calibration dataset. In Figure 3 (a), we visualize the ùë• sorted by the offline calibrated channel maximum of the last block of Mamba2-8B. ùë• remains sorted input with an online ùë°-token sample. The sorted ùë• disentangles the head embedding, allowing head grouping. Figure 4 (c1-c2) shows that heads with similar characteristics are closely grouped, leading to the use of unsupervised clustering into ùëö groups. For each group of heads, we apply the clustering algorithm again to group channels into ùëõ groups. The scaling factor is calculated for every group, leading to total of ùëö ùëõ scaling factors, which are then utilized to quantize ùë•ùë° to 8-bit precision. The detailed sort-and-cluster process is shown in Figure 4. We find that ùëö = 4 and ùëõ = 4 provide sufficiently good results throughout all experiments. The ùë•ùë† ùë° in Figure 5 refers to the activation applied with sort-and-cluster. Per-state-group quantization. Dao and Gu (2024) relax the number of state group size and introduce Multi-input SSM where ùêµùë° , ùê∂ùë° matrices are shared across all channels of the input ùë•ùë° , akin to grouped-query attention (Ainslie et al. 2023) in Transformers. Our findings indicate that the activated states (with larger numerical values) are the same across time steps ùë° and input samples. In Figure 3 (c-f), we visualize the activation distribution of ùêµ and ùê∂ in the last block of Mamba2-8B. The number of groups in ùêµ and ùê∂ is set to 8, where each group has 128 channels. Figure 3 (c-d) shows that only few groups are activated with larger values. For example, in Figure 3 (e-f), group six in ùêµ is mostly activated, while group seven in both ùêµ and ùê∂ has minimal variations. Thus, we apply per-state-group quantization to ùêµ and ùê∂, where each group utilizes single scaling factor. The ùêµùëî ùë° in Figure 5 refer to the activations applied with per-state-group quantization. The per-state-group quantization largely increases the quantization precision in the groups where the value range is small, e.g, group seven in both ùêµ and ùê∂. We show that per-state-group quantization is key to mitigating the performance gaps with the FP16 model for Mamba2-8B. ùë° and ùê∂ùëî"
        },
        {
            "title": "4.2 System and Framework Design\nCluster-aware weight reordering. We create a new channel and head sequence in sort-and-cluster, where the heads\nwithin the same cluster are grouped and their channels are arranged by the pre-calibrated maximum. To produce the\nactivations with the sorting and clustering orders, we use clustering and sorting indices to reorder offline the input\nprojection, causal convolution, normalization, and output projection in the block. The output column of input projection\nweights and the channel of causal convolution weights are reordered. As SSD computing maintains channel order (see\nFigure 2 right), we reorder normalization weights and apply fused Hadamard quantization. Finally, input rows of the\noutput projection are rearranged using the same indices to keep the output the same. The offline cluster-aware weight\nreordering is depicted in Figure 5.",
            "content": "6 ùëõ = ùëõIùëõ where ùëõ denotes Offline Hadamard matrix fusion. Hadamard matrices have the computational property HùëõH ùëõ-dimensional square matrices. We therefore fuse offline the Hadamard matrices into the input and output linear projections. For the output projection, the Hadamard matrices are multiplied at both sides of the weight matrix, such that Wùêª out = ùëõ . Thus, HùëõWoutH pairing Hadamard matrices in input/output projections with online Hadamard quantization results in compute-invariance (Ashkboos et al. 2024a,b), yielding an identical block output. The offline Hadamard matrix fusion is shown in Figure 5. We apply the 4-bit/8-bit quantization on the weights after matrix fusion. ùëõ . We fuse Hadamard matrix at the input side of the input projection weight, such that Wùêª in = WinH Efficient 4-bit/8-bit Mamba blocks. Our framework accommodates W8A8, W4A8, and W4A16 projection kernels, W8A8 causal convolution kernel, 4-bit and 8-bit embedding kernels, and 8-bit selective scan and SSD kernels. For projection layers, we reorder the weights and their per-group scaling factors (Frantar et al. 2024; Lin et al. 2024b; Zhang et al. 2024) to maximize the Tensor Core loading throughput. The output scaling factors are fused to the input scaling factors such that ùëå = ùë†ùëä ùë†fused ùëä ùëã where ùë†fused = ùë†ùëã /ùë†ùëå . We implement W4A8 and W4A16 fused matmul-transpose kernels for the Mamba1 block. For sequence transformations, we load the 8-bit activations and 8-bit cached states to reduce memory pressure, thus improving latency, as shown in Table 3. In the forward Hadamard transform, the scaling factor ùë†ùë¶ is integrated, making ùë¶ùêª = 1 ùë†ùë¶ Hùëõùë¶, thereby avoiding extra computational load during quantization. The efficient kernels of our framework provide generic speed-up and memory reduction, addressing the increasing demands for the deployment of SSM on the cloud and on the edge. Head-to-toe quantization. Quantizing from embedding to the output head (i.e., Head-to-toe quantization) brings additional memory and latency reduction, which is necessary on edge computing platforms with limited memory capacity. As shown in Figure 1, our head-to-toe (H2T) quantization enables the deployment of Mamba2-8B on Nano 8G. Specifically, we employ per-token quantization to the embedding layer, and per-group quantization to the weight of the head. As shown in Table 2, we implement the CUDA kernels and support the 4-bit/8-bit embedding layer and 4-bit/8-bit output head. Therefore, our framework achieves generic 4 memory reduction. Improving robustness via W4Aùëã -mixed. Zhao et al. (2024a) demonstrate that applying W4A4 to all blocks compromises generalizability of Transformers. We extend such analysis to verify SSM robustness and generalizability on MMLU (Hendrycks et al. 2020) dataset. Our findings indicate that while full W4A8 quantization maximizes prefilling speedup, it suffers from notable generalization gap (5.8% on MMLU vs. 2.1% on LAMBADA). In contrast, full W4A16 quantization demonstrate robustness but comes at the cost of increased prefilling latency. To address this, we introduce mixed-precision support in our framework. We automatically search salient blocks based on their performance sensitivity and assign them higher precision. Our W4A{8/16}-mixed SSM achieves 2.9% accuracy improvement on MMLU while incurring only 10% increase in prefilling latency."
        },
        {
            "title": "5.1 Experimental Setup\nWe provide framework design details in Appendix C.",
            "content": "Evaluations. We use LM-EVAL (Gao et al. 2023) to evaluate Quamba2 and baselines on six zero-shot downstream tasks: LAMBADA (Paperno et al. 2016), HellaSwag (Zellers et al. 2019), PIQA (Bisk et al. 2020), ARC (Clark et al. 2018) and WinoGrande (Sakaguchi et al. 2020), and show the average accuracy over five runs in each table. To compare with MambaQuant (Xu et al. 2025), we average the accuracy across five datasets: ARC-easy, ARC-challenge, PIQA, WinoGrande and HellaSwag. The full evaluation is in Appendix Section A, where we follow the evaluation protocol in Mamba1 (Gu and Dao 2024), and report the accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for HellaSwag and ARC-challenge. To show generalizability and robustness, we evaluate the 8B models on MMLU (Hendrycks et al. 2020), large multitask test consisting of multiple-choice questions from various domains. In our W8A8 setting, we compare our framework with the latest quantization methods for SSM, MambaQuant Baselines. (Xu et al. 2025) (W8A8, W4A8) and Quamba (Chiang et al. 2025) (W8A8) on zero-shot downstream tasks. In the Quamba setting (Chiang et al. 2025), we applied the Hadamard transform to the output projection input and implemented percentile clipping on the input SSM, establishing our W8A8 Mamba2 baseline for latency and accuracy. We also provide the latency for W4A8 and W4A16."
        },
        {
            "title": "5.2 Latency and Model Size\nWe test all methods on the A5000 for cloud applications and\non the Orin Nano 8G for edge applications. Time-per-output-\ntoken (TPOT) and time-to-first-token (TTFT) are measured\nfor a batch size of one, recorded in milliseconds (ms). TTFT\nis profiled with 1024 input tokens. The results are shown in\nTable 5 and Figure 1. In the W8A8 setting, head-to-toe quan-\ntization of our framework improves the TPOT latency for\nMamba2-8B by 1.80√ó (22.73 ms vs. 12.61 ms), outperforming\nQuamba 1.61√ó (Chiang et al. 2025) (22.73 ms vs. 14.12 ms). In\nthe W4A8 configuration, Quamba2 achieves 3.89√ó less mem-\nory use, 1.39√ó prefilling, and 3.05√ó faster generation speed\nfor Mamba2-8B on A5000. W4A8 and W4A16 slow down\nTTFT compared to W8A8 and FP16 due to dequantization\noverhead. However, 4-bit weights bring latency benefits in\nthe memory-bound generation stage. Our approach allows\nthe deployment of Mamba2-8B on Nano 8G with a speed of\ngenerating 13 tokens per second, while FP16 and W8A8 fail,\nas illustrated in Figure 1 and Table 5. For the SSD kernel, we\nload the 8-bit activations ( ¬Øùë•, ¬Øùê¥, ¬Øùêµ, ¬Øùê∂, ¬Øùëß) to reduce memory\npressure and improve latency by 1.18√ó, as shown in Table 3.",
            "content": "Table 5: (Mamba2-8B latency.) TPOT and TTFT on Nvidia A5000 GPU and Orin Nano 8G are measured in milliseconds (ms) with one batch. TTFT is profiled with 1024 tokens. (OOM: out-of-memory) Methods Bitwidth A5000 Orin Nano 8G TPOT TTFT TPOT TTFT - FP16 22. 197.80 OOM OOM Quamba W8A8 14.12 124.01 OOM OOM Quamba2 (Ours) W8A8 W4A8 W4A16 12. 122.33 OOM OOM 7.43 7.58 140.78 209. 79.91 78.77 2088.03 2316."
        },
        {
            "title": "5.3 Zero-shot Evaluation on Downstream Tasks\nWe present the average accuracy for Quamba2 over five datasets: ARC-easy, ARC-challenge, PIQA, WinoGrande, and\nHellaSwag, allowing a fair comparison with MambaQuant (Xu et al. 2025). The full evaluation is in the Appendix, where\nwe follow the evaluation protocol in Mamba1 (Gu and Dao 2024). In contrast to Quamba (Chiang et al. 2025), when\napplied to Mamba1, our approach utilizes Hadamard transforms on input and output projections to increase quantization\nprecision, thus enhancing accuracy for Mamba1. As illustrated in Table 6, our techniques sort-and-cluster and per-state-\ngroup quantization surpass clipping in Mamba2 (Dao and Gu 2024). Our framework performs head-to-toe quantization,\noutperforming Quamba in latency and memory usage (refer to Table 5 and 4) for both W8A8 Mamba1 and Mamba2.\nQuamba2 also outperforms MambaQuant in W4A8 Mamba1 and delivers real speedup on computing platforms. Moreover,\nour framework supports W8A8, W4A8, and W4A16 precisions for both Mamba1 and Mamba2 with satisfactory accuracy\nand latency.",
            "content": "Table 6: (Zero-shot evaluation.) We compare our framework with Quamba (Chiang et al. 2025) and MambaQuant (Xu et al. 2025) on the average accuracy of five zero-shot downstream tasks. Table 7: (Five-shot evaluation of Quamba2-8B on MMLU.) We evaluate W4A8, W4A16, and W4Aùëã -mixed on MMLU. Our W4Aùëã -mixed model outperforms mixed by handcrafting (HC) and pure W4A8 models. Bitwidth Methods Mamba1 Mamba 1.4B 2.8B 2.7B 8B Bitwidth Method LAMB MMLU W4A{ùëã } (0-shot) (5-shot) (A8:A16) - 58.6% 62.2% 62.4% 70.8% Quamba 57.3% 61.5% 57.3% 67.0% FP16 W8A8 W4A8 MambaQuant Quamba2 (Ours) 58.3% 57.5% 62.1% 61.8% MambaQuant 54.3% 58.5% Quamba2 (Ours) W4A16 Quamba2 (Ours) 56.7% 57.5% 61.0% 61.9% - - 62.1% - 61.4% 62.3% 69.9% - 69.4% 70.2% FP16 W4A8 W4A16 Mixed Mixed Mixed - - - HC-last HC-first Auto 70.9% 47.0% 68.8% 70.6% 68.3% 68.9% 69.1% 41.2% 45.3% 42.1% 43.1% 44.0% - 56: 0:56 42:14 42:14 42:14 TTFT 197. 140.78 209.19 158."
        },
        {
            "title": "6.2 Ablation study on W4A16",
            "content": "We study the impact of each component in the case of W4A16 Quamba2-8B (weight-only quantization, i.e., ùëä ùëã ), and show the results in Table 9. The table demonstrates that the Hadamard transform combined with per-group weight quantization (PerG + Had.) yields greater accuracy than GPTQ (Frantar et al. 2023) (PerG + GPTQ). Our analysis indicates that the use of the Hadamard transform in the input of the out projection is crucial to narrowing the performance gap in weight-only quantization of SSMs. Specifically, the Hadamard transform eliminates outliers in the half-precision activation, thereby avoiding the amplification of quantization errors from 4-bit weights by large outliers in the output projection such that out < ùëäoutùëãout ùëä outùëãout. By combining all methods (PerG + GPTQ + Had.), the W4A16 models ùëäoutùëãout ùëä outùëã close the performance gap between the half-precision on LAMBADA dataset. Table 8: (Ablation study on W4A8 Quamba2-8B.) The accuracy on LAMBADA dataset is reported. (PerSG: perstate-group quantization for ùêµ and ùê∂, SnC: sort-and-cluster for ùë•, PerG: per-group weight quantization, GPTQ: Frantar et al. (2023), and Had: Hadamard transforms)"
        },
        {
            "title": "Weights\nPerG GPTQ",
            "content": "Had. B/C PerSG SnC Acc. Table 9: (Ablation study on W4A16 Quamba2-8B.) The accuracy on LAMBADA dataset is reported. The Hadamard transform eliminates the large outliers from the half-precision activation, avoiding the amplification of quantization errors from 4-bit weights. (PerG: pergroup quantization, GPTQ: Frantar et al. (2023), and Had: Hadamard transforms) FP 8B W4A8 - - - - - 71.2% Size Bitwidth Weights PerG GPTQ Had. Acc. fail 53.8% 55.1% 60.7% 68.8% FP16 8B W4A16 - - - 71.2% 64.7% 69.6% 69.2% 71.2%"
        },
        {
            "title": "6.3 Quantizing the embedding and output head\nIn Table 10, we perform an analysis of quantizing the em-\nbedding and output head in addition to W4A8 blocks. As\nthe weights in all layers are represented in 4-bit, the half-\nprecision embedding layer and the output head become the\nmemory bottlenecks, preventing the W4A8 models from\nbeing deployed to edge devices (ref. Figure 1 W4A8). As\na result, we experiment on quantizing the embedding and\noutput head in addition to W4A8 blocks and show the re-\nsults in Table 10. We show that larger models present more\nresilience to quantizing both the embedding layer and the\noutput head, as the accuracy on the LAMBADA dataset\nremains nearly unchanged. This finding is particularly use-\nful for deploying large models onto a device with limited\nmemory. Our framework provides different bit-width con-\nfigurations (i.e., 4-bit and 8-bit) for the embedding layer\nand output head, addressing the needs for deploying large\nmodels on edge devices.",
            "content": "Table 10: (Ablation study on the embedding and output head.) We experiment on quantizing the embedding and output head in addition to W4A8 blocks. The accuracy on the LAMBADA dataset is reported. size FP16 W4A8 blocks + 4-bit lm_head + 4-bit embed. + both 130M 43.7% 370M 53.1% 37.6% 50.5% 2.7B 69.5% 65.8% 8B 70.9% 68.5% 37.0% 50.3% 66.1% 68.3% 33.4% 46.2% 66.0% 69.0% 33.4% 46.6% 65.7% 68.8%"
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce Quamba2, robust post-training quantization framework tailored for selective State Space Models, compatible with W4A8, W4A16, and W8A8 on Mamba1 and Mamba2. Using channel order preservation and activation persistence observed in SSMs, we propose sort-and-cluster and per-state-group quantization techniques for the quantization of 8-bit activation. Experiments demonstrate that Quamba2 surpasses previous methods, offering significant reductions in latency and memory for both cloud and edge applications, addressing deployment challenges for emerging SSM-based applications on various platforms."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the ONR Minerva program, NSF CCF Grant No. 2107085, iMAGiNE - the Intelligent Machine Engineering Consortium at UT Austin, UT Cockrell School of Engineering Doctoral Fellowships, NSF Grant No. 2339084, and Taiwans NSTC Grant No. 111-2221-E-A49-148-MY3. References [1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr√≥n, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In: Conference on Empirical Methods in Natural Language Processing (EMNLP). 2023. Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. In: International Conference on Learning Representations (ICLR). 2024. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. In: Advances in Neural Information Processing Systems (NeurIPS) (2024). [2] [3] [4] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). 2020. [5] Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, and Diana Marculescu. Quamba: PostTraining Quantization Recipe for Selective State Space Models. In: International Conference on Learning Representations (ICLR). 2025. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. In: CoRR (2018). [7] Tri Dao. Causal depthwise conv1d in CUDA with PyTorch interface. 2024. url: https : / / github . com / Dao - AILab/causal-conv1d. [8] Tri Dao. Fast Hadamard Transform in CUDA, with PyTorch interface. 2024. url: https://github.com/DaoAILab/fast-hadamard-transform. [9] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In: International Conference on Machine Learning (ICML). 2024. [10] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. In: Advances in Neural Information Processing Systems (NeurIPS) 35 (2022), pp. 3031830332. [11] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In: Advances in Neural Information Processing Systems (NeurIPS) (2024). [12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. In: International Conference on Learning Representations (ICLR) (2023). [13] Elias Frantar, Roberto Castro, Jiale Chen, Torsten Hoefler, and Dan Alistarh. MARLIN: Mixed-Precision AutoRegressive Parallel Inference on Large Language Models. In: arXiv preprint arXiv:2408.11743 (2024). [14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. In: CoRR abs/2101.00027 (2021). [15] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation. 2023. doi: 10.5281/zenodo.10256836. url: https://zenodo.org/records/10256836. [16] Karan Goel, Albert Gu, Chris Donahue, and Christopher R√©. Its raw! audio generation with state-space models. In: International Conference on Machine Learning (ICML). 2022. [17] Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Chengtao Lv, Yunchen Zhang, Dacheng Tao, and Xianglong Liu. Llmc: Benchmarking large language model quantization with versatile compression toolkit. In: Conference on Empirical Methods in Natural Language Processing (EMNLP). 2024. [18] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In: Conference on Language Modeling (COLM). 2024. 11 [19] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R√©. Hippo: Recurrent memory with optimal polynomial projections. In: Advances in neural information processing systems (NeurIPS). 2020. [20] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In: The European Conference on Computer Vision (ECCV). 2020. [21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In: arXiv preprint arXiv:2009.03300 (2020). [22] Tanishq Kumar, Zachary Ankner, Benjamin Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher R√©, and Aditi Raghunathan. Scaling laws for precision. In: International Conference on Learning Representations (ICLR). 2025. [23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. In: Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles (SOSP). 2023. [24] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In: arXiv preprint arXiv:1906.00300 (2019). [25] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. In: European Conference on Computer Vision (ECCV). 2025. [26] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. In: arXiv preprint arXiv:2403.19887 (2024). Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. In: Proceedings of Machine Learning and Systems (MLSYS) (2024). [27] [28] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. In: arXiv preprint arXiv:2405.04532 (2024). [29] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. VMamba: Visual State Space Model. In: Advances in neural information processing systems (NeurIPS). 2024. [30] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. In: The 62nd Annual Meeting of the Association for Computational Linguistics (ACL) (2024). [31] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. SpinQuantLLM quantization with learned rotations. In: arXiv preprint arXiv:2405.16406 (2024). [32] Bruce Lee LY. CUDA HGEMM. 2024. url: https://github.com/Bruce-Lee-LY/cuda_hgemm. [33] Bruce Lee LY. CUDA HGEMV. 2024. url: https://github.com/Bruce-Lee-LY/cuda_hgemv. [34] Denis Paperno, Germ√°n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern√°ndez. The LAMBADA dataset: Word prediction requiring broad discourse context. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, (ACL). The Association for Computer Linguistics, 2016. [35] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for SQuAD. In: arXiv preprint arXiv:1806.03822 (2018). [36] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). 2020. [37] George Saon, Ankit Gupta, and Xiaodong Cui. Diagonal state space augmented transformers for speech recognition. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023. [38] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatronlm: Training multi-billion parameter language models using model parallelism. In: arXiv preprint arXiv:1909.08053 (2019). [39] Neil JA Sloane. library of Hadamard matrices. 1999. url: http://www.neilsloane.com/hadamard/index.html. Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers for Sequence Modeling. [40] In: International Conference on Learning Representations (ICLR). 2023. Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, and Zhiqiang Shen. Bi-Mamba: Towards Accurate 1-Bit State Space Models. In: arXiv preprint arXiv:2411.11843 (2024). [41] 12 [42] Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. Jamba-1.5: Hybrid transformer-mamba models at scale. In: arXiv preprint arXiv:2408.12570 (2024). [43] Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan Yan, Jack Kosaian, Mark Hoemmen, Haicheng Wu, Andrew Kerr, Matt Nicely, Duane Merrill, Dustyn Blasig, Fengqi Qiao, Piotr Majcher, Paul Springer, Markus Hohnerbach, Jin Wang, and Manish Gupta. CUTLASS. Version 3.0.0. Jan. 2023. url: https://github.com/ NVIDIA/cutlass. [44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. In: arXiv preprint arXiv:2307.09288 (2023). [45] Vaswani. Attention is all you need. In: Advances in Neural Information Processing Systems (NeurIPS) (2017). [46] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An Empirical Study of Mamba-based Language Models. In: arXiv preprint arXiv:2406.07887 (2024). Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander Rush. Mambabyte: Token-free selective state space model. In: Conference on Language Modeling (COLM). 2024. [47] [48] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In: International Conference on Machine Learning (ICML). 2023. [49] Zukang Xu, Yuxuan Yue, Xing Hu, Zhihang Yuan, Zixu Jiang, Zhixuan Chen, Jiangyong Yu, Chen Xu, Sifan Zhou, and Dawei Yang. MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods. In: International Conference on Learning Representations (ICLR). 2025. [50] Zhenxuan Yu, Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Slender-Mamba: Fully Quantized Mamba in 1.58 Bits From Head to Toe. In: Proceedings of the 31st International Conference on Computational Linguistics (COLING). 2025. [51] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models. In: Advances in Neural Information Processing Systems (NeurIPS) (2024). [52] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can Machine Really Finish Your Sentence? In: Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL). Ed. by Anna Korhonen, David R. Traum, and Llu√≠s M√†rquez. 2019. [53] Ying Zhang, Peng Zhang, Mincong Huang, Jingyang Xiang, Yujie Wang, Chao Wang, Yineng Zhang, Lei Yu, Chuan Liu, and Wei Lin. QQQ: Quality Quattuor-Bit Quantization for Large Language Models. In: arXiv preprint arXiv:2406.09904 (2024). Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, and Chuan Wu. QSpec: Speculative Decoding with Complementary Quantization Schemes. In: arXiv preprint arXiv:2410.11305 (2024). [54] [55] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. In: Proceedings of Machine Learning and Systems (MLSYS) (2024). [56] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. Survey on Efficient Inference for Large Language Models. In: Transactions on Machine Learning Research (TMLR) (2024). [57] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. In: International Conference on Machine Learning (ICML). 2024. [58] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. survey on model compression for large language models. In: Transactions of the Association for Computational Linguistics (TACL) (2024). 13 Full Results for Six Zero-shot Downstream Tasks In Table 11, we follow the evaluation protocol in Mamba (Gu and Dao 2024), and report the accuracy for LAMBADA (Paperno et al. 2016), WinoGrande (Sakaguchi et al. 2020), PIQA (Bisk et al. 2020) and ARC-easy (Clark et al. 2018), and the accuracy normalized by the sequence length for HellaSwag (Zellers et al. 2019) and ARC-challenge (Clark et al. 2018). Given the slight variation in accuracy across runs, we present the average accuracy over five runs in each table. Our frame work outperforms Quamba (Chiang et al. 2025) in the Mamba1 backbone, providing with more quantization flavors such as W8A8, W4A8 and W4A16 for different use cases. Our method also outperforms Quamba in the Mamba2 backbone, where we apply the clipping technique to Mamba2, by large gap in the average accuracy. Table 11: (Zero-shot accuracy.) We evaluate our framework on six common sense tasks and report the average of five runs. Our framework surpass previous baseline, Quamba (Chiang et al. 2025), in average accuracy on both Mamba1 and Mamba2 backbones, with supporting more quantization flavors."
        },
        {
            "title": "Bitwidth",
            "content": "LA HS PIQA Arc-E Arc-C WG Avg."
        },
        {
            "title": "Mamba",
            "content": "1.4B 2.8B 1.3B Mamba2 2.7B - FP16 64.9% 59.1% 74.2% 65.5% 32.8% 61.5% 59.7%"
        },
        {
            "title": "Quamba",
            "content": "W8A8 61.4% 58.3% 72.7% 64.0% 32.3% 58.8% 57.9% Quamba2 (Ours) W8A8 W4A8 W4A 62.3% 58.6% 73.1% 64.0% 61.5% 57.6% 72.0% 63.0% 63.6% 58.1% 72.6% 64.3% 32.2% 32.2% 32.4% 58.5% 58.1% 58.7% 57.5% 60.5% 58.5% - FP16 69.1% 65.9% 75.6% 69.2% 35.8% 63.0% 63.1%"
        },
        {
            "title": "Quamba",
            "content": "W8A8 65.4% 65.1% 74.2% 68.9% 35.9% 62.6% 62.0% Quamba2 (Ours) W8A8 W4A8 W4A 65.7% 65.4% 74.5% 68.9% 63.5% 64.9% 74.2% 68.2% 66.0% 65.3% 74.6% 69.2% 36.7% 35.3% 36.6% 61.8% 62.2% 62.2% 61.4% 63.6% 62.6% - FP16 65.6% 59.9% 73.3% 64.1% 33.3% 60.8% 59.5%"
        },
        {
            "title": "Quamba",
            "content": "W8A8 49.8% 58.5% 71.2% 61.9% 32.1% 58.1% 55.2% Quamba2 (Ours) W8A8 W4A8 W4A 62.0% 59.2% 72.5% 63.4% 61.0% 58.8% 72.4% 62.7% 64.3% 59.2% 72.6% 63.8% 32.7% 32.6% 33.1% 60.0% 58.3% 59.1% 57.7% 60.3% 58.9% - FP16 69.5% 66.6% 76.4% 69.5% 36.4% 64.2% 63.8%"
        },
        {
            "title": "Quamba",
            "content": "W8A8 52.4% 60.4% 71.6% 62.9% 33.7% 58.0% 56.5% Quamba2 (Ours) W8A8 W4A8 W4A 66.1% 65.5% 74.4% 68.4% 65.6% 65.1% 74.7% 68.1% 68.8% 65.6% 75.5% 68.6% 37.1% 36.1% 36.6% 63.7% 62.5% 62.8% 62.1% 64.9% 63.3% - FP16 70.9% 77.7% 79.7% 76.0% 48.0% 72.0% 70.7%"
        },
        {
            "title": "Quamba",
            "content": "W8A8 54.0% 74.6% 77.1% 73.5% 44.2% 65.5% 64.8% 8B Quamba2 (Ours) W8A8 W4A8 W4A16 69.8% 77.8% 79.1% 75.9% 68.8% 77.1% 79.1% 75.0% 71.2% 76.8% 79.1% 75.2% 46.9% 46.0% 45.9% 69.0% 69.8% 68.7% 69.1% 70.8% 69.8%"
        },
        {
            "title": "B Evaluation Results on Generation Tasks",
            "content": "We evaluate Mamba2-8B with all bit-widths on the generation-based tasks Natural Questions (NQ) (exact match) (Lee, Chang, and Toutanova 2019) and SquadV2 (F1) (Rajpurkar, Jia, and Liang 2018) on the open-source LM-EVAL (Gao et al. 2023). We show the results in Table 12. The W4A16 model closely matches the FP16 model, whereas the W4A8 and W8A8 models, with 8-bit SSM states, preserve the meaningful generation outputs. We show that the searched W4Aùëã also improves the generation scores and outperforms the W4A8 model. This result reveals an interesting observation that cached SSM states are redundant, which can be carefully quantized to 8 bits. Our framework supports 8-bit SSM states for W4A8 and W8A8 models and improves their generation speeds with large batch-size inputs, as the cached states are the major memory and latency bottlenecks. Please refer to Section for more details. Table 12: (Generation tasks.) We evaluate Mamba2-8B with different precisions on the generation tasks. Bit-width FP16 W8A8 W4A8 W4A16 W4AX NQ 17.2 15.0 14.2 16.6 14. SquadV2 51.9 43.6 45.9 50.7 47.4 Implementation and Evaluation Details of Quamba2 Framework Quantization setup. The calibration set is constructed by randomly sampling 512 sentences from the Pile dataset (Gao et al. 2021), where we fixed the random seed in the sampling process. We collect the static scaling factors for each operator based on the absolute maximum value observed from the calibration set to quantize the activations and cached SSM states in both W4A8 and W8A8 settings. The same scaling factors are applied in all our experiments. Implementation. We implement our framework based on CUTLASS (Thakkar et al. 2023), vLLM (Kwon et al. 2023). Our 4-bit and 8-bit matrix multiplication (matmul) kernels are adapted from (Frantar et al. 2024; LY 2024a,b; Xiao et al. 2023; Zhang et al. 2024). We implement W4A8 and W4A16 fused matmul-transpose kernels for the Mamba1 architecture. We apply GPTQ (Frantar et al. 2023) to the projection layers in the 4-bit weight settings. Quantization is integrated and adapted to the CUDA kernels of both the fast Hadamard transform (Dao 2024b) and causal convolution (Dao 2024a). Furthermore, the selective scan and SSD kernels (Dao and Gu 2024; Gu and Dao 2024) are modified to accommodate inputs with quantized weights, activations, and their scaling factors. Latency and model size profiling. We evaluate all methods on the A5000, widely used GPU for AI workloads with 24GB of memory, emulating the setting for cloud applications. For edge applications, we profile all methods on the Nvidia Orin Nano 8G. We perform few warm-up iterations and then report the average latency of the next 100 iterations. We report the size of the model that includes all quantized parameters and buffers for calculation. Details for Mixed Precision Quamba2 In Table 7, we outline the generalizability issue when utilizing the precision of W4A8 only. We show that our W4Aùëã mixed-precision models mitigate accuracy degradation while incurring only marginal latency overhead. Figure 6 visualizes the detailed layer-wise bit-width configuration of Quamba2-8B-W4Aùëã . The handcrafted mixed-precision models. We explored two types of handcrafted (HC) configurations, referred to as HC_first and HC_last, where we apply W4A16 blocks at the beginning and end of the network, respectively. Handcrafted configurations only deliver marginal improvements in the average accuracy (approximately 1% on MMLU), and still fall behind in the upper bound scenario, where all blocks utilize the precision of W4A16, as shown in Table 7. The automated W4Aùëã models. We implement evolutionary search to identify the best mix of precision levels (Guo et al. 2020). We set the population size to 40 and the number of generations to 5. In each generation, the top performing half of the candidates are retained, with 10 mutation and crossover operations applied, respectively, to generate new candidate precision configurations. The search algorithm identifies the sensitive blocks and assigns W4A16 to these blocks. This automated approach searches the best mix-precision configurations and balances between the precision and performance. Our W4Aùëã models addresses the performance gaps in the MMLU dataset, as shown in Table 7, compared to naive mixed-precision and pure W4A8 models. 15 Figure 6: (The layer-wise bit-width for Quamba2-8B-W4Aùëã .) We search the bit-width for Quamba2-8B-W4Aùëã (the last row in red), which outperforms the handcraft counterparts shown in the first (HC_first) and the second (HC_last) rows."
        },
        {
            "title": "E Investigating Memory and Latency with Large Batch Sizes",
            "content": "The cached state sizes. Although the constant state nature of SSMs, the cached states grow linearly with respect to the input batch size. We show theoretical memory breakdowns versus batch size in Figure 7 (a). As the batch size increased, cached states occupied most of the total memory, making state loading and updating the bottleneck during generation. Our framework (W4A8) compresses and updates the states with 8-bit, thus decreasing overall memory usage and generation latency for cloud applications with large batch sizes. Figure 7: (Large batch inputs.) The cached states grow linearly with respect to the input batch size. For batch size of 128, half-precision cached states use most of the memory (a), making state loading and updating the bottleneck during generation. Our framework (W4A8) compresses the states to 8-bit, thereby reducing the total memory and generation latency (b) with large batch size inputs for cloud-based applications. 16 Quantizing cached SSM states. We reduce generation latency by quantizing the cached SSM states to 8-bit for W4A8 and W8A8 models. Since the cached SSM states follow the head reordering and channel grouping indices from the SSM input ùë• (ref. Figure 4), we apply the same ùëö head and ùëõ channel groups to quantize each SSM state before caching them in memory. This finding eliminates the need for additional online reordering of SSM states and only requires calibrating the SSM quantization scales. Our approach introduces dstate ùëö ùëõ floating-point scales with minimal latency overhead, while significantly reducing the state update latency, as shown in Figure 7 (b). Figure 8: (SSM states.) The states are quantized before cached in memory. We apply the same ùëö head and ùëõ channel groups from the SSM input ùë• each SSM. The roofline model. We show the roofline model of A5000 GPU in Figure 9 (w-bita-bit in the figure), and profile the generation latency (i.e., time-per-output-token, TPOT) of Mamba2-8B on A5000 with different batch sizes in Table 10. When the input batch size is small (e.g., b=1 in the table), the generation is memory-bound and therefore loading 4-bit weights (e.g., W4A8 and W4A16) improves the roofline model. As the batch size increased (e.g., b=64 in the table), the W4A16 models are bounded by hardware performance in terms of trillions of operations per second (TOPS). In contrast, the W4A8 and W8A8 models leverage 8-bit computation and deliver better TOPS. The ultimate TOPS of W4A8 is lower than W8A8 due to the extra steps for dequantizing weights from 4-bit to 8-bit (e.g., b=256 in the table). Our framework supports W8A8, W4A8, and W4A16 that are at the frontier of the roofline model to satisfy the deployment needs of most applications for both Mamba1 and Mamba2. Figure 10: (Mamba2-8B TPOT on A5000 24GB.) We compress the cached SSM states with 8-bit, enabling larger batch size inputs under the same memory constraints. We report latency in milliseconds (ms). OOM denotes out-of-memory. Bitwidth b=1 b=32 b=64 b= b=256 FP16 W8A8 W4A8 W4A16 22.73 12.61 7.43 7.58 35.74 23.83 15.05 20.58 49.63 30.82 24.65 38.48 OOM 44.85 44.54 74. OOM 79.65 85.26 OOM Figure 9: (Roofline model of A5000.) 17 Batch size vs. time-to-last-token latency across bit-widths. Figure 11 shows the time-to-last-token (TTLT) of Mamba2-8B quantized with different bit-widths (e.g., W8A8, W4A8, and W4A16) supported by our framework on A5000. We vary the batch size of the input from 1 to 64, and profile the end-to-end latency of pre-filling 2024 tokens and generating 2048 tokens (i.e., TTLT). The latency is estimated for the batch sizes that empirically do not fit A5000 and is represented with dashed lines with unfilled markers. We show that the W4A8 Mamba-8B model is suited for most latency-sensitive applications, serving with general batch sizes (i.e., range from 1 to 64) on both cloud and edge devices. In contrast, W4A16 serves as better option for personal applications (i.e., batch size equal to one) on mobile platforms as it features higher average accuracy (ref. Table 11 and 7). For large batch size (i.e., greater than 128), the W8A8 model delivers the highest performance in terms of latency. Our framework supports all options on the frontier of the roofline model, as shown in Figure 9. Accuracy-latency Trade-off (Batch size vs. time-to-last-token.) Figure 11: W4A8 is suited for most applications serving with general batch sizes among all supported bit-widths. Figure 12 illustrates the average accuracy across six zero-shot tasks (y-axis) Comparing across backbone models. versus latency (x-axis, in log-scale) on cloud-based A5000 GPU. We profile TTFT (time-to-first-token) in milliseconds (msec.) using 4K input tokens, as shown on the left. For comparison of end-to-end latency, we profile TTLT (time-to-lasttoken) in seconds (sec.), with 2K input tokens and 2K generated tokens, as shown on the right. For QuaRot (Ashkboos et al. 2024b), we use the official implementation and profile latency for Llama-2 (Touvron et al. 2023). We note that latency is merely estimated for models that do not fit within the 24GB memory of the GPU and is represented with dashed lines. Quamba2 models are on the Pareto frontier and offers the best trade-off between average accuracy and latency, as well as smallest memory footprints, outperforming other low bit-width SSM and Transformer baselines. Figure 12: (Accuracy-latency trade-off.) Quamba2 models (green) are on the Pareto frontier over other low bit-width SSM (red) and Transformer (purple) baselines, while also featuring lower memory footprints as evidenced in the size of the circle."
        }
    ],
    "affiliations": [
        "Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin",
        "Department of Computer Science, National Yang Ming Chiao Tung University",
        "Department of Electrical and Computer Engineering, Cornell University"
    ]
}