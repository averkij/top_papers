{
    "paper_title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
    "authors": [
        "Zeyu Zhang",
        "Shuning Chang",
        "Yuanyu He",
        "Yizeng Han",
        "Jiasheng Tang",
        "Fan Wang",
        "Bohan Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 3 7 9 2 2 . 1 1 5 2 : r BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation Zeyu Zhang1, Shuning Chang1, Yuanyu He12, Yizeng Han1, Jiasheng Tang13, Fan Wang1, Bohan Zhuang12 1DAMO Academy, Alibaba Group 2ZIP Lab, Zhejiang University 3Hupan Lab Corresponding authors. Generating minute-long videos is critical step toward developing world models, providing foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrarylength video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves 22.2% improvement on VDE Subject and 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Date: December 1, 2025 Project website: https://ziplab.co/BlockVid Inferix (Code): https://github.com/alibaba-damo-academy/Inferix Correspondence: jiasheng.tjs@alibaba-inc.com, bohan.zhuang@gmail.com"
        },
        {
            "title": "1 Introduction",
            "content": "Long video generation is crucial for creating realistic and coherent narratives that unfold over extended durations, which is essential for applications such as filmmaking, digital storytelling, and virtual simulation [44, 35, 20, 26, 28, 36]. Moreover, the ability to generate minute-long videos is key step toward world models, which act as foundational simulators for agentic AI, embodied AI, and gaming [4, 31]. key breakthrough empowering this is the semiautoregressive (block-diffusion, Figure 1 (3)) [2] paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in blocksapplying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences [19, 33]. Notably, it addresses the key limitations of both diffusion and autoregressive (AR) models. Most current video diffusion models [34] rely on the Diffusion Transformer (DiT) [29]which uses bidirectional attention without KV caching. While this Figure 1 Architecture comparison: AR vs. Diffusion vs. Block Diffusion (Semi-AR). Our BlockVid aims to tackle the chunk-wise accumulation error of block diffusion, enabling high-fidelity and coherent minutelong video generation. enables parallelized generation and controllability, decoding is inefficient and restricted to fixed lengths. In contrast, AR-based frameworks [37] support variable-length generation and KV Cache management, but their generation quality lags behind video diffusion, and decoding is not parallelizable. Importantly, block diffusion [19, 33] interpolates between AR and diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation. However, existing block diffusion methods face two fundamental challenges. First, the AR paradigm inevitably suffers from error accumulation, where small prediction errors gradually build up over time and will be directly stored in the KV cache [22]. In long video generation, the accumulated errors typically manifest as quality degradation, color drift, subject and background inconsistency, and visual distortions [27]. As the sequence extends, these errors compound, weakening long-range dependencies and limiting its effectiveness for generating coherent, minute-long videos (Figure 2). Second, the domain is hindered by the lack of fine-grained long video datasets and reliable evaluation metrics. Currently, most open-source datasets consist of only short or fragmented chunks, with few minute-long datasets featuring fine-grained annotations. Meanwhile, existing benchmarks and metrics like VBench [21] focus on diversity or object categories but fail to capture error accumulation and coherence over extended durations. To this end, we propose BlockVid, semi-autoregressive block diffusion model generating minute-long videos in chunk-by-chunk manner, as shown in Figure 3. Three strategies are proposed to systematically address the chunk-level accumulation error induced by the KV cache from both training and inference perspectives. 1) The proposed semantic sparse KV cache selectively stores salient tokens from past chunks and retrieves the most semantically aligned context for the current prompt, thereby efficiently maintaining long-range consistency without propagating redundant errors. 2) We further introduce Block Forcing to regularize chunk-wise predictions, and also integrate the Self Forcing loss [19] to bridge the traininginference gap. This prevents models from drifting over long horizons, such as losing track of subjects or gradually altering scene content. 3) We further develop chunk-level noise scheduler that smoothly increases noise levels and an inter-chunk noise shuffling strategy to enhance temporal consistency and reduce error accumulation over extended durations. To address the lack of long-video datasets and benchmarks, we propose LV-Bench, collection of 1,000 minute-long videos with fine-grained annotations for every 25 second chunk. To better evaluate long video generation quality, we further introduce Video Drift Error (VDE) metrics based on Weighted Mean Absolute Percentage Error (WMAPE) [23, 10], integrated with original VBench metrics, providing more comprehensive reflection of temporal consistency and long-range visual fidelity. Comprehensive experiments are conducted on both LV-Bench and the traditional VBench to demonstrate the superiority of our method. Notably, BlockVid achieves 22.2% improvement on VDE Subject and 19.4% improvement on VDE Clarity in LV-Bench over the state of the art. Our contributions can be summarized as follows: We propose BlockVid, semi-autoregressive block diffusion framework that incorporates semantic sparse KV cache, novel Block Forcing training strategy and chunk-aware noise scheduling and shuffling scheme. These components jointly mitigate chunk-wise accumulation errors and maintain long-range temporal coherence. We introduce LV-Bench, benchmark of 1,000 minute-long videos with fine-grained chunk-level annotations, along with the Video Drift Error (VDE) metric to evaluate temporal consistency and long-horizon visual fidelity. We conduct extensive experiments on LV-Bench and VBench, demonstrating that BlockVid significantly outperforms state-of-the-art baselines across both coherence-aware and perceptual quality metrics."
        },
        {
            "title": "2 Related Work",
            "content": "Long video generation. Minute-long video generation can be roughly grouped into three settings: single-shot, multi-shot generation, and movie-style video composition. 2 Figure 2 Comparison of visualization results between our method and different baselines in terms of accumulation error. Details can be found in Appendix A.7. (1) Single-shot generation aims to produce minute-long chunk within consistent scene and semantic context, emphasizing long-range temporal coherence and visual stability. Approaches fall into AR and semi-AR (block diffusion) families. AR methods, such as FAR [12] and Loong [37], formulate long video generation as next-frame (or next segment) prediction. Semi-AR methods generate videos chunk by chunk, while performing iterative diffusion-based [29] denoising within each chunk. Their key design choice lies in the chunk-level causal conditioning: MAGI-1 [33], Skyreel-V2 [5], and Self Forcing [19] proceed strictly sequentially across chunks, whereas FramePack [46] adopts symmetric schedule that treats both ends as guidance and fills the middle autoregressively. In practice, semi-AR methods typically rely on careful KV cache usage for efficiency and stability over long horizons. (2) Multi-shot generation typically focuses on handling camera motions and transitions across scenes or semantics. Recent systems, such as LCT [13], RIFLEx [48], and MoC [3], organize textvideo units with interleaved layouts and positional extrapolation to accommodate multiple shots. (3) Movie-style generation aims to create cinematic content by stitching multiple chunks with different scenes and styles, while maintaining coherent global narrative or theme. Methods [9, 47, 39, 40] resemble film editing, combining diverse shots into single coherent video guided by chunk-level text descriptions. Block diffusion (semi-autoregressive or chunk-by-chunk diffusion) decodes long sequences in blocks: within each block the model performs iterative diffusion denoising, while across blocks it conditions on previously generated content via KV caches. This paradigm has been explored in both text and video. In language modeling, BD3-LM [2] and SSD-LM [15] demonstrate that blockwise diffusion can combine bidirectional refinement within block with efficient, variable-length decoding through cached context across blocks. In video generation, related formulations adopt chunk-wise diffusion with causal conditioning to interpolate between pure diffusion (e.g. DiT-style bidirectional attention without KV caching) and AR (variable-length decoding with KV caching but weaker visual fidelity and limited parallelism). Representative systems include MAGI-1 [33], Self Forcing [19], CausVid [45], ViD-GPT [11], and SkyReels-V2 [5], which condition each new chunk on past chunks to extend temporal horizons while retaining diffusions denoising quality within chunk. Despite progress, block diffusion methods remain constrained by KV cacheinduced errors, limited scalability, and the lack of long video datasets and coherence-aware metrics. We address these gaps with (1) BlockVid, framework featuring semantic sparse KV cache, Block Forcing, and tailored noise scheduling to enhance long-range coherence, and (2) LV-Bench, benchmark of 1,000 minute-long videos with metrics for evaluating temporal consistency."
        },
        {
            "title": "3.1 Overview: Block Diffusion Architecture",
            "content": "BlockVid introduces semi-AR block diffusion architecture. During training, we are given single-shot long video = {V1, V2, V3, . . . , Vn}, where each video chunk Vi R(1+T )HW 3, with frames, height H, width , and 3 RGB channels. We also have the corresponding chunk level prompts = {yi}n , with yi conditioning Vi. Specifically, the first frame serves as the image guidance. The 3D causal VAE compresses its spatio-temporal dimensions to [(1 + /4), H/8, W/8] while expanding the number of channels to 16, resulting in the latent representation R(1+T /4)H/8W/816. The first frame is compressed only spatially to better handle the image guidance. i=1 During post-training, we introduce Block Forcing, training strategy that stabilizes long video generation by jointly integrating Block Forcing and Self Forcing objectives. Block Forcing aligns predicted dynamics with semantic history to prevent drift, while Self Forcing closes the traininginference gap by exposing the model to its own roll-outs and enforcing sequence-level realism. As shown in Figure 3, in the latent space, the representation is first processed by the block diffusion denoiser to produce the denoised latent Z. During this procedure, the semantic sparse KV cache is dynamically constructed and preserved as compact memory of salient keys and values, serving as semantic guidance for subsequent chunk generation. Subsequently, the denoised latent is projected back into the video space X. Besides, we design noise scheduling strategy that operates both during training and inference to stabilize long video generation. During training, progressive noise scheduling gradually increases noise levels across chunks. While during inference, noise shuffling introduces local randomness at chunk boundaries to smooth transitions and maintain coherence."
        },
        {
            "title": "3.2 Background: Self Forcing\nA major challenge in long video generation is the training-inference gap: during training the model is\nconditioned on ground-truth frames (teacher forcing), but at inference it relies on its own imperfect outputs,\nleading to exposure bias and error accumulation. To address this, we adopt the Self Forcing loss [19], where\nthe model generates a full video sequence ˜x1:T semi-autoregressively and is then penalized at the video level\nby matching its distribution pθ to the real distribution pdata. Concretely, a discriminator D evaluates entire\nvideos, and the generator G is trained to minimize",
            "content": "LSF = min max Expdata[log D(x)] + Expθ[log(1 D(x))], (1) where pθ is obtained by the predictions of G. This formulation exposes the model to its own errors during training and enforces sequence-level realism, thereby reducing exposure bias and improving temporal consistency."
        },
        {
            "title": "3.3 Block Forcing",
            "content": "Although Self Forcing mitigates the traininginference gap, it stabilizes predictions only within single chunk and lacks mechanism for maintaining cross-chunk coherence. Moreover, when generating very long videos, model trained with Self Forcing alone can still lose track of the subject or scene, leading to gradual drift (e.g., the character slowly changing identity or the background progressively melting). To address these limitations, we introduce Block Forcing loss, which decomposes the learning objective into two complementary parts. From fidelity perspective, Block Forcing preserves the reconstruction quality of the current chunk by supervising the model under the stochastic interpolant formulation [1] of Flow Matching [25]. Instead of predicting additive Gaussian noise as in DDPM, Flow Matching constructs continuous trajectory between the real starting frame xstart and Gaussian endpoint ϵ (0, I): 4 Figure 3 Overview of the BlockVid semi-AR framework. The generation of chunk + 1 is conditioned on both local KV cache and globally retrieved context. The global context is dynamically assembled by retrieving top-l semantically similar KV chunks via prompt embedding similarity. Upon generation, the bank is updated with the new chunks most salient KV tokens Along this trajectory, the model predicts the corresponding velocity field xt = (1 t)xstart + tϵ. vt = ϵ xstart, (2) (3) which provides the denoising direction in latent space and ensures accurate reconstruction of the current chunk. From semantic perspective, Block Forcing enforces semantic alignment between the current video chunk and its most relevant historical context. Specifically, the top-l past chunks are resampled to match the temporal length of the current chunk and averaged into semantic reference xcond, which serves as high-level guidance to maintain long-term coherence. In the stochastic interpolant formulation of flow matching [1], the model predicts velocity field vpred that represents the temporal derivative of the interpolated state xt between noise and data: vpred = vt(xt) = dt xt = fθ(xt, t). (4) Formally, the Block Forcing loss penalizes the deviation of the predicted velocity vpred from both the noise term ϵ and the semantic reference xcond, weighted by γ [0, 1]: (cid:104) LBF = vpred (ϵ γ xcond)2(cid:105) . (5) 5 This formulation ensures that the model learns not only to denoise the current chunk correctly but also to remain semantically anchored to the relevant history, thereby reducing temporal drift and improving the stability of long video generation. The final training loss is = LSF + LBF."
        },
        {
            "title": "3.4 Semantic Sparse KV Cache",
            "content": "Long video generation requires preserving dependencies across many chunks. However, storing and conditioning on the full KV context imposes heavy memory, computational burdens, and accumulation error. Moreover, simply caching the most recent chunks fails to capture long-range semantic relations. To address this, we introduce Semantic Sparse KV Cache that selectively stores only the most informative tokens and retrieves relevant past KV chunks, enabling efficient and coherent long-range conditioning. Inspired by ZipVL [16], we first dynamically identify salient tokens with probing mechanism and store the most informative KV tokens as the KV cache. Formally, given the current chunk and its queries Q, keys K, and values , we compute the attention score matrix = Softmax (cid:16) QK + Mask (cid:17) , (6) where the Mask denotes chunk-level causal mask. Then aggregate scores across heads and probe queries to form an importance vector m. Then the important tokens are selected using the top-k indexing method, with being the minimal number of tokens that cover fraction τ of the total importance score: Ikeep = topk_index(m, ). (7) This produces sparse cache (Ksparse, Vsparse) containing only the most relevant context tokens. During generation, the sparse KV caches from past chunks are stored in global KV bank and retrieved based on their semantic similarity with prompt embeddings: simi = cos(cid:0)Ec, Ei (cid:1), {1, . . . , c1}, (8) where Ec is the current prompts embedding and Ei are the past ones. The top-l most similar entries are then selected. Finally, we concatenate the top-l semantic KV caches with the two most recent caches to form the final KV cache: (K , ) = ConcatKV (cid:16) {(Kj, Vj)}jseq_ctx, (cid:17) {(Ki, Vi)}itop-l . (9) where seq_ctx = {c2, c1} (if available). The detailed algorithm is provided in Appendix A.2. Finally, the aggregated KV cache (K , ) serves as conditional context, combined with the current prompt yt to guide the generation of the target chunk: Vt pθ( , , yt) . (10)"
        },
        {
            "title": "3.5 Chunk-Level Noise Scheduling and Shuffling",
            "content": "Long video generation easily accumulates errors across chunks: once later chunks drift, the error spreads and becomes worse [41]. To reduce this drift, we assign lower noise to early chunks so they clearly establish the scene, and higher noise to later chunks so they remain more uncertain and rely on the earlier, more reliable chunks for guidance. This creates more stable cross-chunk behavior and leads to smoother long-range temporal transitions. The core idea here is to assign each chunk different noise level, progressively increasing noise levels rather than using fixed one. Specifically, if we split video into chunks, each chunk is assigned noise level ϵc increasing with c, where = 1, . . . , n. 6 Table 1 Overview of the datasets used for constructing LV-Bench. Dataset Video Number Object Classes DanceTrack GOT-10k HD-VILA-100M ShareGPT4V 66 272 117 545 Humans (66, 100%) Humans (177, 65%) Animals (54, 20%) Environment (41, 15%) Humans (47, 40%) Animals (35, 30%) Environment (35, 30%) Humans (381, 70%) Animals (82, 15%) Environment (82, 15%) LV-Bench Humans (671, 67%) Animals (171, 17%) Environment (158, 16%) We adopt cosine schedule, which provides smooth acceleration and deceleration: ϵc = ϵmin + 1 2 (ϵmax ϵmin) (cid:16) 1 cos(cid:0)π n1 (cid:1)(cid:17) , = 1, 2, . . . , n. (11) In this setting, the first chunk has ϵ0 = ϵmin (nonzero initial noise), and the last chunk has ϵn1 = ϵmax (maximal noise). For more details about the noise schedules, please refer to Appendix A.5. Inspired by FreeNoise [30], we further adapt local noise shuffling to the chunk-by-chunk setting. During inference, each chunk inherits per-frame base noises {ϵ(c) from fixed random seed, where {1, . . . , } indexes frames within chunk and is the number of frames per chunk. To smooth the transition across chunk boundaries, we apply chunk-aware shuffle unit of size to the prefix and suffix regions. Specifically, the last frames of chunk and the first frames of chunk c+1 are shuffled independently within their local window: }T t=1 s+1:T = Shuffle(cid:0)ϵ(c) ϵ(c) s+1:T (cid:1), (12) 1:s = Shuffle(cid:0)ϵ(c+1) ϵ(c+1) This local permutation preserves the global order of chunks while introducing shared stochasticity at the boundaries, which encourages the model to fuse adjacent chunks more smoothly. In contrast to re-sampling entirely new noise for each chunk, this strategy maintains long-range coherence while mitigating abrupt transitions at chunk boundaries. (13) (cid:1). 1:s"
        },
        {
            "title": "4 LV-Bench",
            "content": "Dataset. To tackle the challenge of minute-long video generation, we curate dataset of 1000 videos from diverse open-source sources and annotate them in detail. As shown in Table 1, we collect high-quality video chunks with lengths of at least 50 seconds from DanceTrack [32], GOT-10k [17], HD-VILA-100M [43], and ShareGPT4V [6]. To obtain high-quality annotations, we employ GPT-4o as data engine to generate fine-grained captions for every 23 seconds in each video. The detailed prompt can be found in Appendix A.3. Human-in-the-loop validation consists of manual visual checks at every stage of data production, including data sourcing, chunk splitting, and captioning, to ensure high-quality annotations. In the data sourcing stage, human annotators select high-quality videos and determine whether each raw video is suitable for inclusion. In chunk splitting, human annotators examine samples to verify that each chunk is free of errors such as incorrect transitions. In captioning, human annotators review the generated descriptions to ensure semantic accuracy and coherence. At each stage, at least two human annotators participate to provide inter-rater reliability. We then randomly divided LV-Bench into an 8:2 split for training and evaluation. Metrics. Drift penalties have been widely adopted to address information dilution [24] and degradation [27] in long video generation. For example, IP-FVR [14] focuses on preserving identity consistency, while MoCA [42] employs an identity perceptual loss to penalize frame-to-frame identity drift. Inspired by the commonly used metrics MAPE and WMAPE [23, 10], we propose new metric called Video Drift Error (VDE) to 7 Table 2 Comparison of different methods on LV-Bench. We report LV-Bench results on five VDE metrics and five complementary metrics from VBench [21]. Our method achieves superior performance on the majority of these metrics. Method VDE Subject VDE Background VDE Motion VDE Aesthetic VDE Clarity MAGI-1 Self Forcing PAVDM FramePack SkyReels-V2-DF-1.3B 0.3090 0.3716 1.8292 4.3984 0. 0.5000 1.6108 0.9323 5.9421 0.3179 0.0243 0.1549 0.0461 0.0387 0.0195 3.8286 3.4683 2.8957 1.4751 1.2083 2.7225 3.0798 1.9503 4.2513 0.9365 BlockVid-1.3B (Ours) 0. 0.2945 0.0119 0.9618 0.7551 Method Subject Consistency Background Consistency Motion Smoothness Aesthetic Quality Image Quality MAGI-1 Self Forcing PAVDM FramePack SkyReels-V2-DF-1.3B 0.8992 0.8481 0.8640 0.9001 0. 0.9078 0.8203 0.8924 0.8791 0.9579 0.9947 0.9947 0.9926 0.9949 0.9931 0.6508 0.6283 0.5267 0.6043 0.6035 0.6662 0.6805 0.6567 0.6972 0.6835 BlockVid-1.3B (Ours) 0.9597 0.9588 0.9956 0.6047 0.6852 measure changes in video quality. We further design 5 long video generation metrics based on VDE. The core idea involves dividing long video into multiple segments, each evaluated according to specific quality metrics (clarity, motion smoothness, etc). Specifically, (1) VDE Clarity measures temporal drift in image sharpness, where creeping blur increases the score, while low value indicates stable clarity over time. (2) VDE Motion measures drift in motion smoothness, where low score indicates consistent dynamics without jitter or freezing. (3) VDE Aesthetic measures drift in visual appeal, where low score indicates sustained and coherent aesthetics over time. (4) VDE Background measures background stability, where low score indicates consistent setting without drift or flicker over time. (5) VDE Subject tracks identity drift, where low score indicates the subject remains consistently recognizable over time. Following previous works [13, 3], we also include five complementary metrics from VBench [21]. The details are included in Appendix A.4."
        },
        {
            "title": "5.1 Implementation Details\nLV-1.1M dataset. To improve post-training data for semi-AR models, we introduce LV-1.1M, a private curated\ndataset of 1.1M long-take videos with fine-grained annotations. Each video is segmented into chunks, captioned\nwith GPT-4o, and aligned into coherent storylines, providing reliable supervision for long video generation.\nFor more details see Appendix A.6.",
            "content": "Multi-stage post-training. We adopt two-stage post-training strategy. In Stage 1, we post-train BlockVid on LV-1.1M to enhance its ability to handle long-take videos with coherent semantics, large-scale motions, and diverse content. This stage focuses on improving temporal reasoning and narrative consistency under high-quality but heterogeneous video data. In Stage 2, we further post-train the model on the training split of LV-Bench, dataset containing longer videos (50s) compared to Stage 1, in order to enhance the models extrapolation capability. Training setup. We first initialize our model with SkyReels-V2-DF-1.3B [5], which is customized version of Wan2.1-T2V-1.3B [34]. The video resolution used for training and inference follows the standard 480p (854480). Experiments are conducted on distributed computing cluster equipped with high-performance GPU nodes, each containing 192 CPU cores, 960 GB of system memory, and 8 NVIDIA H20 GPUs (96 8 Table 3 Comparison of different methods on VBench [21]. We report VBench metrics of different methods following the single-shot long video generation setting [13, 3]. Our method achieves superior performance in the majority of these metrics. Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Image Quality Method MAGI-1 Self Forcing PAVDM FramePack SkyReels-V2-DF-1.3B LCT (MMDiT-3B) MoC 0.8320 0.8211 0.8415 0.9019 0.9391 0.9380 0.9398 BlockVid-1.3B (Ours) 0.9410 0.8931 0.9050 0.9273 0.9450 0.9580 0.9623 0.9670 0. 0.9740 0.9799 0.9769 0.9805 0.9838 0.9816 0.9851 0.5537 0.6015 0.6537 0.5715 0.6529 0.6875 0.7500 0.5010 0.5130 0.4970 0.5044 0.5320 0.5200 0.5547 0.6120 0.6218 0.6280 0.6381 0.6315 0.6345 0.6396 0.9870 0. 0.5839 0.6527 GB each). InfiniBand interconnects provide high-bandwidth communication across nodes for distributed training. In Stage 1, we train the model on 32 GPUs, requiring approximately 7 days per configuration to complete one epoch over the entire LV-1.1M dataset. In Stage 2, we further train the model on 32 GPUs, requiring approximately 50 hours per configuration to complete two epochs over the entire LV-Bench training set. We employ AdamW and stepwise decay schedule for all stages of post-training. The initial learning rate is 1 104, then reduced to 5 105, with the weight decay set to 1 104. The noise level at the last time step corresponds to ϵmax, where the SNR is 0.003, which is the default setting in Wan2.1. In noise shuffling, we set the window size to = 4, meaning that shuffling occurs among 4 frames. For the semantic sparse KV cache, due to the limitation of single-GPU memory, we use Top-l semantic retrieval with = 2."
        },
        {
            "title": "5.2 Main Results\nResults on LV-Bench. We first compare our method with several open-source long video generation baselines on\nLV-Bench, including MAGI-1 [33], Self Forcing [19], PAVDM [41], FramePack [46], and SkyReels-V2-DF-1.3B\n[5]. As shown in Table 2, our BlockVid-1.3B consistently outperforms these methods across most VDE metrics\nand complementary metrics from VBench. In particular, BlockVid achieves the lowest error scores on all five\nVDE metrics, reducing subject drift, background inconsistency, motion degradation, and perceptual losses\ncompared to strong baselines such as SkyReels-V2-DF-1.3B. On complementary VBench metrics, BlockVid\nalso delivers the highest subject consistency (0.9597) and background consistency (0.9588), as well as superior\nmotion smoothness (0.9956). Although BlockVid does not achieve the best score on aesthetic quality, it\nmaintains competitive performance in this dimension while delivering state-of-the-art results overall across\nboth VDE and VBench consistency metrics. These results demonstrate that our method not only improves\nlong-term coherence but also balances fidelity and aesthetics in long video generation.",
            "content": "Results on VBench. We further compare our method with state-of-the-art baselines on VBench [21] under the single-shot long video generation setting [13, 3]. As shown in Table 3, BlockVid-1.3B achieves superior performance across the majority of metrics, surpassing both open-source and large-scale proprietary baselines. Specifically, BlockVid achieves the highest scores in subject consistency (0.9410), motion smoothness (0.9870), dynamic degree (0.7720), aesthetic quality (0.5839), and image quality (0.6527), demonstrating its ability to generate temporally coherent, visually appealing, and semantically dynamic long videos. While MoC slightly outperforms BlockVid in background consistency (0.9670 vs. 0.9650), our model delivers the most balanced overall performance. These results highlight the effectiveness of BlockVid in both temporal stability and perceptual quality in long video generation."
        },
        {
            "title": "5.3 Ablation Study\nWe further conduct ablation studies from four perspectives: noise scheduling, KV cache settings, Block Forcing,\nand post-training datasets, as detailed below.",
            "content": "9 Noise scheduling. As shown in Table 4, during post-training, the cosine noise schedule achieves the best overall performance compared to naive or alternative scheduling strategies. During inference, noise shuffle with window size of = 4 further enhances temporal smoothness across chunk boundaries, leading to the most stable and coherent long video generation. Table 4 Ablation on noise schedule. Method Naive Linear Cosine Sigmoid No Shuffle s=2 s= VDE Subject VDE Background VDE Motion VDE Aesthetic VDE Clarity 0.0936 0. 0.0844 0.0961 0.0902 0.0853 0.2894 0.3015 0.2945 0.4027 0.3007 0.2995 0.2311 0.0167 0.0119 0. 0.0281 0.0138 0.9643 0.8910 0.9618 0.9723 0.9635 0.9730 0.0844 0. 0.0119 0.9618 0.7791 0.7610 0.7551 0.8247 0.7580 0.7492 0. KV cache. We further explore rolling KV [19], dynamic sparse KV [16], and our semantic sparse KV under different attention thresholds τ . As shown in Table 5, our semantic sparse KV cache with τ = 0.98 achieves the best overall performance, consistently reducing subject, background, motion, aesthetic, and clarity errors compared to baselines. Table 5 Ablation on KV cache settings."
        },
        {
            "title": "Method",
            "content": "VDE Subject VDE Background VDE Motion VDE Aesthetic VDE Clarity Rolling KV Dynamic Sparse KV (τ = 0.97) Dynamic Sparse KV (τ = 0.98) Semantic Sparse KV (τ = 0.97) Semantic Sparse KV (τ = 0.98) 0.0961 0.0927 0.0910 0.0869 0.3519 0.3074 0.3040 0.2988 0.0547 0.0253 0.0239 0.0153 0.9815 0.9781 0.9716 0.9684 0.7913 0.7730 0.7652 0.7570 0. 0.2945 0.0119 0.9618 0.7551 Block Forcing. Combining Self Forcing [19] and Block Forcing achieves the lowest errors across all VDE metrics, as shown in Table 6. Table 6 Ablation on Block Forcing. Method VDE Subject VDE Background VDE Motion VDE Aesthetic VDE Clarity Naive Self Forcing Velocity Forcing 0.0910 0.0885 0.0861 0.3317 0.3155 0.3015 0.0259 0.0169 0.0137 0.9810 0.9658 0.9673 0.7835 0.7630 0. Ours 0.0844 0.2945 0.0119 0.9618 0. Post-training datasets. As shown in Table 7, Stage 2 training on LV-Bench provides significantly greater improvements than Stage 1 training on LV-1.1M, as long videos (50s) offer crucial extrapolation benefits for minute-long generation. Furthermore, our multi-stage post-training proves essential for achieving the best overall performance. 10 Table 7 Ablation on post-training datasets."
        },
        {
            "title": "Method",
            "content": "VDE Subject VDE Background VDE Motion VDE Aesthetic VDE Clarity Stage 1 only Stage 2 only 0.8891 0.1752 1.1573 0.4722 0.0491 0.0153 1.3742 0.9946 1.2463 0.8452 Stage 1 + 0.0844 0.2945 0.0119 0.9618 0."
        },
        {
            "title": "6 Limitation and Future Work",
            "content": "While our framework performs well in single-shot long video generation, broader settings such as multi-shot composition remain to be explored, particularly regarding coherence across scene transitions. As future work, we aim to study these cases and consider extensions such as larger LV-Bench and 3D-aware modeling to further assess and broaden the methods applicability."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce BlockVid, an effective block diffusion framework for minute-long video generation. The design integrates three key innovations to tackle the chunk-wise accumulation error: semantic sparse KV cache that selectively retrieves salient context to mitigate error accumulation, an advanced training strategy that combines Block Forcing and Self Forcing to reduce temporal drift and close the traininginference gap, and chunk-aware noise scheduling and shuffling scheme that stabilizes long-horizon generation. Together, these components enable BlockVid to significantly improve long-range temporal coherence while maintaining high visual fidelity. To address the absence of suitable evaluation resources, we further propose LV-Bench, fine-grained benchmark of 1,000 minute-long videos with detailed chunk-level annotations. Alongside, the proposed Video Drift Error (VDE) metrics directly quantify coherence degradation over time. Extensive experiments on LV-Bench and VBench demonstrate that BlockVid achieves state-of-the-art performance, outperforming prior open-source and proprietary baselines across both coherence-aware and perceptual quality metrics."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. [2] Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. [3] Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. [4] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. In The Thirteenth International Conference on Learning Representations. [5] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [8] PySceneDetect Contributors. Pyscenedetect. https://www.scenedetect.com. [9] Karan Dalal, Daniel Koceja, Jiarui Xu, Yue Zhao, Shihao Han, Ka Chun Cheung, Jan Kautz, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1770217711, 2025. [10] Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, and Fabrice Rossi. Mean absolute percentage error for regression models. Neurocomputing, 192:3848, 2016. [11] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, and Jun Xiao. Vid-gpt: Introducing gpt-style autoregressive generation in video diffusion models. arXiv preprint arXiv:2406.10981, 2024. [12] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. [13] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. [14] Wenkang Han, Wang Lin, Yiyun Zhou, Qi Liu, Shulei Wang, Chang Yao, and Jingyuan Chen. Show and polish: reference-guided identity preservation in face video restoration. arXiv preprint arXiv:2507.10293, 2025. [15] Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022. [16] Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Zipvl: Efficient large vision-language models with dynamic token sparsification. In ICCV, 2025. [17] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. IEEE transactions on pattern analysis and machine intelligence, 43(5):1562 1577, 2019. 12 [18] Tianchi Huang, Chao Zhou, Xin Yao, Rui-Xiao Zhang, Chenglei Wu, Bing Yu, and Lifeng Sun. Qualityaware neural adaptive video streaming with lifelong imitation learning. IEEE Journal on Selected Areas in Communications, 38(10):23242342, 2020. [19] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. [20] Yuanhui Huang, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Di Zhang, Jie Zhou, and Jiwen Lu. Owl-1: Omni world model for consistent long video generation. arXiv preprint arXiv:2412.09600, 2024. [21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [22] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. Gear: An efficient error reduction framework for kv cache compression in llm inference. In NeurIPS Efficient Natural Language and Speech Processing Workshop, pages 305321. PMLR, 2024. [23] Sungil Kim and Heeyoung Kim. new metric of absolute percentage error for intermittent demand forecasts. International Journal of Forecasting, 32(3):669679, 2016. [24] Zhuoling Li, Hossein Rahmani, Qiuhong Ke, and Jun Liu. Longdiff: Training-free long video generation in one go. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1778917798, 2025. [25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [26] Akide Liu, Zeyu Zhang, Zhexin Li, Xuehai Bai, Yizeng Han, Jiasheng Tang, Yuanjie Xing, Jichao Wu, Mingyang Yang, Weihua Chen, et al. Fpsattention: Training-aware fp8 and sparsity co-design for fast video diffusion. arXiv preprint arXiv:2506.04648, 2025. [27] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. Advances in Neural Information Processing Systems, 37:131434131455, 2024. [28] Jiabin Luo, Junhui Lin, Zeyu Zhang, Biao Wu, Meng Fang, Ling Chen, and Hao Tang. Univid: The open-source unified video model. arXiv preprint arXiv:2509.24200, 2025. [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [30] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. [31] Jingwei Shi, Zeyu Zhang, Biao Wu, Yanjie Liang, Meng Fang, Ling Chen, and Yang Zhao. Presentagent: Multimodal agent for presentation video generation. arXiv preprint arXiv:2507.04036, 2025. [32] Peize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai, Kris Kitani, and Ping Luo. Dancetrack: Multi-object tracking in uniform appearance and diverse motion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2099321002, 2022. [33] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. [34] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [35] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, et al. Lingen: Towards high-resolution minute-length text-to-video generation with linear computational complexity. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25782588, 2025. [36] Weijie Wang, Jiagang Zhu, Zeyu Zhang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Haoxiao Wang, Guan Huang, Xinze Chen, et al. Drivegen3d: Boosting feed-forward driving scene generation with efficient video diffusion. arXiv preprint arXiv:2510.15264, 2025. [37] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. [38] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, and Weisi Lin. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. Equal Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi. [39] Weijia Wu, Mingyu Liu, Zeyu Zhu, Xi Xia, Haoen Feng, Wen Wang, Kevin Qinghong Lin, Chunhua Shen, and Mike Zheng Shou. Moviebench: hierarchical movie level dataset for long video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2898428994, 2025. [40] Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinema: Towards short movie generation. arXiv preprint arXiv:2507.18634, 2025. [41] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 63226332, 2025. [42] Qi Xie, Yongjia Ma, Donglin Di, Xuehao Gao, and Xun Yang. Moca: Identity-preserving text-to-video generation via mixture of cross attention. arXiv preprint arXiv:2508.03034, 2025. [43] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50365045, 2022. [44] Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong Tian, Enze Xie, and Daquan Zhou. Magic 1-for-1: Generating one minute video clips within one minute. arXiv preprint arXiv:2502.07701, 2025. [45] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. 2025. [46] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. [47] Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. [48] Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025."
        },
        {
            "title": "A Appendix",
            "content": "A.1 LLM Use Declaration Large Language Models (ChatGPT) were used exclusively to improve the clarity and fluency of English writing. They were not involved in research ideation, experimental design, data analysis, or interpretation. The authors take full responsibility for all content. A.2 Algorithm: Semantic Sparse KV Cache Algorithm 1 Semantic Sparse KV Cache 1: Input: chunks {Xi}N , prompts {Yi}N i=1 2: Output: final KV cache (K, V) for Xt 3: KV_BANK i=1 , target t=N , threshold τ , top-K, drop pdrop // dict: (cid:55) (K(i) sparse, V(i) sparse) // Stage A: Build and store sparse KV for all prior chunks 4: for {1, . . . , 1} do 5: if / KV_BANK then sparse, V(c) (K(c) KV_BANK[c] (K(c) 6: 7: sparse) BuildSparseKV(Xc, Yc, τ ) sparse, V(c) sparse) end if 8: 9: end for // Stage B: Retrieve Top-K semantic from bank (no recompute) Ei T5-Embed(Yi); simi cos(Et, Ei) 10: seq_ctx {N 3, 2} (if available) 11: Et MeanEmbed(Yt) 12: {1, . . . , 1} seq_ctx 13: for do 14: 15: end for 16: Top-l-Idx argsort({simi}iS )[l :] 17: (Kseq, Vseq) ConcatKV(cid:0){KV_BANK[j] : seq_ctx}(cid:1) 18: (Ksem, Vsem) ConcatKV(cid:0){KV_BANK[i] : TopKIdx}(cid:1) // Stage C: Final merge (seq_ctx + Top-l semantic) & token drop 19: (K, V) ConcatKV(cid:0)(Kseq, Vseq), (Ksem, Vsem)(cid:1) 20: return (K, V) A.3 Prompts for LV-Benchs Data Engine Role. Act as professional video content analyst. Describe given video frame in English. Context. The previous frame was described as: \"{previous_description}\". Use this as context to ensure temporal coherence. Instruction. Write single, descriptive paragraph that: Identifies the main subject, their specific actions, and expressions. Describes the environment and background, including setting and lighting. Highlights the cinematic quality, such as composition, color palette, and atmosphere (e.g., tense, serene, spectacular). Constraints. Output must be one coherent paragraph, written in natural language prose, without bullet points or numbered lists. Return. The paragraph description of the current frame. 15 Algorithm 2 BuildSparseKV: Dynamic Sparse KV Cache 1: function BuildSparseKV(X, Y, τ ) 2: 3: 4: Encode(X, Y) Q, K, Project(H); q_len length(Q) if q_len > 1 then (Q, K) RoPE(Q, K) // model input states // prefill stage (identify salient keys) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: (cid:16) QprobeK Iprobe Concat(Recent(64), Random(64, range=[0, q_len64))) Qprobe Q[:, Iprobe, :] Softmax (cid:80) heads,probe CumMean(s) CoverCount(m, τ ) Ikeep Top-K(m, ) return + CausalMask(Iprobe, q_len) (cid:0)K[:, Ikeep, :], [:, Ikeep, :](cid:1) (cid:17) // aggregate over heads and probe queries // cumulative mean (older tokens discounted) // smallest covering τ (cid:80) else return (K, ) // decode stage: keep all end if 16: 17: end function A.4 LV-Bench Metrics A.4.1 Preliminaries: Mean Absolute Percentage Error Mean Absolute Percentage Error (MAPE) and Weighted Mean Absolute Percentage Error (WMAPE) are widely adopted evaluation metrics in forecasting [23], time series analysis [10], and increasingly in video quality assessment tasks [18]. MAPE measures the average relative deviation between predicted values ˆyi and ground-truth values yi, expressed as percentage: MAPE = 100 (cid:88) i=1 (cid:12) (cid:12) (cid:12) (cid:12) yi ˆyi yi (cid:12) (cid:12) (cid:12) (cid:12) . (14) Although simple and interpretable, MAPE can be biased when actual values yi are close to zero. To address this issue, WMAPE normalizes the absolute error by the sum of actual values, making the metric scale-invariant and more robust in practice: WMAPE = (cid:80)N i=1 yi ˆyi (cid:80)N i=1 yi . (15) These metrics provide interpretable percentage-based measures of consistency and prediction accuracy, and can be directly applied to quantify deviations across frames or segments in video tasks [18]. A.4.2 Video Drift Error (VDE) Inspired by the WMAPE [23, 10], we propose new metric called Video Drift Error (VDE) to measure changes in video quality. The core idea involves dividing long video into multiple smaller segments, each evaluated according to specific quality metrics (such as clarity, motion smoothness, etc). These scores are then used to calculate the relative change compared to the first segment. For long video generation, small quality deviations may accumulate within each short time segment. Over time, these deviations gradually build up [24, 27]. This accumulation error can be quantified and detected through VDE. Specifically, high VDE value indicates significant fluctuations or degradation in video quality as playback progresses, while low VDE value suggests consistent quality levels throughout. Similar drift penalties have been introduced in works such as IP-FVR [14], which focuses on preserving identity consistency, and MoCA [42], which employs an identity perceptual loss to penalize frame-to-frame identity drift. Therefore, monitoring VDE during long-term video generation helps identify potential quality degradation trends and allows timely corrective actions to be taken. Specifically, the method first divides the video into smaller segments of equal duration: = {S1, S2, . . . , SN }, where is the full video, and Si represents the i-th segment. Then the method evaluate each segment by applying quality evaluation function (e.g., metric_function) to compute score Qi for each segment Si: Qi = metric_function(Si), {1, 2, . . . , }. (16) Furthermore, the method compute rate of change which calculates the relative change in quality scores from the first segment (Q1) for all subsequent segments (i 2): = Qi Q1 Q1 . (17) The final VDE value is derived as weighted sum of absolute rate changes, using linear or logarithmic weights wi: (cid:88) VDE = wi i. (18) i=2 A.4.3 VDE Metrics Metric-specific VDEs. Given the VDE shell defined in the preliminaries (reference chunk S1, per-chunk scores mi, and weights wi), each metric instantiates mi as follows; the VDE value is then VDE() = (cid:88) i=2 wi mi m1 m1 , wi (cid:8) + 1, log(N + 1) (cid:9). (19) It evaluates temporal drift in image sharpness (defocus/blur). For long videos, creeping VDE Clarity (). blur or inconsistent deblurring raises VDEclar, while low value indicates stable perceived clarity over time. Let ft Si be frames and Yt their luminance. Define per-frame sharpness by Laplacian variance and average within the chunk: mclar = 1 Si (cid:88) tSi Var(cid:0)2Yt (cid:1), VDEclar = (cid:88) i=2 wi mclar mclar mclar 1 1 . (20) It tracks drift in motion magnitude/smoothness (pace and jitter). Long-sequence generators VDE Motion (). often change kinetic behavior over time; low VDEmot signals consistent dynamics without late-stage jitter or freezing. Let ut denote the optical flow between consecutive frames, and define the per-frame motion energy as E(ut) = ut2. Alternatively, one may compute motion-smoothness score st based on inter-frame differences. The chunk-level score is then mmot = 1 Si 1 (cid:88) tSi E(ut) or mmot = 1 Si (cid:88) tSi st, and the final penalty is VDEmot = (cid:88) i= wi mmot mmot mmot 1 1 . (21) (22) It measures drift in global visual appeal (composition, color harmony, lighting). In long VDE Aesthetic (). videos, style can drift or collapse; low VDEaes indicates sustained, coherent aesthetics along the timeline. Let A(ft) be learned aesthetic predictor applied per frame; average within each chunk: maes = 1 Si (cid:88) tSi A(ft), VDEaes = (cid:88) i=2 wi maes maes 1 maes 1 . (23) 17 It evaluates stability/consistency of the background (camera drift, flicker, texture VDE Background (). boil). Long videos often accumulate spurious background motion; low VDEbg reflects stable setting that does not melt over time. Let Bt be background mask and ut(x) the flow at pixel x. Define per-frame background staticness ϕt = 1 Bt 1(cid:0)ut(x) τ (cid:1) and average per chunk: xBt (cid:80) mbg = 1 Si (cid:88) tSi ϕt, VDEbg = (cid:88) i=2 wi mbg mbg 1 mbg . (24) It captures drift in subject identity/attributes (face morphing, color/outfit changes). VDE Subject (). For long generations, identity can subtly shift; low VDEsubj indicates the protagonist remains recognizably consistent throughout. Let E() be subject-identity encoder and e1 the mean embedding over subject crops in S1. Define per-frame identity similarity st = cos(cid:0)E(crop (cid:1) and average within the chunk: t), msubj = 1 Si (cid:88) tSi st, VDEsubj = (cid:88) i=2 wi msubj msubj msubj 1 . (25) A.4.4 Complementary Metrics Following previous minute-long generation works [13, 3], we additionally include five complementary metrics from VBench [21] that are essential for evaluating long video generation, including: (1) Imaging Quality, which measures the technical fidelity of each video frame by quantifying distortions (e.g., over-exposure, noise, blur), thus reflecting the clarity and integrity of the generated imagery. (2) Motion Smoothness, which assesses the fluidity and realism of movements in the video, ensuring that frame-to-frame transitions are continuous and physically plausible to achieve natural motion. (3) Aesthetic Quality, which evaluates the visual appeal of the video frames, capturing artistic factors like composition, color harmony, photorealism, and overall beauty as perceived in each frame. (4) Background Consistency, which measures the stability of the scenes background across the video, determining whether the backdrop remains visually consistent throughout all frames. (5) Subject Consistency, which evaluates whether subjects appearance remains consistent across every frame of the video, capturing the temporal coherence of that subjects visual identity over the entire sequence. A.5 Other Noise Schedules For example, simple linear schedule with nonzero initial noise level is ϵc = ϵmin + 1 (cid:0)ϵmax ϵmin (cid:1), = 1, 2, . . . , n, ϵmin > 0, (26) so that the first chunk has ϵ0 = ϵmin (nonzero initial noise) and the last has ϵn1 = ϵmax (maximal noise). Similarly, sigmoid (logistic) schedule grows slowly at the beginning and end, with faster change in the middle: ϵc = ϵmin + (cid:0)ϵmax ϵmin (cid:1) (cid:16) 1 α(cid:0) n1 0.5(cid:1)(cid:17) , 1 + exp = 1, 2, . . . , n, (27) where α > 0 controls the steepness of the curve transition (larger α sharper transition). 18 A.6 LV-1.1M Dataset To improve post-training data for semi-AR models, we introduce private dataset named LV-1.1M, which contains fine-grained annotations for each video. We first collect videos from publicly datasets: Panda 70M [7] and HD-VILA-100M [43], and private data (about 1M). These raw data often contain substantial amounts of noisy and low-quality material, lacking in careful curation for content quality and caption coherence. Thus, we devise several filtering criteria to select high-quality, large-motion , and long-take videos. We leverage PySceneDetect [8] to detect scene transitions and employ Q-Align [38] to remove videos with low aesthetics scores. We also use optical flow as clue to filter out static videos with little motion dynamics. The optical flow is calculated between each pair of neighboring frames sampled at 2 fps and discard the videos with low average optical flow score. Finally, we collect 1.1M high-quality long-take videos. To caption them, we segment each long-take video into multiple chunks. The number of frames in each chunk is determined by the maximum input capacity of the corresponding foundational model (for example, Wan2.1 [34] allows up to 81 frames). Keyframes are extracted from each chunk and processed through GPT-4o, utilizing prompt engineering to generate captions for each individual chunk. Subsequently, GPT-4o is employed again to align all chunk-level captions, ensuring coherent storyline throughout the entire video. A.7 Visualization Comparison The full prompts of the Figure 2 are as follows: \"captions\": [ \"A serene white swan glides across misty lake, its reflection shimmering in the calm water (00s - 03s).\", \"The swan dips its head gracefully into the water, creating gentle ripples around it (04s - 07s).\", \"Lifting its head, the swan shakes off droplets, sending small splashes into the air (08s - 11s).\", \"It spreads its wings slightly, flapping them to create splash and adjust its position (12s - 15s).\", \"The swan turns slightly, continuing to glide smoothly as mist hovers over the water (16s - 19s).\", \"With elegant movements, the swan swims forward, its long neck curved gracefully (20s - 23s).\", \"The swan pauses briefly, surveying its surroundings with poised demeanor (24s - 27s).\", \"It resumes swimming, its feathers catching the soft light filtering through the mist (28s - 31s).\", \"Dipping its beak again, the swan appears to forage or drink from the tranquil waters (32s - 35s).\", \"The swan lifts its head once more, shaking off water with delicate motion (36s - 39s).\", \"Turning its body, the swan reveals its full profile against the backdrop of foggy greenery (40s - 43s).\", \"It continues its graceful journey, leaving trail of ripples behind (44s - 47s).\", \"The swans reflection mirrors its every move, enhancing the peaceful ambiance (48s - 51s).\", \"As it drifts further away, the swan becomes part of the misty landscape (52s - 55s).\", \"The swan slows down, almost still, embodying tranquility on the quiet lake (56s - 59s).\"] As shown in Figure 2, all five baselines exhibit varying degrees of severe accumulation errors when generating minute-long videos. MAGI-I [33], Self-Forcing [19], and PAVDM [41] suffer from significant image quality degradation and color distortion after around 12 seconds, with the video gradually deteriorating and eventually collapsing. FramePack [46], on the other hand, avoids severe image distortion but produces poor dynamics and limited content diversity due to its symmetric progression design. SkyReel-V2 [5] is the closest baseline in comparison, yet it still experiences noticeable color drift after 12 seconds, which continues to accumulate until the final chunk. In contrast, our method outperforms all of these approaches, maintaining subject and background consistency, preserving image quality, and preventing color degradation. 19 A.8 Visualization Results Figure 4 More visualization results #1. \"captions\": [ \"A DJ in vibrant Nigerian attire stands behind mixing console, adjusting knobs with focused precision (00s - 03s).\", \"He glances at the audio waveforms on his monitor, syncing his movements to the rhythm of the track (04s - 07s).\", \"With smooth hand gestures, he manipulates the turntables, blending beats seamlessly in the studio (08s - 11s).\", \"The DJ nods along to the music, fully immersed as he fine-tunes levels and effects (12s - 15s).\", \"His reflection is visible in the glass window as he dances subtly while mixing (16s - 19s).\", \"He lifts one hand in the air, hyping the unseen audience as the bass drops (20s - 23s).\", \"Smiling broadly, he spins the jog wheel with flair, showcasing his technical skill (24s - 27s).\", \"He raises both arms triumphantly, feeding off the energy of the music hes creating (28s - 31s).\", \"Leaning into the mic, he speaks or chants rhythmically, engaging listeners through the airwaves (32s - 35s).\", \"He throws his hands up again, eyes closed, lost in the groove hes crafted (36s - 39s).\", \"Adjusting headphones around his neck, he continues to tweak controls with rhythmic precision (40s - 43s).\", \"He gestures toward the camera with confident smile, radiating charisma and passion (44s - 47s).\", \"Moving fluidly between decks, he layers sounds with expert timing and flair (48s - 51s).\", \"He laughs joyfully, clearly enjoying every moment as he commands the radio stations sound (52s - 55s).\", \"Finishing his set with final flourish, he waves to the crowd, leaving the studio buzzing with energy (56s - 59s).\"] 20 Figure 5 More visualization results #2. \"captions\": [ \"The camera glides through dimly lit Victorian hallway, revealing wood paneling, arched ceilings, and warm ambient lighting (00s - 03s).\", \"Classical statues and sconces line the corridor as the view advances through carved archways (04s - 07s).\", \"Past gilded wall art and columns, the camera approaches grand entryway lit by chandelier (08s - 11s).\", \"Inside lavish sitting room, antique furniture and portraits glow under lighting (12s - 15s).\", \"The camera pans across dark paneled walls with vintage posters and sculptures, highlighting the curated elegance (16s - 19s).\", \"A solitary armchair beneath chandelier, flanked by side tables and art pieces, evokes Victorian comfort (20s - 23s).\", \"Rotating slowly, the camera reveals symmetrical decor matching lamps, portraits, and ceiling beams (24s - 27s).\", \"Pulling back, the view widens to two-story foyer with sweeping balcony and dramatic lighting (28s - 31s).\", \"Double doors open to adjacent rooms, while rugs and polished floors reflect the glow (32s - 35s).\", \"The camera ascends to capture the foyers verticality with balconies, lanterns, and sculptural accents (36s - 39s).\", \"Blue accent lighting outlines pillars and doors, contrasting with the warm tones (40s - 43s).\", \"Through the grand archway, the viewer is drawn toward luminous sitting area framed by columns (44s - 47s).\", \"From mid-hall, the layered depth and symmetry of the mansion interior are revealed (48s - 51s).\", \"Pulling back further, soaring ceilings and ornate woodwork are shown in interplay of shadow and glow (52s - 55s).\", \"The final shot retreats outdoors, framing the mansions facade at twilight, glowing windows welcoming the viewer (56s - 59s).\" ] 21 Figure 6 More visualization results #3. \"captions\": [ \"Three friends sit around makeshift table in dimly lit auto shop, clinking beer bottles in cheerful toast (00s - 03s).\", \"They laugh as they settle back into their tire seats, enjoying the camaraderie and casual atmosphere (04s - 07s).\", \"The man on the left raises his bottle in playful gesture, sharing joke that sends everyone into laughter (08s - 11s).\", \"He animatedly tells story, gesturing with his bottle while his friends react with amusement (12s - 15s).\", \"Leaning forward with grin, he holds up his bottle triumphantly as if making point or celebrating (16s - 19s).\", \"He lowers his bottle, chuckling, clearly enjoying the moment and the company of his friends (20s - 23s).\", \"Still smiling, he glances at his buddies, who are equally entertained as the vibe fills the garage (24s - 27s).\", \"One friend takes sip while the other leans back, laughing at the ongoing banter (28s - 31s).\", \"The groups laughter grows louder as the man in the middle throws his head back in delight (32s - 35s).\", \"The man on the left leans in again, speaking animatedly as his friends listen with attention (36s - 39s).\", \"He gestures with his bottle, emphasizing his point, while the others nod and smile (40s - 43s).\", \"He extends his arm to offer bottle to his friend, sparking more laughter (44s - 47s).\", \"The friends continue to enjoy each others company, carefree amid the cluttered garage (48s - 51s).\", \"A fourth friend enters, joining the laughter as the camera pans to capture the group dynamic (52s - 55s).\", \"All four men share in the joyous moment, seated among tools and car parts, embodying friendship (56s - 59s).\"] 22 Figure 7 More visualization results #4. \"captions\": [ \"A young woman with long hair sits at glowing bar, bathed in neon light, holding glass of beer thoughtfully (00s - 03s).\", \"She lifts her gaze, then brings the glass to her lips for slow sip, the vibrant lighting highlighting her contemplative mood (04s - 07s).\", \"After sipping, she lowers the glass and glances around, her reflection visible in the mirror behind the bar (08s - 11s).\", \"She takes another drink while watching her reflection, neon hues shifting across her face and surroundings (12s - 15s).\", \"Lowering her glass again, she looks off to the side with pensive expression as the glow ripples over the scene (16s - 19s).\", \"The camera pans slightly left, revealing glowing bottles, mirrored surfaces, and flickering colored lights (20s - 23s).\", \"She takes one more sip as the background comes alive with movement another woman dances subtly, silhouetted against the bar (24s - 27s).\", \"The camera sweeps further left, showing the bustling bar environment filled with patrons under dynamic neon strips (28s - 31s).\", man leans in from the side, speaking as they exchange words under pulsating lights (32s35s)., \"He turns away, gesturing toward the bar as other guests laugh and chat nearby (36s - 39s).\", \"The camera shifts to bartender engaging with customers while visuals flash on screen behind him (40s - 43s).\", \"He gestures animatedly, his movements synced with the rhythm of the music and lights (44s - 47s).\", Customers smile and clink glasses as the neon-lit bar pulses with energy (48s - 51s)., \"The bartender continues his lively interaction, surrounded by the buzz of conversation and glowing lights (52s - 55s).\", \"The scene ends with wide view of the bar people mingling, laughing, drinking all in neon-drenched party atmosphere (56s - 59s).\"] 23 Figure 8 More visualization results #5. \"captions\": [ \"A sleek black BMW M4 with SCHUBERT decals idles under streetlights on wet European night, its headlights piercing the darkness (00s - 03s).\", \"The camera glides closer, revealing the cars aggressive front grille and glowing LED headlights reflecting off the rain-slicked cobblestones (04s - 07s).\", \"The license plate EM EP99RT comes into focus as the car remains stationary, exuding power and elegance against the backdrop of historic buildings (08s - 11s).\", \"The camera pulls back slightly, capturing the full front view of the BMW as it sits poised in the center of the glistening street (12s - 15s).\", \"The scene widens to show the car framed by grand architecture, with ambient lighting enhancing its glossy finish and sharp lines (16s - 19s).\", \"A low-angle shot emphasizes the cars stance, with reflections dancing across the wet pavement (20s - 23s).\", \"The camera moves to the side, showcasing the BMWs muscular profile and intricate alloy wheels (24s - 27s).\", \"As the camera sweeps along the flank, the wet street mirrors the cars silhouette and nearby street lamps (28s - 31s).\", \"The shot lingers on the rear three-quarter view, capturing the interplay of light and reflection on its polished surface (32s - 35s).\", \"The camera drifts lower, focusing on shimmering puddle that reflects the car and ornate building behind it (36s - 39s).\", \"The reflection becomes the focal point, blending the cars image with the glowing façade of the architecture (40s - 43s).\", \"The camera glides over the reflective surface, emphasizing the serene yet powerful atmosphere of the rainy night (44s - 47s).\", \"The final frames capture rippling reflections, evoking calm and sophistication as the BMW remains motionless in its urban sanctuary (48s - 52s).\"] Figure 9 More visualization results #6. \"captions\": [ \"A close-up shows sleek white water bottle on black stationary bike handlebar, with blurred gym equipment behind (00s03s).\", \"The camera tilts down, framing the bottle and MagSafe tablet mount (04s07s).\", \"Pulling back, the shot reveals more of the bikes white-and-black console and handlebars, with rows of bikes in the backdrop (08s11s).\", \"The digital display flickers to life with workout metrics as the camera pans left, revealing more bikes in soft focus (12s15s).\", \"Continuing the pan, the frame captures rhythmic rows of bikes and lit screens, emphasizing symmetry and tech (16s19s).\", \"The shot shifts right to focus on ergonomic handlebars and seats, as natural light streams through large windows (20s23s).\", \"Moving forward, the camera glides past multiple bikes, showing clean lines and minimalist design in polished wooden-floored gym (24s27s).\", \"Zooming out, the row of bikes extends into the distance, reinforcing the quiet, orderly, high-tech setting (28s31s).\", \"The sweep continues, capturing reflections on glossy floors and uniform digital displays (32s35s).\", \"A pan across the gym reveals more machines, including recumbent bikes, aligned under bright window-lit walls (36s39s).\", \"Shifting focus, the camera moves to the rear, showing more black-and-white cardio equipment in natural light (40s43s).\", \"The perspective shifts to highlight the depth of the space, with machines stretching toward windows, evoking openness and calm (44s47s).\", \"Gliding along recumbent bike, the camera emphasizes its sleek form and monitor, framed by sunlit glass panels (48s51s).\", \"Continuing smoothly, the frame reveals the gyms layout neat rows, reflective floors, and daylight enhancing serenity (52s55s).\", 25 \"A final pan shows the full expanse of the modern studio, empty yet radiating tranquility and precision (56s59s).\" ]"
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "ZIP Lab, Zhejiang University"
    ]
}