{
    "paper_title": "Towards Universal Soccer Video Understanding",
    "authors": [
        "Jiayuan Rao",
        "Haoning Wu",
        "Hao Jiang",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research."
        },
        {
            "title": "Start",
            "content": "Jiayuan Rao1,2, Haoning Wu1,2, Hao Jiang3, Ya Zhang1, Yanfeng Wang1, Weidi Xie1 1School of Artificial Intelligence, Shanghai Jiao Tong University, China 3Alibaba Group, China 2CMIC, Shanghai Jiao Tong University, China https://jyrao.github.io/UniSoccer/ 4 2 0 2 4 ] . [ 2 0 2 8 1 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop comprehensive multi-modal framework for soccer video understanding. Specifically, we make (i) we introthe following contributions in this paper: duce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer standard paradigm for sports understanding research. Football is one of the worlds best means of communication. It is impartial, apolitical, and universal. Franz Beckenbauer (1945 - 2024) 1. Introduction Soccer, celebrated worldwide for its significant commercial value, has recently seen great research interest in integrating artificial intelligence (AI) for soccer video understanding. This is primarily motivated by the sports complexity and the growing demand for enhanced analytics and improved viewing experiences. AI systems facilitate tactical analysis [50], allowing coaches to devise better strategies by uncovering patterns not apparent to the naked eye. In addition, it also supports automated content generation and enriches *: These authors contribute equally to this work. : Corresponding author. Figure 1. Overview. We present the largest soccer dataset to date, termed SoccerReplay-1988, and the first vision-language foundation model for soccer, MatchVision, capable of various tasks such as event classification and commentary generation. fan engagement through interactive and personalized content [34, 36, 40]. These capabilities promote deeper understanding of soccer, simplify content creation, and foster more engaging experience for fans and professionals. The existing literature on soccer video analysis predominantly centers on the SoccerNet series datasets [7, 9, 14], which collectively comprise 500 full-match videos for benchmarking variety of tasks, including event classification [9, 14], commentary generation [34, 36, 40], etc. Despite this extensive coverage, the focus has primarily been on designing specialized models, tailored to narrow, specific tasks, leading to significant gap in compatibility among models. Such fragmentation underscores the need for unified analytical framework that can seamlessly integrate the diverse demands of soccer video understanding, promoting more holistic and scalable solutions. In this paper, we introduce SoccerReplay-1988, the largest and most comprehensive multi-modal soccer video dataset to date, featuring 1,988 complete match videos with rich annotations, such as event labels and textual commentaries. This dataset serves as solid foundation for developing advanced models in the soccer domain and establishes challenging new benchmark for evaluating soccer understanding models. In addition, we have harmonized existing datasets to be compatible with ours, further expanding the data resources available to the field. Leveraging this dataset, we develop the first visuallanguage foundation model tailored for various soccer understanding tasks, termed MatchVision. It employs the cutting-edge visual-language foundation model as the backbone, e.g., SigLIP [57]. We further extend the framewise visual features to be spatiotemporal with temporal attentions [3], by training on diverse visual-language tasks on SoccerReplay-1988, as depicted in Figure 1. As result, MatchVision demonstrates strong adaptability across various tasks, including event classification and commentary generation, serving as universal and unified framework for comprehensive soccer video understanding. To summarize, we make the following contributions in this paper: (i) we construct SoccerReplay-1988, the largest and most diverse soccer video dataset to date, comprising videos of 1,988 soccer matches with rich annotations, supported by an automated curation pipeline. This provides solid data foundation for developing robust and comprehensive soccer understanding models; (ii) we present the first visual-language foundation model for soccer understanding, termed MatchVision, which effectively leverages the spatiotemporal information in soccer videos, and can adapt to various tasks such as event classification and commentary generation, serving as unified framework for soccer understanding; (iii) we establish more comprehensive and challenging benchmarks based on our dataset, enabling more professional evaluation of soccer understanding models; (iv) extensive experiments and ablation studies have demonstrated the superiority of our data and model across various downstream tasks, achieving state-of-the-art performance on both existing benchmarks and our newly established ones. We believe our work potentially offers viable paradigm for sports video understanding in the future. 2. Related Works Sports Understanding [44] is an evolving field that encompasses multiple research topics and integrates diverse data modalities, covering various tasks such as action recognition [9, 14, 15], commentary generation [34, 37, 40, 51, 56], athlete analysis [41, 54], tactical planning [50], sports health [39], and intelligent refereeing [21, 22]. Additionally, with the rapid development of multimodal large language models (MLLMs), recent efforts [26, 52, 53] have attempted to build more generalized frameworks to uniformly handle variety of sports understanding tasks. Visual-Language Models [1, 27, 28, 38, 57] have exhibited remarkable performance across extensive applications like classification, segmentation, image-text retrieval, and image captioning. Furthermore, efforts have also ventured into more challenging video understanding [29, 30, 58] tasks, such as temporal alignment [16, 31], dense captioning [5, 55, 60], and audio description [1719]. However, these efforts typically focus on general scenarios, limiting their adaptability to specific professional fields. Thus, to advance further into specialized fields such as soccer, this paper presents the first visual-language foundation model tailored for comprehensive soccer understanding. Soccer Game Analysis has primarily focused on tasks such as action spotting [14, 34], replay grounding [21, 59], commentary generation [34, 36, 40], player tracking [8, 47], state reconstruction [42], camera calibration [6, 8, 9] and foul recognition [21, 22], as facilitated by the SoccerNet [7, 9, 13, 14] series of datasets, with 500 full-match videos from 2015 to 2017. Unlike existing approaches that target designing specific models for distinct tasks, this paper aims to design unified multi-modal framework that leverages the spatiotemporal information within videos, to serve as foundation model for soccer video understanding. 3. SoccerReplay-1988 Dataset To establish solid foundation for soccer understanding, we have constructed the largest soccer dataset to date, termed as SoccerReplay-1988. Here, we first outline our data collection details and provide an overview of the dataset in Section 3.1; followed by elaborating on our automated data curation pipeline in Section 3.2; lastly, in Section 3.3, we present the data statistics and discussion. 3.1. Dataset Collection To construct the SoccerReplay-1988 dataset, we retrieve and collect untrimmed, full-match videos from the Internet, encompassing total of 1,988 matches from six European major soccer leagues and championships1, spanning the 2014-15 to 2023-24 seasons. For each match, we acquire textual commentaries with second-level timestamps from sports text live website2, with part of them annotated with specific event types such as corner and goal. Additionally, we also incorporate extensive metadata about the matches, including detailed background information about the games, players, coaches, referees, and teams, providing solid foundation for future soccer understanding research. We divide the SoccerReplay-1988 dataset into train, validation, and test sets, containing 1,488, 250, and 250 fullmatch videos with diverse and comprehensive annotations, 1Premier (England), Laliga (Spain), Bundesliga (Germany), Serie-a (Italy), League-1 (France) and UEFA Champions League. 2flashscore.com Existing Datasets # Game Duration(h) # Event # Anno. # Com. SoccerNet-v1 [14] SoccerNet-v2 [9] MatchTime [40] GOAL [36] 500 500 471 20 764 764 716 25.5 7 17 14 - 6.7k 110k 14k - - - 37k 8.9k Our Curated Datasets # Game Duration(h) # Event # Anno. # Com. SoccerNet-pro SoccerReplay-1988 Integrated 500 1,988 2,488 764 3,323 4,087 24 24 102k 150k 252k 37k 150k 187k Table 1. Statistics of Soccer Datasets. Our SoccerReplay-1988 significantly surpasses existing datasets in both scale and diversity. Here, # Anno. and # Com. refer to the number of event annotations and textual commentaries, respectively. 1988, and replace any matched names in textual commentaries with standardized placeholders, such as [PLAYER], [TEAM], [COACH], and [REFEREE], to ensure consistency across the tasks. Moreover, our proposed data curation pipeline can seamlessly extend to existing datasets, converting the SoccerNet series [9, 34] into our unified data format, termed as SoccerNet-pro. This expansion further enlarges the standardized datasets available for soccer understanding tasks. 3.3. Statistics & Discussion Dataset Statistics. As presented in Table 1, our proposed dataset encompasses 3,323 hours of footage from 1,988 soccer matches, with an average duration of 100.3 minutes per match. The videos range in resolution from 360p to 720p and frame rates between 25 and 30 FPS. For textual annotations, this dataset features approximately 150K precisely temporal-aligned commentaries, averaging 76 per match. These commentaries cover 4,467 unique words, significantly surpassing the 2,873 words in existing datasets [34, 40], greatly enriching the textual diversity. Automated event summarization based on these commentaries has yielded about 150K event annotations. SoccerReplay-test Benchmark. To facilitate more comprehensive evaluation of soccer understanding models, we integrate 250 matches from SoccerReplay-1988 with 50 matches from the curated SoccerNet-pro, establishing more challenging benchmark, named SoccerReplay-test, for event classification and commentary generation. This benchmark features nearly four times larger than existing datasets and comprises finer-grained event labels, richer textual commentaries, and up-to-date soccer regulations. Discussion. To summarize, our proposed SoccerReplay1988 dataset demonstrates advancements in three aspects: (i) it is the largest soccer video dataset to date, with nearly four times more videos than existing datasets; (ii) it feaFigure 2. Automated Data Curation Pipeline. The collected soccer video data are automatically processed for temporal alignment, event summarization, and anonymization by our curation pipeline. respectively. These provide diverse training data for downstream tasks, such as event classification and commentary generation, and establish comprehensive and challenging benchmarks for soccer understanding, which will be further discussed in subsequent sections. 3.2. Automated Data Curation Given the potential noise in raw data, such as irrelevant video content, inaccurate timestamps, and incomplete event annotations, we design an automated data curation pipeline for curation, comprising (i) temporal alignment, (ii) event summarization, and (iii) anonymization, as in Figure 2. Temporal Alignment. Here, we divide the match videos into two halves, each starting at the kick-off moment, and adopt the publicly available temporal alignment model from MatchTime [40], to synchronize textual commentary timestamps with those of video frames. Event Summarization. For samples without event annotations, we leverage LLaMA-3-70B [11] to summarize the events based on textual commentaries. Concretely, we have expanded the event categories from 17 in SoccerNet [9] to 24 types, for finer-grained soccer understanding, for example, categorizing penalties into scored and missed, and integrating modern soccer regulations like VAR. The resulting 24 event labels include: corner, goal, injury, own goal, penalty, penalty missed, red card, second yellow card, substitution, start of game (half), end of game (half), yellow card, throw in, free kick, saved by goal-keeper, shot off target, clearance, lead to corner, off-side, var, foul (no card), statistics and summary, ball possession, and ball out of play. More details on the prompts can be found in Section B.1 of the Appendix. Anonymization. Similar to [34], we leverage all person and team entity names from the metadata of SoccerReplay3 tures more professional and diverse annotations, more suitable for fine-grained and comprehensive soccer understanding tasks; (iii) it employs an automated curation pipeline for annotations, thus is scalable to make solid data foundation for future research in soccer understanding. 4. Method In this paper, we aim to develop versatile visual-language foundation model, MatchVision, specifically tailored for analyzing diverse soccer video tasks. We start by outlining our problem formulation in Section 4.1. Next, in Section 4.2, we detail the architecture of MatchVision. The training procedures are thoroughly discussed in Section 4.3. Finally, we describe the configurations for our downstream tasks in Section 4.4, demonstrating the practical applications and effectiveness of our model. 4.1. Problem Formulation In this work, we tackle the challenge of analyzing soccer video segments, denoted as RT 3HW . Our goal is to utilize the visual encoder (ΦMatchVision) to extract spatiotemporal features from these segments, which are then processed by multiple task-specific heads, formulated as: E, C, = Ψ(ΦMatchVision(V)) Here, Ψ = {Ψcls, ΨCmt, ΨFoul} denotes the task-specific heads, while E, C, and refer to the output event types, textual commentaries, and foul types, respectively. This unified framework not only effectively learns relevant spatiotemporal features, but also facilitates seamless integration across various downstream tasks tailored for comprehensive soccer game understanding. 4.2. Architecture MatchVision comprises three key components: (i) Token Embedding, (ii) Spatiotemporal Attention Block, and (iii) Aggregation Layer, as depicted in Figure 3. Token Embedding. In accordance with the convention in Vision Transformer [10], each frame (Ii) from the video segment = {I1, I2, , IT } is divided into nonoverlapping patches of size that span the entire frame. ), where and denote the spatial and temporal positions, respectively. Each vector is transformed via an embedding layer (ΦEmb) into token vector of size R1D, and then added with spas RM D). Subsequently, we tial position embedding (epos concatenate [cls] token along with each frame. Finally, temporal positional embedding (epos RT D) is added across features of all frames, as formulated below: These patches are flattened into vectors (xp , ΦEmb([x1 yi = [xcls = [y1, , yT ] + epos , , xM ]) + epos ] 4 Here, [, ] denotes concatenation, and yi R(M +1)D represents the frame-wise features. The embedded features (z) will then serve as input for spatiotemporal attention blocks. Spatiotemporal Attention Block. Taking inspiration from TimeSformer [3], we utilize interleaved temporal and spatial attention to integrate the spatiotemporal information in soccer videos. Concretely, each spatiotemporal attention block comprises temporal self-attention layer and spatial self-attention layer, i.e., ϕt() and ϕs(), respectively. Given the video feature RT (M +1)D, we alternate the temporal and spatial attention: temporal attention facilitates interactions among tokens at the same spatial positions across distinct frames, while spatial attention enables interactions among tokens within the same frame. Residual connections are employed in each layer. After passing through total of spatiotemporal attention blocks, the resulting feature (F) captures both intra-frame and inter-frame relationships, i.e., = [ϕs(ϕt(z))]K RT (M +1)D. Aggregation Layer. To get the video-level features, we employ an aggregation layer on the frame-wise spatiotemporal features. Specifically, for the i-th frame, we utilize spatial self-attention to aggregate information into its [cls] token, denoted as ˆF cls = ΦAgg(Fi). We then concatenate the [cls] tokens of all frames, to get the final video feature (FV ), that effectively encapsulates the spatiotemporal characteristics of the soccer video segments, and enables it to be applicable for various downstream soccer understanding tasks. This process can be formulated as: FV = ΦMatchVision(V) = [ ˆF cls 1 , , ˆF cls ] RT 4.3. MatchVision Pretraining In this part, we aim to pretrain the visual encoder with triplet samples ({V, E, C}), comprising videos, event labels, and textual commentaries. Concretely, we investigate two distinct pretraining strategies: supervised classification and video-language contrastive learning. Supervised Classification. One way to pretrain the visual encoder is supervised learning on event classification. To be specific, the extracted visual features (FV ) are aggregated by temporal self-attention layer into learnable [cls] token, denoted as cls , similar to the spatial-wise aggregation mentioned above. This token is then fed into linear classifier, and trained with cross-entropy loss for event classification. The objective is denoted as Lsup. Video-Language Contrastive Learning. As an alternative, we can also pretrain our visual encoder with video-text contrastive learning. Specifically, we adopt simple average pooling on the video feature to get the aggregated visual feature (F Avg ), and encode the textual commentary (C) with text encoder (ΦText). We train the model with sigmoid loss (Lsigmoid), as used in SigLIP [57]. Note that, some Figure 3. Overview of MatchVision: (a) The model architecture and its spatiotemporal feature extraction process; (b) Details of visual encoder pretraining, such as supervised training and video-language contrastive learning; (c) Implementation details of specific heads for various downstream tasks, including commentary generation, foul recognition, and event classification. video clips may have highly similar commentaries, for example, start of the game, we treat the commentaries with high similarity in the same batch as positive samples when calculating loss functions. This can be expressed as follows: Lcontra = Lsigmoid(F Avg , ΦText(C)) 4.4. Downstream Tasks After the abovementioned training, MatchVision can now serve as versatile visual encoder, to map the soccer video segments into visual features (FV ), for training task-specific heads Ψ = {Ψcls, ΨCmt, ΨFoul} for different downstream tasks, including: (i) event classification, (ii) commentary generation, and (iii) foul recognition. Event Classification. Similar to the supervised training mentioned above, we concatenate learnable [cls] token to aggregate the frame-wise visual features through temporal self-attention. This token is then fed into linear classifier for event classification. The event classification head (Ψcls) can be trained with cross-entropy loss with the visual encoder frozen. Commentary Generation. On commentary generation, we follow the design in MatchTime [40]. Concretely, the commentary generation head (ΨCmt) employs Perceiver [24] aggregator to consolidate visual features, which are then projected by trainable MLP, serving as prefix embeddings for large language model (LLM). Subsequently, an offthe-shelf LLM decodes these embeddings into textual commentary. We adopt the negative log-likelihood loss, commonly used for auto-regressive next-token prediction. Foul Recognition. As outlined in [21], the foul recognition task takes multi-view videos from the same scene as inputs, with each sample annotated with foul class (8 types) and severity (4 levels). We encode these multi-view videos with MatchVision, and then aggregate the extracted features into single feature vector, via either max or average pooling, following the common practice. Subsequently, the foul recognition head (ΨFoul) employs shared MLP and two task-specific linear classifiers, to predict foul type and severity, respectively. Similar to event classification, we use the combination of cross-entropy losses on the foul type and severity classification to jointly train ΨFoul. Discussion. Pretraining MatchVision on large-scale soccer data equips it with substantial domain-specific knowledge, enabling it to serve as universal visual encoder adaptable to various downstream soccer understanding tasks. 5. Experiments This section begins with the implementation details in Section 5.1; followed by quantitative evaluations across multiple downstream tasks in Section 5.2; then, we conduct ablation experiments on our established SoccerReplay-test benchmark to investigate the effectiveness of the proposed dataset and model in Section 5.3; finally, we provide qualitative results for comparison in Section 5.4. 5.1. Implementation Details In our experiments, all video segments are sampled at 1FPS centered around the annotated timestamps, capturing 30second window for each sample. Each frame is resized to 5 Visual Encoder I3D [4] C3D [45] ResNet [20] CLIP [38] InternVideo [49] SigLIP [57] Baidu [59] SigLIP SigLIP MatchVision MatchVision SigLIP SigLIP MatchVision MatchVision SigLIP SigLIP MatchVision MatchVision Dataset Classification (%) Commentary SN MT SR Acc.@1 Acc.@3 Acc.@5 B@1 B@4 R-L Off-the-shelf Models 45.4 47.8 47.2 48.5 49.9 50.2 82.5 85.1 84.6 85.5 87.0 86.7 93.2 95.0 94.4 95.2 95.9 95.6 26.77 28.13 27.34 26.25 27.12 27. Pretrain with Supervised Classification 56.4 55.9 57.9 82.5 84.0 91.9 89.6 91.7 96.6 97.3 97.3 94.9 97.5 98.8 99.2 31.20 28.51 30.95 29.45 31.05 Pretrain with Visual-Language Contrastive Learning 55.4 66.8 58.9 67.9 88.8 93.7 89.0 93.9 97.0 98.6 97.1 98. 28.72 30.35 30.33 31.94 5.57 6.64 6.57 6.51 6.54 6.98 8.88 7.39 8.56 7.92 9.06 7.72 8.12 7.97 9.12 Pretrain with Hybrid Supervised-Contrastive Training 71.2 67.1 76.4 80.1 94.5 93.2 96.0 97.1 98.7 98.1 99.0 99.1 28.63 30.71 30.65 33.58 7.82 8.78 8.33 9.14 24.17 24.52 24.72 24.27 25.02 25.16 26.56 25.96 25.79 26.01 26.94 25.91 26.05 25.48 26. 25.74 26.26 25.28 26.82 23.12 24.23 24.43 24.75 24.82 25.03 26.61 25.94 26.17 26.21 27.93 26.17 26.38 26.33 27.56 25.35 26.74 26.31 28.21 18.73 27.88 27.29 28.17 29.90 31. 38.93 35.71 38.24 36.15 42.20 32.27 39.41 33.87 40.76 34.09 41.82 37.23 44.18 Table 2. Quantitative Results on Event Classification and Commentary Generation. Here, SN, MT, and SR represent curated SoccerNet-v2 [9], MatchTime [40], and SoccerReplay-1988, respectively. Moreover, B, M, R-L, and refer to BLEU, METEOR, ROUGEL, and CIDEr metrics, respectively. Within each unit, we denote the best performance in RED and the second-best performance in BLUE. 224224 pixels to serve as inputs. We initialize the embedding layer, spatial attention layers, aggregation layer, and text encoder of our MatchVision with the pretrained weights from SigLIP Base-16 [57] and adopt LLaMA-3 (8B) [11] as the off-the-shelf LLM decoder for commentary generation head. All experiments are conducted on 4 Nvidia H800 GPUs, using AdamW [33] as the optimizer. Next, we will elaborate on the training and evaluation details about visual encoder pretraining and downstream tasks. Visual Encoder Pretraining. For both pretraining strategies, we use batch size of 40 for 15 epochs. The learning rate for all randomly initialized modules, including the temporal attention layers, aggregator layer, and linear classifier, is set to 1 104. Meanwhile, the learning rate for modules initialized with pretrained parameters (including the text encoder) is set to 5 105. In contrastive training, we adopt multi-positive strategy where each textual commentary, based on its event label, considers closely related categories (e.g. start of game and offside) as positive samples. Downstream Tasks. In all downstream tasks, unless otherwise specified, we use the frozen visual encoder for feature extraction and only train the task-specific heads with learning rate of 1 104 for 30 epochs. The batch size for event classification, commentary generation, and foul recognition is set to 40, 32, and 8, respectively. We adopt specific evaluation metrics for these three tasks: (i) For event classification, we use the top-1/3/5 classification accuracy; (ii) For commentary generation, we employ several commonly-used language evaluation metrics, including BLEU [35], METEOR [2], ROUGE-L [32], and CIDEr [48]. (iii) For foul recognition, we follow the common practice, and report top-1/2 and top-1 accuracy for the foul type and severity classification, respectively. Benchmarks & Baselines. To ensure fair and reliable comparisons with existing work, we evaluate event classification (24 types) on 100 matches from curated SoccerNetv2 [9] test set, commentary generation on 49 matches from SN-Caption-test-align benchmark manually aligned in [40]; and foul recognition evaluation on MVFoul [21]. We consider various baselines: for the first two tasks, this includes off-the-shelf visual encoders such as ResNet [20], C3D [45], I3D [4], CLIP [38], SigLIP [57], and InternVideo [49], along with Baidu [59] and SigLIP [57] trained under various settings. For foul recognition, we follow 6 Visual Encoder Foul Class Severity"
        },
        {
            "title": "Pretrain",
            "content": "Classification(%) Backbone Train Agg. Acc.@1 Acc.@2 Acc.@1 Sup. Contra. SR Acc.@1 Acc.@3 Acc.@5 ResNet [20] R(2+1)D [46] MViT [12] Mean Max Mean Max Mean Max MatchVision Mean Max 0.31 0.32 0.31 0.32 0.40 0.47 0.44 0.35 0.56 0. 0.55 0.56 0.65 0.69 0.53 0.70 0.34 0.32 0.34 0.39 0.38 0. 0.58 0.46 Table 3. Quantitative Results on Multi-view Fouls Recognition. Our frozen MatchVision encoder can achieve comparable performance with other jointly finetuned visual encoders. previous work and adopt ResNet [20], R(2+1)D [46], and MViT [12] jointly finetuned with classifiers, as baselines. 5.2. Quantitative Evaluation As presented in Table 2, we can draw two observations on event classification and commentary generation: (i) visual encoders trained on soccer data substantially outperform the off-the-shelf general encoders, ResNet, C3D, I3D, CLIP, and InternVideo, underscoring the necessity of building specific foundation models for soccer understanding; (ii) almost all visual encoders, across all training settings, significantly benefit from SoccerReplay-1988, emphasizing the value of constructing high-quality, large-scale data for soccer video understanding. Next, we will delve deeper into each task to discuss the results. Event Classification. With the same training strategies and data, MatchVision considerably outperforms other methods in classification accuracy, demonstrating the superiority of our architecture, which effectively leverages spatiotemporal features within soccer videos. Moreover, models trained via supervised training excel others, primarily because the pretraining task shares the same objectives as the downstream event classification task. Commentary Generation. Visual encoders trained with visual-language contrastive learning exhibit better commentary generation performance than those trained with supervised classification, as this strategy more effectively learns the correlations between visual and textual features. Additionally, the MatchVision encoder trained solely on SoccerNet slightly underperforms the Baidu [59] encoder, but incorporating our SoccerReplay-1988 dataset enables it to outperform on most metrics. This indicates that MatchVision can take the benefit of large-scale datasets. Finally, hybrid training approach, starting with supervised classification followed by visual-language contrastive learning, enables MatchVision to achieve optimal performance. This indicates that learning coarse-grained tasks such as clas62.67 68.03 46.97 57.41 56.86 63.59 83.00 86. 75.53 83.13 80.30 85.21 89.81 92.38 85.85 91.00 88.09 91.63 Table 4. Ablation Studies on Event Classification. We explore the impact of various training settings of our MatchVision encoder on the SoccerReplay-test benchmark. Here, Sup., Contra., and SR refer to supervised classification, visual-language contrastive learning, and the SoccerReplay-1988 dataset, respectively. sification provides foundation for fine-grained tasks like commentary generation, and that fully leveraging data can unleash the potential of soccer understanding. Foul Recognition. As demonstrated in Table 3, MatchVision achieves performance comparable to jointly finetuned state-of-the-art methods in foul recognition, even with frozen visual encoder. This highlights that MatchVision can effectively learn substantial knowledge from large-scale soccer data and adapt seamlessly to downstream tasks. 5.3. Ablation Studies We perform ablation experiments on event classification and commentary generation using our SoccerReplay-test benchmark. These experiments validate the effectiveness of our proposed dataset and model, while establishing baseline for future evaluations on this benchmark. Event Classification. We evaluate event classification on 300 matches from our SoccerReplay-test benchmark using the MatchVision visual encoder pretrained with various strategies. Features are extracted by MatchVision and processed with learnable aggregation layer and linear classifier. The default training set is our curated SoccerNet-pro. As shown in Table 4, integrating our proposed SoccerReplay-1988 for training results in significant performance improvements across all pretraining strategies, yielding the significance of our dataset. Additionally, supervised classification outperforms visual-language contrastive learning and hybrid pretraining. This is due to its closer alignment with downstream event classification, and the scale of event annotations is far larger than that of textual commentaries, further confirming the significant benefits of data scaling for boosting soccer understanding. Commentary Generation. With the pretrained MatchVision encoder, we train the commentary generation head on the MatchTime [40] and SoccerReplay-1988 datasets using various training strategies. By default, only the Perceiver [24] aggregation layer and projection layer within the 7 Figure 4. Qualitative Results for Event Classification and Commentary Generation. Here, w/o SR and w/ SR indicate models trained without and with the SoccerReplay-1988 dataset, respectively. Incorporating the SoccerReplay-1988 dataset improves event classification accuracy. For commentary generation, this enriched training data enables the commentary generation head to demonstrate notable advantages in several aspects: (a) more detailed descriptions, (b) greater linguistic variety, (c) higher accuracy in event depiction, (d) better adherence to updated rules, and (e) improved specificity in scenario response. head are trained. For joint training with the LLM decoder, considering computational costs, we incorporate LoRA [23] layers while keeping the original LLM layers frozen. As shown in Table 5, incorporating SoccerReplay-1988 significantly improves performance on all metrics, confirming the substantial advantages of our proposed dataset. This performance gap also reflects the challenges of our established benchmark, which features diverse vocabulary, richer semantics, and updated soccer rules. Additionally, jointly finetuning the visual encoder and the LLM decoder provides feasible approach for further improvements. 5.4. Qualitative Comparisons As depicted in Figure 4, we present the qualitative results of MatchVision on the new SoccerReplay-test benchmark, comparing models pretrained without and with the SoccerReplay-1988 dataset. For event classification, incorporating our new dataset improves accuracy, and even in the mis-classified cases, the results remain contextually relevant. For commentary generation, after hybrid training on SoccerReplay-1988, MatchVision is capable of producing richer, more detailed textual commentary, demonstrating deeper understanding of soccer dynamics. More qualitative results are available in Section D.2 of the Appendix. 6. Conclusion In this paper, we establish unified, scalable multi-modal framework for soccer understanding. Specifically, we introduce SoccerReplay-1988, the largest and most comprehensive soccer video dataset to date, annotated by an automated curation pipeline. This provides solid foundation Trainable Commentary Metrics B@ B@4 R-L Trained on MatchTime 21.65 27.62 27.04 27. 3.27 7.02 6.41 6.96 21.02 24.03 24.15 24.50 17.79 23.51 23.88 23.33 12.90 30.77 31.91 30.81 Trained on MatchTime & SoccerReplay-1988 24.17 28.98 27.54 29.21 4.09 8.39 7.76 8.22 20.51 24.45 24.50 25.25 20.70 25.35 24.70 25.54 15.70 45.85 42.79 43.18 Table 5. Ablation Studies of Commentary Generation. We investigate the impact of different training strategies and datasets on MatchVision using the SoccerReplay-test benchmark. and denote the visual encoder and the LLM decoder, respectively. for developing multi-modal soccer understanding models and serves as more challenging benchmark. Built upon this, we have developed the first visual-language foundation model for soccer, termed MatchVision, which effectively leverages the spatiotemporal information within soccer videos and can be applied to various tasks such as event classification and commentary generation. Extensive experiments demonstrate the superiority of our proposed model, with MatchVision achieving state-of-the-art performance on both existing benchmarks and our newly established one. We believe this work will set viable, universal paradigm for future research in sports understanding."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: In Advances visual language model for few-shot learning. in Neural Information Processing Systems, pages 23716 23736, 2022. 2 [2] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with huIn Proceedings of the ACL Workshop on man judgments. Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, 2005. 6 [3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In Proceedings of the International Conference on Machine Learning, page 4, 2021. 2, 4 [4] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. 6 [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2 [6] Anthony Cioppa, Adrien Deliege, Floriane Magera, Silvio Giancola, Olivier Barnich, Bernard Ghanem, and Marc Van Droogenbroeck. Camera calibration and player localization in soccernet-v2 and investigation of their representations for action spotting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 45374546, 2021. [7] Anthony Cioppa, Adrien Deliege, Silvio Giancola, Bernard Ghanem, and Marc Van Droogenbroeck. Scaling up soccernet with multi-view spatial localization and re-identification. Scientific data, 9(1):355, 2022. 1, 2 [8] Anthony Cioppa, Silvio Giancola, Adrien Deliege, Le Kang, Xin Zhou, Zhiyu Cheng, Bernard Ghanem, and Marc Van Droogenbroeck. Soccernet-tracking: Multiple object In Protracking dataset and benchmark in soccer videos. ceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 34913502, 2022. 2 [9] Adrien Deliege, Anthony Cioppa, Silvio Giancola, Meisam Seikavandi, Jacob Dueholm, Kamal Nasrollahi, Bernard Ghanem, Thomas Moeslund, and Marc Van Droogenbroeck. Soccernet-v2: dataset and benchmarks for holisIn Proceedtic understanding of broadcast soccer videos. ings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 45084519, 2021. 1, 2, 3, 6, 14, 16, 17 [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations, 2021. 4 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3, 6, 15 [12] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 68246835, 2021. [13] Sushant Gautam, Mehdi Houshmand Sarkhoosh, Jan Held, Cise Midoglu, Anthony Cioppa, Silvio Giancola, Vajira Thambawita, Michael Riegler, Pal Halvorsen, and Mubarak Shah. Soccernet-echoes: soccer game audio commentary dataset. arXiv preprint arXiv:2405.07354, 2024. 2 [14] Silvio Giancola, Mohieddine Amine, Tarek Dghaily, and Bernard Ghanem. Soccernet: scalable dataset for action spotting in soccer videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 17111721, 2018. 1, 2, 3, 14 [15] Xiaofan Gu, Xinwei Xue, and Feng Wang. Fine-grained acIn Internation recognition on novel basketball dataset. tional Conference on Acoustics, Speech, and Signal Processing, pages 25632567. IEEE, 2020. 2 [16] Tengda Han, Weidi Xie, and Andrew Zisserman. TempoIn Proceedral alignment networks for long-term video. ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 29062916, 2022. 2 [17] Tengda Han, Max Bain, Arsha Nagrani, Gul Varol, Weidi Xie, and Andrew Zisserman. Autoad: Movie description in In Proceedings of the IEEE Conference on Comcontext. puter Vision and Pattern Recognition, pages 1893018940, 2023. 2 [18] Tengda Han, Max Bain, Arsha Nagrani, Gul Varol, Weidi Xie, and Andrew Zisserman. Autoad ii: The sequel-who, when, and what in movie audio description. In Proceedings of the International Conference on Computer Vision, pages 1364513655, 2023. [19] Tengda Han, Max Bain, Arsha Nagrani, Gul Varol, Weidi Xie, and Andrew Zisserman. Autoad iii: The prequel - back to the pixels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 18164 18174, 2024. 2 [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770778, 2016. 6, 7 [21] Jan Held, Anthony Cioppa, Silvio Giancola, Abdullah Hamdi, Bernard Ghanem, and Marc Van Droogenbroeck. Vars: Video assistant referee system for automated soccer decision making from multiple views. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 50855096, 2023. 2, 5, 6 [22] Jan Held, Hani Itani, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, and Marc Van Droogenbroeck. X-vars: 9 Introducing explainability in football refereeing with multimodal large language models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 32673279, 2024. [23] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In Proceedings of the International Conference on Learning Representations, 2022. 8, 17 [24] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In Proceedings of the International Conference on Machine Learning, pages 4651 4664. PMLR, 2021. 5, 7, 17 [25] Yudong Jiang, Kaixu Cui, Leilei Chen, Canjin Wang, and Changliang Xu. Soccerdb: large-scale database for comprehensive video understanding. In Proceedings of the 3rd International Workshop on Multimedia Content Analysis in Sports, pages 18, 2020. 14 [26] Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, and Chen Chen. Sports-qa: large-scale video question answering benchmark for complex and professional sports. arXiv preprint arXiv:2401.01505, 2024. 2 [27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the International Conference on Machine Learning, pages 1288812900. PMLR, 2022. 2 [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the International Conference on Machine Learning, pages 1973019742. PMLR, 2023. [29] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 2 [30] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In Proceedings of the European Conference on Computer Vision, 2024. 2 [31] Zeqian Li, Qirui Chen, Tengda Han, Ya Zhang, Yanfeng Wang, and Weidi Xie. Multi-sentence grounding for longIn Proceedings of the European term instructional video. Conference on Computer Vision, 2024. 2 [32] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, 2004. 6 [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, 2019. 6 [34] Hassan Mkhallati, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, and Marc Van Droogenbroeck. Soccernetcaption: Dense video captioning for soccer broadcasts comIn Proceedings of the IEEE Conference on mentaries. Computer Vision and Pattern Recognition Workshops, pages 50745085, 2023. 1, 2, 3, 16, 17 [35] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine In Association for Computational Linguistics, translation. pages 311318, 2002. 6 [36] Ji Qi, Jifan Yu, Teng Tu, Kunyu Gao, Yifan Xu, Xinyu Guan, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li, et al. Goal: challenging knowledge-grounded video captioning benchmark for real-time soccer commentary generation. In Proceedings of the ACM International Conference on Information and Knowledge Management, pages 53915395, 2023. 1, 2, 3, 14 [37] Mengshi Qi, Yunhong Wang, Annan Li, and Jiebo Luo. Sports video captioning via attentive motion representation and group relationship modeling. IEEE Transactions on Circuits and Systems for Video Technology, 30(8):26172633, 2019. 2 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn Proceedings of the International Conference on vision. Machine Learning, 2021. 2, 6 [39] Prem Ramkumar, Bryan Luu, Heather Haeberle, Jaret Karnuta, Benedict Nwachukwu, and Riley Williams. Sports medicine and artificial intelligence: primer. The American Journal of Sports Medicine, 50(4): 11661174, 2022. [40] Jiayuan Rao, Haoning Wu, Chang Liu, Yanfeng Wang, and Weidi Xie. Matchtime: Towards automatic soccer game commentary generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processinng, 2024. 1, 2, 3, 5, 6, 7, 14, 15, 16, 17 [41] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: hierarchical video dataset for fine-grained action understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 26162625, 2020. 2 [42] Vladimir Somers, Victor Joos, Anthony Cioppa, Silvio Giancola, Seyed Abolfazl Ghasemzadeh, Floriane Magera, Baptiste Standaert, Amir Mansourian, Xin Zhou, Shohreh Kasaei, et al. Soccernet game state reconstruction: End-toend athlete tracking and identification on minimap. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 32933305, 2024. 2 [43] Alessandro Suglia, Jose Lopes, Emanuele Bastianelli, Andrea Vanzo, Shubham Agarwal, Malvina Nikandrou, Lu Yu, Ioannis Konstas, and Verena Rieser. Going for goal: resource for grounded football commentaries. arXiv preprint arXiv:2211.04534, 2022. 14 [44] Graham Thomas, Rikke Gade, Thomas Moeslund, Peter Carr, and Adrian Hilton. Computer vision for sports: Current applications and research topics. Computer Vision and Image Understanding, 159:318, 2017. 2 [45] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the Interna- [57] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the International Conference on Computer Vision, pages 1197511986, 2023. 2, 4, 6, 17 [58] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In Proceedings of the Conference on Empirical Methods in Natural Language Processinng, 2023. 2 [59] Xin Zhou, Le Kang, Zhiyu Cheng, Bo He, and Jingyu Xin. Feature combination meets attention: Baidu soccer embeddings and transformer based temporal detection. arXiv preprint arXiv:2106.14447, 2021. 2, 6, 7 [60] Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia In ProceedSchmid. Streaming dense video captioning. ings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2 tional Conference on Computer Vision, pages 44894497, 2015. 6 [46] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. closer look at spatiotemporal In Proceedings of the convolutions for action recognition. IEEE Conference on Computer Vision and Pattern Recognition, pages 64506459, 2018. [47] Renaud Vandeghen, Anthony Cioppa, and Marc Semi-supervised training to imVan Droogenbroeck. In Proceedings prove player and ball detection in soccer. of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 34813490, 2022. 2 [48] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 45664575, 2015. 6, 17 [49] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 6 [50] Zhe Wang, Petar Veliˇckovic, Daniel Hennes, Nenad Tomaˇsev, Laurel Prince, Michael Kaisers, Yoram Bachrach, Romuald Elie, Li Kevin Wenliang, Federico Piccinini, et al. Tacticai: an ai assistant for football tactics. Nature Communications, 15(1):113, 2024. 1, 2 [51] Zeyu Xi, Ge Shi, Lifang Wu, Xuefen Li, Junchi Yan, Liang Wang, and Zilin Liu. Knowledge graph supported benchmark and video captioning for basketball. arXiv preprint arXiv:2401.13888, 2024. [52] Haotian Xia, Zhengbang Yang, Yuqing Wang, Rhys Tracy, Yun Zhao, Dongdong Huang, Zezhi Chen, Yan Zhu, Yuanfang Wang, and Weining Shen. Sportqa: benchmark for sports understanding in large language models. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, 2024. 2 [53] Haotian Xia, Zhengbang Yang, Junbo Zou, Rhys Tracy, Yuqing Wang, Chi Lu, Christopher Lai, Yanjun He, Xun Shao, Zhuoqing Xie, et al. Sportu: comprehensive sports understanding benchmark for multimodal large language models. arXiv preprint arXiv:2410.08474, 2024. 2 [54] Jinglin Xu, Yongming Rao, Xumin Yu, Guangyi Chen, Jie Zhou, and Jiwen Lu. Finediving: fine-grained dataset for procedure-aware action quality assessment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 29492958, 2022. 2 [55] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1071410726, 2023. 2 [56] Huanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang, Jian Zhang, and Xiaokang Yang. Fine-grained video captioning for sports narrative. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6006 6015, 2018. 2,"
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related Works 3. SoccerReplay-1988 Dataset 3.1. Dataset Collection . . . 3.2. Automated Data Curation . . 3.3. Statistics & Discussion . . . . 4. Method . 4.1. Problem Formulation . 4.2. Architecture . . 4.3. MatchVision Pretraining . . 4.4. Downstream Tasks . . . . . . . . . 5. Experiments 5.1. Implementation Details . . 5.2. Quantitative Evaluation . 5.3. Ablation Studies . . 5.4. Qualitative Comparisons . . . . 6. Conclusion . . . . . . . . . . . . . . . . . . . A. SoccerReplay-1988 Dataset Details A.1. Dataset Format . A.2. Additional Dataset Statistics . A.3. Event Summarization Prompt . . . . . . . B. SoccerNet-pro Dataset Details . . . . B.1. SoccerNet-v2 . B.2. MatchTime . . . B.3. Data Split Strategy . . . . . . . . . . . . . . . . . C. Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1. Data Preprocessing . . C.2. Validation Strategy during Training . . C.3. Hyperparameter Selection . . . . . . . . . . . . . D. Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1. Training Curves . D.2. More Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Limitations & Future Work 12 1 2 2 2 3 4 4 4 4 5 5 5 7 7 8 8 . . . . . . . . . . . 13 . 13 . 13 . 15 16 . 16 . 16 . 16 17 . 17 . 17 . 17 18 . 18 . 18 18 . . . . . . . . . . . . . . . . . . . . . . A. SoccerReplay-1988 Dataset Details In this section, we provide additional details of our SoccerReplay-1988 dataset. Specifically, Section A.1 elaborates on the structure and format of the dataset; Section A.2 presents statistical information and analyses of the dataset; and Section A.3 describes the methodology to generate the event labels within the dataset. A.1. Dataset Format The SoccerReplay-1988 dataset consists of match videos, descriptions of events, and related match information of 1988 soccer matches. Each match includes two mkv video files (1-half and 2-half), covering the match from the initial kickoff to the final whistle. Additionally, json file is accompanied by encapsulating detailed information, including event descriptions and comprehensive match background structured as follows: Match Information provides background details of the match, including the competing teams, final results, and match contexts, such as the start time, team formations, and venue details, as illustrated below: { } \"timestamp\": \"2022-08-07 21:00:00\", \"score\": \"1 - 2\", \"home_team\": \"Manchester Utd\", \"away_team\": \"Brighton\", \"home_formation\": \"4 - 3 - 3\", \"away_formation\": \"3 - 4 - 2 - 1\", \"venue\": \"Old Trafford (Manchester)\", \"capacity\": \"75 635\", \"attendance\": \"73 711\", # Match start time # Final score # Home team name # Away team name # Home team formation # Away team formation # Venue and city # Stadium capacity # Number of attendees Referee Information includes details about the primary referee officiating the match, which is formatted as follows: { } \"country\": \"Eng\", \"name\": \"Paul Tierney\" # Referees nationality # Referees name Player Information contains details about various types of individuals involved in the match, including starting players, substitutes, absent players, and coaches. All these types are stored in unified list, with the following format: { \"players_name\": \"Caicedo M.\", \"players_number\": \"25\", \"Full Name\": \"Moises Caicedo\", \"players_rating\": 7.6, \"Country\": \"Ecuador\", \"Image URL\": \"https://static.flashsc...\", \"Role\": \"Midfielder\", \"Age and Birthdate\": \"22, (02.11.2001)\", \"Market Value\": \" C89.4m\" # Players abbreviated name # Jersey number # Players full name # Post-match rating # Players nationality # URL to player image # On-field role # Age and birth date # Players market value } Event Descriptions is list that records all key events during the match, including their types and detailed commentary. typical example of an event entry is shown below: { \"half\": 1, \"time_stamp\": \"00:16\", \"comments_type\": \"shot off target\", \"comments_text\": \"A mistake by Leandro Trossard (Brighton)...\", \"comments_text_anonymized\": \"A mistake by [PLAYER]([TEAM])...\" # Match half (1 or 2) # Timestamp within the half # Event type # Commentary text # Commentary after anonymization } A.2. Additional Dataset Statistics League # Match Season # Match Italy Serie-a England Premier League UEFA Champions League France Ligue 1 Spain LaLiga Germany Bundesliga 367 552 469 123 235 242 2017-2018 2018-2019 2019-2020 2020-2021 2021-2022 2022-2023 2023-2024 172 325 300 323 330 416 Table 6. League-wise Match Statistics. Table 7. Season-wise Match Statistics. 13 To provide comprehensive analysis of the SoccerReplay-1988 dataset, we present statistical summaries and visualizations in the following tables and figures. Specifically, Table 6 and 7 illustrate the distribution of the 1988 matches across different leagues and seasons. Figure 5 (a) compares the SoccerReplay-1988 dataset with other soccer datasets [9, 14, 25, 36, 40, 43, 56], highlighting its unique characteristics. Figure 5 (b) depicts the distribution of event labels for the 24 newly defined categories. Furthermore, Figures 5 (c), (d), (e), and (f) present detailed analyses of the commentary data, including frequency distributions, timestamps, and word counts. Figure 5. Comprehensive Visualizations of SoccerReplay-1988 Dataset. A.3. Event Summarization Prompt Our dataset expands the original 17 event categories to 24 types. For SoccerReplay-1988 and MatchTime [40], LLaMA-3 (70B) [11] is used to analyze commentaries and generate event labels. All commentaries are processed with the following prompt with comprehensive definitions and rich examples to determine their types: <begin of text> <start header id>system<end header id> You are an expert in soccer, you have very important task to summarize soccer commentary into certain types of events. The accuracy of your classification is the most emergency thing. will give you commentary sentence. You need to select one type of event that can best describe this event from the following 24 types: corner, goal, injury, own goal, penalty, penalty missed, red card, second yellow card, substitution, start of game(half), end of game(half), yellow card, throw in, free kick, saved by goal-keeper, shot off target, clearance, lead to corner, off-side, var, foul (no card), statistics and summary, ball possession, ball out of play. Here are some rules you have to obey when summarizing types, you should consider it strictly following these steps: 1. Firstly, you need to find if there is any evidence of foul in commentary, if yes, it can only be foul (no card), yellow card, red card or second yellow card according to the situation, even though it introduces the result free kick or penalty. For example: Per Mertesacker (Arsenal) commits rough foul. Michael Dean stops the game and makes call. Thats free kick to Manchester Utd. can ONLY be foul (no card) since there is foul in commentary, even though the result is free kick. 2. Secondly, only if the word corner is in the commentary, you need to select it from lead to corner. lead to corner means the process of how the corner occurs, which is before the corner kick. For type lead to corner, there will always be words like award corner, will have corner, point at corner flag and so on. For example: Victor Wanyama (Southampton) goes on solo run, but he fails to create chance as an opposition player blocks him. The referee signals corner kick to Southampton. is lead to corner. 3. Thirdly is the most easy-confused part, you need to be cautious: only if the word free-kick/free kick is in the commentary will it be free kick. According to the first rule, if there is foul in the sentence, it cannot be free kick. free kick can only be selected when free-kick/free kick occurs in commentary and is describing the process of free kick attack. For example: Olivier Giroud (Arsenal) gets on the ball and beats an opponent, but his run is stopped by the referee Michael Dean who sees an offensive foul. Its free kick to Burnley, but they probably wont attempt direct shot on goal from here. is foul (no card); Ander Herrera (Manchester United) makes slide tackle, but referee Michael Dean blows for foul. Free kick. Arsenal will probably just try to cross the ball in from here. is foul (no card); Marcos Rojo (Manchester United) connects with the free kick and produces header goalwards which is well blocked. The goalkeeper doesnt have to worry about that one. is free kick. 4. Similarly, penalty and penalty missed only describe things that happen during penalty kick. If it is introducing the reason that leads to penalty, you should return the type describing the reason, like foul (no card), yellow card, and so on. 5. The type statistics and summary includes all the commentaries that are not introducing visually evidential events, but those statistics or overviews of the game. These sentences wont concentrate on certain events, but on the overall game. 6. ball possession represents those commentaries that describe any of the teams controlling the ball possession. 7. You need to be sensitive about the type shot off target; if there is an event of shot happening in the commentary, it is shot. If its not goal, didnt make score, and was not saved by the goalkeeper, it would probably be shot off target. Normally there will be keywords like wide of the right post, over the crossbar, crashes against the crossbar and so on. You have to judge it sensitively about the situation after the shot. 8. An important type after shot: saved by goal-keeper describes that the shot is saved by the goalkeeper; there would be words like blocked, saved, and so on. Especially when goal-keeper/goal keeper occurs in the commentary sentence!!! it will probably be saved by goal-keeper. You need to find it carefully!!! 9. 10. 11. 12. 13. If player lofts or swings pass to penalty area/dangerous area, they might be shot off target, clearance, saved by goal-keeper, and so on. It should NOT be identified as corner or free kick if there is no obvious evidence in commentary! For example: Tomas Rosicky (Arsenal) fails to find any of his teammates inside the box as his pass is blocked. should be clearance rather than corner or free kick. clearance means those good performances in defense; they stop the offense of opponents. If such successful defense happens in the commentary, it can only be clearance. In these commentaries, there are always some words like opponents defense, intercepts the ball, clear the ball, and so on. offside is an obvious event; there are always the words flag, linesman, too fast to defense in the commentary since offside is the player running forward the defense line, and the linesman will raise the flag. ball out of play means any player kicks the ball out of boundary lines. These commentary sentences will mostly end up with throw-ins or goal kicks. throw-in means exactly the process of throw-in balls. 14. Most goals are normal goals. If you see scoring event, you can only identify the score as own goal when there is obvious evidence. <eot id> <start header id>user<end header id> With the classification rules, you should tell me the type of commentary from above candidate options: corner, goal, injury, own goal, penalty, penalty missed, red card, second yellow card, substitution, start of game(half), end of game(half), yellow card, throw in, free kick, saved by goal-keeper, shot off target, clearance, lead to corner, off-side, var, foul (no card), statistics and summary, ball possession, ball out of play. The commentary sentence you need to define type is: [COMMENTARY TEXT HERE (before anonymization)] You need to carefully consider the rules in order and make your final decision. Now, you must return me the name of its type from candidate options (in lower case, only return the name of type, answer it right away after my prompt without any other words). <eot id> 15 B. SoccerNet-pro Dataset Details As discussed in Section 3.2, alongside the SoccerReplay-1988 dataset, we incorporate two existing datasets, SoccerNet-v2 [9] and MatchTime [40], into the training process to enrich the training data. These datasets undergo the following preprocessing strategies and are then unified into the SoccerNet-pro dataset, ensuring consistency in format with SoccerReplay-1988. B.1. SoccerNet-v2 The SoccerNet-v2 [9] dataset comprises over 110k event labels across 500 matches, categorized into 17 distinct classes. Based on soccer rules and specific domain knowledge, these labels are systematically reclassified into 24 categories with our proposed automated data curation pipeline, as detailed in Table 8. Original Label Processed Label Reference Penalty Kick-off Penalty Penalty Missed Scored penalties are categorized as Penalty. Missed penalties are categorized as Penalty Missed. Start of Game (Half) Matches the start of half after goals. Shots off target Shot Off Target Throw-in Throw In Ball out of play Ball Out of Play No change. No change. No change. Foul Foul (No Card) Refers to fouls without cards for only. Yellow card Yellow Card No change. Yellowred card Second Yellow Card No change. Red card Direct free-kick Indirect free-kick Red Card Free Kick Substitution Substitution Goal Clearance Offside Corner Goal Clearance Off-Side Corner No change. Both direct and indirect free kicks are grouped. No change. No change. No change. No change. No change. Table 8. Processing strategy for Mapping SoccerNet-pro Labels. The Reference column describes the specific processing applied to the original 17 classes of labels. B.2. MatchTime The MatchTime dataset [40], curated from the SoccerNet-Caption [34], contains substantial amount of commentary, with only small portion accompanied by event labels. To bridge this gap, we apply the prompt-based approach described in Section A.3 to summarize the commentaries into event labels, assigning each commentary corresponding label. B.3. Data Split Strategy As described in Section 3.1, the SoccerReplay-1988 dataset is divided into 1,488 matches for training, 250 for validation, and 250 for testing. For the processed SoccerNet-pro dataset (including SoccerNet-v2 and MatchTime), we adhere to the original partitioning strategies and match distributions of its source datasets as detailed in Table 9."
        },
        {
            "title": "Train Valid Test Total",
            "content": "SoccerNet-v2 [9] MatchTime [40] 300 373 SoccerReplay-1988 100 49 250 100 49 500 471 1988 Table 9. Dataset Splits for Training, Validation, and Testing. 16 C. Implementation Details In this section, we provide additional implementation details about MatchVision. Section C.1 presents more information on data preprocessing strategies; Section C.2 elaborates on the evaluation strategies used during model training; and Section C.3 discusses several hyperparameter choices inspired by prior works. C.1. Data Preprocessing Our automated data curation pipeline filters out video clips with missing annotations, incorrect cropping, or invalid timestamps. In all experiments, including pretraining and downstream tasks, video frames are resized to 224 224 and preprocessed using the SigLIP [57] image preprocessor, which normalizes frames to mean of 0.5 and standard deviation of 0.5 before serving as the inputs. For overlapping video content between the SoccerNet-v2 [9] and MatchTime [40] datasets, we prioritize using event labels from SoccerNet-v2 for training. C.2. Validation Strategy during Training For all experiments, we select the best-performing checkpoints on the validation set using appropriate evaluation strategies, as outlined below. During MatchVision pretraining: (i) For supervised training, we adopt the top-1/3/5 event classification accuracy on the validation set to select the best model; (ii) For visual-language contrastive learning, video-to-text retrieval is performed, and the top-1/3/5 accuracy of event classification, comparing the retrieved texts event labels to the ground truth, serves as the validation metric. During downstream tasks training: (i) In event classification and foul recognition, classification accuracy on the validation set is used as the evaluation metric; (ii) For commentary generation, the CIDEr [48] score of the models predictions on the validation set is employed to select the best checkpoint. C.3. Hyperparameter Selection Here, we provide further explanations about the hyperparameters in our model, inspired by prior works, as detailed below: Temporal Window Size. We adopt 30-second temporal window to extract video clips. This choice is inspired by MatchTime [40], which demonstrates that 30-second window is sufficient to capture adequate visual information for optimal performance, outperforming the 45-second window used in SoccerNet-Caption [34]. LoRA Rank. For finetuning the commentary generation head, we use LoRA [23] with rank of 16, following the approach outlined in MatchTime [40]. Query Length of Perceiver. For the Perceiver [24] module in the commentary generation head, we utilize query length of 32 for temporal information aggregation, consistent with the optimal configuration reported in [40]. Figure 6. Training Loss Curves of Visual Encoders Pretraining. 17 D. Experiments In this section, we provide additional details to offer deeper insights into our model and its performance. Specifically, Section D.1 presents training curves to clearly illustrate the training process; and Section D.2 showcases more qualitative results, demonstrating the models capability to effectively understand soccer dynamics. D.1. Training Curves We present the loss curves for the visual encoder pretraining in Figure 6. Our MatchVision demonstrates significantly better convergence compared to the SigLIP backbone, indicating that it effectively leverages spatiotemporal attention to utilize temporal information, learning representations better suited for highly dynamic soccer videos. D.2. More Qualitative Results In this part, we provide more qualitative visualizations of commentary generation across various events on the field, as depicted in Figure 7, 8, and 9. E. Limitations & Future Work Although MatchVision explores to establish universal visual-language foundation model tailored for soccer, it is not without its limitations: (i) Currently, MatchVision is adapted to event classification, commentary generation, and foul recognition tasks. In the future, we plan to further extend it to tasks such as player tracking and soccer game state reconstruction, aiming to develop more comprehensive foundation model for soccer analysis. (ii) Following prior works, our commentary generation remains anonymized. This is left for future work, where we aim to fully leverage the contextual information available in our SoccerReplay-1988 dataset to enable more vivid, accurate, and context-aware commentary generation. Figure 7. More Qualitative Results of Commentary Generation. 18 Figure 8. More Qualitative Results of Commentary Generation. 19 Figure 9. More Qualitative Results of Commentary Generation."
        }
    ],
    "affiliations": [
        "Alibaba Group, China",
        "CMIC, Shanghai Jiao Tong University, China",
        "School of Artificial Intelligence, Shanghai Jiao Tong University, China"
    ]
}