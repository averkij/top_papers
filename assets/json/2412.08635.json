{
    "paper_title": "Multimodal Latent Language Modeling with Next-Token Diffusion",
    "authors": [
        "Yutao Sun",
        "Hangbo Bao",
        "Wenhui Wang",
        "Zhiliang Peng",
        "Li Dong",
        "Shaohan Huang",
        "Jianyong Wang",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models."
        },
        {
            "title": "Start",
            "content": "Multimodal Latent Language Modeling with Next-Token Diffusion Yutao Sun Hangbo Bao Wenhui Wang Zhiliang Peng Li Dong Shaohan Huang Jianyong Wang Furu Wei Microsoft Research Tsinghua University https://aka.ms/GeneralAI"
        },
        {
            "title": "Abstract",
            "content": "Multimodal generative models require unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop σ-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-ofthe-art VALL-E 2 model in speaker similarity and robustness, while requiring 10 fewer decoding steps. The results establish LatentLM as highly effective and scalable approach to advance large multimodal models. 4 2 0 2 1 1 ] . [ 1 5 3 6 8 0 . 2 1 4 2 : r Figure 1: Latent Language Modeling (LatentLM) seamlessly handles continuous (e.g., image, audio, video) and discrete (e.g., text and code) data using causal Transformers. We introduce next-token diffusion to autoregressively generate the latent vectors one by one. The proposed method provides general-purpose interface that unifies multimodal generation and understanding. Core contributors. Tech lead. Corresponding author."
        },
        {
            "title": "Introduction",
            "content": "Multimodal generative models need unified modeling method to process both discrete data (e.g., text and code) and continuous data (e.g., video, audio, and robot actions). Most previous systems rely on building pipelines or calling external tools. For example, language models perceive and produce audio or image data using independent modules, i.e., automatic speech recognition, text-to-speech, and text-to-image models. However, it is difficult to perform end-to-end optimization for pipeline-based methods. Information loss between modules also restricts performance, as the modules typically use text prompts for communication. In order to natively handle discrete and continuous data in multimodal large language models, there have been three main strands of research. The first one [RPG+21, WCW+23, Tea24] uses VQ-VAE [vdOVK17, ERO21] to quantize continuous data into discrete codes and treats everything as discrete tokens in autoregressive language models. The continuous data are then recovered by the VQ-VAE decoder by conditioning on discrete codes. The performance is often limited by lossy tokenization, which creates restrictive bottleneck during quantization. The low compression ratio also renders the length of discrete codes long. Given the success of diffusion models on continuous data generation [HJA20, RBL+22], another strand of work [BNX+23b, TYZ+23] unifies the modeling of discrete data into diffusion models. However, the unification compromises by following the diffusion-based method, which harms the modeling performance of discrete data. The third strand of research [ZYB+24] shares model weights while using sequence-level diffusion for continuous data and next-token prediction for discrete data. Although sharing parameters, they have different objectives (i.e., denoising for diffusion of continuous data and next-token prediction for discrete data) and implementation details (i.e., bidirectional attention for diffusion and causal attention for next-token prediction). The bidirectional diffusion also restricts the models applications to variable-length sequences. Moreover, the noise added in diffusion training interferes with joint training of interleaved data. In this work, we propose latent language modeling (LatentLM), which seamlessly supports continuous and discrete data with causal Transformers in unified manner. Specifically, we represent continuous data as latent vectors using variational autoencoder (VAE). We introduce next-token diffusion to autoregressively predict the latent vectors, where diffusion heads produce latent vectors by conditioning on each Transformer hidden state. Then the generated continuous data can be recovered by the VAE decoder. For discrete data, the shared Transformer backbone is used to perform next-token prediction with softmax heads. Moreover, in order to make representations suitable for autoregressive decoding, we propose σ-VAE to maintain the variance of the latent space. LatentLM unifies the generation of discrete and continuous tokens under the language modeling paradigm, allowing information sharing among different modalities. The proposed method simplifies implementation by reusing the existing distributed training infrastructure of large language models. Another advantage is that LatentLM unifies generation and understanding with general-purpose interface, which perceives and produces any combination of multimodal data, e.g., text, image, audio, video, and robot action data. Compared to quantizing continuous data, LatentLM has higher compression ratio while maintaining relatively lossless reconstruction quality. We conduct experiments on image generation, multimodal large language models, and text-to-speech synthesis to show the flexibility and effectiveness of LatentLM across modalities. First, image generation on ImageNet [DDS+09] shows that LatentLM achieves competitive performance with the models based on diffusion or discrete tokens. The results demonstrate that LatentLM outperforms DiT [PX23] in the setting of scaling model size. Second, we train multimodal large language models with text, image-text pairs, and interleaved data. The results show that LatentLM outperforms Transfusion [ZYB+24] and the model with vector-quantized image tokenizers, in terms of language modeling, text-to-image generation, and vision-language understanding metrics. We also scale up the number of training tokens and find that LatentLM has favorable scaling properties. Third, experimental results on text-to-speech synthesis show that LatentLM achieves better performance than previous systems. Because our tokenizer uses continuous representations, the compression ratio is much larger than previous vector-quantized tokenizers, which improves both the training and inference efficiency. Figure 2: LatentLM unifies the modeling of continuous and discrete data. We introduce σ-VAE (Section 2.3) to represent continuous data as latent vectors. We perform next-token diffusion (Section 2.1) to autoregressively predict the latent vectors one by one. The diffusion head generates vectors by conditioning on the output states of Transformer. The predicted vectors can be decoded to produce the final outputs."
        },
        {
            "title": "2 Latent Language Modeling",
            "content": "Latent language modeling (LatentLM) autoregressively perceives and generates multimodal sequences (with discrete and continuous data) in unified way. As shown in Figure 2, the model is causal Transformer, where the t-th token is predicted by conditioning on previous 1 tokens. Continuous data are generated by next-token diffusion (Section 2.1), where the diffusion head is used to produce continuous vectors for each position. In addition, discrete tokens are generated by next-token prediction, similar to conventional language modeling. Specifically, let = x1 xN denote an input sequence of discrete and continuous tokens. For discrete token, we use lookup table to get its vector representation. For continuous data, variational autoencoder (VAE) [KW14] is used as tokenizer to compress input data to latent vectors (Section 2.3). After obtaining the vector representations, we pack the input vectors into 0 = [x1, , xN ] RN d, where represents the hidden dimension of the model. 0 is fed into language model based on causal Transformer. The language model is stacked with Transformer layers. Causal masking is used for autoregressive generation. We also adopt pre-RMSNorm [ZS19] and SwiGLU [Sha20, RZL17] as improvements after LLaMA [TLI+23]. The input 0 is further contextualized to obtain the output L, i.e., = Decoder(X l1), [1, L]. The output states of Transformer [h1, , hN ] = RMSNorm(X L) are used to decode the predictions: Decode(xix<i) = Diffusion(hi) Pd(xix<i) = softmax(hiWv) (cid:26) Sample (Pd(xix<i)) xi is discrete token xi is continuous vector (1) where Wv RdV is the softmax classifier weight, is the vocabulary size, and Sample() is sampling algorithm (e.g., greedy decoding, and top-p sampling). The Diffusion() head is described in Section 2.1, which decodes continuous vectors by conditioning on the hidden state hi. The latent vectors are generated autoregressively one by one, i.e., next-token diffusion. Then the VAE decoder is used to generate raw data from the predicted latent vectors. 2.1 Next-Token Diffusion LatentLM autoregressively generates the continuous tokens. We use diffusion as the language model head for each continuous token. The diffusion head progressively refines and generates the latent vector xi by conditioning on the hidden state hi. Then the predicted xi is used as input for the next step of Transformer. In our experiments, we use either denoising diffusion probabilistic model (DDPM) [HJA20] or flow matching [LCBH+22] as our design choice. We use DDPM as an example to describe the details. Diffusion is formulated as two processes, i.e., the forward process gradually adds noise to the input, and the reverse process learns to denoise step by step. Forward Process Noise is introduced incrementally into the original vector in steps. Let x0 = xi denote the original data and xt the noisy version, where = 1, , . The Markov noise-addition 1 βtxt1 ixt1 process is defined as q(xt ) = (xt , βtI), where Gaussian noise is injected in i; each step, βt follows predefined noise schedule, and is the identity covariance matrix. nice property is that we can directly sample xt from the original data xi through: where αt = (cid:81)t i=1(1 βi), and ϵ (0, I). xt = αtxi + 1 αtϵ (2) Reverse Process The diffusion head is trained to denoise the data step by step to recover the original vectors, which is parameterized by probabilistic model pθ(xt1 i, hi). DDPM learns model ϵθ(xt in the t-th step, conditioning on the Transformer state hi. The model parameters are learned by minimizing the following loss: xt i, t, hi) to estimate the noise ϵ (as described in Equation (2)) of xt LDiff(xi, hi) = Exi,t,ϵ ϵ ϵθ(xt i, t, hi) 2 (3) where ϵ is the actual Gaussian noise. Head Architecture We use lightweight neural network as ϵθ() in Equation (3), which is residual architecture incorporating pre-RMSNorm [ZS19] and feedforward networks [LTL+24]. The network input is vector that contains noise. The output is the predicted noise ϵθ(). We also utilize AdaLN-Zero [PX23] which conditions on both the timestep and the Transformer output hi. This head processes noised continuous vector and predicts the corresponding noise. Inference The Transformer state hi is used as the condition for diffusion head. The diffusion process iteratively denoises data. At first, vector of pure Gaussian noise xT is given. In each step, the predicted noise ϵθ(xt i, t, hi) is used to produce xt1 from xt, which also considers the noise schedule for scaling [HJA20]. In our experiments, we utilize DPM-Solver [LZB+22a, LZB+22b] to accelerate the denoising process, significantly reducing the number of inference steps compared to the training phase. 2.2 Model Training and Inference Training During training, we compute the token-level loss for training sequences. For discrete data, we use the standard language modeling objective to maximize the likelihood of data. Specifically, the loss is computed as LLM = (cid:80) x,i log Pd(xix<i), where the prediction probability is presented in Equation (1). For continuous data, the loss function LDiff described in Equation (3) is used. The training objective is to minimize LLM + αLDiff, where α is hyperparameter. In practice, we sample multiple diffusion timesteps, typically four, for single forward pass [LTL+24]. As the diffusion head is usually lightweight, reusing the computation of the Transformer backbone improves training efficiency while introducing minimal overhead. Inference The decoding process is similar to that of standard causal Transformers, i.e., predicting the next token based on the generation history that has come before it. The tokens are produced following Equation (1). Notice that the Transformer backbone is computed in single pass, and only the lightweight diffusion head requires multiple denoising steps. In addition, we use special tokens to indicate the switch between the language modeling head and the diffusion head. For instance, we use <BOD> to denote the beginning of the diffusion head usage, and <EOD> to indicate the switch back to the language modeling head. 4 Figure 3: Compared to variational autoencoder (VAE), σ-VAE uses fixed variance for the latent space. It avoids variance collapse and makes LatentLM more robust to exposure bias during autoregressive generation. In our method, σ is scalar that is sampled from (0, Cσ) for each example. 2.3 Latent Vector Representation of Continuous Data The tokenizer compresses continuous data into latent vectors. It is based on variational autoencoder (VAE) [KW14], which encodes the input data into latent space and then decodes it back to the original space. Let denote the continuous input and the compressed vector representations. VAEs maximize the evidence lower bound of log-likelihood log p(x) via: maximize Eqϕ(zx) [log pψ(xz)] DKL [qϕ(zx) p(z)] where the encoder qϕ(zx) encodes input to latent vectors z, the decoder pψ(xz) reconstructs data by conditioning on z, and the KL term encourages that the latent space follows Gaussian prior distribution. (4) Because autoregressive generation introduces sampling uncertainty, the representation variance of the latent space affects the performance of next-token diffusion. Larger variance of latent representation makes the model more robust to exposure bias during inference [TEM23], as confirmed in Figure 6. However, for vanilla VAEs, the variance of some channels tends to collapse, which harms autoregressive modeling. In this work, we propose σ-VAE to prevent variance collapse by enforcing fixed variance in the latent space. The reconstruction pass is computed as: µ = Encoderϕ(x) = µ + σ ϵ, where ϵ (0, 1), σ (0, Cσ) ˆx = Decoderψ(z) (5) where σ is scalar, Cσ is hyperparameter, Encoderϕ() and Decoderψ() are learnable models. The input is fed into the encoder to obtain µ. The re-parameterization trick is used to make follow the Gaussian distribution. The variance σ is fixed across channels, and is sampled from (0, Cσ) for each example. It allows us to manipulate the latent space to better align with the expectation of autoregressive models. Then is fed into the decoder for reconstruction. According to Equation (4), the training objective of σ-VAE is: minimize ˆx x2 2 + β µ2 2 (6) where the first term is the reconstruction error, and the hyperparameter β controls the trade-off between reconstruction quality and adherence to the prior distribution [HMP+16]."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate LatentLM through multiple dimensions to thoroughly assess its effectiveness and scalability. We conduct experiments on various types of tasks and modalities as follows: Section 3.1: Image Generation2 Category Image 2 The code and pretrained weights are available at https://aka.ms/next-token-diffusion. 5 Type Model #Params #Epochs FID IS Non-Causal-Masking Generation"
        },
        {
            "title": "Masked Generative",
            "content": "LDM-4 [RBL+22] DiT-XL/2 [PX23] U-ViT-H/2 [BNX+23a] MaskGIT [CZJ+22] MAR-L [LTL+24] Causal-Masking Generation Causal-Discrete Causal-Continuous VQGAN [ERO21] ViT-VQGAN [YLK+21] LlamaGen-XL [SJC+24] LlamaGen-XXL [SJC+24] GIVT-Causal-L+A [TEM23] LatentLM-L (This Work) 400M 675M 501M 227M 479M 1.4B 1.7B 775M 1.4B 1.67B 479M 400 400 300 240 240 300 300 500 400 3.60 2.27 2.29 4.02 1.78 5.20 3.04 2.62 2.34 2.59 2. 247.7 278.2 263.9 355.6 296.0 280.3 227.4 244.1 253.9 253.8 Table 1: Image generation results on ImageNet [DDS+09]. We evaluate FID [HRU+17] and IS [SGZ+16]. LatentLM achieves competitive performance, especially compared with other causalmasking image generation models. Section 3.2: Multimodal Large Language Models 1) Interleaved Image-Text Data; 2) Text Image; 3) Image Text; 4) Text Section 3.3: Text-to-Speech Synthesis Speech Prompt + Text Speech 3.1 Image Generation: Scalable Autoregressive Modeling The image generation experiments are conducted on ImageNet [DDS+09]. Given category, the goal is to generate the corresponding images. First, we systematically benchmark our model against state-of-the-art baselines to demonstrate the advantages of next-token diffusion. We also investigate the scalability of our approach by evaluating it with larger model sizes and higher resolutions. Furthermore, we compare the design choices of σ-VAE tokenizers. Finally, we assess the inference efficiency to highlight the practical deployment benefits of our method. 3.1.1 System Evaluation Setup We scale up model size and number of training steps. We set the Transformers hidden size to 1024 and the number of layers to 32. The intermediate dimension of feedforward networks is 2730. The diffusion head has six layers. We use the AdamW [LH19] optimizer with β = (0.9, 0.98). We use cosine learning rate schedule with the maximal value of 5e-4 and 100 warmup steps. The weight decay is set to 0.1. We train models with 250,000 steps with batch size of 2048. The number of training epochs is about 400. Classifier-free guidance [HS22] is set to 1.65. As shown in Table 1, the model configurations have been aligned with those of previous models to ensure fair comparisons. Table 1 presents comprehensive comparison between LatentLM and various image generation methods. These methods can be categorized into two main groups: (1) non-causal-masking models, including image-level diffusion models (LDM [RBL+22], DiT [PX23], U-ViT [BNX+23a]) and masked generative models (MaskGIT [CZJ+22], MAR [LTL+24]); and (2) causal-masking models, comprising discrete-token generation approaches (VQGAN [ERO21], ViT-VQGAN [YLK+21], LlamaGen [SJC+24]) and continuous autoregressive generation methods (GIVT-Causal [TEM23]). Results Table 1 shows that LatentLM achieves competitive performance compared to previous work. Notice that non-causal-masking models typically require iterative forward computation during inference. Consequently, the inference FLOPs of non-causal-masking models tend to be larger due to multiple forward passes. Moreover, models using continuous representations typically outperform those using discrete code, even though LatentLM-L has fewer parameters. Among the methods, 6 Figure 5: Samples of LatentLM trained on ImageNet. The resolution is 384384. The image is generated by models described in Section 3.1.2. MAR [LTL+24] and GIVT [TEM23] are the most relevant. In comparison, MAR uses bidirectional Transformer to implement masked autoregressive modeling, instead of causal Transformer, which renders MAR unable to reuse key-value caches for multiple forward passes. Furthermore, unifying MAR and language modeling in multimodal models remains challenging due to their distinct modeling approaches. In contrast, Section 3.2 shows that our approach can be naturally applied to multimodal large language models. In addition, GIVT directly predicts latent vectors of VAEs with Gaussian mixture models. The main difference is that LatentLM integrates diffusion into causal Transformers, which tends to offer more powerful modeling expressivity. The results also indicate that our approach outperforms GIVT with smaller model size and fewer training epochs. 3.1.2 Scalability We compare the scalability properties of Diffusion Transformer (DiT) [PX23] and LatentLM, in terms of model size, and image resolution. Setup In order to be consistent with LatentLM, we also augment DiT with RMSNorm [ZS19] and SwiGLU [RZL17, Sha20]. All models were trained with 75,000 steps, i.e., approximately 120 epochs, for scaling experiments. Classifier-free guidance [HS22] is set to 1.75 during inference. Detailed hyperparameters are presented in Appendix A. Scaling Model Size As shown in Figure 4, we trained models of varying sizes, i.e., 455M, 1.03B, 1.82B, 3.68B. LatentLM consistently outperforms DiT models. The results demonstrate our approachs effective scaling properties in terms of model size. Figure 4: Scaling curves of DiT and LatentLM. FID [HRU+17] consistently becomes better with larger model size. Scaling Resolution As shown in Table 2, we conduct experiments at resolution of 384, training 1.82B model for 100,000 steps. The results reveal significant improvements over the 256-pixel resolution when using classifier-free guidance [HS22]. The improvement stems from the richer details and additional information captured in the tokenizer with higher resolutions. Moreover, increasing resolution leads to longer sequences, which scales the decoding computation up. Resolution FID-50k 256 256 384 384 3.19 2.51 Table 2: FID [HRU+17] of scaling up image resolution. 7 Figure 6: Image generation results of Diffusion Transformer (DiT) [PX23] and LatentLM on ImageNet. We report FID [HRU+17] scores (lower is better) in the settings of different tokenizer variance and CFG [HS22] scale. The stars represent the tokenizers tuned for previous image-level diffusion models [RBL+22], which are ineffective for LatentLM. The results indicate that LatentLM favors tokenizers with larger variances. 3.1.3 Effects of Tokenizer As shown in Figure 6, we analyze the effects of σ-VAE tokenizers with various configurations. We evaluate their performance in both the DiT and LatentLM frameworks. Specifically, we train the σ-VAE tokenizers with different variance. To simplify the analysis, we use fixed variance values σ, rather than sampling them from (0, Cσ). Setup We train σ-VAE with perceptual loss [ZIE+18, JAFF16] and GAN loss [IZZE17], following [RBL+22, ERO21]. We initialize the encoder from the base-size BEiT-3 [WBD+23] checkpoint, and append randomly initialized decoder. Both encoder and decoder have 12 Transformer layers, totaling 172 million parameters. The image patch size is 16. We train tokenizers on the ImageNet training set [DDS+09] with 200 epochs. The batch size is 256. The optimizer is AdamW [LH19] with β = (0.0, 0.99) and learning rate of 3e-4. The weight decay is set to 0.01. We apply layer-wise learning rate decay [BDPW22] of 0.65 on the encoder. For DiT and LatentLM training, we follow the training recipes of [PX23]. More training details are presented in Appendix B. Figure 6 presents the FID-50K scores of DiT and LatentLM using tokenizers with different variance. The stars in the figure represent tokenizers that were tuned for previous latent diffusion models [RBL+22], which usually have small variance, i.e., being more like an autoencoder instead of VAE. The other dots are σ-VAE with fixed variance. We summarize the findings as follows: The tokenizers tuned for previous image-level diffusion models are ineffective for LatentLM. For LatentLM, the stars (in Figure 6) perform significantly worse than the others that have larger tokenizer variances. The results indicate that directly adopting tokenizer configurations from previous diffusion models is suboptimal for LatentLM. The tokenizers with small variances are not robust to autoregressive error [TEM23]. LatentLM favors tokenizers with larger variances. For the example without classifier-free guidance (i.e., CFG=1.0 in Figure 6), LatentLM improves monotonically with increased variance. In contrast, the choice of variance is not critical for DiT models. The analysis highlights the advantage of σ-VAE, whose variance is easily controllable. So we recommend to use re-trained σ-VAE as tokenizers for LatentLM, rather than directly using previous ones. 3.1.4 Inference Efficiency As shown in Figure 7, we investigate the inference capabilities of LatentLM by examining the effects of model size and batch size. We perform efficiency comparisons using 20 diffusion inference steps on single H100 GPU. 8 (a) Throughput with increasing model sizes. (b) Throughput with increasing batch sizes. Figure 7: We compare the inference throughput of Diffusion Transformer (DiT) [PX23] and LatentLM in the settings of different model size and batch size. GQA stands for group-query attention [ALTdJ+23]. First, we evaluate models ranging from 1B to 3.8B parameters with fixed batch size of 128. Figure 7a shows that DiTs throughput decreases significantly with larger model size. Because DiT has to iteratively perform multiple forward passes, it incurs higher computational costs. For the largest model with 3.8B parameters, LatentLM achieves 2.47 increase in throughput, demonstrating its scalability advantages. As presented in Figure 7b, we then assess the 1.82B models with varying batch sizes. As the batch size increases, the throughput of LatentLM scales favorably with DiT. In addition, groupquery attention (GQA) [ALTdJ+23] further improves throughput. For batch size of 256, our approach achieves 2.84 throughput improvement. The results indicate that LatentLM benefits from significantly reduced FLOPs compared to image-level diffusion models, particularly at larger batch sizes. Additional experiments on other model sizes are provided in Appendix C. 3.2 Multimodal LLMs: Unified Understanding and Generation We train multimodal large language models with LatentLM for unified understanding and generation. In this section, we focus on vision-language models. By unifying next-token prediction and diffusion, the model can seamlessly handle interleaved image-text data, text-only data, and image-text pairs. The proposed method simplifies the multimodal training and inference processes, allowing to learn in context (e.g., few-shot), follow multimodal instructions, and perform multimodal dialogue. Moreover, unified modeling enables new capabilities. For example, we can edit or generate images by conditioning on text and multiple input images. 3.2.1 Training Setup Training Data We use three types of data in the training stage: text-only data, image-text pair data, and interleaved text-image data. The mix-up ratio is 2:1:1. The data sources are described as follows: Text-Only Data The text training corpus follows [SDZ+24], including Common Crawl, RefinedWeb [PMH+23], and StarCoder [LAZ+23]. Image-Text Pairs We follow [HDW+23, PWD+23] to construct the paired data, i.e., English LAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22], and Conceptual Captions [SDGS18, CSDS21]. Interleaved Image-Text Data We use the same interleaved multimodal documents as in [HDW+23, PWD+23]. The web pages are filtered from Common Crawl archives. The documents are interleaved with text and image. Configuration We train 1.3B-size Transformer as the backbone. We set the hidden size to 2048. The number of layers is 24. The training sequence length is 4096. We use tiktoken-cl100k_base as the text tokenizer. The batch size is 4M tokens. We use the AdamW [LH19] optimizer with Model VQ-MLLM Transfusion LatentLM Text Text-to-Image Image-to-Text Valid PPL FID CLIP MS-COCO VQAv2 2.79 2.74 2.73 16.92 16.10 14.54 29.33 28.66 28.75 37.4 43.4 54.5 30.19 35.36 38.72 Table 3: Results of multimodal large language models on text language modeling, image-to-text, and text-to-image generation. We compare with Transfusion [ZYB+24] and vector quantized models (VQ-MLLM; i.e., using discrete code to represent images). PPL is perplexity. CLIP [RKH+21] score measures the similarity. We report CIDEr [VLZP15] score for MS-COCO [LMB+14a] and accuracy for VQAv2 [GKSS+17]. (a) Text-to-image FID [HRU+17]. (b) Image-to-text validation perplexity. Figure 8: We scale up the number of training tokens for multimodal large language models. LatentLM outperforms vector quantized models (VQ-MLLM) and Transfusion [ZYB+24] for both text-to-image and image-to-text generation. The FID scores are evaluated on MS-COCO [LMB+14a]. β = (0.9, 0.98). The maximal learning rate is 3e-4 with 500 warmup steps. The total schedule is set to 1T tokens. We train the model with 50k steps (i.e., 200B tokens) for comparison. More hyperparameters are detailed in Appendix D. 3.2.2 Results We compare LatentLM with Transfusion [ZYB+24], and vector quantized models (VQ-MLLM; i.e., the models using vector quantized image tokenizers). Specifically, Transfusion shares Transformer weights for autoregressive language modeling and image-level diffusion, which uses bidirectional iterative denoising for images and causal masking for text. Moreover, VQ-MLLM uses VQ-VAE [vdOVK17, ERO21] as the tokenizer for images, where images are compressed to discrete code. We use the VQ-VAE tokenizer open-sourced by LlamaGen [SJC+24] in VQ-MLLM. We use the same training configuration and tokenizer settings for comparison. To align the number of parameters, we use 6-layer ViT as the image head of Transfusion. Language Modeling Table 3 presents the evaluation results on language modeling, text-to-image generation, and multimodal understanding. First, LatentLM achieves better perplexity in language modeling. The results indicate that our method tends to better share knowledge between modalities with less conflicts. The similarity between next-token prediction and next-token diffusion also benefits the unified modeling. Text-to-Image Generation Then we evaluate text-to-image generation on MS-COCO [LMB+14b]. Table 3 shows that LatentLM achieves lower FID scores, i.e., better generation quality. The trend is also consistent with Table 1, where Transfusion is aligned with DiT, and VQ-MLLM with LlamaGen. In addition, Figure 8a presents the scaling curves in terms of the number of training tokens, where LatentLM consistently achieves better FID scores. It is worth noting that the performance of VQ10 (a) majestic mountain range covered in snow. (b) city street nated by lights. illumi- (c) crystal rounded by autumn trees. lake sur- (d) small house in wooden at sunset. Figure 9: Text-to-image examples of LatentLM. MLLM seems saturated compared to the other methods. Figure 9 also shows several text-to-image samples of LatentLM. Image-to-Text Generation Table 3 reports image captioning on MS-COCO [LMB+14b] and visual question answering on VQAv2 [GKSS+17]. LatentLM achieves better performance in both multimodal understanding tasks. Compared to VQ-MLLM, the continuous representations used by Transfusion and LatentLM are more lossless than discrete code. Compared to Transfusion, LatentLM keeps training and inference consistent, rather than adding noise to input images during training. Figure 8b presents the text perplexity on the image-to-text validation data. The results are also consistent with those reported in Table 3. 3.3 Text-to-Speech Synthesis: Higher Compression Ratio, Fewer Decoding Steps We apply LatentLM to text-to-speech synthesis (TTS). Due to continuous representations, σ-VAE achieves superior reconstruction results with significantly higher compression ratio and lower frame rate than previous speech tokenizers [DCSA22, KSL+23, SD24, JJC+24, DMO+24]. LatentLM outperforms the state-of-the-art VALL-E 2 [CLZ+24] model on both speaker similarity score and robustness while requiring 10 fewer decoding steps. 3.3.1 Training Setup Considering the variable-length nature of speech data, our speech tokenizer employs convolutional architecture that supports streaming encoding and decoding. Specifically, σ-VAE for speech consists of convolutional encoder, continuous VAE quantizer, and convolutional decoder. The encoder comprises multiple stages and downsampling layers organized in hierarchical structure. Each stage includes several ConvNeXt blocks [LMW+22], where the original 2D convolution is replaced with 1D causal convolution. For compression ratios of 1600, 3200, and 6400, the downsampling layer reduces the input waveform by factors of [2, 4, 5, 5, 8], [4, 4, 5, 5, 8], and [4, 5, 5, 8, 8], respectively. Each time the downsampling layer is applied, the number of channels doubles, starting from 32 and increasing to 1024. The encoder contains around 120 million parameters in total. The decoder is mirror of the encoder. As for the discriminator, we use the multi-period discriminator [KKB20] and the complex STFT discriminator in DAC [KSL+23]. The hidden size of LatentLM is 1024, with 24 layers and 16 attention heads. The intermediate FFN dimension is set to 4096. The diffusion head contains three layers of feedforward networks. We use the same Transformer architecture as VALL-E 2 [CLZ+24] for comparison. Additional hyperparameters are described in Appendix E. 3.3.2 Training Data Tokenizer We train σ-VAE on large and diverse corpus that includes speech, audio, and music. For speech, we use the clean speech subset from DNS Challenge 4 [DGC+22] and all splits from the Common Voice v7 dataset [ABD+20]. For audio, we use the FSD50K dataset [FFP+21], along with the balanced and unbalanced splits from AudioSet [GEF+17]. For music, we use the 11 System Frame Rate Length/s Ref Utterance as Prompt 3s Prefix as Prompt SIM WER-C WER-H SIM WER-C WER-H Ground Truth VALL-E 2 [CLZ+24] Voicebox [LVS+23] MELLE [MZL+24] LatentLM LatentLM LatentLM - 75 100 62 15 7.5 3.75 0.779 0.643 0.662 0.625 0.697 0.656 0.598 1. 1.5 - 1.5 1.2 1.2 1.7 2.2 2.4 1.9 2.1 1.8 1.7 2.3 0. 0.504 0.593 0.508 0.571 0.532 0.467 1.6 1.6 - 1.5 1.4 1.6 3.1 2. 2.3 2.0 2.0 2.0 2.3 4.5 Table 4: LatentLM outperforms previous systems on zero-shot speech synthesis in both settings. Moreover, the number of decoding steps is much less than others, achieving faster inference speed. The results are reported on LibriSpeech test-clean set. The WER-H and SIM results of VALL-E 2 using 3s prefix as prompt are from [MZL+24]. MUSDB dataset [RLS+17] and the Jamendo dataset [BWT+19]. All the data are resampled to 24kHz monophonic format. TTS Model We utilize Libriheavy corpus [KYY+24] as training data following VALL-E 2 [CLZ+24]. This corpus is labeled version of the Librilight corpus [KRZ+20], which features 50,000 hours of speech from approximately 7,000 different speakers, sourced from open-access English audiobooks associated with the LibriVox project3. 3.3.3 Evaluation Metrics We evaluate our speech tokenizer using several automatic metrics, including: Mel Distance, which measures the distance between log Mel spectrograms as configured in DAC [KSL+23]; PESQ-WB [RBHH01], an intrusive metric for speech quality by comparing perceptual differences; STOI [THHJ10], which assesses speech intelligibility through short-time segment correlation; VISQOL [CLS+20], perceptual quality metric based on spectral similarity; UTMOS [SXN+22], reference-free mean opinion score for audio quality; Speaker Similarity (SIM), measured using WavLM-TDNN [CWC+22]; and Word Error Rate (WER), calculated using both ConformerTransducer [GQC+20] (WER-C) and HuBERT-Large [HBT+21] (WER-H) models. 3.3.4 System Evaluation Table 4 presents zero-shot text-to-speech (TTS) results on the LibriSpeech test-clean dataset. We evaluate the synthesis quality under two distinct settings: (1) using reference utterance from the same speaker as the prompt, and (2) evaluating speech continuation by using the first 3 seconds of speech as the prompt. Our model, operating at frame rate of 15 (i.e., generating 1 second of speech in 15 autoregressive steps), surpasses previous state-of-the-art methods when using same-speaker reference utterance as the prompt. LatentLM with frame rate of 7.5 achieves superior performance compared to the neural codec language model VALL-E 2 [CLZ+24], while requiring an order of magnitude (10) fewer autoregressive inference steps. Moreover, LatentLM eliminates the need for the non-autoregressive (NAR) model employed in VALL-E 2, resulting in improved computational efficiency. Even at lower frame rate of 3.75, LatentLM maintains competitive performance. The higher compression ratio reduces the sequence length, which in turn greatly accelerates the decoding speed. 3.3.5 Evaluating the Quality of Tokenizers Table 5 compares σ-VAE and other codec models on the LibriTTS test-other set. σ-VAE achieves better reconstruction quality in compression ratio of 1600 compared to Encodec [DCSA22] (40), DAClow [SD24] (160), WavTokenizer [JJC+24] (320), and Mimi [DMO+24] (480). Notably, as we further increase the compression ratio, the reconstruction quality does not deteriorate significantly. 3https://librivox.org 12 Tokenizer Nq Frame Rate Comp. Ratio Mel Dist. PESQ STOI VISQOL UTMOS Tokenizers with lower compression ratio 75 75 75 75 75 75 12.5 Encodec [DCSA22] DAC [KSL+23] Encodec [DCSA22] DAC [KSL+23] DAClow [SD24] DAClow [SD24] Mimi [DMO+24] 32 32 8 8 4 2 8 Tokenizers with higher compression ratio WavTokenizer [JJC+24] Mimi [DMO+24] WavTokenizer [JJC+24] σ-VAE32 σ-VAE64 σ-VAE128 75 12.5 40 15 7.5 3.75 1 4 1 1 1 1 10 10 40 40 80 160 240 320 480 600 1600 3200 0.823 0.355 0.987 0.707 0.753 0.916 0.987 0.871 1.458 1.037 0.813 0.798 0.852 3.591 4.424 2.687 3.329 3.107 2.269 3.217 2.266 1.568 1.670 2.724 2.756 2.533 0.962 0.987 0.925 0.941 0.938 0.896 0.946 0.891 0.826 0.834 0.926 0.929 0. 4.536 4.914 4.258 4.485 4.391 3.981 4.332 4.120 3.390 3.782 4.268 4.289 4.165 3.195 3.469 2.656 3.133 3.453 3.297 3.375 3.432 2.652 3.053 3.491 3.505 3.460 Table 5: The σ-VAE tokenizers obtain competitive reconstruction quality while having high compression ratio. We report results on the LibriTTS test-other set. Nq represents the number of quantizers. We define the compression ratio as the audio sample rate divided by Nq and the frame rate. σ-VAE32 denotes that the latent dimension of the tokenizer is 32. Compression Ratio Frame Rate Latent Dimension σ-VAE Reconstruction Zero-Shot TTS Mel Dist. SIM WER-C SIM WER-C 640 1600 1600 37.5 15 15 16 16 32 0.929 1.080 0.950 0.866 0.700 0. 1.9 2.7 1.9 0.655 0.545 0.661 1.4 1.6 1.5 Table 6: Ablation results of different σ-VAE compression ratios and latent dimensions. We report tokenizer reconstruction quality and zero-shot speech synthesis. At compression ratio of 6400, the resulting sequence length when used in language model is already comparable to BPE tokenization [SHB15], approaching 1:1 ratio. 3.3.6 Ablation Studies Compression Ratio and Latent Dimension We find that increasing the latent dimension enables the model to achieve higher compression ratio and lower frame rate. Table 6 presents the σ-VAE reconstruction and zero-shot speech synthesis results with different compression ratios and latent dimensions. We report the in-domain Mel distance performance of σ-VAE, along with the speaker similarity score and WER-C for tokenizer reconstruction and zero-shot speech generation on the LibriSpeech test-clean set. We use 12-layer Transformer model for the TTS ablation studies. If the latent dimension remains unchanged, higher compression ratio leads to decrease in reconstruction performance and TTS speaker similarity score. However, by increasing the latent dimension of σ-VAE, we can compensate for this loss, allowing our model to use higher compression ratio and lower frame rate. Our model can generate 1 second of speech using significantly fewer autoregressive inference steps, compared to VALL-E 2. CFG Scale Figure 10a illustrates the zero-shot speech synthesis results using classifier-free guidance (CFG) [HS22]. When the CFG scale is set to 1, CFG is not applied. The use of classifier-free guidance significantly enhances the models performance. Furthermore, we find that setting the CFG scale to 4 yields the best results. Inference Sampling Step Figure 10b presents the results of zero-shot speech synthesis using different inference sampling steps of the diffusion head. We set the CFG scale to 4 for the ablation studies. More sampling steps require more inference time. We find that sampling step of 3 yields 13 (a) Results using different CFG scales. (b) Results using different sampling steps. Figure 10: Ablation results of different CFG [HS22] scales and inference sampling steps. We report zero-shot speech synthesis results. competitive results, and increasing it to 5 leads to further improvement. When the sampling step is increased further, the results improve only slightly. Using sampling step of 5 allows the model to achieve strong performance while maintaining fast inference speed."
        },
        {
            "title": "4 Conclusion and Future Work",
            "content": "The work can be advanced from the following perspectives: Latent Multimodal Reasoning The proposed unified modeling facilitates complex multimodal reasoning tasks that require simultaneous understanding of multiple modalities. For instance, self-reflection can automatically correct produced images, which requires the multimodal language model to understand the generated image without encoding it again. Moreover, multimodal-native reasoning enables the model to track the search states via latent vectors, for example, step-by-step plotting the planned trajectory on the image of input map. Video Generation and World Modeling The autoregressive nature of LatentLM fits well with video data, which shows particular promise in maintaining temporal consistency and spatial coherence. Moreover, LatentLM can perform planning by generating scripts and videos in an interleaved way, making it particularly suitable for long-video generation. The approachs higher compression ratio compared to traditional quantization methods enables efficient generation without sacrificing visual quality. In addition, we can integrate actions to control and simulate the interactive environment, which can be used as world model. Cross-Modal Transfer Between Speech and Text Because of the high compression ratio of continuous representations, we can use similar tokenization granularity for speech and text data, which tends to ease knowledge transfer across modalities. Similarly, multilingual pretraining enables zero-shot cross-lingual transfer, where training on English benefits other languages. It is useful to achieve seamless code switch between speech and text and opens new opportunities for user interface. Embodied AI and Robot Action By representing robot actions as continuous data, we enable end-to-end learning of robot behaviors that can be seamlessly integrated with language instructions, visual observations, and other sensory inputs. The unified framework simplifies the development of robots that can understand commands in natural language, learn from demonstrations, and adapt to new environments while maintaining consistent internal representation across all modalities. Text Data We can also apply latent language modeling to text data, rather than predicting words as discrete tokens. The VAE tokenizer tends to achieve higher compression ratio than previous discrete tokenizers. The shorter sequence length improves the generation efficiency by reducing the autoregressive steps."
        },
        {
            "title": "Acknowledgement",
            "content": "We would like to acknowledge Ben Huntley for maintaining the GPU cluster, and Zhikang Niu for the help of training speech tokenizer. We implement DiT [PX23] based on https://github. com/facebookresearch/DiT. The experiments on multimodal large language models utilize the curated data from KOSMOS [HDW+23, PWD+23] and REDSTONE [CCD+24]. The implementation is based on the TorchScale [MWH+22] library."
        },
        {
            "title": "References",
            "content": "[ABD+20] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber. Common voice: massively-multilingual speech corpus. In Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pages 42114215, 2020. [ALTdJ+23] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [BDPW22] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image Transformers. In International Conference on Learning Representations, 2022. [BNX+23a] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [BNX+23b] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In Proceedings of the 40th International Conference on Machine Learning, pages 16921717, 2023. [BPK+22] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022. [BWT+19] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. The mtg-jamendo dataset for automatic music tagging. In ICML, 2019. [CCD+24] Yaoyao Chang, Lei Cui, Li Dong, Shaohan Huang, Yangyu Huang, Yupan Huang, Scarlett Li, Tengchao Lv, Shuming Ma, Qinzheng Sun, et al. RedStone: Curating general, code, math, and QA data for large language models. arXiv preprint arXiv:2412.03398, 2024. [CLS+20] Michael Chinen, Felicia SC Lim, Jan Skoglund, Nikita Gureev, Feargus OGorman, and Andrew Hines. Visqol v3: An open source production ready objective speech and audio metric. In 2020 twelfth international conference on quality of multimedia experience (QoMEX), pages 16. IEEE, 2020. [CLZ+24] Sanyuan Chen, Shujie Liu, Long Zhou, Yanqing Liu, Xu Tan, Jinyu Li, Sheng Zhao, Yao Qian, and Furu Wei. VALL-E 2: Neural codec language models are human parity zero-shot text to speech synthesizers. CoRR, abs/2406.05370, 2024. [CSDS21] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35583568, 2021. [CWC+22] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518, 2022. 15 [CZJ+22] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [DCSA22] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. [DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [DGC+22] Harishchandra Dubey, Vishak Gopal, Ross Cutler, Sergiy Matusevych, Sebastian Braun, Emre Sefik Eskimez, Manthan Thakker, Takuya Yoshioka, Hannes Gamper, and Robert Aichner. Icassp 2022 deep noise suppression challenge. In ICASSP, 2022. [DMO+24] Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. Technical report, Kyutai, September 2024. [ERO21] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for highresolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [FFP+21] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra. Fsd50k: an open dataset of human-labeled sound events. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:829852, 2021. [GEF+17] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. [GKSS+17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. [GQC+20] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolutionaugmented transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020. [HBT+21] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE ACM Trans. Audio Speech Lang. Process., 29:34513460, 2021. [HDW+23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023. [HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [HMP+16] Irina Higgins, Loïc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with constrained variational framework. In International Conference on Learning Representations, 2016. 16 [HRU+17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In Proceedings of the 31st International Conference on Neural Information Processing Systems, page 66296640, 2017. [HS22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [IZZE17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. [JAFF16] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694711. Springer, 2016. [JJC+24] Shengpeng Ji, Ziyue Jiang, Xize Cheng, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Ruiqi Li, Ziang Zhang, Xiaoda Yang, et al. Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling. arXiv preprint arXiv:2408.16532, 2024. [KKB20] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial In Advances in neural networks for efficient and high fidelity speech synthesis. information processing systems, volume 33, pages 1702217033, 2020. [KRZ+20] Jacob Kahn, Morgane Rivière, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, PierreEmmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, Tatiana Likhomanenko, Gabriel Synnaeve, Armand Joulin, Abdelrahman Mohamed, and Emmanuel Dupoux. Libri-light: benchmark for ASR with limited or no supervision. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 76697673. IEEE, 2020. [KSL+23] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved rvqgan. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 2798027993, 2023. [KW14] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, 2014. [KYY+24] Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Yifan Yang, Liyong Guo, Long Lin, and Daniel Povey. Libriheavy: 50, 000 hours ASR corpus with punctuation In IEEE International Conference on Acoustics, Speech and casing and context. Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024, pages 1099110995. IEEE, 2024. [LAZ+23] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, and StarCoder Team. StarCoder: may the source be with you! ArXiv, abs/2305.06161, 2023. [LCBH+22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [LMB+14a] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755, 2014. 17 [LMB+14b] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft COCO: Common objects In Computer VisionECCV 2014: 13th European Conference, Zurich, in context. Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [LMW+22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. [LTL+24] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [LVS+23] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, and Wei-Ning Hsu. Voicebox: Text-guided multilingual universal speech generation at scale. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [LZB+22a] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPMSolver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. [LZB+22b] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPMSolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [MWH+22] Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. TorchScale: Transformers at scale. CoRR, abs/2211.13184, 2022. [MZL+24] Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, Helen Meng, and Furu Wei. Autoregressive speech synthesis without vector quantization. CoRR, abs/2407.08551, 2024. [PMH+23] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimée Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data, and web data only. ArXiv, abs/2306.01116, 2023. [PWD+23] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. ArXiv, abs/2306.14824, 2023. [PX23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [RBHH01] Antony Rix, John Beerends, Michael Hollier, and Andries Hekstra. Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. In 2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221), volume 2, pages 749752. IEEE, 2001. [RBL+22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684 10695, 2022. 18 [RKH+21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. [RLS+17] Zafar Rafii, Antoine Liutkus, Fabian-Robert Stöter, Stylianos Ioannis Mimilakis, and Rachel Bittner. The musdb18 corpus for music separation, 2017. [RPG+21] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021. [RZL17] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Swish: self-gated activation function. arXiv: Neural and Evolutionary Computing, 2017. [SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. [SD24] Slava Shechtman and Avihu Dekel. Low bitrate high-quality rvqgan-based discrete speech tokenizer. In Annual Conference of the International Speech Communication Association, 2024. [SDGS18] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 25562565. Association for Computational Linguistics, 2018. [SDZ+24] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [SGZ+16] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS16, page 22342242, 2016. [SH22] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [Sha20] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. [SJC+24] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [SVB+21] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [SXN+22] Takaaki Saeki, Detai Xin, Wataru Nakata, Tomoki Koriyama, Shinnosuke Takamichi, and Hiroshi Saruwatari. Utmos: Utokyo-sarulab system for voicemos challenge 2022. arXiv preprint arXiv:2204.02152, 2022. [Tea24] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. ArXiv, abs/2405.09818, 2024. 19 [TEM23] Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinitevocabulary transformers. arXiv preprint arXiv:2312.02116, 2023. [THHJ10] Cees Taal, Richard Hendriks, Richard Heusdens, and Jesper Jensen. shorttime objective intelligibility measure for time-frequency weighted noisy speech. In 2010 IEEE international conference on acoustics, speech and signal processing, pages 42144217. IEEE, 2010. [TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [TPK24] Michael Tschannen, André Susano Pinto, and Alexander Kolesnikov. JetFormer: An autoregressive generative model of raw images and text, 2024. [TYZ+23] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. arXiv preprint arXiv:2305.11846, 2023. [vdOVK17] Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Neural Information Processing Systems, 2017. [VLZP15] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, pages 45664575, 2015. [WBD+23] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as foreign language: Beit pretraining for vision and vision-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1917519186, June 2023. [WCW+23] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language models are zero-shot text to speech synthesizers. CoRR, abs/2301.02111, 2023. [YLK+21] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. [ZIE+18] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [ZS19] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [ZYB+24] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024."
        },
        {
            "title": "A Hyperparameters for Image Generation Scaling",
            "content": "Table 7 details the hyperparameters used for Section 3.1.2, where we compare the scalability properties of Diffusion Transformer (DiT) [PX23] and LatentLM. We describe the hidden dimension, the number of layers, and the number of heads for the models. Specifically, we follow [PX23] for the DiT architecture. In addition, we augment DiT with RMSNorm [ZS19] and SwiGLU [RZL17, Sha20]. To align the number of parameters, the FFN size for DiT is set to 8 3 d, while for LatentLM, it is set to 4d. We train the models for 75,000 steps, which corresponds to approximately 120 epochs, to facilitate scaling comparisons. Size Hidden Dim. #Layers #Heads Learning Rate Medium 455M 1.03B Large 1.82B XL 3.68B 3B 1024 1536 2048 24 24 24 32 16 12 16 20 8 104 3 104 2 104 1.6 104 Table 7: Model size and hyperparameters used for the scaling experiments in Section 3.1.2."
        },
        {
            "title": "B Hyperparameters for Tokenizer Analysis",
            "content": "We present the hyperparameters used for Section 3.1.3. We follow the training recipes of [PX23] for DiT and LatentLM training. We set the hidden size to 1024. The number of layers is 24. Because LatentLM does not have AdaLN in the Transformer backbone, we adjust the intermediate FFN dimension (i.e., 2730 in DiT, and 4096 in LatentLM) to match their model size. The diffusion head has three layers of feedforward networks. We use the AdamW [LH19] optimizer with β = (0.9, 0.98). We use the cosine learning rate schedule with maximal value of 1e-4 and 100 warmup steps. The weight decay is 0.1. We train models using batch size of 256 for 200,000 steps, which is approximately equivalent to 40 epochs. We use the cosine beta schedule and v-prediction [SH22] for diffusion. We use DDPM [HJA20] with 1000 steps during training. DPM-Solver [LZB+22a, LZB+22b] with 20 steps is used during inference."
        },
        {
            "title": "C Inference Efficiency with Different Model Sizes",
            "content": "In Section 3.1.4, we compare the inference throughput for DiT and LatentLM on H100 GPU card. As shown in Figure 11, we evaluate the efficiency with various model size and batch size. The results show that LatentLMs throughput increases with larger batch size. Our approach benefits from key-value caches of causal Transformers, which avoids recomputation of history predictions. In contrast, DiTs throughput remains similar. In addition, group-query attention (GQA) [ALTdJ+23] further improves the inference efficiency of LatentLM. Another advantage is that we can directly reuse the inference infrastructure of large language models to deploy LatentLM. (a) Model Size: 1.03B (b) Model Size: 3.68B (c) Model Size: 9.35B (d) Model Size: 17.96B Figure 11: Inference throughput of various model size and batch size. GQA stands for group-query attention [ALTdJ+23]."
        },
        {
            "title": "D Hyperparameters for Multimodal Large Language Model",
            "content": "Table 8 details the hyperparameters employed for multimodal large language models, as described in Section 3.2. Params Layers Hidden size FFN size Vocab size Heads Adam β LR Batch size Warmup steps Weight decay Head Layers Values 24 2048 6144 100,288 16 (0.9, 0.98) 3 104 4M 500 0.1 6 Table 8: Hyperparameters used for multimodal large language models in Section 3.2. Hyperparameters for Text-to-Speech Synthesis Table 9 lists the hyperparameters utilized for multimodal large language models, as discussed in Section 3.3. Params Values Layers Hidden size FFN size Heads Adam β LR LR schedule Batch size Warmup steps Training steps Weight decay Head Layers 24 1024 4096 16 (0.9, 0.98) 7.5 104 cosine 5M 10k 100k 0.01 Table 9: Hyperparameters used for text-to-speech synthesis in Section 3.3."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Tsinghua University"
    ]
}