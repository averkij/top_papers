{
    "paper_title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
    "authors": [
        "Zekun Xi",
        "Wenbiao Yin",
        "Jizhan Fang",
        "Jialong Wu",
        "Runnan Fang",
        "Ningyu Zhang",
        "Jiang Yong",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles."
        },
        {
            "title": "Start",
            "content": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking Zekun Xi1,2, Wenbiao Yin2, Jizhan Fang1, Jialong Wu2, Runnan Fang1,2, Ningyu Zhang1,*, Jiang Yong2,, Pengjun Xie2, Fei Huang2, Huajun Chen1,3 1Zhejiang University 2Tongyi Lab, Alibaba Group 3Zhejiang Key Laboratory of Big Data Intelligent Computing {xizekun2023, zhangningyu}@zju.edu.cn"
        },
        {
            "title": "Code",
            "content": "5 2 0 2 6 1 ] . [ 1 1 5 7 9 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the models predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles."
        },
        {
            "title": "Introduction",
            "content": "Education is not the learning of facts, but the training of the mind to think. Albert Einstein Writing is continuous process of collecting information and thinking (Bean and Melzer, 2021). Recent advances in Large Language Models (LLMs) have demonstrated remarkable progress in machine writing such as open domain long-form generation (Liang et al., 2023; Yang et al., 2023; Zhao et al., 2024) or report generation on specific topics (Liu et al., 2018). To seek useful information, as shown in Figure 1, early attempts use Retrieval Augmented Generation (RAG) to expand new information on given topic (Gao et al., 2024; * Corresponding Author. 1 Figure 1: Previous machine writing approaches only expand new information or perspective via RAG and roleplaying. OmniThink expands knowledge boundaries through continuous reflection and exploration, attaching knowledge to an information tree and extracting it into conceptual pool to deepen understanding and uncover more in-depth content. Edge et al., 2024). However, vanilla RAG relies on fixed set of search strategies (Ram et al., 2023), which lack diversity in generation, preventing thorough exploration of the topic and resulting in fragmented and incomplete understanding of the subject (Spink et al., 1998). To address this issue, STORM (Shao et al., 2024a) and CoSTORM (Jiang et al., 2024) have proposed roleplay approach designed to expand the perspective, which means collecting information from multiple perspectives, thus broadening the information space (Shen et al., 2023; Shanahan et al., 2023; Parmar et al., 2010). Yet these approaches are still being thought within the scope of ones own role, making it difficult to generate deep content and break through ones own knowledge boundaries(Ji et al., 2025). In particular, retrieved information often lacks depth, utility and redundancy, directly affecting the quality of generated articles, resulting in shallow, repetitive, and unoriginal outputs (Skarlinski et al., 2024). Note that humans can naturally avoid such pitfalls in the writing process. This phenomenon can be explained through the theory of reflective practice, concept rooted in cognitive science (Osterman, 1990). According to this theory, human writers continuously reflect on previously gathered information and personal experiences, allowing them to reorganize, filter, and refine their cognitive framework. This process prompts writers to iteratively adjust their writing direction and mental pathways, ultimately allowing human authors to generate more profound, nuanced and original content (Bruce, 1978). Motivated by this, we propose OmniThink, new machine writing framework that emulates the human-like cognitive process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they gradually deepen their understanding of complex topics to expand knowledge boundaries. By continuously reflecting on previously retrieved information, OmniThink can determine the optimal steps for further expansion. This expansionreflection mechanism enables the dynamic adjustment of the retrieval strategies, fostering more thorough and comprehensive exploration of relevant information. Once diverse set of information has been gathered, OmniThink transitions to the stages of outline construction and article generation. This iterative thinking process leads to the production of articles of higher quality that contain higher knowledge density of useful, insightful, and original content. We evaluate OmniThink on the WildSeek datasets (Jiang et al., 2024) based on previous metrics as well as new metric, named knowledge density. Experimental results demonstrate that OmniThink enhances the knowledge density of generated articles without compromising key metrics such as coherence and depth. Human evaluations and expert feedback further underscore the potential of our approach in addressing real-world challenges in the the generation of long-form articles. To conclude, our main contributions are as follows: We propose OmniThink, novel writing framework that emulates the human-like process of iterative expansion and reflection. We propose continuous expansion and reflection approach for open-domain article generation, which improves the information desitiy of the generated text. We validate our approach through automatic and human evaluations, demonstrating its effectiveness in generating well-founded, highquality long documents. Expert feedback further reveals new challenges in producing longform documents that are both informative and contextually accurate."
        },
        {
            "title": "2 Background",
            "content": "2.1 Task Definition We focus on the task of open-domain long-form generation for machine writing, which involves retrieving information from an open domain and synthesizing it into coherent article (Fan et al., 2019; Su et al., 2022; Quan et al., 2024). Given an input topic T, the target of open-domain longform generation is to generate long article A. The current standard approach involves two major steps (Zhang et al., 2019; Zheng et al., 2023): (i) Use search engine to retrieve information = S(T) which is related to the topic T; (ii) Generate an outline = Generate(I, T) based on the retrieved information and input topic T. Finally, the article is generated using the outline, expressed as = Generate(O, I)."
        },
        {
            "title": "2.2 Revisiting Previous Methods",
            "content": "Previous works have made numerous efforts to improve the quality of open-domain long-form generation. Co-STORM (Jiang et al., 2024) introduces user-participatory roundtable discussion in step (i) to enhance the diversity of the retrieved information. STORM (Shao et al., 2024a) proposes questioning mechanism to improve the quality and relevance of the generated outlines in step (ii). Although substantial progress has been made in open-domain long-form generation, persistent challenge remains: the generated content frequently suffers from excessive repetition and lacks substantial information. We present case generated by STORM (Shao et al., 2024a) with GPT-4o as the backbone, as shown in Figure 2. In this article, the phrase AlphaFold was developed by Deep2 Mind appears multiple times, whereas it could have been stated only once in the initial mention. Figure 2: case generated by STORM using GPT4o on the topic of AlphaFold. We have marked the repeated expressions in the article regarding AlphaFold is developed by DeepMind. 2.3 Knowledge Density for the Article Previous works mostly focus on whether the article is relevant and correct, but do not consider whether the article has sufficient depth(Li et al., 2024; Que et al., 2024; Liu et al., 2024). Many generated articles contain lot of redundant information, which is very inconsistent with human writing. To address this, we introduce the Knowledge Density (KD) for the generated article, which is defined as the ratio of meaningful content to the overall volume of text (Xu and Reitter, 2017) as: KD = (cid:80)N i=1 ki U(ki) (1) where is the total number of atomic knowledge units identified within the document. The function U(ki) indicates whether the i-th unit information ki is unique. represents the total length of the text. In this formula, the numerator represents the sum of unique units of atomic knowledge extracted from long article. The denominator corresponds to the length of the article. Note that the value of the knowledge density metric lies in its ability to measure the reading cost of generated text from the perspective of information acquisition (Bovair and Kieras, 1991; Dos Santos and Mookerjee, 1993). Readers encountering low KD content often experience fatigue, frustration, or disengagement due to redundant or irrelevant details. In contrast, high-density content provides streamlined experience, enabling efficient knowledge transfer. Previous methods exhibit limited performance on the proposed KD due to the fact that the generated content in open-domain long-form generation 3 Figure 3: We divide OmniThink into three steps. During the Information Acquisition phase, OmniThink primarily forms an Information Tree and Conceptual Pool through continuous Expansion and Reflection, which serve as the foundation for subsequent outline structuring and article composition. must be based on the retrieved information. When the retrieved information is not sufficiently diverse, it often contains large amounts of redundant and repetitive content, leading to repetition and redundancy in the generated article. This leaves room for optimizing the knowledge density in open-domain long-form generation. We can address this issue by incorporating reasoning and planning during Step (i), where we process the gathered content to extract non-overlapping, high-density information(Qiao et al., 2023; Zelikman et al., 2024)."
        },
        {
            "title": "3 OmniThink",
            "content": "As shown in Figure 3, we introduce machine writing framework OmniThink, which emulates the human-like process of iterative reflection and expansion. OmniThink can be divided into three steps: Information Acquisition (3.1), Outline Structuring (3.2), and Article Composition (3.3). 3."
        },
        {
            "title": "Information Acquisition",
            "content": "To acquire diverse and comprehensive information, OmniThink emulates the human learning process, progressively deepening its understanding of the topic through iterative Expansion and Reflection. As shown in Figure 4, we illustrate the specific process of Expansion and Reflection. This iterative process culminates in the construction of an information tree , which organizes the retrieved Figure 4: The specific process of Expansion and Reflection in OmniThink. The concepts marked with the same color scheme in the diagram represent kinship relationships or progressive relationships between the concepts. ( 1 - 4) illustrate the specific process of single Expansion and Reflection cycle in OmniThink. information in structured and hierarchical manner, and conceptual pool P, which represents the LLMs current understanding of the topic at time step m. Together, these components form the foundation of article generation. Initialization The interactive process begins with the initialization of root node based on the input topic T. OmniThink first utilizes search engines, e.g., Google, or Bing, to retrieve information related to T, using the retrieved knowledge to construct the initial root node of the information tree Nr. This initial information in Nr is then organized and analyzed to form preliminary conceptual pool P0, which serves as OmniThinks foundational understanding of the topic and guides subsequent expansion and reflection processes."
        },
        {
            "title": "3.1.1 Expansion",
            "content": "At time step m, OmniThink analyzes all leaf nodes Lm = {N0, N1, . . . , Nn} of the information tree Tm. These leaf nodes are first stored in the conceptual buffer Pb, where each node is evaluated to determine if it requires further expansion. For nodes that need expansion, OmniThink uses the current conceptual pool Pm to identify areas for deeper expansion or suitable directions for expansion. For each leaf node Ni, OmniThink generates kNi sub-nodes, denoted as SUB(Ni) = {S0, S1, . . . , SkNi }, for expansion. Each sub-node represents specific aspect or subtopic identified from the current node Ni. For each sub-node, OmniThink retrieves relevant information and stores it within the respective node, subsequently adding the sub-node to the appropriate position in the updated information tree Tm+1 as follows: Tm+1 = Combine(Tm, SUB(N0), . . . , SUB(Nn))) (2) This targeted retrieval process ensures that OmniThink collects comprehensive and in-depth content for each sub-node, thereby enriching the hierarchical structure of the information tree."
        },
        {
            "title": "3.1.2 Reflection\nIn this phase, OmniThink reflects the newly re-\ntrieved information in all leaf nodes Lm+1 =\n{N0, ...Nn}. The information retrieved from each\nleaf node is analyzed, filtered, and synthesized to\ndistill the core insights Im+1 = {INS0, ..., INSn}.\nThese refined insights are then incorporated into\nthe conceptual pool Pm, which is continuously up-\ndated and enriched throughout the process as fol-\nlows:",
            "content": "Pm+1 = Merge(Im+1, Pm) (3) Using the updated conceptual pool Pm+1, OmniThink further expands the leaf nodes of the information tree iteratively. 4 Algorithm 1 Expansion and Reflection 1: Input: Topic T, Depth 2: Output: Information Tree , Conceptual Pool {Initialization} 3: Initialize Information Tree T0 with root node Nr 4: Retrieve initial information using search engines 5: Organize and analyze information to form Conceptual Pool P0 {Expansion and Reflection} 6: for each time step = 0 to 1 do 7: Lm Leaf Nodes of Tm Store Lm in Conceptual Buffer Pb for each node Ni in Lm do if Needs Expansion(Ni) then Determine expansion areas using Pm Generate sub-nodes SUB(Ni) = {S0, S1, . . . , SkNi } for each sub-node Sj in SUB(Ni) do Retrieve information for Sj Add Sj to Tm+ end for end if end for Lm+1 Leaf Nodes of Tm+1 Analyze, filter, and synthesize information from Lm+1 to obtain insights Im+1 Update Conceptual Pool Pm+1 Merge(Im+1, Pm) if Sufficient information acquired then break 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end if 24: 25: end for 26: Return Final Article guided, clearly structured, and logically coherent, it is essential to have comprehensive and in-depth understanding of the topic. In the previous section, OmniThink maintains concept pool closely related to the topic, which essentially represents the boundaries and depth of the LLMs understanding of the topic. When generating the content outline, we first create draft outline OD, and then ask the LLM to refine and link the content from the concept pool P, ultimately forming the final outline = Polish(OD, P). Through this approach, the LLM is able to comprehensively cover the key points of the topic in the outline and ensure logical consistency and content coherence in the article. 3.3 Article Composition After completing the outline O, we begin writing the content for each section S. At this stage, the LLM would work in parallel to write the content for each section. When writing the content of the section, we use the titles of each section and their hierarchical subsections to retrieve the most relevant documents from the information tree by calculating the semantic similarity (Sentence-BERT (Reimers and Gurevych, 2019) embeddings). After obtaining the relevant information, the LLM is prompted to generate the section content with citations based on the retrieved information. Once all sections are generated, they will be concatenated into complete draft article AD = {S1, ..Sn}. Since these sections are generated in parallel and the specific content of other sections is not yet clear, we prompt the LLM to process the concatenated article, remove redundant information, and form the final article = {S 1, ..S n}. This iterative cycle of expansion and reflection continues until OmniThink determines that sufficient information has been acquired or predefined maximum retrieval depth has been reached. It ensures that the acquired information is relevant, detailed, and diverse, providing robust foundation for generating structured and information-rich articles. The pseudo code of Expansion and Reflection can be found at Algorithm 1."
        },
        {
            "title": "3.2 Outline Structuring",
            "content": "Outline is the core of an article, determining its content direction, structural hierarchy, and logical progression. To create an outline that is well-"
        },
        {
            "title": "4.1 Dataset and Baseline",
            "content": "We use WildSeek as evaluation dataset to verify the effectiveness of our method, following previous work (Jiang et al., 2024; Shao et al., 2024a). WildSeek collects and filters data related to the open-source STORM web application, with each entry consisting of specific topic and users goal. We select representative baselines for comparison, including RAG, oRAG, and STORM (Shao et al., 2024a) and Co-STORM (Jiang et al., 2024). The baseline results are reproduced on the basis of STORM1. 1https://github.com/stanford-oval/storm 5 Backbones Methods Rubric Grading Information Diversity Knowledge Density Relevance Breadth Depth Novelty GPT-4o Qwen-Plus RAG oRAG STORM Co-STORM* OmniThink RAG oRAG STORM Co-STORM* OmniThink 4.65 2.38 4.34 4.37 4. 2.53 2.61 3.50 4.09 3.96 4.55 3.63 4.21 4.66 4.71 2.77 2.61 3.80 4.24 4.10 4.59 2.56 4.21 4.65 4.66 3.03 2.72 3.60 4.26 4.28 4.22 2.27 3.80 3.89 4. 2.18 2.01 2.72 3.78 3.83 0.1042 0.0963 0.6342 0.6285 0.6642 0.0897 0.0855 0.6417 0.5471 0.7114 22.11 19.70 19.33 19.53 22.31 10.46 11.27 9.73 11.36 11.50 Table 1: Results of article quality evaluation. means that this method is different from the original experimental setting, primarily in the human-machine collaboration component. Instead of simulating human involvement through an agent, as done in the original paper (Jiang et al., 2024), we remove the human participation step. 4.2 Evaluation Setup We employ both automatic and human evaluations to assess the generated long-form articles: Automatic Evaluation. For automatic evaluation, we use Prometheus2 (Kim et al., 2024)2 to score articles on scale of 0 to 5, evaluating Relevance, Breadth, Depth, and Novelty. Furthermore, we measure information diversity (Jiang et al., 2024) (cosine similarity differences between web pages) and knowledge density (discussed in detail in 2.3) for information richness. Detailed procedures are provided in the Appendix C. Human Evaluation. We randomly select 20 topics and compare articles generated by our method with those from the Co-STORM (the comprehensive best-performing baseline based on automatic evaluation), scoring them on the same four aspects. More details can be found in the Appendix D. 4."
        },
        {
            "title": "Implementation Details",
            "content": "We build OmniThink based on the DSpy framework (Khattab et al., 2023), and Appendix contains the corresponding prompts we used. During generation, we set the temperature at 1.0 and top_p at 0.9. We use Bings API with the parameter for the number of web pages returned per query set to 5. For the computation of knowledge density, we utilize Factscore3 with GPT-4o-08-06 as the backbone to decompose atomic knowledge (Min et al., 2023). After decomposition, we proceed to use GPT-4o-08-06 for the deduplication of the split atomic knowledge. To avoid the impact of search engine changes over time, all the results in our table 2https://github.com/prometheus-eval/ prometheus-eval 3https://github.com/shmsw25/FActScore are completed within 3 days. More implementation details are presented in Appendix A."
        },
        {
            "title": "5.1 Main Results",
            "content": "Method RAGo STORM Co-STORM* DeepThink Content Guidance Hierarchical Clarity Logical Coherence 3.93 3.92 3.45 4. 3.95 3.99 3.27 4.02 3.97 3.99 3.41 3.99 Table 2: Results of outline quality evaluation. Article Generation. Table 1 presents the evaluation results on the WildSeek dataset employing GPT-4o and Qwen-Plus as backbones. Within the framework of four key grading criteria (Relevance, Breadth, Depth, and Novelty) OmniThink delivers exceptional performance across the board, with GPT-4o as its backbone, particularly distinguishing itself in the Novelty metric. This achievement can be credited to OmniThinks robust reflective capabilities, which enable it to extract and thoroughly explore novel insights from existing knowledge. When employing Qwen-Plus as the backbone, OmniThinks performance see decline; however, it remains highly competitive. OmniThinks strength lies in its multifaceted and profound contemplation of retrieved information, which facilitates access to more profound layers of the external knowledge. This multi-perspective approach not only enriches the diversity of citation sources but also elevates the citation diversity level beyond that of other methodologies. In terms of knowledge density, OmniThink employs continuous and dynamic retrieval strategy to gather wide array of information, which, in turn, allows it to draw upon more extensive range of resources during the content generation phase. This strategic advantage positions OmniThink at an advantage in the knowledge density metric compared to existing benchmark methods. Outline Generation. The outline serves as critical intermediary in the process of article generation, with its quality exerting direct impact on the coherence, logical consistency, and expressive clarity of the final article. We evaluate outline quality from the perspectives of structural soundness, logical consistency, and generative guidance. More details can be found in the Appendix C.1. From Table 2, we notice that OmniThink achieves superior performance in structural soundness and logical consistency. This improvement can be attributed to the unique design of OmniThinks Concept Pool, which enables the LLMs to develop more comprehensive and diverse understanding of the target topic during outline generation. Consequently, this facilitates better guidance for content production and enhances the overall structural coherence of the generated content. However, the logical consistency of the model showed only marginal improvement compared to the baseline. This observation highlights potential direction for future work, focusing on further enhancing logical consistency within the generation process."
        },
        {
            "title": "5.2 Ablation Study",
            "content": "As discussed in 3.1, one of the main components of OmniThink is the introduction of dynamic expansion and reflection. We compare OmniThink with version that does not implement dynamic expansion and reflection. As shown in Figure 5, the simplified version performs worse in various metrics related to article quality than the complete system, particularly in terms of Information Diversity and Novelty. This experiment demonstrates the powerful role of the dynamic expansion and reflection mechanism in enhancing information diversity and article novelty."
        },
        {
            "title": "5.3 Expansion & Reflection Analysis",
            "content": "We provide further analysis of how the expansion and reflection processes shape the various aspects of the final articles and contribute to its overall quality. Given the interdependent nature of expanFigure 5: The comparison between OmniThink and OmniThink w/o E&R, where OmniThink w/o E&R refers to OmniThink without expansion and reflection. sion and reflection in OmniThink, it is impractical to assess their individual impacts in isolation. To address this challenge, we adopt an indirect yet systematic approach to evaluate their collective influence on the final articles quality. During the information acquisition phase, we substitute the model used for expansion with lower-performing model and measured the extent of performance decline in the generated articles metrics, which served as an indicator of the impact of the expansion process on these metrics. Similarly, the same approach is applied to assess the impact of the reflection process. Specifically, based on the experimental results for Qwen-Plus in Table 1, we replace the models used for the expansion and reflection processes from Qwen-Plus to Qwen2.5-7binstruct (Team, 2024) and observe the decline in various evaluation results. This transition allows us to observe and document the subsequent changes in range of evaluation metrics, providing insights into the expansion and reflection processs influence on the articles overall assessment. We report the results in Figure 6. Continuous reflection expands knowledge boundaries. We observe that reflection is much more important than expansion with respect to novelty and informational diversity. Reflection endows the model with the capacity to not only re-evaluate and introspectively consider existing knowledge but also to integrate this information in way that stimulates the emergence of more diverse and expansive range of ideas. This process of deep introspection is essential, as it diversifies the narrative with spectrum of insights, thereby laying the 7 leads to more sophisticated planner, capable of navigating the complexities of information retrieval and utilization with greater finesse. 5.4 Thinking Depth Analysis Our method has made numerous attempts to improve information retrieval, which are essentially scale-ups of the retrieved information. In this section, we discuss the impact of the quantity and depth of the retrieved information on the quality of the generation of articles. From Figure 7, we observe rapid increase in the knowledge density and information diversity of the generated articles as the depth increases from 1 to 3. This indicates that as the depth increases, OmniThink can search for an increasing amount of diverse information on the Web and utilize this information in the generated articles. However, when the depth is raised to 4, the growth rate of knowledge density and information divinity slows down significantly. This may be because the available information on the subject approaches the search limit, making it difficult to retrieve more useful information on the topic. Figure 6: The comparison of the impact of expansion and reflection on various metrics of the generated article. groundwork for piece of writing that is both innovative and diverse. Intrinsically, reflection module as pivotal accelerator for creativity, allowing the model to surpass the constraints of simple information augmentation. It facilitates the crafting of narrative that is uniquely informative, embodying the innovation that arises from nuanced and varied approach to knowledge. Figure 7: The result of thinking depth with changes in knowledge density and information diversity. Expansion enhances knowledge depth and improves information relevance. We notice that expansion is more important than reflection in breadth and depth. The rationale behind this is that expansion inherently sets the trajectory for the models subsequent information retrieval. By establishing more precise and effective directions for the models retrieval process, it becomes more adept at harnessing the retrieved information. This adeptness allows the model to seamlessly integrate the information into the text, thereby enriching the content with greater depth and breadth. This integration not only enhances the relevance of the content but also increases the knowledge density, as the text becomes more comprehensive and nuanced. Consequently, better expansion strategy Figure 8: Comparison of OmniThink and Co-STORM results under human evaluation. The values on the left side represent the average score from OmniThink human evaluators, while the values on the right side represent the average score from Co-STORM human evaluators."
        },
        {
            "title": "6 Human Evaluation Results",
            "content": "To better understand the strengths and weaknesses of OmniThink, we engage 15 well-educated volunteers to conduct human evaluation. In Figure 8, we present the results of human scoring. The findings indicate that OmniThinks average performance surpasses that of the current strongest baseline across various dimensions, with notable 11% improvement in the Breadth metric compared to Co-STORM. However, in terms of the Novelty metric, although automated evaluation shows an 11% enhancement, human assessment reveals only 8 marginal advantage. This discrepancy suggests that the current automated evaluation may not yet be fully aligned with human judgment, highlighting direction for future improvement in the evaluation of long texts. It should also be noted that despite OmniThinks overall superior performance in various dimensions, approximately 30% of the articles are considered equally excellent to the baseline by human evaluators. This could be attributed to the increasing difficulty for humans to discern subtle differences as the foundational writing capabilities of large models improve. Consequently, there is an urgent need to develop more rigorous and fine-grained evaluation methods to assess model performance more accurately."
        },
        {
            "title": "7 Related Work",
            "content": "7."
        },
        {
            "title": "Information Seeking in NLP",
            "content": "Previous studies on information-seeking focused on designing question-answering (QA) systems (Wu et al., 2025). Early open-domain QA methods generally assumed that users could fulfill their information needs through single query (Chen et al., 2017; Levy et al., 2021). Subsequent studies have recognized that, in real-world scenarios, users often struggle to satisfy their information needs with single query (Chen et al., 2017; Levy et al., 2021). To address this limitation, researchers have explored multi sub-query retrieval methods, where single query is decomposed into multiple sub-queries to retrieve distinct pieces of information (Mao et al., 2024; Chen et al., 2011; Peng et al., 2019). The information collected is then aggregated to provide comprehensive answer. Building on these developments, recent advances in open-domain long-form generation require reasoning across multiple information sources (Fan et al., 2019; Ujwal et al., 2024; Wei et al., 2024; Tan et al., 2024). This line of open-domain long-form generatio underscores the importance of integrating information from multiple perspectives. For example, STORM introduces retrieval paradigm that simulates multi-turn interactions from diverse perspectives, aiming to aggregate richer and more diverse information(Shao et al., 2024b). Similarly, Co-STORM employs roundtable discussion paradigm to further expand the diversity of information sources considered during retrieval (Jiang et al., 2024). Although these approaches have made significant advancements from multi-perspective and multi sub-query perspectives, they often fail to leverage the reasoning and introspective abilities of LLMs fully. Specifically, existing approaches do not fully exploit the potential of LLMs to dynamically adjust retrieval strategies and flexibly update information sources as the models understanding of the topic deepens(Qin et al., 2024). Unlike previous works, we propose reflection-based dynamic retrieval framework OmniThink, which facilitates more comprehensive and contextually responsive retrieval process by enabling context-aware, selfreflective adjustments to retrieval strategies. 7.2 Machine Writing Due to the high costs associated with manual writing, machine writing has garnered significant research interest in recent years (Zhou et al., 2023; Pham et al., 2024; Wang et al., 2024a,b). The emergence of LLMs and Retrieval-Augmented Generation (RAG) has opened new possibilities for automated writing (Liang et al., 2024; Balepur et al., 2023; de la Torre-López et al., 2023). To ensure authenticity and real-time relevance, current RAG-based automated writing systems primarily rely on retrieved content to generate articles. For example, STORM (Shao et al., 2024a) introduces role-playing question-and-answer approach to author Wikipedia-like articles, while Co-STORM (Jiang et al., 2024) proposes userparticipated information retrieval paradigm. Besides, AutoSurvey (Wang et al., 2024c) extends this framework into the domain of academic paper writing. However, these methods tend to overlook the issue of information diversity, which can result in outputs with limited practical value. Although these methods demonstrate notable advancements in specific domains, they often neglect the perspective of content utility, resulting in outputs with limited practical value. Unlike previous works, we propose reflection-based dynamic retrieval framework OmniThink, which facilitates more comprehensive and contextually responsive retrieval process by enabling context-aware, self-reflective adjustments to retrieval strategies. The proposed OmniThink adopts the concept of knowledge density, enhancing the informativeness and overall utility of generated text while maintaining its original quality."
        },
        {
            "title": "8 Conclusion and Furture Work",
            "content": "We propose OmniThink, machine writing framework that emulates the human-like process of iterative expansion and reflection. Automatic and human evaluations demonstrate that OmniThink can generate well-founded, high-quality long articles. Our approach is model-agnostic and can be integrated with existing frameworks. In the future, we will explore more advanced machine writing methods that combine deeper reasoning with roleplaying and human-computer interaction."
        },
        {
            "title": "Limitations",
            "content": "Although the proposed OmniThink has demonstrated its advantages in both automatic and human evaluations, several limitations remain. Firstly, the current work is limited to search and text generation, while vast amount of multimodal information in the open domain remains unused. Secondly, we have not considered personalized language styles in text production. As result, the generated texts tend to be academic in nature, which may not be as suitable for general users reading preferences. We plan to address these limitations in future work."
        },
        {
            "title": "References",
            "content": "Nishant Balepur, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Expository text generation: Imitate, retrieve, paraphrase. Preprint, arXiv:2305.03276. John Bean and Dan Melzer. 2021. Engaging ideas: The professors guide to integrating writing, critical thinking, and active learning in the classroom. John Wiley & Sons. Susan Bovair and David Kieras. 1991. Toward model of acquiring procedures from text. Handbook of reading research, 2:206229. Bertram Bruce. 1978. cognitive science approach to writing. Center for the Study of Reading Technical Report; no. 089. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. Preprint, arXiv:1704.00051. Gang Chen, Yongwei Wu, Jia Liu, Guangwen Yang, and Weimin Zheng. 2011. Optimization of sub-query processing in distributed data integration systems. Journal of Network and Computer Applications, 34(4):10351042. José de la Torre-López, Aurora Ramírez, and José Raúl Romero. 2023. Artificial intelligence to automate the systematic review of scientific literature. Computing, 105(10):21712194. Brian Dos Santos and Vijay Mookerjee. 1993. Minimizing information acquisition costs. Decision Support Systems, 9(2):161181. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. Eli5: Long form question answering. arXiv preprint arXiv:1907.09190. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: survey. Preprint, arXiv:2312.10997. Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, and Min Zhang. 2025. Test-time computing: from system-1 thinking to system-2 thinking. Preprint, arXiv:2501.02497. Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J. Semnani, and Monica S. Lam. 2024. Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. Preprint, arXiv:2408.15232. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. Preprint, arXiv:2310.03714. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024. Prometheus 2: An open source language model specialized in evaluating other language models. Preprint, arXiv:2405.01535. Sharon Levy, Kevin Mo, Wenhan Xiong, and William Yang Wang. 2021. Open-domain questionanswering for covid-19 and other emergent domains. Preprint, arXiv:2110.06962. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. 2024. Leveraging large language models for nlg evaluation: Advances and challenges. Preprint, arXiv:2401.07103. Xiaobo Liang, Zecheng Tang, Juntao Li, and Min Zhang. 2023. Open-ended long text generation via masked language modeling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long 10 Papers), pages 223241, Toronto, Canada. Association for Computational Linguistics. Yi Liang, You Wu, Honglei Zhuang, Li Chen, Jiaming Shen, Yiling Jia, Zhen Qin, Sumit Sanghai, Xuanhui Wang, Carl Yang, and Michael Bendersky. 2024. Integrating planning into single-turn long-form text generation. Preprint, arXiv:2410.06203. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. Preprint, arXiv:1801.10198. Xiang Liu, Peijie Dong, Xuming Hu, and Xiaowen Chu. 2024. Longgenbench: Long-context generation benchmark. Preprint, arXiv:2410.04199. Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024. RaFe: Ranking feedback improves query rewriting for RAG. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 884901, Miami, Florida, USA. Association for Computational Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. Preprint, arXiv:2305.14251. Karen Osterman. 1990. Reflective practice: new agenda for education. Education and urban society, 22(2):133152. Bidhan Parmar, Edward Freeman, Jeffrey Harrison, Andrew Wicks, Lauren Purnell, and Simone De Colle. 2010. Stakeholder theory: The state of the art. Academy of Management Annals, 4(1):403 445. Peng Peng, Qi Ge, Lei Zou, Tamer Özsu, Zhiwei Xu, and Dongyan Zhao. 2019. Optimizing multiquery evaluation in federated rdf systems. IEEE Transactions on Knowledge and Data Engineering, 33(4):16921707. Chau Minh Pham, Simeng Sun, and Mohit Iyyer. 2024. Suri: Multi-constraint instruction following for long-form text generation. arXiv preprint arXiv:2406.19371. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with language model prompting: survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53685393, Toronto, Canada. Association for Computational Linguistics. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. 2024. O1 replication journey: strategic progress report part 1. Preprint, arXiv:2410.18982. Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, and Junyang Lin. 2024. Language models can self-lengthen to generate long texts. Preprint, arXiv:2410.23933. Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, and Kai Chen. 2024. Hellobench: Evaluating long text generation capabilities of large language models. Preprint, arXiv:2409.16191. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. Preprint, arXiv:2302.00083. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature, 623(7987):493498. Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. 2024a. Assisting in Writing Wikipedia-like Articles From Scratch with Large Language ModIn Proceedings of the 2024 Conference of els. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, and Diyi Yang. 2024b. Collaborative gym: framework for enabling and evaluating human-agent collaboration. Preprint, arXiv:2412.15701. Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, and David Sontag. 2023. Beyond summarization: Designing ai support for real-world expository writing tasks. Preprint, arXiv:2304.02623. Michael D. Skarlinski, Sam Cox, Jon M. Laurent, James D. Braza, Michaela Hinks, Michael J. Hammerling, Manvitha Ponnapati, Samuel G. Rodriques, and Andrew D. White. 2024. Language agents achieve superhuman synthesis of scientific knowledge. Preprint, arXiv:2409.13740. 11 Amanda Spink, Howard Greisdorf, and Judy Bateman. 1998. From highly relevant to not relevant: examining different regions of relevance. Information processing & management, 34(5):599621. Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. 2023. Doc: Improving long story coherence with detailed outline control. Preprint, arXiv:2212.10077. Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. 2022. Read before generate! faithful long form question answering with machine reading. Preprint, arXiv:2203.00343. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. 2024. Quiet-star: Language models can teach themselves to think before speaking. Preprint, arXiv:2403.09629. Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2019. Outline generation: Understanding the inherent content structure of documents. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 745754. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2024. survey of large language models. Preprint, arXiv:2303.18223. Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. 2023. Outline, then details: Syntactically guided coarse-to-fine code generation. In International Conference on Machine Learning, pages 42403 42419. PMLR. Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023. Recurrentgpt: Interactive generation of (arbitrarily) long text. Preprint, arXiv:2305.13304. Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, and Linqi Song. 2024. Proxyqa: An alternative framework for evaluating long-form text generation with large language models. Preprint, arXiv:2401.15042. Qwen Team. 2024. Qwen2.5: party of foundation models. Utkarsh Ujwal, Sai Sri Harsha Surampudi, Sayantan Mitra, and Tulika Saha. 2024. \" reasoning before responding\": Towards legal long-form question answering with interpretability. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 49224930. Qiyao Wang, Shiwen Ni, Huaren Liu, Shule Lu, Guhong Chen, Xi Feng, Chi Wei, Qiang Qu, Hamid AlinejadRokny, Yuan Lin, and Min Yang. 2024a. Autopatent: multi-agent framework for automatic patent generation. Preprint, arXiv:2412.09796. Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, et al. 2024b. Weaver: Foundation models for creative writing. arXiv preprint arXiv:2401.17268. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang. 2024c. Autosurvey: Large language models can automatically write surveys. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. 2024. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou, Pengjun Xie, and Fei Huang. 2025. Webwalker: Benchmarking llms in web traversal. Preprint, arXiv:2501.07572. Yang Xu and David Reitter. 2017. Spectral analysis of information density in dialogue predicts In Proceedings collaborative task performance. of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 623633, Vancouver, Canada. Association for Computational Linguistics."
        },
        {
            "title": "A Implementation Details",
            "content": "We build OmniThink based on the DSpy framework (Khattab et al., 2023), and STORM. Appendix contains the corresponding prompts we used. During article generation, we set the temperature at 1.0 and top_p at 0.9. The search engine employed is Bings API, with the parameter for the number of web pages returned per query configured to 5. To retrieve information based on the outline, we use SentenceBERT (Reimers and Gurevych, 2019) embeddings to calculate cosine similarity, thereby retrieving the three most similar web pages each time. For the computation of knowledge density, we utilize Factscore4 with GPT4o-08-06 as the backbone to decompose atomic knowledge (Min et al., 2023). After the decomposition, we proceed to use GPT-4o-08-06 for the deduplication of the split atomic knowledge."
        },
        {
            "title": "B Full Prompts in OmniThink",
            "content": "In 3, we introduce the specific process of OmniThink, which is implemented using zero-shot prompting based on GPT-4o-2024-08-06. Lists 1, 2, 3, 4 and 5, respectively document the complete prompts for OmniThinks Expand, Reflect, Write Outline, Write Article, and Polish Article stages. These prompts are designed to guide the model through iterative stages of content generation, ensuring coherence and depth in the produced text. The structured process leverages dynamic adjustments based on intermediate outputs, reflecting balanced integration of retrieval and generation capabilities. This systematic approach highlights OmniThinks ability to adaptively construct wellorganized and contextually relevant articles across diverse topics."
        },
        {
            "title": "C Automatic Evaluation Details",
            "content": "To further ensure reliability, we conducted multiple evaluation rounds using different prompts covering various aspects of outline coherence, structural logic, and topic relevance. This multi-faceted evaluation helps mitigate potential biases and enhances the robustness of the scoring results. C.1 Outline Evaluation Since Prometheus2 (Kim et al., 2024) does not perform targeted optimization on the outline, we decided to use more powerful model to score the 4https://github.com/shmsw25/FActScore Figure 9: The educational background distribution of assessors. outline. To ensure the results are consistent, we set the temperature to 0. Specifically, we use the Prometheus2 framework but replace the underlying evaluation model with GPT-4o-08-06. The scoring criteria for outline quality evaluation and discourse quality evaluation can be found in Lstlisting 10. In addition, since Co-STORM does not have an intermediate outline generation step, we had to extract the outline from the final article for evaluation, which might be the reason for the relatively lower outline scores observed form Co-STORM. C.2 Article Evaluation Following Co-STORM (Jiang et al., 2024), we utilized the Prometheus-7b-v2.0 model for evaluation. Prometheus (Kim et al., 2024) is an open-source scoring model used to assess lengthy texts based on user-defined criteria. Its default temperature value is 1.0, and the top_p value is 0.9. Due to the models limited context window, we exclude reference sections from the article evaluation and trim the input text to fewer than 2000 words to fit within the models context window. This is consistent with STORMs approach (Shao et al., 2024a), where the shortest section is removed each time until the article length meets the specified requirement. The scoring criteria for article quality evaluation can be found in Listing 11."
        },
        {
            "title": "D Human Evaluation Details",
            "content": "The participants in the evaluation voluntarily provided their highest educational qualification to demonstrate their ability to impartially assess the article. As shown in Figure 9, all of our human evaluators have an undergraduate degree or higher, with 53% having graduate degree. As discussed in 6, to compare the merits of OmniThink and 13 Co-STORM, each human evaluator was given scoring criterion and pair of articles. They were required to compare and assign scores, with the scoring criteria being the same as Lstlisting 11. We compiled the average scores given by the human evaluators for OmniThink and Co-STORM and compared their wins and losses."
        },
        {
            "title": "E Case Study",
            "content": "In Figure 12, we present an example of AlphaFold generated by OmniThink. It is generated using GPT-4o as the backbone. We can see that OmniThinks language is more concise compared to other methods, and it contains more information per unit of text length."
        },
        {
            "title": "F Clarification of Reflection",
            "content": "In this paper, our reflection refers to the process where the LLM reflects on the retrieved information based on its current Conceptual Pool, evaluating which parts of the information can enrich the existing Conceptual Pool. The usable information is then extracted as insights and added to the Conceptual Pool. 14 class ExtendConcept(dspy.Signature): \"\"\" You are an analytical robot. will provide you with subject, the information have searched about it, and our preliminary concept of it. need you to generate detailed, in-depth, and insightful report based on it, further exploring our initial ideas. First, break down the subject into several broad categories, then create corresponding search engine keywords for each category. Note: The new categories should not repeat the previous ones. Your output format should be as follows: -[Category 1] --{Keyword 1} --{Keyword 2} -[Category 2] --{Keyword 1} --{Keyword 2} \"\"\" info = dspy.InputField(prefix='The information you have collected from the webpage:', format=str) concept = dspy.InputField(prefix='The summary of the previous concepts:', format=str) category = dspy.InputField(prefix='The broader categories you need to further expand:', format=str) keywords = dspy.OutputField(format=str) Listing 1: Prompts used for expanding in OmniThink. class GenConcept(dspy.Signature): \"\"\" Please analyze, summarize, and evaluate the following webpage information. Think like person, distill the core point of each piece of information, and synthesize them into comprehensive opinion. Present your comprehensive opinion in the format of 1. 2. ... \"\"\" info = dspy.InputField(prefix='The webpage information you have collected:', format=str) concepts = dspy.OutputField(format=str) Listing 2: Prompts used for reflecting in OmniThink. class PolishPageOutline(dspy.Signature): \"\"\" Improve an outline for report page. You already have draft outline that covers the general information. Now you want to improve it based on the concept learned from an information-seeking to make it more informative. Here is the format of your writing: 1. Use \"#\" Title\" to indicate section title, \"##\" Title\" to indicate subsection title, \"###\" Title\" to indicate subsubsection title, and so on. 2. Do not include other information. 3. Do not include topic name itself in the outline. \"\"\" draft = dspy.InputField(prefix=\"Current outline:n \", format=str) concepts = dspy.InputField(prefix=\"The information you learned from the conversation:n\", format=str) outline = dspy.OutputField(prefix='Write the page outline:n', format=str) class WritePageOutline(dspy.Signature): \"\"\" Write an outline for report page. Here is the format of your writing: 1. Use \"#\" Title\" to indicate section title, \"##\" Title\" to indicate subsection title, \"###\" Title\" to indicate subsubsection title, and so on. 2. Do not include other information. 3. Do not include topic name itself in the outline. \"\"\" topic = dspy.InputField(prefix=\"The topic you want to write: \", format=str) outline = dspy.OutputField(prefix=\"Write the report page outline:n\", format=str) Listing 3: Prompts used for writing the outline in OmniThink. 15 class WriteSection(dspy.Signature): \"\"\"Write Wikipedia section based on the collected information. Here is the format of your writing: 1. Use \"#\" Title\" to indicate section title, \"##\" Title\" to indicate subsection title, \"###\" Title\" to indicate subsubsection title, and so on. 2. Use [1], [2], ..., [n] in line (for example, \"The capital of the United States is Washington, D.C.[1][3].\"). You DO NOT need to include References or Sources section to list the sources at the end. 3. The language style should resemble that of Wikipedia: concise yet informative, formal yet accessible. \"\"\" info = dspy.InputField(prefix=\"The Collected information:n\", format=str) topic = dspy.InputField(prefix=\"The topic of the page: \", format=str) section = dspy.InputField(prefix=\"The section you need to write: \", format=str) output = dspy.OutputField( prefix=\"Write the section with proper inline citations (Start your writing with # section title. Don't include the page title or try to write other sections):n\", format=str) Listing 4: Prompts used for writing section in OmniThink. class PolishPage(dspy.Signature): \"\"\" You are faithful text editor that is good at finding repeated information in the article and deleting them to make sure there is no repetition in the article. You won't delete any non-repeated part in the article. You will keep the inline citations and article structure (indicated by \"#\", \"##\", etc.) appropriately. Refine the statement to avoid vague and ambiguous expressions, making it more concise and clear. Do your job for the following article. \"\"\" article = dspy.InputField(prefix=\"The article you need to polish:n\", format=str) page = dspy.OutputField( prefix=\"Your revised article:n\", format=str) Listing 5: Prompts used for polishing article in OmniThink. Criteria Description Guidance for Content Generation: Does the outline effectively guide content generation, ensuring comprehensive coverage of the topic? Score 1 Description The outline fails to guide content generation, omitting significant aspects of the topic or providing insufficient direction. Score 2 Description The outline provides limited guidance, covering some key areas but lacking depth or completeness in addressing the topic. Score 3 Description The outline provides moderate guidance for content generation, addressing most key areas but leaving some gaps or ambiguities. Score 4 Description The outline effectively guides content generation, covering all significant aspects with clear direction, though minor refinements could enhance comprehensiveness. Score 5 Description The outline is exemplary in guiding content generation, thoroughly addressing all aspects of the topic with clear, detailed direction and no significant gaps. Criteria Description Hierarchical Clarity: Does the outline clearly define hierarchy of topics and subtopics, with logical, diverse structure that is easy to understand? Score 1 Description Score 2 Description Score 3 Description Score 4 Description Score 5 Description The outline exhibits no discernible hierarchical structure. Topics and subtopics are jumbled together without logical separation or clear levels, making it nearly impossible to follow or identify any organization. The outline attempts to establish hierarchy but fails to maintain logical consistency. Main topics and subtopics are frequently misclassified, and the structure is overly rigid or disjointed. Subtopics may be missing, misplaced, or redundant, making it hard to grasp the intent of the structure. The outline has recognizable hierarchical structure but lacks diversity in organization style. While main topics are somewhat clear, subtopics occasionally overlap, are misaligned, or follow repetitive format. This restricts flexibility and introduces mild confusion in certain areas. The outline displays clear, logical, and diverse hierarchical structure. Main topics are distinct, and subtopics are properly nested. While most elements are well-placed, there may be minor redundancies or opportunities to introduce more diverse formats for subtopics. Slight adjustments could achieve better precision and variety in style. The outline showcases an exceptional, flawless hierarchical structure. Each main topic is distinct, and subtopics are logically nested with absolute clarity and stylistic diversity. The outline demonstrates flexibility in structure and organization, adapting its style where appropriate for the content and logic. No further refinement is necessary. Criteria Description Logical Coherence: Does the outline logically organize topics and subtopics, ensuring smooth and natural flow of ideas with clear logical transitions? Score 1 Description Score 2 Description Score 3 Description Score 4 Description Score 5 Description The outline is highly disjointed and incoherent. Topics and subtopics appear in random, unordered manner, with no logical flow or sense of progression. Major conceptual gaps and illogical jumps are present throughout the structure. The outline shows some attempt at logical organization, but it contains frequent inconsistencies, abrupt shifts, or logical missteps. Topics and subtopics are misaligned or lack proper transitions, making the reader work hard to follow the structure. The outline demonstrates basic level of logical coherence. Most topics follow general sequence, but some sections feel forced, with weak or unclear transitions. There are small jumps in logic, causing slight confusion or loss of flow at certain points. The outline exhibits strong sense of logical flow, with ideas presented in mostly smooth and connected manner. Transitions between topics and subtopics are clear, but few minor adjustments could make the flow more seamless or natural. The logic is sound, but room for refinement exists. The outline achieves exceptional logical coherence. Each topic and subtopic follows deliberate, thoughtful progression, with clear, natural, and intuitive transitions. The reader experiences seamless flow of ideas, and no adjustments are required to improve logical consistency or flow. Figure 10: Outline scoring rubrics on 1-5 scale for the Prometheus model. 16 Criteria Description Broad Coverage: Does the article provide an in-depth exploration of the topic and have good coverage? Score 1 Description Score 2 Description Score 3 Description Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points. Score 4 Description Good coverage; achieves broad coverage of the topic, hitting on all major points with minimal extraneous information. Score 5 Description Exemplary in breadth; delivers outstanding coverage, thoroughly detailing all crucial aspects of the topic without including irrelevant information. Severely lacking; offers little to no coverage of the topics primary aspects, resulting in very narrow perspective. Partial coverage; includes some of the topics main aspects but misses others, resulting in an incomplete portrayal. Criteria Description Novelty: Does the report cover novel aspects that relate to the users initial intent but are not directly derived from it? Score 1 Description Lacks novelty; the report strictly follows the users initial intent with no additional insights. Score 2 Description Minimal novelty; includes few new aspects but they are not significantly related to the initial intent. Score 3 Description Moderate novelty; introduces some new aspects that are somewhat related to the initial intent. Score 4 Description Good novelty; covers several new aspects that enhance the understanding of the initial intent. Score 5 Description Excellent novelty; introduces numerous new aspects that are highly relevant and significantly enrich the initial intent. Criteria Description Relevance and Focus: How effectively does the report maintain relevance and focus, given the dynamic nature of the discourse? Score 1 Description Very poor focus; discourse diverges significantly from the initial topic and intent with many irrelevant detours. Poor focus; some relevant information, but many sections diverge from the initial topic. Score 2 Description Score 3 Description Moderate focus; mostly stays on topic with occasional digressions that still provide useful information. Score 4 Description Good focus; maintains relevance and focus throughout the discourse with minor divergences that add value. Score 5 Description Excellent focus; consistently relevant and focused discourse, even when exploring divergent but highly pertinent aspects. Criteria Description Depth of Exploration: How thoroughly does the report explore the initial topic and its related areas, reflecting the dynamic discourse? Score 1 Description Very superficial; provides only basic overview with significant gaps in exploration. Score 2 Description Score 3 Description Moderate depth; covers key aspects but may lack detailed exploration in some areas. Score 4 Description Good depth; explores most aspects in detail with minor gaps. Score 5 Description Excellent depth; thoroughly explores all relevant aspects with comprehensive detail, reflecting deep and dynamic discourse. Superficial; offers some detail but leaves many important aspects unexplored. Figure 11: Report scoring rubrics on 1-5 scale for the Prometheus model. AlphaFold # AlphaFold's Capabilities AlphaFold has markedly advanced protein structure prediction, starting with singlechain structures and evolving to AlphaFoldMultimer for multichain complexes and AlphaFold2.3 for larger, complex proteins[1]. In collaboration with the European Bioinformatics Institute (EMBLEBI), AlphaFold released almost all cataloged protein structures in 2022, offered in the AlphaFold Protein Structure Database accessed by 1.4 million users in over 190 countries[1]. Its applications span diverse scientific fields, aiding vaccine development, cancer drug discovery, and creating plastic eating enzymes for environmental issues[1]. Recent advancements underscore AI's potential to enrich our understanding of molecular systems and expedite therapeutic applications[2]. AlphaFold spearheads rapid scientific exploration, with DeepMind and Isomorphic Labs propelling ongoing biological innovations[2]. # Scientific Contributions AlphaFold leverages deep learning to predict protein structures from amino acid sequences, bridging the gap between protein sequences and structures, and providing insights into protein folding and interactions. ## Breakthroughs in Protein Structure Prediction During the Critical Assessment of protein Structure Prediction (CASP) competition, AlphaFold achieved accuracy on par with experimental methods, notably for challenging protein complexes, marking pivotal milestone. ## Impact on Biomedical Research AlphaFold's predictions enhance biomedical research by elucidating challenging protein structures, facilitating new therapeutic strategies and drug target identifications. ## Future Directions and Applications AlphaFold's success opens research applied in biology and medicine, providing insights into complex biological systems and supporting advances in drug discovery and genomic annotation. # Efficiency and Ease of Use AlphaFold, by DeepMind, optimizes vast genetic data processing for rapid, accurate predictions, minimizing complex computational demands. Its userfriendly interface supports various research expertise levels, complemented by guides for broad scientific adoption. # Integration with Existing Technologies Integration tools enhance computational biology by synchronizing AlphaFold with existing technologies, automating workflows, and data connectivity[3]. This integration encourages efficiency in protein predictions and innovation using tools like Zapier or Initus[4]. Challenges include integrating with legacy systems while ensuring data security and privacy compliance[4]. # Ethical Considerations in Technology Technologies like AlphaFold raise ethical considerations around data privacy, security, and potential misuse of predictive algorithms. Balancing innovation with ethical stewardship is critical to managing societal impacts[5]. # AlphaFold's Future Prospects With over 200 million protein structures, AlphaFold significantly enhances biological processes understanding, saving research time and costs, and supporting breakthroughs in diseases and environmental challenges[6]. It also contributes to ecological sustainability goals[6]. # Accessibility and Usability AlphaFold's design focuses on usability, offering clean interfaces for users with varying expertise to navigate efficiently and achieve research objectives effortlessly[7]. # Accelerating Research AlphaFold accelerates research by providing rapid protein predictions, reducing reliance on traditional experimental methods and promoting sustainable scientific practices[8]. # Big Data and AI Synergy AI and big data transformations in computational biology, as demonstrated by AlphaFold, demand effective data management to overcome siloed data and ensure AI solutions succeed[9]. Optimal integration tools are necessary to align with service level agreements and guarantee efficiency. # Societal Challenges and Debates AlphaFold's advent prompts debates about traditional methodologies and proprietary technology transparency in research. Access disparities and pharmaceutical implications also raise intellectual property and regulatory concerns [5][10][11]. # LongTerm Scientific Contributions AlphaFold's predictions enhance biological research, fostering applications in drug discovery and innovative solutions. Its open access and algorithm continuous improvements support crossdisciplinary collaboration for deeper insights into biological systems[12][13][14]. # Interdisciplinary Collaboration AlphaFold exemplifies interdisciplinary collaboration, integrating computational biology with machine learning. Such collaboration maximizes scientific output and promotes comprehensive research approaches[5][15]. Figure 12: case of AlphaFold generated by OmniThink."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group",
        "Zhejiang Key Laboratory of Big Data Intelligent Computing",
        "Zhejiang University"
    ]
}