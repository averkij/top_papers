{
    "paper_title": "Your Group-Relative Advantage Is Biased",
    "authors": [
        "Fengkai Yang",
        "Zherui Chen",
        "Xiaohan Wang",
        "Xiaodong Lu",
        "Jiajun Chai",
        "Guojun Yin",
        "Wei Lin",
        "Shuai Ma",
        "Fuzhen Zhuang",
        "Deqing Wang",
        "Yaodong Yang",
        "Jianxin Li",
        "Yikun Ban"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood. In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training."
        },
        {
            "title": "Start",
            "content": "Your Group-Relative Advantage Is Biased Fengkai Yang1,3,4, Zherui Chen2, Xiaohan Wang4, Xiaodong Lu1,4, Jiajun Chai4, Guojun Yin4, Wei Lin4, Shuai Ma1, Fuzhen Zhuang1, Deqing Wang1, Yaodong Yang3, Jianxin Li1, Yikun Ban1* 1Beihang University 2University of California, Berkeley 3Peking University 4Meituan 6 2 0 2 3 1 ] . [ 1 1 2 5 8 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood. In this work, we uncover fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training."
        },
        {
            "title": "Introduction",
            "content": "After the success of DeepSeek-R1 (DeepSeek-AI, 2025), RLVR has rapidly emerged as simple yet powerful paradigm for training reasoning-oriented LLMs. GRPO (Shao et al., 2024) has gained increasing popularity after PPO (Schulman et al., 2017). Numerous variants of GRPO have been proposed to improve the algorithm, with the goal of achieving better stability and performance. Common variants include GSPO (Zheng et al., 2025), *Corresponding Author. If you have yikunb@buaa.edu.cn or yangfengkai@stu.pku.edu.cn any questions, to contact free feel DAPO (Yu et al., 2025), Dr.GRPO (Liu et al., 2025b) and GMPO (Zhao et al., 2025). In post-training, intra-group advantage estimation is critical to the performance of group-relative RL algorithms. Typically, for each sampled prompt, the algorithm generates only small number of rollouts and uses the within-group average reward as baseline to compute advantages, thereby avoiding the need for separate critic model. While this design is appealing and has attracted broad interest in the RL community, it still lacks detailed theoretical characterization(Xiong et al., 2025; Tan et al., 2025). Your advantage estimation is biased. In this paper, we identify fundamental issue in group-based RL: the group-relative advantage estimator is generally biased relative to the true (expected) advantage. We provide theoretical analysis showing that for hard prompts, the estimator tends to underestimate the expected advantages, whereas for easy prompts, it tends to overestimate the expected advantages, as presented in Section 2.2. Such systematic bias can cause the policy to under-learn from hard questions while over-exploiting easy ones, ultimately hurting both training stability and generalization. As illustrated by the representative example in Figure 1(b), grouprelative estimation can introduce substantial bias in advantage estimation for group-based RL algorithms. Our empirical results further corroborate this phenomenon, with consistent evidence reported in appendix E.1. Motivated by these findings, we propose novel policy optimization algorithm that adaptively reweights advantage estimates to mitigate the bias induced by group-based advantage estimation. The overall framework is depicted in Figure 3. Our main contributions are summarized as follows: [Discovery]. We provide the first theoretical analysis revealing that group-based advantage estima1 Figure 1: (a) Comparison of the performance of RL algorithms with and without HA-DW on Qwen3-4B-Base across five mathematical reasoning benchmarks. (b) Significant biased advantage estimation on the MATH dataset under 8 and 128 rollouts. (c) Performance gain by GRPO+HA-DW on MATH500 stratified by difficulty levels. tion in RLVR is inherently biased, systematically underestimating advantages for hard prompts and overestimating them for easy prompts. [Algorithm]. Motivated by this fundamental discovery, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), which dynamically adjusts advantage weights using an evolving difficulty anchor that integrates long-term reward trends and historical training information. HA-DW compensates for the bias induced by group-relative advantage estimation and enables more principled balance between exploration and exploitation in RL training. [Performance]. As illustrated in Figure 1(a), we validate our approach through extensive experiments on mathematical reasoning benchmarks, demonstrating consistent performance improvements when integrated HA-DW with GRPO and its variants across model scales. Notably, even when compared with GRPO using larger number of rollouts, our method still achieves superior results. Our goal is not to model all RLVR settings, but to expose previously overlooked statistical bias in group-relative algorithms and demonstrate that even lightweight corrections can yield consistent gains."
        },
        {
            "title": "2 Why Your Advantage Estimation is",
            "content": "Biased? In this section, we theoretically analyze the biased estimation in group-relative algorithms. Firstly, we provide the prerequisite definitions."
        },
        {
            "title": "2.1 Definitions",
            "content": "At training step t, we sample prompt xt D. Given xt, group-relative RL algorithm samples responses {yt,i}G i=1 independently from the current policy πθt( xt). Each response yt,i receives corresponding scalar reward rt,i {0, 1}, forming the reward set {rt,i}G i=1, where r() is the reward function and we denote r(yt,i) by rt,i for brevity. The group-relative policy optimization (Group-PO) objective is defined as: Jgroup(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 ψ Å πθ(yt,i xt) πθold(yt,i xt) ã Ä ˆAt,i ϕ ä , (1) where πθold denotes the reference (behavior) policy. The group-relative advantage ˆAt,i is computed as: ˆAt,i = rt,i ˆpt, ˆpt ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 rt,i, (2) where ˆpt is the group baseline: Here, ψ() denotes function applied to the importance sampling ratio (e.g., identity, clipping, or logarithmic transformation), and ϕ() denotes function applied to the advantage term, introduced to maintain generality across different grouprelative policy optimization variants. Definition 1 (Expected Reward). Assume the reward function r() is binary, i.e., r() {0, 1}. Given prompt xt and policy πθt, let yt πθt( xt) be sampled response. The expected reward of policy πθt on prompt xt is defined as: pt = Eytπθt (xt) (cid:2)r(yt)(cid:3) = P(r(yt) = 1 xt, πθt) . (3) In the RLVR setting, pt represents the expected reward under policy πθt given xt, while ˆpt can be regarded as an empirical estimator of pt obtained from finite group of sampled responses. This motivates the following definition. 2 Definition 2 (Expected Advantage). Given prompt xt D, let yt,i πθt( xt) be sampled response with corresponding reward rt,i. The expected advantage is defined as: Accordingly, our analysis focuses on the effective update regime, namely groups for which at least one response receives non-zero advantage. This corresponds to the non-degenerate event At,i = rt,i pt. (4) := {1 1}. (6) Thus, in the RLVR setting, At,i represents the expected advantage of response yt,i under policy πθt given xt, while ˆAt,i can be regarded as an empirical estimator of At,i obtained from finite group of sampled responses. Most group-relative RL algorithms rely on ˆAt,i for policy updates, differing primarily in how ˆAt,i is processed or transformed within their respective optimization objectives. 2.2 Fundamental Discovery Next, we present formal formulation of the problem. Given prompt xt D, let pt denote the expected reward of policy πθt on xt. We then sample responses independently from πθt( xt). In RLVR, rewards are often binary, especially in mathematical and formal reasoning tasks where verifiers return pass/fail signals. Under this widely adopted setting, it is natural to model the reward associated with each response as Bernoulli random variable: rt,i Bernoulli(pt), [G]. (5) Let = (cid:80)G i=1 rt,i denote the total reward within the group. The empirical group baseline is given by ˆpt = R/G. Definition 3 (Prompt Difficulty). Given prompt xt, policy πθt, and [0, 1), we define the difficulty of xt as follows: xt is hard prompt if pt < 0.5 ; xt is moderate prompt if 0.5 pt 0.5 + ; xt is easy prompt if pt > 0.5 + , where is user-defined threshold to customize the prompt difficulty. In group-based policy optimization, the grouprelative advantage estimator satisfies ˆAt,i = 0 for all [G] when either = 0 or = G, resulting in zero gradients and hence no parameter updates. In practice, such degenerate groups do not contribute to learning and are either explicitly discarded or implicitly ignored by GRPO-style algorithms. 3 Importantly, conditioning on does not alter the optimization trajectory, but isolates the subset of samples that actively drive learning, allowing us to precisely characterize the bias in advantage estimation. Next, we present the main results. Theorem 1. Given prompt xt D, let yt,i πθt( xt) denote sampled response with reward rt,i. Suppose 2, and condition on the event = {1 1}. Then, for any [G], we have: Eî ˆAt,i Eî ˆAt,i Eî ˆAt,i = At,i, if and only if pt = 0.5. < At,i, if pt < 0.5; > At,i, if pt > 0.5; ó ó ó Theorem 1 shows that the expectation of the group-based advantage estimator ˆAt,i is lower than the true advantage At,i for difficult prompts, and larger than At,i for easy prompts. The estimator is unbiased only when pt = 0.5. This bias is amplified as pt deviates from 0.5 in Figure 2. However, the expectation-level result in Theorem 1 alone is insufficient to characterize the probability of overestimation or underestimation of ˆAt,i. We provide the following probabilistic result. Theorem 2. Under the condition of Theorem 1, suppose xt is hard prompt (pt < 0.5). Then, for any ϵ (0, E[ˆpt S] pt), we have: Ä = ä At,i ˆAt,i > ϵ (cid:0)G (cid:80)G1 1 (1 pt)G pG k=G(pt+ϵ)+1 (cid:1) pk (1 pt)Gk . (7) Simlarily, suppose xt is an easy prompt (pt > 0.5). Then, for any ϵ (0, pt E[ˆpt S]), we have: ä Ä ˆAt,i At,i > ϵ (cid:0)G (cid:80)G(ptϵ)1 1 (1 pt)G pG (cid:1) pk k=1 = (1 pt)Gk (8) . Theorem 2 provides distribution-level characterization of how likely group-relative advantage estimation is to underestimate or overestimate the true advantage, depending on prompt difficulty. In contrast to expectation-level results, this theorem quantifies the exact probability mass of large estimation errors under finite group sizes. It is well known that generating multiple rollouts per prompt is computationally expensive in practice. Consequently, existing RLVR methods typically sample only small number of responses (e.g., = 8) for each prompt xt to estimate ˆpt (Zhang et al., 2025; Liu et al., 2025a; Shen et al., 2025) . Motivated by this practical constraint, we derive the following corollaries based on Theorem 2, which explicitly characterize the estimation behavior under small group sizes. Corollary 1. Under the condition of Theorem 2, suppose the group size satisfies 2 8, and assume that pt is uniformly distributed over [0, 1]. Then, for any [G], the following inequalities hold: PÄ ˆAt,i < At,i S, pt < 0.5 ä PÄ ˆAt,i > At,i S, pt > 0.5 ä ä PÄ ˆAt,i < At,i S, pt < 0.25 ä PÄ ˆAt,i > At,i S, pt > 0.75 > 0.63, > 0.63, > 0.78, > 0.78. (9) Corollary 1 shows that, with high probability, the group-relative advantage estimator ˆAt,i underestimates the true advantage At,i for hard prompts and overestimates At,i for easy prompts, under the practical set of G. Moreover, as the prompt difficulty becomes more extreme (i.e., as increases), this bias becomes more pronounced, which is also demonstrated in Colloary 2. Corollary 2. Under the condition of Corollary 1, suppose 6. The following inequalities hold: Å ˆAt,i < At,i S, pt < Å ˆAt,i > At,i S, pt > ã"
        },
        {
            "title": "2\nG\nG − 2\nG",
            "content": "> 0.78, (10) ã > 0.78. Corollary 3. Under the condition of Theorem 2, suppose 2. Then, for any [G], the following inequalities hold surely: ˆAt,i < At,i, ˆAt,i > At,i, if pt < 1 , if pt > G1 . (11) 4 Corollary 3 shows that the group-based advantage estimator ˆAt,i necessarily underestimates the true advantage At,i for extremely difficult prompts (pt < 1/G), and overestimates At,i for extremely easy prompts (pt > (G 1)/G). Detailed derivation process is presented in appendix D. Discovery. Group-relative advantage is provably biased except at pt = 0.5. Specifically, it systematically underestimates the true advantage for hard prompts and overestimates it for easy prompts. Moreover, this bias becomes deterministic in extreme regimes: the estimator necessarily underestimates the true advantage for extremely difficult prompts and necessarily overestimates it for extremely easy prompts. Discussion. While the binary reward assumption covers many standard RLVR setups particularly those using hard verifier outcomes we recognize that real-world reward signals can be more general. To address this, we extend our analysis in Appendix D.5 to continuous bounded reward distributions. The results suggest that the core bias phenomenon is not an artifact of the Bernoulli reward assumption but is prevalent across broader class of bounded reward models. Figure 2: Illustration of advantage estimation bias as function of pt and group size G."
        },
        {
            "title": "3 Proposed Solution",
            "content": "Since the group-based advantage estimator is biased, we propose an algorithm to adjust the advantage estimation accordingly. The proposed approach consists of two key components. First, we introduce framework that incorporates crossbatch information into RL training, enabling Figure 3: HA-DW consists of two collaborative phases. In the first phase, an evolving difficulty anchor incorporates cross-batch historical information by propagating the models prior through history buffer, capturing long-term reward trends. In the second phase, prompt weights are adaptively adjusted based on their estimated difficulty under the models evolving state, compensating for biased advantage estimates. history-aware anchor for prompt difficulty. Second, we design an adaptive advantage reweighting algorithm to correct the induced bias."
        },
        {
            "title": "3.1 Evolving Difficulty Anchor",
            "content": "To track the evolving model state across batches, we propose the cross-batch difficulty anchor framework that integrates long-term reward trends and historical information. Let Bt denote the total number of responses in batch t. Model updates are guided by observations of the current batchs prompt accuracy yt together with historical information, defined as: yt ="
        },
        {
            "title": "Kt\nBt",
            "content": ", Kt = Bt(cid:88) i=1 rt,i. (12) We treat the models solving capability Ct as latent belief state. At training step t, the observation yt is used to update the prior belief to the posterior belief C+ via Kalman-style update (Battilotti et al., 2026; Zhang, 2026): C+ ηt [0, 1]. + ηt yt, = (1 ηt) The forgetting factor ηt controls the influence of historical information and is dynamically modulated by model stability. Specifically, we compute the average belief over the previous batches as: (13) Ct = 1 (cid:88) j=1 Ctj, (14) and define the corresponding standard deviation: σt = Ã 1 (cid:88) j= (cid:0)Ctj Ct (cid:1)2. (15) 5 The adaptive forgetting factor is then given by: ηt = η σt, (16) where η is task-dependent hyperparameter. Intuitively, larger ηt is used during early training stages to capture rapid capability shifts, while smaller ηt is adopted in later, more stable stages to preserve historical information and reduce noise. Between consecutive steps, the posterior belief serves as the prior belief for the next batch: C+ C+ (17) t+1. Overall, Ct enables the model to aggregate information across historical batches via belief updates and to condition its training strategy on this evolving belief. This evolving belief serves as historyaware anchor for the subsequent difficulty-adaptive reweighting strategy. We also provide an alternative, hard update variant of Ct in Appendix F."
        },
        {
            "title": "3.2 History Aware Adaptive Difficulty",
            "content": "Weighting (HA-DW) To rectify the inherent bias in group-based advantage estimation, we introduce HA-DW, which dynamically adjusts advantage weights based on the models evolving state while incorporating longterm reward signals. Coupled with the evolving difficulty anchor, we define the history-based prompt difficulty as: diff his = ˆpt , (18) where diff his captures both the magnitude and direction of prompts difficulty relative to the current model belief. To determine the direction of adjustment, we use the evolving difficulty anchor as reference and define: for some (0, 1/2]. Given any δ (0, 1), we can define that: Dt,i = sgn Ä ˆAt,i ä sgn Ä diff his ä , (19) ϵδ := (cid:115) Ç 1 2G log 2 δ(cid:0)1 (1 )G G(cid:1) where sgn() denotes the sign function. Next, we quantify the magnitude of adjustment using the absolute history-based difficulty: Let Mt = (cid:12) (cid:12)diff his (cid:12) (cid:12) (cid:12) (cid:12) . (20) Here, Mt measures the extent to which the prompt deviates from the models current capability. We then define the history-aware reweighting factor as: Φt,i = λscale exp(Dt,i Mt) , (21) where λscale is scaling constant, and the exponential form ensures smooth and multiplicative adjustment of advantage weights. The resulting HA-DW objective is: LHA-DW(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) ψ i=1 Å πθ(yt,i xt) πθold(yt,i xt) ã (22) Ä ˆAt,i ϕ ä Φt,i, where ψ() and ϕ() follow specific definitions in group-relative RL algorithms. Intuitively, Φt,i amplifies the estimated advantage for difficult promptswhere group-based estimation tends to be conservativeand suppresses it for easy promptswhere overestimation is prevalentthereby correcting systematic bias identified in our analysis. HA-DW can be seamlessly integrated as plug-and-play module into GRPO and its variants, improving reasoning performance under fixed rollouts while effectively mitigating biased advantage estimation. Detailed instantiations for GRPO and related algorithms are provided in the appendix B"
        },
        {
            "title": "4 Theoretical Analysis",
            "content": "In this section, we provide theoretical analysis of the effectiveness of the proposed adjustment strategy. We begin by analyzing how reweighting the empirical baseline ˆpt affects the expected bias. Lemma 1 (Baseline Rectification). Given prompt xt and the policy πθt, let pt = ˆpt be the rectified group baseline. Assume pt [, 1 ] å . (23) (24) (25) (26) It := (cid:2)ˆpt ϵδ, ˆpt + ϵδ A(p) := 1 (1 p)G pG. (cid:3) [, 1 ], Fix any ϵ > 0, we define: and: clow := sup pIt (p ϵ) A(p) p(1 pG1) , chigh := inf pIt (p + ϵ) A(p) p(1 pG1) . Then, with probability at least 1 δ conditional on S, for any choice (clow, chigh), (27) we can derive that: E[pt S] (pt ϵ, pt + ϵ). Specifically, we consider adjusting the empirical group baseline using reweighting factor c. From the perspective of the expected estimation bias, Lemma 1 that an appropriate choice of can effectively reduce estimation bias. Detailed derivations are provided in Appendix D.4. Next, we now proceed to present the main theoretical result. Theorem 3. Under the condition of Lemma 1, suppose there exists scaling factor λscale in Equation (21) such that: λscale Ñ 1 + (1chigh)ˆpt exp(Dt,iMt) 1ˆpt é 1 + (1clow)ˆpt exp(Dt,iMt) 1ˆpt Å clow exp(Dt,iMt) chigh exp(Dt,iMt) ã . (28) Then, HA-DW algorithm provably mitigates the bias of group-relative advantage: , , ó (cid:12) Eî ˆAt,i Φt,i (cid:12) (cid:12) (cid:12) Eî ˆAt,i (cid:12) (cid:12) ó At,i At,i (cid:12) (cid:12) (cid:12) . (cid:12) (cid:12) (cid:12) (29) < Algorithm MATH500 AIME25 AMC23 Minerva OlympiadBench AVG GRPO (cid:44) + HA-DW GSPO (cid:44) + HA-DW DAPO (cid:44) + HA-DW GRPO (cid:44) + HA-DW GSPO (cid:44) + HA-DW DAPO (cid:44) + HA-DW GRPO (cid:44) + HA-DW GSPO (cid:44) + HA-DW DAPO (cid:44) + HA-DW 75.4 78.0 75.8 77.6 76.8 78. 78.8 80.0 78.6 80.2 79.2 82.8 51.4 53.2 48.6 50.4 52.4 53.2 Qwen-3-4B-Base 19.6 20.4 20.0 19.6 18.3 21.3 60.3 63.4 62.2 68.6 60.0 65.0 Qwen-3-8B-Base 20.4 22.9 21.7 22.1 20.4 23.3 64.2 72.8 67.0 66.5 67.5 70.0 33.8 36.8 35.3 37.1 35.7 37.5 38.2 39.7 37.9 41.9 39.3 40.8 LLaMA-3.2-3B-Instruct 2.7 3.3 1.9 2.3 2.5 3. 31.7 35.0 30.9 32.7 35.0 37.5 22.8 23.9 23.2 22.4 22.4 24.6 43.5 44.7 42.3 43.2 43.2 45.3 46.4 47.1 45.9 47.6 47.2 50.0 19.9 20.1 19.8 21.0 20.2 22.3 46.5 48.7 47.1 49.2 46.8 49. 49.6 52.5 50.2 51.7 50.7 53.4 25.7 27.1 24.9 25.8 26.5 28.1 Table 1: Overall results across models (Qwen, LLaMA) and different group-relative RL algorithms (GRPO, GSPO, DAPO). We report the performance of different base RL algorithms, and the corresponding accuracy when applied HA-DW for each model scale and family. Figure 4: Comparison of training dynamics under different training strategies. Average accuracy across five benchmarks, training reward and response length of Qwen3-4B-Base and Qwen3-8B-Base on different training methods. Theorem 3 shows that, with an appropriate choice of the scaling parameter λscale, the HADW adjustment yields advantage estimates that are closer to the true advantage At,i in expectation. This theoretical result provides principled guidance for selecting λscale in practice."
        },
        {
            "title": "5 Experiments",
            "content": "Setups. We conduct our experiments on Qwen34B-Base, Qwen3-8B-Base (Team, 2025) and LLaMA-3.2-3B-Instruct on five common-used RLVR benchmarks. We apply our proposed method on top of several representative grouprelative reinforcement learning algorithms: GRPO, GSPO, and DAPO. We compare the performance of group-relative algorithms applying HA-DW to original ones, verifying the effectiveness and scalability of our method. We conduct RL training within the VeRL framework (Sheng et al., 2024) on single node with 8 NVIDIA A100 GPUs. More implementation details in Appendix C."
        },
        {
            "title": "5.1 Main Results",
            "content": "Our main results are presented in Table 1. Notably, group-based RL algorithms ( GRPO, GSPO and DAPO ) equipped with HA-DW outperforms original methods across five benchmarks. We observed clear and consistent improvements across benchmarks on different models of different scales and family. Overall, the results underscore that HADW compensates for advantage estimation bias via dynamic reweighting to fully leverage these overshadowed critical prompts, thereby unlocking the potential performance gains in RL. To validate our methods effectiveness in extending model capabilities, we divided the MATH500 7 Threshold MATH500 AIME AMC23 Minerva OlympiadBench AVG Base 0.4 (fixed) 0.5 (fixed) 0.6 (fixed) Ct 75.4 77.0 76.6 76.8 78. 19.6 18.5 20.0 21.3 20.4 60.3 63.1 62.7 61.1 63.4 33.8 37.5 35.7 36.4 36.8 43.5 44.3 44.0 44.3 44.7 46.5 48.1 47.8 48.0 48.7 Table 2: Ablation on the effectiveness of dynamic threshold for RL training using Qwen3-4B-Base. Ct denotes the dynamic threshold. dataset into three difficulty levels: Easy (Level 1), Mid (Levels 2-3), and Hard (Levels 4-5). We evaluated Qwen3-4B-Base trained with GRPO and GRPO+HA-DW, as shown in Figure 1(c). The performance on Easy and Mid levels was comparable for both methods, but GRPO+HA-DW outperformed GRPO by 3.4% on Hard prompts. This improvement is due to our history-based dynamic reweighting strategy, which enhances exploration on hard prompts while reducing unnecessary exploitation on easy ones. Simultaneously, it substantiates the existence of bias estimation indirectly. Training Dynamics. Figure 4 demonstrates the temporal dynamics of average accuracy across five benchmarks, training rewards throughout the training process, and response lengths of Qwen3-4BBase and Qwen3-8B-Base. RL algorithms applied HA-DW converge to higher performance plateau in accuracy and acquired higher reward compared to the original RL algorithms, suggesting that the application of HA-DW boosts the exploration of hard prompts and weakens the exploitation of easy ones by mitigating the biased advantage estimation. In addition, our method encourages longer reasoning, greatly improving its reasoning abilities (Jin et al., 2024; DeepSeek-AI, 2025). HA-DW is capable of incentivizing the model to produce more sophisticated reasoning chain of thoughts to tackle more challenging tasks. Ablation Study on Ct. We evaluate the effectiveness of the dynamic threshold Ct by comparing it with fixed thresholds across five benchmarks, as shown in Table 2. Experiments on Qwen3-4B-Base with GRPO-based training show that dynamic adjustment achieves the best performance. Removing Ct degrades performance, while fixed threshold still improves over the baseline by partially mitigating biased estimation. By incorporating crossbatch information, Ct captures long-term reward signals and further enhances RL performance. Supplementary Experiments (Appendix E) Due to space limitations, we include the following Dataset MATH500 AIME25 AMC23 Minerva OlympiadBench 8 75.4 19.6 60.3 33.8 43. 16 76.2 19.2 61.6 34.2 43.9 8+HA-DW 78.0 20.4 63.4 36.8 44.7 Table 3: Performance of Qwen3-4B-Base trained with: Rollout=8 with GRPO, Rollout=16 with GRPO and Rollout=8 with GRPO+HA-DW. Rollout=32 with GRPO is out of memory. additional experiments in Appendix E: (1) empirical verification of advantage estimation bias, (2) an ablation study on the group size  (Table 3)  , and (3) an ablation study on the scaling parameter λscale."
        },
        {
            "title": "6 Related Work",
            "content": "GRPO and GRPO Variants. Following the success of Deepseek-R1 (DeepSeek-AI, 2025), GRPO has attracted widespread attention. To achieve better performance, numerous GRPO-based variants have been proposed. Dr.GRPO removes heuristic normalizations to obtain more stable, less biased updates. DAPO stabilizes training with decoupled clipping and dynamic sampling. GSPO uses sequence-level ratios and clipping to improve stability and efficiency, especially for large and MoE models. However, these variants adopt static prompt difficulty and suffer from insufficient exploration of models capability. More related work are placed in Appendix A."
        },
        {
            "title": "7 Conclusion",
            "content": "Our work uncovers fundamental limitation of group-relative RL algorithms: biased advantage estimation. To address this issue, we propose HADW, which dynamically adjusts advantage weights based on the models evolving state. Extensive experiments demonstrate that HA-DW effectively improves reasoning performance by mitigating biased advantage estimation."
        },
        {
            "title": "Acknowledgement",
            "content": "Z.C. acknowledges the Challenge Institute for Quantum Computation (CIQC) funded by NSF through grant number OMA-2016245."
        },
        {
            "title": "Limitations",
            "content": "This work reveals an intrinsic limitation of grouprelative RLnamely, biased advantage estimation under non-degenerate samplingand proposes HA-DW to effectively mitigate this issue. Our study primarily focuses on the issue of group-wise estimation bias, restricting the application of HADW to group-relative methods. Nevertheless, estimation bias is pervasive, and future work will focus on extending this concept to broader scope."
        },
        {
            "title": "References",
            "content": "Stefano Battilotti, Alessandro Borri, Filippo Cacace, Massimiliano DAngelo, and Alfredo Germani. 2026. consensus kalman filter on L2 spaces. Autom., 183:112530. Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. 2013. Concentration Inequalities - Nonasymptotic Theory of Independence. Oxford University Press. Weizhe Chen, Sven Koenig, and Bistra Dilkina. 2025. LSPO: length-aware dynamic sampling for policy optimization in LLM reasoning. CoRR, abs/2510.01459. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Yuyang Ding, Chi Zhang, Juntao Li, Haibin Lin, Xin Liu, and Min Zhang. 2025. FAPO: flawed-aware policy optimization for efficient and reliable reasoning. CoRR, abs/2510.22543. Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. 2018. IMPALA: scalable distributed deep-rl with importance weighted actorlearner architectures. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 14061415. PMLR. Pietro Di Gianantonio and Abbas Edalat. 2025. domain-theoretic framework for conditional probability and bayesian updating in programming. CoRR, abs/2502.00949. Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, and Shuang Qiu. 2025. Segment policy optimization: Effective segment-level credit assignment in RL for large language models. CoRR, abs/2505.23564. Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition. Springer Series in Statistics. Springer. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 38283850. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, and Dacheng Tao. 2025. MAPO: mixed advantage policy optimization. CoRR, abs/2509.18849. Nan Jiang and Lihong Li. 2016. Doubly robust offpolicy value evaluation for reinforcement learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 652 661. JMLR.org. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. 2024. The impact of reasoning step In Findings of length on large language models. the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 18301842. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Anxiang Zeng, and Jinsong Su. 2025a. Spec-rl: Accelerating on-policy reinforcement learning with speculative decoding. Preprint, arXiv:2509.23232. Rollouts generated using vLLM (rollout N=8). Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025b. Understanding r1-zero-like training: critical perspective. CoRR, abs/2503.20783. Rémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. 2016. Safe and efficient offpolicy reinforcement learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 10461054. Kevin P. Murphy. 2012. Machine learning - probabilistic perspective. Adaptive computation and machine learning series. MIT Press. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs/1707.06347. Robert Serfling. 1978. Some elementary results on poisson approximation in sequence of bernoulli trials. Siam review, 20(3):567579. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300. Yuanzhe Shen, Zisu Huang, Zhengkang Guo, Yide Liu, Guanxu Chen, Ruicheng Yin, Xiaoqing Zheng, and Xuanjing Huang. 2025. Intentionreasoner: Facilitating adaptive LLM safeguards through intent reasoning and selective query refinement. CoRR, abs/2508.20151. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Wei Sun, Wen Yang, Pu Jian, Qianlong Du, Fuwei Cui, Shuo Ren, and Jiajun Zhang. 2025. KTAE: modelfree algorithm to key-tokens advantage estimation in mathematical reasoning. CoRR, abs/2505.16826. Zelin Tan, Hejia Geng, Mulei Zhang, Xiaohang Yu, Guancheng Wan, Yifan Zhou, Qiang He, Xiangyuan Xue, Heng Zhou, Yutao Fan, Zhongzhi Li, Zaibin Zhang, Guibin Zhang, Chen Zhang, Zhenfei Yin, and Lei Bai. 2025. Scaling behaviors of LLM reinforcement learning post-training: An empirical study in mathematical reasoning. CoRR, abs/2509.25300. Zichang Tan, Ajian Liu, Jun Wan, Hao Li, Zhen Lei, Guodong Guo, and Stan Z. Li. 2022. Cross-batch hard example mining with pseudo large batch for IEEE Trans. Image ID vs. spot face recognition. Process., 31:32243235. Qwen Team. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. 2020. On mutual information maximization for representation learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Jinpeng Wang, Jieming Zhu, and Xiuqiang He. 2021. Cross-batch negative sampling for training two-tower recommenders. In SIGIR 21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, pages 16321636. ACM. Xun Wang, Haozhi Zhang, Weilin Huang, and Matthew R. Scott. 2020. Cross-batch memory for embedding learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 63876396. Computer Vision Foundation / IEEE. Xinran Wu, Kun Yue, Huashuai Liu, and Liang Duan. 2025. Learning conditional probability distributions for robust probabilistic inference in bayesian network. In Proceedings of the 34th ACM International Conference on Information and Knowledge Management, CIKM 2025, Seoul, Republic of Korea, November 10-14, 2025, pages 34383447. ACM. Xuan Xie, Xuan Wang, and Wenjie Wang. 2025. Dagrpo: Rectifying gradient conflict in reasoning via distinctiveness-aware group relative policy optimization. arXiv preprint arXiv:2512.06337. Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, and Tong Zhang. 2025. Reinforce-ada: An adaptive sampling framework for reinforce-style LLM training. CoRR, abs/2510.04996. Huei-Fang Yang, Kevin Lin, and Chu-Song Chen. 2016. Cross-batch reference learning for deep classification and retrieval. In Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016, Amsterdam, The Netherlands, October 15-19, 2016, pages 12371246. ACM. Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, and Rihui Xin. 2025. DCPO: dynamic clipping policy optimization. CoRR, abs/2509.02333. Zhuliang Yao, Yue Cao, Shuxin Zheng, Gao Huang, and Stephen Lin. 2021. Cross-iteration batch normalization. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 1233112340. Computer Vision Foundation / IEEE. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476. Enci Zhang, Xingang Yan, Wei Lin, Tianxiang Zhang, and Qianchun Lu. 2025. Learning like humans: Advancing LLM reasoning capabilities via adaptive difficulty curriculum learning and expert-guided selfreformulation. CoRR, abs/2505.08364. Qinghua Zhang. 2026. Stability analysis of the kalman filter under practical conditions. Autom., 183:112670. Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, and Furu Wei. 2025. Geometric-mean policy optimization. CoRR, abs/2507.20673. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. 2025. Group sequence policy optimization. CoRR, abs/2507.18071. appendix"
        },
        {
            "title": "A More Related Work",
            "content": "Group-based RLVR. Recent studies have proposed numerous improvements to group-based reinforcement learning algorithms. DaGRPO (Xie et al., 2025) tackles GRPOs instability and poor sample efficiency (caused by low distinctiveness in on-policy rollouts) by introducing sequence-level gradient rectification to filter low-distinctiveness pairs and off-policy anchor augmentation to restore learning signals on hard prompts. To address the advantage reversion and advantage mirror issues of fixed advantage formulations in GRPO that fail to adapt to samples with varying trajectory certainty, MAPO (Huang et al., 2025) introduces Advantage Percent Deviation (APD) for high-certainty trajectories and Trajectory Certainty Reweight (TCR) to dynamically reweight the advantage function, enabling adaptive and reliable trajectory evaluation. LSPO (Chen et al., 2025) adopts length-aware dynamic sampling to retain shortest/longest responses, addressing the ineffectiveness of RLVR training for LLM reasoning. GMPO (Zhao et al., 2025) uses the geometric mean of token-level rewards (replacing GRPOs arithmetic mean) to resolve unstable policy updates from outlier importance sampling ratios. And DCPO (Yang et al., 2025) uses dynamic adaptive clipping and smooth advantage standardization to solve zero limited token exploration, and low gradients, response utilization in RLVR. FAPO (Ding et al., 2025) uses generative reward model (GenRM) to detect flawed-positive rollouts and parameter-free reward penalty, addressing unreliable reasoning patterns and performance limitations caused by such rollouts in RLVR. SPO (Guo et al., 2025) uses segment-level advantage estimation (with Monte Carlo sampling and flexible segmentation) to solve inaccurate advantage estimation of token-level methods and imprecise credit assignment of trajectory-level methods in LLM reinforcement learning. KTAE (Sun et al., 2025) uses statistical analysis to quantify tokens association with correct rollouts and combines it with rollout-level advantages, solving the coarse granularity issue of GRPO that ignores token-specific contributions. Leverage of Cross-batch Signals. Cross-batch signals have found widespread application across numerous domains. XBM (Wang et al., 2020) improves embedding learning by leveraging memory from previous batches to enhance the consistency and quality of embeddings. CBNS (Wang et al., 2021) introduces method to improve negative sampling in embedding learning by utilizing negative samples from different batches, enhancing the models ability to learn more robust and generalized representations. CIBN (Yao et al., 2021) extends traditional batch normalization across iterations, rather than within single batch, to improve model convergence and generalization. CBRL (Yang et al., 2016) utilizes reference samples from different batches during training to improve the learning of deep classification and retrieval models. CBHEM-PLB (Tan et al., 2022) combines cross-batch hard example mining with pseudo large batch strategy to improve face recognition models. Biased Estimation. Considerable research effort has been directed towards addressing the critical challenge of biased estimation. The BiasVariance Tradeoff theory (Hastie et al., 2009; Murphy, 2012) suggests that as models complexity increases, its bias decreases but its variance increases, and vice versa. It emphasizes that there is balance between bias and variance that affects the overall error in model predictions, and finding the optimal model complexity is crucial to minimize both bias and variance. Retrace (Munos et al., 2016) addresses the challenge of bias estimation in off-policy reinforcement learning. It proposes retracing technique to mitigate the bias caused by off-policy data, which can lead to inaccurate value estimates. V-trace (Espeholt et al., 2018; Boucheron et al., 2013) introduces method for improving off-policy reinforcement learning by applying importanceweighted corrections to the value function updates in actor-critic algorithms, mitigating bias in offpolicy data. DR-OVR (Jiang and Li, 2016; Tschannen et al., 2020) combines importance sampling and regression to correct for bias in off-policy value estimation, making it more stable and accurate."
        },
        {
            "title": "Related Algorithms",
            "content": "In this section, we present detailed instantiations of three group-relative reinforcement learning algorithms: GRPO, GSPO, and DAPO. And and τ denote training step and token index in this part. GRPO streamlines PPO by discarding the value network without compromising stability. Instead 12 of fitting baseline, it derives the advantage using group-relative normalization. This groupnormalized advantage is then assigned uniformly to all tokens in the response, formulating the clipped surrogate loss: JGRPO(θ) = 1 G (cid:88) i=1 1 ot,i ot,i (cid:88) τ =1 min Ä rt,i,τ (θ) ˆAt,i,τ , clip (rt,i,τ (θ), 1 ϵ, 1 + ϵ) ˆAt,i,τ , (30) where ϵ is the clipping hyperparameter and rt,i,τ is is the importance sampling ratio comparing the new and old policy: ä rt,i,τ (θ) = πθ(yt,i,τ xt, yt,i,<τ ) πθold(yt,i,τ xt, yt,i,<τ ) . (31) And GRPO defines the group advantage by subtracting the average reward of the group and normalizing by its standard deviation: ˆAt,i,τ = R(xt, ot,τ ) mean Ä {R(xt, ot,j)}G j=1 Ä std {R(xt, ot,j)}G j= ä ä , (32) where R(x, o) denotes the reward function."
        },
        {
            "title": "The objective function of GRPO applied with",
            "content": "HA-DW can be denoted as: JGRPO+HA-DW(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 ot,i ot,i (cid:88) τ =1 Ä min rt,i,τ (θ) ˆAt,i,τ Φt,i, clip (rt,i,τ (θ), 1 ϵ, 1 + ϵ) ˆAt,i,τ Φt,i ä , (33) where Φt,i is the history-aware reweighting factor defined earlier. GSPO optimizes policy learning by defining importance ratios at the sequence level, eliminating the need for critic model. Rather than relying on separate value network, it computes advantages through normalized relative rewards of group responses. This sequence-level advantage is directly used for policy updates without token-level processing, yielding the following objective function: JGSPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) Å rt,i(θ) ˆAt,i, min i=1 Å clip rt,i(θ), 1 ϵ, 1 + ϵ ã ˆAt,i ã , (34) 13 where the sequence-level importance sampling ratio ri(θ) can be denoted as: rt,i(θ) = = πθ(yt,ixt) πθold(yt,ixt) (cid:81)yt,i τ =1 πθ(yt,i,τ xt, yt,i,<τ ) t=1 πθold(yt,i,τ xt, yt,i,<τ ) (cid:81)yt,i (35) , where the advantage for GSPO can be denoted as: ˆAt,i = R(xt, ot,i) mean Ä {R(xt, ot,j)}G j= ä std Ä {R(xt, ot,j)}G j=1 ä (36) And the objective function of GSPO+HA-DW is: JGSPO+HA-DW(θ) Å (cid:88) rt,i(θ) ˆAt,i Φt,i, min = 1 Å i=1 clip rt,i(θ), 1 ϵ, 1 + ϵ (37) ã . ã ˆAt,i Φt,i DAPOs key feature is operating at the token level instead of treating full responses as single units, ensuring each token in sampled output oi contributes proportionally to gradient updates. This fine-grained optimization boosts training stability and delivers more informative feedback for LLMs. The objective function is defined as: JDAPO(θ) = 1 i=1 ot,i (cid:80)G (cid:88) ot,i (cid:88) i= τ =1 Å rt,i,τ (θ) ˆAt,i,τ , min clip(cid:0)rt,i,τ (θ), 1 ϵ, 1 + ϵ(cid:1) ˆAt,i,τ ã . (38) DAPO introduces two key mechanisms: decoupled clipping and dynamic sampling, to address the limitations of traditional group-based methods. Decoupled clipping refines the trust region for more stable updates, while dynamic sampling mitigates estimation bias by adaptively reweighting samples based on their distribution. Applying HA-DW on Equation (38), and we have: JDAPO+HA-DW(θ) = 1 i=1 ot,i (cid:80)G (cid:88) ot,i (cid:88) i=1 τ =1 Å rt,i,τ (θ) ˆAt,i,τ Φt,i, min clip(cid:0)rt,i,τ (θ), 1 ϵ, 1 + ϵ(cid:1) ˆAt,i,τ Φt,i ã . (39)"
        },
        {
            "title": "C Setup Details",
            "content": "Models & Datasets. We conduct our experiments on Qwen3-4B-Base, Qwen3-8B-Base (Team, 2025) and LLaMA-3.2-3B-Instruct to assess the mathematical reasoning performance of different algorithms across models of varying scales and family. Our training dataset is sourced from MATH dataset (Hendrycks et al., 2021; Lightman et al., 2024) which contains 7.5k questions for training. Our evaluation suite includes: MATH500 (Hendrycks et al., 2021), AMC23, AIME25, Minerva, and OlympiadBench (He et al., 2024). To mitigate high variance on small benchmark sets and obtain reliable results, we report avg@16 on AIME25 and AMC23. Baseline. We apply our proposed method on top of several representative group-relative reinforcement learning algorithms: GRPO, GSPO, and DAPO. We compare the performance of group-relative algorithms applying HA-DW to original ones, verifying the effectiveness and scalability of our method. Implementation Details. We conduct RL training within the VeRL framework (Sheng et al., 2024) on single node with 8 NVIDIA A100 GPUs. All experiments use maximum prompt batch size of 1,024 and maximum response length of 4,096. More hyperparameter details are provides in appendix C. Training Hyperparameters. The detailed hyperparameters used during our training process on 6 different methods of 3 models (Qwen34B-Base, Qwen3-8B-Base and LLaMA-3.2-3BInstruct) used in our experiments are demonstrated in Table 8."
        },
        {
            "title": "D Theoretical Proof",
            "content": "D.1 Proof of Theorem 1 In group-relative RL algorithms, the truncation mechanism will discard prompts with all-correct or all-incorrect responses. Under the binary reward setting, the retention condition for the total reward within the group is given by: := {1 1}. Under the retention condition S, [ˆpt S] denotes the conditional expectation of the empirical estimation ˆpt = R/G. And it can be derived what the relationship is between it and the expected reward pt: [ˆpt S] = ò ï (cid:2)R 1{S} P(S) 1 (cid:3) = = = 1 1 E[R] (cid:2)R 1{R=G} P(S) (cid:3) Gpt GP(R = G) P(S) (40) , = pt pG 1 (1 pt)G pG where the indicator function 1{S} takes the value 1 if the event occurs and 0 otherwise. Through the conditional expectation of ˆpt, we can obtain that its expected value is less than pt when pt < 1 2 and the baseline tends to be underestimated. Conversely, when pt > 1 2 , the expected value exceeds pt, leading to an overestimation. Based on Equation (2) and Equation (4), inaccurate baseline estimation will induce biased advantage estimation. From the foregoing analysis, we can derive that: Eî ˆAt,i Eî ˆAt,i Eî ˆAt,i if and only if pt = 0.5. if pt < 0.5; if pt > 0.5; = At,i, < At,i, > At,i, ó ó ó (7) Lemma 2. Under the condition of Theorem 1, the bias induced by the group-relative advantage is formulated as: ó At,i î ˆAt,i pt(1 pt)G + pG+1 1 (1 pt)G pG pG (41) . Proof. = = [ˆpt S] pt pt(1 pt)G + pG+1 pG . (42) 1 (1 pt)G pG Replacing the baseline with the advantage completes the proof. D.2 Proof of Theorem 2 and Corollary 1 For hard prompts, in Theorem 2, we have: (ˆpt pt > ϵ S) (cid:80)G1 k=G(pt+ϵ)+1 (cid:1) pk (cid:0)G 1 (1 pt)G pG (1 pt)Gk (10) . = 14 The above equation is given by the following argument: The conditioning event restricts the sample space by excluding the outcome {0, G} (hence under we only keep {1, . . . , 1}). Let: m(pt) := G(pt + ϵ) + 1. (43) Therefore, within the event S, the deviation event becomes = {R m(pt)} {1 1} = {m(pt) 1}. (44) By definition of conditional probability, the numerator is the (unconditional) probability mass of all outcomes that satisfy the deviation requirement ˆp > ϵ and simultaneously, and satisfy the restriction imposed by S. Because is binomial, for any integer we have: P(R = k) = ÇG å (1 pt)Gk. pk (45) Summing over all admissible counts {m(pt), m(pt) + 1, . . . , 1} yields: P(A S) = G1 (cid:88) P(R = k) k=m(pt) G1 (cid:88) = k=m(pt) ÇG å (1 pt)Gk. pk (46) (47) Thus, based on the formula of conditional probability (Wu et al., 2025; Gianantonio and Edalat, 2025), we can derive the conclusion of Theorem 2. According to Theorem 2, we can formulate: (G, pt) := P(ˆpt pt > ϵ S). (48) Assume that pt follows uniform distribution. And we define: P(G, pt1, pt2) := 1 pt2 pt (cid:90) pt2 (G, pt)dpt pt1 (49) where pt1 and pt2 is the expected reward. And P(G) reflects the probability that, when is fixed, the baseline ˆpt is overestimated of group-relative RL algorithms over certain expected-reward interval. For hard prompts with pt (0, 0.25) under different group size G, when [2, 8], we have: Similarly, we can calculate hard prompts with 4 6 8 P(G, 0, 0.25) 0.999997499987 0. 0.827761785622 0.781129955681 Table 4: P(G, 0, 0.25) as function of [2, 8]. 2 6 8 P(G, 0, 0.5) 0.999994999975 0.776965795853 0. 0.640944744224 Table 5: P(G, 0, 0.5) as function of [2, 8]. We can conclude from Table 4, when 2 8, P(G, 0, 0.25) > 0.78. This reveals that for hard prompts whose pt (0, 0.25) when is limited, its baseline ˆpt of group-relative RL algorithms is substantially likely to be overestimated. Similarly, due to the evident symmetry of the group-relative methods, for easy prompt with pt (0.75, 1), the baseline ˆpt is underestimated with the same probability distribution. Based on the aforementioned conclusions, for group-based algorithms, when [2, 8], the probability of biased advantage estimation can be denoted as: PÄ ˆAt,i < At,i S, pt < 0.25 ä ä PÄ ˆAt,i > At,i S, pt > 0.75 > 0.78, > 0.78. (8) Similarlly, Table 5 can give PÄ ˆAt,i < At,i S, pt < 0.5 ä ä PÄ ˆAt,i > At,i S, pt > 0.5 > 0.63, > 0.63. (50) D.3 Proof of Corollary 2 and Corollary 3 Let be large integer, for hard prompts, according to Theorem 2, we have: (ˆpt pt > ϵ S) = (cid:80)G k=G(pt+ϵ)+1 (cid:1) pk (cid:0)G 1 (1 pt)G pG (1 pt)Gk (10) . And we define: pt (0, 0.5) under different group size G. (pt) := P(ˆpt pt > ϵ S). (51) 15 We analyze the integral in the limit of large using the Poisson approximation (Serfling, 1978). Let us perform the change of variable xt = Gpt. The limits of integration change from [1/G, 2/G] to [1, 2], and dpt = dxt/G. We define the integral of interest: P(G1, G2) = = G2 G1 (cid:90) 2 (xt/G) dxt. (cid:90) G2 f (xt/G) dxt 2 3 5 6 (cid:82) 2/G 0 (pt) dpt 2 0.499997499987 0.749995833315 0. 0.780787089465 0.781154327380 (52) Table 6: 2 (cid:82) 2/G 0 (pt) dpt as function of [2, 6]. 1 First, we determine the summation lower bound m(pt). For pt [1/G, 2/G), we have Gpt [1, 2). Consequently, Gpt = 1, which implies: m(pt) = Gpt + 1 = 2. (53) Next, we approximate the binomial terms. In the limit with Gpt = fixed, the binomial distribution converges to the Poisson distribution with parameter xt. The denominator Z(pt) approximates to: Z(pt) = 1 (xt/G)G (1 xt/G)G 1 ext. (54) The numerator is the probability that Poisson random variable Pois(xt) takes value 2 (ignoring the upper limit 1 as the Poisson tail vanishes exponentially): Thus, 6 is sufficiently large to have Å ˆAt,i < At,i S, pt < ã 2 > 0.78 . (57) Proof of Corollary 3. On we have 1, hence ˆp = X/G 1/G. Since < 1/G, it follows that ˆp 1/G > p. This leads to the Corollary. D.4 Proof of Lemma 1 and Theorem D.4.1 Proof of Lemma 1 Lemma 3. Define the non-degenerate event := {1 1}, and ϵ (0, pt ˆpt). If Ç (pt ϵ) (cid:0)1 (1 pt)G pG pt(1 pG1 (pt + ϵ) (cid:0)1 (1 pt)G pG pt(1 pG1 (58) å (cid:1) (cid:1) ) ) , , t we have (pt) = P(K 2) = (cid:88) k= ext xk k! = 1 P(K = 0) P(K = 1) = 1 ext xext = 1 ext(1 + xt). E[pt S] (pt ϵ, pt + ϵ) . (59) (55) Proof. We define the adjusted factor to compensate for the bias in the advantage estimation which applied on empirical group baseline ˆpt. The globally scaled estimator can be approximated as: Substituting these approximations into (xt/G), we obtain the limiting integrand h(xt): h(xt) = 1 ext(1 + xt) 1 ext xtext 1 ext = 1 = 1 (56) xt ext 1 . Assume that pt follows uniform distribution. Calculating Equation (56) numerically, for sufficiently large G, we can show that P(0, 2) = P(0,1)+(cid:82) 2 1 h(xt) dxt 2 2 (cid:82) 2 0.7818. 0 (xt) dxt = Next, we use numerical computation to show how large should be, whose result can be found in the following table: pt(R) := ˆpt = c"
        },
        {
            "title": "R\nG",
            "content": ". (60) We can derive the conditional expectation of pt on non-degenerate event = {1 1}: E[pt S] = c = = ò ï (cid:12) (cid:12) (cid:12) (cid:12) E[R S] E[R 1{S}] P(S) . And we have: E[R 1{S}] = G1 (cid:88) k=1 P(R = k) = E[R] GP(R = G). (61) (62) (cid:80)G the only term excluded from Because k=0 kP(R = k) = E[R] is the = term (the = 0 term is zero anyway). Using E[R] = Gpt and P(R = G) = pG , we can obtain: E[R 1{S}] = Gpt GpG = Gpt(1 pG ). (63) Thus, we have: = = (pt ϵ) (cid:0)1 (1 pt)G pG pt(1 pG1 ã ) Å (cid:1) (72) 1 c. ϵ pt We can conclude that when: Therefore: E[pt S] = = pt ) Gpt(1 pG1 1 (1 pt)G pG 1 pG1 1 (1 pt)G pG (64) , Ç (pt ϵ) (cid:0)1 (1 pt)G pG pt(1 pG1 (pt + ϵ) (cid:0)1 (1 pt)G pG pt(1 pG1 (cid:1) ) ) t (cid:1) , å (73) , which proves the stated conditional expectation formula. To mitigate the biased estimation, let: we have E[pt S] (pt ϵ, pt + ϵ) . (74) E[pt S] = pt. (65) And we can solve: pt 1 pG1 1 (1 pt)G pG = pt. (66) The analytical solution for this equation is: = 1 (1 pt)G pG 1 pG1 . (67) When the adjustment coefficient falls within specific range of values, we will have pt pt < ˆpt pt. We first let: ϵ = ˆpt pt. (68) For E[ pt S] = pt + ϵ, solve: c+ pt 1 pG1 1 (1 pt)G pG = pt + ϵ. (69) And we can derive: Lemma 4 (pt-free concentration under S). Define the non-degenerate event := {1 1}. Assume pt [, 1 ] for some (0, 1/2]. Then for any ζ > 0, we have: P(ˆpt pt < ζ S) 1 2 exp(2Gζ 2) (1 )G 1 (1 )G . (75) Proof. Let := {ˆpt pt < ζ}. By the definition of conditional probability: P(A S) = P(A S) P(S) . (76) We lower bound the numerator. Since , we have: P(A S) P(A) P(S ). (77) Next, note that = {S = 0} {S = G} and these two events are disjoint. Therefore: c+ = = (pt + ϵ) (cid:0)1 (1 pt)G pG pt(1 pG1 ã ) Å 1 + c. ϵ pt (cid:1) (70) P(S ) = P(S = 0) + P(S = G) = (1 pt)G + pG . (78) Moreover, we can derive that: For E[ pt S] = pt ϵ, solve: P(S) = 1 P(S ) = 1 (1 pt)G pG . (79) pt 1 pG1 1 (1 pt)G pG = pt ϵ. (71) We now lower bound P(A) using Hoeffdings inequality. Since each rt,i [0, 1] almost surely 17 and {rt,i}G Hoeffdings inequality yields: i=1 are independent with E[rt,i] = pt, P(ˆpt pt ζ) 2 exp(2Gζ 2), (80) equivalently: P(A) = P(ˆpt pt < ζ) 1 2 exp(2Gζ 2). (81) It remains to remove the dependence on pt in P(S). Define (p) := pG + (1 p)G. For 1, is symmetric around 1/2 and attains its maximum over [, 1 ] at the endpoints. Hence: (1pt)G+pG = (pt) () = (1)G+G, (82) which implies: P(S) = 1 (pt) 1 (1 )G G. (83) we have P(ˆpt pt γ S) δ, equivalently, P(ˆpt pt < γ S) 1 δ, which proves Equation (86). Lemma 6 (A pt-free feasible range of expressed via ˆpt). Assume the conditions of Lemma 4 and define: (cid:115) Ç 1 2G log ϵδ := 2 δ(cid:0)1 (1 )G G(cid:1) Let: It := (cid:2)ˆpt ϵδ, ˆpt + ϵδ A(p) := 1 (1 p)G pG. (cid:3) [, 1 ], Fix any ϵ > 0, we define: Combining Equation (76) and (77) with Equation (81), (82), and (83), we can obtain that: and: P(A S) (84) clow := sup pIt (p ϵ) A(p) p(1 pG1) , chigh := inf pIt (p + ϵ) A(p) p(1 pG1) . å . (87) (88) (89) (90) P(A) P(S ) P(S) (cid:2)1 2 exp(2Gζ 2)(cid:3) (cid:2)(1 )G + G(cid:3) 1 (1 )G , (85) which completes the proof. Lemma 5 (Conditional pt-free concentration under S). Assume pt [, 1] for some (0, 1/2]. Then for any δ (0, 1), with probability at least 1 δ conditional on S, we have: ˆptpt < (cid:115) Ç 1 2G log 2 δ(cid:0)1 (1 )G G(cid:1) å . (86) Proof. Now choose γ such that the right-hand side of Equation (75) is at most δ, i.e.: 2 exp(2Gγ2) 1 (1 )G δ. Solving for γ gives: (cid:115) Ç 1 2G log γ 2 δ(cid:0)1 (1 )G G(cid:1) å . Therefore, for γ := (cid:115) Ç 1 2G log 2 δ(cid:0)1 (1 )G G(cid:1) å , Then, on the event {ˆpt pt < ϵδ} (which holds with probability at least 1 δ conditional on S), any choice (clow, chigh) (91) implies that the condition (73) holds for the true pt, and hence: E[pt S] (pt ϵ, pt + ϵ). D.4.2 Proof of Theorem 3 When applying adjustment on the advantage ˆAt,i, we do not consider the standard deviation here, and assume that: Φt,i ˆAt,i = rt,i pt = rt,i cˆpt. (92) It is equivalent to: Φt,irt,i Φt,i ˆpt = rt,i cˆpt. (93) And for correct responses with rt,i = 1: Φt,i = 1 cˆpt 1 ˆpt . While for incorrect responses with rt,i = 0: Φt,i = c. According to Equation (21): (94) (95) Φt,i = λscale exp (Dt,i Mt) , (96) Overall, since the difficulty does not affect the expressions, we can further derive Equation (104) as follows: Ñ é 1 + (1chigh)ˆpt exp (Dt,i Mt) 1ˆpt , 1 + (1clow)ˆpt exp (Dt,i Mt) 1ˆpt λscale Å clow exp (Dt,i Mt) , chigh exp (Dt,i Mt) ã . (105) When Equation (105) holds, our method HADW is efficient in compensating biased advantage estimation. D.5 Non-binary Reward Analysis In this section, we extend our analysis to continuous bounded reward distributions (e.g., Beta and truncated Gaussian scores), which better reflect the behavior of soft verifiers and learned reward models commonly used in practice. Our extended analysis demonstrates that, under these more general reward assumptions, the group-relative advantage estimator remains systematically biased in an analogous manner: it tends to underestimate the true advantage for hard prompts and overestimate the true advantage for easy prompts. Moreover, as prompt difficulty becomes more extreme (i.e., as increases), the magnitude of this bias becomes increasingly pronounced. Next, we show the main results. Theorem 4. At training step and let 2, with CDF and PDF . Given prompt xt and draw 2 i.i.d. rewards: rt,1, . . . , rt,G i.i.d. D(pt). (106) And we extend the binary reward setting to nonbinary rewards: rt,i {0, 1} rt,i [0, 1]. (107) The group-relative advantage can be denoted as: ˆAt,i := rt,i ˆpt, ˆpt ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 rt,i, (2) while the expected advantage is defined as: the adjustment of At,i can be categorized into four types. For responses in defined hard prompts with rt,i = 1, the adjusted advantage can be denoted as: t,i = λscale exp (Mt) ˆAt,i. (97) ˆA1 For hard prompts, we have (0, 1) and ˆp (0, 1). Based on Equation (73) and Lemma 6, to mitigate biased estimation, λscale satisfies: λ1 scale Ñ 1 + (1chard high )ˆpt 1ˆpt exp (Mt) , 1 + (1chard low )ˆpt 1ˆpt exp (Mt) é . (98) And for incorrect responses in hard prompts, we have: λscale exp (Mt) ˆAt,i. (99) ˆA2 t,i = And we can set: Ä λ2 scale low exp (Mt), chard chard . (100) For easy prompts, we have > 1 and ˆp (0, 1), high exp (Mt) ä thus for correct answers: ÇÇ λ3 scale 1 + high)ˆpt (1 ceasy 1 ˆpt (1 ceasy 1 ˆpt low )ˆpt å exp(Mt), ã exp(Mt) Å 1 + And for incorrect responses: λ4 scale Ç ceasy low exp (Mt) , ceasy high exp (Mt) å . In training process with HA-DW, to rectify the biased advantage estimation, there exists specfic λscale supposing to satisfy: λscale λ1 scale λ2 scale λ3 scale λ scale (103) which denotes: λscale Ñ 1 + (1chard low )ˆpt 1ˆpt exp (Mt) , 1 + (1chard low )ˆpt 1ˆpt exp (Mt) é ã . (101) (102) å exp(Mt), ã ÇÇ 1 + high)ˆpt (1 ceasy 1 ˆpt low )ˆpt Å ã 1 + (1 ceasy 1 ˆpt Ä low exp (Mt), chard chard å Ç ceasy low exp (Mt) ceasy high exp (Mt) , exp(Mt) At,i := rt,i pt. (108) ä low exp (Mt) Fix constant σ [0, 1] and define the update event: . (104) 19 Sσ := (cid:8) = : rt,i rt,j > σ(cid:9) rt,i σ(cid:9). σ = (cid:8)max rt,i min Sc (109) For [0, 1], define u+ := min{1, + σ}, we have: q(u) := (u+) (u), (110) and: m(u) := E[rt,1 rt,1 u+] (cid:82) u+ xf (x) dx (u+) (u) = (when q(u) > 0). (111) Then the probability of non-update is: P(Sc σ) = (cid:90) 1 0 (u) q(u)G1 du, (112) By symmetry, the conditional mean of each of the 1 non-minimum samples is m(u), hence: (cid:104) (cid:88) rt,i i=1 (cid:12) (cid:12) Sc (cid:12) σ, min = (cid:105) = + (G 1)m(u). (121) So we can derive: E[ˆpt 1{Sc (cid:90) 1 σ}] = 0 (cid:90) 1 0 = + (G 1)m(u) dP(min du, Sc σ) (u + (G 1)m(u))f (u)q(u)G1du. (122) and: P(Sσ) = 1 P(Sc σ). (113) Corollary 4. For Beta(α, β) reward distribution, the Beta density is: Moreover, we have: E[ˆpt Sσ] = pt E[ˆpt 1{Sc P(Sσ) σ}] (114) with: E[ˆpt 1{Sc (cid:90) 1 σ}] = (cid:0)u + (G 1)m(u)(cid:1) (u) q(u)G1 du. 0 (115) Finally, the conditional bias transferred to advantages satisfies, for all i, we have: E[ ˆAt,i At,i Sσ] = pt E[ˆpt Sσ]. (116) Proof. The complement event can be denoted as: (x) = xα1(1 x)β1 B(α, β) , (123) and the CDF is: (x) = Ix(α, β) for [0, 1], (124) where B(, ) is the Beta function and Ix(α, β) is the regularized incomplete beta function. In particular: α α + β . (125) pt = E[rt,1] = Moreover, we have: q(u) = (u+) (u) = Iu+(α, β) Iu(α, β), (126) Sc σ = {max min σ} (117) and the conditional mean over [u, u+] admits the closed form: For absolutely continuous i.i.d. samples, the minimum has density: gmin(u) = Gf (u)(cid:0)1 (u)(cid:1)G1. (118) Condition on min = u. The remaining 1 samples are i.i.d. with the original law conditioned on [u, 1]; imposing max u+ is equivalent to requiring each of those samples lies in [u, u+]. Thus: P(Sc σ min = u) = (cid:16) (u+) (u) 1 (u) (cid:17)G1 , (119) and multiplying by gmin(u) gives: P(Sc σ) = (cid:90) 1 0 (u) q(u)G1du. (120) m(u) = (cid:82) u+ xf (x) dx (cid:82) u+ (x) dx (127) = Bu+(α + 1, β) Bu(α + 1, β) Bu+(α, β) Bu(α, β) , where Bx(, ) denotes the (unregularized) incomplete beta function. Consequently, substituting F, f, q, into conclusions obtained earlier yields explicit onedimensional integral expressions (in standard special functions) for P(Sc Corollary 5. Let the reward Zt,1, . . . , Zt,G be i.i.d. (µ, ξ2) with ξ > 0, and define rt,i to be properly truncated to [0, 1], i.e. rt,i has the conditional law: σ) and E[ˆpt Sσ]."
        },
        {
            "title": "On Sc",
            "content": "σ and min = u, one sample equals the minimum and the other G1 samples lie in [u, u+]. rt,i d= Zt,i (cid:12) (cid:12) (0 Zt,i 1), = 1, . . . , G. (128) Figure 5 illustrates two representative cases corresponding to group sizes = 4 and = 8, as predicted by Corollary 5. In both settings, the magnitude of the bias At,i E[ ˆAt,i Sσ] increases as pt moves farther away from 0.5, corroborating our theoretical analysis. Let u+ := min{1, + c} and define, for [0, 1] with q(u) > 0, we have: q(u) := P(cid:0)u rt,1 u+(cid:1), (129) and: m(u) := E(cid:2)rt,1 rt,1 u+(cid:3). (130) Let Φ and φ be the standard normal CDF and PDF, and set: := 0 µ ξ , := 1 µ ξ . (131) Then the truncated-normal density on [0, 1] is: (x) = φ Ä xµ ξ ä σ(cid:0)Φ(b) Φ(a)(cid:1) 1[0,1](x). (132) Its CDF on [0, 1] is: (x) = Ä xµ ξ Φ ä Φ(a) Φ(b) Φ(a) . (133) Figure 5: Illustration of advantage bias under truncated Gaussian rewards for different group sizes. The mean satisfies: pt = E[rt,1] = µ + ξ φ(a) φ(b) Φ(b) Φ(a) . (134) Moreover: q(u) = (u+) (u), (135) and the conditional mean over [u, u+] has the standard truncated-normal form: m(u) = µ + σ ä Ä uµ ξ Ä u+µ ξ φ Φ φ ä Φ Ä u+µ ξ Ä uµ ξ ä ä . (136) Consequently, substituting F, f, q, to yield explicit one-dimensional integral expressions for P(Sc σ) and E[ˆpt Sσ] in terms of Φ and φ. Remark. Theoretical and numerical evaluations under properly truncated Gaussian reward distributions indicate that the group- (cid:12) (cid:12) (cid:12)At,i E[ ˆAt,i Sσ] (cid:12) (cid:12) relative advantage bias, (cid:12), tends to increase as pt deviates further from 1/2. Intuitively, near these extremes, typical group of samples exhibits reduced dispersion. As result, conditioning on the non-degenerate event Sσ preferentially selects groups with atypically large variability, which in turn shifts the conditional expectation of the advantage away from its true value."
        },
        {
            "title": "E Supplementary Experiments",
            "content": "E.1 Advantage Distribution We conducted an assessment of select prompts from the widely used training dataset MATH and DAPO-Math-17k (Yu et al., 2025) on Qwen3-4BBase across different rollouts. At first, we evaluated the models performance on the dataset at rollout=8. From these, we selected four groups of 50 prompts each: groups with single correct or incorrect response. We then evaluated the outcomes of these selected prompts at rollout=128 where enough rollouts can reflect intrinsic difficulty of these prompts. For those groups with only 1 correct responses at rollout=8, the distribution of the number of correct responses within these groups is shown in Figure 6(a). For the MATH and DAPO-Math-17k datasets, 24 and 15 groups have fewer than 16 correct responses at rollout=128 respectively which suggests that the advantage of correct responses for these prompts are underestimated at rollout=8. And these distinct responses in these most challenging prompts are crucial for pushing the models capability frontier, requiring more exploration. Similarly, for prompts with 1 incorrect answer at rollout=8 and we find that 12 and 21 groups have less than 16 incorrect responses with 128 rollouts on MATH and DAPO-Math-17k which may lead to over-exploitation as Figure 6(b). 21 E.2 Ablation Study on be rewritten as: C+ = 1 + 1 yt = 1 é ytj + yt , Ñh1 (cid:88) j= (137) which indicates that the belief update is effectuated by directly synthesizing the accuracy information derived from the preceding batches with observations from the current iteration, and we leave the remaining update procedures intact. Although this formulation ignores short-term oscillations in belief updates, it significantly simplifies the overall algorithm."
        },
        {
            "title": "G Prompt",
            "content": "Prompt Template {question}. Lets think step by step and output the final answer within boxed{}."
        },
        {
            "title": "H Case Study",
            "content": "This appendix demonstrates some output examples generated by policy models trained with GRPO and GRPO+HA-DW. And the results are shown in Figure 7 and Figure 8. It is widely accepted consensus that increasing the number of rollouts effectively mitigates estimation bias (Xiong et al., 2025). As the group size grows, the empirical advantage distribution converges closer to the true advantage distribution, thereby reducing the variance and bias inherent in the advantage estimation of group-relative RL algorithms. To rigorously validate the effectiveness of HA-DW in mitigating estimation bias under constrained sampling conditions, we conducted comparative analysis of model training performance across varying rollouts. The results presented in Table 3 shows that increasing the number of rollouts can, to certain extent, enhance model performance by providing more stable baseline. Although scaling up the number of rollouts is straightforward method to improve performance, its benefits are often capped by computational constraints. Our method offers more efficient alternative: dynamic advantage adjustment demonstrates superior efficacy even under the limited rollouts condition, and it effectively mitigates the estimation bias that typically plagues low-sample scenarios, achieving robust performance without the need for extensive sampling. E.3 Ablation Study on λscale As illustrated in Section D.4,there exists specific scaling factor λscale satisfying Equation (105) to compensate biased advantage estimation. Table 7 demonstrates the performance of RL training under different values of λscale. When λscale = 1.3 or 1.5, the trained model achieves the best performance across five benchmarks. The results correspond to our analysis that there exists an optimal value that balances the adjustment across prompts of varying difficulties, thereby enhancing RL training performance."
        },
        {
            "title": "F Hard Evolving Difficulty Anchor",
            "content": "To simplify the update process of evolving belief Ct, thereby reducing algorithmic complexity. The synchronization of the models state can be facilitated through hard update mechanism, executed at every training step. Let be hyperparameter denoting the number of most recent training rounds considered. Let be hyper-paramter to represent the last training rounds. The Equation (13) can 22 λscale MATH500 AIME25 AMC23 Minerva"
        },
        {
            "title": "AVG",
            "content": "0.5 0.8 1.0 1.3 1.5 1.7 2.0 75.4 76.8 76.8 78.0 77.8 76.4 76.8 18.1 19.2 18.5 20.4 20.8 20.0 19.0 61.1 61.3 61.6 63.4 63.1 63.4 61.9 34.2 34.9 36.0 36.8 37.1 36.4 35.3 43.7 43.7 44.3 44.7 44.0 44.3 43. 46.5 47.2 47.4 48.7 48.6 48.1 47.3 Table 7: Performance of Qwen3-4B-Base trained with GRPO+HA-DW on different λscale. 23 Figure 6: The distribution of prompts by the number of correct and incorrect responses on the MATH dataset and DAPO-Math-17k under 8 and 128 rollouts of Qwen3-4B-Base. Hyperparameter GRPO GRPO+HA-DW GSPO GSPO+HA-DW DAPO DAPO+HA-DW 1 8 False False 1 256 16 4 3 1.0 8 False False 1 5 16 4 3 1.0 1 False False 1 5 256 4 3 1.0 1 8 False False 1 5 256 16 3 1.0 1 8 False False 1 5 256 16 4 1.0 1 8 False False 5 256 16 4 9 1. AdamW AdamW AdamW AdamW AdamW AdamW General Training nnode gpus per node use kl in reward use kl loss tensor parallel size test frequency train batch size mini batch size micro batch size epoch gradient clip optimizer warmup steps weight decay learning rate 0.1 1 106 10 0.1 1 106 Clipping clip-high clip-low max prompt length max response length Rollout rollout.n do sample filtering dynamic batch size 0.2 0.2 1024 4096 False False True 0.2 0.2 4096 8 False False True 0.1 1 106 0.0004 0.0003 10 0.1 1 106 0. 0.0003 1024 4096 8 False False True 1024 4096 8 False False True 10 0.1 1 106 10 0.1 1 106 0. 0.2 1024 4096 8 False False True 0.28 0.2 1024 4096 False False True Table 8: Hyperparameter settings for Group-relative methods. 24 Case Study Question: In the xy-plane, circle of radius 4 with center on the positive x-axis is tangent to the y-axis at the origin, and circle with radius 10 with center on the positive y-axis is tangent to the x-axis at the origin. What is the slope of the line passing through the two points at which these circles intersect? The final answer can be written in the form , where and are relatively prime positive integers. What is m+n? Lets think step by step and output the final answer within boxed{}. Answer: 7 GRPO Solution: To solve the problem, we need to find the points of intersection of the two circles and then determine the slope of the line passing through these points. 1. Equation of the Circles: The first circle has radius of 4 and is centered on the positive x-axis. Since it is tangent to the y-axis at the origin, its center is at (4, 0). The equation of this circle is: (x 4)2 + y2 = 16. The second circle has radius of 10 and is centered on the positive y-axis. Since it is tangent to the x-axis at the origin, its center is at (0, 10). The equation of this circle is: x2 + (y 10)2 = 100. 2. Finding the Points of Intersection: We need to solve the system of equations: (x 4)2 + y2 = 16 x2 + (y 10)2 = 100 Expanding both equations, we get: x2 8x + 16 + y2 = 16 = x2 8x + y2 = 0 x2 + y2 20y + 100 = 100 = x2 + y2 20y = 0 Subtract from the second equation: (x2 + y2 20y) (x2 8x + y2) = 0 = 20y + 8x = 0 = 8x = 20y = = 5y 2 . Substitute = 5y 25y2+4y2 4 then = 50 2 into the first equation : (cid:0) 5y 2 (cid:1)2 8 (cid:0) 5y (cid:1) + y2 = 0 = 25y2 2 20y = 0 = 29y2 2 = 0, which is the origin and not point of intersection of the two circles. If = 80 4 20y = 0 29y2 80y = 0 = y(29y 80) = 0 So, = 0 or = 80 29 , then = 5 29 4 20y + y2 = 0 = 29 . If = 0, 2 = 200 29 . So, the points of intersection are (cid:0) 200 the two circles. Therefore, the only valid intersection point is (cid:0) 200 29 , 80 29 29 , 29 (cid:1). (cid:1) and (0, 0), but we already know (0, 0) is not valid intersection point for 3. Finding the Slope: The other point of intersection can be found by symmetry or by solving the equations again, but we know the circles are symmetric about the line = x. So, the other point of intersection is (cid:0) 80 (cid:1). The slope of the line passing through the points (cid:0) 200 (cid:1) and (cid:0) 80 = 1. So, the slope is 1, and in the form (cid:1) is: slope = 200 29 80 29 29 200 80 , we have = 1 and = 1. 120 29 120 29 29 , 200 29 , 200 29 , 80 = 29 29 29 Therefore, + = 1 + 1 = 0. The final answer is: 1 Figure 7: An example of GRPO 25 Case Study GRPO+HA-DW Solution: To solve the problem, we need to find the points of intersection of the two circles and then determine the slope of the line passing through these points. 1. Equation of the first circle: The first circle has radius of 4 and is centered on the positive x-axis. Since it is tangent to the y-axis at the origin, the center of the circle is at (4, 0). The equation of the first circle is: (x 4)2 + y2 = 16. 2. Equation of the second circle: The second circle has radius of 10 and is centered on the positive y-axis. Since it is tangent to the x-axis at the origin, the center of the circle is at (0, 10). The equation of the second circle is: x2 + (y 10)2 = 100. 3. Find the points of intersection: We need to solve the system of equations: (x 4)2 + y2 = 16 x2 + (y 10)2 = 100 First, expand both equations: x2 8x + 16 + y2 = 16 = x2 8x + y2 = 0 x2 + y2 20y + 100 = 100 = x2 + y2 20y = 0 Subtract the first expanded equation from the second: 20y + 8x = 0 = 8x = 20y = = 5 2 y. (x2 + y2 20y) (x2 8x + y2) = 0 0 Substitute = 5 = 0 or 2 into the first expanded equation: (cid:0) 5 4 = 20 = = 80 29 . 29 2 y(cid:1)2 8 (cid:0) 5 2 y(cid:1) + y2 = 0. This gives us two solutions for y: For = 0: = 5 So, the other intersection point is (cid:0) 200 29 , 29 (cid:1). 2 (0) = 0. So, one intersection point is (0, 0), which is the origin. For = 80 29 : = 5 2 (cid:0) 80 (cid:1) = 200 29 . 4. Find the slope of the line passing through the points (0, 0) and (cid:0) 200 through two points (x1, y1) and (x2, y2) is given by: = y2y1 x2x1 29 , 80 (cid:1): The slope of the line passing (cid:1): Substituting the points (0, 0) and (cid:0) 200 29 , 80 29 = 80 29 0 200 29 0 = 80 29 200 = 80 200 = 2 5 . Therefore, the slope of the line is 2 5 , and + = 2 + 5 = 7. The final answer is 7 . Figure 8: An example of GRPO+HA-DW"
        }
    ],
    "affiliations": [
        "Beihang University",
        "Meituan",
        "Peking University",
        "University of California, Berkeley"
    ]
}