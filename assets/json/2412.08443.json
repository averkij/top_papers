{
    "paper_title": "POINTS1.5: Building a Vision-Language Model towards Real World Applications",
    "authors": [
        "Yuan Liu",
        "Le Tian",
        "Xiao Zhou",
        "Xinyu Gao",
        "Kavio Yu",
        "Yang Yu",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision-language model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had a fixed image resolution, with a NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Internet and annotate them using a combination of manual and automatic methods. iii) We propose a set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 1 3 4 4 8 0 . 2 1 4 2 : r POINTS1.5: Building Vision-Language Model towards Real World Applications Yuan Liu, Le Tian, Xiao Zhou, Xinyu Gao, Kavio Yu, Yang Yu, Jie Zhou Pattern Recognition Center, WeChat AI, Tencent Inc, China {bensenliu}@tencent.com https://github.com/WePOINTS/WePOINTS https://huggingface.co/WePOINTS/POINTS-1-5-Qwen-2-5-7B-Chat Figure 1: Performance of Open-Source Models on the OpenCompass Leaderboard[Contributors, 2023]. POINTS1.5 ranks first among all models under 10B in size, even outperforming models several times larger. The size of each bubble represents the model size."
        },
        {
            "title": "Abstract",
            "content": "Vision-language models have made significant strides recently, demonstrating superior performance across range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce new visionlanguage model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had fixed image resolution, with NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Technical Report Internet and annotate them using combination of manual and automatic methods. iii) We propose set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters1."
        },
        {
            "title": "Introduction",
            "content": "Vision-language models [Liu et al., 2024b, Li et al., 2024, Bai et al., 2023, Liu et al., 2024d, Dong et al., 2024a, Chen et al., 2024c] have made remarkable strides in recent years, showcasing their potential to tackle complex tasks such as geometry math problems and optical character recognition (OCR). Despite these advancements, open-source models still lag behind closed commercial models like GPT-4o [OpenAI, 2023] and Claude-3.5-Sonnet in addressing certain real-world challenges. To bridge this gap, the open-source community has made significant efforts, exemplified by the Qwen2-VL series [Wang et al., 2024b], which has demonstrated performance comparable to, or even surpassing, these commercial models. In line with this trend, we introduce POINTS1.5, more robust model than its predecessor, POINTS1.0 [Liu et al., 2024d], which currently holds the top position on the OpenCompass leaderboard among models with fewer than 10 billion parameters. The development of vision-language models generally follows two distinct paths: i) The LLaVA-style architecture, which integrates pre-trained vision encoder, randomly initialized projector, and pre-trained large language model; and ii) Models where the large language model is randomly initialized, and both visual and text tokens are jointly used to train the language model, as seen in works like Emu3 [Wang et al., 2024c]. The LLaVA-style architecture has shown superior performance in visual understanding tasks, and POINTS1.5 continues to follow this approach. This architecture involves continual post-training of the large language model to enhance its ability to interpret visual information. The pre-training stage primarily serves to align the projection layer with the space of vision and text tokens [Li et al., 2023b]. We identify two critical factors for developing superior LLaVA-style vision-language model: i) high-performance vision encoder that can accurately and uniquely represent an image, and ii) High-quality visual instruction tuning datasets that enable the model to understand image content and exhibit strong instruction-following capabilities. Based on this analysis, POINTS1.5 introduces the following innovations. Native Dynamic High Resolution. Enabling vision-language model to process images of any resolution without down-sampling offers numerous benefits, such as reducing hallucinations and enhancing performance on text-intensive tasks. Historically, many vision encoders, such as Vision Transformer [Dosovitskiy, 2020] and ConvNext [Liu et al., 2022], could only handle fixed-resolution images. Previous works [Liu et al., 2024a, Dong et al., 2024b, Chen et al., 2024c, Liu et al., 2024d] often split large images into tiles to accommodate the vision encoder, disrupting the spatial structure of the original image. In contrast, POINTS1.5 employs NaViT-style architecture, following the approaches of Qwen2-VL [Wang et al., 2024b] and Idefics2 [Laurençon et al., 2024], allowing it to process arbitrary-resolution images without splitting them, resulting in significant improvements over the dual CLIP vision encoders used in POINTS1.0. Bilingual Support. In POINTS1.0, the English corpus comprised over 95% of the total data. In this version, we have increased the amount of Chinese data for both the pre-training and visual instruction tuning stages. Due to the limited availability of open-source Chinese datasets, gathering large quantity of Chinese corpus is challenging. For the pre-training stage, we followed the strategy used to obtain the 1 million pre-training data in POINTS1.0, creating an additional 1 million Chinese pre-training data from LAION-5B-cn [Schuhmann et al., 2022] using CapFusion [Yu et al., 2024] and perplexity filtering [Liu et al., 2024d]. This was combined with the original English data to form final dataset of 2 million for pre-training. For the visual instruction tuning stage, we employed two strategies: (i) translating existing conversation datasets into Chinese using large language model (LLM), and (ii) for Chinese OCR datasets, collecting related images from the internet and using an 1Result is obtained on December 8th, 2024 Figure 2: POINTS1.5 shows great potential to solve challenging real world problems. existing vision-language model, such as Qwen2-VL-72B, to extract text from these images. Human labelers then verified these annotations, correcting minor errors or discarding them if the errors were significant. Visual Instruction Tuning Set Filtering. We manually reviewed each dataset used in POINTS1.0 and identified two significant issues: i) large number of grammatical errors in some datasets, and ii) Some questions could be answered without referring to the image. To address the first issue, we employed large language model (LLM), such as Qwen2.5-72B [Yang et al., 2024], to detect grammatical errors in the existing data samples. We then either discarded these erroneous samples or corrected them and reintegrated them into the original dataset. For the second issue, we used an LLM to answer the questions without the image. If the LLM provided the correct answer, the corresponding data sample was labeled accordingly. 3 Combining these innovations, POINTS1.5 brings significant improvements compared to POINTS1.0 and performs well across range of real-world applications. Notably, POINTS1.5-7B ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters."
        },
        {
            "title": "2 Model Architecture",
            "content": "Figure 3: POINTS1.5 uses the converntional LLaVA-style architecture, consisting of vision encoder, MLP projector and LLM. Figure 3 illustrates the architecture of POINTS1.5. This model adheres to the traditional LLaVA-style architecture [Liu et al., 2023b], which comprises vision encoder, an MLP projector, and language model (LLM). Vision Encoder As discussed in the previous section, training vision-language model using the LLaVA-style architecture is akin to the continual post-training of the LLM, enabling it to process tokens from the image modality. Therefore, starting with high-quality vision encoder is crucial for the LLM to accurately interpret images. To support images with arbitrary resolutions, POINTS1.0 follows recent works such as LLaVA-Next [Liu et al., 2024a] and InternVL [Chen et al., 2024c], which split large image into several tiles that the vision encoder can process. However, this method has inherent drawbacks, as it disrupts the spatial relationships between patches within an image. Although strategies like adding line splitters [Dong et al., 2024b] and incorporating global view alongside the split patches [Chen et al., 2024c] can mitigate this issue, the problem persists. Consequently, POINTS1.5 replaces the CLIP vision encoder used in POINTS1.0 with NaViT-style vision encoder [Dehghani et al., 2024], following recent advancements [Wang et al., 2024b, Laurençon et al., 2024]. Unlike the CLIP vision encoder, the NaViT-style vision encoder can natively handle images with arbitrary resolutions without the need for splitting. Batch Forwarding with NaViT With the introduction of NaViT, new challenge arises in batch forwarding. Unlike the CLIP vision encoder, where images can be concatenated along the batch size dimension, NaViT processes images that vary in sequence length after being patchified. To address this, we adopt strategy inspired by large language models (LLMs): we pack multiple image sequences into single, long sequence. We then record the start and end indices of each image 4 sequence to ensure that self-attention is applied only within the boundaries of the current image sequence[Dao, 2024]. Projector activation function [Hendrycks and Gimpel, 2016] between the layers to introduce non-linearity. In accordance with POINTS1.0, the projector consists of two-layer MLP with GELU Large Language Model After the release of this paper, we plan to introduce POINTS1.5 with bigger language model. In alignment with POINTS1.0, we have selected Qwen2.5-7B-Instruct."
        },
        {
            "title": "3 Bilingual Support",
            "content": "In this section, we will discuss the curation of the Chinese dataset used in POINTS1.5. But before the discussion, we will refine the chat template used in the pre-training of POINTS1.0. Chat Template As discussed in previous sections, training LLaVA-style vision-language model involves the continual post-training of the LLM. Following POINTS1.0, the LLM of POINTS1.5 is also initialized from the instruction-tuned version of Qwen2.5-7B2. However, during the pre-training stage of POINTS1.0, we use continuation template to pack the data, similar to the one used during the pre-training process of the LLM, which deviates from the template used in the initialized LLM. In this version, we employ the conversation template used in Qwen2.5-7B-Instruct and observe improved performance compared to the continuation template. Since the pre-training data are the image-caption pairs, we add prompt, similar to Please describe this image., to each data sample. To diversify the prompts, we create candidate prompt pool (Figure 5) and randomly sample one for each data sample. Additionally, to distinguish visual tokens from text tokens, we add image prefix and suffix tokens around the visual tokens. Figure 4 shows the difference between the chat template during pre-training between POINTS1.0[Liu et al., 2024d] and POINTS1.5. Figure 4: The chat template during pre-training in POINTS1.0 (above) and POINTS1.5 (below) Chinese Pre-training Dataset Following POINTS1.0, we employ two-step procedure to create the pre-training dataset: i) We use CapFusion [Yu et al., 2024] to merge the caption generated by vision-language Model (VLM) with the images original caption, resulting in the final caption. ii) We filter the generated caption using perplexity. The CapFusion process is described by the following formula: Caption = G(c, F(I)) (1) represents large language model, denotes vision-language model, is the original caption, and is the corresponding image. The Chinese captions were generated during the development of POINTS1.0. For this purpose, we utilize InternLM2 [Cai et al., 2024] as the large language model and InternLM-XComposer2 [Dong et al., 2024a] as the vision-language model. In the future, we plan to generate captions using more advanced models, such as POINTS1.5, which is also expected to further improve performance. Subsequently, we employ perplexity to filter these data: 2https://huggingface.co/Qwen/Qwen2.5-7B-Instruct 5 Perplexity(s) = exp("
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 logP (wiw1, w2, ..., wi1)) (2) Let {w1, . . . , wN } denote the sequence of text tokens for s. We arrange these tokens in ascending order and select the first 20% (approximately 1 million) for the pre-training phase. This subset of the Chinese dataset is then combined with the original 1 million English dataset to pre-train POINTS1.5. Figure 5: Prompts used in the chat template during pre-training stage. Chinese Visual Instruction Tuning Dataset We inherit all visual instruction tuning datasets from POINTS1.0, except for those generated in this section. To create Chinese visual instruction tuning datasets, we employ several strategies: i) Translate existing English datasets (both questions and answers) into Chinese. ii) Use images and questions from existing datasets, and generate corresponding answers using powerful VLM, such as Qwen2-VL-72B. This strategy is only applied to caption datasets. iii) Collect images from the internet, manually design questions (Figure 6), generate answers using powerful VLM, and verify the answers with human labelers. This strategy is primarily used for Chinese OCR datasets. The following table shows the datasets and the corresponding strategies used to construct the Chinese datasets. Datasets Strategy VQAv2[Goyal et al., 2017], GQA[Hudson and Manning, 2019] OKVQA[Marino et al., 2019] LVIS-Instruct4V[Wang et al., 2023], LAION-GPT4V Images collected from Internet Translate English into Chinese Question translation&VLM VLM&Human Check Table 1: Datasets and corresponding strategies to generate Chinese datasets. Figure 6: Prompts to create the Chinese OCR datasets. After the creation of Chinese datasets, we obtain the distribution across 9 categories and the English&Chinese distribution for the final visual instruction tuning datasets we used in POINTS1.5. We observe significant imbalance among the different categories. However, we have not yet identified effective methods to balance the data across these categories, and we leave this challenge for future work. 6 Figure 7: Distribution of visual instruction tuning data in POINTS1.5. The left figure shows the distribution across different categories, and the right figure shows the distribution between English and Chinese."
        },
        {
            "title": "4 Visual Instruction Tuning Set Filtering",
            "content": "Before filtering the visual instruction tuning datasets, we manually check each of these datasets used in POINTS1.0, and identify two significant issues: i) Some questions could be answered without referring to the image (Figure 9). and ii) large number of grammatical errors in some datasets (Figure 8). Figure 8: Procedure to filter out samples containing grammatical errors (a) and distribution between grammatically correct samples and samples containing grammatical errors (b). Questions can be answered without images. It is common sense that the data used to train vision-language model should enable the model to solve problems based on images. If questions can be answered without images, they degenerate into pure-text data [Liu et al., 2023c]. To filter out such data, we use powerful open-source LLM, such as Qwen2.5-72B-Instruct, to answer the questions without the images. If the LLM provides the correct answer, the corresponding data sample is discarded. This filtering strategy is applied only to datasets containing fixed and definite answers, such as AI2D [Kembhavi et al., 2016]. We then train the model with the filtered dataset but observe slightly degraded performance. This phenomenon is consistent with previous works [Dai et al., 2024, 7 Zhang et al., 2024, Yao et al., 2024], which suggest that pure-text data is helpful in maintaining the capability of the pre-trained LLM. Filter out samples containing grammatical errors. For the second type of issue, we design two-step filtering strategy: (i) use large language model (LLM) to detect whether there are grammatical errors in the current sample, and (ii) if grammatical errors exist, we can either choose to drop the sample or use the LLM to fix these errors. After meticulous comparison, we find that the model performs better when directly dropping these samples rather than using the LLM to fix them. As shown in Figure 8(b), we retain about 85% of the original data after filtering. Figure 9: Questions can be answered without referring to the image."
        },
        {
            "title": "5 Training and Model Strategy",
            "content": "Training Strategy. Currently, there is no consensus within the community on how to train each module of LLaVA-style visionlanguage model. As shown in Table 2, different models employ distinct training configurations during pre-training and visual instruction tuning. This raises the question: what is the optimal strategy for the training configuration? In contrast to visionlanguage models, large language models (LLMs) have developed more rapidly, with various development paths converging to unified approach. Before pre-training an LLM, tokenizer must be trained on large corpus using algorithms such as WordPiece [Song et al., 2020] and BPE [Sennrich, 2015], ensuring that each sentence can be uniquely and accurately tokenized into sequence of indices. This tokenizer can also decode sequence of indices back into sentence. During the pre-training and post-training processes, the tokenizer remains fixed, while the word embedding layer and all transformer layers [Vaswani, 2017] are trained end-to-end. Analogously, in the architecture of vision-language model, the vision encoder functions similarly to the text tokenizer, and the projector is akin to the word embedding layer. Therefore, before training vision-language model, the vision encoder must be Figure 11: Unfreezing the vision encoder during pretraining degrades the performance. 8 Figure 10: Data samples containing grammatical errors (marked with red) in visual instruction tuning set. trained separately (e.g., the Qwen2-VL vision encoder used in POINTS1.5). Subsequently, the vision encoder is fixed, and the projector and LLM are trained end-to-end. In practice, since the vision projection layer is randomly initialized, we find that adding an additional stage (the so-called pre-training stage) to warm up the projection layer results in better performance (we fix the vision encoder in this stage, as we find unfreezing it degrades the performance (Figure 11). Our training configuration is summarized in Table 3. Notably, POINTS1.5 follows the path of POINTS1.0 [Liu et al., 2024d] to make computational resources more affordable, totaling less than 5 billion tokens, which is significantly fewer than most previous works [Chen et al., 2024c, Lu et al., 2024a, Wang et al., 2024b]. Model Soup over Best Performing Model Following POINTS1.0, we use model soup [Wortsman et al., 2022] to boost the performance of single model. Model soup is conducted over models that perform best on our evaluation benchmark and mainly consist of models trained with different visual instruction tuning datasets and different visual instruction tuning epochs. The OpenCompass score of"
        },
        {
            "title": "Model",
            "content": "LLaVA-Next[Liu et al., 2024a] OneVision[Li et al., 2024] POINTS[Liu et al., 2024d] InternVL1.5[Chen et al., 2024c]"
        },
        {
            "title": "Vision",
            "content": "Pre-training"
        },
        {
            "title": "Projector LLM",
            "content": "Table 2: Training strategies of different models. Figure 12: We envision that extending large language model with additional modalities using LLaVA-style architecture should follow the three-stage procedure illustrated in this figure. The three icons on the left denote the status of each module during the three stages. From left to right, they are: stage-1, stage-2, stage-3. Settings Datasets Trainable Batch Size Context Length Learning Rate Weight Decay Gradient Clip lr Scheduler Training Tokens Pre-training Stage LAION-5B by CapFusion and Filtering MLP Projector 32 4096 2e-4 0.0 1.0 Cosine 2.1B Visual Instruction Tuning Stage POINTS1.0 + Chinese Datasets MLP Projector + LLM 32 4096 2e-5 0.1 1.0 Cosine 2.3B Table 3: Training configurations for POINTS1.5 the best-performing single model is 66.5, and the final model obtained using model soup achieves score of 67.4. Discussion As discussed in the previous section, extending Large Language Model (LLM) with any modality under the LLaVA-style architecture is akin to the continual post-training of the LLM. We identify three critical factors that determine the final performance of the model: i) High-quality modality tokenizer and detokenizer. The tokenizer should uniquely and accurately encode any 10 modality signal into compressed feature space, while the detokenizer should restore compressed feature to its original modality signal. ii) Modality embedding layer, a.k.a. projection layer. iii) High-quality instruction tuning dataset to endow the LLM with the capability to understand different modalities. Thus, we envision the development of multimodal model should follow three-step strategy in the future (Figure 12): i) Use abundant data to train modality tokenizer and detokenizer, e.g. vision encoder and decoder. ii) Warm up the modality embedding layer to convert any modality signals into the text space of the LLM. During this step, the dataset size does not necessarily need to be very large, as we have found in our experiments and in previous work [Liu et al., 2024c]. iii) Use high-quality instruction tuning dataset to train the modality embedding layer and the LLM, while keeping the tokenizer and detokenizer frozen."
        },
        {
            "title": "6 Evaluation",
            "content": "Before embarking on our exploration, we sought robust evaluation metric to comprehensively assess the various capabilities of our model. We initially selected the eight benchmarks used in the ranking on OpenCompass. These benchmarks include MMBench [Liu et al., 2023c] and MMStar [Chen et al., 2024b] for diagnosing general abilities, MMMU [Yue et al., 2024] for testing STEM-related abilities, HallusionBench [Liu et al., 2023a] for model hallucination, MathVista [Lu et al., 2023] for math-related abilities, AI2D [Kembhavi et al., 2016] for chart-related abilities, OCRBench [Liu et al., 2023d] for OCR capabilities, and MMVet [Yu et al., 2023] for subjective evaluation. Additionally, OpenCompass offers useful tool, VLMEvalKit [Duan et al., 2024], for one-click evaluation. To further complement the evaluation results, we also included ChartQA [Masry et al., 2022], MME [Yin et al., 2023], LLaVA-wild [Kuang et al., 2023], SEEDBench [Li et al., 2023a], ScienceQA [Lu et al., 2022], MATH-Vision[Wang et al., 2024a], MathVerse[Zhang et al., 2025], and MEGEBench [Chen et al., 2024a]. Tables 4 and 5 show the comparison between POINTS1.5 and some representative open-source models. POINTS1.5 demonstrates promising performance, obtaining the top score on most of these benchmarks. In particular, we find the mathematical ability of POINTS1.5 to be quite extraordinary, as evidenced by the results on MathVista, MATH-Vision, and MathVerse. Methods MMB MV HB OCR AI2D MMVet MMStar MMMU GPT-4o-20241120 Gemini-1.5-Pro-002 Claude3.5-Sonnet-20241022 Ovis1.5-LLaMA3-8B InternVL2-8B OneVision-7B-SI POINTS-7B Qwen2-VL-7B POINTS1.5-7B 84.3 82.8 81.7 76.6 79.4 76.8 83.2 81.0 80.7 Proprietary models 80.6 56.2 59.9 77.0 55.9 67.8 65.1 79.8 55.5 Open-source models 45.0 63.0 45.0 58.3 47.5 58.5 46.0 63.1 50.4 61.4 Ours 50.0 74.4 79.4 69.7 72.0 84. 83.2 66.4 84.9 83.3 81.2 82.5 83.6 82.8 80.9 83.0 81.4 74.5 74.6 70. 50.9 54.3 50.6 52.3 61.8 62.2 65.1 67.1 65.1 57.3 61.5 56.7 61.0 60.7 61.1 70.7 68.6 66. 48.3 51.2 46.8 49.4 53.7 53.8 Table 4: Comparison between different methods on OpenCompass benchmarks. MMB: MMBench[Liu et al., 2023c], MV: MathVista[Lu et al., 2023], HB: HallusionBench[Liu et al., 2023a], OCR: OCRBench[Liu et al., 2023d], Ovis1.5-LLaMA3-8B: Ovis1.5[Lu et al., 2024b], OneVision: LLaVA-OneVision[Li et al., 2024]. Results are obtained from the leaderboard of OpenCompass."
        },
        {
            "title": "7 Conclusion",
            "content": "We present POINTS1.5, significantly enhanced model compared to POINTS1.0. This version introduces three major innovations: i) We replaced the original CLIP vision encoder with NaViTstyle vision encoder, enabling native support for images of any resolution without the need for splitting. This allows us to preserve the original spatial relationships between patches within an image. ii) We added bilingual support. By constructing Chinese corpus using combination of manual and automatic strategies, we obtained large quantity of Chinese data for both the pre-training and visual instruction tuning stages. iii) We manually reviewed each dataset from POINTS1.0 and identified two significant issues. We then proposed effective strategies to filter these datasets. Notably, by training 11 Methods ChartQAavg MME Wild SEEDI MEGA-SI SCI M-Vision M-Verse Ovis1.5-LLaMA3-8B OneVision-7B-SI InternVL2-8B POINTS-7B Qwen2-VL-7B POINTS1.5-7B - 80.0 83.3 - 83.0 84.3 Open-source models 1948.5 2146.3 2215.1 2184.1 2276. 79.9 77.6 73.3 72.3 70.1 75.4 75.4 75.4 74.8 76.0 Ours 2222.7 74.6 75. - 25.7 29.2 26.2 36.7 32.7 88.8 86.6 97.1 94.8 85.5 94.8 - 26.2 37.0 - 31.9 36. - - 20.4/18.4 - 22.0/16.3 23.7/21.9 Table 5: Comparison with open-source models of similar size on more benchmarks. SEEDI: SEEDBench[Li et al., 2023a], MEGA-SI: Single image evaluation without few-shot samples of MEGABench[Chen et al., 2024a], SCI: ScienceQA[Lu et al., 2022], Wild: LLaVA-Wild[Kuang et al., 2023], M-Vision: MATH-Vision[Wang et al., 2024a], M-Verse: MathVerse[Zhang et al., 2025]. the model on less than 5 billion tokens, we achieved model that ranks first on the OpenCompass leaderboard."
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024b. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024c. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024a. 12 Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024b. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. arXiv preprint arXiv:2407.11691, 2024. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. ArXiv, abs/1603.07396, 2016. URL https: //api.semanticscholar.org/CorpusID:2682274. Jianfeng Kuang, Wei Hua, Dingkang Liang, Mingkun Yang, Deqiang Jiang, Bo Ren, and Xiang Bai. Visual information extraction in the wild: practical dataset and end-to-end solution. In International Conference on Document Analysis and Recognition, pages 3653. Springer, 2023. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023b. Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. arXiv preprint arXiv:2310.14566, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. 13 Yuan Liu, Le Tian, Xiao Zhou, and Jie Zhou. Rethinking overlooked aspects in vision-language models. arXiv preprint arXiv:2405.11850, 2024c. Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, and Jie Zhou. Points: Improving your vision-language model with affordable strategies. arXiv preprint arXiv:2409.04828, 2024d. Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023d. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024a. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024b. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. OpenAI. Gpt-4 technical report. Technical Report 1, 2, 9, 10, OpenAI, 2023. URL https: //example.com/gpt4-technical-report. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. Rico Sennrich. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. Xinying Song, Alex Salcianu, Yang Song, Dave Dopson, and Denny Zhou. Fast wordpiece tokenization. arXiv preprint arXiv:2012.15524, 2020. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024c. Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, 2022. URL https://arxiv.org/abs/2203.05482. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1402214032, 2024. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2025."
        },
        {
            "title": "A Appendix",
            "content": "We shows some real world examples to demonstrate the performance of POINTS1.5. Figure 13: OCR and reasoning ability. 16 Figure 14: Complex Chinese OCR 17 Figure 15: Complex OCR 18 Figure 16: Summarize key points from an image. 19 Figure 17: Latex formula extraction 20 Figure 18: Mathematical problem solving 21 Figure 19: Image translation 22 Figure 20: Object identification. 23 Figure 21: Key information extraction and reasoning."
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent Inc, China"
    ]
}