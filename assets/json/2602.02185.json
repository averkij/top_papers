{
    "paper_title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
    "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Zhen Fang",
        "Shuang Chen",
        "Yufan Shen",
        "Yishuo Cai",
        "Xiaoman Wang",
        "Zhenfei Yin",
        "Lin Chen",
        "Zehui Chen",
        "Shiting Huang",
        "Yiming Zhao",
        "Yao Hu",
        "Philip Torr",
        "Wanli Ouyang",
        "Shaosheng Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 5 8 1 2 0 . 2 0 6 2 : r Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models Yu Zeng2 Wenxuan Huang1,3(cid:66) Zhen Fang2 Shuang Chen5 Yufan Shen6 Yishuo Cai7 Xiaoman Wang3 Zhenfei Yin8 Lin Chen2 Zehui Chen2 Shiting Huang2 Yiming Zhao2 Yao Hu4 Philip Torr8 Wanli Ouyang1,9 Shaosheng Cao4(cid:66) 1CUHK MMLab 2University of Science and Technology of China 3East China Normal University 4Xiaohongshu Inc. 5The University of California, Los Angeles 6Zhejiang University 9Shenzhen Loop Area Institute wxhuang0616@gmail.com (Wenxuan Huang) *: Equal Contribution : Project Leader (cid:66): Corresponding Author 7Peking University 8University of Oxford"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Multimodal Large Language Models (MLLMs) have advanced VQA and now support VisionDeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via careful, multistage curation pipeline and rigorous expert review, designed to assess the behavior of VisionDeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/ Vision-DeepResearch. The rapid progress of Multimodal Large Language Models (MLLMs) (Chen et al., 2024b; Huang et al., 2025b; Zhao et al., 2025; Chen et al., 2024a;c; Wang et al., 2025; Yu et al., 2024; Qi et al., 2025; Huang et al., 2025a; Han et al., 2026; Zeng et al., 2025a;b) has enabled new class of multimodal deep-research systems (Geng et al., 2025; Wu et al., 2025a; Narayan et al., 2025; Hong et al., 2025), which combine image understanding, web search, and multi-hop reasoning to solve complex visualtextual queries. These systems promise to automate deep visual fact-finding by grounding reasoning in real-world images and external knowledge sources. Despite this progress, evaluating the visual and textual search capabilities of multimodal deep-research systems remains challenging. Existing benchmarks for multimodal search and knowledge-seeking VQA (Geng et al., 2025; Jiang et al.; Fu et al., 2025; Wang et al., 2017; Cheng et al., 2025; Chen et al., 2023) suffer from two fundamental limitations, as illustrated in Fig. 1: First, current benchmarks are not visual searchcentric. Many benchmark questions can be answered without genuine visual verification, i.e., some answers are inferred from language priors with cross-textual cues, and even MLLMs world knowledge. As simple example (shown in Fig 1), some VQA problems in these benchmarks can be resolved by exploiting cross-referencing among textual entities in the question: taking the intersection of multiple candidate sets that satisfy different textual constraints typically yields small set, making it easy to pin down (or even guess) the answer. We view this as shortcut phenomenon. Furthermore, as presented in Fig 1, some VQA instance can simply solved by MLLM prior world knowledge. As result, benchmark performance often reflects memorization or text-only retrieval rather than the the necessary for gathering visual information."
        },
        {
            "title": "Preprint",
            "content": "Figure 1. Motivation. Existing Vision-DeepResearch benchmarks often fail to measure realistic multimodal search: many questions can be solved via text-only cues or model priors without genuine visual verification, and whole-image search frequently retrieves near-duplicate images with identifying metadata (perfect retrieval). VDR-Bench is designed to be visual-searchcentric and to reflect real-world settings that require iterative, entity-level localization (e.g., multi-round cropping), cross-modal evidence collection, and multi-hop reasoning. Second, the evaluation scenario of existing benchmarks rely on overly idealized search settings. On the image-search side, required information is frequently obtained through near-exact whole-image matching, where querying the original image retrieves an identical or near-duplicate source along with its title or metadata. On the text-search side, questions are often formulated in direct and shallow manner, underestimating the difficulty of multi-hop reasoning and evidence aggregation. These conditions fail to reflect the noisy, ambiguous, and iterative nature of real-world visual search. In realistic scenarios, visual search is inherently trial-based and iterative. Images often contain multiple entities, background distractors, and ambiguous visual cues, making whole-image retrieval unreliable. Effective multimodal deep-research systems should therefore localize candidate entities, issue multiple cropped image queries, refine hypotheses across rounds, and verify results through cross-modal reasoning, in order to adapt to real-world requirement. Multi-round, multi-scale cropping is not merely an implementation detail but necessary strategy for robust visual retrieval. To address these challenges, we introduce VDR-Bench (Vision-DeepResearch Benchmark), large-scale benchmark comprising 2,000 carefully curated VQA instances designed to evaluate realistic multimodal search behavior. VDR-Bench is constructed through rigorous multi-stage pipeline that emphasizes visual-first entity discovery, human verification, and multi-hop query expansion, ensuring that solving each instance genuinely requires both visual search and textual reasoning. Unlike prior benchmarks that convert text-based QA into VQA through single-step image retrieval, our benchmark avoids shortcut-prone instances and minimizes perfect-retrieval bias. Furthermore, to improve the insufficient visual retrieval capabilities of current MLLMs, we propose simple yet effective multi-round cropped-search workflow. This strategy iteratively refines visual queries by cropping regions of interest, enabling more accurate entity localization and reducing retrieval noise. Experimental results demonstrate that this workflow substantially improves performance on realistic visual retrieval tasks, highlighting practical direction for building more capable multimodal deep-research systems. Our main contributions are summarized as follows: We identify critical limitations in existing multimodal including the lack of visualsearch benchmarks, searchcentric evaluation and overly idealized retrieval settings."
        },
        {
            "title": "Preprint",
            "content": "We introduce VDR-Bench, new benchmark with 2,000 instances curated through multi-stage, humanverified pipeline that enforces realistic visual and textual search requirements. We propose multi-round cropped-search workflow that significantly improves model performance in realistic visual retrieval scenarios, offering practical guidance for future multimodal deep-research systems. 2. Related Work 2.1. Vision-DeepResearch Systems. Vision-DeepResearch agents are designed to autonomously search, read, and synthesize knowledge from the open web through multi-step reasoning and cross-modal grounding. While recent progress in text-only deep research agents (Team et al., 2025; Wu et al., 2025b; Li et al., 2025; Tao et al., 2025) has been substantial, the increasing visual complexity of real-world web environments has driven paradigm shift toward multimodal Vision-DeepResearch systems. Compared with text-only counterparts, these systems face significantly greater challenges, as they must integrate visual evidence, perform reliable visual search, and conduct cross-modal verification under noisy and ambiguous conditions. To address these challenges, several Vision-DeepResearch frameworks have been proposed. WebWatcher (Geng et al., 2025) reformulates text-based question answering into visual question answering (VQA) via reverse image search. MMSearch-R1 (Wu et al., 2025a) enhances multimodal search strategies through Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Building upon this line of work, DeepMMSearch-R1 (Narayan et al., 2025) further improves visual grounding by employing entity-level image cropping, reducing background noise and enabling more targeted visual retrieval. Despite these advances, existing systems still struggle with the complexity and noise of realistic visual search environments. 2.2. Evaluations of Vision-DeepResearch Systems. The rapid development of Vision-DeepResearch systems requires evaluation benchmarks that accurately measure visual and textual search capabilities under realistic conditions. SimpleVQA (Cheng et al., 2025) assesses multimodal factuality and hallucination robustness, while FVQA (Wang et al., 2017) and InfoSeek (Chen et al., 2023) require models to ground visual evidence in external knowledge sources. LiveVQA (Fu et al., 2025) evaluates reasoning over dynamic, real-time visual information, whereas MMSearch (Jiang et al.) and BrowseComp-VL (Geng et al., 2025) examine agentic competence in multi-step multimodal search and deep research workflows. Although these benchmarks provide valuable progress, they exhibit two critical limitations aligned with our analysis. First, most existing benchmarks are not visual searchcentric: many instances can be solved through language priors or text-only retrieval without requiring genuine visual verification, resulting in shortcut solutions that fail to evaluate true visual retrieval and grounding abilities. Second, current benchmarks rely on overly idealized search settings: on the image side, near-exact whole-image matching often enables perfect retrieval, while on the text side, queries are frequently shallow and insufficiently challenging. These design choices underestimate the noise, ambiguity, and iterative nature of real-world visualtextual search. 3. Quantitative Analysis of Existing Vision-DeepResearch Benchmarks As discussed in the Figure 1, existing Vision-DeepResearch benchmarks suffer from two core limitations: (1) many instances are not truly visual-searchcentric, and (2) image retrieval is often conducted under overly idealized evaluation scenario. To quantify the severity of these issues, we conduct controlled empirical study across representative multimodal search and knowledge-seeking VQA benchmarks, including SimpleVQA (Cheng et al., 2025), LiveVQA (Fu et al., 2025), FVQA (Wang et al., 2017), BrowseComp-VL (BCVL) (Geng et al., 2025), and MMSearch (Jiang et al.). We evaluate representative open and closed-source multimodal large language models, including Gemini-2.5 Pro and Qwen3-VL-30B-A3B-Instruct (Bai et al., 2025), as well as the text-based deep research model Tongyi-DeepResearch30B-A3B (Team et al., 2025). These models are evaluated under controlled retrieval configurations to disentangle the independent contributions of whole-image search, text search, and model prior knowledge. We compare two experimental settings: (1) Given the original image and question: Direct Answer without external search, Whole-Image Search (WIS), Text Search (TS), and WIS + TS. (2) Replacing the image with caption (produced by Qwen3-VL-235BA3B-Instruct) alongside the question: Direct Answer and Text Search only. This design allows us to evaluate whether benchmark instances genuinely require visual search, or can instead be solved via text-only retrieval or model priors. The results are reported in Table 1, where arrows ( / ) indicate changes relative to the No Search baseline. 3.1. Finding 1: Existing Benchmarks Do Not Enforce Visual-SearchCentric Reasoning Tab. 1 reveals clear and concerning issue: substantial portion of instances in existing Vision-DeepResearch benchmarks can be solved without meaningful visual search. Across multiple datasets, enabling TS alone yields signifi-"
        },
        {
            "title": "Preprint",
            "content": "Figure 2. Dataset composition and example instances from VDR-Bench. The figure presents the distribution across ten visual domains, along with representative questionanswer examples for each category. cant performance gains, and in some cases achieves results comparable to or even exceeding those obtained with Image Search. This indicates that many benchmark questions suffer from severe textual cue leakage, where answers can be inferred through cross-validation of textual evidence without requiring image search for visual entities. Additionally, in several benchmarks, the Caption + Direct Answer setting attains competitive performance, suggesting that models can often bypass visual evidence entirely by relying on language priors and parametric world knowledge. Taken together, these findings demonstrate that many existing benchmarks fail to enforce closed visualevidence loop and therefore systematically overestimate models true visual search and verification capabilities. Current benchmark performance often reflects proficiency in text retrieval and prior memorization, rather than genuine visual-searchcentric reasoning. 3.2. Finding 2: Overly Idealized Retrieval Evaluation"
        },
        {
            "title": "Setting",
            "content": "Our analysis further uncovers second limitation in current benchmarks: search is often conducted under unrealistically idealized conditions to evaluate. As shown in Tab. 1, enabling single-shot Whole-Image Search alone frequently yields substantial performance gains relative to the Direct Answer baseline. This behavior reveals strong perfect-match bias, where querying the full 4 Table 1. Performance comparison across visual search benchmarks. WIS: Whole Image Search, TS: Text Search. Arrows (/) denote performance changes relative to the corresponding No Search baseline. BCVL scores represent the average of Level 1 and Level 2."
        },
        {
            "title": "BCVL",
            "content": "MMSearch VDR-Bench"
        },
        {
            "title": "Preprint",
            "content": "Gemini 2.5 Pro m . C Direct Answer WIS TS WIS+TS"
        },
        {
            "title": "Direct Answer\nTS",
            "content": "69.7 75.3 5.6 67.3 2.4 74.7 5.0 54.0 54.7 0.7 Qwen3-VL-30B-A3B-Instruct m . C Direct Answer WIS TS WIS+TS 56.3 67.7 11.4 48.3 8.0 70.0 13."
        },
        {
            "title": "Direct Answer\nTS",
            "content": "51.7 49.0 2.7 Tongyi-DeepResearch-30B-A3B 60.3 64.3 4.0 72.7 12.4 76.3 16.0 46.3 56.0 9.7 42.7 48.3 5.6 53.3 10.6 62.7 20.0 42.0 46.7 4. 60.7 66.7 6.0 69.3 8.6 75.7 15.0 37.3 43.0 5.7 34.7 53.7 19.0 45.0 10.3 73.7 39.0 31.0 43.0 12.0 43.1 42.6 0.5 49.9 6.8 52.9 9.8 38.1 43.9 5. 29.6 35.1 5.5 37.5 7.9 46.6 17.0 31.3 37.9 6.6 39.8 50.3 10.5 57.3 17.5 69.6 29.8 28.1 33.3 5.2 18.7 36.3 17.6 40.9 22.2 66.7 48.0 15.8 38.0 22. . C"
        },
        {
            "title": "Direct Answer\nTS",
            "content": "49.7 46.3 3.4 44.7 60.7 16.0 31.7 45.7 14.0 29.4 44.4 15.0 14.6 45.0 30.4 8.2 9.8 5.4 10. 4.6 5.2 3.8 5.2 5.8 6.0 4.6 7.4 4.0 7.5 image retrieves near-duplicate copy accompanied by identifying titles or metadata, effectively reducing the task to one-shot lookup. Under such conditions, benchmark difficulty is dominated by whether the search engine returns an exact or near-exact match, rather than evaluating an agents ability to localize entities, refine visual queries, or verify evidence across modalities. It is important to note that this setting departs sharply from real-world visual search, where relevant evidence rarely appears as exact duplicates and is often distributed across diverse viewpoints, crops, resolutions, and contextual scenes. Our results indicate that many existing benchmarks contain substantial proportion of images and metadata that can be located nearly exactly on the web, thereby overlooking the iterative localization, visual search, and entity-level cross-modal verification required in real-world scenarios. Furthermore, as shown in Fig 1, most existing benchmarks in text-based multi-hop part remains insufficiently challenging, i.e., their text-side deep-research setups are often overly simplistic, failing to effectively stress-test the increasingly strong text deep-research capabilities of modern MLLMs. 3.3. Why VDR-Bench? Our findings reveal fundamental misalignment between existing benchmark protocols and the capabilities required for realistic multimodal deep-research systems. Many current benchmarks allow models to succeed through text-only shortcuts, language priors, or one-shot near-duplicate retrieval, rather than necessitating iterative visual search, finegrained localization, and cross-modal evidence aggregation. These empirical gaps directly motivate the design of VDRBench. By emphasizing visual-searchcentric reasoning with the tailored designed challenging textual search setting, mitigating perfect-retrieval bias, and evaluating agents under realistic multimodal search conditions, VDR-Bench offers more faithful and rigorous testbed for assessing and advancing multimodal deep-research systems. 4. VDR-Bench In Sec. 4.1, we present detailed description of the data curation process for VDR-Bench, including multi-stage filtering pipeline and rigorous expert review. In Sec. 4.2, we introduce two evaluation metrics specifically designed for this benchmark to assess the accuracy of deep research and entity-level retrieval performance. 4.1. Data Curation Process Our benchmark data is constructed through strict visioncentric search pipeline that begins with raw images and culminates in multi-hop Visual Question Answering (VQA) instances. Throughout the process, we explicitly annotate retrieved entities, thereby providing each sample with grounded visual and textual evidence."
        },
        {
            "title": "Preprint",
            "content": "Figure 3. VDR-Bench is constructed via multi-stage, vision-centric workflow: (1) annotators manually crop salient regions and perform web-scale visual search; (2) candidate entities are extracted from retrieved results and verified through an MLLM-assisted and human checking process; (3) verified visual entities are used to generate seed VQA pairs; (4) question difficulty is expanded via knowledge-graphbased multi-hop reasoning; and (5) automatic solvability checks and human quality filtering ensure that each instance requires visual evidence, remains unambiguous, and avoids trivial or near-duplicate retrieval. Step 0: Multi-Domain Image Pre-Filtering. We first collect images from multiple datasets spanning diverse domains and remove samples with insufficient resolution. We then employ Qwen3-VL-235B-A22B-Instruct as an image filtering model to further select high-quality images that contain multiple entities and reflect realistic, visually rich scenes. Step 1: Manual Cropping and Visual Search. Given an original image, annotators are instructed to extract salient local regions (e.g., objects, logos, landmarks, or individuals) rather than relying on the full image. Each cropped region is subsequently used as query to web-scale image search engine, producing candidate visual search results. Step 2: Visual Entity Extraction and Verification. For the retrieval results associated with each crop, we extract candidate entity names (e.g., persons, products, locations, or organizations) from webpage titles and captions. Specifically, we first prompt Qwen3-VL-235B-A22B-Instruct to filter and verify the consistency between the query crop and search results, followed by human validation to ensure that entity names cannot be trivially obtained through full-image search alone. Step 3: Seed VQA Generation from Visual Entities. For each verified visual entity, we use Gemini-2.5-Pro to synthesize seed VQA pairs that require explicit recognition and grounding of the visual entity, such as identifying landmarks, brands, or object categories depicted in the image. These VQA pairs are then manually reviewed to ensure clarity, answer uniqueness, and strict dependence on both the visual entity and its retrieved context. Step 4: Knowledge-GraphBased Complexity Expansion. To move beyond single-hop recognition, we link each visual entity to an external knowledge graph and perform random walks to retrieve related textual entities (e.g., founders, cities, years, or affiliated organizations). We then construct multi-hop reasoning questions whose inference chains originate from the visual entity and traverse one or more knowledge-graph nodes. For example, questions may ask for the headquarters city of company whose logo appears in the image, or the birth year of the architect who designed depicted building. Step 5: Solvability and Quality Verification. All synthesized questions undergo two-stage solvability check. First, we automatically verify that answers can be recovered by combining the recorded visual search trajectory (cropped regions and retrieved entities) with knowledge-graph expansion, without relying on any hidden information. Second, human annotators further filter questions to ensure the absence of text-only or prior-based shortcuts, and to confirm that reasoning paths are explicit, valid, and unambiguous."
        },
        {
            "title": "Preprint",
            "content": "4.2. Benchmark Composition and Metrics Building on the above vision-centric curation pipeline, we construct VDR-Bench, comprising 2, 000 multi-hop VQA instances spanning 10 diverse visual domains. The benchmark covers broad spectrum of visual complexity, entity density, and reasoning depth. Dataset composition and domain distribution are shown in Fig. 1. We use two core metrics: answer accuracy and entity recall, to comprehensively evaluate the correctness of the answer and the search engine hit rate at the entity level. Answer Accuracy. We extract the agents final answer from the last reasoning step and evaluate its correctness using Qwen3-VL-30B-A3B-Instruct as the judge model. The evaluation follows the judge prompt template from the Tongyi DeepResearch report (Team et al., 2025). Details are provided in the Appendix. Entity Recall. In addition to accuracy, we propose Entity Recall (ER) to evaluate whether multimodal deep-research system successfully discovers the task-relevant entities and then probes deeper. ER measures the alignment between the searched entities and predefined gold entity sequence. For each task i, let Ei = {e1, . . . , eNi} denote the gold entity sequence, and let Si = (cid:83)T t=1 st denote the set of entities extracted from the agents search trajectory across time steps. Specifically, ER is evaluated under an LLM-asa-judge paradigm, which assesses the semantic equivalence between searched entities and gold annotations. Formally, the success of the i-th trajectory is defined as: Si = [LLMeval (Si, Ei) = 1] , (1) where the indicator function [] yields 1 if the LLM judge confirms that the set of retrieved entities Si sufficiently covers the gold requirements Ei through semantic reasoning, and 0 otherwise. Unlike string matching, this approach recognizes semantic synonyms and respects the fact that an agent might follow several equally valid paths to reach the target entities. 5. Experiments 5.1. Experimental Setups We evaluate state-of-the-art VisionLanguage Models (VLMs), including Gemini 2.5 Pro (Comanici et al., 2025), GPT-5 (OpenAI., 2025), Claude-4-Sonnet (Anthropic, 2025), Qwen3-VL-30B-A3B-Instruct (Bai et al., 2025), Qwen3-VL-235B-A22B-Instruct (Bai et al., 2025), Vision-DeepResearch (Huang et al., 2026) on VDR-Bench. Performance is reported as answer accuracy across 10 visual domains with overall results summarized in Table 2. To isolate the impact of search and visual grounding, we benchmark each model under three controlled inference settings: (1) Direct Answer. Models answer questions using only the input image and question, without external search. (2) CIS+TS. Models are equipped with vision-centric cropped-image search (CIS) module and text search (TS) interface, allowing multi-step retrieval over both visual and textual sources. (3) CIS+TS+MVF. Building on CIS+TS, we further enable Multi-turn Visual Forcing (MVF), which encourages iterative region cropping, refined visual querying, and explicit cross-modal evidence verification. All models operate under fixed search budget and identical interaction constraints to ensure fair comparison. This design enables us to quantify the contribution of vision-centric search, textual retrieval, and multi-turn visual grounding to overall reasoning performance. 5.2. Results Analysis. The main results are shown in Tab. 2. We first observe that all models achieve relatively low scores when answering directly, which clearly indicates that VDR-Bench requires models to actively perform search to obtain the answers, rather than relying solely on prior knowledge. When equipped with search tools(CIS+TS), Qwen3-VL235B-A22B-Instruct achieves the highest score of 21.2, even outperforming all closed-source models. We refer to this phenomenon as lazy search: models with strong prior knowledge tend to rely on textual reasoning or avoid search tools altogether when facing visual deep-research tasks. As result, their powerful priors do not effectively translate into improved visual search capability. In contrast, open-source models with weaker prior knowledge demonstrate surprisingly strong search capabilities. Moreover, this ability improves steadily as model scale increases. These findings suggest that enhancing vision deep-research agent cannot rely solely on improving the base model through pretraining; instead, it is crucial to encourage extensive and effective use of search tools when handling such tasks. 5.3. The Correct Paradigm and Challenges. The existence of the lazy search phenomenon raises an important question: why do models strong prior knowledge and general capabilities fail to transfer effectively to vision deep-research tasks? To address this issue, we propose Multi-turn Visual Forcing (MVC), zero-shot method designed to enhance models performance on vision deep-research tasks. MVC first guides the model to conduct fine-grained, multi-scale visual retrieval over the input image, and then encourages deeper Table 2. Performance Comparison of Models Across Different Categories (Accuracy). Direct Answer means model directly answer the VQA without search tools, and CIS+TS means adopting both cropped-image search and text search. MVC means the Multi-turn Visual Forcing strategy."
        },
        {
            "title": "Preprint",
            "content": "Model / Setting Gemini 2.5 Pro Direct Answer CIS+TS CIS+TS+MVF GPT-5 Direct Answer CIS+TS CIS+TS+MVF Claude-4-Sonnet Direct Answer CIS+TS CIS+TS+MVF Qwen3-VL-30B-A3B-Instruct Direct Answer CIS+TS CIS+TS+MVF Qwen3-VL-235B-A22B-Instruct Direct Answer CIS+TS CIS+TS+MVF People Object Arch. Nature Sci&Tech Art&Music Sports Movie Game Other Overall 6.4 14.9 38.3 4.4 20.8 23.4 2.1 14.9 12. 0.0 17.0 25.5 6.2 25.2 25.0 9.8 15.7 23. 9.8 17.6 25.5 3.9 9.8 17.6 3.9 19.6 21. 3.9 19.5 23.5 9.8 27.5 33.3 11.7 14.0 23. 7.8 19.6 24.0 3.9 17.6 23.5 10.0 24.0 30. 8.2 12.2 24.5 12.3 16.7 20.4 6.2 16.3 35. 6.1 16.3 18.4 22.9 21.1 31.2 12.0 24.0 22. 10.0 24.5 18.0 10.0 18.0 15.1 2.0 20.0 8. 7.5 18.5 30.2 11.8 17.6 39.2 7.8 21.2 27. 7.8 11.8 26.9 7.8 5.9 23.5 13.5 17.1 28. 4.2 12.5 25.0 8.4 12.5 22.9 2.2 10.4 16. 2.1 14.6 16.7 6.2 10.7 20.8 2.0 10.2 24. 8.2 19.3 30.6 7.7 1.9 21.2 3.8 20.8 30. 0.0 4.1 3.8 3.8 12.3 23.1 4.1 10.2 18. 3.5 29.1 22.8 0.0 5.8 28.8 7.5 16.6 30. 9.6 25.0 48.1 13.5 25.0 42.3 11.5 23.1 24. 7.7 44.2 26.9 7.5 31.5 32.5 8.2 16.2 30. 9.5 19.2 26.6 5.6 13.2 20.6 3.8 17.2 21. 8.8 21.2 27.4 reasoning grounded in the retrieved visual evidence, thereby better leveraging the models prior knowledge. Through iterative interaction with the search engine, the model becomes more likely to acquire relevant world knowledge about key entities. Tab 2 show that MVC effectively improves the vision deepresearch capabilities of different models. Among them, Gemini achieves the highest score(16.2 30.0). 6. Conclusion We identify two critical flaws in existing VisionDeepResearch benchmarks: reliance on text-only or priorbased shortcuts, and overly idealized one-shot image retrieval that fails to reflect realistic visual search. These issues hinder faithful evaluation of visual grounding and crossmodal verification. To overcome these limitations, we propose VDR-Bench, vision-first benchmark built with multiscale cropping, entity-level verification, and knowledgegraphbased multi-hop reasoning. VDR-Bench enforces visual necessity and mitigates shortcut exploitation, offering more realistic testbed for Vision-DeepResearch systems. Experiments show that strong performance depends on iterative visual search and cross-modal evidence aggregation, and that simple multi-round cropped search strategy already yields meaningful gains. Our results provide practical guidance for building more robust multimodal deep research agents. Figure 4. Relationship between overall answer accuracy and entity-level recall on VDR-Bench. Points correspond to different models evaluated under two retrieval strategies (CIS+TS and CIS+TS+MVF), with colors indicating model families and marker shapes indicating the search mode. The plot shows strong positive association between correctly answering questions and successfully retrieving task-relevant entities, and further indicates that MVF tends to improve both metrics across models."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing claude 4. anthropic.com/news/claude-4, 2025. https://www. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-VL Technical Report. arXiv e-prints, art. arXiv:2511.21631, November 2025. doi: 10.48550/arXiv.2511.21631. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pp. 370387. Springer, 2024a. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37: 2705627087, 2024b. Chen, L., Wei, X., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Tang, Z., Yuan, L., et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024c. Chen, Y., Hu, H., Luan, Y., Sun, H., Changpinyo, S., Ritter, A., and Chang, M.-W. Can pre-trained vision and language models answer visual information-seeking questions? arXiv preprint arXiv:2302.11713, 2023. Cheng, X., Zhang, W., Zhang, S., Yang, J., Guan, X., Wu, X., Li, X., Zhang, G., Liu, J., Mai, Y., et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 46374646, 2025. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Fu, M., Peng, Y., Liu, B., Wan, Y., and Chen, D. Livevqa: Live visual knowledge seeking. arXiv preprint arXiv:2504.05288, 2025. Geng, X., Xia, P., Zhang, Z., Wang, X., Wang, Q., Ding, R., Wang, C., Wu, J., Zhao, Y., Li, K., et al. Webwatcher: Breaking new frontier of vision-language deep research agent. arXiv preprint arXiv:2508.05748, 2025. Han, R., Fang, Z., Sun, X., Ma, Y., Wang, Z., Zeng, Y., Chen, Z., Chen, L., Huang, W., Xu, W.-J., et al. Unicorn: Towards self-improving unified multimodal models through self-generated supervision. arXiv preprint arXiv:2601.03193, 2026. Hong, J., Zhao, C., Zhu, C., Lu, W., Xu, G., and Yu, X. Deepeyesv2: Toward agentic multimodal model. arXiv preprint arXiv:2511.05271, 2025. Huang, W., Chen, S., Xie, Z., Cao, S., Tang, S., Shen, Y., Yin, Q., Hu, W., Wang, X., Tang, Y., et al. Interleaving reasoning for better text-to-image generation. arXiv preprint arXiv:2509.06945, 2025a. Huang, W., Jia, B., Zhai, Z., Cao, S., Ye, Z., Zhao, F., Xu, Z., Hu, Y., and Lin, S. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025b. Huang, W., Zeng, Y., Wang, Q., Fang, Z., Cao, S., Chu, Z., Yin, Q., Chen, S., Yin, Z., Chen, L., Chen, Z., Hu, Y., Torr, P., Zhao, F., and Ouyang, W. Vision-deepresearch: Incentivizing deepresearch capability in multimodal large language models. preprint, 2026. Jiang, D., Zhang, R., Guo, Z., Wu, Y., Qiu, P., Lu, P., Chen, Z., Song, G., Gao, P., Liu, Y., et al. Mmsearch: Unveiling the potential of large models as multi-modal search engines. In The Thirteenth International Conference on Learning Representations. Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025. Narayan, K., Xu, Y., Cao, T., Nerella, K., Patel, V. M., Shiee, N., Grasch, P., Jia, C., Yang, Y., and Gan, Z. Deepmmsearch-r1: Empowering multimodal llms in multimodal web search. arXiv preprint arXiv:2510.12801, 2025. OpenAI. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. Qi, Y., Zhao, Y., Zeng, Y., Bao, X., Huang, W., Chen, L., Chen, Z., Zhao, J., Qi, Z., and Zhao, F. Vcr-bench: A"
        },
        {
            "title": "Preprint",
            "content": "Zhao, Y., Zeng, Y., Qi, Y., Liu, Y., Chen, L., Chen, Z., Bao, X., Zhao, J., and Zhao, F. V2p-bench: Evaluating video-language understanding with visual prompts arXiv preprint for better human-model interaction. arXiv:2503.17736, 2025. comprehensive evaluation framework for video chainof-thought reasoning. arXiv preprint arXiv:2504.07956, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Tao, Z., Wu, J., Yin, W., Zhang, J., Li, B., Shen, H., Li, K., Zhang, L., Wang, X., Jiang, Y., et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025. Team, T. D., Li, B., Zhang, B., Zhang, D., Huang, F., Li, G., Chen, G., Yin, H., Wu, J., Zhou, J., et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. Wang, P., Wu, Q., Shen, C., Dick, A., and Van Den Hengel, A. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):24132427, 2017. Wang, Q., Ding, R., Zeng, Y., Chen, Z., Chen, L., Wang, S., Xie, P., Huang, F., and Zhao, F. Vrag-rl: Empower vision-perception-based rag for visually rich information understanding via iterative reasoning with reinforcement learning. arXiv preprint arXiv:2505.22019, 2025. Wu, J., Deng, Z., Li, W., Liu, Y., You, B., Li, B., Ma, Z., and Liu, Z. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025a. Wu, J., Li, B., Fang, R., Yin, W., Zhang, L., Tao, Z., Zhang, D., Xi, Z., Fu, G., Jiang, Y., et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025b. Yu, S., Tang, C., Xu, B., Cui, J., Ran, J., Yan, Y., Liu, Z., Wang, S., Han, X., Liu, Z., et al. Visrag: Visionbased retrieval-augmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594, 2024. Zeng, Y., Huang, W., Huang, S., Bao, X., Qi, Y., Zhao, Y., Wang, Q., Chen, L., Chen, Z., Chen, H., et al. Agentic jigsaw interaction learning for enhancing visual perception and reasoning in vision-language models. arXiv preprint arXiv:2510.01304, 2025a. Zeng, Y., Qi, Y., Zhao, Y., Bao, X., Chen, L., Chen, Z., Huang, S., Zhao, J., and Zhao, F. Enhancing large visionlanguage models with ultra-detailed image caption generation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 2670326729, 2025b. 10 A. More Results"
        },
        {
            "title": "Preprint",
            "content": "Table 3. Performance comparison of models across different categories (Accuracy) on testmini set. Direct Answer means the model answers VQA directly without search tools. CIS+TS means adopting both the cropped-image search and text search. Model / Setting Gemini 2.5 Pro Direct Answer CIS+TS GPT-5 Direct Answer CIS+TS Claude-4-Sonnet Direct Answer CIS+TS Qwen3-VL-30B-A3B-Instruct Direct Answer CIS+TS Qwen3-VL-235B-A22B-Instruct Direct Answer CIS+TS Vision-DeepResearch-8B Direct Answer CIS+TS Vision-DeepResearch-30B Direct Answer CIS+TS People Object Arch. Nature Sci&Tech Art&Music Sports Movie Game Other Overall 12.5 22.9 6.3 14. 0.0 18.8 2.1 12.5 6.3 25.0 6.3 24.5 6.2 35.4 2.0 21. 9.8 27.5 0.0 11.8 2.0 25.5 2.0 13.7 0.0 35.8 0.0 43. 8.0 14.0 12.0 20.0 0.0 14.0 6.0 16.0 8.0 22.0 10.0 21. 6.0 44.0 8.3 16.7 12.5 29.2 0.0 18.8 4.2 22.9 22.9 20. 2.1 38.4 8.3 33.3 0.0 17.0 11.3 15.1 0.0 13.2 3.8 15. 1.9 17.0 0.0 18.6 5.7 32.1 15.4 17.3 7.7 26.9 0.0 13. 5.8 19.2 9.6 17.3 3.8 33.1 5.8 32.7 6.3 20.8 8.3 10. 0.0 8.3 2.1 10.4 8.3 8.3 0.0 27.7 6.2 35.4 5.3 21. 8.8 14.0 1.8 5.3 3.5 24.6 3.5 26.3 0.0 31.5 0.0 36. 9.4 11.3 15.0 22.5 9.4 11.3 12.5 32.5 3.8 3.8 7.5 3. 12.5 22.5 7.5 45.0 13.2 15.1 7.5 45.0 0.0 30.2 12.5 29. 5.7 35.8 5.0 52.5 8.0 18.8 9.5 20.4 2.0 13.6 3.8 20. 7.9 20.8 3.2 29.2 4.8 37."
        },
        {
            "title": "Preprint",
            "content": "Table 4. Performance comparison of models across different categories (Entity Recall) on testmini set. CIS+TS means adopting both the cropped-image search and text search. CIS+TS+MVF further applies the Multi-turn Visual Forcing strategy on top of CIS+TS. Model / Setting Gemini 2.5 Pro CIS+TS CIS+TS+MVF GPT-5 CIS+TS CIS+TS+MVF Claude-4-Sonnet CIS+TS CIS+TS+MVF Qwen3-VL-30B-A3B-Instruct CIS+TS CIS+TS+MVF Qwen3-VL-235B-A22B-Instruct CIS+TS CIS+TS+MVF Vision-DeepResearch-8B CIS+TS Vision-DeepResearch-30B CIS+TS People Object Arch. Nature Sci&Tech Art&Music Sports Movie Game Other Overall 10.4 27.1 10.4 20. 6.2 25.0 10.4 14.6 14.6 31.2 11.8 39.2 13.7 35.3 7.8 17. 11.8 25.5 9.8 27.5 12.0 32.0 16.0 18.0 6.0 22.0 14.0 16. 16.0 32.0 10.4 41.7 16.7 31.2 6.2 25.0 12.5 22.9 33.3 27. 11.3 28.3 15.1 18.9 7.5 18.9 13.2 17.0 13.2 26.4 11.5 40. 11.5 26.9 7.7 19.2 13.5 21.2 19.2 25.0 14.6 22.9 12.5 16. 6.2 14.6 10.4 12.5 16.7 16.7 14.0 28.1 12.3 33.3 8.8 12. 12.3 26.3 14.0 36.8 11.3 24.5 25.0 37.5 15.1 11.3 17.5 32. 7.5 11.3 17.5 30.0 15.1 3.8 17.5 45.0 22.6 24.5 17.5 37. 13.0 32.0 14.0 24.4 8.0 19.2 13.0 20.0 17.6 28.4 31. 41.2 26.0 43.8 24.5 38.5 33. 36.8 35.8 35.0 34.6 41.7 51. 52.0 39.6 39.6 40.4 41.7 43. 43.4 60.0 45."
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "East China Normal University",
        "Peking University",
        "Shenzhen Loop Area Institute",
        "The University of California, Los Angeles",
        "University of Oxford",
        "University of Science and Technology of China",
        "Xiaohongshu Inc.",
        "Zhejiang University"
    ]
}