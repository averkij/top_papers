{
    "paper_title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
    "authors": [
        "Hongzhi Zang",
        "Mingjie Wei",
        "Si Xu",
        "Yongji Wu",
        "Zhen Guo",
        "Yuanqing Wang",
        "Hao Lin",
        "Liangzhi Shi",
        "Yuqing Xie",
        "Zhexuan Xu",
        "Zhihao Liu",
        "Kang Chen",
        "Wenhao Tang",
        "Quanlu Zhang",
        "Weinan Zhang",
        "Chao Yu",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 0 1 7 6 0 . 0 1 5 2 : r Technical Report @ RLinf Team RLINF-VLA: UNIFIED AND EFFICIENT FRAMEWORK FOR VLA+RL TRAINING Hongzhi Zang1, Mingjie Wei6,2, Si Xu3, Yongji Wu5, Zhen Guo3, Yuanqing Wang4,3, Hao Lin3, Liangzhi Shi1, Yuqing Xie1, Zhexuan Xu1, Zhihao Liu7,2, Kang Chen4,2, Wenhao Tang1, Quanlu Zhang3, Weinan Zhang6, Chao Yu1,2,,, Yu Wang1, Corresponding Authors: zoeyuchao@gmail.com;yu-wang@tsinghua.edu.cn Project Lead 1 Tsinghua University 5 UC Berkeley 7 Institute of Automation, Chinese Academy of Sciences 6 Harbin Institute of Technology 2 Zhongguancun Academy 3 Infinigence AI 4 Peking University https://huggingface.co/RLinf https://github.com/RLinf/RLinf"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring surge of interest in extending such capabilities to embodied settings through visionlanguage-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, unified and efficient framework for scalable RL training of VLA models. The system adopts highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and In particular, for GPU-parallelized simulators, inference in RL+VLA training. RLinf-VLA implements novel hybrid fine-grained pipeline allocation mode, achieving 1.61x-1.88x speedup in training. Through unified interface, RLinfVLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLAOFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, unified model achieves 98.11% across 130 LIBERO tasks and 97.66% across 25 ManiSkill tasks. Beyond empirical performance, our study distills set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinfVLA as foundation to accelerate and standardize research on embodied intelligence. 1 Technical Report @ RLinf Team"
        },
        {
            "title": "CONTENTS",
            "content": "1 Introduction 2 Preliminary for Reinforcement Learning 3 Framework Design"
        },
        {
            "title": "3.1 GPU Allocation Strategies",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.1.4 Configuring GPU Allocation Modes . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.2 Model Compatibility . 3.2.1 LoRA Support 3.2.2 Model type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.3 Unified Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Multiple Simulators Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Unified Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Multi-Algorithm Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 PPO . . 3.4.2 GRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Algorithm Design 4.1 Advantage and Log-Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Advantage with Action Chunks . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Log-Probability Granularity . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.3 Supported Granularity Combinations for Advantage and Log-Probability . 4.2 Design Choices . 4.2.1 PPO . . 4.2.2 GRPO . . . . . . 5 Experiment Results 5.1 High-performance . 5.1.1 ManiSkill . 5.1.2 LIBERO . 5.2 High-efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.2 Results . 5.3 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 4 6 6 7 7 8 8 8 9 9 9 10 10 11 11 12 12 12 13 13 13 15 15 15 16 17 18 20 Technical Report @ RLinf Team"
        },
        {
            "title": "5.3.1 Tips for PPO .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.3.2 Tips for GRPO .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.4 Real-World Deployment",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion 7 Future Work 20 21 24 24 25 3 Technical Report @ RLinf Team Figure 1: Overview of RLinf-VLA. Through unified interface, RLinf-VLA seamlessly supports diverse VLA architectures, multiple RL algorithms, and various simulators. To flexibly accommodate the integration of rendering, training, and inference in RL+VLA training, RLinf-VLA provides three GPU allocation modes: colocated, disaggregated, and novel hybrid mode. single unified model achieves 98.11% success on 130 LIBERO tasks and 97.66% on 25 ManiSkill tasks in simulation, while an RL-trained model demonstrates superior zero-shot generalization on real-world Franka robot compared to the SFT model."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-Language-Action (VLA) models represent new generation of foundation models that aim to unify perception, language understanding, and embodied control (Ma et al., 2024; Firoozi et al., 2025). By leveraging vision-language models pretrained on internet-scale data and further training them on large, heterogeneous robot demonstration datasets (ONeill et al., 2024; Khazatsky et al., 2024), VLAs can map raw sensor observations and natural language instructions directly into robot actions. This paradigm has already shown strong generalization across wide range of tasks (Team et al., 2024; Kim et al., 2024; Liu et al., 2024; Wen et al., 2025; Shah et al., 2023; Black et al., 2024; Intelligence et al., 2025). However, the dominant training paradigm for VLAs remains supervised fine-tuning (SFT) via behavioral cloning on demonstration data (Kim et al., 2025). While effective in matching expert trajectories, SFT is highly vulnerable to distribution shift: small deviations from expert behavior can compound over time, pushing the policy into unfamiliar states and degrading robustness (Ross & Bagnell, 2010; De Haan et al., 2019; Belkhale et al., 2023; Foster et al., 2024). This mismatch between training and deployment distributions fundamentally limits scalability and generalization. Reinforcement learning (RL) offers complementary solution by directly optimizing cumulative task rewards through trial and error (Sutton & Barto, 1998). Unlike SFT, RL enables exploration beyond narrow expert data and equips policies with corrective strategies. Recent evidence suggests that RL fine-tuning can yield stronger out-of-distribution performance than SFT, particularly in both semantic alignment and execution robustness (Liu et al., 2025). Yet, progress on applying RL to VLAs remains small-scale (Liu et al., 2025) or fragmented (Lu et al., 2025; Li et al., 2025), primarily due to the lack of unified and efficient framework to support large-scale experimentation. Unlike RL post-training for LLMs or VLMs, RL for VLAs requires repeated interactions with environments. These environments are typically realized through simulators, which also consume significant GPU memory and thus compete with model training and inference for memory resourcesan issue that existing large-scale RL frameworks handle inefficiently. In addition, different works adopt diverse models, algorithms, and evaluation setups, making it difficult to compare methods fairly or distill general principles. 4 Technical Report @ RLinf Team To address these challenges, we present RLinf-VLA, unified and efficient framework for scalable RL training of VLA models. Unified: RLinf-VLA provides cohesive interface that brings together multiple dimensions of RL for VLA training. It integrates different simulators, including ManiSkill (Tao et al., 2025) and LIBERO (Liu et al., 2023a), with features such as partial environment reset. It supports multiple algorithms, implementing both Proximal Policy Optimization (PPO) (Schulman et al., 2017; Ouyang et al., 2022) and Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025). The framework is compatible with diverse VLA architectures, including OpenVLA (Kim et al., 2024) and OpenVLA-OFT (Kim et al., 2025) with action chunking. To flexibly accommodate the integration of rendering, training, and inference in RL+VLA training, RLinf-VLA provides three GPU allocation modes: colocated, disaggregated, and novel hybrid mode. Switching between these modes can be achieved through straightforward configuration adjustments. Efficient: Beyond unification, RLinf-VLA is designed for efficiency at both the system and algorithmic levels. It achieves 2.27 improvement in throughput compared to the baseline framework. In particular, within GPU-parallelized simulators, the hybrid fine-grained pipelining mechanism further accelerates RL training, yielding 1.611.88 speedup. On the algorithmic side, RLinf-VLA incorporates several design choices, including lightweight critics, loss normalization, action masking, and rollout filtering, which collectively substantially enhance training efficiency. Extensive experiments validate the effectiveness of RLinf-VLA. In simulation, single unified model attains 98.11% success across 130 LIBERO tasks for the first time and 97.66% success across 25 ManiSkill tasks. More importantly, our study distills set of best practices for RL-based VLA training: PPO: action-level value estimation is superior to chunk-level estimation when using action chunks, and partial resets substantially improve sample efficiency. GRPO: trajectory-length normalization and action masking are essential for stable training. General Practices: larger rollout batch size is useful; LoRA may not directly affect performance but typically requires hyperparameter re-tuning. Finally, we demonstrate preliminary deployment on real-world Franka robot, where RL-trained policies exhibit stronger zero-shot generalization than those trained with SFT. Specifically, in pickand-place task with six previously unseen objects, the RL policy successfully completed 8 out of 30 trials, whereas the SFT policy failed in all 30 trials under the same zero-shot conditions. RLinfVLA has been open-sourced and is maintained with frequent updates. We envision RLinf-VLA as foundation to accelerate, standardize, and scale research in embodied intelligence. Our contribution can be summarized as follows: Unified design: RLinf-VLA supports multiple simulators (ManiSkill, LIBERO), diverse VLA architectures (OpenVLA, OpenVLA-OFT), and RL algorithms (PPO, GRPO), with three execution modes, including novel hybrid GPU allocation mode, enabling scalable and configurable training. Efficiency in system and algorithm design: Hybrid fine-grained pipelining for GPUparallelized simulators, colocated allocation for CPU-parallelized simulators, and algorithmic enhancements such as lightweight critics, loss normalization, action masking, and rollout filtering substantially improve training efficiency. Strong empirical performance and generalization: single unified model achieves 98.11% success on 130 LIBERO tasks and 97.66% on 25 ManiSkill tasks in simulation, while an RL-trained model demonstrates superior zero-shot generalization on real-world Franka robot compared to the SFT model. Open-source and maintainable platform: RLinf-VLA is publicly released with frequent updates, providing foundation to accelerate, standardize, and scale research in embodied intelligence. 5 Technical Report @ RLinf Team"
        },
        {
            "title": "2 PRELIMINARY FOR REINFORCEMENT LEARNING",
            "content": "In this paper, we finetune VLA models with reinforcement learning to solve vision-based manipulation tasks, which we formulate as partially observable Markov decision process (POMDP). POMDP extends the standard MDP and is defined by the tuple (S, A, P, r, γ, Ω, O), where denotes the state space, the action space (discrete or continuous, potentially chunked), (s s, a) the state transition function, r(s, a) the reward function, γ [0, 1] the discount factor, Ω the observation space, and O(o s) the observation function mapping state to an observation o. At each timestep t, the agent samples an action at πθ( ot) based on the current observation ot O( st). The environment then executes at, transitions to the next state st+1 via , and produces the next observation ot+1. The goal of RL is to optimize the parameterized policy πθ so as to maximize the expected cumulative discounted reward over trajectories τ = (s0, a0, . . . , sT ): J(θ) = Eτ πθ (cid:34) (cid:88) (cid:35) γtr(st, at) . t=0 (1) We now recall several standard definitions in RL: Definition 1 (Return). The return Rt is the γ-discounted cumulative reward starting from timestep t: Rt = (cid:88) k=0 γkr(st+k, at+k). (2) Definition 2 (Value Function). The value function Vπ(s) is the expected return obtained from state at timestep when following policy π: Vπ(s) = Eπ[Rt st = s]. (3) Definition 3 (Action-Value Function). The action-value function Qπ(s, a) is the expected return obtained by executing action in state at timestep and thereafter following policy π: Qπ(s, a) = Eπ[Rt st = s, at = a]. (4) Definition 4 (Advantage Function). The advantage function Aπ(s, a) quantifies how much better taking action in state is compared to the expected value of the state: Aπ(s, a) = Qπ(s, a) Vπ(s). (5)"
        },
        {
            "title": "3.1 GPU ALLOCATION STRATEGIES",
            "content": "In robotic reinforcement learning, GPU resources are primarily consumed by the training process (Training) and the rollout process (Generation and Simulator). The training loop is shown in Figure 2. Simulators, however, vary by type: CPU-parallelized and GPU-parallelized simulators exhibit fundamentally different patterns of resource utilization. In CPU-parallelized simulators, the simulation logic is distributed across multiple CPU workers, while the GPU is mainly utilized for rendering and policy inference. As result, CPU cores often become the primary bottleneck limiting overall performance. By contrast, GPUparallelized simulators integrate rendering, physics simulation, and inference directly on the GPU, exploiting massive parallelism to achieve significantly higher throughput. However, this tight coupling also causes different components to compete for GPU memory and compute resources, making the overall performance highly sensitive to resource scheduling. Consequently, CPU-parallelized and GPU-parallelized simulators require distinct GPU allocation strategies, and efficient, flexible allocation across simulation, training, and inference is essential to maximize throughput. Figure 2: training loop VLA+RL 6 Technical Report @ RLinf Team (a) Colocated GPU Allocation (b) Disaggregated GPU Allocation (c) Hybrid GPU Allocation with Fine-Grained Pipelining Figure 3: Different GPU allocation strategies. To address this issue, our framework supports three allocation modes: colocated, disaggregated, and hybrid. Users can easily switch between these modes by simply modifying YAML configuration file, greatly reducing the overhead of system customization. In particular, for GPU-parallelized simulators, we introduce novel hybrid allocation with fine-grained pipelining strategy that further improves utilization efficiency. Figure 3 illustrates the core ideas of these allocation modes, while their algorithmic features are summarized here. For the full system design of RLinf, we refer the reader to Yu et al. (2025a). 3.1.1 COLOCATED ALLOCATION In this mode, all components (Training, Generation, and Simulator) share all GPUs simultaneously. This maximizes data parallelism and ensures that no GPU is left completely idle. Figure 3a provides an overview of the colocated allocation mode. Offload Support To make colocated allocation feasible under memory constraints, we introduce offloading support. Idle components can be temporarily moved to CPU memory, freeing GPU resources for the active ones. For Training and Generation, this offloadonload process is relatively straightforward. For the Simulator, however, we must manually store all essential environment states, so that the simulator can resume correctly once reloaded to the GPU. Despite this support, colocated allocation is not always efficient. Two problematic cases often arise: 1) When multiple components reside on the same GPU, they may occupy memory concurrently even though only one is actively computing, leading to wasted capacity. 2) Although offloading reduces memory contention, its overhead can become substantial in practice. In particular, the frequent interaction between Generation and Simulator during rollout makes repeated offloadonload operations prohibitively expensive. 3.1.2 DISAGGREGATED ALLOCATION Figure 3b depicts the central idea of the disaggregated allocation mode. In this mode, each component is assigned to distinct (possibly multi-GPU) partition, with no overlap across components. This ensures that every component can fully exploit its allocated resources. However, disaggregated allocation may still lead to GPU underutilization due to inter-component dependencies. For instance, the Simulator must wait for actions generated by the Generation, leaving some GPUs idle and creating GPU bubbles. 7 Technical Report @ RLinf Team"
        },
        {
            "title": "3.1.3 HYBRID ALLOCATION WITH FINE-GRAINED PIPELINING",
            "content": "To overcome the drawbacks of the above modes, we propose hybrid strategy: hybrid allocation combined with fine-grained pipelining. In hybrid allocation, components can flexibly select GPUs. typical configuration is to assign Generation and Simulator to different GPU partitions, while allowing Training to utilize all GPUs. On top of this, we introduce fine-grained pipelining to mitigate bubbles caused by inter-component dependencies. Specifically, simulator instance on one GPU is partitioned into multiple subsimulators, denoted as S(1), S(2), . . . , S(k). The pipeline proceeds as follows1: 1. At step = 0, S(1) generates the initial observation o(1) 0 , which is sent to the Generation component to produce action a(1) 0 . 2. Meanwhile, S(2) generates o(2) 0 3. Once a(1) in parallel. is ready, it is fed back into S(1) to produce the next observation o(1) 1 , while o(2) 0 is simultaneously processed by the actor to generate a(2) 0 . This scheduling allows Simulator and Generation to run concurrently, reducing idle time while preserving correctness. In this way, our framework avoids the frequent offloading overhead of colocated allocation while also eliminating the GPU bubbles that occur in disaggregated allocation. Figure 3c provides schematic illustration of the hybrid allocation mode with fine-grained pipelining (k = 2). 3.1.4 CONFIGURING GPU ALLOCATION MODES Our framework exposes simple configuration interface that allows users to flexibly choose GPU allocation strategies without explicitly specifying the mode. Instead, users only need to assign GPU IDs for each component via the following fields: cluster.component placement.env for the Simulator, cluster.component placement.rollout for Generation, cluster.component placement.actor for Train. In addition, offloading can be enabled or disabled independently for each component using: env.enable offload, rollout.enable offload, actor.enable offload. Finally, fine-grained pipelining is controlled by the parameter rollout.pipeline stage num (corresponding to in Section 3.1.3). value greater than 1 splits each simulator instance on GPU into multiple pipeline stages, while value of 1 disables pipelining. 3.2 MODEL COMPATIBILITY Our framework is designed to be flexible and extensible, supporting both parameter-efficient training methods and different VLA model variants. In particular, we provide built-in support for Low-Rank Adaptation (LoRA) (Hu et al., 2021), as well as for the OpenVLA (Kim et al., 2024) and OpenVLAOFT (Kim et al., 2025) models. 3.2.1 LORA SUPPORT LoRA (Hu et al., 2021) is parameter-efficient fine-tuning method that adapts large pre-trained models to new tasks without updating all of their parameters. Instead of modifying the full weight matrices, LoRA introduces additional low-rank decomposition layers into linear transformations and trains only these small sets of parameters while keeping the original weights frozen. This 1In this section, superscripts denote simulator indices, while subscripts indicate timesteps. Technical Report @ RLinf Team greatly reduces the number of trainable parameters, lowering memory consumption and speeding up training while maintaining performance comparable to full fine-tuning. In our framework, users can flexibly choose whether to enable LoRA. If is lora is enabled, all linear modules in VLA are designated as target modules by default, ensuring broad coverage of trainable parameters while preserving efficiency."
        },
        {
            "title": "3.2.2 MODEL TYPE",
            "content": "In RLinf-VLA, the argument model name specifies the VLA model to be used, with current support for openvla and openvla oft. We briefly introduce these two models below. OpenVLA OpenVLA (Kim et al., 2024) is VLA model containing about 7B parameters. Despite being smaller than some closed-source counterparts such as RT-2-X (55B) (ONeill et al., 2024), OpenVLA demonstrates superior performance on range of robotic manipulation benchmarks, achieving higher success rates while remaining computationally efficient. Our framework supports training and evaluation of OpenVLA, with extensive experiments conducted in the ManiSkill environment. OpenVLA-OFT OpenVLA-OFT (Optimized Fine-Tuning) (Kim et al., 2025) extends OpenVLA by focusing on fine-tuning efficiency, inference speed, and practical deployability in robotics. Instead of relying on discrete action tokens and complex objectives, OpenVLA-OFT represents actions in continuous space and optimizes them using simple L1 regression loss. This design reduces training overhead and accelerates convergence. Moreover, OpenVLA-OFT incorporates parallel decoding together with action chunking, which significantly improves inference throughput and enables real-time control at higher frequencies. These advances yield strong performance gains in both the LIBERO benchmark and real-world tasks, while also achieving over an order-of-magnitude speedup in action generation. Our framework supports training OpenVLA-OFT with discretized action mode and provides experimental results on ManiSkill and LIBERO environments. 3.2.3 UNIFIED INTERFACE 3.3 MULTIPLE SIMULATORS SUPPORT Simulators play crucial role in reinforcement learning for robotics, providing controllable and efficient environments in which agents can interact, learn, and improve without the high costs and risks of physical hardware. Compared to real-world experiments, simulators offer several key advantages: Scalability of data collection: By leveraging parallel computation, simulators can generate large volume of interaction data (often at least thousands of frames per second), far exceeding what is feasible with physical robots. Task diversity: Simulators provide access to wide range of robotic tasks, including tabletop manipulation, drawing/cleaning, and dexterous manipulation. This diversity is crucial for improving and evaluating the generalization capabilities of VLA models. Safety and controllability: Agents can explore potentially dangerous or failure-prone behaviors without risking damage to hardware. Fast environment reset and reduced human labor: Simulators enable instant and reproducible environment resets, greatly accelerating the training cycle and eliminating the need for manual intervention during repeated experiments. Given these advantages, our framework focuses on implementing robotics RL in simulation. In this work, we select two representative platforms, ManiSkill and LIBERO, as our primary simulators. ManiSkill excels in physics-based manipulation tasks with diverse object assets, while LIBERO emphasizes instruction-conditioned tasks that test generalization and high-level reasoning for VLA models. Together, these simulators provide complementary capabilities, forming solid foundation for training and evaluating our framework. 9 Technical Report @ RLinf Team"
        },
        {
            "title": "3.3.1 UNIFIED INTERFACE",
            "content": "Our framework supports vectorized environments with unified interface across different simulators. For example, we leverage ManiSkills GPU-parallelized vectorized environments and LIBEROs CPU-parallelized vector environment wrapper. On top of these backends, we define consistent interface to seamlessly integrate with our training framework. The unified interface consists of two categories: core functions and utility functions. Core Functions We implement standard Gym-style APIs, including reset and step. The step function additionally supports an auto reset option: when enabled, any subenvironment that terminates or is truncated will be automatically reset, thereby improving sample efficiency by avoiding idle environments. Following the ManiSkill convention, we also support the ignore terminations option. When enabled, termination signals are ignored and only truncation signals are respected, meaning that an episode ends only when the maximum episode length is reached. This feature allows us to flexibly support different implementation variants discussed in Section 4.2.1 and Section 4.2.2. Beyond these, we extend the interface with chunk step function to handle action chunks. Instead of naively looping over the chunk with repeated step calls, this function manages episode termination more carefully. Two modes are supported: (1) reset immediately when subenvironment finishes within the chunk, or (2) defer reset until the entire chunk has been executed. This flexibility ensures the correct handling of episode boundaries when chunked actions are used. Utility Functions Utility functions offer convenient support for training and evaluation. For instance, visualization utilities enable effortless generation of videos during rollouts or evaluation. In addition, we provide specialized utilities required by specific algorithms. For example, GRPO necessitates that all environments within group share the same initial state, which can be ensured by setting use fixed reset state ids=True. 3.3.2 TASKS We implement variety of tasks based on the two simulators in our framework. ManiSkill Following RL4VLA (Liu environment al., PutOnPlateInScene25Main-v3, which consists of 25 pick-and-place tasks with variations in objects and receptacles. We also adopt the out-of-distribution evaluation settings proposed in RL4VLA. By replicating their results, we further confirm the generalizability of VLA-RL models under these task variations. 2025), we construct the et LIBERO We leverage the public benchmark task groups provided by LIBERO (Liu et al., 2023b), including LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-10, and LIBERO-90. Specifically: LIBERO-Spatial: evaluates the models ability to reason about relative object positions. LIBERO-Object: tests generalization across different object types. LIBERO-Goal: examines the models capability to adapt to diverse task goals. LIBERO-10: long-horizon benchmark consisting of 10 tasks. LIBERO-90: long-horizon benchmark covering 90 tasks. Most existing VLA+RL studies train and evaluate models within single task group. In contrast, we train our model on the combined benchmark, LIBERO-130, which comprises 130 tasks specifically constructed to evaluate RLinf-VLAs capability to learn under large-scale task settings, and we assess its performance across these task groups. 3.4 MULTI-ALGORITHM SUPPORT Our framework provides support for multiple reinforcement learning algorithms, with an initial focus on Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Group Relative Policy Technical Report @ RLinf Team Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025). Both are representative on-policy algorithms that have been widely adopted in robotics research. PPO has established itself as robust and general-purpose baseline for continuous control, while GRPO offers simplified alternative by removing the need for value function estimation. Together, they form solid foundation for training and evaluating VLA models in simulators."
        },
        {
            "title": "3.4.1 PPO",
            "content": "PPO is one of the most widely adopted reinforcement learning algorithms in robotics. PPO enhances training stability by constraining policy updates within trust region, thereby preventing overly large changes that could destabilize learning. This is achieved through clipped surrogate objective, which balances exploration and exploitation while maintaining sample efficiency. Due to its robustness and simplicity, PPO has become standard baseline in robotics RL and serves as the foundation of our framework. In PPO, the advantage function is commonly estimated using Generalized Advantage Estimation (GAE) (Schulman et al., 2015): ˆAt = t1 (cid:88) (γλ)k(cid:0)rt+k + γV (st+k+1) (st+k)(cid:1), (6) k=0 where rt denotes the reward at timestep t, (s) is the value function, γ is the discount factor, λ controls the biasvariance trade-off, and is the episode horizon. The PPO optimization objective is defined as: PPO(θ) = Et where (cid:104) min (cid:0)ρt(θ) ˆAt, clip(ρt(θ), 1 ϵ, 1 + ϵ) ˆAt πθold denotes the rollout policy, and ϵ is the clipping parameter.2 ρt(θ) = πθ(at ot) πθold (at ot) , 3.4.2 GRPO (cid:1)(cid:105) , (7) (8) GRPO is recent variant of policy optimization designed to simplify reinforcement learning pipelines. Unlike PPO, which requires training both policy model and value model, GRPO eliminates the need for an explicit value function by leveraging group-based relative comparisons of trajectories. This design reduces the overall model complexity and avoids potential inaccuracies from value estimation. As result, GRPO provides lightweight yet effective alternative to PPO, making it especially attractive for large-scale VLA training where efficiency and simplicity are important. We begin with brief overview of GRPO as applied in LLM reasoning models, and present its specific design for robotics in Section 4.1.2. In GRPO, the important ratio can be defined as: ρ(i) (θ) = πθ(a(i) πθold(a(i) o(i) ) o(i) ) , (9) where denotes i-th trajectory. But the advantage can be defined as: ˆA(i) = ˆA(i) = R(i) mean({R(j)}G std({R(j)}G j=1) , {0, 1, . . . , τ (i)}, (10) j=1) where R(i) denotes the total reward of trajectory τ (i). Note that differs from the discounted return R, and represents the group size. Given an initial observation o0 from sample buffer D, the behavior model πθold generates trajectories {τi}G τ (i) (cid:88) i=1. The GRPO optimization objective is: (cid:88) (cid:16) (cid:34) min (θ) ˆA(i), clip(cid:0)ρ(i) ρ(i) GRPO(θ) = o0D, {τ (i)}πθold 1 1 τ (i) i= t=1 (θ), 1ϵ, 1+ϵ(cid:1) ˆA(i)(cid:17) (11) (cid:35) , where τ (i) is the length of trajectory τ (i), ϵ is the clip parameter. 2The definition of action at differs across LLM and robotics RL settings. Here we treat it as generalized action, with the distinctions discussed in Section 4.1.2. The same applies to GRPO. 11 Technical Report @ RLinf Team"
        },
        {
            "title": "4.1.1 ADVANTAGE WITH ACTION CHUNKS",
            "content": "Figure 4: Illustration of different logprob levels. Action Chunk An action chunk refers to having the policy predict short sequence of future actions rather than single-step action at each timestep. This approach has been shown to better handle non-Markovianity in offline data, such as the non-Markovian behaviors often present in human demonstrations (Zhao et al., 2023). The technique is widely adopted in imitation learning. To integrate action chunks into reinforcement learning, we must define how advantages are associated with actions inside chunk. Consider the t-th chunk ct = (at,1, at,2, . . . , at,C), where denotes the chunk length. Each atomic action at,j produces an immediate reward rt,j. Two formulations are commonly used: 1. Chunk-level: The entire chunk ct is treated as macro-action, with the reward defined as the sum of per-step rewards, i.e., rt = (cid:80)C j=1 rt,j. 2. Action-level: Each at,j is treated as an individual action, with its own reward and advantage. These two formulations lead to different behaviors under GAE-based estimation. Our framework supports both options. Users can control the advantage formulation via the parameter reward type, which can be set to either chunk level or action level. 4.1.2 LOG-PROBABILITY GRANULARITY In algorithms such as PPO and GRPO, computing the log-probabilities of actions under the current policy is essential for estimating the importance sampling ratios ρ(θ) in Equations (8) and (9). However, the precise definition of an action depends on the task domain. In robotics RL, probabilities are typically computed either at the action level or the chunk level, depending on whether each environment step is considered atomic or grouped into chunk. In contrast, in LLM-based reasoning tasks, policies generate sequences of tokens, where the natural atomic unit is token. In this case, action probabilities are computed at the token level, by multiplying (or summing in log-space) the probabilities of individual tokens. framework supports three granularity choices for log-probability computation: Thus, our action level, chunk level, and token level, as shown in Figure 4. The selection is controlled by the parameter logprob type, and each option leads to different optimization dynamics. Formally, we define trajectory as τ = (c0, c1, . . . , cT ), where each chunk ct contains atomic actions, ct = (at,1, at,2, . . . , at,C), and each atomic action at,i consists of tokens, at,i = (dt,i,1, dt,i,2, . . . , dt,i,M ). The policy probabilities decompose as: Chunk-level: π(ctot) = (cid:81)C Action-level: π(at,iot, at,:i1) = (cid:81)M i=1 π(at,iot, at,:i1), j=1 π(dt,i,jot, dt,i,:j1), Technical Report @ RLinf Team Token-level: π(dt,i,jot, dt,i,:j1)."
        },
        {
            "title": "4.1.3 SUPPORTED GRANULARITY COMBINATIONS FOR ADVANTAGE AND",
            "content": "LOG-PROBABILITY Table 1 summarizes the supported combinations of advantage and log-probability granularities. In general, an advantage type is compatible with all log-probability types whose granularity is no finer than its own. When the log-probability is defined at finer granularity, we apply broadcasting mechanism to the advantage values. For instance, if the advantage is defined at the chunk level while the log-probability is computed at the token level, the same chunk-level advantage is assigned to each token within the chunk. Table 1: Supported combinations of advantage and log-probability granularities."
        },
        {
            "title": "Advantage",
            "content": "Chunk-level Action-level Log-Probability Chunk-level Action-level Token-level 4.2 DESIGN CHOICES 4.2.1 PPO In reinforcement learning rollouts, sub-environment Partial Reset Support terminates (done) when it either reaches the maximum episode length or the task is successfully completed. There are two common strategies for handling such terminations. One is the Fixed Episode Length mode, where environments are only reset after all sub-environments reach the maximum episode length. The other is the Partial Reset mode, where any sub-environment is reset immediately upon termination, without waiting for others. Figure 5a and Figure 5b to enable Fixed Episode illustrate the difference between these two modes. Within RLinf, Length, please set ignore terminations=True, while to enable Partial Reset, please set auto reset=True and ignore terminations=False. For PPO, users can flexibly choose between the two modes, which correspond to different optimization objectives. The Fixed Episode Length mode encourages the policy to maintain success until the end of an episode, aligning with the success at end objective defined in ManiSkill (Tao et al., 2025). In contrast, the Partial Reset mode emphasizes achieving success at least once during rollout, i.e., the success once objective. When the target is success once, Partial Reset can substantially improve sample efficiency, as successful episodes are immediately reset and new trajectories are collected. (a) Fixed Episode Length (b) Partial Reset (c) Valid Action Mask Figure 5: Illustration of the three rollout modes. Section 4.2.1 and Section 4.2.2 provide detailed explanations of modes (b) and (c). In the RLinf-VLA framework, modes (a) and (b) are adopted for PPO, while modes (a) and (c) are adopted for GRPO. Critic Design To support PPO in VLA, the design of the critic is crucial. Using an entirely separate critic model would be computationally expensive and complicate GPU resource allocation, Technical Report @ RLinf Team given the large size of VLA. To address this, we let the actor and critic share most of their parameters. Following the design in RL4VLA (Liu et al., 2025), we attach lightweight value head (a three-layer MLP) on top of the language model component of VLA to estimate state values efficiently. Specifically, the value head takes the hidden representation h0 from the final transformer block (Vaswani et al., 2023) at the position of the first action token and maps it to scalar value prediction. Value for Action Chunks As discussed in Section 4.1.1, two formulations of advantage estimation naturally lead to two corresponding formulations of value estimation for an action chunk ct = (at,1, at,2, . . . , at,C): 1. Chunk-level: The critic estimates the value of the entire chunk as macro-action. Formally, : R. 2. Action-level: Each atomic action at,j is assigned its own value estimate. Since only the initial observation ot is available at the beginning of the chunk, the critic outputs Cdimensional vector, providing one value prediction per step within the chunk. Formally, : RC. Our framework supports both formulations, controlled by the parameter value type. By default, the configuration sets value type to match reward type, ensuring consistency between reward aggregation and value estimation. We report results for both options using the OpenVLA-OFT model, and in general, the action-level formulation yields better performance. 4.2.2 GRPO Group Design The core of the GRPO algorithm lies in how groups are defined. In our framework, two trajectories are assigned to the same group if and only if they correspond to the same task and share the same initial state. This design mirrors the LLM setting, where multiple responses belong to the same group if and only if they are generated in response to the same prompt. Valid Action Mask During rollout, each environment runs for at most max episode steps steps. However, many tasks can be successfully completed earlier. This leads to two alternative strategies for policy optimization: (1) using the entire trajectory regardless of when success occurs, or (2) considering only the steps before task completion. Consistent with the Partial Reset mode discussed in Section 4.2.1, these correspond to two distinct optimization objectives, namely success at end and success once. Our codebase supports both objectives. We refer to the latter as the Valid Action Mask setting, which can be enabled by setting auto reset=False and ignore terminations=False. Figure 5c visualizes this option. In experiments, we observed that under the GRPO setting, the valid action mask objective is generally more effective for improving policy performance. detailed analysis is provided in the following section. Loss Normalization by Trajectory Length Unlike in LLM tasks, where longer responses are often more likely to be correct, successful trajectories in robotics are typically not longer than failed ones. To ensure that both successful and failed trajectories contribute equally during optimization, we normalize the policy loss by the trajectory length under the Valid Action Mask setting. Specifically, if trajectory τi has succ valid timesteps, the contribution of each timestep to the objective is scaled by 1/T succ . The GRPO objective under this setting is: GRPO(θ) = Eo0D,{τi}πθold (cid:34) 1 (cid:88) i= 1 succ t=1 succ i(cid:88) (cid:16) min ρi,t(θ) ˆAi, clip(ρi,t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:35) (cid:17) . (12) This prevents longer trajectories from disproportionately dominating the gradient and ensures balanced learning across trajectories of varying lengths. Success Rate Filter Inspired by the dynamic sampling strategy in DAPO (Yu et al., 2025b), we introduce success-rate filter for GRPO. This filter discards groups in which all trajectories either Technical Report @ RLinf Team succeed or fail3, since the advantage is non-zero only when both successful and failed trajectories coexist. Empirically, we find that this filtering mechanism accelerates convergence and consistently improves policy performance."
        },
        {
            "title": "5 EXPERIMENT RESULTS",
            "content": "In this section, we systematically investigate four key questions concerning the effectiveness of the proposed RLinf-VLA framework: (1) Is RLinf-VLA high-performance? We evaluate RLinf-VLA on two representative testbeds, LIBERO and ManiSkill. The results demonstrate that RLinf-VLA achieves success rate of 96.8% in large-scale mixed-task training involving up to 130 tasks, highlighting its strong capability to support large-scale multi-task learning. (2) Is RLinf-VLA high-efficiency? We benchmark the framework across both GPU-parallelized and CPU-parallelized simulators, and observe that the optimal configuration differs across simulator types. This finding underscores the necessity of supporting diverse allocation modes. Notably, RLinf-VLA achieves up to 1.88 speedup over existing frameworks. (3) What are the best practices for applying PPO and GRPO to VLA training? Through extensive ablation studies, we identify the key factors that govern training performance. Moreover, we provide insights into the selection of common hyperparameters, offering practical guidelines for effectively deploying RL in VLA settings. (4) Can models trained in simulation be deployed in the real world? To assess sim-to-real transfer, we conduct preliminary evaluations on six unseen object pick-and-place tasks. The results show that RL-trained models achieve higher zero-shot deployment success rates compared to SFT-trained models, demonstrating the promise of RL-based training for real-world applications. Evaluation. Unless otherwise specified, all curves are smoothed using Gaussian filter (σ = 1), and success rate is computed under the success once criterion, where an episode is considered successful if the success state is reached at least once. 5.1 HIGH-PERFORMANCE 5.1.1 MANISKILL Experiments Setup 1. Training tasks: PutOnPlateInScene25Main-v3. As previously mentioned, it is adopted in RL4VLA (Liu et al., 2025). 2. Evaluation tasks: Various out-of-distribution (OOD) test sets on vision, semantic, and position. Also adopted from RL4VLA. This part checks the capacity of VLA models on vision, language, and action, respectively. All models are evaluated under each sub-setting using 256 randomly episodes. 3. Hyperparameters: Please refer to YAML files in RLinf-VLA open-sourced code, including maniskill ppo openvla.yaml, maniskill ppo openvlaoft.yaml, maniskill grpo openvla.yaml, and maniskill grpo openvlaoft.yaml. 4. Base models: For the OpenVLA model, we adopt the pre-trained checkpoint available at HuggingFace4. For the OpenVLA-OFT model, we perform our own LoRA fine-tuning using motion planning data collected from the PutOnPlateInScene25Main-v3 task. The resulting LoRA model weights are also provided at HuggingFace5. Training Results Figure 6 reports the training performance on ManiSkill across 25 tasks, using the OpenVLA and OpenVLA-OFT models trained with PPO and GRPO. Figure 6a shows the results for 3Following the practice in VeRL (Sheng et al., 2025), we set both lower and upper bounds on the average return of all trajectories within group. 4https://huggingface.co/gen-robot/openvla-7b-rlvla-warmup 5https://huggingface.co/RLinf/RLinf-OpenVLAOFT-ManiSkill-Base-Lora Technical Report @ RLinf Team OpenVLA, while Figure 6b presents the OpenVLA-OFT results. Across all settings, reinforcement learning provides substantial performance gains, improving success rates by 45%70% compared to the baseline. Notably, PPO consistently outperforms GRPO and exhibits greater stability for both OpenVLA and OpenVLA-OFT in the 25-task ManiSkill setting. (a) OpenVLA (b) OpenVLA-OFT Figure 6: Training curves on ManiSkill PutOnPlateInScene25Mani-v3 with OpenVLA and OpenVLA-OFT models, using PPO and GRPO algorithms. PPO consistently outperforms GRPO and exhibits greater stability. Evaluation Results Table 2 summarizes the evaluation results of RLinf-VLA models. For OpenVLA, we include four variants: the base model (OpenVLA (Base)), the RL4VLA baseline trained with PPO (RL4VLA (PPO)), and our RLinf-VLA models trained with GRPO (OpenVLA (RLinfGRPO)) and PPO (OpenVLA (RLinf-PPO)). For OpenVLA-OFT, we report results on three variants: the LoRA fine-tuned base model (OpenVLA-OFT (Base)), along with the RLinf-VLA models trained using GRPO (OpenVLA-OFT (RLinf-GRPO)) and PPO (OpenVLA-OFT (RLinfPPO)). Although there is no fundamental algorithmic difference between the PPO-trained OpenVLA models in RLinf-VLA and in RL4VLA, our framework is carefully optimized to make better use of GPU resources. This enables training for more steps within feasible time budget, leading to slightly better performance in RLinf-VLA compared to RL4VLA. It is important to note that direct comparison between OpenVLA and OpenVLA-OFT on OOD evaluation is not entirely fair, since their base models differ substantially in performance. Specifically, while OpenVLA improves from 39.10% to 81.93% after RL fine-tuning and OpenVLA-OFT improves from 18.29% to 77.05%, the relative gain of OpenVLA-OFT is slightly higher despite its much weaker starting point. This indicates that the base model is very important and plays crucial role in the final generalization. Table 2: Evaluation results on ManiSkill. Values denote success rates. In-Distribution Vision Out-Of-Distribution Semantic Execution Avg. OpenVLA (Base) RL4VLA (PPO) OpenVLA (RLinf-GRPO) OpenVLA (RLinf-PPO) OpenVLA-OFT (Base) OpenVLA-OFT (RLinf-GRPO) OpenVLA-OFT (RLinf-PPO) 53.91% 93.75% 84.38% 96.09% 28.13% 94.14% 97.66% 39.10% 38.75% 35.94% 79.15% 80.47% 75.00% 75.15% 74.69% 72.99% 82.03% 78.35% 85.42% 81.93% 42.11% 81.77% 77.86% 27.73% 12.95% 84.69% 45.54% 92.11% 64.84% 11.72% 44.66% 73.57% 18.29% 60.64% 77.05% 5.1.2 LIBERO Experiments Setup 16 Technical Report @ RLinf Team 1. Tasks: We train single unified model on all 130 tasks in LIBERO and evaluate its performance across the five LIBERO task suites: LIBERO-Object, LIBERO-Spatial, LIBEROGoal, LIBERO-10, and LIBERO-90. 2. Hyperparameters: See the YAML configuration file in the RLinf-VLA open-source repository: libero 130 grpo openvlaoft.yaml. 3. Base models: We fine-tune OpenVLA-OFT with LoRA (r = 32) using demonstrations from all five LIBERO task suites: LIBERO-Object, LIBERO-Spatial, LIBERO-Goal, LIBERO-10, and LIBERO-90. The resulting LoRA model weights are also provided at HuggingFace 6. Training Results Figure 7 illustrates the training curve of OpenVLA-OFT with the GRPO algorithm across the five task groups. The x-axis represents the number of training epochs, while the y-axis indicates the corresponding success rate. The results show that the success rate improves substantially from approximately 73% to 98%, yielding an overall performance gain of about 30%. These findings highlight the effectiveness of the GRPO design in RLinf-VLA for enhancing multitask training. Figure 7: Training curve of OpenVLA-OFT with GRPO on LIBERO-130. Evaluation Results Table 3 summarizes the evaluation results of the OpenVLA-OFT model trained on LIBERO-130 using RLinf-VLA across the five LIBERO task groups. For each task group, the success rate is computed as the average over 50 evaluation episodes per task. For instance, in the LIBERO-90, the reported success rate is averaged over total of 90 50 = 4500 episodes. The results show that the LIBERO-Spatial and LIBERO-Object group reaches over 99%, the LIBERO-Goal and LIBERO-90 task groups achieve up to 98%, and the LIBERO-10 task group exceeds 93%, yielding an overall average success rate of 98.11%. This corresponds to an average performance improvement of 32.68% across task categories. Notably, our approach adopts single model for all tasks, in contrast to the existing RL+VLA paradigm that trains and evaluates separate model for each task group. This demonstrates RLinf-VLAs capability to support large-scale multi-task RL. Table 3: Evaluation results of the unified model on the five LIBERO task groups. Spatial Object Goal 10 90 Avg. OpenVLA-OFT (Base) OpenVLA-OFT (RLinf-GRPO) Improvement 72.18% 71.48% 64.06% 48.44% 70.97% 65.43% 99.40% 99.80% 98.79% 93.95% 98.59% 98.11% +27.22 +28.32 +34.73 +45.51 +27.62 +32. 5.2 HIGH-EFFICIENCY To evaluate the efficiency of our framework, we conduct end-to-end experiments comparing throughput under different GPU allocation modes. 6https://huggingface.co/RLinf/RLinf-OpenVLAOFT-LIBERO-130-Base-Lora 17 Technical Report @ RLinf Team"
        },
        {
            "title": "5.2.1 EXPERIMENT SETUP",
            "content": "Evaluation Metric. We use throughput as the evaluation metric for efficiency. Specifically, throughput is defined as the total number of rollout environment frames divided by the total wallclock time of one training epoch, which approximately equals the sum of rollout time and training time. The evaluation is conducted on 8, 16, and 32 NVIDIA H100 (80GB) GPUs, distributed across 1, 2, and 4 nodes, respectively. Tasks. We evaluate our framework on both ManiSkill and LIBERO setups. For ManiSkill, we select the PutCarrotOnPlateInScene-v2 task for quick test. It is adapted from PutCarrotOnPlateInScene-v1 in ManiSkill (Tao et al., 2025), with modifications to the reset function.7 We use 256 parallel environments, each running for 80 steps. For LIBERO, we adopt the LIBERO-10 task set for quick test. The number of parallel environments is set to 64, 128, and 256 for 8-, 16-, and 32-GPU setups, respectively, with the corresponding number of environment steps set to 4096, 2048, and 1024. Since vectorized environments in LIBERO rely on multi-processing, the number of physical CPU cores per node becomes the upper bound for efficient rollout. Therefore, we scale the number of parallel environments in proportion to the number of nodes, and consequently to the total number of GPUs. Baselines. For ManiSkill, no existing framework supports multi-GPU rollout and training. Thus, we take the naive disaggregated allocation mode as the baseline, and compare it with our colocated mode and hybrid modes, where the hybrid mode includes both one-stage and two-stage fine-grained pipelining configurations. For LIBERO, we compare against SimpleVLA-RL (Li et al., 2025), an open-source framework for VLA+RL training built on VeRL. Since SimpleVLA-RL only supports the colocated mode, we use it as the baseline and additionally evaluate our colocated mode and hybrid mode (with one-stage and two-stage fine-grained pipelining). We omit the disaggregated mode results for LIBERO, as during training the Training component never reuses the same GPUs as the rollout process. Consequently, even when accounting for the offloadonload overhead, the hybrid (one-stage) configuration provides superior performance to the disaggregated mode. GPU Allocation Modes. Tables 4 and 5 summarize the GPU allocation and offloading configurations used in our experiments. Table 4: The setting of enable offload among different GPU allocation modes. Simulator Generation Training Disaggregated Colocated Hybrid False True True False True True False True True 5.2.2 RESULTS Figure 8 presents the end-to-end throughput of RLinf and baseline methods in VLA+RL training under different cluster sizes and placement strategies. As shown in Figure 8a, for the GPU-parallelized ManiSkill simulator with OpenVLA, RLinf-VLA achieves 1.88 speedup compared to the baseline disaggregated mode when using the hybrid configuration with pipe=2 on 8 GPUs. When scaling to 16 and 32 GPUs, additional overhead arises from model loading, offloading, and state switching. Consequently, the colocated mode generalizes 7In the original reset implementation, additional simulation steps were applied to ensure that all objects remained static. However, this significantly increased the reset time compared to the step function and was incompatible with partial reset. In practice, we observed that these extra steps had negligible impact on object states in the PutCarrotOnPlateInScene task, since the initial position of the carrot was already carefully calibrated. We therefore delete the additional simulation steps in the reset function. Technical Report @ RLinf Team Table 5: Hyperparameters of GPU allocation modes. # GPU"
        },
        {
            "title": "Hybrid",
            "content": "0-1 0-7 0-3 0-3 0-15 0-7 0-7 0-31 0-15 2-3 0-7 4-7 4-7 0-15 8-15 8-15 0-31 16-31 4-7 0-7 0-7 8-15 0-15 0-15 16-31 0-31 0-31 (a) GPU-parallelized Simulator ManiSkill with OpenVLA (b) GPU-parallelized Simulator ManiSkill with OpenVLA-OFT (c) CPU-parallelized Simulator LIBERO with OpenVLA-OFT Figure 8: Throughput for different settings. pipe denotes the pipeline stage num for finegrained pipelining. less effectively than the disaggregated mode. However, the hybrid (pipe=2) configuration mitigates these costs through pipelined execution that overlaps computation and communication, still achieving 1.611.69 speedup over the disaggregated mode. Furthermore, comparing hybrid configurations with pipe=1 and pipe=2, we observe that deeper pipelines yield better performance, suggesting that fine-grained pipelining effectively reduces GPU idle time (bubbles) during the rollout phase. As shown in Figure 8b, the results for OpenVLA-OFT exhibit substantial differences. In this case, the colocated mode and the hybrid (pipe=1) mode outperform the disaggregated mode and the hybrid (pipe=2) mode. This discrepancy stems from the use of action chunks. Since OpenVLAOFT predicts an entire chunk of actions in each inference step while the simulator executes them sequentially, the simulators step execution time becomes much longer than the generation time (shifting from 1:1 ratio to approximately 15:1). Consequently, the benefit of pipelining to hide generation latency diminishes. Moreover, since ManiSkill is GPU-parallelized and the rollout speed scales linearly with the number of parallel environments, allocating more GPU resources to the simulator makes the colocated and hybrid (pipe=1) modes more favorable. Notably, when comparing the colocated mode and the hybrid (pipe=1) mode, we observe that the latter achieves progressively better throughput as the number of GPUs increases. This is because, in the colocated configuration, each GPU hosts both the simulator and the actor model simultaneously 8. In contrast, the hybrid configuration splits the cluster evenly, with each half of the GPUs dedicated to one component. As result, the communication overhead in the colocated mode becomes increasingly non-negligible as the number of GPUs grows. 8Note that in the standard colocated setting, both the simulator and the actor model fully occupy all GPUs, and context switching is handled via onload and offload. However, since ManiSkill is GPU-parallelized, this approach incurs excessive overhead during multi-step interactions. Therefore, in our implementation, each GPU allocates half of its memory to each component. Technical Report @ RLinf Team Figure 9: Latency breakdown for LIBERO with the OpenVLA-OFT setting. For the CPU-parallelized LIBERO simulator with OpenVLA-OFT9, the trend differs once again. As shown in Figure 8c, the colocated mode achieves the best performance, yielding 1.342.27 speedup over the SimpleVLA-RL implementation. We further analyze the source of this speedup in Figure 9, where Rollout represents the total time for multi-step interactions between the generator and the simulator. Performance improvements are observed in both the rollout and training stages. Specifically, during rollout, RLinf-VLA employs vectorized environment implementation that is more efficient than the hand-written multiprocess environment workers used in SimpleVLA-RL. In addition, our implementation avoids redundant log-probability recomputation present in SimpleVLA-RL, further improving the rollout performance. During the training stage, RLinf adopts several system-level optimizations, including an adaptive communication mechanism, which leads to greater efficiency gains as the number of GPUs increases. Compared with the colocated mode, the hybrid mode incurs longer rollout times since LIBERO is CPU-parallelized simulator. In this case, computation overlap provides little benefit, and allocating more compute resources to generation yields higher returns. In summary, different types of simulators require distinct allocation modes, and varying interaction characteristics further necessitate adaptive configurations. RLinf-VLA supports three flexible allocation modes, enabling optimal system configurations under diverse scenarios. 5.3 ABLATION STUDY In this section, we present ablation studies under different setups and summarize key insights for PPO and GRPO training. In Section 5.3.1, we highlight PPO-specific strategies, such as step-level value estimation and partial reset. In Section 5.3.2, we discuss GRPO techniques, including trajectory length normalization, valid action masking, and success filtering. Finally, in Section 5.3.3, we analyze the effects of rollout data size and LoRA-based adaptations. 5.3.1 TIPS FOR PPO Action-level value estimation outperforms chunk-level estimation for PPO with action chunks. Figure 10 shows the PPO training curves comparing action-level and chunk-level value estimation as described in Section 4.2.1. The x-axis denotes the number of training epochs, while the y-axis reports the success rate (Figure 10a) and value loss (Figure 10b) on the ManiSkill PutOnPlateInScene25Main task using the OpenVLA-OFT model. Step-level estimation consistently yields higher success rates and lower value loss, demonstrating more effective learning and faster policy improvement. This advantage is not task-specific: similar results are observed in the LIBERO-Goal benchmark (Figure 10c). 9Previous results have shown that the disaggregated mode consistently underperforms the hybrid mode, as the training process in the disaggregated setup cannot utilize all GPUs. Therefore, the disaggregated mode is excluded from comparison here. 20 Technical Report @ RLinf Team (a) Success rate in ManiSkill. (b) Value loss in ManiSkill. (c) Success rate in LIBERO-Goal Figure 10: Ablation of value estimation type on PPO with action chunks. (a) OpenVLA (b) OpenVLA-OFT Figure 11: Ablation study on partial reset with PPO in ManiSkill. Partial reset substantially improves sample efficiency. Figure 11 presents the PPO training curves for the Partial Reset and Fixed Episode Length rollout modes discussed in Section 4.2.1. The x-axis indicates the number of training epochs, and the y-axis shows the success rate. Since the optimization objective is success once, Partial Reset leads to significantly higher success rate. For given number of training epochs, the success rate under Partial Reset consistently exceeds that of the Fixed Episode Length mode. This trend is observed regardless of the model type (OpenVLA or OpenVLA-OFT). 5.3.2 TIPS FOR GRPO Trajectory length normalization in GRPO. As discussed in Section 4.2.2, normalizing the loss by trajectory length aims to reduce bias when episodes vary in length. Figure 12 reports results on LIBERO-Goal, where the x-axis denotes the number of training epochs and the y-axis the success rate. The results show that incorporating trajectory length normalization (w/ Norm) can lead to substantially higher performance compared with the unnormalized setting (w/o Norm). Figure 12: Ablation study on trajectory length normalization. Valid action mask in GRPO. We also study the effect of valid action masking, introduced in Section 4.2.2. Since the optimization objective is defined with respect to success once, excluding actions after reaching the success state improves sample efficiency and avoids redundant updates. Moreover, applying the valid action mask naturally results in shorter trajectories, which further benefits trajectory length normalization. Figure 13a shows results on LIBERO-Goal. Comparing the w/o Mask, w/o Norm and w/ Mask, w/o Norm curves, the setting with valid action mask achieves consistently better performance. The w/ Mask, w/ Norm curve demonstrates that combining the mask with trajectory length normalization provides an additional improvement. However, the effect of these techniques is task-dependent. In the ManiSkill setting, we do not observe clear benefits from either valid action masking or trajectory length normalization. As shown in Figure 13b, the performance of the w/ Mask, w/ Norm configuration is comparable to that of 21 Technical Report @ RLinf Team (a) LIBERO-Goal (b) ManiSkill Figure 13: Ablation studies on valid action mask in GRPO. the w/o Mask, w/o Norm baseline, suggesting that the effectiveness of these strategies may vary across environments. (a) OpenVLA, ManiSkill (b) OpenVLA-OFT, ManiSkill (c) OpenVLA-OFT, LIBERO-Goal Figure 14: Ablation studies on success rate filtering under different settings. Success rate filtering can improve training stability in some settings of GRPO. As discussed in Section 4.2.2, we implement success rate filter for GRPO that discards groups in which all trajectories have identical cumulative rewards. This mechanism can improve the stability of GRPO training. For instance, in the OpenVLA ManiSkill setting, Figure 14a shows that training without the filter (w/o Filter) exhibits clear collapse around step 400, whereas enabling the filter (w/ Filter) largely alleviates this issue. However, the benefit of the filter is not universal: in the OpenVLAOFT ManiSkill setting (Figure 14b) and the OpenVLA-OFT LIBERO-Goal setting (Figure 14c), its effectiveness is much less pronounced. 5.3.3 HYPERPARAMETERS CHOICE Effect of rollout data size. The rollout batch size has notable influence on RL performance, especially for on-policy algorithms such as PPO and GRPO. In general, larger rollouts per epoch enable more substantial policy improvement within each training iteration. As illustrated in Figure 15, larger rollouts consistently achieve higher success rates when evaluated by training epochs. Using LoRA may not directly affect performance but often requires different hyperparameters. Figure 16 shows the training curves with and without LoRA. The x-axis denotes the number of training epochs, while the y-axis shows the success rate in ManiSkill. The curves are overall similar, suggesting that LoRA itself does not substantially change performance. However, the choice of whether to use LoRA can influence the optimal hyperparameters. For example, in GRPO experiments (Figure 16b), using the same learning rate of 1 104 leads to normal improvement with LoRA, but the success rate without LoRA collapses to zero. In contrast, when the learning rate is reduced to 1 105, the non-LoRA setting also achieves stable improvement. These results indicate that different LoRA configurations may require separate hyperparameter tuning. 22 Technical Report @ RLinf Team (a) PPO, LIBERO- (b) GRPO, LIBERO-130 Figure 15: Ablation study on rollout data size. Darker colors correspond to larger rollout datasets. Trajs denotes the number of trajectories included in the rollout data per iteration. (a) PPO (b) GRPO Figure 16: Ablation of LoRA in ManiSKill with OpenVLA-OFT model. Technical Report @ RLinf Team"
        },
        {
            "title": "5.4 REAL-WORLD DEPLOYMENT",
            "content": "We further evaluate the transferability of simulation-trained models by deploying them to the real world. Experimental Setup The hardware setup consists of Franka Panda robotic arm and an Intel RealSense D435 camera. The relative pose between the camera and the robot is calibrated to ensure consistency with the simulation environment. For real-world evaluation, we compare the OpenVLA (RLinf-PPO) model with OpenVLA (SFT). For OpenVLA (SFT), we train OpenVLA to converge on datasets ranging from few hundred to 64k expert trajectories (1.26M transitions) in simulation. We observe that performance plateaus at roughly 16k trajectories, so we adopt the 16k-trajectory SFT checkpoint as the baseline for comparing RL fine-tuning methods. The evaluation involves six previously unseen objects (pepper, cucumber, banana, kiwi fruit, mangosteen, and sponge), each tasked with being placed into bowl. For every object, five trials are conducted under different initial configurations of both the object and the bowl. Table 6: Real-world deployment results. Pick indicates that the policy successfully grasped the object, while Success indicates that the object was both picked and placed into the bowl. Object Type Pepper Cucumber Banana Kiwi Mangosteen Sponge Total OpenVLA (SFT) OpenVLA (RLinf-PPO) Pick Pick Success Success 1/5 1/5 0/5 0/5 1/5 0/5 3/30 0/5 0/5 0/5 0/5 0/5 0/5 0/30 2/5 1/5 3/5 2/5 1/5 4/5 13/30 1/5 1/5 3/5 1/5 1/5 1/5 8/ Figure 17: Example of real-world deployment with an RL-trained model performing the task of placing banana into bowl. Results Table 6 reports the average performance in real-world trials. OpenVLA (RLinf-PPO) substantially outperforms OpenVLA (SFT). Figure 17 illustrates representative example, where the model successfully places banana into bowl. Notably, beyond basic calibration between simulation and the real world, no additional sim-to-real adaptation techniques were employed. Despite the preliminary nature of the experiment, the RL-trained policy demonstrates zero-shot transfer to real-world tasks, revealing the strong generalization capability enabled by simulation-based reinforcement learning. We observe that the SFT policy tends to exhibit excessive motion and overshooting, often resulting in inaccurate grasp poses and task failures. In contrast, the RL-trained policy, though showing minor oscillations, iteratively refines the end-effector pose and achieves higher grasp success rate."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced RLinf-VLA, unified and efficient framework for reinforcement learning-based training of Vision-Language-Action models. RLinf-VLA integrates multiple simulators, algorithms, and VLA architectures, while providing flexible execution modes and system-level optimizations that significantly improve training efficiency. Extensive experiments demonstrate that 24 Technical Report @ RLinf Team single unified model achieves state-of-the-art success across wide range of simulated tasks, and RL-trained policies exhibit stronger zero-shot generalization on real-world Franka robot compared to supervised fine-tuning. Moreover, our study distills actionable best practices for both PPO and GRPO, guiding future research in RL-based VLA training. By open-sourcing RLinf-VLA with ongoing maintenance, we provide the community with foundation to accelerate, standardize, and scale research in embodied intelligence."
        },
        {
            "title": "7 FUTURE WORK",
            "content": "Our long-term goal is to advance the capabilities of VLA models through reinforcement learning. Achieving this requires reliable and extensible infrastructure, which we will continue to actively maintain and expand. Below, we outline several directions for future development. We plan to broaden simulator support. Integration with RoboTwin (Mu et al., 2025) and IsaacLab (Developers) is currently in progress and will be released soon, enabling researchers to explore wider range of embodied environments. Second, we will extend the set of supported models. In particular, integrations with π0 (Black et al., 2024) and π0.5 (Intelligence et al., 2025) will be available in the near future, along with algorithmic tuning to ensure strong performance out of the box. Third, we aim to enrich the algorithmic coverage of the framework. Beyond the current onpolicy methods, we are working to incorporate classical off-policy algorithms such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018), which are known for their sample efficiency and stability. Finally, while our current focus has been on simulation-based RL, we will also integrate real-world reinforcement learning into the codebase. This step is critical for bridging the gap between simulation and real-world deployment, and will further highlight the practicality of our framework. In the longer term, we envision this framework as foundation for establishing standardized benchmarks and fostering community collaboration, thereby accelerating progress in the development and deployment of VLA models. 25 Technical Report @ RLinf Team"
        },
        {
            "title": "REFERENCES",
            "content": "Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Data quality in imitation learning. Advances in neural information processing systems, 36:8037580395, 2023. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: visionlanguage-action flow model for general robot control, 2024. URL https://arxiv.org/ abs/2410.24164. Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. Advances in neural information processing systems, 32, 2019. The Isaac Lab Project Developers. Isaac lab. URL https://github.com/isaac-sim/ IsaacLab. Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, 44(5): 701739, 2025. Dylan Foster, Adam Block, and Dipendra Misra. Is behavior cloning all you need? understanding horizon in imitation learning. Advances in Neural Information Processing Systems, 37:120602 120666, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. CoRR, abs/1801.01290, 2018. URL http://arxiv.org/abs/1801.01290. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021. URL https://arxiv.org/abs/2106.09685. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: vision-language-action model with open-world generalization, 2025. URL https://arxiv. org/abs/2504.16054. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty ElarXiv preprint lis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv:2403.12945, 2024. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, et al. Simplevla-rl: Scaling vla training via reinforcement learning. arXiv preprint arXiv:2509.09674, 2025. 26 Technical Report @ RLinf Team Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023a. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning, 2023b. URL https://arxiv. org/abs/2306.03310. Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. What can rl bring to vla generalization? an empirical study, 2025. URL https://arxiv. org/abs/2505.19789. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. survey on visionlanguage-action models for embodied ai. arXiv preprint arXiv:2405.14093, 2024. Yao Mu, Tianxing Chen, Zanxin Chen, Shijia Peng, Zhiqian Lan, Zeyu Gao, Zhixuan Liang, Qiaojun Yu, Yude Zou, Mingkun Xu, Lunkai Lin, Zhiqiang Xie, Mingyu Ding, and Ping Luo. Robotwin: Dual-arm robot benchmark with generative digital twins. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 2764927660, June 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Joao Silverio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier 27 Technical Report @ RLinf Team Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto MartınMartın, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, and Zipeng Lin. Open xembodiment: Robotic learning datasets and rt-x models : Open x-embodiment collaboration0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903, 2024. doi: 10.1109/ICRA57147.2024.10611477. Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 661668. JMLR Workshop and Conference Proceedings, 2010. John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438, 2015. URL https://api.semanticscholar.org/CorpusID:3075448. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Dhruv Shah, Błazej Osinski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pretrained models of language, vision, and action. In Conference on robot learning, pp. 492504. PMLR, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/ March 2025. 3689031.3696075. R.S. Sutton and A.G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks, 9(5):10541054, 1998. doi: 10.1109/TNN.1998.712192. Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Viswesh Nagaswamy Rajesh, Yong Woo Choi, Yen-Ru Chen, Zhiao Huang, Roberto Calandra, Rui Chen, Shan Luo, and Hao Su. Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. Robotics: Science and Systems, 2025. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv. org/abs/1706.03762. 28 Technical Report @ RLinf Team Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025. Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, and Yu Wang. Rlinf: Flexible and efficient largescale reinforcement learning via macro-to-micro flow transformation, 2025a. URL https: //arxiv.org/abs/2509.15965. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025b. URL https: //arxiv.org/abs/2503.14476. Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware, 2023. URL https://arxiv.org/abs/2304.13705."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "Infinigence AI",
        "Institute of Automation, Chinese Academy of Sciences",
        "Peking University",
        "Tsinghua University",
        "UC Berkeley",
        "Zhongguancun Academy"
    ]
}