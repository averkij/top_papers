{
    "paper_title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines",
    "authors": [
        "Genta Indra Winata",
        "Frederikus Hudi",
        "Patrick Amadeus Irawan",
        "David Anugraha",
        "Rifki Afina Putri",
        "Yutong Wang",
        "Adam Nohejl",
        "Ubaidillah Ariq Prathama",
        "Nedjma Ousidhoum",
        "Afifa Amriani",
        "Anar Rzayev",
        "Anirban Das",
        "Ashmari Pramodya",
        "Aulia Adila",
        "Bryan Wilie",
        "Candy Olivia Mawalim",
        "Ching Lam Cheng",
        "Daud Abolade",
        "Emmanuele Chersoni",
        "Enrico Santus",
        "Fariz Ikhwantri",
        "Garry Kuwanto",
        "Hanyang Zhao",
        "Haryo Akbarianto Wibowo",
        "Holy Lovenia",
        "Jan Christian Blaise Cruz",
        "Jan Wira Gotama Putra",
        "Junho Myung",
        "Lucky Susanto",
        "Maria Angelica Riera Machin",
        "Marina Zhukova",
        "Michael Anugraha",
        "Muhammad Farid Adilazuarda",
        "Natasha Santosa",
        "Peerat Limkonchotiwat",
        "Raj Dabre",
        "Rio Alexander Audino",
        "Samuel Cahyawijaya",
        "Shi-Xiong Zhang",
        "Stephanie Yulia Salim",
        "Yi Zhou",
        "Yinxuan Gui",
        "David Ifeoluwa Adelani",
        "En-Shiun Annie Lee",
        "Shogo Okada",
        "Ayu Purwarianti",
        "Alham Fikri Aji",
        "Taro Watanabe",
        "Derry Tanti Wijaya",
        "Alice Oh",
        "Chong-Wah Ngo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data."
        },
        {
            "title": "Start",
            "content": "WORLDCUISINES: Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines Genta Indra Winata,1,2, Frederikus Hudi,2,3, Patrick Amadeus Irawan,2,4, David Anugraha,5, Rifki Afina Putri,2,6, Yutong Wang,7, Adam Nohejl,3, Ubaidillah Ariq Prathama,4, Nedjma Ousidhoum,8, Afifa Amriani9, Anar Rzayev6, Anirban Das1, Ashmari Pramodya3, Aulia Adila7, Bryan Wilie10, Candy Olivia Mawalim7, Ching Lam Cheng11, Daud Abolade12,13, Emmanuele Chersoni14, E. Santus9, Fariz Ikhwantri9, Garry Kuwanto15, Hanyang Zhao16, Haryo Akbarianto Wibowo17, Holy Lovenia2, Jan Christian Blaise Cruz2,17, Jan Wira Gotama Putra9, Junho Myung6, Lucky Susanto18, Maria Angelica Riera Machin3, Marina Zhukova19, Michael Anugraha9, Muhammad Farid Adilazuarda2,17, Natasha Santosa20, Peerat Limkonchotiwat2,21, Raj Dabre22, Rio Alexander Audino4, Samuel Cahyawijaya2,23, Shi-Xiong Zhang1, Stephanie Yulia Salim7, Yi Zhou8, Yinxuan Gui11, David Ifeoluwa Adelani,12,24,25, En-Shiun Annie Lee,5,26, Shogo Okada,7, Ayu Purwarianti,2,4, Alham Fikri Aji,2,17,18, Taro Watanabe,3, Derry Tanti Wijaya,15,18, Alice Oh,6, Chong-Wah Ngo,11, 1Capital One 2SEACrowd 3NAIST 4ITB 5UofT 6KAIST 7JAIST 8Cardiff University 9Independent 10HKUST 11SMU 12Masakhane 13University of Lagos 14HK PolyU 15Boston University 16Columbia University 17MBZUAI 18Monash University 19UCSB 20Tokyo Tech 21AI Singapore 22NICT 23Cohere 24McGill 25MILA 26Ontario Tech Main Authors Senior Authors"
        },
        {
            "title": "Abstract",
            "content": "Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WORLDCUISINES, massivescale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release knowledge base with annotated food entries and images along with the VQA data. These authors contributed equally. This is an opensource project, and the work was done outside of their affiliations. Contacts: genta.winata@capitalone.com and frederikus.hudi.fe7@is.naist.jp. Figure 1: Images of stuffed pasta and dumplings from our dataset showcase similar culinary concept across different cultures: wrapping meat, dairy (such as cheese), or vegetables in dough. These dishes can be prepared in various ways, including pan-frying, deepfrying, steaming, or boiling."
        },
        {
            "title": "Introduction",
            "content": "Food is an essential medium for the exchange of regional cultures, serving to connect diverse peoples and traditions (Wahlqvist, 2007). Analyzing various culinary practices provides valuable insights into the cultural values, historical narratives, and social customs of the communities that produce and consume these foods (Holtzman, 2006). Furthermore, food plays significant role in shaping lan4 2 0 2 7 2 ] . [ 2 5 0 7 2 1 . 0 1 4 2 : r # VQA # Lang./Dialect # Countries # Food Entries # Images Parallel Data License FoodieQA (Li et al., 2024b) World Wide Dishes (Magomere et al., 2024) xGQA (Pfeiffer et al., 2022) MaXM (Changpinyo et al., 2023) EVJVQA (Nguyen et al., 2023) CulturalVQA (Nayak et al., 2024) SEA-VQA (Urailertprasert et al., 2024) CVQA (Romero et al., 2024) IndiFoodVQA (Agarwal et al., 2024) 659 765 12,578 2,142 33,790 2,378 1,999 9,000 16, WC-VQA 1,152,000 2 131 8 7 3 1 1 26 1 30 1 63 8 7 1 11 8 28 1 60 765 N/A N/A N/A N/A N/A 1,834 255 2,414 389 301 398 335 4,909 2,328 515 4,560 414 6,045 CC BY-NC-ND 4.0 CC-BY 4.0 CC-BY 4.0 Custom N/A N/A Custom Various N/A CC BY-SA 4.0 Table 1: Data statistics for WC-VQA compared to existing VQA datasets. The data samples are sourced from their respective publications. The reported numbers are based on their human-annotated test set. This entry includes the language variations we collected for all languages. guage, which serves as proxy for cultural knowledge (Freedman, 2021). Food choices often reflect intricate community histories, societal transformations, and both individual and collective identities, thereby creating rich tapestry of cultural expression (Almerico, 2014). The relationship between culture and food is dynamic; both evolve in tandem over time, resulting in the emergence of new dishes that are influenced by historical culinary traditions (Anderson, 2014). As result, similar food concepts can be found across different countries, reflecting shared human culinary experience. Researchers use food as proxy to model and analyze cultural dynamics, helping to quantify cultural differences across regions (Adilazuarda et al., 2024). Many cultures have developed their own versions of stuffed pasta or dumplings, each with unique ingredients and preparation methods, often known by different names (Gallani, 2015), as illustrated in Figure 1. Small details like how the dumpling is shaped can signal the cultural background. Conversely, some dishes share the same name but have different meanings; for example, jelly in the U.S. refers to fruit spread, while in the U.K. and parts of Asia, it refers to gelatinous dessert (Poppe, 1992; Abe, 2013). This culinary diversity presents challenge for Vision Language Models (VLMs), which must accurately recognize and differentiate food items based on cultural context for applications like food recognition. These models navigate the complexities of names, ingredients, and preparation styles that vary widely across regions. VLMs have shown effectiveness in text captioning (Liu et al., 2024b,c) and have been adapted to support multiple languages (Geigle et al., 2023; Shin et al., 2024). However, there is limited research on evaluating the multicultural capabilities of VLMs, particularly in terms of multilinguality. The study by Romero et al. (2024) introduce visual question answering (VQA) from multicultural perspective, but it mainly focuses on knowledge and situational context at specific moment, which does not fully assess the ability of VLMs to reason and differentiate between cultures within single question. Moreover, another study on food VQA is limited to Chinese culture and does not explore the broader spectrum of global cultures (Li et al., 2024b). An earlier investigation into cultural bias in language models also found that cultural knowledge is lacking (Naous et al., 2023). Therefore, further research is necessary to address these limitations and enhance our understanding of VLMs multicultural and multilingual capabilities. To facilitate comprehensive analysis of multilingual and multicultural research, we develop resources for evaluating VLMs. Table 1 summarizes how our work compares to previous studies. Our benchmark stands out for its cultural diversity, offering more VQA datasets and broader language and dialect coverage. Our major contributions can be summarized in three-fold: We present WORLDCUISINES, the first massive scale benchmark consisting of 1 million high-quality multilingual and multicultural text-image pairs annotated by native speakers in their local languages. We publicly release our resources, i.e. datasets,1 code,2 and leaderboard3 to support and advance future research in this rapidly evolving field. We evaluate open-source and commercial 1We release WC-VQA at https://huggingface.co/ datasets/worldcuisines/vqa and WC-KB consisting food, location, cuisine, and prompt templates at https:// huggingface.co/worldcuisines. release our worldcuisines/worldcuisines. at https://github.com/ 2We code 3We release our leaderboard at https://huggingface. co/spaces/worldcuisines/worldcuisines. Figure 2: WC-VQA in WORLDCUISINES comprises two primary tasks: (1) predicting dish names and (2) predicting regional cuisines. Task 1 is further divided into three subtasks: (a) no-context, (b) contextualized, and (c) adversarial. We also include two answer types: multiple-choice question (MCQ) and open-ended question (OEQ). VLMs for cultural awareness through two VQA tasks: predicting dish names from images and context, and identifying their geographical origin. We also assess the impact of context, including adversarial scenarios. We create multilingual templates for queries and context (such as the questions in QA pairs) while preserving language varieties, including dialects and registers. This is achieved by creating translations that incorporate different inflections, articles, and contractions. Our goal is to ensure naturalness in each translation and to use appropriate inflections for place names."
        },
        {
            "title": "2 WORLDCUISINES",
            "content": "We propose WORLDCUISINES, an open-source benchmark designed to evaluate the cultural relevance and understanding of VLMs. Figure 2 displays VQA examples in English, alongside selected parallel translations in Japanese and French."
        },
        {
            "title": "2.1 Overview",
            "content": "We develop both VQA dataset (WC-VQA) and curated KB for world cuisines (WC-KB). The WC-VQA dataset is constructed using WC-KB, which serves as the primary data source. We design two tasks as follows: Task 1: Dish Name Prediction. This task involves predicting the name of dish based on its image, question, and contextual information. It comprises three subtasks, each with distinct query types: (a) no-context question, (b) contextualized question, and (c) adversarial contextualized question. Task 2: Location prediction. The task is to predict location where the food is commonly consumed and originated given the dish image, question, and context. WC-KB. KB encompassing 2,414 dishes worldwide includes 6,045 images and metadata, Figure 3: WORLDCUISINES distribution of food entries by country in the World Map. The food entries are distributed across 189 countries, with the highest concentration found in Asia, Europe, and North America. There are also some entries from the continents of Africa, Oceania, and Central and South America. Figure 4: Countries by number of assigned dishes, showing the top 50 countries. covering both coarse-grained (e.g., stew) and finegrained categories (e.g., beef stew), locations, and regional cuisines. It also features multilingual translations of 90 crowd-sourced prompt templates and 401 parallel data entries (i.e., multilingual information) for location and regional cuisine information. tion, (3) quality assurance, and (4) data compilation. Figure 3 provides statistics on the regions covered in our dataset, with detailed information available in Table 9 in the Appendix. Figure 4 shows the distribution of dish frequencies, highlighting the top 50 countries with the most dishes. WC-VQA. multilingual parallel VQA dataset with 1 million samples encompassing over 30 languages and dialects, including various varieties and registers, such as formal and casual styles, with high-quality human annotations. The VQA is designed to evaluate models ability to understand cultural food names and their origins."
        },
        {
            "title": "2.2.1 Dish Selection\nWe compile a comprehensive list of dish names\nsourced from Wikipedia. We manually review\npages that feature lists of dishes to determine\nwhether each dish is a specialty unique to a spe-\ncific culture, as we aim to focus on dishes that have\ndistinct cultural significance. We exclude generic\ncategories, such as ice cream, which lacks a spe-\ncific cultural association. We ensure that each dish\non our list has its own dedicated Wikipedia page.\nIf a dish does not have a Wikipedia page, it is also\nexcluded from our compilation. This meticulous\napproach ensures that our dataset is both culturally\nrelevant and well-documented.",
            "content": "4Wikipedia web pages can be accessed at https:// wikipedia.org. 5Wikimedia Commons web pages can be accessed at https://commons.wikimedia.org."
        },
        {
            "title": "Data\nSplit",
            "content": "(a) no-context # VQA # Images Task 1 (Dish Name) (b) contextualized # VQA # Images (c) adversarial # VQA # Images Task 2 (Location) # VQA # Images Total # VQA Train (1M) Test Small (12k) Test Large (60k) 270,300 3,000 15,000 3,383 100 500 267,930 3,000 15,000 3,555 100 500 271,770 3,000 15,000 3,589 100 270,000 3,000 15,000 3,361 100 499 1,080,000 12,000 60,000 Table 2: Dataset statistics for WC-VQA tasks for train, test small, and test large data splits. Total #VQA represents the total number of VQA from Task 1 and Task 2. pile metadata based on the provided information. This metadata includes: Visual Representation: Images sources from Wikimedia Commons are included, along with their license information. Categorization: Dishes are classified into both coarse-grained (e.g., rice, bread) and finegrained (e.g., fried rice, flatbread) categories. Description: Annotators provide description of each dish based on the content from its Wikipedia page, avoiding the use of the dishs name, origin, or any distinctive keywords that uniquely identify the dish. Cuisine: The dishs origin cuisine and any cuisines with which it is strongly associated. Geographic Distribution: This includes the dishs associated countries, area (city or region), and broader continental region. The metadata description, along with the example, is further elaborated in the Appendix Table 4."
        },
        {
            "title": "2.2.3 Quality Assurance",
            "content": "Before beginning the quality assurance process, we first identify common issues that arise during the annotation and develop automated rules to detect easily identifiable annotation errors, such as incorrect string formatting. Annotators are then asked to correct these errors. To further ensure data quality and validity, we conduct several rounds of quality assurance. Initially, we focus on image quality by removing instances where images are blurry, dark, or contain distracting elements such as people or other dishes. We also verify image licenses by cross-referencing them with information on Wikimedia Commons. Next, we refine dish categorization and descriptions, ensuring consistency in category assignments and maintaining descriptions free from information breaches (e.g., excluding regional details from the description). We standardize cuisine names and eliminate any redundancies. Finally, we meticulously review all country and area information to ensure its accuracy. This comprehensive approach guarantees the integrity and reliability of our dataset."
        },
        {
            "title": "2.3 VQA Generation",
            "content": "In this phase, we generate VQA data by sampling from WC-KB. An entry of VQA data comprises visual image, question text, and answer text. This process involves four stages: (1) conducting similarity search for dish names, (2) constructing questions and contexts, (3) translating these elements into multiple languages, and (4) generating the VQA triplets."
        },
        {
            "title": "Model",
            "content": "Open-Source (a) no-context MCQ OEQ Task 1 (Dish Name) (b) contextualized MCQ OEQ (c) adversarial MCQ OEQ Task 2 (Location) MCQ OEQ MCQ OEQ"
        },
        {
            "title": "Average",
            "content": "Llava1.6 Vicuna 7B Llava1.6 Vicuna 13B Qwen2 VL Instruct 2B Qwen2 VL Instruct 7B Qwen2 VL Instruct 72B Llama 3.2 Instruct 11B Llama 3.2 Instruct 90B Molmo-E 1B Molmo-D 7B Molmo-O 7B Pangea 7B Aria 25B Phi-3.5 Vision 4B Pixtral 12B NVLM-D 72B 34.57 40.17 41.65 61.48 74.19 59.93 77.69 18.81 46.01 39.96 52.35 58.61 43.37 56.65 69.82 1.59 2.79 7.98 6.76 12.67 18.75 16.93 0.01 2.89 5.15 1.52 4.99 2.91 1.22 4.71 43.48 48.17 42.29 67.85 80.79 64.12 82.92 24.22 55.95 44.93 63.07 69.29 48.71 70.69 78.93 4.03 5.85 8.13 10.36 21.31 22.96 23.60 0.23 3.66 6.03 2.73 9.17 4.23 2.94 10.29 34.84 39.05 39.69 53.52 62.43 53.17 63.96 19.55 41.61 38.41 49.17 52.82 40.87 52.12 52. 1.41 2.57 6.74 6.12 8.37 13.39 10.87 0.01 2.31 3.51 1.57 3.39 2.07 1.09 2.89 32.24 37.79 47.85 55.90 61.90 57.93 67.87 18.97 33.35 29.81 48.71 42.82 35.01 46.67 51.97 9.29 10.16 14.55 21.03 27.27 31.58 31.31 1.54 11.45 10.07 20.15 16.20 9.22 14.43 16.68 36.28 41.30 42.87 59.69 69.83 58.79 73.11 20.39 44.23 38.28 53.33 55.89 41.99 56.53 63.21 4.08 5.34 9.35 11.07 17.40 21.67 20.68 0.45 5.08 6.19 6.49 8.44 4.61 4.92 8.64 Proprietary GPT-4o GPT-4o Mini Gemini 1.5 Flash 88.45 72.80 77.05 21.88 10.28 12.81 91.57 81.65 80.97 37.51 20.87 15.16 82.29 57.76 69.13 14.79 5.72 6. 66.52 52.37 71.53 37.13 25.79 30.03 82.21 66.14 74.67 27.83 15.66 16.12 Table 3: Accuracy (%) results of WC-VQA for Test Large (60k). MCQ and OEQ indicate multiple-choice question and open-ended question, respectively. Best and second-best are bolded and underlined, respectively. We employ an optimized prompt provided by the authors (see Subsection D.1 in the Appendix for further details). (1b) contextualized question where we provide additional information related to cuisine or location; and (1c) adversarial contextualized question which are similar to the contextualized questions but may include misleading location information to assess the models robustness to irrelevant details. Only basic question without any provided context is available for regional cuisine prediction (Task 2). The data statistics for each task are presented in Table 2."
        },
        {
            "title": "2.3.3 Multiple Language Translation\nQuestion and Context. All questions and con-\ntexts are initially collected in English, which are\nthen carefully translated by native human speakers\ninto 30 language varieties: 23 different languages\nwith 7 languages having two different varieties\neach. We instructed the translators to prioritise\nthe naturalness, and then followed by the diversity\nof translations when the duplication occurs.",
            "content": "Food Name Alias. Using Wikipedia pages as our primary source, we can verify if the English page has translations available in other languages. This enables us to extract dish names in multiple languages and compile them as translations for each dish. We utilize both the Wikipedia page titles in various languages and the alias text found on the English page. These translations are especially valuable for multilingual prompt translation, as they allow us to use the dishs native name instead of its English equivalent, enhancing cultural relevance and accuracy. We use the English name as default when the translation is unavailable. Locations and Cuisines. As there are more than 400 unique locations, including countries, cities, and areas, we first translate the English locations into other languages by using GPT-4o, followed by proofreading each translation by the native speakers. The string values for the regional cuisines, i.e., the adjective form of the location in English, are translated in the same manner as location. Morphological Inflections. Indo-European languages, such as Czech or Spanish, are rich in inflectional morphology which involves word modification to express different grammatical categories, such as number, gender, or case. For example, the equivalents of the phrases in Japan and from Japan in Czech are Japonsku and Japonska, respectively. We provide framework for the human translators to use the inflections in the prompt template to prioritize the naturalness while keeping the inflections as few as possible. (a) Multiple-choice question (MCQ). Figure 5: Accuracy (%) categorized by language (left), language vitality (center), and language family (right). We classify the language vitality by following the classification from Joshi et al. (2020). (b) Open-ended question (OEQ)."
        },
        {
            "title": "2.3.4 Generating VQA Triplets",
            "content": "To ensure no overlap in train and test subsets, we split the dishes and the multilingual-questions into two subsets each, to ensure no dish or multilingual questions leakage between train and test. For every subset, we apply random sampling to get pair of dish and its multilingual-questions. We use the dish entry in our KB dataset to pick the image and the location to be injected to the context, if any. The answer candidates for multiple-choice were picked by utilizing similarity search (Section 2.3.1). We repeat this process until we reach the desired number of training or test samples, or until all possible dish and question combinations are used, discarding any duplicates."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Metrics. We use accuracy as the primary metric to evaluate predictions. For Task 2 (openended), we employ BERTScore (Zhang et al., 2019) with XLM-R Large (Conneau and Lample, 2019) as secondary metric to determine if the modelgenerated content includes food names similar to those in the gold labels. For open-ended questions, we compute the accuracy of each test sample against multiple references, including translations of the dish in different languages. This approach allows us to accommodate predictions that may not be in the expected language. Models. We evaluate our benchmark on various available VLMs, including 15 open-source models and 3 proprietary models. During the inference of the open-source model, we use 16-bit floating point and employ greedy decoding. We access the proprietary models through API. The complete list of the models is available in Table 3."
        },
        {
            "title": "4.1 Overall Results",
            "content": "The results for WC-VQA are summarized in Table 3. The multiple-choice question (MCQ) results without any context exhibit significant variability, ranging from 30% to 80%, indicating considerable differences in model performance. This variability indicates that predicting MCQs remains challenging task for many models. Notably, proprietary models, particularly GPT-4o, demonstrate exceptional performance, outperforming all other modIn the open-ended question (OEQ) setting, els. (a) MCQ Accuracy vs. Parameters. (b) OEQ Accuracy vs. Parameters. Figure 6: Scaling matters for MCQ (6a) and OEQ (6b). the task proves even more difficult than the MCQ, with models achieving maximum accuracy of less than 20% for dish name predictions and slightly higher for location predictions when no context is provided. However, incorporating context enhances performance across all settings, highlighting that context effectively guides the models in making better predictions. Interestingly, when the adversarial context is introduced, it misleads the models, leading to incorrect predictions and adding further complexity to the task. Among the models evaluated, Llama 3.2 Instruct significantly outperforms other open-source model families, while Qwen2 performs relatively better than Llava 1.6 and Molmo, despite having smaller model sizes."
        },
        {
            "title": "4.2 The Role of Context",
            "content": "For dish name prediction (Task 1), incorporating more relevant context significantly enhances performance across all language families. However, when adversarial context is introduced, performance drops significantly. The adversarial context included in the prompt significantly affects the prediction. Instead of relying solely on the image input, the model often sways and makes predictions based on incorrect location or cuisine information, even when the context is unrelated to the query. This observation is particularly intriguing, as it signifies that such prompts can shift the models attention and influence its generation process."
        },
        {
            "title": "4.3 Results by Language",
            "content": "In Task 1 with OEQ setting (Figure 5b), some languages with non-Latin scripts, such as Arabic, Korean, Japanese, and Marathi, tend to perform poorly, with the exception of Chinese. For Task 2 with OEQ setting, most models struggle with Sino-Tibetan languages (i.e., Chinese, Cantonese, and Hokkien) and Niger-Congo languages (i.e., Yoruba). In contrast, the models demonstrate relatively strong performance with Japonic, Koreanic, Kra-Dai (i.e., Thai), and Turkic (i.e., Azerbaijani) languages. We also observe that answering OEQs in underrepresented languages remains particularly challenging for the models, as shown by the relatively lower results for the left behind, scraping by, and hopeful languages. Interestingly, lower performance in the OEQ does not necessarily translate to the lower performance in the MCQ setting (Figure 5a) where the performance gap between language categories is less pronounced. The gap between OEQ and MCQ, especially for underrepresented languages, suggests that the bottleneck might lie in the factors beyond cultural understanding, such as text generation capabilities."
        },
        {
            "title": "4.4 Scaling Law",
            "content": "It is evident that models with large sizes perform better than smaller ones, showing the scaling law still exists in this experiment, as shown in Figure 6. It is very interesting to see the same trend across different model families (e.g., Llava, Qwen, and even GPT-4o series). However, it is pretty clear for open-source models, Llama 3.2 Instruct has the lead for overall performance, which may be due the coverage of multilingual data used in its training, although it is still unclear since there is no evidence or supporting information that can back up the finding. Regardless, NVLM-D model does not perform as good as their base model Qwen2 VL Instruct in our benchmark. One reason could be the NVLM model is highly tuned for English, but not in languages other than English."
        },
        {
            "title": "5 Related Work",
            "content": "Cultural VQA. Several prior studies have focused on developing culturally relevant VQA benchmarks, including FM-IQA (Gao et al., 2015), MCVQA (Gupta et al., 2020), xGQA (Pfeiffer et al., 2022), MaXM (Changpinyo et al., 2023), MTVQA (Tang et al., 2024), MABL (Kabra et al., 2023), MAPS (Liu et al., 2024a), and MaRVL (Liu et al., 2021). Additionally, CVQA (Romero et al., 2024) and CulturalVQA (Nayak et al., 2024) provide VQA datasets that cover various regions and diverse topics, including food, with CVQA also offering questions in multiple languages alongside English translations. SEA-VQA (Urailertprasert et al., 2024) specifically benchmarks the South East Asian region. In contrast, FoodieQA (Li et al., 2024b) and World Wide Dishes (Magomere et al., 2024) are focused exclusively on food-related benchmarks. Our work is similarly motivated by using food as cultural proxy, but it distinguishes itself with significantly larger dataset and broader coverage of languages and dialects. Multi-modal LLMs. Recent advancements in VLMs have led to the emergence of multi-modal LLMs that can process both images and text. LLaVA (Liu et al., 2024c) exemplifies this approach by utilizing Vicuna (Zheng et al., 2023) as an image encoder, thereby enhancing visual understanding. This architecture has set precedent for other VLMs, including Qwen2-VL (Bai et al., 2023), Llama 3.2 (Dubey et al., 2024), Pixtral (Agrawal et al., 2024), Phi-3.5 Vision (Abdin et al., 2024), Molmo (Deitke et al., 2024), Aria (Li et al., 2024a), Pangea (Yue et al., 2024), and NVLM (Dai et al., 2024), each leveraging their respective large language models for multi-modal tasks. In specialized application, FoodLMM (Yin et al., 2023) focuses specifically on the food domain, training on publicly available food datasets and conversational data generated by GPT-4 (Achiam et al., 2023). Our work evaluates the capabilities of these models within the food domain, offering insights into their performance and potential applications in culinary-related tasks across multicultural settings."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce WORLDCUISINES, an open-source, large-scale benchmark designed for multilingual and multicultural, visually grounded language understanding. It comprises over 1 million data points across 30 languages and dialects. Our findings reveal that this benchmark remains challenging for models, particularly with dishes from specific regions and in low-resource languages. This provides insight into how well models understand regional cuisines. To enhance usability, we offer dedicated evaluation split with two datasets of varying sizes. Our evaluation shows that while VLMs perform better with the correct context, they struggle with adversarial contexts intended to mislead them. Additionally, we are releasing comprehensive knowledge base, VQA dataset, code, and leaderboard as open-source resources to support future research."
        },
        {
            "title": "Limitations",
            "content": "In this paper, we limit our investigation to avoid exhaustively evaluating all possible models due to resource constraints. Our primary focus is on developing benchmark that facilitates exploration for future research. We also provide training data split for reference, allowing other researchers to utilize it to enhance their VLMs and evaluate their models against our test sets. Currently, we include 30 different languages and dialects, establishing one of the largest and most diverse benchmarks for comprehensive multilingual VQA. We aim to extend this benchmark to encompass additional languages in the future, making it more inclusive and representative of broader range of linguistic diversity. Regarding the food entries coverage, it is important to note that our food entries are currently sourced from English Wikipedia. Although we aim to include as many diverse dishes as possible, we acknowledge that this approach limits the coverage of some regions. Nevertheless, our dataset serves as valuable starting point. In future work, we plan to incorporate entries from non-English Wikipedia pages to improve regional representation and cultural diversity. For evaluation purposes, we include accuracy metrics for overall model performance and BERTScore for more detailed analysis. However, we recognize that evaluating VQA model performance on multicultural data remains an open challenge. Appropriate evaluation metrics are needed to effectively model the diversity of cultural contexts and linguistic variations. Addressing this issue will be key focus of our future research efforts."
        },
        {
            "title": "Ethical Considerations",
            "content": "Our research focuses on evaluating VLMs within the context of multilingual and multicultural VQA, field that holds significant implications for diverse multilingual communities. We are committed to conducting our data collection and evaluations with the highest standards of transparency and fairness. To achieve this, we have adopted crowd-sourcing approach for the annotation process, inviting volunteers to contribute and become co-authors if they provide significant contributions. We follow the guidelines from ACL for authorship eligibility as shown in https://www.aclweb.org/adminwiki/index. php/Authorship_Changes_Policy_for_ACL_ Conference_Papers. In line with our commitment to openness and collaboration, we will release our dataset under an open-source license, CC-BY-SA 4.0."
        },
        {
            "title": "Acknowledgements",
            "content": "We extend our gratitude to everyone who has supported our project, especially the numerous annotators who provided meticulous and comprehensive annotations and conducted thorough quality checks. Special thanks to Francesca Porcu for her assistance with the Sardinian language and to Shintaro Ozaki for his help with Japanese. We are also deeply appreciative of Nayeon Lee and Wenliang Dai for their insightful discussions and for integrating NVLM into our benchmark. Additionally, we thank Xiang Yue and Yueqi Song for their help in integrating Pangea into our benchmark."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Marié Abe. 2013. Tokyo, japan. In The Ethnomusicologists Cookbook, pages 4045. Routledge. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Ashutosh Dwivedi, Alham Fikri Aji, Jacki ONeill, Ashutosh Modi, and Monojit Choudhury. 2024. Towards measuring and modeling\" culture\" in llms: survey. arXiv preprint arXiv:2403.15412. question answering and reasoning with knowledgeinfused synthetic data generation pipeline. In Findings of the Association for Computational Linguistics: EACL 2024, pages 11581176. Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, et al. 2024. Pixtral 12b. arXiv preprint arXiv:2410.07073. Gina Almerico. 2014. Food and identity: Food studies, cultural, and personal identity. Journal of International Business and Cultural Studies, 8:1. Eugene Newton Anderson. 2014. Everyone eats: Understanding food and culture. NYU Press. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Lyle Campbell and Verónica Grondona. 2008. EthLanguage, nologue: Languages of the world. 84(3):636641. Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish Thapliyal, Idan Szpektor, Julien Amelot, Xi Chen, and Radu Soricut. 2023. Maxm: Towards multilingual visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 26672682. Alexis Conneau and Guillaume Lample. 2019. Crosslingual language model pretraining. Advances in neural information processing systems, 32. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Paul Freedman. 2021. Why Food Matters. Yale University Press. Pulkit Agarwal, Settaluri Sravanthi, and Pushpak BhatIndifoodvqa: Advancing visual tacharyya. 2024. Barbara Gallani. 2015. Dumplings: global history. Reaktion Books. Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to machine? dataset and methods for multilingual image question. Advances in neural information processing systems, 28. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024b. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306. Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavaš. 2023. mblip: Efficient bootstrapping of multilingual vision-llms. arXiv preprint arXiv:2307.06930. Deepak Gupta, Pabitra Lenka, Asif Ekbal, and Pushpak Bhattacharyya. 2020. unified framework for multilingual and code-mixed visual question answering. In Proceedings of the 1st conference of the AsiaPacific chapter of the association for computational linguistics and the 10th international joint conference on natural language processing, pages 900913. Jon Holtzman. 2006. Food and memory. Annu. Rev. Anthropol., 35(1):361378. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the nlp world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293. Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, and Graham Neubig. 2023. Multi-lingual and multi-cultural figurative language understanding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 82698284. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. 2024a. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993. Wenyan Li, Xinyu Zhang, Jiaang Li, Qiwei Peng, Raphael Tang, Li Zhou, Weijia Zhang, Guimin Hu, Yifei Yuan, Anders Søgaard, et al. 2024b. Foodieqa: multimodal dataset for fine-grained understanding of chinese food culture. arXiv preprint arXiv:2406.11030. Chen Liu, Fajri Koto, Timothy Baldwin, and Iryna Gurevych. 2024a. Are multilingual llms culturallydiverse reasoners? an investigation into multicultural proverbs and sayings. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 20162039. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. 2021. Visually grounded reasoning across languages and cultures. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1046710485. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024c. Visual instruction tuning. Advances in neural information processing systems, 36. Jabez Magomere, Shu Ishida, Tejumade Afonja, Aya Salama, Daniel Kochin, Foutse Yuehgoh, Imane Hamzaoui, Raesetje Sefala, Aisha Alaagib, Elizaveta Semenova, et al. 2024. You are what you eat? feeding foundation models regionally diverse food dataset of world wide dishes. arXiv preprint arXiv:2406.09496. Tarek Naous, Michael Ryan, Alan Ritter, and Wei Xu. 2023. Having beer after prayer? measuring cultural bias in large language models. arXiv preprint arXiv:2305.14456. Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stanczak, and Aishwarya Agrawal. 2024. Benchmarking vision language models for cultural understanding. arXiv preprint arXiv:2407.10920. Ngan Luu-Thuy Nguyen, Nghia Hieu Nguyen, Duong TD Vo, Khanh Quoc Tran, and Kiet Van Nguyen. 2023. Vlsp2022-evjvqa challenge: Multilingual visual question answering. arXiv preprint arXiv:2302.11752. Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, JanMartin Steitz, Stefan Roth, Ivan Vulic, and Iryna Gurevych. 2022. xgqa: Cross-lingual visual question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 24972511. Poppe. 1992. Gelatin."
        },
        {
            "title": "In Thickening and gelling",
            "content": "agents for food, pages 98123. Springer. David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. 2024. Cvqa: Culturally-diverse multilingual visual quesarXiv preprint tion answering benchmark. arXiv:2406.05967. DongJae Shin, HyeonSeok Lim, Inho Won, ChangSu Choi, Minjun Kim, SeungWoo Song, HanGyeol Yoo, SangMin Kim, and KyungTae Lim. 2024. X-llava: Optimizing bilingual large vision-language alignment. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 24632473. Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, et al. 2024. Mtvqa: Benchmarking multilingual text-centric visual question answering. arXiv preprint arXiv:2405.11985. Norawit Urailertprasert, Peerat Limkonchotiwat, Supasorn Suwajanakorn, and Sarana Nutanong. 2024. Sea-vqa: Southeast asian cultural context dataset for In Proceedings of the visual question answering. 3rd Workshop on Advances in Language and Vision Research (ALVR), pages 173185. Mark Wahlqvist. 2007. Regional food culture and development. Asia Pacific journal of clinical nutrition, 16:2. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual e5 text embeddings: technical report. arXiv preprint arXiv:2402.05672. Genta Indra Winata, Ruochen Zhang, and David Ifeoluwa Adelani. 2024. Miners: Multilingual language models as semantic retrievers. arXiv preprint arXiv:2406.07424. Yuehao Yin, Huiyan Qi, Bin Zhu, Jingjing Chen, YuGang Jiang, and Chong-Wah Ngo. 2023. Foodlmm: versatile food assistant using large multi-modal model. arXiv preprint arXiv:2312.14991. Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. 2024. Pangea: fully open multilingual multimodal llm for 39 languages. arXiv preprint arXiv:2410.16153. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623."
        },
        {
            "title": "A Data Statement",
            "content": "A.1 Executive Summary WORLDCUISINES is vision-language benchmark comprised of two resources: (1) WC-VQA, multilingual parallel question answering dataset covering 30 languages and dialects where each dish image is accompanied by questions and context constructed through human translation; and (2) WC-KB, knowledge base containing images and metadata associated with the dishes. we develop WC-VQA and WC-KB. Dish names and their information are collected from English Wikipedia, and the images are selected from Wikimedia Commons to ensure permissive license, with an emphasis on representing wide range of food categories and geographic origins (or where the dish is popular). This selection strategy aims to provide insights into the VLMs ability to generalize across diverse culinary and cultural contexts. A.3 Language Variety WORLDCUISINES covers 30 languages and dialects spoken across diverse countries and regions. The complete list of languages and dialects is shown in Table 5. An example of the multilingual prompt is shown in Table 6. A.4 Annotator Demographic Over 30 annotators are involved in building WORLDCUISINES, specifically in translating the query and context for the WC-VQA dataset. Most annotators are native speakers of the target languages or dialects included in our data; some are L2 speakers with more than 10 years of study in their respective languages. The detailed demographics for each language are elaborated below. A.4.1 Austronesian Indonesian Two native Indonesian speakers are involved as translators. One is in the 2635 age range, and the other is in the 1625 age range. Tagalog One native Tagalog speaker in the 1625 age range is involved as translator. Sundanese Two L2 Sundanese speakers contribute to the translation. One, in the 1625 age range with 15 years of experience with the Sundanese language, assists with translation. The other, in the 2635 age range with 25 years of experience with the language, primarily serves as the proofreader. Javanese One native Javanese speaker with Central Java dialect in the 1625 age range translates for both registers of the language (Krama and Ngoko). A.4."
        },
        {
            "title": "Japonic",
            "content": "A.2 Curation Rationale The goal of WORLDCUISINES is to evaluate the cultural understanding of vision-language models (VLMs) within the food domain. To achieve this, Japanese Three L2 Japanese speakers with over 10 years of language study contribute to the Japanese translation. Two are in the 2635 age range, and one is in the 3645 age range. native Attribute Value Description Example Image of the dish in jpg/png/gif format. Name of the dish. Dorayaki Alias name of the dish (i.e., the name in the original language). どら焼き Image Image String Name Alias String Coarse-grained categories List<String> Coarse-level categories. Fine-grained categories Cuisines Associated Cuisines Area Countries Continents Text Description String String String String String String List<String> Fine-level categories. Name of cuisine. Associated cuisines to the dish. Specific region where the dish is originated Specific region where the dish is originated Specific continent where the dish is originated Short description of the dish, including the ingredients used to prepare the dish or the cooking method. Image License String License of the image [Pancake] [Wagashi Pancake] Japanese Japanese Ueno Japan Eastern Asia The dish consists of two small pancake-like patties made from castella wrapped around filling of sweet bean paste. CC BY-SA 3.0 Table 4: WC-KB attributes in WORLDCUISINES. Language Name Language Vitality Resource Classification Linguistic Register"
        },
        {
            "title": "Austronesian",
            "content": "Indonesian Tagalog Sundanese Javanese Japonic Institutional Institutional Stable Institutional 3 - Rising Star 3 - Rising Star 1 - Scraping by 1 - Scraping by Japanese Institutional 5 - Winners Sino-Tibetan Chinese Cantonese Hokkien Koreanic Korean Kra-Dai Thai Indo-European English Spanish French Russian Czech Italian Hindi Bengali Marathi Sardinian Sinhala Afro-Asiatic Institutional Institutional Institutional 5 - Winners 1 - Scraping by 0 - Left Behind Institutional 4 - Underdog Institutional 3 - Rising Star Institutional Institutional Institutional Institutional Institutional Institutional Institutional Institutional Institutional Endangered Institutional 5 - Winners 5 - Winners 5 - Winners 4 - Underdog 4 - Underdog 4 - Underdog 4 - Underdog 3 - Rising Star 2 - Hopeful 1 - Scraping by 0 - Left Behind Arabic (MSA) Institutional 5 - Winners Niger-Congo Yoruba Turkic Institutional 2 - Hopeful Formal Casual Loma Krama Ngoko Formal Casual Written Spoken Formal Casual Formal Casual Common speech form Central-Java dialect, polite form Central-Java dialect, casual form Polite form or teinei-go Daily conversation Standard Mandarin Medan dialect Medan dialect Latin-American dialect Formal Logudorese (src) Spoken form Azerbaijani Institutional 1 - Scraping by North Variety (azj) Table 5: The details of languages used in the prompt generation for our VQA dataset. Taken from Ethnologue (Campbell and Grondona, 2008). Based on Joshi et al. (2020). Japanese speaker then proofreads the translated sentences. Additionally, one native Japanese speaker from Western Japan in the 1625 age range gives input for the casual form. Language English French Indonesian (Formal) Indonesian (Casual) Japanese (Formal) Japanese (Casual) Javanese (Krama) Javanese (Ngoko) Multi-choice question (MCQ) Open-ended question (OEQ) Question Prompt Answer Text ID Yesterday had nice lunch at Japanese restaurant. am about to have this dish now. What is this dish called? 1. Hangtown fry 2. Zucchini slice 3. Chawanmushi 4. Rolex 5. Egg foo young Print only the answer with single answer id (1,2,3,4,5). Hier, jai pris un bon déjeuner dans un restaurant japonais. Je suis sur le point de manger ce plat maintenant. Comment appelle-t-on ce plat ? 1. Hangtown fry 2. Zucchini slice 3. Chawanmushi 4. Rolex 5. Fu yung hai Print only the answer with single answer id (1,2,3,4,5). Kemarin, saya menyantap makan siang yg nikmat di restoran Jepang. Sekarang saya akan menyantap hidangan ini. Disebut apakah hidangan ini? 1. Hangtown fry 2. Zucchini slice 3. Chawanmushi 4. Rolex 5. Puyunghai Print only the answer with single answer id (1,2,3,4,5). Kemarin aku makan siang enak di restoran Jepang. Sekarang mau makan makanan ini. Makanan ini disebut apa? 1. Hangtown fry 2. Zucchini slice 3. Chawanmushi 4. Rolex 5. Puyunghai Print only the answer with single answer id (1,2,3,4,5). 昨日私は日本料理店で美味しい昼食を食べました 今まさにこの料理を食べようとしています この料理の名前は何ですか? 1. Hangtown fry 2. Zucchini slice 3. 茶碗蒸し 4. Rolex 5. 芙蓉蛋 Print only the answer with single answer id (1,2,3,4,5). 昨日日本料理のお店で美味しいランチを食べたんだけど 今まさに食べてるこの料理の名前は何 1. Hangtown fry 2. Zucchini slice 3. 茶碗蒸し 4. Rolex 5. 芙蓉蛋 Print only the answer with single answer id (1,2,3,4,5). Kaping wingi kula nedha nikmat ing restoran Jepang. Kula kepengin nedha menika malih sakmenika. Naminipun nopo dhaharan menika? 1. Hangtown fry 2. Zucchini slice 3. Chawanmushi 4. Rolex 5. Endhog foo young Print only the answer with single answer id (1,2,3,4,5). Wingi aku mangan enak ndek restoran Jepang. Aku pengen mangan neh saiki. Opo jenenge panganan iki? 1. Hangtown fry 2. Zucchini slice 3. Chawanmushi 4. Rolex 5. Endhog foo young Print only the answer with single answer id (1,2,3,4,5). Yesterday had nice lunch at Japanese restaurant. am about to have this dish now. What is this dish called? 5 Egg foo young Print only the answer. Hier, jai pris un bon déjeuner dans un restaurant japonais. Je suis sur le point de manger ce plat maintenant. Comment appelle-t-on ce plat ? 5 Fu yung hai Print only the answer. Kemarin, saya menyantap makan siang yg nikmat di restoran Jepang. Sekarang saya akan menyantap hidangan ini. Disebut apakah hidangan ini? Puyunghai Print only the answer. Kemarin aku makan siang enak di restoran Jepang. Sekarang mau makan makanan ini. Makanan ini disebut apa? Print only the answer. 5 Puyunghai 昨日私は日本料理店で美味しい昼食を食べました 今まさにこの料理を食べようとしています この料理の名前は何ですか? 5 芙蓉蛋 Print only the answer. 昨日日本料理のお店で美味しいランチを食べたんだけど 今まさに食べてるこの料理の名前は何 芙蓉蛋 Print only the answer. Kaping wingi kula nedha nikmat ing restoran Jepang. Kula kepengin nedha menika malih sakmenika. Naminipun nopo dhaharan menika? Print only the answer. 5 Endhog foo young Wingi aku mangan enak ndek restoran Jepang. Aku pengen mangan neh saiki. Opo jenenge panganan iki? Print only the answer. 5 Endhog foo young Table 6: Multilingual prompt example of Task 1 (c) adversarial in 8 language variants (out of 30). The visual image given is an image of Egg foo young, Chinese cuisine. The qa_id of this example is 1806. Figure 7: BERTScore (%) categorized by language (left), language vitality (center), and language family (right). We classify the language vitality by following the classification from Joshi et al. (2020). A.4.6 Indo-European English Query and context in English are constructed. All are L2 English speakers with over 20 years of study and have lived in the English speaking countries. Four of the annotators are in the 2635 age range, and one is in 3645 age range. Two native English speakers skimmed through the prompt templates. Spanish One native Spanish speaker in the 2635 age range translates the Latin-American versions or dialects of the language. French One native French speaker and one L2 speaker are involved as translators. The native speaker is in the 2635 age range, and the L2 speaker is in the 3645 age range. Russian One native Russian speaker in the 2635 age range is involved as translators. One L2 speaker in 3645 proofreads the template for inflection. Czech One native Czech speaker in the 3645 age range is involved as translator. Italian Two native Italian speakers, both in the 3645 age range, are involved as translators. Hindi One native Hindi speaker in the 2635 age range is involved as translator. Bengali One native Bengali speaker in the 2635 age range is involved as translator. Marathi One native Marathi speaker in the 26 35 age range is involved as translator. Sardinian One native Logudorese Sardinian speaker in the 3645 age range is involved as translator. Figure 8: Model performance evaluated with different references on open-ended question. A.4.3 Sino-Tibetan Chinese One native Chinese speaker in the 16 25 age range is involved as translator. Cantonese Two native Cantonese speakers are involved as translators. One is in 3645 age range, and the other is in the 1625 age range. Hokkien Two native Hokkien speakers in the Medan dialect translate for both written and spoken versions of the language. Both are in the 2635 age range. A.4.4 Koreanic Korean One native Korean speaker in the 1625 age range translates the formal and casual versions of the language. A.4.5 Kra-Dai Thai One native Thai speaker in the 2635 age range is involved as translator. Sinhala One native Sinhala speaker in the 2635 age range is involved as translator. A.4.7 Afro-Asiatic Arabic (MSA) One native Arabic speaker in the 2635 age range is involved in the Modern Standard Arabic (MSA) translation. A.4.8 Niger-Congo Yoruba One native Yoruba speaker in the 1625 age range is involved as translator. A.4.9 Turkic for both Test Small and Test Large. Figure 7 illustrates the models performance categorized by language, language vitality, and language family. Robustness and Error Analysis. Figure 9 illustrates the correlation between BERTScore and accuracy in the open-ended setting through regression analysis. The R-squared value is 0.41, indicating low correlation between BERTScore and accuracy. Despite this, BERTScore remains useful metric for assessing whether the models predictions have semantic similarity to the gold labels, even if they are not exact matches. Azerbaijani One native Azerbaijani speaker in the 1625 age range is involved as translator."
        },
        {
            "title": "D Evaluation",
            "content": "D.1 Prompt Sensitivity We use the same prompts for all models, with the exception of the Pangea 7B model (Yue et al., 2024). This model is particularly sensitive and lacks robustness in handling diverse prompt instructions, often struggling to follow instructions accurately, especially in multiple-choice questions (MCQs), unless specific template is applied. In contrast, models like Llama 3.2 Instruct and Qwen2 VL Instruct are more adaptable to varied instructions. After consulting with the authors, we adopted the prompt Answer with the option letter from the given choices directly. for MCQ queries when using the Pangea 7B model. Open-Source Collaborative Effort The WORLDCUISINES data collection and benchmark construction is fully open-source project. We invite contributions from researchers, practitioners, and grassroots communities, such as local NLP communities, who are interested in participating. Contributions can include data collection, annotation, quality checks, and evaluation. To ensure high-quality data, we engage native speakers of local languages in the annotation process with strict quality control (QC). The contributors who provide substantial contribution are invited to have co-authorship on this paper. We follow the guidelines from ACL for authorship eligibility.6 Our goal is to develop resource and benchmark that will have meaningful impact on future research. To achieve this, we are dedicated to expanding language coverage and ensuring that contributions are as inclusive and diverse as possible."
        },
        {
            "title": "C More Results",
            "content": "C.1 Primary Metric: Accuracy (%) Table 7 presents the comprehensive results of WC-VQA for both Test Small and Test Large. Additionally, we examine the performance gap between different references used in the evaluation, with the results displayed in Figure 8. C.2 Secondary Metric: BERTScore As secondary metric, we employ BERTScore using XLM-R Large as the base model. Table 8 presents the comprehensive results of WC-VQA 6The ACL guidelines can be found at https: //www.aclweb.org/adminwiki/index.php/Authorship_ Changes_Policy_for_ACL_Conference_Papers. Figure 9: Regression Analysis for BERTScore OE vs. Accuracy OE. Figure 10: Model performance with different references on open-ended question. Model (Accuracy %) Test Small (12k) Open-Source Llava1.6 Vicuna 7B Llava1.6 Vicuna 13B Qwen2 VL Instruct 2B Qwen2 VL Instruct 7B Qwen2 VL Instruct 72B Llama 3.2 Instruct 11B Llama 3.2 Instruct 90B Molmo-E 1B Molmo-D 7B Molmo-O 7B Pangea 7B Pangea 7B Aria 25B Phi-3.5 Vision 4B Pixtral 12B NVLM-D 72B Proprietary GPT-4o GPT-4o Mini Gemini 1.5 Flash Test Large (60k) Open-Source Llava1.6 Vicuna 7B Llava1.6 Vicuna 13B Qwen2 VL Instruct 2B Qwen2 VL Instruct 7B Qwen2 VL Instruct 72B Llama 3.2 Instruct 11B Llama 3.2 Instruct 90B Molmo-E 1B Molmo-D 7B Molmo-O 7B Pangea 7B Pangea 7B Aria 25B Phi-3.5 Vision 4B Pixtral 12B NVLM-D 72B Proprietary GPT-4o GPT-4o Mini Gemini 1.5 Flash (a) no-context MCQ OEQ Task 1 (Dish Name) (b) contextualized MCQ OEQ (c) adversarial MCQ OEQ Task 2 (Location) MCQ OEQ MCQ OEQ"
        },
        {
            "title": "Average",
            "content": "33.63 40.87 40.97 63.83 76.13 57.93 77.33 21.87 50.67 46.03 45.33 54.87 65.77 49.27 57.57 75.50 0.87 1.00 3.33 4.07 10.40 14.37 14.27 0.00 1.00 2.13 0.43 0.43 2.67 1.90 0.60 3.13 43.13 50.30 44.40 67.20 81.63 65.57 83.43 24.53 57.00 43.27 59.40 65.77 71.43 53.03 72.33 78.20 2.83 4.17 4.60 8.57 17.43 19.20 22.30 0.13 2.23 4.37 1.33 1.33 6.47 3.03 1.83 7.37 28.67 38.37 47.07 57.00 67.23 56.27 71.23 20.23 48.67 41.60 22.17 55.00 57.13 42.90 55.40 54.67 0.60 1.60 3.43 3.90 6.27 9.50 9.00 0.00 1.73 2.10 0.63 0.63 1.80 1.33 0.57 1. 27.77 31.07 48.37 56.80 56.73 46.60 64.70 19.60 36.73 26.83 34.10 48.47 39.60 31.23 44.73 54.13 7.93 8.63 12.50 21.23 26.07 27.23 29.73 1.27 11.70 9.03 17.90 17.90 15.70 8.43 12.83 17.40 33.30 40.15 45.20 61.21 70.43 56.59 74.17 21.56 48.27 39.43 40.25 56.03 58.48 44.11 57.51 65.62 3.06 3.85 5.96 9.44 15.04 17.58 18.82 0.35 4.16 4.41 5.07 5.07 6.66 3.67 3.96 7.32 88.40 75.33 78.17 16.60 7.30 16. 90.43 83.00 82.07 35.47 17.67 23.53 82.23 64.83 71.33 12.60 3.53 7.33 63.60 52.87 66.00 35.53 26.90 32. 81.17 69.01 74.39 25.05 13.85 19.86 34.57 40.17 41.65 61.48 74.19 59.93 77.69 18.81 46.01 39.96 41.38 52.35 58.61 43.37 56.65 69.82 1.59 2.79 7.98 6.76 12.67 18.75 16.93 0.01 2.89 5.15 1.52 1.52 4.99 2.91 1.22 4.71 43.48 48.17 42.29 67.85 80.79 64.12 82.92 24.22 55.95 44.93 57.95 63.07 69.29 48.71 70.69 78.93 4.03 5.85 8.13 10.36 21.31 22.96 23.60 0.23 3.66 6.03 2.73 2.73 9.17 4.23 2.94 10. 34.84 39.05 39.69 53.52 62.43 53.17 63.96 19.55 41.61 38.41 21.77 49.17 52.82 40.87 52.12 52.12 1.41 2.57 6.74 6.12 8.37 13.39 10.87 0.01 2.31 3.51 1.57 1.57 3.39 2.07 1.09 2.89 32.24 37.79 47.85 55.90 61.90 57.93 67.87 18.97 33.35 29.81 37.15 48.71 42.82 35.01 46.67 51.97 9.29 10.16 14.55 21.03 27.27 31.58 31.31 1.54 11.45 10.07 20.15 20.15 16.20 9.22 14.43 16.68 36.28 41.30 42.87 59.69 69.83 58.79 73.11 20.39 44.23 38.28 39.56 53.33 55.89 41.99 56.53 63.21 4.08 5.34 9.35 11.07 17.40 21.67 20.68 0.45 5.08 6.19 6.49 6.49 8.44 4.61 4.92 8. 88.45 72.80 77.05 21.88 10.28 12.81 91.57 81.65 80.97 37.51 20.87 15.16 82.29 57.76 69.13 14.79 5.72 6. 66.52 52.37 71.53 37.13 25.79 30.03 82.21 66.14 74.67 27.83 15.66 16.12 Table 7: Accuracy (%) results of WC-VQA. MCQ and OEQ indicate multiple-choice question and open-ended question, respectively. Best and second-best are bolded and underlined, respectively. We employ an optimized prompt provided by the authors (see Subsection D.1 in the Appendix for further details). Figure 11: Dish frequency by country showing 189 countries. Model (BERTScore) Test Small (12k) Open-Source Llava1.6 Vicuna 7B Llava1.6 Vicuna 13B Qwen2 VL Instruct 2B Qwen2 VL Instruct 7B Qwen2 VL Instruct 72B Llama 3.2 Instruct 11B Llama 3.2 Instruct 90B Molmo-E 1B Molmo-D 7B Molmo-O 7B Pangea 7B Aria 25B Phi-3.5 Vision 4B Pixtral 12B NVLM-D 72B Proprietary GPT-4o GPT-4o Mini Gemini 1.5 Flash Test Large (60k) Open-Source Llava1.6 Vicuna 7B Llava1.6 Vicuna 13B Qwen2 VL Instruct 2B Qwen2 VL Instruct 7B Qwen2 VL Instruct 72B Llama 3.2 Instruct 11B Llama 3.2 Instruct 90B Molmo-E 1B Molmo-D 7B Molmo-O 7B Pangea 7B Aria 25B Phi-3.5 Vision 4B Pixtral 12B NVLM-D 72B Proprietary GPT-4o GPT-4o Mini Gemini 1.5 Flash (a) no-context Task 1 (Dish Name) (b) contextualized (c) adversarial Task 2 (Location)"
        },
        {
            "title": "Average",
            "content": "81.49 80.50 82.48 82.65 83.78 82.45 82.82 81.17 81.26 82.14 81.29 79.85 80.82 78.84 81.39 84.86 83.10 84.68 81.63 80.65 82.95 82.92 83.72 82.54 83.05 81.17 81.39 82.27 81.40 79.89 80.98 79.00 81.54 85.04 83.19 84.47 82.13 80.65 82.75 83.13 84.63 82.93 83.44 81.12 81.65 82.24 81.78 80.26 79.66 79.12 82.05 86.92 83.91 85. 82.10 80.70 83.10 83.42 85.10 82.79 83.51 81.10 81.63 82.21 81.91 80.20 79.55 79.33 82.17 86.93 84.05 84.97 81.56 80.14 82.34 82.10 83.06 81.64 81.98 81.24 80.55 81.44 80.19 79.86 76.77 78.90 79.98 83.89 82.16 83.11 81.58 80.12 82.81 82.30 83.11 81.64 81.95 81.13 80.73 81.52 80.23 79.83 77.61 78.98 80.05 83.92 82.38 83. 85.45 81.77 84.29 87.22 87.10 82.59 85.70 83.58 84.87 84.38 86.31 80.53 83.25 86.40 85.64 88.98 87.34 89.15 85.81 81.86 84.51 87.39 87.42 82.88 85.85 83.87 85.10 84.63 86.79 80.63 83.31 86.75 85.67 89.06 87.30 89.43 82.66 80.77 82.97 83.78 84.64 82.40 83.48 81.78 82.08 82.55 82.39 80.12 80.12 80.81 82.27 86.16 84.13 85. 82.78 80.83 83.34 84.01 84.84 82.46 83.59 81.82 82.21 82.66 82.58 80.14 80.36 81.02 82.36 86.24 84.23 85.50 Table 8: BERTScore results of WC-VQA. Only the results from open-ended (OEQ) are used. Best and second-best are bolded and underlined, respectively. Continents/Regions # Countries # Food Entries % in Our Data Global Africa"
        },
        {
            "title": "Oceania",
            "content": "Australia & New Zealand Melanesia Micronesia Polynesia N/A 52 96 190 3.98% 7.87% 18 6 7 5 16 15 8 2 10 15 13 9 5 9 12 9 18 2 1 - - 37 47 3 40 17 67 33 60 1.7% 0.7% 2.8% 1.4% 2.5% 472 19.55% 60 134 230 2.5% 5.6% 9.5% 4.5% 808 33.47% 164 237 300 233 6.8% 9.8% 12.4% 9.7% 1, 43.58% 10 420 362 200 155 37 33 4 - - 0.4% 17.4% 15.0% 8.3% 6.4% 1.53% 1.4% 0.2% - - Table 9: Geographical distribution of WC-KB, corresponds to Figure 3. Note that there are food entries linked to multiple regions, with some linked to multiple continents. Global denotes entries with more than five regions."
        },
        {
            "title": "Country",
            "content": "Count %"
        },
        {
            "title": "Country",
            "content": "Count %"
        },
        {
            "title": "Country",
            "content": "Count %"
        },
        {
            "title": "United States\nJapan\nChina\nIndonesia\nPhilippines\nMexico\nIndia\nFrance\nItaly\nSpain\nUnited Kingdom\nGlobal\nGermany\nRussia\nTurkey\nKorea\nIran\nThailand\nSingapore\nPortugal\nBrazil\nIsrael\nRomania\nAustria\nPoland\nPakistan\nVietnam\nCanada\nGreece\nUkraine\nBulgaria\nSlovenia\nEgypt\nSyria\nNepal\nSerbia\nMyanmar\nLebanon\nTunisia\nBangladesh\nMalaysia\nSri Lanka\nNigeria\nJamaica\nNetherlands\nAlbania\nSouth Africa\nAustralia\nHungary\nPalestine\nIraq\nJordan\nBosnia and Herzegovina\nTaiwan\nAlgeria\nSwitzerland\nCyprus\nMorocco",
            "content": "216 182 177 143 133 132 129 117 99 87 80 80 77 76 69 66 65 58 58 57 54 48 47 46 45 44 44 43 42 42 41 39 38 38 37 36 34 34 34 33 32 32 32 32 32 32 32 32 31 30 29 29 29 28 28 27 26 25 9.47 7.98 7.76 6.27 5.83 5.78 5.65 5.12 4.34 3.81 3.51 3.51 3.37 3.33 3.02 2.89 2.85 2.54 2.54 2.50 2.37 2.10 2.06 2.02 1.97 1.93 1.93 1.89 1.84 1.84 1.80 1.71 1.67 1.67 1.62 1.58 1.49 1.49 1.49 1.45 1.40 1.40 1.40 1.40 1.40 1.40 1.40 1.40 1.36 1.32 1.27 1.27 1.27 1.23 1.23 1.18 1.14 1."
        },
        {
            "title": "Argentina\nSaudi Arabia\nNorth Macedonia\nCuba\nLibya\nMontenegro\nChile\nIreland\nPeru\nHong Kong\nDenmark\nColombia\nArmenia\nLithuania\nBelgium\nBrunei Darussalam\nCzech Republic\nNew Zealand\nFinland\nDominican Republic\nYemen\nAzerbaijan\nMoldova\nBhutan\nPuerto Rico\nVenezuela\nUruguay\nBolivia\nTrinidad and Tobago\nGeorgia\nNorway\nCambodia\nAfghanistan\nSlovakia\nEthiopia\nLatvia\nLaos\nGuatemala\nGhana\nUnited Arab Emirates\nKuwait\nParaguay\nEl Salvador\nBahrain\nHaiti\nUzbekistan\nKazakhstan\nEritrea\nOman\nQatar\nSudan\nSuriname\nMauritania\nBahamas\nNicaragua\nSenegal\nBarbados\nDominica",
            "content": "25 24 24 23 23 23 23 23 22 22 22 21 21 21 20 20 20 20 19 18 18 18 18 17 17 17 17 16 16 16 16 15 15 15 15 15 14 14 13 12 12 11 11 11 11 10 10 10 10 10 10 10 9 9 8 8 8 8 Grenada Cameroon Somalia Antigua and Barbuda Maldives Kyrgyzstan Tajikistan Togo Uganda Benin Macau Guyana Saint Kitts and Nevis Saint Lucia Saint Vincent and the Grenadines Fiji Mongolia Liechtenstein Macedonia Malta Mozambique Angola Cabo Verde Turkmenistan Costa Rica Burkina Faso Luxembourg Djibouti Iceland Sierra Leone Niger Mauritius Guinea Zimbabwe Namibia Lesotho Zambia Congo Gambia Liberia Comoros South Korea Wales Honduras Anguilla Western Sahara Faroe Islands Seychelles Burundi Rwanda North Korea Timor-Leste Guernsey Madagascar Central African Republic Monaco 1.10 1.05 1.05 1.01 1.01 1.01 1.01 1.01 0.96 0.96 0.96 0.92 0.92 0.92 0.88 0.88 0.88 0.88 0.83 0.79 0.79 0.79 0.79 0.75 0.75 0.75 0.75 0.70 0.70 0.70 0.70 0.66 0.66 0.66 0.66 0.66 0.61 0.61 0.57 0.53 0.53 0.48 0.48 0.48 0.48 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.39 0.39 0.35 0.35 0.35 0.35 Table 10: Distribution of food entries by country. 8 8 8 7 7 7 7 7 7 7 7 7 7 7 7 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 0.35 0.35 0.35 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.18 0.18 0.18 0.18 0.18 0.18 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0."
        }
    ],
    "affiliations": [
        "AI Singapore",
        "Boston University",
        "Capital One",
        "Cardiff University",
        "Cohere",
        "Columbia University",
        "HK PolyU",
        "HKUST",
        "ITB",
        "Independent",
        "JAIST",
        "KAIST",
        "MBZUAI",
        "MILA",
        "Masakhane",
        "McGill",
        "Monash University",
        "NAIST",
        "NICT",
        "Ontario Tech",
        "SEACrowd",
        "SMU",
        "Tokyo Tech",
        "UCSB",
        "University of Lagos",
        "UofT"
    ]
}