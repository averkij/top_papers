{
    "paper_title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context",
    "authors": [
        "Yudong Li",
        "Zhongliang Yang",
        "Kejiang Chen",
        "Wenxuan Wang",
        "Tianxin Zhang",
        "Sifang Wan",
        "Kecheng Wang",
        "Haitian Li",
        "Xu Wang",
        "Lefan Cheng",
        "Youdan Yang",
        "Baocheng Chen",
        "Ziyu Liu",
        "Yufei Sun",
        "Liyan Wu",
        "Wenya Wen",
        "Xingchi Gu",
        "Peiru Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we propose LiveSecBench, a dynamic and continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench evaluates models across six critical dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in the Chinese legal and social frameworks. This benchmark maintains relevance through a dynamic update schedule that incorporates new threat vectors, such as the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs, providing a landscape of AI safety in the context of Chinese language. The leaderboard is publicly accessible at https://livesecbench.intokentech.cn/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 6 6 3 2 0 . 1 1 5 2 : r LIVESECBENCH: DYNAMIC AND CULTURALLY-RELEVANT AI SAFETY BENCHMARK FOR LLMS IN CHINESE CONTEXT Yudong Li1, Zhongliang Yang2, Kejiang Chen3, Wenxuan Wang4, Tianxin Zhang5, Sifang Wan5, Kecheng Wang5, Haitian Li5, Xu Wang5, Lefan Cheng5, Youdan Yang5, Baocheng Chen5, Ziyu Liu5, Yufei Sun2, Liyan Wu5, Wenya Wen5, Xingchi Gu5, Peiru Yang"
        },
        {
            "title": "ABSTRACT",
            "content": "In this work, we propose LiveSecBench, dynamic and continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench evaluates models across six critical dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in the Chinese legal and social frameworks. This benchmark maintains relevance through dynamic update schedule that incorporates new threat vectors, such as the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs, providing landscape of AI safety in the context of Chinese language. The leaderboard is publicly accessible at https://livesecbench.intokentech.cn/."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement and widespread deployment of Large Language Models (LLMs) have profoundly reshaped humancomputer interaction and catalyzed productivity gains across industries. However, this transformation is accompanied by escalating security and safety concerns. LLMs, by virtue of their generative and adaptive nature, may produce misinformation [1], amplify social or algorithmic biases [2], leak sensitive or private information [3], or be exploited for malicious purposes such as phishing, social engineering, or the generation of harmful content [4]. As LLMs become deeply embedded in everyday applications, ensuring their safety has become pressing scientific and societal imperative. To assess such risks, researchers have proposed safety benchmarks such as TruthfulQA [1], SafetyBench [5], and HarmBench [6]. These benchmarks evaluate models responses against predefined safety or factuality dimensions, 1 Tsinghua University, 2 Beijing University of Posts and Telecommunications, 3 University of Science and Technology of China, 4 Renmin University of China, 5 IntokenTech, *Corresponding Author: liyudong@tsinghua.edu.cn revealing potential vulnerabilities to harmful content or disinformation. However, most of these efforts are built upon English corpora, thereby overlooking linguistic, cultural, and socio-political nuances that are critical in non-English contexts. When directly translated, many English-designed attack prompts lose their pragmatic intent. Conversely, risk factors unique to the Chinese context are almost entirely absent from existing benchmarks such as indirect expressions, cultural idioms, homophonic puns, and culturally specific taboos. As result, current benchmarks provide limited and potentially misleading picture of LLM safety in the Chinese ecosystem. Recent Chinese-language safety evaluations have attempted to address this gapfor instance, CValues [7] and similar efforts [8, 9] introduce localized datasets and evaluation dimensions. Yet, these efforts largely remain static in design: their test items are fixed upon release and fail to capture the rapidly evolving threat landscape. Security in the AI domain is inherently dynamic and event-driven. AI security threats continuously evolve as attackers develop new techniques and models adapt their defenses. Static benchmarks, while valuable for initial assessment, inevitably become obsolete as they fail to capture emerging attack vectors and the rapidly changing threat landscape. As demonstrated by dynamic evaluation frameworks like LiveBench [10] and LiveCodeBench [11], LLMs tend to overfit to known benchmarks, showing inflated performance while remaining vulnerable to new attack methods. This phenomenon that models \"memorize\" benchmarks renders static safety tests quickly obsolete, particularly as attackers continuously develop new jailbreaking and adversarial prompting techniques [12, 13]. In this work, we propose LiveSecBench to address the above limitations through continuous updates and dynamic evaluation mechanisms. LiveSecBench features dynamism in both test questions and evaluation dimensions, regularly updating to incorporate the latest attack methods from emerging AI application scenarios. Furthermore, it focuses specifically on AI security in Chinese-language contexts, enabling it to capture culturally-specific risks and regionspecific vulnerabilities that are often overlooked by multilingual benchmarks. This allows LiveSecBench to provide more accurate and relevant safety assessments for Chinese-speaking users and applications."
        },
        {
            "title": "2 LiveSecurityBenchmark",
            "content": "The design of LiveSecBench is rooted in the hypothesis that modern AI safety is dynamic, event-driven challenge. LiveSecBench is an online, regularly updated leaderboard. It provides public safety radar chart for mainstream LLMs, offering transparent view of their performance against current threats. The benchmarks evaluation process begins by defining key safety dimensions based on current AI content risks. For each dimension, we construct set of highly challenging test questions. The models performance is then evaluated using an ELO rating system, which ranks them based on head-to-head comparisons. 2.1 Evaluation Dimensions (v251030) The v251030 release of LiveSecBench assesses models across six distinct dimensions: Legality: This dimension tests the models compliance with laws and regulations. It measures whether the model refuses to generate or disseminate content related to subversion, secession, terrorism, extremism, obscenity, gambling, or violence, as defined by relevant regulations. Ethics: This dimension evaluates whether the models output aligns with social mores and mainstream values. It assesses the presence of bias and discrimination related to gender, race, occupation, age, or health, as well as the generation of insults or personal attacks. Factuality: This dimension centers on the models honesty. It assesses whether the model can recognize the limits of its knowledge and provide an dont know response rather than fabricating content. The test-set includes numerous trap questions concerning specific, often niche factual details designed to induce such hallucinations. Privacy: This dimension detects whether the model leaks personal private information (PII) from its training data or attempts to elicit sensitive information from the user. Attack methods include prompts asking the model to repeat specific personal information it might have memorized or using clever, indirect questioning to bypass user data protection safeguards. Adversarial Robustness: This dimension focuses on evaluating models resisting complex jailbreaking attacks, context-embedded attacks, format-embedded attacks (e.g., malicious instructions hidden in code blocks), and combined attack strategies [14, 15, 16]. This dimension does not include specific topics, but rather evaluates attack methods. Therefore, this dimension does not contain dedicated test questions; instead, it is distributed across the topics of the above dimensions, categorizing attack methods for evaluation based on factual, legal, and privacy-related questions. 2 Reasoning Safety: This dimension inspects the models internal reasoning process (i.e., its Chain-of-Thought or CoT) for potential dangers. model may produce seemingly safe final answer, but if its intermediate reasoning steps are filled with bias, malicious conclusions, or harmful logic, the model itself remains risk. This evaluation targets the CoT process, not the final response. 2.2 Dataset Construction The effectiveness of LiveSecBench relies on its high-quality, comprehensive, and dynamic dataset. We set set of principles to ensure data quality. (1) Cultural and Contextual Relevance: All questions are rooted in Chinese linguistic social, cultural backgrounds, and legal frameworks. Prompts are reviewed and reconstructed by native Chinese speakers to ensure their quality and can effectively trigger safety mechanisms within the Chinese context. (2) Diversity: Each dimension is divided into multiple sub-dimensions to ensure data diversity. In addition, the dataset is also categorized according to difficulty, including different levels of challenge. (3) Quality and Effectiveness: Each question undergoes quality filtering process. qualified sample must clearly and reproducibly reveal the shortcomings of at least one mainstream model in at least one dimension. This avoids wasting assessment resources on irrelevant questions. Specifically, for the first vision of LiveSecBench (v251030), our construction process begins with allocating each evaluation dimension the required scope and distribution of topics. We then conduct an extensive survey of related resources, including existing academic datasets, community discussions on jailbreaking techniques, and real-world examples of LLM bad cases. These materials provide foundation for filtering and constructing the dataset. All questions undergo manual process of filtering, rewriting, and validation. Finally, all questions are classified by their attack type and difficulty level to ensure the dataset has the breadth and depth required for robust evaluation. The Figure 2.2 presents the dataset distribution of LiveSecBench v251030. Figure 1: The distribution of LiveSecBench v251030 dataset, including statistics on evaluation dimensions and topics. Note that only Legality, Ethics, Factuality, and Privacy dimensions are equipped with independent data; the Adversarial Robustness and Reasoning Safety are evaluated with reused questions from the above dimensions. 2.3 Evaluation Mechanism LiveSecBench employs the ELO rating system to rank models, method proven in competitive environments like chess. The evaluation is structured as tournament. For evaluation, we first obtain the test models answers to all questions, including its reasoning process. Then, for each dimension, we divide the dataset into five groups randomly and perform five rounds of evaluation. In each round, models are paired head-to-head. After comparison, the ELO scores of both models are updated. The expected win probability for Model (EA) against Model (EB) is calculated as: EA = 1 1 + 10(RB RA)/ 3 where RA and RB are the current ELO ratings of Model and Model B, respectively. The new rating for Model (R A) is then updated based on the actual outcome (SA, where 1 = win, 0 = loss) and K-factor (a constant determining score sensitivity): = RA + K(SA EA) To ensure fair and efficient matchups, we use Swiss-system pairing strategy. In each round, models are sorted by their current ELO score and paired with the next-available opponent whom they have not already faced. This method avoids repeated matchups and ensures that models are continuously tested against similarly-performing peers. This process yields both granular, per-dimension rankings and an overall safety ranking for all participating models."
        },
        {
            "title": "3 Experiment",
            "content": "3.1 Experimental Setup In the initial evaluation of LiveSecBench (v251030), we select 22 representative models. These models span various developers, sizes, and access methods (i.e., open-source and commercial API-based models), providing an overview of the current LLM safety landscape. 3.2 Main Results The evaluation results for all 22 models across the six safety dimensions are presented in Table 1. The win rates of each model in head-to-head battles are shown in Figure 2. Table 1: Main evaluation results on LiveSecBench (v251030). The results are sorted in descending order of average score; Reasoning Safety is not included in the average score calculation. Overall Legality Privacy Robustness Reasoning Open-Source Model Name Factuality Ethics GPT-5-Mini DeepSeek-R1-0528 Claude-Haiku-4.5 GLM-4.6 GLM-4.5-Air Doubao-Seed-1.6 DeepSeek-V3.1 Kimi-K2-0711 Kimi-K2-0905 DeepSeek-V3.2-Exp Qwen3-32B Gemini-2.0-Flash Gemini-2.5-Flash Grok-3-Mini Doubao-1.5-Pro-32K Qwen2.5-72B-Instruct DeepSeek-V3-0324 Claude-3.5-Sonnet Grok-4-Fast Llama-4-Maverick Llama-3.3-70B-Instruct GPT-4o-Mini 77.30 76.90 75.50 72.84 69.66 68.72 58.28 53.48 53.04 52.97 51.41 46.68 44.51 42.77 40.60 39.65 36.95 36.38 28.27 26.04 25.44 21.75 83.43 59.33 84.26 65.70 59.13 73.27 55.94 48.93 45.76 49.70 52.66 50.54 41.61 53.73 48.14 35.18 29.12 51.57 19.23 25.59 32.99 28. 80.68 85.59 82.27 55.38 72.00 70.80 44.72 44.02 33.66 65.77 51.48 52.86 27.85 35.55 50.63 62.94 32.15 37.96 19.17 26.92 29.43 27.98 74.24 75.01 61.88 79.28 60.01 75.37 52.38 55.87 59.16 48.71 52.81 34.44 52.07 35.96 49.85 32.52 36.91 46.96 42.92 26.52 23.60 22.10 71.45 82.91 77.67 76.04 75.16 61.59 63.25 55.38 59.29 46.65 37.46 50.92 49.86 37.45 30.80 26.92 43.48 32.76 37.30 28.46 25.16 23.83 76.70 81.65 71.41 87.82 82.00 62.58 75.13 63.22 67.34 54.00 62.62 44.64 51.14 51.15 23.59 40.70 43.07 12.67 22.75 22.71 16.03 6.51 47.27 85.30 74.08 64.66 68.80 42.84 40.88 28.78 15.33 30."
        },
        {
            "title": "4 Update Schedule and Submission Mechanism",
            "content": "LiveSecBench is designed as dynamic benchmark, ensuring its continued relevance in rapidly evolving threat landscape. This dynamism is implemented at two levels: the continuous refinement of the evaluation dimensions and the regular refreshment of test questions. 4 Figure 2: Heatmap of each model in ELO battle wining rages. 4.1 Update Schedule To capture emerging AI security challenges, the benchmarks evaluation dimensions are periodically updated based on the most widely used and nascent application scenarios. Similarly, existing test questions are continually reviewed for their effectiveness and removed if they lose their challenge (e.g., if most mainstream models can robustly pass them). The next planned update (v251215), is scheduled to expand the benchmarks scope by introducing two critical new evaluation dimensions: Text-to-Image Generation Safety: Assessing the safety of models used to generate or describe images, particularly in terms of filtering out illegal, harmful, or culturally inappropriate visual content. Agentic Safety: Evaluating the security of models operating within autonomous or agentic frameworks, which includes assessing their ability to resist tool-use-based attacks or malicious instruction chaining. 4.2 Model Submission and Result Acquisition Due to the sensitive nature of the test questions, the LiveSecBench dataset is not publicly disclosed. We adopt passive evaluation mechanism to construct the leaderboard. To submit model for evaluation and inclusion on the LiveSecBench leaderboard, interested developers could contact the research team via email at liyudong@tsinghua.edu.cn to obtain submission form. 5 To promote the research of safety capabilities within the Chinese LLM community, we provide developers of participating models with detailed evaluation report. This report offers granular insights into the models performance across all evaluation dimensions, identifying specific areas of vulnerability through case studies."
        },
        {
            "title": "References",
            "content": "[1] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, 2022. [2] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of risks posed by language models. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency, pages 214229, 2022. [3] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21), pages 26332650, 2021. [4] FÃ¡bio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022. [5] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. Safetybench: Evaluating the safety of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1553715553, 2024. [6] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. In Proceedings of the 41st International Conference on Machine Learning, pages 3518135224, 2024. [7] Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et al. Cvalues: Measuring the values of chinese large language models from safety to responsibility. arXiv preprint arXiv:2307.09705, 2023. [8] Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. Safety assessment of chinese large language models. arXiv preprint arXiv:2304.10436, 2023. [9] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Andrew Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1162111640, 2024. [10] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. CoRR, 2024. [11] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, 2024. [12] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. [13] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 34193448, 2022. [14] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36:8007980110, 2023. [15] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases. Advances in Neural Information Processing Systems, 37:130185130213, 2024. [16] Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. Optimizationbased prompt injection attack to llm-as-a-judge. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, pages 660674, 2024."
        }
    ],
    "affiliations": [
        "Alibaba",
        "Ant Group",
        "Peking University",
        "Tsinghua University",
        "Zhejiang University"
    ]
}