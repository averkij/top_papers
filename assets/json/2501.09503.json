{
    "paper_title": "AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation",
    "authors": [
        "Junjie He",
        "Yuxiang Tuo",
        "Binghui Chen",
        "Chongyang Zhong",
        "Yifeng Geng",
        "Liefeng Bo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an \"encode-then-route\" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ ."
        },
        {
            "title": "Start",
            "content": "AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation Junjie He Yuxiang Tuo Binghui Chen Chongyang Zhong Yifeng Geng Liefeng Bo Institute for Intelligent Computing, Alibaba Tongyi Lab {hejunjie.hjj, yuxiang.tyx}@alibaba-inc.com chenbinghui@bupt.cn {zhongchongyang.zzy, cangyu.gyf, liefeng.bo}@alibaba-inc.com 5 2 0 J 6 1 ] . [ 1 3 0 5 9 0 . 1 0 5 2 : r Figure 1. Example generations from AnyStory. Our approach demonstrates excellence in preserving subject details, aligning text descriptions, and personalizing multiple subjects. Here, the image with plain white background serves as the reference. For more examples, please refer to Fig. 7 and Fig. 8."
        },
        {
            "title": "Abstract",
            "content": "Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an encodethen-route manner. In the encoding step, AnyStory utilizes universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve highIn the routing step, fidelity encoding of subject features. AnyStory utilizes decoupled instance-aware subject router to accurately perceive and predict the potential location of 1 the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at https://aigcdesigngroup.github. io/AnyStory/. 1. Introduction Recently, with the rapid development of diffusion models [13, 26, 57, 58], many large generative models [3, 7, 8, 42, 43, 47, 49, 51] have demonstrated remarkable textto-image generation capabilities. However, generating personalized images with specific subjects still presents challenges. Early efforts [2, 6, 14, 20, 27, 37, 52] utilize finetuning at test time to achieve personalized content generation. These methods require extensive fine-tuning time and their generalization ability is limited by the number and diversity of tuning images. Recent works [16, 39, 41, 46, 56, 64, 66, 67, 69, 70] have explored zero-shot settings. They have introduced specialized subject encoders to retrain text-to-image models on large-scale personalized image datasets, without the need for model fine-tuning at test time. However, these methods are either limited by the encoders capability to provide high-fidelity subject details [39, 46, 56, 66, 69, 70], or focus on specific categories of objects (such as face identities [41, 64, 67]) and cannot extend to general subjects (such as human clothing, accessories, and non-human entities), limiting their applicability. In addition, previous methods mainly focus on singlesubject personalization. Problems with subject blending often occur in multi-subject generation due to semantic leakage [10, 67]. Some methods [18, 31, 34, 38, 44, 46, 65, 71] address this issue by introducing pre-defined subject masks, but this restricts the diversity and creativity of generative models. Additionally, providing precise masks for subjects with complex interactions and occlusions is difficult. Recent research, i.e., UniPortrait [23], proposes subject router to adaptively perceive and constrain the effect region of each subject condition in the diffusion denoising process. However, the routing features used by UniPortrait are highly coupled with subject identity features, limiting the accuracy and flexibility of the routing module. Furthermore, it primarily focuses on the domain of face identity and does not consider the impact of subject conditions on the background. In this paper, we propose AnyStory, unified singleand multi-subject personalization framework. We aim to personalize general subjects while achieving fine-grained control over multi-subject conditions. Additionally, we aim to allow the variation of subjects backgrounds, poses, and views through text prompts while maintaining subject details, thus creating complex and fantastical narratives. To achieve this, we have introduced two key modules, i.e., an enhanced subject representation encoder and decoupled instance-aware subject router. To be specific, we adopt the encode-then-route design of UniPortrait. In order to achieve general subject representation, we abandon domain-specific expert models, such as the face encoders [12, 29], and instead use powerful and versatile model, i.e., ReferenceNet [28], combined with the CLIP vision encoder [50] to encode the subject. CLIP vision encoder is responsible for encoding the subjects coarse concepts, while ReferenceNet is responsible for encoding the appearance details to enhance subject fidelity. To improve efficiency, we also simplify the architecture of ReferenceNet, skipping all cross-attention layers to save storage and computation costs. In order to avoid the copy-paste effect, we further collect large amount of paired subject data, which is sourced from image, video, and 3D rendering databases. These paired data contain instances of the same subject in different contexts, effectively aiding the encoder in understanding and encoding provided subject concepts. For the subject router, in contrast to UniPortrait, we implement separate branch to allow for specialized and flexible routing guidance. Additionally, we improve the structure of the routing module by modeling it as mini-image segmentation decoder, with masked crossattention [9, 22] and background routing representation being introduced. Combined with instance-aware routing regularization loss, the proposed router can accurately perceive and predict the potential location of the corresponding subject in the latent during the denoising process. In practice, we observe that the behavior of this enhanced subject router is similar to image instance segmentation, which may provide potential approach for image-prompted visual subject segmentation. The experimental results demonstrate the outstanding performance of our method in preserving the fidelity of the subject details, aligning text descriptions, and personalizing for multiple subjects. Our contributions can be summarized as follows: We propose unified singleand multi-subject personIt achieves conalization framework called AnyStory. sistency in personalizing both single-subject and multisubject while adhering to text prompts; We introduce an enhanced subject representation encoder, composed of simplified lightweight ReferenceNet and CLIP vision encoder, capable of high-fidelity detail encoding for general subjects. We propose decoupled instance-aware routing module that can accurately perceive and predict the potential conditioning areas of the subject, thereby achieving flexible and controllable personalized generation of single or multiple subjects. 2 2. Related Work Single-subject personalization. Personalized image generation with specific subjects is popular and challenging topic in text-to-image generation. Early works [2, 6, 14, 15, 20, 27, 37, 52, 61] rely on fine-tuning during testing. These methods typically require several minutes to even hours to achieve satisfactory results, and their generalization abilities are limited by the number of fine-tuned images. Recently, some methods [16, 32, 39, 46, 56, 59, 66, 69] have sought to achieve personalized image generation for subjects without additional fine-tuning. IP-Adapter [69] encodes subjects into text-compatible image prompts for subject personalization. BLIP-Diffusion [39] introduces pretrained multimodal encoder to provide subject representation. SSR-Encoder [70] proposes token-to-patch aligner and detail-preserved subject encoder to learn selective subject embedding. FaceStudio [68], InstantID [64], and PhotoMaker [41] utilize face embeddings derived from face encoders as the condition. Although these methods have made progress, they are either limited by the ability of the image encoder to preserve subject details [16, 39, 46, 56, 66, 69, 70], or focus on specific domains, e.g., face identity, without the ability to generalize to other objects [19, 23, 41, 64, 68]. Multi-subject personalization. Significant progress has been made in single-subject personalization. However, the personalized generation of multi-subject images still presents challenges due to the problem of subject blending [10, 67]. To overcome these challenges, recent studies [18, 34, 38, 44, 46, 65, 71] have utilized predefined layout masks to guide multi-subject generation. However, these layout-dependent methods limit the creativity of the generation models and the diversity of resulting images. Additionally, providing precise layout masks for each subject in complex contexts is challenging. Some methods obtain subject masks from attention maps corresponding to subject tokens [4, 5, 24, 55, 60, 62] or from segmentation of existing images [21, 36], which may result in inaccurate masks for the target subject instances. FastComposer [67], Subject-Diffusion [46], and StoryMaker [73] impose constraints on cross-attention maps for different subjects during training, but this impacts the injection of subject conditions. Recently, UniPortrait [23] introduces subject router to perceive and predict subject potential positions during denoising, avoiding blending adaptively. However, its routing features are highly coupled with subject features, limiting the precision of the routing module. Story visualization. Generating visual narratives based on given scripts, known as story visualization [21, 60, 72, 73], is rapidly evolving. StoryDiffusion [72] proposes consistent self-attention calculation to ensure the consistency of characters throughout the story sequence. ConsiStory [60] proposes training-free approach that shares the internal activations of the pre-trained diffusion model to achieve subject consistency. DreamStory [21] utilizes Large Language Model (LLM) and multi-subject consistent diffusion model, incorporating masked mutual self-attention and masked mutual cross-attention modules, to generate consistent multi-subject story scenes. The proposed method in this paper achieves subject consistency in image sequence generation through routed subject conditioning. 3. Methods We introduce AnyStory, pioneering method for unified singleand multi-subject personalization in text-to-image generation. We first briefly review the background of the diffusion model in Sec. 3.1, and then detail the two proposed key components, i.e., the enhanced subject encoder and the decoupled instance-aware subject router, in Sec. 3.2 and Sec. 3.3, respectively. Finally, we outline our training scheme in Sec. 3.4. The framework of our method is illustrated in Fig. 2. 3.1. Preliminary The underlying text-to-image model we used in this paper is Stable Diffusion XL (SDXL) [49]. SDXL takes text prompt as input and produces the image x0. It contains three modules: an autoencoder (E(), D()), CLIP text encoder τ (), and U-Net ϵθ(). Typically, it is trained using the following diffusion loss: Ldif = Ez0,P,ϵN (0,1),t[ϵ ϵθ(zt, t, τ (P ))2 2] (1) where ϵ (0, 1) is the sampled Gaussian noise, is the time step, z0 = E(x0) is the latent code of x0, and zt is computed by zt = αtz0 + σtϵ with the coefficients αt and σt provided by the noise scheduler. 3.2. Enhanced subject representation encoding Personalizing subject images in an open domain while ensuring fidelity to subject details and textual descriptions remains an unresolved issue. key challenge lies in the encoding of subject information, which requires maximal preservation of subject characteristics while maintaining certain level of editing capability. Current mainstream methods [16, 39, 44, 46, 56, 66, 69, 70] largely rely on CLIP vision encoder to encode subjects. However, CLIPs features are primarily semantic (for the reason of contrastive image-text training paradigm) and of low-resolution (typically 224 224), thus limited to providing thorough details of the subjects. Alternative approaches [19, 41, 48, 64] incorporate domain-specific expert models, such as face encoders [12, 29], to enhance subject identity representation. Despite their success, they are limited in their domain and are not extendable to general subjects. To address these issues, we introduce ReferenceNet [28], powerful and versatile image encoder, to encode the subject in conjunction 3 Figure 2. Overview of AnyStory framework. AnyStory follows the encode-then-route conditional generation paradigm. It first utilizes simplified ReferenceNet combined with CLIP vision encoder to encode the subject (Sec. 3.2), and then employs decoupled instanceaware subject router to guide the subject condition injection (Sec. 3.3). The training process is divided into two stages: the subject encoder training stage and the router training stage (Sec. 3.4). For brevity, we omit the text conditional branch here. with the CLIP vision encoder. ReferenceNet utilizes variational autoencoder (VAE) [35, 49] to encode reference images and then extracts their features through network with the same architecture as U-Net. It boasts three prominent advantages: (1) it supports higher resolution inputs, thereby enabling it to retain more subject details; (2) it has feature space aligned with the denoising U-Net, facilitating the direct extraction of subject features at different depths and scales by U-Net; (3) it uses pre-trained U-Net weights for initialization, which possess wealth of visual priors and demonstrate good generalization ability for learning general subject concepts. CLIP encoding. Following previous approaches [23, 69], we utilize the hidden states from the penultimate layer of the CLIP image encoder, which align well with image captions, as rough visual concept representation of the subject. We first segment the subject area in the reference image to remove background information, and then input the segmented image into the CLIP image encoder to obtain 257-length patch-level feature Fclip. Subsequently, we compress Fclip using QFormer [1, 40] into tokens. The final result, denoted as E, serves as the subject representation derived from the CLIP vision encoder: (2) = QFormer(Fclip), where Rmdc, and dc is the same as the text feature dimension in the pre-trained diffusion model. Empirically, we set to 64 in our experiments. ReferenceNet encoding. implementation [28], ReferenceNet adopts the same architecture as U-Net, including cross-attention blocks with text condition injection. However, since ReferenceNet is only used as visual feature extractor in our task and does not require text condition injection, we skip all cross-attention"
        },
        {
            "title": "In the original",
            "content": "Architecture #Params (B) Speed (ms/img) Original ReferenceNet [28] Simplified ReferenceNet 2.57 2.02 62.0 53. Table 1. Statistics of the simplified ReferenceNet. The speed is measured on an A100 GPU with batch size of 1 and an input (latent) resolution of 64 64. blocks, reducing the number of parameters and computational complexity (see Table 1). Additionally, in order to label the subject areas, we also add subject mask channel to the input of ReferenceNet. Specifically, we feed the segmented subject reference to the VAE encoder for encoding and then concatenate the encoded result with the downsampled subject mask to obtain Fvae. Next, Fvae undergoes the reduced non-cross attention ReferenceNet. The hidden states of each self-attention layer, denoted as {GlGl RhGwGd}l, are extracted as the ReferenceNetencoded representation of the subject: {Gl}L l=2 = ReferenceNet(Fvae), (3) where denotes the layer index, and denotes the total number of layers. Here, we ignore the features of the first self-attention layer in order to better align with the routing module (see Sec. 3.3). 3.3. Decoupled instance-aware subject routing The injection of subject conditions requires careful consideration of injecting positions to avoid the influence on unrelated targets. Previous methods [37, 39, 56, 66, 69, 70] have typically injected the conditional features into the latent through naive attention module. However, due to the soft weighting mechanism, these approaches are prone to semantic leakage [10, 67], leading to subject characteristics 4 blending, especially in the generation of instances with similar appearance. Some methods [18, 34, 38, 44, 46, 65, 71] introduce predefined layout masks to address this issue, but this limits their practical applicability. UniPortrait [23] proposes router to perceive and confine the effect region of subject conditions adaptively; however, its routing features are completely coupled with subject features, which limits the ability of the routing module; also, it does not consider the impact of subject conditions on the background. In this study, we propose decoupled instance-aware subject routing module, which can accurately and effectively route subject features to the corresponding areas while reducing the impact on unrelated areas. Decoupled routing mechanism. Different from UniPortrait [23], we employ an independent branch to specifically predict the potential location of subjects in the latent during the denoising process. As depicted in the Fig. 2, given series of segmented subject images, they are respectively passed through CLIP image encoder and an additional one-query QFormer to obtain the routing features {RiRi Rdr }N i=1, where represents the number of reference subjects. Particularly, we include an additional routing feature RN +1 for the background (zero image as input) to further confine the subjects conditioning areas. The idea behind this is to alleviate the undesirable biases inherent in subject features on the generated image backgrounds (e.g., we used large amount of pure white background data from 3D rendering to train the subject encoder). To accurately route the subjects to their respective positions, we employ an image segmentation decoder [9, 22] to model the router. Specifically, in each cross-attention layer of the U-Net, we first predict coarse routing map by taking the linearly projected inner product of {Ri}N +1 i=1 and Zl. Here, Zl Rhwd represents the latent features in the l-th layer. Subsequently, we refine the routing features {Ri}N +1 i=1 using masked cross attention [9] with the latent feature Zl, where the coarse routing map serves as the attention mask. The updated routing features are then subjected to the projected inner product with Zl again to obtain the refined routing maps {Ml i=1 are finally used to guide the injection of information related to the subjects at that layer. For detailed structure of the router, please refer to the right half of Fig. 2. Instance-aware routing regularization loss. In order to facilitate router learning and to differentiate between different instances of the subjects, we introduce an instanceaware routing regularization loss. The loss function is defined as: [0, 1]hw}N +1 i=1 . {Ml i}N +1 Ml Ll route = λ"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Ml Mgt 2 2 (4) we consider the entire subject instance, such as the full human body, as the routing target, regardless of whether the input subject has been cropped. Routing-guided subject information injection. For CLIP encoded subject representations, we use the decoupled cross attention [69] to incorporate them into the U-Net, but with additional routing-guided localization constraints: ˆZl = Softmax( QKT )V + η +1 (cid:88) i=1 σ(Ml i) Softmax( (5) ˆKl ) ˆVl i, = Ei ˆWl = Ei ˆWl q, = CWl k, and = CWl where = ZlWl represent the query, key, and value matrices for text conditions, represents text embeddings, ˆKl and ˆVl represent the key and value matrices for the CLIP-encoded i-th subject, ˆWl and ˆWl are both trainable parameters, σ(Ml i) represents the 0-1 version of Ml after operations argmax and one-hot for {Ml i}i over dimension, represents element-wise multiplication, and η represents the strength of conditions. It should be noted that here we also include an additional background representation, i.e., EN +1, from CLIP, which similarly corresponds to zero-valued image inputs. This embedding (EN +1) is also utilized as an unconditional embedding during classifierfree guidance sampling training [25]. In regard to the injection of ReferenceNet encoded subject features, we adopt the original reference attention [28] but with an additional attention mask induced from routing maps. With slight abuse of notation, this process can be formulated as follows: Zl = Softmax( Q[K, Kl ] 1, , Kl }i, γ))[V, Vl (6) + Bias({Ml1 1, , Vl ], Wl = Gl q, = ZlWl k, and = ZlWl where = ZlWl represent the query, key, and value matrices for self-attention, Wl Kl indicate the key and value matrices for the ReferenceNet-encoded features of the i-th subject at the l-th layer, [] represents concat operation, Bias({Ml1 }i, γ) represents the applied attention bias, and Vl = Gl 1 Bias({Ml1 ) + γ, , g(Ml }i, γ) = [0, g(Ml1 ) + γ], (7) where γ controls the overall strength of ReferenceNet conditions, g(Ml1 ) {0, }hwhGwG represents the attention bias derived from the routing maps of the preceding cross-attention layer, and its specific calculation is as follows: where Mgt {0, 1}hw represents the downsampled ground truth mask of the i-th subject in the target image. Typically, g(Ml1 )u,v = 5 (cid:26) 0 if σ(Ml1 otherwise )u = 1 . (8) Figure 3. Effect of ReferenceNet encoding. The ReferenceNet encoder enhances the preservation of subject details. Similar to UniPortrait, to ensure proper gradient backpropagation through σ() during training, we employ the Gumbelsoftmax trick [30]. In practice, we observed that the routing map behaves similarly to the instance segmentation mask, providing potential method for reference-prompted image segmentation (first encode the image with VAE, then feed the encoded image and reference into denoising U-Net and router respectively to predict the masks, see Fig. 5). 3.4. Training Following UniPortrait, the training process of AnyStory is divided into two stages: subject encoder training stage and router training stage. Subject encoder training. We train the subject QFormer, ReferenceNet, and corresponding key, value matrices in attention blocks. The ReferenceNet utilizes pre-trained UNet weights for initialization. To avoid the copy-paste effect caused by fine-grained encoding of subject features, we collect large amount of paired data that maintains consistent subject identity while displaying variations in background, pose, and views. These data are sourced from image, video, and 3D rendering databases, captioned by Qwen2-VL [63]. Specifically, the image (about 410k) and video (about 520k) data primarily originate from human-centric datasets such as DeepFashion2 [17] and human dancing videos, while the 3D data (about 5,600k) is obtained from the Objaverse [11], where images of objects from seven different perspectives are rendered as paired data. During the training process, one image from these pairs is utilized as the reference input, while another image, depicting the same subject identity but in different context, serves as the prediction target. Additionally, data augmentation techniques, including random rotation, cropping, and zero-padding, are applied to the reference image to further prevent subject overfitting. The Figure 4. The effectiveness of the router. The router restricts the influence areas of the subject conditions, thereby avoiding the blending of characteristics between multiple subjects and improving the quality of the generated images. training loss in this stage is the same as the original diffusion loss, as shown in Eq. 1. Router training. We fix the subject encoder and train the router. The primary training data consists of an additional 300k unpaired multi-person images from LAION [53, 54]. Surprisingly, despite the training dataset of the router being predominantly focused on human images, it is able to effectively generalize to general subjects. We attribute this to the powerful generalization ability of the CLIP model and the highly compressed single-token routing features. The training loss for this stage includes the diffusion loss (Eq. 1) and the routing regularization loss (Eq. 4), with the balancing parameter λ set to 0.1. 4. Experiments 4.1. Setup We use the stable diffusion XL [49] as the base model. The CLIP image encoder employed is the OpenAIs clip-vit-huge-patch14. Both the subject QFormer and the routing QFormer consist of 4 layers. The input image resolution for ReferenceNet is 512512. All training is conducted on 8 A100 GPUs with batch size of 64, utilizing the AdamW [45] optimizer with learning rate of 1e-4. In order to facilitate classifier-free guidance sampling [25], we drop the CLIP subject conditioning during training on 10% of the images. During the inference process, we employ 25 6 Figure 5. Visualization of routing maps. We visualize the routing maps within each cross-attention layer in the U-Net at different diffusion time steps. There are total of 70 cross-attention layers in the SDXL U-Net, and we sequentially display them in each subfigure in top-to-bottom and left-to-right order (yellow represents the effective region). We utilize = 25 steps of EDM sampling. Each complete row corresponds to one entity. The background routing map has been ignored, which is the complement of the routing maps of all subjects. Best viewed in color and zoomed in. steps of EDM [33] sampling and 7.5 classifier-free guidance scale, and to achieve more realistic image generation, we employ the RealVisXL V4.0 model from huggingface. 4.2. Effect of ReferenceNet encoder Fig. 3 illustrates the effectiveness of the ReferenceNet encoder, which enhances the preservation of fine details in the subject compared to using only CLIP vision encoder. However, it is also evident that using ReferenceNet alone does not yield satisfactory results. In fact, in our extensive testing, we found that the ReferenceNet encoder only achieves alignment of the subject details and does not guide subject generation. We still need to rely on CLIP-encoded features, which are well-aligned with text embeddings, to trigger subject generation. 4.3. Effect of the decoupled instance-aware router Fig. 4 demonstrates the effectiveness of the proposed router, which can effectively avoid feature blending between subjects in multi-subject generation. Additionally, we observe that the use of the router in single-subject settings also improves the quality of generated images, particularly in the image background. This is because the router restricts the influence area of subject conditions, thereby reducing the potential bias inherent in subject features (e.g., simple white background preference learned from large amount of 3D rendering data) on the quality of generated images. Fig. 5 visualizes the routing maps of the diffusion model at different time steps during the denoising process. These results demonstrate that the proposed router can accurately 7 (a) Coarse routing maps Figure 6. Effectiveness of the proposed router structure. For the meaning of each illustration, please refer to Fig. 5. (b) Refined routing maps ject condition injection. Experimental results demonstrate that our method excels in retaining subject details, aligning with textual descriptions, and personalizing for multiple subjects. Limitations and future work. Currently, AnyStory is unable to generate personalized backgrounds for images. However, maintaining consistency in the image background is equally important in sequential image generation. In the future, we will expand AnyStorys control capabilities from the subject domain to the background domain. Additionally, the copy-paste effect still exists in the subjects generated by AnyStory, and we aim to mitigate this further in the future through data augmentation and the use of more powerful text-to-image generation models. perceive and locate the effect regions of each subject condition during the denoising process. The displayed routing maps are similar to image segmentation masks, indicating the potential for achieving guided image segmentation based on reference images through denoising U-Net and trained routers. Additionally, as mentioned in Sec. 3.4, despite our router being trained predominantly on humancentric datasets, it generalizes well to general subjects such as the cartoon dinosaur in Fig. 5. We attribute this to the powerful generalization capability of the CLIP model and the highly compressed single-token routing features. Fig. 6 demonstrates the effectiveness of modeling the router as miniature image segmentation decoder. Compared to the coarse routing map obtained by simple dot product, the refined routing map through lightweight masked cross-attention module can more accurately predict the potential position of each subject. 4.4. Example generations In Fig. 1, Fig. 7, and Fig. 8, we visualize further results of our approach, demonstrating its outstanding performance in preserving subject details, aligning text prompts, and enabling multi-subject personalization. 5. Conclusion We propose AnyStory, unified method for personalized generation of both single and multiple subjects. AnyStory utilizes universal and powerful ReferenceNet in addition to CLIP vision encoder to achieve high-fidelity subject encoding, and employs decoupled, instance-aware routing module for flexible and accurate single/multiple subFigure 7. Example generations II from AnyStory. 9 Figure 8. Example generations III from AnyStory."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 35: 2371623736, 2022. 4 [2] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from single image. In SIGGRAPH Asia 2023 Conference Papers, pages 112, 2023. 2, 3 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 2 [4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In ICCV, pages 2256022570, 2023. 3 [5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM TOG, 42(4):110, 2023. 3 [6] Hong Chen, Yipeng Zhang, Simin Wu, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Identitypreserving disentangled tuning for subject-driven text-toimage generation. arXiv preprint arXiv:2305.03374, 2023. 2, [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. PixArt-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [8] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-Σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, pages 7491. Springer, 2025. 2 [9] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask In CVPR, transformer for universal image segmentation. pages 12901299, 2022. 2, 5 [10] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multisubject text-to-image generation. In ECCV, pages 432448. Springer, 2025. 2, 3, 4 [11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, pages 13142 13153, 2023. 6 [12] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, pages 46904699, 2019. 2, 3 [13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. [14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023. 2, 3 [15] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Designing an encoder In Sigfor fast personalization of text-to-image models. graph, 2023. 3 [16] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM TOG, 42(4):113, 2023. 2, 3 [17] Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. Deepfashion2: versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images. In CVPR, pages 53375345, 2019. 6 [18] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. NeurIPS, 36, 2024. 2, 3, 5 [19] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Pulid: Pure and lightning id arXiv preprint Zhang, and Qian He. customization via contrastive alignment. arXiv:2404.16022, 2024. [20] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact paarXiv preprint rameter space for diffusion fine-tuning. arXiv:2303.11305, 2023. 2, 3 [21] Huiguo He, Huan Yang, Zixi Tuo, Yuan Zhou, Qiuyue Wang, Yuhang Zhang, Zeyu Liu, Wenhao Huang, Hongyang Chao, and Jian Yin. Dreamstory: Open-domain story visualization by llm-guided multi-subject consistent diffusion. arXiv preprint arXiv:2407.12899, 2024. 3 [22] Junjie He, Pengyu Li, Yifeng Geng, and Xuansong Xie. Fastinst: simple query-based model for real-time instance segmentation. In CVPR, pages 2366323672, 2023. 2, 5 [23] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identity-preserving singlearXiv preprint and multi-human image personalization. arXiv:2408.05939, 2024. 2, 3, 4, 5 [24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. ICLR, 2023. [25] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5, 6 [26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 2 [27] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. 2, 3 [28] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In CVPR, pages 81538163, 2024. 2, 3, 4, 5 11 [29] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In CVPR, pages 59015910, 2020. 2, 3 [30] Eric Jang, Shixiang Gu, and Ben Poole. reparameterization with gumbel-softmax. arXiv:1611.01144, 2016. 6 Categorical arXiv preprint [31] Sangwon Jang, Jaehyeong Jo, Kimin Lee, and Sung Ju Identity decoupling for multi-subject perarXiv preprint text-to-image models. Hwang. sonalization of arXiv:2404.04243, 2024. 2 [32] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. [33] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. NeurIPS, 35:2656526577, 2022. 7 [34] Chanran Kim, Jeongin Lee, Shichang Joung, Bongmo Kim, Instantfamily: Masked attention arXiv preprint and Yeul-Min Baek. for zero-shot multi-id image generation. arXiv:2404.19427, 2024. 2, 3, 5 [35] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [36] Zhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, and Wenhan Luo. Omg: Occlusion-friendly personalized multiarXiv preprint concept generation in diffusion models. arXiv:2403.10983, 2024. [37] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, pages 19311941, 2023. 2, 3, 4 [38] Gihyun Kwon, Simon Jenni, Dingzeyu Li, Joon-Young Lee, Jong Chul Ye, and Fabian Caba Heilbron. Concept weaver: Enabling multi-concept fusion in text-to-image models. In CVPR, pages 88808889, 2024. 2, 3, 5 [39] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. NeurIPS, 36, 2024. 2, 3, 4 [40] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 4 [41] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing reIn CVPR, alistic human photos via stacked id embedding. pages 86408650, 2024. 2, 3 [42] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [43] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao 12 Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. 2 [44] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. arXiv preprint arXiv:2305.19327, 2023. 2, 3, 5 [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [46] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image arXiv preprint generation without test-time fine-tuning. arXiv:2307.11410, 2023. 2, 3, [47] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 2 [48] Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji. Portraitbooth: versatile portrait model for fast identity-preserved personalization. In CVPR, pages 2708027090, 2024. 3 [49] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 3, 4, 6 [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 2 [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 2 [52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. 2, 3 [53] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 6 [54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, pages 2527825294, 2022. 6 [55] Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, and Yu Liu. Rethinking the spatial inconsistency in classifierfree diffusion guidance. In CVPR, pages 93709379, 2024. [56] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testIn CVPR, pages 85438552, 2024. 2, 3, time finetuning. 4 [71] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. In CVPR, pages 68186828, 2024. 2, 3, 5 [72] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. arXiv preprint arXiv:2405.01434, 2024. 3 [73] Zhengguang Zhou, Jing Li, Huaxia Li, Nemo Chen, and Xu Tang. Storymaker: Towards holistic consistent characters in text-to-image generation. arXiv preprint arXiv:2409.12576, 2024. 3 [57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In ICML, pages 2256 nonequilibrium thermodynamics. 2265, 2015. 2 [58] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS, 32, 2019. 2 [59] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 2024. [60] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Training-free consistent text-to-image generation. ACM TOG, 43(4):118, 2024. 3 [61] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation. arXiv preprint arXiv:2303.09522, 2023. 3 [62] Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin Zhou, Qian Yu, Lu Sheng, and Dong Xu. Diffusion model is secretly training-free open vocabulary semantic segmenter. arXiv preprint arXiv:2309.02773, 2023. 3 [63] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [64] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 2, 3 [65] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In CVPR, pages 62326242, 2024. 2, 3, [66] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023. 2, 3, 4 [67] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multiarXiv subject image generation with localized attention. preprint arXiv:2305.10431, 2023. 2, 3, 4 [68] Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Facestudio: arXiv preprint Zhang, Pei Cheng, Gang Yu, and Bin Fu. Put your face everywhere in seconds. arXiv:2312.02663, 2023. 3 [69] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 4, 5 [70] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, pages 80698078, 2024. 2, 3,"
        },
        {
            "title": "Appendix",
            "content": "A. Referenced subject images and URLs This section consolidates the sources of the referenced subject images in this paper. We extend our gratitude to the owners of these images for sharing their valuable assets."
        },
        {
            "title": "URL",
            "content": "https://pixabay.com/illustrations/ai-generated-dwarf-story-fantasy-8697130/ https://pixabay.com/illustrations/girl-coat-night-night-city-8836068/ https://pixabay.com/vectors/man-warrior-art-character-cartoon-9093563/ https://pixabay.com/photos/mario-figure-game-nintendo-super-1558068/ https://pixabay.com/illustrations/panda-cartoon-2d-art-character-7918136/ https://pixabay.com/illustrations/avocado-food-fruit-6931344/ https://pixabay.com/vectors/guy-anime-cartoon-chibi-character-7330732/ https://pixabay.com/vectors/guy-anime-cartoon-chibi-character-7330788/ https://pixabay.com/photos/young-male-man-japanese-anime-3815077/ https://pixabay.com/photos/young-male-man-japanese-anime-3816557/ https://pixabay.com/illustrations/shark-jaws-fish-animal-marine-life-2317422/ https://unsplash.com/photos/white-egg-with-face-illustration-WtolM5hsj https://pixabay.com/vectors/alligator-crocodile-suit-cartoon-576481/ https://pixabay.com/illustrations/snowman-winter-christmas-time-snow-7583640/ https://pixabay.com/illustrations/monster-cartoon-funny-creature-8534186/ https://unsplash.com/photos/a-cartoon-character-wearing-a-face-mask-andrunning-6-adg66qleM"
        },
        {
            "title": "URL",
            "content": "https://pixabay.com/illustrations/car-vehicle-drive-transportation-8316057/ https://pixabay.com/vectors/camel-desert-two-humped-animal-7751098/ https://pixabay.com/illustrations/cartoon-samurai-characters-4790355/ https://pixabay.com/illustrations/caveman-prehistoric-character-9211043/ https://pixabay.com/illustrations/boy-walk-nature-anime-smile-8350034/ https://pixabay.com/illustrations/fish-jaw-angry-cartoon-parrot-fish-1402423/ https://pixabay.com/illustrations/fish-telescope-fish-cartoon-1450768/ https://pixabay.com/vectors/cat-pet-animal-kitty-kitten-cute-6484941/ https://pixabay.com/vectors/child-costume-bee-character-8320341/ https://pixabay.com/vectors/guy-anime-cartoon-chibi-character-7330758/ https://pixabay.com/vectors/girl-anime-chibi-cartoon-character-7346667/ https://unsplash.com/photos/white-and-blue-cat-figurine-u3ZUSIH_eis https://unsplash.com/photos/sock-monkey-plush-toy-on-brown-panel-5INN0oj12u4 https://pixabay.com/illustrations/karate-fighter-cartoon-character-8537724/ https://pixabay.com/illustrations/ai-generated-giraffe-doctor-8647702/ https://pixabay.com/illustrations/ai-generated-skull-character-8124354/ https://unsplash.com/photos/a-red-robot-is-standing-on-a-pink-backgroundunt3066GV-E https://pixabay.com/illustrations/cartoon-dinosaur-dragon-animal-8539364/ https://pixabay.com/illustrations/man-book-read-hanfu-chinese-hanfu-7364886/"
        },
        {
            "title": "URL",
            "content": "https://pixabay.com/vectors/muslim-hijab-child-cartoon-doodle-7747745/ https://pixabay.com/illustrations/tambourine-musician-woman-character-9073083/ https://pixabay.com/illustrations/ai-generated-man-agent-character-9050849/ https://pixabay.com/illustrations/ai-generated-superhero-hero-heroine-7977051/ https://unsplash.com/photos/a-woman-in-a-tan-jacket-and-tan-pants-QVyAUDUOlMw https://unsplash.com/photos/a-woman-in-a-yellow-shirt-and-black-pantsrdHrrFA1KKg https://pixabay.com/vectors/fashion-boy-cartoon-spring-summer-8515751/ https://pixabay.com/illustrations/woman-girl-fashion-model-female-8859569/ https://pixabay.com/illustrations/woman-cartoon-character-anime-8926994/ https://pixabay.com/photos/apple-red-delicious-fruit-vitamins-256268/ tps://pixabay.com/photos/apple-food-fresh-fruit-green-1239300/ https://pixabay.com/illustrations/fox-animal-wildlife-wild-mammal-9267914/ https://pixabay.com/illustrations/christmas-deer-animal-rudolph-8380345/ https://pixabay.com/illustrations/ai-generated-man-portrait-7953120/ https://pixabay.com/illustrations/created-by-ai-hedgehog-cartoon-8635844/ https://pixabay.com/vectors/dragon-creature-baby-dragon-8480029/ https://pixabay.com/vectors/boy-cartoon-fashion-chibi-kawaii-8515729/ https://pixabay.com/vectors/blonde-boy-cartoon-character-comic-1300066/"
        }
    ],
    "affiliations": [
        "Institute for Intelligent Computing, Alibaba Tongyi Lab"
    ]
}