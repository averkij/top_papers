{
    "paper_title": "Higher-order Linear Attention",
    "authors": [
        "Yifan Zhang",
        "Zhen Qin",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any $n \\times n$ matrices. We give closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 8 5 2 7 2 . 0 1 5 2 : r Higher-order Linear Attention Yifan Zhang1 1Princeton University Zhen Qin Quanquan Gu2 2University of California, Los Angeles yifzhang@princeton.edu qgu@cs.ucla.edu October 30,"
        },
        {
            "title": "Abstract",
            "content": "The quadratic cost of scaled dot-product attention is central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains constant-size state and computes per-token outputs in linear time without materializing any nn matrices. We give closed-form streaming identities, strictly causal masked variant using two additional summaries, and chunk-parallel training scheme based on associative scans that reproduces the activations of serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA"
        },
        {
            "title": "1 Introduction",
            "content": "The Transformer architecture (Vaswani et al., 2017), powered by scaled dot-product attention, underpins modern large language models (LLMs). Yet the O(n2) computational and memory complexity in sequence length constrains long-context use. rich line of work therefore explores more efficient attention mechanisms (e.g., Linear Attention (Katharopoulos et al., 2020; Wang et al., 2020; Choromanski et al., 2020; Schlag et al., 2021; Sun et al., 2023; Qin et al., 2023; Yang et al., 2023; Qin et al., 2024; Yang et al., 2024b; von Oswald et al., 2025)), Modern Recurrent Neural Networks (RNNs) (Peng et al., 2024; Sun et al., 2024; Peng et al., 2025), Fast Weight Programmers (Delta Networks, Schlag et al. (2021)), State Space Models (SSMs) (Gu et al., 2021; Gu and Dao, 2023; Dao and Gu, 2024) and Memory Networks (Behrouz et al., 2024, 2025a,b), which admit O(1) per-token state updates at inference. We propose Higher-order Linear Attention (HLA), generalizing linear attention by incorporating higher interactions through compact prefix summaries (sufficient statistics). The key observation is that higher attention-like operators admit factorized forms in terms of low-order moments (e.g., sums of key outer products), enabling exact causal streaming without constructing attention matrices. In the second-order case, HLA maintains an constant-size state per head and produces outputs in 1 linear time per token (O(d2+d dv), here is the query/key dimension and dv the value dimension, independent to the sequence length). We address two central challenges: (i) enforcing strict autoregressive causality at second-order without sacrificing streaming updates or introducing any nn intermediates; and (ii) enabling chunk-parallel training that exactly matches the activations of serial recurrence. First, we derive an exact masked formulation that enforces strict autoregressive causality by augmenting the state with two additional summaries; the resulting algorithm remains streaming and efficient. Second, we present chunk-parallel training scheme based on an associative (monoid/semidirect-product) operator that yields the same activations as serial loop while exploiting intraand inter-chunk parallelism. Our contributions are summarized as follows: 1. Exact masked streaming at second order. We give complete algebra of extended summaries that yields strictly causal second-order HLA with per-token constant cost, together with formal statements and proofs establishing masked streaming identities and online updates. The unnormalized HLA is the default operator; the ratio-normalized variant is an option built from the same summaries. 2. Associative scans that match serial activations. We define an associative (semidirect-product) operator for unmasked and masked settings (with and without exponential decay) and prove that standard exclusive scan produces forward activations identical to those of serial recurrence. We also state the reverse-mode algebra. 3. Third-order extension. We present the full masked third-order state and online updates, with strictly causal streaming kernel. HLA is intended as drop-in, attention-like mixer for long-context models. It provides (i) attention-style, data-dependent weighting; (ii) strictly causal streaming with O(1) per-token update memory independent of sequence length; and (iii) parallel training via scans without resorting to approximate backpropagation through time. We deliberately focus on algorithmic structure and implementation."
        },
        {
            "title": "2 Background",
            "content": "Notations. We use bold lowercase for vectors and bold uppercase for matrices/tensors. Token index denotes the current time; is the query/key dimension; dv is the value dimension. Unless otherwise stated, HLA outputs are in the default unnormalized form, which avoids length-dependent renormalization. We adopt row-vector outputs ot R1dv ; ratio-normalized (row-normalized) variant divides by masked scalar denominator built from the same summaries for scale control and comparability with linear attention. Throughout, prefix summaries are statistics computable in streaming fashion with O(1) memory per token and per head."
        },
        {
            "title": "2.1 Scaled dot-product attention",
            "content": "Given queries Rnd, keys Rnd, and values Rndv , scaled dot-product attention (Vaswani et al., 2017) is Attn(Q, K, V) = softmax (cid:18) QK (cid:19) + Λ V, 2 where Λ Rnn is the additive causal mask (zeros on and below the diagonal; above). For algebraic manipulations outside the softmax (e.g., Section 3.1), we use the Hadamard product with binary causal mask, denoted by (ones on and below the diagonal; zeros above), to mask bilinear forms consistently."
        },
        {
            "title": "2.2 Linear attention",
            "content": "Linear attentions (Wang et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2020) approximate the softmax kernel by feature map ϕ : Rd Rr (maybe unnormalized): Attn(Q, K, V)i ϕ(qi)(cid:0) (cid:80) ϕ(qi)(cid:0) (cid:80) (cid:1) ϕ(kj)v ϕ(kj)(cid:1) . Maintaining the running sums (cid:80) memory complexity. ϕ(kj)v and (cid:80) ϕ(kj) yields O(n r(d+dv)) time and O(r dv)"
        },
        {
            "title": "3 Higher-order Linear Attention",
            "content": "In this section, we will introduce Higher-order Linear Attention (HLA). We begin with second-order linear attention as warm-up, and present its extension to third-order linear attention in Section 7. Second-order tensor attention mechanism. Second-order tensor attention can be written as T2 := (QK)(QK) = Q(KK)Q Rnn, so that [T2]ij = KK Rdd, suggesting streaming implementations via prefix moments. (KK)qj. The right-hand side shows dependence on the second moment We maintain prefix summaries at time t:"
        },
        {
            "title": "SK\nt",
            "content": ":="
        },
        {
            "title": "CQV\nt",
            "content": ":= mQ := (cid:88) it (cid:88) it (cid:88) it kik Rdd, qiv Rddv , qi Rd. All the above prefix summaries can be updated in streaming fashion. In particular, the updates of and CQV SK cost O(d2) and O(d dv) time per token, respectively. Unnormalized HLA. The output of second-order HLA at time is the numerator-style bilinear form built from prefix moments: SK This choice avoids length-dependent renormalization while preserving streaming updates and the same state as the normalized variant. := (3.1) ot CQV . Normalized HLA. denominator at as follows: In order to define the normalized output of HLA, we define the numerator and CQV and the normalized output of HLA is given by numt = SK , dent = SK mQ , CQV SK mQ SK where ε > 0 is small constant added for numerical stability. numt dent + ε ot = + ε = , (3.2) Notably, SK acts as learned, data-dependent metric on query space; CQV is value accumulator modulated by past queries; and mQ provides query mass for optional scale control. This mirrors second-order polynomial kernel in (q, k) while remaining strictly streaming and causal once masked (Section 3.1). Connection with linear attention. Setting SK = yields numt = t CQV = (cid:88) (q qi) , it dent = mQ = qi, (cid:88) it So the normalized output reduces to linear-attention form with kernel K(qt, qi) = qi. When queries and keys are tied (qi ki), this coincides with linear attention using the identity feature map ϕ(x) = x. In general, second-order HLA implements the data-adaptive degree-2 polynomial = (cid:80) kernel Kt(q, q) = qSK depends on the past keys, strictly enriching first-order linearizations while retaining streaming. Absent tying k, this differs from identity-feature linear attention. whose metric SK it kik"
        },
        {
            "title": "3.1 Causal masking via extended summaries",
            "content": "Let denote the binary causal mask (lower-triangular, including the diagonal). For the masked second-order matrix, (cid:2)(L QK)(L QK)(cid:3) t,j = (cid:88) (q ki)(q ki) = SK min(t,j)qj. Equivalently, the strictly causal second-order output at time can be written in matrix form by masking on the right before applying values: imin(t,j) (cid:16)(cid:2)(L QK)(L QK)(cid:3) (cid:17) ot = V. t,: This row-wise enforces the restriction when multiplying by V."
        },
        {
            "title": "Define two additional prefix summaries",
            "content": "Gt := ht := (cid:17) (cid:17) kik kik (cid:88) (cid:16) it (cid:88) (cid:16) it"
        },
        {
            "title": "CQV",
            "content": "i1 Rddv , mQ i1 Rd. We have the following theorem, which gives the unnormalized and normalized outputs of HLA with causal mask. 4 Theorem 3.1 (Masked streaming identity for second order). For each t, let nummask = (cid:16) CQV SK Gt (cid:17) , denmask = (cid:16) mQ SK ht (cid:17) . Consequently, the strictly causal, masked default unnormalized output is (cid:16) ot = t CQV SK Gt (cid:17) . An optional linear normalization divides by the masked denominator, ot = (cid:16) (cid:16) t CQV SK mQ SK Gt (cid:17) ht (cid:17) , + ε (3.3) (3.4) where ε > 0 is small constant added for numerical stability. Proof. Let = (QK) with lower-triangular including the diagonal. For the second-order weight matrix, we have WW with entries (WW)t,j = (cid:80) ki). Then the masked, unnormalized numerator at time is imin(t,j)(q ki)(q nummask = (cid:88) jt (WW)t,j j = (cid:88) (cid:16) (cid:88) ki q qj (cid:17) = v (cid:88) (cid:16) (cid:88) (cid:17) kik qjv , jt ij jt ij where the second equality uses the fact that min(t, j) = when t. Interchanging finite sums yields (cid:88) (cid:16) (cid:88) (cid:17) kik qjv = jt ij (cid:88) jt qjv SK = (cid:88) qjv SK (cid:88) (cid:16) (cid:88) (cid:17) kik qjv , (3.5) jt (cid:124) j<it jt (cid:124) (cid:125) where the last equality holds due to SK = SK In Eq. (3.5), the first term I1 equals SK (cid:123)(cid:122) I1 j<it kik . . For the second term I2, swap the order of (cid:0)kik summation: (cid:80) = Gt. This proves the numerator identity. The proof for the denominator is analogous with vj replaced by mQ 1 (i.e., qj replaced by 1-summaries), yielding SK + ε gives Eq. (3.4). ht. Finally, the division by denmask j<i(), we can obtain I2 = (cid:80) (cid:80) CQV i>j() = (cid:80) j<i qjv (cid:1)(cid:16)(cid:80) (cid:123)(cid:122) I2 (cid:80) (cid:80) jt it it (cid:17) (cid:125) i Online updates. Using the fact that (kk)X = k(kX), we have = SK t1 + ktk SK , CQV Gt = Gt1 + kt(k t1),"
        },
        {
            "title": "CQV",
            "content": "t = CQV t1 + qtv , mQ ht = ht1 + kt(k t1). mQ = mQ t1 + qt, Therefore, the per-token cost remains O(d2+d dv) in total."
        },
        {
            "title": "4 Chunk-parallel training via associative scans",
            "content": "In Section 3.1, we have presented the recurrent form for second-order HLA. As we know, training purely recurrent model is inefficient on GPUs. We adopt within-chunk scans with width and inter-chunk scans across chunks (Blelloch, 1990). similar technique has been widely used in the literature of linear attention (Yang et al., 2023; Qin et al., 2024). We write Bc for the number of chunks to avoid overloading elsewhere; thus, inter-chunk scans are across Bc chunks."
        },
        {
            "title": "4.1 Unmasked monoid",
            "content": "Let = (S, C, m) with token deltas St = ktk segments Tt = (St, Ct, mt) and the additive monoid , Ct = qtv , mt = qt. Define elementary (SA, CA, mA) (SB, CB, mB) = (SA+SB, CA+CB, mA+mB). An exclusive Blelloch scan on {T1, . . . , Tw} yields per-token prefixes Pt = (cid:76) i<t Ti, from which the inclusive state at is obtained locally by adding Tt. Here and below, then denotes adjacent segments in time (all indices in precede those in B)."
        },
        {
            "title": "4.2 Masked semidirect product",
            "content": "For the masked case use = (S, C, m, G, h). For single-token segment, = = 0. Concatenation is (SA,CA, mA, GA, hA) (SB, CB, mB, GB, hB) = (cid:0)SA+SB, CA+CB, mA+mB, GA+GB + SBCA, hA+hB + SBmA which is associative (direct expansion). Perform the same exclusive scan; per-token inclusive states follow by adding the local deltas and the cross-terms StCt1 and Stmt1. (cid:1), (4.1) Decay-aware monoid. Let γ (0, 1) be fixed exponential decay and let segment carry its length ℓ(X ) and attenuation ρ(X ) := γℓ(X ). For the unmasked triple = (S, C, m) the decayed concatenation is (SA, CA, mA, ρA) γ (SB, CB, mB, ρB) = (cid:0)ρBSA+SB, ρBCA+CB, ρBmA+mB, ρAρB (cid:1), and analogously for the masked (S, C, m, G, h) state: (SA,CA, mA, GA, hA, ρA) γ (SB, CB, mB, GB, hB, ρB) = (cid:16) ρBSA+SB, ρBCA+CB, ρBmA+mB, ρBGA+GB + SB(ρBCA), ρBhA+hB + SB(ρBmA), ρAρB (cid:17) . Associativity follows from bilinearity and ρ-multiplicativity. Theorem 4.1 (Scan equivalence: serial vs. (decayed) associative scans). Consider sequence of token segments {T1, . . . , Tn} and either (no decay) or γ (with decay). Let Pt be the exclusive prefix obtained by Blelloch scan under the chosen operator. For each t, the inclusive state computed locally from Pt and Tt equals the state produced by serial left-to-right recurrence on tokens 1:t. Consequently, the per-token masked outputs are identical to those of the serial algorithm. Proof. We prove for the masked, decayed case; the other cases are specializations. Define the serial recurrence Xt = Φγ(Xt1, Tt) given by the online updates in Section 3.1 with decay γ. By construction, Φγ coincides with the binary map fγ(X , Y) := γ when is single-token segment. Because γ is associative with identity the zero-length segment (all-zero summaries, ρ = 1), the Blelloch scan yields Pt = γ T1 γ γ Tt1. The local inclusive update computes Pt γ Tt, which equals Xt by associativity and the definition of Φγ. The masked outputs are functions only of the inclusive state (Theorem 3.1), hence coincide with the serial outputs. 6 Backward for gradients. Let denote the vector-Jacobian adjoint of evaluated at the forward states. reverse (decayed) scan applying γ with checkpointing at tile boundaries yields gradients that match those of the serial recurrence, by Theorem 4.1 and the chain rule. Remark 4.2 (Inclusive vs. exclusive scans). Given segments (T1, . . . , Tw) and an associative operator with identity E, the exclusive scan returns prefixes Pt = T1 Tt1, while the inclusive scan returns It = Pt Tt. Our forward algorithms compute Pt via an exclusive Blelloch scan and then form the inclusive state locally by combining Pt with the tokens deltas (and required cross-terms). This choice exposes maximal parallelism and ensures exact equality to the serial recurrence by Theorem 4.1. With decay, the identity is the zero-length segment (0, . . . , 0, ρ=1); the exclusive/inclusive distinction is unchanged. Intra-chunk parallelism. Within chunk of width w, an exclusive Blelloch scan over {T1, . . . , Tw} under (or γ) yields Pt for all in O(log w) span and O(1) auxiliary memory per position. The per-token inclusive states are then computed independently as It = Pt Tt. Inter-chunk parallelism. For Bc chunks, each chunk produces single summary (c) = (cid:76) tchunk Tt. An exclusive scan across the Bc summaries gives carry-in prefixes (cid:98)P (c) for every chunk. Each position in chunk then uses the merged prefix (cid:98)P (c) Pt before adding its local Tt to obtain the inclusive state. This is the same parallel skeleton widely used in modern linear-attention and recurrent networks that maintain streaming sufficient statistics (Sun et al., 2023; Qin et al., 2023; Yang et al., 2023; Qin et al., 2024; Yang et al., 2024b). Connection to linear attention. First-order linear attentions and related modern RNN kernels scan additive/decayed summaries (e.g., (cid:80) ϕ(k)v and denominators) using exactly this intra-/interchunk pattern. HLA plugs into the same infrastructure: only the state tuple and cross-terms change (e.g., (S, C, m, G, h) for second order), while the exclusive/inclusive logic and two-level scan strategy remain identical. Thus, HLA inherits the throughput characteristics of these systems with strictly higher expressivity."
        },
        {
            "title": "4.3 Adding decay and regularization",
            "content": "Decayed states. Introduce time decay γ (0, 1): = γSK SK t1 + ktk , CQV = γCQV t1 + qtv , mQ = γmQ t1 + qt, and the cross-summaries obey Gt = γGt1 + kt (cid:0)k CQV t1 (cid:1), ht = γht1 + kt (cid:0)k mQ t1 (cid:1), which are the decayed analogues of the online updates in Section 3.1. Decay controls spectral growth and improves recency bias while maintaining associativity (with respect to segment-local normalization) (Sun et al., 2023; Qin et al., 2023; Yang et al., 2023, 2024a; Peng et al., 2024; Behrouz et al., 2024; Peng et al., 2025; Behrouz et al., 2025a,b)."
        },
        {
            "title": "Implementation details and complexity",
            "content": "In this section, we discuss the implementation details and provide complexity analysis. Recall that for each token and each head (second order), we have 7 State: SK Rdd, CQV Rddv , mQ Rd (and masked Gt Rddv , ht Rd). Compute: evaluate ut = SK (matvec) and then utCQV corrections explicitly; masked cross-terms still use Gt; the denominator uses utmQ q to avoid cubic cost. ht. This avoids forming SK (rowmatrix), with masked CQV Parallelism: within-chunk Blelloch scans (span O(log w)) and inter-chunk exclusive scans across Bc chunks, both using the same . Algorithm 1 Masked (Second Order) HLA with Within-Chunk Scan Require: Chunk tokens (q[1:w], k[1:w], v[1:w]), ε, optional ridge λ, optional decay γ, optional flag normalize , Ct qtv 1: Token segments: for = 1..w, set St ktk , mt qt, and initialize Gt=0, ht=0. 2: Exclusive scan over {(St, Ct, mt, 0, 0)}w t=1 using in Eq. (4.1) (with decay if used) to obtain prefixes Pt = (St1, Ct1, mt1, Gt1, ht1). St + λI Inclusive state: St γSt1 + St; Ct γCt1 + Ct; mt γmt1 + mt Gt γGt1 + St Ct1; ht γht1 + St mt1 Effective S: Seff Default masked unnormalized output: Seff num Ct ohla num Optional normalization: if normalize then 3: for = 1 to in parallel do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for 18: return {ohla den mt ohla ohla /den end if Gt }w t= ht + ε optional ridge for stability O(d2) matvec O(d dv)"
        },
        {
            "title": "5.1 Pseudocode",
            "content": "We present PyTorch-like reference for masked second-order HLA with within-chunk exclusive scan. Unmasked and/or diagonal-regularized variants follow by removing (G, h). Normalization is optional; by default, the implementation returns the unnormalized output and may divide by the masked denominator if requested. Remark. Adding λI yields stabilized causal variant of the masked operator; it does not correspond to the exact masked bilinear form of (L QK). 5."
        },
        {
            "title": "Implementation considerations",
            "content": "HLA only replaces the standard attention sublayer in the transformer block, while the feed-forward sublayer and normalization sublayers remain unchanged. Drop-in replacement requires only swapping the kernel while keeping positional encodings and masking identical to the baseline. Multi-query keys/values (sharing K, across heads) reduce state from O(h d2) to O(d2+h dv) while preserving the algebra. 8 The summaries (S, C, m, G, h) are per head. With multi-query (K, shared across heads), is shared and stored once per layer (O(d2)), while (CQV the key moment SK , Gt, ht) remain per-head (O(h dv+h d)). This yields total memory of O(d2+h dv) instead of O(h d2+h dv) when each head maintains its own SK . For throughput, maintain SK in packed symmetric layout (store only the upper triangle, 1 2 d(d+1) entries) to reduce bandwidth without changing the algebra. Within chunk of width w, use an exclusive Blelloch scan to obtain prefixes in O(log w) span and constant extra memory per position; inter-chunk scans use the same operator across Bc chunks. , mQ t"
        },
        {
            "title": "6 Asymmetric Higher-order Linear Attention",
            "content": "Motivation. The second-order HLA in Section 3 realizes the symmetric triple product AAV with = QK (masked later). We introduce complementary asymmetric variant that uses the left-cascaded product = (KQ) (KV), AAV (cid:124) (cid:123)(cid:122) (cid:125) AAV and show it admits strictly causal streaming with O(d2+d dv) per-token cost. We call this operator AHLA (Asymmetric Higher-order Linear Attention)."
        },
        {
            "title": "6.1 Definition and masked streaming identity",
            "content": "Let = (QK) be the causally masked affinity, where is the binary lower-triangular mask (including the diagonal). The AAV weights are (AA)t,j = (cid:88) i=j (q ki) (q kj), t. To obtain strictly causal outputs when applying to values, interpret the final multiplication as (cid:0)(AA) L(cid:1)V; the streaming identity in Theorem 6.1 implements exactly this row-wise masking. Consequently, the (unnormalized) output is oAHLA = (cid:88) (cid:88) (q ki)(q kj) . (6.1)"
        },
        {
            "title": "Introduce the streaming prefix summaries",
            "content": "jt i=j"
        },
        {
            "title": "PKV\nt",
            "content": ":= Et := nt := (cid:88) jt (cid:88) it (cid:88) it kjv Rddv , (cid:0)q PKV ki (cid:1) Rddv , (cid:0)q mK ki (cid:1) Rd. mK := (cid:88) jt kj Rd, Note. For chunk-parallel scans used in training, we additionally introduce segment-level cross moment RKQ; see Section 6.2 for its definition and role in the concatenation operator. Theorem 6.1 (Masked streaming identity for AHLA). With the above definitions, oAHLA = Et and (cid:98)oAHLA = Et nt + ε , where the second expression is an optional linear normalization using the masked denominator. The online (strictly causal) updates are"
        },
        {
            "title": "PKV",
            "content": "t = PKV Et = Et1 + kt Proof. From Eq. (6.1), fix and sum over i: (cid:80) (cid:80) t1 + ktv , (cid:0)q PKV mK (cid:1), t1 + kt, = mK nt = nt1 + kt ki)(cid:0)q = , hence nt. The stated updates follow by isolating index i=t and using that PKV PKV Et. Replacing vj by 1 gives the denominator = it ki(q (cid:1) = i kj) ji(q PKV PKV . Then oAHLA (cid:16)(cid:80) = (cid:17) ) (cid:0)q (cid:1). mK = it(q with mK PKV t1 + ktv and mK = mK t1 + kt. R1dv and the Cost. For streaming/serial inference, the dominant work is forming outer product kt(); the total is O(d dv) time and O(d dv+d) state per head (for P, E, m, n). For chunk-parallel scans used in training, an additional block statistic RKQ appears only inside the concatenation operator (Section 6.2), contributing O(d2) memory per chunk summary but not to the streaming path. PKV Decay mechanism. With exponential decay γ (0, 1), = γPKV Et = γEt1 + kt t1 + ktv , (cid:0)q PKV t"
        },
        {
            "title": "PKV",
            "content": "mK (cid:1), t1 + kt, = γmK nt = γnt1 + kt (cid:0)q mK (cid:1), which preserves associativity of the scan operator below."
        },
        {
            "title": "6.2 Chunk-parallel scans for AHLA",
            "content": "Unmasked/masked concatenation. For segment followed by B, consider the augmented state Here RKQ is the segment-level keyquery cross moment, defined by = (RKQ, PKV , mK, E, n). RKQ := (cid:88) kiq Rdd. isegment It is used only during chunk concatenation to form the cross terms RBPA and RBmA in Eq. (6.2); It is not required by the serial/streaming forward path in Algorithm 2. With exponential decay, the segments RKQ attenuates as ρBRA+RB in the decayed concatenation."
        },
        {
            "title": "The undecayed associative concatenation is",
            "content": "(RA, PA, mA, EA, nA) AHLA (RB, PB, mB, EB, nB) = (cid:16) RA+RB, PA+PB, mA+mB, EA+EB+RBPA, nA+nB+RBmA (cid:17) , (6.2) which is associative by direct expansion of (cid:80) the missing cross-prefix equals RBPA (and analogously for n). iAB ki(q Pi) and the observation that for 10 Decay-aware concatenation. Let each segment carry its attenuation ρ() = γℓ(). Then (R, P, m, E, n, ρ) = (ρBRA+RB, ρBPA+PB, ρBmA+mB, ρBEA+EB+RB(ρBPA), ρBnA+nB+RB(ρBmA), ρAρB), which is associative by bilinearity and multiplicativity of ρ. Scan equivalence. An exclusive Blelloch scan under AHLA (or its decayed form) followed by local inclusion reproduces exactly the activations of the serial recurrence given above."
        },
        {
            "title": "6.3 Pseudocode",
            "content": "t ; γm + kt Algorithm 2 AHLA (Second-order) streaming with causal mask and optional decay Require: {qt, kt, vt}n t=1, decay γ (0, 1], stability ε > 0, flag normalize 1: Init: = 0ddv , = 0d, = 0ddv , = 0d 2: for = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: end for 12: return {ot}n γP + ktv P m γE + kt r; γn + kt ot E if normalize then den n + ε; ot ot/den end if t= 1dv scalar Relation to AAV. AHLA emphasizes matrix power of A, weighting each value vj through kj routed by an intermediate key index i. In contrast, the symmetric AAV single pass aggregates via the metric SK and query summaries. Both are second-order, strictly causal, and stream with identical asymptotic costs but induce different inductive biases."
        },
        {
            "title": "7 Third-Order Linear Attention",
            "content": "In this section, we will introduce third-order HLA."
        },
        {
            "title": "7.1 Steaming form of Causal HLA",
            "content": "Third-order tensor attention mechanism. Let = QK Rnn and be the binary causal mask. Unmasked third-order tensor attention uses the matrix AAA. Its (t, j)-entry is [(AAA)]t,j = (cid:88) (cid:88) un in (q ki)(q ki) kj) = t (KK) (q (cid:33) quq kj, (cid:32) (cid:88) which immediately yields streaming factorization through prefix moments. 11 Unmasked factorization. Define prefix summaries SK Rdd, PKV operator is Rddv , mK = (cid:80) it kiv = (cid:80) = (cid:80) it ki Rd. The default (unnormalized) third-order Rdd, SQ it kik it qiq = (cid:80) o(3) = SQ SK An optional normalization divides by SK mK SQ PKV +ε if desired. . Masked streaming summaries. To impose strict causality, we introduce cross-summaries: G(1) := G(2) := G(3) := (cid:88) it (cid:88) it (cid:88) it (kik )SQ i1PKV i1 Rddv , i1(qiq SK )PKV i1 Rddv , i1SQ SK i1(kiv ) Rddv , h(1) := h(2) := h(3) := (cid:88) it (cid:88) it (cid:88) it (kik )SQ i1mK i1 Rd, i1(qiq SK )mK i1 Rd, i1SQ SK i1ki Rd. Then the masked, unnormalized quantities are defined as follows: num(3)mask = (cid:16) SQ SK PKV G(1) G(2) (cid:17) , G(3) (cid:17) mK The following theorem shows that the (normalized) output of third-order HLA can be computed based on num(3)mask = den(3)mask h(3) h(2) h(1) SQ SK and den(3)mask . . (cid:16) Theorem 7.1 (Masked streaming identity for third order). For each t, the strictly causal third-order output in the default (unnormalized) form is An optional normalized variant divides by the masked denominator, = num(3)mask o(3) . o(3) = num(3)mask den(3)mask + ε . and the online updates are = SK SK = PKV PKV t1 + ktk , t1 + ktv , = SQ SQ = mK mK t1 + qtq , t1 + kt. G(1) G(2) G(3) = G(1) = G(2) = G(3) t1 + (ktk t1 + SK t1 + SK )SQ t1(qtq t1SQ t1PKV t1 , )PKV t1 , t1(ktv ). h(1) = h(1) = h(2) h(2) = h(3) h(3) t1 + (ktk t1 + SK t1 + SK )SQ t1(qtq t1SQ t1kt. t1mK )mK t1, t1, 12 (7.1) (7.2) (7.3) Proof. Let = (QK) and consider (WWW)V. The t-th row applied to is (cid:88) (cid:88) jt ut (WW)t,u Wu,j = (cid:88) (cid:88) jt ut kjv . quq SK Using (cid:80) the dependence on future indices relative to each summation boundary yields ) and repeatedly applying (cid:80) iu = (cid:80) ut1(SK + (cid:80) = SK ut SK it (cid:80) u<it to peel off SQ SK kjv (cid:88) jt (cid:88) (kik )SQ i1PKV i1 it i1(qiq SK )PKV i1 (cid:88) it i1SQ SK i1(kiv ), (cid:88) it which is precisely SK gives the masked numerator, and replacing vj by 1 yields the denominator. Online updates follow by isolating the = contributions and using (kk)X = k(kX). . Left-multiplication by PKV G(2) G(3) G(1) SQ"
        },
        {
            "title": "7.2 Pseudocode",
            "content": "We present explicit pseudocode for masked third-order HLA in two parts: (i) strictly causal streaming kernel for inference, and (ii) the associative scan operator used for chunk-parallel training. All operations are per head; shapes follow Section 7.1. t=1, decay γ (0, 1], stability ε > 0, flag normalize G(1)=0ddv , G(2)=0ddv , G(3)=0ddv , h(1)=0d, h(2)=0d, h(3)=0d prev SQ; Pprev PKV ; mprev mK Algorithm 3 Masked (Third Order) HLA Streaming Kernel Require: Sequences {qt, kt, vt}n 1: Init: SK = 0dd, SQ = 0dd, PKV = 0ddv , mK = 0d 2: 3: for = 1 to do 4: 5: 6: 7: 8: 9: prev SK; SQ SK Inclusive first-order updates (with decay): prev + qtq SK γSK PKV γPprev + ktv ; mK γmprev + kt Cross-summaries (matvec/outer-product forms): u1 SQ a2 SK u3 SQ Output (unnormalized by default): SK qt; SQ y; termA zPKV ; termB ot termA termB termC termD if normalize then prev kt; G(1) γG(1) + kt prev qt; G(2) γG(2) + a2 prev kt; a3 SK prev u3; G(3) γG(3) + a3v ; SQ γSQ 1 Pprev Pprev prev + ktk (cid:0)u (cid:0)q 10: denvec SK (SQmK) h(1) h(2) h(3) den denvec + ε; ot ot/den 11: 12: 13: 14: 15: 16: 17: 18: 19: end for 20: return {ot}n end if t=1 (cid:1); h(1) γh(1) + kt (cid:1); h(2) γh(2) + 1 mprev mprev ; h(3) γh(3) + a3 (cid:0)u (cid:0)q (cid:1) (cid:1) G(1); termC G(2); termD G(3) Usage. We provide the strictly causal streaming kernel in Algorithm 3. Designing chunk-parallel scan operator that exactly reproduces serial activations requires additional segment-level summaries beyond (SK, SQ, PKV , mK, G(1:3), h(1:3)); we leave this composition to future work."
        },
        {
            "title": "8 Related Work",
            "content": "The literature on subquadratic sequence modeling spans (i) fast-weight style dynamic-parameter models, (ii) kernel/feature-map linearizations of attention, and (iii) recurrent/state-space approaches. HLA belongs to complementary class: it preserves attention-style, data-dependent mixing but realizes higher interactions through compact prefix moments with exact causal masking and scanparallel training. Fast weights and fast weight programmers (FWPs). Fast weights, dating to early connectionist memory models (Hinton and Plaut, 1987), implement short-term, input-dependent synaptic changes. Schmidhubers fast weight programmers (Schmidhuber, 1992) introduced differentiable controllers that program separate fast-weight matrix; later, Ba et al. (2016) revived this idea to attend to the recent past. series of works made the connection to modern attention explicit: Schlag et al. (2021) showed formal equivalence between linearized self-attention and FWPs, where outer-product updates Wt ktv accumulate an associative memory queried by qt; Irie et al. (2021) extended FWPs with recurrence in the programmer and the fast net. Yang et al. (2024b) utilizes WY Transformations (Bischof and Van Loan, 1987) to implement chunkwise parallel training of Delta Network (Schlag et al., 2021). Parallel lines explore higher or preconditioned mixing by maintaining or inverting second-moment matrices. In our formulation, SK plays the role of learned kernel; working directly with SK avoids explicit matrix inversion and preserves streaming updates, whereas inverse-based methods typically require heavier linear algebra (Behrouz et al., 2025a,b; von Oswald et al., 2025). Linear Attention Mechanisms. common route is to replace the softmax kernel by explicit features ϕ to enable streaming via running sums. Representative examples include Linear Transformers (Katharopoulos et al., 2020), Performers FAVOR+ random features (Choromanski et al., 2020), and Random Feature Attention (Peng et al., 2021). Earlier work also proposed multiplicative rearrangements that yield linear-complexity-efficient attention (Shen et al., 2021). These methods achieve O(ndr) time with feature dimension but are typically first-order in the sense that they maintain only (cid:80) ϕ(k)v and (optionally) scalar denominator. By contrast, second-order HLA maintains the full key moment SK it kik together with query-value and query mass summaries and their masked cross-summaries, yielding strictly causal higher interactions while remaining streaming. Recent linear attention variants include Sun et al. (2023); Qin et al. (2023); Yang et al. (2023); Qin et al. (2024); Yang et al. (2024b); von Oswald et al. (2025). State Space Models. SSMs (e.g., S4) (Gu et al., 2021) and selective SSMs (e.g., Mamba) (Gu and Dao, 2023; Dao and Gu, 2024) realize O(1) per-token state updates via linear recurrences and convolutions. These architectures excel at long-range dependencies but express data-dependent mixing differently from attention. HLA sits in between: it is attention-like (data-dependent queries/keys) yet streams via compact prefix statistics like recurrent models. Modern RNNs. Recent modern RNN designs emphasize gating, decays, and associative scanfriendly recurrences that enable parallel training while preserving strictly constant per-token state at inference. Examples include gated linear mixers and decay-aware updates (Yang et al., 2023, 2024a), efficient gradient routing and training strategies for long sequences (Qin et al., 2024; Peng et al., 2024; Sun et al., 2024), and RWKV-style architectures that replace attention with learned decays and elementwise gating (Peng et al., 2025). These methods typically maintain first-order sufficient statistics and rely on fixed linear dynamics. In contrast, HLA retains attention-style, data-dependent metrics via SK and higher cross-summaries while keeping the same O(1) per-token state update = (cid:80) 14 paradigm, offering complementary inductive bias to RNNs and SSMs. Test Time Training and Memory Networks. Test-time adaptation and explicit long-term memory are emerging tools for extending context without quadratic cost. Test-time training variants adapt parameters from recent tokens to improve local coherence (Sun et al., 2024), whereas memory networks maintain external stores addressable by content keys (Behrouz et al., 2024, 2025a,b). The HLA view is orthogonal: it encodes higher interactions in compact prefix moments that are sufficient for exact masked streaming and scan-parallel training. Associative memory and Hopfield views. Modern Hopfield networks show that transformer attention is single-step retrieval in an energy-based associative memory (Ramsauer et al., 2020; Zhong et al., 2025). While this perspective clarifies why attention uses content-addressable memory, standard Hopfield-style layers remain first-order in their sufficient statistics. HLA complements this view by providing explicit higher sufficient statistics with strict causality."
        },
        {
            "title": "9 Conclusion",
            "content": "We introduced Higher-order Linear Attention (HLA), causal higher attention mechanism with exact streaming updates, strictly causal masked formulation via extended summaries, and associative scans for parallel training that provably match serial recurrences. At second order, HLA maintains O(d2) state per head and computes each token in O(d2) time, with optional normalization and decay that preserve associativity. We further developed an asymmetric variant (AHLA) and complete third-order masked algebra with streaming formulas and scan operators."
        },
        {
            "title": "References",
            "content": "Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, and Vahab Mirrokni. Atlas: Learning to optimally memorize the context at test time. arXiv preprint arXiv:2505.23735, 2025a. Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. Its all connected: journey through test-time memorization, attentional bias, retention, and online optimization. arXiv preprint arXiv:2504.13173, 2025b. Christian Bischof and Charles Van Loan. The wy representation for products of householder matrices. SIAM Journal on Scientific and Statistical Computing, 8(1):s2s13, 1987. Guy Blelloch. Prefix sums and their applications. 1990. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. 15 Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Geoffrey Hinton and David Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pages 177186, 1987. Kazuki Irie, Imanol Schlag, Robert Csordas, and Jurgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. Advances in neural information processing systems, 34:77037717, 2021. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, et al. Eagle and finch: Rwkv with matrixvalued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu, Janna Lu, William Merrill, et al. Rwkv-7 goose with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456, 2025. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, et al. Transnormerllm: faster and better large language model with improved transnormer. arXiv preprint arXiv:2307.14995, 2023. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024. Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020. Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 93559366. PMLR, 2021. Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131139, 1992. 16 Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: In Proceedings of the IEEE/CVF winter conference on Attention with linear complexities. applications of computer vision, pages 35313539, 2021. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, (cid:32)Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, et al. Mesanet: Sequence modeling by locally optimal test-time training. arXiv preprint arXiv:2506.05233, 2025. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024a. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024b. Shu Zhong, Mingyu Xu, Tenglong Ao, and Guang Shi. Understanding transformer from the perspective of associative memory. arXiv preprint arXiv:2505.19488, 2025."
        }
    ],
    "affiliations": [
        "Princeton University",
        "University of California, Los Angeles"
    ]
}