{
    "paper_title": "ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix Factorization",
    "authors": [
        "Lawrence Liu",
        "Alexander Liu",
        "Mengdi Wang",
        "Tuo Zhao",
        "Lin F. Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) present significant deployment challenges due to their immense computational and memory requirements. While semi-structured pruning, particularly 2:4 sparsity, offers a path to practical hardware acceleration, existing methods often incur substantial performance degradation. To bridge this gap, we introduce ARMOR: (Adaptive Representation with Matrix-factORization), a novel one-shot post-training pruning algorithm. Instead of directly pruning weights, ARMOR factorizes each weight matrix into a 2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These wrappers act as efficient pre and post-transformation error correctors, offering greater flexibility to preserve model quality compared to conventional 2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen through a block coordinate descent algorithm that minimizes a layer-wise proxy loss. We theoretically prove this optimization is guaranteed to converge to a solution with a proxy loss less than or equal to state-of-the-art pruning algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and significantly outperforms state-of-the-art 2:4 pruning methods across a wide range of downstream tasks and perplexity evaluations. ARMOR achieves this superior performance while retaining the inference speedups and substantial memory usage reductions of 2:4 pruning, establishing a more effective trade-off between model compression and task accuracy"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 8 2 5 5 0 . 0 1 5 2 : r Preprint. Under review. ARMOR: HIGH-PERFORMANCE SEMI-STRUCTURED PRUNING VIA ADAPTIVE MATRIX FACTORIZATION Lawrence Liu1 Alexander Liu Mengdi Wang2 Tuo Zhao3 Lin F. Yang1 1University of California, Los Angeles 2Princeton University 3Georgia Institute of Technology"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) present significant deployment challenges due to their immense computational and memory requirements. While semi-structured pruning, particularly 2:4 sparsity, offers path to practical hardware acceleration, existing methods often incur substantial performance degradation. To bridge this gap, we introduce ARMOR: (Adaptive Representation with MatrixfactORization), novel one-shot post-training pruning algorithm. Instead of directly pruning weights, ARMOR factorizes each weight matrix into 2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These wrappers act as efficient preand post-transformation error correctors, offering greater flexibility to preserve model quality compared to conventional 2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen through block coordinate descent algorithm that minimizes layer-wise proxy loss. We theoretically prove this optimization is guaranteed to converge to solution with proxy loss less than or equal to state-of-the-art pruning algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and significantly outperforms state-of-the-art 2:4 pruning methods across wide range of downstream tasks and perplexity evaluations. ARMOR achieves this superior performance while retaining the inference speedups and substantial memory usage reductions of 2:4 pruning, establishing more effective trade-off between model compression and task accuracy."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities (Park et al., 2023; Huang & Yang, 2025), yet their immense computational and memory requirements pose significant barriers to practical deployment. As result, techniques for reducing model sizes and computational costs while retaining performance are of significant interest for the research community. particular area of focus are one-shot post training compression techniques, highly efficient model compression regime where already trained models are compressed in single pass without iterative fine-tuning. Pruning, the removal of model parameters is particularly compelling avenue of compression as it offers direct path to inference acceleration via dedicated hardware support for specific sparsity patterns (Kwon et al., 2022), and its benefits can be compounded with orthogonal methods like quantization (Frantar et al., 2022; Lin et al., 2024; Li et al., 2025a). However, critical trade-off plagues existing pruning techniques. Methods capable of delivering tangible inference acceleration do so by sacrificing significant model accuracy, while the most accurate techniques offer largely theoretical speedups. Bridging this gap is the central focus of our work. Pruning algorithms can be broadly divided into three categories: structured, unstructured, and semistructured pruning. Structured pruning removes entire weight structures, such as rows or columns of weight matrices (Ashkboos et al., 2024), attention heads (Ma et al., 2023), or even full layers (Men et al., 2024). This coarse-grained approach is highly compatible with existing hardware and software, as it results in smaller, dense matrices that can be processed efficiently by standard libraries, leading to direct improvements in inference speed (Ma et al., 2023). However, this rigidity comes Correspondence to lawrencerliu@ucla.edu 1 Preprint. Under review. Figure 1: Illustration of proposed ARMOR factorization. For given LLM, each weight matrix is pruned individually. Instead of naively pruning the weight matrix, ARMOR wraps the sparse core with pair of block diagonal matrices and uses unique optimization algorithm to find the optimal structured pruning mask. {0, 1}doutdin represents the 2:4 binary mask. at cost; by removing large, contiguous blocks, structured pruning can lead to significant degradation in model accuracy and often has lower limit on the achievable sparsity before performance collapses (Ashkboos et al., 2024). At the other end of the spectrum, unstructured pruning offers the highest flexibility by removing individual weights from any part of the model. Leading unstructured pruning algorithms have shown that it is possible to remove up to 50% of the weights from an LLM with minimal loss in performance (Frantar & Alistarh, 2023). However, the resulting irregular, sparse matrices disrupt the parallel processing capabilities of modern hardware like GPUs, which are optimized for dense matrix operations. Thus it is difficult to translate the theoretical model size reductions into practical inference speedups is difficult (Xia et al., 2023). To bridge the gap between hardware efficiency and model performance, semi-structured pruning has emerged as compelling compromise. popular variant is N:M sparsity, which enforces regular pattern by ensuring that within any contiguous block of weights, only weights are nonzero. This regularity is key, as it can be directly accelerated by specialized hardware. For instance, NVIDIAs Ampere and subsequent GPU architectures provide native support for 2:4 semi-structured sparsity, which can theoretically double the throughput of matrix operations (Hu et al., 2024; Mishra et al., 2021). However, the constraint of fixed pruning pattern within each small block restricts the algorithms ability to retain the most critical weights, leading to significantly increased drop in performance. For example, applying state-of-the-art 2:4 pruning method to Llama-7B increases Wikitext2 perplexity by nearly 59% over its 50% unstructured counterpart, creating an undesirable choice between theoretical efficiency and practical accuracy (Sun et al., 2024). In this work, we seek to close this performance gap by introducing ARMOR: (Adaptive Representation with Matrix-factORization), an theoretically grounded one-shot model pruning algorithm. We reframe semi-structured pruning as matrix factorization problem. Instead of directly pruning weights, our key insight is to factor each weight matrix into constrained sparse core that adheres to the 2:4 hardware pattern, preand post-multiplied by lightweight block diagonal matrices, this factorization is illustrated in Figure 1. These block diagonal matrices, which are highly parameter-efficient due to their sparse structure (containing only O(N ) parameters compared to O(N 2) for dense matrix), can be multiplied with activations efficiently on modern hardware. They act as learned, low-overhead linear transformations that rotate the activation and weight spaces into basis where the 2:4 pruning constraint is less lossy, providing enhanced flexibility and preserving model quality more effectively than naive 2:4 pruning algorithms. Through extensive experiments on Llama (Dubey et al., 2024; Touvron et al., 2023) and Qwen (Yang et al., 2025) family models, we show that ARMOR consistently and significantly outperforms existing 2:4 pruning methods on both perplexity and downstream tasks. For example, on Llama-2-13B, ARMOR reduces the perplexity gap between the 2:4 pruned model and the dense original by alPreprint. Under review. most 50%. We show that these accuracy gains are achieved while preserving the practical inference speedups and memory reduction inherent to native 2:4 sparsity. Our work suggests that rethinking the fundamental representation of weights, rather than simply removing them, is promising direction for future hardware-software co-design in efficient deep learning."
        },
        {
            "title": "1.1 RELATED WORK",
            "content": "Existing one-shot unstructured/semi-structured pruning methods largely formulate the compressed weight matrix ˆW as the element-wise product of dense matrix and binary mask , where ˆW = . Research within this paradigm focuses on two primary challenges: identifying an optimal mask (importance scoring) and updating the unpruned weights in to compensate for the removed connections. The simplest approaches are weight-update-free methods like Wanda, which fix the unpruned weights to their original values (W = ) and focus on finding good mask through element-wise metrics (Sun et al., 2024; Liu et al., 2025; Liu et al.; Dong et al., 2024; Das et al., 2023; Zhang et al., 2024). In contrast, more complex weight-update methods such as SparseGPT (Frantar & Alistarh, 2023) aim for higher accuracy by iteratively pruning weights while simultaneously adjusting the remaining ones to minimize proxy reconstruction loss, often based on Hessian sketch. While effective, this introduces significant computational overhead, such as the costly inversion of the Hessian sketch matrix. Both methods suffer significantly increased performance loss when applied to 2:4 semi-structured pruning compared with their unstructured counterparts. This has motivated explorations into alternative pruning formulations beyond simple element-wise masking. These include factorization-based methods like DSF (Boˇza & Macko, 2024), which represents the weight matrix as product of two sparse matrices, and decomposition-based methods like WRP (Tan et al., 2024), which represents it as sum of semi-structured and an unstructured sparse matrix. However, these alternative approaches face significant practical barriers that prevent theoretical gains from translating into real-world inference efficiency. DSF, for instance, is incompatible with hardware-accelerated 2:4 semi-structured sparsity, as the product of two 2:4 matrices yields no compression over dense equivalent. Similarly, WRPs performance is bottlenecked by its reliance on highly unstructured sparse matrix component, for which no efficient matrix-multiplication kernels exist. Consequently, gap remains between achieving high compression rates and realizing tangible speedups on modern hardware."
        },
        {
            "title": "2 PROBLEM STATEMENT",
            "content": "To remain computationally tractable, one-shot compression pruning methods regularly adopt layerby-layer framework. For given linear layer with weight matrix Rdoutdin , the objective is to find compressed representation ˆW that minimizes data-aware proxy loss, LW,X ( ˆW ). This loss quantifies the approximation error using small calibration dataset Rndin . The exact formulation of this proxy loss is key design choice and varies across different pruning algorithms, we define our proxy loss in Section 3.2. We constrain ˆW to leverage 2:4 semi-structured sparse component to guarantee hardware acceleration. This requires that the underlying binary mask {0, 1}doutdin defining this components pattern has exactly two non-zero entries in every group of four consecutive columns per row. Formally we express this constraint as (cid:13) (cid:13) [dout], [din/4] where we adopt (cid:13)0 = 2, Mi,[k] = Mi,4(k1)+1:4k as shorthand. The general layer-wise optimization problem is therefore: (cid:13)Mi,[k] min params of ˆW LW,X ( ˆW ) s.t. (cid:13) (cid:13)Mi,[k] (cid:13) (cid:13)0 = 2, [dout], [din/4] Regardless of optimization algorithm, this layer-wise approach is inherently one-shot because the compression is completed in single pass over the networks layers without any global retraining."
        },
        {
            "title": "3 METHODS",
            "content": "In this section we introduce the ARMOR pruning process. Section 3.1 introduces the ARMOR In Section 3.2 we discuss the proxy loss optimization objective and factorization and notation. 3 Preprint. Under review. initialization. In Section 3.3 we introduce the ARMOR optimization algorithm to optimize the ARMOR factorization. This algorithm consists of two alternating steps, the continuous parameter update step, Section 3.3.1, and the sparse core update step, Section 3.3.2. Finally we introduce the main theoretical result in 3.4."
        },
        {
            "title": "3.1 ARMOR MATRIX FACTORIZATION",
            "content": "For each layer in an LLM, let the original weight matrix be Rdoutdin . Our approach seeks to find compressed matrix, ˆW Rdoutdin , with the following factorization: ˆW (A, B, , ) := (W ) B, (1) where the goal is to optimize for the parameters A, B, , and . Specifically, Rdoutdin is dense matrix representing transformed weights, and {0, 1}doutdin is binary mask that imposes sparsity through element-wise multiplication (). The matrices Rdoutdout and Rdindin are block-diagonal. The block size, dblock, is chosen hyperparameter, selected such that it divides both dout and din. We refer to the set of all learnable parameters as θ = (A, B, , ). The key to ARMOR is the diagonal matrix wrappers, and B, that surround the sparse core . Compared to the naive approach of directly pruning, these diagonal matrix wrappers offer additional flexibility compared with the naive sparse-core-only approach, while having low overhead and existing implementations on hardware for storage and inference. We can store and as tensors of size (dout/dblock) dblock dblock and (din/dblock) dblock dblock respectively. Matrix multiplication at inference time can be performed as batched matrix multiplication. As result the overhead of storing and and performing inference grows with O((dout + din)dblock), which is sublinear to the number of original parameters in the layer, doutdin. Notation We denote the individual blocks of and as A(i) Rdblockdblock and B(j) Rdblockdblock, ie: = diag (cid:0)A(1), A(2), ..., A(dout/dblock)(cid:1) and likewise for B. More generally for any matrix Rdoutdin we denote the dblock dblock matrix blocks as (i,j), ie (i,j) = C(i1)dblock+1:idblock,(j1)dblock+1:jdblock [dout/dblock], [din/dblock]. detailed illustration of this notation can be found in appendix A. 3.2 OPTIMIZATION OBJECTIVE We optimize ˆW by minimizing the NoWag layerwise proxy loss (Liu et al., 2025). LW,X (θ) = ˆW 2 F,diag(XX ) := (cid:88) (cid:88) ( Wij ˆWij)2Xj2 2, (2) where Rdoutdin is the row and column normalized version of : (cid:118) (cid:117) (cid:117) (cid:116) (cid:118) (cid:117) (cid:117) (cid:116) [din], Wij = dout(cid:88) (cid:32) (cid:33) r(2) = r(1) = 2 ij, , din(cid:88) 1 r(2) Wij r(1) i=1 j=1 (cid:33)2 (cid:32) Wij r(1) , [dout]. We adopt the NoWag objective function due to its data-aware nature and decomposable structure. The diag (cid:0)XX (cid:1) term weights the squared Frobenius norm by the magnitude of the corresponding input activations, which focuses the optimization on preserving weights that are most influential for the given calibration data. Furthermore, compared to Hessian sketch based methods, this objective function requires less calibration data. key advantage of this objective is that the loss can be decomposed into independent elementwise subproblems. While ARMOR factorizations wrapper matrices prevent direct element-wise optimization, this structure is still critical as it allows for the problem to be broken down into independent block-level subproblems, which is cornerstone of our greedy sparse core optimization strategy. After optimizing, we scale ˆW back by denormalizing, which is performed by pre-scaling the rows and columns of and by r(1) and r(2) respectively before inference. 4 Preprint. Under review. Algorithm 1 ARMOR Optimization Algorithm Require: Original weight , calibration data X, tolerance ϵ. Ensure: Optimized factorization parameters θ = {A, B, , }. 1: 2: Normalize(W ) 3: (θ)0 Initialize(W , X) 4: for = 1, 2, 3, ..., niters do 5: 6: 7: end for 8: return At, Bt, , Mt (A)t, (B)t, (W )tcont ContinuousUpdate((A)t1, (B)t1, (W )t1, (M )t1, , X) (W )t, (M )t SparseCoreUpdate((A)t, (B)t, (W )tcont, (M )t1, , X) Initialization Using row and column normalization from NoWag Initialize according to Eq"
        },
        {
            "title": "3.3 OPTIMIZATION ALGORITHM",
            "content": "To optimize the ARMOR factorization, we utilize block coordinate descent (Wright, 2015) algorithm that alternates between updating the continuous parameters A, B, and the sparse core . These alternating updates are performed for niters iterations. The overall algorithm is outlined in Algorithm 1. detailed technical description of the optimization algorithm can be found in appendix B. Initialization We initialize our factorization (θ)0 = ((A)0, (B)0, (W )0, (M )0) as: (A)0 = I, (B)0 = I, (W )0 = , (Mij)0 = (cid:0)Wi,j top (cid:0)Wi,[c] (cid:1)(cid:1) [dout], [din], (3) where = j/4 and denotes the identity matrix of appropriate size. This initialization is chosen as it is the optimal solution to equation 2 if ˆW only consisted of the naive sparse core, and is equivalent to the pruning result of NoWag-P pruning algorithm (Liu et al., 2025). 3.3.1 UPDATING OF CONTINUOUS PARAMETERS This step uses sequential gradient descent to update A, B, and sequentially, with the learning rate determined via local β-smoothness. Algorithm 2 depicts single step in detail. In practice, we replace these sequential steps with joint Adam (Kingma & Ba, 2014) optimization that updates A, B, and simultaneously, choice driven primarily by efficiency. This approach requires only one forward/backward pass per iteration and eliminates the need to recalculate local β-smoothness at each step. While Adam introduces minor additional memory overhead, this is negligible since we optimize each layer independently. We present the sequential gradient descent version here to provide strict theoretical guarantees of convergence. In practice, however, joint Adam optimization yields no significant differences compared to sequential gradient descent. 3.3.2 UPDATING THE SPARSE CORE This step updates the sparse core to reduce the proxy loss. To avoid the exponential large search space, we adopt greedy approach, where we select and update fraction of the elements of the sparse core to reduce the proxy loss. An illustration of the algorithm is depicted in Figure 2 and mathematical description can be found in Appendix B.1. Below we elaborate the technical novelty and efficiency of the procedure. Leveraging the 2:4 Pattern If we freeze all of the sparse core beyond single consecutive sparse group, finding the optimal values for this sparse group can be performed as follows. Since there are only (cid:0)4 (cid:1) = 6 possible mask choices for group, it is computationally tractable to consider each possible mask. For possible mask choice, finding the proxy loss minimizing values for the 2 nonzero elements of the group is least squares problem. Thus we can solve the least squares for all 6 possible mask choices and select the one with the smallest minimal proxy loss. 2 5 Preprint. Under review. Figure 2: An illustration of the sparse core update step of the ARMOR optimization algorithm Leveraging the Elementwise Property of the Proxy Loss Updating single 4 element sparse group usually has minimal impact on the layerwise proxy loss, since 4 elements account for less than 0.2 106 of the parameters of typical LLM layer. To enable updates of more than 4 elements at time, we leverage the element wise property of the proxy loss. As discussed previously, this allows us to break the optimization problem into broken independent block-level subproblems: LW,X (θ) = (cid:88) (cid:88) i= j=1 ℓ(i,j) (θ(i,j)) (cid:88) (cid:88) = i= j=1 (cid:13) (i,j) A(i) (cid:16) (cid:13) (cid:13) (i,j) (i,j)(cid:17) B(j)(cid:13) 2 (cid:13) (cid:13) F,diag(XX )(j) , (4) [din/dblock]. Thus we can optimize each block, (cid:0)W (i,j) (i,j)(cid:1), where [dout/dblock], independently through the greedy least squares process outlined above. This allows for parallel updates of (dindout)/d2 block groups at time, which for standard LLM translates to on the order of 103 more elements updated at once. Selecting The Sparse Group We select the sparse groups randomly, with their selection probabilities weighted by their proxy loss gradient. For block (i, j), the probability p(i,j) (i,k) of selecting group i, is: p(i,j) (i,k) (cid:16) (cid:13) (cid:13) (cid:13) (cid:13) (W )(i,j) ℓ(i,j) (cid:17) i,[k] (cid:13) (cid:13) (cid:13) (cid:13)1 [dblock], [dblock/4]. Such selection heuristic results in more focus on the important sparse groups through proxy loss gradient weighting. Additionally, the randomness helps prevent selecting the same group over and over again. Empirically we observer that this leads to faster and better convergence of the proxy loss, and thus better overall LLM performance retention. An ablation of this selection heuristic can be found in Appendix E.1. 3.4 THEORETICAL RESULTS In this section, we establish the following theorem to show that our algorithm guarantees convergence Theorem 3.1. {LW,X ((θ)t)}t0 converges and LW,X ((θ)t) LW,X ((θ)0) > 0. the ARMOR optimization algorithm). (Convergence of The sequence The proof can be found in Appendix C. Since ARMOR factorization initialization is equivalent to NoWag-P, LW,X ((θ)0) is the proxy loss of NoWag-P. Thus ARMOR will perform at least equivalently to NoWag-P SOTA pruning algorithm, in terms of proxy loss. 6 Preprint. Under review."
        },
        {
            "title": "4 RESULTS",
            "content": "Models and Experimental Setup We demonstrate the efficacy of our pruning method on two contemporary model families: Qwen 2.5 (7B, 14B, 32B, and 72B) and Qwen 3 (8B and 14B) (Yang et al., 2025). Our investigation targets foundational base models of 7B parameters and larger. This focus on base models allows for direct assessment of our algorithms effect on the core knowledge acquired during pre-training, removing confounding variables introduced by instruction tuning or other post-training modifications. Furthermore, we concentrate on dense architectures, excluding Mixture-of-Experts (MoE) variants, as recent work suggests that MoE models benefit from specialized pruning strategies (Xie et al., 2024; Li et al., 2025b). For all pruning experiments, we configured the block size, dblock, to 128 and executed the proxy loss optimization for 20,000 iterations. Additional implementation details are provided in Appendix G. Evaluation To comprehensively assess performance degradation, we employed two-pronged evaluation strategy. First, to measure practical performance on downstream tasks, we evaluated the pruned Qwen models on suite of seven industry-standard benchmarks using the LM Eval Harness (Gao et al., 2024). These benchmarks cover range of capabilities, including commonsense and complex reasoning, mathematical problem-solving, and world knowledge. detailed description of each benchmark is available in Appendix F. Second, to ensure comparability with the broader model compression literature, which often relies on perplexity metrics, we conducted an additional set of experiments. For this, we pruned models from the Llama-2 (7B, 13B, and 70B) (Touvron et al., 2023) and Llama-3 (8B and 70B) (Dubey et al., 2024) families. We then evaluated their perplexity on the test split of Wikitext2 (Merity et al., 2016) and subset of the C4 validation split (Dodge et al., 2021), following standard evaluation protocols in the field. Baselines We compare ARMOR against 3 leading pruning methods, SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2024), NoWag-P (Liu et al., 2025). Using NoWag-P is of particular interest, since as discussed previously, ARMOR uses the same proxy loss and is initalized at NoWagP. Therefore comparing ARMOR against NoWag-P servers as an ablation to evaluate the empirical effectiveness of the ARMOR factorization and optimization algorithm. 4.1 TASK BASED EVALUATIONS Results of the task based evaluations on Qwen 2.5 (7B/14B/32B/72B) and Qwen 3 (8B/14B) models are shown in Tables 1 and 2 respectively. Across all 7 tasks and all models evaluated, ARMOR consistently and significantly outperforms the state-of-the-art pruning methods. For example, on GPQA with the Qwen 2.5-32B model, ARMOR achieves score of 39.51, outperforming even the dense models performance (38.84) and vastly exceeding the next best pruning method, SparseGPT, which scored 30.36. The performance gains are especially pronounced in reasoning or domain expertise heavy tasks like GSM8K, BBH, GQPA, demonstrating that our factorization approach is more effective at preserving the complex capabilities of the model compared to simply removing weights. 4.2 PERPLEXITY BASED EVALUATIONS Results of perplexity based evaluations on Llama-2 (7B/13B/70B) and Llama-3 (8B/70B) models are reported in Table 3. Closely reflecting the task based results, ARMOR consistently and significantly outperforms the state-of-the-art pruning methods in retaining lower perplexity, key indicator of language modeling quality. For example, on Llama-2-13B evaluated on Wikitext2, ARMOR achieves perplexity of 6.37. This is dramatic improvement over the next-best baseline (NoWag-P at 8.28) and represents reduction of nearly 50% in the perplexity gap relative to the original dense model. We observe similar substantial gains across all evaluated Llama models and datasets, reinforcing that the ARMOR factorization preserves model quality more effectively than existing 2:4 pruning techniques. 7 Preprint. Under review. Method Dense (2.5-7B) Dense (2.5-14B) Dense (2.5-32B) Dense (2.5-72B) SparseGPT (2.5-7B) Wanda (2.5-7B) NoWag-P (2.5-7B) ARMOR (2.5-7B) SparseGPT (2.5-14B) Wanda (2.5-14B) NoWag-P (2.5-14B) ARMOR (2.5-14B ) SparseGPT (2.5-32B) Wanda (2.5-32B) NoWag-P (2.5-32B) ARMOR (2.5-32B ) SparseGPT (2.5-72B) Wanda (2.5-72B) NoWag-P (2.5-72B) ARMOR (2.5-72B ) Sparsity 0 0 0 0 2:4 2:4 2:4 2:4+4.95% 2:4 2:4 2:4 2:4+4.17% 2:4 2:4 2:4 2:4+3.44% 2:4 2:4 2:4 2:4+2.4% MMLU 74.19 79.8 83.24 86.06 56.91 52.21 53.51 65.56 64.21 59.98 58.45 70.55 75.26 75.45 74.89 78.18 78.71 79.61 78.93 82. GSM8K 82.33 88.02 88.78 89.54 36.69 31.00 28.28 53.28 46.55 49.36 45.87 67.17 66.03 72.93 68.69 78.77 72.27 75.66 75.13 82.11 Task Accuracy (%) () BBH 69.16 75.18 81.72 85.01 46.31 41.39 39.98 55.11 56.44 54.51 52.1 67.24 71.31 70.17 69.53 76.56 75.31 75.96 76.04 79.42 GPQA 33.03 38.17 38.84 42.63 29.69 25.45 27.23 31.47 31.92 29.91 30.36 33.48 30.36 35.27 27.01 39.51 27.46 23.88 28.35 40.40 ARC-C Wino 76.09 59.55 80.9 64.51 81.22 66.30 82.16 68.52 68.35 43.43 63.37 37.80 64.01 39.16 70.96 48.63 71.51 48.21 69.38 46.16 68.67 43.77 74.82 53.75 79.01 57.08 77.82 55.72 76.64 55.29 79.32 60.15 80.03 62.29 80.19 62.37 79.08 60.84 80.90 63.40 Hella 60.03 63.46 65.12 67.63 47.04 43.81 44.33 51.67 49.73 48.24 48.42 55.18 55.73 55.71 55.69 59.78 58.71 59.43 59.31 62.64 Table 1: Results of Qwen-2.5 7B/14B/32B/72B. The additional o% for ARMOR pruned models represent the relative overhead of the block diagonal matricies. ARC-C is short for ARC-Challenge, Wino is short for WinoGrande, Hella is short for HellaSwag. (2.5-7B) denotes Qwen 2.5-7B, (2.514B) denotes Qwen 2.5-14B, etc. Method Dense (3-8B) Dense (3-14B) SparseGPT (3-8B) Wanda (3-8B) NoWag-P (3-8B) Ours (3-8B) SparseGPT (3-14B) Wanda (3-14B) NoWag-P (3-14B) Ours (3-14B ) Sparsity 0 0 2:4 2:4 2:4 2:4+5.03% 2:4 2:4 2:4 2:4+3.89% MMLU GSM8K 76.82 80.47 55.77 55.75 54.1 66.22 64.73 62.93 61.69 71.43 84.91 84.0 33.36 27.45 28.28 50.8 48.22 52.16 46.63 63.38 Task Accuracy (%) () BBH 77.42 78.50 52.96 46.32 43.37 60.13 61.5 57.53 56.11 68.28 GPQA 42.86 39.29 32.14 29.46 28.57 33.93 27.9 29.02 27.46 29. ARC-C Wino 76.87 63.65 79.08 66.64 66.46 44.54 62.75 41.55 61.48 40.02 68.59 50.34 71.27 53.58 69.14 50.51 68.27 48.89 74.35 56.31 Hella 58.92 61.81 44.74 42.92 42.78 49.55 50.22 48.61 48.42 53.77 Table 2: Results of Qwen-3 8B/14B Base. Same setup as the Qwen 2.5 results, Once again (3-8B) denotes Qwen-3 8B etc 4.3 INFERENCE EFFICIENCY To quantify the real world performance efficiency benefits of ARMOR pruned models, we benchmarked the generation speed, max VRAM, and model size of the original, 2:4 pruned, and ARMOR pruned Qwen 2.5 7B and 14B parameter models. The results are shown in Table 4. Additionally, we benchmarked the individual batched (batch size of 8192) Matrix Vector timing for standard layer of Qwen 2.5-14B comparing dense matrix, 2:4 sparse matrix, and ARMOR factorized. Both experiments confirm that the ARMOR block diagonal matrices carry only minor overhead, enabling ARMOR to retrain the speedup, model size reduction, and max VRAM reduction of properties of 2:4 pruning. 8 Preprint. Under review. Method Dense SparseGPT Wanda NoWag-P ARMOR Sparsity 0% 2:4 2:4 2:4 2:4+o 2-7B 5.12 10.16 11.35 11.14 7.21 Wikitext 2 () 2-70B 3.12 5.39 5.20 5.17 4. 3-8B 5.54 14.18 22.42 24.0 10.10 2-13B 4.57 8.39 8.36 8.28 6.37 3-70B 2.58 8.65 8.29 7.52 5.95 2-7B 6.63 11.98 13.80 13.91 9.36 2-13B 6.05 10.22 10.96 11.05 8.59 C4 () 2-70B 4.97 7.20 7.19 7.23 6. 3-8B 7.10 13.88 21.63 23.5 11.22 3-70B 5.78 9.27 9.63 9.18 7.50 Table 3: Wikitext 2 and C4 perplexities on Llama-2 7B/13B/70B and Llama-3 8B/70B. Perplexity evaluations performed at 4096 and 8192 context length for Llama-2 and Llama-3 models respectively. denotes the relative overhead of the block diagonal matricies, which is 4.94%, 3.95%, 2.42% for the 7B, 13B, and 70B parameter models respectively. Following established notation 27B denotes Llama-2 7B etc Qwen 2.5 7B Qwen 2.5 14B Dense 2:4 ARMOR Tokens/s 4461 5430 (1.217x) 5090 (1.141x) Max VRAM Model Size 14.23GB 8.89GB 9.25GB 32.84GB 27.52GB 28.11GB Tokens/s 2013 2157 (1.071x) 2096 (1.041x) Max VRAM Model Size 27.65GB 16.81GB 17.85GB 41.13GB 30.29GB 31.32GB Batched MatVec (ms) 9.04 4.85 (1.86x) 5.77 (1.57x) Table 4: Inference speed, Max VRAM, and Model Size for Dense, naive 2:4 pruning, and ARMOR pruned Qwen 2.5 7B/14B. Qwen2.5 7B and Qwen2.5 14B generation was performed at batch size 2048 and 512 respectively. Rightmost column lists the timings and speedup of batched Matrix Vector multiplication between batch of 8192 input activations and dense, 2:4 pruned, and ARMOR factorized matrix for standard gate proj layer of Qwen 2.5 14B. Figure 3: Left: Relative average Proxy Loss and C4 Perplexity of Llama-2 7B across 20,000 iterations of the ARMOR Proxy Loss optimization algorithm with block size 128. Right: Relative C4 Perplexity for Lama-2 7B/13B, and Llama-3 8B across block sizes of 1, 8, 16, 32, 64, and 128. Each block size was only optimized for 5000 iterations due to time constraints. Relative perplexity is with respect to initial and optimal (dense) perplexities. 4.4 ABLATIONS To validate our proxy loss choice, we track its relative value against the models relative C4 perplexity during 20,000 iterations of the ARMOR proxy loss optimization algorithm. The left plot in Figure 3 shows strong correlation between the two metrics; as the proxy loss decreases, so does perplexity, confirming its utility as surrogate for overall model performance. Furthermore, we observe that the majority of the performance loss reduction was achieved within the first 2,500 iterations. We also performed an ablation study on block size to understand its impact on perplexity (Figure 3, right). This ablation reveals clear trend across all models: increasing the block size improves performance by lowering perplexity in an exponential decaying manner. Additional ablations are detailed in Appendix E. Preprint. Under review."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce ARMOR, novel one-shot algorithm that addresses the significant performance degradation of hardware-accelerated 2:4 pruning. Instead of simply removing weights, ARMOR reframes the problem by factorizing each weight matrix into 2:4 sparse core and adaptive, low-overhead block diagonal wrappers that act as error correctors. This approach is theoretically guaranteed to converge to solution with proxy loss less than or equal to state-of-the-art methods and is empirically validated on Llama and Qwen family models, where it consistently and significantly outperforms existing 2:4 pruning techniques on both perplexity and downstream tasks. Crucially, ARMOR achieves these accuracy gains while retaining the majority of the inference speedups and memory reduction of native 2:4 sparsity. Our work demonstrates that rethinking weight representation is powerful path toward establishing more effective trade-off between the performance and efficiency of large language models."
        },
        {
            "title": "6 REPRODUCIBILITY STATEMENT",
            "content": "We are committed to ensuring the reproducibility of our results, the source code for this project is currently being prepared for public release and will be made available soon. To facilitate replication, we have also provided detailed descriptions of our model architecture, algorithms, hyperparameter settings, and experimental setup in Appendix G."
        },
        {
            "title": "7 ACKNOWLEDGMENTS",
            "content": "LY and LL are supported in part by NSF Grant 2221871. LY is also supported by an Amazon Faculty Award."
        },
        {
            "title": "REFERENCES",
            "content": "Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024. Vladimır Boˇza and Vladimır Macko. Two sparse matrices are better than one: Sparsifying neural networks with double sparse factorization. arXiv preprint arXiv:2409.18850, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Rocktim Jyoti Das, Mingjie Sun, Liqun Ma, and Zhiqiang Shen. Beyond size: How gradients shape pruning decisions in large language models. arXiv preprint arXiv:2311.04902, 2023. Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, and Matt Gardner. Documenting the english colossal clean crawled corpus. ArXiv, abs/2104.08758, 2021. URL https://api.semanticscholar.org/CorpusID:233296858. Peijie Dong, Lujun Li, Zhenheng Tang, Xiang Liu, Xinglin Pan, Qiang Wang, and Xiaowen Chu. Pruner-zero: Evolving symbolic pruning metric from scratch for large language models. arXiv preprint arXiv:2406.02924, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. 10 Preprint. Under review. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning, pp. 1032310337. PMLR, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR), 2021a. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021b. Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, and Jun Zhu. Accelerating transformer pretraining with 2: 4 sparsity. arXiv preprint arXiv:2404.01847, 2024. Yichen Huang and Lin Yang. Gemini 2.5 pro capable of winning gold at imo 2025. arXiv preprint arXiv:2507.15855, 2025. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Woosuk Kwon, Sehoon Kim, Michael Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. fast post-training pruning framework for transformers. Advances in Neural Information Processing Systems, 35:2410124116, 2022. Xinlin Li, Osama Hanna, Christina Fragouli, and Suhas Diggavi. Icquant: Index coding enables low-bit llm quantization. arXiv preprint arXiv:2505.00850, 2025a. Zichong Li, Chen Liang, Zixuan Zhang, Ilgee Hong, Young Jin Kim, Weizhu Chen, and Tuo Zhao. Slimmoe: Structured compression of large moe models via expert slimming and distillation. arXiv preprint arXiv:2506.18349, 2025b. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024. Lawrence Liu, Inesh Chakrabarti, Yixiao Li, Mengdi Wang, Tuo Zhao, and Lin Yang. Nowag: unified framework for shape preserving compression of large language models. arXiv preprint arXiv:2504.14569, 2025. Lian Liu, Xiandong Zhao, Guanchen Li, Dong Li, Mengdi Wang, Yinhe Han, Xiaowei Li, et al. Bawa: Automatic optimizing pruning metric for large language models with balanced weight and activation. In Forty-second International Conference on Machine Learning. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. 11 Preprint. Under review. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pp. 122, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Daria Soboleva, and Nolan Dey. Faisal Al-Khateeb, Robert Myers, tness, plicated slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. Joel Hesdeduand https://www.cerebras.net/blog/ Jacob Steeves, cleaned 627B token SlimPajama: RedPajama. version of Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=PxoFut3dWW. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Zhendong Tan, Xingjun Zhang, and Zheng Wei. Wrp: Weight recover prune for structured sparsity. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 64336443, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Stephen Wright. Coordinate descent algorithms. Mathematical programming, 151(1):334, 2015. Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity. arXiv preprint arXiv:2309.10285, 2023. Yanyue Xie, Zhi Zhang, Ding Zhou, Cong Xie, Ziang Song, Xin Liu, Yanzhi Wang, Xue Lin, and An Xu. Moe-pruner: Pruning mixture-of-experts large language model using the hints from its router. arXiv preprint arXiv:2410.12013, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. Plugand-play: An efficient post-training pruning method for large language models. 2024. Preprint. Under review."
        },
        {
            "title": "A NOTATION IN DEPTH",
            "content": "As established in section 3.1, for block diagonal matrix Rdd with block size dblock, we denote the individual blocks as: = D(1) 0 ..."
        },
        {
            "title": "0\nD(2)\n...\n0",
            "content": ". . . 0 0 ... (cid:16) dblock (cid:17) where D(i) Rdblockdblock [1, d/dblock]. More generally for matrix Rd1d2 we denote the dblock dblock matrix blocks as: = C (1,1) ... (cid:16) d1 dblock (cid:17) ,1 (1,2) ... (cid:16) dblock (cid:17) ,2 . . . (cid:16) 1, d2 dblock (cid:17) ... (cid:16) d1 dblock (cid:17) , d2 dblock"
        },
        {
            "title": "B ARMOR OPTIMIZATION ALGORITHM IN DEPTH",
            "content": "In section B.1, we elaborate on the sparse core update step more formally. Additionally we provided pseudo code for both the sparse core update and the continuous parameter update steps in B.2. B.1 THE SPARSE CORE UPDATE STEP IN DEPTH Decomposing equation 2 into independent subproblems at the block matrix level: LW,X (θ) = (cid:88) (cid:88) i=1 j=1 ℓ(i,j) (θ(i,j)) = (cid:88) (cid:88) i= j=1 (cid:13) (i,j) A(i) (cid:16) (cid:13) (cid:13) (i,j) (i,j)(cid:17) B(j)(cid:13) 2 (cid:13) (cid:13) F,diag(XX )(j) (5) where [dout/dblock] and [din/dblock]. We optimize each block subproblem in parallel, for the remainder of this section we will consider block (i, j). Optimizing these subproblems still requires sweep over exponential search space, so we adopt greedy approach that optimizes single sparse group for each subproblem. Optimizing the Sparse Group After selecting group i, k, we freeze all values beyond (W )(i,j) i,[k], which we optimize with the 2:4 constraint. This is computationally feasible (cid:1) = 6 possible mask choices. Let us consider one such mask since we only have to sweep over (cid:0)4 {0, 1}4 s.t. m0 = 2, which has unmasked indices i1, i2 [4] ie mi1 = mi2 = 1. Let us denote the unmasked elements for this mask as wmR2, and = 4(k 1). For this mask choice, the corresponding best-case proxy loss is given by: 2 ℓ(i,j) (m) = min wm (cid:13) (i,j) A(i)W (i,j)B(j) A(i) (cid:13) (cid:13) :,iwmB(j) {k+i1,k+i2},: (cid:13) 2 (cid:13) (cid:13) F,diag(XX )(j) (6) where (i,j) represents the frozen remainder of the sparse core: (i,j) n,p = (cid:40) 0 (i,j) n,p (i,j) n,p if = i, + 1 + 4 otherwise n, [dblock] Equation 6 can be arranged into linear least squares problem with closed form solution. For ease of notation, let = (i,j) A(i)W (i,j)B(j), = B(j) :,i and D(j) = diag (cid:0)XX (cid:1)(j) . Solving the least squares problem results in an optimal ℓ {k+i1,k+i2},:, = A(i) = 2 ℓ F,D(j) (cid:16) 1 a2 BD(j)W (cid:17)T (cid:16) BD(j)BT (cid:17) (cid:16) BD(j)W (cid:17) (7) 13 Preprint. Under review. The first term is independent of the mask choice for the group, thus when sweeping over the possible masks, we only compute the second term for efficiency. The optimum is achieved with unmasked weights m: = 1 a2 2 (cid:16) BD(j)BT (cid:17) (cid:16) BD(j)W (cid:17) (8) After sweeping over all 6 possible masks, we select the mask m, with nonzero values at 1, i2, that achieves the minimum best-case proxy loss as given by equation 7. We substitute the corresponding optimal = given by equation 8 into to fully update sparse core, i.e. we set (i,j) new (cid:17) (cid:16) i,k+i w1 and (cid:16) (i,j) new (cid:17) = w2. i,k+i The 2:4 semi-structured sparsity allows for efficient calculation of equations 7 and 8. Since there are only 2 unmasked values for each group (cid:0)BD(j)BT (cid:1) and (cid:0)BD(j)W a(cid:1) are of shape 2 2 and 2 1. Thus the computational cost for solving for (cid:0)BD(j)BT (cid:1) (cid:0)BD(j)W a(cid:1) are negligible. Rather, calculating equations 7 and 8 are dominated by calculating (cid:0)BD(j)BT (cid:1) and (cid:0)BD(j)W a(cid:1), which scale with d2 block blocks at once, thus the overall computational complexity is O(dindout), scaling linearly with parameter size. block. We update all dindout/d2 B.2 ALGORITHM PSEUDOCODE ˆW A(W )B ˆW F,diag(XX ) Algorithm 2 Continuous Optimization via Sequential Gradient Descent Require: Current parameters A, B, , mask , normalized weight , calibration data X. Ensure: Updated continuous parameters A, B, . 1: function CONTINUOUSUPDATE(A, B, , M, , X) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end function ηW ComputeLearningRate(W , L) ηW L return A, B, ηB ComputeLearningRate(B, L) ηBBL ηA ComputeLearningRate(A, L) ηAAL Compute proxy loss from Eq. 2 Update block diagonal matrix Based on local smoothness, Eq. 9 Update underlying weight matrix Eq. 11 Update block diagonal matrix Eq. 10 Preprint. Under review. Algorithm 3 Greedy Sparse Core Update Require: Current parameters A, B, , mask , normalized weight , calibration data X. Ensure: Updated sparse core parameters , . 1: function SPARSECOREUPDATE(A, B, , M, , X) 2: 3: 4: for each sparse group (i, k) in block (i, j) do for each block (i, j) in parallel do Select single 2:4 sparse group to update within the block Calculate selection probability Find the best mask and weights for the selected group (cid:1) = 6 possible masks for group of 4. (cid:13) (cid:13) (cid:13)(M )(i,j) p(i,j) (i,k) end for Select group (i, k) based on probabilities p(i,j). ℓ(i,j)(cid:13) (cid:13) (cid:13)1 ,[k] Let be the set of all (cid:0)4 lbest for each candidate mask do Calculate the resulting loss if 2 using Eq. 7 < lbest then lbest m end if end for Calculate optimal weights Update mask for group (i, k) in (i,j) with m. Update corresponding weights in (i,j) with m. 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end function end for return , for the unmasked entries using Eq. 8 Update the mask and weights for the chosen group"
        },
        {
            "title": "C PROOFS",
            "content": "PROOF FOR PROPOSITION 1 Proposition 1. The proxy loss is bounded below by 0 and is convex with respect to A, B, and individually. formal proof is provided in Appendix Proof. The proxy loss is defined as: LW,X (θ) = ˆW 2 F,diag(XX ) = (W ij ˆWij)2Xj2 (cid:88) i,j where ˆW = A(W )B. 1. Bounded Below by 0: For each term in the summation, (W ij ˆWij)2 0 as it is squared real number. The weighting term Xj2 2 is the squared Euclidean norm of vector, which is also non-negative. sum of non-negative terms is non-negative. Thus, LW,X ( ˆW ) 0. 2. Convexity with respect to A: When B, , and are held fixed, let = (W )B. The proxy loss function can be written as function of A: LW,X (A) = AS2 F,D where = diag(XX ). This is weighted least squares objective. The function (A) = AS is an affine (linear) transformation of A. The function g(Z) = Z2 F,D is squared weighted Frobenius norm, which is convex function. The composition of convex function with an affine transformation is convex. Therefore, the proxy loss is convex with respect to A. 15 Preprint. Under review. 3. Convexity with respect to B: Similarly, when A, , and are held fixed, let = A(W ). The proxy loss function can be written as function of B: LW,X (B) = SB2 F,D This is also weighted least squares objective. Using the same reasoning as for A, the function is composition of convex function (squared norm) with an affine transformation of B, and thus is convex with respect to B. 4. Convexity with respect to : When A, B, and are held fixed, the reconstructed weight matrix ˆW = A(W )B is linear function of the elements of . Let = vec(W ), the vector of elements in . Then vec( ˆW ) is linear transformation of w. The objective function LW,X ( ˆW ) is quadratic function of the elements of ˆW , and therefore quadratic function of the elements of . Specifically, it is positive semidefinite quadratic form, which is convex. Therefore, the proxy loss is convex with respect to . This completes the proof. PROOF FOR LEMMA C.1 Lemma C.1. The continuous parameter update step results in equal or lower proxy loss: LW,X (cid:16) ˆW (cid:0)(A)t+1, (B)t+1, (W )t+1cont, (M )t (cid:1)(cid:17) LW,X (cid:16) ˆW ((A)t, (B)t, (W )t, (M )t) (cid:17) . Proof. The paper states that the continuous optimization step updates the parameters A, B, and using sequential gradient descent (Algorithm 2). The process is iterative: 1. Update A: (A)t+1 = (A)t ηAALW,X ((A)t, (B)t, (W )t, (M )t) 2. Update B: (B)t+1 = (B)t ηBBLW,X ((A)t+1, (B)t, (W )t, (M )t) 3. Update : (W )t+1cont = (W )t ηW LW,X ((A)t+1, (B)t+1, (W )t, (M )t) From Proposition 1, the loss function LW,X is convex with respect to each of A, B, and individually. As Algorithm 2 states, the learning rate η is determined via local β-smoothness, which are calculated in D. For convex and β-smooth function (x), the gradient descent update xk+1 = xk ηf (xk) with step size 0 < η 1/β guarantees that (xk+1) (xk). Applying this property to each step of the sequential update: 1. The update of ensures that LW,X ((A)t+1, (B)t, (W )t, (M )t) LW,X ((A)t, (B)t, (W )t, (M )t) 2. The update of B, starting from the new A, ensures that LW,X ((A)t+1, (B)t+1, (W )t, (M )t) LW,X ((A)t+1, (B)t, (W )t, (M )t) 3. The update of , starting from the new and B, ensures that LW,X ((A)t+1, (B)t+1, (W )t+1cont, (M )t) LW,X ((A)t+1, (B)t+1, (W )t, (M )t) Chaining these inequalities together, we get: LW,X ((A)t+1, (B)t+1, (W )t+1cont, (M )t) LW,X ((A)t+1, (B)t+1, (W )t, (M )t) LW,X ((A)t+1, (B)t, (W )t, (M )t) LW,X ((A)t, (B)t, (W )t, (M )t) Thus, each continuous optimization step is guaranteed to not increase the proxy loss. 16 Preprint. Under review. PROOF FOR LEMMA C.2 Lemma C.2. The sparse core update step results in equal or lower proxy loss: LW,X (cid:16) ˆW ((A)t+1, (B)t+1, (W )t+1, (M )t+1) (cid:17) LW,X (cid:16) ˆW (cid:0)(A)t+1, (B)t+1, (W )t+1cont, (M )t (cid:1)(cid:17) . Proof. The discrete optimization step (Algorithm 3) seeks to improve the sparse core . The total proxy loss is decomposable into independent subproblems for each matrix block (i, j), as shown in Equation 5: LW,X (θ) = dout/dblock (cid:88) din/dblock (cid:88) i= j=1 (i,j) A(i)((W )(i,j))B(j)2 F,diag(XX )(j) The algorithm updates single 2:4 sparse group (i, k) within each block (i, j). Lets focus on one such update. Let the loss before the update for this specific group be lbef ore. The current mask for this group is mold, which is one of the (cid:0)4 The algorithm proceeds as follows: (cid:1) = 6 possible masks. 2 1. For each of the 6 possible masks in the set of valid masks M, it calculates the optim that minimize the loss for that group, assuming that mask is chosen mal weights (Equation 8). This results in the best possible loss for that mask choice (Equation 7). 2. It then selects the mask that yields the minimum loss among all 6 possibilities: lbest = minmM m. 3. The algorithm updates the mask for group (i, k) to and its corresponding weights in to m . The loss with the original mask mold and its weights before the update is lbef ore. The optimized loss for this original mask, mold , must be less than or equal to lbef ore, since Equation 7 finds the optimal weights for any given mask. mold lbef ore By definition, the selected mask is the one that minimizes this optimal loss across all 6 choices. Therefore, its loss lbest must be less than or equal to the optimal loss for the old mask mold. lbest mold Combining these, we have lbest lbef ore. The update for the selected group can only decrease or maintain the proxy loss. Since this holds for the update in each block, and the block losses are additive, the total proxy loss after the discrete optimization step is guaranteed to be less than or equal to the loss before the step. PROOF FOR THEOREM 3.1 (CONVERGENCE OF THE ARMOR OPTIMIZATION ALGORITHM) Theorem 3.1. {LW,X ((θ)t)}t0 converges and LW,X ((θ)t) LW,X ((θ)0) > 0. the ARMOR optimization algorithm). (Convergence of The sequence Proof. Let Lt = LW,X ((θ)t) be the value of the proxy loss at the end of iteration t. Each iteration of the ARMOR optimization algorithm consists of continuous optimization step followed by discrete optimization step. From Lemma C.1, the continuous step does not increase the loss. Let the loss after the continuous step at iteration be Lt+1cont. We have: Lt+1cont Lt From Lemma C.2, the sparse core step, which follows the continuous step, also does not increase the loss. Let the loss after the discrete step be Lt+1. We have: Lt+1 Lt+1cont 17 Preprint. Under review. Combining these two results gives: Lt+1 Lt This shows that the sequence of proxy losses {Lt}t0 is monotonically non-increasing. By induction, this implies that for any > 0, Lt L0. Furthermore, from Proposition 1, the proxy loss is bounded below by 0. Therefore, {Lt}t0 is monotonically non-increasing sequence that is bounded below. By the Monotone Convergence Theorem, any such sequence must converge to limit. Therefore, the sequence {LW,X ((θ)t)}t0 converges, and its value is always less than or equal to its initial value."
        },
        {
            "title": "D BETA SMOOTHNESS OF PROXY LOSS",
            "content": "In this section we present the proofs that the proxy loss is β-smooth with respect to each of the parameters A, B, . And using them, derive the learning rates for the sequential gradient descent based continous optimization step. D.1 PROXY LOSS β-SMOOTH WITH RESPECT TO First let us consider the β-smoothness of the proxy loss for block (i, j) with respect to A(i). We have that: ℓ(i,j) (cid:16) θ(i,j)(cid:17) = (i,j) A(i)(M (i,j) (i,j))B(j)(cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) F,diag(XX )(j) Where θ(i,j) = (A(i), B(j), (i,j), (i,j)). We have that: θ(i,j)(cid:17) S(i,j)D(j)S(i,j)T (cid:17) = 2A(i) (cid:16) A(i)ℓ(i,j) (cid:16) 2 (i,j)D(j)S(i,j)T Where S(i,j) = (M (i,j) (i,j))B(j) and D(j) = diag (cid:0)XX (cid:1)(j) A(1), A(2) Rdblockdblock. Let us denote θ(i,j) (A(i) (2), B(j), (i,j), (i,j)). Then we have that: Then we have that: (1) = (A(i) . (1), B(j), (i,j), (i,j)) and θ(i,j) Consdier two (2) = (cid:13) (cid:13)A(i)ℓ(i,j) (cid:13) (θ(i,j) (1) ) A(i)ℓ(i,j) (cid:13) (θ(i,j) (cid:13) (2) ) (cid:13)F = 2 2 (cid:17) (cid:16) (cid:16) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)A(i) (cid:13) (1) A(i) A(i) (2) (cid:13) (1) A(i) (cid:13) (cid:13)F (2) S(i,j)D(j)S(i,j)T (cid:17)(cid:13) (cid:13) (cid:13)F (cid:13)S(i,j)D(j)S(i,j)T (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) Where the inequality follows from the submultiplicativity of the Frobenius norm. Thus we have that the proxy loss ℓ(i,j) (θ(i,j)) is β-smooth with respect to A(i) with β(i,j) (cid:13)F . We now consider the overall proxy loss, for some A(1), A(2) Rdoutdin. Let θ(1) = (A(1), B, , ) and θ(2) = (A(2), B, , ). Then we have that: (cid:13)S(i,j)D(j)S(i,j)T (cid:13) = 2 (cid:13) (cid:13) (cid:13)ALW,X (θ(1)) ALW,X (θ(2))(cid:13) (cid:13)F = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) dout dblock(cid:88) din dblock(cid:88) i=1 j=1 A(i)ℓ(i,j) (θ(i,j) (1) ) A(i)ℓ(i,j) (θ(i,j) (2) ) dout dblock(cid:88) din dblock(cid:88) i=1 j=1 (cid:13) (cid:13)A(i)ℓ(i,j) (cid:13) (θ(i,j) (1) ) A(i)ℓ(i,j) (θ(i,j) (2) ) 2 (cid:13) (cid:13)A(1) A(2) (cid:13) (cid:13)F dout dblock(cid:88) din dblock(cid:88) i=1 j=1 (cid:13) (cid:13) (cid:13)S(i,j)D(j)S(i,j)T (cid:13) (cid:13) (cid:13)F 18 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13)F Preprint. Under review. Where the first inequality follows from the triangle inequality and the second follows from the previous result. Therefore the overall loss LW,X (θ) is β-smooth with respect to with βA = 2 (cid:80) (cid:80) dout dblock i=1 din dblock j=1 (cid:13)S(i,j)D(j)S(i,j)T (cid:13) (cid:13) 1 βA ηA = = (cid:13)F . This leads us to learning rate of: 1 (cid:13)S(i,j)D(j)S(i,j)T (cid:13) (cid:13) (cid:13)F din dblock j=1 dout dblock i=1 (cid:80) 2 (cid:80) (9) D.2 PROXY LOSS β-SMOOTH WITH RESPECT TO Once again, we start by considering the β-smoothness of the proxy loss for block (i, j) with respect to B(j). Reusing the notation we introduced in section, we have that: B(j) ℓ(i,j) (θ(i,j)) = 2S(i,j)T S(i,j)B(j)D(j) 2S(i,j)T (i,j)D(j) Where S(i,j) = A(i)(M (i,j) (i,j)) and D(j) = diag (cid:0)XX (cid:1)(j) B(1), B(2) Rdblockdblock. Let us denote θ(i,j) (A(i), B(j) (1) = (A(i), B(j) . (1), (i,j), (i,j)) and θ(i,j) Consdier two (2) = (2), (i,j), (i,j)). Then we have that: (1) ) B(j)ℓ(i,j) (cid:13) (θ(i,j) (cid:13) (2) ) (cid:13)F (θ(i,j) (cid:13) (cid:13)B(j) ℓ(i,j) (cid:13) = 2 2 (cid:13) (cid:13) (cid:13)S(i,j)T S(i,j) (cid:16) (cid:13)S(i,j)T S(i,j)(cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:17) (2) (1) B(j) B(j) (cid:13) (cid:13)B(j) (cid:13) (1) B(j) (2) D(j)(cid:13) (cid:13) (cid:13)F (cid:13)D(j)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13)F Thus Where the inequality follows from the submultiplicativity of the Frobenius norm. the proxy loss ℓ(i,j) (θ(i,j)) is β-smooth with respect = we have that (cid:13)S(i,j)T S(i,j)(cid:13) 2 (cid:13) Following the same procedure as the case, consider some (cid:13)F B(1), B(2) Rdindin . Let θ(1) = (A, B(1), , ) and θ(2) = (A, B(2), , ). Then we have that for the overall proxy loss: to B(j) with β(i,j) (cid:13)D(j)(cid:13) (cid:13) (cid:13)F . (cid:13)BLW,X (θ(1)) BLW,X (θ(2))(cid:13) (cid:13) (cid:13)F 2 (cid:13) (cid:13)B(1) B(2) (cid:13) (cid:13)F dout dblock(cid:88) din dblock(cid:88) i=1 j=1 (cid:13) (cid:13) (cid:13)S(i,j)T S(i,j)(cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13)D(j)(cid:13) (cid:13) (cid:13)F Therefore the overall 2 (cid:80) din dblock j=1 dout dblock i=1 (cid:80) loss LW,X (θ) is β-smooth with respect to with βB = (cid:13)S(i,j)T S(i,j)(cid:13) (cid:13) (cid:13)F 1 βB ηB = = (cid:13)D(j)(cid:13) (cid:13) (cid:13)F . This leads us to learning rate of: 2 (cid:80) dout dblock i=1 1 (cid:80) din dblock j=1 (cid:13) (cid:13)S(i,j)T S(i,j)(cid:13) (cid:13)F (cid:13) (cid:13)D(j)(cid:13) (cid:13)F (10) D.3 PROXY LOSS β-SMOOTH WITH RESPECT TO Finally, we consider the β-smoothness of the proxy loss with respect to . We have that: LW,X (θ) = (cid:0)2AT A(W )Bdiag (cid:0)XX (cid:1) BT 2AT diag (cid:0)XX (cid:1) BT (cid:1) Consdier two (A, B, (2), ). Then we have that: (1), (2) Rdoutdin . Let us denote θ(1) = (A, B, (1), ) and θ(2) = (cid:13) (cid:13)W LW,X (θ(1)) LW,X (θ(2))(cid:13) (cid:13)F = 2 (cid:16) (cid:16) AT (cid:16) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)AT (cid:13) (cid:13)AT A(cid:13) (cid:13)F (cid:13)AT A(cid:13) (cid:13)F (W (1) (2)) (cid:17) (W (2)) (1) (cid:13)Bdiag (cid:0)XX (cid:1) BT (cid:13) (cid:13) (cid:13)F (cid:13)Bdiag (cid:0)XX (cid:1) BT (cid:13) (cid:13) (cid:13)F 2 (cid:13) 2 (cid:13) (cid:17) Bdiag (cid:0)XX (cid:1) BT (cid:17) Bdiag (cid:0)XX (cid:1) BT (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13)(W (cid:13) (cid:13) (cid:13)W (cid:13) (1) (2)) (cid:13) (cid:13) (cid:13)F (1) (2) (cid:13) (cid:13) (cid:13)F Preprint. Under review. Where the first and third inequalities follow from the property that F max F for any matrices V, of the same shape, and the second follows from the submultiplicativity of the Frobenius norm. Thus we have that the proxy loss LW,X (θ) is β-smooth with respect to with βW = 2 (cid:13) (cid:13)Bdiag (cid:0)XX (cid:1) BT (cid:13) (cid:13) (cid:13)F . This leads us to learning rate of: (cid:13)AT A(cid:13) (cid:13)F ηW = 1 βW = 1 2 AT AF Bdiag (XX ) BT (11)"
        },
        {
            "title": "E ABLATIONS",
            "content": "E.1 SELECTION HEURISTIC We investigated four choices for the sparse group selection heuristic used in the Discrete Optimization steps: Uniform Random Selection (Random). We select sparse group from the available d2 block/4 sparse groups within sparse core block. L1 Greedy Selection, We directly select the sparse group based on which group has the maximum L1 gradient norm. L2 Random Selection, We draw sparse group with probability that is based on the L2 norm of the gradient, instead of the L1 norm: p(i,j) (i,k) (cid:13) (cid:13) (cid:13)(M )(i,j) i,[k] ℓ(i,j) (cid:13) (cid:13) (cid:13)2 [dblock], [dblock/4]. L1 Random Selection, This is the selection heuristic discussed in the main text and that we use. The results of this ablation is listed in table 5. L1 random and L2 random perform roughly equivalently. Method Random L1 Greedy L2 Random L1 Random Wikitext 2 () 2-13B 2-7B 7.05 8.17 7.28 8.46 7.00 7.95 6.99 7.99 C4 () 2-7B 10.49 10.87 10.27 10.30 2-13B 9.48 9.72 9.38 9.39 Table 5: C4 and Wikitext 2 perplexities of ARMOR pruned Llama-2-7B/13B for different sparse group selection heuristics. Block size of 128 was used, and ARMOR optimization was ran for 2000 iterations to expedite runtime. E.2 CALIBRATION DATASET Dataset RedPajama-Data-1T SlimPajama-627B Wikitext 2 () 7.59 7.68 C4 () 9.89 9.88 Table 6: Perplexity of ARMOR pruned Llama-2-7B on Wikitext2 and C4 Datasets when using RedPajama-Data-1T vs SlimPajama-627B for our calibration dataset. ARMOR is minimally sensitive to the choice of dataset, so long as it is representative of the pre-training dataset. Block size of 128 was used, and ARMOR optimization was ran for 5000 iterations. Preprint. Under review."
        },
        {
            "title": "F FEW SHOT TASK DESCRIPTIONS",
            "content": "ARMOR prund Qwen2.5 and Qwen 3 models were evaluated on an industry standard suite of 7 few shot task benchmarks. The results were reported in Tables 2 and 1. detailed description of each task is included below. HuggingFace accelerate (Gugger et al., 2022) was used for parallel processing, and the EleutherAI LM evaluation harness was used (Gao et al., 2024). max_model_len = 4096 for all benchmarks for speedup. 1. MMLU (Hendrycks et al., 2021b;a) The Massive Multitask Language Understanding (MMLU) benchmark is 57-subject multiple-choice benchmark spanning STEM, humanities, social sciences, and professional domains that evaluates broad knowledge and basic reasoning. We report the 5-shot accuracy. (acc,none). 2. GSM8K (Cobbe et al., 2021) Grade School Math 8K (GSM8K) is dataset of 8.5K high quality linguistically diverse grade school math word problems. Problems require no concepts beyond the level of early Algebra, and solutions are provided in natural language. We report the 8-shot strict match: (exact_match,strict-match). 3. ARC-c (Clark et al., 2018) The challenge subset of the AI2 Reasoning Challenge (ARC), which is composed of grade-school science questions authored for human tests. The challenge subset is composed of questions that baseline algorithms have failed on. We report the 25-shot accuracy. (acc,none). 4. HellaSwag (Zellers et al., 2019) The HellaSwag challenge dataset has models choose the most commonsense continuation of everyday scenarios among four options. The benchmark is intentionally challenging for models. We report the 0-shot acurracy acc,none. 5. BBH (Suzgun et al., 2022) BIG-Bench Hard (BBH) is suite of 23 especially challenging tasks from the Beyond the Imitation Game benchmark (BIG-Bench). These tasks require multi-step reasoning and compositional generalization. We report the 3 shot match (exact_match,get-answer). 6. WinoGrande (Sakaguchi et al., 2021) WinoGrande is large dataset of 44k problems inspired by those of the Winograd Schema Challenge, systematically designed to minimize bias. The questions are of 2-choice fill in the blank format that tests commonsense reasoning. We report the 5-shot accuracy (acc,none). 7. GPQA (Main Set) (Rein et al., 2024) The Graduate-Level Google-Proof Q&A benchmark (GPQA) main set is multiple-choice benchmark of 448 graduate-level biology, physics, and chemistry questions authored by experts. The main set excludes questions that are likely to be not objective and difficult. We report the 5-shot accuracy (acc,none)."
        },
        {
            "title": "G IMPLEMENTATION DETAILS",
            "content": "We used block size of 128 for all models. For optimization, we used learning rate of 104 for ADAM, and ran for 20,000 iterations. We used 128 samples of the SlimPajama-627B dataset (Soboleva et al., 2023) as our calibration dataset. Each sample had context length of 4096 for Llama-2 family models, and 8192 for Llama-3, Qwen 2.5, and Qwen 3 family models. Inference benchmarks were performed on 48GB 4090."
        },
        {
            "title": "H LLM USAGE",
            "content": "We used LLMs to polish and refine our language. Some icons, such as the robot in Figure 1 and the fire and snowflake icon in Figure 2 were generated with generative models. Furthermore, we used LLMs to conduct some literature review to supplement and double check our manual literature review."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Princeton University",
        "University of California, Los Angeles"
    ]
}