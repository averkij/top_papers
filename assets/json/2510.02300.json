{
    "paper_title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models",
    "authors": [
        "Runqian Wang",
        "Yilun Du"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 0 0 3 2 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "EQUILIBRIUM MATCHING: GENERATIVE MODELING WITH IMPLICIT ENERGY-BASED MODELS Runqian Wang MIT raywang4@mit.edu Yilun Du Harvard University ydu@seas.harvard.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Equilibrium Matching (EqM), generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, timeconditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with unified equilibrium landscape, EqM offers tighter bridge between flow and energybased models and simple route to optimization-driven inference. Project website: https://raywang4.github.io/equilibrium_matching/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Generative modeling has advanced rapidly with diffusion and flow-based methods (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al.; Lipman et al.; Liu et al.), which map simple noise distributions to complex data by defining forward noising process and learning its reverse. While they achieve state-of-the-art sample quality (Nichol and Dhariwal, 2021; Dhariwal and Nichol, 2021; Karras et al., 2022), these models employ non-equilibrium dynamics at both training and inference. Diffusion/flow models are conditioned on input timestep and learn distinct dynamics for inputs at different noise levels. This non-equilibrium design imposes practical constraints such as noise level schedule and fixed integration horizon during sampling. Existing approaches that attempt to learn equilibrium dynamics suffer from different problems. Sun et al. (2025) have shown that forcing diffusion models to learn equilibrium dynamics by simply removing time (noise1) conditioning leads to worse generation quality. Energy-based models (EBMs) (LeCun et al., 2006; Du and Mordatch, 2019; Carreira-Perpinan and Hinton, 2005) directly learn equilibrium energy landscapes, but they often suffer from training instabilities (Du et al., 2020b) and poor sample quality (Du and Mordatch, 2019; Nijkamp et al., 2020). More recent approaches such as Energy Matching (Balcerak et al., 2025) involve separate training stages and fail to surpass the generation quality of flow-based method on large-scale datasets. In this work, we introduce Equilibrium Matching (EqM), generative modeling framework from an equilibrium perspective. Equilibrium Matching replaces the time-conditional non-equilibrium dynamics of diffusion/flow models with single time-invariant equilibrium gradient over an implicit energy landscape. We hypothesize that the quality degradation in noise-unconditional diffusion models originates from the incompatibility between the target gradient and equilibrium dynamics. To this end, we introduce new family of target gradients that align with an implicit energy function. We also provide model variants that explicitly learn this energy function. By learning single equilibrium dynamics, Equilibrium Matching supports optimization-based sampling, where samples are obtained by gradient descent on the learned landscape. Unlike existing 1In the context of this work, time conditioning and noise conditioning are equivalent."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Conceptual 2D Visualization. We compare the conceptual 2D dynamics of Equilibrium Matching and Flow Matching under 2 ground truths (marked by stars). Left: Flow Matching learns non-equilibrium velocity that varies over time. Right: Equilibrium Matching learns an equilibrium gradient that is time-invariant. diffusion samplers that integrate along prescribed trajectory, optimization-based sampling supports different step sizes and adaptive optimizers. Existing gradient optimization techniques such as Nesterov Accelerated Gradient can be naturally adopted to achieve better generation quality. Equilibrium Matching can also allocate inference-time compute adaptively. It adjusts the sampling steps for each sample independently based on gradient norm and can save up to 60% of function evaluations. We validate Equilibrium Matching through both theoretical analysis and empirical evidence. Theoretically, we show that Equilibrium Matching is guaranteed to learn the data manifold and produce samples from this manifold using gradient descent. Empirically, Equilibrium Matching achieves 1.90 FID on ImageNet 256256 generation, outperforming existing diffusion and flow-based counterparts in generation quality. Equilibrium Matching also exhibits strong scaling behavior, exceeding the flow-based counterpart at all tested scales. These results suggest that Equilibrium Matching is promising alternative for generative modeling. Beyond generation, Equilibrium Matching demonstrates unique properties that traditional diffusion/flow-based models lack. Equilibrium Matching can generate high-quality samples directly from partially noised inputs, whereas flow-based models only perform well when starting with pure noise. Moreover, Equilibrium Matching can perform out-of-distribution (OOD) detection without relying on any external module. We also show that different Equilibrium Matching models can be added together to generate compositional images in similar way as EBMs. Our results show that Equilibrium Matching offers capabilities unseen in traditional diffusion/flow models. We hope that Equilibrium Matching provides principled way to unify flow-based and energy-based perspectives and can enable new inference-time strategies in the future."
        },
        {
            "title": "2 PRELIMINARIES: FLOW MATCHING",
            "content": "Diffusion and Flow Matching models learn non-equilibrium dynamics. Flow Matching (FM), for example, learns to match the conditional velocity along linear path connecting noise and image samples. During sampling, Flow Matching starts from pure Gaussian noise and iteratively denoises the current sample using the velocity predicted by . This process is governed by differential equation framework, in which the predicted velocity is treated as the time derivative of the desired sampling path and integrated over total length of 1. Formally, let denote the generative model, let ϵ be Gaussian noise, let be real image sample from the training dataset, and let be timestep sampled uniformly between 0 and 1. The training objective of FM can be written as: LFM = (cid:0)f (xt, t) (x ϵ)(cid:1)2 Here xϵ is the target velocity. The model takes both xt and as inputs, where is the corresponding noise level for xt. (1) . Noise-Unconditional Model. One approach to make Flow Matching learn equilibrium dynamics is to directly remove as conditioning input for (Sun et al., 2025). In this noise-unconditional version, the model is no longer conditioned on the timestep or noise level, giving new objective: Luncond-FM = (cid:0)f (xt) (x ϵ)(cid:1)2 The only difference is the absence of conditioning on in . The sampling and training procedures are otherwise identical to those of the original Flow Matching. However, removing noise conditioning (2) ."
        },
        {
            "title": "Preprint",
            "content": "like the above degrades generation quality, and this unconditional variant does not exhibit unique properties that differ from those of the original model."
        },
        {
            "title": "3 EQUILIBRIUM MATCHING",
            "content": "Equilibrium Matching (EqM) learns time-invariant gradient field that is compatible with an underlying energy function, eliminating time/noise conditioning and fixed-horizon integrators. Conceptually  (Fig. 1)  , EqMs gradient vanishes on the data manifold and increases toward noise, yielding an equilibrium landscape in which ground-truth samples are stationary points. We illustrate the difference between Equilibrium Matching and Flow Matching in Fig. 1. Flow Matching learns varying velocity that only converges to ground truths at the final timestep, whereas EqM learns time-invariant gradient landscape that always converges to ground-truth data points."
        },
        {
            "title": "3.1 TRAINING",
            "content": "To train an Equilibrium Matching model, we aim to construct an energy landscape in which the target gradient at ground-truth samples is zero. To do so, we first define corruption scheme that provides transition between data and noise. Let γ be an interpolation factor sampled uniformly between 0 and 1, let ϵ be Gaussian noise, and let be sample from the training set. Denote xγ = γx + (1 γ)ϵ as an intermediate corrupted sample. Unlike in FM, our γ is implicit and not seen by the model. Our goal is to define target gradient at these intermediate samples xγ that matches an implicit energy landscape. Using gradient direction pointing from noise to data, we write the Equilibrium Matching training objective as: LEqM = (cid:0)f (xγ) (ϵ x)c(γ)(cid:1)2 (3) where c(γ) controls the gradient magnitude. The target gradient (ϵ x)c(γ) has direction (ϵ x) that points from noise to data and magnitude c(γ) that vanishes as we get closer to the data manifold. We explicitly enforce c(1) = 0, ensuring that the energy landscape has vanishing gradients at real samples. Together, the direction and magnitude result in target gradient that supports an implicit energy landscape. When c(γ) = 1, the EqM objective is exactly the negation of FMs objective. , Compared with Flow Matching, EqMs objective is derived from an EBM perspective rather than normalizing flows perspective. This results in different direction of target, where EqM learns the gradient ϵ and FM learns the velocity ϵ. The difference in perspectives also results in different fundamental constraints. EqM requires c(1) = 0 to construct an energy landscape with local minima at the data manifold, whereas FM requires (cid:82) 1 γ=0 c(γ) = 1 to construct valid integration path. Next, we investigate several simple choices for in EqM. Linear Decay. natural choice for is linear function that decays from 1 to 0. In the energy landscape, this is equivalent to assigning noise high gradient and making the gradient decay linearly to 0 toward the ground-truth image: clinear(γ) = 1 γ. (4) Truncated Decay. Beyond linear decay, we may want the gradient to remain constant when far away from data. This leads to truncated decay, where the target gradient stays at 1 when γ (a [0, 1)) and then decays linearly to 0 when approaching the data manifold: ctrunc(γ) = (cid:40) 1, γ 1γ 1a , γ > . (5) Piecewise. We can also vary the constant segment of the truncated decay function and set its starting value to b, with [0, ). This gives piecewise function that starts at b, decays linearly down to 1 at γ = a, and then decays linearly down to 0 at γ = 1: b1 1γ 1a , γ, γ γ > cpiece(γ) = (6) (cid:40) . Gradient Multiplier. The above choices for have varying magnitudes. Thus, we introduce an additional gradient multiplier λ on top of these gradient fields to control the overall scale. Using linear decay as an example, the final function becomes c(γ) = λclinear(γ) = λ(1 γ)."
        },
        {
            "title": "3.2 LEARNING EXPLICIT ENERGY",
            "content": "Previously, we treat the energy landscape as an implicit underlying structure and learned the gradient of this implicit energy function. We can also modify the Equilibrium Matching model to learn explicit energy values. For an explicit energy model g, we match the gradient g(xγ) at corrupted samples and take = g(xγ) as the energy at xγ. This naturally constructs an energy landscape in which real samples are assigned low energy while noises are assigned high energy. The Equilibrium Matching with Explicit Energy (EqM-E) objective can be written as: LEqM-E = (g(xγ) (ϵ x)c(γ))2. g(xγ) is the derivative of with respect to input xγ. To obtain scalar function g, we follow Du et al. (2023) and discuss two different ways to construct from an existing Equilibrium Matching model without having to introduce new parameters. (7) Dot Product. The first approach uses the dot product between the input xγ and the output (xγ), defined as g(xγ) = xγ (xγ). The corresponding derivative with respect to xγ is g(xγ) = (xγ) + xT γ (xγ). Squared L2 Norm. The second approach uses the squared L2 norm of the output (xγ) with factor of one half to simplify coefficients: g(xγ) = 1 2. The corresponding derivative is g(xγ) = (xγ)f (xγ). 2 (xγ)2 3.3 SAMPLING Because of its equilibrium nature, Equilibrium Matching generates samples via optimization on the learned landscape. In contrast to diffusion/flow models that integrate over fixed time horizon, EqM decouples sample quality from prescribed trajectory. It formulates the sampling process as gradient descent procedure and supports adaptive step sizes, optimizers, and compute, offering additional flexibility at inference time. Gradient Descent Sampling (GD). simple way to sample from an EqM model is to apply vanilla gradient descent. Let xk denote the sample after steps, then an update step with step size η is: xk+1 xk ηE(xk), (8) where E(xk) is the predicted gradient at xk and may be learned implicitly (E(x) = (x)) or explicitly (E(x) = g(x)). Sampling with Nesterov Accelerated Gradient (NAG-GD). Building on gradient descent sampling, we can adopt existing optimization techniques in our sampling procedure. As an example, we use Nesterov Accelerated Gradient (Nesterov, 1983), which applies look-ahead step at each update and evaluates the gradient at that look-ahead point: xk+1 xk ηE(xk + µ(xk xk1)), (9) where µ is the look-ahead factor controlling how far to look ahead at each step. Sampling with Differential Equations. EqM also naturally supports integration-based samplers. ODE-based diffusion samplers can be viewed as special case of our gradient-based method. We discuss this further in Appendix D. Sampling with Adaptive Compute. Another advantage of gradient-based sampling is that instead of fixed number of sampling steps, we can allocate adaptive compute per sample by stopping when the gradient norm drops below certain threshold gmin. For step size η and threshold gmin, we perform sampling steps xk+1 xk ηE(xk) until E(xk)2 < gmin. This mechanism allows the sampler to adaptively adjust the number of steps for each sample and terminate automatically when close to the data manifold. 3.4 IMPLEMENTATION Equilibrium Matching is simple to implement. We provide example pseudocode for training in Algorithm 1 and sampling in Algorithm 2. During training, we first compute an interpolated corrupted sample xγ from noise ϵ, image x, and factor γ, then train the model to predict the target gradient"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Equilibrium Matching Training The loss function takes as input model , noise ϵ (eps), image x, and an interpolation factor γ (g), and returns the EqM loss. def training_loss(f, eps, x, g): Algorithm 2 Equilibrium Matching Sampling The sampling function takes as input pretrained model , initialization st, step size η, and total steps , and returns the generated sample. def generate(f, st, eta, N): xg = (1-g)*eps + g*x target = (eps-x)*c(g) loss = (f(xg) - target)**2 return loss xn = st for in range(N): xn = xn - eta*f(xn) return xn (ϵ x)c(γ) by minimizing mean squared error objective. At inference time, Equilibrium Matching uses the predicted gradients to iteratively optimize via gradient descent. We adopt transformer-based backbone from Ma et al. (2024) to implement our Equilibrium Matching model. We use the exact model implementation from Ma et al. (2024) to ensure that no architectural differences influence the results. To remove conditioning on from this backbone, we set the input to 0. For further details on model configurations, see Appendix A."
        },
        {
            "title": "4 ANALYSIS",
            "content": "We provide mathematical justifications for Equilibrium Matching. We show that under certain assumptions, Equilibrium Matching learns ground-truth samples as local minima and converges to these minima with bounded convergence rate. Statement 1 (Learned Gradient at Ground-Truth Samples). Let be an Equilibrium Matching model with c(1) = 0, and let x(i) be ground-truth sample in Rd. Assume perfect training, i.e., exactly minimizes the training objective. Then, in high-dimensional settings, we have: (x(i))2 0. where x(i) is an arbitrary sample from the training dataset. In other words, Equilibrium Matching assigns ground-truth images with approximately 0 gradient. (Derivation in Appendix C.1) Statement 2 (Property of Local Minima). Let be an Equilibrium Matching model with c(1) = 0, and let ˆx be an arbitrary local minimum where (ˆx) = 0. Assume perfect training, i.e., exactly minimizes the training objective. Then, in high-dimensional settings, we have: (ˆx ) 1. where is the ground-truth dataset. In other words, all local minima are approximately samples from the ground-truth dataset. (Derivation in Appendix C.2) Combining Statement 1 and Statement 2, Equilibrium Matching learns ground-truth images as local minima during training. Next, we show that sampling on an Equilibrium Matching model with gradient descent converges at rate of O( 1 Statement 3 (Convergence of Gradient-Based Sampling). Let be an Equilibrium Matching model with corresponding energy function such that E(x) = (x). Suppose is L-smooth and bounded below by E(x) Einf. Then, gradient descent with step size η [0, 1 ), where is the total number of sampling steps. ] satisfies: min 0k<K (xk)2 2(E(x0) Einf) ηK , where xk is the iterate after steps and is the total number of optimization steps performed. (Derivation in Appendix C.3) We have demonstrated theoretically that under the given assumptions, Equilibrium Matching produces samples close to ground truths. Next, we empirically validate our method."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We validate the practical performance of Equilibrium Matching from four major perspectives. First, we demonstrate the advantages in generation quality through series of experiments on ImageNet (Deng et al., 2009). Then, we examine the properties and performance of our gradient-based sampling method. Next, we illustrate the effectiveness of our gradient landscape via ablation studies. Finally, we show unique properties of Equilibrium Matching that are not inherently supported by diffusion/flow methods. For details of the training and sampling settings used in our experiments, see Appendix A."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Curated Samples. We present curated samples generated by our EqM-XL/2 model. Figure 3: Scalability of Equilibrium Matching. EqM scales across training epochs (left), parameter count (middle), and patch size (right), and outperforms Flow Matching at all tested scales by significant margin. model StyleGAN-XL VDM++ DiT-XL/2 SiT-XL/2 EqM-XL/2 method FID GAN 2.30 Diffusion 2.12 Diffusion 2.27 2.06 FM EqM 1.90 model sampler η µ FID 2.10 SiT-XL/2 Euler (ODE) 0.0040 2.06 Heun (SDE) 0.0040 SiT-XL/2 1.93 EqM-XL/2 Euler (ODE) 0.0017 EqM-XL/2 1.93 0.0017 EqM-XL/2 NAG-GD 0.0017 0.3 1.90 GD - - - - ImageNet Class-Conditional Table 1: 256256 Generation. EqM-XL/2 achieves 1.90 FID, surpassing other tested methods. Table 2: Sampler Comparison. EqM exceeds Flow Matching in performance (measured by FID) using both integrationbased ODE sampler and gradient-based samplers. 5.1 IMAGE GENERATION ImageNet Results. We report performance on class-conditional ImageNet (Deng et al., 2009) 256256 image generation. We compare Equilibrium Matching with prior generative methods, including StyleGAN (Sauer et al., 2022), VDM++ (Kingma and Gao, 2023), DiT (Peebles and Xie, 2023), and SiT (Ma et al., 2024). Results are shown in Table 1. Equilibrium Matching achieves an FID of 1.90, outperforming both diffusion and flow counterparts by significant margin across all tested models. We also plot the evolution of FID over time while training our EqM-XL/2 model and compare it with the training FID curve of SiT-XL/2 in Fig. 3. Equilibrium Matching consistently improves over the Flow Matching baseline throughout training, further demonstrating that it produces higher-quality samples than existing generative methods. Visualizations. We present curated samples from our EqM-XL/2 model in Fig. 2 and visualizations of the sampling process in Fig. 4. For both EqM and FM, we use XL/2 models trained for 1400 epochs to produce the visualizations. In Fig. 4, we observe that EqM converges much faster than its FM counterpart at inference. In Fig. 5, we show the top-3 nearest neighbors (measured by mean squared distance) in the training set for EqM-generated samples. The nearest neighbors differ from the generated samples, indicating that EqM does not only memorize the training data and can generalize to unseen samples at inference time. Scalability. To assess scalability, we vary training length, model size, and patch size. We report the evolution of FID over training using the XL/2 training curves. For model-size scaling, we fix the patch size to 2 and the number of training epochs to 80, and evaluate all four model sizes: S, B, L, and XL. For patch-size scaling, we fix the model size to and training to 80 epochs, and evaluate patch sizes 8, 4, and 2. As shown in Fig. 3, Equilibrium Matching scales well along all axes and consistently outperforms Flow Matching under all tested configurations. These results suggest that Equilibrium Matching has strong scaling potential and is promising alternative to Flow Matching. 5. INFERENCE-TIME EXPERIMENTS Different Gradient Samplers. We evaluate our proposed gradient-based samplers on ImageNet generation using the EqM-B/2 model. For NAG-GD, we use µ = 0.35, which we found empirically to work best. As shown in Fig. 6, NAG-GD yields improved FID across all tested number of steps. Moreover, the quality gap increases as the total number of steps decreases. This aligns with our intuition that with fewer total steps, gradient descent requires more assistance to reach desirable local minimum, making NAG more effective. In Table 2, we also compare traditional integration samplers with the proposed gradient descent samplers on EqM-XL/2. Euler ODE sampler, plain gradient descent, and NAG-GD all exceed the Flow Matching baseline by large margin, with"
        },
        {
            "title": "Preprint",
            "content": "FM"
        },
        {
            "title": "EqM",
            "content": "samples"
        },
        {
            "title": "NNs",
            "content": "samples"
        },
        {
            "title": "NNs",
            "content": "Figure 4: Sampling Process Visualization. We present intermediate samples from XL/2 models using the same 0.004 step size. EqM (bottom) produces realistic images much earlier than FM (top). Figure 5: Nearest Neighbors (NNs) of EqM Samples in the Training Set. EqM produces samples that are not in the training set, suggesting that it generalizes and does not only memorize the training images. Figure 6: Sampling with Nesterov Accelerated Gradient. NAG-GD achieves better sample quality than GD, with the gap being more significant when using fewer steps. Figure 7: Different Sampling Step Sizes. EqM is robust to wide range of step sizes, whereas Flow Matching only functions properly at one specific step size. Figure 8: Total Steps Under Adaptive Compute. EqM assigns different numbers of steps for different samples, adaptively adjusting compute at inference time. NAG-GD performing the best. These results show that the NAG technique, commonly used in optimization, is also effective during sampling in Equilibrium Matching, providing further evidence that our approach enables new opportunities at inference time. Flexible Step Size. Viewing sampling through an optimization perspective implies that the step size can be adjusted freely. We evaluate this claim by varying the sampling step size on EqM-B/2. For comparison, we also report the performance of the FM baseline, where we use η to replace the ODE update length at each step. We use total of = 250 sampling steps. From Fig. 7, we observe that our EqM models generation quality remains high and exceeds the Flow Matching baseline across all tested step sizes. By contrast, Flow Matching requires specific step size of η = 0.004 = 1 to function properly, and small fluctuations in step size lead to significantly worse performance. This suggests that EqM constructs fundamentally different landscape than FM, which enables new sampling schemes not supported by FM models. Adaptive Compute. We test our adaptive compute sampling using gradient norm threshold of 10 on EqM-B/2 model. We observe that EqM is able to generate reasonably good samples by adaptive compute, achieving reasonable FID of 33.79 (32.85 without adaptive compute) using the EqM-B/2 model. We present the distribution of total sampling steps for 1024 samples in Fig. 8, which suggests that EqM assigns different inference-time compute for different samples and manages to lower the total compute to 40% of the original compute (original sampling uses fixed 250 steps). Our results offer promising evidence that EqM can enable new inference-time improvements. 5.3 ABLATION STUDY piecewise c(γ) constant linear truncated - - - - FID 40.81 0.5 - 0.8 - Hyperparameter Choices. To determine the best target landscape, we search over choices and hyperparameters for c(γ). We use our EqM-B/2 model training and 80 epochs of for all hyperparameter experiments. We tune the step size of GD sampler and report the best result for each setting in Table 3. The best-performing gradient landscape is truncated decay with = 0.8. Our results suggest that it is helpful to keep constant target gradient at the start of the trajectory before decaying to 0. We then sweep the gradient multiplier λ on this best-performing gradient field. As shown in Fig. 9, multiplier of 4 significantly improves FID. We use this setting (ctrunc with = 0.8 and λ = 4) as default in our experiments. Table 3: Different Target Gradient Fields. Several settings exceed the noise unconditional Flow Matching baseline in performance. Best performance achieved with the truncated decay ctrunc and hyperparameter = 0.8. 50.47 38.98 38.34 41.22 38.84 38. 0.8 0.8 0.8 1.4 0.9 -"
        },
        {
            "title": "Preprint",
            "content": "noise conditioning c(γ) yes yes no no const trunc const trunc FID 36.68 41.89 40.81 32.85 energy model FID none dot product L2 norm 57.54 73.40 75.53 Figure 9: Different Gradient Multipliers. λ = 4 performs the best (32.85). Table 4: Noise Conditioning Ablation. Our truncated decay ctrunc improves generation only when not conditioned on noise level, matching our hypothesis. Table 5: EqM-E Variants. The dot product variant of EqM-E performs the best. Noise Conditioning. We compare our new target gradient with the baseline under both noiseconditional and noise-unconditional settings in Table 4. We adopt the 80 epochs B/2 model for both the FM baseline and EqM. Our target gradient improves performance only in the noise-unconditional case, which aligns with our expectation that an energy landscape with zero gradient at real samples is more favorable under equilibrium dynamics. Energy Formulations. We evaluate two EqM-E variants, the dot product and the L2 norm, on the EqM-B/4 model. We train the dot product EqM-E model from scratch for 80 epochs, and we train the L2 norm model by first initializing from pretrained EqM model (10 epochs) and then continuing training for 70 epochs. We adopt this scheme due to stability concerns, as the L2 norm variant is sensitive to initialization. From Table 5, we find that both formulations degrade performance, and among the two, the L2 norm variant performs worse. Since it also requires careful initialization to train stably, we hypothesize that the L2 norm variant is harder to optimize than the dot product variant. Consequently, we recommend using the dot product variant for explicit energy. 5.4 UNIQUE PROPERTIES In this subsection, we investigate unique properties of Equilibrium Matching that are not supported by traditional diffusion/flow models. Partially Noised Image Denoising. By learning an equilibrium dynamic, Equilibrium Matching can directly start with and denoise partially noised image. Existing diffusion/flow models require an explicit noise level as input to process partially noised images, but our EqM model does not have such limitation. We evaluate EqMs generation quality from partially noised inputs using noised samples from the ImageNet validation set. As shown in Fig. 10, Equilibrium Matching behaves very differently from traditional Flow Matching. EqM-B/4s FID improves significantly when fed less noisy samples, whereas the Flow-based SiT-B/4 cannot handle partially noised images as raw input and its generation quality drops quickly when not starting from pure noise. These results further support that Equilibrium Matching enables capabilities that traditional methods cannot naturally offer. Figure 10: Partially Noised Image Generation. EqMs generation quality improves with less noisy inputs while FMs quality drops. Out-of-Distribution Detection. Another unique property of the EqM model is its inherent ability to perform out-of-distribution (OOD) detection using energy value. In-distribution (ID) samples typically have lower energies than OOD samples. To this end, we use our dot product variant of the EqM-E-B/4 model and perform OOD detection with CIFAR-10 as ID. We use PixelCNN++ (Salimans et al., 2017), GLOW (Kingma and Dhariwal, 2018), and IGEBM (Du and Mordatch, 2019) as baselines and adopt the numbers reported by Yoon et al. (2023). We report the area under the ROC curve (AUROC) in Table 6. Compared with these baselines, Equilibrium Matching provides reasonable OOD detection across all tested datasets and achieves the best overall performance, suggesting that Equilibrium Matching indeed learns valid energy landscape. Composition. EqM also naturally supports the composition of multiple models by adding energy landscapes together (corresponding to adding the gradients of each model). We test composition by combining models conditioned on different ImageNet class labels. We use our EqM-XL/2 model with GD sampler and add two conditional gradients together as the update gradient at each sampling step. In Fig. 11, we present the generation results using panda and valley (top left), car mirror and volcano (top right), ice cream and chocolate syrup (bottom left), and broccoli and cauliflower (bottom"
        },
        {
            "title": "Preprint",
            "content": "model SVHN Textures constant avg. 0.32 PixelCNN++ GLOW 0.24 IGEBM 0.63 EqM 0. 0.33 0.27 0.48 0.49 0.71 - 0.39 1.00 0.45 0.26 0.50 0. Table 6: OOD Detection. EqM achieves reasonable AUROC under all tested OOD datasets and has the best average result among all tested models. Figure 11: Image Composition. We present compositional samples from EqM-XL/2 each using two ImageNet labels: panda and valley (leftmost), car mirror and volcano (second left), ice cream and chocolate syrup (second right), and broccoli and cauliflower (rightmost). right). These results demonstrate that EqM is easily composable by optimizing the summed gradient. This is similar to the composability of EBMs (Du et al., 2020a), while the composition of diffusion is significantly more complex to accurately implement (Du et al., 2023)."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Diffusion Models and Flow Matching. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al.; Nichol and Dhariwal, 2021; Dhariwal and Nichol, 2021; Karras et al., 2022) generate images from pure noise through series of noising and denoising steps that are conditioned on noise level. The sampling process of diffusion models is often formulated as solving differential equation by integrating the models predicted velocity over noise level (Ho et al., 2020; Song et al., 2020; Lu et al., 2022; Karras et al., 2022). Flow Matching (Lipman et al.; Liu et al.; Albergo and Vanden-Eijnden, 2023) is more recent generative method that adopts linear interpolation between noise and real images, which eliminates the need for complex noise scheduling. Energy-Based Models. Energy-based models (EBMs) (Hinton, 2002; LeCun et al., 2006; Xie et al., 2016; Du and Mordatch, 2019; Du et al., 2020b; Nijkamp et al., 2020; Gao et al., 2020) learn an energy landscape that defines the unnormalized log-density of data distribution. EBMs are versatile in different modalities and tasks (e.g., OOD detection) thanks to the equilibrium energy landscape (Du and Mordatch, 2019; Grathwohl et al., 2019). However, EBMs suffer from training instabilities (Carreira-Perpinan and Hinton, 2005; Song and Ou, 2018; Gutmann and Hyvärinen, 2010) and are hard to scale (Du et al., 2020b). Existing Efforts. Prior efforts on improving generative modeling have attempted to merge diffusion and energy training. In order to make diffusion learn equilibrium dynamics, Sun et al. (2025) attempt to directly remove noise conditioning from diffusion models, but this leads to worse generation quality. Energy Matching (Balcerak et al., 2025) adopts two-stage training strategy where the model is first trained using flow objective and then trained with Langevin-based dynamics like EBM near the data manifold. However, Energy Matching is outperformed by Flow Matching on large-scale experiments like ImageNet. Our work is different from Energy Matching in that we train with single objective that unifies the dynamics near and away from data. Contrary to Energy Matching, EqM offers promising scalability, optimization-based sampling, and additional flexibility."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We propose Equilibrium Matching, generative model that learns equilibrium dynamics in simple and effective way. Equilibrium Matching combines the advantages of energy-based and flow-based models without compromising performance. Our method is easy to train, achieves strong generation quality, and provides an interpretable energy landscape that supports wide range of sampling methods. We hope that the equilibrium dynamics learned by Equilibrium Matching will inspire more effective and scalable inference algorithms in the future."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We greatly thank Harvard Kempner Institute for granting us access to compute resources. We thank Michael Albergo, Hansen Lillemark, Haldun Balim, and Kaiming He for helpful discussions. We thank Boyuan Chen for help on distributed training."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In International Conference on Learning Representations (ICLR), 2023. 9 Michal Balcerak, Tamaz Amiranashvili, Antonio Terpin, Suprosanna Shit, Lea Bogensperger, Sebastian Kaltenbach, Petros Koumoutsakos, and Bjoern Menze. Energy matching: Unifying flow matching and energy-based models for generative modeling. arXiv preprint arXiv:2504.10612, 2025. 1, 9 Miguel Carreira-Perpinan and Geoffrey Hinton. On contrastive divergence learning. In International workshop on artificial intelligence and statistics, pages 3340. PMLR, 2005. 1, 9 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 5, Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1, 9 Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems, 32, 2019. 1, 8, 9 Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models. Advances in Neural Information Processing Systems, 33:66376647, 2020a. Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Improved contrastive divergence training of energy based models. arXiv preprint arXiv:2012.01316, 2020b. 1, 9 Yilun Du, Conor Durkan, Robin Strudel, Joshua Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International conference on machine learning, pages 84898510. PMLR, 2023. 4, 9 Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik Kingma. Learning energy-based models by diffusion recovery likelihood. arXiv preprint arXiv:2012.08125, 2020. 9 Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019. Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 297304. JMLR Workshop and Conference Proceedings, 2010. 9 Geoffrey Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):17711800, 2002. 9 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 9 Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. 1, Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36:6548465516, 2023. 6 Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. 8 Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009."
        },
        {
            "title": "Preprint",
            "content": "Yann LeCun, Sumit Chopra, Raia Hadsell, Ranzato, Fujie Huang, et al. tutorial on energy-based learning. Predicting structured data, 1(0), 2006. 1, 9 Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations. 1, 9 Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky TQ Chen, David Lopez-Paz, Heli Ben-Hamu, and Itai Gat. Flow matching guide and code. arXiv preprint arXiv:2412.06264, 2024. 13 Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations. 1, 9 Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 5, 6, 13 Yurii Nesterov. method of solving convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27(2):372376, 1983. 4 Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning (ICML). PMLR, 2021. 1, 9 Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of mcmc-based maximum likelihood learning of energy-based models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 52725280, 2020. 1, William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 6 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention (MICCAI), 2015. 13 Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017. 8 Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. 1, 9 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations. 1, 9 Yunfu Song and Zhijian Ou. Learning neural random fields with inclusive auxiliary generators. 2018. 9 Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Qiao Sun, Zhicheng Jiang, Hanhong Zhao, and Kaiming He. Is noise conditioning necessary for denoising generative models? arXiv preprint arXiv:2502.13129, 2025. 1, 2, 9 Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025."
        },
        {
            "title": "Preprint",
            "content": "Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. theory of generative convnet. In International conference on machine learning, pages 26352644. PMLR, 2016. 9 Sangwoong Yoon, Young-Uk Jin, Yung-Kyun Noh, and Frank Park. Energy-based models for anomaly detection: manifold diffusion recovery approach. Advances in Neural Information Processing Systems, 36:4944549466, 2023."
        },
        {
            "title": "Preprint",
            "content": "In Appendix A, we show the detailed experimental settings. In Appendix B, we present additional experiments including CIFAR-10 experiments and other metrics on ImageNet. In Appendix C, we show the derivations of the statements made. In Appendix D, we demonstrate the relationship between ODE-based integration sampler and our gradient descent sampler."
        },
        {
            "title": "A EXPERIMENTAL SETTING",
            "content": "A.1 TRAINING SETTING We present the training setting of our Equilibrium Matching model in Table 7. A."
        },
        {
            "title": "INFERENCE SETTING",
            "content": "We present the majority of our sampler settings in Table 7. More specifically, in Section 5.1, we adopt the SiT results reported by Ma et al. (2024) and Wang and He (2025), and use our own NAG-GD sampler for EqM results. In Section 5.2, Section 5.3, and Section 5.4, we adopt the Euler sampler for SiT experiments and the vanilla GD sampler for EqM experiments."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTS",
            "content": "B.1 CIFAR-10 EXPERIMENTS We also evaluate our approach on non-transformer architectures in the CIFAR-10 dataset (Krizhevsky, 2009), where the commonly used network architecture is U-Net (Ronneberger et al., 2015). The experiments are based on the publicly available code of Flow Matching (Lipman et al., 2024). We use the same hyper-parameters as the original codebase. Table 8 presents the FID results. We keep the same a, hyperparameters of c(γ) as in the ImageNet experiments and search over the gradient multiplier λ and report the best result with step size of 1. Equilibrium Matching still outperforms the noise unconditional Flow Matching baseline, but fails to improve relative to the standard Flow Matching. We attribute this performance gap to the extensive optimization of this baseline, where noise scheduling and sampling step scheduling have been carefully selected based on the Flow Matching baseline performance, making it an unfair comparison for our method. Nonetheless, we are still able to improve upon the noise unconditional baseline, suggesting that our Equilibrium Matching is universal approach to generative modeling. B.2 OTHER METRICS We report Equilibrium Matchings performance on other evaluation metrics including sFID and Inception Score (IS) in Table 9. Equilibrium Matching also achieves relatively good sFID and IS compared against other generative methods."
        },
        {
            "title": "C DERIVATIONS",
            "content": "C.1 LEARNED GRADIENT AT GROUND-TRUTH SAMPLES Statement 1 (Learned Gradient at Ground-Truth Samples). Let be an Equilibrium Matching model with c(1) = 0, and let x(i) be ground-truth sample in Rd. Assume perfect training, i.e., exactly minimizes the training objective. Then, in high-dimensional settings, we have: where x(i) is an arbitrary sample from the training dataset. In other words, Equilibrium Matching assigns ground-truth images with approximately 0 gradient. (x(i))2 0. Derivation of Statement 1. Under perfect training, the squared-error objective = Ex,ϵ,γ (cid:13)f (xγ) (ϵ x) c(γ)(cid:13) (cid:13) 2 (cid:13)"
        },
        {
            "title": "Preprint",
            "content": "model S/2 B/2 L/2 XL/2 model configurations 33 params (M) 12 depth 384 hidden dim 2 patch size 6 heads 130 12 768 2 12 458 24 1024 2 16 675 28 1152 2 16 80 80 - 1400 80 ImageNet training configurations epochs batch size optimizer optimizer β1 optimizer β2 weight decay learning rate (lr) lr schedule lr warmup"
        },
        {
            "title": "256\nAdamW\n0.9\n0.999\n0.0\n1 × 10−4\nconstant\nnone",
            "content": "gradient field hparams choice of c(γ) c(γ) multiplier λ truncated decay 4.0 0.8 - integration sampler configurations sampler NFE η 0.004 0.004 dopri5 dopri5 dopri 0.004 250 Euler 0.0017 gradient sampler configurations sampler steps η µ 0.003 0. 0.003 0.35 GD/NAG-GD 250 0.003 0.35 0.0017 0.3 Table 7: Equilibrium Matching Configurations. is minimized when (xγ) = E(cid:2)(ϵ x) c(γ) (cid:12) (cid:12) xγ (cid:3). (1) We use the forward noising model xγ = γ + (1 γ) ϵ, where is one of finitely many training points and ϵ (0, Id). For any fixed γ and x, the conditional density of xγ given t, p(xγ γ) = (cid:0)γ x, (1 γ)2Id (cid:1), is continuous Gaussian on Rd. At γ = 1, xγ equals exactly with probability one (a Dirac mass on each ). Since is finite discrete set, in the limit the Gaussian density at any exact training point x(i) for γ < 1 vanishes exponentially in d, whereas the mass at γ = 1 remains. Hence (cid:0)γ = 1 xγ = x(i)(cid:1) (d ). (2) Plugging equation 2 into equation 1, we get (cid:0)x(i)(cid:1) = E(cid:2)(ϵ x) c(γ) (cid:12) (cid:12) xγ = x(i)(cid:3) (ϵ x(i)) c(1) = 0, since c(1) = 0. Therefore (x(i))2 0, as claimed."
        },
        {
            "title": "Preprint",
            "content": "setting FM FM (uncond) EqM FID 2.09 3.96 3.36 Table 8: CIFAR-10 Experiments. Equilibrium Matching improves upon noise unconditional Flow Matching, but is worse than standard Flow Matching. We suspect this is due to over-optimization on the CIFAR-10 baseline. C.2 PROPERTY OF LOCAL MINIMA model method FID sFID IS StyleGAN-XL VDM++ DiT-XL/2 SiT-XL/2 EqM-XL/"
        },
        {
            "title": "GAN\nDiffusion\nDiffusion\nFM\nEqM",
            "content": "2.30 2.12 2.27 2.06 1.90 4.02 - 4.60 4.49 4.54 265.1 267.7 278.2 277.5 275.7 Table 9: Class-conditional Generation on ImageNet 256256 with Additional Metrics. Equilibrium Matching achieves the best FID and relatively good sFID and IS. Statement 2 (Property of Local Minima). Let be an Equilibrium Matching model with c(1) = 0, and let ˆx be an arbitrary local minimum where (ˆx) = 0. Assume perfect training, i.e., exactly minimizes the training objective. Then, in high-dimensional settings, we have: (ˆx ) 1. where is the ground-truth dataset. In other words, all local minima are approximately samples from the ground-truth dataset. Derivation of Statement 2. By the same argument as before, perfect training implies 0 = (ˆx) = E(cid:2)(ϵ x) c(γ) (cid:12) (cid:12) xγ = ˆx(cid:3). (1) Since c(γ) 0 for all γ and c(1) = 0 while c(γ) > 0 for γ < 1, the only way the vector-valued expectation in equation 1 can vanish in high dimension is if the posterior mass concentrates at γ = 1. We can argue exactly as in equation 2: for any ˆx that equals some x(i) , we have and for γ < 1 the density is exponentially small in d. Since at γ = 1, all xγ are in , (γ = 1 xγ = ˆx) 1, establishing the claim. (ˆx ) 1. C.3 CONVERGENCE OF GRADIENT-BASED SAMPLING Statement 3 (Convergence of Gradient-Based Sampling). Let be an Equilibrium Matching model with corresponding energy function such that E(x) = (x). Suppose is L-smooth and bounded below by E(x) Einf. Then, gradient descent with step size η (cid:2)0, 1 (cid:3) satisfies: min 0k<K (xk)2 2(cid:0)E(x0) Einf η (cid:1) , where xk is the iterate after steps and is the total number of optimization steps performed. Derivation of Statement 3. By L-smoothness of E, for any x, Rd, E(y) E(x) + E(x)(y x) + x2. Take = xk+1 = xk η (xk) and recall E(xk) = (xk). Then 2 η2 (xk)2 (xk)2 E(xk) η E(xk+1) E(xk) η (xk)f (xk) + = E(xk) η (cid:16) (cid:17) 1 Lη 2 2 (xk)2,"
        },
        {
            "title": "Preprint",
            "content": "where the last inequality holds because η 1/L = 1 Lη 2 1 2 . Summing from = 0 to = 1 gives E(xK) E(x0) η 2 K1 (cid:88) k= (xk)2 η 2 min 0k<K (xk)2. Since E(xK) Einf , rearrange to obtain min 0k<K (xk)2 2(cid:0)E(x0) Einf η (cid:1) , as required. RELATION BETWEEN INTEGRATION-BASED AND OPTIMIZATION-BASED"
        },
        {
            "title": "SAMPLING",
            "content": "D.1 ODE SAMPLING AS SPECIAL CASE OF OPTIMIZATION Consider the ODE = v(x), and its explicit (forward) Euler discretization with uniform time grid on [0, 1]: xk+1 = xk + v(xk), = , = 0, . . . , 1. When the velocity field is conservative with potential E, i.e., v(x) = E(x), the Euler step becomes: xk+1 = xk E(xk), which is exactly gradient-descent update with step size η = = 1 unit time horizon, the gradient-descent sampler with η = 1 sampler. . Hence, with steps on coincides with the explicit Euler ODE D.2 GENERAL INTEGRATION SAMPLERS The equivalence above suggests broader correspondence: integration-based samplers can be interpreted as optimization-based methods by viewing the velocity as descent direction. In particular, when v(x) = E(x), any time integrator induces an optimization update rule. Practically, ODE samplers in generative modeling are often implemented on uniform grid with step size = 1 . Under the optimization view, we are not bound to this constraint. We can adopt adaptive step sizes while retaining the same underlying direction field. This perspective enables direct adaptation of existing integration-based samplers within Equilibrium Matching."
        }
    ],
    "affiliations": [
        "Harvard University",
        "MIT"
    ]
}