{
    "paper_title": "Learning to Repair Lean Proofs from Compiler Feedback",
    "authors": [
        "Evan Wang",
        "Simon Chess",
        "Daniel Lee",
        "Siyuan Ge",
        "Ajit Mallavarapu",
        "Vasily Ilin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback, predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at \\href{https://huggingface.co/datasets/uw-math-ai/APRIL}{this link}."
        },
        {
            "title": "Start",
            "content": "Yiran Wang 1 2 3 Simon Chess * 1 2 3 Daniel Lee * 1 2 Siyuan Ge * 1 2 Ajit Mallavarapu * 1 2 Vasily Ilin 1 3 6 2 0 2 3 ] . [ 1 0 9 9 2 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as supervised learning problem: given an erroneous proof and compiler feedback, predict both corrected proof and natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our singleshot repair evaluation setting, finetuned 4Bparameter model outperforms the strongest opensource baseline. We view diagnostic-conditioned supervision as complementary training signal for feedback-using provers. Our dataset is available at this link. 1. Introduction Formal theorem proving offers strict setting for studying machine learning: proofs must be written in formal language and verified by proof assistant such as Lean, leaving no ambiguity of correctness. This property allows learning systems to receive exact verification signals and has motivated surge of recent work on learning-based automated theorem proving. Large Language Models (LLMs) have achieved substantial gains on formal benchmarks and even reached Olympiad-level performance (Lin et al., 2025; Chen et al., 2025; Achim et al., 2025). *Equal contribution 1Math AI Lab, University of Washington, Seattle, United States 2Department of Computer Science and Engineering, University of Washington, Seattle, United States 3Department of Mathematics, University of Washington, Seattle, United States. Correspondence to: Vasily Ilin <vilin@uw.edu>. Preprint. February 4, 2026. Figure 1. Overview of our dataset collection pipeline that collects paired correct and incorrect proof and their contextual information. We collected correct proofs from public datasets (Herald, Lean Workbook, NuminaMath-Lean). The paired error proofs that maintain the same proof sketch are generated by mutating tactics, lines of code, or theorems. Then, we used the Lean compiler to filter out the error proofs and extract their error messages, error lines, and goal states. Finally, we prompted an LLM to generate error explanations and fix suggestions based on the paired proofs and Lean Infoview. The resulting dataset pairs erroneous proofs and error information with structured labels containing the error interpretation from LLM and the corresponding correct proofs. Lean Error Correction Despite these advancements, the dominant objective in current systems remains end-to-end proof generation: given formal statement, the goal is to produce complete proof that verifies successfully. Training data therefore consists primarily of correct proofs. Evaluation is likewise based on whether valid proof is eventually found. Failures encountered during generation are typically used to guide exploration or policy optimization, but are not treated as supervised learning targets. As such, models receive limited supervision for interpreting diagnostics, explaining what went wrong, or proposing targeted edits. Recent work has also shown that iterative proof refinement using compiler feedback is 32-128x more efficient than pass@k (Chen et al., 2025). In contrast, human proof development is inherently iterative and error-driven. Proof engineers routinely write partial proofs, inspect compiler diagnostics, and incrementally revise the code until verification succeeds. Intermediate failures are fully expected in the development process and the same feedback is used both to decide what to change and to communicate why the change is correct. However, most publicly available proof corpora preserve only final proofs and omit failed attempts. Large-scale libraries such as mathlib provide extensive collections of formally verified theorems, but do not record intermediate error states encountered during development (The mathlib Community, 2020). Datasets derived from natural language autoformalization pipelines, including Lean Workbook and Herald, provide aligned naturalformal pairs and synthetic proof data, but likewise consist almost entirely of valid formal artifacts rather than trajectories of failed attempts and subsequent repairs (Ying et al., 2024; Gao et al., 2025). As result, models trained on these resources have little direct exposure to feedback-conditioned iteration, including both producing corrected code and articulating diagnoses grounded in compiler messages. This work studies proof correction from compiler feedback, and introduces dataset that supports two aligned feedbackconditioned tasks: (1) producing corrected proof, and (2) producing natural-language diagnosis and fix suggestion grounded in the same feedback. These targets share the same underlying evidence (error message and local proof state), but emphasize different outputs: formal repair versus human-interpretable debugging. To enable controlled analysis, we construct datasets by systematically mutating correct proofs to generate plausible failures produced by controlled mutations, and we evaluate language models under supervised finetuning without reinforcement learning or inference-time search, isolating the models ability to interpret diagnostics and apply targeted repairs. Our contributions are threefold. First, we introduce large-scale dataset of 260K Lean proof-repair examples, each consisting of an erroneous proof, compiler diagnostics including error messages and local proof state, and corresponding corrected proof; dataset available huggingface.co/datasets/uw-math-ai/APRIL. Second, we develop systematic mutation pipeline that generates realistic failures by substituting semantically related theorems, swapping similar tactics, and eliciting plausible but incorrect line and multi-line completions from language models. Third, we demonstrate that supervised finetuning on this data substantially improves repair accuracy under our evaluation setting: 27.4% correction accuracy in our single-shot repair evaluation (no search/iteration), compared to 1.1% for the base model and 26.8% for Goedel-Prover-V2-32B under the same protocol. Because Goedel is typically deployed with search/iteration, this comparison should be interpreted as an ablation of compiler-feedback-conditioned repair rather than end-to-end proving performance. 2. Related Work Neural Theorem Proving in Lean. Recent learningbased systems for Lean primarily target end-to-end proof generation, combining large language models with search, reinforcement learning, and synthetic data pipelines (Xin et al., 2024a; Ren et al., 2025; Lin et al., 2025; Wang et al., 2025). Representative approaches include whole-proof generation with expert iteration, where models generate complete proofs from statements and only Lean-verified successes are retained for retraining (Lin et al., 2025; Xin et al., 2024a); reinforcement learning with binary verifier rewards that optimize tactic selection and subgoal decomposition strategies (Ren et al., 2025; Xin et al., 2024b); and searchbased methods such as Monte Carlo tree search over Lean proof states, guided by learned policy and value models conditioned on proof history and informal reasoning traces (Achim et al., 2025; Lample et al., 2022). Infrastructure such as LeanDojo has enabled systematic benchmarking and retrieval-augmented proving (Yang et al., 2023), while the Lean 4 proof assistant itself provides the formal foundation for this work (de Moura & Ullrich, 2021). Evaluation across these systems is framed in terms of theorem-level success metrics such as pass@k on benchmarks like miniF2F (Zheng et al., 2022). The Role of Verifier Feedback. Verifier feedback and errors play an important role in existing pipelines, but primarily as control signals for exploration and policy optimization rather than as supervised correction targets. In expert-iteration settings, failed proof attempts are discarded rather than repaired (Lin et al., 2025; Xin et al., 2024a); in search-based systems, failing branches are pruned and alternative trajectories explored (Achim et al., 2025; Lample et al., 2022); and in reinforcement learning approaches, compiler success serves as sparse binary reward (Xin et al., 2 Lean Error Correction 2024b; Ren et al., 2025). More recent refinement-based systems incorporate compiler diagnostics into prompts to regenerate proofs across multiple iterations, demonstrating that iterative repair can be substantially more efficient than independent sampling (Chen et al., 2025; Zhou et al., 2025). However, these approaches treat repair as an emergent capability of prompted models rather than as an explicit supervised learning objective. Proof Datasets and Training Corpora. Large-scale libraries such as mathlib provide extensive collections of formally verified theorems, but do not record intermediate error states encountered during development (The mathlib Community, 2020). Datasets derived from natural language autoformalization pipelines, including Lean Workbook, Herald, and NuminaMath, provide aligned naturalformal pairs and synthetic proof data, but likewise consist almost entirely of valid formal artifacts rather than trajectories of failed attempts and subsequent repairs (Ying et al., 2024; Gao et al., 2025; Li et al., 2024). Prior work on autoformalization has shown that LLMs can translate between informal and formal mathematics (Wu et al., 2022), and specialized training pipelines can adapt general-purpose models to Lean (Wang et al., 2024; Azerbayev et al., 2024). However, the scarcity of error trajectories in public corpora remains key bottleneck for training models that can interpret and act on compiler diagnostics. Program Repair from Compiler Feedback. In software engineering, program repair has been widely studied as supervised or self-supervised learning problem over paired erroneous and corrected programs. Early neural approaches such as DeepFix train models to localize an erroneous line and predict corrected statement, iteratively applying localized edits rather than regenerating entire programs (Gupta et al., 2017). Subsequent work such as DrRepair explicitly conditions models on diagnostic feedback, modeling relationships between source code and compiler error messages to guide repair (Yasunaga & Liang, 2020). More recent systems such as TFix formulate program repair as text-totext transformation task, training large Transformer models on aligned errorfix pairs mined from real-world version control histories and conditioning on static analyzer outputs (Berabi et al., 2021). Across these approaches, diagnostic signals serve as informative inputs that enable targeted corrections rather than global resampling. Synthetic Error Generation for Repair Training. Several lines of work explore how to generate repair supervision at scale when paired errorfix data is scarce. Break-It-Fix-It (BIFI) uses compiler or analyzer as critic to validate candidate fixes on real inputs and iteratively harvest new training pairs, while simultaneously training breaker model to generate realistic synthetic errors from correct code (Yasunaga & Liang, 2021). More recently, Self-play SWE-RL trains single language model in reinforcement learning self-play setting to deliberately inject bugs and learn to repair them, using automatically validated bug artifacts as supervision (Wei et al., 2025). These results demonstrate that synthetic and model-generated errors can provide scalable learning signals for correction in programming domains. Our work applies similar philosophy to the formal verification setting: we systematically mutate correct Lean proofs to generate plausible failures, then use these paired examples to train models for diagnostic-conditioned repair. 3. Methodology 3.1. Data Collection Each entry in APRIL consists of (i) an erroneous Lean proof that fails to compile, (ii) corresponding fixed proof that is syntactically similar but compiles successfully, (iii) the compiler error messages and proof state, (iv) natural-language explanation of the errors, and (v) natural-language fix suggestion describing how to transform the erroneous proof into the corrected one. Because large-scale corpora of human-generated Lean errors are scarce while correct proofs are widely available, we construct APRIL backward: beginning with correct proof and systematically introducing errors into it. Although our failures are induced by controlled mutations, every example is anchored in the expected elaboration behavior: the error message, offending location, and local goal state are produced by the Lean compiler in fixed environment, and the repair target is verified proof in that same environment. This makes APRIL supervision directly aligned with the capability required by interactive theorem proving pipelines. We start from verified correct proofs sourced from the Herald, Lean Workbook, and Numina datasets, retaining only those that compile under Lean 4.22.0-rc4. We then introduce errors using the mutation strategies described below and keep only mutated proofs that trigger compiler errors. To retrieve compiler feedback and compilation rates, we use Lean-Interact to interact with the Lean REPL (Poiroux, 2025). For each retained example, we use DeepSeek-V3-0324 (Liu et al., 2024) to generate an explanation of the compiler feedback and corresponding fix suggestion, yielding aligned tuples of code, diagnostics, explanations, and repair guidance for training and evaluation. APRIL will be made publicly available upon acceptance. Theorem Mutation Errors Theorem mutation is designed to imitate errors that arise from subtle semantic mismatches. The primary motivation is to force models to reason precisely about types, hypotheses, and goal struc3 Lean Error Correction Table 1. Statistics of Correct Proof After Filtering. We report the number of raw and filtered proofs for three source datasets, along with the average proof length, average number of have statements per proof, and number of proofs containing at least one have statement. Across all datasets, we observe significant variation in proof length and have statement usage, with proofs sourced from human annotators in Numinas dataset demonstrating the most complexity in terms of length and have statements. DATASET RAW FILTERED AVG. LINES AVG. have CONTAIN have HERALD LEAN WORKBOOK NUMINA AUTOFORMALIZER NUMINA HUMAN 30,190 10,433 6,039 9,428 16,010 9,491 5,925 8,066 TOTAL 56, 39,492 4.02 2.71 10.20 50.93 14.21 0.13 0.16 2.17 8.90 2.24 1,500 820 2,839 6, 11,287 Table 2. Number of erroneous proofs by split ERROR TYPE TRAIN VAL TEST TOTAL FULL 249,027 9,263 1,835 260, TACTIC LINE THEOREM MULTI-LINE 59,688 17,098 148,362 23,879 2,064 1,021 5,415 763 398 200 1,093 144 62,150 18,319 154,870 24,786 ture, since many failures in real proof development occur when theorem is almost applicable but differs in its required premises or conclusion. By substituting one valid theorem with another that is semantically related but typeincompatible in the current context, we generate failures that require understanding why particular statement does not fit the goal, rather than merely detecting malformed code. To construct such mutations, we begin with proofs that are known to compile successfully. For each proof, we extract the theorems used in the proof. Each resulting (proof, theorem) pair is treated as separate mutation candidate, allowing single proof to yield multiple independent mutation opportunities. For each selected theorem occurrence, we retrieve semantically related declarations using the LeanExplore semantic search engine, queried through its Python API (Asher, 2025). LeanExplore indexes Lean 4 declarations using combination of symbolic features and learned embeddings, enabling retrieval of theorems that are conceptually related even when their names or namespaces differ. From the retrieved candidates, we filter out trivial replacements, including the original theorem itself and declarations that differ only in the namespace. The remaining candidates typically differ in their hypotheses or conclusion in ways that are small but proof-critical. For each mutation candidate, we retrieve up to 5 nearest semantic neighbors from LeanExplore and randomly sample from the filtered candidate set to produce single substitution per candidate. In practice, single proof can yield Figure 2. Statistics based on mutation type. For each dataset, we report the total number of erroneous proofs generated and detail the distribution of specific mutation types: Line Mutation Error (LME), Multi-line Mutation Error (MLME), Tactic Mutation Error (TME), and Theorem Mutation Error (THME). The percentages denote the proportion of each mutation type within its respective dataset. multiple independent theorem mutations corresponding to different theorem occurrences. Mutation is performed by replacing exactly one occurrence of the selected theorem identifier in the proof text with retrieved alternative, leaving the rest of the proof unchanged. This design encourages errors that will sometimes propagate through later steps of the proof, often surfacing at points far from the mutation site, similar to failures encountered during real interactive development. Tactic Mutation Errors Similar to theorem mutations, tactic mutations involve substitution of similar, yet in4 Lean Error Correction each successful tactic mutation, metadata, specifically the original line, substituted line, and the line number, are included to provide additional context for error explanation. Only unique proofs that failed to compile are kept. Figure 3. An example of theorem mutation errors with an illustration of the generation model Figure 5. An example of line mutation errors with an illustration of the generation model Line Mutation Errors To produce line mutation error, we begin with the corrected proof and at random replace one of its proof lines (a line occurring after the main by) with REDACTED, preserving its original indentation. This redacted proof is then passed to DeepSeek-V3-0324 (Liu et al., 2024) with instructions to provide the redacted line that will cause the code to compile. By instructing the model to produce accurate code, but using model that lacks the ability to consistently do so, we hope to produce realistic errors, similar to those that would be produced by more powerful model attempting the entire proof. Each proof is processed independently, and single line is selected uniformly at random from the proof body for redaction. The resulting model-generated completion is reinserted into the proof and compiled with Lean. As with other mutation types, only mutated proofs that fail to compile are retained, and duplicate failures are removed. This procedure yields diverse set of realistic near-miss errors that reflect plausible but incorrect model completions. Multi-Line Mutation Errors Multi-line mutations are produced similarly to line mutations. In multi-line mutations, however, the entirety of the proof after the randomly selected line is redacted, rather than only the selected line. The model is then allowed to produce as many lines as it pleases to replace the redacted lines, rather than producing only single line. In order to ensure that the erroneous proof resembles the correct proof, no more than half of the proof is redacted. Figure 4. An example of tactic mutation errors with an illustration of the generation model correct tactic. This approach requires grouping tactics that serve similar roles, such as arithmetic solvers or rewriting tactics, to ensure that substitution is syntactically valid and plausible. For example, an arithmetic-solving tactic such as nlinarith may be substituted with linarith, norm_num, or ring, which are syntactically valid but likely to fail in its current proof state. We define fixed set of tactic equivalence classes based on common proof roles, including arithmetic solvers (e.g., linarith, nlinarith, norm_num, ring), rewriting tactics (e.g., rw, simp, simp_rw), structural tactics (e.g., intro, intros, rintro), and proof-construction tactics (e.g., apply, refine, exact, assumption). Substitutions are only performed within the same equivalence class to ensure syntactic validity and plausibility. For each proof that is known to compile successfully, we identify each occurrence of swappable tactic and randomly select between one and three tactic occurrences. Each selected tactic is substituted with similar alternative. With 5 Lean Error Correction experiments. Training Format and Objective Each training example is converted into chat-formatted prompt using the corresponding model tokenizer. The input consists of system message and user message concatenating the prover error, local proof state, and failing proof. The supervised target is the assistant completion, structured to encourage an explicit diagnostic reasoning step prior to code generation. The data format is as illustrated in Figure 7 in Appendix . LoRA Setup We apply LoRA adapters with rank 32 to both attention and MLP projection layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) in each transformer block. This configuration is identical across all base models. Optimization and Training Hyperparameters We use AdamW (Loshchilov & Hutter, 2019) with learning rate 1 104, cosine decay, and linear warmup. Training runs for up to 15,000 steps with effective batch size 8 and maximum sequence length 2048. Early stopping with patience 5 (i.e., 1,250 steps) terminates training if validation loss does not improve, and we select the checkpoint with lowest validation loss. Full hyperparameter details are provided in Appendix B. Compute and Implementation Details All experiments are conducted on NVIDIA L40S and H200 GPUs using bfloat16 precision. We enable FlashAttention-2 (Dao, 2024) when available, otherwise falling back to SDPA. 4. APRIL 4.1. Dataset Composition Correct Proof Sources. To construct paired correct and incorrect proofs that remain structurally aligned, we begin from large public corpora of Lean proofs and inject controlled errors into initially correct proofs. We collect correct proofs from the Herald, Lean Workbook, and NuminaMathLean datasets, and retain only samples that successfully compile under Lean 4.22.0-rc4. In total, APRIL contains 39,492 unique compiled theorems with diverse proof styles and complexity  (Table 1)  . Approximately 40.5% of proofs originate from Herald, consisting of modular fragments extracted from intermediate states of human-written mathlib4 proofs. Another 24.0% come from Lean Workbook and are short, low-complexity machinegenerated proofs. The remaining proofs are drawn from NuminaMath-Lean: 15.0% are produced by an autoformalizer and 20.4% are annotated by human experts. These proofs exhibit longer average length and more frequent use of have statements, indicating higher structural complexity. Figure 6. An example of multi-line mutation errors with an illustration of the generation model Explanation and Fix Generation All mutated proofs are compiled with Lean, and only those that produce compiler error are retained, since some changes still lead to correct proofs especially with the use of powerful tactics. For each retained theorem mutation, we also store contrastive metadata, including the names and formal statements of both the intended (correct) theorem and the substituted (incorrect) theorem, along with informal descriptions when available. This metadata is later provided to the explanation and fixsuggestion model to enable contrastive reasoning about why the substituted theorem fails and how replacing it resolves the mismatch, improving the quality of the explanations without resorting to larger reasoning models. For each retained erroneous proof, we automatically generate natural language explanation of the failure and suggested fix using separate language model. The model is prompted with the original proof, the mutated proof, the Lean compiler error message, and any available mutation metadata (e.g., substituted lines or contrastive theorem information). The explanation prompt asks the model to identify the source of the error and describe why the mutation violates typing, hypotheses, or goal structure, while the fix prompt requests concrete edit that would restore correctness. These annotations are generated asynchronously and stored alongside each example, enabling supervised evaluation of both diagnostic interpretation and corrective reasoning. 3.2. Model finetuning We finetune Qwen3-4B-Instruct-2507 (Yang et al., 2025), Kimina-Prover-Distill-8B, and Goedel-Prover-V2-8B using an identical supervised finetuning (SFT) pipeline with LowRank Adaptation (LoRA) (Hu et al., 2022). All training procedures and hyperparameters are shared across all the Lean Error Correction Generated Incorrect Proofs. From the 39,492 compiled theorems, we generate 260,125 incorrect proofs using four mutation operators (Figure 2): (i) theorem substitution, (ii) tactic replacement, (iii) line-level redaction or modification, and (iv) multi-line redaction or modification. Each mutation yields syntactically valid Lean file that fails to compile and produces concrete compiler error trace. Each dataset instance contains: the original correct proof, mutated incorrect proof, the Lean compiler error message and local goal state at the failure location, repaired proof target, and natural-language explanation and fix suggestion aligned with the compiler feedback. Theorem substitution errors constitute the largest fraction of incorrect proofs (59.5%), reflecting the prevalence of typeand goal-mismatch failures in real proof development. See Appendix for detailed description of the discarded strategies for generating erroneous proofs. 4.2. Dataset Split To prevent data leakage, we split the dataset at the level of original theorems rather than individual mutated proofs. All mutated variants derived from the same original theorem are assigned to the same split. This ensures that the model cannot observe identical correct proofs across training and evaluation. We additionally anonymize all theorem declarations by renaming them to canonical identifier (lean_problem), preventing models from exploiting crossdataset name correlations or memorizing problem-specific identifiers. We perform stratified splitting based on (i) source dataset (Herald, Lean Workbook, NuminaMath-Lean) and (ii) proof length, preserving the distribution of proof complexity and domains across splits. The proportions of each mutation type are also maintained in each split  (Table 2)  . Additional details on split statistics are provided in Appendix A. 5. Results To validate the utility of the proposed dataset, we treat it as supervision source for training language models to repair Lean proofs. We evaluate proof repair accuracy on held-out test set of 1,835 erroneous proofs spanning all four mutation types. repair is considered successful if the models output compiles under Lean 4.22.0-rc4. We compare against two baselines: the base Qwen3-4B-Instruct model without finetuning, and Goedel-Prover, the strongest open-source theorem prover available in 8B and 32B variants. We run it under the same single-shot repair interface (no search/iteration) for comparability. 5.1. Main Results Table 3 presents proof repair accuracy across models and error types and demonstrates the effectiveness of the dataset as supervision signal for formal proof repair. Across all evaluated models, finetuning on the dataset yields substantial performance improvements, indicating that the dataset encodes repair-relevant structure that is not captured by pretraining alone. For Qwen3-4B-Instruct, finetuning on the full dataset increases repair accuracy from 1.1% to 27.4%, representing 25 improvement over the base model. Under the same evaluation protocol, this performance slightly exceeds that of Goedel-Prover-V2-32B (26.8%), despite an eight-fold difference in parameter count. This comparison suggests that targeted repair supervision can partially offset the advantages of scale for this task. Similar trends hold for other models. Kimina-Prover models improve steadily with scale, but finetuning on the dataset consistently yields substantial gains regardless of model size. Notably, finetuned 8B models achieve repair accuracies in the 3135% range, outperforming the 32B Goedel baseline by wide margin. Because all models are evaluated in the same single-shot repair setting, the primary takeaway is not architectural novelty but the effectiveness of error-centric supervision: targeted feedback-conditioned training allows small models to close or surpass the gap to much larger open-source provers under this protocol. We emphasize that these numbers reflect single-shot repair from failing attempt, rather than theorem-level success under multi-sample search or exploration. 5.2. Analysis by Error Type Table 3 further breaks down repair performance by mutation category, revealing systematic variation in difficulty across error types while preserving consistent trends across models. Tactic mutations yield the highest repair rates, with top performance reaching 42.5%, reflecting the relatively local nature of these errors within fixed proof context. Theorem mutations exhibit intermediate difficulty, while line mutations are the most challenging, with maximum accuracy of 13.5%, as they often involve correcting semantically inconsistent or hallucinated proof steps. Importantly, models trained jointly on the full dataset maintain strong performance when evaluated on individual error categories. For example, Qwen3-4B-Instruct fine-tuned on the full dataset achieves competitive accuracy across tactic, theorem, and multi-line errors without explicit specialization. Comparable behavior is observed for Goedel-Prover7 Table 3. Proof repair accuracy by model and error type when training jointly for repair and explanation. Results outside the parentheses correspond to models jointly finetuned on all error types and evaluated in each category. Parenthesized values denote models finetuned separately on single error type and evaluated on that same type. Section 5.3 analyzes the effect of removing explanations. Lean Error Correction BASELINE MODEL GOEDEL-PROVER-V2-8B GOEDEL-PROVER-V2-32B KIMINA-PROVER-1.7B KIMINA-PROVER-8B QWEN3-4B-INSTRUCT-2507 FULL 15.5% 26.8% 8.4% 11.1% 1.1% TACTIC 19.6% 34.2% 15.1% 17.3% 1.8% LINE 20.0% 28.5% 13.5% 14.5% 2.5% THEOREM MULTI-LINE 12.7% 23.0% 4.8% 7.9% 0.5% 19.4% 32.6% 10.4% 13.9% 0.0% FINETUNED GOEDEL-8B FINETUNED KIMINA-8B FINETUNED QWEN3-4B 34.6% 41.7% (46.5%) 31.9% 38.9% (47.2%) 27.4% 39.7% (42.5%) 18.5% (21.5%) 18.5% (23.5%) 16.0% (13.5%) 36.8% (37.0%) 34.1% (37.0%) 26.8% (26.9%) 20.8% (22.9%) 14.6% (21.5%) 13.2% (18.8%) Table 4. Pass@1 on error types matching the training regime. denotes the change from w/o exp (color-coded). Model Method w/o exp w/ exp Goedel-8B Kimina-8B Full Tactic Line Theorem Multi-Line Full Tactic Line Theorem Multi-Line 36.7% 34.6% -2.1% 48.5% 46.5% -2.0% 25.5% 21.5% -4.0% 37.5% 37.0% -0.5% 24.3% 22.9% -1.4% 36.9% 31.9% -5.0% 49.7% 47.2% -2.5% 23.5% 23.5% 0.0% 37.4% 37.0% -0.4% 26.4% 21.5% -4.9% 8B and Kimina-Prover-8B, where training on the full dataset results in only modest differences relative to models trained separately on each error type. The limited degradation observed under joint training indicates that the datasets mutation types share substantial structural overlap. Rather than inducing negative interference, joint training enables models to learn repair strategies that generalize across error categories. This property is particularly important for realistic proof repair settings, where error types are heterogeneous and not known priori. 5.3. Effect of Explanations We ablate the effect of jointly supervising natural-language explanations alongside proof repair. Specializing exclusively on repair increases pass@1 from 27.4% to 31.2%. This reveals controllable trade-off: training exclusively for repair maximizes autonomous pass@1, while joint supervision yields human-interpretable diagnoses that can support human-in-the-loop debugging or downstream tool-using agents. The explanations that finetuned Qwen produce are valuable in their own right, however. Model explainability is often hard to come by, and this is especially true for error correction in Lean. Top models (Goedel and Kimina) are specialized in error correction to the point that they have lost much of their ability to produce meaningful natural language, including to explain their work, making their reasoning very difficult to understand. Training on our dataset allows Qwen to improve significantly at correcting Lean errors and at the cost of an only slight reduction in error correction accuracy it can also produce explanations of its reasoning. As small demonstration of the downstream utility of explanations, we provide DeepSeek with the same failing instance augmented by an explanation, and observe substantially higher success rates when using explanations produced by the explanationtrained model. DeepSeek succeeds with rate of 4% when aided by base Qwens explanations and 29% when aided by the trained models explanations, performing better than either model individually. 6. Conclusion We introduced Lean proof repair as supervised learning task: given failing proof and compiler feedback, predict corrected proof that compiles. To support this, we constructed APRIL, dataset of paired erroneous and verified proofs augmented with compiler diagnostics and additional natural-language annotations. Supervised finetuning on this data substantially improves repair accuracy, with finetuned 4B model outperforming larger open-source provers under our evaluation setup. These results suggest that error-centric supervision is strong and underutilized training signal for building agentic theorem provers that can iteratively refine proofs from feedback."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank the UW eScience School and UW IT for the AWS credits. We thank the UW Department of Applied Mathematics for access to GPUs. And we thank Nebius for providing the LLM cloud credits for this work. 8 Lean Error Correction"
        },
        {
            "title": "References",
            "content": "Achim, T., Best, A., Bietti, A., Der, K., Fédérico, M., Gukov, S., Halpern-Leistner, D., Henningsgard, K., Kudryashov, Y., Meiburg, A., et al. Aristotle: Imo-level automated theorem proving. arXiv preprint arXiv:2510.01346, 2025. Asher, J. Leanexplore: search engine for lean 4 declarations. arXiv preprint arXiv:2506.11085, 2025. Azerbayev, Z., Schoelkopf, H., Paster, K., Dos Santos, M., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics. In International Conference on Learning Representations (ICLR), 2024. Brief: Open math LLM; commonly used as baseline backbone in formal/informal math experiments and data generation. Berabi, B., He, J., Raychev, V., and Vechev, M. Tfix: Learning to fix coding errors with text-to-text transformer. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. Chen, L., Gu, J., Huang, L., Huang, W., Jiang, Z., Jie, A., Jin, X., Jin, X., Li, C., Ma, K., et al. Seed-prover: Deep and broad reasoning for automated theorem proving. arXiv preprint arXiv:2507.23726, 2025. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Efficient attention kernel used in transformer training and inference. de Moura, L. and Ullrich, S. The Lean 4 theorem prover and programming language. In Automated Deduction CADE 28, pp. 625635. Springer, 2021. Brief: System description of Lean 4 (kernel, elaborator, metaprogramming, and language design) used as the proof assistant in this work. Gao, G., Wang, Y., Jiang, J., Gao, Q., Qin, Z., Xu, T., and Dong, B. Herald: natural language annotated lean 4 dataset. In International Conference on Learning Representations (ICLR), 2025. Google. Gemini 3. https://blog.google/ products-and-platforms/products/gemini/ gemini-3/, 2025. Accessed 2026-01-24. Gupta, R., Pal, S., Kanade, A., and Shevade, S. Deepfix: Fixing common language errors by deep learning. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI), 2017. Lample, G., Lacroix, T., Lachaux, M.-A., Rodriguez, A., Hayat, A., Lavril, T., Ebner, G., and Martinet, X. Hypertree proof search for neural theorem proving. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Brief: Tree-search theorem proving with neural guidance; relevant comparison to your \"no search, pure repair\" setting. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Lin, Y., Tang, S., Lyu, B., Wu, J., Lin, H., Yang, K., Li, J., Xia, M., Chen, D., Arora, S., and Jin, C. Goedelprover: frontier model for open-source automated theorem proving. arXiv preprint arXiv:2502.07640, 2025. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations (ICLR), 2019. Introduces AdamW optimizer. Poiroux, A. lean 4. LeanInteract, 2025. GitHub repository. Leaninteract: python interface for https://github.com/augustepoiroux/ Ren, Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., et al. Deepseekprover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025. The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs (CPP). ACM, 2020. The Rocq Development Team. Rocq prover 9.0.0 release notes. https://rocq-prover.org/releases/9.0. 0, 2025. Released 2025-03-12; accessed 2026-01-24. Wang, H., Unsal, M., Lin, X., Baksys, M., Liu, J., Santos, M. D., Sung, F., Vinyes, M., Ying, Z., Zhu, Z., et al. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. arXiv preprint arXiv:2504.11354, 2025. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. Parameterefficient fine-tuning method used for adapting LLMs. Wang, R., Zhang, J., Jia, Y., Pan, R., Diao, S., Pi, R., and Zhang, T. Theoremllama: Transforming general-purpose LLMs into Lean4 experts. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. Brief: Training pipeline to 9 Lean Error Correction Zheng, K., Han, J. M., and Polu, S. Minif2f: crosssystem benchmark for formal olympiad-level mathematics. In International Conference on Learning Representations (ICLR), 2022. Brief: Standard formal benchmark (Lean/others) used widely to evaluate theorem provers; relevant for baseline comparisons. Zhou, Y., Zhao, J., Zhang, Y., Wang, B., Wang, S., Chen, L., Wang, J., Chen, H., Jie, A., Zhang, X., et al. Solving formal math problems by decomposition and iterative reflection. arXiv preprint arXiv:2507.15225, 2025. specialize general LLMs for Lean 4; relevant for discussion of general vs specialized models and data bootstrapping. Wei, Y., Sun, Z., McMilin, E., Gehring, J., Zhang, D., Synnaeve, G., Fried, D., Zhang, L., and Wang, S. Toward training superintelligent software agents through self-play swe-rl. arXiv preprint arXiv:2512.18552, 2025. Wu, Y., Jiang, A. Q., Li, W., Rabe, M. N., Staats, C., Jamnik, M., and Szegedy, C. Autoformalization with large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Brief: Autoformalization pipelines mapping natural language to formal statements/proofs; relevant contrast to \"error trajectories\" scarcity. Xin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan, C., Li, W., and Liang, X. Deepseek-prover: Advancing theorem proving in LLMs through large-scale synthetic data, 2024a. Brief: Whole-proof generation in Lean 4 trained with large-scale synthetic formal data; key endto-end prover baseline family. Xin, H., Ren, Z. Z., Song, J., Shao, Z., Zhao, W., Wang, H., Liu, B., Zhang, L., Lu, X., Du, Q., Gao, W., Zhu, Q., Yang, D., Gou, Z., Wu, Z. F., Luo, F., and Ruan, C. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024b. Brief: Uses proof-assistant feedback for RL and MCTS-style search (RMaxTS); relevant for comparing \"repair/refinement\" vs search/RL. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yang, K., Swope, A., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, S., Prenger, R. J., and Anandkumar, A. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems, 36:2157321612, 2023. Yasunaga, M. and Liang, P. Graph-based, self-supervised program repair from diagnostic feedback. In Proceedings of the 37th International Conference on Machine Learning (ICML), 2020. Yasunaga, M. and Liang, P. Break-it-fix-it: Unsupervised learning for program repair. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. Ying, H., Wu, Z., Geng, Y., Wang, J., Lin, D., and Chen, K. Lean workbook: large-scale lean problem set formalized from natural language math problems. In NeurIPS 2024 Datasets and Benchmarks Track, 2024. A. Dataset Description Lean Error Correction Table 5. Basic dataset statistics by split. NUM AVG. LINES AVG. HAVE CONTAIN HAVE 14.22 13.74 14.29 14.21 2.25 1.85 1.71 2.24 10,966 (28.64%) 267 (26.70%) 54 (27.00%) 11,287 (28.58%) SPLIT TRAIN VAL TEST 38,292 1,000 200 TOTAL 39,492 Table 6. Statement source distribution by split. SPLIT LEAN WORKBOOK HERALD NUMINA AUTO NUMINA HUMAN TRAIN VAL TEST 9,159 (23.92%) 276 (27.60%) 56 (28.00%) 15,471 (40.40%) 450 (45.00%) 89 (44.50%) 5,833 (15.23%) 77 (7.70%) 15 (7.50%) 7,829 (20.45%) 197 (19.70%) 40 (20.00%) TOTAL 9,491 (24.03%) 16,010 (40.54%) 5,925 (15.00%) 8,066 (20.42%) B. Finetuning Setting Figure 7. template of our finetuning data format C. Prompts for Explanation and Fix Suggestion Generation DeepSeek is involved in the generation of each type of error. In the case of theorem and tactic mutations, we only use it for generating the explanation and fix suggestion. In the case of the line and multiline mutations, we also use it to produce the errors themselves. We provide our prompts for each of these protocols below. C.1. Line Mutation Errors Error generation system prompt: 11 Lean Error Correction Table 7. Optimization and training hyperparameters used in all experiments. Hyperparameter Value Evaluation metric Effective batch size Optimizer Learning rate LR scheduler Warmup ratio Weight decay Max gradient norm Sequence length Sequence packing Precision Evaluation frequency Checkpoint frequency Checkpoint selection Rank (r) Scaling factor (α) Dropout Bias Target modules eval_loss 8 AdamW 1 104 Cosine decay 0.1 0.01 1.0 2048 Disabled bfloat16 (bf16) Every 250 steps Every 250 steps Best validation loss 32 64 0.1 None q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj You are Lean 4 programmer. Error generation instruction block: One line has been redacted in this lean4 proof. Please complete the proof by providing the correct contents of the redacted line. Your response will be automatically searched for your answer. To facilitate this, please write \"MY ANSWER\" before your answer. Your answer should be exactly one line long and should contain no semicolons. For example, if you were given ```lean4 theorem very_simple: 1+1=2 := by REDACTED ``` you might respond with \"\"\" This is very easy, `rfl` accomplishes this in Lean 4. MY ANSWER ```lean4 rfl ``` \"\"\" Now try this theorem ```lean4 {broken_proof} ``` Explanation system prompt You are Lean 4 programmer diagnosing one failing proof. Assume you ONLY see the incorrect proof text, the infoview state near the failure, and Lean's error message. Explanation instruction block. 12 Explain why the incorrect proof fails and how to correct it using only the incorrect proof, infoview state, and error. Lean Error Correction Return ONLY one JSON object with exactly these two fields: { \"explanation\": \"1--3 sentences explaining the concrete reason the proof fails\", \"fix_suggestion\": \"1 sentence with high-level fix (no code); } No code blocks. No extra fields. Both fields must be non-empty. C.2. Theorem Mutation Errors System prompt. You are Lean 4 programmer diagnosing one failing proof. You will see the incorrect proof, its state/error, and cheatsheet of metadata detailing the intended (correct) theorem versus the one that was substituted (incorrect). Use this metadata to explain the failure. Instruction block. Explain why the proof fails by contrasting the incorrect vs intended theorem. Return ONLY one JSON object with exactly these two fields: { explanation: 1--3 sentences explaining the concrete reason the proof fails, fix_suggestion: Start with: Replace (incorrect_name) with (correct_name), and briefly say why that resolves the mismatch. } No code blocks. No extra fields. Both fields must be non-empty. C.3. Tactic Mutation Errors Explanation system prompt You are Lean 4 programmer diagnosing one failing proof. You will see an incorrect proof containing one or more invalid tactics. You will also see its state/error and 'cheatsheet' of metadata detailing the intended (correct) line versus the current (incorrect) line containing swapped tactic. The proof may contain multiple independent tactic failures. The compiler error may only reflect the first encountered failure. Your explanation and fix suggestion should consider all incorrect tactics shown. Use this metadata to explain the failure. Instruction block. Explain why the proof fails by contrasting the incorrect line vs the intended line. Return ONLY one JSON object with exactly these two fields: { explanation: 1-3 sentences explaining the concrete reason why the applied tactic(s) fail to make progress on the goal,including reasoning about goal structure, type, or required properties, without directly mentioning the replacement tactic., Lean Error Correction fix_suggestion: \"Start with EXACTLY the following format: 'Replace `FULL_INCORRECT_TACTIC` with `FULL_INTENDED_TACTIC` on Line because EXPLANATION'. Use the full tactic call including arguments. If multiple errors exist, list fixes for all. } No code blocks. No extra fields. Both fields must be non-empty. D. Unsuccessful Attempts for Data Synthesis In addition to the mutation strategies described above, we explored several alternative approaches for generating realistic erroneous proofs. These methods were motivated by the goal of producing more diverse and challenging failures, but in practice they exhibited limitations in controllability, fidelity, or scalability. We briefly summarize these unsuccessful attempts below to clarify the design choices that led to our final dataset construction pipeline. Introduce an (interesting) error. One approach we explored was directly prompting language model to introduce an \"interesting\" error into an otherwise correct proof. In practice, this strategy exhibited very low diversity: even when increasing sampling temperature, the model tended to produce the same small set of superficial modifications. The resulting errors often did not meaningfully stress semantic reasoning about the proof state. Single-line translation to natural language. We additionally explored translation-based error generation by \"translating\" an individual Lean proof into natural language and then converting them back into Lean. Since formal logic is removed when converting to natural language, we hoped that this approach would produce realistic, interesting errors. However, we found that single-line translations struggled without the context of the theorem, resulting in errors that were unrealistic in the context of the theorem. Full-proof translation to natural language. Since single-line translation performed poorly due to lack of context, we repeated this translation process with the entire proof. For this approach, we focused solely on the proof body and separated it from the theorem statement. When converting from natural language back to Lean, we included the formalized theorem header to provide context. Despite these changes, the output proof was deemed too dissimilar from the original proof to be considered useful for error correction. Full-proof translation to alternative proof assistants. One method we attempted for producing interesting errors was having an LLM translate the entire proof into another proof assistant and then back to Lean 4. We attempted this with both Rocq (The Rocq Development Team, 2025) and Lean 3 but were unsuccessful in both cases. When translating to Rocq and back, the incorrect proof that was produced would be too dissimilar from the correct proof to be considered an erroneous version of it, making it unhelpful for training. In the case of Lean 3, the proofs would often be similar, but the proof would often compile even after the translation. In the cases when it did not compile, the errors rarely resembled those that would actually be made in typical effort to prove the theorem, often being overly simplistic. Common Lean pitfalls. We explored synthetic data generation by prompting large language model to introduce single, controlled Lean proof errors drawn from well-known Lean pitfalls (e.g., misuse of have for data extraction, rewriting under binders, or confusing > with < b). Despite providing explicit pitfall categories and constraining the model to preserve the original statement structure, the generated outputs were largely unusable. The model frequently ignored the specified pitfall, instead modifying unrelated parts of the proof, altering the theorem statement itself, or introducing multiple cascading errors rather than single localized failure. In many cases, the modified proofs still compiled, indicating that the intended error was not semantically realized in Lean. When errors did occur, they often reflected generic type mismatches or syntax issues rather than the targeted pitfall (e.g., confusing Prop and Bool or mishandling inequalities). Manual inspection showed that the model tended to delete or rewrite 510 lines arbitrarily, sometimes removing entire proof segments, and lacked sensitivity to Leans proof-state evolution and elaboration constraints. Random Multi-line redactions. We attempted to produce type of error similar to line mutations in which random number of consecutive lines at random location were redacted and model was prompted to fill them in. This was unsuccessful because the models would fail to take the Lean code that followed their section into account, resulting in \"no Lean Error Correction goals to solve\" errors, as well as problems with indentation and other syntactical frustrations. This occurred even when using more powerful models such as Gemini 3 (Google, 2025). Proof Repair via Prover-Based Pipelines. We evaluated two-stage proof generation and repair pipeline. First, we prompted Kimina-Prover-Distill-1.7B (Wang et al., 2025) to provide proofs for theorems in the dataset. The proofs that failed to verify were then passed to Goedel-Prover-V2-8B. This approach rarely produced valid proof repairs: Goedel-Prover typically rewrote the proof from scratch, substantially altering the proof structure and reasoning. As result, the outputs could not reasonably be characterized as corrections of the original incorrect proofs, but rather as independent re-proofs."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, University of Washington, Seattle, United States",
        "Department of Mathematics, University of Washington, Seattle, United States",
        "Math AI Lab, University of Washington, Seattle, United States"
    ]
}