{
    "paper_title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders",
    "authors": [
        "Orion Weller",
        "Kathryn Ricci",
        "Marc Marone",
        "Antoine Chaffin",
        "Dawn Lawrie",
        "Benjamin Van Durme"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training."
        },
        {
            "title": "Start",
            "content": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders Orion Weller ι Kathryn Ricci ι Marc Marone ι Antoine Chaffin α Dawn Lawrie ι Benjamin Van Durme ι 5 2 0 2 J 5 1 ] . [ 1 2 1 4 1 1 . 7 0 5 2 : r ι Johns Hopkins University α LightOn"
        },
        {
            "title": "Abstract",
            "content": "The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data ETTIN1 suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. 400M encoder outperforms 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training."
        },
        {
            "title": "Introduction",
            "content": "The rise of neural language models (LMs) was spurred by encoder-only models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). However, the community generally shifted to decoder-only (i.e. GPT-style, ala Brown et al. (2020)) models due to their exceptional performance at sequence generation. Due to this lack of popularity for encoder-only models there was limited new model development, thus, we still frequently see usage of older models (i.e. from 2019) by the subset of the community focused on retrieval/classification or fast on-device inference. Although nascent work is attempting to revive encoder-only development (Samuel, 2024; Warner et al., 2024; Lee et al., 2025), there still exists wide gap between the development of encoder-only and decoder-only models (synonymously referred to in this work as encoders or decoders). Part of this gap is due to the sentiment within the community that decoders can be adapted for use in tasks that were once predominantly encoder-focused (e.g. classification, embeddings), especially as they can often be used in zero-shot fashion (i.e. without fine-tuning). As decoder models are more studied, more over-trained (Hoffmann et al., 2022), and are generally larger, they are claiming the top of the leaderboards for previously encoder-centric tasks (Enevoldsen et al., 2025). 1Named for the two-headed mythological Norse giant, symbolizing the two language models heads. 2Models, code, and data are available at https://github.com/JHU-CLSP/ettin-encoder-vs-decoder Many works have challenged this assumption by comparing encoder-only and decoder-only models of roughly the same sizes (Ethayarajh, 2019; Charpentier & Samuel, 2024; Harrag et al., 2021). However, these analyses have to be done with incomparable models: using different architectures, different pre-training data, different learning schedules, etc. Our work aims to provide the foundation to compare encoder-only and decoder-only models by open-sourcing suite of models trained with the same data, the same architecture, and the same training recipe. Our ETTIN suite contains 10 models (5 pairs) ranging from 17 million to 1 billion parameters, and trained for up to 2 trillion tokens. This allows us to quantify the differences between these models (including the effects of scaling parameter size) in an apples-to-apples comparison. Our models provide state-of-the-art performance for their size among open-data models. Surprisingly, they do so in both encoder settings (w.r.t. ModernBERT) and decoder settings (w.r.t. LLaMA 3.2 and SmolLM2) despite using the same recipe. Notably, our work also provides the first open-data replication of ModernBERT, allowing the community to further build upon our recipe. We find that, like previous work, encoders excel at classification and retrieval while decoders excel at generative tasks. However, we go beyond previous work to examine the increasingly common setting (BehnamGhader et al., 2024) where decoder-models are continued trained as encoders (i.e. cross-objective training). We show results for this cross-objective training in both directions: training encoders for casual language modeling (CLM) and decoders with masked language modeling (MLM). We find that despite continued training for much longer than previous work (50B tokens) these models do not surpass those that started with this objective, i.e. 400M encoder outperforms 1B decoder continue-trained with MLM on MNLI, and vice versa for generative tasks. Our work also provides the ability to compare these training objectives on other aspects, comparing how they learn. We provide case study showing the effects of these objectives on gender bias. Overall, our work provides the first suite of models enabling fair comparison between encoder-only and decoder-only architectures (while also showing SOTA performance), enabling future work to analyze the effects of these training objectives on downstream tasks."
        },
        {
            "title": "2 Related Work",
            "content": "We describe encoder models as the community is generally more familiar with decoder LMs.3 It is worth noting that our approach was inspired by Pythia (Biderman et al., 2023b) which was the first to explore open-data decoder-only models at multiple sizes. Encoder-only Models Encoder-only architectures were the predominant architecture for early transformer models, popularized by models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and DeBERTa (He et al., 2023). These models showed significantly improved performance over the previous SOTA LSTM models on classification and retrieval tasks. This created flurry of activity in the encoder space, with models improving on the BERT recipe: the RTD objective from DeBERTa, better data and objectives from RoBERTa, and many smaller variants such as TinyBERT (Jiao et al., 2019), DistilBERT (Sanh et al., 2019), BERT-small (Turc et al., 2019), and MiniLM L12 (Wang et al., 2020). However, these encoders lacked the easy ability to generate text and have generally fallen out of popularity in favor of decoder-only GPT-2 style models. Despite this shift, encoders still maintain frequent usage for many tasks that dont require generative output. For example, in March 2025 alone, BERT-base had 90 million downloads on HuggingFace. Recently, there has been renewed interest in encoders, as demonstrated by NomicBERT (Nussbaum et al., 2024), MosiacBERT (Portes et al., 2023), and ModernBERT (Warner et al., 2024). Unfortunately, ModernBERT (the most performant) does not provide access to their training data. Hence, we use publicly available data sources in order to replicate the training process. Comparisons between Encoders and Decoders Previous work has compared encoder-only and decoder-only models on wide assortment of tasks. For example Charpentier & Samuel (2024) 3For those interested in decoders, please see early works such as GPT-2 (Radford et al., 2019) and modern models such as OpenAIs GPT-4 (Achiam et al., 2023), Googles Gemini (Team et al., 2023), Alibabas Qwen series Yang et al. (2025), and Metas LLaMA models (Grattafiori et al., 2024) 2 Parameter Layers Hidden Size Intermediate Size Attention Heads Learning Rate Weight Decay Warmup Tokens (B) BS Warmup (B) 17M 32M 68M (XS) (XXS) (Small) 150M 400M (Large) (Base) 7 256 384 4 3e-3 3e-4 4 125 10 384 576 6 3e-3 3e-4 4 100 19 512 768 8 3e-3 3e-4 3 75 22 768 1152 12 8e-4 1e-5 3 50 28 1024 2624 16 5e-4 1e-5 2 10 1B (XL) 28 1792 3840 28 5e-4 5e-5 2 3 Table 1: Configuration for each Ettin model size. Both encoders and decoders use the exact same configuration, differing only in attention (bidirectional vs causal) and objective (MLM vs CLM). compare DeBERTa and GPT-2 in similar sizes. Other work (Yang et al., 2023; Qu et al., 2020; Zheng et al., 2021; Rehana et al., 2023) compares them on downstream tasks. However, all of these comparisons have the same underlying limitation: the models they are comparing have different numbers of parameters, different architectures, different training recipes, and different pre-training data. Although some work has attempted to address this (Charpentier & Samuel, 2024; Gisserot-Boukhlef et al., 2025), they have only done so in limited settings with very small amounts of pre-training data. In contrast, we train SOTA models, allowing for an exact comparison. Bidirectional Attention for Decoders Although we cannot cover it all here, there have been attempts to use bidirectional attention for standard decoder usage. This includes prefix LM attention (Artetxe et al., 2022; Chowdhery et al., 2023; Du et al., 2021) and other mixed training such as BiTune (Kopiczko et al., 2024). However, most modern LM training still favors pure CLM.4 Checkpoint-Level Model Analyses There has also been much work exploring how models learn via their training data. This was popularized by the Pythia (Biderman et al., 2023a) paper and includes many aspects of learning such as data quality and selection (Longpre et al., 2024), how frequency of entities impacts model learning (Oh et al., 2024), effects of the recency of data (Cheng et al., 2024), and whether you can recognize and extract training data from models (Zhang et al., 2024). Our work allows these experiments to be done on more recent SOTA models and provides way to compare encoders and decoders on various facets of learning."
        },
        {
            "title": "3 Experimental Settings",
            "content": "3.1 Training Data We create an open-source replication of ModernBERT (Warner et al., 2024) due to it being the strongest publicly available encoder-only model and using comparable techniques to decoder-only models. This provides the best starting place for recipe that spans both training objectives. However, ModernBERTs data is not publicly available thus, we aim to replicate the recipe using open-data. We do so by pulling from the best publicly available datasets used for training decoder-only models, such as Olmo (Groeneveld et al., 2024; OLMo et al., 2025). Thus, we use mix of DCLM (Li et al., 2024) combined with various curated sources from Dolma v1.7 (Soldaini et al., 2024). In the process of training, the Olmo 2 paper (OLMo et al., 2025) described their approach of using filtered DCLM and other higher-quality sources for the decay phase (similar to FineWeb-Edu filtering Penedo et al. (2024); Lozhkov et al. (2024)). We decided to use these newer sources for our later phases.5 To allow others to easily extend our work, we provide both formats: the data which can be used for training as well as the data seen by the models in batch order for future analyses. 4The the best of our knowledge, as much of the details of the best LMs now goes unpublished. 5We ablated with the non-filtered data and found worse results. Category Dataset Tokens (B) % Tokens (B) % Tokens (B) % Pre-training Mid-training Decay Phase CC News Starcoder Code_Repos CC Head DCLM DCLM (Dolmino) Open-Web-Math Algebraic StackExchange Math (Dolmino) News Code Code Crawl Crawl Crawl Math Math Math Scientific PeS2o Scientific Arxiv Reddit Social StackExchange Social Social StackExchange (Dolmino) Reference Textbooks Reference Dolma Books Reference Wikipedia Instruction Tulu Flan 7.3 0.4 263.9 15.5 356.6 20.9 837.2 49.1 0.7 12.7 0.7 12.6 3.4 57.3 1.6 28.0 4.7 80.3 1.1 19.6 0.3 5.3 0.4 7.3 1.0 16.6 38.4 15.4 175.5 70.4 4.2 10.4 3.3 8.3 1.6 4.1 2.5 6.2 1.1 2.7 0.3 0.8 0.2 0.5 1.0 2.4 20.2 26.5 26.0 34.1 6.6 5.0 3.9 3.0 5.2 4.0 0.5 0.7 10.5 13.8 3.9 3.0 5.4 4.1 Total 1,704.7 100.0 249.3 100.0 76.3 100. Table 2: Training data mixture across the various training stages (pre-training, mid-training, decay). Later stages use higher quality data, from the recently released Dolmino dataset (OLMo et al., 2025). Dashes indicate that no data from that source was used. We trained for 1.7T tokens for pre-training, 250B for mid-training, and 50B for the decay phase. We sample from the dataset and repeat (or under-sample) as needed to hit the token counts used for training. 3.2 Architecture As ModernBERT has only two sizes, we develop new shapes for our smaller and larger models  (Table 1)  . We aim to follow the design espoused by MobileLLM (Liu et al., 2024) with deep but thin models. However, for the 1B model, we keep the same number of layers but make the model wider. We choose models parameter sizes at roughly 2x increments while matching common encoder sizes, e.g. 17M, 32M, 68M, 150M, 400M, and 1B. For detailed list of the differences, see Table 1. 3.3 Training Recipe We use the same general process described by open-data models (which was followed by ModernBERT) for training both encoder-only and decoder-only models with few specific changes for the encoder architecture (i.e. masking ratio). In summary, we include three general phases: base pre-training, mid-training/context extension, and decay. See Table 2 for the precise sources of training data in each phase. We use trapezoidal learning rate scheduler, with general hyperparameters shown in Appendix and size-dependent hyperparameters in Table 1. The only differences between the encoder and decoder models are: (1) the objective function, i.e. masked language modeling (MLM) for the encoder6 vs causal language modeling (CLM) for the decoder and (2) the attention pattern, i.e. causal for the decoder and bidirectional for the encoder. We checkpoint every 8.5B tokens, with 236 checkpoints per model. Combined with the batch ordering of the data, this enables precise pinpointing of what the model learned between each checkpoint. Base Pre-training This stage encompasses the warmup and stable phase of the trapezoidal learning rate, training for 1.7T tokens. We use both learning rate and batch size warmup. The data in this stage comprises wide mix of sources to allow for general learning. Context Extension / Mid-Training In this phase we increase the quality of the data and change both the data and base RoPE (Su et al., 2024) to handle longer context. We update the data length to be up to 8000 tokens and RoPE parameters to 160k (for global and local layers). For the data, 6For the encoder we use 30% masking ratio except for the decay phase, which is 15%. 4 we drop the noisiest sections (older Dolma common crawl, CC News, general StackExchange) and include filtered DCLM, math, and StackExchange. We then train for 250B tokens and use an inverse square root learning rate schedule from the peak learning rate to 1/2 of the peak. Decay Phase Finally, we use one more inverse square root learning rate schedule to decay for 50B tokens. We follow the general ProLong recipe (Gao et al., 2024b), increasing long context data such as Dolma books, Wikipedia, and open-access textbooks. We decay to 0.02 of the peak LR. 3.4 Compute Configuration We train the models on comparatively small compute cluster. We train each model on 4xH100 node using NVLink. The pre-training phase (the longest) takes approximately 6 days for the 17M model. The longest was for the 1B model, which we trained for approximately 40 days. Unfortunately, we did not have enough compute availability to train the 1B to the full 2T tokens. Thus the 1B models are scaled to 1/3 of the data (e.g. 667B instead of 2T tokens). However, this is still more than chinchilla optimal (Hoffmann et al., 2022) and it still outperformed other 1B models trained longer. 3.5 Major training differences from ModernBERT concise summary of the largest differences from the ModernBERT recipe are (1) the use of opendata, (2) decay in the context extension phase, (3) no model merging,7 (4) lower masking ratio for the decay phase (15% instead of 30%) and (5) local and global RoPE to be the same value. 3.6 Cross-Objective Training As encoder models have gone out of popularity, decoder models have increased in size (both parameters and pre-training data). Thus, these newer decoder models are typically trained for much longer than previous encoder models (i.e. BERT). Due to this it has become common to adapt these larger decoder models to what was previously encoder-centric tasks (Zhang et al., 2025). With paired encoder and decoder models, we can now answer the question of how effective this continued pre-training approach is and whether it is still worth training both types of models. We call this cross-objective training: taking the final model and continue pre-training it on the reverse objective.8 We train for 50B tokens with the reverse objective, which is far more than previous work has attempted (BehnamGhader et al., 2024), which is usually around 10B tokens. Although the ratio of pre-training and cross-objective training is unbalanced, this mimics the realistic setting where the adaptation is often done very small amounts of data comparatively. We do this cross-objective training on the highest quality data we have available, which was used in the last decay phase.9 We use new trapezoidal learning rate schedule with 3B tokens of warmup and 10B tokens of decay.10 Thus, by the end we have an encoder-from-decoder (i.e. decoder further pretrained with MLM11) and decoder-from-encoder (i.e. an encoder further pre-trained with CLM)."
        },
        {
            "title": "4 Experiments",
            "content": "We aim to compare encoder and decoder models. However, first, to give those experiments credence, we show that our models are SOTA. This strengthens our claim and helps alleviate concerns that we made training choices that favored one architecture over the other instead we have SOTA models in both architectures for their sizes, showing our methods effectiveness. Note though, that the purpose of our paper was not to be SOTA overall (i.e. compared to OpenAI, etc.), but to provide comparison for encoders and decoders. For space and to avoid repetition, specific model size details are in Table 1. 7We do this for ease of scientific comparison, however, if one was to use this for downstream applications simple merge would likely boost performance another point or two. 8Following BehnamGhader et al. (2024), we do not use MLM but rather use MNTP, that is, the masked token is predicted using the hidden state of the previous token to better align with CLM. 9We note that this means it repeating this data twice, however, as shown by previous work (Muennighoff et al., 2023) two repetitions on high quality data has no adverse effects. 10For the 1B model this is scaled by 1/3 again due to compute availability. 11We use 15% masking rate for the encoder-from-decoder as to maintain middle ground masking ratio. 5 Model Name BERT-mini TinyBERT Ettin-Enc-17m BERT-small MiniLM L12 Ettin-Enc-32m DistilBERT DistilRoBERTa Ettin-Enc-68m BERT base ModernBERT base Ettin-Enc-150m BERT large ModernBERT large Ettin-Enc-400m DeBERTa-v1-xl Ettin-Enc-1B Embedding Tasks CSN MLDR Clustering Retrieval MTEB v2 GLUE Tasks SST-2 MNLI GLUE Avg XXS Models (7-17M parameters) 39.0 37.4 39.1 34.7 33.3 35.6 49.2 49.7 48.9 XS Models (28-33M parameters) 39.6 37.8 39. 38.1 38.4 39.7 51.1 51.3 50.9 Small Models (68-82M parameters) 39.8 39.3 40.1 40.8 40.0 43.1 52.7 51.8 52. Base Models (123-150M parameters) 40.4 41.3 41.5 41.2 43.9 45.7 52.9 54.0 54.0 16.8 14.2 24.4 19.9 19.6 28. 23.7 19.7 30.1 24.8 30.4 31.8 Large Models (353-395M parameters) 25.3 34.9 36.2 28.1 40.2 41.5 41.5 41. 42.9 47.0 48.4 53.8 55.0 55.5 XL Models (884M-1.2B parameters) 42.5 41.9 47.2 50.1 56.4 56. 41.3 39.8 59.1 46.0 48.3 69.2 47.9 60.3 75.1 51.0 75.9 76.3 54.4 78.3 80.7 75.6 82. 88.3 91.2 91.2 90.1 93.3 92.0 92.2 93.1 94.4 93.1 96.0 95.8 93.3 97.1 96.7 97.1 97. 77.2 80.9 79.5 79.2 85.6 83.4 82.7 84.7 87.0 85.4 89.1 89.2 86.3 90.8 91.3 91.7 91. 76.4 77.0 79.2 79.0 84.6 83.5 81.5 83.8 87.2 84.7 88.4 88.9 85.2 90.4 90.8 90.7 91. Table 3: ETTIN encoders compared to other encoder-only models across various sizes on retrieval and GLUE tasks. Due to space, we show two representative tasks from MTEB v2 and two from GLUE, as well as code-based retrieval evaluation (CodeSearchNet) and long-context evaluation (MLDR). See Appendix for the full tables of GLUE and MTEB v2. ETTIN shows significant gains over baseline encoders, including ModernBERT, while also having both larger and smaller sizes. 4.1 Individual Evaluations Encoder-Only Results We use two baselines for each size type: extra extra small (XXS) BERT-mini and TinyBERT, extra small (XS) models MiniLM L12 and BERT-small, small (S) models DistilBERT and DistilRoBERTa, base (B) models BERT and ModernBERT, large (L) models BERT-large and ModernBERT-large, and an extra large (XL) model DeBERTA v2 XL.12 We evaluate on various encoder tasks, including GLUE (Wang et al., 2018), MTEB v2 English (Enevoldsen et al., 2025), MDLR for long context (Chen et al., 2024), and CodeSearchNet for code evaluation (Husain et al., 2019). We use the same evaluation setup as ModernBERT for the evaluation for an equal comparison (see Appendix for hyperparameter details). We find in Table 3 that ETTIN compares favorably overall. The relatively larger gains in the bigger sizes is likely because the smaller model baselines are heavily optimized with distillation.13 Even so, we see that they generally outperform the baselines without doing any distillation: e.g. Ettin-68m with GLUE average of 87.2 compared to the next best DistilRoBERTa at 83.8. Even for the more recent ModernBERT baselines we see improved performance (88.9 GLUE average vs 88.4 for the base size). Thus we can see that ETTIN matches or improves the SOTA for encoder-only models. 12We also ran experiments with DeBERTa XXL as shown in the appendix. However, due to the size and slowness of the architecture we could not do an comparable grid search. Our results in the appendix are after 300 days of H100s hours, but still did not complete the full sweep. Furthermore, as DeBERTa XXL is > 1.5B we exclude it as it is significantly larger than 1B (i.e. > 50% larger). 13We also note that MiniLM L12 has twice the amount of non-embedding parameters 21M vs 12M. 6 Model Name ARC HS LMB OBQA PIQA SciQ SIQA TQA WG WSC Avg XXS Models (14-17M parameters) Pythia-14m Ettin-Dec-17m 21.2 21.3 26.0 27.1 7.1 23.0 26.2 27. 55.2 57.7 43.8 71.1 33.4 35.4 0.0 2.6 50.3 50.9 51.6 48. 31.5 36.4 XS Models (32M parameters) Ettin-Dec-32m 23.5 28.5 28. 28.2 57.7 77.5 36.4 3.8 53. 50.2 38.7 Small Models (68-82M parameters) DistilGPT Ettin-Dec-68m 23.0 25.3 27.5 33. 25.0 35.2 26.8 29.4 59.8 61.8 62.6 83.2 36.1 38.8 0.3 5. 50.4 50.1 53.8 55.3 36.5 41.8 SmolLM2-135m 29.1 24.0 Pythia-160m 28.6 Ettin-Dec-150m Pythia-410m 24.7 SmolLM2-360m 37.6 33.6 Ettin-Dec-400m OLMo-1B-0724 Llama-3.2-1B Ettin-Dec-1B 32.3 36.2 39.7 Base Models (135-160M parameters) 42.9 32.9 43.2 32.4 26.4 29.2 68.4 62.0 66.6 78.5 67.2 89. 39.4 36.9 40.1 Large Models (360-410M parameters) 51.5 53.5 52.3 29.4 37.6 34.4 67.0 71.8 71.0 72.3 86.6 91. 39.0 40.7 45.5 XL Models (908M-1.2B parameters) 61.0 62.1 58.4 35.6 37.2 41.6 75.1 75.0 74.4 91.8 88.4 93. 49.2 43.2 48.2 43.1 30.2 40.3 40.6 56.3 54.3 66.1 63.7 62.9 5.0 0.4 11.2 1.8 18.4 18. 1.2 24.9 29.3 53.7 52.4 53.7 53.6 58.6 57.6 61.6 60.6 62.7 59.7 58.2 59.0 65.2 70.3 71. 76.9 74.7 79.1 45.2 39.1 46.2 44.5 53.1 53.1 55.1 56.6 59.0 Table 4: Performance comparison of decoder-only models across tasks, organized by size categories. We see that ETTIN decoders compare favorably, matching or exceeding the previous opendata SOTA. Task names in order are ARC, Hellaswag, LAMBADA, OpenBookQA, Social IQA, TrivialQA, Winogrande, and Winograd Schema Challenge. Decoder-Only Results We use two baselines for each size type when available, but few very small decoder-only LMs exist: One extra extra small (XXS) model Pythia-14M,14 no models in the extra small (XS) category that we could find, one small (S) model DistilGPT (Sanh et al., 2019), two base (B) models Pythia 160M (Biderman et al., 2023a) and SmolLM2 135M, two large (L) models Pythia 410M and SmoLM2 360M (Allal et al., 2025), and extra large (XL) models Olmo 1B 0724 Groeneveld et al. (2024) and Llama 3.2 1B (Dubey et al., 2024). We evaluate on wide range of tasks using the Eleuther AI harness (Gao et al., 2024a) (see Appendix for details), consolidating tasks used in the Pythia and SmolLM papers including: the ARC Challenge (ARC) Clark et al. (2018), HellaSwag (HS) (Zellers et al., 2019), LAMBADA (LMB) (Paperno et al., 2016), OpenBookQA (OBQA) (Mihaylov et al., 2018), Social IQA (SIQA) (Sap et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), Winogrande (WG) (Sakaguchi et al., 2021), and the Winograd Schema Challenge (WSC) (Levesque et al., 2012). We see the results in Table 4 and see that ETTIN performs well compared to baseline models, such as ETTIN-150m outperforming SmolLM2 46.2 to 45.2 and ETTIN-1Bs 59.0 to Llama 3.2 1Bs 56.6 average). Thus we can see that ETTIN improves the SOTA for open-data decoder-only models. 4.2 Encoders vs Decoders Now that the strength of the training recipe is established, we can compare the two training objectives. For simplicity, we show the two most representative encoder tasks (MS MARCO dev Bajaj et al. (2016) for retrieval and classification on MNLI) and keep the average generative score.16 We evaluate the decoders on encoder-only tasks and vice versa, and similarly with the cross-objective trained models. We evaluate encoder-only models on decoder generative tasks using the method proposed in 14This is likely just debug-sized run and not an official size, as they do no include it in their paper. However, as not other decoder models in this size could be found, we use it as reference. 15What is considered 1B\" has large range, up to nearly 2B parameters. We thus restrict our range to < 1.2B parameters to be 1B\" model, which excludes models like SmolLM2 1.7B and Olmo 2 1B (actually 1.5B). 16As decoder evaluations are significantly quicker than fine-tuning which is required for encoder tasks. 7 Figure 1: Encoder vs decoder comparison across model size. In most cases, models in the preferred architecture (e.g. encoders in MNLI) do better than the opposite architecture even with an order of magnitude greater size, e.g. 400M decoder outperforming the 1B encoder. Notably, in generative tasks, decoders-from-encoders scale poorly with size, matching performance at small sizes but seeing wide gap as model size grows. table version of this plot can be found in Table 8. Samuel (2024), i.e. using three mask tokens at the end of the sequence and filling in the first token iteratively. Figure 1 shows the results of this comparison across models sizes. MNLI Classification On the representative classification task, we see that encoders dominate decoders, as typically found. Furthermore, we see that even cross-objective continued pre-training does not resolve this gap, with enc-from-dec performance remaining similar to the original decoder model. Furthermore, the pure encoder models are typically better than the next larger sizes of decoders, e.g. the 150M encoder scoring 89.2 compared to the 400M decoders 88.2. MS MARCO Dev Retrieval For retrieval we see similar encoder dominance like classification, but notably improved performance when continue pre-training the decoder (i.e. the encoder-fromdecoder). The MTNP continue pre-training significantly helps the decoder at all sizes, yet even the additional 50B tokens of pre-training is not enough to match the performance of the encoder (i.e. for the 400M size we have 42.2 for the encoder vs 41.4 for the encoder-from-decoder). Although the difference is not as pronounced as in classification, we find that continued pre-training decoder for retrieval is still subpar compared to simply using an encoder, even despite the additional 50B tokens. Generative Tasks We find the reverse of the previous tasks: decoders do better than the decoderfrom-encoder in general, with widening gap (from similar scores at 68m parameters to greater than 6 point difference at 1B) as model size increases. Notably, it appears that continued training of encoders-from-decoders scales poorly, perhaps why there is little-to-no previous work on the topic. Despite this, we note that this average hides some nuance: on generative\" tasks that are more classification focused (such as ARC and SciQ) encoder models used in generative fashion actually exceed decoder performance (i.e. for the 400M size the encoder scores 35.6 ARC vs the decoders 33.6). However, decoders show huge gains on tasks such as HellaSwag, TriviaQA, and SiQA, making it so the average is strongly in favor of the decoders. See Table 7 for all sub-task results. 8 Figure 2: Gender pronoun predictions on the Gotcha split of WinoGender (Rudinger et al., 2018), 50/50 stereotypical male/female split. We see that encoder models are more likely to use gender neutral pronouns whereas both are biased towards male pronouns."
        },
        {
            "title": "5 Case study: Gender Bias",
            "content": "Due to the Ettin suites open-pretraining data, we can also analyze other aspects of learning across pre-training objectives. As one example of future research that can be done, we analyze which objective learns more biased gender representations. We use the WinoGender benchmark (Rudinger et al., 2018) using the Gotcha\" split that has 50/50 split of male/female stereotypical pronouns (i.e. female for nurse). However, the standard coreference task is hard for most of our small models. Thus, we show results for an easier task: simply predicting the pronoun in the sentence. For the standard coreference task results, see Table in the appendix. We have each model predict the pronouns (i.e. by using mask token for encoders or by choosing the lower perplexity sentence with decoders) and show the distribution of predicted pronouns per model (male, female, or gender neutral).17 The results are in Figure 2, which shows that encoders are much more likely to use gender neutral pronoun overall. In both encoders and decoder, female pronouns become more used as the size of the model gets larger: for decoders there is clear trend of progressively smaller amounts of male pronouns, whereas for encoders the trend is more stochastic. For effects of the cross-training objectives on the model, see Figure 3 in the appendix. Overall As both models saw the exact same training data, we see that the MLM objective encourages the model to choose more neutral pronouns over female pronouns. However, male gender bias seems strong in both models, if slightly higher for decoders. This shows just one type of training-data based analysis that can be done and we leave others to future work."
        },
        {
            "title": "6 Discussion",
            "content": "Our work suggests the following conclusions: (1) as speculated by previous work, MLM and CLM objectives do convey different strengths MLM for classification and retrieval and CLM for generative tasks. However, (2) we also went step further to show that simply continued pre-training on the reverse objective does not make up the difference from not using the preferred architecture. This has several implications for those using models for classification or retrieval: currently the top models on leaderboards like MTEB are 7B+. However, based on our experiments, it is likely 3B encoder model would outperform it. The lack of large encoder-only size models means that 17There are more than three types of pronouns used in English beyond what is in this dataset. However, WinoGender is only designed for these three. We leave extensions of this dataset to future work. 9 approaches that continue pre-train decoders will likely outperform all other options (as is currently seen on the leaderboards). In the small scale regime (1B or less) it easier to train more niche\" encoder models for classification and retrieval and based on our results it seems likely encoders will continue to outperform all others in their size range. Our results also suggest that encoders and decoders learn differently in other aspects as well, such as gender bias. Although this is just one example, we look forward to future research that discovers other differences and provide our artifacts to enable such work."
        },
        {
            "title": "7 Conclusion",
            "content": "We provide the first suite of paired models that use the same training recipe to compare encoder-only and decoder-only models. Our models are SOTA in their size for open-data models, and are the first public recipe for ModernBERT-style models. We show that encoders are strong in classification and retrieval, while decoders are strong in generative tasks. Furthermore, we show that this difference can not easily be solved by continued training with the reverse objective. We show that this suite allows the analysis of how pre-training objective impacts learning, showing case study in gender bias. We release all artifacts (including training data order) to help future researchers analyze these models."
        },
        {
            "title": "Acknowledgments",
            "content": "This work has been supported by both DARPA SciFy and the U.S. National Science Foundation under grant 2204926. Any opinions, findings, and conclusions or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of the National Science Foundation or DARPA. OW is supported by an NSF GRFP fellowship."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. Mikel Artetxe, Jingfei Du, Naman Goyal, Luke Zettlemoyer, and Ves Stoyanov. On the role of bidirectionality in language model pre-training. arXiv preprint arXiv:2205.11726, 2022. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023a. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023b. 10 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Lucas Georges Gabriel Charpentier and David Samuel. GPT or BERT: why not both? In Michael Y. Hu, Aaron Mueller, Candace Ross, Adina Williams, Tal Linzen, Chengxu Zhuang, Leshem Choshen, Ryan Cotterell, Alex Warstadt, and Ethan Gotlieb Wilcox (eds.), The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning, pp. 262283, Miami, FL, USA, November 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.conll-babylm.24/. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024. Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, and Benjamin Van Durme. Dated data: Tracing knowledge cutoffs in large language models. arXiv preprint arXiv:2403.12958, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171 4186. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Annual Meeting of the Association for Computational Linguistics, 2021. URL https://api.semanticscholar.org/ CorpusID:247519241. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, et al. Mmteb: Massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, 11 Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024a. URL https://zenodo.org/records/12608602. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024b. Hippolyte Gisserot-Boukhlef, Nicolas Boizard, Manuel Faysse, Duarte M. Alves, Emmanuel Malherbe, André F. T. Martins, Céline Hudelot, and Pierre Colombo. Should we still pretrain encoders with masked language modeling?, 2025. URL https://arxiv.org/abs/2507.00994. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. Fouzi Harrag, Maria Debbah, Kareem Darwish, and Ahmed Abdelali. Bert transformer model for detecting arabic gpt2 auto-generated tweets. arXiv preprint arXiv:2101.09345, 2021. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=sE7-XhLxHA. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Dawid Kopiczko, Tijmen Blankevoort, and Yuki Asano. Bitune: Bidirectional instruction-tuning. arXiv preprint arXiv:2405.14862, 2024. Simon Lee, Anthony Wu, and Jeffrey Chiang. Clinical modernbert: An efficient and long context encoder for biomedical text. arXiv preprint arXiv:2504.03964, 2025. Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In 13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012, Proceedings of the International Conference on Knowledge Representation and Reasoning, pp. 552561. Institute of Electrical and Electronics Engineers Inc., 2012. ISBN 9781577355601. 13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012 ; Conference date: 10-06-2012 Through 14-06-2012. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692. 12 Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing In Forty-first International sub-billion parameter language models for on-device use cases. Conference on Machine Learning, 2024. Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 32453276, 2024. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36:5035850376, 2023. Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed: Training reproducible long context text embedder. CoRR, abs/2402.01613, 2024. doi: 10.48550/ARXIV. 2402.01613. URL https://doi.org/10.48550/arXiv.2402.01613. Byung-Doh Oh, Shisen Yue, and William Schuler. Frequency explains the inverse correlation of large language models size, training data amount, and surprisals fit to reading times. arXiv preprint arXiv:2402.02255, 2024. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. arXiv preprint arXiv:1808.08949, 2018. Jacob Portes, Alexander Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. Mosaicbert: bidirectional encoder optimized for fast pretraining. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/095a6917768712b7ccc61acbeecad1d8-Abstract-Conference.html. Yuanbin Qu, Peihan Liu, Wei Song, Lizhen Liu, and Miaomiao Cheng. text generation and prediction system: pre-training on new corpora using bert and gpt-2. In 2020 IEEE 10th international conference on electronics information and emergency communication (ICEIEC), pp. 323326. IEEE, 2020. 13 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Hasin Rehana, Nur Bengisu Çam, Mert Basmaci, Jie Zheng, Christianah Jemiyo, Yongqun He, Arzucan Özgür, and Junguk Hur. Evaluation of gpt and bert-based models on identifying proteinprotein interactions in biomedical text. ArXiv, pp. arXiv2303, 2023. Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. David Samuel. Berts are generative in-context learners. Advances in Neural Information Processing Systems, 37:25582589, 2024. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, et al. Reasonir: Training retrievers for reasoning tasks. arXiv preprint arXiv:2504.20595, 2025. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962, 2019. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33:57765788, 2020. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, et al. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv preprint arXiv:2412.13663, 2024. Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models to follow instructions. arXiv preprint arXiv:2403.15246, 2024. Orion Weller, Benjamin Chang, Eugene Yang, Mahsa Yarmohammadi, Samuel Barham, Sean MacAvaney, Arman Cohan, Luca Soldaini, Benjamin Van Durme, and Dawn Lawrie. mfollowir: multilingual benchmark for instruction following in retrieval. In European Conference on Information Retrieval, pp. 295310, 2025. 14 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Binxia Yang, Xudong Luo, Kaili Sun, and Michael Luo. Recent progress on text summarisation In International conference on knowledge science, engineering and based on bert and gpt. management, pp. 225241. Springer, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Frank Yang, and Hai Li. Min-k%++: Improved baseline for detecting pre-training data from large language models. arXiv preprint arXiv:2404.02936, 2024. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Xianrui Zheng, Chao Zhang, and Philip Woodland. Adapting gpt, gpt-2 and bert language models for speech recognition. In 2021 IEEE Automatic speech recognition and understanding workshop (ASRU), pp. 162168. IEEE, 2021."
        },
        {
            "title": "A All Encoder Results",
            "content": "We show results for the full MTEB v2 eng and GLUE results in Table 5 and Table 6 respectively."
        },
        {
            "title": "B Decoder Evaluation Frameworks",
            "content": "tasks, generative For commit 867413f8677f00f6a817262727cbb041bf36192a. We also use forked version of the Eleuther AI harness for evaluating encoders in generative fashion (see Github for details). Following previous work (Allal et al., 2025) the ARC score is the average of the easy and challenge sets. Eleuther harness with use the we AI For encoders evaluated on generative tasks we use three mask tokens followed by the EOS token. At each step, we predict the first MASK token and iteratively generate. However, this approach still could be improved, in particular around the EOS token. Encoder models are non-calibrated for when this should appear, so we had to make small changes to this setup for two tasks: for TriviaQA we change the EOS token to newline character (as the harness stops on newlines for TriviaQA also) and for the Lambada OpenAI task we do not score the EOS token. All other tasks proceed with the three masks + EOS token as proposed by Samuel (2024)."
        },
        {
            "title": "C Architecture Details",
            "content": "Architecture and training details for all models are found in Table E. These are generally the same as ModernBERT except for the same value of local and global RoPE and slightly shorter context length (7999)."
        },
        {
            "title": "D Model Sizes",
            "content": "Model sizes (both embedding and non-embedding) are found in Table 10. We group models by total parameters, although we note that some have more vocab parameters vs non-vocab parameters, e.g. MiniLM L12 has almost 2x the number of non-embedding parameters compared to Ettin-32m (21M vs 12M)."
        },
        {
            "title": "E Encoder Evaluation Sweep Parameters",
            "content": "Below we detail the sweep hyperparameters for the retrieval and classification tasks that require fine-tuning. GLUE We re-use ModernBERTs evaluation setup but slightly increase the learning rate sweep in order to better fit the smaller parameter models (which typically use higher LRs). As the best LRs chosen by BERT and ModernBERT were lower than these, it does not affect their scores. We sweep for learning rates over {1e-5, 3e-5, 5e-5, 8e-5, 1e-4}, weight decay values over {1e-6, 5e-6, 8e-6, 1e-5}, and batch sizes over {16, 32}. We also sweep over epochs {1, 2, 3, 4} if task {mnli, sst2, rte}, otherwise {2, 5, 10, 12}. This was total of 160 sweeps per model, of which we select the best score per task per model to report. We start from the best MNLI checkpoint for fine-tuning on RTE, STS-B, and MRPC, following ModernBERT. Retrieval We sweep four LRs ({1e-4, 3e-4, 5e-4, 7e-4}) on MS MARCO dev and choose the best performing one to evaluate on the other retrieval datasets. We use new retrieval training script due to being unable to exactly reproduce ModernBERTs precise scores. While doing so, we also improve the training process for all models due to the use of more negatives in training, achieving higher scores than that in the ModernBERT paper (which was not trying to optimize scores, but shows generally that our training script is effective). We trained with an effective batch size of 1024 with 4 accumulation steps. For DeBERTa-v2 it diverged for all learning rates we tried. Thus, to get it to converge, we changed the warmup to 20% from 5% and lowered the learning rate to 1e-5. As Ettin models have been trained with instructions during pre-training, it is likely they are also more capable for instruction-based retrieval (Shao et al., 2025; Weller et al., 2024, 2025), however, we leave that for future work. Model Name Mean (Task) Mean (Type) Class. Clus. Pair. Class. Rerank. Retrieval STS Summ. TinyBERT BERT-mini Ettin-Enc-17m BERT-small MiniLM L12 Ettin-Enc-32m DistilBERT DistilRoBa Ettin-Enc-68m BERT-base ModernBERT-base Ettin-Enc-150m BERT-large ModernBERT-large Ettin-Enc-400m DeBa-v1-xl DeBa-v2-xxl* Ettin-Enc-1b 52.1 51.8 52.3 54.0 53.9 54.2 55.5 55.2 56.1 56.0 57.1 57. 57.2 58.6 59.4 59.5 60.5 60.4 XXS Models (11-25M parameters) 49.7 49.2 48.9 64.6 61.7 63.3 37.4 39.0 39. 78.1 77.8 74.8 XS Models (28-33M parameters) 51.1 51.3 50.9 64.7 64.9 64.2 39.6 37.8 39.6 79.4 79.8 77. Models (68-82M parameters) 52.7 51.8 52.6 66.5 67.3 66.6 39.8 39.3 40.1 80.7 78.9 79.3 Base Models (86-150M parameters) 52.9 54.0 54.0 67.2 67.5 68.6 40.4 41.3 41.5 80.5 80.4 80.2 Large Models (305-395M parameters) 53.8 55.0 55. 68.3 69.1 69.9 41.5 41.5 41.8 81.1 82.2 82.6 XL Models (750-1565M parameters) 56.4 57.4 56.0 70.8 71.7 72. 42.5 44.4 41.9 82.7 82.4 83.3 41.9 41.5 42.0 41.7 43.5 42.6 42.8 43.5 43.3 43.1 44.7 44. 44.3 45.5 45.6 45.7 46.5 46.4 33.3 34.7 35.6 38.1 38.4 39.7 40.8 40.0 43.1 41.2 43.9 45. 42.9 47.0 48.4 47.2 47.7 50.1 71.9 70.8 71.6 73.0 73.2 73.2 74.1 74.2 74.2 74.8 75.3 74. 76.1 76.5 77.2 77.1 78.3 77.7 20.6 19.3 16.0 21.6 21.4 19.9 24.0 19.4 21.7 23.1 25.2 22. 22.5 23.5 22.6 28.6 30.9 20.4 Table 5: MTEB v2 English results. Class. = Classification, Clus. = Clustering, Pair. Class. = Pair Classification, Rerank. = Reranking, STS = Semantic Textual Similarity, Summ. = Summarization. DeBERTa v2 XXL did not converge with the standard learning rate sweeps, so we used lower learning rate in order to help it to converge. 16 Single Sentence Paraphrase and Similarity Natural Language Inference Model Name CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE Avg BERT-mini TinyBERT Ettin-Enc-17m BERT-small MiniLM L12 Ettin-Enc-32m DistilBERT DistilRoBERTa Ettin-Enc-68m BERT-base ModernBERT-base Ettin-Enc-150m BERT-large ModernBERT-large Ettin-Enc-400m DeBERTa-v2-XL DeBERTa-v2-XXL Ettin-Enc-1b 33.9 22.4 43.9 44.8 59.1 57. 56.9 61.9 64.8 59.0 65.1 66.9 56.2 71.4 71.3 75.3 71.6 74.4 XXS Models (11-25M parameters) 88.3 91.2 91. 83.8 87.5 86.0 86.4 87.8 87.2 89.3 89.4 89.8 XS Models (28-33M parameters) 90.1 93.3 92.0 83.1 91.2 89. 87.6 89.2 89.5 90.1 91.5 91.0 Models (68-82M parameters) 92.2 93.1 94.4 86.8 89.0 92.2 87.4 88.9 91. 90.8 91.5 91.9 77.2 80.9 79.5 79.2 85.6 83.4 82.7 84.7 87.0 Base Models (86-150M parameters) 93.1 96.0 95. 89.5 92.2 92.6 89.4 91.8 92.2 91.4 92.1 92.4 85.4 89.1 89.2 Large Models (305-395M parameters) 93.3 97.1 96. 87.8 91.7 93.6 90.6 92.8 92.7 90.9 92.7 93.0 86.3 90.8 91.3 XL Models (750-1565M parameters) 97.1 - 97. 91.7 - 94.4 92.5 - 93.2 92.6 - 93.0 91.7 91.2 91.8 85.4 88.4 87.3 88.2 91.9 90. 89.5 91.7 92.9 91.6 93.9 94.0 92.8 95.2 95.2 95.9 96.0 96.0 67.1 68.2 69.0 69.3 74.7 74. 66.1 69.7 83.8 78.2 87.4 87.7 83.8 92.1 92.8 89.2 - 93.1 76.4 77.0 79.2 79.0 84.6 83. 81.5 83.8 87.2 84.7 88.4 88.9 85.2 90.4 90.8 90.7 - 91.6 Table 6: GLUE benchmark results across model sizes and architectures. DeBERTa v2 XXL was run for 300+ GPU hours before ruling it out due to its large size (> 1.5B), results are incomplete. Model Name ARC HS LMB OBQA PIQA SIQA SciQ TQA WG WSC Avg Ettin-Enc-from-Dec-17m Ettin-Enc-17m Ettin-Dec-From-Enc-17m Ettin-Dec-17m Ettin-Enc-from-Dec-32m Ettin-Enc-32m Ettin-Dec-From-Enc-32M Ettin-Dec-32m Ettin-Enc-from-Dec-68m Ettin-Enc-68m Ettin-Dec-from-Enc-68m Ettin-Dec-68m Ettin-Enc-from-Dec-150m Ettin-Enc-150m Ettin-Dec-from-Enc-150m Ettin-Dec-150m 27.7 28.3 22.7 21.3 28.2 28.7 20.5 23.5 30.0 29.5 24.8 25.3 33.5 32.5 25.0 28.6 39.7 Ettin-Enc-from-Dec-400m Ettin-Enc-400m 35.6 EttinDec-from-Enc-400m 29.9 33.6 Ettin-Dec-400m Ettin-Enc-from-Dec-1B Ettin-Enc-1B Ettin-Dec-from-Enc-1B Ettin-Dec-1B 42.4 37.3 32.5 39.7 XXS Models (17M parameters) 23.4 24.1 21.9 23.0 31.4 34.0 24.6 27.2 56.0 54.2 56.1 57. 34.6 34.4 70.9 71.1 XS Models (32M parameters) 29.5 33.6 27.7 28.5 33.8 34.8 27.0 28.2 55.6 56.7 58.1 57.7 34.7 34.4 77.2 77. Small Models (68M parameters) 31.8 36.1 35.8 35.2 33.6 35.4 29.4 29.4 57.1 58.4 60.7 61.8 36.1 35.6 84.6 83.2 27.2 26.4 26.8 27. 27.9 28.0 28.3 28.5 30.4 31.6 31.9 33.4 Base Models (150M parameters) 36.3 36.5 36.0 40.3 39.2 41.6 39.4 43.2 34.4 37.4 30.0 29. 63.9 63.0 62.9 66.6 39.7 38.5 84.7 89.6 Large Models (400M parameters) 47.7 46.8 45.8 54.3 53.0 52.3 52.5 62.9 44.9 50.5 46.4 52. 38.6 38.0 33.6 34.4 66.3 64.7 66.9 71.0 43.4 43.9 92.1 91.8 XL Models (1B parameters) 49.3 54.0 49.1 58.4 39.2 38.4 35.8 41. 70.0 67.6 69.9 74.4 46.3 46.3 93.1 93.8 45.9 44.0 35.7 35.4 45.6 41.4 36.0 36.4 55.3 49.6 38.3 38.8 74.3 59.8 40.4 40. 70.4 65.6 45.3 45.5 74.9 64.5 48.5 48.2 0.5 0.1 0.9 2.6 0.1 0.2 3.0 3.8 1.9 1.1 5.8 5.6 4.7 1.6 7.4 11. 7.7 6.4 13.3 18.3 14.9 7.6 13.1 29.3 51.2 52.6 53.4 50.9 53.2 51.4 50.2 53.1 51.1 51.3 53.1 50.1 51.5 54.9 52.9 53. 56.4 59.7 53.9 57.6 62.3 63.2 58.6 62.7 52.7 52.7 53.8 48.0 50.9 56.4 52.7 50.2 52.7 62.6 56.0 55.3 59.3 63.0 57.5 59. 68.9 70.7 63.7 71.8 73.3 75.8 69.2 79.1 35.1 35.1 36.7 36.4 36.0 36.6 38.1 38.7 38.0 39.1 42.1 41.8 43.7 42.9 43.6 46. 48.4 48.2 49.1 53.1 52.5 50.7 52.2 59.0 Table 7: Performance comparison of all models evaluated on generative tasks. Enc-from-Dec are trained with MTNP from decoders, while Dec-from-Enc are encoders trained with CLM. 18 Model Name Retrieval (nDCG@10) MNLI (Accuracy) Generative Avg XXS Models (17M parameters) Ettin-Enc-17m Ettin-Dec-17m Ettin-Enc-from-Dec-17m Ettin-Dec-from-Enc-17m Ettin-Enc-32m Ettin-Dec-32m Ettin-Enc-from-Dec-32m Ettin-Dec-from-Enc-32m Ettin-Enc-68m Ettin-Dec-68m Ettin-Enc-from-Dec-68m Ettin-Dec-from-Enc-68m 30.93 29.11 31.01 28.52 XS Models (32M parameters) 35.13 32.93 34.66 32.32 79.5 77.6 77.7 78.8 83.4 80.4 80.9 82.6 Small Models (66-70M parameters) 38.17 36.12 37.87 36.31 87.0 83.9 83.9 85. Medium Models (150M parameters) Ettin-Enc-150m Ettin-Dec-150m Ettin-Enc-from-Dec-150m Ettin-Dec-from-Enc-150m 39.97 37.71 39.49 37.55 89.2 85.6 85.8 86.8 Large Models (400M parameters) Ettin-Enc-400m Ettin-Dec-400m Ettin-Enc-from-Dec-400m Ettin-Dec-from-Enc-400m 42.24 39.93 41.44 39.69 XL Models (1B parameters) Ettin-Enc-1b Ettin-Dec-1b Ettin-Enc-from-Dec-1b Ettin-Dec-from-Enc-1b 43.35 41.70 43.24 40.77 91.3 88.2 87.6 89.4 91.8 89.9 89.0 90. 35.1 36.4 35.1 36.7 36.6 38.7 36.0 38.1 39.1 41.8 38.0 42.1 42.9 46.2 43.7 43.6 48.2 53.1 48.4 49.1 50.7 59.0 52.5 52. Table 8: Table version of Figure 1. The generative eval breakdowns can be found in Table 7. 19 Model Name Overall Female Male Neutral Overall Female Male WinoGender All WinoGender Gotcha XXS Models (17M parameters) Ettin-Enc-from-Dec-17m Ettin-Enc-17m Ettin-Dec-from-Enc-17m Ettin-Dec-17m 50.8 1.9 50.6 1.9 49.9 1.9 51.1 1.9 51.7 3.2 50.0 3.2 50.0 3.2 50.0 3.2 50.8 3.2 50.8 3.2 50.0 3.2 51.2 3.2 50.0 3.2 50.8 3.2 49.6 3.2 52.1 3. 50.4 3.2 50.0 3.2 49.6 3.2 49.2 3.2 45.8 4.6 46.7 4.6 47.5 4.6 45.0 4.6 55.0 4.6 53.3 4.6 51.7 4.6 53.3 4.6 XS Models (32M parameters) Ettin-Enc-from-Dec-32m Ettin-Enc-32m Ettin-Dec-from-Enc-32m Ettin-Dec-32m 53.6 1.9 53.2 1.9 51.1 1.9 50.8 1. 52.5 3.2 53.8 3.2 51.7 3.2 50.4 3.2 53.8 3.2 52.9 3.2 51.2 3.2 50.8 3.2 54.6 3.2 52.9 3.2 50.4 3.2 51.2 3.2 53.3 3.2 52.9 3.2 51.7 3.2 50.0 3.2 50.0 4.6 52.5 4.6 49.2 4.6 50.0 4.6 56.7 4.5 53.3 4.6 54.2 4.6 50.0 4. Small Models (66-70M parameters) Ettin-Enc-from-Dec-68m Ettin-Enc-68m Ettin-Dec-from-Enc-68m Ettin-Dec-68m 51.9 1.9 56.1 1.9 51.8 1.9 54.2 1.9 52.5 3.2 55.8 3.2 51.7 3.2 55.0 3.2 51.7 3.2 56.7 3.2 52.1 3.2 53.8 3.2 51.7 3.2 55.8 3.2 51.7 3.2 53.8 3. 51.2 3.2 56.7 3.2 50.8 3.2 52.9 3.2 53.3 4.6 56.7 4.5 51.7 4.6 56.7 4.5 49.2 4.6 56.7 4.5 50.0 4.6 49.2 4.6 Medium Models (150M parameters) Ettin-Enc-from-Dec-150m Ettin-Enc-150m EttinDec-from-Enc-150m Ettin-Dec-150m 52.8 1.9 57.5 1.8 53.3 1.9 54.7 1. 50.8 3.2 57.1 3.2 52.9 3.2 53.3 3.2 54.2 3.2 57.9 3.2 52.9 3.2 55.8 3.2 53.3 3.2 57.5 3.2 54.2 3.2 55.0 3.2 52.1 3.2 57.5 3.2 52.1 3.2 52.9 3.2 47.5 4.6 55.8 4.6 55.8 4.6 56.7 4.5 56.7 4.5 59.2 4.5 48.3 4.6 49.2 4. Large Models (400M parameters) Ettin-Enc-from-Dec-400m Ettin-Enc-400m Ettin-Dec-from-Enc-400m Ettin-Dec-400m 55.1 1.9 70.3 1.7 54.0 1.9 55.3 1.9 55.4 3.2 68.8 3.0 53.8 3.2 54.6 3.2 54.6 3.2 70.8 2.9 55.0 3.2 55.8 3.2 55.4 3.2 71.2 2.9 53.3 3.2 55.4 3. 55.4 3.2 69.2 3.0 52.5 3.2 52.9 3.2 53.3 4.6 69.2 4.2 55.8 4.6 51.7 4.6 57.5 4.5 69.2 4.2 49.2 4.6 54.2 4.6 XL Models (1B parameters) Ettin-Enc-from-Dec-1B Ettin-Enc-1B Ettin-Dec-from-Enc-1B Ettin-Dec-1B 57.9 1.8 68.2 1.7 55.8 1.9 56.7 1. 56.7 3.2 67.1 3.0 55.8 3.2 56.7 3.2 58.3 3.2 66.2 3.1 56.7 3.2 55.4 3.2 58.8 3.2 71.2 2.9 55.0 3.2 57.9 3.2 54.2 3.2 65.8 3.1 54.2 3.2 52.1 3.2 48.3 4.6 65.8 4.3 54.2 4.6 50.8 4.6 60.0 4.5 65.8 4.3 54.2 4.6 53.3 4. Table 9: WinoGender accuracy results (values: Accuracy % Std Error %). Results taken from the Eleuther AI harness. Many of the small models do not get above random performance (50%). 20 Figure 3: Full gender pronoun predictions results on the Gotcha split of WinoGender (Rudinger et al., 2018), 50/50 stereotypical split. We see that encoder models are more likely to use gender neutral pronouns whereas both are biased towards male pronouns. 21 Model Name Total Params Embed Params Non-Embed Params Pythia-14m BERT Tiny TinyBERT Ettin-17m BERT Small Ettin-32m MiniLM L12 XXS Models (7-17M parameters) 7.6M 11.2M 14.4M 16.8M 6.4M 7.9M 9.7M 12.9M XS Models (28-33M parameters) 28.8M 31.9M 33.4M 15.9M 19.3M 11.9M Small Models (68-82M parameters) DistilBERT Base Ettin-68m DistilGPT2 DistilRoBERTa Base 66.4M 68.1M 81.9M 82.1M 23.8M 25.8M 39.4M 39.0M Base Models (123-150M parameters) BERT-base Pythia-160m SmolLM2-135m ModernBERT-base Ettin-150m BERT-large Pythia-410m SmolLM2-360m ModernBERT-large Ettin-400m 109.5M 123.7M 134.5M 149.0M 149.0M 23.8M 38.6M 28.3M 38.7M 38.7M Large Models (353-395M parameters) 335.1M 353.8M 361.8M 394.8M 394.8M 31.8M 51.5M 47.2M 51.6M 51.6M XL Models (884M-1.2B parameters) DeBERTa v2 XLarge Pythia 1B Ettin-1B OLMo 1B 0724 Llama 3.2 1B 884.6M 908.8M 1028.1M 1176.8M 1235.8M 197.6M 103.0M 90.3M 103.0M 262.7M 1.2M 3.2M 4.7M 3.9M 12.9M 12.5M 21.4M 42.5M 42.4M 42.5M 43.1M 85.6M 85.1M 106.2M 110.3M 110.3M 303.4M 302.3M 314.6M 343.2M 343.2M 687.0M 805.7M 937.8M 1073.7M 973.1M Table 10: Parameter breakdown of language models organized by size categories. Models are grouped by total parameter count and show the distribution between embedding and non-embedding parameters across different architectures. Parameter counts are the same for Ettin encoders, decoders, and cross-objective trained versions. 22 Parameter"
        },
        {
            "title": "Vocabulary Size\nMax Sequence Length\nTokenizer\nAttention Layer\nAttention Dropout\nAttention Output Bias\nAttention Output Dropout\nAttention QKV Bias\nTransformer Layer\nEmbedding Dropout\nEmbedding Norm\nFinal Norm\nSkip First PreNorm\nEmbedding Layer\nMLP Dropout\nMLP Input Bias\nMLP Layer Type\nMLP Output Bias\nNormalization\nNorm Epsilon\nNorm Bias\nHidden Activation\nHead Pred Activation\nActivation Function\nPadding\nRotary Embedding Base\nRotary Embedding Interleaved\nAllow Embedding Resizing\nSliding Window\nGlobal Attention Every N Layers\nUnpad Embeddings",
            "content": "Value 50,368 1024->7999 ModernBERT RoPE 0.0 false 0.1 false prenorm 0.0 true true true sans_pos 0.0 false GLU false LayerNorm 1e-12 false GELU GELU GELU unpadded 160,000.0 false true 128 3 true Table 11: Common pre-training configuration parameters across all models"
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "LightOn"
    ]
}