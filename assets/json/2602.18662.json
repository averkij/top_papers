{
    "paper_title": "Large Causal Models for Temporal Causal Discovery",
    "authors": [
        "Nikolaos Kougioulis",
        "Nikolaos Gkorgkolis",
        "MingXue Wang",
        "Bora Caglayan",
        "Dario Simionato",
        "Andrea Tonon",
        "Ioannis Tsamardinos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Causal discovery for both cross-sectional and temporal data has traditionally followed a dataset-specific paradigm, where a new model is fitted for each individual dataset. Such an approach limits the potential of multi-dataset pretraining. The concept of large causal models (LCMs) envisions a class of pre-trained neural architectures specifically designed for temporal causal discovery. Prior approaches are constrained to small variable counts, degrade with larger inputs, and rely heavily on synthetic data, limiting generalization. We propose a principled framework for LCMs, combining diverse synthetic generators with realistic time-series datasets, allowing learning at scale. Extensive experiments on synthetic, semi-synthetic and realistic benchmarks show that LCMs scale effectively to higher variable counts and deeper architectures while maintaining strong performance. Trained models achieve competitive or superior accuracy compared to classical and neural baselines, particularly in out-of-distribution settings, while enabling fast, single-pass inference. Results demonstrate LCMs as a promising foundation-model paradigm for temporal causal discovery. Experiments and model weights are available at https://github.com/kougioulis/LCM-paper/."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 2 ] . [ 1 2 6 6 8 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Large Causal Models for Temporal Causal\nDiscovery",
            "content": "Nikolaos Kougioulis1,2 ((cid:0)), Nikolaos Gkorgkolis1,2, MingXue Wang3, Bora Caglayan3, Dario Simionato3, Andrea Tonon3, and Ioannis Tsamardinos1,2 1 Institute of Applied & Computational Mathematics, FORTH 2 Computer Science Department, University of Crete nikolaos.kougioulis@iacm.forth.gr {gkorgkolis, tsamard}@csd.uoc.gr 3 Huawei Ireland Research Centre, Dublin, Ireland {wangmingxue1, bora.caglayan}@huawei.com dario.simionato1@h-partners.com andrea.tonon1@huawei-partners.com Abstract. Causal discovery for both cross-sectional and temporal data has traditionally followed dataset-specific paradigm, where new model is fitted for each individual dataset. Such an approach limits the potential of multi-dataset pretraining. The concept of large causal models (LCMs) envisions class of pre-trained neural architectures specifically designed for temporal causal discovery. Prior approaches are constrained to small variable counts, degrade with larger inputs, and rely heavily on synthetic data, limiting generalization. We propose principled framework for LCMs, combining diverse synthetic generators with realistic time-series datasets, allowing learning at scale. Extensive experiments on synthetic, semi-synthetic and realistic benchmarks show that LCMs scale effectively to higher variable counts and deeper architectures while maintaining strong performance. Trained models achieve competitive or superior accuracy compared to classical and neural baselines, particularly in out-of-distribution settings, while enabling fast, single-pass inference. Results demonstrate LCMs as promising foundation-model paradigm for temporal causal discovery. Experiments and model weights are available at github.com/kougioulis/LCM-paper/. Keywords: Causal Discovery, Foundation Models, Large Causal Models, TimeSeries"
        },
        {
            "title": "Introduction",
            "content": "Causal discovery (CD) aims to recover the causal structure of data [40, 47]. In many domains, data are inherently temporal, motivating causal discovery from multivariate time series [4345]. Classical temporal CD methods provide strong identifiability guarantees under explicit assumptions [43, 47], yet rely on conditional independence testing 2 N. Kougioulis et al. or combinatorial search, whose computational and statistical complexity scales poorly with input dimensionality. Methods assuming known functional forms (e.g., linearity) [26, 38] degrade when assumptions are violated. Neural methods have been introduced that attempt to amortize CD, include variational approaches [1, 19, 34], autoregressive and attention-based models [28], and adversarial-based frameworks [21,22]. While promising, most remain dataset-specific and struggle to generalize across heterogeneous systems, especially in zero-shot settings. In parallel, foundation models (FMs) [3] have demonstrated that large neural architectures trained on diverse data can generalize effectively across tasks [13, 29,32,33,42,52], thus motivating Large Causal Models (LCMs): pre-trained FMs that can leverage multiple datasets and perform efficient, fast temporal CD. Prior work [48] remains proof-of-concept, limited to low dimensions and narrow synthetic training distributions. Importantly, key obstacle to scaling large causal models is the scarcity of large, diverse time-series datasets with ground-truth causal graphs, with prior work relying on synthetic generators [2], which limits generalization. In this work, we demonstrate that LCMs can be made scalable and robust through data-centric training on large and heterogeneous corpus, enabling reliable zero-shot generalization and systematic evaluation across diverse temporal settings. Our contributions can be summarized as follows: 1. We formalize Large Causal Models (LCMs) as FMs for temporal CD across heterogeneous data distributions, replacing dataset-specific algorithms with foundation model-style pretraining, 2. We demonstrate that scaling failures in prior work stem from insufficient training distribution diversity, 3. We curate large-scale collection of time-series samples and causal graph pairs suitable for both training of LCMs and benchmarking CD methods, 4. We demonstrate superior or competitive performance compared to classical CD methods, with substantially reduced runtime via single-pass CD. Temporal Structural Causal Models (TSCMs). Structural Causal Model (SCM) represents each endogenous variable as function of its causal parents and an ext ), , . . . , ogenous noise term [40,41,47]. For multivariate time series Vt = (V 1 + ϵj Temporal SCM (TSCM) is defined as [43] Z, , where Pa(V ) denotes the set of lagged causal parents drawn from the past ℓmax time steps, Pa(V ) {Vt1, . . . , Vtℓmax}. Here, ℓmax is fixed maximum causal lag. We assume additive noise models, causal stationarity, no latent confounders, and no contemporaneous effects (i.e. effects within lag ℓ = 0). Detailed definitions are provided in Appendix .1. := j(cid:16) Pa(V ) (cid:17) Lagged Causal Graphs. Temporal causal relationships in TSCM are represented by lagged causal graph (window graph) [2], DAG whose nodes correspond to time-indexed variables and whose directed edges are of the form for tℓ Large Causal Models for Temporal Causal Discovery 3 Lag ℓ = A(1) 1 2 3 1 0 0 0 2 0 0 1 3 0 0 0 Lag ℓ = 1 A(0) 1 2 1 0 1 0 2 0 0 0 3 0 0 0 (b) Binary lagged adjacency tensor slices Entry A(ℓ1) = 1 represents tℓ j,i 1 t2 1 t1 f2 2 t2 2 t1 V 3 t2 3 t1 1 2 V 3 (a) Lagged causal graph Fig. 1. Temporal causal dependencies represented as (a) lagged causal graph and (b) binary adjacency tensor. Each slice A(ℓ1) encodes edges at discrete lag ℓ ℓmax, where entry A(ℓ1) tℓ . j,i = 1 denotes ℓ {1, . . . , ℓmax}. Edges are oriented forward in time and denote direct causal effects, while indirect effects are encoded via directed paths. Adjacency Tensor Representation. For variables and maximum lag ℓmax, the lagged causal graph is equivalently represented by binary adjacency tensor {0, 1}V ℓmax, where A(ℓ) [20, 48](Figure 1). j,i = 1 indicates directed edge tℓ t"
        },
        {
            "title": "2 Related Work",
            "content": "Despite advances in deep learning, few works explore pre-trained models for Causality. Most approaches remain task or dataset specific and do not exhibit the broad generalization associated with foundation models. In causal inference, [54] propose CInA, transformer-based framework for estimating average treatment effects (ATEs) [41] from observational data, which targets treatment effect estimation rather than causal discovery. Neural approaches to CD include attention-based models for static graph prediction under i.i.d. assumptions [53], supervised sequence formulations in atemporal setups [28] and amortized variational methods under Granger causality assumptions [35]. These methods typically operate on limited synthetic datasets, output summary graphs and do not demonstrate large-scale zero-shot generalization, particularly in temporal or heterogeneous settings. The closest work is [48], which formulates CD as multi-class classification problem, and introducing various architectural backbones, notably transformers [50,55]. Although conceptually aligned with our notion for LCMs, it remains 4 N. Kougioulis et al. Fig. 2. Overview of the large causal model (LCM) pipeline. (1) Synthetic and realistic TSCM generators produce training pairs of multivariate time series and their lagged causal graphs. (2) The LCM is trained via supervised learning on these pairs to discover lagged adjacency tensor ˆA for time series RLV , padded and normalized for stability. (3) At inference (CD phase), the pre-trained LCM predicts causal strengths on unseen datasets in zero-shot manner. proof of concept; limited to small systems, training relies on narrow synthetic distributions, and performance degrading sharply with increasing dimensionality [48, Figure 3]. Positioning of Our Work. We study Large Causal Models as robust, scalable FMs for temporal CD. Unlike prior work, we emphasize data diversity as prerequisite for scaling, train on large heterogeneous corpus, and evaluate zero-shot and out-of-distribution generalization across diverse benchmarks. This positions LCMs as practical alternatives to existing CD methods."
        },
        {
            "title": "3 Problem Formulation",
            "content": "We study temporal CD across heterogeneous datasets and underlying causal mechanisms. The task is to construct Large Causal Model (LCM), i.e. an FM that infers lagged causal graphs from multivariate time series, while generalizing across domains, lengths, and causal mechanisms. Let = {Xt}L t=1 RLV denote multivariate time series with variables. Each dataset = is generated by TSCM G, PD, where is the (ground truth) lagged adjacency tensor of G. The task is to infer from via parafθ ˆA, where fθ is neural model and ˆA RVmaxVmaxℓmax metric mapping encodes the discovered lagged causal graph (Figure 1). Entry ˆAj,i,ℓmaxℓ represents the confidence in tℓ . Probabilistically, the LCM learns an amortized approximation of the conditional distributions P(Aj,i,ℓmaxℓ = 1X) under the training distribution over Large Causal Models for Temporal Causal Discovery 5 Fig. 3. multivariate time series is embedded via Conv1D layers and positional encodings, processed through Transformer encoder stack with optional distillation blocks, and augmented with lagged cross-correlations (training aids). feedforward head outputs lagged adjacency tensor representing the discovered temporal causal graph. the TSCMs. Rather than performing dataset-specific DAG search, the model learns parametric approximation to the mapping from time-series samples to causal graphs under the training distribution. The discovered lagged adjacency tensor hence represents edge probabilities under the implicit prior induced by the training corpus. Model Hyperparameters. Lmax (max time-series length) and Vmax (maximum input variables) are model-specific, akin to token limits in language models [8], while ℓmax (max assumed lag) follows standard assumptions in temporal CD [43]."
        },
        {
            "title": "4 Model Overview",
            "content": "The implemented LCM, illustrated in Figure 3, follows convolution-enhanced Transformer encoder [48, 55]: Input Embeddings. Input samples RLV are normalized and noise-padded (Appendix .2) to RLmaxVmax, projected via Conv1D to extract local temporal features at dimension dmodel, with positional encodings added element-wise [50]. Encoder Stack. layers of multi-head self-attention and feedforward networks with residual connections produce contextualized representations. An optional Conv1D-based distillation layer aims to reduces sequence length for long-horizon processing. Training Aids. Lagged cross-correlations are concatenated to the final encoder representations to enhance inductive bias (correlation injection [48]). 6 N. Kougioulis et al. Feedforward Head. fully-connected network, followed by sigmoid activation outputs ˆA [0, 1]VmaxVmaxℓmax, where Aj,i,ℓ is the probability of tℓ . The framework is encoder-agnostic; we adopt Transformers as scalable, well-understood backbone for multivariate time series [13, 32, 33] rather than as novel contribution; details are provided in Appendix .4."
        },
        {
            "title": "4.1 Loss Function",
            "content": "Supervised training of LCMs requires an informative objective that encourages accurate edge predictions while leveraging any observed statistical dependencies. composite loss is employed, combining supervised edge prediction with correlation-based regularizing term. Edge Prediction Loss. Let ˆA RVmaxVmaxℓmax denote the discovered lagged tℓ adjacency tensor and the ground truth. Each potential edge is treated as binary classification problem, optimized via binary cross-entropy: Ledge ="
        },
        {
            "title": "1\nV 2\nmaxℓmax",
            "content": "(cid:88) i,j,ℓ"
        },
        {
            "title": "BCE",
            "content": "(cid:16) Aj,i,ℓ, ˆAj,i,ℓ (cid:17) Regularization term. As observed by [48], causal discovery FMs for time-series can benefit from the inclusion of observed statistics. To improve stability and generalization, an auxiliary term is added, aligning predictions with lagged crosscorrelations observed in input time-series. For = {Xt}n t=1, correlations up to lag ℓmax are computed and normalized (we denote this tensor CC). The correlation loss corresponds to weighted mean squared error: Lcorr = E(cid:2)( ˆA CC)2 γ(cid:3) where γ > 1 emphasizes strong correlations. This term encourages confiCC dent causal predictions to align with empirical temporal dependencies without imposing hard constraints. Although [48] adopt different regularizing term, we adopt their terminology correlation regularization, CR for clarity. Overall Objective. The final loss combines the two components: = λedge Ledge+ λcorr Lcorr, where λedge = 1 and λcorr = 3/4 are selected by default."
        },
        {
            "title": "5.1 Datasets",
            "content": "LCMs require paired samples (D, G) for supervised training. To promote zeroshot generalization under distribution shift, we construct large, diverse corpus combining synthetic, semi-synthetic, and realistic time-series datasets. Large Causal Models for Temporal Causal Discovery 7 Synthetic. We generate random TSCMs by sampling graphs from parametrized random families (e.g., ErdősRényi [7,17,23]) and performing ancestral sampling [36, Section 4.2.5], dynamically varying the number of variables, ℓmax, graph density, and functional mechanisms (linear and nonlinear additive noise models [27]). Our primary large-scale collection (Synthetic_2) contains 230k instances; CDML [31] and Synthetic_1 [48] provide additional synthetic evaluation. Details in Appendix .3. Semi-synthetic. For OOD evaluation, we use mechanistic simulators with known causal connectivities: MRI-based dynamic nonlinear models (f MRI_5, MRI) [9,35,46] and Kuramoto coupled oscillator systems (Kuramoto_5, Kuramoto_10) [30, 35]. These are never used for training and serve exclusively as OOD benchmarks. Realistic. We incorporate realistic TSCMs derived from real-world time series following [20], to the best of our knowledge the only method constructing calibrated causal twins from observed, real data. Input datasets span multiple domains (energy, weather, transportation) [24], yielding 45k instances. Additional realistic datasets are further reserved for OOD evaluation (Appendix .3). Mixture Collections. To analyze synthetic-to-realistic ratio effects, we construct four mixture datasets (MIX_1MIX_4) with proportions from 100/0 to 20/80, each containing 50k instances with identical temporal characteristics. Large-Scale Training Corpus. Our main corpus (LS) combines synthetic and realistic data, totaling 275k instances (137.5M time points). All evaluations use holdout test sets for in-distribution scenarios and separate OOD benchmarks. Beyond LCM training, these corpora constitute substantial benchmark resource for systematic evaluation of existing and future temporal CD methods."
        },
        {
            "title": "6 Experimental Setup",
            "content": "Dataset configurations are described in Section 5.1. Regarding LCMs, we consider multiple variants with increasing model capacity, from 900K to 24M parameters. Larger models are used to study scalability in variable count and data size, while smaller models are employed for ablation studies and comparisons (Subsections 7.1, 7.2). The selection of such parameter counts is motivated by established time-series forecasting FMs, such as MOIRAI [51], Timer [33], and TimesFM [13]. Configurations, implementation and training details are provided in Appendix .7."
        },
        {
            "title": "6.1 Evaluation Metrics",
            "content": "Edge discovery performance is evaluated by the Area Under the ROC Curve (AUC) [4], computed directly from the predicted lagged adjacency tensor [11,20]. 8 N. Kougioulis et al. Edges are ranked by confidence, and the area under the TPRFPR curve is computed across thresholds. Higher AUC scores indicate better recovery of the ground-truth lagged causal graph. Standard errors are estimated across datasets within each collection. Statistical significance of AUC differences between LCMs and baseline methods is asserted using the Wilcoxon signed-rank test [12], non-parametric paired test appropriate for potentially non-Gaussian AUC distributions, under Bonferroni correction (αcorrected = α/k) where is the number of performed comparisons and α the level of significance."
        },
        {
            "title": "6.2 Baselines",
            "content": "We compare LCMs against established temporal CD methods: PCMCI [44], DYNOTEARS [38] and VARLinGAM [26], representing constraint-based, score-based, and functional-model approaches respectively. For small datasets (V = 5, ℓmax = 3), we additionally include the pre-trained Transformer from [48] ( 1.4M parameters). All baselines use official implementations. Outputs are converted to lagged adjacency tensors for fair comparison. Regarding constraint-based methods, edge confidence corresponds to inverse p-values. For methods producing causal effect estimates, edge probabilities are estimated via bootstrap resampling (Appendix .8 & Algorithm 1)."
        },
        {
            "title": "7 Experimental Results",
            "content": "We present representative results in each of the following subsections. Our experimental evaluation spans broad range of settings, including both in-distribution (holdout, out-of-sample test sets) and out-of-distribution scenarios. Across these settings, we show that LCMs consistently match or outperform classical baselines, which require fitting new model for each individual dataset. This further highlights the effectiveness of LCMs for efficient CD."
        },
        {
            "title": "7.1 Observed Statistics Improve LCM Performance",
            "content": "We study the impact of incorporating observed statistics into LCM training through set of training aids, as defined in Section 4. We evaluate baseline LCM, LCM + CI, and LCM + CI + CR following grid search procedure [25] over λcorr {1/4, 1/2, 3/4, 1}, keeping all other hyperparameters fixed. Experiments are conducted on the Synthetic_1 holdout set. Using these training aids substantially improves performance. CI alone yields significant improvement over the baseline, and adding CR further increases AUC across all tested weights. The highest mean AUC is achieved for λcorr [0.25, 0.75], with no statistically significant difference among them. We adopt λcorr = 0.75 for all subsequent experiments. In the remainder of this paper, we refer to the combination of CI and CR collectively as training aids. Large Causal Models for Temporal Causal Discovery 9 Table 1. Ablation of training aids on Synthetic_1. Asterisks (*) indicate statistical significance over the preceding model (left column). LCM +CI +CR (λ) .25 .5 .75 1.0 AUC .868 .914* .926* .918* .926 . Table 2. Out-of-distribution performance of LCMs trained on varying synthetic/realistic mixtures, evaluated on semi-synthetic MRI benchmarks and the OOD Power & Climate benchmarks. Statistical significance versus the 80/20% reference (bold) is indicated by an asterisk (*). Synth/Sim% fMRI_5 fMRI Power Climate 100/0% 80/20% 50/50% 20/80% .924* .050 .966 .028 .951 .050 .968 .004 .874* .142 .960 .028 .944* .966* .047 .036 .966* .011 .981 .006 .979 .008 .975* . .938* .035 .982 .011 .988 .010 .986 ."
        },
        {
            "title": "7.2 LCMs benefit from training on mixtures of synthetic and",
            "content": "realistic data Training LCMs solely on synthetic data limits generalization beyond narrowly defined causal structures. To mitigate this, we adopt dataset mixing approach, combining synthetic with realistic datasets during training (Subsection 5.1). Table 2 shows OOD CD performance on semi-synthetic MRI benchmarks and the real-dataderived Power and Climate collections. Models trained solely on synthetic cases are consistently outperformed by mixed-dataset models. Adding modest fraction of realistic data (20%) yields systematic, statistically significant performance gains. Higher proportions provide marginal or inconsistent improvements. Thus, an 80/20 synthetic-to-realistic mixture balances structural diversity and real-world fidelity, consistent with recent time-series FM findings [13], and is adopted for all subsequent large-scale experiments."
        },
        {
            "title": "7.3 Rich Training Data Enable Scalable, Large-Scale LCMs",
            "content": "Prior work suggests that scaling LCMs to higher-dimensional settings leads to near-random performance, even with increased model capacity [48, Figure 3], implying the need for prohibitively deep models. Our results indicate that these scaling failures are inherent to limitations of the training data distribution rather than architectural constraints. By training on rich datasets, LCMs can scale beyond previously studied small systems (e.g., 35 variables), demonstrating stable convergence and generalization up to Vmax = 12 in our experiments. 10 N. Kougioulis et al. Performance on Synthetic Data Table 3 shows that LCMs achieve state-ofthe-art performance on synthetic benchmarks, outperforming all baselines. We attribute their lower performance to their restrictive assumptions, as our models benefit from exposure to diverse large-scale training corpus. Table 3. Causal discovery performance (AUC) of large-scale LCMs and baseline methods across synthetic benchmarks. Results are reported as mean standard deviation across datasets. An asterisk indicates statistically significant difference with respect to the best-performing model (LCM-12.2M), which achieves the highest mean AUC across all semi-synthetic collections. Model Synthetic_1 Synthetic_2 CDML LCM2.5M LCM9.4M LCM12.2M LCM24M CP (Stein et al.) PCMCI DYNO VARLiNGAM .995* .013 .995* .013 .996 .013 .990* .012 .943* .151 .671* .444 .551* .216 .922* .145 .903* .107 .906* .106 .909 .103 .865* .133 .800 ** .181 .605* .152 .861* . .772 .177 .768 .171 .773 .172 .727* .183 .521* .108 490* .087 .517* .099 Out-of-distribution / Zero-shot Performance We evaluate zero-shot transfer on semi-synthetic and real-world datasets (Subsection 5.1), unseen during training, to assess generalization under distribution shifts, in Tables 4 & 5 respectively. An asterisk indicates statistically significant difference with respect to the best-performing model (LCM24M for Table 4, LCM2.5M for Table 5), achieving the highest mean AUC across most collections. Results demonstrate that all model sizes achieve superior or competitive zeroshot performance compared to baseline methods. This highlights that richer, diverse training data enable strong generalization, confirming that scaling failures stem from insufficient training distributions rather than architectural limitations."
        },
        {
            "title": "7.4 Running Times",
            "content": "We compare computational runtimes of LCMs and conventional CD algorithms (Section 6.2) to highlight scalability advantages. For an LCM, this corresponds to single forward pass, effectively replacing iterative search of classic CD methods with parametric approximation. Each method is run 10 times per dataset, and results are averaged (Figure 4). key observation is that LCM runtimes remain effectively independent of input dimensionality, as computations operate on learned representations rather Large Causal Models for Temporal Causal Discovery 11 Table 4. Out-of-distribution (zero-shot) causal discovery performance (AUC) of largescale LCMs and baseline methods across semi-synthetic benchmarks. Results are reported as mean standard deviation across datasets. Model fMRI_5 fMRI Kuramoto_5 Kuramoto_10 LCM2.5M LCM9.4M LCM12.2M LCM24M CP (Stein et al.) PCMCI DYNO VARLiNGAM .981 .017 .983 .007 .986 .007 .987 .006 .883* .078 .746* .111 .528* .152 .793* .095 .978 .017 .979 .012 .982 .012 .984 .010 .768* .110 .483* .159 .781* . .956* .017 .972 .009 .953* .016 .973 .007 .541* .108 .476* .103 .498* .077 .479* .105 .919* .018 .924* .008 .921* .014 .931 .010 .635* .046 .519* .027 .647* .053 Table 5. Out-of-distribution (zero-shot) causal discovery performance (AUC) of largescale LCMs and baseline methods across realistic benchmarks. Results are mean std. across datasets. AQ = AirQualityMS, Clim = Climate, Gar = Garments, ETT = ETTm2. Model LCM2.5M LCM9.4M LCM12.2M LCM24M AQ Clim Gar ETT Gear Pow .985.014 .982.013 .977 .970 .018 .020 .989.007 .983.012 .987.010 .982.014 .995.004 .996.004 .996.003 .995.006 .977.007 .965.024 .975.010 .972.053 .998.007 .998.005 .999.003 .996. .980 .007 .983.005 .983 .979 .005 .008 .565 PCMCI .706 DYNO VARLiNGAM .539 .209 . .146 .927.096 .708 .938 .118 .075 .972 .536 .984 . .051 .020 .898 .566 .906 .035 .037 . .980.040 .693 .995 .083 .009 .966.028 .551 .941 .053 . than fitting new model per input dataset. In contrast, traditional methods exhibit superlinear scaling with variable count and temporal lag, possibly limiting applicability in real-time, high-dimensional settings."
        },
        {
            "title": "8 Model Complexity",
            "content": "We derive closed-form expression for the number of trainable parameters of our LCM models. Let denote the number of encoder blocks, dmodel the model dimension, nheads the number of attention heads, and dff the feedforward dimension. We introduce the binary indicators Itrain, Idistil {0, 1} for the use of our correlation-based training aids and attention distillation respectively. The kernel size of the convolutional embedding is denoted by k. The total number of trainable parameters decomposes into terms for (i) input token embeddings, (ii) encoder blocks, (iii) (B 1) distillation layers when enabled, and (iv) the final causal prediction head. Summing the above, 12 N. Kougioulis et al. Fig. 4. Running times (in seconds) for LCMs and baseline algorithms on the Synthetic_2 holdout set, averaged over 10 runs. Traditional methods (e.g., PCMCI & DYNOTEARS) scale superlinearly with lag and variable count, while Transformerbased LCMs remain effectively independent of input dimensionality due to their constant-time forward pass. Ptotal = (dmodelVmax + dmodel) + B(cid:0)4 d2 + Idistil(B 1)(cid:0)d2 + (cid:0)(dmodel + Itrain A)dff + dff model + 2dff dmodel + dff + 9 dmodel model + 3 dmodel (cid:1) (cid:1) + (dff + A). (cid:1) where = 2 maxℓmax and dmodel is assumed to be divisible by nheads. Dominant term grows as Θ(V 2 maxℓmax), i.e., pairwise causal edges in the feedforward head. For Vmax = 25, ℓmax = 3 with 24M hyperparameters, the head alone contributes 2M parameters, total 75M. Our considered input dimensionality (Vmax = 12, ℓmax = 3) represents careful balance between scalability, generalization, and applicability, with higher-dimensional extensions possible via time-series selection [49]."
        },
        {
            "title": "9 Training Data–Model Size Convergence",
            "content": "We study the interaction between model capacity and training data size by training 500K, 1M, and 2M parameter LCMs on subsampled datasets of sizes {10K, 25K, 50K, 100K} from our large-scale corpus (five random seeds; fixed validation/test sets of 2 103 samples). As shown in Figure 5, smaller models saturate early with diminishing returns from additional data, while larger models continue to improve and increasingly outperform them at scale. Results confirm that LCM performance is jointly constrained by model capacity and data availability: scaling either alone yields limited gains, but scaling both together leads to consistent improvements. We do Large Causal Models for Temporal Causal Discovery 13 Fig. 5. Empirical convergence of LCMs with increasing training data. Test AUC for 500K, 1M, and 2M parameter models trained on subsampled datasets. Validation/test sets are fixed to isolate the effect of data scale. not claim universal scaling law; rather, these findings inform practical design choices for large-scale temporal CD."
        },
        {
            "title": "10 Concluding Remarks & Future Work",
            "content": "Large Causal Models (LCMs) are foundation models for temporal causal discovery that amortize CD under supervised scheme, enabling fast, single-pass graph prediction without any optimization. Trained on mixed data collections, LCMs exhibit strong inand out-of-distribution generalization, consistently matching or outperforming established baselines. Future work includes relaxing causal sufficiency to handle latent confounders, contemporaneous effects, and scaling to higher-dimensional systems and longer horizons."
        },
        {
            "title": "11 Limitations",
            "content": "LCMs operate under the causal assumptions defined in Section 1 and do not detect or correct violations. When assumptions are violated, predictions may reflect training distribution biases rather than true causal structure, and should be interpreted accordingly."
        },
        {
            "title": "Impact Statement",
            "content": "LCMs enable scalable, zero-shot temporal causal discovery without datasetspecific retraining, potentially accelerating scientific discovery in fields such as biology and marketing by reducing reliance on costly randomized control trials & A/B testing. 14 N. Kougioulis et al."
        },
        {
            "title": "References",
            "content": "1. Ashman, M., Ma, C., Hilmkil, A., Jennings, J., Zhang, C.: Causal reasoning in the presence of latent confounders via neural ADMG learning. arXiv preprint arXiv:2303.12703 (2023) 2. Assaad, C.K., Devijver, E., Gaussier, E.: Survey and evaluation of causal discovery methods for time series. Journal of Artificial Intelligence Research 73, 767819 (2022) 3. Bommasani, R.: On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021) 4. Bradley, A.P.: The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognition 30(7), 11451159 (1997) 5. Breiman, L.: Random forests. Machine Learning 45(1), 532 (2001) 6. Brockwell, P.J., Davis, R.A.: Time series: Theory and Methods. Springer Science & Business Media (1991) 7. Brouillard, P., Lachapelle, S., Lacoste, A., Lacoste-Julien, S., Drouin, A.: Differentiable causal discovery from interventional data. Advances in Neural Information Processing Systems (2020) 8. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in Neural Information Processing Systems 33, 18771901 (2020) 9. Buxton, R.B., Wong, E.C., Frank, L.R.: Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic Resonance in Medicine 39(6) (1998) 10. Chen, T., Guestrin, C.: Xgboost: scalable tree boosting system. In: Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. pp. 785794 (2016) 11. Cheng, Y., Wang, Z., Xiao, T., Zhong, Q., Suo, J., He, K.: Causaltime: Realistically generated time-series for benchmarking of causal discovery. International Conference on Learning Representations (2024) 12. Conover, W.J.: Practical Nonparametric Statistics. Wiley & Sons (1999) 13. Das, A., Kong, W., Sen, R., Zhou, Y.: decoder-only foundation model for timeseries forecasting. In: International Conference on Machine Learning. vol. 41 (2024) 14. Dickey, D.A., Fuller, W.A.: Distribution of the estimators for autoregressive timeseries with unit root. Journal of the American statistical association 74(366a), 427431 (1979) 15. Dinh, L., Sohl-Dickstein, J., Bengio, S.: Density estimation using real nvp. In: International Conference on Learning Representations (2017) 16. Durkan, C., Bekasov, A., Murray, I., Papamakarios, G.: Neural spline flows. Advances in neural information processing systems 32 (2019) 17. Fienberg, S.E.: brief history of statistical models for network analysis and open challenges. Journal of Computational and Graphical Statistics 21(4), 825839 (2012) 18. Fix, E.: Discriminatory analysis: nonparametric discrimination, consistency properties, vol. 1. USAF school of Aviation Medicine (1985) 19. Geffner, T., Antoran, J., Foster, A., Gong, W., Ma, C., Kiciman, E., Sharma, A., Lamb, A., Kukla, M., Pawlowski, N., et al.: Deep end-to-end causal inference. Transactions of Machine Learning Research (2024) 20. Gkorgkolis, N., Kougioulis, N., Wang, M., Caglayan, B., Tonon, A., Simionato, D., Tsamardinos, I.: Temporal causal-based simulation for realistic time-series generation. arXiv preprint arXiv:2506.02084 (2025) Large Causal Models for Temporal Causal Discovery 21. Goudet, O., Kalainathan, D., Caillou, P., Guyon, I., Lopez-Paz, D., Sebag, M.: Causal generative neural networks. arXiv preprint arXiv:1711.08936 (2017) 22. Goudet, O., Kalainathan, D., Caillou, P., Guyon, I., Lopez-Paz, D., Sebag, M.: Learning functional causal models with generative neural networks. Explainable and Interpretable Models in Computer Vision and Machine Learning pp. 3980 (2018) 23. Hagberg, A., Swart, P.J., Schult, D.A.: Exploring network structure, dynamics, and function using networkx. Tech. rep., Los Alamos National Laboratory (LANL) (2007) 24. Hahn, Y., Langer, T., Meyes, R., Meisen, T.: Time series dataset survey for forecasting with deep learning. Forecasting 5(1), 315335 (2023) 25. Hutter, F., Kotthoff, L., Vanschoren, J.: Automated Machine Learning: Methods, Systems, Challenges. Springer Nature (2019) 26. Hyvärinen, A., Zhang, K., Shimizu, S., Hoyer, P.O.: Estimation of structural vector autoregression model using non-gaussianity. Journal of Machine Learning Research 11(5) (2010) 27. Kalainathan, D., Goudet, O., Guyon, I., Lopez-Paz, D., Sebag, M.: Structural agnostic modeling: Adversarial learning of causal graphs. Journal of Machine Learning Research 23(219), 162 (2022) 28. Ke, N.R., Chiappa, S., Wang, J.X., Bornschein, J., Goyal, A., Rey, M., Weber, T., Botvinick, M., Mozer, M.C., Rezende, D.J.: Learning to induce causal structure. In: International Conference on Learning Representations (2023) 29. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Neil, H., Sylvain, G., Unterthiner, T., Zhai, X.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2021) 30. Kuramoto, Y.: Self-entrainment of population of coupled non-linear oscillators. In: International Symposium on Mathematical Problems in Theoretical Physics: Kyoto University, Kyoto/Japan. Springer (1975) 31. Lawrence, A.R., Kaiser, M., Sampaio, R., Sipos, M.: Data generating process to evaluate causal discovery techniques for time series data. NeurIPS Causal Discovery & Causality-Inspired Machine Learning Workshop (2020) 32. Liu, Y., Qin, G., Huang, X., Wang, J., Long, M.: Timer-xl: Long-context transformers for unified time series forecasting. arXiv preprint arXiv:2410.04803 (2024) 33. Liu, Y., Zhang, H., Li, C., Huang, X., Wang, J., Long, M.: Timer: Generative pretrained transformers are large time series models. arXiv preprint arXiv:2402.02368 (2024) 34. Lorch, L., Sussex, S., Rothfuss, J., Krause, A., Schölkopf, B.: Amortized inference for causal structure learning. Advances in Neural Information Processing Systems 35, 1310413118 (2022) 35. Löwe, S., Madras, D., Zemel, R., Welling, M.: Amortized causal discovery: Learning to infer causal graphs from time-series data. In: Conference on Causal Learning and Reasoning. pp. 509525. PMLR (2022) 36. Murphy, K.P.: Probabilistic Machine Learning: Advanced Topics. MIT Press (2023) 37. Nauta, M., Bucur, D., Seifert, C.: Causal discovery with attention-based convolutional neural networks. Machine Learning and Knowledge Extraction 1(1), 19 (2019) 38. Pamfil, R., Sriwattanaworachai, N., Desai, S., Pilgerstorfer, P., Georgatzis, K., Beaumont, P., Aragam, B.: DYNOTEARS: Structure learning from time-series data. In: International Conference on Artificial Intelligence and Statistics. pp. 15951605. Proceedings of Machine Learning Research (2020) 16 N. Kougioulis et al. 39. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems 32 (2019) 40. Pearl, J.: Causality. Cambridge University Press, 2nd edn. (2009) 41. Peters, J., Janzing, D., Schölkopf, B.: Elements of causal inference: foundations and learning algorithms. The MIT Press (2017) 42. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training. OpenAI (2018) 43. Runge, J.: Causal network reconstruction from time series: From theoretical assumptions to practical estimation. Chaos: An Interdisciplinary Journal of Nonlinear Science 28(7) (2018) 44. Runge, J., Bathiany, S., Bollt, E., Camps-Valls, G., Coumou, D., Deyle, E., Glymour, C., Kretschmer, M., Mahecha, M.D., Muñoz-Marí, J., et al.: Inferring causation from time series in earth system sciences. Nature communications 10(1), 2553 (2019) 45. Runge, J., Gerhardus, A., Varando, G., Eyring, V., Camps-Valls, G.: Causal inference for time series. Nature Reviews Earth & Environment 4(7), 487505 (2023) 46. Smith, S.M., Miller, K.L., Salimi-Khorshidi, G., Webster, M., Beckmann, C.F., Nichols, T.E., Ramsey, J.D., Woolrich, M.W.: Network modelling methods for fMRI. Neuroimage 54(2), 875891 (2011) 47. Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction, and Search. MIT Press (2001) 48. Stein, G., Shadaydeh, M., Denzler, J.: Embracing the black box: Heading towards foundation models for causal discovery from time series data. AAAI Workshop (AI4TS) (2024) 49. Vareille, E., Linardi, M., Tsamardinos, I., Christophides, V.: Chronoepilogi: Scalable time series selection with multiple solutions. Advances in Neural Information Processing Systems 37, 134287134316 (2024) 50. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in Neural Information Processing Systems 30 (2017) 51. Woo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., Sahoo, D.: Unified training of universal time series forecasting transformers. PMLR (2024) 52. Wu, D., He, Y., Cao, Y., Fan, J., Liu, H.: Transformers and their roles as time series foundation models. arXiv preprint arXiv:2502.03383 (2025) 53. Wu, M., Bao, Y., Barzilay, R., Jaakkola, T.: Sample, estimate, aggregate: recipe for causal discovery foundation models. In: ICLR Workshop on Machine Learning for Genomics Explorations (2024) 54. Zhang, J., Jennings, J., Hilmkil, A., Pawlowski, N., Zhang, C., Ma, C.: Towards causal foundation model: On duality between causal inference and attention. arXiv preprint arXiv:2310.00809 (2023) 55. Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., Zhang, W.: Informer: Beyond efficient transformer for long sequence time-series forecasting. In: Proceedings of AAAI. pp. 1110611115 (2021) Large Causal Models for Temporal Causal Discovery 17 .1 Causal Assumptions & Brief Definitions All Causal Discovery algorithms are governed by set of assumptions, regarding the statistical properties of the data and the underlined structure of the causal model. The following assumptions are defined in way to be applied to either (standard) causal graph or temporal causal graph. Definition 1 (Causal Markov Condition, [40,47]). Let be causal graph with vertex set and probability distribution over V, generated by the causal structure induced by G. Then and satisfy the Markov Condition if and only if V, is independent of its non-descendants on the causal DAG (non causal effects) given its parents (direct causes) on G. Definition 2 (Faithfulness, [40, 47]). Let be causal graph and probability distribution over V. We say that and are faithful to each other if and only if the all and only the independence relations of are entailed by the Causal Markov condition of G. Specifically, and satisfy the Faithfulness Condition if-f every conditional independence relation true in is entailed by the Causal Markov Condition applied to G. Definition 3 (Causal Sufficiency, [40, 47]). set of variables is causally sufficient for population if and only if in the population every common cause of any two or more variables in is in , or has the same value for all units in the population. The common cause of two or more variables in DAG is called confounder of and . Hence Causal Sufficiency implies no unobserved confounders. The notion of causal sufficiency is being used without explicitly mentioning the population. Definition 4 (Causal Stationarity, [43]). Consider the SCM description for time from Section 1. If the causal relationships between variables tτ , i lag τ > 0 also hold for all time-shifted versions , the described process is causally stationary. Informally, the graph structure and noise distribution of the SCM are time-invariant. tτ , i (cid:16) (cid:17) (cid:16) (cid:17) Definition 5 (Time-series Stationarity, [6]). Let {Xt}tZ be time-series with index set = {0, 1, 2, . . .}. The series is said to be stationary if: 1. EXt2 < , 2. EXt = m, 3. γX (r, s) = γX (r + t, + t), t r, s, where γX (r, s) = Cov(Xr, Xs) = [(Xr EXr)(Xs EXs)] , r, is the autocovariance function of the stochastic process {Xt}tZ. We now give definitions on used terminology for our causal graphs. 18 N. Kougioulis et al. Definition 6 (Lagged Causal Graph - Window Causal Graph). Lagged Causal Graph is directed acyclic graph (DAG) that represents the causal relationships between time-shifted variables in multivariate time-series. Formally, let Vt = {V 1 } denote the set of observed variables at time step t. tτ lagged causal graph contains directed edges of the form , where indicates that τ {1, 2, . . . , ℓmax} is positive time lag. An edge is direct cause of with time lag of τ . tτ , . . . , , 2 Definition 7 (Summary Causal Graph). Given lagged causal graph with edges of the form for various lags τ , the corresponding summary graph contains an edge j if there exists at least one lag τ such that tτ is direct cause of i tτ in G. Essentially, summary graph is simplified representation of the lagged causal structure in time-series, where the temporal information about lags is abstracted away. The summary graph captures whether causal relationship exists between two variables, but it does not specify the lag or delay at which the causal influence occurs. ."
        },
        {
            "title": "Input Handling and Padding Strategies",
            "content": "W/o Training Aids W/ Training Aids dmodel dmodel + 2 2 max ℓmax Input dimensions Hidden state Fig. 6. Hidden state representations with and without auxiliary training aids. In order to handle variable-length time-series, variable numbers of time-series and differing maximum lags across datasets, we adopt set of standardized padding strategies, both for the input time-series and the ground truth lagged adjacency tensor. This enables consistent batch processing and generalization of trained models across heterogeneous dataset configurations, as the model is able to handle an input of fixed dimensions, as in conventional foundation models. Time-Series Padding. When the number of observed time-steps (samples) is less than the configured maximum number of samples Lmax, the time-series is padded with Gaussian noise (0, 0.01) along the time-step dimension. This prevents introducing zero artifacts, preserving the marginal statistics, and makes the model robust to handling varying sequence lengths. When inputs are longer than the configured maximum (either in time-steps or the number of timeseries dmax), the excess is truncated to fit the maximum dimensions. This is conceptually similar to how Foundation models like large language models - LLMs truncate input lengths that exceed the maximum allowed tokens [8]. Large Causal Models for Temporal Causal Discovery Causal Graph Padding. similar procedure is carried out for padding of the lagged causal graph. When dataset has fewer variables than Vmax, or the lagged causal graph assumes lower maximum lag < ℓmax, the lagged adjacency tensor is zero-padded alongside the unused variable and lag dimensions. Such padding explicitly encodes the absence of nodes or time dependencies outside the observed configuration. Such step, together with time-series padding described previously, is crucial for both training of the LCMs, as well as during evaluation against the ground truth causal graph, as it allows handling of time-series and causal graph pairs of variable time-step, node and lag dimensions. An illustration of the above is shown in Figure .2. Input Concatenation. Implemented LCMs, unless specified otherwise, concatenate lagged crosscorrelations (training aids), which are flattened along the batch dimension to the last hidden state (as illustrated in Figure 3 of the main text). We depict the above in Figure 6. Observed Time-Series (V < Vmax) X1 X2 X3 X1 t+1 X2 t+ X3 t+1 N N Lagged Adjacency Tensor (V < Vmax) W W W 0 0 0 0 0 0 0 0 0 0 0 0 0 Gaussian Noise Padding Zero Padding Fig. 7. Illustration of the padding strategies in LCMs. Left: Gaussian noise padding for time-series where < Vmax and < Lmax. Right: Zero padding of lagged adjacency tensors when < Vmax or ℓ < ℓmax. Red dashed boxes indicate padded regions. denotes (0, 0.01) for brevity. .3 Datasets Synthetic In line with the synthetic temporal data generators provided in Tigramite [45], we implement temporal SCM-based time-series generator to produce synthetic datasets for training and validating our LCMs. The generator produces both observational samples by simulating temporal structural equation models with additive noise, as defined in Section 1. Synthetic data allow us to evaluate and compare algorithms under controlled conditions while ensuring reproducibility and flexibility. The data generation process consists of several steps. Initially, lagged temporal causal graph is constructed by sampling random directed acyclic graph (DAG) following the Erdős-Rényi scheme [7]. Similar to [31], our implementation 20 N. Kougioulis et al. offers extensive configurability, including the number of variables and samples, the minimum and maximum lags ℓmin and ℓmax, as well as the graph density. To model variety of realistic causal mechanisms, we incorporate diverse set of (equiprobable) functional dependencies in the SCM, including both linear and nonlinear relationships, and additive Gaussian noise; examples are summarized in Table 7. Observational samples are generated via ancestral sampling, similar to the procedure used in latent variable models [36, Section 4.2.5], by traversing the topological order of the lagged causal graph. The overall sampling procedure is summarized in Algorithm 2. For ensuring stability of the generated time-series samples, we employ warm-up period as in [45], discarding an initial set of samples before collecting data. Table 6. Synthetic time-series dataset collections. = linear, NL = non-linear functional relationships. Collection Pairs Variables Timesteps ℓmax Func. Rels. Synthetic_1 Synthetic_2 CDML 100k 270k 240 35 312 411 500 500 3 3 3 L, NL L, NL L, NL An important consideration in synthetic data generation is the stationarity of the resulting time-series. The generator in Tigramite addresses this by performing eigenvalue tests on the stability matrix and aborting the generation if instability is detected, without adjusting the parameters. In contrast, [48] handle instability by repeatedly resampling until stationarity is achieved. Both approaches, however, become impractical when generating large, high-dimensional datasets needed for training our LCMs. We mitigate this issue by wrapping the outcomes of unbounded functional dependencies with bounded functions, such as the hyperbolic tangent tanh or the sigmoid σ. This strategy caps the growth of unbounded functions, ensuring stationarity without altering the underlying causal relationships, and thereby improves the stability and scalability of our synthetic data generation while preserving causal structure. Semi-synthetic Semi-synthetic datasets combine analytically defined structural equations with generative processes and experimental knowledge (e.g., Markov processes of physical systems). These datasets simulate complex systems where the underlying causal graph is known, allowing for controlled benchmarking under quasi-realistic dynamics. An overview is provided in Table 8. The MRI collection comprises 27 datasets with variable counts of 5, 10, and 15, and observation lengths ranging from 50 to 5000, simulating BOLD (Blood-Oxygen-Level Dependent) responses from different regions of interest in the brain. The simulation process follows non-linear balloon model [9], translating neural activity into hemodynamic responses, and is designed to mimic Large Causal Models for Temporal Causal Discovery Table 7. Functional dependencies used in synthetic SCM generation. Category Example Function Description Linear (x) = ax + Additive linear effect (x1, x2) = a1x1 + a2x2 + Linear combination of parents Non-linear (x) = αnxn + . . . α0x0 (x) = ex (x) = sin(x) (x1, x2) = x1x Polynomial Exponential Transformation Sinusoidal Multiplicative interaction (x) = log(1 + x) Logarithmic transformation Bounded (x) = tanh(x) (x) = σ(x) Hyperbolic tangent (bounded in [1, 1]) Sigmoid (cid:0) (cid:1) 1 1+ex Table 8. Overview of semi-synthetic time-series dataset collections. Collection Datasets Vars Timesteps Max Lag Rels. MRI5 MRI Kuramoto_5 Kuramoto_10 21 26 1000 5 12005000 510 12005000 5 500 500 1 1 1 Non-linear Non-linear Non-linear Non-linear realistic neural connectivity patterns based on known structural priors. Due to our model input limitations, the 15-variable case is discarded. Realistic To generate realistic (simulated) time-series datasets paired with ground-truth temporal causal graphs, we employ the methodology of [20]. TCS takes as input real multivariate time-series data, learns Temporal Structural Causal Model (TSCM) consisting of directed causal graph, functional mechanisms, and noise models, and synthesizes arbitrary numbers of time-series samples from the learned causal model. This procedure produces fully supervised datasetgraph pairs suitable for training LCMs. For any of these input time-series, we verify for stationarity using the Augmented Dickey-Fuller test [14] (ADF). In case non-stationarity is observed, it is normalized using finite differences up to second order. This step assures that data fed into training of LCMs do not possess extreme trends, possibly resulting in unstable training. We follow the python implementation of statsmodels with maximum lag order 12 (cid:0) where is the number of observed timesteps. (cid:1)1/4 100 22 N. Kougioulis et al. Table 9. Overview of real time-series datasets used for generating realistic data pairs, using the methodology by [20]. Collection Variables Timesteps Domain WTH ETTh1 ETTm AirQualityUCI Bike-usage Outdoors ETTm2 Climate Garments power-consumption Gearbox-fault 12 7 7 5 3 7 4 7 3 AirQualityMS 36 35064 17420 9357 52584 1440 69680 1463 52416 2000 4937 Weather Power Power Weather Transportation Environmental Power Environmental Manufacturing Power Automotive Weather Used configurations & algorithms We implement the same methods for CD, functional dependency and noise estimation for the generation of simulated data as provided by the official implementation of [20] 4, apart from Random Forest Regressors. We instead use kNN Regressors [18] and Gradient Boosting [10] for improved computational efficiency of generating large amounts of data pairs. An overview is shown in Table 10. Apart from the generation of different time-series samples and causal graph data pairs generated from different configurations of algorithms in Phases 1-3 of TCS, we apply two augmentation strategies before processing them through the TCS pipeline to increase the number of simulated datasets: i) time window sub-sampling and ii) node subsets sub-sampling. Time Window Sub-sampling Given multivariate time-series of minimum length = 400, we extract multiple non-overlapping windows of fixed length = 200. Each window is treated as an independent dataset and passed through the full TCS pipeline (causal discovery, function approximation, and noise modeling), resulting in distinct simulated time-series and corresponding lagged causal graph. Node sub-sampling For each time window, we additionally generate subsets of variables of size {3, 5, 7}. Each subset is independently simulated via TCS, with their total count dependent on the dimension of the original dataset. The 4 https://github.com/gkorgkolis/TCS Large Causal Models for Temporal Causal Discovery 23 Table 10. Implemented Methods for Temporal Functional Dependency Estimation, Noise Density Estimation in Temporal Causal Discovery. Method PCMCI [43] Causal Discovery Description Constraint-Based Algorithm DYNOTEARS [38] Constraint Optimization Algorithm CP [48] Pre-trained Transformer Functional Dependencies Random Forest Regressor [5] Ensemble Learning Regressor kNN Regressors [18] Non-parametric Supervised Method XGBoost [10] Gradient Boosting Tree Algorithm AD-DSTCN [37] Time-series Forecasting Model (TCDF module) TimesFM [13] Foundational Time-series Forecaster Noise Density Normal Distribution Fitted Parametric Distribution Uniform Distribution Fitted Parametric Distribution Neural Splines [16] Normalizing Flows RealNVP [15] Normalizing Flows above strategies increase variability of training pairs by exposing the model to smaller-scale graphs. The final collections for training and evaluation are provided in Table 11. .4 Architecture of LCMs This section provides detailed description of the implemented architecture [48, 55] used throughout experiments, consistent with the formulation presented in the main text. Input Handling & Embeddings Input Embeddings. The model receives an observational multivariate time series RBLV , where denotes the batch dimension, the number of timesteps, and the number of variables. During inference, = 1. All inputs are minmax normalized per variable. To allow training across datasets with heterogeneous dimensionality and sequence length, inputs are padded (or truncated) to fixed 24 N. Kougioulis et al. Table 11. Overview of realistically generated time-series collections for training and OOD evaluation of LCMs. Collections in bold notate inclusion in the SIM training corpus and the large scale, LS corpus. Collection Variables ℓmax Num. Datasets WTH ETTh1 ETTm 3-12 3-7 3-7 AirQualityUCI 3-12 Bike-usage Outdoors ETTm2 Climate Garments Power Gearbox-fault 3-5 3 3-7 3,4 3-7 33 AirQualityMS 3-12 3 3 3 3 3 3 3 3 3 3 20200 9000 3900 1800 800 15 17 18 20 54 upper bounds Lmax = 500 and Vmax = 12, as described in Subsection .4. Temporal padding is performed using i.i.d. Gaussian noise (0, 0.01). .5 MinMax Normalization To ensure consistent scaling across heterogeneous datasets and to stabilize optimization, each variable is minmax normalized prior to being processed. This transformation is applied during both training and inference. For variable xv , the normalized value xv is defined as xv = min(xv) xv max(xv) min(xv) + ϵ , (1) where ϵ > 0 ensures numerical stability. After minmax normalization, sequences are padded along both temporal and variable dimensions to fixed sizes Lmax and Vmax with sequences longer than Lmax truncated, as in Appendix .2. Temporal Convolutional Embedding. The normalized and padded input is then projected into latent representation using one-dimensional convolution operating along the temporal dimension. After permuting the tensor to channel-first cout = dmodel, format [B, Vmax, Lmax], convolution is applied with cin = Vmax, where dmodel denotes the model dimension. The resulting embeddings are Xtok = Conv1D(X) RBLmaxdmodel. Large Causal Models for Temporal Causal Discovery 25 This convolutional projection serves as learnable feature extractor capturing local temporal dependencies prior to global attention. It replaces the purely linear token embedding of the vanilla Transformer, following the Informer design. Positional Encoding. Because self-attention is permutation-invariant with respect to temporal order, explicit positional information is injected using sinusoidal encodings [50]: (cid:18) PE(t,2i) = sin PE(t,2i+1) = cos (cid:18) 100002i/dmodel 100002i/dmodel (cid:19) (cid:19) , . (2) The final embedding representation is thus Xemb = Xtok + PE, followed by dropout for regularization. .6 Transformer Encoder The embedding representation Xemb is processed by stack of Informer-style encoder layers. For layer l: E(l) = LayerNorm E(l+1) = LayerNorm (cid:16) (cid:16) E(l) + SelfAttn(E(l)) (cid:17) E(l) + FFN(E(l)) . (cid:17) , Where SelfAttn() denotes scaled multi-head self-attention and FFN(x) = Conv1D (cid:0)GELU(cid:0)Conv1D1(x)(cid:1)(cid:1) (3) (4) is two-layer convolutional feed-forward network applied position-wise. Residual connections and layer normalization are also applied after both sublayers to ensure stable propagation of gradients. Temporal Distillation. Between encoder blocks, self-attention distillation layer is inserted (included in all trained models). It performs convolutional downsampling followed by normalization, activation, and max pooling: Xdistil = MaxPool (cid:16) ELU(cid:0)BatchNorm(Conv1D(E(l)))(cid:1)(cid:17) . (5)"
        },
        {
            "title": "This reduces the temporal dimension and lowers computational cost while",
            "content": "preserving dominant temporal patterns. Let the final encoder output be Eenc RBLdmodel (L Lmax) where only the final timestep embedding is retained: = Eenc[:, 1, :] RBdmodel. This design relies on the global receptive field of self-attention: the final token representation can attend to all preceding timesteps and therefore serves as global summary of the sequence. 26 N. Kougioulis et al. Auxiliary Correlation Injection To assist the model in inferring directed dependencies, empirical Pearson cross-correlations are computed for each ordered variable pair (X i, j) and lag τ {1, . . . , ℓmax}, which are flattened and concatenated to the learned neural representations of the final encoder block. ρX i,X (τ ) = (cid:113) Cov(X τ +1:T ) 1:T τ , 1:T τ ) Var(X τ +1:T ) . Var(X These values form tensor CC RBVmaxVmaxℓmax, which is flattened and concatenated with the encoder representation: = Concat(cid:0)Flatten( X), Flatten( CC)(cid:1). This correlation injection acts as statistical training aid, aligning learned representations with empirically observable lagged dependencies. Feedforward Head and Output Interpretation The concatenated representation is passed through two-layer feedforward head with layer normalization and SwiGLU activation: = SwiGLU(cid:0)LN(W1Z + b1)(cid:1), ˆAflat = W2H + b2. After reshaping and sigmoid activation for the transformation of logits, the lagged adjacency tensor corresponding to the discovered lagged causal graph is ˆA = σ (cid:16) (cid:17) Reshape( ˆAflat) [0, 1]BVmaxVmaxℓmax Each entry ˆAj,i,τ represents the predicted probability that directed lagged edge exists. Since multiple edges and lags may simultaneously be active, sigmoid activation is used instead of softmax, thus framing temporal causal discovery as multi-label classification problem. tτ Limitations. The flattened output head does not enforce permutation equivariance by design. Future directions can explore equivariant neural models that meet this desiderata by design, while retaining expressivity. .7 Model Sizes & Training Setup Table 12 summarizes the implemented models and their configurations used in Section 7. All models are implemented in Python 3.10 using PyTorch 2.2 [39]. Training uses an effective batch size of 64, with gradient accumulation when necessary to accommodate larger models. Learning rate scheduling and early stopping based on validation AUC are employed to ensure stable convergence. Training was conducted on dedicated workstation equipped with NVIDIA RTX 4090 GPU (32 GB VRAM) and 128 GB physical memory. Training of large-scale models took around two weeks to complete. Causal Discovery (LCM Inference step) is performed on an AMD Ryzen 5 CPU. Large Causal Models for Temporal Causal Discovery Table 12. Model configurations and parameter sizes for the LCM variants used in the ablations and additional results in Section 7. Model #Params Enc. Layers dmodel Attn. Heads dff Usage - - LCM-2.5M LCM-9.4M 905K-914K 1M 2.5M 9.4M LCM-12.2M 12.2M LCM-24M 24M 1 1 4 4 4 256 256 256 512 512 2 2 4 4 4 128 256 256 512 512 Subsection 7. Subsection 7.2 Subsection 7.3 Subsection 7.3 Subsection 7.3 1024 Subsection 7. Algorithm 1 Bootstrap-based Estimation of Edge Probabilities Require: Time-series dataset D, maximum lag ℓmax, number of bootstraps n, causal discovery algorithm configuration BCD Ensure: Soft adjacency matrix [0, 1]V ℓmax 1: Initialize accumulator 0V ℓmax 2: for = 1 to do 3: {Bootstrap Sampling Phase} Draw bootstrap sample D(b) from by resampling with replacement 4: 5: {Causal Discovery Phase} 6: A(b) causalAlg(D(b), BCD) 7: 8: 9: 10: end for 11: {Normalization Phase} 12: S/n 13: return {Edge Confidence Update} + (A(b)) {f extracts edge indicators or confidence scores} .8 Model Evaluation Methodology To ensure fair and consistent evaluation across all causal discovery methods, we compute performance metrics based on each models native output format while adapting our evaluation pipeline accordingly. For models that produce realvalued edge confidences (our LCMs and CausalPretraining models), the values are used verbatim as continuous scores for computing the Area Under the ROC Curve (AUC) and the (binarized) ground-truth adjacency labels. For constraintbased methods like PCMCI, we use the inverse of the edge p-values as confidence scores. For models that output an adjacency matrix of causal effect coefficients (like VARLiNGAM and DYNOTEARS), results are not directly comparable to our LCMs that output soft adjacency probabilities and computing AUCs is not directly applicable. For fair comparison, we follow bootstrap-based procedure to estimate the probability of each causal edge, as shown in the official 28 N. Kougioulis et al. documentation5. Specifically, we run VARLiNGAM = 10 times with resampled datasets and compute the proportion of times each edge is discovered, resulting in soft adjacency matrix of edge probabilities. We follow the same procedure for DYNOTEARS and the same number of boostrapped datasets, as in Algorithm 1. .9 Pseudocodes Algorithm 2 Temporal SCM Ancestral Sampling (ANCESTRAL) Input: Temporal Causal Graph of time-series {Xt}ℓmax t=1 , causal discovery algorithm configuration BCD, predictive model configuration Bpred, noise estimator configuration Bnoise, max lag ℓmax BCD, number of timesteps , number of warmup steps Output: Generated time-series sample {Xt}T Initialize // Forward Sampling for = + ℓmax + 1 to do t=0 for all {1, . . . , } with random noise t=1 for each variable in topological order based on do tk (X j, i) G, 1 ℓmax} ; Determine lagged parents PaXi // Lagged parents of at time Compute := fi(Pai t) + ϵi {X end end Return generated time-series dataset {Xi t}T +ℓmax Algorithm 2 describes the ancestral sampling procedure for obtaining multivariate time-series samples from known TSCM. Initial warming steps are discarded as way to ensure stability [43]. .10 Illustrative Example & Visualizations To provide an intuitive demonstration of LCMs, we construct synthetic temporal causal process consisting of three variables 1, 2 and V3. The example is governed by simple, linear temporal structural causal model: 1 := ϵ1(t), 2 := 3V 1 t1 + ϵ2 , 3 := 2 t2 + 5V 1 t3 + ϵ3 , where ϵi (0, 1) {1, 2, 3}, Z. The TSCM thus induces the following lagged causal relationships: 1 lag=1 2, 1 lag=3 3, 2 lag=2 3. 5 https://lingam.readthedocs.io/en/latest/tutorial/var.html Large Causal Models for Temporal Causal Discovery 29 Fig. 8. Heatmap visualization of the discovered lagged adjacency matrix (bottom row) compared to the ground truth (top row) in an example with nmax = 12 and ℓmax = 3 using the pretrained LCM-2.4M model. Brighter colors in the predicted adjacency indicate stronger confidence for edge existence. In Figure 8 we visualize the discovered causal graph of the final feedforward head against the ground truth; the pretrained LCM successfully captures the underlying causal structure of this simple temporal dataset, resulting in perfect AUC score of 1.00. .11 Statistical Significance of Results We evaluate statistical significance of AUC differences between LCMs and baseline methods using the Wilcoxon signed-rank test [12], non-parametric paired test appropriate for potentially non-Gaussian AUC distributions. To control the family-wise error rate across multiple pairwise comparisons, we apply Bonferroni correction (αcorrected = α/k) where is the number of performed comparisons. For each dataset collection, we report the median paired difference in AUC (AUC) relative to common reference model (the highest average AUC per collection). Figures 911 visualize the paired effects from Subsection 7.3. Asterisks denote statistically significant differences after correction. 30 N. Kougioulis et al. f a . t i s i fi c i t i v i x i d - k s t n r c e o . . 9 . r e n C ff n r t t e e c d c s n t o c n . i e u i i e r d Large Causal Models for Temporal Causal Discovery 31 i t S . a f d r e i s a i o . i l c h S - S r e r ff U n e r . 0 1 . . t r n e B w t a - g o l i e e d e c fi s 32 N. Kougioulis et al. d r e a c n n - k t h f o c e o . . 1 1 . r e n d ff n a s a t o c n . i e u n a i o p o n . t i s i fi c e"
        }
    ],
    "affiliations": [
        "Computer Science Department, University of Crete",
        "Huawei Ireland Research Centre, Dublin, Ireland",
        "Institute of Applied & Computational Mathematics, FORTH"
    ]
}