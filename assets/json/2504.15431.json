{
    "paper_title": "Trillion 7B Technical Report",
    "authors": [
        "Sungjun Han",
        "Juyoung Suk",
        "Suyeong An",
        "Hyungguk Kim",
        "Kyuseok Kim",
        "Wonsuk Yang",
        "Seungtaek Choi",
        "Jamin Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10\\% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7B's robust multilingual performance and exceptional cross-lingual consistency."
        },
        {
            "title": "Start",
            "content": "Trillion 7B Technical Report"
        },
        {
            "title": "Trillion Labs",
            "content": "research@trillionlabs.co"
        },
        {
            "title": "Abstract",
            "content": "We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours ($148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7Bs robust multilingual performance and exceptional cross-lingual consistency. Preview: Trillion-7B Trillion-LLaVA-7B 5 2 0 2 1 2 ] . [ 1 1 3 4 5 1 . 4 0 5 2 : r Figure 1 Trillion-7B significantly advances the Pareto-frontier across all aspects. Trillion 7B Technical Report"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Cross-lingual Document Attention (XLDA) 2.1 2.2 Strategic Batch-level Document Packing . . . . . . . . . . . . . . . . . . . . . . . . Selective Attention Masking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.3 Design Rationale for XLDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 Pre-training"
        },
        {
            "title": "3.1 Pre-training Data .",
            "content": ". . ."
        },
        {
            "title": "3.2 Two-stage Pretraining .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.3 Experiments on Scalable Training Recipe",
            "content": ". . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.4 Model Architecture .",
            "content": "3.5 Tokenizer . . . . . . . . . . ."
        },
        {
            "title": "3.6 Training Infrastructure .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3 4 5 6 6 6 7 9 9 3.7 Context Length Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.1 Benchmarks .",
            "content": ". . . 5.2 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6 Ablations",
            "content": "6.1 Data Composition and Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Impact of Data Diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Vocabulary Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7.1 Cross-lingual Consistency .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7.2 Generalization to Vision .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Full Evaluation Results and Details",
            "content": "2 10 11 11 11 12 13 13 14 14 15 28 28 29 Trillion 7B Technical Report 1. Introduction Recently, significant advancements have been made in developing massively multilingual large language models (LLMs) (DeepSeek-AI et al., 2025; Grattafiori et al., 2024; Qwen et al., 2025; Team et al., 2024; Touvron et al., 2023) expanding their coverage to hundreds of languages. However, substantial performance disparities still persist between high-resource languages like English and less resourced languages like Korean. The fundamental reason for this discrepancy is data scale. Despite being spoken by over 81 million people worldwide, Korean content represents less than 2% of English data volume in common web-crawled datasets like mC4 (Xue et al., 2021). This severe imbalance means that languages other than English and Chinese simply cannot follow the same scaling trajectory that has proven successful for extremely high-resource languages (Figure 3). Addressing this limitation is critical to developing AI systems beneficial to all of humanity, as achieving true artificial general intelligence (AGI) inherently necessitates robust performance across diverse linguistic contexts. In this technical report, we tackle the critical issue of data imbalance in multilingual training. We introduce Trillion-7B, Korean-targeted multilingual model, which relies on our novel Crosslingual Document Attention (XLDA) mechanism. XLDA functions as form of architectural code-switching, efficiently transferring linguistic knowledge from resource-rich language (like English) to target less-resourced language (like Korean) without degrading English capabilities. Combined with other innovative training methodologies, we achieve exceptional multilingual performance, especially in Korean, while using significantly less multilingual tokens than prominent Korean-dominant approaches (Kim et al., 2021; Ko et al., 2023; Research et al., 2024a,b; Yoo et al., 2024b). Our comprehensive description of Trillion-7B focuses on its multilingual pre-training and post-training, detailing the architecture, methodologies, and transfer strategies that enable robust performance across diverse benchmarks. Our main contributions are as follows: Cross-lingual Document Attention (XLDA): novel mechanism designed explicitly for efficient cross-lingual knowledge integration. Multilingual Token Efficiency Training: Achieving competitive multilingual performance while dedicating approximately 10% (< 220B tokens, with less than 180B in Korean) of the total 2T training tokens to multilingual data. Complementary Technical Innovations: Including optimized multilingual data mixtures, language-specific data filtering, customized tokenizer construction, and establishing empirical scaling laws to guide effective model scaling and training efficiency."
        },
        {
            "title": "Training Costs",
            "content": "Pre-Training Post-Training Total in H100 GPU Hours in USD 59K $147K 0.36K $896 59.4K $148K Table 1 Training costs of Trillion 7B on 2T tokens, given H100 price as $2.49 per GPU hour. 2. Cross-lingual Document Attention (XLDA) During language model pre-training, it is common to pack several short documents into single long sequence to maximize GPU utilization. Typically, training pipelines introduce 3 Trillion 7B Technical Report Figure 2 Cross-Lingual Document Attention. multilingual batch (left) is packed so that each sequence contains contiguous spans from at least two languages (e.g. English + Korean). The XLDA mask (centre) keeps full self-attention across language blocks (blue cells) while standard causal mask (right) blocks attention across document boundaries (grey cells). segmentation masks at each document boundary to prevent tokens from attending to tokens in previous documents, thus avoiding cross-attention contamination (Zhao et al., 2024a). However, in multilingual pre-training, this intra-document packing may unintentionally block beneficial cross-lingual correspondences that could naturally emerge when documents from different languages share the same context window. To address this limitation, we introduce novel technique called Cross-Lingual Document Attention (XLDA), specifically designed to manage cross-lingual interactions effectively and improve token-level training efficiency. 2.1. Strategic Batch-level Document Packing The first mechanism of XLDA focuses on how documents are packed into training batches. As illustrated in Figure 2 (left), strategic batch-level document packing ensures that each sequence contains documents from multiple language sourcesspecifically, contiguous spans from at least two languages (English + non-English). We employ controlled sampling strategy that combines documents from diverse languages at predetermined rate, creating opportunities for cross-lingual learning. This intentional interleaving of linguistic content creates rich training environment where the model can identify cross-lingual patterns and correspondences. Formally, let = {ð‘™1, ð‘™2, . . . , ð‘™ð‘š} be the set of ð‘š languages in our corpus. For each training sequence ð‘†, we construct it as: ð‘† = [ð‘‘ð‘™ð‘– 1 , ð‘‘ð‘™ ð‘— 2 , . . . , ð‘‘ð‘™ð‘› ð‘˜ ] (1) where ð‘‘ð‘™ð‘¥ ð‘¡ represents the ð‘¡-th document in language ð‘™ð‘¥. We enforce the constraint that for each sequence ð‘†, there exist at least two documents ð‘‘ð‘™ ð‘ ð‘— such that ð‘™ ð‘ ð‘™ð‘ž, with mixing probability ðœŒ that controls the likelihood of cross-lingual document adjacency. The sampling probability for constructing batch with language ð‘™ð‘– is given by: ð‘– and ð‘‘ð‘™ð‘ž ð‘ƒ(ð‘™ð‘–) = ð›¼ ð·ð‘™ð‘– (cid:205)ð‘š ð‘—=1 ð·ð‘™ ð‘— + (1 ð›¼) ð›½ð‘™ð‘– (2) where ð·ð‘™ð‘– is the corpus size for language ð‘™ð‘–, ð›¼ is temperature parameter that controls sam4 Trillion 7B Technical Report Figure 3 Discrepancy in scaling curves of Llama. The above plots suggest that brute-force scaling (by Llama 2 & 3) results in huge performance gaps between English and Korean, whereas Trillion-7B shows more desirable scaling laws for Korean performance closing the wide gap. pling smoothness, and ð›½ð‘™ð‘– is language-specific upsampling factor for low-resource languages. 2.2. Selective Attention Masking The second mechanism of XLDA introduces novel approach to attention masking. As depicted in Figure 2, the XLDA mask keeps full self-attention across language blocks which allows tokens from different language documents to attend to each other. This contrasts with the standard causal mask that blocks attention across document boundaries. The visualization in Figure 2 clearly illustrates how XLDA fundamentally differs from conventional masking approaches by permitting beneficial cross-document and cross-language attention flows. Note that this attention masking is effective because of the above enforced cross-lingual document packing. 2.3. Design Rationale for XLDA Cross-lingual In-context Pretraining. We draw on insights from recent studies that proper document sequence packing can significantly enhance knowledge integration between the sequences being trained together in-context (Levine et al., 2022). Through XLDA, our nonEnglish data is consistently meta-learned (Lampinen et al., 2024; Ortega et al., 2019) within linguistic context that includes English, thereby strengthening the models ability to generalize learned concepts across languages. Synthetic Code Switching Recent works on code-switching (Wang et al., 2025; Yoo et al., 2024a) demonstrates substantial improvements in language alignment when models are exposed to mixed-language contexts. XLDA builds on this insight by enabling cross-document attention between languages during pretraining, creating natural opportunities for cross-lingual knowledge transfer without explicit code-switching data. Trillion 7B Technical Report 3. Pre-training 3.1. Pre-training Data The pretraining corpus for Trillion comprises approximately 2T tokens spanning English, multilingual, mathematical, and coding domains. The token distribution follows an 8.5 : 1 : 0.5 ratio (English : Korean : Other languages/Math/Code), creating an extremely English-predominant data distribution. This drastic imbalance in pretraining distribution is made viable specifically because of our XLDA mechanism, which enables effective cross-lingual transfer despite the asymmetric data representation. Within the portion other than English and Korean, we include Japanese, Chinese, Code, Math, and additional multilingual data. This language mixture encourages the model to develop core language-agnostic representations primarily in English (Wendler et al., 2024; Zhao et al., 2024b), contrasting with an equally-balanced language distribution that are difficult to scale (Workshop et al., 2023; Yoo et al., 2024b) and can lead to negative interference between languages (Ye et al., 2023). We also emphasize substantial domain diversity within each language subset, driven by empirical observations indicating that data diversity significantly enhances model performance. This finding aligns with existing literature advocating for diverse pretraining data (Longpre et al., 2023). Filtering The scoring is performed using Qwen-72B-Instruct (Qwen et al., 2025), state-of-theart multilingual LLM that demonstrated strong correlation with GPT-4 (OpenAI et al., 2024) in our evaluations. Due to computational constraints, we restrict direct scoring by Qwen-72BInstruct to carefully sampled subset of one million documents per language. We then utilize portion of these scored documents to distill smaller, multilingual embedding model, reserving the remainder as held-out test set. By treating Qwens assessments as gold-standard labels and binarizing scores at threshold of 3, the distilled scoring model achieves an F1 score of 0.734 on our internal test set. We use this distilled model for quality scoring across all pretraining data. For English, we retain the top 80% of documents, while for other languages, we apply much stricter filtering threshold, keeping only the top 50% of multilingual data. 3.2. Two-stage Pretraining Trillion 7B is pretrained in two stages, using curriculum approach inspired by (Hu et al., 2024; HÃ¤gele et al., 2024; OLMo et al., 2025). We employ warmup-stable-decay (WSD) scheduler that initially warms up to high constant learning rate, followed by learning rate decay. We define the annealing phase as the point when the learning rate begins to decrease, and we adjust the data composition at this stage. During the annealing phase, we enhance overall data quality and modify the mixture composition. For English-language data, we further curate documents by selecting only those within the top 20% in terms of quality. For multilingual data, we apply an even stricter criterion, selecting only documents in the top 10% quality threshold. This heightened quality reduces gradient noise, facilitating more effective optimization and enabling the model to better consolidate knowledge as it navigates through the loss valley (Hu et al., 2024). Additionally, we significantly increase the proportion of multilingual data during the annealing phase, tripling its volume to further encourage cross-lingual knowledge transfer. We present ablative experiments on the effect of increasing quality and source distribution in Section 6.1. 6 Trillion 7B Technical Report Figure 4 Proxy model and emergence point. We trained 1.8B parameter models on approximately 100 billion tokens to serve as proxy models for determining optimal training configurations. This specific configuration, represented by red star in the figure, identifies the most FLOP-efficient setting at which downstream task improvements become observable. 3.3. Experiments on Scalable Training Recipe Efficiently validating training recipes and hyperparameters for large language models requires the strategic use of smaller-scale experiments. We employ hybrid strategy combining established empirical scaling laws (DeepSeek-AI et al., 2024; Hoffmann et al., 2022; Kaplan et al., 2020) with experimental evidence collected at smaller scales. Emergence and Proxy Model In selecting suitable configurations for small-scale experiments, we prioritize addressing the emergence phenomena observed in downstream tasks. These phenomena reflect non-linear improvements in downstream task performance after surpassing critical threshold in validation loss (Du et al., 2025). We observe that this emergent threshold is tightly coupled with the models emergence of in-context learning ability (Brown et al., 2020). Thus, we specifically curate benchmarks that emphasize few-shot learning along with multilingual validation set consisting of approximately 100,000 documents. We find that 1.8-billion-parameter model trained on approximately 100 billion tokens can serve as good proxy for determining optimal training configurations as it can be efficiently trained while still observing emergence in downstream tasks improvements. Figure 4 illustrates the effectiveness of our proxy model on the KoBEST benchmark (Jang et al., 2022). It can be observed that 0.5B scale does not provide good test-bed due to large variance, while the emergence point is easily surpassed by the 1.8B model. In short, we identify the optimal training recipe in our 1.8 billion parameter setting and apply the known scaling laws to find the training recipe at 7 billion parameter scale. Learning Rate For critical hyperparameters such as learning rate, we rely on results from Deepseek-V1 DeepSeek-AI et al. (2024) which found the law as ðœ‡ ð¶ 0.125, while holding constant less influential parameters. Applying this, we scale the optimal learning rate ðœ‡ by 0.57 7 Trillion 7B Technical Report when scaling up. Vocabulary Size For vocabulary size, we utilize the findings from (Tao et al., 2024) which notes the power law relation of non-vocabulary size with compute ð‘ð‘£ ð¶0.42. Upon the experimental results on tokenizer vocabulary size (Section 6.3), applying this law suggests that we should increase the number of multilingual tokens by at least 6.3 times. 3.4. Model Architecture Trillion is based on Transformer decoder (Vaswani et al., 2023) architecture with RoPE (Su et al., 2023) , SwiGLU (Shazeer, 2020), RMSNorm (Zhang and Sennrich, 2019). It consists of 32 layers with hidden size of 4096 and feedforward dimension of 11008. We normalize the hidden representations before and after each transformer, attention, and feedforward layer. See Table 2 for details. Model size"
        },
        {
            "title": "Hidden dimension\nNumber of layers\nFeedforward dimension\nNumber of heads\nNumber of KV heads",
            "content": "7B 4096 32 11,008 32 32 Activation Function Max sequence length Positional Embeddings Vocab size SwiGLU 4096 RoPE (ðœƒ = 100, 000) 128,256 Table 2 Model hyperparameters of Trillion Multi-token Prediction We adopt multi-token prediction (MTP) motivated by (DeepSeek-AI et al., 2025; Gloeckle et al., 2024). Along with the next token prediction (NTP) loss, we also predict the second next token by stacking new transformer layer after the last layer for the prediction. The MTP loss is combined with the NTP loss through the hyperparameter ð›¼. This MTP layer is discarded after pretraining and is not used during post-training. Training and Hyperparameter Details We utilize AdamW optimizer with learning rate of 2 104, ( ð›½1, ð›½2) = (0.9, 0.95), weight decay of 0.1, and 2000 warmup steps. We use RoPE ðœƒ of 100,000. Batch size is slowly increased from 1M to 2M in the first 1T tokens as we observe that this leads to better training stability and faster emergence. context length of 4096 is used throughout pretraining. MTP ð›¼ of 0.2 is used. As mentioned above, we utilize the WSD learning rate scheduler, decaying the learning rate using inverse proportional decay function to 10% (2 105) of its max learning rate in the last 10% of training. Hence, the model is trained at high learning rate for 1.8T followed by 0.2T at the decaying rate. We also lower weight decay to 0.033 and ð›¼ to 0.1 at the annealing stage. 8 Trillion 7B Technical Report Figure 5 Average Korean throughput measured on 1,000 selected Korean documents using vLLM. We choose Korean vocabulary size of 24,552 tokens, surpassing the scaling-law optimal size of around 13,000 tokens, yet still positioned just before the plateau in inference speed gains for Korean. This decision strategically balances theoretical optimality against practical improvements in inference speed. 3.5. Tokenizer Trillion 7B uses byte-level byte pair encoding (BPE) tokenizer (Kida et al., 1999) 1. Results from our preliminary experiments suggest that an excessively large vocabulary for non-English languages negatively impacts model performance due to sparse updates for infrequently appearing tokens, while too small vocabulary size reduces training efficiency by increasing token count. Further details are provided in Section 6.3. Our final tokenizer comprises 128,256 byte-level tokens, allocating approximately 100,000 tokens for English, 24,552 tokens for Korean, and the remaining tokens for other multilingual content. We accept slight deviation from optimality (which is around 13,000 tokens suggested by the scaling law (Tao et al., 2024)) due to improvements in inference speed and consequently increased effective context length. In Figure 5, it can be clearly seen that our chosen vocabulary is situated close to the inflection point of the inference speed plateau. Our adopted vocabulary size offers over 35% increase in Korean inference speed while 13k only offers 11% increase compared to Llama 3 tokenizer (leftmost point on the plot). 3.6. Training Infrastructure Trillion 7B model was trained using 256 H100 GPUs, each with 80GB HBM3, using mixed precision training with BF16. We used fully sharded data parallelism (Zhao et al., 2023). To reduce the high communication cost of the all-gather operation of FSDP, we sharded optimizer states only. We achieve HFU and MFU of 47.5% and 42.5% respectively. See Table 1 training cost 1We utilize the tokenizers library for tokenizer training. 9 Trillion 7B Technical Report for Trillion-7B. 3.7. Context Length Extension Longer context windows have become increasingly important for real-world applications in recent years. For context length extension, we employ two-stage approach. On top of the pretrained model, we additionally train for 60B tokens with context window of 32,768 tokens. We also extend the RoPE base frequency from 100,000 to 1,000,000 using the Adaptive Base Frequency technique (Xiong et al., 2023). Following (Gao et al., 2025), we construct our training set with 60% long context data and 40% high-quality data with sequence lengths of at most 4,096 tokens. To support this long-context training, we utilize tensor parallelism (Shoeybi et al., 2020) along with FSDP. 4. Post-training For post-training, we closely follow the TÃ¼lu 3 framework consisting of Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2025). Since our work focuses primarily on multilingual pretraining, we adopt this established open-source post-training recipe to complete our model development pipeline. Supervised Fine-Tuning (SFT) While primarily leveraging English data from Tulu 3 (Lambert et al., 2025), we supplement our datasets with smaller portion of open-sourced non-English prompt-response pairs in Korean, Japanese, and Chinese (KÃ¶pf et al., 2023). We filter responses using the LLM-as-a-judge method (Kim et al., 2024a,b; Zheng et al., 2023), employing Qwen-2.572B to score responses on scale from 0 to 5. Only responses scoring above 3 are retained. The final SFT dataset contains approximately 800,000 prompt-response pairs. Importantly, we only apply the loss on responses. We leverage model merging (Wortsman et al., 2022) to combine three different checkpoints trained with different random seeds to produce the final SFT model. Direct Preference Optimization (DPO) Following SFT, the model undergoes refinement via Direct Preference Optimization (DPO) (Rafailov et al., 2024), integrating around 200,000 pairs of preferred (winning) and non-preferred (losing) responses. We prompt our model to generate on-policy responses and pair these with TÃ¼lu 3 off-policy responses. The chosen-rejected pairs are selected using Qwen-2.5-72B as the reward model. To avoid unintended length biases, we carefully removed pairs with significantly different lengths. Reinforcement Learning with Verifiable Rewards (RLVR) Finally, Trillion 7B is fine-tuned on targeted set of 10,000 prompts, predominantly mathematical questions from the training sets of GSM8k (Cobbe et al., 2021b) and MATH (Hendrycks et al., 2021b). This stage uses reinforcement learning with verifiable feedback via Group Relative Policy Optimization (GRPO) (Shao et al., 2024). We carefully design reward functions for both mathematical reasoning and instruction-following abilities. We found that using problems with appropriate difficulty levels and implementing the right system prompt are crucial factors in optimizing performance. 10 Trillion 7B Technical Report 5. Evaluation This section presents the evaluation setup and results of Trillion language models, compared with open language models of comparable size across benchmarks. For details on the baseline models, see Appendix B. Plotting Trillion and the baselines in terms of compute against the multilingual benchmark scores, we achieve strong performance, see Figure 1. 5.1. Benchmarks We evaluated our models on 27 benchmarks across four languages, categorized into General Reasoning, Knowledge, Mathematical Reasoning, Coding, and Instruction-Following tasks. Table 3 summarizes these benchmarks, and full details are in Appendix C. We provide detailed information about the prompts and example instances used in our evaluation in Appendix C. Category Name Language Prompting Metric accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy General Reasoning English HellaSwag Zellers et al. (2019) English TruthfulQA_mc1 Lin et al. (2022) English TruthfulQA_mc2 Lin et al. (2022) English ARC:C Clark et al. (2018) Korean HAERAE Son et al. (2024a) Korean KoBEST Jang et al. (2022) BBH Suzgun et al. (2022) English xwinograd_en Muennighoff et al. (2023) English xwinograd_jp Muennighoff et al. (2023) Japanese xwinograd_zh Muennighoff et al. (2023) Chinese 0-shot 6-shot 6-shot 0-shot 3-shot 5-shot 3-shot, CoT 0-shot 0-shot 0-shot 5-shot 5-shot 5-shot 5-shot 5-shot 5-shot 4-shot Knowledge Math Coding Instruction-following KMMLU Son et al. (2024b) MMLU Hendrycks et al. (2021a) GMMLU-en Singh et al. (2024) GMMLU-ko Singh et al. (2024) GMMLU-ja Singh et al. (2024) GMMLU-zh Singh et al. (2024) GPQA Rein et al. (2023) GSM8k Cobbe et al. (2021a) MATH Hendrycks et al. (2021c) HRM8k Ko et al. (2025) MBPP Austin et al. (2021) HumanEval Chen et al. (2021) IFEval Zhou et al. (2023a) Ko-IFEval MT-Bench Zheng et al. (2023) KO-MT-Bench Research (2024) LogicKor Park (2024) Our in-house instruction following evaluation set in Korean. Korean English English Korean Japanese Chinese English English English Korean English English English Korean English Korean Korean 0-shot, CoT 0-shot, CoT 0-shot, CoT 0-shot, CoT 0-shot, CoT exact-match exact-match exact-match pass@1 pass@1 strict-average strict-average 0-shot 0-shot LLM-as-judge LLM score LLM-as-judge LLM score LLM-as-judge LLM score Table 3 Summary of evaluation benchmarks 5.2. Evaluation Results Benchmark Results Table 4 presents the performance of Trillion models compared to baselines across all benchmarks. Our models demonstrated strong capabilities across various languages, with particularly impressive results on multilingual chat and instruction following. 11 Trillion 7B Technical Report Category Trillion-7B Qwen2.5-7B EXAONE-3.5-8B Gemma-2-9B Llama-3.1-8B Mistral-7B Training Tokens General Reasoning Knowledge Coding Math Instruction & Chat 2T 64.67 56.17 47.94 45.02 71. 17T 66.56 61.36 66.35 67.29 69.20 9T 65.58 53.56 70.33 65.82 78.27 8T 64.10 60.93 34.69 43.60 70. 15T 62.27 53.79 53.44 46.68 56.28 4T 61.60 47.68 36.30 18.65 48.71 Average (Rank) 57.15 (3) 66.15 (2) 66.71 (1) 54.68 (4) 54.49 (5) 42.59 (6) Table 4 Model Performance Summary by Category (%). Macro-averaged. 6. Ablations We present various ablative experiments illustrating the effectiveness of Trillion 7Bs training recipe. Our ablations specifically focus on four languages: English, Korean, Japanese, and Chinese. 6.1. Data Composition and Quality"
        },
        {
            "title": "HellaSwag COPA HellaSwag",
            "content": "ZH JP No QF QF-Top50 0.3990 0.4399 0.616 0.671 0.370 0. 0.6429 0.6448 0.5892 0.6173 Table 5 Performance comparison with and without quality filtering on Korean data. Pre-training We present the effectiveness of our quality filtering approach for the pretraining phase using 1.8 billion parameter model on 100 billion tokens. Specifically, we improved the quality of Korean documents by selecting the 50 percentile (QF-Top50) while leaving other languages unchanged and compare this to the baseline with no quality filtering (No QF). Table 5 presents the results, showing improvement across all languages despite only enhancing the quality of Korean data. Annealing We conducted ablative experiments to evaluate the significance of the annealing stage by altering the composition and quality of training data over an additional 20 billion tokens, starting from 7B model trained up to 1.8 trillion tokens at high learning rate. Using model trained with consistently high learning rate as our baseline (High Learning Rate), we experimented with three variants: Anneal (decayed learning rate with original data mixture), Anneal+Quality (decayed learning rate with high-quality filtered data), and Anneal+Quality+Composition (decayed learning rate with high-quality filtered data and up-sampled multilingual, math and code composition). Table 6 presents the Global MMLU results for English, Korean, Japanese, and Chinese. The findings underscore the critical role of quality filtering, which significantly enhances performance across all languages. An intriguing observation is the improvement in English performance under the Anneal+Quality+Composition condition, despite reduction in English data volume, highlighting the cross-lingual knowledge bridging effects discussed in Section 2. Another contributing factor for this improvement is the up-sampling of math and Trillion 7B Technical Report code data, which is aligned with previously reported empirical results on the effects of such data on downstream performance (Blakeney et al., 2024; Petty et al., 2025). GMMLU Checkpoint EN KO JA ZH"
        },
        {
            "title": "0.530\nHigh Learning Rate\n0.5675\nAnneal\nAnneal+Quality\n0.61\nAnneal+Quality+Composition 0.6325",
            "content": "0.44 0.445 0.4550 0.4875 0.4225 0.435 0.4675 0.50 0.4275 0.4875 0.4977 0.505 Table 6 Checkpoint performance on GMMLU for different annealing strategies. GMMLU results are shown for English (EN), Korean (KO), Chinese (ZH), and Japanese (JA). Bold indicates the best scores per column. 6.2. Impact of Data Diversity We further investigated the impact of multilingual data diversity by combining the non-English multilingual FineWeb dataset (Penedo et al., 2024) with our in-house Korean pretraining data. Three models at the 1.8B scale were trained: one baseline model using only FineWeb as the multilingual dataset, and two variants with incrementally increased diversity from our in-house dataset. Deduplication was applied with FineWeb to ensure results were not confounded by duplicated texts. Results shown in Table 7 reveal that enhancing diversity in the Korean dataset consistently boosts performance in Korean-specific benchmarks. Furthermore, the results confirm effective multilingual transfer, as evidenced by performance gains in other languages. Notably, mutual knowledge transfer improvements are observed among English, Japanese, and Korean benchmarks."
        },
        {
            "title": "HellaSwag",
            "content": "FineWeb FineWeb + In-House 1 FineWeb + In-House 2 0.544 0.603 0."
        },
        {
            "title": "XWinogrande",
            "content": "JA ZH 0.696 0.705 0.711 0.698 0.700 0."
        },
        {
            "title": "KoBEST Average",
            "content": "0.516 0.549 0.635 0.614 0.639 0.660 Table 7 Korean performance comparison by increasing data diversity 6.3. Vocabulary Size We perform an ablative experiment to find the optimal vocabulary size for Korean. We show that using smaller vocabulary can actually be beneficial for Koreans, contrary to the established knowledge that increasing language-specific vocabulary can lead to an improved performance (Seo et al., 2025). With 1.8B model trained with 100 billion tokens, we conduct controlled experiment by ablating the number of Korean tokens while keeping other non-Korean entries fixed in the tokenizer. The resulting tokenizers are subsets of their larger counterparts. See Figure 6 for results. While we see relatively stable performance in English, we see drastic performance difference in Korean, observing that our optimal Korean vocabulary size is situated between 1500 and 5000 tokens, which is further adjusted by the vocabulary scaling laws suggested from (Tao et al., 2024). Trillion 7B Technical Report Figure 6 Tokenizer ablation experiments on Korean vocabulary size with 1.8B model trained on 100 billion tokens. We report KoBEST for Korean and HellaSwag for English. 7. Analysis 7.1. Cross-lingual Consistency We demonstrate our model builds robust multilingual representations capable of effective cross-lingual generalization by assessing prediction consistency across languages. We focus on whether English knowledge transfers properly to Korean using parallel question sets from Global-MMLU (Singh et al., 2024), translated by language experts. We define three metrics based on prediction consistency. The primary metric measures whether correct English predictions lead to correct Korean predictions: ð¸(ð‘‡) ð¾ (ð‘‡) = Eð‘–ð‘„ (cid:2)Model(ð‘„ð‘˜ð‘œ ð‘– , ð´ð‘˜ð‘œ ð‘– ) = 1 Model(ð‘„ð‘’ð‘› ð‘– , ð´ð‘’ð‘› ð‘– ) = 1(cid:3) (3) ð‘– and ð´ð‘’ð‘› Here, ð‘„ð‘’ð‘› indicates correct prediction. Similarly for Korean ð‘„ð‘˜ð‘œ predictions in one language can revert in another: ð‘– represent the English question and gold answer, while Model(ð‘„ð‘’ð‘› ð‘– , ð´ð‘’ð‘› ð‘– and ð´ð‘˜ð‘œ ð‘– ð‘– ) = 1 . We also assess whether incorrect ð¸(ð¹) ð¾ (ð‘‡) = Eð‘–ð‘„ (cid:2)Model(ð‘„ð‘˜ð‘œ ð‘– , ð´ð‘˜ð‘œ ð‘– ) = 1 Model(ð‘„ð‘’ð‘› ð‘– , ð´ð‘’ð‘› ð‘– ) = 0(cid:3) (4) This metric reveals consistency in knowledge representation across languages. Table 8 shows Trillion achieves superior consistency across all measures compared to leading multilingual models, confirming its robust cross-lingual generalization capability."
        },
        {
            "title": "Model",
            "content": "Tokens E(T)K(T) K(F)E(T) E(F)K(T) Trillion 7B Llama 3.1 7B Exaone 3.5 7.8B Qwen 2.5 7B 2T 15T 9T 17T 77.5% 67.7% 76.4% 77.4% 29.8% 42.0% 38.5% 42.8% 18.73% 18.74% 21.21% 19.83% Table 8 Consistency comparison between Korean and English. 14 Trillion 7B Technical Report 7.2. Generalization to Vision We demonstrate Trillion 7Bs robust multilingual representation enables effective transfer to vision modalities. Following LLaVAs approach (Liu et al., 2023), we finetune our model into Vision Language Model (VLM) using identical training framework for controlled comparison. Despite training exclusively on English vision-language instruction pairs, the model shows strong performance on Korean visual reasoning tasks. Table 9 compares Trillion-LLaVAs performance on English and Korean VLM benchmarks (Ju et al., 2024) against comparable models. MMBENCH SEED-I MMStar K-DTCB Model En Llava-1.5-Vicuna-7B 0.64 Llava-1.6-Mistral-7B 0.68 Trillion-LLaVA-7B 0. Ko 0.43 0.49 0.61 En Ko En Ko 0.66 0.72 0.68 0.52 0.61 0.66 0.34 0.36 0.37 0.33 0.33 0.37 Ko 0.30 0.30 0. Table 9 Performance comparison (English, Korean) across different vision-language models. Although Trillion-LLaVA-7B was trained only on English vision-language instruction pairs, it outperforms other VLMs on Korean benchmarks. This zero-shot cross-lingual capability has significant implications for multilingual applications. While LLaVA models based on Vicuna (Chiang et al., 2023) and Mistral (Jiang et al., 2023) show limited Korean performance, Trillion-LLaVA excels despite identical English-only training. This suggests robust multilingual pre-training transfers effectively to multimodal tasks, potentially decoupling language-specific requirements from visual alignment training and streamlining multilingual vision-language system development. These results indicate our models multilingual foundation enables effective transfer of visual reasoning across languages without language-specific visual training data, raising questions about similar transfer effects for other languages and modalities. 8. Conclusion, Limitations and Future Works We present Trillion-7B, demonstrating that effective multilingual capabilities can be achieved through architectural innovation rather than massive data scaling alone. Through our novel Cross-lingual Document Attention (XLDA) mechanism, strategic data filtering, and optimized tokenization strategy, we establish new efficiency frontier for multilingual models, requiring just 59.4K H100 GPU hours while dedicating only 10% of training tokens to non-English languages. The consistent performance across languages confirms XLDAs effectiveness at bridging knowledge between linguistic domains. Our extensive experiments and ablation studies validate that properly designed cross-lingual training strategies significantly outperform traditional approaches that rely heavily on languagespecific data scaling. These findings have important implications for democratizing access to powerful language models across diverse linguistic communities, as they suggest path forward for developing high-performing multilingual models without the prohibitive computational and data requirements typically associated with frontier AI systems. Limitations Our current model has several limitations. We dedicated limited resources to mathematical and coding data (less than 2% of total training data) during pretraining, as our 15 Trillion 7B Technical Report primary focus was on cross-lingual knowledge transfer. Consequently, Trillion-7B may exhibit sub-optimal performance on technical tasks. We plan to address this in the next generation of our models. Additionally, we performed minimal post-training optimization, and our safety mechanisms require further development to reach production standards. Users should note these limitations when deploying our models and must not use them to cause harm. Future Works Future research directions include further improving multilingual model efficiency through enhanced training methods and architectural refinements. We plan to build upon our approach and enhance Trillion through dedicated post-training optimization and improved reasoning capabilities. Additionally, we aim to apply our techniques to vision models, as preliminary results already demonstrate the strength of our approach for cross-modal applications. Finally, we intend to scale up our methodology to develop family of multilingual data-efficient models ranging from 70B-scale dense architectures to 400B-scale Mixture-of-Experts models, further democratizing access to powerful multilingual AI systems."
        },
        {
            "title": "References",
            "content": "J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton. Program synthesis with large language models, 2021. URL https: //arxiv.org/abs/2108.07732. C. Blakeney, M. Paul, B. W. Larsen, S. Owen, and J. Frankle. Does your data spark joy? performance gains from domain upsampling at the end of training, 2024. URL https: //arxiv.org/abs/2406.03476. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code, 2021. URL https: //arxiv.org/abs/2107.03374. W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. 16 Trillion 7B Technical Report K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021a. URL https://arxiv.org/abs/2110.14168. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021b. URL https://arxiv.org/abs/2110.14168. DeepSeek-AI, :, X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, H. Gao, K. Gao, W. Gao, R. Ge, K. Guan, D. Guo, J. Guo, G. Hao, Z. Hao, Y. He, W. Hu, P. Huang, E. Li, G. Li, J. Li, Y. Li, Y. K. Li, W. Liang, F. Lin, A. X. Liu, B. Liu, W. Liu, X. Liu, X. Liu, Y. Liu, H. Lu, S. Lu, F. Luo, S. Ma, X. Nie, T. Pei, Y. Piao, J. Qiu, H. Qu, T. Ren, Z. Ren, C. Ruan, Z. Sha, Z. Shao, J. Song, X. Su, J. Sun, Y. Sun, M. Tang, B. Wang, P. Wang, S. Wang, Y. Wang, Y. Wang, T. Wu, Y. Wu, X. Xie, Z. Xie, Z. Xie, Y. Xiong, H. Xu, R. X. Xu, Y. Xu, D. Yang, Y. You, S. Yu, X. Yu, B. Zhang, H. Zhang, L. Zhang, L. Zhang, M. Zhang, M. Zhang, W. Zhang, Y. Zhang, C. Zhao, Y. Zhao, S. Zhou, S. Zhou, Q. Zhu, and Y. Zou. Deepseek llm: Scaling open-source language models with longtermism, 2024. URL https://arxiv.org/abs/2401.02954. DeepSeek-AI, A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Wang, J. Chen, J. Chen, J. Yuan, J. Qiu, J. Li, J. Song, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Wang, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Wang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge, R. Zhang, R. Pan, R. Wang, R. Xu, R. Zhang, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye, S. Ye, S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Pan, T. Wang, T. Yun, T. Pei, T. Sun, W. L. Xiao, W. Zeng, W. Zhao, W. An, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen, X. Chen, X. Zhang, X. Chen, X. Nie, X. Sun, X. Wang, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yu, X. Song, X. Shan, X. Zhou, X. Yang, X. Li, X. Su, X. Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Y. Zhang, Y. Xu, Y. Xu, Y. Huang, Y. Li, Y. Zhao, Y. Sun, Y. Li, Y. Wang, Y. Yu, Y. Zheng, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Tang, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Wu, Y. Ou, Y. Zhu, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Zha, Y. Xiong, Y. Ma, Y. Yan, Y. Luo, Y. You, Y. Liu, Y. Zhou, Z. F. Wu, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Huang, Z. Zhang, Z. Xie, Z. Zhang, Z. Hao, Z. Gou, Z. Ma, Z. Yan, Z. Shao, Z. Xu, Z. Wu, Z. Zhang, Z. Li, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Gao, and Z. Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Z. Du, A. Zeng, Y. Dong, and J. Tang. Understanding emergent abilities of language models from the loss perspective, 2025. URL https://arxiv.org/abs/2403.15796. T. Gao, A. Wettig, H. Yen, and D. Chen. How to train long-context language models (effectively), 2025. URL https://arxiv.org/abs/2410.02660. F. Gloeckle, B. Y. Idrissi, B. RoziÃ¨re, D. Lopez-Paz, and G. Synnaeve. Better & faster large language models via multi-token prediction, 2024. URL https://arxiv.org/abs/2404.1 9737. A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, 17 Trillion 7B Technical Report A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Wyatt, D. Esiobu, D. Choudhary, D. Mahajan, D. GarciaOlano, D. Perino, D. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. GuzmÃ¡n, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Thattai, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. A. Ibarra, I. Kloumann, I. Misra, I. Evtimov, J. Zhang, J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Prasad, K. Upasani, K. Plawiak, K. Li, K. Heafield, K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu, K. Bhalla, K. Lakhotia, L. Rantala-Yeary, L. van der Maaten, L. Chen, L. Tan, L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat, L. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri, M. Kardas, M. Tsimpoukelli, M. Oldham, M. Rita, M. Pavlova, M. Kambadur, M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi, N. Bashlykov, N. Bogoychev, N. Chatterji, N. Zhang, O. Duchenne, O. Ã‡elebi, P. Alrassy, P. Zhang, P. Li, P. Vasic, P. Weng, P. Bhargava, P. Dubal, P. Krishnan, P. S. Koura, P. Xu, Q. He, Q. Dong, R. Srinivasan, R. Ganapathy, R. Calderer, R. S. Cabral, R. Stojnic, R. Raileanu, R. Maheswari, R. Girdhar, R. Patel, R. Sauvestre, R. Polidoro, R. Sumbaly, R. Taylor, R. Silva, R. Hou, R. Wang, S. Hosseini, S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie, S. Narang, S. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang, S. Vandenhende, S. Batra, S. Whitman, S. Sootla, S. Collot, S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha, T. Georgiou, T. Scialom, T. Speckbacher, T. Mihaylov, T. Xiao, U. Karn, V. Goswami, V. Gupta, V. Ramanathan, V. Kerkez, V. Gonguet, V. Do, V. Vogeti, V. Albiero, V. Petrovic, W. Chu, W. Xiong, W. Fu, W. Meers, X. Martinet, X. Wang, X. Wang, X. E. Tan, X. Xia, X. Xie, X. Jia, X. Wang, Y. Goldschlag, Y. Gaur, Y. Babaei, Y. Wen, Y. Song, Y. Zhang, Y. Li, Y. Mao, Z. D. Coudert, Z. Yan, Z. Chen, Z. Papakipos, A. Singh, A. Srivastava, A. Jain, A. Kelsey, A. Shajnfeld, A. Gangidi, A. Victoria, A. Goldstand, A. Menon, A. Sharma, A. Boesenberg, A. Baevski, A. Feinstein, A. Kallet, A. Sangani, A. Teo, A. Yunus, A. Lupu, A. Alvarado, A. Caples, A. Gu, A. Ho, A. Poulton, A. Ryan, A. Ramchandani, A. Dong, A. Franco, A. Goyal, A. Saraf, A. Chowdhury, A. Gabriel, A. Bharambe, A. Eisenman, A. Yazdan, B. James, B. Maurer, B. Leonhardi, B. Huang, B. Loyd, B. D. Paola, B. Paranjape, B. Liu, B. Wu, B. Ni, B. Hancock, B. Wasti, B. Spence, B. Stojkovic, B. Gamido, B. Montalvo, C. Parker, C. Burton, C. Mejia, C. Liu, C. Wang, C. Kim, C. Zhou, C. Hu, C.-H. Chu, C. Cai, C. Tindal, C. Feichtenhofer, C. Gao, D. Civin, D. Beaty, D. Kreymer, D. Li, D. Adkins, D. Xu, D. Testuggine, D. David, D. Parikh, D. Liskovich, D. Foss, D. Wang, D. Le, D. Holland, E. Dowling, E. Jamil, E. Montgomery, E. Presani, E. Hahn, E. Wood, E.-T. Le, E. Brinkman, E. Arcaute, E. Dunbar, E. Smothers, F. Sun, F. Kreuk, F. Tian, F. Kokkinos, F. Ozgenel, F. Caggioni, F. Kanayet, F. Seide, G. M. Florez, G. Schwarz, G. Badeer, G. Swee, G. Halpern, G. Herman, G. Sizov, Guangyi, Zhang, G. Lakshminarayanan, H. Inan, H. Shojanazeri, H. Zou, H. Wang, H. Zha, H. Habeeb, H. Rudolph, H. Suk, H. Aspegren, H. Goldman, H. Zhan, I. Damlaj, I. Molybog, I. Tufanov, I. Leontiadis, I.-E. Veliche, I. Gat, J. Weissman, J. Geboski, J. Kohli, J. Lam, J. Asher, J.-B. Gaya, J. Marcus, J. Tang, J. Chan, J. Zhen, J. Reizenstein, J. Teboul, J. Zhong, J. Jin, J. Yang, J. Cummings, J. Carvill, J. Shepard, J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu, K. H. U, K. Saxena, K. Khandelwal, K. Zand, K. Matosich, K. Veeraraghavan, K. Michelena, K. Li, K. Jagadeesh, K. Huang, K. Chawla, K. Huang, L. Chen, L. Garg, L. A, L. Silva, L. Bell, L. Zhang, L. Guo, L. Yu, L. Moshkovich, L. Wehrstedt, M. Khabsa, M. Avalani, M. Bhatt, M. Mankus, M. Hasson, M. Lennie, M. Reso, M. Groshev, M. Naumov, M. Lathi, M. Keneally, M. Liu, M. L. Seltzer, M. Valko, M. Restrepo, M. Patel, M. Vyatskov, M. Samvelyan, M. Clark, M. Macey, M. Wang, M. J. Hermoso, M. Metanat, M. Rastegari, M. Bansal, N. Santhanam, N. Parks, N. White, 18 Trillion 7B Technical Report N. Bawa, N. Singhal, N. Egebo, N. Usunier, N. Mehta, N. P. Laptev, N. Dong, N. Cheng, O. Chernoguz, O. Hart, O. Salpekar, O. Kalinli, P. Kent, P. Parekh, P. Saab, P. Balaji, P. Rittner, P. Bontrager, P. Roux, P. Dollar, P. Zvyagina, P. Ratanchandani, P. Yuvraj, Q. Liang, R. Alao, R. Rodriguez, R. Ayub, R. Murthy, R. Nayani, R. Mitra, R. Parthasarathy, R. Li, R. Hogan, R. Battey, R. Wang, R. Howes, R. Rinott, S. Mehta, S. Siby, S. J. Bondu, S. Datta, S. Chugh, S. Hunt, S. Dhillon, S. Sidorov, S. Pan, S. Mahajan, S. Verma, S. Yamamoto, S. Ramaswamy, S. Lindsay, S. Lindsay, S. Feng, S. Lin, S. C. Zha, S. Patil, S. Shankar, S. Zhang, S. Zhang, S. Wang, S. Agarwal, S. Sajuyigbe, S. Chintala, S. Max, S. Chen, S. Kehoe, S. Satterfield, S. Govindaprasad, S. Gupta, S. Deng, S. Cho, S. Virk, S. Subramanian, S. Choudhury, S. Goldman, T. Remez, T. Glaser, T. Best, T. Koehler, T. Robinson, T. Li, T. Zhang, T. Matthews, T. Chou, T. Shaked, V. Vontimitta, V. Ajayi, V. Montanez, V. Mohan, V. S. Kumar, V. Mangla, V. Ionescu, V. Poenaru, V. T. Mihailescu, V. Ivanov, W. Li, W. Wang, W. Jiang, W. Bouaziz, W. Constable, X. Tang, X. Wu, X. Wang, X. Wu, X. Gao, Y. Kleinman, Y. Chen, Y. Hu, Y. Jia, Y. Qi, Y. Li, Y. Zhang, Y. Zhang, Y. Adi, Y. Nam, Yu, Wang, Y. Zhao, Y. Hao, Y. Qian, Y. Li, Y. He, Z. Rait, Z. DeVito, Z. Rosnbrick, Z. Wen, Z. Yang, Z. Zhao, and Z. Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009 .03300. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b. URL https://arxi v.org/abs/2103.03874. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021c. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/22 03.15556. S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao, X. Zhang, Z. L. Thai, K. Zhang, C. Wang, Y. Yao, C. Zhao, J. Zhou, J. Cai, Z. Zhai, N. Ding, C. Jia, G. Zeng, D. Li, Z. Liu, and M. Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024. URL https://arxiv.org/abs/2404.06395. A. HÃ¤gele, E. Bakouch, A. Kosson, L. B. Allal, L. V. Werra, and M. Jaggi. Scaling laws and compute-optimal training beyond fixed training durations, 2024. URL https://arxiv.or g/abs/2405.18392. M. Jang, D. Kim, D. S. Kwon, and E. Davis. KoBEST: Korean balanced evaluation of significant tasks. In N. Calzolari, C.-R. Huang, H. Kim, J. Pustejovsky, L. Wanner, K.-S. Choi, P.-M. Ryu, H.-H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S.-H. Na, editors, Proceedings of the 29th International Conference on Computational Linguistics, pages 36973708, Gyeongju, Republic of Korea, Oct. 2022. International Committee on Computational Linguistics. URL https://aclantho logy.org/2022.coling-1.325/. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, 19 Trillion 7B Technical Report T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/23 10.06825. J. Ju, D. Kim, S. Park, and Y. Kim. Varco-vision: Expanding frontiers in korean vision-language models, 2024. URL https://arxiv.org/abs/2411.19103. J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models, 2020. URL https://arxiv. org/abs/2001.08361. T. Kida, S. Fukamachi, M. Takeda, A. Shinohara, T. Shinohara, and S. Arikawa. Byte pair encoding: text compression scheme that accelerates pattern matching. 1999. URL https: //api.semanticscholar.org/CorpusID:18801509. B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park, S. Kim, S. Kim, D. Seo, H. Lee, M. Jeong, S. Lee, M. Kim, S. H. Ko, S. Kim, T. Park, J. Kim, S. Kang, N.-H. Ryu, K. M. Yoo, M. Chang, S. Suh, S. In, J. Park, K. Kim, H. Kim, J. Jeong, Y. G. Yeo, D. Ham, D. Park, M. Y. Lee, J. Kang, I. Kang, J.-W. Ha, W. Park, and N. Sung. What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers, 2021. URL https://arxiv.org/abs/2109.04650. S. Kim, J. Shin, Y. Cho, J. Jang, S. Longpre, H. Lee, S. Yun, S. Shin, S. Kim, J. Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations. S. Kim, J. Suk, J. Y. Cho, S. Longpre, C. Kim, D. Yoon, G. Son, Y. Cho, S. Shafayat, J. Baek, et al. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models. arXiv preprint arXiv:2406.05761, 2024a. S. Kim, J. Suk, S. Longpre, B. Y. Lin, J. Shin, S. Welleck, G. Neubig, M. Lee, K. Lee, and M. Seo. Prometheus 2: An open source language model specialized in evaluating other language models, 2024b. URL https://arxiv.org/abs/2405.01535. H. Ko, K. Yang, M. Ryu, T. Choi, S. Yang, J. Hyun, S. Park, and K. Park. technical report for polyglot-ko: Open-source large-scale korean language models, 2023. URL https://arxiv. org/abs/2306.02254. H. Ko, G. Son, and D. Choi. Understand, solve and translate: Bridging the multilingual mathematical reasoning gap, 2025. URL https://arxiv.org/abs/2501.02448. A. KÃ¶pf, Y. Kilcher, D. von RÃ¼tte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, S. ES, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire, C. Schuhmann, H. Nguyen, and A. Mattick. Openassistant conversations democratizing large language model alignment, 2023. URL https://arxiv.org/abs/2304.07327. N. Lambert, J. Morrison, V. Pyatkin, S. Huang, H. Ivison, F. Brahman, L. J. V. Miranda, A. Liu, N. Dziri, S. Lyu, Y. Gu, S. Malik, V. Graf, J. D. Hwang, J. Yang, R. L. Bras, O. Tafjord, C. Wilhelm, L. Soldaini, N. A. Smith, Y. Wang, P. Dasigi, and H. Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. A. K. Lampinen, S. C. Y. Chan, A. K. Singh, and M. Shanahan. The broader spectrum of in-context learning, 2024. URL https://arxiv.org/abs/2412.03782. 20 Trillion 7B Technical Report Y. Levine, N. Wies, D. Jannai, D. Navon, Y. Hoshen, and A. Shashua. The inductive bias of in-context learning: Rethinking pretraining example design, 2022. URL https://arxiv.or g/abs/2110.04541. S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022. URL https://arxiv.org/abs/2109.07958. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning, 2023. URL https://arxiv.org/ abs/2304.08485. S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts, B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno, and D. Ippolito. pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity, 2023. URL https://arxiv.org/abs/2305 .13169. N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, X. Tang, D. Radev, A. F. Aji, K. Almubarak, S. Albanie, Z. Alyafeai, A. Webson, E. Raff, and C. Raffel. Crosslingual generalization through multitask finetuning, 2023. URL https://arxiv.org/abs/2211.01786. T. OLMo, P. Walsh, L. Soldaini, D. Groeneveld, K. Lo, S. Arora, A. Bhagia, Y. Gu, S. Huang, M. Jordan, N. Lambert, D. Schwenk, O. Tafjord, T. Anderson, D. Atkinson, F. Brahman, C. Clark, P. Dasigi, N. Dziri, M. Guerquin, H. Ivison, P. W. Koh, J. Liu, S. Malik, W. Merrill, L. J. V. Miranda, J. Morrison, T. Murray, C. Nam, V. Pyatkin, A. Rangapur, M. Schmitz, S. Skjonsberg, D. Wadden, C. Wilhelm, M. Wilson, L. Zettlemoyer, A. Farhadi, N. A. Smith, and H. Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656. OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti, T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn, H. Jun, T. Kaftan, Åukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Åukasz Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. MÃ©ly, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. OKeefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, 21 Trillion 7B Technical Report M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. B. Thompson, P. Tillet, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. P. A. Ortega, J. X. Wang, M. Rowland, T. Genewein, Z. Kurth-Nelson, R. Pascanu, N. Heess, J. Veness, A. Pritzel, P. Sprechmann, S. M. Jayakumar, T. McGrath, K. Miller, M. Azar, I. Osband, N. Rabinowitz, A. GyÃ¶rgy, S. Chiappa, S. Osindero, Y. W. Teh, H. van Hasselt, N. de Freitas, M. Botvinick, and S. Legg. Meta-learning of sequential strategies, 2019. URL https://arxi v.org/abs/1905.03030. J. Park. Logickor. 2024. doi: doi:10.57967/hf/2440. URL https://github.com/instructk r/LogicKor. G. Penedo, H. KydlÃ­Ë‡cek, L. B. allal, A. Lozhkov, M. Mitchell, C. Raffel, L. V. Werra, and T. Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https: //arxiv.org/abs/2406.17557. J. Petty, S. van Steenkiste, and T. Linzen. How does code pretraining affect language model task performance?, 2025. URL https://arxiv.org/abs/2409.04556. Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv. org/abs/2305.18290. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/ab s/2311.12022. L. A. Research. Komt-bench. https://huggingface.co/datasets/LGAI-EXAONE/KoMT -Bench, 2024. L. A. Research, :, S. An, K. Bae, E. Choi, S. J. Choi, Y. Choi, S. Hong, Y. Hong, J. Hwang, H. Jeon, G. J. Jo, H. Jo, J. Jung, Y. Jung, E. Kim, H. Kim, J. Kim, S. Kim, S. Kim, S. Kim, Y. Kim, Y. Kim, E. H. Lee, H. Lee, H. Lee, J. Lee, K. Lee, M. Lee, S. Lee, W. Lim, S. Park, S. Park, Y. Park, B. Seo, S. Yang, H. Yeen, K. Yoo, and H. Yun. Exaone 3.0 7.8b instruction tuned language model, 2024a. URL https://arxiv.org/abs/2408.03541. L. A. Research, S. An, K. Bae, E. Choi, K. Choi, S. J. Choi, S. Hong, J. Hwang, H. Jeon, G. J. Jo, H. Jo, J. Jung, Y. Jung, H. Kim, J. Kim, S. Kim, S. Kim, S. Kim, Y. Kim, Y. Kim, Y. Kim, E. H. Lee, H. Lee, H. Lee, J. Lee, K. Lee, W. Lim, S. Park, S. Park, Y. Park, S. Yang, H. Yeen, and 22 Trillion 7B Technical Report H. Yun. Exaone 3.5: Series of large language models for real-world use cases, 2024b. URL https://arxiv.org/abs/2412.04862. J. Seo, J. Kim, S. Byun, and H. Shin. How does language-specific tokenizer affect llms?, 2025. URL https://arxiv.org/abs/2502.12560. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. N. Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.0 5202. M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. URL https://arxiv.org/abs/1909.08053. S. Singh, A. Romanou, C. Fourrier, D. I. Adelani, J. G. Ngui, D. Vila-Suero, P. Limkonchotiwat, K. Marchisio, W. Q. Leong, Y. Susanto, R. Ng, S. Longpre, W.-Y. Ko, M. Smith, A. Bosselut, A. Oh, A. F. T. Martins, L. Choshen, D. Ippolito, E. Ferrante, M. Fadaee, B. Ermis, and S. Hooker. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation, 2024. URL https://arxiv.org/abs/2412.03304. G. Son, H. Lee, S. Kim, H. Kim, J. Lee, J. W. Yeom, J. Jung, J. W. Kim, and S. Kim. Hae-rae bench: Evaluation of korean knowledge in language models, 2024a. URL https://arxiv.org/ab s/2309.02706. G. Son, H. Lee, S. Kim, S. Kim, N. Muennighoff, T. Choi, C. Park, K. M. Yoo, and S. Biderman. Kmmlu: Measuring massive multitask language understanding in korean, 2024b. URL https://arxiv.org/abs/2402.11548. J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104.09864. M. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, , and J. Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. C. Tao, Q. Liu, L. Dou, N. Muennighoff, Z. Wan, P. Luo, M. Lin, and N. Wong. Scaling laws with vocabulary: Larger models deserve larger vocabularies, 2024. URL https://arxiv.org/ abs/2407.13623. G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. RamÃ©, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome, A. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J.-B. Grill, B. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic, A. Carl, A. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal, C. Chen, C. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger, D. Vijaykumar, D. Rogozi nska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter, E. Eltyshev, F. Visin, G. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-Pluci nska, H. Batra, H. Dhand, I. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P. Zhou, J. Carrasqueira, J. Iljazi, J. Becker, J. Fernandez, J. van Amersfoort, J. Gordon, J. Lipschultz, J. Newlan, J. yeong Ji, K. Mohamed, Trillion 7B Technical Report K. Badola, K. Black, K. Millican, K. McDonell, K. Nguyen, K. Sodhia, K. Greene, L. L. Sjoesund, L. Usui, L. Sifre, L. Heuermann, L. Lago, L. McNealus, L. B. Soares, L. Kilpatrick, L. Dixon, L. Martins, M. Reid, M. Singh, M. Iverson, M. GÃ¶rner, M. Velloso, M. Wirth, M. Davidow, M. Miller, M. Rahtz, M. Watson, M. Risdal, M. Kazemi, M. Moynihan, M. Zhang, M. Kahng, M. Park, M. Rahman, M. Khatwani, N. Dao, N. Bardoliwalla, N. Devanathan, N. Dumai, N. Chauhan, O. Wahltinez, P. Botarda, P. Barnes, P. Barham, P. Michel, P. Jin, P. Georgiev, P. Culliton, P. Kuppala, R. Comanescu, R. Merhej, R. Jana, R. A. Rokni, R. Agarwal, R. Mullins, S. Saadat, S. M. Carthy, S. Cogan, S. Perrin, S. M. R. Arnold, S. Krause, S. Dai, S. Garg, S. Sheth, S. Ronstrom, S. Chan, T. Jordan, T. Yu, T. Eccles, T. Hennigan, T. Kocisky, T. Doshi, V. Jain, V. Yadav, V. Meshram, V. Dharmadhikari, W. Barkley, W. Wei, W. Ye, W. Han, W. Kwon, X. Xu, Z. Shen, Z. Gong, Z. Wei, V. Cotruta, P. Kirk, A. Rao, M. Giang, L. Peran, T. Warkentin, E. Collins, J. Barral, Z. Ghahramani, R. Hadsell, D. Sculley, J. Banks, A. Dragan, S. Petrov, O. Vinyals, J. Dean, D. Hassabis, K. Kavukcuoglu, C. Farabet, E. Buchatskaya, S. Borgeaud, N. Fiedel, A. Joulin, K. Kenealy, R. Dadashi, and A. Andreev. Gemma 2: Improving open language models at practical size, 2024. URL https://arxiv.org/abs/2408.00118. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762. Z. Wang, J. Li, H. Zhou, R. Weng, J. Wang, X. Huang, X. Han, J. Feng, C. Deng, and S. Huang. Investigating and scaling up code-switching for multilingual language model pre-training, 2025. URL https://arxiv.org/abs/2504.01801. C. Wendler, V. Veselovsky, G. Monea, and R. West. Do llamas work in english? on the latent language of multilingual transformers, 2024. URL https://arxiv.org/abs/2402.10588. B. Workshop, :, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. CastagnÃ©, A. S. Luccioni, F. Yvon, M. GallÃ©, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. LaurenÃ§on, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada, E. Levkovizh, E. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli, H. Elsahar, H. Benyamina, H. Tran, I. Yu, I. Abdulmumin, I. Johnson, I. Gonzalez-Dios, J. de la Rosa, J. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee, K. Almubarak, K. Chen, K. Lo, L. V. Werra, L. Weber, L. Phan, L. B. allal, L. Tanguy, M. Dey, M. R. MuÃ±oz, M. Masoud, M. Grandury, M. Å aÅ¡ko, M. Huang, M. Coavoux, M. Singh, M. T.-J. Jiang, M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis, O. Nguyen, O. Espejel, O. de Gibert, P. Villegas, P. Henderson, P. Colombo, P. Amuok, Q. Lhoest, R. Harliman, R. Bommasani, R. L. LÃ³pez, R. Ribeiro, S. Osei, 24 Trillion 7B Technical Report S. Pyysalo, S. Nagel, S. Bose, S. H. Muhammad, S. Sharma, S. Longpre, S. Nikpoor, S. Silberberg, S. Pai, S. Zink, T. T. Torrent, T. Schick, T. Thrush, V. Danchev, V. Nikoulina, V. Laippala, V. Lepercq, V. Prabhu, Z. Alyafeai, Z. Talat, A. Raja, B. Heinzerling, C. Si, D. E. Tasar, E. Salesky, S. J. Mielke, W. Y. Lee, A. Sharma, A. Santilli, A. Chaffin, A. Stiegler, D. Datta, E. Szczechla, G. Chhablani, H. Wang, H. Pandey, H. Strobelt, J. A. Fries, J. Rozen, L. Gao, L. Sutawika, M. S. Bari, M. S. Al-shaibani, M. Manica, N. Nayak, R. Teehan, S. Albanie, S. Shen, S. Ben-David, S. H. Bach, T. Kim, T. Bers, T. Fevry, T. Neeraj, U. Thakker, V. Raunak, X. Tang, Z.-X. Yong, Z. Sun, S. Brody, Y. Uri, H. Tojarieh, A. Roberts, H. W. Chung, J. Tae, J. Phang, O. Press, C. Li, D. Narayanan, H. Bourfoune, J. Casper, J. Rasley, M. Ryabinin, M. Mishra, M. Zhang, M. Shoeybi, M. Peyrounette, N. Patry, N. Tazi, O. Sanseviero, P. von Platen, P. Cornette, P. F. LavallÃ©e, R. Lacroix, S. Rajbhandari, S. Gandhi, S. Smith, S. Requena, S. Patil, T. Dettmers, A. Baruwa, A. Singh, A. Cheveleva, A.-L. Ligozat, A. Subramonian, A. NÃ©vÃ©ol, C. Lovering, D. Garrette, D. Tunuguntla, E. Reiter, E. Taktasheva, E. Voloshina, E. Bogdanov, G. I. Winata, H. Schoelkopf, J.-C. Kalo, J. Novikova, J. Z. Forde, J. Clive, J. Kasai, K. Kawamura, L. Hazan, M. Carpuat, M. Clinciu, N. Kim, N. Cheng, O. Serikov, O. Antverg, O. van der Wal, R. Zhang, R. Zhang, S. Gehrmann, S. Mirkin, S. Pais, T. Shavrina, T. Scialom, T. Yun, T. Limisiewicz, V. Rieser, V. Protasov, V. Mikhailov, Y. Pruksachatkun, Y. Belinkov, Z. Bamberger, Z. Kasner, A. Rueda, A. Pestana, A. Feizpour, A. Khan, A. Faranak, A. Santos, A. Hevia, A. Unldreaj, A. Aghagol, A. Abdollahi, A. Tammour, A. HajiHosseini, B. Behroozi, B. Ajibade, B. Saxena, C. M. Ferrandis, D. McDuff, D. Contractor, D. Lansky, D. David, D. Kiela, D. A. Nguyen, E. Tan, E. Baylor, E. Ozoani, F. Mirza, F. Ononiwu, H. Rezanejad, H. Jones, I. Bhattacharya, I. Solaiman, I. Sedenko, I. Nejadgholi, J. Passmore, J. Seltzer, J. B. Sanz, L. Dutra, M. Samagaio, M. Elbadri, M. Mieskes, M. Gerchick, M. Akinlolu, M. McKenna, M. Qiu, M. Ghauri, M. Burynok, N. Abrar, N. Rajani, N. Elkott, N. Fahmy, O. Samuel, R. An, R. Kromann, R. Hao, S. Alizadeh, S. Shubber, S. Wang, S. Roy, S. Viguier, T. Le, T. Oyebade, T. Le, Y. Yang, Z. Nguyen, A. R. Kashyap, A. Palasciano, A. Callahan, A. Shukla, A. Miranda-Escalada, A. Singh, B. Beilharz, B. Wang, C. Brito, C. Zhou, C. Jain, C. Xu, C. Fourrier, D. L. PeriÃ±Ã¡n, D. Molano, D. Yu, E. Manjavacas, F. Barth, F. Fuhrimann, G. Altay, G. Bayrak, G. Burns, H. U. Vrabec, I. Bello, I. Dash, J. Kang, J. Giorgi, J. Golde, J. D. Posada, K. R. Sivaraman, L. Bulchandani, L. Liu, L. Shinzato, M. H. de Bykhovetz, M. Takeuchi, M. PÃ mies, M. A. Castillo, M. Nezhurina, M. SÃ¤nger, M. Samwald, M. Cullan, M. Weinberg, M. D. Wolf, M. Mihaljcic, M. Liu, M. Freidank, M. Kang, N. Seelam, N. Dahlberg, N. M. Broad, N. Muellner, P. Fung, P. Haller, R. Chandrasekhar, R. Eisenberg, R. Martin, R. Canalli, R. Su, R. Su, S. Cahyawijaya, S. Garda, S. S. Deshmukh, S. Mishra, S. Kiblawi, S. Ott, S. Sang-aroonsiri, S. Kumar, S. Schweter, S. Bharati, T. Laud, T. Gigant, T. Kainuma, W. Kusa, Y. Labrak, Y. S. Bajaj, Y. Venkatraman, Y. Xu, Y. Xu, Y. Xu, Z. Tan, Z. Xie, Z. Ye, M. Bras, Y. Belkada, and T. Wolf. Bloom: 176b-parameter open-access multilingual language model, 2023. URL https://arxiv.org/abs/2211.05100. M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, and L. Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, 2022. URL https://arxiv.org/abs/2203.05482. W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, and H. Ma. Effective long-context scaling of foundation models, 2023. URL https://arxiv.org/abs/2309.16039. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mt5: massively multilingual pre-trained text-to-text transformer, 2021. URL https: //arxiv.org/abs/2010.11934. 25 Trillion 7B Technical Report J. Ye, X. Tao, and L. Kong. Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability, 2023. URL https://arxiv.org/abs/2306.06688. H. Yoo, C. Park, S. Yun, A. Oh, and H. Lee. Code-switching curriculum learning for multilingual transfer in llms, 2024a. URL https://arxiv.org/abs/2411.02460. K. M. Yoo, J. Han, S. In, H. Jeon, J. Jeong, J. Kang, H. Kim, K.-M. Kim, M. Kim, S. Kim, D. Kwak, H. Kwak, S. J. Kwon, B. Lee, D. Lee, G. Lee, J. Lee, B. Park, S. Shin, J. Yu, S. Baek, S. Byeon, E. Cho, D. Choe, J. Han, Y. Jin, H. Jun, J. Jung, C. Kim, J. Kim, J. Kim, D. Lee, D. Park, J. M. Sohn, S. Han, J. Heo, S. Hong, M. Jeon, H. Jung, J. Jung, W. Jung, C. Kim, H. Kim, J. Kim, M. Y. Kim, S. Lee, J. Park, J. Shin, S. Yang, J. Yoon, H. Lee, S. Bae, J. Cha, K. Gylleus, D. Ham, M. Hong, Y. Hong, Y. Hong, D. Jang, H. Jeon, Y. Jeon, Y. Jeong, M. Ji, Y. Jin, C. Jo, S. Joo, S. Jung, A. J. Kim, B. H. Kim, H. Kim, J. Kim, M. Kim, M. Kim, S. Kim, Y. Kim, Y. Kim, Y. Kim, D. Ko, D. Lee, H. Y. Lee, J. Lee, J. Lee, J. Lee, J. Lee, M. Y. Lee, Y. Lee, T. Min, Y. Min, K. Moon, H. Oh, J. Park, K. Park, Y. Park, H. Seo, S. Seo, M. Sim, G. Son, M. Yeo, K. H. Yeom, W. Yoo, M. You, D. Ahn, H. Ahn, J. Ahn, S. Ahn, C. An, H. An, J. An, S.-M. An, B. Byun, E. Byun, J. Cha, M. Chang, S. Chang, H. Cho, Y. Cho, D. Choi, D. Choi, H. Choi, M. Choi, S. Choi, S. Choi, W. Choi, S. Chun, D. Y. Go, C. Ham, D. Han, J. Han, M. Hong, S. B. Hong, D.-H. Hwang, S. Hwang, J. Im, H. J. Jang, J. Jang, J. Jang, S. Jang, S. Jang, J. Jeon, D. Jeong, J. Jeong, K. Jeong, M. Jeong, S. Jin, H. Jo, H. Jo, M. Jo, C. Jung, H. Jung, J. Jung, J. H. Jung, K. Jung, S. Jung, S. Ka, D. Kang, S. Kang, T. Kil, A. Kim, B. Kim, B. Kim, D. Kim, D.-G. Kim, D. Kim, D. Kim, E. Kim, E. Kim, G. Kim, G. R. Kim, H. Kim, H. Kim, I. Kim, J. Kim, J. Kim, J. Kim, M. Kim, M. Kim, P. H. Kim, S. Kim, S. Kim, S. Kim, S. Kim, S. Kim, S. Kim, S. Kim, T. Kim, W. Kim, Y. Kim, Y. J. Kim, Y. Kim, B. Kwon, O. Kwon, Y.-H. Kwon, A. Lee, B. Lee, C. Lee, D. Lee, D. Lee, H.-R. Lee, H. Lee, H. Lee, H. Lee, I. Lee, J. Lee, J. Lee, J. Lee, J. Lee, J. Lee, J. Lee, J. H. Lee, J. Lee, J. Lee, S. Y. Lee, S. Lee, S. Lee, S. Lee, W. Lee, Z. H. Lee, J. K. Lim, K. Lim, T. Lim, N. Na, J. Nam, K.-M. Nam, Y. Noh, B. Oh, J.-S. Oh, S. Oh, Y. Oh, B. Park, C. Park, D. Park, H. Park, H. T. Park, H. Park, J. Park, J. Park, J. Park, J. Park, M. Park, S. H. Park, S. Park, S. Park, T. Park, W. Park, H. Ryu, J. Ryu, N. Ryu, S. Seo, S. M. Seo, Y. Shim, K. Shin, W. Shin, H. Sim, W. Sim, H. Soh, B. Son, H. Son, S. Son, C.-Y. Song, C. Song, K. Y. Song, M. Song, S. Song, J. Wang, Y. Yeo, M. Y. Yi, M. B. Yim, T. Yoo, Y. Yoo, S. Yoon, Y. J. Yoon, H. Yu, U. S. Yu, X. Zuo, J. Bae, J. Bae, H. Cho, S. Cho, Y. Cho, T. Choi, Y. Choi, J. Chung, Z. Han, B. Heo, E. Hong, T. Hwang, S. Im, S. Jegal, S. Jeon, Y. Jeong, Y. Jeong, C. Jiang, J. Jiang, J. Jin, A. Jo, Y. Jo, H. Jung, J. Jung, S. Kang, D. H. Kim, G. Kim, H. Kim, H. Kim, H. Kim, H. Kim, H.-A. Kim, J. Kim, J.-H. Kim, J. Kim, J. Kim, J. Y. Kim, R. Y. Kim, S. Kim, S. Kim, S. Kim, S. Kim, S. Kim, T. Kim, N. Ko, B. Koo, H. Kwak, H. Kwon, Y. Kwon, B. Lee, B. W. Lee, D. Lee, E. Lee, E. Lee, H. G. Lee, H. Lee, H. Lee, J. Lee, J. Lee, J. Lee, J. Lee, J. Lee, M. Lee, N. Lee, S. Lee, S. Y. Lee, S. Lee, S. J. Lee, S. Lee, Y. Lee, Y. Lee, Y. Lee, Y. Lee, S. Li, T. Liu, S.-E. Moon, T. Moon, M.-L. Nihlenramstroem, W. Oh, Y. Oh, H. Park, H. Park, J. Park, N. Park, S. Park, J. Ryu, M. Ryu, S. Ryu, A. Seo, H. Seo, K. Seo, J. Shin, S. Shin, H. Sin, J. Wang, L. Wang, N. Xiang, L. Xiao, J. Xu, S. Yi, H. Yoo, H. Yoo, H. Yoo, L. Yu, Y. Yu, W. Yuan, B. Zeng, Q. Zhou, K. Cho, J.-W. Ha, J. Park, J. Hwang, H. J. Kwon, S. Kwon, J. Lee, S. Lee, S. Lim, H. Noh, S. Choi, S.-W. Lee, J. H. Lim, and N. Sung. Hyperclova technical report, 2024b. URL https://arxiv.org/abs/2404.01954. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830. B. Zhang and R. Sennrich. Root mean square layer normalization, 2019. URL https://arxiv. org/abs/1910.07467. Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, P. Damania, B. Nguyen, G. Chauhan, Y. Hao, A. Math26 Trillion 7B Technical Report ews, and S. Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https://arxiv.org/abs/2304.11277. Y. Zhao, Y. Qu, K. Staniszewski, S. Tworkowski, W. Liu, P. MiÅ‚os, Y. Wu, and P. Minervini. Analysing the impact of sequence composition on language model pre-training. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 78977912. Association for Computational Linguistics, 2024a. doi: 10.18653/v1/2024.acl-long.427. URL http://dx.doi.org/10.18653/v1/2024.ac l-long.427. Y. Zhao, W. Zhang, G. Chen, K. Kawaguchi, and L. Bing. How do large language models handle multilingualism?, 2024b. URL https://arxiv.org/abs/2402.18815. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023a. URL https://arxiv.org/abs/2311.07911. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023b. URL https://arxiv.org/abs/2311.07911. 27 Trillion 7B Technical Report A. Contribution Pretraining and Post-training Suyeong An, Seungtaek Choi, Sungjun Han, Joanna Hong2, Hyungguk Kim, Kyuseok Kim, Jamin Shin, Juyoung Suk, Wonsuk Yang Data Collection and Curation Juneyoung Park, Yusik Kim Infrastructure Sunghoon Kang B. Baseline Models Trillion-7B is compared against several widely recognized baseline models, all of which were accessed via the HuggingFace platform. Table 10 summarizes each baseline model used in our experiments. Model Name Provider Our Model Trillion-7B-preview EXAONE-3.5-7.8B-Instruct LG Gemma-2-9B-it Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Mistral-7B-Instruct-v0."
        },
        {
            "title": "Google\nMeta\nAliBaba\nMistral AI",
            "content": "Table 10 Baseline models utilized for comparative evaluation. All models were accessed through the HuggingFace platform. 2All work done while at Trillion labs. Currently at Google Deepmind. 28 Trillion 7B Technical Report C. Full Evaluation Results and Details Full Evaluation Results We detail full evaluation scores. Benchmark Trillion-7B EXAONE-3.5 Gemma-2-9b Llama-3.1 Qwen2.5 SOLAR-10.7B Mistral-7B HellaSwag TruthfulQA_mc1 TruthfulQA_mc2 ARC:C HAERAE KoBEST KMMLU MMLU GMMLU-en GMMLU-ko GMMLU-ja GMMLU-zh BBH xwinograd_en xwinograd_jp xwinograd_zh GPQA Average 58.94 36.10 54.10 54.44 80.02 79.61 48.09 63.52 67.75 60.75 60.75 59.50 41.94 87.78 79.98 73. 32.81 61.17 60.04 40.64 59.74 56.40 76.08 78.57 45.39 65.65 69.50 60.00 45.75 50.00 53.30 87.10 74.45 69. 38.61 60.63 59.72 42.96 60.09 62.97 68.01 79.98 46.66 72.24 76.25 64.25 66.50 63.75 28.77 89.55 80.92 68. 36.83 62.79 59.81 38.07 54.54 53.58 63.15 70.09 41.41 68.32 67.50 54.00 54.50 60.25 43.16 88.09 76.02 76. 30.58 58.78 61.97 47.74 64.72 52.99 65.17 79.24 50.15 74.23 77.25 59.25 65.75 68.75 53.68 85.63 72.89 81. 34.15 64.42 Table 11 General Reasoning and Knowledge 68.72 56.18 70.64 60.07 60.86 75.20 41.66 65.20 71.75 53.75 50.75 57.00 52. 87.35 72.58 74.60 28.35 61.62 65.79 42.47 59.41 58.11 47.75 66.50 33.59 61.84 65.50 43.00 50.00 47.25 45. 88.39 70.70 71.83 32.59 55.87 Benchmark Trillion-7B EXAONE-3.5 Gemma-2-9b Llama-3.1 Qwen2.5 SOLAR-10.7B Mistral-7B HumanEval MBPP Average 55.48 40.40 47.94 79.26 61.40 70.33 60.98 8.40 34. 67.68 39.20 53.44 81.71 51.00 66.35 34.76 29.40 32. 36.59 36.00 36.30 Table 12 Coding Benchmark Trillion-7B EXAONE-3.5 Gemma-2-9b Llama-3.1 Qwen2.5 SOLAR-10.7B Mistral-7B GSM8k MATH HRM8k Average 72.25 32.70 30.10 45.02 87.79 70.68 38.99 65.82 73.69 41.06 16.04 43. 74.98 38.30 26.77 46.68 88.86 71.50 41.51 67.29 62.93 14.38 20.68 32. 35.94 12.12 7.89 18.65 Table 13 Mathematical Reasoning Benchmark Trillion-7B EXAONE-3.5 Gemma-2-9b Llama-3.1 Qwen2.5 SOLAR-10.7B Mistral-7B IFEval koIFEval MT-Bench (1-10) KO-MT-Bench (1-10) LogicKor (1-10) Average 79.13 66.58 7.00 6.27 8.14 71.96 81.42 54.65 8.15 8.13 9.25 78.27 75.48 43.30 7.81 7.01 8. 70.06 74.93 36.07 6.32 4.27 6.45 56.28 75.85 48.55 7.86 6.31 7.99 69.20 51.61 26.12 6.76 2.89 1. 38.55 52.64 34.22 6.84 4.07 4.76 48.71 Table 14 Instruction Following and Chat. For computing the average, the scores on the 0-10 scale are multiplied by 10. 29 Trillion 7B Technical Report Prompts We present the prompts used for evaluation reported in Table 4. GSM8K/MATH prompt (CoT) Q: {question} Put your answer within boxed{}. Lets think step by step. Figure 7 Prompt for evaluating GSM8K (CoT) and MATH (CoT) MBPP prompt (CoT) You are an expert Python programmer, and here is your task: {text} Your code should pass these tests: {test_list[0]} {test_list[1]} {test_list[2]} Wrap your code in python . Lets reason step by step. Figure 8 Prompt for evaluating MBPP (CoT)"
        },
        {
            "title": "Humaneval",
            "content": "You are an expert Python programmer, and here is your task: {text} Wrap your code in python . Lets reason step by step. Figure 9 Prompt for evaluating Humaneval KoIFEval To evaluate whether language models adhere effectively to instructions presented in Korean, we constructed the Korean Instruction Following benchmark (KoIFEval) based on the IFEval framework proposed in (Zhou et al., 2023b). This benchmark dataset comprises 22 instruction types, selected from the original 25 of IFEval that are applicable to Korean. Each prompt in the benchmark includes one or more instructions and model performance is assessed at two levels: prompt-level, which evaluates whether the generated response satisfies all included instructions, and instruction-level, which assesses compliance with each instruction individually. The reported KoIFEval scores in this study represent the mean of these two metrics. All data samples are generated using ChatGPT-4o, each conditioned on selected instructions. We verify whether the generated prompts appropriately include the selected instructions by employing rule-based verification process. 30 Trillion 7B Technical Report KoIFEval á„€á…©á†¯á„ƒá…³á†« á„‰á…³á„á…¦á„‹á…µá„á…³ á„‹á…¯á„…á…µá„‹á…¥á„‰á…³ á„‰á…¥á†«á„‰á…®á„ƒá…³á†¯á„‹á…´ á„€á…§á†¼á„€á…µá„…á…§á†¨ á„’á…£á†¼á„‰á…¡á†¼á„‹á…³á†¯ á„‹á…±á„’á…¡á†« á„‰á…³á„‘á…©á„Žá…³ á„‰á…µá†·á„…á…µá„’á…¡á†¨á„Œá…¥á†¨ á„Œá…¥á†¸á„€á…³á†« á„‡á…¡á†¼á„‹á…¡á†«á„‹á…³á†¯ á„Œá…¦á„‰á…µ á„’á…¡á„‰á…¦á„‹á…­. á„ƒá…® á„€á…¢á„‹á…´ á„‰á…¥á„…á…© á„ƒá…¡á„…á…³á†« á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…³á†¯ á„Œá…¡á†¨á„‰á…¥á†¼á„’á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…³á†« á„‡á…¡á†«á„ƒá…³á„‰á…µ 6á„€á…¢á„‹á…´ á„‡á…§á†¯á„‘á…­á„…á…© á„€á…®á„‡á…®á†«á„’á…¢á„‹á…£á„’á…¡á†¸á„‚á…µ á„ƒá…¡. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦ á„‡á…¡á†«á„ƒá…³á„‰á…µ á„Œá…¦á„†á…©á†¨á„‹á…³á†¯ á„‘á…©á„’á…¡á†·á„’á…¢á„‹á…£ á„’á…¡á„†á…§, á„Œá…¦á„†á…©á†¨á„‹á…³á†« á„€á…µá„ˆá…³á†·á„‹á…´á„‰á…µ á„‹á…ª á„€á…¡á‡€á„‹á…µ á„‹á…µá„Œá…®á†¼ á„á…¥á†©á„‰á…¬ á„€á…ªá†¯á„’á…©á„…á…© á„€á…¡á†· á„Šá…¡á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. Figure 10 Example of evaluating Ko-IFEval Instruction Group Instruction Keywords Include Keywords Keywords Keyword Frequency Keywords Forbidden Words Keywords Letter Frequency Language Response Language Length Constraints Number Paragraphs Length Constraints Number Words Length Constraints Number Sentences Length Constraints Number Paragraphs + First Word in i-th Paragraph Detectable Content Postscript Detectable Content Number Placeholder Detectable Format Number Bullets Detectable Format Title Detectable Format Choose From Detectable Format Minimum Number Highlighted Section Detectable Format Multiple Sections Detectable Format JSON Format Combination Repeat Prompt Combination Two Responses Start with / End with End Checker Start with / End with Quotation Punctuation No Commas Description á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦ á„‡á…¡á†«á„ƒá…³á„‰á…µ {keywords}á„…á…³á†¯ á„‘á…©á„’á…¡á†·á„’á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦á„‰á…¥ á„ƒá…¡á†«á„‹á…¥ {keyword}á„‹á…µ(á„€á…¡) á„‡á…¡á†«á„ƒá…³á„‰á…µ á„Œá…¥á†¼á„’á…ªá†¨á„’á…µ {N}á„‡á…¥á†« {relation} á„ƒá…³á†¼á„Œá…¡á†¼á„’á…¢á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦ {forbidden_words}á„…á…³á†¯ á„‘á…©á„’á…¡á†·á„’á…¡á„Œá…µ á„†á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦á„‰á…¥ á„†á…®á†«á„Œá…¡ {letter}á„‹á…µ(á„€á…¡) á„‡á…¡á†«á„ƒá…³á„‰á…µ á„Œá…¥á†¼á„’á…ªá†¨á„’á…µ {N}á„‡á…¥á†« {relation} á„ƒá…³á†¼á„Œá…¡á†¼á„’á…¢á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„‹á…³á†¼á„ƒá…¡á†¸ á„Œá…¥á†«á„Žá…¦á„…á…³á†¯ {language}á„…á…© á„Œá…¡á†¨á„‰á…¥á†¼á„’á…¡á„‰á…¦á„‹á…­. á„ƒá…¡á„…á…³á†« á„‹á…¥á†«á„‹á…¥á„‚á…³á†« á„’á…¥á„‹á…­á†¼á„ƒá…¬á„Œá…µ á„‹á…¡á†­á„‰á…³á†¸á„‚á…µá„ƒá…¡. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦á„‚á…³á†« á„‡á…¡á†«á„ƒá…³á„‰á…µ Ná„€á…¢á„‹á…´ á„ƒá…¡á†«á„…á…¡á†¨á„‹á…µ á„‘á…©á„’á…¡á†·á„ƒá…¬á„‹á…¥á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„ƒá…¡á†«á„…á…¡á†¨á„ƒá…³á†¯á„‹á…³á†« á„†á…¡á„á…³á„ƒá…¡á„‹á…®á†« á„€á…®á„‡á…®á†«á„‰á…¥á†« * * *á„‹á…³á„…á…© á„€á…®á„‡á…®á†«á„’á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…³á†¯ {relation} {N}á„€á…¢ á„ƒá…¡á†«á„‹á…¥á„…á…© á„Œá…¡á†¨á„‰á…¥á†¼á„’á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…³á†¯ {N}á„€á…¢ {relation} á„†á…®á†«á„Œá…¡á†¼á„‹á…³á„…á…© á„Œá…¡á†¨á„‰á…¥á†¼á„’á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦á„‚á…³á†« á„‡á…¡á†«á„ƒá…³á„‰á…µ {N}á„€á…¢á„‹á…´ á„ƒá…¡á†«á„…á…¡á†¨á„‹á…µ á„‹á…µá†»á„‹á…¥á„‹á…£ á„’á…¡á„†á…§, á„ƒá…¡á†«á„…á…¡á†¨á„‹á…³á†« á„ƒá…® á„€á…¢á„‹á…´ á„Œá…®á†¯ á„‡á…¡á„á…®á†·á„‹á…³á„…á…©á„†á…¡á†« á„€á…®á„‡á…®á†«á„ƒá…¬á†¸á„‚á…µá„ƒá…¡. {i}á„‡á…¥á†«á„á…¢ á„ƒá…¡á†«á„…á…¡á†¨á„‹á…³á†« á„‡á…¡á†«á„ƒá…³á„‰á…µ {first_word}á„…á…© á„‰á…µá„Œá…¡á†¨á„’á…¢á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…´ á„†á…¡á„Œá…µá„†á…¡á†¨á„‹á…¦ á„‡á…¡á†«á„ƒá…³á„‰á…µ {postscript_marker}á„…á…© á„‰á…µá„Œá…¡á†¨á„’á…¡á„‚á…³á†« á„Žá…¥á†·á„‹á…¥á†« á„Žá…®á„€á…¡á„’á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦á„‚á…³á†« á„‡á…¡á†«á„ƒá…³á„‰á…µ á„Žá…¬á„‰á…© {N}á„€á…¢á„‹á…´ á„Œá…¡á„…á…µ á„‘á…­á„‰á…µá„Œá…¡á„€á…¡ á„‘á…©á„’á…¡á†·á„ƒá…¬á„‹á…¥á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦á„‚á…³á†« á„‡á…¡á†«á„ƒá…³á„‰á…µ á„Œá…¥á†¼á„’á…ªá†¨á„’á…µ {N}á„€á…¢ {relation} á„€á…³á†¯á„†á…¥á„…á…µ á„€á…µá„’á…© á„†á…©á†¨á„…á…©á†¨á„‹á…µ á„‘á…©á„’á…¡á†·á„ƒá…¬á„‹á…¥á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„†á…¡á„á…³á„ƒá…¡á„‹á…®á†« á„’á…§á†¼á„‰á…µá†¨ (á„‹á…¨: * á„‹á…µá„€á…¥á†ºá„‹á…³á†« á„’á…¡á†¼á„†á…©á†¨á„‹á…µá†¸á„‚á…µá„ƒá…¡.)á„‹á…³á†¯ á„‰á…¡á„‹á…­á†¼á„’á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦ á„‡á…¡á†«á„ƒá…³á„‰á…µ á„Œá…¦á„†á…©á†¨á„‹á…³á†¯ á„‘á…©á„’á…¡á†·á„’á…¢á„‹á…£ á„’á…¡á„†á…§, á„Œá…¦á„†á…©á†¨á„‹á…³á†« á„€á…µá„ˆá…³á†·á„‹á…´ á„‰á…µá„‹á…ª á„€á…¡á‡€á„‹á…µ á„‹á…µá„Œá…®á†¼ á„á…¥á†©á„‰á…¬ á„€á…ªá†¯á„’á…©á„…á…© á„€á…¡á†·á„Šá…¡á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„ƒá…¡á„‹á…³á†· á„‹á…©á†¸á„‰á…§á†« á„Œá…®á†¼ á„’á…¡á„‚á…¡á„…á…© á„‹á…³á†¼á„ƒá…¡á†¸á„’á…¡á„‰á…¦á„‹á…­: {options} á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…¦á„‰á…¥ á„Žá…¬á„‰á…© Ná„€á…¢á„‹á…´ á„‡á…®á„‡á…®á†«á„‹á…³á†¯ á„†á…¡á„á…³á„ƒá…¡á„‹á…®á†«á„‹á…³á†¯á„‰á…¡á„‹á…­á†¼á„’á…¡á„‹á…§ á„€á…¡á†¼á„Œá…©á„’á…¡á„‰á…¦á„‹á…­. á„‹á…¨: *á„€á…¡á†¼á„Œá…©á„ƒá…¬á†«á„‡á…®á„‡á…®á†«*. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…³á†« á„‡á…¡á†«á„ƒá…³á„‰á…µ {N}á„€á…¢á„‹á…´ á„‰á…¦á†¨á„‰á…§á†«á„‹á…³á„…á…© á„€á…®á„‰á…¥á†¼á„ƒá…¬á„‹á…¥á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„€á…¡á†¨ á„‰á…¦á†¨á„‰á…§á†«á„‹á…´ á„‰á…µá„Œá…¡á†¨á„‹á…³á†« {section_splitter} {á„‡á…¥á†«á„’á…©}. á„…á…© á„‘á…­á„‰á…µá„’á…¡á„‰á…¦á„‹á…­. á„Žá…®á†¯á„…á…§á†¨ á„Œá…¥á†«á„Žá…¦á„…á…³á†¯ JSON á„’á…§á†¼á„‰á…µá†¨á„‹á…³á„…á…© á„€á…¡á†·á„Šá…¡á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„†á…¥á†«á„Œá…¥ á„‹á…­á„Žá…¥á†¼á„‹á…³á†¯ á„‡á…§á†«á„€á…§á†¼ á„‹á…¥á†¹á„‹á…µ á„‡á…¡á†«á„‡á…©á†¨á„’á…¡á†« á„’á…®, á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…³á†¯á„Œá…¡á†¨á„‰á…¥á†¼á„’á…¡á„‰á…¦á„‹á…­. (á„‹á…­á„Žá…¥á†¼á„‹á…³á†¯ á„‡á…¡á†«á„‡á…©á†¨á„’á…¡á„€á…µ á„Œá…¥á†«á„‹á…¦á„‚á…³á†« á„‹á…¡á„†á…® á„†á…¡á†¯á„ƒá…© á„’á…¡á„Œá…µ á„†á…¡á„‰á…¦á„‹á…­. á„‡á…¡á†«á„‡á…©á†¨á„’á…¢á„‹á…£ á„’á…¡á†¯ á„‹á…­á„Žá…¥á†¼á„‹á…¦á„‚á…³á†« á„‹á…µ á„†á…®á†«á„Œá…¡á†¼á„‹á…µ á„‘á…©á„’á…¡á†·á„ƒá…¬á„Œá…µ á„‹á…¡á†­á„‰á…³á†¸á„‚á…µá„ƒá…¡.) á„ƒá…® á„€á…¢á„‹á…´ á„‰á…¥á„…á…© á„ƒá…¡á„…á…³á†« á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…³á†¯ á„Œá…¡á†¨á„‰á…¥á†¼á„’á…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…³á†« á„‡á…¡á†«á„ƒá…³á„‰á…µ 6á„€á…¢á„‹á…´ á„‡á…§á†¯á„‘á…­(******)á„…á…© á„€á…®á„‡á…®á†«á„’á…¢á„‹á…£ á„’á…¡á†¸á„‚á…µá„ƒá…¡. á„‹á…³á†¼á„ƒá…¡á†¸á„‹á…´ á„†á…¡á„Œá…µá„†á…¡á†¨á„‹á…³á†¯ á„‡á…¡á†«á„ƒá…³á„‰á…µ á„‹á…µ á„Œá…¥á†¼á„’á…ªá†¨á„’á…¡á†« á„†á…®á†«á„€á…® {end_phrase}á„…á…©á„á…³á‡€á„‚á…¢á„‰á…¦á„‹á…­. á„‹á…µá„’á…®á„‹á…¦á„‚á…³á†« á„ƒá…¡á„…á…³á†« á„ƒá…¡á†«á„‹á…¥á„€á…¡ á„„á…¡á„…á…¡á„‹á…©á„†á…§á†« á„‹á…¡á†« á„ƒá…¬á†¸á„‚á…µá„ƒá…¡. á„‹á…³á†¼á„ƒá…¡á†¸ á„Œá…¥á†«á„Žá…¦á„…á…³á†¯ á„á…³á†«á„„á…¡á„‹á…©á†·á„‘á…­á„…á…© á„€á…¡á†·á„Šá…¡á„‰á…¦á„‹á…­. á„‹á…³á†¼á„ƒá…¡á†¸ á„Œá…¥á†«á„Žá…¦á„‹á…¦á„‰á…¥ á„‰á…±á†·á„‘á…­(,)á„…á…³á†¯ á„‰á…¡á„‹á…­á†¼á„’á…¡á„Œá…µ á„†á…¡á„‰á…¦á„‹á…­. Table 15 We selected 22 instructions from the 25 proposed in (Zhou et al., 2023b), focusing on those that are adaptable to Korean-centric context. Each instruction was refined with Korean-oriented description, and the final set of 22 instructions was evenly distributed to ensure balanced coverage."
        }
    ],
    "affiliations": []
}