{
    "paper_title": "Sequence Matters: Harnessing Video Models in 3D Super-Resolution",
    "authors": [
        "Hyun-kyu Ko",
        "Dongheok Park",
        "Youngin Park",
        "Byeonghyeon Lee",
        "Juhee Han",
        "Eunbyung Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D super-resolution aims to reconstruct high-fidelity 3D models from low-resolution (LR) multi-view images. Early studies primarily focused on single-image super-resolution (SISR) models to upsample LR images into high-resolution images. However, these methods often lack view consistency because they operate independently on each image. Although various post-processing techniques have been extensively explored to mitigate these inconsistencies, they have yet to fully resolve the issues. In this paper, we perform a comprehensive study of 3D super-resolution by leveraging video super-resolution (VSR) models. By utilizing VSR models, we ensure a higher degree of spatial consistency and can reference surrounding spatial information, leading to more accurate and detailed reconstructions. Our findings reveal that VSR models can perform remarkably well even on sequences that lack precise spatial alignment. Given this observation, we propose a simple yet practical approach to align LR images without involving fine-tuning or generating 'smooth' trajectory from the trained 3D models over LR images. The experimental results show that the surprisingly simple algorithms can achieve the state-of-the-art results of 3D super-resolution tasks on standard benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets. Project page: https://ko-lani.github.io/Sequence-Matters"
        },
        {
            "title": "Start",
            "content": "Sequence Matters: Harnessing Video Models in 3D Super-Resolution Hyun-kyu Ko1*, Dongheok Park2*, Youngin Park3, Byeonghyeon Lee1, Juhee Han1, Eunbyung Park1,2 1Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea 2Department of Artificial Intelligence, Sungkyunkwan University, Suwon, Korea 3Visual Display Division, Samsung Electorics 4 2 0 2 8 1 ] . [ 2 5 2 5 1 1 . 2 1 4 2 : r Abstract 3D super-resolution aims to reconstruct high-fidelity 3D models from low-resolution (LR) multi-view images. Early studies primarily focused on single-image super-resolution (SISR) models to upsample LR images into high-resolution images. However, these methods often lack view consistency because they operate independently on each image. Although various post-processing techniques have been extensively explored to mitigate these inconsistencies, they have yet to fully resolve the issues. In this paper, we perform comprehensive study of 3D super-resolution by leveraging video super-resolution (VSR) models. By utilizing VSR models, we ensure higher degree of spatial consistency and can reference surrounding spatial information, leading to more accurate and detailed reconstructions. Our findings reveal that VSR models can perform remarkably well even on sequences that lack precise spatial alignment. Given this observation, we propose simple yet practical approach to align LR images without involving fine-tuning or generating smooth trajectory from the trained 3D models over LR images. The experimental results show that the surprisingly simple algorithms can achieve the state-of-the-art results of 3D super-resolution tasks on standard benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets. Project Page: https://ko-lani.github.io/Sequence-Matters"
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in 3D reconstruction from multi-view images, e.g., Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated outstanding performance across various tasks, such as novel view synthesis (Mildenhall et al. 2021; Muller et al. 2022; Chen et al. 2022; Fridovich-Keil et al. 2022; Kerbl et al. 2023) and surface reconstruction (Wang et al. 2021; Yariv et al. 2021, 2023; Guedon and Lepetit 2024; Huang et al. 2024; Fan et al. 2024). In addition, these techniques have proved highly effective in creating 3D scenes and assets when combined with the generative model approaches (Poole et al. 2022; Liu et al. 2023). The versatility of these methods and their ability to generate accurate and detailed 3D models have broadened their applicability to various tasks (Wang et al. 2024; Yu et al. 2024b). *These authors contributed equally. Corresponding author. Utilizing high-quality or high-resolution multi-view input images is crucial for obtaining high-fidelity 3D models from these techniques. However, meeting this requirement in real-world settings is often infeasible due to various constraints, e.g., equipment limitations or adverse environmental conditions. To overcome these challenges, several recent studies have investigated the 3D super-resolution task, which aims to generate high-fidelity 3D models from low-resolution multi-view images (Wang et al. 2022; Han et al. 2023; Yoon and Yoon 2023; Lin et al. 2024; Feng et al. 2024a; Lee, Li, and Lee 2024; Wu et al. 2024; Feng et al. 2024b; Shen et al. 2024; Yu et al. 2024a). The early approaches have utilized single-image super-resolution (SISR) models. They first upscale low-resolution (LR) input images to high-resolution (HR) images and then apply NeRF or 3DGS techniques to represent the 3D models. However, they face critical limitation; the generated HR images usually lack 3D consistency since the input view images are processed individually. Although numerous works have improved 3D consistency using refinement stages, these solutions introduced additional computational complexity and could not fully resolve the problems. recent work (Shen et al. 2024) has explored the use of Video Super-Resolution (VSR) models (Xu et al. 2024b) to improve the 3D consistency. Inspired by the latest studies showing video generative models can achieve highly accurate 3D spatial consistency across the generated video frames (Voleti et al. 2024; Zuo et al. 2024), it repurposes VSR models to upsample LR multi-view images. This approach first constructs low-resolution 3D representation using 3DGS from LR input images and then generates an LR video (a sequence of multi-view LR images) rendered from smooth camera trajectory. This VSR-friendly smooth LR video serves as the input for the VSR model, and it is upscaled to an HR video (a sequence of multi-view HR images) from which the HR 3D model is subsequently produced. While promising, the empirical evaluation has revealed certain limitations of this approach. The distribution shift between the training data (natural LR videos) and the testing data (the rendered LR videos from 3D models, e.g., 3DGS) negatively impacted the pre-trained VSR models. The rendered images from 3DGS frequently introduce stripy or blob-like artifacts, degrading the VSR models performance. Although fine-tuning the VSR models on the rendered images from 3DGS could mitigate the distribution mismatch issue, posed multi-view image data is not abundant compared to natural videos, which limits the generalization performance. In addition, it is time-consuming and computationally heavy since it requires training 3DGS to obtain 3D representations for rendering input images. Consequently, the up-to-date 3D super-resolution techniques utilizing the VSR models have yet to demonstrate superior results over those leveraging SISR models (Lim et al. 2017; Wang et al. 2018; Liang et al. 2021). In this work, we propose method that ensures the VSR models receive their desired input without fine-tuning them. We have made two critical observations regarding VSR models: 1. The artifacts introduced by the rendered images substantially comprise the performance, and 2. The VSR models maintain strong performance even when input videos do not adhere to smooth camera trajectories. Given these critical observations, we propose surprisingly simple yet effective algorithms to order training datasets into structured video-like sequences. These video-like sequences lead to improved VSR results, while eliminating the need for fine-tuning VSR models as they are composed of ground truth LR images, ensuring freedom from stripy or blob-like artifacts. The experimental results have shown that our proposed algorithms achieved state-of-the-art results on the NeRF synthetic and Mip-NeRF 360 datasets, underscoring their efficacy and robustness. Our key contributions are summarized below. We propose novel method that leverages VSR models to bridge the gap between low-resolution and highresolution images. By generating input video sequences that are sufficiently smooth and exhibit minimal artifacts, we optimize their suitability for VSR models. We propose surprisingly simple yet effective ordering algorithms, demonstrating superior performance compared to the existing prior arts. Our method achieves state-of-the-art performance on both object-level and scene-level datasets, including the NeRF Synthetic and Mip-NeRF 360 datasets, highlighting the robustness and effectiveness of our approach in both object and scene datasets."
        },
        {
            "title": "2 Related Work\nNovel View Synthesis Novel view synthesis (NVS) is the\ntask of synthesizing images from novel viewpoints given\nmulti-view images. With the rise of deep learning, Neural\nRadiance Fields (NeRF) (Mildenhall et al. 2021) achieved\nremarkable results by learning a continuous function of\nthe scene with MLP and can render the novel views with\na volumetric renderer. In contrast to NeRF and its vari-\nants (Mildenhall et al. 2021; Barron et al. 2021, 2022; MÂ¨uller\net al. 2022; Chen et al. 2022; Fridovich-Keil et al. 2022),\nwhich learns the implicit 3D representation of the scene, 3D\nGaussian Splatting (3DGS) (Kerbl et al. 2023) learns the\npoint cloud-based explicit 3D representation. Since 3DGS\nemploys explicit representation and renders images through\nrasterization, it achieves real-time rendering without com-\npromising the quality of rendered images. However, to learn",
            "content": "high-fidelity 3D representation, these neural fields require high-resolution images, which is not always guaranteed in real-world environments. In this work, we study 3D superresolution task, where we build 3D representations given the only LR images. 3D Super-resolution Despite the great success of 3D neural fields in various applications, it is challenging to reconstruct high-resolution (HR) radiance fields using lowresolution datasets. Recently, several studies (Wang et al. 2022; Feng et al. 2024a) have attempted to achieve 3D super-resolution using super-sampling techniques without the guidance of off-the-shelf models, such as image restoration or generative models. In contrast, another line of research (Han et al. 2023; Yoon and Yoon 2023; Lin et al. 2024; Lee, Li, and Lee 2024; Feng et al. 2024b; Yu et al. 2024a) has focused on improving the resolution of 3D representations with the aid of these established models. They utilize SISR models to upsample low-resolution images and incorporate additional modules or techniques to enhance multi-view consistency across the upsampled images. Recent works in this line (Han et al. 2023; Yoon and Yoon 2023; Feng et al. 2024b) utilize SISR models to upsample training datasets. On the other hand, another work (Lin et al. 2024) upsamples rendered images for fast inference speed. Additionally, other studies (Lee, Li, and Lee 2024; Yu et al. 2024a) employ latent diffusion model (LDM) (Rombach et al. 2022) and score distillation sampling (SDS) loss (Poole et al. 2022) to achieve 3D super-resolution. recent work (Shen et al. 2024), closely related to ours, leverages VSR model as an upsampler for the LR dataset. Unlike SISR models, which often do not consider other frames during super-resolution, VSR models reference adjacent frames, thereby enhancing multi-view consistency. Specifically, this work starts by training the 3DGS with LR images and renders LR video frames from the trained LR 3DGS. Subsequently, with the LR video rendered from the LR 3DGS, the VSR model generates the training dataset of HR 3DGS. However, the distribution of the LR dataset differs from that of the rendered LR video, thereby introducing stripy or blob-like artifacts in the upsampled video. Finetuning the VSR model with rendered LR videos can mitigate this distribution shift, but it is time-consuming process, as it involves extensive training and rendering of LR 3DGS to generate the fine-tuning dataset (LR rendered videos). In this work, we propose method that does not involve additional finetuning or training 3DGS on LR images to render smooth video. in image evolved from advancements Video Super-resolution Video super-resolution (VSR) has superresolution (Wang et al. 2018; Liang et al. 2021; Zhang et al. 2021; Chen et al. 2023; Tian et al. 2024), generating high-resolution video frames by utilizing information from adjacent frames to enhance the current frames resolution. BasicVSR (Chan et al. 2021) introduced bidirectional propagation approach to achieve balanced references from both directions and compute optical flow from features rather than images for more accurate alignment. BasicVSR++ (Chan et al. 2022) built on this by using Algorithm 1: Simple Greedy Algorithm Input: set of unordered images, = {Ij}N Output: An ordered sequence of images, 1: S1 = I1, {I1} 2: for 1 to 1 do Sj+1 = argmin 3: sim(Sj, Ik) j=1 IkI {Sj+1} 4: 5: end for 6: return stance, which increases significant computational complexity. Furthermore, the multi-view image dataset is less abundant than natural videos, limiting the generalization performance of finetuned VSR models. We have investigated these blob-like, stripy artifacts of the rendered images from the 3DGS models trained on LR multi-view images. These are primarily observed in the regions where high-fidelity information from HR images is lost in the LR images. The VSR models take the damaged images as inputs and upsample them, preserving or often magnifying the artifacts, which significantly degrades the output quality. As shown in Fig. 1, the regions with lost details in the LR images become severe artifacts in the upsampled images."
        },
        {
            "title": "3.2 A Simple Greedy Algorithm",
            "content": "In Sec. 3.1, we demonstrated the limitations of training 3DGS with low-resolution (LR) images to obtain smooth video. This section explores alternative approaches that exploit the raw unordered LR multi-view images to create video-like sequence. Determining the most desirable order for generating video-like sequences from unordered LR datasets is challenging due to the absence of clear criteria, such as how video-like sequence should be or what makes sequence good video for VSR models. Our objective is to arrange sequence of images to maximize the quality of the highresolution (HR) images produced by VSR models. However, the absence of ground-truth HR images, as defined by the problem, makes it infeasible to establish clear objective function. We consider rather simple approach: good video is sequence in which each frame is similar to its adjacent frames, ensuring smooth visual flow. Although these criteria were well-defined, finding the optimal sequence remains NP-hard due to the combinatorial nature of the problem. Our investigation, however, demonstrated that VSR models are sufficiently robust to nonoptimal sequences, effectively utilizing distant multi-view references for upsampling. Given the observation, we propose simple yet practical greedy algorithm (Alg. 1). Starting from an initial image S1, it repeatedly finds the next image by using the nearest neighbor based on the similarity score sim(, ). We explore two similarity measures, camera poses and visual features. By utilizing camera poses, we can spatially connect images that are close to each other to form video. Figure 1: Illustration of stripy or blob-like artifacts generated in VSR outputs of LR videos rendered from 3DGS. VSRRender shows the VSR outputs of the LR rendered videos, while VSR-GT displays the VSR outputs of the ground truth (GT) LR videos. second-order grid propagation, which extracts features through multiple stages and incorporates information from non-adjacent frames, enhancing robustness to occlusion. VRT (Liang et al. 2024) advanced VSR by combining recurrent model-based approaches with transformer structures and PSRT (Shi et al. 2022) proposed patch alignment, which aligns image patches rather than individual pixels, utilizing self-attention to enhance alignment and performance. Additionally, IART (Xu et al. 2024a) introduced neural network-based resampling strategy, employing sinusoidal positional encoding and transformer-based coordinate network to preserve high-frequency details and reduce spatial distortions. In this work, we harness the recent VSR models capability to improve the multi-view 3D super-resolution tasks."
        },
        {
            "title": "3.1 Rendering Artifacts\nIn most multi-view datasets and image acquisition scenarios,\nimages are hardly spatially ordered (except for a few cases,\ne.g., a monocular camera captures a sequence of images and\nrecords the time they are taken), which are unfavored for\nVSR models. A straightforward approach to obtaining the\nspatially ordered images from LR multi-view images would\ninvolve obtaining 3D representations with LR images, such\nas 3DGS, and rendering a smooth video from them (Shen\net al. 2024). However, this approach introduces a significant\nproblem due to the mismatch between the rendered images\nfrom the 3DGS trained on LR multi-view images and im-\nages from LR video datasets on which the VSR model was\ntrained, e.g., bicubic downsampled from HR videos. This\nmismatch often results in blob-like or stripy artifacts from\n3DGS, which degrades the performance of VSR models.\nShen et al. (2024) partially addressed this issue by finetuning\nthe VSR models on the images rendered from 3DGS. While\neffective, it demands training 3DGS for each training in-",
            "content": "Algorithm 2: Adaptive-length Subsequening Input: set of unordered images, = {Ij}N j=1 Output: Multiple ordered sequences, {S(j)}N 1: for 1 to do S(i) {Ij}N 1 = Ii, 2: for 1 to 1 do 3: S(i) j+1 = argmin 4: sim(S(i) j=1 {Ii} , Ik) j=1 IkI , S(i) j+1) < Ïµ then 5: if sim(S(i) S(i) = S(i) 1:j break end if {S(i) 6: 7: 8: 9: 10: 11: end for 12: return {S(j)}N end for j=1 j+1} // The length of S(i) becomes Although this approach is conceptually sound, it may lack generalizability across diverse datasets, such as Mip-NeRF 360 dataset, which is not object-centric (i.e., the images are not all focused on the same object). As an alternative, we explore the visual feature-based similarity. We evaluated multiple feature extractors (Lowe 2004) (Rosten and Drummond 2006) (Bay, Tuytelaars, and Van Gool 2006) (Calonder et al. 2010) and found that ORB (Oriented FAST and Rotated BRIEF) feature (Rublee et al. 2011) offers balance of computational efficiency and robustness in feature matching."
        },
        {
            "title": "3.3 Adaptive-Length Subsequence\nWhile promising, the proposed simple greedy algorithm\nfaces two challenges when connecting all images into a sin-\ngle video sequence. First, the resulting sequence often ex-\nhibits abrupt transition due to the inherent weaknesses of\ngreedy algorithms (illustrated in Fig. 3-(b)). For instance,\nthe nearest neighbor of Sk may have already been included\nin the processed list (S1, ..., Skâ1), forcing the selection of\na far-distant image.",
            "content": "Second, the results are highly influenced by the choice of the initial image S1. To address these challenges, we improve the algorithm by 1. stopping building sequence when the similarity score does not meet certain threshold and 2. creating multiple subsequences starting from each image in the dataset. Alg. 2 describes the detailed algorithm, and each subsequence S(j) is an ordered sequence starting from the initial image Ij, and each subsequence has different lengths. Finally, we apply VSR models to upsample the subsequence and aggregate the outputs to generate the final upsampled sequence ËI = { ËIj}N j=1 as follows, ËI = agg({ ËS(j)}N ËS(j) = VSR(S(j)), (1) where agg is an aggregate operator that takes multiple upsampled sequences as input and produces the final upsampled sequence. During aggregation, it removes redundant images, retaining only the image from the earliest subsequence (I = ËI = ). j=1), Each similarity measure has its own limitations. Pose similarity suffers from the different orientations of cameras. The proximity in camera position does not account for the fact that the cameras may be facing in different directions, leading to connections between unrelated images. On the other hand, feature similarity can lead to incorrect alignments (significantly different image pairs often have high similarity score). We observed that when dividing sequences into subsequences, pose and feature can complement each other. For example, we can use feature similarity for sim in line 4 and pose similarity for sim in line 5 of Alg. 2. Multi-threshold Subsequence Generation We generate subsequences from each multi-view image in the dataset based on uniform threshold. However, applying uniform threshold across all sequences can lead to inefficiencies. Setting high threshold imposes strict constraints, ensuring that only very closely related images are connected, which results in smoother sequences but shorter sequences. On the other hand, lower threshold ensures longer sequences, which often compromises the smoothness of the resulting sequences. To leverage both advantages, we introduce multithreshold generation approach as illustrated in Fig. 2. Initially, we apply high threshold for creating the subsequences, prioritizing the smoothest subsequences. These subsequences are then processed through the VSR model for upsampling. However, since not all images can be processed by high-threshold approach (note that VSR models require certain number of frames), we then lower the threshold in the next iteration, creating more relaxed and less smooth subsequences to include the remaining images. Please refer to more detailed algorithms in the Appendix."
        },
        {
            "title": "3.4 Training Objective\nWe use a VSR model to upsample LR images to enhance\nmulti-view consistency. However, generated high-frequency\ndetails are not always consistent across different views,\nwhich leads to degrade the quality of 3D reconstruction. Fol-\nlowing (Wang et al. 2022; Feng et al. 2024b), we use sub-\npixel constraints to regularize inconsistent high-frequency\ndetails. In practice, since bicubic interpolation is used to\ngenerate the low-resolution (LR) dataset, we also utilize\nbicubic interpolation when downsampling the sub-pixels.\nThe sub-pixel loss Lsp is LR 3DGS loss calculated between\nLR images and downsampled rendered images. Then, the fi-\nnal loss of our framework is expressed as below:",
            "content": "L = Î»renLren + (1 Î»ren)Lsp, (2) where Lren HR 3DGS loss. Please see Appendix. for more details."
        },
        {
            "title": "4.1 Setup\nDatasets We\nSynthetic Blender\n(Mildenhall et al. 2021) and the Mip-NeRF\ndataset\n360 dataset (Barron et al. 2022). The Blender dataset\nconsists of 8 synthetic object-centric scenes, with each",
            "content": "the NeRF use Figure 2: Overview of the proposed method. Given LR multi-view images, we generate subsequences (Sec. 3.3) starting from each image using simple greedy algorithm (Sec. 3.2) and these subsequences are bounded by multiple thresholds (Sec. 3.3). Finally, we train 3DGS model for 3D reconstruction using the upsampled HR images. In our experiments, the artifacts from compositing with white background significantly degraded the output quality (empirically, by about 0.3 to 0.4 on PSNR). Since most of the previous works have not released their code and do not mention the background issue, we are unable to determine which background they used for their metrics. Baseline Models As baseline, we examined NeRFSR (Wang et al. 2022), ZS-SRT (Feng et al. 2024a), CROP (Yoon and Yoon 2023), FastSR-NeRF (Lin et al. 2024), DiSR-NeRF (Lee, Li, and Lee 2024), SRGS (Feng et al. 2024b), GaussianSR, and SuperGaussian, following the metrics used by these models. Unfortunately, only two models, NeRF-SR and DiSR-NeRF, have provided their codes publicly. We have added three additional baseline methods: Bicubic, SwinIR, and Render-SR. For Bicubic and SwinIR, we upsampled LR images using bicubic interpolation and the SwinIR model, respectively. For RenderSR, we trained 3DGS with LR images in SuperGaussian manner and upsampled the rendered smooth video using PSRT (Shi et al. 2022). After the upsampling process, we used 3DGS for the 3D reconstruction of these models. For NeRF-SR, DiSR-NeRF, and the three additional baselines, rendering is conducted with white background by default. Note that our model and SuperGaussian are based on video super-resolution models, whereas GaussianSR, NeRF-SR, ZS-SRT, CROP, and SRGS are all based on single-image super-resolution models. Additionally, NeRF-SR, ZS-SRT, FastSR-NeRF, CROP, and DiSRNeRF are NeRF-based models, while SRGS, GaussianSR, SuperGaussian, and our model are based on 3DGS. Implementation Details We implement our method using the open-source 3D Gaussian Splatting code base. Following the 3DGS protocol, we train both coarse and fine 3DGS models for 30,000 iterations. To create the lowresolution (LR) dataset, we downsample the high-resolution (HR) dataset using bicubic interpolation with downscale factor of 4. As VSR backbone of our model, we employed PSRT (Shi et al. 2022). Please refer to the Appendix for further details. Figure 3: Illustration of subsequence generation. (a) is an unordered multi-view image dataset. (b) is the result of using simple greedy algorithm, Alg. 1. (c) highlights misalignments incurred by the algorithm, and we propose to split it into subsequences based on pose difference threshold (red dotted line) between consecutive frames. scene with resolution of 800 800. For our experiments, we downsampled the images with bicubic interpolation by factor of 4 to create low-resolution (LR) dataset (200 200). The Mip-NeRF 360 dataset contains 9 real-world scenes. The resolutions vary across the scenes, but each scene has higher resolution compared to the Blender dataset. We downsampled the dataset with bicubic interpolation by factor of 8 to create the LR dataset. Metrics Following the previous works, we evaluate the quantitative results using PSNR, SSIM, and LPIPS. Some previous works (Han et al. 2023; Lee, Li, and Lee 2024; Wu et al. 2024; Shen et al. 2024) emphasize the importance of perceptual metrics such as NIQE and LPIPS rather than fidelity metrics like PSNR. However, we prioritize the PSNR metric, as we regard the super-resolution task as subset of reconstruction tasks, where accurate reconstruction of the original image is crucial. Background Impact on Metrics When measuring metrics on the Blender dataset, we follow DiSR-NeRF and RaFE by using black background where the alpha channel value is 0, unlike NeRF-SR, which used white background. We observed that compositing with white background introduces black artifacts around the edges of the images, making it difficult to obtain accurate measurements. Table 1: The quantitative results of the proposed ordering algorithms. S: the simple greedy algorithm, ALS: the adaptive-length subsequence. and denote the PSNR of the left and right image in two image pairs from Fig. 4. Index 1 2 3 4 5 6 7 34.06 33.12 32.90 33.47 34.07 32.65 32.71 ALS 37.18 34.33 33.37 34.41 35.68 34.41 33.43 34.53 34.67 31.62 33.68 32.77 32.05 32. ALS 35.52 36.63 34.26 34.29 35.31 32.77 34.66 Table 2: The comparison of the proposed ordering algorithms in the NeRF-synthetic dataset. Chair Drums Ficus Hotdog Lego Materials Mic Ship 32.11 29.74 35.31 37.85 33.30 35.24 31.38 30.03 ALS 32.74 30.26 35.96 38.32 34.73 35.85 31.62 30.48 synthetic in Tab. 3. Due to the space constraints, we provided the quantitative results on the Mip-NeRF 360 dataset in the Appendix. The 3DGS-HR is the result of 3DGS trained on ground-truth HR images, which is considered as the upper bound. The values with were taken from the original papers as their codes are not publicly available. In all baseline comparisons, the best performance is highlighted in bold. Our methods (Ours-ALS) consistently outperformed other baseline models across all metrics. The Comparison against Render-SR and SuperGaussian, clearly highlights that the proposed methods do not suffer from stripy or bloblike artifacts. Qualitative Results While we showed improvements in various aspects, super-resolution tasks are notoriously challenging to improve quantitative metrics, such as PSNR. This is due to the most improvement comes from small parts of the images or high-frequency details, which conventional metrics do not accurately capture. We provide few qualitative results to demonstrate the effectiveness of the proposed algorithms (Fig. 5 and Fig. 6). We compared ours to the baseline models whose codes are available, such as NeRFSR and DiSR-NeRF. On the NeRF-synthetic dataset, we compared our model against Bicubic, SwinIR, Render-SR, NeRF-SR, and DiSR-NeRF. For the Mip-NeRF 360 dataset, we compared ours with Bicubic and SwinIR, as the NeRF models for NeRF-SR and DiSR-NeRF do not perform well on the Mip-NeRF 360 dataset. In both datasets, our model retains more high-frequency details and best reconstructs the ground truth. Figure 4: An example result from the simple greedy algorithm applied to the NeRF-synthetic dataset (Lego). Two neighboring images highlighted in red demonstrate abrupt transitions caused by misalignments."
        },
        {
            "title": "4.2 Results",
            "content": "The Effect of The Proposed Algorithms Fig. 4 shows resulting sequence of the proposed simple greedy algorithm (Lego, training images from the NeRF-synthetic dataset). We used the visual feature for similarity measure in this example, and two neighboring images with red color highlight the abrupt transition between two subsequent images due to the misalignment of the algorithm. We upsampled the ordered sequence using the VSR model and calculated the PSNR of the upsampled images highlighted in Fig. 4 with the ground-truth HR images. Tab. 2 shows the comparison between the simple greedy algorithm (S) and the adaptive-length subsequence algorithm (ALS). Since ASL offers smoother transitions and is more VSR-friendly, it allows the VSR model to reference more information from neighboring images. This significantly enhances the quality of the upsampled images, demonstrating the effectiveness of the approach. The Tab. 2 further shows the consistent improvement of ALS over the simple greedy algorithm. First we ran the simple greedy algorithm to order the sequence and find the image pairs that their angles are more than 45 degrees. For those non-smooth image pairs, we compared the performance of the proposed ordering algorithms. 3D Super-Resolution Results We provide the 3D superresolution results, where we measure the metrics on testview images rendered from the trained 3DGS models. The quantitative comparison with baseline models on NerfFigure 5: Qualitative results on the NeRF-synthetic dataset. The PSNR values against GT are embedded in each image patch. Ours have shown superior results than the existing baselines, especially for high-frequency details. Table 3: Comparison of different methods for 3D superresolution (4 1) in Blender Dataset. The numbers marked with are sourced from their respective paper, as the code is not available at this time. Bicubic SwinIR Render-SR NeRF-SR ZS-SRT CROP FastSR-NeRF DiSR-NeRF SRGS GaussianSR SuperGaussian Ours-ALS 3DGS-HR PSNR 27.56 30.77 28.90 28.46 29.69 30.71 30.47 26.00 30.83 28.37 28.44 31.41 33. SSIM LPIPS 0.1040 0.9150 0.0550 0.9501 0.0683 0.9346 0.0760 0.9210 0.0690 0.9290 0.0671 0.9459 0.0750 0.9440 0.1226 0.8898 0.0560 0.9480 0.0870 0.9240 0.0670 0.9459 0.0540 0.9520 0.0303 0.9695 Figure 6: Qualitative results on Mip-NeRF 360 dataset. The PSNR values against GT are embedded in each image patch. Ours have shown superior results than the existing baselines, especially for high-frequency details."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce simple yet practical algorithms to leverage the existing VSR models to improve the 3D super-resolution task. We proposed simple greedy algorithm to efficiently generate desirable sequence for the VSR models. We further improved the resulting sequence with the adaptive-length sequence technique. Using the proposed algorithms, we addressed the issue of stripy or bloblike artifacts caused by the trained 3D models on LR images and achieved promising performance without involving fine-tuning the VSR models. The experimental results demonstrated the effectiveness of the proposed algorithms, showing the state-of-the-art results on standard benchmark datasets. We believe this work paves the way for more robust and efficient 3D super-resolution techniques by rethinking how to leverage VSR models, offering valuable insights for future research and development in this field. In Proceedings of References Barron, J. T.; Mildenhall, B.; Tancik, M.; Hedman, P.; Martin-Brualla, R.; and Srinivasan, P. P. 2021. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, 58555864. Barron, J. T.; Mildenhall, B.; Verbin, D.; Srinivasan, P. P.; and Hedman, P. 2022. Mip-nerf 360: Unbounded antithe aliased neural radiance fields. IEEE/CVF conference on computer vision and pattern recognition, 54705479. Surf: Bay, H.; Tuytelaars, T.; and Van Gool, L. 2006. In Computer VisionECCV Speeded up robust features. 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part 9, 404417. Springer. Calonder, M.; Lepetit, V.; Strecha, C.; and Fua, P. 2010. Brief: Binary robust independent elementary features. In Computer VisionECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11, 778792. Springer. Chan, K. C.; Wang, X.; Yu, K.; Dong, C.; and Loy, C. C. 2021. Basicvsr: The search for essential components in In Proceedings of video super-resolution and beyond. the IEEE/CVF conference on computer vision and pattern recognition, 49474956. Chan, K. C.; Zhou, S.; Xu, X.; and Loy, C. C. 2022. Basicvsr++: Improving video super-resolution with enIn Proceedings of hanced propagation and alignment. the IEEE/CVF conference on computer vision and pattern recognition, 59725981. Chen, A.; Xu, Z.; Geiger, A.; Yu, J.; and Su, H. 2022. Tensorf: Tensorial radiance fields. In European conference on computer vision, 333350. Springer. Chen, X.; Wang, X.; Zhou, J.; Qiao, Y.; and Dong, C. 2023. Activating more pixels in image super-resolution In Proceedings of the IEEE/CVF conference transformer. on computer vision and pattern recognition, 2236722377. Fan, L.; Yang, Y.; Li, M.; Li, H.; and Zhang, Z. 2024. Trim 3D Gaussian Splatting for Accurate Geometry Representation. arXiv preprint arXiv:2406.07499. Feng, X.; He, Y.; Wang, Y.; Wang, C.; Kuang, Z.; Ding, J.; Qin, F.; Yu, J.; and Fan, J. 2024a. ZS-SRT: An efficient zeroshot super-resolution training method for Neural Radiance Fields. Neurocomputing, 590: 127714. Feng, X.; He, Y.; Wang, Y.; Yang, Y.; Kuang, Z.; Jun, Y.; Fan, J.; et al. 2024b. SRGS: Super-Resolution 3D Gaussian Splatting. arXiv preprint arXiv:2404.10318. Fridovich-Keil, S.; Yu, A.; Tancik, M.; Chen, Q.; Recht, B.; and Kanazawa, A. 2022. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 5501 5510. Guedon, A.; and Lepetit, V. 2024. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction In Proceedings of the and high-quality mesh rendering. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 53545363. Han, Y.; Yu, T.; Yu, X.; Wang, Y.; and Dai, Q. 2023. SuperNeRF: View-consistent Detail Generation for NeRF superresolution. arXiv preprint arXiv:2304.13518. Huang, B.; Yu, Z.; Chen, A.; Geiger, A.; and Gao, S. 2024. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 Conference Papers, 111. Kerbl, B.; Kopanas, G.; Leimkuhler, T.; and Drettakis, G. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph., 42(4): 1391. Lee, J. L.; Li, C.; and Lee, G. H. 2024. DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2056120570. Liang, J.; Cao, J.; Fan, Y.; Zhang, K.; Ranjan, R.; Li, Y.; Timofte, R.; and Van Gool, L. 2024. Vrt: video restoration transformer. IEEE Transactions on Image Processing. Liang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and Timofte, R. 2021. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, 18331844. Lim, B.; Son, S.; Kim, H.; Nah, S.; and Mu Lee, K. 2017. Enhanced deep residual networks for single image superresolution. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 136144. Lin, C.-Y.; Fu, Q.; Merth, T.; Yang, K.; and Ranjan, A. 2024. Fastsr-nerf: Improving nerf efficiency on consumer devices with simple super-resolution pipeline. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 60366045. Liu, R.; Wu, R.; Van Hoorick, B.; Tokmakov, P.; Zakharov, S.; and Vondrick, C. 2023. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, 92989309. Lowe, D. G. 2004. Distinctive image features from scaleInternational journal of computer viinvariant keypoints. sion, 60: 91110. Mildenhall, B.; Srinivasan, P. P.; Tancik, M.; Barron, J. T.; Ramamoorthi, R.; and Ng, R. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1): 99106. Muller, T.; Evans, A.; Schied, C.; and Keller, A. 2022. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4): 1 15. Poole, B.; Jain, A.; Barron, J. T.; and Mildenhall, B. 2022. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684 10695. Rosten, E.; and Drummond, T. 2006. Machine learning for In Computer VisionECCV high-speed corner detection. Yariv, L.; Hedman, P.; Reiser, C.; Verbin, D.; Srinivasan, P. P.; Szeliski, R.; Barron, J. T.; and Mildenhall, B. 2023. Bakedsdf: Meshing neural sdfs for real-time view synthesis. In ACM SIGGRAPH 2023 Conference Proceedings, 19. Yoon, Y.; and Yoon, K.-J. 2023. Cross-guided optimization of radiance fields with multi-view image super-resolution for high-resolution novel view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1242812438. Yu, X.; Zhu, H.; He, T.; and Chen, Z. 2024a. GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors. arXiv preprint arXiv:2406.10111. Yu, Z.; Chen, A.; Huang, B.; Sattler, T.; and Geiger, A. 2024b. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1944719456. Zhang, K.; Liang, J.; Van Gool, L.; and Timofte, R. 2021. Designing practical degradation model for deep blind image super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 47914800. Zuo, Q.; Gu, X.; Qiu, L.; Dong, Y.; Zhao, Z.; Yuan, W.; Peng, R.; Zhu, S.; Dong, Z.; Bo, L.; et al. 2024. Videomv: Consistent multi-view generation based on large video generative model. arXiv preprint arXiv:2403.12010. 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part 9, 430443. Springer. Rublee, E.; Rabaud, V.; Konolige, K.; and Bradski, G. 2011. ORB: An efficient alternative to SIFT or SURF. In Proceedings of the IEEE International Conference on Computer Vision, 25642571. IEEE. Shen, Y.; Ceylan, D.; Guerrero, P.; Xu, Z.; Mitra, N. J.; Wang, S.; and Frustuck, A. 2024. SuperGaussian: Repurposing Video Models for 3D Super Resolution. arXiv preprint arXiv:2406.00609. Shi, S.; Gu, J.; Xie, L.; Wang, X.; Yang, Y.; and Dong, C. 2022. Rethinking alignment in video super-resolution transformers. Advances in Neural Information Processing Systems, 35: 3608136093. Tian, Y.; Chen, H.; Xu, C.; and Wang, Y. 2024. Image Processing GNN: Breaking Rigidity in Super-Resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2410824117. Voleti, V.; Yao, C.-H.; Boss, M.; Letts, A.; Pankratz, D.; Tochilkin, D.; Laforte, C.; Rombach, R.; and Jampani, V. 2024. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. arXiv preprint arXiv:2403.12008. Wang, C.; Wu, X.; Guo, Y.-C.; Zhang, S.-H.; Tai, Y.-W.; and Hu, S.-M. 2022. Nerf-sr: High quality neural radiance fields using supersampling. In Proceedings of the 30th ACM International Conference on Multimedia, 64456454. Wang, P.; Liu, L.; Liu, Y.; Theobalt, C.; Komura, T.; and Wang, W. 2021. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689. Wang, S.; Leroy, V.; Cabon, Y.; Chidlovskii, B.; and Revaud, J. 2024. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2069720709. Wang, X.; Yu, K.; Wu, S.; Gu, J.; Liu, Y.; Dong, C.; Qiao, Y.; and Change Loy, C. 2018. Esrgan: Enhanced superresolution generative adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, 00. Wu, Z.; Wan, Z.; Zhang, J.; Liao, J.; and Xu, D. 2024. RaFE: arXiv preprint Generative Radiance Fields Restoration. arXiv:2404.03654. Xu, K.; Yu, Z.; Wang, X.; Mi, M. B.; and Yao, A. 2024a. Enhancing Video Super-Resolution via Implicit Resamplingbased Alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2546 2555. Xu, Y.; Park, T.; Zhang, R.; Zhou, Y.; Shechtman, E.; Liu, F.; Huang, J.-B.; and Liu, D. 2024b. VideoGigaGAN: Towards Detail-rich Video Super-Resolution. arXiv preprint arXiv:2404.12388. Yariv, L.; Gu, J.; Kasten, Y.; and Lipman, Y. 2021. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems, 34: 48054815. Appendix Flexibility with VSR Baseline Models One of the key advantages of our model is its ability to utilize any pre-trained VSR model as backbone, unlike SuperGaussian, which necessitates extensive and costly training to align the pre-trained data distribution with that of Gaussian splats. We demonstrate the robustness of our approach by integrating various pre-trained VSR models. Specifically, we evaluated three VSR modelsVRT, PSRT, and IARTalongside ablation studies on the simple greedy algorithm (S) and adaptive-length subsequence generation (ALS), comparing these with the original unordered sequence and SISR (where the VSR model processes each image individually without reference frames). Tab. 4 demonstrate that our method is flexible with the choice of VSR models. Implementation Details We implement our method using the open-source 3D Gaussian Splatting code base. Following the 3DGS protocol, we train the 3DGS model for 30,000 iterations. To create the low-resolution (LR) dataset, we downsample the highresolution (HR) dataset using bicubic interpolation with downscale factor of 4. To evaluate the generalization capabilities of Video Super-Resolution (VSR) models within our framework, we conduct ablation experiments on three VSR models: VRT, IART, and PSRT. By default, these models are pre-trained on the LR Vimeo-90K dataset, which is downsampled using bicubic interpolation. For the synthetic Blender dataset, we utilize nearest neighbor ordering based on ORB features and apply thresholds based on pose similarity. In contrast, for the Mip-NeRF 360 dataset, we employ nearest neighbor matching based on pose and apply thresholds using ORB features. This approach is justified by the object-centric nature of the Blender dataset and the non-object-centric characteristics of the MipNeRF 360 dataset. For the Adaptive-length Sequences, we set the three thresholds (angle between two camera positions) to 15, 30, 45 on both Blender dataset. We set two thresholds (the number of candidates by distances) to 30 and 50. Misalignment Error Misalignment errors  (Fig. 9)  occur due to inaccuracies in aligning frames, particularly when ORB features are used to link them. As the number of frames in sequence increases, the probability of connecting unrelated featuresespecially from temporally distant framesalso rises, as images that have been connected once cannot be reconnected. This misalignment may cause the model to rely on incorrect or irrelevant frame information during upsampling, thereby compromising the quality of the output. Our analysis reveals that misalignment errors escalate as the sequence length increases, since longer sequences provide more opportunities for feature mismatches. To quantify this, we first extract the cameras center position and direction (z-axis of the camera coordinates) in world coordinates Figure 7: Comparison with baselines. Figure 8: Misalignment trends within sequence. Figure 9: Misalignment Error. from the transformation matrix. We classify frame as misaligned if the consecutive images exhibit an angular difference greater than 45, when measured between the vectors drawn from the camera positions to the origin. To demonstrate the impact of misalignments, we conducted toy experiment. As illustrated in Fig. 8, misalignments tend to increase towards the end of the sequence, where unconnected images are forcefully connected, which significantly degrades the performance of our simple greedy algorithm. To quantify this degradation, we focus on the last 25% of the sequence generated by the greedy algorithm. To construct complete sequence, we apply our greedy algorithm starting from each image, collecting the last 25% of each sequence. These segments are then combined to form Table 4: Ablation comparison of Blender dataset (4 1) on various VSR models. SISR refers to Single-Image SuperResolution (single image VSR), refers to ordering by simple greedy algorithm (order: feature), and ALS refers to using adaptive-length subsequence (order: feature) with multi-threshold (threshold: pose). SISR ALS PSNR 31.20 31.25 31.37 VRT SSIM LPIPS 0.0567 0.9497 0.0557 0.9505 0.0544 0. PSNR 31.10 31.32 31.35 IART SSIM LPIPS 0.0590 0.9484 0.0550 0.9513 0.0548 0.9514 PSNR 31.10 31.35 31.41 PSRT SSIM LPIPS 0.0543 0.9516 0.0548 0.9513 0.0540 0.9520 Table 5: Impact of misalignment on 3D super-resolution. (last 25%) ALS PSNR 31.32 31.41 SSIM LPIPS 0.0552 0.9511 0.0540 0.9520 the final sequence, which is expected to exhibit high degree of misalignment. For any images that could not be included in the final sequence using this method, we directly use the upsampled images generated by the ALS (adaptive-length subsequence). This approach allows us to highlight the misalignment issues inherent in the greedy algorithm, in comparison to ALS (adaptive-length subsequence). The results are shown in Tab. 5. Per-object and Per-scene Quantitative"
        },
        {
            "title": "Results",
            "content": "We present per-object (synthetic Blender) and per-scene (Mip-NeRF 360) PSNR comparisons with different baseline models. Our method uses PSRT as our VSR backbone using adaptive-length subsequence (ALS) with multi-threshold. Note that value marked with is taken from the respective paper, as the code for the model is not available. The results can be found in Tab. 6 and Tab. 9. Multi-threshold Subsequence In the main paper, we introduced the concept of multithreshold subsequence generation. To summarize briefly, applying uniform threshold across all sequences can be inefficient due to varying image densities. strict threshold ensures that only closely situated images are connected, resulting in smoother trajectory. However, in sparsely populated regions, images are rarely connected with strict threshold, leading to loss of reference when upsampling. Conversely, loose threshold connects images even over greater distances, ensuring that most images are connected, but potentially sacrificing smoothness in densely populated regions. To address this, we propose multi-threshold subsequence generation method. We first upsample images using strict threshold to benefit from smoother trajectories in dense regions. Then, we gradually loosen the threshold to generate less smooth trajectories; this way, we can ensure that most images achieve the smoothest trajectory possible. Sub-pixel Loss and Final Loss In this section, we will provide detailed explanation of sub-pixel loss and final loss through equations. Let ËI, I, RHW 3 denotes rendered image from 3DGS, the upsampled image via VSR models, and the ground-truth image, respectively. H, refers to the height and width of the HR images (we omitted the image index for brevity). According to 3DGS, the objective is written as below, Lren = (1 Î»1)L1( ËI, I) + Î»1LDSSIM( ËI, I). We use Î»1 = 0.2 in all our experiments. L1(, ) is L1 loss and LDSSIM(, ) is defined as 1 SSIM(, ). (3) Lsp = (1Î»1)L1( ( ËI), (I))+Î»1LDSSIM( ( ËI), (I)), (4) where () is bicubic downsampling. And the final loss is defined as, = Î»renLren + (1 Î»ren)Lsp. (5) We use Î»ren = 0.6 for Blender dataset and Î»ren = 0.4 for Mip-NeRF 360 dataset. ORB Feature Matching In the main paper, we discussed how ORB features are suitable for ordering unordered multi-view images into video sequences. In this section, we provide detailed explanation of computing similarity scores using the ORB feature. The similarity score sim(, ) between two images Ii and Ij is computed as follows, S(Ii, Ij) = 1 (Ii, Ij) (cid:88) dist(fi,k, fj,l), (k,l)M (Ii,Ij ) where (Ii, Ij) is set of indices for matched descriptors between Ii and Ij, fi is the ORB feature extracted from the image Ii, and fi,k {0, 1}P is binary feature vector for k-th keypoint in the image Ii, and = 256. The Hamming distance dist(, ) between two binary descriptors fi,k and fj,l is calculated as follows, dist(fi,k, fj,l) = (cid:88) b=1 (fi,k,b fj,l,b) , where fi,k,b {0, 1} denotes b-th bits of descriptors fi,k, and is XOR operator. Descriptors between images Ii and Ij are then matched using bidirectional matching approach, also known as Table 6: Per-object PSNR comparison on the synthetic Blender dataset (4 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS). Bicubic PSRT (SISR) SwinIR+3DGS Render-SR NeRF-SR DiSR-NeRF CROP Ours-S Ours-ALS HR-3DGS chair 29.02 30.94 31.02 30.23 30.16 27.55 31.53 31.33 31.36 35.79 drums 23.75 25.56 25.48 24.04 23.46 22.63 24.99 25.58 25.65 26.14 ficus 28.24 33.49 32.49 28.63 26.64 25.64 31.50 33.71 33.69 34. hotdog 31.86 35.82 35.60 33.78 34.40 30.07 35.62 35.95 36.18 37.72 lego materials 27.46 32.20 32.05 29.23 29.13 26.43 32.88 32.98 33.03 35.77 26.47 30.06 29.58 27.34 28.02 24.71 29.16 30.09 30.17 29.97 mic 27.97 31.75 31.75 30.53 27.25 26.49 31.76 31.91 31.93 35.36 ship 25.71 28.96 28.20 27.35 26.61 24.47 28.23 29.26 29.26 30.89 average 27.56 31.10 30.77 28.90 28.21 26.00 30.71 31.35 31.41 33. Table 7: Per-object SSIM comparison on the synthetic Blender dataset (4 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS). Bicubic PSRT (SISR) SwinIR+3DGS Render-SR NeRF-SR DiSR-NeRF CROP Ours-S Ours-ALS HR-3DGS chair 0.9194 0.9475 0.9469 0.9432 0.9366 0.9035 0.9513 0.9538 0.9539 0.9874 drums 0.9003 0.9386 0.9412 0.9163 0.9019 0.8618 0.9236 0.9391 0.9405 0.9544 ficus 0.9430 0.9762 0.9760 0.9539 0.9026 0.9117 0.9709 0.9779 0.9777 0.9872 hotdog 0.9526 0.9721 0.9728 0.9677 0.9629 0.9332 0.9725 0.9738 0.9744 0. lego 0.9059 0.9572 0.9601 0.9379 0.9292 0.8875 0.9641 0.9646 0.9649 0.9828 materials 0.9220 0.9544 0.9558 0.9322 0.9319 0.8816 0.9468 0.9541 0.9555 0.9603 mic 0.9481 0.9732 0.9747 0.9671 0.9432 0.9335 0.9740 0.9747 0.9750 0.9914 ship 0.8291 0.8688 0.8731 0.8582 0.8357 0.8053 0.8637 0.8724 0.8741 0.9067 average 0.9150 0.9516 0.9501 0.9346 0.9180 0.8898 0.9459 0.9513 0.9520 0.9694 Table 8: Per-object LPIPS comparison on the synthetic Blender dataset (4 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS). Bicubic PSRT (SISR) SwinIR+3DGS Render-SR NeRF-SR DiSR-NeRF CROP Ours-S Ours-ALS HR-3DGS chair 0.0899 0.0553 0.0577 0.0563 0.0687 0.0943 0.0567 0.0478 0.0478 0.0117 drums 0.1106 0.0609 0.0565 0.0743 0.1091 0.1429 0.0856 0.0585 0.0576 0.0371 ficus 0.0619 0.0237 0.0221 0.0396 0.1014 0.0905 0.0317 0.0216 0.0216 0.0116 hotdog 0.0768 0.0421 0.0401 0.0462 0.0591 0.1001 0.0481 0.0395 0.0388 0.0199 lego 0.1272 0.0595 0.0498 0.0691 0.0976 0.1378 0.0496 0.0470 0.0465 0. materials 0.0892 0.0480 0.0420 0.0597 0.0770 0.1293 0.0622 0.0488 0.0464 0.0341 mic 0.0626 0.0254 0.0203 0.0312 0.0805 0.0751 0.0251 0.0240 0.0233 0.0060 ship 0.2136 0.1567 0.1511 0.1698 0.1984 0.2106 0.1776 0.1509 0.1501 0.1063 average 0.1040 0.0544 0.0550 0.0683 0.0990 0.1226 0.0671 0.0547 0.0540 0.0303 cross-checking. This process ensures robust matching by retaining only mutual best matches. Specifically, for each descriptor fi,k in image Ii, the descriptor fj,l in image Ij with the smallest Hamming distance is identified, and vice versa. Only pairs (fi,k, fj,l) that are mutual best matches are retained. The set of indices of these matched descriptor pairs is denoted as (Ii, Ij)."
        },
        {
            "title": "H Temporal Consistency",
            "content": "Temporal consistency is crucial for our task, as VSR models rely on the coherence of neighboring frames to achieve better performance. By leveraging this temporal relationship, our method ensures 3D spatial consistency improving the 3D reconstruction quality. To evaluate temporal coherence, we use the Frechet Video Distance (FVD) metric on the Blender dataset, where smooth video trajectories from the test split serve as ground truth. As shown in Tab. 13, our method (Ours-S and Ours-ALS) achieves the lowest FVD scores among all compared methods, demonstrating superior temporal consistency in video metrics. This improvement is attributed to the structured video-like sequences generated by our ordering algorithms, which enhance both frame-to-frame coherence and spatial reconstruction accuracy. Table 9: Per-scene PSNR comparison on the Mip-NeRF 360 dataset (8 2). Ours-ALS refers to our method using adaptivelength subsequencing (ALS). Bicubic SwinIR + 3DGS Ours-S Ours-ALS HR-3DGS bicycle 24.02 24.54 24.42 24.50 24.41 flowers 21.24 21.18 21.13 21.17 20. garden 25.14 25.81 26.04 25.99 26.58 stump 26.30 26.38 26.40 26.46 26.28 treehill 22.25 22.16 22.26 22.26 22.27 room counter 28.15 30.47 28.71 31.30 28.96 31.47 28.90 31.52 29.12 31.52 kitchen 28.23 29.82 30.79 30.73 31.57 bonsai 30.21 31.26 31.69 31.68 32. average 26.22 26.80 27.02 27.02 27.19 Table 10: Per-scene SSIM comparison on the Mip-NeRF 360 dataset (8 2). Ours-ALS refers to our method using adaptivelength subsequencing (ALS). Bicubic SwinIR + 3DGS Ours-S Ours-ALS HR-3DGS bicycle 0.6401 0.6810 0.6752 0.6783 0.7007 flowers 0.5321 0.5498 0.5512 0.5503 0.5445 garden 0.6648 0.7259 0.7476 0.7462 0. stump 0.7324 0.7468 0.7481 0.7467 0.7571 treehill 0.5880 0.6020 0.6048 0.6028 0.6269 room counter 0.8573 0.8877 0.8837 0.9063 0.8936 0.9123 0.8918 0.9123 0.9144 0.9263 kitchen 0.8128 0.8724 0.9071 0.9062 0.9325 bonsai 0.8980 0.9235 0.9328 0.9323 0.9465 average 0.7348 0.7657 0.7747 0.7741 0. Table 11: Per-scene LPIPS comparison on the Mip-NeRF 360 dataset (8 2). Ours-ALS refers to our method using adaptive-length subsequencing (ALS). Bicubic SwinIR + 3DGS Ours-S Ours-ALS HR-3DGS bicycle 0.3688 0.3220 0.3344 0.3261 0.3230 flowers 0.4315 0.4065 0.4091 0.4062 0.4188 garden 0.3469 0.2784 0.2613 0.2607 0.1777 stump 0.3334 0.3098 0.3142 0.3117 0. treehill 0.4391 0.4116 0.4162 0.4134 0.3997 room counter 0.2671 0.2750 0.2216 0.2354 0.2074 0.2218 0.2104 0.2218 0.1800 0.1931 kitchen 0.2598 0.1973 0.1536 0.1542 0.1136 bonsai 0.2392 0.2035 0.1927 0.1925 0.1758 average 0.3290 0.2873 0.2790 0.2774 0.2550 Table 12: Comparison with baseline models in Mip-NeRF 360 dataset (8 2). Bicubic SwinIR SRGS Ours 3DGS-HR PSNR 26.22 26.80 26.88 27.02 27.19 SSIM 0.7349 0.7657 0.7670 0.7747 0.7710 LPIPS 0.3290 0.2873 0.2860 0.2790 0.2802 Table 13: Temporal Consistency and Spatial Quality Metrics on Blender Dataset. Method Bicubic SwinIR Render-SR NeRF-SR DiSR-NeRF Ours-S Ours-ALS FVD 195 113 134 169 304 110 109 PSNR 27.56 30.77 28.90 28.21 26.00 31.35 31.41 Figure 10: Qualitative results on the NeRF-synthetic dataset. The PSNR values against GT are embedded in each image patch. Ours have shown superior results than the existing baselines, especially for high-frequency details."
        }
    ],
    "affiliations": [
        "Department of Artificial Intelligence, Sungkyunkwan University, Suwon, Korea",
        "Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea",
        "Visual Display Division, Samsung Electronics"
    ]
}