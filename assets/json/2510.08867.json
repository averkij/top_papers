{
    "paper_title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review",
    "authors": [
        "Gaurav Sahu",
        "Hugo Larochelle",
        "Laurent Charlin",
        "Christopher Pal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. We introduce ReviewerToo, a modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. We validate ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR 2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing a paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. Our analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, we propose guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. Our work provides a foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing."
        },
        {
            "title": "Start",
            "content": "ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review REVIEWERTOO: SHOULD AI JOIN THE PROGRAM COMMITTEE? LOOK AT THE FUTURE OF PEER REVIEW Gaurav Sahu1,2,3 Hugo Larochelle1 Laurent Charlin1,2,5 Christopher Pal1,3,4,5,6 1Mila Quebec AI Institute, 2HEC Montreal, 3ServiceNow Research 4Universite de Montreal, 5Canada CIFAR Chair, 6Polytechnique Montreal"
        },
        {
            "title": "ABSTRACT",
            "content": "Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. We introduce ReviewerToo, modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. We validate ReviewerToo on carefully curated dataset of 1,963 paper submissions from ICLR 2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. Our analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, we propose guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. Our work provides foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing."
        },
        {
            "title": "INTRODUCTION",
            "content": "Major machine learning conferences such as ICLR and AAAI now receive (tens of) thousands of submissions every year, creating enormous pressure on the peer-review process. To cope with this scale, several venues begin experimenting with large language models (LLMs) as review assistants.1 These early deployments demonstrate both promise (Liu & Shah, 2023; Petrescu & Krishen, 2022; Checco et al., 2021) and risk (Liang et al., 2024c; Latona et al., 2024): LLMs can generate consistent and scalable reviews, but they also produce superficial or misleading assessments that may erode confidence in the process.2 Despite their visibility, such deployments remain one-off interventions constrained by conference timelines and are difficult to study in reproducible manner. central challenge is that most reported outcomes of AI-assisted peer review remain anecdotal (even if large-scale), offering little scientific basis for best practices. Without systematic and reproducible evaluations, the community cannot determine where AI helps, where it harms, or how it might be responsibly 1E.g. AAAI 2026 (https://aaai.org/conference/aaai/aaai-26/instructions-for-aaai-26-reviewers/) 2https://www.nature.com/articles/d41586-025-00894-7 1 5 2 0 2 ] . [ 1 7 6 8 8 0 . 0 1 5 2 : r ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Figure 1: Performance of Different Reviewers on the ICLR-2k dataset. integrated into review pipelines. Progress requires platforms that support controlled, transparent, and repeatable experimentsmuch like benchmarks have done for other areas of machine learning. In this work, we introduce ReviewerToo, modular framework for studying and deploying AI-assisted peer review that can complement human judgment with systematic and consistent assessments. ReviewerToo enables researchers to design, test, and compare AI reviewers under standardized conditions, and it is partially or fully adopted in real conference workflows. We take inspiration from recent work on LLM-based social simulations (Anthis et al., 2025), which propose using language models as proxies for human subjects in the study of collective behavior. In this spirit, ReviewerToo treats peer review as socio-technical process shaped by diverse reviewer roles, biases, and interactions. By instantiating reviewer personassuch as empiricists, theorists, and pedagogical reviewerswe use LLMs to simulate distinct reviewing philosophies and study how they align with human decisions. We validate ReviewerToo on curated dataset of ICLR 2025 submissions obtained from the OpenReview platform. This dataset consists of 1,963 papers sampled to balance acceptance and rejection decisions while preserving diversity across score ranges and decision categories. We refer to this dataset as the ICLR-2k dataset. This scope enables controlled yet realistic evaluation of AI-assisted reviewing at scale, yielding both methodological and empirical insights. Our analysis shows that ReviewerToo produces reasonable reviews, surfaces systematic biases across personas, and highlights dimensions where AI reviewers are particularly strong (e.g., fact-checking, literature coverage) or weak (e.g., assessing methodological novelty and theoretical contributions). These findings provide an evidence-based perspective on the opportunities and limitations of AI in peer review, moving beyond anecdote toward systematic study. In sum, this paper makes three contributions: 1. We conceptualize peer review as socio-technical process and propose ReviewerToo, modular framework for evaluating AI-assisted reviewing under controlled and transparent conditions. 2. We present large-scale empirical study on the ICLR-2k dataset, analyzing the performance and biases of different reviewer personas and their alignment with meta-review outcomes. 2 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review 3. We derive set of guidelines for integrating AI into peer-review pipelines, informed by both quantitative performance metrics and qualitative analyses of reviewer behavior. Together, these contributions provide foundation for systematic and consistent integration of AI into the peer-review process."
        },
        {
            "title": "2 BACKGROUND",
            "content": "Challenges in Traditional Peer Review Peer review has long faced well-documented challenges, including reviewer fatigue, bias, and low inter-reviewer agreement (Cortes & Lawrence, 2021; Adam, 2025). Large-scale experiments at venues such as NeurIPS revealed that acceptance decisions can vary almost randomly (Cortes & Lawrence, 2021) and exhibit low inter-rater reliability. Combined with the rapid growth of submissions at top conferences (e.g., 11k+ and 25k+ at ICLR 2025 and NeurIPS 2025, respectively) and widespread reports of reviewer fatigue, scalability has become pressing concern (Adam, 2025). AI and LLMs as Peer-Review Assistants Recent advances in natural language processing (NLP) and large language models (LLMs) have spurred interest in using AI to assist peer review (Liang et al., 2024c; Tyser et al., 2024). Publishers and researchers have piloted systems for automated review generation, citation verification, fact-checking, and meta-review synthesis (Hossain et al., 2024). Surveys suggest that substantial minority of reviewers are already using AI tools to speed up report writing, with some conferences estimating that 1520% of reviews contain AI-assisted content (Latona et al., 2024; Naddaf, 2025). Empirical studies show mixed results: while LLM-generated reviews can be helpful according to authors, they also risk hallucinations and lack more in-depth judgment (Liang et al., 2024b). Ongoing work thus emphasizes AI-in-the-loop designs, where models act as assistants for specific subtasks rather than as replacements for expert judgment (Idahl & Ahmadi, 2024; Liang et al., 2024a). Despite this growing body of research, relatively little attention has been paid to modeling reviewer diversity itself. In practice, reviewers embody distinct philosophiessome emphasizing theoretical rigor, others empirical robustness, clarity of exposition, or long-term vision. Prior work on LLM-based social simulation shows that instantiating multiple role-specific agents can capture diverse perspectives in human decision processes (Sahakyan & AlShebli, 2025; Anthis et al., 2025). Inspired by this, we introduce REVIEWERTOO, modular framework that explicitly models plurality of reviewer personas. By simulating heterogeneous reviewer roles (e.g., theorist, empiricist, or pedagogical), our framework enables analysis not only of predictive accuracy against ground truth but also of the structure of inter-reviewer disagreement. This pluralistic design contributes both to practical peer-review augmentation and to the scientific understanding of reviewer dynamics."
        },
        {
            "title": "3 SYSTEM OVERVIEW",
            "content": "REVIEWERTOO is modular framework for studying and deploying AI-assisted peer review. It proceeds in structured sequence: ingestion of the submitted manuscript, construction of targeted literature review, generation of reviews by diverse panel of reviewer agents, drafting of consolidated rebuttal by an author agent, and finally metareview that integrates the full record. The full workflow is shown in Figure 2. We adopt single-turn interaction protocol, in which each agent contributes once per stage (with the option for reviewers to issue one short post-rebuttal response). This choice reflects the conventions of many academic conferences, where reviewers typically provide single review, authors submit one rebuttal, and only limited clarifications follow. While multi-turn deliberation could in principle be supported, our design prioritizes realism, and tractability, as LLMs have been shown to lose context in long, multi-turn discussions (Laban et al., 2025). We now discuss the different agents in our framework. 3 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Figure 2: The ReviewerToo Framework. paper passes through literature, reviewer, author, and metareviewer agents. The module design allows both humans and LLMs to participate at each stage, with optional feedback loops for iterative improvement. The Literature Review Agent. For literature review, we use LitLLM (Agarwal et al., 2025), retrievaland-summarization agent proposed for automated literature review. Given manuscript, LitLLM generates search queries and submits them to Semantic Scholar. Retrieved papers are ranked using debate-based method introduced in the original work, after which the top-k candidates are selected. The agent summarizes these papers into concise literature review that grounds subsequent reviewer, author, and metareviewer agents. Reviewer Agents. Each reviewer agent receives the manuscript (converted to Markdown), an optional literature summary, and prompts encoding specific reviewing persona or evaluation criteria. Reviewers generate structured assessments along axes commonly used in conference rubrics: paper summary, explicit strengths and weaknesses, novelty, soundness, experimental validity, results/discussion quality, organization/presentation, and impact. For each dimension, reviewers must ground their judgments in either (i) explicit spans of the manuscript, or (ii) retrieved evidence from the literature summary. Additionally, the reviewer agent is also grounded in the official ICLR reviewer guidelines.3 If no grounding can be located, the agent is rerun with stricter retrieval until verifiable justification is produced. At the end of their report, reviewers provide categorical recommendation from {Accept (Oral), Accept (Spotlight), Accept (Poster), Reject, Desk Reject}. To surface complementary strengths and disagreements, we instantiate diverse panel of personas. For brevity, we only mention subset here, and we refer the reader to Table 5 for more details: Stance-based personas: critical (reject-biased), permissive (accept-biased), and default (neutral). Epistemic personas: e.g., theorist (formal emphasis), empiricist (experimental rigor), pedagogical (clarity and exposition), and pragmatist (practical impact). Stylized personas: caricatured reviewer archetypes such as visionary (long-term potential), probabilistic (uncertainty reasoning), and impact-driven (field-level relevance). Author Agent. The author agent takes the manuscript, the full set of reviewer reports, and the literature summary as input. It generates consolidated rebuttal that addresses the most severe criticisms, clarifies potential misunderstandings, and, when appropriate, proposes concrete revisions such as releasing code or adding ablation studies. The rebuttal must explicitly cite either reviewer claims or relevant literature, ensuring that clarifications are verifiable rather than speculative. Rebuttals are stored per review configuration to facilitate analysis. 3https://iclr.cc/Conferences/2025/ReviewerGuide 4 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Table 1: Main Results on ICLR-2k Dataset. Best results (per block, per column) are in bold."
        },
        {
            "title": "Agent",
            "content": "5-way 2-way ELO R A R A"
        },
        {
            "title": "Critical\nPermissive\nDefault",
            "content": "Meta w/ Top-3 Meta w/ Top-3+Base Meta (all) Majority (Top-3) Majority (all) Average (all) XGBoost (Bert) XGBoost (tfidf) Bert FT Human (avg) Human (top-1%) 31.0 27.1 32.5 12.5 10.5 26. 28.6 26.7 32.1 30.5 30.7 32.5 12.9 17.4 25.7 15.2 31."
        },
        {
            "title": "ReviewerToo Agents",
            "content": "22.6 21.0 20.6 11.9 7.5 21.8 27.1 19.5 28.1 25.9 27.9 22.7 45.9 51.7 50.7 49.6 19.1 43. 49.5 30.4 52.5 52.0 49.2 42.2 72.1 72.9 69.7 76.8 73.3 72.4 74.2 74.7 79.3 73.1 75.1 68."
        },
        {
            "title": "Supervised Baselines",
            "content": "14.4 17.4 22.5 13.7 29.7 20.0 21.4 22.4 37.6 56.4 59.0 70.4 84.2 85.2 93. 24.0 23.0 22.5 17.0 16.8 24.5 32.1 26.1 32.4 28.5 30.0 26.4 20.0 21.4 26.4 12.4 30. 72.1 68.9 66.1 50.1 50.3 71.5 76.3 63.6 80.1 70.0 75.2 65.0 55.8 63.7 29.1 84.1 91. 71.9 68.3 65.3 35.0 32.4 70.5 73.4 57.3 79.3 69.8 75.1 60.3 71.9 70.3 67.6 53.6 46.8 70. 74.8 61.2 81.8 71.2 75.1 64.8 44.3 58.2 43.24 83.8 90.4 55.8 63.7 65.43 83.9 92. 1463 1256 1558 423 880 1136 1329 1154 1657 540 Metareviewer Agent. The metareviewer integrates all reviewer reports, the author rebuttal, and any optional post-rebuttal reviewer responses. Its role is to synthesize consensus while controlling for reviewer disagreement and bias. Concretely, it: (1) summarizes reviewer stances and scores pre-rebuttal, (2) identifies common strengths and weaknesses, (3) evaluates rebuttal effectiveness, (4) tracks stance shifts post-rebuttal, and (5) highlights lingering concerns or unresolved disagreements. To avoid being swayed by overly negative or idiosyncratic reviewers, the metareviewer includes factchecking module. This module verifies reviewer-stated claims against both the manuscript and the literature summary, discarding unsupported statements. Each fact is also assigned significance score, indicating its weight in shaping the final decision. The final metareview thus reflects combination of consensus synthesis, rebuttal analysis, and fact-weighted evidence assessment. Notably, the metareviewer agent is also grounded in the official Area Chair guidelines from the ICLR.4 We include the implementation details of the system in Appendix and include our prompts in the supplementary material. 4https://iclr.cc/Conferences/2025/ACGuide 5 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review"
        },
        {
            "title": "4.1 DATASETS",
            "content": "All experiments are conducted on dataset derived from ICLR 2025 submissions to the OpenReview platform. From the full pool of 11,672 submissions, we curate stratified subset of 1,963 papers, which we refer to as the ICLR-2k dataset. We focus on this subset for all reported results, as it enables balanced coverage of decision categories and controlled ablation studies. Each submission is annotated with the official conference decision, which serves as the ground truth for both five-way and binary evaluations. We consider five categories: Accept (Oral), Accept (Spotlight), Accept (Poster), Reject, and Desk Reject. Withdrawn papers are merged into the Reject category, while Desk Reject is preserved separately to test the systems ability to detect incomplete or rule-violating submissions. To ensure representativeness, we first rank all 12k submissions (213 orals, 380 spotlights, 3115 posters, 7894 rejected, 70 desk rejected) by average reviewer score and then sample proportionally across the score distribution. Specifically: (i) For Accepted (Poster), we select 300 of 3,115 submissions, sampling evenly from the top, middle, and bottom thirds of the ranked list; (ii) For Reject, we include 500 of 5,019 submissions using the same stratification, and add 500 randomly sampled withdrawn papers; (iii) For Accepted (Oral), Accepted (Spotlight), and Desk Reject, we include all available cases. This design balances acceptance and rejection while preserving diversity across decision types and score ranges."
        },
        {
            "title": "4.2 BASELINES",
            "content": "We evaluate REVIEWERTOO on multiple baselines ranging from trivial heuristics to human-derived signals. Our baselines fall into four groups: (1) Supervised Baselines. We include three supervised baselines where we train an XGBoost classifier with TF-IDF features (XGBoost (tfidf)), and XGBoost classifier with frozen BERT embeddings as features (XGBoost (bert)), and BERT classifier finetuned on the dataset (Bert FT). (2) Single-agent reviewers. To isolate the contribution of structured protocols, we ablate on the different conditioning variables: (a) ϕ: represents reviewer agent without any conditioning on conference instructions, or literature review, or rebuttal. It only takes as input the manuscript and responds according to its base personality imbued in the system prompt. (b) CI: adds ICLR reviewers guidelines for the reviewer agents and area chair guidelines for metareviewer agent in addition to the persona-specific instructions. (c) RB: extends conference conditioning with an author rebuttal and one round of reviewer response. (d) LitLLM: further incorporates external retrieval and summarization (LitLLM). This sequence reflects controlled ablation from bare-bones to fully contextualized reviewing. (3) Reviewer ensembles. We test whether diversity and aggregation improve fidelity. (a) Majority vote: across all reviewer personas. (b) Extremal ensembles: combining permissive and critical personas to probe systematic bias. (c) Metareviewer aggregation: synthesizing all reviews and rebuttals into calibrated consensus. Together, these baselines span uninformed heuristics, isolated reviewer agents, structured multi-agent protocols, ensembles, and human artifacts. This progression allows us to evaluate two complementary questions: (1) how effective LLMs are as reviewers in absolute terms, and (2) which design choices most narrow the gap to human decision-making."
        },
        {
            "title": "4.3 EVALUATION METRICS\nWe assess REVIEWERTOO along multiple axes that capture predictive accuracy, reviewer agreement, review\nquality, and rebuttal helpfulness. We evaluate alignment with real conference decisions by measuring both\nthe 5-way classification performance (Oral, Spotlight, Poster, Reject, Desk Reject) and the binary Accep-\nt/Reject task; we report macro-averaged Precision, Recall, and F1, with macro averaging across classes c.\nWe also report overall Accuracy, and False Positive Rate (for binary task). We quantify consistency among\nreviewers and with the metareviewer. For two reviewers i, j, we compute Cohen’s κ",
            "content": "6 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Review quality. We assess the quality of review text through LLM-based judgments. We conduct largescale pairwise comparisons where held-out LLM acts as the judge. For each paper, two reviews are shown side by side and evaluated along five axes: (1) Depth of engagement with the papers methodology and arguments; (2) Actionability, i.e., whether weaknesses are paired with concrete suggestions and is the feedback constructive; (3) Summary, i.e. whether the agent identified strengths and weakness of the paper in balanced manner; (4) Clarity, reflecting readability, structure and professionalism; and (5) Helpfulness of the review to the author. The judge assigns win, loss, or draw outcome to each review. From the full set of pairwise outcomes we compute an ELO rating per system using the standard logistic update. We include the complete protocol in Appendix B. Table 2: Ablation Results for conference instructions (CI), LitLLM, and rebuttal (RB). F1 FPR FNR ELO Theorist (ϕ) Agent (Configuration) 67.4 69.9 71.9 63.8 63.6 1371 1422 1463 1299 +CI +CI+LitLLM +CI+RB +CI+LitLLM+RB"
        },
        {
            "title": "5 RESULTS\nReviewer Performance. Table 1 reports the per-\nformance of REVIEWERTOO agents, supervised\nbaselines, and human references. Figure 1 visual-\nizes the F1 Scores from that table. Among single-\nagent reviewers, the EMPIRICIST, PEDAGOGICAL,\nand THEORIST personas achieve the strongest over-\nall performance on the 5-way classification task,\nwith the EMPIRICIST attaining the highest precision\n(32.5) while THEORIST secures the best F1 score\nIn terms of binary accept/reject accuracy,\n(22.6).\nthese reviewers approach 70% accuracy, narrowing\nthe gap to human baselines. Ensembling further\nboosts performance: majority voting improves sta-\nbility, while the metareviewer aggregation (“Meta\n(all)”) outperforms both single-agent and majority ensembles across all metrics, reaching 32.1 precision,\n32.4 recall, and 28.1 F1 on the 5-way task, and 81.8% accuracy on the binary task. This model also achieves\nthe strongest ELO score of 1657, surpassing all other agents and aligning closely with the top-1% human\nbaseline.",
            "content": "+CI +CI+LitLLM +CI+RB +CI+LitLLM+RB +CI +CI+LitLLM +CI+RB +CI+LitLLM+RB 1345 1256 1216 1103 1122 1502 1427 1558 1316 1332 84.0 43.8 45.4 75.7 73.5 96.1 73.6 76.0 85.6 88. 70.3 45.4 48.7 78.9 76.3 69.1 64.8 70.7 59.7 60.4 75.5 70.5 68.2 61.9 63.0 80.5 90.9 88.8 49.9 49.8 55.9 86.3 87.1 47.8 48.5 42.0 71.3 68.4 46.3 44. Pedagogical (ϕ) Empiricist (ϕ) Error Analysis via Confusion Matrices. Figures 36 present normalized confusion matrices for each agent. We observe consistent difficulty in distinguishing between oral and spotlight accept decisions across nearly all personas, indicating sensitivity to fine-grained acceptance tiers. Expectedly, the PERMISSIVE persona over-predicts acceptance decisions, while the CRITICAL persona strongly favors rejection. By contrast, the EMPIRICIST and PEDAGOGICAL show more balanced error profiles, though they still overpredict rejections relative to ground truth. These error modes highlight both biases induced by personas and systematic challenges in conference calibration. Reviewer Agreement. We quantify inter-reviewer consistency using Cohens κ (Figure 4). Agreement levels vary substantially across personas: MAJORITY and DEFAULT show moderate alignment (κ 0.5), while PERMISSIVE and CRITICAL show near-zero or even negative agreement with other reviewers, underscoring their extremal tendencies. Human reviewers exhibit low to moderate agreement with LLM reviewers (κ 0.10.2), consistent with known levels of disagreement in real peer review. Ensembles such as MAJORITY and META yield higher agreement with ground truth, validating aggregation as stabilizing mechanism. Review Quality and ELO. Beyond predictive accuracy, we assess review quality through LLM-based pairwise judgments, aggregated with ELO ratings. The META (ALL) agent again dominates, achieving the highest ELO of 1657. Among single-agent reviewers, the EMPIRICIST leads with 1558, while the PEDAGOGICAL and THEORIST trail but still outperform most supervised baselines. Interestingly, human reviewers exhibit striking disparity: the average human ELO is very low (540), yet the top 1% of human reviewers ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review 8 Figure 3: Confusion Matrices for binary Classification Task ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review achieve an ELO of 1316, comparable to the best single-agent reviewers. At the same time, both average and top-1% humans maintain strong binary F1 performance (83.8 and 90.4, respectively). This suggests that while humans are highly effective at holistic judgments of paper quality, the textual reviews they produce are often less helpful by the criteria used in our LLM-as-judge frameworkparticularly with respect to actionability and usefulness to authors. These findings reinforce the potential of structured protocols, diversity, and meta-reviewing to not only improve decision alignment but also to generate more constructive review text. Comparison with Supervised Baselines. Supervised baselines such as XGBoost and BERT fine-tuning achieve modest predictive performance, with binary F1 scores ranging from 43.2 to 65.3. In contrast, REVIEWERTOO agents not only match or exceed these baselines in decision accuracy but also generate substantive reviews that achieve competitive or superior ELO ratings. Unlike humans, who remain strong on both axesachieving high binary F1 performance while also producing text that can be judged for quality supervised models cannot bridge the gap between decision fidelity and helpful feedback. This underscores the unique advantage of structured reviewer agents in combining predictive alignment with author-facing utility. Ablation Studies. Table 2 examines the impact of conditioning variables. Removing conference instructions systematically reduces both F1 and ELO, indicating their critical role in reviewer fidelity. For example, the EMPIRICIST with full conditioning (+CI+LitLLM) achieves the highest ELO (1558), whereas ablations removing literature grounding drop performance sharply (e.g., ELO 1332). Interestingly, PEDAGOGICAL shows the highest raw F1 score (75.5) in its base persona, though its ELO is lower, suggesting less consistent quality under comparative evaluation. Overall, ablations confirm the complementary value of structured conference context, literature retrieval. We also see that the F1 score for all the reviewer agents drops post rebuttal. Upon closer inspection, we note that this is due to an increase in false positives post discussion while both false negatives and true negatives decrease (compare CI+LitLLM v/s CI+LiLLM+RB rows in Table 2). This may hint towards sycophantic tendencies of LLMs to accept papers after reading the rebuttals, as from their point-of-view, they come from real humans. The increase in false positives can also be visually seen confusion matrices for different reviewer agents before and after discussion (see Figure 3 v/s Figure 5 & Figure 6 v/s Figure 7). Qualitative Examples. We show the reviews generated for two papers, one from the accept category (PaperID: 6Mxhg9PtDE) and one from the reject category (PaperID: j7b4mm7Ec9). For each paper, we compare the reviews written by the top-3 reviewer agents (pedagogical, empiricist, and theorist) with human review (we only include one human review but the links to the full threads are provided). Full examples are provided in Appendix C. For the accepted paper, we observe that the reviewer agents not only reach the same accept decision as the human reviewers but also produce comprehensive and well-structured feedback, covering key aspects such as novelty, clarity, and experimental rigor. This consistency suggests that, for strong submissions, the agents are capable of recognizing high-quality research and articulating reasoned justifications similar to human reviewers. The rejected paper presents more nuanced case. Interestingly, while all human reviewers gave positive (accept) scores, the meta-reviewers final decision was to reject the paper. Notably, some of our reviewer agents also assigned reject decisions for similar reasons as the metareviewer (revolving around the true robustness of the proposed approach). This alignment with the final decision, despite divergence from the human reviewers initial scores, indicates that the agents can independently identify underlying weaknesses in paper that might be overlooked in human assessments. Such behavior demonstrates the potential of AI reviewer agents to provide balanced, critical evaluations and to meaningfully contribute to peer review deliberations. Summary. Taken together, these results demonstrate that REVIEWERTOO can reasonably approximate human-level decision making, especially when aggregating diverse reviewers through metareview protocols. Single-agent personas exhibit distinctive biases, but structured ensembles yield both higher predictive accuracy and higher judged review quality. Agreement analysis highlights persistent reviewer variance, mirroring 9 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review human peer review. Finally, ablation studies confirm that conference conditioning, rebuttals, and literature access are each essential to closing the gap with human reviewers."
        },
        {
            "title": "6 DISCUSSION",
            "content": "Our experiments on the ICLR-2k dataset provide first large-scale analysis of how LLM-based reviewer agents perform relative to humans, supervised classifiers, and ensemble protocols. The results reveal both opportunities and limitations of using AI in peer review. Here, we synthesize these findings into broader lessons and propose practical guidelines for integrating AI into peer-review pipelines. AI reviewers approximate but do not replace humans. The results show that single-agent reviewer personas achieve accuracy close to 70% on the binary accept/reject task, narrowing the gap with human baselines. However, their five-way performance remains substantially lower, and confusion matrices highlight consistent difficulty in distinguishing fine-grained acceptance tiers (e.g., oral vs. spotlight). This suggests that LLM reviewers can approximate coarse-grained decision making, but conference-level calibration still requires human expertise. Importantly, human reviewers maintain higher binary F1 scores, underscoring their ability to holistically evaluate paper quality. Ensembles and metareviewing stabilize and improve fidelity. Our ensemble protocols consistently outperform single-agent reviewers, with the META (ALL) agent achieving the strongest results across accuracy, F1, and ELO. Aggregating multiple perspectives reduces individual biases (e.g., permissive vs. critical personas) and yields more reliable decision-making. This mirrors existing human peer review, where program committees rely on multiple reviews and meta-review synthesis to mitigate individual noise. Our findings indicate that metareviewing is crucial design principle for AI-assisted peer review. Quality of review text remains challenge. ELO ratings highlight that while reviewer agents can generate more constructive feedback than supervised baselines, the quality of their review text is not always aligned with human expectations. Average human reviews perform poorly under ELO, suggesting that even human-authored text often fails on criteria such as actionability and helpfulness to authors. At the same time, the top 1% of human reviewers achieve high ELO, showing that exemplars exist. These results caution that AI reviews should be seen as complementsproviding structured, constructive feedbackrather than replacements for nuanced human judgment. Rebuttals introduce sycophancy risks. Ablation studies reveal that performance systematically drops after rebuttal rounds, potentially due to sycophantic tendencies of LLMs: they may defer excessively to rebuttals without maintaining independent judgment. This highlights need for careful design of how LLM reviewers handle author feedback. Safeguards, such as explicit calibration instructions or adversarial prompting, may be required to prevent performance degradation in rebuttal phases. Reviewer agreement mirrors human inconsistency. Pairwise Cohens κ shows that LLM reviewers vary substantially in their agreement, with some personas (e.g., permissive, critical) diverging strongly from others. This echoes longstanding challenges in human peer review, where reviewer disagreement is common. Our findings suggest that AI reviewers will not eliminate variance in peer review but can be structured to reduce it through ensembles and consensus protocols."
        },
        {
            "title": "6.1 GUIDELINES FOR INTEGRATING AI INTO PEER REVIEW\nFrom these quantitative and qualitative findings, we propose a set of guidelines for integrating AI into peer-\nreview pipelines:",
            "content": "1. Use AI reviewers as complements, not replacements. LLM reviewers can provide scalable, structured feedback and approximate decision accuracy, but final judgments should remain with humans, particularly for borderline and high-stakes decisions. 10 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review 2. Prioritize ensemble protocols. Single-agent reviewers exhibit strong biases; aggregation through majority voting or metareviewing produces more reliable and fair outcomes. AI systems in peer review should default to ensemble-based designs. 3. Incorporate structured conditioning. Conference-specific guidelines, literature retrieval, and rebuttal phases each add value, but must be carefully balanced to avoid overfitting or sycophancy. Conditioning improves fidelity, but uncritical incorporation of rebuttals can degrade performance. 4. Evaluate not just accuracy, but also review quality. Our ELO analysis highlights that decision fidelity alone is insufficient; reviews must also be actionable and useful to authors. AI reviewers should be explicitly optimized for feedback quality as well as predictive accuracy. 5. Human-AI collaboration as the design goal. The stark gap between average and top-1% human reviewers suggests role for AI in raising the floor: providing consistent, constructive baseline reviews that can complement and support human judgment, rather than competing with it. 6. Mitigate bias and disagreement through protocol. Extremal personas can systematically overor under-predict acceptance. Careful design of reviewer ensembles and meta-review synthesis is essential to reduce variance and ensure fairness in outcomes."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Peer review is central to scientific publishing but remains plagued by inconsistency, subjectivity, and scalability limits. We introduced REVIEWERTOO, modular framework for AI-assisted peer review that leverages structured reviewer personas, ensemble protocols, and systematic evaluation. On the ICLR-2k dataset, LLM reviewers approached human-level decision accuracyespecially under metareviewingand produced reviews often judged more constructive than the human average. Yet challenges such as fine-grained calibration, susceptibility to sycophancy during rebuttals, and variable persona agreement highlight the continued need for human expertise. From these results we propose guidelines for hybrid peer review: deploy AI reviewers as complements rather than replacements, prioritize ensembles and meta-review protocols, condition agents with structured context, and optimize for both review quality and decision fidelity. With such workflows, AI can enhance consistency, coverage, and fairness, while humans provide the nuanced judgments essential for advancing science."
        },
        {
            "title": "8 ETHICS STATEMENT",
            "content": "This work involves the use of publicly available data from the OpenReview platform, which hosts peer review information for academic conferences. We strictly adhered to OpenReviews terms of use and community guidelines in collecting and analyzing this data. All data used were publicly accessible at the time of collection and we did not pass any personally identifiable information to the LLM beyond what was intentionally made public by authors or reviewers. The goal of this research is to improve the transparency, scalability, and fairness of the peer review process. Our experiments are designed to complement, not replace, human reviewers, with the intent of assisting editorial processes and studying the potential of AI tools in structured academic evaluation. We emphasize that ReviewerToo is meant for research and controlled integration scenarios, and not for unsupervised or fully automated decision-making in academic publishing. Finally, the use of large language models in review generation and evaluation was conducted with attention to ethical implications, including the risks of bias propagation, overreliance on model outputs, and possible reinforcement of systemic inequities. We provide concrete guidelines and limitations in our discussion to promote responsible adoption of AI-assisted review systems. 11 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review"
        },
        {
            "title": "REFERENCES",
            "content": "David Adam. The peer-review crisis: how to fix an overloaded system. Nature, 644(8075):2427, 2025. Shubham Agarwal, Gaurav Sahu, Abhay Puri, Issam H. Laradji, Krishnamurthy Dj Dvijotham, Jason Stanley, Laurent Charlin, and Christopher Pal. LitLLMs, LLMs for literature review: Are we there yet? Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https://openreview. net/forum?id=heeJqQXKg7. Jacy Reese Anthis, Ryan Liu, Sean Richardson, Austin Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, and Michael Bernstein. Llm social simulations are promising research method. arXiv preprint arXiv:2504.02234, 2025. Alessandro Checco, Lorenzo Bracciale, Pierpaolo Loreti, Stephen Pinfield, and Giuseppe Bianchi. Aiassisted peer review. Humanities and social sciences communications, 8(1):111, 2021. Corinna Cortes and Neil Lawrence. Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. arXiv preprint arXiv:2109.09774, 2021. Eftekhar Hossain, Sanjeev Kumar Sinha, Naman Bansal, Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Md Mahadi Hassan, et al. Llms as meta-reviewers assistants: case study. arXiv preprint arXiv:2402.15589, 2024. Maximilian Idahl and Zahra Ahmadi. Openreviewer: specialized large language model for generating critical scientific paper reviews. arXiv preprint arXiv:2412.11948, 2024. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. Llms get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120, 2025. Giuseppe Russo Latona, Manoel Horta Ribeiro, Tim Davidson, Veniamin Veselovsky, and Robert West. The ai review lottery: Widespread ai-assisted peer reviews boost paper scores and acceptance rates. arXiv preprint arXiv:2405.02150, 2024. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, et al. Monitoring ai-modified content at scale: case study on the impact of chatgpt on ai conference peer reviews. arXiv preprint arXiv:2403.07183, 2024a. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, Daniel A. McFarland, and James Zou. Can large language models provide useful feedback on research papers? large-scale empirical analysis. NEJM AI, 1(8): AIoa2400196, 2024b. doi: 10.1056/AIoa2400196. URL https://ai.nejm.org/doi/full/10. 1056/AIoa2400196. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, et al. Can large language models provide useful feedback on research papers? large-scale empirical analysis. NEJM AI, 1(8):AIoa2400196, 2024c. Ryan Liu and Nihar Shah. Reviewergpt? an exploratory study on using large language models for paper reviewing. arXiv preprint arXiv:2306.00622, 2023. Miryam Naddaf. Ai is transforming peer reviewand many scientists are worried. Nature, 639(8056): 852854, 2025. Maria Petrescu and Anjala Krishen. The evolving crisis of the peer-review process. Journal of Marketing Analytics, 10(3):185186, 2022. 12 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Maria Sahakyan and Bedoor AlShebli. Disparities in peer review tone and the role of reviewer anonymity. arXiv preprint arXiv:2507.14741, 2025. Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, Madeleine Udell, et al. Ai-driven review systems: evaluating llms in scalable and bias-aware academic reviews. arXiv preprint arXiv:2408.10365, 2024."
        },
        {
            "title": "A LLM USAGE",
            "content": "We have used LLMs to improve the text. Specifically, we have use chatGPT to improve the language of some paragraphs and we have used LitLLM to retrieve relevant works. LLM-AS-A-JUDGE PROTOCOL FOR ELO We use the following update formula for ELO: = RA + (SA EA), EA = 1 1 + 10(RB RA)/400 , where RA is the rating of system A, SA {0, 0.5, 1} is the observed score, and is the update constant. This produces comparative ranking of review-writing quality across human and AI reviewers that integrates all five evaluation dimensions. To ensure reliability and fairness in our LLM-based ELO evaluations, we use: Blinding. All reviews are anonymized prior to evaluation. System identities (e.g., human, persona X, metareviewer) are removed, and formatting is standardized so that the judge cannot infer the source from stylistic cues. Randomization. For each pairwise comparison, the left/right order of reviews is randomized. The prompt to the judge LLM explicitly instructs it not to infer authorship based on order or style. Outcome aggregation. The raw win/loss/draw outcomes are aggregated into ELO ratings using the logistic update formula described earlier in this section. For stability, we initialize all systems with identical ratings of 1,000 and use moderate update constant (K = 32) for the first 30 matches of an agent, then reduced to = 16 until the agent has played 500 matches, after which, it is fixed to = 10. Final ratings are reported after convergence over the full set of pairwise matches. Match stratification. In large-scale settings, the number of possible review pairs can approach one million, which is computationally prohibitive. When fewer comparisons are run than the full set of possible matches, we employ stratified sampling strategy: matches are distributed proportionally across (i) distinct query papers, and (ii) distinct parent review sources (e.g., human, persona, metareviewer). This ensures balanced coverage of both paper-level diversity and system-level diversity, while keeping the number of matches tractable. Quality control. random subset of judgments (5%) is manually inspected by the authors to verify adherence to the evaluation rubric. Discrepancies between human inspection and the LLM judge are rare (< 3%) and do not materially affect rankings."
        },
        {
            "title": "ELO Scores",
            "content": "13 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Figure 4: Pairwise Cohens κ for different types of reviewers 14 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Figure 5: Confusion Matrices for binary Classification Task Post-Discussion ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review 16 Figure 6: Confusion Matrices for 5-way Classification Task ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Figure 7: Confusion Matrices for 5-way Classification Task Post-Discussion ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Table 3: Reviewer Persona ELO. Reviewer Persona ELO big picture critical permissive reproducibility default pedagogical pragmatist empiricist theorist visionary impact probabilistic fairness 364 423 880 989 1136 1345 1182 1558 1463 1097 1121"
        },
        {
            "title": "C QUALITATIVE EXAMPLES",
            "content": "Paper 6Mxhg9PtDE (Real Decision: Oral) (Real Decision: Oral): Human Review Summary: This paper demonstrates the shallow safety alignment issue through variety of case studies. Essentially, the authors show that variety of alignment attacks are successful because of common issue within safety-aligned LLMS: only the first few output tokens are adapted during the model alignment process. Then the paper offers ways to mitigate this problem, which includes data augmentation approach and constrained optimization loss function. Strengths: The paper is addressing an important problem, the vulnerability of safety alignment for LLMs, that can be very useful to real world problems. The paper ties together prior works in way that makes it easier to learn from them (i.e. highlighting the common thread amongst successful alignment attacks: their exploitation of shallow safety alignment). The contributions of this paper lay the groundwork for future safety alignment solutions. They do offer couple mitigation strategies, but exposing the shallow alignment issue could inspire many more mitigation approaches. It could also help us understand the success of other attacks and the success/failure of existing attack mitigation strategies. The paper includes good variety of experiments (models, datasets, attacks types) and includes both empirical and theoretical support for their claims. The paper flows nicely. It is nicely organized. This makes the paper easy to follow and it makes the main point/contribution of the paper very clear. Weaknesses: The explanation of related work is lacking. The related works are listed, but there is not much information that actually explains how your work differs from related work. For instance, you say some works have also noted asymmetries... But it would be nice to know how this differs from what youve observed. lot of the statements you make about related work are very broad and could benefit from more detail. Our work ties these potential failure modes. . . to potential shortcuts - does your work do this for all pre-existing methods for improving alignment? Are there some failures that your work does not encapsulate? Also, you never seem to mention any solutions to these alignment failures. Are your methods (e.g. the data augmentation and constrained optimization) the only known mitigation strategies? If so, you should state this. If not, other mitigation strategies should be mentioned. After applying your mitigation strategies, the ASR is still not zero and often isnt even that close to zero. This isnt ever really explained in the paper. You at one point say the augmented model is still vulnerable. . . , but the paper would be stronger if you give more explanation. For instance, does the non-zero ASR mean that there is some other vulnerability apart from the shallow alignment issue? Or are your strategies just not fully fixing the shallow alignment problem? Your contribution would be stronger if it were explained more clearly. When you say things like this work is the first work attempting to consolidate the unified notion behind these attacks. . . dont quite understand what you mean. If other works have identified the shallow safety alignment effect, then what does it mean for you to consolidate the unified notion? Is shallow safety alignment new term that you are introducing, because if so, think you should make it more clear that you are introducing this new concept? 18 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review It is also hard to imagine this problem in real-world setting/application. The paper would be stronger if, for example in the introduction, we were given an example of the effect that jailbreaks can have (e.g. him what scenario would some attacker be able to provide deployed model with the start to response) Questions: Why do you think the ASR still isnt 0 (and in many cases is not close to 0) after using your mitigation strategies? It seems like there could potentially be problems with the data augmentation approach since you are providing the model with these strange texts (e.g., you mention that the new texts are not coherent). Do you think that this matters? Is the models learning going to be compromised when it is learning with these incoherent texts? Rating: 10: strong accept, should be highlighted at the conference Paper 6Mxhg9PtDE (Real Decision: Oral): Empiricist Summary of Contributions The paper introduces the notion of shallow safety alignment, i.e., that current alignment procedures for large language models (LLMs) mainly modify the probability distribution of the first few output tokens to induce refusal prefix. The authors provide: 1. Empirical evidence (per-token KL divergence, token-wise loss and gradient analyses) that alignment effort is concentrated on early tokens. 2. unified explanation of several known jailbreak and fine-tuning attacks as consequences of this shallow alignment. 3. Two mitigation strategies: Data-augmentation deep safety alignment that mixes safety-recovery examples (harmful prefix safe continuation) into continued fine-tuning. token-wise constrained fine-tuning objective that penalises deviation from the original aligned model more strongly on the first few tokens. 4. Extensive experiments on Llama-2-7B-Chat and Gemma-1.1-7B-IT showing reduced attack success rates (ASR) on prefilling, GCG, decodingparameter, and fine-tuning attacks, while preserving utility on standard benchmarks (AlpacaEval, MMLU, BBH, MATH, GSM8K). Strengths Aspect Comments Empirical rigor Unified perspective Mitigation proposals Experimental breadth Reproducibility details Ethical awareness"
        },
        {
            "title": "Weaknesses",
            "content": "The paper systematically quantifies token-wise KL divergence and gradient norms, and links these to observed vulnerabilities. Provides clear, parsimonious explanation that ties together variety of jailbreak and fine-tuning attacks under single mechanism. Both the data-augmentation and constrained-fine-tuning approaches are simple to implement and require only modest additional compute. Evaluates on multiple models, several attack families, and suite of utility benchmarks. Appendices contain compute resources, optimizer settings, hyper-parameter values, and ablation studies. Includes an ethics statement acknowledging the dual-use nature of the work. Issue Impact Reliance on GPT-4 automatic judging Limited statistical analysis Scope of models and data Definition of deep alignment Utility trade-off Over-claiming Ablation depth Comparison to strongest baselines Safety evaluation depends on single black-box judge; no human verification or inter-annotator agreement is reported, raising concerns about label noise. Results are reported as meanstd over 3 runs (or 10 for GCG). No significance testing or confidence intervals; the small number of seeds may mask variability. Experiments are limited to 7B-parameter models and modest augmentation set (256 triplets). Generalisation to larger models or different alignment pipelines (e.g., RLHF vs DPO) is not demonstrated. The term is operationalised only through the proposed augmentation; alternative ways of deepening alignment (e.g., longer refusal prefixes, hierarchical prompts) are not explored. While utility scores drop only slightly, the paper does not analyse qualitative impact on instruction-following; some degradation is visible (e.g., AlpacaEval loss from 51.8% to 49.5%). The conclusion that future safety alignment should be made more than just few tokens deep is plausible but not fully substantiated beyond the attacks studied. The ablation on the number of deep tokens (parameter C) shows diminishing returns beyond 100 tokens, but the paper does not discuss practical limits of how deep alignment can realistically be made. The constrained fine-tuning is compared to Standard SFT and Vaccine, but not to more recent jailbreak-defense techniques (e.g., self-refine, circuit-breakers)."
        },
        {
            "title": "Questions for the Authors",
            "content": "1. Human verification Did you manually inspect subset of GPT-4 judgements to estimate label accuracy? If not, could you provide small human-annotated validation set? 2. Seed sensitivity How much do the ASR numbers vary across random seeds (especially for the constrained fine-tuning)? Have you tried more than three repeats? 19 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review 3. Generalisation to larger models Do you have preliminary results on 13B or 70B-scale models, or on models aligned with pure RLHF rather than the proprietary pipeline used for Llama-2-Chat? 4. Effect on non-harmful instruction following Beyond aggregated utility metrics, does the constrained objective affect the style or length of benign responses? 5. Computational overhead in practice The constrained loss requires storing per-token probabilities of the original aligned model. How does this scale to models with >10B parameters and longer context windows? Suggestions for Improvement 1. 2. Include small human-rated safety benchmark to validate GPT-4 judgments and report inter-annotator agreement. Increase the number of random seeds ( 5) and report confidence intervals or perform hypothesis tests to substantiate the observed reductions in ASR. 3. Test the methods on at least one larger model (e.g., Llama-2-13B-Chat) and on model aligned via pure RLHF to assess generality. 4. Compare shallow vs. deep alignment under different training regimes (SFT only, RLHF, DPO) to clarify whether the phenomenon is specific to the proprietary pipeline. 5. Provide examples of benign responses before/after mitigation to illustrate subtle shifts in style or completeness. 6. Expand discussion on scalability of deep alignment (diminishing returns, token budget, impact on generation latency). 7. Release augmentation scripts and constrained-loss implementation to facilitate reproducibility. Overall Verdict The paper presents compelling empirical diagnosis of concrete failure mode in current LLM safety alignment and offers two practical mitigation strategies that show measurable improvements across range of attacks. While the experimental methodology could be strengthened (human safety validation, more extensive statistical analysis, broader model coverage), the contribution is novel, analyses thorough, and mitigation ideas of immediate practical interest to the community. Final Recommendation: Accept (Poster) Paper 6Mxhg9PtDE (Real Decision: Oral): Theoretical Summary of Contributions The paper introduces the notion of shallow safety alignment the observation that current safety-aligned large language models (LLMs) primarily modify the generative distribution of only the first few output tokens to achieve refusal behavior. The authors: 1. Empirically characterize this phenomenon across Llama-2 and Gemma models, showing that KL-divergence between aligned and base models is concentrated in the early token positions and that prefixed refusal tokens dramatically reduce harmfulness. 2. Demonstrate that variety of known jailbreak and fine-tuning attacks (adversarial suffix, prefilling, decoding-parameter exploits, and few-step harmful fine-tuning) can be explained as exploiting this shallow alignment. 3. Propose two mitigation directions: (a) Data-augmentation deep safety alignment that augments training data with safety-recovery examples where the model must return to refusal after few harmful tokens. (b) Token-wise constrained fine-tuning that penalises deviation from the original aligned distribution on early tokens via novel regularised loss (Eq. 3). 4. Provide extensive experiments showing that both approaches improve robustness to the attacks above while preserving utility on standard benchmarks. Strengths Aspect Novel conceptual framing empirical Comprehensive analysis Concrete mitigation proposals Thorough evaluation Theoretical grounding Ethics discussion Reproducibility"
        },
        {
            "title": "Weaknesses",
            "content": "Comments The paper formalises an intuitive observation (the refusal-prefix shortcut) as shallow safety alignment and connects it systematically to broad set of jailbreak and fine-tuning attacks. This unifying view is valuable for the safety community. Per-token KL, gradient-norm, and loss-norm studies across multiple models and datasets convincingly demonstrate the early-token bias. The prefixed-prefix experiments  (Table 1)  are simple yet powerful. Both the data-augmentation and constrained-fine-tuning methods are easy to implement on top of existing pipelines, requiring only modest compute overhead  (Table 12)  . The authors evaluate against wide suite of attacks (prefilling, GCG, decoding-parameter, OOD code attacks, fine-tuning attacks) and report utility on AlpacaEval, MMLU, BBH, MATH, GSM8K. Ablation studies on hyper-parameters, β-schedules, and warm-up are included. Section provides clear derivation of the constrained loss, linking it to token-wise KL-regularised RL and to soft-plus surrogate of max-margin objective. Theorems 13 give solid interpretation of the role of β. The paper acknowledges that exposing failure modes can aid adversaries but argues the net benefit for safety research, satisfying the ICLR Code of Ethics. Detailed appendix sections (BG) list compute resources, data construction, and hyper-parameters. Code-wise the methods are straightforward. 20 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Issue Impact Limited scope of deep alignment Evaluation on narrow model family Reliance on GPT-4 as safety judge Constrained loss parameter sensitivity Utility degradation hyperMissing comparison to recent circuit-breaker defenses The data-augmentation approach only trains on tiny synthetic safety-recovery set (256 examples). While it demonstrates feasibility, it is unclear how this scales to larger vocabularies, more diverse harmful content, or multi-turn dialogues. Experiments focus on Llama-2-7B-Chat and Gemma-1.1-7B-IT. It remains an open question whether the findings transfer to larger models (e.g., 70B) or to models with different fine-tuning pipelines (e.g., RLHF-only). All safety metrics (harmfulness rate, ASR) are obtained via GPT-4 classifier. Potential bias or miscalibration of this judge could affect conclusions; human-in-the-loop validation would strengthen the claims. While ablations on uniform vs. biased β are provided, selecting β values for each token position may be non-trivial in practice. The paper does not propose an automated way to set β (e.g., based on per-token KL statistics). Although utility drops are modest (24% on AlpacaEval), the constrained loss sometimes harms downstream performance more severely (e.g., Table 4 shows utility loss on GSM8K). The trade-off analysis could be deeper. The related work mentions short-circuiting (Zou et al., 2024) but does not empirically compare against it. head-to-head would clarify the relative merits of the proposed methods."
        },
        {
            "title": "Questions for the Authors",
            "content": "1. Scaling of safety-recovery data How does performance vary when the safety-recovery set is enlarged (e.g., 1 k, 10 examples) or when the harmful prefixes are sampled from real user queries rather than synthetic? 2. Token-wise β selection Did you try learning βt (or schedule) from data (e.g., based on per-token KL divergence) instead of hand-crafting step function? 3. Multi-turn dialogues Does shallow alignment manifest similarly in multi-turn chat settings where the model can recover after refusal in later turn? 4. Effect on non-refusal safe behaviors Some safety signals (e.g., content filtering via internal classifiers) are not captured by refusal prefixes. Does deepening alignment improve robustness to attacks that target those signals? 5. Human evaluation Have you performed any manual verification of the GPT-4 safety judgments, especially for borderline cases (e.g., low-severity harmful content)? 6. Compatibility with RLHF Can the constrained fine-tuning objective be combined with RLHF (e.g., as an additional KL term) without destabilising policy-gradient updates? Suggestions for Improvement 1. Broaden the safety-recovery dataset Include diverse harmful continuations (different domains, code, multi-modal prompts) and evaluate the impact of dataset size on deep alignment. 2. Automated β scheduling Propose simple heuristic (e.g., βt KLt) or learnable gating network to set token-wise constraints, reducing manual tuning. 3. Human-in-the-loop validation Sample subset of outputs for each attack and have annotators verify the GPT-4 labels; report inter-annotator agreement. 4. Compare with contemporaneous defenses Include baselines such as the short-circuit method (Zou et al., 2024) and the Vaccine approach (Huang et al., b) in the same experimental protocol. 5. Evaluate on larger models Even limited experiment on Llama-2-13B-Chat or 70B model would strengthen the claim of generality. 6. Ablation on multi-turn safety Test whether the deep alignment persists across conversational turns, perhaps using dialogue benchmark (e.g., SafeChat). 7. Improve manuscript readability Ensure all equations render correctly, add schematic diagram of the shallow vs. deep alignment pipeline, and provide clearer captions for figures and tables. Overall Verdict The paper presents well-motivated, theoretically grounded, and empirically substantiated analysis of an important safety failure mode in current LLM alignment pipelines. The notion of shallow safety alignment is novel and unifies variety of jailbreak attacks, offering fresh lens for the community. The mitigation strategies are simple, computationally inexpensive, and show promising improvements across broad attack suite. While there are limitations concerning scalability, evaluation breadth, and presentation clarity, these do not undermine the core scientific contribution. Final Recommendation: Accept (Spotlight) Paper j7b4mm7Ec9 (Real Decision: Reject): Human Review Summary: The paper provides proper exploration of parameter lightening for watermarking models. The problem of mismatch between the actual decoding objective and the optimization objective of the commonly used decoding loss is solved. The solution to the above problem is attempted from the point of view of adding projection blocks and proxy losses. And the impact of each block on robustness at fine granularity is discussed after subdividing the watermarking framework. Strengths: For the first time, we identify the mismatch between the optimization objectives of commonly used decoding losses (e.g., mean-square error and binary cross-entropy loss) and the actual decoding objectives, and confirm the existence of such mismatch and its impact through ablation studies, which provides new perspective for model optimization. The proposed separable projection head (PH) and decodingoriented alternative loss (DO) effectively mitigate the negative impact of irrelevant optimization directions, allowing the lightweight model to 21 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review achieve SOTA performance while maintaining high performance. The lightweight model outperforms existing models in terms of invisibility, robustness, and efficiency for other domains with limited computational resources, and minimizes performance loss by further compressing the model with fine-grained deep watermarking framework. Weaknesses: the authors dont seem to have considered the issue of capacity, and suggest discussing this part; there are some spelling problems, e.g. robustness in table9 at line 1005 we know that robustness depends mainly on the type and intensity of noise added during the training phase, i.e., the NWIP module, but in the The paper does not give detailed experimental parameters, but only describes the noise parameter settings in the testing phase. Questions: In terms of comparison experiments, because the main concern in watermarking is capacity, imperceptibility, robustness and efficiency. In this paper, we mainly focus on the efficiency improvement, but we should ensure that the other parameters are the same for fair comparison.HiDDens algorithm can guarantee that the embedding in the grey scale image of size 1616 embedded in the length of 52 bits of information, the embedded information capacity can be up to 0.203 BPP. And the image used in this paper is 3128*128 colour image, the embedded information length is 64 bits, the capacity is much lower than HiDDens, so the result of such comparison should be understood as unfair, please explain why. Besides, the embedding capacities of several other compared algorithms are not consistent, how are they compared? Discriminative networks have been used in image watermarking frameworks since HiDDen and have shown advantages in enhancing the invisibility of watermarking frameworks. The authors of this paper did not consider this module in the newly proposed framework, please explain the reason, is it because high invisibility can be achieved without discriminative networks, to reduce the parameters so it is not used or there is another reason, please explain. In response to the decomposition of the MSE loss function in the paper, we see the role of the different loss components, would like to ask if this decomposition is first proposed in this paper? Or is there already readily available scheme in the field of knowledge distillation that performs similar decomposition of the loss function, and this will affect the reassessment of the papers innovativeness. Rating: 8: accept, good paper Paper j7b4mm7Ec9 (Real Decision: Reject): Empiricist Summary of Contributions The paper tackles the problem of building lightweight deep learning-based image watermarking systems that retain high invisibility and robustness. Its main contributions are: 1. Analysis of the loss mismatch decomposition of the commonly used MSE (and BCE) surrogate losses into deflation, inflation and regularization components, arguing that only the deflation term directly serves the decoding objective. 2. Decoding-oriented surrogate loss (DO) modified loss that retains the deflation term and restricts the inflation term to bits that lie within safe distance ε from the decision boundary. 3. Detachable projection head (PH) an auxiliary projection module added only during training to absorb the inflation/regularization directions; it is removed at inference time. 4. five-module architectural framework (image-preprocessing, message-preprocessing, feature-fusion, noised-image-preprocessing, message-extraction) that enables fine-grained ablations and module-wise pruning. 5. lightweight encoder/decoder built solely from transposed-convolution and convolution layers ( 16 parameters) that, when trained with DO or PH, achieves PSNR 4142 dB and average decoding accuracy 99.3 % on suite of combined distortions, outperforming several larger SOTA models (HiDDeN, MBRS, CIN, FIN) while using only 2 % of their parameters. Strengths"
        },
        {
            "title": "Weaknesses",
            "content": "The explicit decomposition of MSE into deflation/inflation/regularization and the identification of irrelevant optimization directions is insightful and not previously articulated in watermarking literature. Both DO and PH are plug-and-play; they do not require architectural redesigns and can be applied to existing models. The lightweight model trained with DO/PH reaches higher PSNR than the strongest baselines (FIN) and matches or exceeds their robustness on the combined-noise benchmark. The paper includes loss-component ablations, module-wise removals, PH block-number/channel-size studies, and hyper-parameter sweeps (ε, λ weights). Reducing parameter count and FLOPs by > 95 % is valuable for deployment on edge devices, scenario explicitly motivated in the introduction. Public codebases for baselines are used; training details (optimizer, learning rate, hardware) are listed; all additional metrics (SSIM, LPIPS, l2, l) are reported. 1. Statistical rigor All reported numbers are single-run averages; no confidence intervals, standard deviations, or significance tests are provided. Given the high accuracies ( 99 %), even small variances could alter the ranking against baselines. 22 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review 2. Limited attack spectrum Robustness is evaluated on six synthetic distortions (combined noise) and two diffusion-based attacks that are approximated by Gaussian-noise + median-filter pipeline. Real-world attacks (cropping, rotation, scaling, aggressive JPEG with varying QF, format conversion) are missing, and the geometric-distortion results show the lightweight model lagging behind MBRS. 3. Training overhead of PH While PH is removed at inference, its impact on training time, memory consumption, and energy is only qualitatively described (inefficiency) without quantitative measurement. This makes it hard to assess the overall cost-benefit. 4. Hyper-parameter sensitivity The safe distance ε and the λ-weights strongly influence performance (Tables 18-19). The paper acknowledges manual tuning but does not provide systematic tuning protocol or sensitivity analysis beyond few discrete values. 5. Theoretical justification The decomposition proof is presented, but there is no formal guarantee that minimizing DO leads to tighter bound on decoding error than standard MSE/BCE, nor any convergence analysis for the PH-augmented training. 6. Perceptual quality assessment PSNR, SSIM and LPIPS are reported, yet no user study or visual comparison beyond single figure is provided; this limits confidence in the claim of invisibility for human observers. 7. Clarity and presentation The manuscript contains many typographical artefacts (e.g., stray line numbers, inconsistent notation) that hinder readability and reproducibility. Questions for the Authors 1. Variability How much does the decoding accuracy vary across random seeds and different train/validation splits? Could you provide standard deviations for the main tables? 2. Training cost of PH What is the increase in GPU memory usage and wall-clock training time when PH is employed (e.g., for 4 projection blocks, 32 channels each)? 3. Effect of ε on different distortions Table 19 shows the safe distance tuned for combined noise; does the same ε transfer well to singledistortion or geometric-distortion settings? 4. Generalisation to other datasets Have you evaluated the model on datasets other than USC-SIPI (e.g., Kodak, DIV2K) to verify that the reported PSNR/accuracy are not dataset-specific? 5. Ablation on loss weighting How sensitive are results to the choice of λDO1 , λDO2 (or λP H1 , λP H2 )? Is there principled way to set them? 6. Comparison with recent lightweight baselines There are newer compact watermarking models (e.g., based on MobileNet or pruning techniques). How does your method compare to those? Suggestions for Improvement 1. Statistical reporting Include at least three independent runs per configuration with mean std and perform paired statistical tests against baselines. 2. Broader robustness evaluation Add experiments with geometric transformations (rotation, scaling, cropping) and with range of JPEG qualities. If possible, test against state-of-the-art watermark removal attacks (e.g., GAN-based erasers). 3. Training-efficiency analysis Quantify the additional GPU memory, FLOPs, and training time introduced by PH; possibly explore lightweight PH (e.g., 1 block) to balance cost and gain. 4. Hyper-parameter tuning protocol Provide systematic search (grid or Bayesian) for ε and λ values, and report the resulting sensitivity curves. 5. Theoretical bound Derive (or cite) bound that relates the DO loss to the expected decoding error, clarifying why the reduction in inflation improves robustness. 6. Human perceptual study Conduct small user study (e.g., MOS) to corroborate PSNR/SSIM findings on invisibility. 7. Manuscript polish Remove stray line numbers, ensure all equations are rendered, and unify notation (e.g., Ldeflation vs Ldeflation). Overall Verdict The paper introduces thoughtful analysis of loss mismatches in deep watermarking and proposes two practical techniques (DO and PH) that demonstrably improve very compact models performance. The empirical results are promising and the ablation studies are thorough, but the lack of statistical validation, limited attack diversity, and missing analysis of training overhead reduce the confidence in the claimed superiority. With the suggested revisions, the work would be solid contribution to the ICLR community. Final Recommendation Final Recommendation: Accept (Poster) the contribution is novel and empirically strong enough for inclusion, but further work is needed before it could be highlighted as an oral or spotlight paper. Paper j7b4mm7Ec9 (Real Decision: Reject): Theorist Summary of Contributions The manuscript studies deep learning-based image watermarking with an emphasis on lightweight models. Its main contributions are: 1. Identification of loss-mismatch the authors argue that commonly used surrogate losses (MSE, BCE) contain components (inflation, regularization) that do not directly serve the decoding objective and thus waste capacity in small models. 2. Two training-time remedies 23 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review (a) Detachable Projection Head (PH): an auxiliary head added only during training to absorb the irrelevant optimisation directions; it is removed at inference. (b) Decoding-Oriented surrogate loss (DO): reformulated loss that keeps only the deflation term and introduces safe-distance hyperparameter to limit the influence of inflation. 3. fine-grained five-module watermarking framework (image-preprocess, message-preprocess, feature-fusion, noised-image-preprocess, message-extraction) that enables module-wise ablations and parameter reductions. 4. lightweight encoder-decoder architecture ( 16 parameters) that, when trained with PH or DO, attains robustness and invisibility comparable to much larger SOTA models. 5. Extensive empirical evaluation on COCO/USC-SIPI, covering combined noise, diffusion-based attacks, geometric distortions, and knowledge-distillation baselines. Strengths Practical relevance Reducing model size while preserving watermark robustness is an important engineering problem for deployment on edge devices, diffusion-based generative models, and neural-radiance-field pipelines. Comprehensive experiments The authors evaluate many distortion types (six combined noises, diffusion attacks, geometric transforms) and report wide range of metrics (PSNR, SSIM, LPIPS, accuracy). The ablation studies on individual modules and on the number of PH blocks/channels are thorough. Plug-and-play nature Both PH and DO are described as modular additions that can be applied to existing lightweight watermarking pipelines without architectural changes. Open-source spirit The paper mentions public code bases for baselines (HiDDeN, MBRS, CIN, FIN) and reports reproducibility details (datasets, optimizer, hardware). Weaknesses 1. Theoretical novelty and rigor The decomposition of MSE/BCE into deflation / inflation / regularisation terms is essentially reparameterisation of the standard squared-error expansion; similar analyses exist in the learning-to-hash and metric-learning literature. The paper does not provide new theorems, nor does it prove that removing inflation/regularisation is necessary for lightweight models beyond empirical observation. The proposed DO loss is heuristic truncation of the full surrogate loss plus manually tuned safety margin ϵ. No justification is given for the specific form of the safe-distance term, nor is there any analysis of its effect on the loss landscape (e.g., gradient bias, convergence guarantees). The PH module resembles an auxiliary classifier head often used for stabilising training (e.g., in deep metric learning). The manuscript does not situate this design within that broader context, making the claimed novelty ambiguous. 2. Empirical methodology concerns Hyper-parameter sensitivity: Both PH (λ values, number of blocks, channel width) and DO (ϵ, λ-weights) require extensive manual tuning. The reported gains are strongly dependent on these settings, raising concerns about reproducibility and fairness. Statistical significance: Results are presented as single mean values; confidence intervals or multiple random seeds are absent. Given the modest absolute differences (often < 0.5 dB or < 1 % accuracy), it is unclear whether improvements are statistically robust. Baseline selection: Comparisons exclude recent invertible or flow-based watermarking methods, which may impact the state-of-the-art claim. Training cost: No quantitative measurement of training overhead is provided, though PH increases memory and computation. 3. Presentation and clarity Numerous typographical errors, inconsistent notation, and low-quality figures hinder readability and verification. 4. Ethical considerations While aligned with ethical standards, the lack of dedicated Broader Impact section violates conference requirements. Questions for the Authors 1. Can you provide formal analysis (e.g., bound) showing that removing the inflation and regularisation terms reduces the required capacity for given decoding error? 2. How sensitive is the final performance to the choice of ϵ across different noise levels? Is there an automated way to set ϵ without exhaustive search? 3. What is the additional GPU memory consumption and wall-clock time for training with PH compared to the baseline MSE training? 4. Have you evaluated the variance of PSNR/accuracy over multiple random seeds? Could you report 95 % confidence intervals for the key tables? 5. How do you envision the misuse of highly robust lightweight watermark that could be embedded in malicious media (e.g., deepfakes)? Do you foresee any safeguards? Suggestions for Improvement Strengthen the theoretical contribution by linking the loss decomposition to calibration theory or re-framing it as an empirical design study. Automate hyper-parameter selection via validation-based scheduling for λ and ϵ. Report standard deviations or confidence intervals and conduct statistical tests. Add Broader Impact section discussing positive and negative societal implications. Include more recent baselines such as diffusion-based or invertible watermarking models. Improve manuscript clarity: fix typos, include key equations in main text, and enhance figure quality. 24 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review Overall Verdict The paper addresses an important problem and provides extensive experiments, but lacks theoretical rigor, shows marginal gains, and suffers from clarity issues. Therefore, it does not meet ICLR 2025 acceptance standards. Final Recommendation: Reject Paper j7b4mm7Ec9 (Real Decision: Reject): Pedagogical Summary of Contributions The paper tackles the problem of building lightweight deep-learning based image watermarking systems that retain high invisibility and robustness. Its main ideas are: 1. Identifying loss-misalignment the commonly used MSE/BCE surrogate losses contain inflation and regularisation terms that do not directly contribute to decoding accuracy, leading to unnecessary parameter consumption in small models. 2. Two training-time remedies (a) Detachable Projection Head (PH): an auxiliary head is attached during training to absorb the irrelevant optimisation directions and is discarded at inference. (b) Decoding-Oriented surrogate loss (DO): reformulated loss that retains only the deflation term and limits the influence of inflation via safe distance ε. 3. five-module decomposition of the encoder/decoder (image-preprocess, message-preprocess, feature-fusion, noised-image-preprocess, message-extraction) that enables fine-grained ablations and parameter pruning. 4. Extensive experiments showing that model with 0.02 parameters ( 2 % of SOTA) can achieve comparable or superior PSNR and decoding accuracy when trained with PH or DO, and that the methods are plug-and-play for other lightweight designs. Strengths Aspect Comments Technical relevance Empirical breadth Modular framework Plug-and-play claims"
        },
        {
            "title": "Weaknesses",
            "content": "Category Clarity & Intuition Narrative Flow Presentation Accessibility Reproducibility Overall readability The problem of efficient watermarking is important for many downstream applications (e.g., diffusion models, NeRF). The identification of loss-misalignment is useful observation that could inspire further work. The authors evaluate on wide range of distortions (combined noise, diffusion-based attacks, geometric transforms) and compare against several recent SOTA watermarking models. The five-module decomposition is clearly motivated and the ablation tables (e.g., Table 7) illustrate its practical utility. PH and DO are presented as methods that can be added to existing lightweight backbones without architectural changes potentially valuable engineering contribution. Issues The core ideas (deflation vs. inflation) are explained only after dense algebraic derivation that is largely hidden behind Appendix A. The safe-distance ε is introduced abruptly, without visual illustration of its effect on the loss landscape. The paper jumps between high-level motivation, low-level algebra, and experimental details without smooth transitions. Section headings are all-caps and do not follow typical ICLR style. Related Works mixes historical and modern papers without clear taxonomy. Numerous line-numbers appear in text. Tables are inconsistently formatted. Figures are referenced but missing. Many typographical errors reduce professionalism. newcomer would struggle to understand deflation, inflation, and regularisation without diagrams. Prior works are not clearly compared. Hyper-parameter tuning lacks guidance in main text. No source-code link. Training details are scattered and incomplete. The manuscript reads like collection of logs; the abundance of numbers without interpretation hampers comprehension."
        },
        {
            "title": "Questions for the Authors",
            "content": "1. Safe distance ε How is ε chosen in practice? Would validation curve help? 2. Projection head architecture Please include clear diagram of the projection block. 3. Training overhead Quantify additional GPU memory and time for PH training. 4. Generalisation Have PH/DO been tested on larger backbones (e.g., ResNet)? 5. Code release Will the implementation be released for reproducibility?"
        },
        {
            "title": "Suggestions for Improvement",
            "content": "1. Rewrite the core methodological section with diagrams and intuitive explanation. 2. Remove extraction artefacts and follow ICLR template. 25 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review 3. Provide complete figures and well-formatted tables. 4. Add concise Practical Guide with recommended values for ε, λP , λDO. 5. Improve reproducibility by including implementation details and code link. 6. Strengthen related-work organisation into clear categories. 7. Thorough proofreading for grammar and typography. Overall Recommendation The paper addresses an interesting niche (lightweight watermarking) and proposes novel loss-based ideas that yield strong results. However, communication quality is currently below ICLR standardsabsent figures, fragmented narrative, and formatting artefacts make evaluation difficult. recommend rejection and suggest substantial revision. Final Recommendation: Reject"
        },
        {
            "title": "D IMPLEMENTATION DETAILS",
            "content": "The system is implemented in Python with asynchronous orchestration and semaphores to control parallelism. All agents persist their outputs in standardized directory layout, enabling caching, reproducibility, and downstream analysis. We use gpt-oss-120b as the primary large language model for all roles, served on 8xH100 GPUs with vllm. Reviewer prompts combine base rubric with persona-specific instructions and the official reviewer guidelines from the ICLR website (including the code of ethics). 5 No fine-tuning is performed; all agents operate in instruction-following mode. For training the XGBoost classifiers, we perform 5-fold cross validation and report the mean results across the 5 folds in Table 1. For XGBoost, we use 200 estimaters, max depth of 6, learning rate of 0.1, and for the Bert classifier, we finetune it for 20 epochs, use learning rate of 2e-5, batch size of 16. We use an 70-15-15 training-validation-test split and perform hyperparameter tuning on the validation set to test between learning rates {1e-5, 2e-5, 3e-5} and number of epochs in {3, 5, 10, 20, 30, 50}. 5https://iclr.cc/Conferences/2025/ReviewerGuide 26 ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review"
        },
        {
            "title": "Responsibilities",
            "content": "Retrieve and rank relevant papers Summarize top-k works into concise review Provide grounding for reviewer, author, and metareviewer Paper summary Explicit strengths and weaknesses Checks: novelty, soundness, experiments, results/discussion Organization/presentation and impact Grounds judgments in manuscript or literature Categorical recommendation (Oral, Spotlight, Poster, Reject, Desk Reject) Synthesizes rebuttal from reviews and literature Addresses criticisms, clarifies misunderstandings Proposes revisions (e.g., code release, ablations) Explicitly cites reviewer claims or literature Summarizes reviewer stances and scores (pre-rebuttal) Identifies shared strengths and weaknesses Evaluates rebuttal effectiveness Tracks stance shifts post-rebuttal Highlights lingering concerns or disagreements Fact-checks reviewer claims; assigns significance scores Categorical recommendation Table 4: Overview of agents in the REVIEWERTOO pipeline and their responsibilities. ReviewerToo: Should AI Join The Program Committee? Look At The Future of Peer Review"
        },
        {
            "title": "Style and primary focus",
            "content": "default critical permissive empiricist pragmatist theorist pedagogical big picture reproducibility impact visionary fairness probabilistic Balanced, rubric-following reviewer aligned with ICLR 2025 guidance; covers soundness, novelty, impact, clarity without strong bias. Skeptical, flaw-finding stance; stress-tests novelty claims, methodology rigor, and baselines. Supportive lens; highlights strengths and potential, assumes good faith, emphasizes positive interpretations of results. Evidence-first; scrutinizes datasets, baselines, metrics, statistical validity, and whether results support claims. Real-world utility; feasibility, scalability, deployment cost, practitioner relevance, and adoption barriers. Conceptual rigor; coherence and elegance of core ideas, logical soundness, evidence-theory alignment. Communication quality; clarity, intuition, narrative flow, figure/table interpretability, accessibility to newcomers. Vision-first; long-term significance, paradigm-shift potential, conceptual promise over implementation details. Replication rigor; missing hyperparameters, data splits, seeds, envs; checklist compliance and ambiguity removal. Foundations and representations; depth, interpretability, principles that advance long-term AI understanding. Bold paradigm shifts and learning dynamics; speculative but plausible mechanisms and broader implications. Practical elegance and scalability; efficient, implementable methods with robust large-scale validation. Probabilistic rigor and generative modeling; uncertainty handling, principled inference, socially meaningful applications. metareviewer majority Synthesis and calibration; aggregates reviewers, evaluates rebuttal effectiveness, extracts/verifies facts, assigns significance, and produces AC-facing briefings and recommendations. metareviewing baseline taking majority vote of all the reviewer agents. Table 5: Reviewer and metareviewer personas used in ReviewerToo and their primary emphases."
        }
    ],
    "affiliations": [
        "Canada CIFAR Chair",
        "HEC Montreal",
        "Mila Quebec AI Institute",
        "Polytechnique Montreal",
        "ServiceNow Research",
        "Universite de Montreal"
    ]
}