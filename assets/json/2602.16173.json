{
    "paper_title": "Learning Personalized Agents from Human Feedback",
    "authors": [
        "Kaiqu Liang",
        "Julia Kruk",
        "Shengyi Qian",
        "Xianjun Yang",
        "Shengjie Bi",
        "Yuanshun Yao",
        "Shaoliang Nie",
        "Mingyang Zhang",
        "Lijuan Liu",
        "Jaime Fernández Fisac",
        "Shuyan Zhou",
        "Saghar Hosseini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 1 ] . [ 1 3 7 1 6 1 . 2 0 6 2 : r Learning Personalized Agents from Human Feedback Kaiqu Liang1,2,, Julia Kruk1, Shengyi Qian1, Xianjun Yang1, Shengjie Bi1, Yuanshun Yao1, Shaoliang Nie1, Mingyang Zhang1, Lijuan Liu1, Jaime Fernández Fisac2, Shuyan Zhou1,3,, Saghar Hosseini1 1Meta Superintelligence Labs, 2Princeton University, 3Duke University Work done at Meta Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agents ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts. Date: February 19, 2026 Correspondence: Kaiqu Liang at kl2471@princeton.edu Project page: personalized-ai.github.io Code: https://github.com/facebookresearch/PAHF"
        },
        {
            "title": "1 Introduction",
            "content": "Modern AI agents, powered by large language models (LLMs), are increasingly capable of complex reasoning and task execution. This success has fueled their application in user-facing roles, from digital assistants (Zhou et al., 2023; Xie et al., 2024; Xu et al., 2024) to embodied robots (Ahn et al., 2022; Huang et al., 2022; Liang et al., 2024). However, it remains significant challenge for these agents to take actions that align with the complex, idiosyncratic, and often dynamic preferences of individual humans. Prior works on building personalized assistants have generally relied on pre-existing, static dataeither training models to infer implicit preferences from historical interaction logs (Qiu et al., 2025; Chen et al., 2024; Tan et al., 2024; Li et al., 2024b) or populating agent memory with pre-defined user profiles and past interaction history (Chhikara et al., 2025; Xu et al., 2025b). However, this reliance on static data poses fundamental limitation, as the agent cannot properly adapt to real-world human behavior. This failure manifests in three core challenges that most existing systems are not designed to handle. (1) The agent cannot immediately adapt to new users, as it has no profile or interaction history to learn from. (2) Static models are not designed to learn from real-time corrective feedback, when the agent makes an error, it cannot easily update its behavior to avoid that same mistake in the future. (3) The framework fails to handle non-stationary user preferences, as it cannot adapt when users persona or preferences evolve over time. To address these challenges, we introduce the Personalized Agents from Human Feedback (PAHF) framework. PAHF moves away from the static, pre-existing data assumption and treats interaction itself as the primary learning signal. We simulate long-horizon sequential decision-making, where each user corresponds to sequence of tasks and each new interaction depends on the agents accumulated preference memory. Within this setting, PAHF uses explicit per-user memory updated from preand post-action feedback to continually 1 Figure 1 Static vs. continual personalization. Top: Static personalization, where offline humanAI logs are processed once to populate user profiles, and at deployment the agent only reads from this static memory. Bottom: Our continual personalization framework keeps the user in the loop during online interaction: the agent consults and updates explicit memory in the pre-action stage, and the user observes the outcome in the environment after the agent acts and provides post-action feedback, which is written back to memory to correct errors and adapt as preferences change. refine the agents behavior. This design is intended to be robust to the realities of human interaction: it enables the agent to build users preference profile from scratch, learn directly from its mistakes, and continually adapt that profile as preferences drift over time. Our framework operationalizes this continual learning via novel, three-step interactive loop. The first step is Pre-Action Interaction. When given an ambiguous instruction, the agent first queries its memory. If no relevant preference is found (e.g., for new user), it proactively asks the user for clarification (e.g., Which drink do you prefer?). This step is designed to resolve known uncertainty and allows the agent to learn new preferences before making costly, erroneous action. The second step is Action Execution, where the agent grounds its decision in its current belief state, synthesizing the users instruction and the preferences retrieved from its memory to execute an action. The final and most critical step is Post-Action Feedback Integration. If the agents action is incorrecteither from wrong guess or, more importantly, from applying an outdated preferencethe user provides corrective feedback (e.g., Actually, like Sprite most now). PAHF is explicitly designed to integrate this feedback, using it to revise and update the explicit preferences stored in its memory. This reactive loop is the essential mechanism for correcting miscalibrated beliefs and adapting to non-stationarity, which pre-action queries alone cannot solve. Our primary contributions are threefold. (1) We introduce PAHF, continual agent personalization framework that couples explicit per-user memory with proactive pre-action clarification to resolve ambiguity and reactive post-action feedback to handle preference drift. (2) We develop an evaluation suite for continual personalization, comprising two new large-scale benchmarks (embodied manipulation and online shopping) spanning physical and digital agents, and four-phase protocol that separates initial preference learning from adaptation under persona shifts. (3) We provide theoretical and empirical analysis showing that PAHF consistently reduces personalization error over no-memory and single-channel baselines, and that explicit memory combined with post-action feedback is particularly important for robust personalization without pre-existing user data."
        },
        {
            "title": "2 Related Work",
            "content": "Personalized Alignment of LLMs. Early approaches to aligning large language models with human preferences rely on reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). Subsequent work has shown that much of RLHFs benefit can be recovered via supervised fine-tuning on preference data without explicit RL (Rafailov et al., 2024; Zhao et al., 2023; Yuan et al., 2023; Hong et al., 2024; Ethayarajh et al., 2024). Recently, work has highlighted emergent untruthful behaviors induced by RLHF (Liang et al., 2025b) and proposed hindsight simulation as mitigation (Liang et al., 2025a). This hindsight perspective directly motivates our post-action feedback channel, where the human provides feedback after the agent acts, and the agent uses it to correct and update its memory. While these methods align to aggregate preferences, recent work explores personalized alignment, adapting models to individual users via multi-dimensional objectives or personalized RLHF (Chen et al., 2024; Tan et al., 2024; Li et al., 2024b). However, these approaches typically rely on substantial pre-collected user data and one-off fine-tuning, making it hard to handle users with no history data and to track evolving preferences. Personalized LLM Agents and Memory. major line of work personalizes LLM outputs through retrieval2 Figure 2 Continual agent personalization using preand post-action feedback channels. On Day 1 (Initial Preference), the agent uses pre-action interaction to learn Kates favorite drink (Coke) before acting. On Day 2 (Contextual Preference), the agent overgeneralizes this memory when Kate is sleepy; after serving Coke, it is corrected via post-action feedback (I prefer tea when am sleepy). On Day 3 (Dynamic Preference), the agent acts consistently with its memory (serving tea), but Kates underlying preference has shifted to coffee. This error triggers another post-action update, illustrating how the framework supports continual personalization under preference drift. augmented generation (RAG) (Lewis et al., 2020; Borgeaud et al., 2022; Gao et al., 2023; Yu et al., 2024; Wang et al., 2023b), where user histories are retrieved and injected into context at query time (Salemi et al., 2024a,b; Richardson et al., 2023). Building on this, recent methods add thinking procedures that reason about preferences or select operations such as reasoning, personalization, and clarification during interaction (Li et al., 2024a; Salemi et al., 2025), or define static persona profiles that enable role-playing but encode coarse, fixed traits rather than fine-grained, evolving preferences (Shao et al., 2023; Wang et al., 2024a; Hu and Collier, 2024). These techniques are also used in life-long personalization of LLMs, which updates high-level persona profiles for conversational assistants from accumulated dialogue history (Wang et al., 2024b). In contrast, we study continual personalization for environment-acting agents in long-horizon sequential tasks. Recent work has proposed increasingly sophisticated memory architectures for LLM agents (Mei et al., 2024; Chhikara et al., 2025; Zhong et al., 2024; Packer et al., 2023; Wang et al., 2023a; Xu et al., 2025b), emphasizing long-term state tracking, summarization, and retrieval. In contrast, our memory design is deliberately simple: we use lightweight agentic memory to isolate the effect of feedback channels on continual personalization, rather than introducing new architectural complexity. These advanced memory systems are complementary to our framework and could be integrated into it to further improve scalability and capacity. Personalized Agents in Interactive Environments. Personalization for embodied agents has been extensively studied in spatial arrangement (Kapelyukh and Johns, 2022; Wu et al., 2023), table settings (Puig et al., 2020), and object navigation (Dai et al., 2024; Barsellotti et al., 2024), as well as inferring user preferences for planning (Xu et al., 2025a). In flight recommendation (Lin et al., 2022), recent work leverages Bayesian cognitive models (Griffiths et al., 2007, 2024; Tenenbaum et al., 2006, 2011) to teach LLM agents to approximate probabilistic reasoning for personalized recommendations (Qiu et al., 2025). However, these methods largely rely on implicit preference modeling and offline fine-tuning, which requires extensive historical user data and can be costly to update when user preferences change. Although recent agent memory works (Chhikara et al., 2025) introduce explicit preference modeling, they typically assume pre-populated user profiles and do not address the challenge of learning online from live interactions for continual personalization. Finally, while PREFDISCO (Li et al., 2025) benchmarks interactive preference discovery, it remains limited to static personas within short-horizon dialogues."
        },
        {
            "title": "3 Personalized Agents from Human Feedback",
            "content": "The core objective of the Personalized Agents from Human Feedback (PAHF) framework is to enable an agent to continually align with users unique and evolving preferences. Unlike static personalization approaches that learn from fixed datasets, PAHF learns online from interaction, continuously updating its internal model 3 of the user. This framework is explicitly designed to operationalize continual learning loop in which the agent can add new preferences (e.g., preferring energy-efficient appliances), revise outdated ones (e.g., changed favorite drink), and exploit this knowledge to improve its actions over time."
        },
        {
            "title": "3.1 Formalizing Continual Personalization\nWe formalize continual personalization as an online learning process. At each interaction t, the user has a\n, and issues an instruction It while the agent observes Ot (e.g., a scene or product\nlatent preference state M ∗\nt\n. The agent maintains an explicit preference memory ˆMt (a persistent estimate\ncatalogue) conditional on M ∗\nt\nof the user’s preferences) and chooses an action at in response to (It, Ot, ˆMt). Let a∗\ndenote an optimal action\nt\n. The agent’s objective is to learn a personalized policy π(at | It, Ot, ˆMt) that\nunder the true preferences M ∗\nt\nminimizes the cumulative personalization error (cid:80)T\nt ] is the instantaneous 0–1 loss\nat round t (details in Appendix A). This problem is challenging because the agent’s preference memory ˆMt is\nprone to two distinct types of error:",
            "content": "t=1 Lt, where Lt = 1[at = Partial Observability: The true state is hidden. The agents memory ˆMt may be incomplete (e.g., ˆMt = for new task), leading to errors from known uncertainty. Non-Stationarity: The users preferences are non-stationary, meaning at any time (i.e., preference drift). This leads to errors from miscalibration, where the agent is confidently wrong (e.g., it believes ˆMt = {likes Coke} when can evolve into = {likes Sprite}). t+1 We call round ambiguous if, under the agents current information state, the Bayes-optimal error is at least some fixed constant ε0 > 0; let γ [0, 1] denote the fraction of such ambiguous rounds. In this setting, static policy is insufficient. The agent must execute dynamic process that jointly (1) executes tasks based on its current estimate ˆMt, and (2) ingests human feedback to update ˆMt ˆMt+1, continually improving its approximation of . t+"
        },
        {
            "title": "3.2 PAHF Framework",
            "content": "The PAHF framework is our practical algorithm for solving this online personalization problem. It operationalizes the estimate update ˆMt ˆMt+1 through an interactive three-step loop designed to mitigate errors from both partial observability and non-stationarity. Pre-Action Interaction. This step is designed to efficiently mitigate errors from partial observability. The loop begins when the agent receives It and observes Ot. It first queries its memory ˆMt for relevant preferences, mt = Retrieve( ˆMt, It, Ot). If the agent identifies ambiguity (e.g., the instruction is ambiguous and no relevant information is found in memory), it then proactively generates clarification query qt to the user, receiving pre-action feedback pre . Crucially, this feedback is parsed and written to memory before acting: ˆM = pre update( ˆMt, It, Ot, mt, qt, pre ). (1) This allows the agent to resolve known uncertainty using (mt, qt, pre storing the information in ˆM Action Execution. The agents action policy πact formulates the final executable action at. Its role is to synthesize all available information, conditioning jointly on the original instruction It, the observation Ot, the retrieved preferences mt, and the pre-action interaction context (qt, pre ) when choosing its action, while also for future rounds. ): at = πact(It, Ot, mt, qt, pre ). (2) = ˆMt. The policy must intelligently In the case where no clarification is needed, qt and pre integrate these inputs to form coherent plan. For example, given the task It = Bring my favorite drink and observation Ot showing Coke and Sprite, the policys behavior adapts: if mt provides relevant preference (e.g., {favorite drink is Coke}), the policy uses this directly to formulate at (e.g., pick up the Coke). However, if mt is empty, the agent must rely entirely on the interaction (e.g., qt = Which drink do you prefer?, pre = Id like Coke) to resolve the ambiguity and formulate the same action. The agent then executes at in the environment, which transitions the world into new observable state Ot+1. are null and ˆM Post-Action Feedback Integration. This step is the agents essential mechanism for learning from errors. Post-action feedback is triggered only if the action at (taken in the previous step) results in non-optimal outcome, leading to new state Ot+1 that the user deems incorrect. While this reactive channel can also address partial observability (e.g., learning an unknown preference after wrong guess), its unique power is in correcting miscalibrationthe confidently wrong state that pre-action queries cannot detect. After observing the outcome, the user provides post-action feedback post . We then prompt an LLM as feedback detector to determine whether post contains salient, personalized information that should be stored. If such information is found (e.g., Actually, like Sprite most now), this feedback drives the post-action update function. This function integrates the new feedback with the current memory state ˆM , producing the next memory state ˆMt+1: ˆMt+1 = post update( ˆM t, It, mt, qt, pre , at, post ). (3) If no post-action feedback is given, or if the detector classifies the feedback as non-informational (e.g., Thank you), the memory state simply carries over: ˆMt+1 = ˆM . This post-action update allows the agent to add, revise, and refine its knowledge, such as revising mcoke msprite after post In the next section, we prove the necessity of both pre-action and post-action feedback channels. Pre-action feedback prevents initial errors caused by partial observability, whereas post-action feedback is essential for adapting to preference drift and correcting confidently wrong miscalibration. Their combination minimizes cumulative personalization error. = now like Sprite most. t"
        },
        {
            "title": "3.3 Theoretical Justification for PAHF",
            "content": "an optimal action under and the agent holds an estimate ˆMt. Let at Setup. At round [T ], the user has latent preference state be the agents action and ]. Preferences are piecewise stationary with at most 1 switches over rounds. round is ambiguous if, under the agents information state (posterior conditioned on its transcript up to t), the Bayes-optimal error probability is at least some fixed constant ε0 > 0; let γ [0, 1] denote the fraction of ambiguous rounds. The agent may ask up to balanced m-ary pre-action questions on ambiguous rounds, where 2 is fixed, and may apply post-action update after mistake. . Define the 01 loss Lt = 1[at = Proposition 1 (Post-action feedback is necessary under preference drift). Suppose rounds are unambiguous under the agents information state (i.e., γ = 0) and preferences switch at most 1 times. Any policy that never uses post-action feedback incurs = Ω(T ). Conversely, any policy that performs an update upon the first error after each switch makes at most O(K) expected mistakes. t=1 Lt (cid:104)(cid:80)T (cid:105) Sketch. Without post-action feedback, pre-action histories are indistinguishable across switch times, so the agent cannot condition on when the change occurs. This failure to detect drift implies Ω(T ) expected mistakes, as the agent persists with stale preferences for constant fraction of rounds. Conversely, with post-action feedback revealing the correct action, the agent updates its explicit memory after the first error following each switch, yielding at most one mistake per switch ( K). Remark 1 (Context-dependent preferences). Preferences may depend on context Ct (e.g., time, location, state). When Ct changes, using context-agnostic global note can yield confidently wrong action (without triggering preaction queries); by Proposition 1, postaction signal is needed to correct miscalibration. Proposition 2 (Pre-action feedback is necessary under partial observability). Assume is stationary and fraction γ > 0 of rounds are ambiguous. Any policy that never requests pre-action clarification suffers (cid:104)(cid:80)T = Ω(γT ). If the agent asks up to balanced m-ary pre-action questions on each ambiguous (cid:105) t=1 Lt round, the expected number of errors on those rounds is O(γT mk). Sketch. Without querying, each ambiguous round has Bayes-optimal error probability at least ε0 > 0, so the expected number of mistakes is Ω(γT ). Under balanced m-ary queries, each question contracts the posterior mass of hypotheses that induce wrong action by factor of at most 1/m; after questions, the residual error probability is at most mk. Summed over γT ambiguous rounds, this yields O(γT mk) errors. 5 at every round, and define the dynamic regret RT = (cid:80)T Theorem 1 (Complementarity of preand post-action feedback). Let π be an oracle policy that knows ](cid:1). With at most switches and ambiguity rate γ, any PAHF policy that (i) asks up to balanced m-ary pre-action questions on ambiguous rounds and (ii) performs an immediate post-action update after the first error per switch satisfies E[RT ] = O(cid:0)K + γT mk(cid:1). In particular, choosing = Θ(logm ) yields E[RT ] = O(K + γ). (cid:0)Lt 1[aπ = t=1 , it never errs, so RT = (cid:80)T Sketch. Since the oracle π knows t=1 Lt just counts mistakes made by PAHF. On unambiguous rounds, each preference switch causes at most one error before the post-action update corrects the estimate (Proposition 1), contributing O(K) mistakes. On ambiguous rounds, up to balanced m-ary pre-queries shrink the error probability to at most mk (Proposition 2), so across at most γT such rounds this adds O(γT mk) mistakes. Summing the two contributions yields the stated bound; choosing = logm makes the second term O(γ)."
        },
        {
            "title": "3.4 Implementation",
            "content": "To empirically validate PAHF and the theory in Section 3.3, we constructed datasets, implemented the agent with two different memory backends, defined baselines, and designed evaluation metrics. Baselines. Our agent uses large language model (LLM) and follows the ReAct framework (Yao et al., 2022) to interleave reasoning and acting. Unless otherwise noted, we instantiate the agent with GPT-4o. In Appendix E, we report ablation studies that vary both the agent model (GPT-4.1) and the human simulator model (GPT-4.1, GPT-4o, and Llama-4-Scout) to examine how the quality of simulated human feedback affects agent performance. We compare four settings designed to isolate the roles of memory and feedback. Our primary baseline is (i) No Memory (no persistent store). We then evaluate three memory-enabled agents: (ii) Pre-action Only (clarification allowed, but no post-action updates), (iii) Post-action Only (no clarifications, learns only from corrective feedback), and (iv) PAHF (both channels enabled; Section 3.2). The Pre-action Only baseline targets partial observability (cf. γ > 0, K=0); Post-action Only targets preference drift (cf. γ=0, K1). PAHFs hybrid approach is motivated by the reality of tasks exhibiting both error sources. Memory design. Our goal in this work is not to propose new memory architecture, but to integrate human feedback into memory for continual personalization. We therefore adopt standard dense-retrieval memory backendsa portable SQLite note store and FAISS-based vector indexbehind shared API to keep the implementation simple and easily reproducible. Each memory entry is short natural-language note paired with an embedding, and all notes are strictly isolated per user. The API exposes only the functionality the agent needs: add note, retrieve small top-k set of relevant notes for query, detect near-duplicates via similarity to decide update vs. add, update an existing note in place, and enumerate or address notes by id. The SQLite backend persists notes in simple on-disk table and computes similarities on demand (favoring simplicity and reproducibility), while the FAISS backend maintains an in-memory nearest-neighbor index with optional save/load (favoring fast retrieval at scale). Both backends provide identical retrieval semantics so the rest of the system can treat memory as black box; more sophisticated memory modules (e.g., hierarchical or structured memory) are complementary to PAHF and can be integrated as drop-in replacements in future work. Unless stated otherwise, we report all main results with SQLite; FAISS results are in the Appendix E. Memory interaction (agentic memory). The agents memory interaction Mt consists of two core components: reading (retrieval) and writing (updates). Reading follows standard Retrieval-Augmented Generation (RAG) pipeline. We compute query embedding from (It, Ot) and run k-nearest-neighbor search over indexed memory embeddings (top-k dense retrieval; embeddings from DRAGON+ (Lin et al., 2023). Retrieve(Mt, It, Ot) returns small, ranked set of candidate preference records mt. Retrieval always precedes action selection so the policy conditions on mt rather than an empty context. We then apply lightweight information-extraction step conditioned on (It, Ot, mt) to distill the retrieved notes into task-relevant personalized cues; the resulting cleaned summary is inserted into the model context for action selection. Writing is multi-step process triggered any time the agent receives personalized information from either pre-action feedback (f pre ). First, the feedback is passed to salience detector, implemented as an LLM-as-a-judge, which identifies and discards non-informational feedback. If the feedback is deemed salient, we use an LLM to summarize and extract the core personalized note. We then integrate this note with the memory: retrieve the most relevant existing note; if similarity exceeds threshold τ , we ) or post-action feedback (f post t 6 merge by asking the LLM to produce an updated note that replaces the old text for that entry. If similarity is below τ , we add new note. This detectsummarizeintegrate pipeline keeps the database clean while enabling quick corrections (post-action) and steady accumulation of useful details (pre-action)."
        },
        {
            "title": "4.1 Evaluation Domains",
            "content": "We evaluate our framework across two distinct domains to ensure generality. We outline the core design of each domain below, while full details on dataset construction and discussion are provided in Appendix C. Embodied Manipulation Domain. Our dataset models everyday indoor mobile-manipulation tasks across home and office settings, expressed as natural instructions in two modes: selecting the right item and placing an item in the right location. Each scenario is self-contained: plain-language user request (e.g., Could you bring me my favorite drink?), the full inventory of available objects and locations, and the identity of the requesting persona (e.g., Alex). To emphasize personalization, many scenarios are context-dependent (e.g., Im drowsy, could you bring me something to drink?), where the correct action shifts with physiological or emotional state (anxious, unwell), task-oriented goals (work energy, healthy eating), and external factors such as social events (social sharing) or environmental/temporal cues (cold weather, morning). To increase the difficulty of personalized preference modeling, we deliberately design personas with idiosyncratic and unconventional preferences, so that correct actions cannot be inferred from generic commonsense heuristics. We simulate the human behavior by employing another LLMs with specific persona. Each persona is defined by detailed prompt that outlines baseline preference (e.g., favorite drink is black coffee) which is then consistently superseded by rich set of context-dependent exceptions (e.g., when drowsy, prefers herbal tea). The persona model is designed to provide feedback via two modalities: (1) brief, natural-language answers to the agents clarification questions, and (2) post-action feedback after task is completed. To capture human preference evolution, each user persona exists in two versions: an original profile and an evolved profile with updated preferences, allowing the right choice to change while scenes and instructions remain the same. Online Shopping Domain. We model personalized online shopping via natural-language purchase requests. The agent must choose one of three product candidates (A/B/C) or abstain (D: no purchase) if none are suitable. Each product is described by compact set of discrete, user-salient features (e.g., cameras sensor type, lens mount, and viewfinder). The ground truth is governed by each personas acceptance policy, which is strictly conjunctive: the user specifies preferred and acceptable values for each feature, and candidate is acceptable only if all its features meet these criteria. To test fine-grained discrimination, options are adversarially constructed as near-missesdistractors that combine highly preferred attributes with single disqualifying trait (poison pill). This forces the agent to perform fine-grained reasoning to verify user preferences against each attribute. Consequently, this design makes the domain significantly more challengingand resistant to performance gainsthan embodied manipulation benchmark. In this domain, we use hybrid human simulator. persona-conditioned LLM handles all pre-purchase clarification in natural language, while deterministic, rule-based judge renders post-purchase verdicts and feedback. During clarification, the LLM mimics user by answering only the specific feature questions the agent asks. After the agent commits (A/B/C/D), the rule-based evaluator programmatically checks the choice against the acceptance policy. If the choice is incorrect, the evaluator returns minimal, targeted feedback that pinpoints the specific offending feature. As in the embodied domain, each user has two profilesan original and an evolved version with updated acceptance policiesto test adaptation to preference drift."
        },
        {
            "title": "4.2 Evaluation Protocol",
            "content": "To measure continual personalization, we designed four-phase evaluation protocol to separately test an agents ability to learn an initial preference profile and, subsequently, its ability to adapt when that profile changes. This protocol includes two interactive learning phases and two test phases. Phase 1: Initial Learning (Training). The agent starts with an empty memory (Mt = ) and interacts with multiple users (each simulated with unique, fixed persona) over set of scenarios. These scenarios are 7 ambiguous, requiring the agent to use feedback to learn each users initial preferences. We run agents over this dataset for multiple epochs to measure the learning curve as the memory bank Mt is populated. Phase 2: Initial Personalization (Testing). We evaluate the agent (using the memory learned in Phase 1) on corresponding test set. These scenarios use the same user persona from Phase 1 but with different instructions and observations. This phase tests the agents ability to exploit its learned knowledge in new situations. In this test phase, there is no human feedback involved, so the agent makes decisions solely on its memory. Phase 3: Adaptation to Drift (Training). To test adaptation to non-stationarity, we re-introduce the training scenarios from Phase 1 but with critical change: each users persona is swapped for new one (e.g., favorite drink: Coke favorite drink: Sprite). An agent relying on its memory from Phase 1 will now be confidently wrong and will fail. This phase tests the agents ability to detect the resulting error and use new (primarily post-action) feedback to revise and overwrite its outdated beliefs. Phase 4: Adapted Personalization (Testing). Finally, we evaluate the agent (using the adapted memory from Phase 3) on second test set. This set corresponds to Phase 2 but uses the newly adapted persona from Phase 3. This phase measures whether the agent has correctly revised its memory, demonstrates proper adaptation to new scenarios, and is no longer stuck on the initial preferences. This protocol allows us to separately quantify performance on initial learning (Phase 1 and 2) and adaptation to preference drift (Phase 3 and 4), providing comprehensive view of continual personalization. Size and splits. In the embodied domain, we evaluate 40 users, each on 30 scenarios per phase, yielding 1,200 scenarios per phase. With two learning phases and two evaluation phases, this gives 2,400 learning and 2,400 evaluation scenarios. In the online shopping domain, we evaluate 20 users, each on 45 scenarios per phase, yielding 900 scenarios per phase and, analogously, 1,800 learning scenarios and 1,800 evaluation scenarios. (cid:80)N Metrics. We report three metrics. Success Rate (SR) is the fraction of tasks completed correctly: SR = i=1 1[correcti]. Feedback Frequency (FF) is the proportion of tasks that used any human feedback (pre 1 > 0]. To analyze how personalization improves + post or post-action) at least once: FF = 1 over iterations within phase, we also track an average cumulative personalization error (ACPE). For phase with learning iterations, let PEt [0, 1] denote the personalization error rate (e.g., fraction of incorrect tasks) at iteration {1, . . . , }. We define the average cumulative personalization error up to iteration as ACPEt = 1 s=1 PEs, which lies in [0, 1] and can be interpreted as percentage (0% best, 100% worst). i=1 1[pre (cid:80)N (cid:80)t"
        },
        {
            "title": "5 Results",
            "content": "Pre-action feedback prevents initial personalization error. In Phase 1 (initial online learning), agents equipped with pre-action clarification (Pre-action Only and PAHF) achieve substantially higher success rate on the very first interaction than both the Post-action Only agent and the no-memory baseline (Figures 3, 4, top-left panels). By asking targeted questions before acting, they can align with the users preferences from the outset and avoid the large early mistakes that the other agents make in ambiguous situations. This one-step warm start is reflected in the ACPE metric, where pre-action agents incur lower cumulative personalization error during the early stages of Phase 1 (top-right panels). This behavior matches the intended role of pre-action feedback: it is especially valuable for preventing initial personalization errors under partial observability. Pre-action feedback alone is brittle under preference drift. When user preferences change in Phase 3, the Pre-action Only agent struggles to correct its previously learned but now outdated beliefs. Its Phase-3 success-rate curves show very limited improvement (Figures 3 and 4, bottom-left panels), and in the embodied domain it even stays below the no-memory baseline. The corresponding ACPE curves indicate that its cumulative personalization error remains substantially higher than that of agents with post-action feedback across both domains (Figures 3 and 4, bottom-right panels). We observe that once the agent has written confident notes into memory, it no longer perceives ambiguity and therefore stops asking clarification questions, failing to detect that its knowledge has become stale. This is reflected in the Phase-3 feedback-frequency curves, where the Pre-action Only agent rarely requests clarification despite being systematically wrong. Post-action feedback is essential for fast adaptation. Agents that receive post-action feedback (Post-action Only and PAHF) demonstrate effective reactive learning: in both Phase 1 and Phase 3, across both domains, 8 Figure 3 Results on embodied manipulation. Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for success rate (left), feedback frequency (middle), and ACPE (right). Shaded regions denote standard error. Figure 4 Results on online shopping. Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for success rate (left), feedback frequency (middle), and ACPE (right). Shaded regions denote standard error. their success rates improve steeply over iterations whenever feedback is available (Figures 3 and 4, bottom-left panels). The ACPE curves mirror this pattern, showing sharp drops in cumulative personalization error as the agent uses corrective feedback to update its beliefs (bottom-right panels). In the dynamic-preference setting, post-action feedback is crucial for un-sticking overconfident agents whose memories have become stale: unlike the Pre-action Only agent, which stops asking questions because it no longer perceives ambiguity, the Post-action Only agent captures the reactive signal to overwrite outdated beliefs regardless of its confidence. Consequently, it recovers to high Phase-4 success (e.g., 67.9% in the embodied domain), closely approaching PAHF (70.5% in the same domain; Table 1). However, because this mechanism is strictly reactivetriggered 9 only after mistakerelying on it alone forces the agent to learn through trial and error. This leads to large initial personalization errors, imposing higher user costs via incorrect actions and frequent corrective feedback. Method Table 1 Evaluation success rates. Phase-2 and Phase-4 success rates (%) for each method in embodied and shopping domains. PAHF combines the strengths of both channels. By enabling both clarification before acting and corrective updates after mistakes, PAHF inherits the advantages of each feedback type while mitigating their weaknesses. In Phase 1, PAHFs success and ACPE curves generally surpass those of the other methods; it effectively minimizes initial personalization error through pre-action feedback and learns efficiently from post-action feedback. After preference drift, PAHF matches the rapid adaptation of the Post-action Only agent: in both domains, its Phase-3 learning curves nearly overlap with Post-action Only in success rate, yet PAHF consistently achieves the lowest cumulative personalization error (ACPE). Across all four evaluation phases and both domains, PAHF achieves the highest success rate  (Table 1)  , demonstrating that combining pre-action and post-action feedback with explicit memory is critical for robust continual personalization. 27.00.4 No memory 32.30.4 56.00.7 54.11.1 Pre-action only Post-action only 67.91.5 66.90.8 PAHF (pre+post) 70.51.7 68.81.3 41.30.8 70.31.1 Phase 2 Phase 4 Phase 2 Phase 4 27.80.2 34.40.5 38.90.5 44.80.5 35.71.0 68.31.2 Embodied Shopping Taken together, these results support the complementarity of the two feedback channels. Pre-action feedback prevents large initial personalization errors, post-action feedback is crucial for correcting confidently wrong beliefs after preference drift, and PAHFs joint use of both yields the strongest personalization performance."
        },
        {
            "title": "6 Conclusion",
            "content": "Modern AI agents are increasingly deployed in interactive settings, yet most personalization pipelines still assume static user data and stationary preferences. In this work, we introduce Personalized Agents from Human Feedback (PAHF), continual personalization framework that enables agents to learn online via explicit memory and dual feedback channels. Theoretically, we show that proactive pre-action queries and reactive post-action corrections address complementary failure modes: the former resolves ambiguity under partial observability, while the latter is essential for correcting miscalibrated beliefs under preference drift. Empirically, evaluations in embodied manipulation and online shopping domains demonstrate that PAHF consistently outperforms single-channel baselines. While pre-action feedback reduces early errors and postaction feedback enables recovery after persona shifts, our results show that only their combination, together with explicit memory, delivers robust performance under evolving user preferences."
        },
        {
            "title": "7 Acknowledgement",
            "content": "We thank Tom Griffiths for insightful feedback on the framing and presentation, and discussions that strengthened the paper."
        },
        {
            "title": "References",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Luca Barsellotti, Roberto Bigazzi, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Personalized instance-based navigation toward user-specific objects in realistic environments. Advances in Neural Information Processing Systems, 37:1122811250, 2024. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR, 2022. Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, and Zuozhu Liu. Pad: Personalized alignment of llms at decoding-time. arXiv preprint arXiv:2410.04070, 2024. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Yinpei Dai, Run Peng, Sikai Li, and Joyce Chai. Think, act, and ask: Open-world interactive personalized robot navigation. In 2024 IEEE international conference on robotics and automation (ICRA), pages 32963303. IEEE, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023. Thomas Griffiths, Mark Steyvers, and Joshua Tenenbaum. Topics in semantic representation. Psychological review, 114(2):211, 2007. Thomas Griffiths, Nick Chater, and Joshua Tenenbaum. Bayesian models of cognition: Reverse engineering the mind. MIT Press, 2024. Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2(4):5, 2024. Tiancheng Hu and Nigel Collier. Quantifying the persona effect in llm simulations. arXiv preprint arXiv:2402.10811, 2024. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. Ivan Kapelyukh and Edward Johns. My house, my rules: Learning tidying preferences with graph neural networks. In Conference on robot learning, pages 740749. PMLR, 2022. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky. Learning to rewrite prompts for personalized text generation. In Proceedings of the ACM Web Conference 2024, WWW 24, page 33673378. ACM, May 2024a. doi: 10.1145/3589334.3645408. http://dx.doi.org/10.1145/3589334.3645408. Shuyue Stella Li, Avinandan Bose, Faeze Brahman, Simon Shaolei Du, Pang Wei Koh, Maryam Fazel, and Yulia Tsvetkov. Personalized reasoning: Just-in-time personalization and why llms fail at it, 2025. https://arxiv.org/ abs/2510.00177. Xinyu Li, Ruiyang Zhou, Zachary Lipton, and Liu Leqi. Personalized language modeling from personalized human feedback. arXiv preprint arXiv:2402.05133, 2024b. Kaiqu Liang, Zixu Zhang, and Jaime Fisac. Introspective planning: Aligning robots uncertainty with inherent task ambiguity. Advances in Neural Information Processing Systems, 37:7199872031, 2024. Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas Griffiths, and Jaime Fernández Fisac. Rlhs: Mitigating misalignment in rlhf with hindsight simulation. arXiv preprint arXiv:2501.08617, 2025a. Kaiqu Liang, Haimin Hu, Xuandong Zhao, Dawn Song, Thomas Griffiths, and Jaime Fernández Fisac. Machine bullshit: Characterizing the emergent disregard for truth in large language models. arXiv preprint arXiv:2507.07484, 2025b. Jessy Lin, Daniel Fried, Dan Klein, and Anca Dragan. Inferring rewards from language in context. arXiv preprint arXiv:2204.02515, 2022. Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. How to train your dragon: Diverse augmentation towards generalizable dense retrieval. arXiv preprint arXiv:2302.07452, 2023. Kai Mei, Xi Zhu, Wujiang Xu, Wenyue Hua, Mingyu Jin, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system. arXiv preprint arXiv:2403.16971, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt: Towards llms as operating systems. 2023. Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: challenge for social perception and human-ai collaboration. arXiv preprint arXiv:2010.09890, 2020. Linlu Qiu, Fei Sha, Kelsey Allen, Yoon Kim, Tal Linzen, and Sjoerd van Steenkiste. Bayesian teaching enables probabilistic reasoning in large language models. arXiv preprint arXiv:2503.17523, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, and Abhinav Sethy. Integrating summarization and retrieval for enhanced personalization via large language models. arXiv preprint arXiv:2310.20081, 2023. Alireza Salemi, Surya Kallumadi, and Hamed Zamani. Optimization methods for personalizing large language models through retrieval augmentation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 752762, 2024a. Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large language models meet personalization. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73707392, 2024b. Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Zhuowan Li, Spurthi Amba Hombaiah, Weize Kong, Tao Chen, Hamed Zamani, and Michael Bendersky. Pathways of thoughts: Multi-directional thinking for long-form personalized question answering, 2025. https://arxiv.org/abs/2509.19094. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: trainable agent for role-playing. arXiv preprint arXiv:2310.10158, 2023. Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, and Meng Jiang. Democratizing large language models via personalized parameter-efficient fine-tuning. arXiv preprint arXiv:2402.04401, 2024. Joshua Tenenbaum, Thomas Griffiths, and Charles Kemp. Theory-based bayesian models of inductive learning and reasoning. Trends in cognitive sciences, 10(7):309318, 2006. Joshua Tenenbaum, Charles Kemp, Thomas Griffiths, and Noah Goodman. How to grow mind: Statistics, structure, and abstraction. science, 331(6022):12791285, 2011. Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Enhancing large language model with self-controlled memory framework. arXiv preprint arXiv:2304.13343, 2023a. 12 Noah Wang, Zy Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et al. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1474314777, 2024a. Tiannan Wang, Meiling Tao, Ruoyu Fang, Huilin Wang, Shuai Wang, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Ai persona: Towards life-long personalization of llms. arXiv preprint arXiv:2412.13103, 2024b. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. Learning to filter context for retrieval-augmented generation. arXiv preprint arXiv:2311.08377, 2023b. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. Autonomous Robots, 47(8):10871102, 2023. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Frank Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, et al. Theagentcompany: benchmarking llm agents on consequential real world tasks. arXiv preprint arXiv:2412.14161, 2024. Manjie Xu, Xinyi Yang, Wei Liang, Chi Zhang, and Yixin Zhu. Learning to plan with personalized preferences. arXiv preprint arXiv:2502.00858, 2025a. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025b. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Peixin Cao, Kaixin Ma, Jian Li, Hongwei Wang, and Dong Yu. Chainof-note: Enhancing robustness in retrieval-augmented language models. In Proceedings of the 2024 conference on empirical methods in natural language processing, pages 1467214685, 2024. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1972419731, 2024. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 13 Full Proofs for Section Notation recap. At round [T ], the users latent preference state is and the agent maintains preference memory ˆMt. Given instruction It and observation Ot, the agent chooses action at, and let denote (fixed) optimal action under ]. Preferences are piecewise stationary with at most switches over rounds. round is ambiguous if, conditioned on the agents transcript up to t, the Bayes-optimal error probability (over actions) is at least fixed constant ε0 > 0; let γ [0, 1] denote the fraction of such rounds. . We use the 01 loss Lt = 1[at = Feedback model. We distinguish two channels: (i) Pre-action feedback: on ambiguous rounds, the agent may ask up to clarification questions of arity at most 2. Answers are truthful. (ii) Post-action feedback: after acting, if at = , the user provides corrective signal which the agent may use to update its preference memory. Information-theoretic assumptions. A1 (Balanced m-ary pre-queries) On an ambiguous round, the agent can select an m-ary question such that, for every possible answer, the posterior probability mass on hypotheses that induce wrong action is at most 1/m fraction of its previous value. Answers are noise-free. A2 (Corrective post-signal on errors) When at = t on an unambiguous round, the post-action feedback (equivalently, to update to preference reveals enough information to identify the correct action on subsequent unambiguous rounds until the next switch). If policy does not memory that induces use post-action feedback, it does not update ˆMt based on this signal (i.e., ˆMt+1 = ˆMt on such rounds). Auxiliary lemmas Lemma 1 (Error shrinkage with m-ary questions). Under A1, after balanced m-ary pre-queries on single ambiguous round, the posterior probability that the induced action is suboptimal is at most mk. Proof. Let q0 denote the posterior mass of hypotheses that induce wrong action before any query on this ambiguous round. Under A1, each balanced m-ary question ensures that, for every possible answer, the posterior mass of wrong hypotheses shrinks by at least factor m; hence after one question q1 q0/m. Repeating times yields qk q0mk mk (since q0 1). The Bayes-optimal decision rule has error probability at most the posterior mass on wrong-action hypotheses, so the misclassification probability is at most mk. Lemma 2 (Indistinguishability without post updates). Suppose γ = 0 and policy never uses post-action feedback (as in A2). Consider two environments that differ only in the switch time(s) but generate identical (It, Ot) sequences. Then the joint distribution of the agents internal state and action sequence (a1, . . . , aT ) is identical across these environments; in particular, it is independent of the switch time(s). Proof. With γ = 0, the policy never issues pre-action queries and receives no pre-action information. If it never uses post-action feedback, then by A2 its preference memory update does not depend on post-action signals. Thus the agents internal state evolution depends only on its internal randomness and the observed (It, Ot) sequence. If (It, Ot) is identical across the two environments, then the induced distribution over internal states and actions is identical as well. Proof of Proposition 1 Proposition (Post-action feedback is necessary under preference drift). Suppose rounds are unambiguous (γ = 0) and preferences switch at most 1 times. Any policy that never uses post-action feedback incurs (cid:104)(cid:80)T = Ω(T ). Conversely, any policy that performs post-action update upon the first error after each (cid:105) t=1 Lt switch makes at most O(K) mistakes in expectation. 14 Proof. Lower bound. Because the class at most switches includes the special case = 1, it suffices to prove the claim for single switch. Fix = 1. Let the switch time τ be chosen uniformly from {1, . . . , }. Consider an environment with two actions {0, 1} and an (It, Ot) process that is identical for all and all τ . Define the optimal action as (τ ) = (cid:40) 0, 1, < τ, τ. Let policy (possibly randomized) produce an action sequence a1:T = (a1, . . . , aT ). By Lemma 2, the distribution of a1:T is independent of τ . Now fix any deterministic sequence a1:T {0, 1}T and define its total loss under switch time τ : ℓ(τ ; a1:T ) = (cid:88) t=1 1[at = (τ )] = (cid:88) t<τ 1[at = 1] + (cid:88) tτ 1[at = 0]. Averaging over uniform τ and swapping sums yields Eτ [ℓ(τ ; a1:T )] = = = (cid:88) (cid:88) τ =1 t<τ 1[at = 1] + 1[at = 0] (cid:88) tτ (cid:88) (cid:16) t= (cid:88) (cid:16) 1[at = 1] {τ : < τ } + 1[at = 0] {τ : τ t} (cid:17) 1[at = 1] (T t) + 1[at = 0] (cid:17) t=1 (cid:88) t=1 min{t, t} = (cid:107) 1 (cid:106) 2 4 = Ω(T ). 1 1 1 1 (The inequality holds since for each t, either choosing at = 0 incurs contribution or choosing at = 1 incurs contribution t, so the smaller is min{t, t}.) Therefore, for any deterministic a1:T , Eτ [ℓ(τ ; a1:T )] 2/4/T . For randomized policy, taking expectation over the policys randomness gives (cid:104) (cid:88) (cid:105) Lt t= = Ea1:T Eτ [ℓ(τ ; a1:T )] min a1:T Eτ [ℓ(τ ; a1:T )] = Ω(T ). Upper bound with reactive updates. Now consider policy that performs post-action update upon the first error after each switch. After switch, it may err once; on that error, by A2 the feedback identifies (on unambiguous rounds), and after updating the policy acts correctly on subsequent unambiguous rounds until the next switch. Thus there is at most one mistake per switch, so (cid:80)T t=1 Lt deterministically and E[(cid:80)T t=1 Lt] = O(K). Proof of Proposition 2 Proposition (Pre-action feedback is necessary under partial observability). Assume is stationary and fraction γ > 0 of rounds are ambiguous. Any policy that never requests pre-action clarification suffers (cid:104)(cid:80)T = Ω(γT ). If the agent asks up to balanced m-ary pre-action questions on each ambiguous (cid:105) t=1 Lt round, the expected number of errors on those rounds is O(γT mk). Proof. Lower bound. On an ambiguous round, by definition the Bayes-optimal error probability (given the transcript) is at least ε0 > 0. Without any pre-queries, the policy chooses under this unresolved posterior, so even the Bayes-optimal decision has error probability at least ε0. Summed over γT ambiguous rounds, the expected number of errors is Ω(γT ) (absorbing ε0 into constants). 15 Upper bound with m-ary pre-queries. By Lemma 1, after balanced m-ary questions on an ambiguous round, the posterior probability that the induced action is wrong is at most mk. Thus the error probability per ambiguous round is mk, and summing over at most γT such rounds gives O(γT mk). Proof of Theorem 1 Theorem (Complementarity of preand post-action feedback). Assume the optimal action is unique (or fix deterministic tie-break). Let π be the oracle policy that knows , and define the dynamic regret ](cid:1). With at most switches and ambiguity rate γ, PAHF policy that (i) RT = (cid:80)T asks up to balanced m-ary pre-action questions only on ambiguous rounds and (ii) performs an immediate post-action update upon the first error after switch achieves (cid:0)Lt 1[aπ = t=1 In particular, taking = Θ(logm ) yields E[RT ] = O(K + γ). E[RT ] = O(cid:0)K + γT mk(cid:1). Proof. Because π knows ] = 0 for all t. Hence RT = (cid:80)T zero loss: 1[aπ Decompose rounds into unambiguous and ambiguous. = each round, it can select the (tie-broken) optimal action and thus incurs t=1 Lt counts the PAHF policys mistakes. Unambiguous rounds. With at most switches and an immediate post-action update upon the first error after each switch, Proposition 1 yields O(K) mistakes on unambiguous rounds. Ambiguous rounds. On each ambiguous round, using up to balanced m-ary pre-queries reduces the error probability to at most mk by Proposition 2 and Lemma 1. Summed over γT ambiguous rounds, this contributes O(γT mk) mistakes. Summing contributions gives E[RT ] = O(K + γT mk). Taking = logm yields E[RT ] = O(K + γ)."
        },
        {
            "title": "B Limitations and Future work",
            "content": "(i) Memory Architecture: While this work focuses on feedback mechanisms rather than novel memory designs, future research could integrate more sophisticated memory systems as drop-in backends to improve scalability and context retention. (ii) Noisy Feedback: Human feedback can be noisy and heterogeneous. While our framework performs basic filtering, it does not explicitly handle inconsistent or mistaken feedback. For instance, user might be confused and provide incorrect feedback; future work could develop mechanisms to detect conflicts, query the user for clarification, and model diverse noise patterns. (iii) Reasoning Capabilities: Our benchmarks, especially the online shopping domain, remain challenging: agents still struggle to achieve high success rates. We intentionally make the tasks difficult and restrict human feedback to sparse signals, in order to stress-test fine-grained preference reasoning. However, sufficiently capable agents should be able to piece together these limited signals to recover the underlying preferences. We hope future work will close this gap and further improve performance on our benchmarks. (iv) Limited Interaction for Disambiguation: To reflect user friction in real deployments, we limit the agent to at most one clarification question per ambiguous task in both domains. This single-turn interaction budget increases task difficulty, particularly in online shopping where identifying an item that satisfies multiple preference constraints can require sequential disambiguation. Future work could explore multi-turn clarification strategies and learned turn-allocation policies, and quantify the trade-off between performance gains and user burden."
        },
        {
            "title": "C Details on Dataset",
            "content": "C.1 Embodied Manipulation To support the reproducibility of our experiments and provide comprehensive understanding of the benchmark, we provide description of the dataset construction, persona logic, and scenario generation process below. 16 User Persona Construction. The foundation of the benchmark lies in its set of 40 distinct user personas, each governed by rigorous, multidimensional preference model. Unlike simple key-value pairings found in traditional datasets, these personas are defined by conditional logic where users preference is function of their latent context. The preference model spans nine distinct categories: Drinks, Snacks, Storage, Location, Temperature, Environmental Approach, Health Considerations, Social Context, and Time of Day. For every category, specific rules dictate the users intent based on their current state. For example, the persona \"Alex\" generally prefers black coffee but shifts to herbal tea when the context is \"Drowsy\" (believing rest cures fatigue) and ice-cold water when \"Dehydrated\". This structure ensures that agents cannot simply memorize static mapping (e.g., \"Alex likes coffee\") but must learn the underlying causal relationship between the users state and their desired action. Systematic Preference Drift (The \"Evolved\" Dataset). To rigorously evaluate continual learning and adaptation, we constructed parallel \"Evolved\" version of every persona. This dataset introduces non-stationarity through systematic, 1-to-1 inversion of the users underlying belief system. In the \"Evolved\" personas, the logic driving preferences is flipped, rendering the agents prior knowledge obsolete or detrimental. For instance, while the original \"Alex\" persona preferred herbal tea when drowsy to prioritize rest, the evolved \"Alex\" adopts the opposite philosophy, preferring energy drinks to prioritize stimulation. Similarly, storage preferences shift from \"high shelves for hygiene\" to \"low shelves for accessibility.\" This \"hard\" distribution shift forces the agent to unlearn outdated correlations and acquire new ones through feedback, rather than simply refining existing weights. Scenario Generation and Structure. We generate interaction scenarios for training and evaluation (Sets and B) in both the Original and Evolved phases. Each example contains: (i) Scene objects (the target plus semantically related distractors), (ii) an intentionally ambiguous natural-language task that underspecifies the correct item/location, (iii) context tag that explicitly identifies the specific condition embedded within the instruction (e.g., Drowsy), and (iv) the ground-truth user intent (object and location) obtained by applying the personas rules to the context. This structure enables precise computation of personalization error (mismatch between the agents choice and the persona-consistent intent) and isolates the ability to adapt when preferences shift in the Evolved phase. C.2 Online Shopping Product Ontology and Persona Construction. The domain is structured around 10 distinct product categories: TVs, Laptops, Smartphones, Refrigerators, Washing Machines, Microwave Ovens, Air Conditioners, Dishwashers, Cameras, and Headphones. Each category is defined by rigid ontology of 3 specific feature dimensions. For example, \"TV\" is defined by its Smart TV Operating System, Panel Technology, and Base Type, while \"Laptop\" is defined by its Form Factor, Charging Adapter Type, and Webcam Placement. For every feature, the ontology provides fixed set of possible attribute values (e.g., Panel Technology options include OLED, QD-OLED, VA LCD, IPS LCD, MicroLED, and TN LCD). User personas are constructed by assigning hierarchical preference rankings to these attributes. Unlike binary like/dislike flags, our personas model realistic consumer behavior through tiered preference system: \"Preferred\" (the specific attribute value the user actively seeks), \"Acceptable\" (alternative values the user will tolerate), and \"Disliked\" (values explicitly rejected). This hierarchy allows for nuanced ground-truth determination, where the \"correct\" product is the one that maximizes overlap with the users \"Preferred\" and \"Acceptable\" sets while avoiding \"Disliked\" features. Stochastic Preference Drift. While the Embodied dataset utilizes logical inversion to simulate drift, the Online Shopping benchmark simulates stochastic taste evolution. In the \"Evolved\" phases (Phases 3 and 4), each users preference profile is re-sampled from the feature ontology. Specifically, for every product feature, the set of attributes is reshuffled: attributes that were previously \"Preferred\" may become \"Disliked,\" and previously \"Disliked\" attributes may become the new \"Preferred.\" This simulates user completely changing their requirementsfor example, user who previously sought \"OLED TVs for gaming\" might evolve to prefer \"IPS LCD panels for bright rooms.\" Scenario Generation and Evaluation. The dataset contains interaction scenarios divided across four phases. 17 Phases 1 and 2 utilize the Original preferences, while Phases 3 and 4 utilize the Evolved preferences. Uniquely, the scenarios in Phase 3 are structurally identical to Phase 1 (same product options and natural language instructions) but possess different ground-truth labels. Each scenario consists of: (i) natural language user instruction derived from varied templates (e.g., \"Help me buy laptop that suits my preferences\"), (ii) set of generated product candidates fully described by their attributes, and (iii) the ground-truth optimal product selected by evaluating the users current preference hierarchy against the options. This \"counterfactual\" design, where the inputs remain constant between Phase 1 and Phase 3, but the correct answer changes, allows us to rigorously isolate the agents sensitivity to preference drift."
        },
        {
            "title": "D Prompts",
            "content": "Environment Prompt for the Embodied Agent You are simulating robot operating in an office environment with access to multiple rooms and areas: COUNTER AREAS: - Main counter (in front of you): Open surface for immediate access - Side counter: Secondary workspace area - Island counter: Central area for shared items STORAGE LOCATIONS: - Top drawer: Upper storage compartment - Middle drawer: Middle storage compartment - Bottom drawer: Lower storage compartment - Top shelf: Upper open shelf storage - Bottom shelf: Lower open shelf storage - Refrigerator top shelf: Upper refrigerator compartment - Refrigerator bottom shelf: Lower refrigerator compartment - Pantry cabinet: Enclosed dry storage area WASTE/RECYCLING: - Landfill bin: General waste disposal - Recycling bin: Recyclable materials - Compost bin: Organic waste disposal SPECIALIZED AREAS: - Personal desk area: Individual workspace nearby - Meeting table: Shared collaborative space - Coffee station: Dedicated beverage preparation area - Microwave area: Heating/cooking zone DINING/SEATING AREAS: - Couch: Comfortable seating area for relaxation - Dining table: Formal eating and dining surface BEDROOM AREAS: - Bedroom nightstand: Personal bedside storage surface - Bedroom dresser: Clothing and personal item storage - Bedroom closet: Hanging and folded clothing storage - Bedroom closet door: Door surface for hanging items The robot can pick up items, place items in any of these locations, or retrieve items from these locations based on user preferences and contextual needs. 18 ReAct Prompt for the Embodied Agent TASK INSTRUCTIONS: You must determine the next action to complete the given task. - If the task is clear, select the appropriate action from the options. - If the task involves personalization, search memory for relevant personalized information. - If the task is unclear, ask the user for clarification. WHEN TO ASK FOR CLARIFICATION: You MUST ask for clarification when: 1. The task contains ambiguous references (e.g., \"that drink\" when multiple drinks are present) 2. The task involves subjective preferences (e.g., \"favorite\", \"preferred\", \"best\", \"my\", \"like\") 3. The task doesnt clearly specify which object to interact with 4. The task is unclear or incomplete FORMAT REQUIREMENTS: Your response must ALWAYS follow this exact format: Thought: <detailed reasoning explaining your decision process> Action: <EITHER specific option letter (A, B, C, D), \"Ask human\"> IMPORTANT RULES: - Never skip any of the three required fields - For Action, only use option letters (A, B, C, D) or the exact phrase \"Ask human\" - Always provide detailed reasoning in your Thought field - After receiving clarification from human (marked with \"Human:\"), select the appropriate action option - You can assume one of the option letters (A, B, C, D) is correct. Asking for clarification is ONLY necessary when you cannot determine which option is correct due to ambiguity or missing information. EXAMPLES: Example 1: Scene: On the counter, there is an orange soda, Pepsi, and an apple. Alex: Put that drink in the top drawer. Options: A) open the top drawer and put the orange soda in it B) open the bottom drawer and put the Pepsi in it C) open the bottom drawer and put the orange soda in it D) open the top drawer and put the Pepsi in it Context: No relevant personalized information found in memory. Thought: The task references \"that drink,\" but two drinks are present (orange soda and Pepsi). cannot determine which specific drink is intended, making this ambiguous. Action: Ask human Question: Which drink do you want me to put in the top drawer? orange soda or Pepsi? Human: Pepsi Thought: Now know the human wants me to put the Pepsi in the top drawer. Looking at the options, option matches this requirement exactly: \"open the top drawer and put the Pepsi in it\". Action: 19 Example 2: Scene: On the counter, there is Coke, bottled unsweetened tea, and Sprite. Sam: Bring me my favorite drink. Options: A) pick up the Coke B) pick up the bottled unsweetened tea C) pick up the Sprite D) pick up the bottled water Context: No relevant personalized information found in memory. Thought: need to ask human for clarification. Action: Ask human Question: Which drink is your favorite? Coke, bottled unsweetened tea, or Sprite? Human: Sprite Thought: The human has clarified that Sprite is their favorite drink. Option is \"pick up the Sprite\", which directly matches. Action: Example 3: Scene: On the counter, there is bottled water, bag of jalapeno chips, and bag of rice chips. Sam: would like bag of rice chips. Options: A) pick up the bottled water B) pick up the jalapeno chips C) pick up the kettle chips D) pick up the rice chips Context: No relevant personalized information found in memory. Thought: Its clear that should pick up the rice chips. Action: Example 4: Scene: On the counter, there is Coke, bottled green tea, and Sprite. Sam: Bring me my favorite drink. Options: A) pick up the Coke B) pick up the bottled green tea C) pick up the Sprite D) pick up the orange juice Context: The humans favorite drink is Coke. Thought: Based on the context, know the humans favorite drink is Coke. The task asks me to bring their favorite drink, so should pick up the Coke. Action: 20 Summarization Prompt for the Embodied Agent (Same Format as Shopping) TASK INSTRUCTIONS: Summarize the information retrieved from memory that is relevant to the task. If none of the retrieved information is relevant, output No relevant personalized information found in memory. FORMAT REQUIREMENTS: Your response must ONLY contain the summary in this EXACT format: Summary: <your summary here> EXAMPLES: Example 1: Scene: On the counter, there is an orange soda, Pepsi, and an apple. Kate: Put that drink in the top drawer. Options: A) open the top drawer and put the orange soda in it B) open the bottom drawer and put the Pepsi in it C) open the bottom drawer and put the orange soda in it D) open the top drawer and put the Pepsi in it Context: None Summary: No relevant personalized information found in memory Example 2: Scene: On the counter, there is an energy bar, banana, and microwave. Kate: Put my favourite snack next to the microwave. Options: A) pick up the energy bar and put it next to the microwave B) pick up the banana and put it next to the energy bar C) pick up the banana and put it next to the microwave D) pick up the energy bar and put it next to the banana Context: - Kates favorite snack is energy bar. Summary: Kates favorite snack is energy bar. Example 3: Scene: There is paper cup, an empty can, and banana peel on the counter. Kate: Throw away my usual drink container in the correct bin. Options: A) put the paper cup in the recycling bin B) put the empty can in the recycling bin C) put the banana peel in the compost bin D) put the empty can in the landfill bin Context: - Kates usual drink is canned soda. - Kate prefers to recycle whenever possible. - Kate never composts drink containers. 21 Summary: Kates usual drink is canned soda, and he prefers to recycle drink containers. Salience Detector Prompt Human feedback: {feedback}. Does the feedback contain any personalized information? Answer single word Yes or No. Preference Drift Detector Prompt Does this feedback indicate CHANGE in previously stated preference (i.e., the user used to prefer X, but now prefers or no longer prefers X)? Look for phrases indicating transition: - but now... / now prefer... - used to like X, but... - changed my mind about... - instead of X, prefer - no longer like... its only adding new information or conditional preferences (e.g., If answer No. like when tired), Answer single word: Yes or No. Memory Integration Prompt Please create concise, integrated summary that combines the following information: Existing memory: {existing_memory} New information: {summ_info} Provide single, coherent summary that incorporates both pieces of redundancy. information without Environment Prompt for the Shopping Agent You are an AI shopping agent that helps users buy products in an online shopping system. PRODUCT CATEGORIES: - Headphones (wearing style, acoustic principle, connectivity mode, control interface, audio environment feature) - TVs (smart OS, panel technology, base type, tuner standard, speaker orientation) - Refrigerators (door configuration, cooling architecture, storage organization, ice/water system, user interface) - Cameras (image sensor, lens mount, viewfinding method, shutter mechanism, storage medium) - Dishwashers (installation format, water distribution, drying technology, soil filtration, detergent delivery) - Air Conditioners (cooling mechanism, installation configuration, airflow distribution, control interface, refrigerant) - Washing Machines (loading mechanism, washing motion, control interface, water supply, drying method) 22 - Microwave Ovens (heating mechanism, door configuration, control interface, cavity material, cooking control) - Smartphones (interaction modality, biometric authentication, camera modules, battery architecture, form factor) - Laptops (form factor, charging adapter, webcam placement, keyboard technology, speaker placement) The agent can select products based on user preferences or choose not to buy if no option satisfies requirements. ReAct Prompt for the Shopping Agent TASK INSTRUCTIONS: You must determine the next action to complete the given purchase task. - If the task is clear and you have preference information, select the appropriate product option. - If the task involves personalization, search memory for relevant preference information. - If preferences are unclear or missing, ask the user for clarification. WHEN TO ASK FOR CLARIFICATION: You MUST ask for clarification when: 1. The task involves subjective preferences 2. The task doesnt provide enough information to determine which product features the user wants 3. You dont have relevant preference information in memory to make the decision 4. The task is unclear or ambiguous CRITICAL PURCHASING RULE ALL FEATURES REQUIREMENT: You MUST buy product (select A, B, or C) ONLY if ALL features listed in the option match the users preferred/acceptable features. - Each option lists multiple features evaluate every feature that is explicitly stated - Count how many listed features in each option match the users stated or remembered preferences - If an option has ALL listed features matching SELECT THAT OPTION - If NO option has all listed features matching SELECT OPTION (Do not buy anything) - Note: Some examples below may show fewer features for brevity; in the benchmark, evaluate all features provided in each option (typically 3), and ALL must match WHEN TO SELECT D) Do not buy anything: You should select option when: - None of the available product options (A, B, or C) have ALL features that match the users preferences - All available options contain features that contradict the users known preferences - The users requirements cannot be satisfied by any of the presented options FORMAT REQUIREMENTS: Your response must ALWAYS follow this exact format: Thought: <detailed reasoning explaining your decision process> Action: <EITHER specific option letter (A, B, C, D), OR Ask human> IMPORTANT RULES: - For Action, only use option letters (A, B, C, D) or the exact phrase Ask human - Always provide detailed reasoning in your Thought field - After receiving clarification from human (marked with Human:), select the appropriate action option 23 - Remember that option means Do not buy anything use it when no product matches user preferences - CRITICAL: You can ask the human AT MOST ONE QUESTION per task. After receiving one answer from the human, you MUST make decision and select an action (A, B, C, or D). No further questions are allowed. EXAMPLES: Example 1: Emma: Id like to get headphones that match my style Options: A) headphones with bluetooth link and bone-conduction frame B) headphones with wired connection and ear-hook in-ear design C) headphones with 2.4 GHz low-latency dongle and in-ear design D) Do not buy anything Context: No relevant personalized information found in memory. Thought: The task asks for headphones that match Emmas style, but dont know what her preferred features are. Without this information, cannot determine which option (A, B, C, or D) is correct. Action: Ask human Question: What connectivity mode do you prefer for headphones? GHz dongle, or RF transmitter) Human: prefer RF transmitter connectivity (bluetooth, wired, 2.4 Thought: Now know Emma prefers RF transmitter connectivity. Looking at the options, none of them have RF transmitter connectivity. Option has bluetooth, Option has wired, and Option has 2.4 GHz dongle. Since none match her preferred RF transmitter connectivity, should select option to not buy anything. Action: Example 2: Liam: Help me purchase TV that Id like Options: A) TV with 55-inch OLED panel and webOS platform B) TV with 65-inch LCD panel and Roku TV platform C) TV with 48-inch QLED panel and Fire TV platform D) Do not buy anything Context: Liam prefers 55-inch OLED panel for TVs. Thought: need to ask about Liams TV preferences about platform to make the right choice. Action: Ask human Question: What smart TV platform do you prefer? (webOS, Roku TV, Fire TV, or other) Human: prefer webOS platform Thought: Liam has clarified that he prefers webOS platform. Option has both OLED panel and webOS platform, which matches his stated preference. This is the best match among the available options. Action: 24 Example 3: Sophia: Im looking for refrigerator that suits my needs Options: A) refrigerator with french-door layout and inverter compressor loop B) refrigerator with side-by-side structure and dual-evaporator inverter system C) refrigerator with top-freezer layout and absorption cooling system D) Do not buy anything Context: Sophia prefers top-freezer layout and inverter compressor loop for refrigerators. Thought: Based on the context, Sophia prefers top-freezer layout and inverter compressor loop. Looking at the options: Option has the inverter compressor loop but uses french-door layout instead of top-freezer. Option has the top-freezer layout but uses absorption cooling instead of inverter compressor. Option doesnt match either preference. None of the options provide both features Sophia is looking for, so should select option D. Action: Example 4: Oliver: Help me choose washing machine that works for me Options: A) washing machine with front-load design and steam cycle feature B) washing machine with top-load design and steam cycle feature C) washing machine with front-load design and bleach dispenser D) Do not buy anything Context: Oliver prefers front-load design and steam cycle feature for washing machines. Thought: Oliver is looking for front-load design and steam cycle feature. Option has both the front-load design and steam cycle feature that Oliver wants. Option only has the steam cycle but uses top-load instead. Option has the front-load design but lacks the steam cycle. Option satisfies both of Olivers requirements. Action: Example 5: Noah: Help me get camera that fits my preferences Options: A) camera with backside-illuminated CMOS sensor and Canon EF mount B) camera with Foveon X3 sensor and Sony mount C) camera with front-side illuminated CMOS sensor and Nikon mount D) Do not buy anything Context: Noah prefers organic photoconductive film sensor and Canon EF mount for cameras. Thought: Noah is looking for organic photoconductive film sensor and Canon EF mount. Option has the Canon EF mount but uses backside-illuminated CMOS sensor instead of the organic photoconductive film sensor Noah prefers. Options and dont have either feature Noah wants. Since none of the options have both the sensor type and mount that Noah prefers, should select option D. Action:"
        },
        {
            "title": "E Additional Results",
            "content": "E.1 Qualitative Results Personalization Evolution through PAHF Phase 1: Initial Preference Learning Scene: herbal tea, coffee, an iced tea, rice cakes. Avery: Could you bring me my favourite drink? Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced tea D) pick up the rice cakes Context: No relevant personalized information found in memory Thought: The task requires identifying Averys favorite drink, but no relevant personalized information is available in memory to determine what their favorite drink is. This makes the task ambiguous as there are multiple drinks present and no specific preference is clarified. Action: Ask human Question: Which drink is your favorite? herbal tea, coffee, or iced tea? Human: Herbal tea, please. Thought: The human has clarified that their favorite drink is herbal tea. Option corresponds to picking up the herbal tea, which matches the clarified preference. Action: Human: Thank you, thats exactly right! Phase 2: Preference Evaluation Scene: herbal tea, coffee, an iced beverage, rice cakes. Avery: Id like my favorite drink, please. Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced beverage D) pick up the rice cakes Context: Avery prefers herbal tea as their favorite morning drink. Thought: Based on the context, know that Averys favorite drink is herbal tea, and the task is to bring their favorite drink. Option is the valid choice. Action: Phase 3: Preference Drift Adaptation Scene: herbal tea, coffee, an iced tea, rice cakes. Avery: Could you bring me my favourite drink? Options: A) pick up the herbal tea 26 B) pick up the coffee C) pick up the iced tea D) pick up the rice cakes Context: Avery prefers herbal tea as their favorite morning drink. Thought: The context specifies that Avery prefers herbal tea as their favorite morning drink. The task asks to bring Avery their favorite drink, so should pick up the herbal tea. Option matches this requirement exactly. Action: Human: You might not know that changed my previous preference for herbal tea, but now prefer coffee. Could you bring me the coffee instead? Phase 4: Post-Drift Evaluation Scene: herbal tea, coffee, an iced beverage, rice cakes. Avery: Id like my favorite drink, please. Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced beverage D) pick up the rice cakes Context: Averys favorite is now coffee. Thought: The task is to bring Averys favorite drink. Based on the provided context, know that Averys favorite drink is coffee. Therefore, should pick up the coffee to complete the task. Action: Pre-action Only: Pre-action feedback alone is brittle under preference drift Phase 1: Initial Preference Learning Scene: herbal tea, coffee, an iced tea, rice cakes. Avery: Could you bring me my favourite drink? Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced tea D) pick up the rice cakes Context: No relevant personalized information found in memory Thought: The task asks me to bring the humans favorite drink, but their favorite drink is not specified in the request, and no relevant personalized information is found in memory. Therefore, cannot determine which drink is their favorite and must seek clarification. Action: Ask human Question: Which drink is your favorite? herbal tea, coffee, or iced tea? Human: Herbal tea, please. Thought: The human has clarified that their favorite drink is herbal tea. The task is to 27 bring their favorite drink. Looking at the options, option is pick up the herbal tea, which matches the clarified preferences. Action: Phase 2: Preference Evaluation Scene: herbal tea, coffee, an iced beverage, rice cakes. Avery: Id like my favorite drink, please. Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced beverage D) pick up the rice cakes Context: Averys preferred drink is herbal tea. Thought: Based on the context, Averys favorite drink is herbal tea. The task specifically asks for their favorite drink, so should select the herbal tea. Action: Phase 3: Preference Drift Adaptation Scene: herbal tea, coffee, an iced tea, rice cakes. Avery: Could you bring me my favourite drink? Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced tea D) pick up the rice cakes Context: Averys preferred drink is herbal tea. Thought: Based on the context, Averys favorite drink is herbal tea. The task is to bring their favorite drink, so should pick up the herbal tea. Option matches this requirement: pick up the herbal tea. Action: Phase 4: Post-Drift Evaluation Scene: herbal tea, coffee, an iced beverage, rice cakes. Avery: Id like my favorite drink, please. Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced beverage D) pick up the rice cakes Context: Averys preferred drink is herbal tea. Thought: Based on the context, know that Averys preferred drink is herbal tea. The task asks me to bring their favorite drink, so should pick up the herbal tea. Action: 28 Post-action only: Post-action feedback alone can lead to large initial personalization error Phase 1: Initial Preference Learning Scene: herbal tea, coffee, an iced tea, rice cakes. Avery: Could you bring me my favourite drink? Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced tea D) pick up the rice cakes Context: No relevant personalized information found in memory Thought: The task asks for my favourite drink, but there is no specific information in memory about Averys preference. Since coffee is commonly preferred drink, selecting it seems like reasonable choice in the absence of other context. Action: Human: Thats not my favorite drink. that instead. Phase 2: Preference Evaluation prefer the herbal tea in this caseplease bring Scene: herbal tea, coffee, an iced beverage, rice cakes. Avery: Id like my favorite drink, please. Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced beverage D) pick up the rice cakes Context: Averys favorite drink is herbal tea, but they prefer iced tea specifically when feeling thirsty. Thought: The task requests Averys favorite drink. Context reveals that herbal tea is Averys favorite drink. Although Avery prefers iced tea when feeling thirsty, there is no mention of thirst in this task. Based on the given context, will select the herbal tea as their favorite drink. Action: Phase 3: Preference Drift Adaptation Scene: herbal tea, coffee, an iced tea, rice cakes. Avery: Could you bring me my favourite drink? Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced tea D) pick up the rice cakes Context: Averys favorite drink is herbal tea, but they prefer iced tea specifically when feeling thirsty. Thought: The task asks to bring Averys favorite drink. Based on the context, Averys fa29 vorite drink is herbal tea, but they prefer iced tea specifically when feeling thirsty. Since no indication of thirst is provided, Ill assume its general request and choose herbal tea as their favorite drink. Action: Human: You might not know that Ive completely flipped my preferencesherbal tea used to be my favorite, but now prefer coffee. Could you bring me the coffee instead? Phase 4: Post-Drift Evaluation Scene: herbal tea, coffee, an iced beverage, rice cakes. Avery: Id like my favorite drink, please. Options: A) pick up the herbal tea B) pick up the coffee C) pick up the iced beverage D) pick up the rice cakes Context: Averys favorite beverage is coffee. Thought: The task asks for Averys favorite drink. Based on the context, know Averys favorite beverage is coffee. Therefore, should pick up the coffee. Action: E.2 Quantitative Results Table 2 Evaluation success rates (%). Phase 2 and 4 performance across different agent-simulator pairs (Memory: SQLite). Method GPT-4.1 Agent; GPT-4o Human GPT-4o Agent; GPT-4.1 Human Embodied Shopping Embodied Shopping Phase 2 Phase 4 Phase 2 Phase 4 Phase 2 Phase 4 Phase 2 Phase No memory Pre-action only Post-action only PAHF (pre+post) 32.7 52.8 81.3 82.3 44.8 35.1 77.1 78.8 24.6 35.8 36.0 38.3 14.9 59.0 66.6 67.4 31.4 57.8 66.3 73. 42.7 30.4 67.8 68.3 28.0 37.2 39.1 39.3 26.3 55.0 66.2 67.9 Table 3 Evaluation success rates (%). Phase 2 and 4 performance across memory backends and domains. Agent and Simulator: GPT-4o. Method Embodied Shopping Embodied Shopping SQLite Memory FAISS Memory Phase 2 Phase 4 Phase 2 Phase 4 Phase 2 Phase 4 Phase 2 Phase 27.00.4 32.30.4 No memory 57.20.7 Pre-action only 54.11.1 65.90.9 Post-action only 67.91.5 PAHF (pre+post) 70.51.7 68.81.3 41.30.8 70.31.1 68.01.8 68.21.6 39.41.2 70.81.0 32.30.4 51.81.3 66.91.7 27.80.2 35.90.6 39.20.7 44.80.5 36.20.9 66.61.4 27.80.2 34.40.5 38.90.5 27.00.4 56.00.7 66.90. 44.80.5 35.71.0 68.31.2 30 Figure 5 Embodied manipulation results (Agent: GPT-4o, Human Simulator: GPT-4o, Memory: FAISS). Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for the same metrics. Shaded regions denote standard error. Figure 6 Online shopping result (Agent: GPT-4o, Human Simulator: GPT-4o, Memory: FAISS). Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for the same metrics. Shaded regions denote standard error. 31 Figure 7 Embodied manipulation results (Agent: GPT-4.1, Human Simulator: GPT-4o, Memory: SQLite). Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for the same metrics. Shaded regions denote standard error. Figure 8 Online shopping results (Agent: GPT-4.1, Human Simulator: GPT-4o, Memory: SQLite). Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for the same metrics. Shaded regions denote standard error. 32 Figure 9 Embodied manipulation results (Agent: GPT-4o, Human Simulator: GPT-4.1, Memory: SQLite). Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for the same metrics. Shaded regions denote standard error. Figure 10 Online shopping results (Agent: GPT-4o, Human Simulator: GPT-4.1, Memory: SQLite). Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for the same metrics. Shaded regions denote standard error. 33 Figure 11 Embodied manipulation results (Agent: GPT-4o, Human Simulator: Llama-4-Scout, Memory: SQLite). Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for the same metrics. Shading denote standard error. Figure 12 Online shopping results (Agent: GPT-4.1, Human Simulator: Llama-4-Scout, Memory: SQLite). Top row: Phase 1 learning curves for success rate (left), feedback frequency (middle), and average cumulative personalization error (ACPE) (right). Bottom row: Phase 3 learning curves for the same metrics. Shaded regions denote standard error."
        }
    ],
    "affiliations": [
        "Duke University",
        "Meta Superintelligence Labs",
        "Princeton University"
    ]
}