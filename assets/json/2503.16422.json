{
    "paper_title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
    "authors": [
        "Yuheng Yuan",
        "Qiuhong Shen",
        "Xingyi Yang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) \\textbf{Short-Lifespan Gaussians}: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) \\textbf{Inactive Gaussians}: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present \\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a $41\\times$ reduction in storage and $9\\times$ faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io."
        },
        {
            "title": "Start",
            "content": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering Yuan Yuheng Qiuhong Shen Xingyi Yang Xinchao Wang National University of Singapore {yuhengyuan,qiuhong.shen,xyang}@u.nus.edu, xinchao@nus.edu.sg 5 2 0 2 0 2 ] . [ 1 2 2 4 6 1 . 3 0 5 2 : r Figure 1. Compressibility and Rendering Speed. We introduce 4DGS-1K, novel compact representation with high rendering speed. In contrast to 4D Gaussian Splatting (4DGS) [40], we can achieve rasterization at 1000+ FPS while maintaining comparable photorealistic quality with only 2% of the original storage size. The right figure is the result tested on the N3V [18] datasets, where the radius of the dot corresponds to the storage size."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 4D Gaussian Splatting (4DGS) has recently gained considerable attention as method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) ShortLifespan Gaussians: 4DGS uses large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) Inactive Gaussians: When rendering, only small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present 4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the SpatialTemporal Variation Score, new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves 41 reduction in storage and 9 faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at 4DGS-1K. Novel view synthesis for dynamic scenes allows for the creation of realistic representations of 4D environments, which is essential in fields like computer vision, virtual reality, and augmented reality. Traditionally, this area has been led by neural radiance fields (NeRF) [2, 12, 18, 21, 25], which model opacity and color over time to depict dynamic scenes. While effective, these NeRF-based methods come with high training and rendering costs, limiting their practicality, especially in real-time applications and on devices with limited resources. Recently, point-based representations like 4D Gaussian Splatting (4DGS) [40] have emerged as strong alternatives. 4DGS models dynamic scene using set of 4D Gaussian primitives, each with 4-dimensional mean and 4 4 covariance matrix. At any given timestamp, 4D Gaussian is decomposed into set of conditional 3D Gaussians and marginal 1D Gaussian, the latter controlling the opacity at that moment. This mechanism allows 4DGS to effectively capture both static and dynamic features of scene, enabling high-fidelity dynamic scene reconstruction. However, representing dynamic scenes with 4DGS is both storage-intensive and slow. Specifically, 4DGS often requires millions of Gaussians, leading to significant storage demands (averaging 2GB for each scene on the N3V [18] dataset) and suboptimal rendering speed. In comparison, mainstream deformation field methods [39] require only about 90MB for the same dataset. Therefore, reducing the storage size of 4DGS [40] and improving rendering speed are essential for efficiently representing complex dynamic scenes. We look into the cause of such an explosive number of Gaussian and place specific emphasis on two key issues. (Q1) large portion of Gaussians exhibit short temporal span. In empirical experiments, 4DGS tends to favor flicking Gaussians to fit complex dynamic scenes, which just influence short portion of the temporal domain. This necessitates that 4DGS relies on large number of Gaussians to reconstruct high-fidelity scene. As result, substantial storage is needed to record the attributes of these Gaussians: (Q2) Inactive Gaussians lead to redundant computation. During rendering, 4DGS needs to process all Gaussians. However, only very small portion of Gaussians are active at that moment. Therefore, most of the computation time is spent on inactive Gaussians. This phenomenon greatly hampers the rendering speed. In this paper, we introduce 4DGS-1K, framework that significantly reduces the number of Gaussians to minimize storage requirements and speedup rendering while maintaining high-quality reconstruction. To address these issues, 4DGS-1K introduces two-step pruning approach: Pruning Short-Lifespan Gaussians. We propose novel pruning criterion called the spatial-temporal variation score, which evaluates the temporal impact of each Gaussian. Gaussians with minimal influence are identified and pruned, resulting in more compact scene representation with fewer Gaussians with short temporal span. Filtering Inactive Gaussians. To further reduce redundant computations during rendering, we use key-frame temporal filter that selects the Gaussians needed for each frame. On top of this, we share the masks for adjacent frames. This is based on our observation that Gaussians active in adjacent frames often overlap significantly. Besides, the pruning in step 1 enhances the masking process in step 2. By pruning Gaussians, we increase the temporal influence of each Gaussian, which allows us to select sparser key frames and further reduce storage requirements. We have extensively tested our proposed model on various dynamic scene datasets including real and synthetic scenes. As shown in Fig. 1, 4DGS-1K reduces storage costs by 41 on the Neural 3D Video datasets [18] while maintaining equivalent scene representation quality. Crucially, it enables real-time rasterization speeds exceeding 1,000 FPS. These advancements collectively position 4DGS-1K as practical solution for high-fidelity dynamic scene modeling without compromising efficiency. In summary, our contributions are three-fold: We delve into the temporal redundancy of 4D Gaussian Splatting, and explain the main reason for the storage pressure and suboptimal rendering speed. We introduce 4DGS-1K, compact and memory-efficient framework to address these issues. It consists of two key components, spatial-temporal variation score-based pruning strategy and temporal filter. Extensive experiments demonstrate that 4DGS-1K not only achieves substantial storage reduction of approximately 41 but also accelerates rasterization to 1000+ FPS while maintaining high-quality reconstruction. 2. Related Work 2.1. Novel view synthesis for static scenes Recently, neural radiance fields(NeRF) [25] have achieved encouraging results in novel view synthesis. NeRF [25] represents the scene by mapping 3D coordinates and view dependency to color and opacity. Since NeRF [25] requires sampling each ray by querying the MLP for hundreds of points, this significantly limits the training and rendering speed. Subsequent studies [5, 11, 26, 31, 32, 35, 37, 38] have attempted to speed up the rendering by introducing specialized designs. However, these designs also constrain the widespread application of these models. In contrast, 3D Gaussian Splatting(3DGS) [14] has gained significant attention, which utilizes anisotropic 3D Gaussians to represent scenes. It achieves high-quality results with intricate details, while maintaining real-time rendering performance. 2.2. Novel view synthesis for dynamic scenes Dynamic NVS poses new challenges due to the temporal variations in the input images. Previous NeRF-based dynamic scene representation methods [2, 4, 12, 17, 18, 21, 24, 30, 34, 36] handle dynamic scenes by learning mapping from spatiotemporal coordinates to color and density. Unfortunately, these NeRF-based models are constrained in their applications due to low rendering speeds. Recently, 3D Gaussians Splatting [14] has emerged as novel explicit representation, with many studies [3, 6, 13, 22, 39, 41] attempting to model the dynamic scenes based on it. 4D Gaussian Splatting(4DGS) [40] is one of the representatives. It utilizes set of 4D Gaussian primitives. However, 4DGS often requires huge redundant number of Gaussians for dynamic scenes. These Gaussians lead to tremendous storage and suboptimal rendering speed. To this end, we focus on analyzing the temporal redundancy of 4DGS [40] in hopes of developing novel framework to achieve lower storage requirements and higher rendering speeds. 2.3. Gaussian Splatting Compression 3D Gaussian-based large-scale scene reconstruction typically requires millions of Gaussians, resulting in the requirement of up to several gigabytes of storage. Therefore, subsequent studies have attempted to tackle these is2 sues. Specifically, Compgs [27] and Compact3D [16] employ vector quantization to store Gaussians within codebooks. Concurrently, inspired by model pruning, some studies [1, 8, 9, 20, 28, 29] have proposed criterion to prune Gaussians by specified ratio. However, compared to 3DGS [14], 4DGS [40] introduces an extra temporal dimension to enable dynamic representation. Previous 3DGSbased methods may therefore be unsuitable for 4DGS. Consequently, we first identify key limitation leading to this problem, referred as temporal redundancy. Furthermore, we propose novel pruning criterion leveraging spatialtemporal variation, and temporal filter to achieve more efficient storage requirements and higher rendering speed. Temporal Redundancy. Despite achieving high quality, 4DGS requires huge number of Gaussians to model dynamic scenes. We identify key limitation leading to this problem: 4DGS represents scenes through temporally independent Gaussians that lack explicit correlation across time. This means that, even static objects are redundantly represented by hundreds of Gaussians, which inconsistently appear or vanish across timesteps. We refer to this phenomenon as temporal redundancy. As result, scenes end up needing more Gaussians than they should, leading to excessive storage demands and suboptimal rendering speeds. In Sec. 4, we analyze the root causes of this issue and propose set of solutions to reduce the count of Gaussians. 3. Preliminary of 4D Gaussian Splatting 4. Methodology framework builds on 4D Gaussian Splatting Our [40], which reconstructs dynamic scenes by (4DGS) optimizing collection of anisotropic 4D Gaussian primitives. For each Gaussian, it is characterized by 4D mean µ = (µx, µy, µz, µt) R4 coupled with covariance matrix Σ R44. By treating time and space dimensions equally, the 4D covariance matrix Σ can be decomposed into scaling matrix S4D = (sx, sy, sz, st) R4 and rotation matrix R4D R44. R4D is represented by pair of left quaternion ql R4 and right quaternion qr R4. During rendering, each 4D Gaussian is decomposed into conditional 3D Gaussian and 1D Gaussian at specific time t. Moreover, the conditional 3D Gaussian can be derived from the properties of the multivariate Gaussian with: µxyzt = µ1:3 + Σ1:3,4Σ1 Σxyzt = Σ1:3,1:3 Σ1:3,4Σ 4,4(t µt) 4,4Σ4,1:3 (1) Here, µ1:3 R3 and Σ1:3,1:3 R33 denote the spatial mean and covariance, while µt and Σ4,4 are scalars representing the temporal components. To perform rasterization, given pixel under view and timestamp t, its color I(u, v, t) can be computed by blending visible Gaussians that are sorted by their depth: I(u, v, t) = (cid:88) i1 (cid:89) ci(d)αi (1 αj) (2) j=1 with αi = pi(t)pt(u, vt)σi pi(t) (t; µt, Σ4,4) (3) where ci(d) is the color of each Gaussian, and αi is given by evaluating 2D Gaussian with covariance Σ2D multiplied with learned per-point opacity σi and temporal Gaussian distribution pi(t). In the following discussion, we denote Σ4,4 as Σt for simplicity. Our goal is to compress 4DGS by reducing the number of Gaussians while preserving rendering quality. To achieve this, we first analyze the redundancies present in 4DGS, as detailed in Sec. 4.1. Building on this analysis, we introduce 4DGS-1K in Sec. 4.2, which incorporates set of compression techniques designed for 4DGS. 4DGS-1K enables rendering speeds of over 1,000 FPS on modern GPUs. 4.1. Understanding Redundancy in 4DGS This section investigates why 4DGS requires an excessive number of Gaussians to represent dynamic scenes. In particular, we identify two key factors. First, 4DGS models object motion using large number of transient Gaussians that inconsistently appear and disappear across timesteps, leading to redundant temporal representations. Second, for each frame, only small fraction of Gaussians actually contribute to the rendering. We discuss those problems below. Massive Short-Lifespan Gaussians. We observe that 4DGS tends to store numerous Gaussians that flicker in time. We refer to these as Short-Lifespan Gaussians. To investigate this property, we analyze the Gaussians opacity, which controls visibility. Intuitively, Short-Lifespan Gaussians exhibit an opacity pattern that rapidly increases and then suddenly decreases. In 4DGS, this behavior is typically reflected in the time variance parameter Σtsmall Σt values indicate short lifespan. Observations. Specifically, we plot the distribution of Σt for all Gaussians in the Sear Steak scene. As shown in Fig. 2a, most of Gaussians has small Σt values (e.g. 70% have Σt < 0.25). Moreover, as shown in Fig. 3, we visualize the spatial distribution of Σt values. We take the reciprocal of Σt and then normalize it. Therefore, brighter regions in the image indicate smaller Σt. Most of these Gaussians are concentrated along the edges of moving objects. Therefore, in 4DGS, nearly all Gaussians have short lifespan, especially around the fast-moving objects. This property leads to high storage needs and slower rendering. 3 (a) (b) (c) Figure 2. Temporal redundancy Study. (a) The Σt distribution of 4DGS. The red line shows the result of vanilla 4DGS. The other two lines represent our model has effectively reduced the number of transient Gaussians with small Σt. (b) The active ratio during rendering at different timestamps. It demonstrates that most of the computation time is spent on inactive Gaussians in vanilla 4DGS. However, 4DGS-1K can significantly reduce the occurrence of inactive Gaussians during rendering to avoid unnecessary computations. (c) This figure shows the IoU between the set of active Gaussians in the first frame and frame t. It proves that active Gaussians tend to overlap significantly across adjacent frames. dundant computations while preserving rendering quality. 4.2. 4DGS-1K for Fast Dynamic Scene Rendering Building on the analysis above, we introduce 4DGS-1K, suite of compression techniques specifically designed for 4DGS to eliminate redundant Gaussians. As shown in Fig. 4, this process involves two key steps. First, we identify and globally prune unimportant Gaussians with low SpatialTemporal Variation Score in Sec. 4.2.1. Second, we apply local pruning using temporal filter to inactive Gaussians that are not needed at each timestep in Sec. 4.2.2. 4.2.1. Pruning with Spatial-Temporal Variation Score We first prune unimportant 4D Gaussians to improve efficiency. Like 3DGS, we remove those that have low impact on rendered pixels. Besides, we additionally remove short-lifespan Gaussiansthose that persist only briefly over time. To achieve this, we introduce novel spatialtemporal variation score as the pruning criterion for 4DGS. It is composed of two parts, spatial score that measures the Gaussians contributions to the pixels in rendering, and temporal score considering the lifespan of Gaussians. Spatial score. Inspired by the previous method [8, 9] and α-blending in 3DGS [14], we define the spatial score by aggregating the ray contribution of Gaussian gi along all rays across all input images at given timestamp. It can accurately capture the contribution of each Gaussian to one pixel. Consequently, the spatial contribution score is obtained by traversing all pixels: HW (cid:88) i1 (cid:89) αi = (1 αj) k=1 j=1 (4) (cid:81)i1 j=1(1 αj) reflects the contribution of ith where αi Gaussian to the final color of all pixels according to the alpha composition in Eq. (2). Figure 3. Visualizations of Distribution of Σt. Most of these Gaussians are concentrated along the edges of moving objects. Inactive Gaussians. Another finding is that, during the forward rendering, actually, only small fraction of Gaussians are contributing. Interestingly, active ones tend to overlap significantly across adjacent frames. To quantify this, we introduce two metrics: (1) Active ratio. This ratio is defined as the proportion of the total number of active Gaussians across all views at any moment relative to the total number of Gaussians. (2) Activation Intersection-over-Union (IoU). This is computed as IoU between the set of active Gaussians in the first frame and in frame t. Observations. Again, we plot the two metrics from Sear Steak scene. As shown in Fig. 2b, nearly 85% of Gaussians are inactive at each frame, even though all Gaussians are processed during rendering. Moreover, Fig. 2c demonstrates that the active Gaussians remain quite consistent over time, with an IoU above 80% over 20-frame window. The inactive gaussians bring significant issue in 4DGS, because each 4D Gaussian must be decomposed into 3D Gaussian and 1D Gaussian before rasterization (see Eq. (1)). Therefore, large portion of computational resources is wasted on inactive Gaussians. In summary, redundancy in 4DGS comes from massive Short-Lifespan Gaussians and inactive Gaussians. These insights motivate our compression strategy to eliminate re4 Figure 4. Overview of 4DGS-1K. (a) We first calculate the spatial-temporal variation score for each 4D Gaussian on training views, to prune Gaussians with short lifespan (The Red Gaussian). (b) The temporal filter is introduced to filter out inactive Gaussians before the rendering process to alleviate suboptimal rendering speed. At given timestamp t, the set of Gaussians participating in rendering is derived from the two adjacent key-frames, t0 and t0+t . Temporal score. It is expected to assign higher temporal score to Gaussians with longer lifespan. To quantify this, we compute the second derivative of temporal opacity function pi(t) defined in Eq. (3). The second derivative p(2) (t) is computed as p(2) (t) = ( (t µt)2 Σ2 Intuitively, large second derivative magnitude corresponds to unstable, short-lived Gaussians, while low second derivative indicates smooth, persistent ones. )pi(t) 1 Σt (5) Moreover, since the second derivative spans the real number domain R, we apply tanh function to map it to the interval (0, 1). Consequently, the score for opacity variation, , of each Gaussian gi,t is expressed as: V = (cid:88) t=0 1 (cid:12) (cid:12)p(2) (cid:12) 0.5 tanh( . (cid:12) (cid:12) (t) (cid:12)) + 0.5 (6) In addition to the opacity range rate, the volume of 4D Gaussians is necessary to be considered, as described in Eq. (1). The volume should be normalized following the method in [8], denoted as γ(S 4D) = orm(V (S 4D)). Therefore, the final temporal score = γ(S4D ) Finally, by aggregating both spatial and temporal score, the spatial-temporal variation score Si can be written as: Si = (cid:88) t=0 S (7) Pruning. All 4D Gaussians are ranked based on their spatial-temporal variation score Si, and Gaussians with 5 lower scores are pruned to reduce the storage burden of 4DGS [40]. The remaining Gaussians are optimized over set number of iterations to compensate for minor losses resulting from pruning. 4.2.2. Fast rendering with temporal filtering Our analysis reveals that inactive Gaussians induces unnecessary computations in 4DGS, significantly slowing down rendering. To address this issue, we introduce temporal filter that dynamically selects active Gaussians. We observed that active Gaussians in adjacent frames overlap considerably (as detailed in Sec. 4.1), which allows us to share their corresponding masks across window of frames. Key-frame based Temporal Filtering. Based on this observation, we design key-frame based temporal filtering for active Gaussians. We select sparse key-frames at even intervals and share their masks with surrounding frames. Specifically, we select list of key-frame timestamps {ti}T i=0, where depends on the chosen interval t. For each ti, we render the images from all training views at current timestamp and calculate the visibility list {mi,j}N j=1, where mi,j is the visibility mask obtained by Eq. (2) from the jth training viewpoint at timestamp ti and is the number of training views at current timestamp. The final set of active Gaussian masks is given by (cid:110)(cid:83)N j=1 mi,j (cid:111)T . i= Filter based Rendering. To render the images from any viewpoint at given timestamp ttest, we consider its two nearest key-frames, denoted as tl and tr. Then, we perform rasterization while only considering the Gaussians marked by mask . This method explicitly filters (cid:110)(cid:83)N (cid:111) j=1 mi,j i=l,r Table 1. Quantitative comparisons on the Neural 3D Video Dataset. Method Neural Volume1[21] DyNeRF1[18] StreamRF[17] HyperReel[2] K-Planes[12] Dynamic 3DGS[23] 4DGaussian[39] E-D3DGS[3] STG[19] 4D-RotorGS[7] MEGA[43] Compact3D[16] 4DGS[40] 4DGS2[40] Ours Ours-PP PSNR 22.80 29.58 28.26 31.10 31.63 30.67 31.15 31.31 32.05 31.62 31.49 31.69 32.01 31.91 31.88 31.87 SSIM - - - 0.927 - 0.930 0.940 0.945 0.946 0.940 - 0.945 - 0.946 0.946 0.944 LPIPS 0.295 0.083 - 0.096 0.018 0.099 0.049 0.037 0.044 0.140 0.056 0.054 0.055 0.052 0.052 0.053 Storage(MB) - 28 5310 360 311 2764 90 35 200 - 25 15 - 2085 418 50 FPS - 0.015 10.90 2.00 0.30 460 30 74 140 277 77 186 114 90 805 Raster FPS - - - - - - - - - - - - - 118 1092 1092 #Gauss - - - - - - - - - - - - - 3333160 666632 666632 1 The metrics of the model are tested without coffee martini and the resolution is set to 1024 768. 2 The retrained model from the official implementation. Table 2. Quantitative comparisons on the D-NeRF Dataset. Method DNeRF[30] TiNeuVox[10] K-Planes[12] 4DGaussian[39] Deformable3DGS[41] 4D-RotorGS[7] 4DGS[40] 4DGS1[40] Ours Ours-PP PSNR 29.67 32.67 31.07 32.99 40.43 34.26 34.09 32.99 33.34 33. SSIM 0.95 0.97 0.97 0.97 0.99 0.97 0.98 0.97 0.97 0.97 LPIPS 0.08 0.04 0.02 0.05 0.01 0.03 0.02 0.03 0.03 0.03 Storage(MB) - - - 18 27 112 - 278 42 7 FPS 0.1 1.6 1.2 104 70 1257 - 376 1462 1462 Raster FPS - - - - - - - 1232 2482 2482 #Gauss - - - - 131428 - - 445076 66460 1 The retrained model from the official implementation. out inactive Gaussians to speed up rendering. Note that using long intervals may overlook some Gaussians, reducing rendering quality. Therefore, we fine-tune Gaussians recorded by the masks to compensate for losses. 5. Experiment 5.1. Experimental Settings Datasets. We utilize two dynamic scene datasets to demonstrate the effectiveness of our method: (1) Neural 3D Video Dataset (N3V) [18]. This dataset consists of six dynamic scenes, and the resolution is 2704 2028. For fair comparison, we align with previous work [19, 40] by conducting evaluations at half-resolution of 300 frames. (2) D-NeRF Dataset [30]. This dataset is monocular video dataset comprising eight videos of synthetic scenes. We choose standard test views that originate from novel camera positions not encountered during the training process. Evaluation Metrics. To evaluate the quality of rendering dynamic scenes, we employ several commonly used image quality assessment metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [42]. Following the previous work, LPIPS [42] is computed using AlexNet [15] and VGGNet [33] on the N3V dataset and the D-NeRF dataset, respectively. Moreover, we report the number of Gaussians and storage. To demonstrate the improvement in rendering speed, we report two types of FPS: (1) FPS. It considers the entire rendering function. Due to interference from other operations, it cant effectively demonstrate the acceleration achieved by our method. (2) Raster FPS. It only considers the rasterization, the most computationally intensive component during rendering. Baselines. Our primary baseline for comparison is 4DGS [40], which serves as the foundation of our model. Moreover, we compare 4DGS-1K with two concurrent works on 4D compression, MEGA [43] and Compact3D [16]. Certainly, we conduct comparisons with 4DRotorGS [7] which is another form of representation for 4D Gaussian Splatting with the capability for real-time render6 Ground Truth 4DGS Ours Ours-PP Ground Truth 4DGS Ours Ours-PP (a) Results on Sear Steak Scene. (b) Results on Trex Scene. Figure 5. Qualitative comparisons of 4DGS and our method. Table 3. Ablation study of per-component contribution. PP PSNR SSIM LPIPS Storage(MB) FPS Raster FPS #Gauss 31.91 31.51 29.56 31.92 31.88 31.63 31. 0.9458 0.9446 0.9354 0.9462 0.9457 0.9452 0.9444 0.0518 0.0539 0.0605 0.0513 0.0524 0.0524 0.0532 2085 2091 2091 417 418 418 50 90 242 300 312 805 789 805 118 561 561 600 1092 1080 1092 3333160 3333160 3333160 666632 666632 666632 ID MethodDataset Filter Pruning vanilla 4DGS1 1,2 2 2 d 1 The result with environment map. 2 The result without finetuning. ing speed and high-fidelity rendering results. In addition, we also compare our work against NeRF-based methods, like Neural Volume [21], DyNeRF [18], StreamRF [17], HyperReel [2], DNeRF [30] and K-Planes [12]. Furthermore, other recent competitive Gaussian-based methods are also considered in our comparison, including Dynamic 3DGS [23], STG [19], 4DGaussian [39], and E-D3DGS [3]. Implementation Details. Our method is tested in single RTX 3090 GPU. We train our model following the experiment setting in 4DGS [40]. After training, we perform the pruning and filtering strategy. Then, we fine-tune 4DGS-1K for 5,000 iterations while disabling additional clone/split operations. For pruning strategy, the pruning ratio is set to 80% on the N3V Dataset, and 85% on the D-NeRF Dataset. For the temporal filtering, we set the interval between key-frames to 20 frames on the N3V Dataset. Considering the varying capture speeds on the D-NeRF dataset, we select 6 key-frames rather than specific frame interval. Additionally, to further compress the storage of 4DGS [40], we implement post-processing techniques in our model, denoted as Ours-PP. It includes vector quantization [27] on SH of Gaussians and compressing the mask of filter into bits. Note that we dont apply environment maps implemented by 4DGS on Coffee Martini and Flame Salmon 7 scenes, which significantly affects the rendering speed. Subsequent results indicate that removing it for 4DGS-1K does not significantly degrade the rendering quality. 5.2. Results and Comparisons Comparisons on real-world dataset. Tab. 1 presents quantitative evaluation on the N3V dataset. 4DGS-1K achieves rendering quality comparable to the current baseline. Compared to 4DGS [40], we achieve 41 compression and 9 faster in rendering speed at the cost of 0.04dB reduction in PSNR. In addition, compared to MEGA [43] and Compact3D [16], two concurrent works on 4D compression, the rendering speeds are 10 and 4 faster respectively while maintaining comparable storage requirement and high quality reconstruction. Moreover, the FPS of 4DGS-1K far exceeds the current state-of-the-art levels. It is nearly twice as fast as the current fastest model, Dynamic 3DGS [23] while requiring only 1% of the storage size. Additionally, 4DGS-1K achieves better visual quality than that of Dynamic 3DGS [23], with an increase of about 1.2dB in PSNR. Compared to the storage-efficient model, E-D3DGS [3] and DyNeRF [18] we achieve an increase of over 0.5dB in PSNR and fast rendering speed. Fig. 5 offers qualitative comparisons for the Sear Steak, demonstrating that our results contain more vivid details. Comparisons on synthetic dataset. In our experiments, we benchmarked 4DGS-1K against several baselines using the monocular synthetic dataset introduced by DNeRF [30]. The result is shown in Tab. 2. Compared to 4DGS [40], our method achieves up to 40 compression and 4 faster rendering speed. It is worth noting that the rendering quality of our model even surpasses that of the original 4DGS, with an increase of about 0.38dB in PSNR. Furthermore, our approach exhibits higher rendering quality and smaller storage overhead compared to most Gaussian-based methods. We provide qualitative results in Fig. 5 for more visual assessment. 5.3. Ablation Study To evaluate the contribution of each component and the effectiveness of the pruning strategy for temporal filtering, we conducted ablation experiments on the N3V dataset [18]. More ablations are provided in the supplement(See Sec. 8). Table 4. Ablation study of Spatial-Temporal Variation Score. We compare our Spatial-Temporal Variation Score with other variants, and report the PSNR score of each scene. ID d Model 4DGS w/o Prune SS Only ST Only Si (w. p(1) Si (w. Σt) Ours (t)) Sear Steak 33.60 33.62 33.59 33.67 33.47 33. Flame Salmon 29.10 28.75 28.79 28.81 28.71 28.90 8 Pruning. As shown in Tab. 3, our pruning strategy reduces the number of Gaussians by 80%, and achieves 5 compression ratio and 5 faster rasterization speed while slightly improving rendering quality. As shown in Fig. 2a, our pruning strategy also reduces the presence of Gaussians with short lifespan. As such, 4DGS-1k processes far fewer unnecessary Gaussians (See Fig. 2b) during rendering. Furthermore, we compare our Spatial-Temporal Variation Score with serveral variants. Specific settings are described in Sec. 8. As shown in Tab. 4, using spatial and temporal scores separately reduce the PSNR. This occurs because separate scores can amplify extreme Gaussians. For instance, using only the spatial score (b) may retain Gaussians that cover just single frame but occupy large spatial volume. Our combined score balances these factors. For variant d, using the first derivative may cause some small Gaussians to have large compared to ours. Moreover, since most Gaussians have small Σt, it is difficult to distinguish them by using Σt along (See e). Moreover, as shown in Fig. 2c, the pruning process expands the range of adjacent frames. It allows larger intervals for the temporal filter. We will discuss it in the next part. Temporal Filtering. As illustrated in Tab. 3, the results of and are obtained by directly applying the filter to 4DGS without fine-tuning. It proves that this component can enhance the rendering speed of 4DGS. However, as mentioned in Sec. 4.1, the 4DGS contains huge number of short lifespan Gaussians. It results in some Gaussians being overlooked in the filter, causing slight decrease in rendering quality. However, through pruning, most Gaussians are ensured to have long lifespan, making them visible even at large intervals. Therefore, it alleviates the issue of Gaussians being overlooked (See f). Furthermore, appropriate fine-tuning allows the Gaussians in the active Gaussians list to relearn the scene features to compensate for the loss incurred by the temporal filter (See and f). 6. Conclusion In this paper, we present 4DGS-1K, compact and memory-efficient dynamic scene representation capable of running at over 1000 FPS on modern GPUs. We introduce novel pruning criterion called the spatial-temporal variation score, which eliminates significant number of redundant Gaussian points in 4DGS, drastically reducing storage requirements. Additionally, we propose temporal filter that selectively activates only subset of Gaussians during each frames rendering. This approach enables our rendering speed to far surpass that of existing baselines. Compared to vanilla 4DGS, 4DGS-1K achieves 41 reduction in storage and 9 faster rasterization speed while maintaining high-quality reconstruction. 1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering"
        },
        {
            "title": "Supplementary Material",
            "content": "The Supplementary material is organized as follows: Sec. 7: provides additional visualization results and quantitative results. Furthermore, it also shows the resource consumption which reveals the potential of 4DGS-1K for deployment on low-performance hardware. Sec. 8: provides additional ablation study. It firstly provides the variant settings in the main text, then it presents more additional ablation study to illustrate that our parameter selection is the result of trade-off between rendering quality and storage size. Sec. 9: discusses the reason of improved performance for 4DGS-1K. Furthermore, we introduce the limitations and potential future directions of 4DGS-1K. 7. Experimental Results 7.1. Per scene result We provide per-scene quantitative comparisons on the N3V Dataset [18]( Tab. 5) and D-NeRF Dataset [30]( Tab. 6). Compared to vanilla 4DGS [40], our model significantly reduces the storage requirements and enhances rendering speed while maintaining high-quality reconstruction. Fig. 12 and Fig. 13 show more visual comparisons on the N3V Dataset. Fig. 14, Fig. 15 and Fig. 16 show visual comparisons on the D-NeRF Dataset. 7.2. Resource consumption We present the resource consumption metrics, including training time, GPU memory allocation and additional storage space. On the N3V dataset [18], 4DGS-1K only takes approximately 30 minutes to fine-tune, with GPU memory allocation of 10.54GB. During rendering, it only consumes 1.62GB of GPU memory. For storage requirement, 4DGS1K requires additional storage for the mask of filter and codebook; however, these occupy only minimal portion of the total storage, approximately 1 MB per scene. These parts are also included in the final experiment results. The above results demonstrate the potential of 4DGS1K for deployment on low-performance hardware. Consequently, we further test 4DGS-1K on TITAN GPU, where 4DGS-1K maintains 200+ FPS on the N3V dataset, still far outperforming vanilla 4DGS (20 FPS). 7.3. Additional experiments for redundancy In this section, we provide additional experiments for redundancy study as supplement to Sec. 4.1. It is composed of two parts: first, the visualization of the Gaussian with short lifespan distribution, and secondly, the relationship between FPS and the number of inactive Gaussians. lifespan. Visualization of Gaussians with small In Sec. 4.1, we argue that in vanilla 4DGS, nearly all Gaussians have short lifespan, especially around the edge of fast-moving objects. Therefore, we visualize the spatial distribution of Σt to better support our redundancy study in Sec. 4.1. Specifically, we visualize the distribution of Σt at several timestamps in Sear Steak Scene. The visualization results are shown in Fig. 6. For visualization, we take the reciprocal of Σt during rendering and then normalize it. Therefore, brighter regions in the rendered image indicate smaller Σt. As shown in Fig. 6, Gaussians with short lifespan are primarily concentrated in regions of object motion, such as the moving person and dog. Moreover, we observe that Gaussians with small Σt also appear on the edges of some objects which exhibit significant color variation. This is because small Gaussians are preferred in these regions to capture the high-frequency details in the spatial dimension. As vanilla 4DGS [40] treats time and space dimensions equally, these Gaussians also have short lifespan in the temporal dimension. Relationship between FPS and the number of inactive Gaussians. In Sec. 4.1, our primary prior assumption is that the number of inactive Gaussians affects the FPS. Therefore, we visualize the relationship between FPS and the number of inactive Gaussians. However, only limiting the total number of Gaussians is incorrect in this task. As the total number increases, the number of active Gaussians and inactive Gaussians also increases, which cannot clarify whether the FPS variation is caused by active or inactive Gaussians. Consequently, we first identify the active Gaussians by rendering and then add mount of inactive Gaussians among these Gaussians. We visualize the result in the Sear Steak Scene(See Fig. 7). The FPS decreases as the number of inactive Gaussians increases. This phenomenon strongly supports our redundancy study in Sec. 4.1. 7.4. Visualizations of Pruned Gaussians We provide the visualization of pruned Gaussians in the Sear Steak Scene, as shown in Fig. 8. Our pruning strategy can accurately identify Gaussians with short lifespan(See Fig. 8c) while maintaining the high quality reconstruction(See Fig. 8d). The quantized results after pruning are presented in Tab. 3. Our pruning technique achieves 5 compression ratio and 5 faster rasterization speed while slightly improving rendering quality. 9 Figure 6. Visualizations of Distribution of Σt. els in rendering, and temporal score considering the lifespan of Gaussians. By aggregating both spatial and temporal score, our score Si can be written as: Si = (cid:88) t= S (8) Therefore, the variant scores in Tab. 4 can be written as follow. (b) Only: only considering the spatial part of our score. It can be written as: Si = (cid:88) t=0 (9) (c) i Only: only considering the temporal contribution part of our score. It can be written as: Si = (cid:88) t=0 i (b) Si (w. p(1) temporal score (t)): Replace the p(2) . It can be written as: (t) with p(1) (10) (t) in Si = = = (cid:88) t=0 (cid:88) t=0 (cid:88) t= S S γ(S4D )S (11) 1 (cid:12) (cid:12)p(1) (cid:12) 0.5 tanh( (t) (cid:12) (cid:12) (cid:12)) + 0.5 γ(S4D )S . (c) Si (w. Σt) Replace the V with Σt. It can be written as: Si = = (cid:88) t= (cid:88) t=0 S Σtγ(S4D )S (12) 10 Figure 7. Relationship between rendering speed and the number of inactive Gaussians. 7.5. Video result In this work, we propose novel framework for dynamic 3D reconstruction. Therefore, we provide several videos that are rendered from testing viewpoints on the N3V datasets and D-NeRF datasets to show the reconstruction quality and temporal consistency of 4DGS-1K. These videos are composed by concatenating each frame of 4DGS and our method. 8. Additional ablation study In this section, we firstly provide the variant settings of Tab. 4. Furthermore, in addition to the ablation study in the main text, we also investigate the impact of the pruning ratio and different key-frames intervals on rendering quality. We select three distinct scenes, Cook Spinach, Cut Roasted Beef, and Sear Steak on the N3V dataset [18] due to the varying performance across different scenes resulting from their unique characteristics. These results show that our default configuration is well-rounded choice for wide range of scenes. Variant Settings. As described in Sec. 4.2.1, our SpatialTemporal Variation Score is composed of two parts, spatial score that measures the Gaussians contributions to the pix- (a) Ground Truth (b) Distribution of Σt (c) Pruned Gaussians (d) Ours Figure 8. Visualization of Pruned Gaussians. Performance change with pruning ratio. As illustrated in Fig. 10, we analyze the relationship between the pruning ratio and rendering quality. This reveals that our spatialtemporal variation score based pruning can even improve scene rendering quality when the pruning ratio is relatively low in the Cook Spinach and Sear Steak scenes. Moreover, at higher thresholds, it can maintain results comparable to the vanilla 4DGS [40]. Our default setting represents balanced trade-off between rendering quality and storage size. This setting allows us to achieve 5 compression ratio while still maintaining high-quality reconstruction. Performance change with key-frames intervals. As shown in Fig. 11, although the temporal filter effectively improves rendering speed, its performance degrades significantly when the filter is with long-interval keyframes. However, by integrating the temporal filter into the fine-tuning process, this limitation can be mitigated. The fundamental reason is that some Gaussians which may carry critical scene information are being overlooked due to overly long intervals. However, the fine-tuning process effectively compensates for the loss of this portion of information. This allows us to utilize longer intervals to reduce the additional computational overhead caused by mask calculations. 9. Discussion (a) Ground Truth (b) 4DGS (c) Ours Improved performance. As shown in Tab. 2, our model achieves slight PSNR improvement on the D-NeRF Dataset [30]. This is because vanilla 4DGS often suffers from floaters and artifacts, due to the limited training viewpoints on the D-NeRF Datasets. However, in our study, 4DGS-1K not only can prune the Gaussians with short lifespan, but also reduce the occurrence of floaters and artifacts, as shown in Fig. 9. We visualize two scenes, Bouncingballs and Jumpingjacks, on the D-NeRF Dataset. These two scenes exhibit floaters and artifacts issues due to limited training viewpoints, as shown in the red box. However, this issue does not appear in 4DGS-1K. Through pruning and filtering, 4DGS-1K successfully mitigates the occurrence of this phenomenon. Limitations and Future work. As shown in Tab. 5 and Tab. 6, due to the acceleration provided by the temporal filter, the proportion of time spent on the rasterization process sharply decreases relative to the total rendering time. Therefore, the time consumed by preliminary preparation stages has not gradually become negligible. We hope that future work will focus on optimizing these additional operations within the rendering module to improve its computational performance. Moreover, during the pruning process, we specified predefined pruning ratio. This pruning ratio is influenced by the inherent characteristics of the scene. As shown in Fig. 10, an improper pruning ratio will cause sharp drop in rendering quality. Therefore, identifying the minimal number of Gaussians required to maintain high-quality rendering across different scenes remains challenge. Lastly, there is significant amount of existing work on Gaussian-based novel view synthesis for dynamic scenes, whereas our model is specifically tailored to particular model, 4DGS [40]. Therefore, developing universal compression method for these Gaussian-based models is promising direction for subsequent research endeavors. Figure 9. Visualization of improved performance. 11 (a) Cook Spinach (b) Cut Roasted Beef (c) Sear Steak Figure 10. Rate-distortion curves evaluated on diverse scenes with different pruning ratios. (a) Cook Spinach (b) Cut Roasted Beef (c) Sear Steak Figure 11. Rate-distortion curves evaluated on diverse scenes with different key-frames interval. Table 5. Per-scene results of N3V datasets. 4DGS Ours Ours-PP Scene PSNR SSIM LPIPS Storage(MB) FPS Raster FPS #NUM PSNR SSIM LPIPS Storage(MB) FPS Raster FPS #NUM PSNR SSIM LPIPS Storage(MB) FPS Raster FPS #NUM Coffee Martini 27.9286 0.9160 0.0759 2764 43 75 4441271 28.5780 0.9185 0.0726 557.4 696 901 888254 28.5472 0.9166 0.0744 64.94 696 901 Cook Spinach 33.1651 0.9545 0.0449 2211 89 103 3530165 33.2613 0.9553 0.0459 443.11 803 1088 706033 33.0641 0.9540 0.0467 52.04 803 1088 706033 Cut Roasted Beef 33.8849 0.9589 0.0408 1863 103 122 2979832 33.6092 0.9570 0.0435 374.05 853 1163 595967 33.7767 0.9562 0.0445 44.54 853 1163 595967 Flame Salmon 29.1009 0.9236 0.0691 2969 31 70 4719443 28.8488 0.9221 0.0707 592.4 680 879 943889 28.9878 0.9209 0.0712 69.24 680 879 943889 Flame Steak 33.7970 0.9615 0.0383 1536 122 148 2457356 33.2804 0.9598 0.0417 308.4 864 1189 491471 33.2519 0.9581 0.0421 36.94 864 1189 491471 Sear Steak 33.6031 0.9607 0.0418 1167 152 195 1870891 33.7150 0.9615 0.0401 234.8 935 1332 374178 33.6053 0.9604 0.0402 29.34 935 1332 374178 Average 31.9133 0.9459 0.0518 2085 90 118 3333160 31.8821 0.9457 0.0524 418.36 805 1092 666632 31.8722 0.9444 0.0532 49.50 805 1092 12 Ground Truth 4DGS Ours Ours-PP (a) Results on Coffee Martini Scene. (b) Results on Cook Spinach Scene. (c) Results on Cut Roasted Beef Scene. Figure 12. Qualitative comparisons of 4DGS and our method on the N3V dataset. To be continued in the next page. 13 Ground Truth 4DGS Ours Ours-PP (a) Results on Flame Salmon Scene. (b) Results on Flame Steak Scene. (c) Results on Sear Steak Scene. Figure 13. Qualitative comparisons of 4DGS and our method on the N3V dataset. 14 Ground Truth 4DGS Ours Ours-PP (a) Results on Bouncingballs Scene. (b) Results on Hellwarrior Scene. Figure 14. Qualitative comparisons of 4DGS and our method on the D-nerf dataset. To be continued in the next page. (c) Results on Hook Scene. 15 Ground Truth 4DGS Ours Ours-PP (a) Results on Jumpingjacks Scene. (b) Results on Lego Scene. Figure 15. Qualitative comparisons of 4DGS and our method on the D-nerf dataset. To be continued in the next page. (c) Results on Mutant Scene. 16 Ground Truth 4DGS Ours Ours-PP (a) Results on Standup Scene. (b) Results on Trex Scene. Figure 16. Qualitative comparisons of 4DGS and our method on the D-nerf dataset. Table 6. Per-scene results of D-NeRF datasets. 4DGS Ours Ours-PP Scene PSNR SSIM LPIPS Storage(MB) FPS Raster FPS #NUM PSNR SSIM LPIPS Storage(MB) FPS Raster FPS #NUM PSNR SSIM LPIPS Storage(MB) FPS Raster FPS #NUM Bouncingballs 33.3472 0.9821 0.0252 83.69 462 1951 133762 33.4532 0.9826 0.0248 12.56 1509 2600 20065 33.4592 0.9821 0.0259 4.12 1509 2600 20065 Hellwarrior 34.7296 0.9516 0.0652 156.53 426 1433 250201 35.0316 0.9530 0.0644 23.38 1517 2665 37368 35.1570 0.9537 0.0629 5.29 1517 2665 37368 Hook 31.9369 0.9635 0.0385 164.91 414 1309 263593 32.5118 0.9653 0.035 24.63 1444 2634 39360 32.5498 0.9671 0.0345 5.39 1444 2634 39360 Jumpingjacks 30.8247 0.9684 0.0340 510.99 267 489 816773 31.8045 0.9716 0.0322 76.19 1491 2476 121776 31.8467 0.9728 0.0309 11.04 1491 2476 Lego 25.3320 0.9178 0.0819 351.19 317 634 561357 26.8319 0.9280 0.0674 52.45 1318 2067 83837 27.2850 0.9315 0.0646 8.48 1318 2067 83837 Mutant 38.9257 0.9903 0.0090 73.24 463 1861 117062 37.1916 0.9886 0.0124 10.97 1518 2598 17527 37.0218 0.9883 0.0139 3.56 1518 2598 17527 Standup 39.0411 0.9896 0.0094 95.38 457 1878 152454 39.3990 0.9896 0.0099 14.25 1539 2644 22768 39.0713 0.9896 0.0109 3.88 1539 2644 22768 Trex 29.8542 0.9795 0.0193 791.66 202 302 1265408 30.4726 0.9811 0.0180 118.24 1361 2174 188986 30.6063 0.9821 0.0173 16.11 1361 2174 188986 Average 32.9989 0.9678 0.0353 278.45 376 1232 445076 33.3370 0.9699 0.0330 41.58 1462 2482 66460 33.3746 0.9709 0.0326 7.23 1462 2482"
        },
        {
            "title": "References",
            "content": "[1] Muhammad Salman Ali, Maryam Qamar, Sung-Ho Bae, and Enzo Tartaglione. Trimming the fat: Efficient compression of 3d gaussian splats through pruning. arXiv preprint arXiv:2406.18214, 2024. 3 [2] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew OToole, and Changil Kim. Hyperreel: High-fidelity In Pro6-dof video with ray-conditioned sampling. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1661016620, 2023. 1, 2, 6, 7 [3] Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, and Youngjung Uh. Per-gaussian embedding-based deformation for deformable 3d gaussian splatting. arXiv preprint arXiv:2404.03613, 2024. 2, 6, 7, 8 [4] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141, 2023. 2 [5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European conference on computer vision, pages 333 350. Springer, 2022. 2 [6] Devikalyan Das, Christopher Wewer, Raza Yunus, Eddy Ilg, and Jan Eric Lenssen. Neural parametric gaussians for monocular non-rigid object reconstrucIn Proceedings of the IEEE/CVF Conference tion. on Computer Vision and Pattern Recognition, pages 1071510725, 2024. [7] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d-rotor gaussian splatting: towards efficient novel view synthesis for dynamic scenes. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 6 [8] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. arXiv preprint arXiv:2311.17245, 2023. 3, 4, 5 [9] Guangchi Fang and Bing Wang. Mini-splatting: Representing scenes with constrained number of gaussians. arXiv preprint arXiv:2403.14166, 2024. 3, 4 [10] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In SIGGRAPH Asia 2022 Conference Papers, pages 19, 2022. 6 In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5501 5510, 2022. 2 [12] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahand Angjoo bæk Warburg, Benjamin Recht, Kanazawa. K-planes: Explicit radiance fields in In Proceedings of space, the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. 1, 2, 6, time, and appearance. [13] Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, and Houqiang Li. Motion-aware 3d gaussian splatting for efficient dynamic scene reconstruction. arXiv preprint arXiv:2403.11447, 2024. 2 Thomas [14] Bernhard Kerbl, Georgios Kopanas, Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 4 [15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. 6 [16] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian repIn Proceedings of the resentation for radiance field. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2171921728, 2024. 3, 6, 8 [17] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Ping Tan. Streaming radiance fields for 3d video synthesis. Advances in Neural Information Processing Systems, 35:1348513498, 2022. 2, 6, [18] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video syntheIn Proceedings of the sis from multi-view video. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55215531, 2022. 1, 2, 6, 7, 8, 9, 10 [19] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85088520, 2024. 6, 7 [20] Wenkai Liu, Tao Guan, Bin Zhu, Lili Ju, Zikai Song, Dan Li, Yuesong Wang, and Wei Yang. Efficientgs: Streamlining gaussian splatting for largearXiv scale high-resolution scene representation. preprint arXiv:2404.12777, 2024. 3 [11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. [21] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. arXiv:1906.07751, 2019. 1, 2, 6, 7 arXiv preprint the IEEE/CVF international conference on computer vision, pages 1433514345, 2021. 2 [22] Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, and Yuchao Dai. 3d geometry-aware deformable gaussian splatting for the dynamic view synthesis. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89008910, 2024."
        },
        {
            "title": "In Proceedings of",
            "content": "[23] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713, 2023. 6, 7, 8 [24] Ben Mildenhall, Pratul Srinivasan, Rodrigo OrtizCayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (ToG), 38 (4):114, 2019. 2 [25] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1): 99106, 2021. 1, 2 [26] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. 2 [27] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed PirCompgs: Smaller and faster gaussian siavash. In European splatting with vector quantization. Conference on Computer Vision, 2024. 3, [28] Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, and Federico Tombari. Radsplat: Radiance field-informed gaussian splatting for robust arXiv preprint real-time rendering with 900+ fps. arXiv:2403.13806, 2024. 3 [29] Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, and George Drettakis. Reducing the memory footprint of 3d gaussian splatting. Proceedings of the ACM on Computer Graphics and Interactive Techniques, 7(1):117, 2024. 3 [30] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1031810327, 2021. 2, 6, 7, 8, 9, 11 [31] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of [32] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. Voxgraf: Fast 3d-aware image synthesis with sparse voxel grids. Advances in Neural Information Processing Systems, 35:33999 34011, 2022. 2 [33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 6 [34] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerfplayer: streamable dynamic scene representaIEEE tion with decomposed neural radiance fields. Transactions on Visualization and Computer Graphics, 29(5):27322742, 2023. 2 [35] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54595469, 2022. [36] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei Song, and Huaping Liu. Mixed neural voxels for fast multi-view video synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1970619716, 2023. 2 [37] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling neural radiance field to neural light field for In European Conferefficient novel view synthesis. ence on Computer Vision, pages 612629. Springer, 2022. 2 [38] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and Lan Xu. Fourier plenoctrees for dyIn Pronamic radiance field rendering in real-time. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1352413534, 2022. 2 [39] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for realtime dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2031020320, 2024. 2, 6, 7 [40] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023. 1, 2, 3, 5, 6, 7, 8, 9, 11 [41] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene 19 reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2033120341, 2024. 2, 6 [42] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [43] Xinjie Zhang, Zhening Liu, Yifan Zhang, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Zehong Lin, Shuicheng Yan, and Jun Zhang. Mega: Memoryefficient 4d gaussian splatting for dynamic scenes. arXiv preprint arXiv:2410.13613, 2024. 6,"
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}