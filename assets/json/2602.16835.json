{
    "paper_title": "NeST: Neuron Selective Tuning for LLM Safety",
    "authors": [
        "Sasha Behrouzi",
        "Lichao Wu",
        "Mohamadreza Rostami",
        "Ahmad-Reza Sadeghi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains. We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment."
        },
        {
            "title": "Start",
            "content": "NeST: Neuron Selective Tuning for LLM Safety"
        },
        {
            "title": "Mohamadreza Rostami\nTechnical University of Darmstadt",
            "content": "Ahmad-Reza Sadeghi Technical University of Darmstadt 6 2 0 2 8 1 ] . [ 1 5 3 8 6 1 . 2 0 6 2 : r Abstract Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains. these shortcomings, we present NeST, To address lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to 17,310 decrease in updated parameters compared to full fine-tuning and 9.25 reduction relative to LoRA, while consistently achieving stronger safety performance for alignment."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have become central component of modern natural language processing, demonstrating strong performance across tasks such as text generation, translation, question answering, reasoning, and conversational interaction. Trained on large-scale text corpora using transformer architectures, LLMs show behaviors such as in-context learning, instruction following, and step-by-step reasoning, which allow them to perform wide range of tasks without task-specific training [13]. Despite their strong capabilities, large language models can be induced to generate harmful or unsafe content through range of black-box jailbreak techniques that manipulate inputs without access to model internals. These include prompt-based attacks and adaptive methods that automatically or iteratively refine adversarial prompts [424]. In contrast, white-box approaches study internal neuron activations and learned representations to understand how specific behaviors, including safety mechanisms, are encoded within the model [25]. LLM safety alignment methods. Prior work on safety alignment for large language models can be broadly grouped into output-level fine-tuning approaches, studies of internal safety representations, and targeted or parameter-efficient intervention methods. Safety alignment via output-level fine-tuning steers language models toward helpful behavior by shaping output distributions with human or automated feedback. Common approaches include reinforcement learning from human feedback (RLHF) [26], supervised refusal tuning [27], and preference-based optimization methods such as DPO and SafeDPO [28, 29]. These techniques reduce the generation of toxic and unsafe samples and improve instruction following [2630], but rely on broad parameter updates and treat the model as largely homogeneous parameter space, without explicitly modeling how safety behavior is internally encoded. In the localization of safety representations, recent work shows that safety-relevant behavior is localized within specific internal components rather than being uniformly distributed across the network [25, 3136]. Ignoring this structure during safety fine-tuning can disrupt safety-critical components, leading to instability, inconsistent behavior, and unnecessary computational cost. More fundamentally, most existing meth1 ods constrain outputs without constraining the internal mechanisms that generate safe behavior. Targeted and parameter-efficient intervention approaches aim to influence safety behavior with fewer parameter updates. Circuit breakers [37] intervene on internal activations at inference time, redirecting harmful generative trajectories toward refusal or incoherent states. While effective, they operate primarily as inference-time controls and depend on prompt-triggered representations. Parameter-efficient fine-tuning methods such as LoRA [38] further reduce training cost by restricting updates to low-rank subspaces, but operate at the level of entire layers and remain agnostic to the localization of safetyrelevant components. As result, existing methods do not fully exploit the internal organization of safety behavior to jointly improve alignment quality and training efficiency. Our goal and contribution. Building on the abovementioned insight, we introduce NeST, the first neuronstructured framework that enables efficient and practical safety fine-tuning by explicitly aligning parameter updates with the internal mechanisms governing safety behavior. NeST bridges the gap between output-level safety fine-tuning and evidence that safety behavior is localized within specific internal components. Unlike prior output-level approaches that rely on broad parameter updates, NeST explicitly accounts for the internal organization of safety behavior by coupling safety adaptation to neuron-level safety representations. Rather than updating entire layers or projections, NeST performs targeted adaptation by first clustering functionally related safety components and then learning compact set of shared update directions for each cluster, while freezing the remainder of the model. This cluster-based formulation concentrates learning capacity precisely where safety behavior resides and ensures coherent updates across neurons that jointly implement safety decisions. In contrast to inference-time control mechanisms such as circuit breakers [37], NeST performs structure-aware safety fine-tuning during training, directly shaping the internal mechanisms that underpin safe behavior. Compared to parameterefficient methods such as LoRA [38], which operate at the level of entire layers and remain agnostic to safety localization, NeST aligns parameter updates with neuron-level safety structure. As result, NeST provides principled, structureaware form of safety tuning that is both highly parameterefficient and stable, without relying on layer-wide or global modifications. Across diverse range of model families and scales, NeST substantially reduces unsafe generation while preserving core model capabilities, and is complementary to existing safety mechanisms. Our key contributions are summarized as follows: We introduce NeST, novel neuron-structured safety fine-tuning framework that performs targeted safety adaptation by aligning parameter updates with the internal organization of safety behavior, while remaining sufficiently lightweight to enable fast and resource-efficient safety alignment. We propose cluster-based safety adaptation mechanism that groups safety-relevant neurons based on activation similarity and enforces shared updates within each group, enabling coherent, stable modification of safety behavior while freezing the remainder of the model. We show that NeST achieves stronger safety alignment than existing approaches, reducing the average attack success rate from 44.5% to 4.36%, corresponding to an average reduction of 93.0% compared to Circuit Breaker and 42.4% compared to LoRA, evaluated across 10 opensource LLMs spanning multiple model families and sizes from Alibaba, Meta, Google, and Microsoft. We demonstrate that NeST is highly parameter-efficient, requiring on average 99.99% fewer trainable parameters than full safety fine-tuning, 98.50% fewer than circuit breaker, and 89.19% fewer than LoRA, while preserving core model capabilities on standard reasoning and knowledge benchmarks across the same set of models. We show that NeST remains robust across diverse inference settings and input modalities, including text-only, image-only, and reasoning-augmented multimodal generation, reducing average attack success rate from 55.3% to 1.1% across multiple multimodal LLMs. The remainder of this paper is structured as follows. Section 2 provides the necessary background information. In Section 3, we describe the design of NeST in detail; the implementation is introduced in Section 4. case study on characterizing the behaviour of safety neurons is conducted in Section 5. Section 6 empirically evaluates the NeST approach and benchmark with state-of-the-art works. Section 7 studies the influence of critical hyperparameters on NeST. Section 8 explores broader implications. Section 9 discusses related works. Section 10 summarizes this work."
        },
        {
            "title": "2.1 Language Model",
            "content": "Large Language Models (LLMs) are typically implemented as deep transformer architectures trained to model the conditional distribution of text. Given an input sequence, token representations are progressively refined through stack of transformer blocks, each composed of self-attention module (Attn()) followed by position-wise feed-forward network (FNN()). At each layer ℓ, this process produces hidden representation (hℓ) for each token, capturing contextual information aggregated from the sequence via self-attention and transformed by the feed-forward sublayer [39]. While 2 self-attention enables contextual information exchange across tokens, the feed-forward sublayers account for substantial fraction of model parameters and play critical role in shaping token-level transformations. Formally, the hidden representation hℓ Rd at layer ℓ is updated via residual connection as hℓ+1 = hℓ + FNN(Attn(hℓ)), (1) where the feed-forward network (FNN) is applied independently to each token position. Modern LLMs commonly employ gated feed-forward architectures, in which the transformation takes the form FNN(x) = (cid:2)σ(xWup) (xWgate)(cid:3)Wdown, (2) where Wup,Wgate Rddff project the input from the model dimension into higher-dimensional feed-forward space of size dff, Wdown Rdffd maps the gated activations back to the model dimension, σ() denotes nonlinear activation function such as SiLU, and denotes element-wise multiplication. Prior analyses indicate that FNN layers selectively activate internal components, and that these activations are progressively integrated across layers via residual connections [40]. This gated formulation enables selective activation of feedforward neurons in token-dependent manner, supporting expressive and structured transformations. Such architectures are now standard across contemporary LLM families, including GPT [41], LLaMA [42], Qwen [43], and Gemma [44]."
        },
        {
            "title": "2.2 Parameter Efficient Fine Tuning",
            "content": "Fine-tuning adapts pretrained large language model (LLM) to downstream task by optimizing its parameters on taskspecific data. Let θ denote the pretrained model parameters and the downstream dataset consisting of inputoutput pairs (x, y). Fine-tuning is formulated as: θ = arg min θ E(x,y)D (cid:2)L( (x; θ), y)(cid:3), (3) where θ denotes the optimized model parameters obtained after fine-tuning, (x; θ) is the model prediction, and is task-specific loss function that penalizes the discrepancy between the prediction and the target y. Although effective, full fine-tuning requires updating all model parameters, which is computationally expensive and memory-intensive for largescale models. To mitigate this cost, parameter-efficient fine-tuning methods freeze the pretrained parameters and introduce small number of trainable parameters. widely adopted approach is Low-Rank Adaptation (LoRA), which augments linear transformation with pretrained weights by adding trainable low-rank update while keeping fixed: xout = (W + γBA) xin. (4) Here, Rrdin and Rdoutr with min(din, dout). Only the matrices and are optimized and stored separately from the pretrained weights, enabling efficient task adaptation with significantly fewer trainable parameters."
        },
        {
            "title": "2.3 Safety Alignment in Language Models",
            "content": "Safety alignment aims to ensure that language model produces helpful and policy-compliant responses while avoiding harmful or disallowed content. In practice, alignment is typically achieved through training signals that encourage appropriate refusals and harmless behavior on safety-critical inputs. Most existing approaches operate at the level of model outputs. common strategy is supervised fine-tuning with curated safe and refusal responses, which can be viewed as applying the standard fine-tuning objective introduced in Section 2 to safety-labeled dataset, where targets correspond either to compliant helpful answers or explicit refusals. Through this labeland target-driven optimization, the model is realigned toward desired behavioral constraints without modifying the underlying training objective itself [26, 45]. Additional alignment is often achieved using reinforcement learning from human feedback (RLHF), which optimizes model outputs according to learned human preference signals [26, 46], and preferencebased optimization methods such as Direct Preference Optimization (DPO), which directly shift the output distribution toward preferred responses without explicit reward modeling [47]. These techniques effectively shape the distribution of generated text, but largely treat the model as homogeneous parameter space and do not explicitly account for how safety behavior is represented internally. Beyond output-level supervision, safety alignment can also be approached through the internal structure of the model. In this perspective, safetyrelevant behavior is encoded within particular layers, activations, or neurons whose responses influence whether outputs comply with safety constraints. Emerging methods therefore aim to identify these localized regions and selectively adapt or reinforce them during fine-tuning, enabling targeted safety improvements without broadly modifying the entire parameter space [25, 37, 48]."
        },
        {
            "title": "3.1 Threat Model",
            "content": "Adversary. We consider defense-aware black-box adversary who interacts with the model solely at inference time via prompts. The adversarys objective is to elicit harmful, unsafe, or policy-violating outputs by crafting adversarial inputs, including jailbreak prompts, indirect requests, role-playing scenarios, and adaptive multi-turn conversations. The adversary has no access to the models internal parameters, neuron activations, gradients, or training data, and cannot 3 Figure 1: An overview of the NeST in safety neurons detection and clustering steps. modify or inspect the training process. We therefore exclude white-box attacks, training-time data poisoning, and direct parameter manipulation, as these threats are orthogonal to the focus of this work. Although the adversary may be aware that NeST is deployed and understand its high-level design, their interaction with the model remains strictly limited to blackbox, prompt-based access, reflecting common real-world deployment conditions. Defender. The defender is the model provider or deployer with full access to the model architecture and parameters during training and fine-tuning. The defenders goal is to improve robustness against prompt-based safety violations while preserving general-purpose capabilities. The defender may analyze internal representations offline and apply structured safety-oriented training interventions before deployment. NeST operates entirely at training time and introduces no inference-time mechanisms, e.g., external filters. 3.2 Idea and High-level Design The core idea behind NeST is to leverage the internal structure of safety behavior in language models to enable targeted and efficient safety fine-tuning. Rather than treating safety alignment as property that must be enforced through broad or dense parameter updates, NeST focuses on selectively adapting small subset of neurons that are directly involved in safety-related decisions. This design allows safety behavior to be strengthened while minimizing interference with the models general-purpose capabilities. Figure 1 illustrates the safety neuron detection and clustering stages of the NeST framework. Starting from safety-aligned base model, we analyze internal activations in response to benign and harmful prompts to identify neurons that are strongly associated with safety behavior. Crucially, NeST does not treat these safety neurons as independent units. Instead, it groups them into clusters based on similarity in their activation pat4 terns across prompts. This clustering step is central to NeST: it captures shared functional roles among safety neurons and reveals coherent groups of units that jointly implement safetyrelated behavior within the model. In the final stage, shown in Figure 2, NeST performs neuronselective tuning by introducing small set of trainable clusterlevel update vectors and restricting all parameter updates to these vectors during supervised fine-tuning. Neurons belonging to the same cluster share common update, while all other model parameters remain frozen. This cluster-based reparameterization is key departure from existing safety fine-tuning and parameter-efficient adaptation methods, which operate at the level of entire layers or low-rank subspaces without regard to neuron-level structure. By learning explicit update directions aligned with functionally coherent safety neuron clusters, NeST enables structured, low-dimensional adaptation that directly targets the internal mechanisms responsible for safety behavior. After training, the learned updates are merged into the base model weights, yielding standard language model with improved robustness to adversarial prompting and no additional inference-time overhead. Figure 2: An overview of the NeST in neuron-selective tuning step."
        },
        {
            "title": "3.3 Safety Neurons Detection",
            "content": "Each safety neuron is represented by an activation vector Safety alignment leaves structured signatures in internal representations, giving rise to neuron-level patterns in which small subset of units responds differently to benign and harmful prompts [25]. We formalize safety neurons through an activation-based probing procedure that quantifies how individual neurons contribute to safety-related distinctions. Let fθ denote safety-aligned language model, and let hℓ(x) Rd be the hidden representation produced at layer ℓ in response to an input prompt x. To obtain prompt-level activation signal that captures salient neuron responses during generation, we aggregate token-level activations using max pooling across the sequence: hℓ(x) = max ttokens(x) hℓ,t (x), (5) where hℓ,t (x) denotes the hidden state at token position t. To measure the association between neuron activations and safety behavior, we train lightweight linear probe on frozen representations to distinguish harmful prompts Dharm from benign prompts Dbenign: pℓ(x) = σ (cid:16) ℓ (cid:17) hℓ(x) , (6) where wℓ Rd are the probe weights and σ() denotes the sigmoid function. The learned weights provide neuron-level importance signal indicating how strongly each activation dimension contributes to safety discrimination. We identify safety neurons at layer ℓ as those whose probe weights are both highly discriminative and positively aligned with refusal behavior: Sℓ = (cid:8)i (cid:12) (cid:12) zℓ,i > zthr wℓ,i > 0(cid:9) , (7) where zℓ,i denotes the standardized importance score of neuron i, obtained by normalizing its probe weight relative to the mean and standard deviation of weights at layer ℓ, and zthr is fixed threshold."
        },
        {
            "title": "3.4 Clustering Safety Neurons",
            "content": "Once safety neurons have been identified at each layer ℓ (Section 3.3), we further organize them into groups according to similarity in their activation patterns across prompts. The goal of this clustering step is to capture shared functional behavior among safety neurons and to enable parameter-efficient adaptation by tying updates within each group. Let Sℓ = {i1, . . . , im} (8) denote the indices of the safety neurons selected at layer ℓ. For given prompt x, let hℓ(x) Rd denote the pooled hidden representation at layer ℓ, as defined in Eq. (5). aℓ,i = (cid:2)hℓ,i (x1), . . . , hℓ,i (xN)(cid:3) RN, (9) which characterizes its response profile across diverse set of benign and harmful prompts. We cluster the set of activation vectors {aℓ,i }m j=1 using k-means clustering in activation space. For candidate number of clusters k, we solve min {C1,...,Ck} c=1 jCc (cid:13) (cid:13)aℓ,i µc (cid:13) 2 (cid:13) 2 , (10) where µc denotes the centroid of cluster Cc. This objective groups safety neurons whose activation vectors exhibit similar response patterns across prompts. In practice, clustering is applied directly to the raw activation vectors without additional normalization, preserving magnitude information that distinguishes consistently strong safety responses from weaker or more selective activations. To determine the number of clusters, we evaluate k-means solutions for {2, . . . , kmax}, (11) where kmax is user-defined upper bound, set to kmax = 2 in our experiments for stability and parameter efficiency. For each candidate k, we compute the silhouette score Silhouette(k) = 1 j=1 b(i j) a(i j) max{a(i j), b(i j)} , (12) where a(i j) is the mean Euclidean distance between neuron and other neurons in its assigned cluster, and b(i j) is the minimum mean distance between neuron and neurons in any other cluster. We select the clustering with the highest silhouette score, preferring smaller in the case of ties. If the best silhouette score falls below fixed threshold γ (set to γ = 0.1), or if clustering degenerates to single effective cluster, we revert to single-cluster assignment and treat all safety neurons in layer ℓ as homogeneous group. The final clustering defines mapping cℓ : Sℓ {0, . . . , kℓ 1}, (13) which assigns each safety neuron to cluster index cℓ(i) and is subsequently used to tie parameter updates across neurons with similar activation behavior during fine-tuning."
        },
        {
            "title": "3.5 Neuron-Selective Tuning",
            "content": "We perform neuron-selective tuning by restricting parameter updates to clustered safety neurons while keeping all other model parameters fixed (Figure 2). For each linear projection matrix Wℓ Rdoutdin within the feed-forward network (FFN) at layer ℓ, we freeze the base weights and introduce set of trainable cluster-level update vectors Uℓ Rkℓdin , (14) 5 Algorithm 1 NeST: Neuron-Structured Safety Tuning Require: Safety-aligned model fθ; benign and harmful prompt sets Require: Threshold zthr; clustering parameters (kmax, γ) Ensure: Updated parameters θ 1: Collect pooled activations hℓ(x) for all prompts (Eq. (5)). 2: Detect safety neurons Sℓ via linear probing and thresholding (Eq. (7)). 3: Cluster safety neurons using k-means and silhouette selection to obtain cℓ() (Eq. (13)). 4: Introduce cluster updates Uℓ Rkℓdin and freeze base parameters. 5: Fine-tune only {Uℓ} using shared neuron updates (Eq. (15)). 6: Merge updates into base weights to obtain θ. return θ where din and dout denote the input and output dimensionalities of the FFN projection at layer ℓ, and kℓ is the number of safety neuron clusters identified in that layer. Each safety neuron Sℓ receives an additive update determined solely by its cluster assignment, such that the effective weight row used during training is given by ℓ,i,: = Wℓ,i,: +Uℓ,cℓ(i),:, (15) while all non-safety neurons remain unchanged. This reparameterization enforces shared updates among neurons with similar activation behavior and substantially reduces the number of trainable parameters. During supervised fine-tuning, gradients are propagated exclusively through the cluster-level parameters Uℓ, ensuring that adaptation is localized to safety-relevant subspaces of the model. After training, the learned updates are merged into the base weights, yielding standard model without additional inference-time overhead. The overall NeST pipeline is summarized in Algorithm 1."
        },
        {
            "title": "4 Implementation",
            "content": "This section describes the concrete implementation of NeST, detailing how safety-relevant neurons are detected, organized into clusters, and selectively fine-tuned within large language models. All components are implemented in PyTorch using the HuggingFace Transformers and TRL libraries. Experiments are conducted on NVIDIA H100 GPUs, and all training and analysis are performed using frozen base models with parameter updates restricted to NeSTs cluster-level adaptation parameters."
        },
        {
            "title": "4.1 Safety Neuron Detection",
            "content": "We implement safety neuron detection following the procedure described in Section 3.3, using forward-hook-based activation extraction and layer-wise linear probing on frozen model representations. To identify safety-relevant neurons, we construct labeled prompt dataset consisting of both harmful and benign inputs. Harmful prompts are collected from three public safety benchmarks: CatHarmfulQA [49], HarmfulQA [50], and the LLM-LAT harmful dataset [51], while benign prompts are drawn from the training split of the Natural Reasoning dataset [52]. For balanced probing, the number of benign prompts is matched to the total number of harmful prompts, resulting in an equal number of harmful and benign samples with binary labels. We register forward hooks on the FNN submodules of each transformer block, specifically the gate_proj and up_proj linear projections, and for each prompt perform forward pass through the frozen model while recording token-level hidden states at the selected modules. Following Eq. (1), activations are aggregated using max pooling across the sequence dimension to obtain single prompt-level activation vector per layer and projection. Activations are collected in batches and concatenated across the full dataset to form matrix of shape (N, d), where is the number of prompts and is the hidden dimension. For each layer and projection, we train an independent linear probe on the pooled activations to distinguish harmful from benign prompts using binary cross-entropy loss, repeating training with multiple random initializations to reduce variance. Probe weights are standardized within each layer using z-score normalization, and neurons whose standardized scores exceed fixed threshold (zthr) and whose weights are positive are selected as safety neurons, indicating positive alignment with refusal behavior as formalized in Eq. (7). This procedure yields sparse set of safety neurons for each layer and projection."
        },
        {
            "title": "4.2 Safety Neuron Clustering",
            "content": "We implement safety neuron clustering following the procedure described in Section 3.4, using activation patterns collected during the detection stage to group safety neurons with similar response profiles. Clustering is performed independently for each for each FNN projection (gate_proj and up_proj), using only the neurons identified as safety-relevant in Section 4.1. For each layer and projection, we construct neuron-specific activation matrix from the pooled activations collected during safety neuron detection by extracting the columns corresponding to safety neurons, yielding matrix of shape (N, m), where is the number of prompts and is the number of safety neurons. Each safety neuron is thus represented by an activation vector characterizing its response behavior across both benign and harmful inputs. Prior to clustering, each neurons activation vector is standardized across prompts to zero mean and unit variance to improve numerical stability and reduce scale differences. We then apply k-means clustering in activation space, treating each safety neuron as data point. For each layer and projection, we evaluate candidate clusterings with varying numbers of clusters, up to the number of available safety neurons, and select the so6 lution with the highest silhouette score, preferring smaller numbers of clusters in the case of ties. The final output of the clustering stage is mapping from safety neuron indices to cluster identifiers for each layer and projection, which is fixed prior to fine-tuning and subsequently used to tie parameter updates across neurons within the same cluster during neuron-selective safety fine-tuning."
        },
        {
            "title": "4.3 Neuron-Selective Tuning",
            "content": "We implement neuron-selective tuning following the formulation in Section 3.5 by restricting all trainable parameters to cluster-level updates applied exclusively to safety neurons while freezing the remainder of the model. Prior to fine-tuning, all base model parameters are frozen, and neuron-selective adaptation is enabled only for the FNN gate_proj and up_proj linear projections associated with detected safety neurons. For each projection, we reparameterize the corresponding weight matrix by introducing trainable clusterlevel update matrix Uℓ Rkℓdin, as defined in Eq. (14). Instead of directly updating the original weight matrix Wℓ, we employ parametrization mechanism that reconstructs the effective weight at each forward pass by adding the clusterspecific update vector to the rows corresponding to safety neurons according to Eq. (15), while leaving rows corresponding to non-safety neurons unchanged. All safety neurons assigned to the same cluster share single update vector, enforcing coordinated adaptation among neurons with similar activation behavior. During fine-tuning, gradients are propagated exclusively through the cluster-level update parameters, while all original model weights and bias terms remain fixed, ensuring that learning capacity is concentrated within safety-relevant subspaces and substantially reducing the number of effective trainable parameters. Neuron-selective tuning is performed using supervised fine-tuning on safety-aligned data, optimizing standard language modeling objectives under these parameter constraints. After training, the learned cluster-level updates are folded into the base weight matrices and all reparameterization modules are removed, yielding standard model with identical architecture and inference-time cost as the original base model, and no additional parameters or runtime overhead introduced by NeST."
        },
        {
            "title": "5 Case Study",
            "content": "To build intuition for the structure exploited by NeST, we visualize safety-neuron organization in representative layer of safety-aligned model. We focus on LLaMA-3.2-1B-Instruct and select layer 6 as mid-network layer where safety behavior is typically salient. For this layer, we take the detected safety neurons from both the gate_proj and up_proj of the FNN. We collect their activation vectors over diverse prompt set, cluster neurons in the original activation space, (a) FNN gate projection (b) FNN up projection Figure 3: PCA visualization of clustered safety neurons in representative mid-layer (Layer 6) of LLaMA-3.2-1B-Instruct. Each point denotes safety neuron. Clusters in both FNN projections reveal coherent neuron groups that motivate NeSTs cluster-level update tying and use PCA to project them into two dimensions for visualization. Each point corresponds to safety neuron, colored by its cluster assignment. Figure 3 shows that safety neurons form compact groups in both projections, indicating that they organize into coherent clusters rather than behaving as isolated units. This directly motivates NeSTs cluster-level updates: tying adaptation within each cluster preserves coherence among neurons that respond similarly, yielding targeted and structured safety tuning. We observe similar patterns across layers and model variants. To further substantiate that the discovered clusters capture structure relevant for optimization and not merely similarity in activation space, we additionally analyze gradient alignment among safety neurons during fine-tuning. While the cluster visualizations in Figure 3 demonstrate that safety neurons organize into coherent groups based on their activation behavior, gradient alignment directly tests whether neurons within cluster also tend to receive updates in similar directions. During supervised fine-tuning, we extract gradient vectors corresponding to the weight rows of safety neurons and compute pairwise cosine similarity between neuron updates. We compare similarities for neuron pairs belonging to the same cluster against pairs drawn from different clusters. Figure 4 summarizes these distributions across layers for LLaMA-3.21B-Instruct. Across all layers, within-cluster gradient alignment is consistently high, whereas between-cluster alignment is substantially lower and often centered near zero. This clear separation indicates that neurons grouped together by activationbased clustering also exhibit strongly aligned update directions during training. Consequently, the cluster-tied update rule in Eq. (15), implemented via the shared update vectors Uℓ, closely matches the models underlying optimization structure: neurons that NeST updates jointly would otherwise receive highly similar gradients. Together with the observed Method Scope Train Data Epochs/Steps Eval. Data Baseline Full FT LoRA (r=1) Circuit Breaker (r=32) NeST None All params FNN FNN+ATTN Neuron clust. 20k 20k 25k 20k 5 epochs 5 epochs 150 steps 5 epochs 1k 1k 1k 1k 1k Table 1: Training and evaluation configurations for all methods. Full FT, LoRA, and NeST use balanced 20k dataset, while Circuit Breaker builds on its original 5k training setup with an additional 20k prompts. denotes the dimensionality of low-rank adaptation. All methods are evaluated on 1k heldout WildJailbreak prompts. pled from the training split of the Natural Reasoning [52] dataset, with the number of benign prompts matched to the harmful set. The combined dataset is shuffled and split into training and validation sets using 95/5 split, and the same data and preprocessing are used across these methods to ensure fair and controlled comparison. For Circuit Breaker [37], we follow the authors original training setup, which uses curated circuit breaker dataset designed to elicit harmful internal representations together with retain dataset composed of benign and refusal-aligned examples. In addition to this original configuration, we evaluate an expanded setting in which the circuit breaker training data is augmented with 5,000 harmful prompts sampled from WildJailbreak [59], while retaining the original retain dataset. All methods are trained using consistent supervised finetuning configuration to enable controlled comparison of safety robustness and parameter efficiency. Baseline refers to the original, released instruction-tuned model without additional fine-tuning. Full FT performs standard supervised finetuning over all model parameters for five epochs. LoRA [38] applies parameter-efficient fine-tuning by inserting low-rank adapters into FNN projections. small rank of = 1 is used to keep the number of trainable parameters close to that of NeST, enabling direct comparison of parameter efficiency. Circuit Breaker [37] follows the Representation Rerouting framework, introducing LoRA-based circuit-breaking modules while freezing the base model. Training is performed using the authors recommended configuration with rank-32 adapters, dropout 0.05, bfloat16 precision, gradient checkpointing, and up to 150 optimization steps. Finally, NeST restricts all updates to cluster-level parameters associated with safety-relevant neurons, leaving the remainder of the model unchanged and training for five epochs under the same optimization settings as Full FT and LoRA. All evaluation results are computed on held-out set of 1,000 adversarial harmful prompts sampled from the WildJailbreak [59] benchmark. This evaluation set is strictly disjoint from all datasets used for safety neuron detection, clustering, and fine-tuning, ensuring that reported attack success rates reflect generalization to unseen harmful Figure 4: Gradient alignment of safety neurons across layers. Within-cluster pairs (blue) exhibit higher cosine similarity than between-cluster pairs (orange), indicating aligned update directions that justify NeSTs cluster-level parameter tying. clustering patterns, this gradient alignment provides direct empirical support for neuron-selective tuning in NeST."
        },
        {
            "title": "6.1 Performance Evaluation",
            "content": "We evaluate NeST on diverse set of 10 open-weight instruction-tuned large language models spanning roughly 1B to 14B parameters and covering multiple major model families, including Metas Llama, Alibabas Qwen2.5 and Qwen3 series [43, 5355], Microsofts Phi-4 models [56, 57], and Googles Gemma [44, 58]. All evaluated models are publicly released instruction-following systems that incorporate general-purpose safety alignment, typically via supervised fine-tuning and reinforcement learning from human feedback, with the goal of reducing harmful, unsafe, or policy-violating outputs under standard prompting. This setup reflects realistic deployment conditions for open-weight LLMs, where safety mechanisms are present but may remain vulnerable to blackbox adversarial attacks. We evaluate NeST in comparison to the original baseline models as well as three representative safety adaptation approaches: full-parameter supervised finetuning (Full FT), Circuit Breaker [37], which intervenes on internal activations to suppress unsafe behavior, and parameterefficient LoRA [38] fine-tuning applied to FNN projections. Restricting LoRA to FNN projections ensures fair comparison in both trainable parameter count and structural similarity to NeST. This comparison enables systematic assessment of safety robustness and training efficiency across diverse architectures and model scales. For Full FT, LoRA [38], and NeST, we use balanced supervised fine-tuning dataset composed of harmful and benign prompts. Harmful examples are drawn from the WildJailbreak [59] benchmark, using up to 10,000 adversarial harmful prompts, while benign examples are sam8 inputs rather than memorization or data leakage. Evaluation is conducted using standardized attack success rate (ASR) protocol. For each target model and defense configuration, prompts are formatted using the models native instruction template, and responses are generated using greedy decoding with model-specific maximum generation lengths. To determine whether response constitutes successful jailbreak, an independent safety judge model, Qwen3Guard [60], is used to classify each response as safe or unsafe. prompt is counted as successful attack if the judge labels the response as unsafe and the output does not contain an explicit refusal. The final ASR is computed as the fraction of prompts that result in successful attacks, and this procedure is applied uniformly across all models and methods. The training and evaluation configurations for all compared methods are summarized in Table 1. We report evaluation results using two complementary metrics that capture both safety effectiveness and training efficiency. Table 2 presents the attack success rate (ASR) for each target model and defense method, measuring the fraction of adversarial prompts that result in unsafe responses, where lower values indicate stronger safety robustness. Table 3 reports the number of trainable parameters (in millions) introduced by each method, quantifying the parameter efficiency of different safety adaptation strategies. Together, these metrics enable direct comparison of the trade-offs between safety performance and training cost across models and methods. Target Model Baseline Full FT Circuit Breaker LoRA NeST Llama-3.2-1B Llama-3.2-3B Qwen2.5-7B Qwen2.5-14B Phi-4-mini Phi-4 gemma-2b-it gemma-7b-it Qwen3-4B Qwen3-14B Average 40.5% 54.2% 69.4% 58.1% 25.7% 25.2% 22.0% 34.0% 65.1% 50.9% 44.5% Orig. Exp. 35.8% 48.1% 38.2% 20.1% 24.6% 23.6% 8.6% 12.9% 32.0% 27.1% 58.1% 74.8% 79.1% 65.2% 44.5% 68.2% 39.7% 40.6% 76.4% 72.1% 2.1% 1.9% 1.6% 0.8% 0.3% 1.9% 0.1% 0.4% 0.7% 0.6% 1.4% 3.4% 19.2% 13.0% 5.7% 17.8% 4.2% 6.6% 1.6% 2.8% 1.7% 2.8% 11.0% 2.9% 6.7% 0.0% 0.7% 4.1% 1.5% 12.2% 1.04% 27.1% 61.9% 7.57% 4.36% Table 2: ASR with different methods. For Circuit Breaker, Orig. denotes evaluation on the original traintest setup, while Exp. denotes evaluation under our experimental setup. Across all evaluated models, the original baseline exhibits high attack success rates, indicating substantial vulnerability to adversarial prompts despite existing safety alignment. Full fine-tuning consistently reduces ASR to very low levels across model families, but does so at the cost of updating billions of parameters. LoRA provides moderate improvements in safety with significantly fewer trainable parameters, though its effectiveness varies across architectures and degrades on larger models. Circuit Breaker reduces unsafe behavior under the authors original evaluation setting, but shows substantial variability across models and configurations; when evaluated Target Model Full FT CircuitB. LoRA NeST Llama-3.2-1B Llama-3.2-3B Qwen2.5-7B Qwen2.5-14B Phi-4-mini Phi-4 gemma-2b-it gemma-7b-it Qwen3-4B Qwen3-14B Average 1235.81 3212.75 7615.61 14770.03 3836.00 14660,00 2506.17 8537.68 4022.46 15768.31 7616. 8.45 18.23 30.27 51.60 13.36 68.4 14.16 37.50 19.26 31.52 29.28 3.27 5.04 0.45 18.81 1.62 0.62 6.63 1.54 0.89 1.80 4.07 0.13 0.36 0.45 1.01 0.21 0.42 0.17 0.36 0.38 0.87 0. Table 3: Trainable parameters (million) with different methods. under the same data and protocol used for other methods, its performance degrades markedly and ASR remains high across most models. In contrast, NeST achieves consistently low ASR across wide range of model sizes and architectures, while requiring orders of magnitude fewer trainable parameters than full fine-tuning. Table 2 quantifies these trends across individual models. Baseline attack success rates average 44.5% and exceed 50% on several models, including Qwen2.5-7B, Qwen3-4B, and Llama-3.2-3B, highlighting the fragility of standard safety alignment. Full fine-tuning reduces ASR to 1.4% on average across all models, but at the cost of updating large fraction of model parameters. As shown in Table 3, Full FT requires an average of 7.6 billion trainable parameters, making it computationally expensive and impractical in many settings. Parameter-efficient methods reduce this cost substantially but exhibit different safetyefficiency trade-offs. LoRA reduces the number of trainable parameters to an average of 4.07 million, representing substantial reduction relative to Full FT, yet achieves inconsistent safety gains. ASR remains above 10% on larger models such as Qwen2.5-7B and Qwen2.5-14B, indicating limited robustness when adaptation capacity is constrained. Circuit Breaker requires an average of 24.9 million trainable parameters and shows wide variability across models. Under the unified evaluation protocol used in this work, which evaluates all methods on the same held-out WildJailbreak prompt set, Circuit Breaker ASR remains comparable to or only modestly lower than the baseline, in contrast to its stronger performance when evaluated using the authors original training and test configuration. In contrast, NeST achieves favorable balance between safety effectiveness and parameter efficiency. Across all evaluated models, NeST reduces ASR to an average of 4.8% while requiring only 0.44 million trainable parameters on average. This corresponds to 99.99% reduction in trainable parameters relative to Full FT and approximately 90% reduction relative to LoRA. Despite this significantly smaller parameter footprint, NeST consistently matches or outperforms LoRA in terms of safety. For example, on Qwen2.5-14B, NeST reduces ASR to 2.9% using 1.01M trainable parameters, compared to 9 LoRAs ASR of 13.0% with 18.8M parameters. Similarly, on Llama-3.2-3B, NeST achieves an ASR of 2.8% with 0.36M parameters, while LoRA attains 3.4% ASR with over 5M trainable parameters. These results demonstrate that explicitly targeting safety-relevant neuron structure enables robust and scalable safety improvements without broad or expensive parameter updates."
        },
        {
            "title": "Modalities",
            "content": "In this section, we further evaluate the stability of NeST under different inference settings and input modalities, including text-only, image-only, and their combinations with explicit reasoning. Table 4 reports the attack success rate (ASR) of multiple multimodal models with and without NeST. Without NeST, the base models exhibit consistently high ASR across all configurations, with an average ASR of 55.3%. In particular, strong multimodal models such as Gemma-327B and Qwen3-VL-8B show especially high vulnerability in image-based settings, reaching up to 85.0% ASR. This indicates that both modality and inference style (e.g., reasoningenabled decoding) do not inherently mitigate the attack. In contrast, enabling NeST leads to dramatic and consistent reduction in ASR across all models, modalities, and inference settings. The average ASR drops to only 1.14%, representing relative reduction of over 54.2%. Notably, this suppression remains effective for both text and image inputs, as well as for reasoning-augmented inference, where attacks are often more difficult to control due to longer generation trajectories. Base Model Infer. Setting w/o NeST w/ NeST gemma-3-12b gemma-3-27b Text Text & Reasoning Image Image & Reasoning Text Text & Reasoning Image Image & Reasoning Qwen3-VL-8B (Instruct) Text Image Qwen3-VL-8B (Thinking) Text & Reasoning Image & Reasoning Average 52.6% 52.4% 40.8% 39.8% 57.1% 58.5% 84.2% 85.0% 43.9% 84.0% 43.1% 22.2% 55.3% 4.4% 5.1% 0.2% 1.0% 0.2% 0.2% 0.0% 0.0% 0.0% 0.2% 1.2% 1.2% 1.1% Table 4: ASR with Different Inference Settings. These results demonstrate that NeST is highly stable and robust across heterogeneous inference conditions, and its effectiveness does not depend on specific modalities or decoding strategies. This suggests that the protection mechanism introduced by NeST generalizes well across different generation pathways, making it suitable for deployment in real-world 10 multimodal and reasoning-enabled LLM systems. In particular, the near-zero ASR observed on large models such as Gemma-3-27B indicates that the defense scales favorably with model size and complexity."
        },
        {
            "title": "6.3 Utility Analysis",
            "content": "We evaluate model utility before and after NeST fine-tuning on three standard reasoning benchmarks. GSM8K [61]focuses on multi-step mathematical reasoning, ARC [62] evaluates abstract and commonsense reasoning through multiple-choice science questions, and MMLU [63] measures broad-domain knowledge and reasoning across diverse academic subjects. Together, these benchmarks capture mathematical, logical, and general knowledge reasoning performance. Figure 5 compares model performance before and after NeST fine-tuning on the outlined benchmarks, using standard accuracy as the evaluation metric. Overall, NeST preserves model utility across all benchmarks, with only minor performance changes. On GSM8K, the average accuracy drops by only 0.9 percentage points (from 61.2% to 60.3%). Similarly, ARC shows modest average decrease from 74.0% to 69.1%, while MMLU exhibits small drop from 60.9% to 57.2%. Although some smaller models, such as gemma-2b-it, show more noticeable declines, several models including Qwen2.5-14B, Qwen3-4B, and Phi-4 maintain or slightly improve performance. These results indicate that NeST preserves model capability across diverse reasoning tasks without substantially degrading utility."
        },
        {
            "title": "7 Ablation and Hyperparameter Study",
            "content": "7.1 Impact of the z-Threshold The z-threshold (zthr) controls the selectivity of safety neuron detection by specifying how strongly neurons probe score must deviate from the layer-wise mean to be considered safety-relevant, as formalized in Eq. (7). Lower thresholds include larger set of neurons, potentially introducing noisy or weakly relevant features, while higher thresholds yield smaller and more selective neuron sets that may underrepresent safety-critical behavior. We evaluate three representative thresholds, zthr {2, 3, 4}, while keeping all other components of NeST fixed. Table 5 reports the resulting Attack Success Rates (ASR) across ten target models. On average, moderate threshold of zthr = 3 achieves the lowest ASR at 4.4%, slightly improving over zthr = 2 (4.5%) and substantially outperforming the more restrictive setting zthr = 4, which yields an average ASR of 10.1%. This more than 2.3 increase in ASR at zthr = 4 indicates that overly aggressive neuron filtering can significantly weaken safety enforcement. Beyond the average case, lower and higher thresholds exhibit markedly different stability characteristics. The lower thresh- (a) GSM8K (b) ARC (c) MMLU Figure 5: Utility evaluation before and after NeST fine-tuning across three reasoning benchmarks. NeST preserves performance across mathematical (GSM8K), commonsense (ARC), and broad-domain knowledge (MMLU) tasks. Target Model zthr = 2 zthr = 3 zthr = 4 Llama-3.2-1B Llama-3.2-3B Qwen2.5-7B Qwen2.5-14B Phi-4-mini Phi-4 gemma-2b-it gemma-7b-it Qwen3-4B Qwen3-14B Average 0.5% 1.3% 5.3% 1.9% 12.5% 0.5% 0.5% 1.5% 9.4% 11.4% 4.5% 1.7% 2.8% 11.0% 2.9% 6.7% 0.0% 0.7% 4.1% 1.5% 12.2% 4.4% 4.0% 1.8% 12.6% 14.8% 24.5% 2.7% 25.0% 5.3% 1.8% 8.8% 10.1% Table 5: Impact of the zthr. the same adaptation parameters. This setting is maximally parameter-efficient, as it introduces the smallest number of trainable parameters; (ii) strong clustering (maximum setting) treats each safety neuron as its own cluster, effectively removing parameter sharing and allowing fully independent adaptation, but at the cost of significantly larger number of trainable parameters; (iii) The default NeST setting lies between these two extremes, grouping safety neurons into moderate number of clusters per layer based on activation similarity. This ablation isolates the trade-off between adaptation granularity, safety performance, and parameter efficiency. old zthr = 2 shows increased variance across models, with ASR ranging from 0.5% to 12.5%, suggesting that including broader set of neurons can dilute the safety signal with less relevant activations. In contrast, the highest threshold zthr = 4 leads to pronounced performance degradation on several models, with ASR increasing up to 25.0% on gemma-2b-it. Modelspecific trends further highlight this trade-off. While some models (e.g., Llama-3.2-3B and Qwen3-4B) remain relatively stable under stricter thresholds, others experience increases of over 18% to 24% when moving from zthr = 3 to zthr = 4. Overall, these results indicate that excessively selective neuron identification can exclude small but crucial subset of safety-critical neurons. Based on both average performance and cross-model robustness, we adopt zthr = 3 as the default threshold in all main experiments, as it provides the most reliable balance between neuron coverage and selectivity across architectures and model scales. 7."
        },
        {
            "title": "Impact of Clustering Strength",
            "content": "In this section, we control the degree of parameter sharing among safety neurons through the silhouette threshold used during clustering. Three settings are considered: (i) weak clustering (minimum setting) allocates only single cluster per layer, meaning that all safety neurons within layer share Target Model Weak NeST Setting Strong Llama-3.2-1B Llama-3.2-3B Qwen2.5-7B Qwen2.5-14B Phi-4-mini Phi-4 gemma-2b-it gemma-7b-it Qwen3-4B Qwen3-14B Average 3.3% 3.4% 16.3% 2.5% 13.3% 15.9% 24.3% 15.9% 6.9% 10.4% 11.2% 1.7% 2.8% 11.0% 2.9% 6.7% 0.0% 0.7% 4.1% 1.2% 12.2% 4.3% 1.7% 0.3% 10.4% 2.6% 1.6% 0.7% 1.0% 5.5% 1.8% 15.0% 4.1% Table 6: Impact of Clustering Strength. Table 6 reports the resulting attack success rates across models. On average, weak clustering leads to an ASR of 11.2%, while the default NeST setting achieves 4.3%, and strong clustering yields comparable 4.1%. This shows that although weak clustering is the most computationally efficient, it suffers from substantial degradation in safety performance. Indeed, under weak clustering, forcing all safety neurons in layer to share single set of parameters significantly restricts the expressive capacity of safety adaptation. While it minimizes parameter overhead, it collapses heterogeneous safety behaviors into single update direction, leading to underfitting. In contrast, strong clustering maximizes adaptation flexibility by allowing each safety neuron to be updated 11 independently. This increases the number of trainable parameters and provides the finest granularity of control, which can slightly improve or match performance in some models, such as Llama-3.2-3B, Phi-4-mini, and gemma-2b-it. However, the overall performance gain over the default setting is marginal (4.1% vs. 4.3% on average), despite the significantly higher parameter cost. Moreover, in certain models, such as Qwen3-14B, strong clustering even degrades performance, indicating that fully independent neuron-level updates can introduce instability or overfitting. Overall, these results highlight clear efficiencyperformance trade-off. Weak clustering minimizes computation and memory overhead but sacrifices safety effectiveness, while strong clustering increases parameter cost without delivering commensurate performance gains. The default NeST setting achieves the best balance between these extremes, providing near-optimal safety performance with substantially fewer trainable parameters."
        },
        {
            "title": "8 Discussion",
            "content": "Task-Specific and Structure-Aware Adaptation. Unlike LoRA, which is primarily designed as lightweight, generalpurpose adaptation mechanism, NeST is task-structured by design. While LoRA introduces low-rank updates without regard to the semantic role of individual neurons, NeST explicitly aligns adaptation with the internal structure of target behavior. In this work, we instantiate NeST for safety alignment by detecting and adapting safety-relevant neurons, but the framework itself is not inherently limited to safety. The central design principle of NeST is to first identify neurons that are causally and functionally associated with given behavior, and then restrict adaptation to those neurons in structured manner. This formulation naturally extends beyond safety alignment. By modifying the neuron detection procedure, NeST can be applied to harden or specialize models for other targeted behaviors. In such settings, neuron detection is performed with respect to the relevant behavioral distinction, while the same cluster-based adaptation mechanism enables lightweight, behavior-specific fine-tuning aligned with the models internal structure. Viewed more broadly, NeST provides general framework for neuron-structured, task-specific parameter-efficient adaptation. Unlike generic parameter-efficient methods that apply uniform constraints across layers or projections, NeST defines where to adapt based on internal representation structure, making it particularly well-suited for tasks where the target behavior is localized within specific neuron groups. Post-Hoc Safety Hardening Under Downstream FineTuning. Another important consideration is the interaction between safety hardening and downstream fine-tuning. In practice, deployed models are frequently adapted after release through additional supervised fine-tuning or preference optimization to support new tasks, domains, or user requirements. Such downstream fine-tuning can inadvertently weaken or overwrite previously learned safety behaviors, even when the original safety alignment was effective. NeST is particularly well-suited to this setting due to its lightweight and localized nature. Because NeST modifies only small subset of safety-relevant neurons, it can be efficiently re-applied after downstream fine-tuning to re-establish safety guarantees without retraining the full model or introducing inferencetime defenses. This positions NeST as practical post-hoc safety re-hardening mechanism, enabling model providers to restore safety properties after capability-driven updates. More broadly, NeST complements downstream fine-tuning in modern LLM development pipelines by serving as final-stage hardening step, helping preserve safety behavior as models evolve through repeated adaptation cycles. Compatibility with Diverse Safety Alignment Methods. In this work, NeST is implemented on top of supervised finetuning for safety alignment; however, the framework itself is not tied to specific optimization objective. The neuron detection, clustering, and selective adaptation mechanisms operate directly at the level of model parameters and gradients, making NeST compatible with wide range of alignment paradigms. In principle, NeST can be integrated with reinforcement learningbased approaches such as reinforcement learning from human feedback (RLHF), group-relative policy optimization (GRPO), and other preference-based optimization methods. In these settings, NeST would restrict gradient updates during policy optimization to the clustered safetyrelevant neurons, rather than allowing unrestricted parameter updates. This positions NeST as structural constraint on alignment optimization, agnostic to whether the underlying objective is supervised, preference-based, or reinforcementdriven. Known-Defense and White-Box Attacks. NeST is designed for deployment settings in which attackers interact with the model exclusively through prompts and do not have access to internal parameters or activations, consistent with the black-box adversary defined in Section 3.1. Neuron-level white-box attacks that directly manipulate internal representations [25,64] therefore fall outside the threat model considered in this work. While NeST distributes safety behavior across coordinated neuron groups rather than isolated units, which may increase the difficulty of precise neuron-targeted manipulation, no training-time alignment method can guarantee robustness against adversaries with unrestricted white-box access or the ability to modify model weights. More broadly, defenses against white-box or known-defense attackers typically require complementary system-level protections, such as secure model hosting, access control, monitoring, or runtime detection mechanisms, which operate beyond the scope of intrinsic safety tuning. Accordingly, NeST should be viewed as strengthening robustness within the realistic black-box deployment regime described in our threat model, rather than as universal defense against all possible attack surfaces. Addressing stronger adversarial capabilities remains an important direction for future work."
        },
        {
            "title": "9 Related Works",
            "content": "Practical deployment of large language models requires adaptation methods that are both highly parameter-efficient and capable of reliably enforcing safety behavior. However, prior work lacks unified framework that simultaneously achieves strong parameter efficiency and structure-aware safety alignment. Existing approaches typically fall into two complementary directions: parameter-efficient fine-tuning methods, which focus on reducing training and storage cost while adapting model behavior, and safety-localization or intervention methods, which aim to identify and control internal components responsible for unsafe generation. While each direction addresses an important aspect of practical safety alignment, they are generally developed in isolation, leaving open the challenge of achieving safety-aware adaptation that is both structurally grounded and highly parameter-efficient. Parameter-efficient fine-tuning (PEFT) methods address the efficiency requirement of practical large language model adaptation by updating only small fraction of parameters. Representative approaches include adapter-based tuning [65,66] and lightweight parameter updates such as bias-only or scalingbased tuning [67, 68]. LoRA [38] has emerged as widely adopted PEFT method by injecting low-rank updates into existing weight matrices while freezing the pretrained parameters. While these methods are effective for reducing training and storage costs, they generally treat model parameters as homogeneous space and apply uniform structural constraints, without explicitly considering where task-relevant or safetyrelevant behavior is encoded within the model. Consequently, PEFT approaches provide strong parameter efficiency but do not directly capture or exploit the localized neural mechanisms underlying safety behavior. small but growing body of work has explored neuron and representation-level approaches to safety alignment, motivated by findings that safety behavior is often localized within specific internal components of large language models. Among these, SN-Tune [48] is the most closely related to our work. SN-Tune identifies safety-relevant neurons and selectively fine-tunes them to improve refusal behavior while freezing the remainder of the model. SN-Tune treats safety neurons as independent units and updates them individually, unlike our structured, cluster-based adaptation. In addition, its neuron detection procedure is computationally heavy, and the method relies primarily on unsafe samples paired with refusal targets during fine-tuning. This training setup can increase over-refusal and reduce the likelihood of helpful responses, 13 leading to high refusal rates. Due to these limitations, SNTune is not primary focus of comparison in this work. Another line of work focuses on mitigating unsafe behavior through representation-level intervention mechanisms. Circuit Breakers [37] learn auxiliary LoRA-based modules that modify internal representations linked to harmful generation, rerouting these representations toward incoherent or refusal states. This approach can reduce unsafe behavior without fullparameter fine-tuning, but still relies on additional learned modules and LoRA-scale parameter updates. Moreover, its optimization objective is sensitive to the data used to produce harmful representations, making robustness dependent on dataset coverage and limiting generalization under distribution shift. Collectively, these limitations highlight the need for approaches that directly target localized safety structure while maintaining strong parameter efficiency and robust generalization. Taken together, prior approaches either emphasize parameter efficiency without explicitly modeling the neural structure underlying safety behavior, or improve safety control through neuron-level or representation-level interventions. In contrast, NeST provides unified framework for structureaware safety adaptation with extreme parameter efficiency. Unlike SN-Tune, which relies on computationally heavy neuron detection and unsafe-only supervision that can increase over-refusal, and Circuit Breakers, which depend on LoRAscale auxiliary modules and exhibit sensitivity to training data coverage, NeST enables localized safety adaptation while maintaining strong parameter efficiency and generalization. Moreover, the proposed framework naturally extends beyond safety to broader task-specific neuron-structured adaptation."
        },
        {
            "title": "10 Conclusions",
            "content": "This paper presents NeST, lightweight, efficient, and structure-aware framework for safety alignment in large language models. The method enables targeted, task-specific safety adaptation, and its lightweight design allows it to be applied post hoc to efficiently re-harden models after downstream fine-tuning. By selectively tuning clustered safetyrelevant components while freezing the remainder of the model, NeST achieves large reductions in attack success rates across diverse open-weight language and multimodal models using orders-of-magnitude fewer trainable parameters than full fine-tuning and lower cost than existing parameterefficient or representation-level defenses. At the same time, the method preserves core reasoning and knowledge capabilities and remains stable across inference settings and modalities. These results show that robust and scalable safety alignment can be achieved through structured, localized adaptation, providing practical path for maintaining safety as models evolve through repeated task adaptation and deployment."
        },
        {
            "title": "Ethical Considerations",
            "content": "Overview. This work proposes NeST, lightweight and structure-aware framework for strengthening safety alignment in large language models. The objective is to reduce unsafe generations, improve robustness to harmful prompts, and support safer deployment of open-weight and multimodal LLM systems. Stakeholders. Relevant stakeholders include LLM developers and deployers, organizations integrating AI into user-facing systems, safety researchers, and end users who may be affected by unsafe model behavior. The method is intended to improve reliability while preserving useful model capabilities. Data and Experimental Protocols. All experiments use publicly available datasets, open-weight pretrained models, and controlled offline evaluation. No private, proprietary, or personally identifiable data were collected or processed, and no real-world deployment was involved. Potential Impact. NeST strengthens intrinsic safety mechanisms while maintaining utility and enabling efficient posthoc safety hardening after downstream fine-tuning, contributing to scalable and maintainable safety practices for evolving LLM systems. Mitigations for Negative Impacts. The framework operates entirely through offline training and evaluation and does not introduce mechanisms for generating harmful content or weakening safeguards. Released artifacts focus on defensive safety alignment and include documentation emphasizing responsible research use. Decision to Conduct and Publish the Study. We report this work to advance scalable safety alignment and support the development of more robust and trustworthy LLM deployments as real-world use continues to expand. Research Team Considerations. The study involves only controlled benchmark evaluation and follows standard responsible research practices to ensure safe handling of safetyrelated data and contributor well-being."
        },
        {
            "title": "References",
            "content": "[1] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao, Large language models: survey, arXiv preprint arXiv:2402.06196, 2024. [2] A. Matarazzo and R. Torlone, survey on large language models with some insights on their capabilities and limitations, arXiv preprint arXiv:2501.04040, 2025. [3] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy, Challenges and applications of large language models, arXiv preprint arXiv:2307.10169, 2023. [4] F. Perez and I. Ribeiro, Ignore previous prompt: Attack techniques for language models, arXiv preprint arXiv:2211.09527, 2022. [5] S. Jiang, X. Chen, and R. Tang, Prompt packer: Deceiving llms through compositional instruction with hidden attacks, arXiv preprint arXiv:2310.10077, 2023. [6] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, \" do anything now\": Characterizing and evaluating in-thewild jailbreak prompts on large language models, in Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, 2024, pp. 16711685. [7] H. Jin, R. Chen, A. Zhou, Y. Zhang, and H. Wang, Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models, arXiv preprint arXiv:2402.03299, 2024. [8] J. Yu, X. Lin, Z. Yu, and X. Xing, Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts, arXiv preprint arXiv:2309.10253, 2023. [9] , {LLM-Fuzzer}: Scaling assessment of large language model jailbreaks, in 33rd USENIX Security Symposium (USENIX Security 24), 2024, pp. 46574674. [10] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, Hotflip: White-box adversarial examples for text classification, arXiv preprint arXiv:1712.06751, 2017. [11] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, Autoprompt: Eliciting knowledge from language models with automatically generated prompts, arXiv preprint arXiv:2010.15980, 2020. [12] B. Lester, R. Al-Rfou, and N. Constant, The power of scale for parameter-efficient prompt tuning, arXiv preprint arXiv:2104.08691, 2021. [13] L. Qin, S. Welleck, D. Khashabi, and Y. Choi, Cold decoding: Energy-based constrained text generation with langevin dynamics, Advances in Neural Information Processing Systems, vol. 35, pp. 95389551, 2022. [14] Y. Wen, N. Jain, J. Kirchenbauer, M. Goldblum, J. Geiping, and T. Goldstein, Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery, Advances in Neural Information Processing Systems, vol. 36, pp. 51 00851 025, 2023. [15] E. Jones, A. Dragan, A. Raghunathan, and J. Steinhardt, Automatically auditing large language models via discrete optimization, in International Conference on Machine Learning. PMLR, 2023, pp. 15 30715 329. [16] A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson, Universal and transferable adversarial attacks on aligned language models, arXiv preprint arXiv:2307.15043, 2023. [17] F. Wu, X. Liu, and C. Xiao, Deceptprompt: Exploiting llm-driven code generation via adversarial natural language instructions, arXiv preprint arXiv:2312.04730, 2023. [18] X. Liu, N. Xu, M. Chen, and C. Xiao, Autodan: Generating stealthy jailbreak prompts on aligned large language models, arXiv preprint arXiv:2310.04451, 2023. [19] T. Gao, A. Fisch, and D. Chen, Making pre-trained language models better few-shot learners, arXiv preprint arXiv:2012.15723, 2020. [20] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng, Automatic prompt optimization with\" gradient descent\" and beam search, arXiv preprint arXiv:2305.03495, 2023. [21] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, Large language models are humanlevel prompt engineers, in The Eleventh International Conference on Learning Representations, 2022. [22] P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong, Jailbreaking black box large language models in twenty queries, arXiv preprint arXiv:2310.08419, 2023. [23] A. Mehrotra, M. Zampetakis, P. Kassianik, B. Nelson, H. Anderson, Y. Singer, and A. Karbasi, Tree of attacks: Jailbreaking black-box llms automatically, Advances in Neural Information Processing Systems, vol. 37, pp. 61 06561 105, 2024. [24] Z. Chang, M. Li, Y. Liu, J. Wang, Q. Wang, and Y. Liu, Play guessing game with llm: Indirect jailbreak attack with implicit clues, arXiv preprint arXiv:2402.09091, 2024. 15 [25] L. Wu, S. Behrouzi, M. Rostami, M. Thang, S. Picek, and A.-R. Sadeghi, Neurostrike: Neuron-level attacks on aligned llms, Network and Distributed System Security (NDSS) Symposium, 2026. [26] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., Training language models to follow instructions with human feedback, Advances in neural information processing systems, vol. 35, pp. 27 73027 744, 2022. [27] Y. Yuan, W. Jiao, W. Wang, J.-t. Huang, J. Xu, T. Liang, P. He, and Z. Tu, Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training, in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025, pp. 31493167. [28] F. Bianchi, M. Suzgun, G. Attanasio, P. Röttger, D. Jurafsky, T. Hashimoto, and J. Zou, Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions, arXiv preprint arXiv:2309.07875, 2023. [29] G.-H. Kim, Y. Jang, Y. J. Kim, B. Kim, H. Lee, K. Bae, and M. Lee, Safedpo: simple approach to direct preference optimization with enhanced safety, arXiv preprint arXiv:2505.20065, 2025. [30] H. Shen, P.-Y. Chen, P. Das, and T. Chen, Seal: Safetyenhanced aligned llm fine-tuning via bilevel data selection, arXiv preprint arXiv:2410.07471, 2024. [31] A. Kádár, G. Chrupała, and A. Alishahi, Representation of linguistic form and function in recurrent neural networks, Computational Linguistics, vol. 43, no. 4, pp. 761780, 2017. [32] S. Na, Y. J. Choe, D.-H. Lee, and G. Kim, Discovery of natural language concepts in individual units of cnns, arXiv preprint arXiv:1902.07249, 2019. [33] J. Mu and J. Andreas, Compositional explanations of neurons, Advances in Neural Information Processing Systems, vol. 33, pp. 17 15317 163, 2020. [34] X. Suau, L. Zappella, and N. Apostoloff, Finding experts in transformer models, arXiv preprint arXiv:2005.07647, 2020. [35] O. Antverg and Y. Belinkov, On the pitfalls of analyzing individual neurons in language models, arXiv preprint arXiv:2110.07483, 2021. [36] Y. Lakretz, G. Kruszewski, T. Desbordes, D. Hupkes, S. Dehaene, and M. Baroni, The emergence of number and syntax units in lstm language models, arXiv preprint arXiv:1903.07435, 2019. [37] A. Zou, L. Phan, J. Wang, D. Duenas, M. Lin, M. Andriushchenko, R. Wang, Z. Kolter, M. Fredrikson, and D. Hendrycks, Improving alignment and robustness with circuit breakers, 2024, URL https://arxiv. org/abs/2406.04313, vol. 1, no. 6, p. 15. [38] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models. ICLR, vol. 1, no. 2, p. 3, 2022. [39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [40] M. Geva, R. Schuster, J. Berant, and O. Levy, Transformer feed-forward layers are key-value memories, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 5484 5495. [41] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [42] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [43] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025. [44] G. Team, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. Ramé, M. Rivière et al., Gemma 3 technical report, arXiv preprint arXiv:2503.19786, 2025. [45] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan et al., Training helpful and harmless assistant with reinforcement learning from human feedback, arXiv preprint arXiv:2204.05862, 2022. [46] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, Deep reinforcement learning from human preferences, Advances in neural information processing systems, vol. 30, 2017. [47] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, Direct preference optimization: Your language model is secretly reward model, Advances in neural information processing systems, vol. 36, pp. 53 72853 741, 2023. 16 [48] Y. Zhao, W. Zhang, Y. Xie, A. Goyal, K. Kawaguchi, and M. Shieh, Understanding and enhancing safety mechanisms of llms via safety-specific neuron, in The Thirteenth International Conference on Learning Representations, 2025. [49] R. Bhardwaj, D. D. Anh, and S. Poria, Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic, 2024. [50] R. Bhardwaj and S. Poria, Red-teaming large language models using chain of utterances for safety-alignment, 2023. [51] LLM-LAT, harmful-dataset, [Online]. Available: https://huggingface.co/datasets/LLM-LAT/ harmful-dataset 2024. [52] W. Yuan, J. Yu, S. Jiang, K. Padthe, Y. Li, D. Wang, I. Kulikov, K. Cho, Y. Tian, J. E. Weston, and X. Li, Naturalreasoning: Reasoning in the wild with 2.8m challenging questions, 2025. [Online]. Available: https://arxiv.org/abs/2502.13124 [53] Q. Team, Qwen2.5: party of foundation models, September 2024. [Online]. Available: https://qwenlm. github.io/blog/qwen2.5/ [54] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Dang et al., Qwen2. 5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. 2024. [Online]. Available: https://arxiv.org/abs/2406. [60] H. Zhao, C. Yuan, F. Huang, X. Hu, Y. Zhang, A. Yang, B. Yu, D. Liu, J. Zhou, J. Lin et al., Qwen3guard technical report, arXiv preprint arXiv:2510.14276, 2025. [61] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. [62] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, Think you have solved question answering? try arc, the ai2 reasoning challenge, arXiv preprint arXiv:1803.05457, 2018. [63] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, Measuring massive multitask language understanding, arXiv preprint arXiv:2009.03300, 2020. [64] T. Krauß, H. Dashtbani, and A. Dmitrienko, Twinbreak: Jailbreaking llm security alignments based on twin prompts, arXiv preprint arXiv:2506.07596, 2025. [65] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, Parameter-efficient transfer learning for nlp, in International conference on machine learning. PMLR, 2019, pp. 27902799. [55] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin et al., Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement, arXiv preprint arXiv:2409.12122, 2024. [66] J. Pfeiffer, A. Rücklé, C. Poth, A. Kamath, I. Vulic, S. Ruder, K. Cho, and I. Gurevych, Adapterhub: framework for adapting transformers, arXiv preprint arXiv:2007.07779, 2020. [67] E. B. Zaken, Y. Goldberg, and S. Ravfogel, Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2022, pp. 19. [68] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. Raffel, Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning, in Advances in Neural Information Processing Systems (NeurIPS), 2022. [56] M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann et al., Phi-4 technical report, arXiv preprint arXiv:2412.08905, 2024. [57] A. Abouelenin, A. Ashfaq, A. Atkinson, H. Awadalla, N. Bach, J. Bao, A. Benhaim, M. Cai, V. Chaudhary, C. Chen et al., Phi-4-mini technical report: Compact yet powerful multimodal language models via mixtureof-loras, arXiv preprint arXiv:2503.01743, 2025. [58] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love et al., Gemma: Open models based on gemini research and technology, arXiv preprint arXiv:2403.08295, 2024. [59] L. Jiang, K. Rao, S. Han, A. Ettinger, F. Brahman, S. Kumar, N. Mireshghallah, X. Lu, M. Sap, Y. Choi, and N. Dziri, Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models,"
        }
    ],
    "affiliations": [
        "Technical University of Darmstadt"
    ]
}