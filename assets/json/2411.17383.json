{
    "paper_title": "AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation",
    "authors": [
        "Ziyi Xu",
        "Ziyao Huang",
        "Juan Cao",
        "Yong Zhang",
        "Xiaodong Cun",
        "Qing Shuai",
        "Yuchen Wang",
        "Linchao Bao",
        "Jintao Li",
        "Fan Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The automatic generation of anchor-style product promotion videos presents promising opportunities in online commerce, advertising, and consumer engagement. However, this remains a challenging task despite significant advancements in pose-guided human video generation. In addressing this challenge, we identify the integration of human-object interactions (HOI) into pose-guided human video generation as a core issue. To this end, we introduce AnchorCrafter, a novel diffusion-based system designed to generate 2D videos featuring a target human and a customized object, achieving high visual fidelity and controllable interactions. Specifically, we propose two key innovations: the HOI-appearance perception, which enhances object appearance recognition from arbitrary multi-view perspectives and disentangles object and human appearance, and the HOI-motion injection, which enables complex human-object interactions by overcoming challenges in object trajectory conditioning and inter-occlusion management. Additionally, we introduce the HOI-region reweighting loss, a training objective that enhances the learning of object details. Extensive experiments demonstrate that our proposed system outperforms existing methods in preserving object appearance and shape awareness, while simultaneously maintaining consistency in human appearance and motion. Project page: https://cangcz.github.io/Anchor-Crafter/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 3 8 3 7 1 . 1 1 4 2 : r AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation Ziyi Xu1, Ziyao Huang1, Juan Cao1, Yong Zhang2, Xiaodong Cun3, Qing Shuai4, Yuchen Wang1, Linchao Bao4, Jintao Li1, Fan Tang1 1 Institute of Computing Technology, Chinese Academy of Sciences 2 Meituan 3 Great Bay University 4 Tencent xuziyi23@mails.ucas.ac.cn {huangziyao19f, caojuan}@ict.ac.cn {zhangyong201303, vinthony}@gmail.com wangyuchen231@mails.ucas.ac.cn {jtli, tangfan}@ict.ac.cn Figure 1. We propose AnchorCrafter, diffusion-based human video generation framework for creating high-fidelity anchor-style product promotion videos by animating reference human images with specific products and motion controls. By incorporating human-object interaction into the generation process, AnchorCrafter achieves high preservation of object appearance and enhanced interaction awareness."
        },
        {
            "title": "Abstract",
            "content": "The automatic generation of anchor-style product promotion videos presents promising opportunities in online commerce, advertising, and consumer engagement. However, this remains challenging task despite significant advancements in pose-guided human video generation. In addressing this challenge, we identify the integration of human-object interactions (HOI) into pose-guided human video generation as core issue. To this end, we introduce AnchorCrafter, novel diffusion-based system de- *Equal contribution. Corresponding author: Fan Tang. signed to generate 2D videos featuring target human and customized object, achieving high visual fidelity and Specifically, we propose two controllable interactions. key innovations: the HOI-appearance perception, which enhances object appearance recognition from arbitrary multi-view perspectives and disentangles object and human appearance, and the HOI-motion injection, which enables complex human-object interactions by overcoming challenges in object trajectory conditioning and interocclusion management. Additionally, we introduce the HOI-region reweighting loss, training objective that enhances the learning of object details. Extensive experiments demonstrate that our proposed system outperforms existing methods in preserving object appearance and shape awareness, while simultaneously maintaining consistency in human appearance and motion. Project is available at https://cangcz.github.io/Anchor-Crafter/ 1. Introduction Viewing unboxing videos and live-streamed product promotions by content creators and live streamerscollectively referred to as anchorson platforms such as YouTube, TikTok, Douyin, etc. has become an integral part of the online shopping experience for users. Recent advancements in video generation [2, 12, 13, 28, 39] have made it possible to automate the creation of such unboxing and promotional content. However, generating high-fidelity, realistic anchorstyle product promotion videos remains significant technical challenge. Pose-guided human video generation [4, 14, 16, 32, 36, 41, 42] aligns closely with anchor-style product promotion videos. Existing diffusion-based methods generate temporally consistent, high-fidelity human videos using pose and appearance references. Make-Your-Anchor [16] pioneered personalized anchor-style video generation, while methods like AnimateAnyone [14] and MimicMotion [41] animate static human images with precise poses, adaptable for anchor-style video generation. However, these approaches fail to generate human-driven product displays. As shown in Fig. 2, they treat objects as static textures, preventing appropriate interaction movement. To address this issue, the main challenge lies in the lack of human-object interaction (HOI) capabilities [5, 37]. Specifically, existing methods fail to understand objects as secondary entities, cannot control object trajectories, and struggle to interpret the complex interactions between human movements and objects. Recent studies on HOI generation [5, 37] primarily focus on hand-centric videos or image domains, which do not offer the degrees of freedom necessary for anchor-style product promotion video generation. Therefore, integrating HOI into pose-guided human video generation remains critical challenge. In this study, we propose AnchorCrafter, novel system for generating anchor-style product promotion videos by integrating human-object interaction (HOI) into poseguided human video diffusion models. By learning from reference videos, our system automatically creates realistic human-object interaction videos using images of humans and objects, motion sequences, and object trajectories. AnchorCrafter achieves this through two core components: object appearance preservation and interaction guidance. For appearance preservation, we introduce HOIappearance perception, which improves object appearance accuracy by fusing multi-view features and using decoupling network structure to disentangle human-object appearances. For interaction guidance, HOI-motion injection Reference Input AnimateAnyone MimicMotion Ours Figure 2. Animating reference image of peron holding an object, existing methods accurately follow human poses but fail to generate hand-object interaction (AnimateAnyone [14]) or misinterpret the object as part of the human, leading to no movement (MimicMotion [41]). Our method accurately generates humanobject interactions while preserving the objects appearance. enables precise control of object trajectories using depth and hand 3D mesh input, while mitigating interaction artifacts with occlusion handling strategies. Besides, we use an HOI-region reweighting loss during training to enhance the detail of objects by emphasizing the hand-object interaction region. We conduct extensive experiments on real-world objects and demonstrate that AnchorCrafter achieves object appearance preserved and interaction awareness results, which are not accomplished by current approaches. Additionally, our method achieves competitive performance with state-ofthe-art approaches across various image and video quality evaluations. In summary, our contributions are as follows: We propose AnchorCrafter, novel system incorporating the human-object interaction into pose-guided human video models, and generating realistic anchor-style product promotion videos by animating reference anchors. We propose an HOI-appearance perception that utilizes novel multi-view feature fusion and decoupled injection structure to achieve object appearance preservation, an HOI-motion injection to condition object motion and handle inter-occlusion, and an HOI-region reweighting loss to enhance object details. We conducted quantitative and qualitative evaluations to demonstrate the effectiveness of AnchorCrafter, comparing it with state-of-the-art diffusion-based human video generation and editing methods. 2. Related Work Pose-Guided Human Video Generation. Pose-guided human video generation utilized sequence of poses to control the human motion in the generated videos, commonly applied in scenarios such as dance and speech video generation. With the rapid advancements in diffusion models [11, 26], pose-guided human video human generation archives remarkable progress. The common approaches leverage ControlNet [40], parallel control branch attached to the diffusion UNet, to inject human skeletal motion sequences [4, 16, 30, 32, 36], thereby achieving advanced pose guidance. AnimateAnythe human one [14] proposed ReferenceNet to control appearance, which injects the information through the selfattention mechanism. Champ [42] enhances motion control by combining four different types of poses. MimicMotion [41] engineered lightweight pose guidance network to incorporate human pose conditions, and employed confidence-aware pose guidance to enhance the quality of hand motion synthesis. These studies demonstrate that by incorporating human poses such as OpenPose [3], DensePose [9], SMPL-X [22], control over human movement can be achieved. However, the important human-object interaction generation is ignored, despite its importance for real-world applications such as anchor-style product promotion video generation, while AnchorCrafter incorporates HOI into human video generation to accomplish it. Human-Object Interaction Generation. The goal of Human-Object Interactions Generation (HOI) is to generate visual content that accurately represents the relationships between humans and objects. Some studies primarily focused on HOI motion generation. OMOMO [20], HOIDiff [24] and InterDiff [35] predict human motions during interactions with objects based on 3D object representations. IMoS [8] generates full-body human and 3D object motions from textual input, but its focus is primarily on hand-grasping small objects. HOI-Swap [37] zeroes in hand-centric videos, utilizing two-stage editing technique to swap the object in hand while keeping the hand-object interactions. VirtualModel [5] stands as the most relevant work in literature, albeit within the domain of image generation. It excels in synthesizing interactive images of characters by leveraging input objects and human poses. Existing methods primarily focus on HOI image generation or object swapping in hand-centric videos, leading to the absence of the ability to generate videos with sufficient degrees of freedom, particularly videos that include the entire human body. In contrast, our approach integrates HOI capabilities into human video generation model, enabling the generation of high-quality, controllable anchor-style product promotion videos. 3. System Setting Overview. Even though existing approaches can generate high-quality human videos from single image, we argue that for real-world applications like product promotion, single image as input cannot accurately capture the products information for video generation. In this study, we accomplish anchor-style product promotion generation by setting the task as learning paradigm for customized products: the information of objects is learned from several sample videos, and the model can generate the interaction video with arbitrary human appearance and motion controls. In practical applications, by capturing interaction guidance provided by an actor and providing only one image of the target anchor, the model can animate the anchor to display the product, which is applicable to crafting real-world unboxing videos or live-streamed product promotions. Training. During training, set of anchor-style product interaction videos is required to depict interactions between humans and specific products. Each source video denoted as = {xi:n} with frames allows the model to learn the target object and the interaction distribution between humans and the object. The video duration for each object is configured to exceed twenty seconds to ensure sufficient interaction data. For appearance, reference human image IH for the target anchor and multi-view images of the target object IO = {i1 O} are required. We employ multiply comprehensive conditions to control the HOI motions, including the sequences of human skeletons = {p1:n}, 3D hand meshes = {h1:n} and depth maps of the object = {d1:n}. The system generates sequence of frames = {y1:n} with target human and customized object that captures controlled human-object interactions. O, i3 O, i2 Inference. In the inference stage, we use HOI motions performed by actors to drive the generation of anchor videos. Actors demonstrate the required interaction actions by holding the products learned during training, and the extracted control conditions , H, will serve as inputs to the system. Subsequently, our model can drive the generation of product-promotion videos for any unseen anchor image IH . 4. Methodology The whole pipeline of our proposed system is illustrated in Fig. 3, where two primary components are compromised: the HOI-appearance perception, and the HOI-motion injection. At the beginning of this section, we will briefly introduce the video diffusion model architecture utilized in AnchorCrafter in Sec. 4.1. HOI-appearance perception, which is proposed for appearance control, will be detailed in Sec. 4.2. Next to be introduced in Sec. 4.3 is the HOImotion injection, where motion guidance with object track and inter-occlusion issues are considered. Furthermore, we adopt an HOI-region reweighting loss aimed at enhancing object details in Sec. 4.4. Figure 3. Training pipeline for AnchorCrafter: Based on video diffusion model, AnchorCrafter injects human and multi-view object references into the video via HOI-appearance perception. The motion is controlled through HOI-motion injection, with the training objective reweighted in the HOI region. 4.1. Video Diffusion Model AnchorCrafter is based on video diffusion model [2] architecture, comprising diffusion UNet [26, 27] with temporal layers, denoted as ϵθ, and variational autoencoder (VAE) [17] composed of an encoder Enc and decoder Dec for compressing and uncompressing the video frames. During training, the video sequence is encoded into the latent space as Z0 = Enc(X) = {z1:n 0 }. The core training objective for the video diffusion model is: Ldif = EϵN (0,I),Z,c,t[ϵ ϵθ(Zt, c, t)2 2], (1) where is conditional signal, Zt = {z1:n } is the latent feature diffused from Z0 through deterministic Gaussian process over timesteps by adding noise ϵ. During inference, the initial noises = {u1:n} sampled from Gaussian noise are denoised with timesteps to get the estimated ˆZ0, and the output video with = Dec( ˆZ0) = {y1:n}. 4.2. HOI-Appearance Perception HOI-appearance perception is illustrated in Fig. 4. The design of this module is centered around the extraction of appearance features for both the human and the object, followed by their integration into the backbone network. We begin with the extraction of human appearance features. We refer to previous approaches [14, 32, 34, 41] by integrating both global context and local details from the VAE encoder Enc the input images. maps human images to latent space compatible with the diffusion model ZH = Enc(IH ), which preserves the local detail. Then repeat ZH times and concatenate ZH with the input for each frame Zt of the UNet. Meanwhile, the Specifically, Figure 4. HOI-appearance perception: The feature of the target object fO is extracted through multi-view object feature fusion and combined with the human reference feature fH within humanobject dual adapter to achieve improved disentanglement results. CLIP [25] features of the human image are extracted as fH = CLIP (IH ) for global representation. O, O, i3 To accurately depict objects, especially the product being promoted, the model must perceive their shape and texture from multiple perspectives. Instead of single reference image, we utilize multi-view object reference images IO = {i1 O} captured from 45 left, front, and 45 right rotations. Similar to human reference, the VAE feature of the front reference i2 O) and then repeats and concat with the Zt. Different from the process for human reference, we propose multi-view object feature fusion to extract richer information from multi-view images of the input object images. is extracted as ZO = Enc(i2 Multi-View Object Feature Fusion. We introduce the multi-view object feature fusion to understand object appearance inherently. The process begins by feeding multiview object reference IO into the pre-trained DINOv2large model [21]. The three-view DINO features are then processed through two distinct branches, as illustrated in Fig. 4. To facilitate the models learning of the 3D consistency of objects from multi-view features, three-view DINO features are passed through multi-view self-attention layer. We only retain the output features corresponding to i2 O, and then another self-attention operation is applied to this tensor. After these self-attention layers, we retained the first twelve dimensions of the features as the final output, with the size of R121024. In addition, we concatenate the first [CLS] feature from three-view DINO features vector of size R31024 for global representation, then pass through linear layer. The two resulting outputs are concatenated as the final multiview object representation, fO R151024. This module is designed to allow the model to exploit the robust representative capability of DINO in extracting 3D consistency features from multi-view object images [29]. Human-Object Dual Adapter. We observed that directly integrating object features fO into UNet along with human features fH leads to the appearance entanglement of the objects and humans. We propose human-object dual adapter by replacing the cross-attention layers in each block of the diffusion UNet to achieve better disentanglement between humans and objects. The human feature fH extracted from CLIP [25] and the object feature fO obtained by multiview object feature fusion are each fed into cross-attention layer, and the formula can be written as: HumanCA := Sof tmax( QK ) VH , (2) and ObjectCA := Sof tmax( QK ) VO, (3) where = WQ Zt, KH = WKH fH , KO = WKO fO, VH = WV fH , VO = WV fo, Zt = {z1:n } is the input latent frames on diffusion timestep and WQ, WKH , WKO, WV are learnable weights for attention modules. The output features from the HumanCA and ObjectCA layers are then summed to produce the final feature set. 4.3. HOI-Motion Injection Despite the appearance of preservation, another key challenge lies in generating specified interactive motions between humans and objects. In this section, three issues are considered for injecting the HOI motions. To condition the movement of objects in video generation, critical challenge is how to provide an unambiguous representation of the orientation and positional trajectory in 3D space. We utilize the depth map as input for the trajectory of the object. We estimate and crop the depth map of the target object, and lightweight ninelayer convolution network is employed to process D. The resulting features are element-wise added to the output of the first convolutional layer of the UNet. Building upon the control of object trajectories, we further consider the occlusion between the object and the human hands during interactions. In addition to the commonly used human skeleton for controlling the overall human pose, we extract the 3D mesh sequences of the human hands and mask the corresponding 3D mesh where the object occludes the hands. The two conditions are concatenated and processed through convolution network with the same structure as the object interaction but with unshared parameters. The features extracted by this network are subsequently added to the features within the U-Net. We observe that spatial differences between the input pose sequence , H, and and the reference human image IH affect the generated results. For example, when the reference IH is an upper-body picture, and the input poses includes the full body, the inference based on the fullbody pose may lead to the speculation of unseen lowerbody details, resulting in distortion of the consistency and accuracy of the generated videos. To address this problem, we estimate similarity matrix from the first frame of to the pose of IH , and then the similarity matrix is performed on all motion conditions including , H, and to match the spatial position with IH . 4.4. HOI-Region Reweighting Loss By constructing modules for HOI, our model enables control over the generated humans, objects, and their interactions. Accordingly, based on the diffusion models loss function as described in Eq. 1, our training objective can be formulated with = {P, H, D, IH , IO}. However, we observed that standard training loss caused the model to fail to adequately learn object appearances. To mitigate this, we propose HOI-region reweighting loss, which enhances the models attention to interaction regions throughout the training process: Lobject = η Sobj + Shand Minter Ldif , (4) where S, Sobj, Shand is the area of the whole image, object, and hands, Minter indicates the mask of the interaction region consists of object and hands, and η is hyperparameter. Using the inverse of the image area occupied by the interaction region as weight, the model can assign higher training importance to the interaction area. The final loss is: Lf inal = (1 Minter) Ldif + Lobject. (5) After applying the final loss, pose-driven capability across diverse human appearances is preserved and objects are learned precisely. 5. Experiments 5.1. Experimental Settings Dataset. We collected human-object interaction dataset through online video collection and self-recording. The dataset consists of 44 online videos and 307 real-world videos, each approximately 25 seconds long at 30 fps. It includes interactions involving eleven individuals and 286 objects, primarily common objects such as toys, cups, and boxes. For each object, we captured reference images with frontal view, 45 left rotation, and 45 right rotation. For each video, we utilize DWPose [38] for human skeletal pose sequences, HaMeR [23] for hand 3D mesh sequences, SAM-Track [7, 18] for object motion mask and ViTA [33] for the depth map. Implementation Details. Pre-trained weight of MimicMotion is used for basic pose-driven human animation. We freeze VAE, CLIP, and DINO, training all other parameters for 40K steps on four NVIDIA A100 GPUs (40GB). Due to memory limitations, our training frame number is set to 6 frames. The learning rate is 105 with 300 iteration warm-up. HOI-region reweighting loss uses η = 1. During inference, CFG is set to 4.0, and we apply progressive latent fusion from MimicMotion [41] for long video generation. Inferring 6-frame video takes 20 seconds and 5.13GB VRAM. Comparison Methods. We compare AnchorCrafter with four SOTAs: AnimateAnyone [14], MimicMotion [41], AnyV2V [19], and AnyDoor [6]. AnimateAnyone and MimicMotion are pose-guided human video generation methods. We use pre-trained weights from the open-source implementation of AnimateAnyone and provide human images with objects pasted, as these methods cannot input objects. AnyV2V is video editing method that propagates features from the first frame, for which we provide first frame and reference video. AnyDoor is an image editing model based on object mask positions; we train it with our object dataset, use MimicMotion for reference animation, and apply AnyDoor to edit each frame individually. Metrics. We evaluate across multiple dimensions, using FID [10] for image quality, FID-VID [1] and FVD [31] for video quality, and VBench [15] for subject and background consistency following HOI-Swap [37]. For object generation, we introduce Object-IoU, which uses SAMTrack to compute trajectory accuracy, and Object CLIPScore, which measures appearance consistency by comparing generated objects to reference objects. Human pose consistency is measured using Landmark Mean Distances (LMD)[16] on body and hands with OpenPose[3]. 5.2. Main Results Quantitative Results. Table 1 presents our quantitative results, demonstrating that our approach achieves superior performance in terms of video quality, object perception, and hand generation. For object motion, AnchorCrafter achieves significantly higher Object-IoU than other methods. Although AnyDoor was provided with object location masks for each frame, it struggles to maintain the shape https://github.com/MooreThreads/Moore-AnimateAnyone.git of the objects. Other models completely fail to produce reasonable object motions. Regarding object appearance preservation, AnchorCrafter attains superior Object-CLIP scores compared to other models. It is important to note that while AnimateAnyone and MimicMotion achieved not bad scores, this was due to them treating objects as part of the human clothing or background, keeping them static in the video, rather than as independent entities. Qualitative Results. Our qualitative results, illustrated in Fig. 5, showcase the performance of our method. In these figures, red boxes signify inadequate object appearance, orange boxes indicate flawed hands, and green highlight unrealistic anchors. Ours achieves superior preservation of human and object appearances, while faithfully reflecting their motions. MimicMotion generates high-quality, posedriven human images, but objects and humans exhibit coupling issues, with the model treating objects as static parts of the clothing. AnimateAnyone fails to maintain human appearance and suffers from severe frame jitters. AnyV2V is unable to propagate the first frame to subsequent frames, completely collapsing despite being provided with perfect first frame. The combination of MimicMotion and Anydoor maintains reasonable movements of objects and humans, but the appearance preservation is inferior to AnchorCrafter. Our method, leveraging the HOI-appearance perception and HOI-motion injection, preserves both the appearance and motion control of humans and objects, generating highquality human-object interaction videos. 5.3. Ablation Study Validation of HOI-Appearance Perception. We conducted detailed ablation experiments on HOI-appearance perception. First, we ablated the human-object dual adapter, as shown in Table 1. The Object-IoU metric remains superior to other models, which can be attributed to the detailed object trajectory guidance. However, the ObjectCLIP score decreased, suggesting that merely injecting the latent representation of objects is insufficient for object perception, and the coupling with human subjects leads to decline in overall video quality. Fig. 6 presents qualitative results, demonstrating that objects are significantly influenced by human subjects and backgrounds. The HOI-appearance perception effectively learns object representations and decouples human and object features. Next, we ablated the multi-view object feature fusion by removing the links that included multi-view fusion selfattention links. As shown in Fig. 6, the model exhibits poorer texture representation of objects. Validation of HOI-Motion Injection. We removed the hand 3D mesh information injection from the HOI-motion injection. As shown in Table 1, the absence of detailed Figure 5. Qualitative comparisons with other methods. Different colored squares highlight the different types of generation artifacts. In the results of MimicMotion and AnimateAnyone, the objects fail to maintain their appearance and cannot move in sync with the hands, while AnyV2V generates apparent artifacts in the edited videos. The combined results of AnyDoor and MimicMotion demonstrate lack of preservation in object details. Method AnyV2V MimicMotion+AnyDoor AnimateAnyone MimicMotion Ours w/o Human-Object Dual Adapter w/o Multi-View Object Feature Fusion w/o 3D Hand Mesh w/o HOI-Region Reweighting Loss FID FVD FID-VID Obj-IoU Obj-CLIP LMD (Hand) LMD (Body) Subj-Cons Back-Cons 234.1 167.8 172.8 138.1 141. 170.5 177.0 164.0 164.7 2873.3 1668.9 2267.0 1444.9 736.5 913.0 1371.9 920.1 807.3 53.1 37.7 24.2 22.3 15.0 24.2 52.0 21.3 69.9 0.241 0.647 0.361 0.411 0. 0.802 0.845 0.847 0.846 0.744 0.863 0.832 0.876 0.919 0.886 0.908 0.907 0.895 94.6 13.2 23.2 12.1 11.7 12.1 12.0 12.0 11.8 30.5 26.0 41.5 24.3 25. 20.9 24.6 22.3 22.0 68.2 93.4 94.9 96.3 97.4 96.4 97.2 97.3 97.0 77.9 90.7 94.1 93.9 95.3 94.9 95.3 95.0 94.8 Table 1. Quantitative results of our method compared with SOTAs and ablation studies. Our method significantly outperforms existing approaches in terms of numerical performance for spatial movement and appearance preservation of objects, while also matching or exceeding current methods in image and video quality, as well as human pose control capability. Subject consistency(Subj-Cons) and background consistency(Back-Cons) are percentages. demonstrates that without region enhancement, the model struggles to adequately learn object features. 5.4. User Study We conducted user preference evaluation with 50 participants, who rated 30 videos from each method based on five criteria: appearance preservation for humans and objects, motion accuracy for humans and objects, and overall quality. Appearance preservation assessed alignment with the reference image, while motion accuracy focused on adherence to pose conditions, particularly for hands and objects. Each criterion was rated on five-point scale, with five being the highest score. As shown in Table 2, our model consistently outperformed others across all criteria. Method Appearance Motion Human Object Human Object Overall AnyV2V [19] MimicMotion+AnyDoor [6] AnimateAnyone [14] MimicMotion [41] Ours 1.60 3.13 1.81 3.93 4.00 1.86 2.33 2.13 3.20 4.26 1.06 2.53 1.61 4.06 4. 1.40 2.06 1.72 2.80 4.60 1.06 2.13 1.95 2.80 4.47 Table 2. User study scores. The rating score is on scale from one to five, where five is the highest score, and one is the lowest. 5.5. Limitation AnchorCrafter is primarily evaluated on rigid objects and doesnt perform well on transparent or nonrigid bodies. In Fig. 7, it fails with mirror penetration. In the future, we will investigate the challenges associated with transparent and nonrigid objects. 6. Conclusion Figure 7. tions. LimiWe present AnchorCrafter, novel diffusion-based system for anchor-style product promotion video generation, incorw/o Dual Adapter w/o Multi-View Obj. Ours w/o 3D Hand Mesh w/o Re. Loss Ours Figure 6. Ablation studies. Our modules improve the preservation of the object and its interactions with the hands. Multi-view loss is HOI-region obj. reweighting loss. is multi-view object feature fusion. re. hand guidance resulted in decrease in LMD (hand). Fig. 6 presents qualitative results, indicating the presence of artifacts in hand generation. This may be attributed to the complexity of hand-object interactions and the inaccuracies in hand pose recognition provided by DWPose, especially when the hand is largely occluded by objects or positioned at the edge of the frame. Validation of HOI-Region Reweighting Loss. In conjunction with the HOI-region reweighting loss, we assigned higher weights to the hand-object regions during training Fig. 6 to enhance the learning of objects and hands. porating human-object interaction into pose-guided human video generation. By introducing HOI-appearance perception and HOI-motion injection, our system addresses key challenges in object motion guidance, appearance preservation, and complex human-object interactions. We also propose an HOI-region reweighting loss to improve object details during training. Extensive experiments show that AnchorCrafter outperforms existing methods, achieving superior object appearance preservation and shape awareness while ensuring high-quality video generation with consistent human appearance and motion."
        },
        {
            "title": "References",
            "content": "[1] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional gan with discriminative filter generation for text-to-video synthesis. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI), page 19952001. AAAI Press, 2019. 6 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 4 [3] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 72917299, 2017. 3, 6 [4] Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In Forty-first International Conference on Machine Learning (ICML), 2023. 2, 3 [5] Binghui Chen, Chongyang Zhong, Wangmeng Xiang, Yifeng Geng, and Xuansong Xie. Virtualmodel: Generating object-id-retentive human-object interaction image by diffusion model for e-commerce marketing. arXiv preprint arXiv:2405.09985, 2024. 2, 3 [6] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level In Proceedings of the IEEE/CVF image customization. Conference on Computer Vision and Pattern Recognition (CVPR), pages 65936602, 2024. 6, 8 [7] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023. 6 [8] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Imos: Intent-driven full-body motion synthesis for human-object interactions. In Computer Graphics Forum (CGF), pages 112. Wiley Online Library, 2023. Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 72977306, 2018. 3 [10] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems (NeurIPS), 30, 2017. 6 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems (NeurIPS), 33:68406851, 2020. 2 [12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems (NeurIPs), 35:86338646, 2022. 2 [14] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 81538163, 2024. 2, 3, 4, 6, 8 [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2180721818, 2024. 6 [16] Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, and Tong-Yee Lee. Make-your-anchor: diffusion-based 2d avatar generation framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 69977006, 2024. 2, 3, [17] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment In Proceedings of the IEEE/CVF International anything. Conference on Computer Vision (ICCV), pages 40154026, 2023. 6 [19] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. 6, 8 [20] Jiaman Li, Jiajun Wu, and Karen Liu. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG), 42(6):111, 2023. 3 [21] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [9] Rıza Alp Guler, Natalia Neverova, and Iasonas Kokkinos. In Densepose: Dense human pose estimation in the wild. [22] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and video depth estimation. IEEE Transactions on Multimedia (TMM), 2023. 6 [34] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision (ECCV), pages 399 417. Springer, 2025. 4 [35] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Interdiff: Generating 3d human-object interactions Gui. In Proceedings of the with physics-informed diffusion. IEEE/CVF International Conference on Computer Vision (CVPR), pages 1492814940, 2023. 3 [36] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image In Proceedings of the animation using diffusion model. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14811490, 2024. 2, [37] Zihui Xue, Mi Luo, Changan Chen, and Kristen Grauman. Hoi-swap: Swapping objects in videos with hand-object arXiv preprint arXiv:2406.07754, interaction awareness. 2024. 2, 3, 6 [38] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCVW), pages 42104220, 2023. 6 [39] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [40] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 38363847, 2023. 3 [41] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation arXiv preprint with confidence-aware pose guidance. arXiv:2406.19680, 2024. 2, 3, 4, 6, [42] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision (ECCV), 2024. 2, 3 Expressive body capture: 3d hands, Michael Black. In Proceedings of face, and body from single image. the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1097510985, 2019. 3 [23] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3d with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 98269836, 2024. 6 [24] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang. Hoi-diff: Text-driven synthesis of 3d human-object interactions using diffusion models. arXiv preprint arXiv:2312.06553, 2023. 3 [25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superviIn International conference on machine learning sion. (ICML), pages 87488763. PMLR, 2021. 4, 5 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1068410695, 2022. 2, 4 [27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [28] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [29] Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, and Rakesh Ranjan. Mvdiffusion++: dense highresolution multi-view diffusion model for single or sparseview 3d object reconstruction. In European Conference on Computer Vision (ECCV), pages 175191. Springer, 2024. 5 [30] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor: Editing video motion via content-aware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 78827891, 2024. 3 [31] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [32] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, ChungChing Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 93269336, 2024. 2, 3, 4 [33] Ke Xian, Juewen Peng, Zhiguo Cao, Jianming Zhang, and Guosheng Lin. Vita: Video transformer adaptor for robust"
        }
    ],
    "affiliations": [
        "Great Bay University",
        "Institute of Computing Technology, Chinese Academy of Sciences",
        "Meituan",
        "Tencent"
    ]
}