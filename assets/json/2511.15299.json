{
    "paper_title": "Taming Generative Synthetic Data for X-ray Prohibited Item Detection",
    "authors": [
        "Jialong Sun",
        "Hongguang Zhu",
        "Weizhe Liu",
        "Yunda Sun",
        "Renshuai Tao",
        "Yunchao Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1 Taming Generative Synthetic Data for X-ray"
        },
        {
            "title": "Prohibited Item Detection",
            "content": "Jialong Sun, Hongguang Zhu, Weizhe Liu, Yunda Sun, Renshuai Tao and Yunchao Wei 5 2 0 2 9 1 ] . [ 1 9 9 2 5 1 . 1 1 5 2 : r AbstractTraining prohibited item detection models requires large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/. Index TermsImage Generation, Synthetic Data, X-ray Security Image Synthesis, X-ray Prohibited Item Detection. I. INTRODUCTION UTOMATIC prohibited item detection [1][12] aims to detect all contraband from single X-ray security image. Training such models usually requires large amount of annotated data, but both collecting and annotating X-ray security images are time-consuming and laborious, resulting in high labor cost for obtaining well-annotated images. For example, collecting single image from X-ray security inspection equipment can take up to one minute, and multiple rounds of iterative labeling by professional annotators further increase time costs. To reduce the cost of collecting hand-annotated X-ray security images, utilizing synthetic data has emerged as an effective way. Previous X-ray image synthesis methods mainly utilize two methods to synthesize images: Threat Image Projectionbased (TIP-based) synthesis [13], [14] and Generative Adversarial Network-based (GAN-based) synthesis [15][19]. 1) TIP-based synthesis involves fusing the prohibited item with Jialong Sun, Weizhe Liu, Renshuai Tao and Yunchao Wei are with Institute of Information Science, Beijing Jiaotong University, Beijing, 100044, China (email: {sunjialong, liuweizhe, rstao, yunchao.wei}@bjtu.edu.cn). Hongguang Zhu is with Faculty of Data Science, City University of Macau, China (email: zhuhongguang1103@gmail.com). Yunda Sun is with Nuctech Company Limited, Beijing, 100083, China (email: sunyunda@nuctech.com). Fig. 1: Analysis of existing X-ray security image synthesis methods. Previous two-stage synthesis methods introduce inevitable labor cost in the first stage (e.g, foreground preparation process), which hinders the efficiency of the whole synthesis pipeline. In contrast, Xsyn is simple and effective one-stage synthesis pipeline, which can automatically refine the synthetic annotation and enhance the synthetic complexity, thereby generating high-quality synthetic data and eliminating extra labor costs. the background image through morphological operations [13] or an image fusion neural network [14]. However, it either requires laborious mask annotation for foreground extraction or time-consuming Foreground Threat Image (FTI) collection for fusion network training. 2) GAN-based synthesis enriches the foreground diversity by adopting GAN [15] to generate prohibited items with varying poses and shapes. However, training GAN on foreground images also brings inevitable extra labor cost on data collection and annotation (e.g, FTI collection [16], trimap [18], and semantic label [19]). As shown in Figure 1, we analyze existing X-ray security image synthesis methods, observing that there is one common limitation in previous methods: they all suffer from inevitable extra labor (e.g, FTI collection and annotation). We argue that this limitation stems from the fact that previous methods primarily follow two-stage synthesis pipeline, where the first stage involves extracting foregrounds for the second synthesis stage, thus introducing inevitable extra labor shown in Figure 1 (a). For instance, TIP-based methods directly extract image foregrounds, and GAN-based methods imitate these foregrounds on the basis of extraction. Therefore, the question arises: Can we achieve high-quality X-ray security image synthesis without extra labor? JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 2 In this paper, we propose simple and effective one-stage X-ray security image synthesis (Xsyn) pipeline to eliminate extra labor cost. The basic idea is illustrated in Figure 1 (b). Our method is based on the text-grounded inpainting pipeline, which requires no extra labor cost and can generate high-quality X-ray security images by bridging the generative power of the diffusion model and the perception capability of SAM [20]. Specifically, we fine-tune the layout-to-image diffusion model through text-grounded inpainting training and then inpaint X-ray security images by providing grounding conditions (e.g, bounding boxes with class names). To refine synthetic annotations of the generated X-ray security images, we propose Cross-Attention Refinement (CAR), which refines the bounding box through the cross-attention map from the diffusion model. By designing median point sampling strategy based on the most class-discriminative part of the cross-attention map, we augment the bounding box prompt and input it to SAM, thus obtaining precise position prediction. Considering the common background occlusion in realworld baggages, we further introduce Background Occlusion Modeling (BOM) to enhance synthetic complexity, which explicitly models background occlusion in the latent space of the diffusion model. We propose to automatically search the background occluder and then fuse the background occluder with the foreground parts of the latent at the end of the denoising process. With the above strategies, our synthesis method can generate high-quality X-ray security images without laborintensive cost. These synthetic images can be used to train prohibited item detection models, supplementing real images. To summarize, our contributions are threefold: We propose Xsyn, simple and effective one-stage synthesis pipeline in the X-ray security domain. To the best of our knowledge, Xsyn is the first to achieve highquality X-ray security image synthesis without incurring additional labor-intensive foreground preparation. We present two effective strategies to enhance the usability of synthetic data. The CAR strategy automatically refines the synthetic image annotations, and the BOM strategy explicitly models the background occlusion in Xray security images to enhance their imaging complexity. Experiments on public X-ray security datasets demonstrate that the generated images from our synthesis pipeline are beneficial to improve prohibited item detection performance. II. RELATED WORK X-ray Security Image Synthesis. Prohibited item detection models require large amount of data. Considering the training need, X-ray security image synthesis [13], [14], [16][19] has emerged as an effective way to deal with data insufficiency. It can mainly be categorized into two ways: TIPbased synthesis [13], [14] and GAN-based synthesis [16][19]. 1) TIP-based synthesis augments X-ray imagery datasets by superimposing prohibited items onto available X-ray security baggage images. For example, TIP [13] blends isolated threat objects onto benign X-ray images through multistage morphological operations and composition. RWSC-Fusion [14] trains an end-to-end region-wise style-controlled fusion network that superimposes prohibited items onto normal X-ray security images to synthesize realistic composite images. 2) GANbased synthesis aims to directly generate prohibited items. Yang [18] proposes to extract prohibited items with KNNmatting [21] and improve CT-GAN [22] for prohibited item generation. Li [19] presents GAN-based method for synthesizing X-ray security images with multiple prohibited items by establishing semantic label library. Zhu [16] propose an improved Self-Attention GAN (SAGAN) [23] to generate diverse X-ray images of prohibited items and integrate them with background images. However, the aforementioned methods all suffer from inevitable extra labor costs, including time-consuming FTI collection [13], [14], [16], mask [13], trimap [17], [18], and semantic label [19] annotation cost. In contrast to previous methods, our method removes extra labor costs and can generate high-quality X-ray security images through an automatic synthesis pipeline. Generative Data Synthesis for Detection. series of methods [24][28] have utilized generative models for detection data generation in the natural image domain, and can mainly be divided into two manners [24]: copy-paste synthesis [25], [27] and layout-to-image (L2I) generation [24], [28], [29]. 1) Copy-paste synthesis aims to generate separate foreground objects and fuse them with background images. Ge [25] decouples detection data generation into foreground object mask generation and background image generation through DALL-E [30]. Zhao [27] leverages CLIP [31] and Stable Diffusion [32] to obtain images with accurate categories for copy-paste synthesis. However, copy-paste synthesis requires separate foreground image generation, which can bring inevitable extra labor costs in the X-ray security domain. 2) The L2I methods, on the other hand, directly generate the whole image with objects from the layout instruction (e.g, bounding boxes with object categories), avoiding the need to generate foregrounds separately. To achieve better controllable generation, GLIGEN [29] integrates novel gated self-attention mechanism into text-to-image diffusion models for better layout control. GeoDiffusion [24] further translates geometric conditions into text prompts to generate high-quality detection data. To eliminate the extra labor cost, our method is built upon layout-to-image generation, but extends it into textgrounded inpainting to deal with the background distribution discrepancy in the X-ray security domain, and distinctively proposes two effective strategies to improve the usability of generated images. III. PRELIMINARY Latent Diffusion Model [32] is kind of diffusion model that performs the diffusion process in the latent space for textto-image generation. Specifically, given noisy latent zt RH at each timestep {0, ..., 1}, denoising UNet [33] ϵθ() is trained to recover its clean version z0 by predicting the added noise, and the training objective can be formulated as follows: LLDM = Ez0,ϵN (0,1),tϵ ϵθ(zt, t, c) (1) JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 3 Fig. 2: Qualitative comparisons between L2I generation and grounded inpainting. The background of the L2I-generated image (middle) differs lot from the real-world baggage (left), which may hinder the detection performance. Therefore, we choose grounded inpainting (right) to retain the background. where ϵ is the added random Gaussian noise and is the generalized condition. For text-to-image generation, is the text prompt which will be encoded by pre-trained CLIP [31] text encoder. For layout-to-image generation, further incorporates the grounding condition (e.g, bounding boxes with categories). Eq. 1 can be further reformulated to support inpainting tasks. Specifically, given an inpainting mask and the input image, the input image latent zinput can be extracted by pre-trained Vector Quantized Variational AutoEncoder (VQVAE) [34], and its masked version zmask is the multiplication 0 of zinput and mresize, where mresize is obtained by resizing 0 to the latent size. Based on [29], the input for UNet is expanded as zinpaint , mresize), which is = Concat(zt, zmask fed into Eq. 1 to replace zt for inpainting training. Then, at each sampling step t, the noisy latent zt is updated as follows before denoising: 0 Fig. 3: Cross-Attention Refinement. To obtain the spatialaligned annotation, we leverage SAM to locate the generated prohibited item based on the rich class-discriminative spatial localization information in the cross-attention map. Please see how the bounding box (blue box) of the generated item is refined. image annotation L, where = {(ci, bi)}N i=1, ci represents the class name, and bi represents the i-th annotation box, sharing the same format as the grounding box. Specifically, we first let Gmod = so that we can reuse the annotation and modify the geometry of the original prohibited items (e.g, shape and pose). To add new prohibited item to an image, we first use SAM to segment all elements within the image. Subsequently, we discard the two largest masks by area to prevent out-ofboundary generation. Because they typically correspond to the background and the whole baggage region. Then we select an idle region lb from the rest masks randomly, and lb satisfies the following criterion, mresize (2) lb {l dis(l, bi) < d, = 1, 2, . . . , }, (3) zt = zt+1 (1 mresize) + zinput is the noisy version of zinput . where zinput IV. METHODOLOGY A. Generation Pipeline Previous L2I methods [24], [28] in the natural image domain directly use the generated images as synthetic data. However, we find that such an approach is not feasible in the X-ray security image domain since the background of the generated image is uncontrollable and its distribution deviates significantly from that of the real background, as shown in Figure 2. To avoid the above problem, we base the generation pipeline on text-grounded inpainting. In general, given an X-ray security image RHW 3, text prompt , and grounding condition G, the textgrounded inpainting process can be formulated as function = (I, Y, G). The grounding condition = {(ei, li)}M i=1, where ei represents the textual description of the object (e.g, class name), and li = [xi,1, yi,1, xi,2, yi,2] denotes the i-th grounding box (i.e, top-left and bottom-right coordinates). The output is an image with the grounding region being repainted, as specified by the text prompt and the grounding condition G. where = {sk}K k=1, sk is the bounding box of the kth object segmented by SAM in image I, dis(, ) measures the IoU between two bounding boxes and is the pre-defined threshold. In practice, boxes that are too small will be filtered out. Then we select category cb for lb from class group which corresponds to specific region areas and let eb = cb to obtain Gadd = {(eb, lb)}. By concatenating the class names as the text prompt, we get Ymod = Concat({ci}N i=1) and Yadd = {eb}. Finally, we can generate new image in two different ways as follows, mod = (I, Ymod, Gmod), add = (I, Yadd, Gadd) (4) Therefore, we can construct two variants of synthetic data using Eq. 4, named Xsyn-M and Xsyn-A, respectively. This generation pipeline has two advantages. First, it does not require any extra labor cost (e.g, FTI collection) compared with previous synthesis methods. Second, it focuses on generating foreground items by altering only portion of the background, without affecting the overall distribution. B. Cross-Attention Refinement To generate new X-ray security image, we design two kinds of grounding conditions Gmod and Gadd based on the Because it is hard for the generated item to be tightly within the grounding box, directly using the grounding box JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 4 Fig. 4: Median Point Sampling. Considering the background in the bounding box may interfere with the refinement, we propose to enhance the localization by sampling median points as foreground points in recursive manner. Different colors refer to different division levels. as the annotation box to train detection models will lead to performance degradation of downstream tasks. Instead of forcing the generative model to generate spatially aligned items, we retain the generated item and propose CAR to refine its location to obtain the aligned annotation. Given an input X-ray security image, we first inpaint it using the proposed generation pipeline. Directly using SAM to refine the location by taking the grounding box as input is suboptimal (refer to Table VII), because the background can affect the performance of SAM. To address the above issue, we step out of the image domain and propose CAR based on the cross-attention map in the diffusion model. Figure 3 shows the process of CAR. For simplicity, we only discuss the refinement process for one generated item. For the generated item corresponding to gi = (ei, li) G, we obtain the average cross-attention map Mi RHW from the diffusion model for the text token corresponding to ei. The CAR process takes as input the generated image , the cross-attention map Mi, and the grounding location li. The output is the refined annotation location for the generated item. Specifically, we first obtain the most class-discriminative region ri by using SAM to segment Mi within li. To help SAM better locate the generated item based on li, we then propose median point sampling strategy to sample points Pi from li and combine these points with li as prompt Pi for SAM to locate the generated item, where Pi = Pi {li}. Median Point Sampling (MPS). Figure 4 depicts the basic idea of median point sampling. We aim to sample foreground points inside ri and background points outside ri. Specifically, we choose the point with the minimum activation value outside ri as the background point pb . To sample foreground points, we first sort all points within ri by their activation values and choose the median point pf1 as the first foreground and r2 point. Then we divide ri , are all below that of pf1 where the activation values in r1 , are all above that of pf1 and the activation values in r2 . into two sub-regions r1 Fig. 5: Background Occlusion Modeling. BOM performs occlusion through regional recombination in the latent space. For simplicity, we omit other variables and components of the diffusion model since the whole generation process has been elaborated. , . . . , pf2n1 By extension, we perform the same sort-and-divide operation on the subsequent sub-regions recursively and gather all the median points as foreground points. Therefore, we obtain the final point set Pi = {pf1 , pf2 , pb } which has 2n 1 foreground points and one background point in total, where indicates the division times. For example, the red, orange, and green points in Figure 4 are in the 1st, 2nd, and 3rd divisions, respectively. We argue that median points describe the central tendency of data points belonging to the prohibited item, which are less affected by extreme activation values in the cross-attention map. , . . . , pf2n1 Finally, the refinement process uses SAM to segment by taking Pi as visual prompts and assigns the bounding box of the segmented region to be the annotation box, thus obtaining more precise location prediction for the generated item. The CAR strategy takes advantage of the segmentation capability of SAM and the cross-attention map of the diffusion model to obtain the refined bounding box annotation. Despite its simplicity, our CAR strategy can achieve automatic annotation refinement that benefits prohibited item detection performance. C. Background Occlusion Modeling The generated prohibited items are too clear, which is inconsistent with complex real-world occlusion scenarios and may induce overfitting problems for detection. To address the above problem and further enrich the imaging complexity of synthetic images, we simulate the common background occlusion in real baggage by applying background occlusion modeling shown in Figure 5, which fuses the specified background region with foreground regions in the latent space to occlude prohibited items. Specifically, given an input X-ray security image I, we first select an occluder from the background in pixel space by using SAM to segment every object in I, and use Eq. 3 to determine the location of the occluder1. Next, we adopt the proposed generation pipeline to inpaint but slightly modify the latent sampling process. As shown in Figure 5, 1Eq. 3 used here is reformed as lo {l dis(l, bi) < d, dis(l, lb) < d, = 1, 2, . . . , } if we use Gadd. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 5 TABLE I: Category groups of X-ray security datasets. We split the categories into three groups according to their mean areas for each dataset. The classes in the same group share similar mean areas, and each group belongs to an area interval. Taking PIDray as an example, the area intervals of group1, group2 and group3 are [0, 10000], [10000, 25000] and [25000, max] respectively, where max is the maximum object area in PIDray. The area boundaries in the table indicate two endpoints dividing the three intervals. Dataset PIDray [35] Group1 Lighter, Bullet Group2 Group3 Area Boundaries Knife, Gun, Powerbank, Wrench, HandCuffs, Baton, Pliers, Scissors, Sprayer Hammer (10000, 25000) OPIXray [36] Multi-tool Knife, Folding Knife Straight Knife, Utility Knife Scissor (10000, 15000) HiXray [37] Portable Charger 1, Portable Charger 2, Water, Mobile Phone, Cosmetic, Nonmetallic Lighter Tablet Laptop (40000, 100000) noisy latent zT RH sampled from the standard normal distribution (0, 1) is passed to the denoising UNet, to obtain the denoised latent z0 after steps of denoising. If we directly decode z0 to the pixel space, then we will get the original result with no occlusion. To occlude the prohibited item, we perform weighted recombination of the occluder region and foreground regions in latent space for one more step as follows: zj 0 = zb 0 α + zj 0 (1 α) zj 0 = Crop(z0, j) zb 0 = Crop(z0, o) (5) (6) (7) 0 and zb where zj 0 is the j-th occluded foreground region and the occluder region of z0 respectively. α adjusts the degree of occlusion. Crop() represents the process of cropping z0 to the region corresponding to the occluded region or the o,2], and o, where occluder region can be obtained as follows: = [x o,1, o,2, o,1, {Re(lj, o) lj L, = 1, 2, . . . , + } (8) where Re() first projects lj to latent space and then perturbs it as follows: j,1 = Rand(M ax(x j,1 = Rand(M ax(y j,1 + j,2 = in(x j,1 + j,2 = in(y o, 0), j,1 j,1 o, 0), ), o, o, ) j,2), j,2), (9) where Rand() randomly samples an integer between the lower bound and the upper bound. is the width and height of j,2] be j,2, the j-th occluded region. The hidden version of z0 is termed 0 . Finally, we decode zh as zh 0 to pixel space and obtain the hidden result shown in Figure 5. respectively. We let and = [x j,1, j,1, Through the regional recombination enabled by BOM, the foreground region can be occluded by random item from the background, which enhances the imaging complexity of synthetic images. It is worth noting that the original result in Figure 5 is used by CAR to obtain the refined annotation, and we adopt the hidden result as the final synthetic image. V. EXPERIMENTS A. Experimental Setups Datasets. We conduct experiments on three widely used X-ray security datasets: PIDray [35], OPIXray [36], and HiXray [37]. Specifically, PIDray dataset consists of 29,454 training images and 18,220 validation images with bounding box and mask annotations from 12 classes, while OPIXray dataset has 7,109 training images and 1,776 validation images with bounding box annotations from 5 classes. HiXray dataset is composed of 36,295 training images and 9,069 validation images with bounding box annotations from 8 classes. Implementation Details. Generation. We base the generation pipeline on GLIGEN [29]. Specifically, we finetune GLIGEN for 180K steps for text grounded generation training and 50K steps for inpainting training with the batch size set to 8. During inference, we sample images using the DDIM [38] scheduler for 50 steps with the classifier-free guidance (CFG) set as 7.5. Synthetic images for training. Taking data annotations of the training set as input, we generate synthetic images using the proposed generation pipeline and apply CAR and BOM to these images. Specifically, we construct two variants of synthetic images, named Xsyn-M and Xsyn-A, respectively. For both Xsyn-M and Xsyn-A generations, we filter out the bounding boxes smaller than threshold ratio of the image area, and the threshold ratio is 0.1%, 0.4%, and 0.5% for PIDray, OPIXray, and HiXray, respectively. For XsynA generation, we generate the prohibited item in random idle region from the background. The class of the generated item is determined by the area of the idle region to avoid the mismatch between the object size and the actual area. To deal with the above issue, we split all classes into three groups corresponding to an area interval by calculating the average area for each class, where the average area for each class is defined as the average area of all objects with the same class (see Table for the details of class groups of each dataset). We use IoU to measure the distance between two bounding boxes, and the threshold is set to 0.2 in Eq. 3. We adopt the ViT-H SAM [20] model throughout our experiments. To prevent the disparities in generated data from affecting the models generalization on real data, we combine the generated images with real images as the final training set, as adopted in DetDiffusion [28]. The spatial resolution of synthetic images is 512512. Detection. We use MMDetection [39] to train downstream detectors. DINO [40] detector with ResNetJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 6 TABLE II: Comparisons on PIDray dataset. We compare our approach with previous synthesis methods using DINO with ResNet-50 backbone on the PIDray dataset. Easy, Hard, and Hidden refer to different levels of detection difficulty. BA, PL, HM, PO, SC, WR, GU, BU, SP, HA, KN and LI suggest Baton, Pliers, Hammer, Powerbank, Scissors, Wrench, Gun, Bullet, Sprayer, HandCuffs, Knife and Lighter, respectively. *: represents the original L2I generation setting."
        },
        {
            "title": "Method",
            "content": "mAP AP50 Easy Hard Hidden BA Average Precision PL"
        },
        {
            "title": "SC WR GU",
            "content": "BU SP HA KN LI"
        },
        {
            "title": "Real only",
            "content": "68.4 81.7 74.0 69.7 52.1 76. 86.1 83.9 74.8 72.1 90.6 29. 62.2 56.2 89.6 38.7 61.0 TIP [13] CT-GAN [22] SAGAN [23] GeoDiffusion [24] GLIGEN* [29] Xsyn-M Xsyn-A 69.0 69.4 69.5 64.6 64.9 69.1 70.7 82.0 82.4 82.2 78.4 78.6 82.1 83.8 74.9 75.3 75. 71.6 73.1 75.5 76.8 70.9 71.0 70.9 64.6 65.2 70.8 71.7 51.1 52.1 53.5 47.6 45.3 50.7 54.1 75.9 75.9 76. 72.6 72.0 73.4 76.7 86.4 86.4 88.1 82.2 83.2 86.5 85.6 84.0 83.7 85.0 78.8 76.6 84.2 85.1 74.7 74.0 75. 73.6 71.4 75.8 76.1 74.5 73.2 74.5 69.8 69.4 72.9 74.8 91.4 91.8 91.7 88.1 88.0 91.0 91.7 27.4 35.4 29. 25.1 28.0 35.5 36.8 63.2 62.2 62.5 57.5 57.6 63.6 64.1 59.2 59.3 61.7 56.2 55.6 60.2 63.5 89.5 90.2 89. 86.7 88.4 89.8 89.2 43.1 39.5 40.7 28.0 32.5 36.1 44.5 58.8 60.8 59.5 56.6 56.1 60.0 60.1 TABLE III: Performance on OPIXray and HiXray. Our method is effective for various X-ray security datasets. Dataset Setting mAP AP50 AP75 OPIXray [36] HiXray [37] Real only +Xsyn-A Real only +Xsyn-A 39.5 40.1 49.3 50.4 90.2 90. 83.4 83.9 26.0 26.1 53.2 55.5 methods. We train all detectors for 6 epochs and reduce the learning rate by factor of 10 in the last epoch, following their default model and training parameters in MMDetection [39]. Comparison experiments. We choose TIP [13], CT-GAN [22], SAGAN [23], GLIGEN [29] and GeoDiffusion [24] for comparison experiments following their default parameter settings. Specifically, for X-ray image synthesis methods, we extract foreground threat images using the mask annotation of PIDray dataset to implement TIP, CT-GAN, and SAGAN. We follow Yang [18] and Zhu [16] to train CT-GAN and SAGAN on each class for 200K steps, with image size set to 128 and batch size set to 8. For L2I methods, we train GLIGEN for pure layout generation for 180K steps and train GeoDiffusion for 230K steps for fair comparison. The batch size is set to 8 for both methods. Comparisons with previous methods. Table II shows the results of object detection on PIDray dataset. Our Xsyn-M achieves superior performance compared with methods in the natural image domain, revealing the advantages of the proposed synthesis pipeline. Besides, Xsyn-M can achieve competitive performance, i.e, 69.1% v.s. 69.5% for mAP compared with SAGAN [23], and Xsyn-A can further surpass it by 1.2% mAP. It is worth noting that our synthetic data does not require additional labor compared with previous methods, while data produced by TIP [13], CT-GAN [22], and SAGAN [23] rely on laborious pixel-wise foreground extraction. Both Xsyn-M and Xsyn-A show consistent improvement for almost all classes, especially for some difficult classes (e.g, +7.2% for Gun with Xsyn-A). Potential of synthetic data. As shown in Figure 6, we plot Fig. 6: Potential of synthetic data. Our synthetic data achieves the best detection performance throughout the whole training period. backbone is used to evaluate the dataset following the default DINO configuration of MMDetection. For all detectors, we uniformly train them for 6 epochs. 4 NVIDIA RTX 3090 GPUs are used for all experiments. Evaluation Metrics. Mean average precision (mAP), as the common metric in object detection tasks [41], is used to evaluate the performance. We also evaluate AP for each category and for different occlusion levels on PIDray. B. Main Results In this section, we evaluate the performance of the proposed synthesis method for object detection training by supplementing real images with synthetic images generated by our method. To this end, we first compare our approach with previous methods on the PIDray dataset, and then investigate the potential of synthetic data by varying the amount of real images. Finally, we test the effectiveness of our method across various X-ray security datasets and detectors. Setup. For comparison experiments, we compare Xsyn-M with synthesis methods of the natural image domain and XsynA with previous labor-intensive X-ray security image synthesis JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 7 TABLE IV: Performance on various detectors. Our method can improve prohibited item detection performance consistently, regardless of detectors and backbone architectures. TABLE VI: Ablation studies on proposed strategies. We first add CAR and then BOM to investigate their performance separately. Best results are achieved when both strategies are adopted. Type Stage Method Backbone mAP AP50 AP75 CNN-based one two Transformer-based ATSS [42] +Xsyn-A C-RNN [43] +Xsyn-A C-RNN [43] +Xsyn-A DINO +Xsyn-A DINO +Xsyn-A R101 R101 R101 X101 X101 R50 R50 Swin Swin 65.2 65.5 68.0 69.1 69.6 70.2 68.4 70.7 76.1 78.1 80.8 81. 82.6 83.4 83.7 84.3 81.7 83.8 88.6 89.9 72.6 73.0 75.5 76.4 77.0 77.4 73.5 76.7 81.8 83.5 Method mAP AP50 AP75 Real only +Xsyn-A (w/o CAR) +Xsyn-A (w/o BOM) +Xsyn-A 68.4 69.6 70.3 70.7 81.7 82.3 83.1 83. 73.9 75.5 76.0 76.7 TABLE VII: Ablations on hyper-parameters of proposed strategies. We ablate the choice of division times for CAR and latent occlusion coefficient α for BOM respectively on Xsyn-A. TABLE V: Comparison results on OPIXray and HiXray. Type Setting mAP AP50 AP75 Method OPIXray HiXray mAP AP50 AP75 mAP AP50 AP GLIGEN 36.7 40.1 Ours 88.6 90.1 19.1 26.1 49.1 50.4 82.0 83.9 53.2 55. the validation mAP curve on PIDray, and the synthetic data generated by our method has better training efficiency compared with previous methods. It indicates that our synthetic data has learned the distribution of X-ray prohibited items and can lead faster training convergence. Performance on more datasets and detectors. We extend the evaluation of our method on the OPIXray and HiXray datasets, respectively. The results in Table III demonstrate that our method improves detection performance across various datasets. We further conduct experiments on various detectors, including CNN-based and Transformer-based [44] architectures, to evaluate the generalization ability. As shown in Table IV, our synthetic images achieve consistent improvement regardless of the detection models. Comparison on other datasets. Since OPIXray and HiXray datasets lack mask annotations and FTI images, we cannot implement two-stage methods on them. Table shows the comparison between our method and GLIGEN, and the result demonstrates the superiority of our method. C. Ablation Study In this section, we conduct ablation studies on the proposed strategies and their specific design choices, respectively. We first ablate the parameter setting of CAR, and then ablate BOM on the basis of CAR. All ablation studies are conducted on the best-performing Xsyn-A, and PIDray dataset is used for all experiments. The proposed strategies. Table VI presents the impact of the proposed strategies. We analyze the effect of each proposed strategy by sequentially adding 1) CAR and 2) BOM. The results demonstrate the relative importance of each strategy, with all strategies performing the best. Hyper-parameters of proposed strategies. Median point sampling. Table VII (upper) shows the performance of CAR using different division times n, where = 0 means that CAR-n BOM-α 0 1 2 3 4 0.1 0.3 0.5 0. 69.7 69.9 70.1 70.2 70.3 70.3 70.7 70.2 69.8 82.5 82.7 83.0 82.8 83.1 83.1 83.8 82.8 82.5 75.6 75.9 75.9 76.0 76.0 76.3 76.7 76.0 75. we only use the grounding box to implement refinement. The gain reaches its biggest when = 4, indicating the benefit of incorporating median points and suggesting that MPS has good scalability for annotation refinement. We set to 4 for other experiments. Latent occlusion coefficient. Table VII (bottom) provides the ablation study for occlusion coefficient α. The performance increases when α changes from 0.1 to 0.3, while it decreases from 0.3 to 0.7. The result suggests that medium occlusion coefficient is beneficial to enhance the imaging complexity, while too small or too large occlusion coefficient cannot model the complex occlusion in real-world baggage. Therefore, the optimum α is set to 0.3 for better imaging complexity enhancement. Point sampling strategies. We study the choice of point sampling strategies in CAR by designing top-k point sampling strategy for comparison. Specifically, the top-k strategy samples k+1 points in total, which consists of foreground points with top-k activation values and one background point with the minimum activation value within the cross-attention map. We set to 1 and 15 to compare with MPS (n = 1) and MPS (n = 4), respectively. The performance comparison between our median point sampling strategy and the top-k point sampling strategy is presented in Table VIII. The result demonstrates the superiority of our strategy, indicating that the MPS strategy has better scalability for annotation refinement. Occlusion space and period. The ablation study for occlusion space and period is shown in Table IX. We fuse the occluder region with foreground regions in the original image to implement occlusion modeling in pixel space. The result shows that modeling occlusion in latent space achieves better performance than in pixel space. We also investigate the influence of the occlusion period by modeling occlusion JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 8 Fig. 7: Qualitative results on PIDray dataset. Our method can synthesize well-annotated and realistic X-ray security images. The blue boxes in the 3rd column and the last two columns refer to the input grounding boxes and the refined annotation boxes, respectively. Please zoom in for better visualization. TABLE VIII: Study on point sampling strategies in CAR. Point Sampling mAP AP50 AP75 Top-1 Top-15 MPS (n = 1) MPS (n = 4) 69.8 69.8 69.9 70.3 82.7 82.9 82.7 83.1 75.8 75.7 75.9 76. TABLE IX: BOM ablations on occlusion space and period. Period mAP AP50 AP75 Latent Space Pixel Space - 69.9 70.7 69.9 82.9 83. 82.7 75.5 76.7 75.8 at each denoising step t, but the performance is much lower than the original version. We argue that such an approach may destroy the distribution of foregrounds in the cross-attention map, thus affecting the process of CAR. D. Qualitative Results We provide qualitative results on the PIDray dataset shown in Figure 7. The original results in the 3rd column have obvious spatial misalignment between the generated prohibited item and the bounding box. When we apply CAR to the original results, the bounding box is refined to enclose the prohibited item tightly, as shown in the 4th column. We further enhance the imaging complexity in the 5th column by using BOM to occlude the prohibited item. It is worth mentioning that we perform CAR on the original image and apply BOM to obtain the hidden image, which ensures that the annotation refinement will not be compromised by the introduction of occlusion. Figure 8 presents more visualization of X-ray security images synthesized by our approach. The synthetic images from Xsyn-A can effectively simulate the scenario of concealing contraband in real-world baggage, which is especially well illustrated by the second image in the last column. Figure 9 further shows qualitative results for OPIXray and HiXray datasets, demonstrating that our method can synthesize realistic objects across different datasets. VI. DISCUSSIONS Our method can be adapted to other domains (e.g, remote sensing) where data collection and annotation are hard. With the great development of security inspection equipment, Computed Tomography (CT) has emerged as more advanced imaging modality for contraband detection. However, compared with 2D X-ray security images, collecting and annotating CT images is more challenging due to their 3D volumetric characteristics. Therefore, it is promising direction to explore CT image synthesis using generative models and exploit the synergy between X-ray and CT imaging modalities. We hope that this paper encourages further research on security image (e.g, X-ray and CT) generation to benefit automatic prohibited item detection. VII. CONCLUSION In this paper, we propose Xsyn, simple and effective one-stage X-ray security image synthesis pipeline to generate high-quality prohibited item detection data. In contrast to the previous two-stage methods, for the first time, our method removes the labor-intensive foreground extraction procedure. To improve the usability of generative synthetic data, our method incorporates two effective strategies to automatically refine the synthetic annotation and enhance the synthetic complexity. The synthetic images generated by our method can improve the prohibited item detection performance across various public datasets and detectors. We hope Xsyn can bring new inspiration for exploiting the potential of generative synthetic data in the X-ray security domain. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 9 Fig. 8: More qualitative results on PIDray dataset. The solid-line and dashed-line boxes in the 2nd column are grounding boxes for Xsyn-M and Xsyn-A generation, respectively. From top to bottom, the text prompt for Xsyn-M in each row is Scissors, Knife, and Knife, and the text prompt for Xsyn-A in each row is Lighter, Knife, and HandCuffs, respectively. Fig. 9: More qualitative results on OPIXray and HiXray dataset. The blue box refers to the original object, and the red box refers to the generated object using our method. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER"
        },
        {
            "title": "REFERENCES",
            "content": "[1] C. Wang, Y. Yan, J.-H. Xue, and H. Wang, 2 ol-net: Intra-inter objectness learning network for point-supervised x-ray prohibited item detection, IEEE Transactions on Information Forensics and Security, 2025. [2] F. Yang, R. Jiang, Y. Yan, J.-H. Xue, B. Wang, and H. Wang, Dualmode learning for multi-dataset x-ray security image detection, IEEE Transactions on Information Forensics and Security, vol. 19, pp. 3510 3524, 2024. [3] R. Chen, Y. Yan, J.-H. Xue, Y. Lu, and H. Wang, Augmentation matters: mix-paste method for x-ray prohibited item detection under noisy annotations, IEEE Transactions on Information Forensics and Security, 2024. [4] B. Ma, T. Jia, M. Li, S. Wu, H. Wang, and D. Chen, Toward dualview x-ray baggage inspection: large-scale benchmark and adaptive hierarchical cross refinement for prohibited item discovery, IEEE Transactions on Information Forensics and Security, vol. 19, pp. 38663878, 2024. [5] C. Zhao, L. Zhu, S. Dou, W. Deng, and L. Wang, Detecting overlapped objects in x-ray security imagery by label-aware mechanism, IEEE transactions on information forensics and security, vol. 17, pp. 998 1009, 2022. [6] B. Wang, F. Zhang, X. Fang, R. Ji, R. Tao, Y. Cao, B. Liu, and J. Liu, Exploring x-ray prohibited item detection from long-tailed learning perspective, IEEE Transactions on Information Forensics and Security, 2025. [7] B. Isaac-Medina, S. Yucer, N. Bhowmik, and T. Breckon, Seeing evaluation of prohibited item through the data: statistical detection benchmark datasets in Proc. Conf. Computer Vision and Pattern Recognition Workshops. IEEE/CVF, [Online]. Available: https: //breckon.org/toby/publications/papers/isaac23evaluation.pdf for x-ray security screening, June 2023, pp. 524533. [8] T. Webb, N. Bhowmik, Y. Gaus, and T. Breckon, Operationalizing convolutional neural network architectures for prohibited object detection in x-ray imagery, in Proc. Int. Conf. on Machine Learning Applications. IEEE, December 2021, pp. 610615. [Online]. Available: https://breckon.org/toby/publications/papers/web21xray.pdf [9] N. Bhowmik, Y. Gaus, and T. Breckon, On the impact of using x-ray energy response imagery for object detection via convolutional neural networks, in Proc. Int. Conf. on Image Processing. IEEE, September 2021, pp. 12241228. [Online]. Available: https: //breckon.org/toby/publications/papers/bhowmik21energy.pdf [10] B. Isaac-Medina, C. Willcocks, object detection using epipolar ray security imagery, in Proc. IEEE, October 2020, pp. 98899896. //breckon.org/toby/publications/papers/isaac20multiview.pdf and T. Breckon, Multi-view constraints within cluttered xInt. Conf. Pattern Recognition. [Online]. Available: https: [11] Y. Gaus, B. Isaac-Medina, N. Bhowmik, Y. Lam, and T. Breckon, Semi-supervised object-wise anomaly detection for firearm and firearm component detection in x-ray security imagery, in Proc. Computer Vision Pattern Recognition Workshops. June 2025, [Online]. Available: https://breckon.org/toby/publications/ to appear. papers/gaus25anomaly.pdf IEEE/CVF, [12] N. Bhowmik and T. Breckon, Joint sub-component level segmentation and classification for anomaly detection within dual-energy x-ray security imagery, in Proc. Int. Conf. on Machine Learning Applications. IEEE, December 2022, pp. 14631467. [Online]. Available: https: //breckon.org/toby/publications/papers/bhowmik22subcomponent.pdf [13] N. Bhowmik, Q. Wang, Y. F. A. Gaus, M. Szarek, and T. P. Breckon, The good, the bad and the ugly: Evaluating convolutional neural networks for prohibited item detection using real and synthetically composited x-ray imagery, arXiv preprint arXiv:1909.11508, 2019. [14] L. Duan, M. Wu, L. Mao, J. Yin, J. Xiong, and X. Li, Rwsc-fusion: Region-wise style-controlled fusion network for the prohibited x-ray security image synthesis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22 39822 407. [15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative adversarial nets, Advances in neural information processing systems, vol. 27, 2014. [16] Y. Zhu, Y. Zhang, H. Zhang, J. Yang, and Z. Zhao, Data augmentation of x-ray images in baggage inspection based on generative adversarial networks, IEEE Access, vol. 8, pp. 86 53686 544, 2020. [17] Z. Zhao, H. Zhang, and J. Yang, gan-based image generation method for x-ray security prohibited items, in Pattern Recognition and Computer Vision: First Chinese Conference, PRCV 2018, Guangzhou, China, November 23-26, 2018, Proceedings, Part 1. Springer, 2018, pp. 420430. [18] J. Yang, Z. Zhao, H. Zhang, and Y. Shi, Data augmentation for x-ray prohibited item images using generative adversarial networks, IEEE Access, vol. 7, pp. 28 89428 902, 2019. [19] D.-s. Li, X.-b. Hu, H.-g. Zhang, and J.-f. Yang, gan based method for multiple prohibited items synthesis of x-ray security image, Optoelectronics Letters, vol. 17, no. 2, pp. 112117, 2021. [20] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., Segment anything, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 40154026. [21] Q. Chen, D. Li, and C.-K. Tang, Knn matting, IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 9, pp. 21752188, 2013. [22] X. Wei, B. Gong, Z. Liu, W. Lu, and L. Wang, Improving the improved training of wasserstein gans: consistency term and its dual effect, arXiv preprint arXiv:1803.01541, 2018. [23] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, Self-attention generative adversarial networks, in International conference on machine learning. PMLR, 2019, pp. 73547363. [24] K. Chen, E. Xie, Z. Chen, Y. Wang, L. Hong, Z. Li, and D.-Y. Yeung, Geodiffusion: Text-prompted geometric control for object detection data generation, arXiv preprint arXiv:2306.04607, 2023. [25] Y. Ge, J. Xu, B. N. Zhao, N. Joshi, L. Itti, and V. Vineet, Dall-e for detection: Language-driven compositional image synthesis for object detection, arXiv preprint arXiv:2206.09592, 2022. [26] H. Fang, B. Han, S. Zhang, S. Zhou, C. Hu, and W.-M. Ye, Data augmentation for object detection via controllable diffusion models, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 12571266. [27] H. Zhao, D. Sheng, J. Bao, D. Chen, D. Chen, F. Wen, L. Yuan, C. Liu, W. Zhou, Q. Chu et al., X-paste: Revisiting scalable copy-paste for instance segmentation using clip and stablediffusion, in International Conference on Machine Learning. PMLR, 2023, pp. 42 09842 109. [28] Y. Wang, R. Gao, K. Chen, K. Zhou, Y. Cai, L. Hong, Z. Li, L. Jiang, D.-Y. Yeung, Q. Xu et al., Detdiffusion: Synergizing generative and perceptive models for enhanced data generation and perception, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 72467255. [29] Y. Li, H. Liu, Q. Wu, F. Mu, J. Yang, J. Gao, C. Li, and Y. J. Lee, Gligen: Open-set grounded text-to-image generation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22 51122 521. [30] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, Zero-shot text-to-image generation, in International conference on machine learning. Pmlr, 2021, pp. 88218831. [31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable language supervision, in International visual models from natural conference on machine learning. PMLR, 2021, pp. 87488763. [32] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, Highresolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 68410 695. [33] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in International Conference on Medical image computing and computer-assisted intervention. Springer, 2015, pp. 234241. [34] A. Van Den Oord, O. Vinyals et al., Neural discrete representation learning, Advances in neural information processing systems, vol. 30, 2017. [35] L. Zhang, L. Jiang, R. Ji, and H. Fan, Pidray: large-scale xray benchmark for real-world prohibited item detection, International Journal of Computer Vision, vol. 131, no. 12, pp. 31703192, 2023. [36] Y. Wei, R. Tao, Z. Wu, Y. Ma, L. Zhang, and X. Liu, Occluded prohibited items detection: An x-ray security inspection benchmark and de-occlusion attention module, in Proceedings of the 28th ACM international conference on multimedia, 2020, pp. 138146. [37] R. Tao, Y. Wei, X. Jiang, H. Li, H. Qin, J. Wang, Y. Ma, L. Zhang, and X. Liu, Towards real-world x-ray security inspection: high-quality benchmark and lateral inhibition module for prohibited items detection, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 92310 932. [38] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, in International Conference on Learning Representations, 2021. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 11 [39] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Xu, Z. Zhang, D. Cheng, C. Zhu, T. Cheng, Q. Zhao, B. Li, X. Lu, R. Zhu, Y. Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. C. Loy, and D. Lin, MMDetection: Open mmlab detection toolbox and benchmark, arXiv preprint arXiv:1906.07155, 2019. [40] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum, Dino: Detr with improved denoising anchor boxes for end-toend object detection, arXiv preprint arXiv:2203.03605, 2022. [41] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, Microsoft coco: Common objects in context, in Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13. Springer, 2014, pp. 740755. [42] S. Zhang, C. Chi, Y. Yao, Z. Lei, and S. Z. Li, Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 97599768. [43] Z. Cai and N. Vasconcelos, Cascade r-cnn: Delving into high quality object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 61546162. [44] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017."
        }
    ],
    "affiliations": [
        "Faculty of Data Science, City University of Macau",
        "Institute of Information Science, Beijing Jiaotong University",
        "Nuctech Company Limited"
    ]
}