{
    "paper_title": "Activation-Informed Merging of Large Language Models",
    "authors": [
        "Amin Heyrani Nobari",
        "Kaveh Alimohammadi",
        "Ali ArjomandBigdeli",
        "Akash Srivastava",
        "Faez Ahmed",
        "Navid Azizan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\\% increase in benchmark performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 1 2 4 2 0 . 2 0 5 2 : r ACTIVATION-INFORMED MERGING OF LARGE LANGUAGE MODELS Amin Heyrani Nobari Massachusetts Institute of Technology ahnobari@mit.edu Kaveh Alim Massachusetts Institute of Technology mrz@mit.edu Ali ArjomandBigdeli Stony Brook University aarjomandbig@cs.stonybrook.edu Akash Srivastava RedHat AI Innovation & MIT-IBM Watson AI Lab akash@redhat.com Faez Ahmed Massachusetts Institute of Technology faez@mit.edu Navid Azizan Massachusetts Institute of Technology azizan@mit.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Model merging, method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40% increase in benchmark performance."
        },
        {
            "title": "Introduction",
            "content": "Foundation models are rapidly becoming the dominant force for building Artificial Intelligence (AI) systems. In many cases, researchers build their machine learning models by starting from pre-trained foundation models and fine-tuning (FT) these pre-trained models for some desired target task Pareja et al. (2024). In such paradigm, numerous fine-tuned models are developed for various tasks. However, an important opportunity is missed, as these fine-tuned task-specialized models typically operate in isolation without leveraging the rich features that each possesses Sudalairaj et al. (2024). This fact highlights the importance of growing area of research focused on combining multiple task-specialized models fine-tuned from the same base foundation model. In particular, as large language models (LLMs) continue to evolve, it becomes increasingly important to develop methods that can effectively fuse the specialized knowledge of various fine-tuned models derived from the same foundation model. Model merging has shown broad applications, including enhancing accuracy and robustness Wortsman et al. (2022), improving generalization Rame et al. (2023), multi-modal models Sung et al. (2023), and model alignment to human feedback Rame et al. (2024); Rame et al. (2024). Given these benefits, substantial amount of attention has been devoted to developing more effective merging algorithms for LLMs. In the vast majority of cases, merging LLMs is done using algorithms that explore the weight space of models and do not leverage the information in the activation space. Activation space information has been widely used to develop model pruning and compression methods, both in the context of general deep learning methods Frantar & Alistarh (2022), and more specifically for large language models Lin et al. (2024). However, this direction has remained under-explored Figure 1: Overview of the proposed activation-informed merging (AIM) in LLMs. for developing more robust merging algorithms. We hypothesize that the activation space information may hold key insights useful for model merging and explore this in our work. In this paper, we view the problem of merging from continual learning perspective. Specifically, we explore how FT models and merging them can significantly deviate from the pre-trained base model, potentially leading to overall performance degradation. This is analogous to the common catastrophic forgetting problem in continual learning Farajtabar et al. (2020); Wang et al. (2024b); Sudalairaj et al. (2024). Given this perspective, we incorporate the activation space information, taking into account the importance of the base model in preserving its pre-trained capabilities while integrating new knowledge from fine-tuned models. To achieve this, we introduce new method named Activation-Informed Merging (AIM), which modifies the update step in the merging process to ensure that the most influential weights of the base model, identified through its activations, undergo minimal changes. Figure 1 demonstrates this basic process of AIM. AIM fundamentally relates to the widely used approach of weight regularization in continual learning Kirkpatrick et al. (2017); Ritter et al. (2018); Aljundi et al. (2018). When merging large language models fine-tuned from the same base model, the goal is to maintain the base models performance while improving the merged models expertise using the fine-tuned models. Various methods have been proposed to merge fine-tuned LLMs Yu et al. (2024b,a); Ilharco et al. (2023); Wortsman et al. (2022); Davari & Belilovsky (2024); Yadav et al. (2023); Matena & Raffel (2022); Jin et al. (2023). Although useful, these methods are fragile to outlier and low-quality fine-tuned models and may perform worse than the base model. Hence, we take new perspective toward merging and adopt the continual learning view to prevent catastrophic forgetting. Our extensive experimental study shows that AIM is complementary solution that can be applied in conjunction with any prior merging methods and consistently improves their performances across all tested benchmarksincluding math, code, and instruction following by up to 40%. Despite its simplicity, AIM shows the effectiveness and importance of the activation space information for more effective model merging."
        },
        {
            "title": "2 Background & Related Work",
            "content": "LLM Merging. Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Often, fine-tuned models are readily available, but their training data is not due to data privacy or intellectual property concerns Dodge et al. (2020). With many fine-tuned checkpoints from the same pre-trained model, LLM merging has emerged as complementary approach to fine-tuning, combining multiple task-specialized models into unified architecture. This technique offers several advantages: it reduces storage and inference costs by consolidating expertise into single model, facilitates compositional generalization by integrating specialized capabilities from different fine-tuned models, and supports decentralized model development by allowing independently trained models to be merged efficiently Yadav et al. (2024). Model soup, proposed by Wortsman et al. (2022), demonstrates even simple averaging of the weights of multiple finetuned models can enhance accuracy and robustness in image classification and natural language processing applications. Further, Rame et al. (2023) show that model merging can improve out-of-distribution generalization performance. Extending this approach beyond unimodal settings, Sung et al. (2023) empirically demonstrate that model merging 2 is also effective in multimodal setups. Beyond generalization benefits, model merging has also been explored for alignment; WARP Rame et al. (2024) and WARM Rame et al. (2024) introduce weight merging strategies to improve alignment in reinforcement learning from human feedback. WARP demonstrates that merging rewarded policies enhances model quality and alignment, while WARM shows that weight-averaged reward models improve robustness and help mitigate reward hacking. Recently, many methods have been proposed to go beyond simple weight averaging to merge fine-tuned LLMs into multitask model by combining their capabilities. Spherical Linear intERPolation (SLERP) Shoemake (1985), originally proposed for animating rotations, which interpolates between two checkpoints, can be seen as modification to simple weight averaging that finds spherical path instead of linear path in models parameter space. main shortcoming of this approach is that it does not support merging more than two models. Jang et al. (2025) also leverages geometric insights, showing that merging only two fine-tuned models can provide superior in-distribution and out-of-distribution performance compared to ensembling multiple models. Task Arithmetic Ilharco et al. (2023) generalizes simple weight averaging by introducing task vectors. It suggests moving the base model parameters in the direction of weighted average of fine-tuned model differences with respect to the base model. Followed by the introduction of task vectors, Model Breadcrumbs Davari & Belilovsky (2024), Trim, Elect Sign & Merge (TIES merging) Yadav et al. (2023), and Drop And REscale (DARE) Yu et al. (2024a) leverage pruning techniques for better and more scalable ways of merging task vectors. WeIght DisENtanglement based merging (WIDEN) Yu et al. (2024b) takes more sophisticated approach to model merging by disentangling and analyzing weight components. Another line of work on model merging takes advantage of the information in the model activations of the training data. Matena & Raffel (2022) propose Fisher merging, which leverages the Laplace approximation by using the diagonal of each models Fisher information. Jin et al. (2023) attempt to minimize prediction differences between the merged model and the individual models and introduce the Regression Mean (RegMean) method that calculates the optimal weights with respect to Euclidean distance to model predictions. We encourage the reader to refer to Yang et al. (2024) for more comprehensive literature overview. The study provides detailed discussion of model merging methods and theories, explores their applications in LLMs and multimodal large language models, and highlights future research directions. Continual Learning. Continual learning strategies can be categorized into five overarching approaches: regularization, where parameters are constrained or updated based on past data; replay, where past data is replayed for the model as it encounters new data; optimization, where the loss function and optimizer are targeted; representation, where new data representations and learned embeddings can be exploited for less forgetting; and architecture, where models and parameters can expand as new data arrives Wang et al. (2024a). To avoid catastrophic forgetting of the base models abilities, we view the model merging problem through the lens of CL, primarily focusing on regularization-based methods. Regularization-based methods penalize deviation from the base model according to some norm Shi et al. (2024). Various methods have been proposed to mitigate catastrophic forgetting such as Aljundi et al. (2018); Kirkpatrick et al. (2017); Ritter et al. (2018). In particular, Elastic Weight Consolidation (EWC) employs Fisher Information Matrix to identify and protect parameters crucial for previous tasks by adding quadratic penalty on the parameter shifts. Integrating CL approaches into the merging framework involves defining weighted regularization term that selectively constrains parameter updates in critical areas for retaining previously learned tasks. This integration not only mitigates the risk of catastrophic forgetting but also enhances the adaptability and utility of the merged model. Model Compression. Using activation space information has been shown to be useful in the context of model compression. Frantar & Alistarh (2022) show that using calibration dataset, deep learning models can be quantized and/or pruned efficiently. Lin et al. (2024) introduce Activation-aware Weight Quantization (AWQ) for LLM compression and show that protecting only 1% salient weights can greatly reduce quantization error. Building on these ideas, this paper presents complementary merging approach that utilizes base model activations and principles from AWQ. Our method efficiently sketches delta parameters, ensuring the base model retains its original capabilities while incorporating expertise from fine-tuned models."
        },
        {
            "title": "3 Methodology: Activation-Informed Merging",
            "content": "As discussed in Section 2, most existing approaches for merging FT LLMs primarily focus on the weight space of the models being merged. However, it is well established that the activation space of these models contains crucial insights into the degree of importance of different parameters of LLMs. This was shown to be the case, for instance, in the work done by Lin et al. (2024) on Activation-aware Weight Quantization (AWQ), outperforming traditional quantization 3 methods by including insight from the activation space of LLMs. Given this, we hypothesize that the activation space of LLMs likely holds useful clues for model merging as well. Inspired by AWQ, we introduce Activation-Informed Merging (AIM) for merging FT LLMs. In this section, we detail our proposed solution and discuss some of the inner workings of AIM. 3.1 The Merging Problem and Connections to Continual Learning Consider the merging of models with parameters θ1, θ2, , θN fine-tuned on different tasks from common pre-trained model with parameters θpre. For each fine-tuned LLM, we are essentially creating experts on specific tasks that move away from the generalist pre-trained model, hence usually degrading performance in some tasks while improving performance on the task for which the model is fine-tuned. In this sense, each FT model with parameters θn can be seen as model fitted to new task Dn = {Xn, Yn} in continual learning scenario with the potential for catastrophic forgetting on the generalist pre-trained model, which may not perform as well on the specific task but will have more balanced performance across various tasks. As such, we hypothesize that when merging FT LLMs adapted from the same base model, emphasis on the base model can build better robustness to large performance degradation across numerous tasks while still allowing capturing each FT experts capabilities. AIM seeks to achieve this by relaxing the changes to the salient weights of the base model in the final merged model. In this way, AIM is analogous to weight regularization in many continual learning approaches Wang et al. (2024b); Ritter et al. (2018); Kirkpatrick et al. (2017); Aljundi et al. (2018); Shi et al. (2024). Notably, the saliency of weights is determined by analyzing the activation space of the base model rather than just regularizing based on the weight space. 3.2 Activation Space Analysis AIM determines the saliency of models weights by looking at the scale of activations by passing calibration dataset to the model and recording the scale of activations in each channel. To better understand why, we will analyze how the perturbation of weights of given model affects the model outputs. Let the original weights be RN and the perturbation be δw RN , such that the perturbed weights are: = + δw (1) The output of linear layer with input RN and perturbed weights is: = wx = x(w + δw) = xw + x(δw) The error due to the perturbation is: Error = = x(δw) And the magnitude of the error scales with the magnitude of activation x: Errorp = xδwp xδw1 xi (cid:88) j= (cid:88) i=1 δwij (2) (3) (4) For any specific input channel xi, the error contribution from perturbation in the i-th row of w, δwi is amplified by the magnitude of the same channel in the input. As such, one could selectively regularize the weights based on the importance of the input channels, i.e., their magnitudes. In this way, we use calibration dataset to capture the average magnitude of the input channels for each layer of the base model and determine the saliency of weights in the base model from the activation space. We choose the calibration dataset to be subset of the validation data from the pile dataset Gao et al. (2020) which is similar in distribution to most pre-training data and is the choice of calibration data for AWQ Lin et al. (2024) as well (based on the latest implementation on GitHub at the time of writing). This calibration dataset is considered to be fairly diverse and not task-specific, while also being similar to most pre-training data, which should allow for capturing representative set of activation magnitudes, and hence salient weights. Using this data to capture the average magnitudes of the input activation, we come up with complementary solution to apply relaxation to weights of merged models using any merging method and show that our adaptable relaxation scheme helps improve the performance of merged models significantly. 3.3 Adaptable Relaxation Scheme As discussed, we introduce an adaptable relaxation scheme based on the activations of the base model, which we wish not to stray away from significantly. To make the scheme adaptable to any merging algorithm in the weight space, we formulate our relaxation scheme in terms of the changes in the weights. Given task-agnostic representative calibration corpus D. We can pass this corpus through the model and accumulate the activations from each token in the dataset. This will yield the average magnitude of activations for each channel in all layers of the model. Let vector ai be the normalized (i.e., normalized by division by the largest activation magnitude in layer i) average of each activation channel in layer given the calibration corpus D. We then associate these activations with the corresponding model parameters, such as the columns/rows of weight matrices in linear layers. This results in per-weight activation mapping, denoted by A, which has the same dimensionality as the model parameters θ. Next, we define the action of any given merging method by the changes that it applies to the model weights with respect to each of the fine-tuned models being merged (i.e., θ1, θ2, . . . , θN ). Specifically, we denote the weight update contributed by fine-tuned model (with parameters θi) to the model parameters by (e.g, = θiθpre for weight averaging). Now, we propose an adaptive relaxations scheme that adjusts the final model as follows: θmerged = θpre + (1 Apre(1 ω)) (cid:88) i=1 λi i, (5) where ω is the relaxation factor that controls how much relaxation is applied (an ω of 0.0 would revert the most salient weight to the base weights and an ω of 1.0 applies no relaxation), and λi are the weight factors for each and the subscript pre refers to the pre-trained model. In this work, since we do not explicitly look at the merging algorithms inner workings, we can simply fuse the terms (cid:80)N i=1 λi into single algorithm-agnostic term merged and simplify the relaxation scheme to: θmerged = θpre + (1 Apre(1 ω))merged. (6) In our experiments, we apply this relaxation scheme to several different merging methods and explore how the hyper-parameter ω affects the merged models behavior, and present these results in Section 5."
        },
        {
            "title": "4 Experimental Setup and Evaluation Metrics",
            "content": "We conduct two separate experiments with AIM: 1) we apply AIM to 5 different merging methods including the two latest works on the topic with different numbers of experts being merged and report the performance of the models on 6 different benchmarks; 2) we conduct an ablation study on the ω parameter in AIM and analyze how ω affects each of the merging methods in scenario where 3 different experts are being merged. 4.1 Selection of FT Expert LLMs To understand how AIM reacts with different merging methods, we conduct experiments with merging different experts fine-tuned from the same base model. The set of experts we use is the same set of experts used by the two latest LLM merging algorithms in the literature, namely DARE Yu et al. (2024a) and WIDEN Yu et al. (2024b), which use the same three experts fine-tuned from Llama-2-13b Touvron et al. (2023). These experts include the WizardLM-13B Xu et al. (2024) model fine-tuned for instruction following, WizardMath-13B Luo et al. (2025) fine-tuned for superior mathematical reasoning, and llama-2-13b-code-alpaca which serves as the code expert Chaudhary (2023). Note On Weights: The weights we use in our experiments may not be exactly identical to the weights used in the experiments by Yu et al. (2024a) and Yu et al. (2024b), since the referenced weights for WizardMath-13B are no longer available publicly, instead we use publicly available copy of the model. See Appendix for more details. 4.2 Merging Methods Implementations In our experiments, we implement the latest merging methods in the literature for LLM merging. These include newly developed DARE and WIDEN Yu et al. (2024a,b) methods as well as some of the long-established approaches of TIES merging Yadav et al. (2023), and task arithmetic Ilharco et al. (2023). For all merging methods except WIDEN, we use the comprehensive MergeKit implementations developed by Goddard et al. (2024), and for WIDEN we use the publicly available implementation provided by the authors of the paper. We note that in many of the merging algorithms, many hyper-parameters can be adjusted. In these cases, we use the author-recommended values where available and the default parameters recommended by Goddard et al. (2024). Note that it is possible to perform grid search on these hyper-parameters to find optimal values for each benchmark, however, this would essentially be over-fitting on benchmarks and does not provide any value to our analysis of the proposed complementary relaxation scheme which applies adjustments to the merged models. For reproducibility, all of our checkpoints and code to reproduce the results will be made publicly available. 5 4.3 Benchmarks Used For Evaluations Given that the expert models we use in our experiments involve fine-tuning on instruction following, mathematical reasoning, and code generation we use several common benchmarks for each of these tasks. Specifically, we measure model performance on language understanding with the MMLU Hendrycks et al. (2021a) benchmark, instruction following with IFEval Zhou et al. (2023) benchmark, code generation with HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) benchmarks, and mathematical reasoning with the MATH Hendrycks et al. (2021b) and GSM8K Cobbe et al. (2021) benchmarks. For all benchmarks, we use the latest versions and up-to-date implementations developed by Gao et al. (2024) except for mathematical reasoning for which we use the chain of thought prompting used by Luo et al. (2025) to replicate the results of the original model as closely as possible. The code we use for these benchmark results will also be made publicly available for reproducibility. In addition to the common benchmarks that we use to evaluate merged models, we also propose new evaluation metric for LLM merging (or merging of different experts in general) which we believe helps better contextualize the value added by any given merging algorithm which we discuss in the following section. 4.4 Measuring Performance From an Optimization Perspective We note that in most cases LLMs are not meant to operate as narrow expert models, unlike large portion of deep learning applications where models are trained to perform very specific tasks such as classification or regression. LLMs in contrast, are generalist language models aiming to assist across wide variety of tasks and applications. As such LLMs can be viewed from multi-objective optimization perspective. In merging scenarios specifically, multiple expert models are brought together to create merged model aiming to find the balance of performance across the different expertise of the fine-tuned models. In this sense, each expert can be thought of as optimized for specific objective. This perspective lends itself rather well to multi-objective optimization view of the problem. Given this, only looking at how each model performs on each of the benchmarks does not give us full picture of the multi-objective goal of merging. To obtain more comprehensive view of merging performance, we propose hypervolume-based metric that quantifies the contribution of the merged model to the multi-objective frontier of FT LLMs. Consider performance space defined over benchmarks, where each models performance is represented as point in an -dimensional space. The performance on each benchmark is normalized to the range [0, 1], where 0 represents the worst-case (reference point), and 1 represents the best possible performance with 100% accuracy on the benchmark in question. Let = (r1, r2, . . . , rN ) denote the reference point in this space, which we set to (0, 0, . . . , 0) to ensure hypervolume calculations are consistently defined. Given set of FT models and the pre-trained base model, let denote the subset of models that are Pareto-optimal, i.e., models that are not dominated by any other model in S. The hypervolume of this set, denoted as HV (S), is defined as: HV (S) = λ (cid:33) dom(x, r) , (cid:32) (cid:91) xS (7) where λ() denotes the Lebesgue measure (i.e., volume in RN ), and dom(x, r) represents the hypervolume dominated by with respect to the reference point r. When merged model is introduced, the new set becomes = {M }, and the updated Pareto-optimal set is denoted as S. Given this set including the merged model we can measure the added value of the merged model from multi-objective perspective as the normalized hypervolume gain (HV Gain) as result of adding this merged model: HV Gain = d(cid:112)HV (S) HV (S) (8) Where is the number of dimensions/benchmarks. Since hypervolume is computed only over Pareto-optimal models, we have HV (S) HV (S), ensuring that HV Gain 0. This metric provides an aggregated measure of merging effectiveness, capturing trade-offs across multiple benchmarks rather than focusing on isolated improvements thus providing full picture of merging performance. In our experiments, we track HV Gain along with the 6 aforementioned benchmarks as described here. 6 Figure 2: The Pareto fronts of models under different scenarios. Note that the points in these plots represent all models benchmarked in Table 1, however, to make the plots more readable, we only visualize the dominating points in each case. The measured increases in HV Gain when AIM is applied can be clearly seen in the Pareto frontier shifting further forward when AIM is applied compared to when only population of merged models is evaluated."
        },
        {
            "title": "5 Experiments & Results",
            "content": "In this section, we present our results on applying AIM to different merging methods as well as an ablation study on how the ω hyper-parameter affects the performance of each merging method. 5.1 AIM Applied to Various Merging Approaches To demonstrate the effectiveness of AIM, we conduct experiments on 5 different merging methods under 4 different scenarios. As mentioned before we use 3 different FT LLM experts in our experiments. As such we merge models using each merging method for all 4 possible permutations of these expert LLMs. Then we apply AIM to all merged models and measure the performance of each model in all 6 benchmarks. We also report the HV gain for each merged model compared to the population of the base model and the models being merged (in cases with 2 models the population will only include the models used for merging). These results are presented in Table 1. For this experiment, we used ω = 0.4, which we found to be the best balance of performance among the various merging methods we use. This choice was informed by our analysis in Section 5.2. In Table 1 we have highlighted the gain/loss of performance for each benchmark due to AIM and we can see that in the vast majority of cases, AIM causes significant performance boost, with an Average Percentage-Point Change of 13% (ignoring the Inf value) and more than 40% HV Gain in 20% cases, further highlighted by the fact that the top performers on each benchmark, as well as the largest hypervolume gain, are all in models merged with AIM. We observe HumanEval (10 out of 20) and MBPP (17 out of 20) often see large boosts with AIM, especially when merging Instruction Tuned models with others. Some merges also reveal small drops in GSM8K or IFEval even when other benchmarks improve, reflecting the inherent trade-offs in merging specialized models. Overall, clear majority (80%) of merges exhibit improved HV Gain under AIM, reinforcing that the method often enhances multi-task performance overall. We can further visualize this increase in hypervolume by looking at how AIM pushes the Pareto frontier. Figure 2 shows how applying AIM to existing merging methods extends the Pareto optimal frontier, which we also quantitatively measured using HV gain. These results showcase the efficacy of the proposed method across variety of merging methods and reinforce the hypothesis that the activation space encompasses useful insight for merging. 5.2 Ablation study To understand the effects of changing ω in AIM, we conduct an ablation study on the case of merging all three expert LLMs. For this study, we apply AIM with ω {0.0, 0.2, 0.4, 0.6, 0.8} and run the benchmarks on each merged model with each value of ω. For brevity we do not report all benchmark results for each value here, instead, we track the hypervolume gain (The full set of results are presented in Appendix B). Specifically, to better visualize the effect of ω we measure the relative change in HV Gain compared to no AIM (i.e. ω = 1.0). We present these results in Figure 3. In most merging methods, we see that decreasing ω to even 0 still benefits the model performance while, in some cases, still increases the performance. However, in TIES merging particularly, we see that decreasing ω beyond 0.4 seems to degrade performance, and setting ω to the most extreme case of 0.0 does see some degradation in WIDEN as well. Given this, it seems that in these experiments, value of 0.4 balances the performance gains in methods responding Method Model(s) AIM HumanEval MBPP MMLU MATH GSM8K IFEval HV Gain - - - - DARE Task Arithmetic DARE Ties Task Arithmetic Ties Merging WIDEN Base Code Instruction Tuned Math Code + Instruction Tuned Code + Math Instruction Tuned + Math Code + Instruction Tuned + Math Code + Instruction Tuned Code + Math Instruction Tuned + Math Code + Instruction Tuned + Math Code + Instruction Tuned Code + Math Instruction Tuned + Math Code + Instruction Tuned + Math Code + Instruction Tuned Code + Math Instruction Tuned + Math Code + Instruction Tuned + Math Code + Instruction Tuned Code + Math Instruction Tuned + Math Code + Instruction Tuned + Math - - - - No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes 17.07 17.07 26.83 15.24 Base Models 27.80 31.60 34.80 27. Merged Models 26.83 29.27 (+9.09%) 16.46 15.85 (-3.71%) 5.49 12.20 (+122.22%) 11.59 15.85 (+36.76%) 30.49 30.49 17.07 17.68 (+3.57%) 8.54 15.85 (+85.60%) 13.41 19.51 (+45.49%) 29.27 29.88 (+2.08%) 18.29 17.68 (-3.34%) 4.27 8.54 (+100.00%) 11.59 15.24 (+31.49%) 16.46 15.24 (-7.41%) 15.85 15.85 28.05 27.44 (-2.17%) 21.34 20.73 (-2.86%) 26.22 25.61 (-2.33%) 17.07 17.07 24.39 23.78 (-2.50%) 25.00 26.83 (+7.32%) 34.40 36.00 (+4.65%) 28.60 29.60 (+3.50%) 19.00 28.20 (+48.42%) 19.60 27.00 (+37.76%) 35.20 36.80 (+4.55%) 27.40 29.00 (+5.84%) 23.80 30.20 (+26.89%) 21.20 28.60 (+34.91%) 33.80 35.80 (+5.92%) 28.60 29.20 (+2.10%) 20.20 26.40 (+30.69%) 19.60 27.40 (+39.80%) 23.60 24.20 (+2.54%) 26.80 28.60 (+6.72%) 34.60 35.00 (+1.16%) 29.20 29.20 35.60 34.60 (-2.81%) 29.40 29.60 (+0.68%) 30.40 32.00 (+5.26%) 33.20 32.80 (-1.20%) 52.18 52.91 53.41 51.89 0.70 6.00 7.50 13.10 4.20 24.10 43.40 59. 25.10 26.25 35.67 21.58 - - - - 53.53 54.18 (+1.21%) 51.96 52.50 (+1.04%) 51.08 52.72 (+3.21%) 50.89 52.59 (+3.34%) 53.40 54.02 (+1.16%) 51.92 52.61 (+1.33%) 51.39 52.89 (+2.92%) 51.15 52.63 (+2.89%) 53.44 54.12 (+1.27%) 52.10 52.52 (+0.81%) 51.50 52.83 (+2.58%) 51.20 52.63 (+2.79%) 52.70 53.15 (+0.85%) 51.86 52.29 (+0.83%) 54.45 54.74 (+0.53%) 53.97 54.46 (+0.91%) 54.90 54.97 (+0.13%) 53.35 53.36 (+0.02%) 54.20 54.69 (+0.90%) 54.58 54.98 (+0.73%) 8.40 8.30 (-1.19%) 15.10 14.80 (-1.99%) 9.80 12.90 (+31.63%) 9.10 12.20 (+34.07%) 8.60 8.60 14.90 15.20 (+2.01%) 9.20 11.60 (+26.09%) 8.70 11.60 (+33.33%) 8.60 7.80 (-9.30%) 15.00 14.60 (-2.67%) 10.00 12.80 (+28.00%) 9.00 12.00 (+33.33%) 2.70 2.60 (-3.70%) 14.30 15.30 (+6.99%) 8.70 9.30 (+6.90%) 6.30 5.70 (-9.52%) 8.30 8.20 (-1.20%) 14.20 14.30 (+0.70%) 14.60 15.10 (+3.42%) 13.50 14.40 (+6.67%) 45.80 46.20 (+0.87%) 64.70 64.10 (-0.93%) 54.30 62.20 (+14.55%) 49.70 60.70 (+22.13%) 46.20 47.20 (+2.16%) 63.60 63.90 (+0.47%) 54.10 57.80 (+6.84%) 51.50 57.00 (+10.68%) 47.10 46.60 (-1.06%) 64.70 64.50 (-0.31%) 54.20 61.30 (+13.10%) 52.70 58.10 (+10.25%) 5.40 5.20 (-3.70%) 62.60 63.80 (+1.92%) 44.70 46.10 (+3.13%) 29.20 23.70 (-18.84%) 45.00 44.10 (-2.00%) 64.40 62.20 (-3.42%) 66.00 68.20 (+3.33%) 64.20 64.00 (-0.31%) 33.42 32.00 (-4.25%) 22.02 21.91 (-0.50%) 32.35 31.96 (-1.21%) 33.20 33.59 (+1.17%) 33.28 33.16 (-0.36%) 22.53 21.10 (-6.35%) 33.89 35.63 (+5.13%) 35.75 36.20 (+1.26%) 31.60 32.01 (+1.30%) 21.92 21.54 (-1.73%) 31.31 32.62 (+4.18%) 32.87 33.91 (+3.16%) 24.48 22.87 (-6.58%) 21.63 22.64 (+4.67%) 34.04 34.51 (+1.38%) 26.95 25.98 (-3.60%) 30.42 31.60 (+3.88%) 24.02 23.95 (-0.29%) 30.82 31.23 (+1.33%) 31.44 32.82 (+4.39%) 0.27 0.28 (+2.49%) 0.23 0.23 (-1.65%) 0.18 0.26 (+40.71%) 0.16 0.23 (+40.59%) 0.28 0.29 (+1.63%) 0.23 0.24 (+4.00%) 0.20 0.26 (+31.22%) 0.17 0.24 (+41.28%) 0.28 0.28 (+0.61%) 0.24 0.24 (-2.65%) 0.18 0.24 (+34.52%) 0.16 0.22 (+31.97%) 0.00 0.05 (+inf%) 0.20 0.23 (+13.55%) 0.23 0.25 (+6.38%) 0.11 0.11 (+4.33%) 0.27 0.26 (-0.93%) 0.24 0.24 (-1.22%) 0.30 0.31 (+2.54%) 0.29 0.30 (+4.70%) Table 1: Benchmark Results Across Various Merging Scenarios and Methods. Percentage changes are shown relative to models merged without AIM, with these differences highlighted for each metric. The highest-performing fine-tuned large language models and base models are highlighted in yellow, while the best-performing merged models are marked in blue. The results demonstrate that, in most cases, applying AIM significantly enhances the performance of merged models across all benchmarks, often increasing HV Gain. well to AIM and the potential degradation of methods that benefit less from AIM. However, given this observation that in some cases pushing ω to 0 still yields benefits, there may be some value in exploring non-linear scaling of activation magnitudes and non-linear relaxation schemes that could further boost performance in some cases."
        },
        {
            "title": "6 Conclusion and Outlook",
            "content": "In this work, we introduced Activation-Informed Merging (AIM) as complementary algorithm to existing model merging techniques for large language models (LLMs). We hypothesized that the activation space of LLMs harbors useful information that is often overlooked in model merging, as most existing methods operate purely on the weight space. To explore this potential information in the activation space, we viewed the problem from continual learning perspective and proposed leveraging the activation space information from task-agnostic calibration set. This approach selectively preserves critical weights from the pre-trained model, mitigating catastrophic forgetting while incorporating knowledge from fine-tuned models, yielding overall higher-performing models. Through extensive empirical evaluations across multiple merging methods and benchmark tasks, we demonstrated that AIM consistently improves performance, often yielding superior results in comparison to the original merging methods it was applied to. These results empirically confirm our hypothesis on the importance of the activation space. Notably, AIM boosted merged model performance by up to 40% in some cases, underscoring the crucial rule and the potential of activation information in merging methods. Our findings highlight the necessity of incorporating activation-informed strategies when merging multiple fine-tuned models. Moving forward, our findings open up several promising directions for future research. First, our results indicate that even aggressively preserving salient weights of the pre-trained model is effective across many merging scenarios. This highlights the promise for more advanced activation-informed strategies and non-linear relaxation methods to enhance performance potentially further. Beyond the pre-trained activations explored in this work, there is room to improve existing merging methods by leveraging the broader activation space of the models being merged. So far, AIM has 8 Figure 3: Ablation Study of the Impact of the Relaxation Factor ω on Merged Model Performance. This figure plots the relative change in HV Gain compared to scenarios without relaxation. The x-axis represents 1 ω, reflecting that decreasing ω results in more aggressive relaxation. The plot indicates that for some tasks, such as Arithmetic and DARE, smaller values of ω continue to yield benefits. Overall, ω settings of 0.4 or 0.6 appear to strike good balance across various methods. only considered the activations of the pre-trained model, while the activations of the expert LLMs remain unexplored. Future research should focus on developing methods that also encompass information from the expert model activations. Additionally, in future works, more theoretically grounded approaches for incorporating the activation space of LLMs in merging should be developed and tested. These integrations will hold great value in improving the quality and performance of merging methods in an increasingly competitive and ever more efficient landscape of LLMs, which could benefit from smaller and more efficient yet more powerful models. Overall, AIM serves as robust and adaptable augmentation to existing LLM merging techniques, offering principled way to incorporate activation information for more effective model fusion. By prioritizing the activation-aware perspective, we take step towards more stable, efficient, and generalizable merged models that better leverage the strengths of multiple fine-tuned experts."
        },
        {
            "title": "References",
            "content": "Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and Tuytelaars, T. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European conference on computer vision (ECCV), pp. 139154, 2018. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Chaudhary, S. Code alpaca: An instruction-following llama model for code generation. https://github.com/ sahil280114/codealpaca, 2023. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. 2021. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Davari, M. and Belilovsky, E. Model breadcrumbs: Scaling multi-task model merging with sparse masks, 2024. URL https://arxiv.org/abs/2312.06795. Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H., and Smith, N. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, 2020. URL https://arxiv.org/abs/2002.06305. Farajtabar, M., Azizan, N., Mott, A., and Li, A. Orthogonal gradient descent for continual learning. In International Conference on Artificial Intelligence and Statistics, pp. 37623773. PMLR, 2020. Frantar, E. and Alistarh, D. Optimal brain compression: framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:44754488, 2022. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https://arxiv.org/abs/2101.00027. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. Goddard, C., Siriwardhana, S., Ehghaghi, M., Meyers, L., Karpukhin, V., Benedict, B., McQuade, M., and Solawetz, J. Arcees MergeKit: toolkit for merging large language models. In Dernoncourt, F., Preotiuc-Pietro, D., and Shimorina, A. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 477485, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry.36. URL https://aclanthology.org/2024.emnlp-industry.36. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021a. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021b. URL https://arxiv.org/abs/2103.03874. Ilharco, G., Ribeiro, M. T., Wortsman, M., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with In The Eleventh International Conference on Learning Representations, 2023. URL https: task arithmetic. //openreview.net/forum?id=6t0Kwf8-jrj. Jang, D.-H., Yun, S., and Han, D. Model stock: All we need is just few fine-tuned models. In European Conference on Computer Vision, pp. 207223. Springer, 2025. Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Dataless knowledge fusion by merging weights of language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=FCnohuR6AnM. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. 10 Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for llm compression and acceleration, 2024. URL https://arxiv.org/ abs/2306.00978. Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., Tang, Y., and Zhang, D. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct, 2025. URL https: //arxiv.org/abs/2308.09583. Matena, M. S. and Raffel, C. A. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems, 35:1770317716, 2022. Pareja, A., Nayak, N. S., Wang, H., Killamsetty, K., Sudalairaj, S., Zhao, W., Han, S., Bhandwaldar, A., Xu, G., Xu, K., Han, L., Inglis, L., and Srivastava, A. Unveiling the secret recipe: guide for supervised fine-tuning small llms, 2024. URL https://arxiv.org/abs/2412.13337. Rame, A., Ahuja, K., Zhang, J., Cord, M., Bottou, L., and Lopez-Paz, D. Model ratatouille: Recycling diverse models for out-of-distribution generalization. In International Conference on Machine Learning, pp. 2865628679. PMLR, 2023. Rame, A., Vieillard, N., Hussenot, L., Dadashi-Tazehozi, R., Cideron, G., Bachem, O., and Ferret, J. WARM: On the benefits of weight averaged reward models. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 4204842073. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/rame24a.html. Rame, A., Ferret, J., Vieillard, N., Dadashi, R., Hussenot, L., Cedoz, P.-L., Sessa, P. G., Girgin, S., Douillard, A., and Bachem, O. Warp: On the benefits of weight averaged rewarded policies, 2024. URL https://arxiv.org/abs/ 2406.16768. Ritter, H., Botev, A., and Barber, D. Online structured laplace approximations for overcoming catastrophic forgetting. Advances in Neural Information Processing Systems, 31, 2018. Shi, H., Xu, Z., Wang, H., Qin, W., Wang, W., Wang, Y., Wang, Z., Ebrahimi, S., and Wang, H. Continual learning of large language models: comprehensive survey. arXiv preprint arXiv:2404.16789, 2024. Shoemake, K. Animating rotation with quaternion curves. In Proceedings of the 12th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 85, pp. 245254, New York, NY, USA, 1985. Association for Computing Machinery. ISBN 0897911660. doi: 10.1145/325334.325242. URL https://doi.org/10.1145/ 325334.325242. Sudalairaj, S., Bhandwaldar, A., Pareja, A., Xu, K., Cox, D. D., and Srivastava, A. Lab: Large-scale alignment for chatbots, 2024. URL https://arxiv.org/abs/2403.01081. Sung, Y.-L., Li, L., Lin, K., Gan, Z., Bansal, M., and Wang, L. An empirical study of multimodal model merging. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview. net/forum?id=vVdRgpC1Oh. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Wang, L., Zhang, X., Su, H., and Zhu, J. comprehensive survey of continual learning: Theory, method and IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(8):53625383, 2024a. doi: application. 10.1109/TPAMI.2024.3367329. Wang, L., Zhang, X., Su, H., and Zhu, J. comprehensive survey of continual learning: Theory, method and application, 2024b. URL https://arxiv.org/abs/2302.00487. Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pp. 2396523998. PMLR, 2022. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., and Jiang, D. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=CfXh93NDgH. 11 Yadav, P., Tam, D., Choshen, L., Raffel, C., and Bansal, M. Ties-merging: Resolving interference when merging models, 2023. URL https://arxiv.org/abs/2306.01708. Yadav, P., Vu, T., Lai, J., Chronopoulou, A., Faruqui, M., Bansal, M., and Munkhdalai, T. What matters for model merging at scale?, 2024. URL https://arxiv.org/abs/2410.03617. Yang, E., Shen, L., Guo, G., Wang, X., Cao, X., Zhang, J., and Tao, D. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities, 2024. URL https://arxiv.org/abs/2408.07666. Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. Language models are super mario: Absorbing abilities from homologous models as free lunch. In International Conference on Machine Learning. PMLR, 2024a. Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. Extend model merging from fine-tuned to pre-trained large language models via weight disentanglement. arXiv preprint arXiv:2408.03092, 2024b. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911."
        },
        {
            "title": "A Reproducibility Details",
            "content": "Here we provide the specific details for reproducing the results presented in the paper. Checkpoints: Firstly, we specify the publicly available checkpoints we use in our experiments. Below is list of checkpoints used and the links to the publicly available weights for these models: Base Model: https://huggingface.co/unsloth/llama-2-13b Code Model: https://huggingface.co/layoric/llama-2-13b-code-alpaca Math Model: https://huggingface.co/vanillaOVO/WizardMath-13B-V1.0 Instruction Tuned Model: https://huggingface.co/WizardLMTeam/WizardLM-13B-V1.2 Code & Data: Aside from the checkpoint we provide our code and the link to the publicly available calibration data we use in our work. Our code is publicly available at https://github.com/ahnobari/ActivationInformedMerging.git and the calibration data can be found at https://huggingface.co/datasets/mit-han-lab/pile-val-backup. Checkpoints: We also provide the checkpoints for the main experiment on hugging face: Merged Models: HF Collection AIM Merged Models: HF Collection"
        },
        {
            "title": "B Ablation Detailed Results",
            "content": "Here we present the full results of the ablation study we conducted. Table 2 includes the granular values for all benchmarks we ran for different values of ω."
        },
        {
            "title": "DARE Task Arithmetic",
            "content": "DARE Ties Task Arithmetic WIDEN ω 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1. 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 HumanEval MBPP MMLU MATH GSM8K IFEval HV Gain 19.51 19.51 20.73 20.12 20.73 21.34 18.90 16.46 15.85 15.24 14.02 11. 25.61 21.34 19.51 15.85 14.02 13.41 18.90 15.24 15.24 15.24 13.41 11.59 27.44 28.05 26.83 25.61 26.22 25.00 29.0 28.6 29.2 27.8 27.8 29.2 29.4 29.0 27.0 27.0 22.2 19.6 29.6 29.4 28.6 26.0 24.0 21. 29.4 27.6 27.4 25.4 21.8 19.6 33.2 33.0 32.8 33.4 32.6 33.2 54.32 54.35 54.46 54.25 54.09 53.97 53.42 52.98 52.59 52.18 51.53 50.89 53.31 53.11 52.63 52.22 51.6 51.15 53.42 52.97 52.63 52.13 51.61 51. 55.26 55.16 54.98 54.77 54.64 54.58 4.6 5.3 5.7 6.6 6.6 6.3 13.8 12.9 12.2 11.8 9.9 9.1 12.0 12.1 11.6 10.1 10.0 8.7 13.8 13.0 12.0 11.4 10.2 9.0 14.0 14.2 14.4 14.2 14.0 13. 17.6 21.2 23.7 32.1 33.7 29.2 60.5 61.5 60.7 58.1 54.0 49.7 59.4 58.7 57.0 54.2 53.1 51.5 60.5 59.8 58.1 57.4 56.2 52.7 64.9 65.6 64.0 63.0 64.1 64.2 26.24 25.37 25.98 23.9 24.07 26. 35.49 34.97 33.67 33.95 32.69 33.2 34.64 36.76 36.2 34.82 35.51 35.75 35.49 35.27 33.88 33.29 32.74 32.95 32.39 32.39 32.76 32.06 31.74 31.44 0.1015 0.1069 0.1143 0.1155 0.1131 0.1096 0.2623 0.2434 0.2257 0.2142 0.1828 0. 0.2669 0.2590 0.2426 0.2019 0.1917 0.1717 0.2623 0.2296 0.2165 0.2069 0.1862 0.1643 0.3027 0.3066 0.3013 0.2947 0.2941 0.2879 Table 2: Performance metrics for different methods with varying ω values."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "RedHat AI Innovation & MIT-IBM Watson AI Lab",
        "Stony Brook University"
    ]
}