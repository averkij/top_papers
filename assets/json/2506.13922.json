{
    "paper_title": "DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance",
    "authors": [
        "Maximilian Du",
        "Shuran Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 2 9 3 1 . 6 0 5 2 : r DynaGuide: Steering Diffusion Polices with"
        },
        {
            "title": "Active Dynamic Guidance",
            "content": "Maximilian Du Stanford University maxjdu@stanford.edu Shuran Song Stanford University shuran@stanford.edu dynaguide.github.io"
        },
        {
            "title": "Abstract",
            "content": "Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of situation. Most common steering approaches, like goal-conditioning, require training the robot policy with distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in series of simulated and real experiments, showing an average steering success of 70% on set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io"
        },
        {
            "title": "Introduction",
            "content": "The rise of large datasets and expressive policies has enabled complicated skills in robots like folding clothes [3, 53], making sandwiches [44], and washing dishes [7]. When these large policies are deployed in the dynamic real world, we face the challenge of steering them to match the needs of specific scenario. This means finding subset of the policys behaviors that are appropriate for the scenario [48]. The policys complexity means that we ideally want to accomplish this steering without needing to retrain the policy, sample excessively from it, or anticipate all the possible steering during policy training. Most existing works for policy steering, however, rely on at least one of those assumptions. Even language or goalconditioned policies are trained on set of Preprint. Under review. Figure 1: DynaGuide steers pretrained diffusion policies by adding guidance from dynamics model into the action denoising process. This dynamics-based guidance can take diverse behavior base policy and steer it towards one single behavior (left), multiple behaviors (middle), and even removing behavior (right) all without fine-tuning. goal distributions, which requires foresight for the goal distribution that satisfies all future steering needs [45]. In our work, we distance ourselves from these assumptions. We propose separating the steering forces from the base behavior policy and combining the influence from the steering model with the strong action prior of the base policy to generate guided behaviors that satisfy inference-time objectives while leveraging the base policys existing skills. Steering robot behavior requires understanding how current actions influence future outcomes, feat well-suited for dynamics models [39, 16]. To adapt dynamics model for policy steering, we craft latent visual dynamics model that predicts the final or far-future observation of trajectory given the current observation and action, allowing it to understand long-horizon dependencies. This dynamics model learns from unstructured environment interaction data and operates on visual observations projected into the expressive DinoV2 latent space [37] previously shown to be useful for robot planning [54]. In this latent space, we can compute the distance between the predicted future observation and observations of other desired/undesired outcomes, creating differentiable metric that encompasses the guidance objectives. Previous methods that steer policies with external metrics will sample actions from the policy and pick the action that best satisfies the metric [39, 34]. Instead, our approach leverages the denoising stochastic process of diffusion-based policies. Taking inspiration from classifier guidance [9] and dynamics diffusion guidance in other applications [51], we take the gradient of the metric through the dynamics model and use this dynamics gradient to directly modify the action denoising process. This separate dynamics model and the active diffusion guidance make up DYNAGUIDE, new way of steering pretrained diffusion policies towards complex objectives. This design offers several key advantages over existing steering approaches: Flexible Steering Structure. Goal-conditioned formulations have rigid input structures that cannot be modified during inferenceand most policies accept only single goal condition. DynaGuide accepts collection of guidance conditions encompassing any number of positive and/or negative objectives (4.3). Increased Steering Robustness. Goal-conditioned approaches require training the input condition alongside the policy, causing out-of-distribution (OOD) failures when new or lower-quality objectives are introduced during inference. The DinoV2 embedder and separation of guidance and policy in DynaGuide means that no individual part of the system is OOD under this same situation, and the averaging effect of the guidance metric (Eq. 2) means that DynaGuide still extracts meaningful guidance signal (4.2). The additional experience present in the dynamics model also enables guidance towards novel behavior (4.5). Plug-and-Play Modularity. Unlike fine-tuning approaches, DynaGuide only modifies the inference process of the diffusion policy, allowing different dynamics models to be swapped in and out without changing the base policy. This modularity also allows an off-the-shelf policy to be steered without further modifications (4.5). Enhancing Underrepresented Behaviors. Sample-based steering approaches [39, 34] select the best action from set of proposed actions by the base policy. Not only does this require many inferences of the policy per step, these sampling approaches also struggles with expressing behaviors that are underrepresented in the base policy (4.4). In contrast, our Dynamics Guidance directly influences the denoising process. Through one denoising sequence, we guide the action towards the specified objectives, even if the behavior is less common in the base policy (4.4). To investigate these claims, we conduct five experiments using simulated CALVIN environment tasks [30] and three experiments on real robot arm. We show that DynaGuide successfully guides the base policy up 70% of the time (4.1) and outperforms goal conditioning by 5.4x on lower quality objectives (4.2). It successfully amplifies underrepresented behaviors over sampling methods (4.4) and accommodates multiple positive and negative objectives (4.3). DynaGuide is also successful on an existing real robot policy, achieving up to 80% guidance success and even creating novel behavior (4.5). We will make code and collected data publicly available. Videos and visualizations can be found on our website, https://dynaguide.github.io"
        },
        {
            "title": "2 Related Works",
            "content": "2 Conditioning Policies on Goal Representations. common approach that influences robot behavior is goal conditioning, where the policy is given representation of desired outcome. The common representation is natural language [5, 55, 23, 36], which can also be used as steering mechanism for human-in-the-loop execution [45, 8]. Language-conditioned policies require special data and/or supervision to obtain and use the language labels, and it may be difficult to specify the exact steering needs through language alone. Other methods with fewer assumptions can learn goal-conditioned policies from trajectories by conditioning on learned latent embeddings [21, 28] or using future observations directly as the goal [41, 4, 38, 10, 2]. Like DynaGuide, these conditioned policies learn to accomplish the outcome represented by the goal observation. However, unlike DynaGuide, these approaches require training policy that can take the goal condition as the input. DynaGuides external dynamics model means that it works on top of any diffusion policy. It also supports multiple desirable and undesirable outcomes, whereas the observation-based goal conditioning typically supports only one desirable outcome. In our experiments, we compare the performance of DynaGuide with an observation goal-conditioned policy. Ours PG GC GPC Untrained Goals (4.2) Movable Objects (4.1) Multiple Conditions (4.3) Enhance Rare Behavior (4.4) Leveraging Dynamics Models to Improve Robot Policies. Models can be trained to predict future states through latent dynamics by taking in an observation latent, action(s), and outputting predicted future latent state [42, 31, 16, 14, 15, 54, 25]. These latent dynamics are commonly used for training reinforcement learning agents by simulating trajectories through the dynamics model [16, 14, 15, 40, 24] and have seen success on real robots [50]. They can also be used for Model Predictive Control (MPC) to generate trajectories directly [54, 43, 17, 13, 12, 47] and combined RL/Planning approaches [18, 19]. Especially relevant to DynaGuide are approaches that use dynamics models to filter samples from policies according to predicted value [34] or engineered reward [39]. Like some of these prior works and especially Dino-WM [54], DynaGuide uses transformer-based dynamics model to guide action creation. However, instead of sampling from the base policy, running RL, or MPC, DynaGuide steers the trained base policy directly through the action diffusion process. Table 1: Method Ability Comparisons. Goal-conditioned (GC) and sampling methods (GPC) [39] can steer policy behavior, but GC is not prepared for untrained goals, and rare behaviors are challenging for GPC. External guidance methods address these shortcomings, and unlike Position Guidance (PG) [49], DynaGuide (Ours) can guide towards complicated objectives without precise coordinate input. Inference-Time Steering. After policy has been trained, its behavior can still be changed without modifying its weights or requiring goal conditioning [48]. Safety value functions can intervene with recovery policy or request human assistance when abnormal behavior is predicted [35, 27, 11]. More specific to diffusion policies, classifier-free guidance [20] can directly influence the action diffusion process [41] or modify the model that predicts future states used in inverse-dynamics planning [1]. While classifier-free guidance is effective for steering, it still requires training the policy on supplied conditions, akin to goal conditioning. The alternative steering method is classifier guidance, which leverages an external classifier to influence diffusion generation process [9]. Classifier guidance in diffusion policies is akin to seeking an optimal trajectory defined by the classifier [22]. Applied to robot policies, classifier guidance can apply post-hoc constraints [32] or seek objects near human-supplied keypoint [49]. DynaGuide is classifier guidance approach, but it seeks to guide the policy with more complicated objectives using feedback from trained dynamics model acting as the classifier. Dynamics-based diffusion guidance has succeeded in other applications like hardware generation [51] and DynaGuide brings it to robot policies."
        },
        {
            "title": "3 DynaGuide Method",
            "content": "As shown in Fig. 2, we consider the problem of steering diffusion policy πθ(a o) during its inference process. We are given set of guidance conditions g, which are represented as image observations that contains partial state information. Some guidance conditions show desirable g+ outcomes g+ = , we 1 , ...gj } { ot), the probability p(g+ want to create πθ(a ot, a) is as high ot, a) is as low as possible. To accomplish this, we train as possible and the probability p(g and others are undesirable outcomes = o) such that for every g1 , ...gi } { . Using = g+ πθ( 3 Figure 2: Achieving Dynamics Guidance. (A): DynaGuide combines action denoising gradients εp from the pretrained policy with guidance gradient ak that increases the likelihood of accomplishing set of guidance conditions G. (B): Inside the guidance module, dynamics model predicts future outcomes ˆzt+H and compares them to guidance conditions (desired / undesired outcomes). We use the latent distances to define guidance metric (Equation 2) and take the gradient to get the guidance signal ak used by DynaGuide. (C): An example of one denoising step. The pretrained policy seeks behavior modes in the data, while the guidance gradient selects particular mode. dynamics model and use the model to approximate these probabilities (3.1) and then use the gradient of that dynamics model to create π by modifying its action denoising process (3.2). 3.1 Dynamics Model Capable of Guidance Creating model to approximate p(g+ ot, a) has two requirements: 1) the model needs to predict future outcome ˆot+H from current observation ot and action sequence a, 2) we need to compare ot+H to to the guidance conditions. Here, can be large value or + can be the end of the trajectory. ot, a) and p(g Past works on predicting future observations for planning and reinforcement learning have used latent dynamics models to satisfy these two requirements. These dynamics models hθ(ϕ(ot), a) operate in learned latent space zt = ϕ(ot). The recent work of Dino-WM [54] demonstrated the usefulness of latent distances in the pretrained DinoV2 image embeddings [37] for planning. Taking inspiration from Dino-WM, we leverage the same DinoV2 image latent space for our observation comparisons. Concretely, we use the patch embeddings from DinoV2 as zt = ϕ(ot) and train transformer hθ(a, zt) to predict latent outcome representation ˆzt+H . Because ϕ is frozen and we have access to full trajectories of data during training, it is sufficient to train hθ through regression objective (Eq. 1). For more details, refer to Appendix B. (ot, ot+H , a) = ϕ(ot+H ) hθ(ϕ(ot), a) 2 2 (1) With this dynamics model trained, we define guidance metric that compares the predicted outcome with the guidance conditions. Given one guidance condition g+ , we want to approximate p(g+ ot, a), which can be done by comi paring the predicted outcome ˆzt+H to g+ . The condition g+ is visual observation, so we project it into the same latent space z+ = ϕ(g+ ). This allows us to directly compare z+ and ˆzt+H . If we roughly assume that the latent space is gaussian in structure, reasonable approximation of log p(g+ ot, a) is proportional z+ 2 to 2, metric found in similar forms elsewhere for latent reinforcement learning [52, 33] and planning [54]. ˆzt+H Algorithm 1 DynaGuide (Inference-Time) 1: Input: Positive guidance conditions g+, 2: Input: Action denoiser ϵϕ(a, o), current obs ot 3: Input: Trained latent dynamics model hθ and Action Denoising Stochastic Sampling image embedder ϕ 4: aK Sample from (0, I) 5: for in to 1 do 6: 7: 8: 9: 10: 11: 12: end for for in 1 to do ϵ ϵϕ(ak, ot) Eq. 2 ˆϵ(ak, ot) ϵ ak1 Denoise ak using ˆϵ end for 1 αkak ot, a) across the set of guidance conditions g+, we can try to When we try to maximize log p(g+ maximize the combined probability of achieving each outcome log (cid:80) p(g+ ot, a). The negative is true for the undesired outcomes. We use our approximation of log p(g+ ot, a) and variance hyperparameter σ to get the guidance metric d(g+, g, ot, a) (Eq. 2). The log-sum-exp acts as soft maximum, feature useful for diverse objectives (4.3) and lower quality guidance conditions (4.2). For more detailed derivation of and explanation of design choices, refer to Appendix B.3. = log (cid:34) (cid:88) ϕ(g+ ) exp hθ(ϕ(ot), a) σ 2 2 (cid:35) log (cid:88) ϕ(gj ) exp hθ(ϕ(ot), a) σ 2 2 (2) We train the dynamics model using diverse robot interaction data. For the Calvin experiments 4.1-4.4 we use existing human-collected play data from the benchmark. For the real-world experiments 4.5, we use open-source data collected on the UMI interface [6]. Because we will be querying the dynamics model with noisy actions during the guidance process, we augment the training by adding gaussian noise to the actions using the same noise scheduler used during inference time, with noise step picked from geometric distribution. For more details about implementation, see Appendix B. 3.2 Guiding the Action Diffusion Process To sample an action from diffusion policy π( ϵ(ak, ot), that moves an initial noise sample aK models can be guided by classifier p(y [9]. The guidance signal comes from the gradient of the classifier with ϵ(ak, ot) during the denoising process. a) is our derived metric in Eq. 2. Like In our case, is the guidance conditions, and the log p(y past work [51, 41], we use the Denoising Diffusion Implicit Models (DDIMs) [46] as our sampling method. Under DDIM, the combined denoising and guidance signal is ot), the policy predicts sequence of denoising steps to denoised action sequence a0. Diffusion a) a) and is combined a) towards particular set of samples that maximize p(y log p(y ˆϵ(ak, ot) = ϵ(ak, ot) αkak d(g+, g, ot, ak) (3) where αk = (cid:81)k αl, αl is the noise scheduler, and is guidance scaling parameter [9]. The higher the s, the stronger the signal to adhere to the guidance requirements. However, past work has discovered that higher also create less smooth or even incoherent trajectories [49], creating delicate balance between trajectory validity and adherence to guidance conditions. This phenomenon happens because ad can pull ϵ out of distribution, leading to erroneous noise predictions that compound as the denoising process continues. To resolve this, Wang et al. [49] proposed the Stochastic Sampling solution of running each denoising step multiple (M ) times to stabilize the guidance signals influence on the action denoising process through MCMC sampling. Empirically, this trick allows to be pushed higher to increase guidance success without sacrificing trajectory validity. See Algorithm 1 for an overview of DynaGuide. We investigate these hyperparameters in Appendix A.2."
        },
        {
            "title": "4 Experiments",
            "content": "To understand the features, benefits, and limitations of DynaGuide, we conduct five sets of experiments on the Calvin environment (Figure 3, 4.1-4.4). To demonstrate its practical feasibility on real robots, we steer publicly available robot policy in real task using DynaGuide (4.5). We also conduct experiments on toy 2D navigation environment, which can be found in Appendix A.1. Wherever relevant, we compare DynaGuide to representative set of baselines. Base Policy. This is the diffusion policy [7] that we steer with DynaGuide. This policy offers fairly uniform distribution across valid behaviors. DynaGuide-Sampling (GPC) [39]. Instead of diffusion guidance, we sample from the base policy multiple times and pick the action sequence that best satisfies metric (Eq 2), an idea demonstrated in GPC-Rank with engineered reward functions [39]. We re-implement this idea with our dynamics model and use this baseline to explicitly test the benefits of diffusion guidance. 5 Figure 3: Experiment Setup. In the CALVIN simulator [30], we propose four experimental setups designed to showcase DynaGuide and its advantages over other steering approaches. First, we test performance with high quality outcome observations as guidance conditions (Fully-Specified Objective). Next, we reduce the guidance condition quality by randomizing robot states and other states not relevant to the target object (Underspecified Objective). Finally, we look at how we can guide the base policy in complex ways, including achieving multiple behaviors and avoiding behaviors (Multiple Objectives). Goal Conditioning. We train policy to take an additional visual observation as the goal. Following past work in visual goal conditioning [2, 10, 36, 41], we train the policy by using the last observation of each trajectory as the goal. We compare this model to DynaGuide by sampling from g+ as our goal during inference-time. Position Guidance (ITPS) [49]. Past work has achieved diffusion policy guidance that steers the robot to particular location in 3D space supplied by human. For objects that do not move, we replicate this guidance by finding the average location of the robot after accomplishing the desired behavior and using position guidance on that location. 4.1 Experiments 1 & 2: Steering in Complex 3D Environments Our first two experiments examine how DynaGuide can steer robots to particular behaviors in the CALVIN environment [30]. The CALVIN setup allows robot to interact with desks with drawers, switches, buttons, and cabinets  (Fig. 3)  . It can articulate these objects and also rearrange set of three colored cubes that are placed randomly on the table. For these two experiments, we focus on Guidance Conditions g+ that represent desired outcomes, taken as the last observation of demonstration trajectories showing the target behavior. We use these same observations for the goal-conditioned baseline by sampling g+ g+ for each rollout as the goal. In this set of experiments, we are interested in the ability of DynaGuide to steer the base policy towards behaviors that interact with the articulated table elements (ArticulatedParts) and randomized cube objects (MovableObjects). Overall, for every target behavior across both experiments, DynaGuide significantly increases the frequency of the target behavior over that of the base policy (Fig. 4, Top, Bottom-Left). On ArticulatedParts, DynaGuide boosts the base policys behavior by 8.7x and achieves an average target behavior success of 70%. Because the objects in ArticulatedParts stay in place, we can run the ITPS baseline [49]. Using position guidance boosts the target behavior over the base policy, but the reasoning abilities of the dynamics model allow DynaGuide to outperform it across the tasks (Fig. 4, Top). The location consistency of ArticulatedParts also means that the g+ outcomes closely match the true outcome of the current environment if the desired behavior is executed. Therefore, the goal-conditioned baseline is in-distribution and performs near perfectly. This is expected, as the goal-conditioned model was trained for this exact setup. The MovableObjects experiment poses new set of challenges. Instead of being fixed in location, the cube objects are small and randomly arranged every environment reset. All steering performances decrease in MovableObjects, but two notable differences arise. First, the GPC baseline performance drops to the base policy. Sampling approaches like GPC rely on actions drawn from the base policy. If these actions have high variance, then the selected actions may also have high variance. Compared to the steady guidance of DynaGuide, the increased variance leads to more erratic actions and impacts performance on precision tasks. The second difference is the drastic drop in goal conditioned performance. Unlike ArticulatedParts, the randomized cubes mean that not all outcomes in g+ are attainable in the current setup. For example, some outcomes in g+ interact with the blue cube on the left side of the table, while test-time setup might have the blue cube on the right side. This mismatch means that the goal conditioned policy is out of distribution and is affected more than DynaGuide. We explore this phenomenon in the next experiment. 6 Figure 4: Steering Ability and Robustness in the Calvin Environment DynaGuide enhances the target behavior (horizontal axis) significantly across all experiments (Section 4.1). The goal conditioning baseline performs very well on clean fixed articulated setup, but it drops steeply with lower goal quality while DynaGuide remains more robust (Section 4.2). For more precise tasks with movable cube objects, the active guidance in DynaGuide outperforms sampling-based approach with the same dynamics model (Section 4.1) 4.2 Experiment 3: Robustness to Lower Quality Guidance Conditions The cube randomization in MovableObjects created mismatch between desired outcome observation and attainable outcome in the current environment, challenge that disproportionally impacted goal conditioning over DynaGuide. While MovableObjects had an inevitable mismatch, we explore deliberate mismatch in this experiment that models more practical use case of inference-time steering. The two prior experiments had used g+ drawn from trajectories where the robot achieves the desired behavior. For each guidance condition, the combined robots position and the target objects position fully indicate the desired behavior. For practical deployment, it is easier to take pictures of the desired scene without collecting trajectory demonstrations or placing the robot in the correct position. We mimic this requirement in UnderspecifiedObjectives, where we set the target articulated table part to the desired pose and randomize all other objects, including the robot. We use images from this setup as g+. As expected, the lower quality guidance conditions pushed the goal-conditioned policy out of distribution, leading to average success rates below 10% (Fig. 4, bottom right). In contrast, the dynamics model guidance approaches (DynaGuide and DynaGuide-Sampling) were still able to increase the target behavior over base policy rates and outperformed the goal conditioned policy by 5.4x on average. DynaGuide also performed more consistently than its sampling counterpart, further illustrating the importance of active guidance. These under-specified objectives showcase the benefits of separating the policy and guidance. Although the guidance conditions are still mismatched for DynaGuide, the inputs (g+, ot, a) were all individually in-distribution to the parameterized encoder, base policy, and dynamics model of DynaGuide. The mismatch, therefore, occurs in the latent space of large visual encoder. With nothing out-of-distribution, we face an easier challenge of extracting meaningful signal from these latent distances. Fortunately, DynaGuide also makes use of all the conditions in the latent space for the guidance metric (Eq. 2). While each latent distance di might be noisy and inaccurate, the combined metric will cancel some of the noise, leading to meaningful guidance. 4.3 Experiment 4: Steering with Multi-Objectives Separating the guidance and the policy improves the robustness of DynaGuide to lower quality guidance conditions, and it also allows the guidance signals to be combined, steering the policy towards multimodal set of desired outcomesand even away from set of negative outcomes. This capacity is not possible for an unmodified goal-conditioned policy that takes single observation as the goal. Figure 5: Multiple Objectives and Underrepresented Behaviors. DynaGuide is able to steer the base policy towards multiple behaviors while minimizing other behaviors and failures (Left). DynaGuide is also able to avoid undesired behaviors by performing other behaviors successfully (Middle). On these complicated objectives and in lower data regimes (right), DynaGuide performs better than sampling approaches. In MultiObjectives, we add multiple target behaviors to g+ and populate with undesired behaviors. Good steering will allow all represented behaviors in g+ to be executed while minimizing behaviors in and overall behavior failure rate (e.g., doing nothing). Indeed, DynaGuide steers the policy towards multiple target behaviors with fair representation of each behavior (Fig. 5, Left), achieving up an average of 80% success with nearly no full behavior failures. When faced with avoiding behaviors in g, DynaGuide achieves nearly perfect success in avoiding the undesired behavior while executing variety of other behaviors (Fig. 5, Middle). Because DynaGuide and GPC both use the same dynamics model for steering, they can both steer to multiple objectives. However, the sampling approach of GPC had lower overall success of desired behaviors and allowed more execution of undesired behaviors, especially when trying to avoid outcomes in (Fig. 5, Middle). When avoiding g, GPC also produces more behavior failures where nothing is executed during the trajectory horizon. Sampling approaches like GPC rely on the base policy to readily produce actions that satisfy the objective. However, especially when the robot is close to an object, the base policy might only sample actions for one behavior, making this behavior impossible to avoid. In contrast, active guidance during the diffusion process can seek rare modes in the action distribution, leading to higher chance of accomplishing complicated objectives like the ones in this experiment. 4.4 Experiment 5: Enhancing Underrepresented Behaviors In MultiObjectives, we observed that the GPC sampling approach struggles to satisfy complicated objectives because it relies on adequate samples from the base policy. In UnderrepresentedBehaviors, we test this feature explicitly. Trained robot policies will offer behaviors based on its representation in the training data. However, when we steer policy, we also want to steer it into behavior modes that are underrepresented in the training data. To simulate data underrepresentation, we reduce the presence of Switch-On behavior in the training set of the base policy. As expected, the base policy offers more Switch-On behavior as more data is included (Fig. 5, Right). GPC also increases with more data representation, as it can boost the behavior by selecting the right action. Most importantly, DynaGuide consistently outperforms its sampling counterpart at these lower data regimes, achieving 40% success rate with only 1% of the original Switch-On behavior included in the trained base policy (Fig. 5 Right). By directly influencing the action diffusion process, DynaGuide can leverage the knowledge of the dynamics model to seek areas in the action distribution that satisfy an objective, even if they are underrepresented. 4.5 Experiment 6: Steering Off-The-Shelf Policies with Real Robot In the simulated Calvin environment, we used existing data to train both the base policy and the dynamics model. DynaGuide works with this setup, but it should also work with any pretrained diffusion policy and it should also work on real robots. To test these two claims, we set up real robot experiment leveraging publicly available pretrained policy [6] that picks up cup and places it on 8 Figure 6: Real World Experiments On cup arrangement task (Left, Center), we show that DynaGuide can guide pretrained robot policy to express preference over cup color. Using the same policy, we also show that we can create novel mouse-grabbing behavior (Right) by leveraging the additional knowledge inside the trained dynamics model. saucer. We train the dynamics model using open-source data from the UMI collection interface [6] and set of demonstrations collected on the experimental setup. For more details about the setup, refer to Appendix B.5. We conducted three experiments on this real-world setup. The first two, CupPreference and HiddenCup, present the robot with two different-colored cups equidistant from the robots starting gripper. The base policy will select cup at random and place it on the saucer. Applying DynaGuide in CupPreference creates preference for cup color (Fig. 6 Left), leading to an average target behavior success of 72.5%. In HiddenCup, the red mug is hidden behind the grey mug. The base policy will typically go for the closer grey mug, but the active guidance in DynaGuide enables the robot to find the red cup 80% of the time (Fig. 6 Middle). Inspired by the performance of DynaGuide with data representation at 1% (4.4), we tested the ability of DynaGuide to steer the pretrained policy towards novel behavior: touching computer mouse. In NovelBehavior, the dynamics model was trained on various object manipulations, including mugs and computer mice from other open-source datasets [26]. We used the same off-the-shelf base policy that was only trained to arrange mugs. Although the steered policy still expressed preference for the mug, DynaGuide was able to double the number of interactions with the novel object."
        },
        {
            "title": "5 Conclusion and Discussion",
            "content": "In this paper, we proposed novel method of steering pretrained diffusion policies by using separately trained latent dynamics model. We demonstrated the ability of DynaGuide to enhance target behavior performance across simulated and real experiments, outperforming baselines like position guidance and goal conditioning in some or all setups. Finally, we demonstrated that DynaGuide can be steered to achieve multiple behaviors, avoid undesirable behaviors, and enhance behaviors that were underrepresented in the training of the base robot policy. Real-world robot deployment requires robots to be highly steerable in their skills, and DynaGuide shows one potential avenue for this ability. 5.1 Limitations and Future Work DynaGuide can successfully steer pretrained policies towards particular objectives, but currently it is difficult to specify the method of achieving this objective. This limitation is due to the current form of guidance conditions as observations that represent desired/undesired outcomes. There are other, more complicated guidance modalities, including language and kinesthetic demonstrations. Future work includes enabling multi-modal guidance conditions, which can hopefully enable very fine-grained guidance for all parts of the trajectory. In the future, we also hope to add an ability for the base policy to remember past guidance, consolidate the knowledge, and apply it automatically to new tasks. 9 5.2 Acknowledgments Maximilian Du is supported by the Knight-Hennessy Fellowship and the NSF Graduate Research Fellowships Program (GRFP). This work was supported in part by the NSF Award #2143601, #2037101, and #2132519, and Toyota Research Institute. We would like to thank ARX for the robot hardware and Yihuai Gao for assisting with the policy deployment on the ARX arm. We appreciate Zhanyi Sun for discussions on classifier guidance and all members of the REAL lab at Stanford for their detailed feedback on paper drafts and experiment directions. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors. The Calvin Experiments (Sections 4.1 - 4.4) were made possible with the Calvin benchmark codebase [30]. The diffusion policy was adapted from the Robomimic repository [29]. The dynamics model was inspired by the Dino-WM implementation [54] and leverages representations from Dino-V2 [37]."
        },
        {
            "title": "References",
            "content": "[1] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? URL http://arxiv. org/abs/2211.15657. [2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. URL https://proceedings.neurips.cc/paper_files/paper/2017/ hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html. [3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. Pi-0: vision-language-action flow model for general robot control, . URL http://arxiv.org/abs/ 2410.24164. [4] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusion models, . URL http://arxiv.org/abs/2310.10639. [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: Robotics transformer for real-world In Robotics: Science and Systems XIX. Robotics: Science and Systems control at scale. Foundation. ISBN 978-0-9923747-9-2. doi: 10.15607/RSS.2023.XIX.025. URL http: //www.roboticsproceedings.org/rss19/p025.pdf. [6] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and Shuran Song. Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. In Robotics: Science and Systems XX. Robotics: Science and Systems Foundation. ISBN 979-8-9902848-0-7. doi: 10.15607/RSS.2024.XX.045. URL http://www.roboticsproceedings.org/rss20/p045.pdf. [7] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin CM Burchfiel, and Shuran Song. Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. In Proceedings of Robotics: Science and Systems, Daegu, Republic of Korea, July 2023. doi: 10.15607/RSS.2023.XIX.026. 10 [8] Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar, Percy Liang, and Dorsa Sadigh. No, to the right: Online language corrections for robotic manipulation via shared autonomy. In Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction, HRI 23, pages 93101. Association for Computing Machinery. ISBN 978-14503-9964-7. doi: 10.1145/3568162.3578623. URL https://dl.acm.org/doi/10.1145/ 3568162.3578623. [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synIn Advances in Neural Information Processing Systems, volume 34, pages 8780 thesis. 8794. Curran Associates, Inc. URL https://proceedings.nips.cc/paper/2021/hash/ 49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html. [10] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation learning. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. URL https://proceedings.neurips.cc/paper_files/paper/2019/ hash/c8d3a760ebab631565f8509d84b3b3f1-Abstract.html. [11] Maximilian Du, Alexander Khazatsky, Tobias Gerstenberg, and Chelsea Finn. To err is robotic: Rapid value-based trial-and-error during deployment. URL http://arxiv.org/abs/2406. 15917. [12] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. URL http://arxiv.org/abs/1812.00568. [13] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 27862793. doi: 10. 1109/ICRA.2017.7989324. URL https://ieeexplore.ieee.org/document/7989324/. [14] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. . URL http://arxiv.org/abs/1912.01603. [15] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models, . URL http://arxiv.org/abs/2010.02193. [16] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse control tasks through world models. 640(8059):647653, . ISSN 1476-4687. doi: 10.1038/ s41586-025-08744-2. URL https://www.nature.com/articles/s41586-025-08744-2. Publisher: Nature Publishing Group. [17] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018. [18] Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control. In ICML, 2022. [19] Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous control, 2024. [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. URL http://arxiv.org/ abs/2207.12598. [21] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In Proceedings of the 5th Conference on Robot Learning, pages 9911002. PMLR. URL https://proceedings.mlr.press/v164/jang22a.html. ISSN: 2640-3498. [22] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, 2022. [23] Gi-Cheon Kang, Junghyun Kim, Kyuhwan Shim, Jun Ki Lee, and Byoung-Tak Zhang. CLIPRT: Learning language-conditioned robotic policies from natural language supervision. URL http://arxiv.org/abs/2411.00508. [24] Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic laIn Adtent actor-critic: Deep reinforcement learning with latent variable model. vances in Neural Information Processing Systems, volume 33, pages 741752. Curran Associates, URL https://proceedings.neurips.cc/paper/2020/hash/ 08058bf500242562c0d031ff830ad094-Abstract.html. Inc. [25] Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. URL http://arxiv.org/abs/2503.00200. [26] Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, and Yang Gao. Data scaling laws in imitation learning for robotic manipulation. arXiv preprint arXiv:2410.18647, 2024. [27] Huihan Liu, Yu Zhang, Vaarij Betala, Evan Zhang, James Liu, Crystal Ding, and Yuke Zhu. Multi-task interactive robot fleet learning with visual world models. In 8th Annual Conference on Robot Learning (CoRL), 2024. [28] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In Proceedings of the Conference on Robot Learning, pages 11131132. PMLR. URL https://proceedings.mlr.press/v100/ lynch20a.html. ISSN: 2640-3498. [29] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martín-Martín. What matters in learning from offline human demonstrations for robot manipulation. In arXiv preprint arXiv:2108.03298, 2021. [30] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. CALVIN: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. URL http://arxiv.org/abs/2112.03227. [31] Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample-efficient world models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=vhFu1Acb0xb. [32] Utkarsh Aashu Mishra, Shangjie Xue, Yongxin Chen, and Danfei Xu. Generative skill chaining: Long-horizon skill planning with diffusion models. In Proceedings of The 7th Conference on Robot Learning, pages 29052925. PMLR. URL https://proceedings.mlr.press/v229/ mishra23a.html. ISSN: 2640-3498. [33] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. URL http://arxiv.org/abs/1807.04742. [34] Mitsuhiko Nakamoto, Oier Mees, Aviral Kumar, and Sergey Levine. Steering your generalists: Improving robotic foundation models via value guidance. Conference on Robot Learning (CoRL), 2024. [35] Kensuke Nakamura, Lasse Peters, and Andrea Bajcsy. Generalizing safety beyond collisionavoidance via latent-space reachability analysis. 2025. [36] Iman Nematollahi, Branton DeMoss, Akshay Chandra, Nick Hawes, Wolfram Burgard, and Ingmar Posner. Lumos: Language-conditioned imitation learning with world models. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Atlanta, USA, 2025. URL http://ais.informatik.uni-freiburg.de/publications/papers/ nematollahi25icra.pdf. [37] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. URL http://arxiv.org/abs/2304.07193. 12 [38] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. Zero-shot visual imitation. In ICLR, 2018. [39] Han Qi, Haocheng Yin, Yilun Du, and Heng Yang. Strengthening generative robot policies through predictive world modeling. URL http://arxiv.org/abs/2502.00622. [40] Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning from images with latent space models. In Proceedings of the 3rd Conference on Learning for Dynamics and Control, pages 11541168. PMLR. URL https://proceedings. mlr.press/v144/rafailov21a.html. ISSN: 2640-3498. [41] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal conditioned imitation learning using score-based diffusion policies. In Robotics: Science and Systems, 2023. [42] Jan Robine, Marc Höftmann, Tobias Uelwer, and Stefan Harmeling. Transformer-based world models are happy with 100k interactions. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=TdBaDGCpjly. [43] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with learned model. 588(7839):604609. ISSN 1476-4687. doi: 10.1038/s41586-020-03051-4. URL https://www.nature.com/articles/s41586-020-03051-4. Publisher: Nature Publishing Group. [44] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, and Chelsea Finn. Hi robot: Open-ended instruction following with hierarchical vision-language-action models. URL http://arxiv.org/abs/2502.19417. [45] Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, and Chelsea Finn. Yell at your robot: Improving on-the-fly from language corrections. arXiv preprint arXiv: 2403.12910, 2024. [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. URL http://arxiv.org/abs/2010.02502. [47] E. Todorov and Weiwei Li. generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems. In Proceedings of the 2005, American Control Conference, 2005., pages 300306 vol. 1. doi: 10.1109/ACC.2005.1469949. URL https://ieeexplore.ieee.org/document/1469949. ISSN: 2378-5861. [48] Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, and Tommaso Biancalani. Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review. URL http://arxiv.org/abs/2501.09685. [49] Yanwei Wang, Lirui Wang, Yilun Du, Balakumar Sundaralingam, Xuning Yang, Yu-Wei Chao, Claudia Perez-DArpino, Dieter Fox, and Julie Shah. Inference-time policy steering through human interactions. URL http://arxiv.org/abs/2411.16627. [50] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Ken Goldberg, and Pieter Abbeel. Daydreamer: World models for physical robot learning. Conference on Robot Learning, 2022. [51] Xiaomeng Xu, Huy Ha, and Shuran Song. Dynamics-guided diffusion model for sensor-less robot manipulator design. In 8th Annual Conference on Robot Learning. [52] Ruihan Zhao, Ufuk Topcu, Sandeep Chinchali, and Mariano Phielipp. Learning sparse control tasks from pixels by latent nearest-neighbor-guided explorations, . URL http://arxiv.org/ abs/2302.14242. [53] Tony Z. Zhao, Jonathan Tompson, Danny Driess, Pete Florence, Seyed Kamyar Seyed Ghasemipour, Chelsea Finn, and Ayzaan Wahid. ALOHA unleashed: simple recipe for robot dexterity. In Proceedings of The 8th Conference on Robot Learning, pages 19101924. PMLR, . URL https://proceedings.mlr.press/v270/zhao25b.html. ISSN: 2640-3498. 13 [54] Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. DINO-WM: World models on pre-trained visual features enable zero-shot planning. URL http://arxiv.org/abs/2411. 04983. [55] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: Vision-language-action models transfer web knowledge to robotic control. In Proceedings of The 7th Conference on Robot Learning, pages 21652183. PMLR. URL https://proceedings.mlr.press/v229/zitkovich23a.html. ISSN: 2640-3498. 14 Additional Experiments, Visualizations, and Ablations A.1 Steering in Simple 2D Block Task Figure 7: The BlockTouch Environment. This 2D environment requires the agent (black circle) to navigate to colored target (square). The base policy picks an arbitrary target, and DynaGuide steers it towards particular color (A). This steering works to express square color preference (B). The dynamics model also enables the base policy to accomplish novel tasks not explicitly trained on the base policy, including picking target from tight cluster of squares (C) and navigating past three close cubes to far target (D). In visuals - D, the distribution of squares are indicated by the shaded regions. The average square location is represented by the solid square. As an additional way of understanding how guidance works in DynaGuide, we look at toy BlockTouch environment. In BlockTouch, the agent navigates 2D environment and touches colored square. We train the vision-based base policy and dynamics model on synthetic data that shows navigation to squares in any location of any color. Then, during test time, we use the dynamics model to guide the policy towards cubes of one particular color. In Figure 7A, we visualize the creation of an action chunk under dynamics guidance. In the initial noisier steps, the guidance signal and the denoising policy signal compete for the actions direction. In the example, the agent is nearly touching the yellow square, so the base policy wants to denoise the action towards the yellow square. The guidance wants to move the agent towards the blue square. The vector sum of these two forces pushes the actions towards the blue square. In the later stages of denoising, the guidance and the policy signals start to work together to craft the final action sequence that points to the blue square. This guidance works in situations where early decisions are important for hitting the correct color (Figure 7B) and in situations where late decisions are important (Figure 7C). Even though the dynamics and base policies are both trained on random navigation behavior, we can leverage the dynamics model to accomplish novel tasks not explicitly present in the base policy. The Late Decision setup (Figure 7C) contains squares that are closer together than the training data, requiring high navigational precision. The Go to Furthest Cube setup (Figure 7D) places the target much further away than three squares, requiring deliberate navigation around the closer squares. The base policy goes to the closer squares, but the dynamics model was able to reliably guide the policy between the squares and to the target. Note that the visuals in B-D show square distributions and the black dot at the yellow end of the trajectory shows the first and only contact with square in the environment. A.2 Ablations and Hyperparameter Investigation To understand how various hyperparameters and components of DynaGuide contribute to the final success rate, we run set of experiments on the Switch-On task in the ArticulatedParts experiment. In general, we observe that the two main hyperparameters σ (Eq. 2) and (Eq. 3) are robust to reasonable changes, meaning that DynaGuide is not difficult to tune. At very low values of s, success rates are lower because there is not enough guidance. Success rates increase with higher strength until the strength becomes too high and creates instability. Very low values of σ decreases guidance success by making the guidance conditions too far away for any meaningful signal. Increasing the number of stochastic sampling steps generally increases the performance of guidance at cost of 15 computational expense. Surprisingly, DynaGuide is very robust to the number of guidance conditions , achieving comparable guidance even with one guidance condition. In practice, we use guidance strength and σ chosen from gridsearch for each experiment, although the result of the gridsearch generally yields [20, 40]. On new task, it is sufficient to pick starting values of s, σ from these ranges and adjusting them based on performance. We use stochastic sampling = 4 for our experiments as balance of stability and computation efficiency. We use 20 guidance conditions per task, which is more than sufficient for Switch-On but is more important for underspecified goals (4.2) and harder objectives. [1.2, 1.8] and σ Critically, we discover that pretraining the dynamics model with noised action is essential for performance  (Fig. 9)  . This makes sense as the noise would otherwise be out of distribution. Figure 8: Hyperparameter sweeps and ablations. We look at the impact of inference-time hyperparameters and noise pretraining on final performance. For guidance strength, we hold σ = 40, = 4, where is the number of stochastic sampling steps. For σ parameter, we hold = 1.5, = 4 where is guidance strength. For Stochastic Sampling, we hold = 3, σ = 40. We use higher to demonstrate the impact of stochastic sampling on stability. For Guidance Conditions we use = 1.5, σ = 40, = 4."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 DynaGuide Implementation Details Base Diffusion Policy The diffusion policy is trained to take 2-step history stack of visual observations and robot proprioception and predict chunk of 16 actions. It uses Resnet-18 image encoder that conditions U-Net with 4 encoding and 4 decoding layers. This U-Net predicts the noise during training using the standard diffusion policy objective [7]. During inference, we use the noise predictor with the DDIM noise scheduler to craft the generated action chunk. Together, the diffusion policy has around 18 million parameters. We use the Adam optimizer with learning rate of 1e-4. We train the model for 200k gradient steps with batch size of 16. All parts of the diffusion policy are trained together using expert data. During execution, 14 actions are executed in the environment open-loop before the policy is queried again. Figure 9: Ablating Noise Pretraining. Removing the dynamic models exposure to noised actions greatly decreases its ability to steer the action diffusion process. Dynamics Model. We use the DinoV2 patch embeddings as the latent space, which provides 256 patches of embeddings with size 384. While this is large embedding size (256 384), past work has shown the effectiveness of this exact embedding in robot planning [54]. Empirically, both in past work and in our development, we discovered that the CLS token in DinoV2 (1 384) does not carry as much spatial information needed for meaningful latent guidance. For the latent dynamics model, we use 6-layer Transformer Encoder with 8 heads that takes in the 256 patches, token for robot proprioception, and an action token. This action token is created by embedding each action in ak using the same MLP and concatenating the embeddings into the token. We use learned position embedding. The first 256 outputs of the transformer are taken as the predicted future state. We regress these patches to the DinoV2 embeddings of the last (or later) observation of the trajectory. For the CALVIN experiment, we regressed to the final latent of the 16 trajectory. For the real-world experiment, we regressed to the latent of the observation after executing the proposed chunk of actions in the environment. We use the Adam optimizer at learning rate of 1e-4. Because the DinoV2 embedding is frozen, the transformer can be trained end-to-end using regression objective. This transformer has approximately 16 million parameters. We train the model for 600k gradient steps with batch size 16, point past model convergence. During development, we showed that validation performance depends strongly on the actions presence, indicating that the dynamics model has learned to listen to actions. To account for noisy actions during guidance, we add noise to the actions used to train the dynamics model. 50% of the fed actions are noiseless, and 50% are noised based on the same DDIM scheduler used during inference. We select the noising step using geometric random variable with an expected noise step of 20. The scheduler starts at step 100 with pure gaussian noise, so at step 20, it is noisy but still meaningful action that the dynamics model can use. Inference-Time Steering. Before inference-time, we take the guidance conditions and pre-compute their embeddings. Then, during inference-time, we compute the objective (Eq. 2). Empirically, the Euclidean distance yields more stable results in lieu of squared distance in Eq. 2. We backpropagate this metric through the dynamics model and use the gradient with respect to the input actions as the guidance signal. We find α using the DDIM scheduler. To implement stochastic sampling, we repeat the same denoising step multiple times within the denoising loop. Compute Hardware. All policies and dynamics models were trained on single RTX 3090 GPUs with 24GB VRAM, taking 24-48 hours to convergence. The dynamics model is 15M trainable paramters, which takes up 4GB of GPU memory during training and inference. All experiments were conducted on single RTX 3090 GPUs taking 10-20 minutes per seed per task. Development of the method and experiments were also conducted on RTX 3090 GPUs, with the total compute-hours for development estimated at 10-20 times the results shown in the paper. Most of this compute was spent developing the dynamics model. For more details on the exact implementation, refer to the submitted code. B.2 Implementation of Baselines Goal Conditioning. We use the provided state-conditioning implementation in Robomimic [29] and condition the diffusion policy on an additional input consisting of the final observation in the trajectory. We train this policy using the same hyperparameters as the base diffusion policy. Position Guidance (ITPS) [49]. The ITPS algorithm excels at steering the policy towards target point in 3D specified by human. To compare ITPS to our approach that does not require human specification, we manually compute the average final position of the robot for set of 20 trajectories showing the desired behavior. We then use this position as the target point in ITPS. Sampling (GBC) [39]. Previous work that introduced GBC-Rank used an objective function to rank action samples from policy to select the best sample. We take that idea and apply it to our guidance metric (Eq. 2). We sample the base policy 5 times per inference and pick the best action to execute based on the metric. B.3 Distance Metric: Derivation and Design Choices In this section, we will conduct more detailed derivation of the metric shown in Eq. 2. While other works [54] simply assume the metric to be Euclidian, we use probabilistic motivation to understand the log-sum-exp, which we discovered empirically to be very important for the performance of DynaGuide. For the first part of this derivation, we will only talk about the desired outcomes z+, and the undesired outcomes follow by negation. We start from the very rough approximation that the latent space is Gaussian with diagonal variance Σ. It is an approximation that methods using squared latent distances already implicitly make. Here, the probability p(z+ , µ = ˆzt+H , Σ = σI). The log probability can be computed as follows: ˆzt+H ) = (z+ 17 log p(z+ ˆzt+H ) = log 1 (2π)n/2 exp (cid:112) Σ ˆzt+H )T Σ = 1 2 (z+ (cid:18) 1 2 (z+ ˆzt+H )T Σ 1(z+ (cid:19) ˆzt+H ) 1(z+ ˆzt+H ) + log (cid:124) 1 (2π)n/2 (cid:123)(cid:122) Constant (cid:112) Σ (cid:125) = (z+ ˆzt+H )T (z+ 1 2σ 1 2σ There are different ways of combining the influence of multiple z+ does not work is adding the log-probabilities together: z+ ˆzt+H ) + ˆzt+H 2 2 + = in z+. One way that empirically (cid:88) = 1 2σ z+ ˆzt+H 2 2 This is not surprise, as adding log-probabilities is equivalent to multiplying the probabilities together. As previously discussed, not all the z+ are achievable and some may be mutually exclusive. For example, in the multi-objective setup (4.3), some z+ might correspond to pressing the button and others to opening the drawer. The influence of any single z+ is too powerful in this setup because of the product. In contrast, it is more sensible to add the probabilities. This way, changes that generally increase the likelihood of the z+ will increase the metric, even if it decreases the likelihood of some other competing z+ s. To add the probabilities, we can do the following: log p(z+ ˆzt+H ) = log = log = log (cid:88) (cid:88) (cid:88) ˆzt+H ) p(z+ (cid:18) exp 1 2σ z+ ˆzt+H 2 2 + (cid:19) exp(C) exp (cid:18) 1 2σ z+ (cid:19) 2 2 ˆzt+H (cid:19) = + log (cid:18) (cid:88) exp 1 2σ z+ ˆzt+H 2 2 Which creates an intuitive result, as the log-sum-exp is soft maximum. The metric focuses on the distances that are closer to the desired outcome. The value of σ modulates the sharpness of the soft maximum. We compute the same metric for z. Because we want results in to be mutually exclusive to z+, we want to divide the probabilities: = p(z+) p(z) Intuitively, it means that there is strong gradient incentive to push p(z) as small as possible, which is not as strong if we were to use p(z+) p(z). We observe the benefits of the division empirically. This setup corresponds to subtracting the undesirable outcome metric from the desirable outcome metric. To create the final d, we absorb the constant 2 into σ and discard the constant C. This gives us the expression in Eq. 2: = log (cid:34) (cid:88) ϕ(g+ ) exp hθ(ϕ(ot), a) σ 2 2 (cid:35) log (cid:88) ϕ(gj ) exp hθ(ϕ(ot), a) σ 2 2 18 Empirically, we discover that Eq. 2 works best with the squared L2 distance substituted with Euclidean distance. This deviation from the theoretical result is best attributed to the Gaussian latent assumption of DinoV2 not being fully true to the real embedding space. B.4 Experimental Setup: Simulation CALVIN Environment: Data. Although CALVIN is used for benchmarking, we just use the CALVIN tasks for the data and our own sets of experiments. The CALVIN data is provided as continuous set of transitions. We used privileged state information to segment these transitions into trajectories showing one behavior per trajectory: switch on, switch off, drawer open, drawer close, door left, door right, button on, button off, red touch, red displace, blue touch, blue displace, pink touch, pink displace. We did our own segmentations to be consistent with our evaluation criteria (See later paragraphs). The segmentations were also important to extract end observations for the goal-conditioning baseline and training the dynamics model. CALVIN Environment: Base Policy. We use the CALVIN-D dataset to train the base policy. We use third-person observation, wrist camera observation, and full robot proprioception for the base diffusion policy. Because the demonstrations show large variety of behavior, the trained base policy also offers wide variety of behaviors. CALVIN Environment: Dynamics Model. Because the dynamics model can take wider range of data, we train the dynamics model on the CALVIN-ABCD dataset, which is the full data split provided by the benchmark. We discover that adding the non-relevant environments improved convergence and reduced overfitting. We use the third-person observation and full robot proprioception as inputs to the dynamics model. The short tasks horizon of the CALVIN tasks meant that it was advantageous to train the dynamics model to predict the final state observation of the single task trajectory. We use the trajectory segmentations to obtain the final observation target during training. CALVIN Environment: Evaluations. We segment the provided validation CALVIN dataset and randomly select 20 trajectories per desired behavior to extract g+, using the last state observation (except for the UnderspecifiedObjectives experiment). For each evaluation, we perform 50 trials with horizon of 400. We use privileged state information to monitor the object interactions and we stop the rollout when an object is sufficiently articulated or moved. If no object is sufficiently moved or articulated after 400 steps, we count the trial as failure (no behavior). For each target behavior (horizontal axis in Fig. 4), we compute the success rate by finding the frequency of trials that show the target behavior. We reset the robot randomly by sampling starting pose in validation set of trajectories. All error bars in simulation results (Figures 4, 5) are 1-sigma error bars. The standard error is calculated by evaluating DynaGuide on six policies trained on individually separate train-validation splits of the Calvin dataset. Because each of the six success rates is already an average across 100 trials, we use the standard error of the mean as the sigma. We assume Normally-distributed errors, so the standard error is computed with the standard σ/n formula where σ is calculated through Numpy function. For the line plot (Fig. 5 Right), the shaded regions were computed as standard error of the mean of six success ratesthe same as the bar graphs. ArticulatedParts Experiment. To count as being sufficiently articulated, we require buttons and switches to be fully pressed/flipped such that the light changes state. For drawers and doors, they must be articulated past halfway from the starting location to their end location. MovableObjects Experiment. In this task, we focused on the ability to steer the policy towards the colored cubes. To be successful, we required the robot to be touching the cube and displace it slight distance, which can be possible by lifting the cube or nudging it. UnderspecifiedObjectives Experiment. In this task, we used the ArticulatedParts experiments but fed all the guidance approaches with lower quality guidance condition. Instead of taking the observations from the final states of validation trajectories, we manually set the target object to the desired pose (e.g. drawer open, or door to the left). We sample states for all the other objects, and we randomize the robot by setting it to start pose sampled from the validation trajectories. The critical difference between these g+ and the g+ used in the other experiments is that the robot is no longer shown directly interacting with the object of interest. Even though the robot position is randomized, it is still possible to figure out the target behavior because it is the one object state that stays constant between conditions. MultipleObjectives Experiment. This task, we extracted guidance conditions from validation trajectories that represented multiple types of behaviors. We are not interested in chaining the behaviors, but rather steered policy that offer multiple desired behaviors with comparable frequency while avoiding the undesired behaviors. During evaluation, we still terminated the rollout after single behavior and we compute the success rates in the same way as all previous experiments. However, instead of reporting single success rate per task, we report the whole task distribution (Fig. 5, Left). UnderrepresentedBehaviors Experiment. This task required retraining the base policies with modified training data. Using privileged state information to get trajectory behavior labels, we intentionally removed Switch-On behavior from the training set of the base diffusion policy to create sets where 1%, 2%, 5%, 10%, 20%, 40%, 60%, and 80% of the original Switch-On data was kept. BlockTouch Experiments. We collect synthetic data of the agent navigating to randomly selected square by creating bezier curve with 0-2 intermediate points. This bezier curve means that the dynamics model cant infer the final destination with perfect accuracy based on the current direction of travel. Instead of using the metric d, we directly train the dynamics model to classify the output square color as 4-dimensional vector categorical distribution. During inference-time, we use cross entropy metric between this distribution and one-hot vector representing the desired square color. During training and all data collection, we randomize the location of the squares. During test-time, we introduce specific square arrangements to test the properties of the guidance In the EarlyDecision test, we still randomize the squares but ensure that the cubes stay in their own regions, forcing the agent to make an early directional decision. In LateDecision, we keep the squares close together, and in Go to Furthest Cube, we always have the blue square on the opposite side of the environment as the starting agent location. This agent must move past the three other squares to find the blue square. We terminate the environment once any square has been touched. B.5 Experimental Setup: Real Robot Base Policy. We use publicly available trained diffusion policy provided by the Universal Manipulation Interface repository (github link). It was trained on 2k trajectories of cup grasping, reorientation (to move the handle to the left), and placement on saucer. The policy takes images from gopro with Max Lens mod and outputs 16 actions per generation. These relative actions represent the change in the robot from its current position [6]. This policy has mostly seen single cups and saucers in the environment, but when provided with multiple cups, it will choose cup at random. To demonstrate DynaGuide on existing policies, we do not modify this policy. Although the policy offers random choice, it also has bias towards the left side of the environment. To account for this bias, we randomize the location of the desired cup in CupPreference. We also place the computer mouse in the area opposite to this bias in NovelBehavior to ensure that all improvements are due to steering and not base policy bias. Data. We train the dynamics model on the open-source cup rearrangement data provided by the Universal Manipulation Interface repository (data link). Because we want to steer the policy towards one of two cups, we need to train the dynamics model on these decisions. Using the Universal Manipulation Interface hardware, we collect 500 demonstrations that shows two cups and one saucer in the experiment scene. We pick cup and place it on saucer. We use different cups and saucers, with no correlation between cup pairs and saucers. We combine this data with the existing cup arrangement data for the dynamics model. For the NovelBehavior experiment, we additionally train the dynamics model on 3648 publicly available trajectories of computer mouse rearrangement [26], which gives it the experience needed to steer the base policy towards the computer mouse. Setup. We use an ARX5 robot arm equipped with soft Fin Ray fingers and gopro with Max Lens mod that mimics the setups used to collect base policys training data. We use the hardware controller stack provided by the Universal Manipulation Intervace adaptation for ARX5 arms (Github Link) and power supply with overcurrent protection for safety during deployment. Evaluations. We conduct 20 trials for each base/guided policy. Real-world evaluations are very resource intensive, so we did not create confidence intervals. This is very common for real robot results, even for large projects [5, 55]. CupPreference Experiment. We arrange the red and grey cups such that the starting distance to either cup is roughly equidistant from the robot. We randomize this distance by sometimes placing 20 the two cups close, and other times placing the two cups far away. We place the saucer randomly in the environment. For 50% of the trials, the red cup is on the left, and the other 50% the red cup is on right. We measure success as picking up cup and placing it on the saucer. HiddenCup Experiment. We arrange the red cup in front of the grey cup, such that the grey cup is always closer to the robot than the red cup. We randomize the distance between the red and grey cup, as well as the overall location of the two cups from the robot. We randomly place saucer in the environment, although we count cup grasp as success without needing this saucer. We discovered that the two cup setup in this configuration is out of distribution enough for the base policy to increase the mistake of cup placement. Because we are trying to steer for cup color preference, the act of grabbing the cup is sufficient for this experiment. The previous experiment tested full behavior success. NovelBehavior Experiment. Like CupPreference, we arrange the red cup and black computer mouse equidistant from the robots starting location. As previously mentioned, we place the mouse in the area opposite of its side bias to ensure that the observed effects are attributed to steering and not base policy bias. Like in HiddenCup, we count grasp attempt of the computer mouse as success. This is because the computer mouse is very out of distribution for this base policy and it will not successfully grab the mouse. What matters in this experiment is being able to reach for novel object through dynamics guidance."
        },
        {
            "title": "C Broader Impact",
            "content": "DynaGuide contributes to the field of robotics by improving the ability for pretrained policies to be steered without retraining, potentially reducing energy consumption otherwise needed during retraining. Adding steering onto pretrained policies also potentially increases accessibility for labs that would otherwise be unable to retrain large policies. This steering approach can also be an effective way of removing unwanted or problematic biases in trained robot policies after the training process. The dynamics model is trained on data that is either publically available or easy to collect, and the data contains no private or sensitive information. Because DynaGuide allows steering of existing policies, there is risk of bad actors steering offthe-shelf policies to do dangerous or malicious behaviors, risk also present for other generative models. To mitigate these risks, trainers of the base policy can ensure that malicious behaviors are not represented at all. Knowing the mechanism of DynaGuide, it may also be possible to train base policies to act adversarially to the guidance signal if asked to do malicious behaviors, making the guidance as difficult as retraining the policy from scratch. Code, Assets, and Licenses All our models are trained on publicly available data and leverages codebases and assets under these licenses: CALVIN Environment and Data (MIT License) [30] Robomimic Codebase (MIT License) [29] DinoV2 Model (Apache license) [37] Universal Manipulation Interface Codebase, pretrained model, and data (MIT License) [6, 26] Our code will be publicly released under the MIT license in the near future. Provided in the supplemental material is barebones version of the code."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}