{
    "paper_title": "WorldPM: Scaling Human Preference Modeling",
    "authors": [
        "Binghai Wang",
        "Runji Lin",
        "Keming Lu",
        "Le Yu",
        "Zhenru Zhang",
        "Fei Huang",
        "Chujie Zheng",
        "Kai Dang",
        "Yang Fan",
        "Xingzhang Ren",
        "An Yang",
        "Binyuan Hui",
        "Dayiheng Liu",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang",
        "Yu-Gang Jiang",
        "Bowen Yu",
        "Jingren Zhou",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations."
        },
        {
            "title": "Start",
            "content": "2025-05-16 WorldPM: Scaling Human Preference Modeling Binghai Wang2,3, Runji Lin3, Keming Lu3, Le Yu3, Zhenru Zhang3, Fei Huang3, Chujie Zheng3, Kai Dang3, Yang Fan3, Xingzhang Ren3, An Yang3, Binyuan Hui3, Dayiheng Liu3, Tao Gui1,2, Qi Zhang1,2, Xuanjing Huang1,2, Yu-Gang Jiang1,2, Bowen Yu3, Jingren Zhou3, Junyang Lin3 1Institute of Trustworthy Embodied Artificial Intelligence, Fudan University 2School of Computer Science, Fudan University 3Qwen Team, Alibaba Group https://github.com/QwenLM/WorldPM"
        },
        {
            "title": "Abstract",
            "content": "Motivated by scaling laws in language modeling that demonstrate how test loss scales as power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling (WorldPM) to emphasize this scaling potential, where World Preference embodies unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPMs scalability potential; (3) Subjective metrics (subjective preferences from limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations. 5 2 0 2 5 1 ] . [ 1 7 2 5 0 1 . 5 0 5 2 : r Figure 1: Test loss analysis across domains for 1.5B-72B models with increasing training data. The x-axis is scaled logarithmically (base 2). Work done during the internship at Qwen. Corresponding authors: tgui@fudan.edu.cn, feihu.hf@alibaba-inc.com, yubowen.ybw@alibaba-inc.com. Work done while working at Qwen."
        },
        {
            "title": "Introduction",
            "content": "Kaplan et al. (2020) demonstrates that language modeling performance exhibits scaling laws, where cross-entropy loss scales as power law with model size, dataset size, and the amount of compute. These laws has achieved remarkable success in next-token prediction tasks, continuously advancing the capabilities of language models (Bai et al., 2023; Yang et al., 2024a;b). We argue that such scaling laws can be extended to preference modeling and thus propose World Preference Modeling (WorldPM), which aims to achieve unified representation of preferences by leveraging these laws. key challenge to this investigation lies in the scalability constraints of manually annotated preference data (Wang et al., 2024c; Bai et al., 2022b; Lee et al., 2023). As platforms that facilitate large-scale interaction and opinion sharing, online forums aggregate vast amounts of user preference data through voting mechanisms, substantially advancing research on collective decision making in social sciences and psychology (Easley et al., 2010; Pal et al., 2012; Hu et al., 2008). Despite the immense value of these preference signals, they have rarely been effectively leveraged at scale by artificial intelligence. In one of the few efforts, Askell et al. (2021) utilized forum data for Preference Model Pre-training (PMP). However, they mainly focus on how PMP improves sample efficiency during the preference fine-tuning process, leaving the scaling properties of preference pre-training itself unexplored. We first collect preference data from public forums, including StackExchange, Reddit, and Quora. After assessing the quality of different data sources, we find that StackExchange exhibits the best generalization and cross-domain transfer capabilities. Therefore, we select preference data from StackExchange as the representative source for WorldPM. We conduct large-scale preference training with 15 million training samples on language models with parameters ranging from 1.5B to 72B. Subsequently, we evaluate the test loss across various model sizes and training scales using 12 test sets, broadly classified into adversarial, objective, and subjective categories based on the capabilities they assess. The main results in Figure 1 prove that WorldPM displays distinct scaling patterns across different domains. Specifically: (1) In adversarial evaluation, test losses demonstrate power law decrease, underscoring the models enhanced ability to detect responses containing intentional errors, as well as those that are well-written but irrelevant or incomplete. This finding suggests that subtle deficiencies commonly plaguing discriminative models (Park et al., 2024; Zeng et al., 2024) can be effectively mitigated through large-scale training. (2) The objective evaluation reveal an emergent scaling phenomenon, where larger models demonstrate power law reduction in test losses across more benchmarks, whereas smaller models show limited or no such improvements. These results suggest that WorldPM represents challenging task that requires larger models to elicit preferences for objective knowledge, pointing to its substantial potential for further advancement. (3) Subjective evaluations show no scaling trends. We analyze potential reasons from the perspective of style preferences (e.g., preference for longer or richer responses). While WorldPM becomes more style-neutral as it scales up, some subjective evaluations exhibit style preferences, resulting in lower evaluation performance. Similar phenomena have been observed in many other studies (Dubois et al., 2024; Park et al., 2024; Chiang et al., 2024). We further explore the potential of WorldPM as foundation model to facilitate preference fine-tuning. Unlike Askell et al. (2021)s initial work that was limited to small datasets and in-distribution testing, we conduct comprehensive experiments using three human preference datasets (7K, 100K and 800K samples) and evaluate the model on multiple benchmarks. Results demonstrate that WorldPM significantly improves the generalization capabilities of preference fine-tuning, achieving performance gains exceeding 5% on many key subtasks. Notably, the improvements are more pronounced when the amount of preference data is limited, with some subtasks showing gains of over 10%. We also reveal positive correlation between the scale of WorldPM and the benefits obtained from fine-tuning, indicating scalable advantages as training scale increases. Finally, we integrate WorldPM into our model training pipeline through preference fine-tuning and Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Results show substantial improvements in both in-house and open-sourced alignment evaluations compared to baselines without WorldPM. Notably, our in-house evaluations reveal significant gains ranging from 4% to 8%."
        },
        {
            "title": "2 Background & Related Work",
            "content": "2.1 Reinforcement Learning from Human Feedback (RLHF) Reinforcement Learning from Human Feedback (RLHF) has emerged as critical technique for aligning large language models (LLMs) with human preferences (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Dubey et al., 2024; Yang et al., 2024a;b). The framework typically involves two key stages: reward modeling and reinforcement learning optimization (Bai et al., 2022a; Zheng et al., 2023b). As it focuses on human preferences, reward modeling is also known as preference modeling (PM) (Ouyang et al., 2022). We use both terms interchangeably in this work. Reward Modeling: reward model (RM) is trained using pairwise comparison data. For each training instance (x, y0, y1), we define preference label {0, 1} where = indicates yi is the preferred/chosen response (while the other is rejected). The reward model computes scores rθ(x, y0) and rθ(x, y1) for each response, where rθ denotes the reward function parameterized by θ. Following the Bradley-Terry (BT) model (Bradley and Terry, 1952), the probability of y0 being preferred over y1 given prompt is: P(Y = 0x, y0, y1) = sigmoid(rθ(x, y0) rθ(x, y1)), (1) Typically, the last decoding layer of an LLM is replaced with linear layer that maps the hidden state of the last token to scalar value. The training objective minimizes the negative log-likelihood of human preference data, known as the BT loss (Sun et al., 2024): LBT = (x,y0,y1,Y)D[log P(Yx, y0, y1)], (2) where represents the dataset of preference pairs. Reinforcement Learning: The trained RM guides policy optimization through RL algorithms like GRPO (Shao et al., 2024), which optimizes the policy πϕ by maximizing expected rewards while minimizing KL divergence from reference policy πref: max ϕ xDprompt,{yi}G i=1 πϕ(x) (cid:34) 1 G i=1 rθ(x, yi) µ σ βDKL(πϕ(yx)πref(yx)) , (3) (cid:35) where rθ (x,yi)µ computes the relative advantage of responses within the group, and DKL constrains policy optimization from deviating too far from the initial model through KL divergence. σ 2.2 Best-of-N Sampling as an Alignment Alternative Best-of-N (BoN) sampling provides simple yet effective alternative to RL-based alignment (Xu et al., 2024; Gao et al., 2023). The method operates as follows: 1. Responses Sampling: For given prompt x, generate candidate responses {y1, ..., yN} using policy model (e.g., SFT model). 2. Reward Ranking: Score all candidates using reward model rθ(x, y). 3. Response Selection: Output the response with the highest reward score: = arg maxyi rθ(x, yi). BoN decouples alignment from complex RL-based optimization, offering stability and reproducibility by directly leveraging RM rankings. Empirical studies (Gao et al., 2023; Coste et al., 2023; Ivison et al., 2024; Li et al., 2023) demonstrate that BoN achieves competitive performance with RLHF while avoiding optimization instability. 2.3 Preference Model Pre-training Given the high cost of preference annotation, Askell et al. (2021) propose using large-scale public forum data like StackExchange for Preference Model Pre-training (PMP), making several key findings: (1) PMP improves sample efficiency in small-scale preference fine-tuning; (2) PMP datasets are transferable to different fine-tuning datasets; (3) binary preference modeling outperforms rank modeling. Building upon these findings, we maintain consistency with their binary preference modeling approach. However, our work differs in several aspects: (1) While they demonstrate PMPs benefits in improving sample efficiency during preference fine-tuning, they do not investigate the inherent scaling properties of large-scale preference training. Our work provides an in-depth analysis of these aspects; (2) We find that PMP datasets not only transfer to different fine-tuning datasets but also generalize directly to various test sets; (3) Due to the scarcity of preference data and evaluation sets at that time, they could only experiment with 5K human preference samples and validate on in-distribution sets. In contrast, we utilize more advanced human preference datasets and conduct comprehensive evaluations across wide range of benchmarks."
        },
        {
            "title": "3 Modeling World Preference",
            "content": "3.1 Experimental Setup 3.1.1 Data Collection To start modeling world preference, we first collect data from multiple public forums, including StackExchange* (a professional Q&A platform), Reddit (a social news and community discussion platform), and Quora (a knowledge sharing and Q&A community). Each forum contains numerous posts, typically in the form of questions, with responses from different users. Users can upvote or downvote these responses, naturally establishing relative preference pattern. For each post serving as prompt x, we randomly sample two responses with different net votes (upvotes minus downvotes) from its response list to form preference pairs, where we denote the response with higher and lower net votes as yw and yl respectively. Detailed forum data analysis and preference pair sampling strategies (e.g., controlling for vote margin between pairs) are provided in Appendix A. Training examples are presented in Appendix E. 3.1.2 Training Methods Our approach to world preference modeling follows the general human preference modeling framework as described in Section 2.1. Given pair of preference samples, we use the preference model to predict their respective rewards and optimize the BT-loss (Eq. 2). For models of different sizes, we maintain consistent hyperparameters with learning rate of 3e-6 and batch size of 10K. Ablation studies on learning rate and batch size configurations are provided in Appendix A.3. 3.1.3 Evaluation Methods To comprehensively evaluate WorldPM, we utilize different domain test sets from multiple RM benchmarks. Given our specific evaluation needs, we do not strictly follow the evaluation protocols provided with these benchmarks. Detailed explanations can be found in Appendix A.4. This is primarily because: (1) we use BT-Loss (Eq.2) to calculate test performance, which requires original preference pairs (RMB), and (2) we provide detailed style analysis, thus discarding the style control from the RM-Bench benchmark. The benchmarks used in this work are as follows: PPE (Frick et al., 2024b): This includes evaluations of both subjective and objective parts. The data for the subjective part comes from real user annotations in the Chatbot Arena. The objective part collects queries from the MMLU-Pro (Wang et al., 2024b), IFEval (Zhou et al., 2023), GPQA (Rein et al., 2023), MATH (Hendrycks et al., 2021), and MBPP-Plus (Austin et al., 2021) datasets, and gathers responses from state-of-the-art models. The correctness of the responses is validated against real answers, thereby forming preference pairs (with chosen as correct and rejected as incorrect). RMB (Zhou et al., 2024): This benchmark relies on GPT4 as the primary annotator, with additional human verification processes. It encompasses diverse scenarios and is fundamentally divided into two aspects: helpfulness and harmlessness. RM-Bench (Liu et al., 2024): This consists of evaluations in four domains: chat, code, math, and safety. The chat domain evaluation assesses models ability to identify factual errors by inserting them into responses. Prompts for code and math are sourced from the HumanEvalPack (Muennighoff et al., 2023) and MATH (Hendrycks et al., 2021) datasets, respectively, with validation against real answers. The safety section includes both pseudo-harmful and genuinely harmful questions to evaluate the models safety assessment capability. Reward Bench (Lambert et al., 2024): This includes evaluations in four domains: chat, chat-hard, reasoning, and safety. The chat-hard section, primarily sourced from LLMBar (Zeng et al., 2024), challenges reward models through the construction of subtly flawed responses designed to mislead evaluation. *https://stackexchange.com https://www.reddit.com https://www.quora.com https://lmarena.ai/ 4 Offset Bias (Park et al., 2024): This dataset constructs high-quality but incorrect responses (rejected responses) to challenge reward models, including off-topic responses and responses containing errors. HelpSteer2 (Wang et al., 2024c): This dataset, carefully annotated and filtered by trained human annotators, serves dual purposes: while it will be used as training set in later sections, during this phase it functions as test set for evaluating WorldPMs subjective performance. The capabilities tested by the above benchmarks can be broadly classified into three categories: (1) adversarial (identifying flaws in responses, such as constructing irrelevant rejected responses). (2) objective (identifying correct responses for querys with ground-truth answers), and (3) subjective (including human or AI subjective preferences). To evaluate WorldPM models effectiveness in downstream alignment tasks, we employ two benchmarks (Alpaca Eval (Dubois et al., 2024) and Arena Hard (Li et al., 2024)) and implement BoN sampling as an alternative to RLHF. These two benchmarks use AI as the subjective evaluator. 3.1.4 Training Data Source Selection Dataset PPE-Human PPE-Objective RMB RewardBench RM-Bench Quora Reddit StackExchange (cid:44) Math SE ArmoRM 60.2 55.1 62.8 62.9 60. 57.7 59.4 62.1 62.0 64.3 66.1 59.5 76.7 75.0 72.3 69.0 77.0 84.4 83.3 89. 60.9 60.7 72.5 75.0 75.4 Table 1: Performance comparison of general human preference across different data sources. StackExchange significantly outperforms other sources, approaching or surpassing open-source preference models (ArmoRM-Llama3-8B-v0.1). Math SE, mathematics-specific board of StackExchange, demonstrates strong general human preference despite its domain-specific nature. We collect 800K preference pairs from each forum and train them on Qwen2.5-7B. As shown in Table 1, we evaluate them using multiple benchmarks, where PPE-Object is the average of five PPE objective evaluation sets. RMB, RewardBench, and RM-Bench calculate the average of their respective subsets. We also include ArmoRM-Llama3-8B-v0.1(Wang et al., 2024a) as reference. It can be found that StackExchange preference data shows the highest quality and demonstrates strong out-of-domain generalization ability, approaching or even surpassing open-source preference models. Specifically, StackExchange data significantly outperforms Reddit and Quora across all evaluation sets, demonstrating its high-quality. Comparing StackExchange with ArmoRM, we find comparable performance on PPE-Objective and RM-Bench, while even surpassing ArmoRM on PPE-Human and RMB. Given that StackExchanges content is entirely out-of-domain from downstream general preference evaluations, its strong generalization ability is particularly impressive. Figure 2: Cross generalization across different data sources including StackExchange, Reddit, Quora, and HelpSteer2, where models trained on one source and predict preferences on the others. The values indicate test accuracy. To further understand StackExchanges out-of-domain generalization ability, we consider an extreme data source: StackExchanges Math board, which contains only mathematics-related content. Models trained on this board perform similarly to those trained on StackExchange data across various human preference benchmarks, indicating that general human preferences can transfer across different domains. Besides, we verify StackExchanges generalization ability across different data sources, including HelpSteer2 as few-human-annotated data source. Models are trained separately on StackExchange, Reddit, Quora, and HelpSteer2, then cross-evaluated on each others data, as shown in the Figure 2, colors are normalized per column due to varying dataset difficulties, while values show original accuracies. StackExchange shows the best generalization ability among multiple forum data sources and can represent different data sources. Given StackExchanges superior quality compared to other sources, we select it as the representative forum data source for modeling world preference. 5 3.2 Scaling Trends We collect as many posts, responses, and votes as possible from StackExchange to construct preference pairs, yielding approximately 15M pairs. We conduct world preference modeling on Qwen2.5 models (Yang et al., 2024b) ranging from 1.5B to 72B parameters. During large-scale WorldPM training, we observe moment of epiphany. As shown in Figure 3, the training loss exhibits logarithmic decrease with increasing training data volume. All training is completed within single epoch, without reusing any training samples. At approximately 12.6M training samples, we observe critical transition: sudden drop in loss coincides with sharp spike in gradients that quickly subsides. This synchronized patternthe abrupt gradient surge indicating the discovery of steep optimization direction, and the subsequent loss reduction suggesting successful transition to better solution spaceimplies that the model has discovered more general and effective preference representation, potentially leading to stronger generalization capabilities across different preference scenarios. Figure 3: moment of epiphany occurs during WorldPM training on Qwen 72B, characterized by sudden drop in loss and corresponding spike in gradients. Subsequently, we evaluate WorldPMs out-of-domain generalization capabilities across various test sets. We find that WorldPM exhibits different scaling properties across various domains and model sizes, as illustrated in Figure 1. The test loss is measured using BT loss (Equation 2). Overall, WorldPM demonstrates scaling trends on test sets with well-defined answers (Objective and Adversarial), where more challenging test sets require larger models to achieve scaling benefits. However, no clear scaling trends are observed on test sets with open-ended answers (Subjective). We observe power law decrease in test losses in adversarial evaluation. To construct adversarial evaluations, we utilized three test sets: factual error detection (from RM-Bench chat), Offsetbias, and LLMBar (from RewardBench chat-hard). These test sets challenge reward models robustness by either inserting factual errors into responses or constructing off-topic/incomplete responses. Notably, models of varying sizes exhibited power-law decrease in test losses across all three test sets. Multiple studies (Park et al., 2024; Zeng et al., 2024) have identified inherent biases and vulnerabilities in reward models that can be exploited by RLHF for reward hacking (e.g., assigning high scores to superficially good but unfaithful responses). Our findings suggest that such vulnerabilities may stem from insufficient training data. As training data expands, reward models demonstrate increasingly stronger capability to identify flawed responses. We observe an emergent phenomenon in objective metrics, where larger models demonstrate power law decrease in test losses across more benchmarks. We evaluate multiple aspects using six objective test sets: coding (MBPP-Plus, HumanEvalPack), mathematics (MATH), knowledge-based QA (MMLU-Pro, GPQA), and instruction following (IFEval). MATH and HumanEvalPack are sourced from RM-Benchs code and math domains, while others are from PPE. We find that the 1.5B model fails to generalize on any objective test set. The 7B model shows weak power law decrease on MBPP-Plus and IFEval, but exhibits increasing losses on more challenging benchmarks like HumanEvalPack and GPQA. In contrast, the 72B model exhibits consistent scaling trends across all aspects. These results indicate that preference modeling is an inherently challenging task, where certain capabilities emerge only at larger model scales - exemplifying the emergence phenomenon in large language models (Wei et al., 2022). Test losses in subjective evaluations quickly converge or even show an increasing trend. We collected annotations from three distinct sources: 1. Crowdsourced annotations (ChatBot Arena) from PPEs Human Preference test set, which contains authentic human preference selections from ChatBot Arena without strict control over the annotation process; 2. Expert annotations from HelpSteer2, following specific guidelines and employing multiple annotators with consensus-based quality control; 3. GPT4 annotations from RMBs Helpfulness evaluation, which implements set of human preference guidelines for GPT4-based annotation - strategy widely adopted in alignment evaluations, including Alpaca Eval and Arena Hard. We find that models from 1.5B to 72B do not show decreasing test losses with increased training data. From the model size perspective, the 1.5B model shows significant performance differences from the 6 other two, while there is no notable distinction between 7B and 72B models. These phenomena stand in stark contrast to results from other evaluations. We hypothesize that this might be due to conflicts between WorldPM and subjective evaluation in certain dimensions. Subjective evaluation encompasses rich variety of dimensions. When determining which response is better, humans or AI may consider multiple aspects such as usefulness, relevance, and conciseness (Li et al., 2024). Different individuals may have varying preferences - some may favor concise responses while others prefer verbose ones. Furthermore, within the same dimension, different people may have different interpretations; for example, given the same pair of responses, some people might find response more useful while others might consider response more helpful. When these noisy preference annotations are used as test sets for evaluation, they can introduce uncertainty into the assessment results (Dubois et al., 2024; Park et al., 2024). For instance, when preference model rewards brevity while subjective evaluation favors verbosity, the final evaluation results may appear poor, even though the preference model performs well in other dimensions such as usefulness. Appendix presents several cases of questionable human annotations from ChatBot Arena. 3.3 Style Impact Analysis 3.3.1 Style-Content Separation in Evaluation To investigate challenges in subjective evaluation, we focus on style preference as reliably quantifiable aspect, which has been shown to influence LLM evaluation in multiple studies (Zheng et al., 2023a; Dubois et al., 2024; Chiang et al., 2024; Feuer et al., 2024). Inspired by Chiang et al. (2024), we propose to separate style evaluation and content evaluation during the preference model assessment. For given preference pair (x, y0, y1), we define the reward model scores as rθ(x, y0) and rθ(x, y1). Consequently, the difference in model scores is given by D(x, y0, y1) = rθ(x, y0) rθ(x, y1). Let RS denote the style features, where is the number of style features. Consistent with Chiang et al. (2024), we quantify responses style using four features: token length, number of markdown lists, headers, and bold elements. The style difference for the preference pair can be defined as: Z(x, y0, y1) = normalize (cid:18) Z(y0) Z(y1) Z(y0) + Z(y1) (cid:19) . We combine the score difference and the style difference linearly to form the final evaluation metric: = DTα + ZT β, (4) where α and β are weight coefficients that adjust the influence of score differences and style differences in the final evaluation. The optimal values of α and β are obtained by minimizing Equation 5 through the linear regression algorithm. This formula maintains consistency with the training objective in Equation 2. ˆα, ˆβ = arg min βR,γRS 1 i=1 (Yi log(sigmoid(Ri)) + (1 Yi) log(1 sigmoid(Ri))), (5) Figure 4: Comparison of test loss on subjective evaluation sets across annotation sources, with and without style control. HelpSteer2s expert annotations show minimal impact from style control, while crowdsourced annotations (ChatBot Arena) and AI annotations (GPT4) show substantial variations. The gap between controlled and uncontrolled conditions grows with training data and model size, reflecting WorldPMs reduced style preference. 7 where Ri is the final reward difference for each preference pair. From an intuitive perspective, explicitly modeling stylistic factors during evaluation maximizes the stylistic gain in evaluation outcomes across different models, thus effectively eliminating stylistic influences when conducting comparisons across various models. We compare test losses with and without style control across subjective test sets (style control uses Ri as the final score, while no control sets β = 0 in Equation 4), as shown in Figure 4. Expert annotations (HelpSteer2) results remain stable - potentially due to strict quality control in its construction. However, test sets from crowdsourced annotations (ChatBot Arena) and AI annotations (GPT4) show significant variations. This suggests that without careful annotation, subjective evaluations are highly sensitive to style factors, while well-controlled test sets remain stable. Examining the gap between controlled and uncontrolled accuracies (shown as the space between dashed and solid lines), we observe that this gap widens with increased training and model scale, consistent with WorldPMs gradual reduction in style preference, as discussed in the following sections. After style control, the 72B model outperforms the 7B model (though still slightly underperforming on GPT4 evaluations, possibly due to additional uncontrolled preferences). This validates the benefits of model scaling in subjective tasks, although the lack of improvement with training scale remains an open question - we suspect that world preferences may still conflict with certain preference in subjective evaluations. Further analysis of stylistic factors in evaluation is presented in the Appendix C, encompassing impact across different domains and feature andablation studies. Although stylistic influence varies across different domain test sets, it does not alter the primary trends observed. 3.3.2 Style Effects on Training Dynamics (a) The left panel shows continuous performance improvements, while the right panel demonstrates decreasing correlation between model predictions and stylistic features as model size and training scale increase. However, this correlation remains higher than that (Oracle) between original labels and style, suggesting overreliance on stylistic features. (b) An asymmetric learning dynamic in style preference mitigation. The model rapidly fits majoritystyle (Chosen Longer) data while gradually incorporating minority-style instances (Chosen Shorter), maintaining performance on the former (Chosen Longer) throughout the training process. Our previous analysis revealed style preference in subjective evaluation, showing increasing distortion in evaluation results as training scale and model size expand. To investigate the underlying causes of this amplified distortion, we analyze the influence of stylistic factors during WorldPMs training process. We find that models initially over-rely on stylistic features; however, this stylistic bias is reduced as training data and model size increase. To measure the extent to which WorldPM predictions are influenced by stylistic features, given preference pair (x, y0, y1), we define S(x, y0, y1) to indicate which response has better stylistic features. P(x, y0, y1) represents the models prediction of which response is preferable, while G(x, y0, y1) is the human preference label. We use the Phi coefficient (ϕ) to determine their correlation, which characterizes the correlation between two binary variables (Cramer, 1946). Specifically, ϕ(P, S) measures how strongly model predictions are influenced by stylistic features, while ϕ(G, S) reflects the relationship between human preferences and style. ϕ is calculated as: ϕ(i, j) = n11n00 n10n01 (cid:112)(n1)(n0)(n1)(n0) (6) This differs from original test loss calculation as α is still optimized for controlled comparison, yielding lower values where nij denotes the count of preference pairs where the first variable is and the second is j, with i, 0, 1. The dot notation ni or nj represents the marginal sum over the corresponding index. As shown in Figure 5a, we conduct style preference analysis on the in-distribution validation set. The left panel demonstrates improving valid accuracy with increased training and model scale, indicating enhanced model performance. For analyzing stylistic influence, we consider two factors: Slength (indicating which response is longer) and Smarkdown (indicating which response contains more markdown markers). The right panel shows that with larger training data and model sizes, the correlation between model predictions and these stylistic features tended to decline, suggesting the models predictions became gradually less dependent on stylistic features. The vertical lines represent the correlation between human preference labels and style S. While human preferences show positive correlation with stylistic features, this correlation is consistently lower than that between model predictions and style, which suggests that the model has over-relied on stylistic features for predictions. This observation aligns with (Geirhos et al., 2020; Holtzman et al., 2021; Zhang et al., 2016): when training is insufficient or the models capabilities are limited, it tends to prioritize surface-level, easily discernible features for making predictions. Furthermore, we observe that both human labels and model predictions show stronger correlations with text length compared to markdown marker frequency, confirming previous findings(Chiang et al., 2024) that markdown usage serves as secondary stylistic feature. The model quickly fits majority-style data while maintaining long-term memory, spending most training time gradually learning from minority-style instances. We categorize the validation set into two groups based on length, as it represents the primary stylistic factor: one group where the chosen responses are longer than the rejected ones (Chosen Longer), and the other where the opposite is true (Chosen Shorter). As shown in Figure 5b, we find that the model could classify Chosen Longer data with an 82% accuracy rate at very early stage, indicating that it easily learns to distinguish this portion of the data. In contrast, the accuracy for Chosen Shorter data started at only 57%. As training progressed, the accuracy for Chosen Longer data remained stable overall, whereas the accuracy for Chosen Shorter data exhibited trend of logarithmic growth with training volume. This reveals the general learning process of WorldPM: the model first captures preference patterns exhibited by the majority of the dataset. However, the remaining data demonstrates contrary characteristics, forcing the model to discover underlying representations that can simultaneously model both majority and minority cases to further reduce loss, thereby overcoming initial biases. 3.4 Alignment Performance To further understand WorldPMs performance in subjective aspects, we evaluate its preference modeling capability indirectly by using PM for BoN sampling to align language models, assessing their performance on Alpaca Eval and Arena Hard. Specifically, for queries from both benchmarks, we generate 256 samples from Qwen2.5-7B-Instruct and let different checkpoints of 7B and 72B WorldPM models select the best response for evaluation. The results are shown in Figure 6, presenting both the original scores and scores after applying their respective style control strategies. We observe that the average length of responses selected by WorldPM gradually decreases across both benchmarks, with 72B generating shorter responses than 7B, consistent with our previous observations. Arena Hard shows stable trends before and after style control, with 72B significantly outperforming 7B, further validating that WorldPMs subjective performance improves with model size. For Alpaca Eval, without style control, performance strongly correlates with length, making it difficult to distinguish between 72B and 7B performance. This again echoes our previous observations - only with style control do we observe clear performance differences between them. Further examining their optimal training scales, we find that the 72B model achieves optimal performance on Alpaca Eval at 0.6M; direct RM evaluation on three test sets shows performance saturation around 3M; in contrast, Arena Hard performance continues to improve until 10M. Comparing the prompts of these two evaluators (Figure 11 and Figure 12), we find that Arena Hard explicitly requires answers to be helpful, relevant, and concise (with the conciseness requirement aligning with WorldPM). In contrast, Alpaca Eval only asks models to select the best output from human perspective and lacks Arena Hards Chain-of-Thought (COT) process. Consequently, without length control, Alpaca Eval results are strongly correlated with response length. This explains our observation that as training scale increases, performance on Arena Hard continues to improve over longer period, while performance on Alpaca Eval reaches optimal levels early on. This comparison highlights the challenges in subjective evaluation. Subjective evaluation encompasses multiple assessment dimensions, such as usefulness, relevance, and conciseness. As training scale 9 increases, WorldPM may improve in aspects like conciseness, but some subjective evaluations (like Alpaca Eval) might prefer complex and detailed responses, which conflicts with the goal of conciseness. We can further speculate that there might be more dimensions where WorldPMs scaling direction diverges from evaluation criteria, leading to continuous increases in losses across these evaluation dimensions, resulting in what appears to be overfitting in subjective evaluations. Figure 6: Analysis of alignment performance across WorldPM training scales. Both Alpaca Eval and Arena Hard implement distinct style control mechanisms to mitigate style preference in AI-based subjective evaluation. The figure demonstrates performance under both controlled and uncontrolled conditions. Arena Hard exhibits stable performance trends across control conditions, with 72B consistently superior to 7B and optimal performance achieved at larger training scales. However, Alpaca Eval shows substantial sensitivity to style control, with evaluation scores highly correlated with response length when style control is absent."
        },
        {
            "title": "4 Preference Fine-Tuning based on WorldPM",
            "content": "The previous experiments comprehensively verified the trends observed in various out-of-domain evaluations as the scale and size of training data expanded. Although the subjective evaluations have stagnated, continuous improvements in other aspects suggest that the model has learned more universal representations from WorldPM. Can these representations be utilized during the preference fine-tuning phase? To figure out that, we further validate during fine-tuning based on WorldPM. 4.1 Experimental Setup To evaluate WorldPM as foundation for preference fine-tuning, we conduct comprehensive experiments using several open-source human preference datasets. These include HelpSteer2 (Wang et al., 2024c), UltraFeedback (Cui et al., 2023), and pair data v2 800K wsafety from RLHFlow (hereafter referred to as RLHFlow). HelpSteer2 comprises approximately 7K preference comparisons across five dimensions. Since we consider only single reward, we selected helpfulness scores as the preference labels. UltraFeedback contains four responses per question; we extracted two groups of responses per question (ensuring no overlap between responses), yielding approximately 100K preference pairs. RLHFlow encompasses approximately 800K data points. These three datasets, representing different data scales, enable us to understand the effectiveness boundary of WorldPM. Given HelpSteer2s smaller size, we set its batch size to 128, while maintaining 512 for the other two datasets. All datasets underwent training for minimum of two epochs, with the final model selected based on the minimum loss achieved on the same distribution validation set. We comprehensively validate the effectiveness of WorldPM in preference model fine-tuning through two approaches. First, we evaluate various RM benchmark metrics. Second, considering that the primary objective of the preference model is to align language model outputs with human preferences, we employ best-of-N sampling for alignment evaluation. The alignment performance is assessed on two benchmarks - Arena Hard and Alpaca Eval - following our previously described evaluation protocol. Specifically, we generate 64 samples from Qwen2.5-7B-Instruct and Qwen2.5-72B-Instruct for the 7B and 72B models. 4.2 Evaluation Results The results for both the 7B and 72B models are shown in Table 2 and 3, with style-controlled scores reported for all RM benchmarks (see Appendix B.2 for uncontrolled results). We observe that as an https://huggingface.co/datasets/RLHFlow/pair data v2 80K wsafety 10 Metrics PPE-Human RMB-Helpful RB-Chat PPE-MBPP PPE-GPQA PPE-IFEval PPE-MATH PPE-MMLU RM-B-Code RM-B-Math RB-Reason RB-Chat-Hard RM-B-Chat OffsetBias RMB-Harmless RB-Safety RM-B-Safety-R RM-B-Safety-F Arena Hard Alpaca Eval Arena Hard-SC Alpaca Eval-LC Helpsteer2 UltraFeedback RLHFlow w/o WorldPM w/ WorldPM w/o WorldPM w/ WorldPM w/o WorldPM w/ WorldPM 63.32 74.33 97.21 72.24 56.27 58.81 71.04 63.55 57.68 70.70 88.02 81.25 78.29 78.87 72.73 90.68 90.45 99.65 65.88 43.73 65.07 39.98 Subjective Evaluation 64.93 (2.54%) 77.02 (3.61%) 97.07 (0.14%) 63.22 78.37 97.49 63.86 (1.01%) 78.82 (0.58%) 97.21 (0.29%) Objective Evaluation 73.55 (1.82%) 56.70 (0.76%) 57.34 (2.49%) 71.64 (0.85%) 66.52 (4.67%) 60.96 (5.70%) 75.99 (7.49%) 96.44 (9.57%) 69.77 56.19 57.42 71.76 65.64 59.21 72.12 92. 73.55 (5.42%) 55.43 (1.36%) 57.03 (0.68%) 74.36 (3.62%) 66.66 (1.55%) 62.94 (6.30%) 75.71 (4.98%) 95.39 (3.29%) Adversarial Evaluation 83.99 (3.37%) 82.17 (4.95%) 86.60 (9.80%) 77.52 80.23 81.68 79.82 (2.97%) 81.01 (0.97%) 82.94 (1.55%) Safety Evaluation 73.99 (1.72%) 94.53 (4.25%) 93.95 (3.87%) 99.30 (0.35%) 69.74 91.76 96.50 98.94 70.92 (1.69%) 89.46 (2.50%) 96.82 (0.33%) 98.94 (0.00%) Alignment Evaluation 70.27 (6.66%) 46.29 (5.84%) 70.22 (7.91%) 42.14 (5.41%) 67.93 46.16 66.19 44. 72.12 (6.17%) 46.35 (0.40%) 70.36 (6.30%) 46.13 (3.76%) 66.18 78.66 98.04 75.06 57.34 62.09 78.98 67.97 65.35 76.56 95.91 81.47 82.17 83.43 62.86 90.88 92.99 98.59 72.41 53.24 72.77 46. 66.78 (0.91%) 79.27 (0.78%) 97.77 (0.28%) 76.83 (2.37%) 57.60 (0.44%) 62.38 (0.47%) 80.16 (1.48%) 70.27 (3.39%) 67.76 (3.69%) 80.43 (5.06%) 96.82 (0.95%) 82.57 (1.35%) 82.56 (0.47%) 85.69 (2.71%) 63.79 (1.48%) 92.03 (1.26%) 96.18 (3.42%) 99.12 (0.54%) 74.62 (3.05%) 50.97 (4.27%) 74.29 (2.09%) 46.32 (0.17%) Table 2: Effect of WorldPM initialization on 7B models PM fine-tuning performance across different evaluation categories. Abbreviations: RB - RewardBench, RM-B - RM-Bench, Safety-R/F - SafetyResponse/Refuse. Metrics PPE-Human RMB-Helpful RB-Chat PPE-MBPP PPE-GPQA PPE-IFEval PPE-MATH PPE-MMLU RM-B-Code RM-B-Math RB-Reason RB-Chat-Hard RM-B-Chat OffsetBias RMB-Harmless RB-Safety RM-B-Safety-R RM-B-Safety-F Arena Hard Alpaca Eval Arena Hard-SC Alpaca Eval-LC Helpsteer2 UltraFeedback RLHFlow w/o WorldPM w/ WorldPM w/o WorldPM w/ WorldPM w/o WorldPM w/ WorldPM 64.76 77.76 98.04 70.50 57.66 59.24 69.94 70.20 60.75 72.78 96. 84.87 81.01 83.14 69.42 92.03 95.54 99.65 88.52 60.34 88.47 53.26 Subjective Evaluation 66.68 (2.97%) 78.53 (1.00%) 98.88 (0.85%) 65.12 79.80 97. 66.94 (2.80%) 81.73 (2.41%) 96.37 (1.29%) Objective Evaluation 78.76 (11.72%) 59.63 (3.42%) 63.63 (7.42%) 77.17 (10.33%) 74.96 (6.79%) 69.30 (14.08%) 77.69 (6.75%) 98.25 (1.66%) 75.02 59.22 60.57 72.77 72.42 67.32 73.72 96.30 79.46 (5.92%) 61.19 (3.33%) 64.34 (6.22%) 78.16 (7.41%) 75.08 (3.67%) 70.18 (4.23%) 77.79 (5.51%) 97.48 (1.23%) Adversarial Evaluation 87.28 (2.84%) 82.17 (1.44%) 89.76 (7.96%) 84.54 80.23 87.29 83.00 (1.82%) 79.46 (0.97%) 88.05 (0.88%) Safety Evaluation 70.34 (1.33%) 93.51 (1.62%) 97.13 (1.67%) 99.12 (0.53%) 69.57 93.45 98.41 98. Alignment Evaluation 89.72 (1.36%) 63.39 (5.07%) 90.54 (2.34%) 55.45 (4.12%) 89.16 59.95 88.82 56.83 68.43 (1.64%) 93.51 (0.07%) 98.41 (0.00%) 98.24 (0.36%) 90.37 (1.36%) 63.73 (6.31%) 90.74 (2.16%) 59.80 (5.23%) 68.14 79.26 98. 82.28 62.52 64.26 79.36 79.04 72.15 87.81 97.52 84.21 85.66 87.89 60.47 92.09 98.41 96.83 90.41 65.51 91.06 56.95 68.93 (1.16%) 80.89 (2.05%) 97.91 (0.57%) 82.39 (0.14%) 63.75 (1.97%) 65.39 (1.76%) 83.01 (4.60%) 79.30 (0.32%) 73.46 (1.82%) 86.20 (1.83%) 98.15 (0.64%) 84.54 (0.39%) 84.50 (1.36%) 87.47 (0.47%) 59.96 (0.84%) 92.03 (0.07%) 97.13 (1.29%) 97.18 (0.36%) 90.82 (0.45%) 66.26 (1.14%) 91.76 (0.77%) 55.58 (2.40%) Table 3: Effect of WorldPM initialization on 72B models PM fine-tuning performance across different evaluation categories. Abbreviations: RB - RewardBench, RM-B - RM-Bench, Safety-R/F - SafetyResponse/Refuse. 11 initialization for PM fine-tuning, WorldPM universally enhances performance across diverse domains and fine-tuning datasets of varying scales. The detailed findings are as follows: In subjective domains, PPE-Human and RMB-Helpful show notable improvements, further indicating that WorldPM learns useful general representations for subjective evaluation. RewardBenchChat shows slight decrease; however, with accuracy consistently above 97%, this metric appears saturated and offers limited insight. In objective domains, other PPE metrics, RM-Benchs code and math metrics, and RewardBenchs reasoning metrics demonstrate broad improvements. The 72B model shows larger gains compared to 7B, consistent with our WorldPM phase findings: while 7B struggles with objective generalization, 72B exhibits continuous improvement across all objective domains. In adversarial domains, including Reward bench chat-hard, RM-Bench Chat, and OffsetBias, datasets with initially lower performance (e.g., HelpSteer2) show notable improvements, while others show minimal changes or slight decreases. However, WorldPM itself already achieves high accuracy on these metrics (around 90% for RM-Bench Chat and OffsetBias, as shown in Figure 16). The post-fine-tuning decrease might be attributed to simple features being more susceptible to disruption, with final performance approaching that of the fine-tuning datasets. In safety evaluations, including RMB-Harmless, RewardBench-Safety, and RM-Bench-Safety, the 72B model shows mixed performance trends without consistent improvement. This might be related to WorldPMs defensive behavior against pseudo-harmful content in safety domains. Detailed analysis can be found in Appendix B.1. For downstream alignment evaluation: Comparing Arena Hard and Alpaca Eval results with and without style control, most metrics show improvement. Exceptions occur in RLHFlow datasets Alpaca Eval, where 7B shows decreased performance in raw results and 72B shows decreased performance after style control. This may be attributed to RLHFlows large scale and high quality, making WorldPMs benefits less pronounced. Overall, human preference datasets of different scales all benefit from WorldPM initialization, demonstrating its effectiveness as starting point. Considering the difficult and expensive annotation process for human preference datasets, large-scale WorldPM serves as crucial preliminary step before training on these datasets. 4.3 The Impact of WorldPM Training Scale on Fine-tuning To investigate how WorldPM training scale affects fine-tuning benefits, we conduct experiments using different WorldPM checkpoints of the 72B model (trained with 5M, 10M and 15M samples) and the baseline without WorldPM. Each checkpoint served as an initialization point for fine-tuning on Helpsteer2 and Ultrafeedback datasets, with final models selected based on minimum validation loss. Our evaluation encompassed style-controlled PPE metrics (averaging all objective metrics) and downstream alignment performance assessed through Best of 64 sampling on Qwen2.5-72B-Instruct using AlpacaEval and Arena-Hard benchmarks. As illustrated in Figure 7, substantial gains emerge at the 5-million sample threshold, consistent with scaling laws that predict linear performance improvements require exponential growth in training data. Notably, PPE objective metrics exhibit the most consistent improvement with increasing data scale, aligning with our WorldPM phase observations. Other metrics, all of which are subjective evaluations, demonstrate substantial fluctuations; nonetheless, we can observe that larger-scale WorldPM consistently achieves better performance on fine-tuning. 4.4 Applying WorldPM to RLHF We integrate WorldPM into our internal RLHF pipeline, employing GRPO as the RL optimization algorithm. We conduct preference training on two 72B models using in-house preference data, one initialized with WorldPM and the other without. The results are presented in Table 4. The evaluation includes both in-house benchmarks (first five columns, which are established following the Arena Hard methodology across multiple domains) and open-sourced benchmarks (last three columns). Our findings indicate that WorldPM initialization leads to better alignment with human preferences and improves overall alignment effectiveness. Figure 7: Comparison of PM fine-tuning performance across different WorldPM training scales and baseline without WorldPM. Larger WorldPM scales demonstrate enhanced fine-tuning benefits. In-house benchmark Open-sourced benchmark Math Code OpenQA Writing IF Arena Hard MT-Bench Alpaca Eval w/o WorldPM 60.06 62.37 w/ WorldPM 56.63 64.51 70.32 71.42 73.52 78.07 61.19 66.22 91.06 93. 8.56 8.62 90.90 91.04 Table 4: GRPO alignment results on QwQ-32B: WorldPM-initialized preference models show improved performance across various in-house (Math, Code, OpenQA, Writing, IF) and open-sourced benchmarks. Here, Writing stands for Creative Writing, and IF stands for Instruct Following."
        },
        {
            "title": "5 Dicussion",
            "content": "In this work, our primary contribution extends the exploration in PMP by shifting the focus from evaluating the benefits of reward model (RM) pre-training for downstream RLHF tasks to deeper investigation of the scalability properties inherent to RM pre-training itself. Our findings reveal clear scalability trends across objective and adversarial domains, with both parameter count and data volume contributing to performance improvements. However, in subjective domains where such scalable trends are not observed, we identify style preference as potential limiting factor. We find that WorldPM naturally mitigates style preference during training; however, unavoidable style preference within subjective human evaluations can still lead to inappropriate assessments of subjective performance. This brings us to fundamental question: what role do RMs play in current systems? Over the past four years, since the introduction of PMP, the role of Reward Models (RMs) has been progressively integrated into more comprehensive reward systems. In objective evaluations, areas such as math and coding now benefit from highly accurate rule-based rewards; factuality assessments increasingly utilize retrievalaugmented methods; and tasks with available references can improve reward signal quality through generative matching techniques. Thanks to extensive engineering efforts, most objective dimensions we care about can be reliably supervised while significantly reducing dependence on RMs. Thus, while we can continue scaling RM pre-training, how RMs should be integrated with other sources of reward signals remains an open question requiring further exploration. Importantly, in subjective areaswhere it is inherently difficult to define accurate rulesthe RMs role remains indispensable. However, improving RM performance in subjective domains is less about scaling pre-training data, but rather more about developing better annotation strategies and preference modeling frameworks that go beyond merely capturing surface-level preferences. In characterizing human preferences, it is crucial to minimize the subjective cognition of small number of humans, which is essential for the scalability of preference modeling, as we dont truly understand human preferences ourselves. Neural networks can only truly understand human preferences when we stop teaching them through conscious labeling and instead incentivize natural alignment with human choices."
        },
        {
            "title": "6 Limitaiton",
            "content": "Our dataset comprises 15M preference pairs (approximately 30G tokens) from StackExchange, modest in contrast to conventional next-token prediction pre-training, which routinely utilizes datasets of trilliontoken scale. Significant opportunities exist for expanding preference pretraining data through untapped sources such as various forums and social media platforms. 13 In addressing subjective evaluation biases, we control only for length and Markdown formatting. Many other influential factors, including emotional preferences and cultural tendencies, remain challenging to capture, underscoring the complexity of subjective assessment. The interconnected nature of various aspects in subjective evaluation makes comprehensive and granular assessment particularly challenging, as these elements prove difficult to isolate effectively."
        },
        {
            "title": "References",
            "content": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova Dassarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, John Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, and Jared Kaplan. general language assistant as laboratory for alignment. ArXiv, abs/2112.00861, 2021. URL https://api.semanticscholar.org/CorpusID:244799619. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021. URL https://api.semanticscholar.org/CorpusID: 237142385. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. ArXiv, abs/2309.16609, 2023. URL https://api. semanticscholar.org/CorpusID:263134555. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Perez, Jamie Kerr, Jared Mueller, Jeff Ladish, Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova Dassarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Sam Bowman, Zac HatfieldDodds, Benjamin Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom B. Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback. ArXiv, abs/2212.08073, 2022b. URL https://api.semanticscholar.org/CorpusID:254823489. Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39:324, 1952. URL https://api.semanticscholar.org/CorpusID: 125209808. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023. H. Cramer. Mathematical Methods of Statistics. Goldstine Printed Materials. Princeton University Press, 1946. ISBN 9780691080048. URL https://books.google.com.sg/books?id= db1jwEACAAJ. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, and et al. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. URL https://api.semanticscholar.org/CorpusID:271571434. 14 Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. David Easley, Jon Kleinberg, et al. Networks, crowds, and markets: Reasoning about highly connected world, volume 1. Cambridge university press Cambridge, 2010. Ben Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, and John P. Dickerson. Style outweighs substance: Failure modes of llm judges in alignment benchmarking. ArXiv, abs/2409.15268, 2024. URL https://api.semanticscholar.org/CorpusID:272827428. Evan Frick, Peter Jin, Tianle Li, Karthik Ganesan, Jian Zhang, Jiantao Jiao, and Banghua Zhu. Athene-70b: Redefining the boundaries of post-training for open models, July 2024a. URL https://nexusflow.ai/ blogs/athene. Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios Nikolas Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. How to evaluate reward models for rlhf. ArXiv, abs/2410.14872, 2024b. URL https://api.semanticscholar.org/CorpusID:273502060. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. Robert Geirhos, orn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2: 665 673, 2020. URL https://api.semanticscholar.org/CorpusID:215786368. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isnt always right. ArXiv, abs/2104.08315, 2021. URL https: //api.semanticscholar.org/CorpusID:233296182. Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets. 2008 Eighth IEEE International Conference on Data Mining, pages 263272, 2008. URL https://api. semanticscholar.org/CorpusID:10537313. Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah Smith, Yejin Choi, and Hannaneh Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. arXiv preprint arXiv:2406.09279, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020. URL https://api.semanticscholar.org/CorpusID:210861095. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling, 2024. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar. org/CorpusID:261493811. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. URL https://lmsys. org/blog/2024-04-19-arena-hard/. Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient method for aligning large language models. arXiv preprint arXiv:2310.10505, 2023. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Rm-bench: Benchmarking reward models of language models with subtlety and style. ArXiv, abs/2410.16184, 2024. URL https://api. semanticscholar.org/CorpusID:273507377. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. 15 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Aditya Pal, F. Maxwell Harper, and Joseph A. Konstan. Exploring question selection bias to identify experts and potential experts in community question answering. ACM Trans. Inf. Syst., 30:10:110:28, 2012. URL https://api.semanticscholar.org/CorpusID:2855569. Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators. ArXiv, abs/2407.06551, 2024. URL https://api.semanticscholar. org/CorpusID:271064337. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. ArXiv, abs/2311.12022, 2023. URL https://api.semanticscholar.org/CorpusID:265295009. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. URL https://api.semanticscholar.org/CorpusID:267412607. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. Hao Sun, Yunyi Shen, and Jean-Francois Ton. Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives. ArXiv, abs/2411.04991, 2024. URL https: //api.semanticscholar.org/CorpusID:273877679. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In EMNLP, 2024a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max W.F. Ku, Kai Wang, Alex Zhuang, Rongqi Richard Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. ArXiv, abs/2406.01574, 2024b. URL https://api.semanticscholar.org/ CorpusID:270210486. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. ArXiv, abs/2406.08673, 2024c. URL https://api.semanticscholar. org/CorpusID:270440126. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. ArXiv, abs/2206.07682, 2022. URL https://api.semanticscholar.org/CorpusID:249674500. Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? comprehensive study. arXiv preprint arXiv:2404.10719, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-Yang Chen, Kexin Yang, Mei Li, Min Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yunyang Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, and Zhi-Wei Fan. Qwen2 technical report. ArXiv, abs/2407.10671, 2024a. URL https://api.semanticscholar.org/CorpusID:271212307. Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024b. URL https://api.semanticscholar.org/CorpusID:274859421. 16 Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In International Conference on Learning Representations (ICLR), 2024. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ArXiv, abs/1611.03530, 2016. URL https://api. semanticscholar.org/CorpusID:6212000. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685, 2023a. URL https://api. semanticscholar.org/CorpusID:259129398. Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Wei-Yuan Shen, Bing Wang, Yan Liu, Senjie Jin, Qin Liu, Limao Xiong, Luyao Chen, Zhiheng Xi, Yuhao Zhou, Nuo Xu, Wen-De Lai, Minghao Zhu, Rongxiang Weng, Wen-Chun Cheng, Cheng Chang, Zhangyue Yin, Yuan Hua, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. Secrets of rlhf in large language models part i: Ppo. ArXiv, abs/2307.04964, 2023b. URL https://api.semanticscholar.org/CorpusID: 259766568. Enyu Zhou, Guodong Zheng, Bing Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Rmb: Comprehensively benchmarking reward models in llm alignment. ArXiv, abs/2410.09893, 2024. URL https://api. semanticscholar.org/CorpusID:273346208. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/ abs/2311.07911. 17 Figure 8: Distribution of reply lengths, upvotes, and the number of replies per post across all StackExchange forum data. All these characteristics demonstrate evident long-tail distributions. The last subplot indicates weak correlation between reply length and upvotes."
        },
        {
            "title": "A Detail of Settings",
            "content": "A.1 Forum Data Analysis Different dimensions of forum data generally follow long-tail distributions. We analyze all the data collected from StackExchange, as shown in Figure 8. Our analysis focuses on three key dimensions: reply length, reply upvotes, and the number of replies per post. We find that forum data, or naturally generated human data, typically exhibits strong long-tail characteristics. For instance, most forum replies are under 1K in length, receive fewer than 10 upvotes, and posts typically have fewer than 6 replies. Notably, the majority of posts have only one reply, and many replies have zero upvotes. Consequently, we need to filter out significant portion of data where preference relationships cannot be defined, as establishing preference pair requires at least two replies to post with different numbers of upvotes. Its worth noting that there are relatively few extremely short replies in the forum data, as shown in the first graph. We hypothesize this is due to StackExchanges relatively strict content moderation mechanisms, which filter out replies lacking substantial information. We further analyze the relationship between reply length and upvotes, as shown in the last column of the figure. Overall, the distribution of reply lengths appears relatively uniform across different upvote counts, although there remains positive correlation between reply length and upvotes. This correlation is natural in human-generated content, as longer replies typically indicate greater effort from the author, potentially resulting in higher quality content. A.2 Preference Data Construction On StackExchange, users can upvote or downvote replies, and post authors can choose to accept specific replies. We incorporate the authors acceptance as an additional upvote. The final score for each reply is calculated by subtracting downvotes from upvotes. Using these scores, we define preference directions: when two replies have different scores, we consider them to form preference relationship. Score differences do not affect preference performance. One potential concern is whether replies with close scores (e.g., one reply with 1 upvote versus another with 2 upvotes) lack sufficient distinction. To address this, we bucket the score differences between reply pairs into groups: 1-2, 3-5, 6-10, and 11+ differences. As shown in Table 5, we find similar performance across different benchmarks for each bucket, with no bucket showing significant performance variations. Score Diff PPE-Human PPE-Objective RMB RewardBench RM-Bench 1-2 3-5 6-10 11 63.1 62.6 62.3 62.5 59.4 59.5 59.3 60.5 73.8 76.7 76.1 74. 79.9 84.2 85.5 85.2 75.2 73.6 73.2 71.1 Table 5: Bucketing preference pairs by score differences to evaluate the impact of score gaps on preference data quality. Similar performance across different topic domains. Another potential concern is that different boards of StackExchange, covering diverse content areas, might affect downstream generalization, especially given that our downstream evaluation spans general dialogue, mathematics, coding, and other aspects. 18 Figure 9: Comparison of different batch sizes (from 2.5K to 40K) on Qwen2.5 7B with fixed 256 steps. Larger batch sizes consistently yield better performance. To address this, we divide StackExchange data into three segments: StackOverflow (computer sciencerelated topics), Math StackExchange (mathematics-related topics), and Others. This division reflects that the majority of StackExchange content comes from the first two sections, while the remaining 170+ sections contain diverse topics with relatively few questions each. We train models separately on each section and evaluate them across all test sets, as shown in Table 6. Our results indicate no significant performance variations across different sections. Furthermore, StackExchange provides topic tags for each question, allowing for more granular topic identification. For instance, StackOverflow uses specific tags like c++, java, and pointers. These tags also follow long-tail distribution, with common programming questions constituting the majority. We conduct additional experiments with topic resampling (StackOverflow-TR) on StackOverflow data, deliberately oversampling from less common tags to enhance data diversity. As shown in the table, this resampling approach yields similar results. These findings suggest that human preferences are domain-agnostic: despite varying discussion topics, users within the same forum community demonstrate consistent preference patterns. Category PPE-Human PPE-Objective RMB RewardBench RM-Bench StackExchange Other Math StackExchange StackOverflow StackOverflow-TR 62.8 62.7 62.9 63.3 62.4 62.1 62.0 62.0 63.0 62.8 76.7 76.2 75.0 75.0 75.3 84.4 85.7 83.3 83.3 84.8 72.5 72.3 75.0 74.9 73.6 Table 6: Analysis of performance across different forum categories and topic distributions within individual categories indicates that human preferences exhibit robust cross-domain transferability. A.3 Experimental Settings We conduct world preference modeling experiments on Qwen2.5 models ranging from 1.5B to 72B parameters, with batch size of 10K, training steps of 1536, and learning rate of 3e-6. For smaller datasets and comparative experiments, we typically set the batch size to 2048. We use the Adam optimizer and employ learning rate warmup with ratio of 0.1 and the weight decay coefficient of 0.1. We employ learning rate warmup with ratio of 0.1, followed by constant learning rate. The context length is set to 2048. We discover that when modeling preferences, larger batch sizes lead to better performance under the same number of training steps, as shown in Figure 9. Performance continues to improve even with batch sizes up to 40K, indicating that preference modeling is indeed challenging task that benefits from more accurate loss estimation through larger batch sizes. However, considering the limited training data available, we ultimately adopt batch size of 10K to minimize loss noise while maintaining computational efficiency. We additionally conduct comprehensive ablation studies on learning rates, as shown in Figure 10. Our systematic comparison of learning rates at 1e-6, 3e-6, and 1e-5 demonstrates that model performance remains stable across this range, with 3e-6 exhibiting marginally superior results. This finding suggests that preference modeling is robust to learning rate variations within reasonable bounds. 19 Figure 10: Through our experiments with different learning rates on Qwen2.5 7B, we observe that the models final performance remains relatively stable across various learning rates, with 3e-6 showing slightly superior results. This suggests that the models performance is robust to learning rate variations within reasonable range. A.4 Evaluation Settings We evaluate our preference models using multiple RM benchmarks, including PPE, RMB, RM-Bench, and RewardBench. However, we modify some of their evaluation methodologies for our specific context. RMB offers PairWise and Best-of-N (BoN) evaluations, where BoN measures models ability to select the best reply from multiple options, and PairWise is essentially BoN with N=2. We focus on PairWise evaluation because: (1) BT loss computation inherently involves pairs of replies, making BoN evaluation computationally challenging, and (2) the original paper shows strong correlation between PairWise and BoN results. For RM-Bench, we utilize only the models original responses to form preference pairs, rather than their style-modified versions, as we propose our own method for style-content separation evaluation in this paper. For PPE, we identify significant bias issue in the MBPP-Plus test set. This test set is constructed by sampling from four advanced models, where each model generates both correct and incorrect answers to form preference pairs. In our analysis, we observe unusually volatile metrics and conduct detailed investigation. We define two binary variables: one indicating which answer is correct, and another indicating which answer is longer. By measuring the correlation (Phi coefficient) between answer correctness and length, we find that Claude-3-Haiku samples show strong negative correlation (-0.6) between these variables, where shorter answers are predominantly correct. This correlation significantly deviates from other models patterns, indicating severe style bias. Consequently, we exclude Claude-3Haiku samples from MBPP-Plus test set to ensure reliable evaluation results. A.5 Evaluator Prompts The evaluation prompts used in Arena Hard and Alpaca Eval are shown in Figure 11 and Figure 12, respectively."
        },
        {
            "title": "B Additional Experimental Results",
            "content": "B.1 Scaling Trends Across All Test Sets Model Gemma-2-9B-IT GPT-4o-Mini Llama-3-8B-Instruct Claude-3-Haiku Correlation +0.047 0.133 0.115 0.619 Figure 13 illustrates the comprehensive scaling trends of world preference across all evaluation benchmarks. In HelpSteer2s five-dimensional assessment framework, we observe strong correlations among Helpfulness, Correctness, and Coherence dimensions, while Complexity and Verbosity demonstrate high mutual correlation. The latter two metrics show strong association with response length, as discussed in Section C.2. As elaborated in the main text, WorldPM demonstrates progressive mitigation of length preference, manifesting as increasing loss on these metrics. Table 7: Correlation coefficients between response length and answer quality across different models. Safety evaluation, omitted from the main text due to space constraints, encompasses RMB-Harmlessness, 20 Arena Hards Prompt Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below . You will be given assistant ' answer and assistant ' answer . Your job is to evaluate which assistant ' answer is better . Begin your evaluation by generating your own answer to the prompt . You must provide your answers before judging any answers . When evaluating the assistants ' answers , compare both assistants ' answers with your answer . You must identify and correct any mistakes or inaccurate information . Then consider if the assistant ' answers are helpful , relevant , and concise . Helpful means the answer correctly responds to the prompt or follows the instructions . Note when user prompt has any ambiguity or more than one interpretation , it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions . Relevant means all parts of the response closely connect or are appropriate to what is being asked . Concise means the response is clear and not verbose or excessive . Then consider the creativity and novelty of the assistant ' answers when needed . Finally , identify any missing important information in the assistants ' answers that would be beneficial to include when responding to the user prompt . After providing your explanation , you must output only one of the following choices as your final verdict with label : 1. Assistant is significantly better : [[ >> ]] 2. Assistant is slightly better : [[A >B ]] 3. Tie , relatively the same : [[ A=B ]] 4. Assistant is slightly better : [[B >A ]] 5. Assistant is significantly better : [[ >> ]] Example output : \" My final verdict is tie : [[ A=B ]]\". Figure 11: Evaluation prompts used in Arena Hard 21 Alpaca Evals Prompt < im_start > system You are highly efficient assistant , who evaluates and selects the best large language model ( LLMs ) based on the quality of their responses to given instruction . This process will be used to create leaderboard reflecting the most accurate and human - preferred answers . < im_end > < im_start > user require leaderboard for various large language models . ' ll provide you with prompts given to these models and their corresponding outputs . Your task is to assess these responses , and select the model that produces the best output from human perspective . ## Instruction { } \" instruction \": \"\"\"{ instruction }\"\"\" , ## Model Outputs Here are the unordered outputs from the models . Each output is associated with specific model , identified by unique model identifier . \" model_identifier \": \"m\", \" output \": \"\"\"{ output_1 }\"\"\" \" model_identifier \": \"M\", \" output \": \"\"\"{ output_2 }\"\"\" { } { }, { } ## Task Evaluate the models based on the quality and relevance of their outputs , and select the model that generated the best output . Answer by first providing concise explanation and then end your answer by providing the model identifier of the best output . We will use the last character of your output ` output [ -1] ` as the name of the best model , so make sure you finish with the token of the model identifiers and nothing else : `m ` or `M ` ( no quotes , no dots , no backticks , no new lines , ...) . For example : ### Concise explanation ... some text ... ### Which is best , or M? Now is your turn . ## Your answer : \" Concise explanation \" followed by \" Which is best , or ?\" < im_end > Figure 12: Evaluation prompts used in Alpaca Eval. 22 Figure 13: Loss trajectories across evaluation benchmarks as function of training scale and model size, plotted on log2 scale. RewardBench-Safety, and RM-Bench-Safety metrics. RewardBench-Safety exhibits distinct scaling patterns across model sizes: the 1.5B model shows continuous decline, while 7B and 72B models stabilize post-decline, achieving accuracy exceeding 90%, indicating performance saturation. RMB-Harmlessness presents heterogeneous patterns: declining for 1.5B, saturating for 7B, and ascending for 72B. These diverse patterns can be partially interpreted through RM-Bench-Safetys subdomain analysis: pseudoharmful queries versus genuinely harmful queries (RM-Bench-Safety-Response), as RMB-Harmlessness also incorporates both aspects. Our analysis reveals decreasing and saturating loss patterns for genuine harmful content detection, while pseudo-harmful query response shows ascending loss in later training stages. This suggests sustained efficacy in identifying genuinely harmful content, coupled with increased conservatism toward pseudo-harmful queries. We hypothesize that seemingly benign queries in specific contexts (e.g., gaming-related weapon discussions) might inadvertently enable harmful applications (e.g., transferable knowledge to real weapons). B.2 Preference Fine-tuning Evaluation Without Style Control Tables 8 and 9 present the uncontrolled versions of preference fine-tuning results for 7B and 72B models, comparing initialization with and without WorldPM. Overall, the findings align with those observed in 23 the style-controlled versions. Metrics PPE-Human RMB-Helpful RB-Chat PPE-MBPP PPE-GPQA PPE-IFEval PPE-MATH PPE-MMLU RM-B-Code RM-B-Math RB-Reason RB-Chat-Hard RM-B-Chat OffsetBias RMB-Harmless RB-Safety RM-B-Safety-R RM-B-Safety-F HelpSteer2 UltraFeedback RLHFlow w/o WorldPM w/ WorldPM w/o WorldPM w/ WorldPM w/o WorldPM w/ WorldPM 62.05 72.49 97. 67.07 55.41 57.50 70.80 63.20 55.92 69.09 81.62 62.50 51.94 63.76 68.41 82.84 87.90 98.59 Subjective Evaluation 63.80 (2.81%) 75.84 (4.62%) 97.21 (0.29%) 60.88 75.62 97. 61.64 (1.25%) 75.70 (0.11%) 97.21 (0.29%) Objective Evaluation 71.31 (6.33%) 56.56 (2.08%) 56.17 (2.31%) 71.74 (1.32%) 66.21 (4.76%) 57.68 (3.14%) 74.10 (7.25%) 96.12 (17.77%) 66.60 55.62 56.21 71.64 65.51 57.02 70.23 83.19 72.24 (8.46%) 55.18 (0.81%) 56.64 (0.76%) 73.77 (2.97%) 66.17 (1.01%) 62.06 (8.85%) 73.44 (4.58%) 91.86 (10.42%) Adversarial Evaluation 74.89 (19.82%) 65.12 (25.37%) 82.40 (29.23%) 61.51 57.36 72.98 67.32 (9.45%) 66.67 (16.22%) 78.17 (7.11%) Safety Evaluation 73.58 (7.56%) 90.54 (9.30%) 92.99 (5.80%) 97.54 (1.07%) 69.22 89.05 95.54 96. 70.23 (1.46%) 87.84 (1.37%) 94.27 (1.33%) 96.48 (0.00%) 65.54 77.64 98.04 71.97 56.76 61.87 78.79 67.56 61.40 75.99 94.13 66.01 56.98 73.54 66.78 85.95 91.08 95.77 66.03 (0.75%) 78.33 (0.88%) 97.77 (0.28%) 74.44 (3.43%) 56.70 (0.10%) 62.54 (1.07%) 79.94 (1.46%) 70.20 (3.90%) 67.76 (10.36%) 79.58 (4.73%) 96.16 (2.15%) 69.96 (5.98%) 66.67 (17.01%) 80.50 (9.47%) 66.84 (0.08%) 86.35 (0.47%) 93.63 (2.80%) 95.42 (0.37%) Table 8: Effect of WorldPM initialization on 7B models PM fine-tuning performance across different evaluation categories (without style control). Abbreviations: RB - RewardBench, RM-B - RM-Bench, Safety-R/F - Safety-Response/Refuse. Metrics PPE-Human RMB-Helpful RB-Chat PPE-MBPP PPE-GPQA PPE-IFEval PPE-MATH PPE-MMLU RM-B-Code RM-B-Math RB-Reason RB-Chat-Hard RM-B-Chat OffsetBias RMB-Harmless RB-Safety RM-B-Safety-R RM-B-Safety-F HelpSteer2 UltraFeedback RLHFlow w/o WorldPM w/ WorldPM w/o WorldPM w/ WorldPM w/o WorldPM w/ WorldPM 62.22 76.40 97.91 70.19 58.03 58.96 69.73 69.98 61.18 70.04 96.61 69.41 77.52 76.29 68.96 90.14 94.59 97.89 Subjective Evaluation 65.53 (5.32%) 77.45 (1.38%) 98.88 (1.00%) 62.09 75.60 97.21 64.51 (3.89%) 79.10 (4.64%) 96.09 (1.15%) Objective Evaluation 77.45 (10.34%) 59.28 (2.15%) 63.50 (7.68%) 76.64 (9.92%) 74.69 (6.73%) 69.30 (13.26%) 76.75 (9.58%) 98.22 (1.66%) 74.32 59.06 60.16 72.58 72.07 65.79 71.55 95. 78.65 (5.82%) 60.76 (2.88%) 63.83 (6.10%) 77.89 (7.32%) 74.92 (3.96%) 68.64 (4.33%) 77.32 (8.06%) 96.44 (0.77%) Adversarial Evaluation 80.26 (15.64%) 75.97 (2.00%) 85.79 (12.46%) 75.66 70.93 84.47 77.63 (2.61%) 70.54 (0.55%) 85.86 (1.64%) Safety Evaluation 70.10 (1.66%) 91.35 (1.35%) 94.27 (0.34%) 97.18 (0.72%) 68.58 89.12 97.45 97.01 67.93 (0.95%) 90.61 (1.67%) 98.09 (0.65%) 96.48 (0.54%) 67.63 78.10 98.32 81.00 62.19 63.32 79.10 78.52 71.93 87.52 96.33 74.12 77.13 83. 63.70 84.19 94.90 88.73 68.29 (0.98%) 80.08 (2.53%) 98.04 (0.28%) 81.85 (1.05%) 63.44 (2.01%) 65.04 (2.71%) 82.52 (4.32%) 79.28 (0.97%) 72.81 (1.22%) 85.63 (2.16%) 97.38 (1.09%) 75.33 (1.63%) 75.58 (2.01%) 81.90 (1.54%) 62.65 (1.64%) 83.24 (1.12%) 93.63 (1.34%) 89.08 (0.40%) Table 9: Effect of WorldPM initialization on 72B models PM fine-tuning performance across different evaluation categories (without style control). Abbreviations: RB - RewardBench, RM-B - RM-Bench, Safety-R/F - Safety-Response/Refuse."
        },
        {
            "title": "C Details of Style Control",
            "content": "C.1 Ablation Study on Style Control Factors We control two style features: length and markdown formatting. As shown in Figure 14, we compare the impact on subjective evaluation performance when controlling length alone, markdown formatting alone, and both factors simultaneously. Our findings indicate that markdown formatting has less influence 24 Figure 14: Impact of controlling different style factors on subjective evaluation performance. than length, further confirming that length is the primary style factor. Moreover, modeling both factors together enables more effective style control. C.2 Effect of Style Control Across Test Set Evaluation Figure 16 and Figure 15 demonstrates the impact of style control on evaluation results (test accuracy and test loss) across all test sets. We observe several key findings: In HelpSteer2, the dimensions of Helpfulness, Correctness, and Coherence maintain substantial consistency. Complexity and Verbosity exhibit high correlation with length characteristics. Upon implementing length control, these two metrics show significant changes while maintaining stable trends. Some objective and robustness evaluation sets demonstrate notable shifts in performance (e.g., PPE-MBPP-Plus, PPE-GPQA, and RM-Bench-Chat). However, the performance gap between style-controlled and uncontrolled versions either remains constant or narrows with increased training scale, contrasting with the widening gap in subjective domains. This suggests that modeling world preference gradually overcomes length bias, converging toward more accurate evaluation results. In the safety domain, RM-Bench-Safety-Refuse demonstrates exceptionally high accuracy even without style control, indicating WorldPMs inherent capability for safety discrimination. Other safety-related benchmarks show declining accuracy in later stages, potentially related to increased defense against pseudo-harmful queries."
        },
        {
            "title": "D Reflections on Potential Noise in Forum Data",
            "content": "D.1 Impact of RM Filtering on Different Data Sources In the early stages of our research, we hypothesized that forum preference data contained significant noise, leading us to invest considerable effort in denoising attempts. One primary approach involved using existing reward models for noise reduction. Specifically, we applied Qwen-2.5-72B-RM (Yang et al., 2024b) to score preference data from Quora and StackExchange, filtering out instances where the Chosen score was lower than the Rejected score. This approach effectively creates an intersection of two preference sources: human preferences and RM preferences. We conducted training experiments on the 7B model, with results shown in Table 10. Our findings reveal that for Quora data, RM-based filtering significantly improves performance across all benchmarks. However, for StackExchange data, the improvements are less pronounced, with notable gains only in PPE-Objective. This disparity suggests that StackExchanges inherently stricter quality control mechanisms result in more reliable data, making additional filtering less impactful. D.2 Impact of Various Reward Model Filtering We expand the StackExchange training dataset to 2 million samples and employ two state-of-the-art models, Athene-70B-RM (Frick et al., 2024a) and Qwen-2.5-72B-RM (Yang et al., 2024b), for both independent Figure 15: Effect of style control on test loss across all test set evaluation. Dataset Quora Quora+Filter StackExchange StackExchange+Filter PPE-Human PPE-Objective RMB RewardBench RM-Bench 60.2 62. 62.8 63.5 57.7 62.9 62.1 64.1 66.1 76.1 76.7 76.3 69.0 85. 84.4 84.9 60.9 74.4 72.5 73.5 Table 10: Impact of RM-based Filtering on Preference Data from Quora and StackExchange. 26 Figure 16: Effect of style control on performance across all test set evaluation. 27 and joint filtering before training on the 7B model to further investigate the impact of RM filtering. As shown in Table 11, while filtering operations yield modest improvements across different benchmarks, the overall impact remains limited. Notably, even joint filtering using both models fails to demonstrate significant performance gains, despite maintaining considerable performance gap compared to the filtering models themselves. Dataset StackExchange Athene70B Athene70B-Filter Qwen72B Qwen72B-Filter Athene70B-Qwen72B-Filter PPE-Human PPE-Objective RMB RewardBench RM-Bench 63.1 66.4 63.7 63.7 63.1 63. 63.2 70.5 65.1 72.8 65.1 65.5 75.6 80.7 77. 72.3 76.9 77.5 85.9 88.3 86.0 91.9 86.9 85. 74.0 79.4 75.3 84.5 75.9 75.6 Table 11: Comparison of Independent and Joint Filtering Effects Using Athene-70B-RM and Qwen-2.572B-RM on StackExchange Data, showing limited gains despite performance gap with original models. D.3 Potential Biases in RM Filtering We bucket the scoring results from Qwen-2.5-72B-RM by calculating the score difference between Chosen and Rejected responses for each sample to investigate the distinctions between filtered and filteredout data. As shown in Table 12, we categorize score differences into various intervals. We find that filtered data shows relatively consistent performance across different intervals, with only the 00.7 interval performing marginally worse than others. For filtered-out data (score range -10.0 0), we present their evaluation results alongside their inverse scores (100 minus the original score) in parentheses. Notably, the performance of these filtered-out data closely aligns with that of the filtered data, suggesting that both sets possess similar modeling capabilities but with opposing preference directions. We conclude that the filtering operation essentially aligns with RMs assessment patterns, retaining data that conforms to its discrimination criteria while filtering out non-conforming instances. However, this implies that RM-based filtering may lead models to adopt RMs own discrimination patterns. Therefore, we argue that applying RM filtering diverges from capturing world preference. Instead of assuming forum data contains noise, we should interpret apparent contradictions as manifestations of genuine human preferences, allowing models to discover underlying commonalities within these surface-level conflicts. Score Range [2.8, 10.0] [1.6, 2.8] [0.7, 1.6] [0.0, 0.7] [1.6, 0] [10.0, 1.6] PPE-Human PPE-Objective RMB RewardBench RM-Bench 62.8 63.4 63.4 62.9 38.1 (61.9) 37.3 (62.7) 64.6 64.8 64.1 62.4 36.0 (64.0) 35.0 (65.0) 76.6 76.9 76.4 76.2 24.9 (75.1) 23.0 (77.0) 85.4 85.7 84.3 84.5 20.3 (79.7) 15.8 (84.2) 75.2 75.8 74.9 74.0 23.7 (76.3) 24.6 (75.4) Table 12: Performance comparison across different RM score ranges and metrics."
        },
        {
            "title": "E Case Study",
            "content": "We present several examples from PPE-Human, including Figures 17, 18, and 19. In Figures 17 and 18, general humans chose incorrect answers, while WorldPM correctly identified the errors. Figures 19 demonstrates that data from general humans can be low-quality and uninformative, thus interfering with evaluation. We also present several examples of training data used for modeling world preference, including Figures 20, 21, 22, and 23. These data often contain rich information, enabling the model to learn complex representations. 28 Query Prove or disprove: for all integers a, b, c, d, if and d, then (a + c) (b + d). Chosen Response (World Preference Model Score: -1.84) This statement is true. This is because absolute values of sums of integers are equal to the sums of their absolute values. In this case, since (a divides b) and (c divides d), then certainly and are both divisible by and c, respectively. So, setting = and = c, we have that both = for some integer and = for some integer r. Then, + = (q + r) + is divisible by both and , which are equal to and c. So (b + d) is divisible by the sum + c. Thus, (a + c) (b + d). Rejected Response (World Preference Model Score: 0.76) To prove or disprove the statement for all integers a, b, c, if and d, then + + d, we can approach it by first understanding what the statement means and then testing it with specific examples. Definitions: - means that there exists an integer such that = ak. - means that there exists an integer such that = cm. Claim: - + + means there exists an integer such that + = (a + c)n. We need to see if from = ak and = cm, it follows that + = (a + c)n for some integer n. Counterexample: Lets consider specific numbers to see if this statement holds. - Choose = 2, = 4, = 3, = 6. - Clearly, 2 4 because 4 = 2 2. - Clearly, 3 6 because 6 = 3 2. - Now, + = 2 + 3 = 5 and + = 4 + 6 = 10. We need to check if 5 10. Since 10 = 5 2, in this case, 5 does actually divide 10. However, we need to check more examples to see if there are any that fail. Another Example: - Choose = 2, = 4, = 3, = 9. - Again, 2 4 since 4 = 2 2. - 3 9 since 9 = 3 3. - Calculate + = 2 + 3 = 5 and + = 4 + 9 = 13. Now, we need to check if 5 13. Since 13 is not divisible by 5 (there is no integer such that 13 = 5n), this proves that 5 does not divide 13. Conclusion: The counterexample where = 2, = 4, = 3, = 9 disproves the statement. Even though divides and divides d, + does not necessarily divide + d. Thus, the statement for all integers a, b, c, if and d, then + + is false. Figure 17: Example from the PPE-Human test set where the chosen responses proof is wrong while the rejected responses proof is correct, and the world preference model identify this error. 29 Query userscript to copy the text of the hovered link when the key combination alt+c is pressed. just present the code and make it concise. Chosen Response (World Preference Model Score: -0.66) // @require https :// code . jquery . com / jquery -3.6.0. min . js $ ( document ). keydown ( function ( event ) { if ( event . altKey && event . keyCode == 67) { var link = $ (\" hoveredElement \") . text () ; clipboard . writeText ( link ); } }) ; Rejected Response (World Preference Model Score: 1.42) // == UserScript == // @name // @namespace // @version // @description // @author // @match // @grant // ==/ UserScript == Copy Hovered Link Text http :// tampermonkey . net / 1.0 Copy the text of the hovered link when Alt + is pressed You *://*/* none ( function () { ' use strict '; let hoveredLinkText = ''; document . addEventListener ( ' mouseover ' , function (e) { if (e. target . tagName . toLowerCase () === 'a ') { hoveredLinkText = e. target . innerText e. target . textContent ; } }) ; document . addEventListener ( ' keydown ' , function (e) { if (e. altKey && e. key === 'c ') { if ( hoveredLinkText ) { navigator . clipboard . writeText ( hoveredLinkText ). then (() => { console . log (' Text copied to clipboard : ', hoveredLinkText ) ; }) . catch ( err => { console . error (' Failed to copy text : ', err ); }) ; } } }) ; }) () ; Figure 18: Example from the PPE-Human test set, where Chosen uses an invalid selector $(hoveredElement), while Rejected provides correct solution. 30 Query gpt 4o? answer is only yes or no Chosen Response Rejected Response Yes No. Figure 19: Example from the PPE-Human test set, where the relative quality cannot be determined. Query (From freelancing.stackexchange) Title: Where can front end web developer start freelancing? Lets say youre beginning web development, and you want to start earning money. Where do you get started? know portfolio would be start, but dont know where to apply just as front-end developer for jobs. dont know and dont want to know PHP, Ruby, Rails, and other stuff. just want to do front end development part time, because find it interesting. By the way, if it helps, live in NYC. Chosen Response (Upvotes: 11) The challenge that youre going to run into with Freelance web development is that, in my experience, many Web developers have pretty broad understanding of the entire development stack from databases to server-side coding to front-end HTML, CSS, and JavaScript. Thus, if youre just starting out and wish to avoid learning other parts of the stack, you may run into trouble finding work. In my experience, when Freelancer needs to outsource something, its because he or she needs specialist in that area. Using myself as an example, lack knowledge in design, so Im less likely to find work building websites since most people who build websites can also manage to do the little bit of scripting necessary to make the site functional. However, specialize in JavaScript. If you have Web application and are stuck somewhere or are doing something complicated, thats where come in. If youre going to specialize in nothing but the frontend, youre going to need to specialize in it, which means youre going to need to be rally good, write really clean, W3C-valid HTML code that works in every browser, and have deep understanding of front-end User Experience. Even then, you need to understand at least little server-side code. The best front-end coders Ive ever worked with still knew little PHP so that we could understand how we were going to integrate things. They werent experts in it, but they werent ignorant to it either; its naive to think that you can get away with never ever knowing how the server-side works. With that said, you have to start somewhere, and the best way for you to do that is to look for small jobs that require just minor changes to existing front-end code. This will help build your skill-set in that area while hopefully giving you some income. Rejected Response (Upvotes: 8) If youre doing front end development, you will need portfolio before you get any big jobs, end of story. So how do you get portfolio without the jobs? What did before was make sample pages for friends of the family, and couple volunteer organisations belonged to. It was all under the impression that they only paid if they used my designs, but accept that its just start. could at least get some styles that could do. That portfolio should be online though on your personal website, so potential customers can see your skill, design, and technique. Again, just make designs, no matter who for, and make sure they show your best talents Figure 20: Example 1 drawn from StackExchange training dataset. Query (From physics.stackexchange) Title: Could non-rotating planet, in binary situation, have gravity? Is it possible for singular planet to rotate around single sun in the same way that our moon rotates around the Earth? If so, would it be possible for that planet, if as large or larger than Earth, to have gravity? Chosen Response (Upvotes: 4) Yes. Literally everything that has mass (and even some things that dont) will have gravity. It is certainly possible for single planet to orbit single star (as opposed to our planet, for example, which is accompanied by the other planets of the solar system). In fact, if anything, single planet will be *more* stable because it will not be perturbed by other massive bodies. But in all of these cases, gravity will certainly be present. For example, the reason planet will even stick together to *be* planet is because gravity holds it together. Rejected Response (Upvotes: 0) Responding directly: Yes it is possible and is expected in cosmological time scale. It is expected for both bodies to have the same face to the other with sufficient time, but the one of smaller mass will generally slow its rotation with respect to the body of major mass first, as strictly that phenomenon is connected to the angular moment of rotation, that moment is proportional to mass and speed of rotation, but other factors influence the loss of rotation (braking), such as the existence of fluids in the celestial body (plasma in the stars and atmosphere and water or other fluid in the planets and satellites, the denser the braking of the celestial body, the energy in this case becomes tidal energy in the fluids. What speeds up the process is proximity. The body that will first present the same face is the one that has the lowest energy (moment) and the set of conditions that predispose them to lose energy faster, such as less mass, lack of atmosphere and oceans. The earth does not yet have the same face for the Moon, but millions of years ago the day on earth lasted only about 10 hours, from here few million years ago when the day on earth lasts for lunar month, both the Moon and Earth will show the same face and will be synchronized. In his question the case is of the existence of single planet in the system, this only facilitates the synchronization because there is less gravitational perturbation of other celestial bodies of great mass, but remembering that the proximity is the main factor in the speed of the process. See Wikipedia article: Gliese 581c Search the internet for these titles for more information: Tidal coupling (Tidal Locking) You will see that the Pluto system and its moon Charon are already synchronized each showing the same face to the other component of the system and there is planet in the star Tau Bootes that presents this synchronization. There is also an excellent article on the subject in the book Asimov explains by Isaac Asimov Answering the second question: Yes, the planet independent of size will have gravity, but the resulting value will be different on each side. On the side permanently presented to the star will be smaller than on the other side but will not be zero. There will be point between the star and the planet where gravity will vanish, but it will still be off the planet. The loss of momentum of rotation partly alters the moment of translation, perhaps making it happen to the planet what already happens to our Moon that moves away from Earth at speed of 3.78 centimeters per year, with the possibility of tearing from the earth. Excuse my english please. Figure 21: Example 2 drawn from StackExchange training dataset. 32 Query (From tex.stackexchange) Title: How to add custom C++ keywords to be recognized by Minted? have defined several structures and types in C++ project. have to write some documentation for this code, and use the minted package to typeset my code. would like to be able to add some keywords to the minted database, so minted will also recognize these and give them the appropriate color. (I would like to have them colored in the same way type, eg. double would be colored.) [1]: http://www.ctan.org/pkg/minted Chosen Response (Upvotes: 14) Minted uses Pygments to parse source-code, so one way to go about this is by modifying pygments/lexers/compiled.py and adding your types there (Somewhere about line 210 looks reasonable to me). This is what had to do when Pygments was incorrectly parsing symbol in some Erlang Id written. The obvious disadvantage of this is that Pygments is now non-standard. This will mean number of things: - This change has to be made on all PCs that want to compile this document, meaning the document will be built inconsistantly or lots of people have to have modified Pygments installations. - Future updates to Pygments could break your modifications and so your document may not be correctly syntax-highlighted in the future. - You may incorrectly parse/highlight future documents if you use these keywords in different way (So it may print variable name in the same way it prints type, for example). Rejected Response (Upvotes: 4) Another (obvious) solution would be to write your own lexer (i.e. superset of c++) [1]: http://pygments.org/docs/lexerdevelopment/ Figure 22: Example 3 drawn from StackExchange training dataset. 33 Query (From math.stackexchange) Title: Does every continuous map from to extends continuously as map from to R? Given continuous function : ,does there exist continuous function : such that gQ = ? What have no Idea about how to attempt this Question! Any suggestion will be very helpful. Chosen Response (Upvotes: 16) Not in general. Consider the function f0 : defined by f0(x) = (cid:26)0 1 if x2 < 2 if x2 > 2 . It is easy to see that f0 is continuous on (since to R, then g0 is necessarily discontinuous at 2. It is worth noting that if : can be extended to continuous function : R, then such an extension is unique. That is, if : and : are continuous functions such that g(x) = h(x) for all Q, then = h. This is because is dense subset of R. 2 / Q), but if g0 : is an extension of f0 Rejected Response (Upvotes: 6) Yes, its possible, iff is sequentially continuous, in the sense that if an and bn are two sequences of rational numbers that converge to the same real number, then (an) and (bn) are two converging sequences that converge to the same real number. If you want characterisation that doesnt mention at all, you probably have to use the notion of Cauchy: an and bn are Cauchy sequences such that an bn converges to 0. Figure 23: Example 4 drawn from StackExchange training dataset."
        }
    ],
    "affiliations": [
        "Institute of Trustworthy Embodied Artificial Intelligence, Fudan University",
        "Qwen Team, Alibaba Group",
        "School of Computer Science, Fudan University"
    ]
}