{
    "paper_title": "MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model",
    "authors": [
        "Hsiao-Ying Huang",
        "Yi-Cheng Lin",
        "Hung-yi Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large audio-language models (LALMs) show strong zero-shot ability on speech tasks, suggesting promise for speech emotion recognition (SER). However, SER in real-world deployments often fails under domain mismatch, where source data are unavailable and powerful LALMs are accessible only through an API. We ask: given only unlabeled target-domain audio and an API-only LALM, can a student model be adapted to outperform the LALM in the target domain? To this end, we propose MI-Fuse, a denoised label fusion framework that supplements the LALM with a source-domain trained SER classifier as an auxiliary teacher. The framework draws multiple stochastic predictions from both teachers, weights their mean distributions by mutual-information-based uncertainty, and stabilizes training with an exponential moving average teacher. Experiments across three public emotion datasets and six cross-domain transfers show consistent gains, with the student surpassing the LALM and outperforming the strongest baseline by 3.9%. This approach strengthens emotion-aware speech systems without sharing source data, enabling realistic adaptation."
        },
        {
            "title": "Start",
            "content": "MI-FUSE: LABEL FUSION FOR UNSUPERVISED DOMAIN ADAPTATION WITH CLOSED-SOURCE LARGE AUDIO-LANGUAGE MODEL Hsiao-Ying Huang*, Yi-Cheng Lin*, Hung-yi Lee National Taiwan University, Taiwan 5 2 0 2 5 2 ] . [ 1 6 0 7 0 2 . 9 0 5 2 : r ABSTRACT Large audiolanguage models (LALMs) show strong zero-shot ability on speech tasks, suggesting promise for speech emotion recognition (SER). However, SER in real-world deployments often fails under domain mismatch, where source data are unavailable and powerful LALMs are accessible only through an API. We ask: given only unlabeled target-domain audio and an API-only LALM, can student model be adapted to outperform the LALM in the target domain? To this end, we propose MI-Fuse, denoised label fusion framework that supplements the LALM with source-domain trained SER classifier as an auxiliary teacher. The framework draws multiple stochastic predictions from both teachers, weights their mean distributions by mutual-information-based uncertainty, and stabilizes training with an exponential moving average teacher. Experiments across three public emotion datasets and six cross-domain transfers show consistent gains, with the student surpassing the LALM and outperforming the strongest baseline by 3.9%. This approach strengthens emotion-aware speech systems without sharing source data, enabling realistic adaptation. Index Terms Speech emotion recognition, Source-free unsupervised domain adaptation, Large audiolanguage models, Mutual information 1. INTRODUCTION LALMs such as Desta2.5-Audio [1], Qwen2.5-Omni [2], and Gemini 2.5 [3] have recently demonstrated impressive general-purpose capabilities across spoken language understanding, paralinguistics, and speaker-related tasks [4, 5]. Their versatility and strong zeroshot performance highlight their potential as universal backbones for various speech processing problems, including speech emotion recognition (SER) [6]. SER is essential in applications ranging from healthcare and mental health monitoring to empathetic virtual assistants and call center analytics [7, 8]. However, real-world deployment of SER systems remains challenging because performance often degrades under domain mismatch, especially when training and deployment differ in corpus, speakers, channel/noise, or language [9, 10]. In practice, two constraints frequently arise: (i) the sourcedomain data used to train specialized SER models is unavailable at adaptation time due to privacy and ownership restrictions [11], and (ii) the target domain we need to serve is unlabeled. This combination yields the source-free unsupervised domain adaptation (SFUDA) setting: we must adapt to the target domain using only an already-trained source model and unlabeled target audio. Modern deployments pose an even harder challenge. State-ofthe-art LALMs, such as Gemini, are closed-source and accessible only through an API, preventing fine-tuning or inspection of their parameters. We therefore study harder, practical SFUDA protocol *Equal Contribution in which the source model is black-box LALM. Concretely, we use the LALM as the source model for SFUDA training and can query it, but cannot fine-tune or inspect its weights. Beyond LALMs, practitioners also often have domain-specific teacher classifiers trained on different corpora, which they wish to evaluate on new target domain. Our goal is not just to transfer knowledge, but to train student that surpasses the LALM and the domain-specific teacher in the target domain. This motivates us to the central research question: given only unlabeled target-domain audio and an API-only LALM, can we adapt student model for SER that outperforms the LALM in the target domain? To answer this question, we propose MI-Fuse, denoised label fusion framework that supplements the LALM with an additional source-domain trained SER classifier acting as an auxiliary teacher. Specifically, we generate multiple stochastic predictions from each teacher and compute their mean distributions. We then merge them using weighted rule based on mutual information, where lower uncertainty across multiple generations receives higher weight. To stabilize training, the classifier teacher is further updated via an exponential moving average (EMA) of the student model, ensuring that supervision evolves smoothly across iterations. In summary, the main contributions of this work are as follows: We formalize realistic and harder SFUDA scenario for speech in which the source model is closed-source, API-only LALM, aligning with real deployment constraints. We propose denoised label fusion approach that integrates mutual-information weighting and an EMA-updated classifier teacher to mitigate noisy pseudo-labels during adaptation. We conduct extensive experiments on multiple SER datasets, demonstrating that our approach consistently outperforms existing SFUDA methods and achieves better adaptation performance. 2. RELATED WORK To adapt source-domain trained model to an unlabeled target domain, most SFUDA methods take inspiration from the semisupervised learning (SSL) field and employ some common SSL techniques during adaptation to make full use of the unlabeled target data and conquer domain mismatch at the same time. These techniques could be broadly categorized into pseudo-labeling, consistency regularization, and clustering-based training. Common strategies for generating pseudo-labels to guide training involve class centroid-clustering [12, 13], neighborhood aggregation (affinity) [14], label ensembling from more than one model [15, 16, 17], and some even integrating complementary labels [18] into the process. Further, consistency regularization aims to improve model robustness by enforcing consistent network predictions under either data or model variations [19], which may also serve as another pseudo-labeling source under some settings. Besides pseudo-labeling signals (instruction), many works also integrate entropy minimization or information maximization [20] into the 3.2. Model uncertainty estimation In our adaptation framework, pseudo-labels are obtained from two teachers: the fLALM and fcls. Under domain shift, their predictions may be noisy, and directly trusting them risks propagating errors to the student ftgt. We address this by explicitly estimating each teachers uncertainty and using it in label fusion. We quantify uncertainty using mutual information (MI) between the predicted label and the model parameters Θ (either fLALM or fcls), conditioned on an input x. MI reflects how much predictions vary across stochastic perturbations such as dropout. For each input x, we perform stochastic forward passes through teacher model (fLALM or fcls). This produces set of predictive distributions {pk(yx)}K k=1 (detailed in Sec. 3.3). The average distribution is p(yx) = 1 (cid:88) k= pk(yx). From this, the predictive entropy (cid:88) H(p) = p(yx) log p(yx) captures the total uncertainty, while the expected entropy 1 (cid:88) k=1 H(pk) (1) (2) (3) reflects only aleatoric uncertainty, i.e., ambiguity inherent in the input signal. Their difference, I(Y, Θ x) = H(p) 1 (cid:88) k=1 H(pk), (4) is the mutual information (MI), which captures epistemic uncertainty as the models disagreement across stochastic predictions. This measure is directly applicable to SFUDA. high MI value means the teachers predictions vary significantly across samples, signaling that its output is unstable under domain shift and should be down-weighted. low MI value indicates stable and consistent predictions that are more trustworthy. We weight each teachers contribution by eM to suppress noise and amplify consistency. Integrating MI into our label fusion mechanism provides the student model ftgt with less noisy and more robust pseudo-labels. This allows ftgt to benefit from both the generalization ability of the LALM and the domain-specific knowledge of the classifier, while mitigating the risk of propagating errors from either teacher. 3.3. Label fusion We propose novel label fusion framework for SFUDA in SER. Our approach integrates predictions from source-pretrained SER model and an LALM to mitigate the noisy label problem. We begin by initializing the student model ftgt with the parameters of the classifier teacher fcls. The student is then adapted using only unlabeled target domain data. For each unlabeled target sample xt, we apply Monte Carlo (MC) dropout [22] to the classifier teacher and obtain Ncls stochastic forward passes. This produces set of predicted probability distributions {p(1) cls (yxt), p(2) cls (yxt), . . . , p(N ) cls (yxt)}. The mean distribution is used as the aggregated classifier teacher prediction: pcls(yxt) = 1 Ncls Ncls(cid:88) i=1 p(i) cls (yxt). (5) Fig. 1. Overview of the SFUDA setting and our proposed method. process to reduce the uncertainty of network predictions, and further promote clustering among the target features based on the clustering assumptions [21] in SSL. Unlike prior SFUDA works that rely solely on traditional SSL or self-training techniques, our method introduces general-domain guidance by leveraging LALMs during adaptation. Specifically, we fuse pseudo-labels from general-domain LALM with those from source-domain teacher, enabling complementary supervision for more robust target-domain adaptation under source-free constraints. 3. METHODOLOGY 3.1. Problem formulation We study SFUDA for SER. The overall workflow is illustrated in Fig. 1. Suppose we have labeled source-domain dataset Ds = {(xs, ys)}, where xs is speech utterance, ys is its emotion label. The set of all possible labels is denoted by = {1, ..., C}, where is the number of emotion classes. classifier fcls is first trained on Ds and captures domain-specific SER knowledge. In the SFUDA setting, the original source dataset Ds is no longer accessible due to privacy or storage restrictions. We may still keep the trained classifier fcls, but its performance often degrades under domain shift. In addition, we assume access to closed-source LALM, denoted fLALM , which is available only through an API. While fLALM offers strong generalization, its predictions can be noisy and cannot be fine-tuned or inspected internally. Finally, we are given an unlabeled target-domain dataset Dt = {xt}. The target distribution is different from the source distribution because of mismatched corpora, speakers, recording channels, or languages. The labels yt are unknown, and the goal is to adapt student model ftgt that performs well on the target domain without using any labeled target data. major challenge is that adaptation typically relies on pseudolabels predicted by the source model, which can be noisy under domain shift. To overcome this, we combine the predictions of the LALM fLALM and the auxiliary classifier fcls into denoised label distribution. This fused supervision provides more reliable training signal for the student model ftgt, enabling it to generalize more effectively to the target domain. In parallel, we query the LALM NLM times with carefully designed prompts that request probability distribution over emotion classes in natural language. This yields {p(1) LM (yxt), p(2) LM (yxt), . . . , p(N ) LM (yxt)}. We then compute the mean LALM probability distribution: pLM(yxt) = 1 NLM NLM(cid:88) i=1 p(i) LM(yxt). (6) We fuse the outputs from two models by weighted averaging. The weights are determined by the exponential of the negative mutual information (MI) associated with each models distribution, pfused(yxt) = eMI1 pcls(yxt) + eMI2 pLM(yxt) eMI1 + eMI , (7) thereby assigning greater importance to the model with lower uncertainty. We set NLM = 5 and Ncls = 8 passes in our experiments. 3.4. Diversity Loss and EMA Teacher Update The student model ftgt is trained on the fused pseudo-labels by minimizing cross-entropy loss with soft targets. However, relying solely on pseudo-label supervision risks two common issues in SFUDA: (1) class collapse, where the model overfits to subset of emotions while ignoring others, and (2) unstable supervision, where the teachers predictions degrade over time due to noisy labels and representation drift. To address these issues, we introduce two complementary mechanisms: diversity loss and an exponential moving average (EMA) teacher update. Diversity loss. To prevent class collapse, we encourage the model to maintain high entropy across predictions at the batch level, following [23]. Specifically, given batch of student predicted probability distributions {ptgt(yxi i=1, we compute the average prediction: t)}B 1 (cid:88) i=1 The diversity loss is then defined as Ldiv = H(pbatch), which pushes the model to spread its predictions more evenly across emotion classes. In practice, this promotes balanced learning and reduces the effect of noisy pseudo-labels. EMA teacher update. To mitigate unstable supervision, we update the classifier teacher fcls as an exponential moving average of the student ftgt. At each step, the teacher parameters θcls are updated by θcls α θcls + (1 α) θtgt, (9) where θtgt are the student parameters and α [0, 1) is momentum factor. This ensures that the teacher evolves smoothly with the student, filtering out short-term noise and providing more stable guidance during training. We use α = 0.999. Overall objective. The final training objective of the student model combines supervised alignment with regularization: = LCE + λdivLdiv, (10) where LCE is the cross-entropy between the student predictions and the fused pseudo-labels, and λdiv are hyperparameters balancing the contributions of the two regularizers. We set λdiv = 1. 4. EXPERIMENTS 4.1. Setup Dataset. This study uses three publicly available emotion databases, MSP-Podcast[24], IMPROV[25], and IEMOCAP[26], which are denoted as POD, IMP, and IEM, respectively. They cover real-world and acted emotions across different ethnic groups, ensuring diversity. Both IMPROV and IEMOCAP undergo cross-validation settings, including six and five folds for thorough evaluation, while theres only one fold in Podcast. To perform cross-dataset learning, we further filter the datasets to 4 emotion categories (happy, sad, angry, and neutral). We use unweighted accuracy as the evaluation metric. Model. We use Gemini 2.5 flash as the LALM studied, with temperature of 0.6 for text generation. For the classifier teacher and student, we use the WavLM base+ model [27] with weighted sum across layers for feature extraction. The extracted representations are then passed through two linear layers to predict the final emotion category, following [28, 29, 30]. The classifier teacher is trained on the source domain using cross-entropy loss LCE. Optimization. We optimize the networks using the AdamW [31] optimizer, with batch size of 32. For regularization, we apply dropout with rate of 0.4 in the linear layers, and an L2 regularization with weight 0.1. The teacher classifier fcls is trained with learning rate of 5e-4, and the student model ftgt is scanned through learning rates of {7.5e-4, 5e-4, 1e-4, 5e-5, 1e-6}. The models are trained until the loss stops decreasing for 1000 steps. 4.2. Compare with other methods We compare six baselines spanning zero-shot inference, singleteacher target adaptation, and standard SFUDA. No baseline uses source-domain audio. LLM zero-shot: Query the LALM fLALM for class probabilities and take argmax. No target training. Source model zero-shot: Evaluate the domain-specific classifier fcls directly on target utterances. No target training. LALM / Source model SFUDA: Adapt student ftgt on unlabeled target data using pseudo-labels from fLALM/fcls only; the other teacher is unused. SHOT [12]/ NRC [14]: State-of-the-art SFUDA methods impleOverall performance. Our denoised label fusion framework consistently outperforms all baselines. Averaged across six transfer settings, MI-Fuse achieves 58.38% unweighted accuracy, which is 3.9% higher than the best-performing baseline (LALM SFUDA). This demonstrates that combining the API-based LALM with source-trained classifier through denoised label fusion provides more reliable supervision than relying on either teacher alone. Performance on individual transfers. MI-Fuse achieves the highest accuracy in four of the six transfer directions. For example, in the IMP POD setting, the LALM zero-shot baseline already performs strongly (61.44%), but MI-Fuse further improves to 61.92%. In the IMP IEM setting, where the source model performs better than the LALM (53.75% vs. 45.96%), our framework effectively integrates both sources of information and raises performance to 59.09%, far exceeding the strongest SFUDA baselines SHOT (50.13%) and NRC (52.09%). These results highlight the advantage of uncertainty-aware label fusion when teacher reliability varies across tasks. Competitive performance in challenging cases. Even in the two settings where MI-Fuse does not achieve the top score, it still ranks second. For instance, in IEM POD, our accuracy of 59.85% is close to the LALM zero-shot performance of 61.44%. Importantly, while conventional SFUDA methods such as SHOT and NRC are sometimes competitive when adapting to the IEMOCAP corpus, they lag behind in other domains. Our approach maintains consistently high performance across all transfer directions, demonstrating stronger generalization. pbatch(y) = ptgt(yxi t). (8) mented with our backbone using fcls teacher. Table 1. Performance across dataset transfer tasks in Accuracy (%). Best results are bolded and second-best are underlined. Method IMP POD POD IMP IEM IMP IMP IEM POD IEM IEM POD Avg. LALM SFUDA LALM zero-shot Source model SFUDA Source model zero-shot SHOT NRC MI-Fuse 60.59 61.44 41.34 41.37 41.58 41.37 61. 56.74 53.66 56.74 56.74 56.51 56.74 57.48 51.75 53.66 51.48 50.50 50.64 50.48 54. 48.40 45.96 53.75 49.08 50.13 52.09 59.09 51.27 45.96 53.85 41.27 55.94 59.61 57. 58.12 61.44 48.90 48.90 48.90 48.90 59.85 54.48 53.69 51.01 49.64 50.62 51.53 58. Table 2. Ablation study on fusion strategies on IEMOCAP (Accuracy %). Best performances are in bold. Generation Similarity Weighting IMP IEM POD IEM Multi Single Direct KL MI (Ours) Entropy Equal MI Entropy Equal No Fusion - Direct KL Entropy Equal Entropy Equal No Fusion - 59.09 57.34 57.98 55.23 55.13 55.17 56.08 56.82 58.26 54.05 54.16 54. 57.07 55.53 56.64 55.86 55.09 55.12 55.43 55.85 55.23 51.71 51.79 51. 4.3. Ablation study To validate the effectiveness of our label fusion framework, we implement plausible alternatives for each component. (i) Generation strategy: Multi uses NLM = 5, Ncls = 8 stochastic passes for teachers (temperature sampling for fLALM and Monte Carlo dropout for fcls) and averages the resulting distributions; Single disables stochasticity (NLM =1) for fLALM using zero temperature. (ii) Similarity gate: Direct fusion always fuses the two mean distributions; KL fuses only when DKL(pclspLM) τ (with τ grid-searched from {0.4, 0.6, 0.8}), otherwise it selects the lower-entropy teacher; No Fusion always uses the labels from the teacher with lowest entropy. (iii) Weighting: when fusing, we combine soft labels from teachers using one of: MI (weights eMI per teacher), Entropy (weights eH ), or Equal (uniform average). Table 2 shows that our full method MI-Fuse (Multi + Direct Fusion + MI) achieves the best accuracy on both transfer directions. KL gating consistently underperforms direct fusion across weightings, indicating that hard disagreement gating discards useful complementary cues. Single-teacher training is clearly weaker, and entropy-based weighting lags behind MI, confirming the benefit of epistemic-uncertainty aware fusion. 4.4. Training stability analysis Fig. 2 shows how the development set accuracy evolves during the adaptation, revealing that our proposed approach achieves not only higher final accuracy but also more stable training dynamics compared to the classifier teacher and LALM teacher baselines. The classifier teacher declines after 400 steps, due to overfitting on early pseudo-labels from the EMA teacher. The LALM teacher performs the worst, dropping sharply at the start and then stagnating, which Fig. 2. Development set accuracy over training steps on IMP IEM fold 1. Our method (red) consistently outperforms both baselines: the classifier teacher (purple) and the LALM teacher (blue). reflects unreliable predictions under domain shift. In contrast, our method steadily improves throughout training, effectively balancing information from both teachers while suppressing noise. 5. LIMITATION First, MI-Fuse depends on LALMs that can produce meaningful probabilistic predictions over emotion categories. Although models such as Gemini 2.5 are becoming increasingly accessible, their inference cost, latency, and reliance on proprietary APIs may hinder practical deployment in resource-constrained or privacy-sensitive settings. Second, the label fusion scheme assumes fixed set of discrete emotion categories across datasets. In real-world applications, however, emotion taxonomies may vary. This mismatch can hinder the direct applicability of MI-Fuse when adapting to target domains with different label spaces. 6. CONCLUSION We present MI-Fuse, denoised label fusion framework for SFUDA in SER under realistic closed-source LALM constraints. By integrating mutual-information-aware fusion, diversity loss, and an EMA-updated teacher, our approach produces stable pseudo-labels. It effectively balances the strengths of both general-purpose LALMs and domain-trained classifiers. Extensive experiments across multiple datasets demonstrate that MI-Fuse consistently improves crossdomain performance, surpasses strong baselines, and enables student models to outperform closed-source LALMs on target domains. These results establish MI-Fuse as practical recipe for deploying emotion-aware speech systems under realistic constraints. 7. REFERENCES [1] Ke-Han Lu et al., Desta2. 5-audio: Toward general-purpose large audio language model with self-generated cross-modal alignment, arXiv preprint arXiv:2507.02768, 2025. [2] Jin Xu et al., Qwen2. 5-omni technical report, arXiv preprint arXiv:2503.20215, 2025. [3] Gheorghe Comanici et al., Gemini 2.5: Pushing the fronlong context, arXiv preprint tier with advanced reasoning, multimodality, and next generation agentic capabilities, arXiv:2507.06261, 2025. [4] Chien yu Huang et al., Dynamic-SUPERB phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks, in The Thirteenth International Conference on Learning Representations, 2025. [5] Haibin Wu et al., Towards audio language modelingan overview, arXiv preprint arXiv:2402.13236, 2024. [6] Jaime Bellver et al., Multimodal audio-language model for speech emotion recognition, in The Speaker and Language Recognition Workshop (Odyssey 2024), 2024, pp. 288295. [7] Nelly Elsayed et al., Speech emotion recognition using supervised deep recurrent system for mental health monitoring, in 2022 IEEE 8th World Forum on Internet of Things (WF-IoT), 2022. [8] Farideh Majidi and Marzieh Bahrami, Utilizing speech emotion recognition and recommender systems for negative emotion handling in therapy chatbots, arXiv preprint arXiv:2311.11116, 2023. [9] Cheng Lu et al., Progressively discriminative transfer network for cross-corpus speech emotion recognition, Entropy, vol. 24, 2022. [10] Siddique Latif et al., Self supervised adversarial domain adaptation for cross-corpus and cross-language speech emotion recognition, IEEE Transactions on Affective Computing, vol. 14, no. 3, pp. 19121926, 2023. [11] Fabian Ritter-Gutierrez et al., Dataset-Distillation Generative Model for Speech Emotion Recognition, in Interspeech 2024, 2024, pp. 26402644. [12] Jian Liang, Dapeng Hu, and Jiashi Feng, Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation, in International conference on machine learning. PMLR, 2020, pp. 60286039. [13] Jian Liang et al., Source data-absent unsupervised domain adaptation through hypothesis transfer and labeling transfer, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 11, pp. 86028617, 2021. [14] Shiqi Yang et al., Exploiting the intrinsic neighborhood structure for source-free domain adaptation, Advances in neural information processing systems, vol. 34, pp. 2939329405, 2021. [15] Nazmul Karim et al., C-sfda: curriculum learning aided self-training framework for efficient source free domain adaptation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 2412024131. [16] Antti Tarvainen and Harri Valpola, Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results, Advances in neural information processing systems, vol. 30, 2017. [17] Yuang Liu et al., Source-free domain adaptation for semantic segmentation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 1215 1224. [18] Jincen Wang et al., Confidence-aware hypothesis transfer networks for source-free cross-corpus speech emotion recognition, in Proc. Interspeech 2024, 2024, pp. 10501054. [19] JoonHo Lee and Gyemin Lee, Feature alignment by uncertainty and self-training for source-free unsupervised domain adaptation, Neural Networks, vol. 161, pp. 682692, 2023. [20] Andreas Krause et al., Discriminative clustering by regularized information maximization, Advances in neural information processing systems, vol. 23, 2010. [21] Xiangli Yang et al., survey on deep semi-supervised learning, IEEE transactions on knowledge and data engineering, vol. 35, no. 9, pp. 89348954, 2022. [22] Yarin Gal and Zoubin Ghahramani, Dropout as bayesian approximation: Representing model uncertainty in deep learning, in Proceedings of The 33rd International Conference on Machine Learning. 2016, vol. 48 of Proceedings of Machine Learning Research, pp. 10501059, PMLR. [23] Jian Liang et al., Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation, in Proceedings of the 37th International Conference on Machine Learning, 2020. [24] Reza Lotfian and Carlos Busso, Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings, IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 471483, 2019. [25] Carlos Busso, Srinivas Parthasarathy, Alec Burmania, Mohammed AbdelWahab, Najmeh Sadoughi, and Emily Mower Provost, Msp-improv: An acted corpus of dyadic interactions to study emotion perception, IEEE Transactions on Affective Computing, vol. 8, no. 1, pp. 6780, 2017. [26] Carlos Busso et al., Iemocap: Interactive emotional dyadic motion capture database, Language resources and evaluation, 2008. [27] Sanyuan Chen et al., Wavlm: Large-scale self-supervised pretraining for full stack speech processing, IEEE Journal of Selected Topics in Signal Processing, 2022. [28] Yi-Cheng Lin et al., Emo-bias: Large Scale Evaluation of Social Bias on Speech Emotion Recognition, in Interspeech 2024, 2024, pp. 46334637. [29] Yi-Cheng Lin et al., Emo-debias: Benchmarking gender debiasing techniques in multi-label speech emotion recognition, arXiv preprint arXiv:2506.04652, 2025. [30] Yi-Cheng Lin et al., Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: Pseudo-Labeling and Unsupervised Learning Approach, in Interspeech 2025, 2025. [31] Ilya Loshchilov and Frank Hutter, Decoupled weight decay regularization, in International Conference on Learning Representations, 2019."
        }
    ],
    "affiliations": [
        "National Taiwan University, Taiwan"
    ]
}