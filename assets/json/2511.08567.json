{
    "paper_title": "The Path Not Taken: RLVR Provably Learns Off the Principals",
    "authors": [
        "Hanqing Zhu",
        "Zhenyu Zhang",
        "Hanxian Huang",
        "DiJia Su",
        "Zechun Liu",
        "Jiawei Zhao",
        "Igor Fedorov",
        "Hamed Pirsiavash",
        "Zhizhou Sha",
        "Jinwon Lee",
        "David Z. Pan",
        "Zhangyang Wang",
        "Yuandong Tian",
        "Kai Sheng Tai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR. Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 6 5 8 0 . 1 1 5 2 : r The Path Not Taken: RLVR Provably Learns Off the Principals Hanqing Zhu1,2,, Zhenyu Zhang2, Hanxian Huang1, DiJia Su1, Zechun Liu1, Jiawei Zhao1, Igor Fedorov1, Hamed Pirsiavash1, Zhizhou Sha2, Jinwon Lee1, David Z. Pan2, Zhangyang Wang2,, Yuandong Tian1,, Kai Sheng Tai1, 1Meta AI, 2The University of Texas at Austin Work done during an internship at Meta AI., Equal advisory contribution. Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves large-language-model reasoning while apparently modifying only small fraction of parameters. We revisit this paradox for fixed and show that sparsity is surface artifact of model-conditioned optimization bias: pretrained model, updates consistently localize to model-preferred parameter regions, remain highly consistent across runs, and are largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with Three-Gate Theory: Gate (KL Anchor) imposes KL-constrained update; Gate II (Model Geometry) steers steps off principal directions into low-curvature, spectrumpreserving subspaces; Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, to our knowledge, provide the first parameter-level characterization of RLVRs learning dynamics: RLVR learns off-principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR. Together, these results provide the first parameter-space account of RLVRs training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts path toward white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics. Author emails:: hqzhu@utexas.edu"
        },
        {
            "title": "1 Introduction",
            "content": "Large Reasoning Models (LRMs), such as OpenAI-o3 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025), have advanced the ability of large language models to solve complex mathematical and programming tasks. key driver is large-scale Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple, easy-to-verify rewards to incentivize complex, multi-step reasoning. Yet, despite these advances, the mechanisms by which RL shapes model representations and behavior remain poorly understood. Given the substantial computational resources devoted to RL (xAI, 2025), especially relative to supervised fine-tuning (SFT), and the emergence of striking new behaviors, one might naturally assume that such progress arises from significant parameter changes. However, recent evidence points in the opposite direction: RL induces sparse parameter updates, whereas SFT yields dense ones (Mukherjee et al., 2025). This counterintuitive finding reveals paradox, high-cost, high-gain process that relies on surprisingly minimal weight modification. Key observation. We resolve this paradox by uncovering deeper mechanism behind the apparent sparsity: persistent, model-conditioned optimization bias. For fixed pretrained model, this bias concentrates visible updates into narrow, stable subset of parameters and remains strikingly invariant across diverse algorithms 1 Figure 1 SFT vs. RLVR: optimization geometry and evidence. (a) SFT follows an externally guided route and traverses high-curvature directions (over the mountain) to reach the target. (b) RLVR, without an explicit teacher, behaves as if steered by an implicit compass (a model-conditioned optimization bias), taking low-curvature detour. (c) Evidence. Left: positional maps comparing the update mask (non-zero parameter updates) with the principal mask (positions aligned with top-k singular subspaces, defined by the largest-magnitude entries of the rank-k SVD reconstruction Liu et al. (2025c); details in Sec. 4.2). RLVR updates avoid principal-weight positions, whereas SFT targets them (Meng et al., 2024a; Liu et al., 2025c). Right: principal-angle curves of the top-k subspaces show that RLVR rotates less (spectrum preserved), while SFT rotates more. and datasetsa model-conditioned feature. bfloat16 precision further accentuates the apparent sparsity by attenuating micro-updates in non-preferred regions. As illustrated in Fig. 1, we depict this bias as an implicit compass: unlike SFT with an explicit teacher, RLVR is subtly guided during optimization even without one. Research Question. These phenomena raise central question about RLs learning dynamics: Where does this optimization bias originate, and how does it shape parameter evolution during training? Mechanistic explanation. We formalize the mechanism behind RLVRs optimization dynamics through Three-Gate Theory. Gate (KL Anchor) enforces KL-constrained update at each on-policy step. Gate II (Model Geometry) then steers this update off the principal directions toward low-curvature, spectrumpreserving subspaces embedded in the structured optimization landscape of pretrained model, unlike training from randomly initialized model. This geometry gate explains the model-conditioned nature of the bias: it arises from the pretrained landscape rather than particular datasets or RL recipes. Gate III (Precision) acts as realization filter by hiding those micro-updates in non-preferred regions, making the off-principal bias appear sparse. Experimental validation. We validate this theory with comprehensive suite of experiments, uncovering striking optimization dynamics: RLVR learns off the principal directions, operating in regime disjoint from SFTs. We show that (i) RLVR preserves the pretrained spectral structure with , whereas SFT distorts it; (ii) RLVR avoids principal weightsthe high-energy directions indicated by rank-k SVD reconstructionswhereas parameter-efficient SFT targets them (Liu et al., 2025c); and (iii) RLVR depends on the pretrained geometry: function-preserving orthogonal rotations abolish the effect of update locality overlap, consistent with model-conditioned optimization bias. Rethinking learning algorithms for RLVR. Beyond characterizing learning dynamics, our findings reveal that RLVR operates in regime fundamentally distinct from SFT. Consequently, direct carry-over of SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, especially those over-aligned with SFTs optimization geometry. (1) Sparse fine-tuning. Restricting updates to principal weights, an SFT prior (Liu et al., 2025c), yields the weakest optimization trajectory and markedly degrades performance, as reflected by both forward-KL drift (Shenfeld et al., 2025) and accuracy. Conversely, updating non-principal, low-magnitude weights, precisely the off-principal regime predicted by our theory, closely tracks the dense RLVR trajectory, validating our parameter-level understanding. (2) LoRA variants. recent report (Schulman & Lab, 2025) finds that low-rank LoRA (even rank-1) can match full-parameter performance in RL. However, our analysis challenges their belief that advanced LoRA variants such as PiSSA (Meng et al., 2024a) bring further gains: PiSSA explicitly targets principal weights-suitable for SFT but misaligned with RLVRs off-principal dynamics. Empirically, PiSSA offers no obvious gain over standard LoRA. Moreover, enforcing principal-direction updates-e.g., via learning-rate scaling, common requirement to match full-parameter performance with low-rank adapters, often destabilizes training and precipitates early collapse. Contributions. Our work makes the following key contributions: Observation (Sec. 2). We identify persistent, model-conditioned optimization bias in RLVR fine-tuning that is largely invariant to datasets and RL variants, yet highly consistent for fixed pretrained model. Theory (Sec. 3). We propose the Three-Gate TheoryKL Anchor, Model Geometry, and Precisionwhich mechanistically explains how RL updates are constrained, steered, and filtered to produce the observed optimization pattern. Evidence (Sec. 4). We provide parameter-level validation contrasting the training dynamics of RL and SFT, including reduced spectral drift, smaller principal-subspace rotation, low overlap with principal weights, and basis-rotation interventions that isolate geometry as the steering core. Insight (Sec. 5). We show that SFT-era sparse and low-rank priors (e.g., principal-targeted variants) are misaligned with RLVRs off-principal dynamics, motivating geometry-aware, RLVR-native learning algorithms. Together, these findings provide the first parameter-space account linking RL optimization dynamics to weight evolution, complementing concurrent work that focuses primarily on policy-level or distributional effects (Wu et al., 2025; Shenfeld et al., 2025). Crucially, RLVR operates in distinct, geometry-driven optimization regime from SFT, calling for the development of RL-native, geometry-aware PEFT methods (see Sec. 5) and marking step toward white-box understanding of RLVR training."
        },
        {
            "title": "2 A Persistent, Model-Conditioned Optimization Bias in RLVR",
            "content": "We begin with the observation that RL induces sparse parameter updates, and go beyond quantification to ask where RL localizes these changes in order to understand the underlying mechanism. Our analysis reveals model-conditioned optimization bias: for fixed pretrained model, RL consistently routes visible updates to specific regions of the network, highly consistent across runs and largely invariant to datasets and RL variants. We further find that the observed sparsity is superficial readout of this bias, amplified by bfloat16 precision, which attenuates micro-updates in non-preferred regions. Model suite. We analyze publicly released checkpoints, as shown in Tab. 1. The suite spans multiple RLVR variants (e.g., GRPO, DAPO, Reinforcement++), diverse data domains, and several model families and types (dense and Mixture-of-Experts). We place particular emphasis on DeepSeek-R1-Distill-Qwen-1.5B (DS-Qwen-1.5B), for which long-horizon RL checkpoint is available (Liu et al., 2025a). This model serves as robust case study given its extensive training for over 3,000 steps on diverse data mixture encompassing mathematics, coding, STEM, logic puzzles, and instruction-following tasks."
        },
        {
            "title": "2.1 A Robust, bfloat16-aware Analysis of Update Sparsity\nA bfloat16-aware probe for unchanged weights. bfloat16 (bf16) is standard in modern RL frameworks like\nverl (Sheng et al., 2024), to improve throughput without compromising performance. However, analyzing\nparameter changes under bf16 requires a careful probe. Its unique numerical format, with only 7 mantissa\nbits for precision, means that the smallest representable difference between two numbers scales with their\nmagnitude. Consequently, a fixed absolute-tolerance check as used in (Mukherjee et al., 2025), is unreliable,\nwhich can over- or under-report the fraction of unchanged weights (see Appendix E.1).",
            "content": "To ensure rigorous report, we adopt numerically robust, bfloat16-aware probe to define the update sparsity sparsitybf16 as the fraction of parameters that remain unchanged. 3 Table 1 Update sparsity in SFT vs. RLVR. Higher sparsitybf16 indicates more weights unchanged. RLVR is consistently much sparser than SFT. Mixed denotes diverse data source combining math, coding, STEM, logic puzzles, and instruction-following Liu et al. (2025a). Finetuned (FT) Model Base Model Qwen-1.5B DS-R1-Distill-Qwen-1.5B DS-R1-Distill-Qwen-1.5B DeepScaleR-1.5B-Preview DS-R1-Distill-Qwen-1.5B DeepCoder-1.5B-Preview DS-R1-Distill-Qwen-1.5B Archer-Code-1.5B DS-R1-Distill-Qwen-1.5B NV-ProRL DS-R1-Distill-Qwen-1.5B NV-ProRL-v2 Data Algorithm Mixed SFT Math GRPO Code GRPO Code GRPO GRPO Mixed Reinforcement++ Mixed sparsitybf16 2.8% 53.8% 45.5% 52.5% 38.4% 36.3% Qwen3-8B-Base Klear-Reasoner-8B-SFT Klear-Reasoner-8B Qwen3-8B-Base Qwen3-8B-Base GT-Qwen3-8B-Base OURS Klear-Reasoner-8B-SFT SFT GRPO GRPO DAPO Qwen3-14B-Base Qwen3-14B-Base UniReason-Qwen3-14B-think-SFT SFT UniReason-Qwen3-14B-RL GRPO Qwen3-4B DS-R1-Distill-Qwen-7B Qwen3-30B-A3B Polaris-4B-Preview Polaris-7B-Preview UloRL-A3B DAPO DAPO GRPO Math+Code Math+Code Math Math Math Math Math Math Math 0.6% 69.5% 79.9% 79.7% 18.8% 68.3% 79.3% 61.7% 91.7% Definition 2.1 (Unchanged Weight in bf16). Let wi, wi be scalars stored in bf16 (finite, nonzero). We say wi is unchanged with respect to wi iff wi wi η max(wi, wi), η = 103. (1) Choosing η=103 < 29 makes equation 1 equivalent to bitwise equality (See Appendix E.2,). Definition 2.2 (bf16-aware Update Sparsity). Write bf16 θ0 θ1 θ0bf16 } and the corresponding sparsity 0,η = { θ1 /bf η η for Def. 2.1. Define the bf16 change count sparsitybf16(θ0, θ1; η) = 1 θ1 θ0 bf16 0,η /n. (2) where is the total number of parameters. Values near 1 indicate few stored changes, while values near 0 indicate dense apparent change. RLVR update sparsity results. As shown in Tab. 1, our analysis confirms that RL yields substantially higher update sparsity than SFT. Across models, SFT sparsity is consistently low (typically 0.6%18.8%), whereas RL sparsity is an order of magnitude higher, ranging from 36% to 92%. However, absolute levels on recent checkpoints are lower than earlier reports (Mukherjee et al., 2025), underscoring the need for bf16-aware probes and re-evaluation on current models. Table 2 Cross-run stability for 13th block. ℓ ] (Def. 2.2) against the base weights"
        },
        {
            "title": "2.2 RLVR Exhibits Model-Conditioned Update Locality\nMagnitude alone does not reveal where changes occur, impeding deep analysis of how sparse changes arise.\nWe therefore examine the updated subnetwork. We use\n5 independent RLVR checkpoints from the same DS-\nQwen-1.5B in Tab. 1, trained on different datasets and\nRLVR algorithms. For each layer ℓ and run r, we first\nform the bf16-aware changed mask M (r)\n/≈bf16\nη\nW 0\nStability across runs (Is the bias persistent?) We first\nanalyze their spatial agreement using Jaccard Overlap.\nFor runs r, s, let A = {(i, j) ∶ M (r)\nℓ,ij = 1} and B = {(i, j) ∶\nM (s)\nℓ,ij = 1}. We report the mean off-diagonal of the\npairwise Jaccard matrix J(A, B) = ∣A∩B∣\npq\n.\np+q−pq\n∣A∪B∣\nAs summarized in Tab. 2, Jaccard is consistently high across runs, confirming a shared footprint when trained\nfrom the same base model, with Jaccard matrix shown in Fig. 12.",
            "content": "Layer O MLP-down MLP-up MLP-gate and compare it to the independent Bernoulli baseline E[J] = 0.430 0.413 0.467 0.373 0.453 0.443 0.437 0.580 0.580 0.597 0.552 0.585 0.578 0.575 Jaccard Overlap Random Baseline = 1[ (r) . ℓ ℓ ℓ 4 Figure 2 Consensus ratio of weight updates. Across five RLVR runs, we plot the 13th layers projections (Q/K/V/O) and the MLP down projection. Lighter bands mark coordinates updated in most runs, revealing stable, stripe-like routing pattern rather than random scatter (zoom in to see fine structure). ℓ,ij R Mℓ,ij(t) and column-wise ratio κℓ,j(t) = 1 mℓ Consensus ratio (Where do updates land?) Stability alone does not indicate where updates land. We therefore r=1 (r) visualize and analyze the consensus ratio Cℓ,ij = 1 , the fraction of runs realizing weight update at coordinate (i, j). Values near 1 indicate that all runs consistently change that weight; values near 0 indicate that none do. As shown in Fig. 2, consensus maps reveal contiguous row/column bands, stripe-like, localized routing rather than scattered noise. Especially, there are obvious row-wise stripes in Q/K/V projections and column-wise stripes in projections. This exposes clear optimization bias: RLVR consistently concentrates updates in specific regions of the parameter matrices for fixed pretrained model, even though the five runs use disjoint data and RL variants. Temporal stability (How does the bias emerge over time?) To examine within-run dynamics, we track the row-wise ratio ρℓ,i(t) = 1 Mℓ,ij(t) across checkpoints at steps. On nℓ DS-Qwen-1.5B (training setting in Appendix C.1), the relative profiles ρℓ,(t) and κℓ,(t) remain aligned while overall density grows as shown in Fig. 3: peaks and troughs persist. The routing bias emerges early and is reinforced over training, indicating temporally stable phenomenon rather than transient artifact. Moreover, the peak is consistent with the bias structure shown in Fig. 2. We also show their remaining column-wise (Q) and row-wise (O) update ratio dynamics in Fig. 14, without clear trend, indicating the bias is indeed structured, not random. Across model families:(Is the bias generic?) We observe similar stripe-structured footprints on Llama and Mistral (Fig. 13 in Appendix), suggesting the routing bias is generic to RLVR. Figure 3 Temporal emergence of the optimization bias with row and column-wise update ratios for the 13th attention block across gradient update steps (t {240, 720, 1200}), smoothed with 3-step window. The row-dominant (Q) and column-dominant (O) patterns are consistent with the bias structures in Fig. 2. We visualize the head boundaries with grey dashed lines. The bias appears not only across heads but also within heads."
        },
        {
            "title": "2.3 Sparsity Is a Superficial Artifact of the Optimization Bias",
            "content": "The stable footprint of where updates land, persisting both throughout training and in the final model, suggests the focus should move from sparsity itself to the underlying optimization bias. We find that sparsity is actually the readout of this optimization bias, whose visibility is amplified by the precision limits of bf16 storage. Because bf16 has limited mantissa, changes smaller than the unit-in-the-lastplace (ULP) threshold (Lemma E.2) are not representable. Therefore, if RLVR consistently routes sub-ULP updates toward particular subset of parameters, the stored values will not change, and the result appears as sparsity. 5 We test this hypothesis by increasing the learning rate to scale otherwise sub-ULP updates above the representable threshold. As predicted, the apparent update sparsity largely disappears. This directly challenges the interpretation of (Mukherjee et al., 2025) that sparsity stems from zero gradients. Consistent with this view, concurrent work observes that sparsity mostly vanishes under float32 storage (Shenfeld et al., 2025) by increasing the precision, even though task performance does not improve. Hence, our results point to sparsity as byproduct of an optimization bias interacting with finite precision. It may be tempting to blame precision limit for sparsity. In fact, verl keeps Clarification on precision. optimizer states and gradient reductions/accumulation in float321. Thus, sparsity cannot be explained by precision alone. It requires consistent optimization bias during RL that concentrates visible changes in specific parameter regions throughout training. Aha Finding! RLVR exhibits persistent, model-conditioned optimization bias in where updates landhighly consistent across runs and largely invariant to datasets and RL recipes. The observed sparsity is superficial readout of this bias, amplified by bf16 precision."
        },
        {
            "title": "3 A Mechanistic Theory of RL’s Unique Optimization Dynamics",
            "content": "In the post-training era, RL has become key stage, albeit with intensive compute (xAI, 2025). Paradoxically (Sec. 2), these gains arise not from broad parameter changes but from selective, patterned edits that reveal persistent optimization bias. Understanding this distinctive training behavior raises the central question: Where does this optimization bias originate, and how does it shape parameter evolution? We characterize these optimization dynamics with the Three-Gate Theory, KL Anchor, Model Geometry, and Precision, which mechanistically explains how on-policy RL updates are constrained via Gate (KL Anchor; Sec. 3.1), steered via Gate II (Model Geometry; Sec. 3.2), and filtered via Gate III (Precision; Sec. 3.3) into the observed update pattern. Notations. We consider large language model with parameters θ, defining conditional distribution πθ(y x) over possible output token sequences = (y1, . . . , yT ) given prompt from the space . Each sequence is composed of tokens from vocabulary of size ."
        },
        {
            "title": "3.1 Gate I: On-Policy RL Imposes a One-Step KL Leash\nWe first show that online policy gradient updates yield a per-step policy KL bound (an anchoring effect),\nwhich in turn limits parameter movement during the RLVR update.",
            "content": "RLVR objective. Various RLVR algorithms including PPO, GRPO, DAPO, and REINFORCE++, learn policy πθ by optimizing variants of KL-regularized objective: max θ Eyπθ (x),xX [R(x, y) βKL(πθ( x) πref ( x))]. (3) where πref is fixed reference policy and β 0 controls the KL regularization (β = 0 recovers the clip-only variants such as DAPO). Rewards R(x, y) are verifiable and (after normalization) bounded (e.g., pass/fail or execution scores). Moreover, the surrogate typically uses the token-wise importance ratio wt = πθ(ytx,y<t) πold(ytx,y<t) with clipping relative to πold. One-step surrogate. With equation 3, standard sequence-level online policy-gradient surrogate is LPG(θ) = ExX , yπθ (x)[A (x, y) log πθ(y x)], (4) where is (normalized) advantage estimate, optionally shaped by reference-KL log-ratio term. In practice, updates are performed over mini-batches, with collected batch of data, not in fully on-policy manner. But the resulting error after small step size θ is O(θ2) (Lemma F.1). 1verl mixed-precision settings with {reduce_type, buffer_dtype}=float32. 6 Implicit KL leash. The KL leash emerges as policy gradient methods can be understood as conservative projection, keeping new policy close to its starting point while reweighting it toward higher-reward outcomes, not pulling it toward potentially distant external distribution like SFT: Proposition 3.1 (One-step policy-KL leash). Let q( x) be full-support reference and let qβ( x) q( x) exp(R/β) denote the soft-regularized improvement oracle. Let θ+ be the parametric fit obtained by the -projection of qβ onto the policy class, θ+ arg minθ DKL(qβπθ). Then, for sufficiently small one-step update, DKL(πθ+ πθ) (1 + o(1)) DKL(qβ πθ), (5) where the o(1) term vanishes as DKL(qβπθ) 0. Notably, even when the explicit KL term is removed (e.g., in DAPO with β = 0), the ratio clipping trick still imposes KL bound O(ε2) in the small-step regime (Appendix. F.2.4), confirmed empirically with bounded KL divergence change during DAPO run  (Fig. 15)  . Weight update constraint. Now we show the KL leash puts constraint on the weight update Proposition 3.2 (Policy-KL leash weight bound). Assume log πθ is 3 and let (θ) denote the Fisher information. If one-step update θ+ = θ + satisfies DKL(πθ+ πθ) and, on the update subspace, (θ) µI for some µ > 0, then for sufficiently small (θ) (θ) 2K (1 + o(1)), 2 2K µ (1 + o(1)). (6) Consequently, for any weight matrix block θ, F See detailed proof for Proposition 3.1 in Appendix F.2.1 and Proposition 3.2 in Appendix F.2.2. 2K/µ (1 + o(1)). Take-away 1: RL update imposes an implicit KL leash (anchor effect), ensuring that the per-step drift from the current policy is small. This aligns with recent work arguing that even the final policy is KL-proximal (Wu et al., 2025; Shenfeld et al., 2025). Our focus, however, is to understand how this leash affects the weight change dynamics."
        },
        {
            "title": "3.2 Gate II: Model Geometry Determines Where a KL-Bounded Step Goes\nFrom Gate I to location. Gate I supplies a one-step KL leash that bounds the move, but it does not specify\nwhere the update lands. We propose Gate II(Model Geometry), where we argue that, unlike a randomly\ninitialized network, a well-pretrained model possesses a highly structured geometry, e.g., spectral statistics\nand high-curvature directions during optimization, that determines where a KL-constrained update goes.",
            "content": "Layerwise norm bound from the KL leash. Let W0 be pretrained linear block, W+ = W0+W the post-step block, and let SW µW be per-layer curvature proxy. If the per-layer KL budget satisfies 1 vec W, SW vec 2 δW , then (Appendix F.10) F 2δW µW , 2 2δW µW . (7) We then show that this conservative update yields three consequences, preserving the pretrained weight spectrum rather than destroying it based on weight perturbation theory (Stewart, 1998). Limited subspace rotation. First, as shown in Theorem 3.3, the angle between the original and updated subspaces is quadratically bounded, meaning the fundamental directions are preserved. Theorem 3.3 (Constrained subspace rotation with Wedins sinΘ theorem (Wedin, 1972)). Let γk = σk(W0) σk+1(W0) be the singular value gap. For any with γk > 0, max( sin Θ(Uk(W0), Uk(W+))2 , sin Θ(Vk(W0), Vk(W+))2 2 γk 2δW /µW γk . (8) Singular value stability. Second, the magnitudes of the principal components themselves are preserved. The change in each singular value is bounded by the norm of the update. 7 Figure 4 Spectral geometry under SFT vs. RLVR on Qwen3-8B (Su et al., 2025). Left: for an exemplar layer, top-k principal angles and singular-value curves. Right: across all layers, maximum principal angle and normalized spectral drift. RLVR maintains stable top-k spectrum with minimal subspace rotation, unlike SFT. See DS-Qwen-1.5B in Fig. 16 and Qwen3-14B-Base in Fig. 17. Corollary 3.4 (Singular-value stability). For each k, σk(W+) σk(W0) 2 2δW µW , 2 (σi(W+) σi(W0)) 2 2δW µW . (9) Top-k energy preservation. Finally, these effects combine to ensure the cumulative energy of the top-k components of the weights remains stable. Corollary 3.5 (Top-k energy and Ky Fan norms). Let (k) = i=1 σi() be the Ky Fan k-norm. Then W+(k) W0(k) i= σi(W+) σi(W0) 2 2δW µW . (10) See the detailed proofs in Appendix F.3 for all results presented here. Take-away 2: Under the KL leash, RL updates tend to preserve the models original weight structure rather than destroy it. This naturally favors updates in low-curvature directions of the optimization landscape, which avoids dramatic changes in model behavior. Since directly quantifying curvature in LRM with long CoTs is computationally prohibitive, we instead adopt powerful and efficient proxy, principal weights (Liu et al., 2025c), as detailed in Sec. 4.2."
        },
        {
            "title": "3.3 Gate III: Precision Acts as a Lens Revealing the Compass\nBuilding on the optimization bias, the bfloat16 with limited precision acts as a lens: it hides those micro-updates\nthat occur where the RL consistently holds a weak willingness to apply large changes.",
            "content": "2 ULPbf16(Wij). Corollary 3.6 (Magnitude-dependent realization threshold). stored weight Wij changes at step iff Wij 1 The effect of this gate has been discussed aforementioned. We would emphasize again that precision is more an amplifier for visible sparsity, not the cause of optimization bias, as optimizer states, etc., are still in float32 (See Sec. 2.3)."
        },
        {
            "title": "4 Theory-Guided Validation of RLVR’s Optimization Dynamics",
            "content": "We conduct theory-guided experiments analyzing how RLVR modifies parameters and interacts with pretrained geometry. These results validate our central prediction: the pretrained model geometry steers KL-constrained 8 Figure 5 RL avoids updating principal weights. We compare the RL update mask with principal weight mask Mprinc, . The layer-wise overlap between RL updates and principal low magnitude mask Mlow, and the one Mprinc weights is consistently sub-random, an effect more pronounced when removing its overlapped weights with Mlow, i.e., Mprinc updates, yielding distinct, off-principal optimization dynamics that set RLVR apart from SFT. low low ."
        },
        {
            "title": "4.1 RLVR Preserves Spectral Geometry, While SFT Distorts It",
            "content": "We begin by probing spectral changes to test whether RL updates are steered toward low-curvature, spectrumpreserving directions. If so, RLVR should largely preserve the pretrained spectral structure, whereas SFT, lacking this steering, should significantly distort it. Setups. We analyze checkpoints from standard SFTRLVR pipeline on Qwen3-8B-Base (Su et al., 2025) and long-horizon RL run on DS-Qwen-1.5B (Liu et al., 2025a). We also consider setting where SFT and RL are applied separately to Qwen3-14B-Base, matched on in-domain math performance (Huan et al., 2025). In all cases, we compare base weights W0 and fine-tuned weights W+ Metrics. We compare the base weights W0 with the finetuned weights W+ Subspace rotation. For the top-k left (U )/right(V ) singular subspaces, we check the rotation using principal : . angles via cos θi(U ) = σi(U 0,kU+,k) and cos θi(V ) = σi(V 0,kV+,k). Spectrum drift. Beyond showing the singular value curve, we quantify singular-value change with normalized ℓ2 shift: NSS(W ) = σ(W+) σ(W0)2/σ(W0)2 Our findings. RLVR checkpoints exhibit Insightably stable spectrum within the top principal components: across layers, RLVR shows consistently small principal-subspace rotation and minimal spectral drift. The singular-value profiles are even nearly identical to the base model. By contrast, SFT induces substantially larger rotations and pronounced drifts on the same metrics  (Fig. 4)  ."
        },
        {
            "title": "4.2 RLVR Avoids Principal Weights, While SFT Targets Them",
            "content": "We now move from macro-level spectral analysis to micro-level examination of individual weights, probing which parameters RLVR favors or avoids to update, deeper investigation into the parameter-space dynamics. Principal weights as proxy for high-curvature directions. Directly identifying high-curvature directions is computationally prohibitive, especially given LRM with long CoTs. Instead, we adopt powerful proxy from recent work Liu et al. (2025c), principal weights, which is defined as the weights with the largest magnitude after low-rank approximation, representing its most influential computational pathways. The validity of this proxy is confirmed by their perturbation studies, which show that modifying these specific weights causes sharp reasoning performance degradation. This degradation is directly linked to high-curvature regions via 9 Taylor expansion of the loss. The principal mask, (k) weights with the highest score, s(k) Low-magnitude weights as low-resistance pathway. We further include the top-α lowest magnitude weights, as Mlow = Bottomα(W0). The magnitude is also bias from the model geometry (distribution prior), impacting how easily the weights can be updated based on our precision gate. ij ), is defined as the top-α fraction of is the rank-k SVD reconstruction of W0. princ = Topα(s(k) (i, j), where 0 ij = (k) 0 itself., i.e., α. with it, defined as Overlap(M, ) = MM Metrics. Let be the weight update update mask from an RLVR run. We report the overlap ratio between our identified mask ., with random guess baseline overlap ratio as the density of Our findings. Fig. 5 visualizes the RL update mask in relation to the principal mask Mprinc and the low-magnitude mask Mlow, reporting their layer-wise overlap against random baseline as well. The results show clear dichotomy. RL updates exhibit sub-random overlap with principal weights, indicating strong tendency to avoid them. Conversely, the updates show super-random overlap with low-magnitude weights due to their low resistance to micro-updates. Besides, we found that the residual overlap between updates and principal weights is highly accounted for by weights that are both principal (defined by the rank-k approximation of W0) and low-magnitude (original W0). After excluding this intersection, i.e., Mprinc , the overlap drops significantly. low Insight. This points to central implication: RLVR and SFT operate in distinct optimization regions of parameter space, even at comparable task performance. RLVR avoids high-curvature, principal regions, whereas SFT targets them. This regional mismatch helps explain the limited transferability of SFT-oriented PEFT under RL (Sec. 5)."
        },
        {
            "title": "4.3 RLVR Relies on Model Geometry, Disrupting Geometry Destroys the Bias",
            "content": "Gate II posits that the pretrained models geometry steers RL updates. To test this causal link, we deliberately \"scramble\" the geometry of specific layers in Qwen3-4B-Base model using orthogonal rotations for O/V layers (Rotate) and head permutations for all Q/K/V/O layers (Permute) (details in Appendix D) and compare the update overlap ratio Overlap(M, ) = MM . between the base run with another independent run without intervention and one run with intervention. Our Findings. We modify (i) layer 20 with Rotate+Permute, and (ii) layer 25 with Rotate. As shown in Fig. 6, the update overlap collapsed to random level in the intervened layers, while remaining high in all untouched layers. This provides strong causal evidence that the pretrained models geometry is the source of the optimization bias. Figure 6 Overlap ratio after intervention."
        },
        {
            "title": "4.4 RLVR signatures persist in agentic tasks and RLHF\nSetup. We analyze additional agent and RLHF (RL with human feedback) checkpoints and apply the same\nweight–space diagnostics as in Sec. 4.1 and Sec. 4.2: (i) principal-subspace rotation, (ii) spectral drift, and\n(iii) update–principal misalignment. The extended model suite is summarized in Tab. 3. Agents. We evaluate\npolicies from AgentFlow (Li et al., 2025) and VERL-Agent (Feng et al., 2025) on multi-turn and long-\nhorizon tasks. We also assess tool-augmented agents from SkyRL (Cao et al., 2025) and VERL-Tool (Jiang\net al., 2025) on WebSearch, DeepSearch, and SWE. RLHF. We include preference-optimized models trained\nwith DPO (Rafailov et al., 2023a) and SimPO (Meng et al., 2024c), primarily targeting instruction following.",
            "content": "10 Figure 7 Updateprincipal misalignment in RL-trained agents. For representative layers from agentflow-planner-7b (Layer 16, o_proj; top row) and SkyRL-Agent-WebResearch-8B (Layer 11, k_proj; bottom row), the left panels visualize the bf16-aware update mask Mℓ (locations that changed under RL), while the right panels show the principal mask (k) (top-k singular-subspace support typically favored by SFT). Dashed red boxes highlight stripe regions where RL updates concentrate outside principal-weight bands, indicating robust off-principal routing in agent and tool-use settings. ℓ Table 3 Model List for analyzed checkpoints for agentic tasks and RLHF algorithms. Category Base Model Qwen3-8B Qwen3-8B Qwen3-8B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Agent FT Model SkyRL-Agent-WebResearch-8B VT-deepsearch-8B VT-SWE-8B agentflow-planner-7b GiGPO-Qwen2.5-7B-Instruct-WebShop GiGPO GiGPO-Qwen2.5-7B-Instruct-ALFWorld GiGPO Algorithm GRPO GRPO GRPO Flow-GRPO Data WebResearch Deepsearch SWE Planning WebShop ALFWorld RLHF Meta-Llama-3-8B-Instruct Llama-3-Instruct-8B-DPO Meta-Llama-3-8B-Instruct Llama-3-Instruct-8B-SimPO DPO SimPO instruction-following instruction-following (i) Stable spectra, minimal rotation. Across agents and RLHF, top-k subspaces rotate only slightly, Our Findings. and layer spectra remain near-identical to the base model (Fig. 8; Fig. 18), matching the spectrum-preserving, off-principal regime observed earlier. (ii) Off-principal updates. Update masks in agent and RLHF checkpoints consistently avoid principal weights: the most active bands are spatially misaligned with the principal mask  (Fig. 7)  . Takeaway. RLVRs optimization dynamicsminimal rotation, spectrum preservation, off-principal routingpersist beyond verifiable math/code to agents and RLHF, indicating common, model-conditioned optimization bias within KL-anchored RL post-training game, consistent with our Three-Gate Theory. Takeaway 3. RLVR learns off the principals: it preserves spectral geometry, avoids principal weights, and its optimization bias vanishes when pretrained geometry is disrupted."
        },
        {
            "title": "5 Theory-Guided Rethinking of Learning Algorithms for RL",
            "content": "A good theory should not only explain observations but also inform design. Our account shows that RLVR and SFT follow disjoint optimization dynamics in parameter space, which implies that many SFT-era PEFT methods, especially those aligned with principal directions through sparse or low-rank priors, transfer poorly to RLVR. This section validates our predictions and demonstrates how they guide the redesign of learning algorithms for RL. 11 In agent settings, including multi-turn interactions and tool use, RL Figure 8 Spectrum under RL in agent tasks. leaves layer singular-value spectra nearly unchanged and induces only small rotations of the top-k singular subspaces, consistent with the spectrum-preserving, off-principal RLVR regime. Results for RL with human feedback (RLHF), which exhibit the same optimization signature, appear in Fig. 18. For consistency, we use the second block O-projection layer as an exemplar single-layer readout."
        },
        {
            "title": "5.1 Probing Sparse Fine-Tuning in RL\nWe use sparse RL fine-tuning to probe RL’s optimization dynamics by asking which weights can be frozen\nwithout materially altering the training trajectory. We construct a parameter mask directly from the pretrained\nmodel without any additional training and apply it to perform sparse RL fine-tuning. Following (Shenfeld et al.,\n2025), we track the token-wise forward KL divergence KL(π ∥ πref) between the fine-tuned policy and the base\nmodel throughout training. This metric quantifies how closely a sparse run follows the dense baseline trajectory,\nif pruning certain weights impedes learning, the KL drift will slow, indicating blocked optimization progress.",
            "content": "princ Mask design. We evaluate several masks constructed directly from the pretrained model: (i) Mprinc (principal-only, top-50% principal weights), (ii) (non-principal-only, the complementary subspace), (iii) Mlow (low-magnitude-only), (iv) (safe mask, favoring non-principal Mlow and low-magnitude weights), , and (v) random mask with the same layer-wise sparsity as (iv). We choose 50% for (i) as we want to isolate the effect of the number of parameters for fair comparison to see the difference between (i) and (ii). princ princ Our findings. (See KL in Fig. 9; accuracy in Tab. 4 and the extended 500-step results in Tab. 5.) The safe mask Mlow most closely tracks the dense RLVR KL curve and reaches comparable final accuracy, indicating that our theory correctly identifies highly touchable weights. By contrast, the principal-only mask yields the worst optimization trajectoryits KL curve rises slowlyshowing excessive intervention and degraded training dynamics. This directly confirms that principal-targeted directions favored in SFT are ineffective for RL. Figure 9 KL loss curves on DS-Qwen-1.5B under different masks. Insight. Our results suggest simple yet effective alternative: Freezing principal and large-magnitude weights while updating non-principal, low-magnitude ones closely reproduces dense RLVR behavior (KL trajectory and final accuracy) using roughly 70% the parameters This shows that our theory provides practical guidance for identifying the effective subspace of RL updates, entirely without additional training. While the masks used here are one-shot and fixed, combining this framework with dynamic mask refresh or adaptive scheduling (Zhao et al., 2024; Zhu et al., 2024; Liu et al., 2025c) is promising next step."
        },
        {
            "title": "5.2 Revisiting LoRA Through the Lens of Our Theory",
            "content": "A recent report (Schulman & Lab, 2025) finds that low-rank LoRA, even rank-1, can match full-parameter RL performance. Our theory offers an explanation: in full-parameter RL, effective updates lie off the principal directions and induce only small spectral changes. Low-rank adapters can approximate these off-principal updates, while freezing the base weights regularizes training and discourages moves toward principal directions. With an appropriately scaled learning rate, the limited adapter capacity is therefore sufficient to catch up to full-parameter performance at least in the short run. However, the same report suggests principal-targeted variants such as PiSSA (Meng et al., 2024a) should yield further gains. Our geometry account disagrees: aligning updates to top-r principal directions enforces SFT-style behavior that is misaligned with RLVRs off-principal bias. Empirical test. On DS-Qwen-1.5B with DeepMath-103K (He et al., 2025), we sweep ranks {8, 32, 64} and learning rates {1104, 5105, 1105} for 200 steps, and report pass@1 (mean over 16 samples) on AIME24 and AMC23  (Fig. 10)  . To control for model effects, we repeat on Llama-3.2-3B-Instruct with Math corpus and report pass@1 (mean over 4) on MATH500  (Fig. 11)  . 13 Figure 10 LoRA vs. PiSSA on DS-Qwen-1.5B (DeepMath-103K). We sweep ranks {8, 32, 64} and learning rates {1104, 5 105, 1105 } for 200 steps, reporting pass@1 (avg@16) on AIME24 (top) and AMC23 (bottom). Across settings, PiSSA (principal-targeted) provides no additional gains over LoRA and, at higher learning rates that force principal-direction updates, often collapses early; LoRA remains more stable. This supports our geometric account: forcing updates into principal directions (favored in SFT) is misaligned with RL, offering no obvious gain and leading to training collapse when scaling up learning rates. Our findings. Across settings, the principal-targeted PiSSA provides no clear gain over LoRA. At the higher learning rates used for low-rank adapters to match full-parameter performance, PiSSA often becomes unstable and collapses earlier than LoRA. This occurs because scaling the learning rate in PiSSA enforces updates along principal directions, higher-curvature and spectrum-distorting, precisely the directions RLVR tends to avoid. The result is brittle optimization and early collapse, whereas LoRAs off-principal updates remain better aligned with RLVRs geometry. Insight. These results support the geometry-based account: principal-aligned LoRA variants are over-fit to SFTs update geometry and misaligned with RLs training dynamics, so success in SFT does not transfer to RL. Takeaway 4. RLVR operates in distinct, geometry-driven optimization regime, so your old PEFT tricks may not work. Methods over-aligned with SFTs principal-targeted dynamics (e.g., sparse or low-rank adapters) fail to transfer, calling for new generation of RL-native, geometry-aware parameter-efficient algorithms."
        },
        {
            "title": "6 Conclusion",
            "content": "We revisited the paradox of visible update sparsity in RLVR and showed that it is superficial readout of deeper, model-conditioned, geometry-aligned optimization bias that determines where updates land. We formalized this mechanism with the Three-Gate Theory: KL anchor constrains each on-policy step; pretrained geometry steers updates off principal directions into low-curvature, spectrum-preserving subspaces; and finite precision renders the bias visible as sparsity by masking micro-updates. Empirically, RLVR preserves spectral structure and avoids principal weights, whereas SFT targets principal directions and distorts the spectrum; when the pretrained geometry is disrupted, these signatures vanish, establishing geometry as the steering core. Beyond explanation, our case studies bridge mechanism and practice: SFT-era principal-aligned PEFT (e.g., sparse/low-rank variants) often misaligns with RLVRs off-principal regime. Taken together, these results provide the first parameter-level account of RLVRs training dynamics, replacing black-box view with 14 Figure 11 LoRA vs. PiSSA on LLaMA-3.2-3B. We sweep learning rates {1104, 5105, 1105 } with fixed rank of 64 for 200 steps, reporting pass@1 (mean@4) on MATH500. Consistent with the DS-Qwen-1.5B results in Fig. 10, PiSSA provides no additional gain over LoRA and, under higher learning rates that emphasize principal-direction updates, often collapses early. white-box understanding of how parameters evolve under RLVR, and laying the foundation for geometry-aware, RLVR-native parameter-efficient learning algorithms."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank Zhengqi Gao (Massachusetts Institute of Technology) for insightful discussion on the verl framework and idea discussion."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao Wang, Akshay Malik, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-v0: Train real-world long-horizon agents via reinforcement learning. https://novasky-ai.notion.site/skyrl-v0, 2025. NovaSky AI, Notion page. Accessed 2025-11-10. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https: //arxiv.org/abs/2501.12948. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305, 2020. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Daya Guo, Dejian Yang, Haowei Zhang, and Junxiao Song. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.07570, 2025. URL https://arxiv.org/abs/2501.07570. Seungwook Han, Jyothish Pari, Samuel Gershman, and Pulkit Agrawal. General reasoning requires learning to reason from the get-go. arXiv preprint arXiv:2502.19402, 2025. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018. Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Hao-Shu Fang, et al. Toward general-purpose robots via foundation models: survey and meta-analysis. arXiv preprint arXiv:2312.08782, 2023. Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning. arXiv preprint arXiv:2507.00432, 2025. Aaron Jaech et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, and Pan Lu. In-the-flow agentic system optimization for effective planning and tool use. arXiv preprint arXiv:2510.05592, 2025. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Zihang Liu, Tianyu Pang, Oleg Balabanov, Chaoqun Yang, Tianjin Huang, Lu Yin, Yaoqing Yang, and Shiwei Liu. Lift the veil for the truth: Principal weights emerge after rank reduction for reasoning-focused supervised fine-tuning. arXiv preprint arXiv:2506.00772, 2025c. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level1cf81902c14680b3bee5eb349a512a51, 2025a. Notion Blog. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025b. Notion Blog. MAA. American mathematics contest 12 (amc 12), November 2023. URL https://artofproblemsolving.com/wiki/ index.php/AMC_12_Problems_and_Solutions. MAA. American invitational mathematics examination (aime), February 2024. URL https://artofproblemsolving. com/wiki/index.php/AIME_Problems_and_Solutions. MAA. American invitational mathematics examination (aime), February 2025. URL https://artofproblemsolving. com/wiki/index.php/AIME_Problems_and_Solutions. Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. Advances in Neural Information Processing Systems, 37:121038121072, 2024a. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024b. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024c. Sagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tur, and Hao Peng. Reinforcement learning finetunes small subnetworks in large language models. Advances in Neural Information Processing Systems, 2025. Long Ouyang et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pp. 2773027744, 2022. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. arXiv preprint arXiv:2303.08774, 2018. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023a. 17 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023b. Negin Raoof, Etash Kumar Guha, Ryan Marten, Jean Mercat, Eric Frankel, Sedrick Keh, Hritik Bansal, Georgios Smyrnis, Marianna Nezhurina, Trung Vu, Zayne Rea Sprague, Mike Merrill, Liangyu Chen, Caroline Choi, Zaid Khan, Sachin Grover, Benjamin Feuer, Ashima Suvarna, Shiye Su, Wanjia Zhao, Kartik Sharma, Charlie Cheng-Jie Ji, Kushal Arora, Jeffrey Li, Aaron Gokaslan, Sarah Pratt, Niklas Muennighoff, Jon Saad-Falcon, John Yang, Asad Aali, Shreyas Pimpalgaonkar, Alon Albalak, Achal Dave, Hadi Pouransari, Greg Durrett, Sewoong Oh, Tatsunori Hashimoto, Vaishaal Shankar, Yejin Choi, Mohit Bansal, Chinmay Hegde, Reinhard Heckel, Jenia Jitsev, Maheswaran Sathiamoorthy, Alex Dimakis, and Ludwig Schmidt. Automatic evals for llms, 2025. URL https://github.com/mlfoundations/evalchemy. Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. John Schulman and Thinking Machines Lab. Lora without regret. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20250929. https://thinkingmachines.ai/blog/lora/. Zhihong Shao et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less. arXiv preprint arXiv:2509.04259, 2025. Guangming Sheng et al. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Gilbert Stewart. Perturbation theory for the singular value decomposition. 1998. Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, and Guorui Zhou. Klear-reasoner: Advancing reasoning capability via gradient-preserving clipping policy optimization. arXiv preprint arXiv:2508.07629, 2025. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Per-Åke Wedin. Perturbation bounds in connection with singular value decomposition. BIT Numerical Mathematics, 12(1):99111, 1972. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. Fang Wu, Weihao Xuan, Ximing Lu, Zaid Harchaoui, and Yejin Choi. The invisible leash: Why rlvr may not escape its origin. arXiv preprint arXiv:2507.14843, 2025. xAI. Grok: Ai assistant, 2025. URL https://x.ai/grok. Accessed: 2025-09-24, continuously updated. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025a. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025b. URL https://arxiv.org/abs/2503.18892. 18 Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems, 37:110935110971, 2024. Xiaojiang Zhang et al. Srpo: cross-domain implementation of large-scale reinforcement learning on llm. arXiv preprint arXiv:2504.14286, 2025. Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024. Hanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu, Sem Park, Vikas Chandra, Bo Long, David Pan, Zhangyang Wang, and Jinwon Lee. Apollo: Sgd-like memory, adamw-level performance. arXiv preprint arXiv:2412.05270, 2024. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A Clarification of LLM Usage",
            "content": "In this work, we employ LLMs to polish the writing throughout the paper and to assist in generating code for figure plotting. Besides, we use it for drawing the teaser figure."
        },
        {
            "title": "B More Related works",
            "content": "Post-training Large-scale models pre-trained on broad domains serve as general-purpose backbones with extensive domain knowledge and notable zero-shot capabilities (Radford et al., 2021; Achiam et al., 2023; Touvron et al., 2023; Hu et al., 2023; Li et al., 2024; Radford et al., 2018; Brown et al., 2020). However, such pre-trained models often fail to meet the specific application requirements or align with domain-specific constraints. Post-training methods address this gap by adapting foundation models to downstream tasks. Common approaches include supervised fine-tuning on curated datasets (Howard & Ruder, 2018; Dodge et al., 2020; Wei et al., 2021; Chung et al., 2024), reinforcement learning from human or automated feedback (Ziegler et al., 2019; Ouyang et al., 2022; Guo et al., 2025; Zhai et al., 2024), and other recent techniques (Rafailov et al., 2023b). Especially, the recent advances in LLM reasoning (DeepSeek-AI, 2025) highlight the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR), which replaces subjective human judgments with automatically verifiable signals. RLVR has been shown to significantly enhance reasoning ability using policy optimization algorithms such as PPO (Ouyang et al., 2022) and GRPO (Shao et al., 2024). Building on these successes, growing body of work (Yu et al., 2025; Liu et al., 2025b; Luo et al., 2025a; Zhang et al., 2025; Liu et al., 2025a; Xiong et al., 2025) continues to refine RL methods tailored for LLM reasoning. SFT versus RL. Prior work comparing these paradigms has largely focused on downstream performance. foundational result shows that on-policy RL can outperform offline SFT even with the same expert data (Ross et al., 2011). Recent empirical studies consistently reinforce this, finding that RL-tuned models often generalize better out-of-distribution (Han et al., 2025; Chu et al., 2025) and transfer more effectively to new tasks (Huan et al., 2025) than their SFT counterparts. While these studies establish performance hierarchy, our work investigates different dimension: how these distinct methods affect the models internal structure. recent study observed that RL fine-tunes only fraction of the networks parameters (Mukherjee et al., 2025), but this empirical finding left the underlying mechanism unexplored and did not characterize or predict the affected subnetwork. Our work aims to bridge this gap by providing mechanistic explanation for this phenomenon."
        },
        {
            "title": "C Experimental Details",
            "content": "C.1 Training Settings Models & Datasets. We run post-training experiments on three open models: DeepSeek-R1-Distill-Qwen-1.5B (Yang et al., 2024), Qwen2.5-Math-7B (Yang et al., 2024), and Qwen3-Base (Team, 2025). The maximum context length is set to 8192 for DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-Math-7B, and to 20480for Qwen3-4B-Base. We evaluate primarily on mathematics using two training corpora to reduce dataset-specific confounds. (1) DAPO+MATH (DM): union of the DAPO-Math-17k set2 and the MATH dataset (Hendrycks et al., 2021). (2) DS+SR: the 47k DeepScaler collection (Luo et al., 2025b) combined with high-difficulty (levels 35) problems extracted from SimpleRL (Zeng et al., 2025a).We use the version from Huan et al. (2025). Training details. We implement RLVR on the VeRL pipeline (Sheng et al., 2024) (v4.0) and use vLLM (Kwon et al., 2023)(v8.5) for rollouts. We use FSDPv2 with the default mixed precision configuration. All experiments run on NVIDIA H200 GPUs. Unless otherwise noted, we use DAPO (Yu et al., 2025) without an explicit reference-KL penalty (ratio clipping as in DAPO), global batch size of 256 (mini-batch 64) with 4 gradient update per step. We use DAPO primarily to eliminate the confounding effect of the KL penalty, which can otherwise obscure the intrinsic parameter update dynamics during training. Per-model configurations without specific mention: Qwen2.5-Math-7B on DM: 16 rollouts per prompt; 8 H200 GPUs; 300 training steps. DeepSeek-R1-Distill-Qwen-1.5B on DS+SR: 12 rollouts per prompt; 16 H200 GPUs; 320 steps. Qwen3-4B-Base on DS+SR: 16 rollouts per prompt; 32 H200 GPUs; 150 steps. For LoRA and PiSSA studies, to reduce compute cost during learning-rate sweeps, we use the same DAPO recipe with global batch size of 128 (mini-batch 32), four gradient updates per step, and 16 rollouts per prompt. Both DeepSeek-R1-Distill-Qwen-1.5B and LLaMA-3.2-3B are trained for 200 steps. The actor is optimized with AdamW (Loshchilov & Hutter, 2017) (constant learning rate 1106, β1=0.9, β2=0.999). Rewards are verifiable: +1.0 if the extracted final answer is correct and 1.0 otherwise (no separate format score), following the verifier implementation of Su et al. (2025). We enable an over-length penalty with an additional 1024-token budget and penalty factor of 1.0. C.2 Evaluation settings We evaluate models on four widely used benchmarks: AIME24 (MAA, 2024), AIME25 (MAA, 2025), AMC23 (MAA, 2023), MATH-500 (Lightman et al., 2023), as we main train using math daastets. We used Eval-Chemy (Raoof et al., 2025) with their default temperature 0.7 and 0.8 as the top-p value. In our experiments, we used the averaged accuracy, i.e., pass@1(avg@k) for all benchmarks. to evaluate the models performance. Specifically, for AIME24 and AIME 25, we averaged accuracy on 64 samples, for AMC, we average accuracy on 32 samples, For MATH 500, our score is the average accuracy over 2 samples."
        },
        {
            "title": "D Intervention details",
            "content": "Intervention 1: losspreserving V/O rotation. Let be the head dimension, Hq the number of query heads, Hkv the number of key/value heads, and nrep = Hq/Hkv (grouped GQA). Denote 2DAPO-Math-17k Wv Rdmodel(HkvD), Wo Rdmodel(HqD). 20 Draw any orthogonal RDD (Haar/Hadamard) and form the block rotations Rkv = diag(R, . . . , Hkv ) R(HkvD)(HkvD), Rq = diag(R, . . . , nrep , R, . . . , nrep , . . .) R(HqD)(HqD). We edit the weights by rightmultiplication along the head axis: = WvRkv, = WoRq. (11) If bv exists, reshape bv per head and set = bvRkv. Proposition D.1 (Exact invariance). Let Ctx = Attn(Q, K, ) R(HqD). Under equation 11, out = Attn(Q, K, Rkv) (WoRq) = Ctx RqR = Ctx = out. Intervention 2: head shuffle (lossless). Let Pkv be permutation of the Hkv KV heads and Pq its grouped expansion to Hq heads. Apply cols of (Wk, Wv) Pkv, cols of Wq Pq, columns of Wo 1 . This relabels which head carries which subspace, while leaving the block function unchanged. We show that after weight intervention, the model weights update position has sub-random overlap while those untouched weights stay high overlap."
        },
        {
            "title": "E Examples of why previous identified method fails",
            "content": "E.1 Failures of Fixed Absolute Tolerance Rule False positives at large scale. Within [210, 211)=[1024, 2048), the bf16 spacing is ULPbf16 = 2107 = 8. Numbers like 1024.001 and 1024.002 differ by 103>105, hence would be flagged as changed by the 105 rule, yet both round to the same bf16 code (1024), i.e., no storage-level change. False negatives at small scale. Around 106 220, the bf16 spacing is ULPbf16 = 227 7.45109. Weights w=106 and w=2106 differ by 106105 and would be marked equal by the 105 rule, yet they are separated by 134 ULPs and quantize to different bf16 codes. E.2 Justification of our probe Lemma E.1 (Gap between distinct bf16 representables). If are normalized bf16 numbers in the same binade [2e, 2e+1), then y 2 e7 and y max(x, y) > 28. The strict inequality also holds across the binade boundary. Lemma E.2 (ULP lens: magnitude-dependent threshold). For normalized bf16 values with [2e, 2e+1), ULPbf16(x) (28, 27] = (0.390625%, 0.78125%]. Hence the minimal realized relative update at magnitude is 1 particular, larger requires larger absolute step to register. Proposition E.3 (Soundness and completeness of the probe). Let wi, wi be normalized bf16 values (finite, nonzero), and suppose η < 2 ULPbf16(x)/x (0.195%, 0.391%]. In 2 minx ULPbf16(x)/x = 29 1.953 103. Then wi wi η max(wi, wi) bf16(wi) = bf16( wi). 21 Proof. ()If wi wi, Lemma E.2 gives wi wi/ max(wi, wi) > 28 > 2η, contradiction. Hence wi = wi as bf16 numbers. () If the stored bf16 values are equal, the difference is 0, which satisfies equation 1. Corollary E.4 (Choice η = 103 is safe). Since 103 < 29, Proposition E.3 applies: the test equation 1 passes iff the two bf16 entries are bit-wise identical (or both zero). Thus η = 103 yields scale-aware probe that flags equality only when storage is unchanged."
        },
        {
            "title": "F Math Analysis",
            "content": "F.1 Policy-Gradient Fine-Tuning (DAPO) Assume an old policy πold that we use to sample candidate completions y1G for each prompt . For single token yi,t (token in completion i) we define the importance-weighted advantage wi,t = πθ(yi,tx, y<t) πold(yi,tx, y<t) importance ratio ˆAi,t Iclip R, where ˆAi,t is the estimated advantage and Iclip {0, 1} implements the usual trust-region clipping. Token-level objective. The DAPO loss can be written as sum of weighted log-probabilities JRL(θ) = ExX , y1Gπold [ 1 yi i= yi t=1 wi,t log πθ(yi,t x, yi <t)]. (1) (2) F.2 Proof of Gate I: On-Policy RL Implies One-Step KL Leash This appendix provides the standard tilting oracle and -projection facts, local second-order expansions, and the proof of the one-step policy-KL leash (Prop. 3.1 in the main text). We keep the proof concise, otherwise too lengthy, especially for those has shown in some prior work Shenfeld et al. (2025); Wu et al. (2025). Our one-step analysis is inspired by recent work Wu et al. (2025); Shenfeld et al. (2025), which uses similar variational approach to show that even the final converged policy remains KL-proximal to the base policy. We also record trust-region/clipping bound used when β = 0. Throughout, is fixed, q( x) has full support on Y, and πθ( x) is 3 parametric family with log-density log πθ locally smooth. Expectations without explicit subscript are conditional on x. We first show useful lemmas here. Lemma F.1 (Frozen-policy surrogate is second-order tight). Let (θ) = LPG(θ) in equation 4 and g(θ) = LPG(θ; θt) be the frozen-policy surrogate with Aθt. Then (θt) = g(θt) and (θt) = g(θt). If and are L-Lipschitz in neighborhood of θt, then (θt + θ) g(θt + θ) 2 θ2. Proof. At θt, both objectives evaluate to Eπθt and the centering of Aθt , both yield Eπθt is the standard second-order Taylor remainder under Lipschitz gradients. [Aθt log πθt]. For the gradient, using the log-derivative trick [Aθt log πθt]. Thus (θt) = g(θt) and (θt) = g(θt). The bound 1: Exponential tilting and M-projection Lemma F.2 (Gibbs variational principle / exponential tilting). Fix β > 0 and full-support reference q( x). Then max πq {Eyπ[R(x, y)] β DKL(πq)} 22 is uniquely maximized by qβ(y x) = q(y x) exp(R(x, y)/β) Eyq[exp(R(x, y)/β)] . Proof. Consider L(π, λ) = Eπ[R] βEπ[ log π hence π eR/β. Strict concavity in π yields uniqueness. ] + λ(y π(y) 1). Stationarity in π gives log π = R/β λ 1, Lemma F.3 (Policy Gradient Update as Parametric -projection). For fixed qβ, arg min θ DKL(qβπθ) = arg max θ Eyqβ [log πθ(y x)]. Proof. DKL(qβπθ) = Eqβ [log qβ] Eqβ [log πθ], where the first term is θ-independent. We omit the full proof here, with one can be found in Shenfeld et al. (2025). 2: Local second-order identities Lemma F.4 (Local Pythagorean identity for the -projection). Let (θ) = DKL(qβπθ) = Eqβ [ log πθ] + const. Assume log πθ is 3 near θ, and let θ+ arg min . Writing = θ+ θ, for small, (θ) (θ+) = 1 2 Hq(θ) + O(3), Hq(θ) = Eqβ [2 log πθ]. Proof. Taylor-expand at θ+: (θ) = (θ+)+ 1 implies Hq(θ+) = Hq(θ) + O(), which is absorbed into the cubic remainder. 2 Hq(θ+)+O(3) since (θ+) = 0. Local 3 smoothness Lemma F.5 (Quadratic expansion of policy KL). Let (θ) = Eπθ [2 log πθ] be the Fisher information. Then DKL(πθ+πθ) = 2 (θ) + O(3). Proof. Expand log πθ+ πθ use Eπθ [ log πθ] = 0 and Eπθ [2 log πθ] = (θ). = log πθ + 1 2 2 log πθ + O(3), take expectation under πθ+ = πθ + O(), 3. Relating projection Hessian and Fisher under small tilt Lemma F.6 (HessianFisher proximity). Suppose 2 log πθ(y x)op uniformly near θ. Then Hq(θ) (θ) op 2L TV(qβ, πθ) 2 DKL(qβπθ). In particular, with κ = DKL(qβπθ) 0, we have Hq(θ) = (1 + O( κ)) (θ) as quadratic forms. Proof. For bounded matrix-valued h, Eqh Eπhop 2h TV(q, π). Apply this with = 2 log πθ and Pinskers inequality TV(p, q) 1 2 DKL(pq). 4. Remainder control Lemma F.7 (Cubic remainder is o(f )). If Hq(θ) mI on the update subspace (local strong convexity), then for small 2 2 (f (θ) (θ+)), O(3) = o(f (θ)). Proof. From Lemma F.4, (θ) (θ+) 2 the cubic term is lower order. 2 + O(3). Rearranging yields 2 = O(f (θ) (θ+)), so F.2.1 Proof of Proposition 3.1 Proof of Proposition 3.1. Let (θ) = DKL(qβπθ) and = θ+ θ. By Lemma F.4, (θ) (θ+) = 1 2 Hq(θ) + O(3). By Lemma F.5, By Lemma F.6 with κ = (θ), = (1 + O( DKL(πθ+ πθ) = 2 (θ) + O(3). κ)) Hq. Hence DKL(πθ+ πθ) = (1 + O( κ)) (f (θ) (θ+)) + O(3). Since (θ+) 0, (θ) (θ+) (θ) = κ. By Lemma F.7, O(3) = o(f (θ)). Therefore DKL(πθ+ πθ) (1 + o(1)) (θ) = (1 + o(1)) DKL(qβπθ), which is the desired inequality. F.2.2 Proof of Proposition 3.2 Proof of Proposition 3.2. By the quadratic expansion of policy KL (Lemma F.5), DKL(πθ+πθ) = 1 2 (θ) + R(), R() (12) for some local constant > 0 (from 3 smoothness). Let = (θ). Using the spectral lower bound (θ) µI on the update subspace, 2 µ . (13) Combining equation 12equation 13 yields DKL(πθ+πθ) 2 ( µ 3/2 ) . Since DKL(πθ+ πθ) K, we have 1 2 µ3/2a3/2. (14) For sufficiently small (equivalently, small), the cubic term is dominated by the linear term: choose a0 > 0 so that µ3/2 whenever 0 < a0. Then from equation 14 1 4 ( 1 2 1 4 )a = 1 4 4K. Substituting 4K back into equation 12 refines the remainder: R() C3 C(a/µ)3/2 = O(K 3/2) = o(K), so DKL(πθ+πθ) = 2 + o(K). Hence = 2 DKL(πθ+πθ) + o(K) 2K + o(K), i.e. Taking square roots gives the Fisher-norm bound in equation 6: (θ) The Euclidean bound follows from equation 13: = (θ) 2K (1 + o(1)). (θ) 2K (1 + o(1)). 2 (θ) µ 2K µ (1 + o(1)). Finally, for any parameter block θ, its Frobenius change is the ℓ2-norm of the corresponding subvector of ; therefore F 2. F.2.3 One-step KL budget (used in Gate II) Corollary F.8 (KL budget). If DKL(πθ+ πθ) K, then 1 2 (θ) (1 + o(1)). Proof. Apply Lemma F.5 and Lemma F.7. 24 F.2.4 Trust-region / clipping bound (for β = 0) Lemma F.9 (Implicit KL leash from ratio clipping). Let rt = πθ+ (ytx,y<t) πθ(ytx,y<t) rt [1 ε, 1 + ε] on the batch. Then and suppose clipping enforces DKL(πθ+ πθ) E[T (x)] max{ log(1 ε), log(1 + ε)} = O(ε) E[T (x)], and in the small-step regime (mean-zero advantage) this tightens to O(ε2). Proof. Autoregressive factorization gives DKL(πθ+πθ) = Eπθ+ [t log rt]. Because log rt [log(1ε), log(1+ε)], we have log rt c(ε); summing over and taking batch expectation yields the stated bound. Using log(1 ε) = ε + O(ε2) and small-step arguments gives O(ε2). F.3 Proofs for Gate II (Sec. 3.2) Setup (layer-conditioned budget). Partition θ = (vec(W ), θW ) and let the Fisher at θ = θt be (θ) = [ FW,W FW,W FW,W FW,W ] 0. For one-step update θ, the global KL leash implies 1 curvature 2 θF (θ)θ K. Define the layer-conditioned and the per-layer budget δW = 1 subspace. SW = FW,W FW,W 1 2 vec(W )SW vec(W ) K. Let µW = λmin(SW ) > 0 on the update W,W FW,W 0, Lemma F.10 (Layer-conditioned Frobenius/operator bounds). F 2δW /µW and 2 F . Proof. Since SW µW I, δW 1 2 µW 2 . Lemma F.11 (Wedins sinΘ). For W+ = W0 + , let γk = σk(W0) σk+1(W0) be the singular value gap. For any where γk > 0, the principal subspace angles satisfy sin Θ(Uk(W0), Uk(W+))2 2/γk and similarly for Vk. Lemma F.12 (Weyl/Mirsky and HoffmanWielandt). σk(W+) σk(W0) 2 and i(σi(W+) σi(W0))2 2 . Corollary F.13 (Projection stability). With the same assumptions (including γk > 0), Uk(W0)Uk(W0) Uk(W+)Uk(W+) = sin Θ(Uk(W0), Uk(W+)) 2 2δW /µW γk . The analogous bound holds for the right subspaces with Vk. Interpretation. The leading invariant subspaces rotate by at most O( δW /µW /γk); when the gap is moderate, the rotation is small."
        },
        {
            "title": "G More Visualization",
            "content": "G.1 Jaccard matrix RL updates are highly consistent across independent training runs. Fig. 12 shows the pair-wise Jaccard similarity between the final update masks from five RLVR runs on different data and algorithms. The high similarity scores demonstrate that the optimization process consistently targets the same subset of parameters, providing strong evidence for deterministic, non-random optimization bias. G.2 Spectrum shift for DS-1.5B and Qwen3-1 We also show the spectrum shift for DS-1.5B and Qwen3-14B here. 25 Figure 12 Pair-wise Jaccard similarity of update masks from five independent RLVR runs on Layer 13 of the DS-DistillQwen-1.5B model. Table 4 Performance of DS-Qwen-1.5B with different masking strategies at 320 steps. Parameter counts shown are for linear layers only, excluding the embedding and head layers. Detailed evaluation settings are available in Appendix C.2. We observe that training only on principal weights Mprinc results in clear accuracy gap compared to both the dense baseline and its complement princ Mlowest masks achieve performance closest to the dense baseline. princ. The models using the Mlow and Model Mask Math500 AMC AIME24 AIME25 Average #params DS-Qwen-1.5B Dense Mprinc princ Mlow princ Mlow Random-M princ Mlow 84.20 83.60 82.70 84.50 85.20 84.50 81.56 77.19 78.90 80.08 78.83 77.35 36.98 30.16 34.28 35.62 34.74 34. 27.03 24.32 25.73 26.56 26.20 25.01 57.44 53.82 55.40 56.69 56.24 55.34 100% 50% 50% 58.59% 74.02% 74.02% 26 Figure 13 Structured Update observed on Llama(Llama-3.1-8B) and Mistral (Mistral-Small-24B) models. Here we plot the weight update mask using the zero-RL checkpoints from Zeng et al. (2025b). Figure 14 Temporal emergence of the optimization bias with row and column-wise update ratios for the 13th attention block across gradient update steps (t {240, 720, 1200}), smoothed with 3-step window. The column-wise (Q) and row-wise (O) update ratios show much weaker bias. Table 5 Performance of DS-Qwen-1.5B with different masking strategies with extended training window to 500 steps. Parameter counts shown are for linear layers only, excluding the embedding and head layers. Detailed evaluation settings are available in Appendix C.2. We observe that training only on principal weights Mprinc results in clear accuracy gap compared to both the dense baseline and its complement princ Mlowest masks achieve performance closest to the dense baseline. princ. The models using the Mlow and Model Mask Math500 AMC AIME24 AIME25 Average #params DS-Qwen-1.5B Dense Mprinc princ Mlow princ Mlow Random-M princ Mlow 84.5 83.60 84.0 83.8 84.10 84.10 83.52 78.83 77.97 82.42 81.41 81.72 38.28 34.06 38.64 37.03 40.30 34. 28.075 25.63 27.81 27.82 27.70 27.34 58.59 55.44 56.90 57.77 58.37 56.89 100% 50% 50% 58.59% 74.02% 74.02% 27 Figure 15 Token-wise KL loss. We show the token-wise KL loss during DAPO run without KL loss penalty, which shows steadily increasing KL loss instead of being unconstrained. Figure 16 The spectrum probe results on the RL and SFT version on the DS-Distill-Qwen-1.5B Liu et al. (2025a). RLVR shows surprisingly stable top-k spectrum with minimal subspace rotation and top-k eigenvalue changes. Figure 17 The spectrum probe results on the RL and SFT version on the Qwen3-14B Huan et al. (2025). RLVR shows surprisingly stable top-k spectrum with minimal subspace rotation and top-k eigenvalue changes. 28 Figure 18 Spectral geometry under RLHF setting Meng et al. (2024b). Across RLHF checkpoints, RL training preserves layer spectra and induces only minor rotation of the top-k subspaces, consistent with the RLVR regime."
        }
    ],
    "affiliations": [
        "Meta AI",
        "The University of Texas at Austin"
    ]
}