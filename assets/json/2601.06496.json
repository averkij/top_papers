{
    "paper_title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
    "authors": [
        "Hao Tang",
        "Ting Huang",
        "Zeyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2."
        },
        {
            "title": "Start",
            "content": "IJCV manuscript No. (will be inserted by the editor) 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence Hao Tang Ting Huang Zeyu Zhang 6 2 0 2 0 ] . [ 1 6 9 4 6 0 . 1 0 6 2 : r Received: date / Accepted: date Abstract Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on frozen CLIP-based semantic prior, spatially-aware 3D scene encoder for geometry, and multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 Hao Tang School of Computer Science, Peking University E-mail: bjdxtanghao@gmail.com Ting Huang School of Computer Science, Peking University E-mail: hting247@gmail.com Zeyu Zhang School of Computer Science, Peking University E-mail: steve.zeyu.zhang@outlook.com Equal contribution. Corresponding author. CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/ 3DCoCav2. Keywords Spatial Intelligence 3D Captioning Contrastive Learning Test-Time Search Out-ofDistribution Generalization Fig. 1: Overview of 3D CoCa v2 and OOD results on TOD3Cap. (a) 3D CoCa v2 extends 3D CoCa [17] with an inference-only test-time search (TTS) module and an external LLM judge. (b) Zero-shot OOD performance on TOD3Cap [21] comparing 3D-VLP [51], 3D CoCa [17], and 3D CoCa v2 under standard captioning metrics at IoU 0.25 and 0.5. 1 Introduction Developing spatial intelligence in real-world environments requires models that can not only perceive 3D geometry but also communicate spatial understanding 2 Hao Tang, Ting Huang, and Zeyu Zhang through natural language. In recent years, 3D representation learning has attracted increasing attention due to its broad impact on robotics, autonomous driving, and augmented reality [12, 13]. In parallel, the convergence of computer vision and natural language processing has fostered vision-language tasks that connect perception with linguistic understanding, where captioning serves as an intuitive interface for interpreting complex scenes. Although large-scale vision-language models have led to substantial progress in 2D captioning, extending captioning to 3D remains considerably more challenging: point clouds are sparse and irregular, objects are cluttered or partially observed, and faithful descriptions require not only recognizing object attributes but also reasoning about their spatial context. Early 3D captioning methods, therefore, largely adopted two-stage detect-then-describe paradigm, first generating object proposals and then describing each region. Scan2Cap [7] is an early representative that cascades 3D detection and caption generation, followed by efforts that incorporate language pre-training and cross-modal alignment to improve 3D captioning quality [22]. Despite their effectiveness, two-stage pipelines have well-known drawbacks: the detection stage often produces redundant or noisy proposals and requires additional post-processing, such as Non-Maximum Suppression [32]. Meanwhile, the quality of captions becomes tightly coupled to detection accuracy. To alleviate these issues, one-stage end-to-end frameworks have gained popularity. Vote2Cap-DETR [10] and Vote2Cap-DETR++ [11] adopt Transformer-based formulations that jointly localize and describe objects. Recent designs such as BiCA [24] and See-It-All [23] further enhance contextual aggregation in 3D scenes. Meanwhile, TOD3Cap [21] targets outdoor settings and highlights the growing need to handle diverse real-world environments. However, existing 3D captioners still face two fundamental challenges, especially under OOD deployment: (i) their grounding degrades markedly when moving beyond the training domain. For example, models trained on indoor RGB-D reconstructions often encounter drastically different geometries, sensing artifacts, and scene layouts in outdoor environments, which lead to unreliable spatial grounding and increased hallucination. (ii) current methods generally lack strong and transferable cross-modal alignment between 3D observations and language, resulting in limited OOD generalization across environments. Addressing these challenges requires not only improving in-domain spatial reasoning but also introducing principled mechanisms that enhance robustness under distribution shifts, particularly for indoor-to-outdoor transfer. promising direction is to leverage strong visuallinguistic priors from large-scale pre-training to improve semantic grounding and cross-modal alignment. Foundation vision-language models such as CoCa [48] demonstrate that contrastive pre-training on large image-text corpora yields representations with rich semantics and strong alignment between modalities. Motivated by this, we develop 3D CoCa v2, unified 3D captioning framework that combines contrastive vision-language learning with 3D caption generation in shared feature space. 3D CoCa v2 builds on frozen CLIP vision-language backbone for semantic priors, spatially-aware 3D scene encoder for geometric context, and multi-modal decoder that jointly optimizes contrastive and captioning objectives. This unified design avoids reliance on external detectors or handcrafted proposals and establishes strong captioner with improved semantic grounding. However, even strong unified captioner can still produce suboptimal outputs under domain shift, as standard decoding typically commits to single caption without considering alternative hypotheses. This motivates our key observation: test-time search over multiple candidates can improve robustness and faithfulness without updating model parameters. To this end, we introduce Test-Time Search for 3D CoCa v2. As illustrated in the right panel of Fig. 1(a), TTS is an inference-only module built on top of the 3D CoCa [17] backbone; it generates multiple caption candidates and performs reward-guided selection conditioned on compact scene summary. By explicitly searching among plausible captions and selecting the one best supported by scene evidence, TTS serves as simple plug-and-play mechanism that improves caption faithfulness under distribution shift, without additional training or parameter updates. We evaluate 3D CoCa v2 in both in-domain and out-of-distribution settings. On the indoor benchmarks ScanRefer [5] and Nr3D [1], 3D CoCa v2 consistently improves over 3D CoCa by applying test-time search. To assess crossenvironment generalization, we further evaluate on the outdoor benchmark TOD3Cap [21] in zero-shot OOD setting. As summarized in Fig. 1(b), 3D CoCa v2 achieves further +3.6 CIDEr@0.5 improvement over 3D CoCa, demonstrating stronger robustness under distribution shift. In summary, the main contributions of this work include: We present 3D CoCa v2, unified and end-to-end 3D captioning framework that combines contrastive vision-language learning with 3D caption generation in shared feature space, avoiding external detectors or handcrafted proposals. 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence 3 We introduce Test-Time Search (TTS), judgeguided reward-based inference strategy that generates diverse caption candidates and performs reward-guided selection using compact scene summary, thereby improving robustness under domain shift without updating the captioner parameters. Extensive evaluations demonstrate that 3D CoCa v2 consistently improves over 3D CoCa [17] on the indomain benchmarks ScanRefer and Nr3D. With the best-of-N Test-Time Search (N =8 by default), 3D CoCa v2 achieves +1.50 CIDEr@0.5 on ScanRefer (w/o additional 2D input) and +1.61 CIDEr@0.5 on Nr3D. Moreover, under OOD evaluation on TOD3Cap in zero-shot setting, it yields further +3.6 CIDEr@0.5 gain over [17], demonstrating improved robustness under distribution shift. 2 Related Work 3D captioning localizes objects in 3D scenes and describes them in natural language. As high-level semantic interface, 3D captioning plays crucial role in enabling spatial intelligence by requiring models to jointly perceive geometry, reason about spatial relationships, and communicate scene understanding through language. Early works typically followed twostage detect-then-describe paradigm. Scan2Cap [7] pioneered this task by coupling 3D object localization with caption generation, and subsequent methods enhanced relational reasoning and contextual modeling, e.g., MORE [20]. Transformer-based architectures further accelerated progress. SpaCap3D [42] employed an encoder-decoder design with spatially guided representations for geometry-aware captioning, while χ-Trans2Cap [50] distilled semantic knowledge from 2D vision-language models into 3D captioner. Recent works also pursue unified multi-task formulations, such as 3DJCG [3] and UniT3D [8], which jointly optimize captioning with related grounding or scene understanding objectives. To mitigate error propagation from staged pipelines, end-to-end paradigms have been explored. Vote2Cap-DETR [10] and Vote2CapDETR++ [11] reformulate dense captioning as setprediction problem and jointly localize and describe objects in single forward pass. TOD3Cap [21] further targets outdoor environments and highlights the importance of robustness under domain shift, which remains challenging for indoor-trained 3D captioners. 3D pre-training and vision-language models. Another line of research focuses on learning transferable 3D representations via pre-training. Unsupervised 3D representation learning can be broadly grouped into global contrastive methods [41, 31], local contrastive objectives [46, 44], and masked point modeling approaches [49, 33]. At high level, contrastive learning [4, 39, 53] encourages transferable representations by pulling semantically similar samples together in the embedding space while pushing apart dissimilar ones, making it natural choice for unsupervised and weakly supervised 3D pre-training. These methods learn strong geometric features but do not explicitly ground 3D representations in natural language. To bridge this gap, 3D vision-language pre-training aligns 3D regions or segments with text descriptions [18]. For example, 3DVLP [51] aligns point cloud segments with language using contrastive learning, and UniT3D [8] demonstrates that pre-training on large-scale point cloudcaption pairs benefits multiple 3D understanding tasks. Beyond pre-training, recent 3D vision-language foundation models further improve 3Dlanguage alignment and generalization through enhanced reasoning and instruction tuning. For instance, 3D-R1 [16] studies unified 3D reasoning across diverse scene understanding tasks, and its reasoning-oriented representations have also shown promise for downstream embodied settings that require grounded perception and decision making [15, 29, 47, 26, 38, 37]. Overall, these advances motivate unified contrastivegenerative frameworks that strengthen cross-modal alignment and semantic grounding for 3D captioning. Test-time search and judging. Beyond trainingtime modeling, inference-time strategies have been studied to improve generation quality without updating model parameters. common approach is to generate multiple candidates and select the best output using an auxiliary scoring signal, including selfconsistency style aggregation [43] and related samplingand-selection schemes [19]. Recent advances also explore large language models as judges to provide preference signals for open-ended evaluation and selection, such as MT-Bench and Chatbot Arena [52] and rubricbased frameworks like G-Eval [28]. In the agent setting, AgentRM [45] investigates judge-guided search for improved generalization, and training-free pipelines employ LLM-driven components for open-world decision making. These developments motivate using test-time search with external judging signals as plug-and-play mechanism for robustness. Different from prior textonly judging setups, 3D captioning requires bridging the modality gap between point clouds and languagebased judges, which motivates compact scene summaries for reliable test-time selection. 4 Hao Tang, Ting Huang, and Zeyu Zhang Fig. 2: Overview of 3D CoCa v2. (a) 3D CoCa learns aligned 3Dtext representations by jointly optimizing contrastive alignment and caption generation: point-cloud scene encoder and text encoder produce fused features for multi-modal decoder to generate draft caption. (b) Test-Time Search (inference-only) improves robustness without any parameter updates by generating best-of-N candidate captions from the backbone, conditioning an external LLM judge on compact scene summary, and selecting the highest-scoring candidate as the final caption. 3 The Proposed Method 3.1 Overview In this section, we present 3D CoCa v2, generalizable framework that bridges 3D point cloud representations and natural language for 3D captioning. 3D CoCa v2 follows unified contrastive-generative design, drawing inspiration from CLIP-style vision-language pre-training [36] and the Contrastive Captioner (CoCa) paradigm [48]. As illustrated in Fig. 2(a), the backbone consists of four components: 3D Scene Encoder, Text Encoder, Contrastive Learning module, and MultiModal Fusion Decoder. Compared with 3D CoCa, the key extension of 3D CoCa v2 is an inference-time Test-Time Search procedure, as illustrated in Fig. 1(a) and Fig. 2(b), which improves robustness under distribution shifts without updating the captioner parameters. Instead of producing single caption by standard decoding, TTS generates set of diverse candidates and performs reward-guided selection using compact scene summary. In our setting, the reward is provided by an external large language model acting as judge. The backbone is trained end-to-end with contrastive and captioning objectives, while TTS is applied only at inference time and requires no additional training. 3.2 3D Scene Encoder The 3D scene encoder transforms an unstructured point cloud into set of latent tokens that capture geometric and semantic content. It integrates point-based 3D processing with frozen 2D CLIP visual backbone to capture both geometry and semantics. It comprises three components: (i) point cloud tokenizer that partitions raw point clouds into patch tokens, (ii) learnable task tokens that inject 3D captioning context, and (iii) frozen CLIP Vision Transformer that encodes the concatenated token sequence. As shown in Fig. 2 (top-left), the encoder converts the raw 3D input into structured representation suitable for multimodal reasoning. Point cloud tokenizer. Given an input point cloud RN (3+F ), each point is described by 3D coordinates (x, y, z) and additional features (e.g., color or 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence 5 normals). We convert the point cloud into discrete token sequence for transformer processing. We use farthest point sampling (FPS) to select representative points as patch centers. For each center, we gather its nearest neighbors to form local patch, producing patches {P1, . . . , PM }, each containing points. Each patch is encoded by lightweight point-wise encoder implemented as multi-layer perceptrons (MLPs), yielding point tokens of dimension Dp: Ep(P ) = [ep1, ep2 , . . . , epM ] RM Dp , (1) where epi denotes the embedding of the i-th patch. Task token. Point tokens capture local geometry and appearance but lack explicit task awareness. To guide the model toward captioning, we introduce mt learnable task tokens that are prepended to the point token sequence. Inspired by prompt tuning [27], task tokens act as high-level prompts that aggregate global semantic cues (e.g., layout and salient objects) via self-attention and condition the encoder for language-relevant feature extraction. Frozen CLIP vision encoder. We concatenate the point tokens and task tokens into unified sequence: [ep1 , . . . , epM ; t1, . . . , tmt], (2) where tj denotes the j-th task token. This sequence is fed into frozen CLIP Vision Transformer [36]. All CLIP weights are kept frozen to preserve pre-trained visual representations and stabilize optimization. The CLIP encoder outputs latent embeddings that jointly capture 3D geometry and task context. We extract global scene representation fenc RD as the scene embedding used for contrastive alignment and captioning. 3.3 Text Encoder The text encoder maps natural language descriptions into semantically aligned embedding space. We adopt the Transformer-based CLIP text encoder [36] and keep its weights frozen to retain the rich linguistic knowledge acquired during large-scale pretraining. Text tokenizer. Given an input caption , we tokenize it into subword tokens and map them to embeddings: Et(T ) = [et1, et2 , . . . , etL] RLDt, (3) where eti is the embedding of the i-th token. We add positional encodings and prepend special token used as sentence-level aggregator. Frozen CLIP text encoder. The token embeddings are processed by the CLIP text Transformer, consisting of Nte layers: with 0 = Et(T ). All weights are frozen. We take the hidden state of the special token from the final layer as enc RDt, which serves the global text representation as the language-side embedding in contrastive learning."
        },
        {
            "title": "3.4 Contrastive Learning",
            "content": "To align 3D scenes and text, we employ contrastive objective that maps the scene feature fenc and the text feature enc into shared embedding space. Matched 3D-text pairs are brought together, while mismatched pairs are pushed apart, following the CLIP paradigm. Feature alignment. We project both features into shared space using learnable projection heads: fenc = MLPv(fenc), enc = MLPt(f enc), (5) and apply L2 normalization: ˆfenc = fenc fenc2 , ˆf enc = enc enc2 . (6) Contrastive loss. For batch of paired samples, the cosine similarity between scene and text is sim (cid:16) ˆfenc,i; ˆf enc,j (cid:17) = ˆfenc,i ˆf enc,j (cid:13) (cid:13) (cid:13) ˆfenc,i ˆf (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) enc,j , (cid:13) (cid:13) (cid:13) (7) We use an InfoNCE loss: LCon = 1 (cid:80)N i=1 log exp(cid:0)sim( ˆfenc,i; ˆf j=1 exp(cid:0)sim( ˆfenc,i; ˆf enc,i)/τ(cid:1) enc,j )/τ(cid:1) , (8) (cid:80)N where τ is learnable temperature. 3.5 Multi-Modal Fusion Decoder The multi-modal fusion decoder generates captions conditioned on the input 3D scene. It is implemented as an autoregressive Transformer decoder with crossattention, generating tokens one by one. At time step t, the decoder predicts yt conditioned on the previously generated tokens y<t via causal self-attention and on the encoded scene representation via cross-attention: (yt y<t, fenc). Cross-attention mechanism. Let Qtext be the query matrix from the decoder hidden states, and let Kscene, Vscene be the key and value matrices derived from scene tokens. Cross-attention is computed as Attention(Qtext, Kscene, Vscene) = softmax (cid:16) QtextK dk scene (cid:17) Vscene, (9) = TransformerBlockl(H l1), [1, . . . , Nte], (4) where dk is the key dimensionality. 6 Hao Tang, Ting Huang, and Zeyu Zhang Algorithm 1: Training of 3D CoCa v2 Require: Point cloud , paired caption 1: Ep Tokenizer3D(P ) 2: Et Tokenizertext(T ) 3: fenc CLIPvis(Ep) {frozen} 4: enc CLIPtxt(Et) {frozen} 5: ˆfenc, ˆf enc ProjNorm(fenc, 6: LCon InfoNCE(ˆfenc, ˆf 7: ˆC Decoder(fenc) 8: LCap CE( ˆC, ) 9: LT otal LCon + λLCap 10: Update trainable parameters with LT otal enc) enc) Algorithm 2: TTS for 3D CoCa Require: Point cloud , candidate size , summary size Ks Ensure: Final caption 1: fenc CLIPvis(Tokenizer3D(P )) {frozen backbone} 2: ˆfenc ProjNorm(fenc) 3: S(P ) RetrieveSummary(ˆfenc, Ks) 4: Generate {Ci}N Decoder(fenc) 5: for = 1 to do 6: 7: end for 8: arg maxCi ri i=1 by stochastic decoding from ri J(S(P ), Ci) {external judge}"
        },
        {
            "title": "3.6 Training Objectives and Joint Optimization",
            "content": "We train the backbone with combination of contrastive loss and captioning loss. The overall backbone training procedure is summarized in Alg. 1. The contrastive loss LCon in Eq. (8) aligns scene and text features in shared space. The decoder is supervised with standard cross-entropy captioning loss. Given predicted caption ˆY = (ˆy1, . . . , ˆyL) and the corresponding ground-truth sequence = (y1, . . . , yL), the captioning loss is defined as: LCap = (cid:88) t=1 log (ˆyt = yt ˆy<t, fenc) , (10) where fenc is the global 3D scene embedding used to condition the decoder via cross-attention. The overall objective is LTotal = LCon + λ LCap, (11) where λ balances alignment and generation. Notably, the proposed Test-Time Search is applied only during inference and does not introduce additional trainable parameters or losses in backbone optimization. frozen CLIP text encoder, and let ϕ() denote their embeddings. Given ˆfenc, we retrieve the top-Ks descriptors and concatenate them into compact summary: S(P ) = Concat (cid:16) (cid:17) TopK(sim( ˆfenc, ϕ(bk))) . (12) This summary provides high-level evidence, such as salient objects and scene types. Alternatively, S(P ) can be produced by structured decoding; we adopt retrieval for simplicity and reproducibility. Candidate generation and selection. Given , we generate set of diverse candidates {Ci}N i=1 using stochastic decoding. An external large language model acts as judge and outputs scalar reward for each candidate conditioned on S(P ): ri = J(S(P ), Ci). The final caption is selected by = arg max Ci ri, (13) (14) and top-k voting can be applied when multiple highscoring candidates are retained. All prompts and scoring rubrics used for the judges are fixed across datasets and are reported for reproducibility. 3.7 Test-Time Search 4 Experiments Standard decoding commits to single caption and may be brittle under distribution shifts. To improve faithfulness without updating model parameters, 3D CoCa v2 introduces Test-Time Search, which performs rewardguided selection over multiple candidate captions. The full inference-time procedure is summarized in Alg. 2. Compact scene summary. language-only judge cannot directly interpret raw point clouds. TTS conditions the judge on compact scene summary S(P ) derived from the scene embedding. We construct S(P ) via retrieval in the shared contrastive space. Let = {bk} be bank of short textual descriptors encoded by the 4.1 Datasets and Evaluation Metrics Datasets. We evaluate in-domain 3D captioning on ScanRefer [5] and Nr3D [1], which provide humanannotated descriptions for objects in indoor 3D scenes. ScanRefer contains 36,665 descriptions for 7,875 objects across 562 scenes, while Nr3D includes 32,919 descriptions for 4,664 objects in 511 scenes. Both benchmarks are derived from ScanNet [14], which comprises 1,201 reconstructed indoor scenes. We follow the standard validation splits used in prior work: ScanRefer includes 9,508 descriptions for 2,068 objects in 141 scenes, and 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence 7 Table 1: Comparison on ScanRefer [5]. We evaluate the performance of each method, with and without additional 2D input, at IoU thresholds of 0.25 and 0.5. Metrics include CIDEr (C) [40], BLEU-4 (B-4) [34], METEOR (M) [2], and ROUGE-L (R) [25]. Method Scan2Cap [7] MORE [20] SpaCap3d [42] 3DJCG [3] D3Net [6] 3D-VLP [51] Vote2Cap-DETR [10] Unit3D [8] Vote2Cap-DETR++ [11] BiCA [24] See-It-All [23] 3D CoCa [17] 3D CoCa v2 (Ours) w/o additional 2D input 53.73 58.89 58.06 60.86 - 64.09 71.45 - 76.36 78.42 78.68 IoU = 0.25 26.14 26.36 26.16 27.45 - 27.65 28.25 - 28.70 28.82 29.21 B-4 34.25 35.41 35.30 39.67 - 39.84 39.34 - 41.37 41.46 43.25 54.95 55.41 55.03 59.02 - 58.78 59.33 - 60.00 60.02 63.06 35.20 38.98 42.76 47.68 - 50.02 61.81 - 67.58 68.46 73. IoU = 0.50 21.44 21.65 22.84 24.28 - 24.53 26.22 - 26.89 27.56 28.19 B-4 22.36 23.01 25.38 31.53 - 31.87 34.46 - 37.05 38.23 40.91 43.57 44.33 45.66 51.80 - 51.17 54.40 - 55.64 58.56 60.46 56.82 62.91 63.30 64.70 - 70.73 72.79 - 77.03 78.35 78.05 IoU = 0.25 26.29 26.75 26.71 27.66 - 28.14 28.06 - 28.53 28.82 28.74 B-4 34.18 36.25 36.46 40.17 - 41.03 39.17 - 40.99 41.20 42. w/ additional 2D input IoU = 0.50 55.27 56.33 55.71 59.23 - 59.72 59.23 - 59.59 59.80 61.70 39.08 40.94 44.02 49.48 46.07 54.94 59.32 46.69 64.32 66.47 69.86 B-4 23.32 22.93 25.26 31.03 30.29 32.31 32.42 27.22 34.73 36.13 37.89 21.97 21.66 22.33 24.22 24.35 24.83 25.28 21.91 26.04 26.71 27. 44.78 44.42 45.36 50.80 51.67 51.51 52.53 45.98 53.67 54.54 57.33 61.98 85.42 63.98 86.95 +1.53 +0.43 +0.60 +0.92 45.56 45.99 30.95 31.55 57.40 77.13 61.95 78.63 +1.50 +0.32 +0.43 +1.49 28.52 28. 41.23 41.55 61.45 86.12 62.45 87.05 +0.93 +0.31 +1.03 +0.75 44.79 45.10 30.75 31.78 55.23 74.52 55.63 75.60 +1.08 +0.38 +0.22 +0.4 28.03 28. 38.42 38.70 Table 2: Results on Nr3D [1] at IoU=0.5. We report CIDEr (C) [40], BLEU-4 (B-4) [34], METEOR (M) [2], and ROUGE-L (R) [25]. Method Scan2Cap [7] SpaCap3d [42] D3Net [6] 3DJCG [3] Vote2Cap-DETR [10] Vote2Cap-DETR++ [11] BiCA [24] 3D CoCa [17] 3D CoCa v2 (Ours) C@0.5 B-4@0.5 M@0.5 R@0.5 27.47 33.71 33.85 38.06 43.84 47.08 48. 17.24 19.92 20.70 22.82 26.68 27.70 28.35 21.80 22.61 23.13 23.77 25.41 25.44 25.60 49.06 50.50 53.38 52.99 54.43 55.22 55.81 52.84 54.45 +1.61 29.29 29.85 +0.56 25.55 56.43 57.12 25.95 +0.35 +0. Nr3D includes 8,584 descriptions for 1,214 objects in 130 scenes. All evaluation scenes are drawn from the ScanNet validation set, ensuring consistent protocol across benchmarks. To assess OOD generalization across environments, we further evaluate on the outdoor 3D dense captioning benchmark TOD3Cap [21]. Since our focus is on caption generalization rather than detection, we adopt an oracle-box setting on TOD3Cap, where ground-truth 3D boxes are provided at test time to isolate the captioning component under distribution shift. Evaluation metrics. We report standard captioning metrics, including CIDEr [40], BLEU-4 [34], METEOR [2], and ROUGE-L [25], denoted as C, B-4, M, and R, respectively. Following common practice in 3D dense captioning [7, 20, 42, 10, 3], we evaluate caption quality under the localization-aware m@kIoU protocol [7]. Given annotated objects, the metric is defined as: m@kIoU = 1 (cid:80)N i=1 m(ˆci, ci) (cid:110) IoU (cid:16)ˆbi, bi (cid:17) (cid:111) , (15) where ˆci and ci denote the predicted and ground-truth captions, ˆbi and bi are the predicted and ground-truth 3D bounding boxes, and m() is captioning metric (e.g., CIDEr, METEOR, BLEU-4, ROUGE-L). For ScanRefer and Nr3D, we report results at the standard IoU thresholds (0.25 and 0.5 for ScanRefer and 0.5 for Nr3D). We evaluate on TOD3Cap following the benchmarks standard captioning metrics and IoU thresholds and report CIDEr at IoU {0.25, 0.5} for consistency with prior work. Since our goal is to assess caption generalization rather than detection, we adopt an oraclebox setting on TOD3Cap, where ground-truth 3D boxes are provided at test time (i.e., ˆbi = bi). Therefore, the IoU-gating indicator in Eq. (15) is always satisfied, and the reported scores reflect caption quality conditioned on correct localization. To quantify faithfulness under test-time search, we additionally report hallucination rate measuring the fraction of generated captions that mention objects or attributes not supported by scene evidence. We compute this metric using the same verifier for all methods 4.2 Implementation Details Input representation. The input point cloud RN (3+F ) contains = 40,000 points. Each point includes its 3D coordinates and additional features. In the w/o additional 2D setting, we use color, normal, and height as per-point features. In the w/ additional 2D setting, we replace the raw color with 2D multiview features extracted using ENet [9], following the established protocol in [7]. Backbone training. We train the 3D CoCa v2 backbone with the joint contrastive and captioning objective described in sec. 3.6 (Alg. 1). Optimization uses AdamW [30] with learning rate η = 0.1, batch size = 4, and cosine annealing schedule. Models are trained for = 1080 epochs on ScanRefer and Nr3D. All experiments are conducted on 2 NVIDIA RTX 4090 GPUs. Test-Time Search (TTS). TTS is applied only at inference time and does not update backbone parame8 Hao Tang, Ting Huang, and Zeyu Zhang Table 3: Results on TOD3Cap [21] under two protocols. Top: in-domain training on TOD3Cap. Bottom: zero-shot OOD evaluation on TOD3Cap (trained on indoor data only, no TOD3Cap fine-tuning). indicates replacing the scene encoder with BEV encoder for adaptation, following [21]. Method Venue C@0.25 B-4@0.25 M@0.25 R@0.25 C@0.5 B-4@0.5 M@0.5 R@0.5 Scan2Cap [7] CVPR 2021 Vote2Cap-DETR [10] CVPR 2023 Vote2Cap-DETR++ [11] T-PAMI 2024 TOD3Cap [21] ECCV 2024 50.6 72.8 78.8 85.3 34.3 41.6 42.6 43.0 25.2 29.5 29.9 29.9 57.9 60.6 60.8 60. In-domain (train on TOD3Cap) OOD generalization (zero-shot on TOD3Cap) Scan2Cap [7] Vote2Cap-DERT [10] Vote2Cap-DETR++ [11] 3D-VLP [51] CVPR 2021 CVPR 2023 T-PAMI 2024 AAAI 2024 3D CoCa [17] 3D CoCa v2 (Ours) 3DV 2026 - - 42.6 49.8 52.1 52.9 55.8 59.6 +3.8 23.1 27.0 28.1 28.7 30.5 31.6 +1.1 20.8 22.6 23.1 23.4 24.0 24.6 +0. 48.3 51.2 52.0 52.6 54.2 55.7 +1.5 43.3 62.6 66.5 74.4 36.4 43.2 45.4 45.9 47.2 50.8 +3.6 31.3 35.9 36.9 39. 20.4 24.2 25.0 25.4 26.8 27.6 +0.8 22.8 27.4 27.4 27.2 19.4 21.2 21.5 21.7 22.1 22.6 +0.5 50.8 55.8 56.2 55. 45.1 48.7 49.4 49.8 50.1 51.3 +1.2 ters (Alg. 2). For each scene, we sample = 8 candidate captions using stochastic decoding. We construct compact scene summary S(P ) via retrieval in the frozen contrastive space: we pre-encode bank of short textual descriptors with the frozen CLIP text encoder and retrieve the top-Ks descriptors by cosine similarity to the scene embedding. An external large language model acts as judge and assigns scalar reward to each candidate given S(P ) and the candidate caption. The final caption is selected by maximum reward, with optional top-k voting when multiple high-scoring candidates are retained. 4.3 In-Domain Evaluation We compare 3D CoCa v2 against representative 3D dense captioning methods on ScanRefer and Nr3D using C, M, B-4, and R. We report results under IoU thresholds of 0.25 and 0.5 on ScanRefer, and 0.5 on Nr3D, following prior work. To highlight the contribution of inference-time optimization, we report two decoding settings for our method: (i) standard decoding using greedy or beam search, and (ii) Test-Time Search (TTS) with best-of-N candidate selection. Unless otherwise specified, we use best-of-N TTS with = candidates and set = 8 as default trade-off between quality and inference cost. TTS is an inferenceonly add-on. When TTS is disabled, the model reduces to the same contrastive-generative backbone as our 3D CoCa baseline, evaluated with standard decoding. ScanRefer. Table 1 shows that 3D CoCa v2 improves over 3D CoCa across both IoU thresholds and input settings. Without additional 2D input, 3D CoCa v2 raises CIDEr from 85.42 to 86.95 at IoU=0.25 and from 77.13 to 78.63 at IoU=0.50, with consistent gains on BLEU-4, METEOR and ROUGE-L. Overall, these results indicate that 3D CoCa v2 produces more informative captions while maintaining strong localization-aware caption quality under the standard ScanRefer protocol. Nr3D. On Nr3D  (Table 2)  , 3D CoCa v2 achieves consistent improvements over prior methods at IoU= 0.5. These gains indicate stronger semantic grounding for the free-form referring expressions in Nr3D, where captions must capture fine-grained attributes and contextual cues beyond object categories. 4.4 Out-of-Domain Evaluation TOD3Cap. We evaluate cross-environment generalization on TOD3Cap using zero-shot OOD protocol: all models are trained on indoor data only and evaluated on TOD3Cap without any TOD3Cap fine-tuning. As shown in Table 3 (bottom), 3D CoCa v2 achieves the best zero-shot performance and consistently improves over 3D CoCa. These results indicate that the proposed inference-time optimization improves robustness under OOD shifts from indoor to outdoor scenes. For completeness, Table 3 (top) also reports the in-domain upper bound when methods are trained to convergence on TOD3Cap. 4.5 Ablation Study Effect of the contrastive loss weight. We analyze the sensitivity of the backbone to the contrastive objective by varying the loss weight λ {0, 0.1, 0.5, 1.0, 2.0} while using standard decoding throughout (no TTS). As shown in Table 4, removing the contrastive term (λ=0) leads to the lowest captioning scores. Increasing λ improves performance, with CIDEr@0.25 rising from 74.12 to 79.55 when λ increases from 0 to 0.5. The best overall results are obtained at λ=1.0. Further increasing 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence Fig. 3: Qualitative comparisons on ScanRefer [5]. We visualize four representative scenes and the captions generated by 3D CoCa, 3D CoCa v2, and the ground truth (GT). Compared to the baseline, 3D CoCa v2 produces more detailed and better-grounded descriptions, capturing richer scene semantics and functional cues. Red-highlighted phrases mark the additional informative content provided by our method beyond the baseline. Table 4: Effect of contrastive loss weight λ on ScanRefer. Standard decoding is used throughout (no TTS). The best CIDEr is achieved at λ=1.0. λ 0.0 0.1 0.5 1.0 2. C@0.25 B-4@0.25 M@0.25 R@0.25 74.12 77.30 79.55 85.42 76.89 40.98 41.80 42.55 45. 41.50 27.45 28.10 28.75 30.95 28.00 58.76 59.60 60.40 61. 59.30 the weight to λ=2.0 degrades performance, suggesting that overly strong contrastive regularization can harm caption generation quality. Decoder architecture. We study the effect of the captioning decoder in the 3D CoCa backbone by replacing the proposed CoCa-style multimodal decoder with GPT-2 captioner while keeping the scene encoder and training protocol unchanged. All results are reported with standard decoding (no TTS). As shown in Table 5, using the CoCa-style decoder yields consistently higher scores across all metrics. This indicates that an explicitly cross-attentive multimodal decoder is beneficial for exploiting the contrastively aligned scene representations during caption generation. 3D scene encoder. We evaluate the contribution of the 3D scene encoder in the 3D CoCa by replacing the proposed point-tokenizer-based encoder (followed by frozen CLIP vision transformer) with conventional PointNet++ encoder [35], while keeping the remaining Table 5: The impact of different caption generation decoders. Comparison of the description indicators of the original GPT-2 generator and the CoCastyle multimodal decoder in this paper under the same visual features. Standard decoding is used throughout (no TTS). Decoder Setting C@0.25 B-4@0.25 M@0.25 R@0.25 GPT-2 Captioner (Baseline) 76.20 CoCa Transformer (3D CoCa) 85.42 41.00 45.56 27. 30.95 59.50 61.98 Table 6: Comparison of the impact of different 3D point cloud encoder architectures on description performance. All results use standard decoding (no TTS). Encoder Architecture C@0.25 B-4@0.25 M@0.25 R@0.25 PointNet++ (Baseline) 72.48 Our scene encoder 85.42 38.95 45. 26.80 30.95 56.30 61.98 components and training protocol unchanged. All results are reported with standard decoding (no TTS). As shown in Table 6, the proposed scene encoder achieves consistently higher captioning scores across all metrics, suggesting that the transformer-based tokenization and CLIP-informed representation provide stronger interface for multimodal decoding. Ablation on the LLM judge. We study how the external judge J() affects TTS. We fix the backbone, candidate generation (best-of-N , =8), the compact scene summary S(P ), and the scoring prompt, and only Hao Tang, Ting Huang, and Zeyu Zhang Fig. 4: Qualitative results on TOD3Cap [21] (OOD, zero-shot). We compare captions generated by the indoor-trained Vote2Cap-DETR++, 3D CoCa and 3D CoCa v2 on outdoor scenes with paired front and back views. Vote2Cap-DETR++ and 3D CoCa often exhibit strong indoor bias, producing generic indoor descriptions, whereas 3D CoCa v2 generates more scene-consistent outdoor captions that better reflect key semantics. Groundtruth (GT) captions are shown for reference. Red words highlight informative details captured by 3D CoCa v2 but missing in the baseline. Table 7: Ablation on the LLM judge for TTS. We vary only the external judge while keeping the backbone, best-of-N decoding (N =8), scene summary S(P ), and the scoring prompt fixed. We report CIDEr@0.5 and hallucination rate (Hall, %). highlighted in red, our captions more frequently capture salient functional cues and fine-grained evidence (e.g., clutter attributes and object-level details), while avoiding generic or underspecified descriptions. Judge (TTS) Qwen3-VL Pro Qwen3-VL Max Gemini 3 Flash Gemini 3 Pro ScanRefer (in-domain) Nr3D (in-domain) TOD3Cap (OOD, zero-shot) C@0.5 Hall C@0.5 Hall C@0.5 78.00 78.30 78.20 78.55 9.2 8.9 9.0 8.6 8.2 53.70 54.00 53.90 54.25 54.45 9.8 9.5 9.6 9. 8.8 49.20 49.80 49.60 50.30 50.80 Hall 13.6 12.9 13.1 12.4 11. GPT-5 (3D CoCa v2) 78.63 vary the judge model. As shown in Table 7, the relative ordering is consistent across datasets: stronger judges generally yield slightly higher CIDEr and lower hallucination rates. In particular, GPT-5 achieves the best overall trade-off while also attaining the lowest hallucination rates. Gemini 3 Pro is close second, whereas lighter judges (Gemini 3 Flash and Qwen3-VL Pro) remain competitive with only modest drop in CIDEr and small increase in hallucinations. These results indicate that TTS is not tied to specific judge and can be paired with different LLMs to balance caption quality and faithfulness under varying inference budgets without updating the captioner parameters. 4.6 Qualitative Results In-domain qualitative results. Fig. 3 presents qualitative comparisons on the in-domain ScanRefer [5] benchmark. Compared with 3D CoCa, 3D CoCa v2 generates captions that are more informative and better grounded, especially in cluttered indoor scenes. As Effect of test-time search (TTS). To further illustrate how TTS improves caption grounding, Fig. 5 shows side-by-side comparisons between standard decoding (w/o TTS) and our inference-only TTS (w/ TTS) on ScanRefer [5]. The magenta box indicates the target object, and green text highlights object-specific details introduced by TTS. Without TTS, the model tends to produce generic room-level descriptions. In contrast, TTS favors candidates that are better supported by the highlighted region and the surrounding 3D context, resulting in more specific object identities and layout cues and fewer underspecified statements. OOD qualitative results. Fig. 4 further shows qualitative results on TOD3Cap under OOD evaluation. The indoor-trained Vote2Cap-DETR++ and 3D CoCa baseline exhibit noticeable indoor bias, often describing outdoor driving scenes using indoor concepts (e.g., room or corridor) with generic layouts. In contrast, 3D CoCa v2 produces captions that better match the outdoor context across front and back views, correctly emphasizing roads, buildings, trees, signboards, and vehicles. Overall, these examples qualitatively support our OOD improvements, indicating that TTS mitigates domain-induced hallucinations and improves caption faithfulness under distribution shift. 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence 11 Fig. 5: Qualitative comparisons on ScanRefer [5] (w/o TTS vs w/ TTS). For each example, we show the reconstructed 3D scene (top) and zoomed-in view (bottom), where the target object is indicated by the magenta box. Compared with standard decoding (w/o TTS), Test-Time Search (w/ TTS) yields more specific and better-grounded captions, capturing object identities and layout cues supported by the highlighted region rather than generic room-level descriptions. Green text marks the object-specific details introduced by w/ TTS. 5 Test-Time Efficiency 3D CoCa v2 adds an inference-time Test-Time Search that generates caption candidates and selects the best one using an external LLM judge. Compared to standard decoding (TTS off; 3D CoCa), TTS incurs additional cost due to repeated decoding and judge scoring. Table 8 reports wall-clock latency per scene (batch size 1) with the default setting =8. TTS increases the total latency from 0.55s to 1.78s (3.24), while keeping the one-time backbone encoding cost unchanged (0.18s) and concentrating the overhead in the decode+judge stage (1.60s). Despite this overhead, the latency remains competitive with detector-heavy pipelines (e.g., 2.35 for Scan2Cap), and the added cost is justified by consistent gains in caption quality and faithfulness, particularly under OOD evaluation. In practice, the quality-cost trade-off can be adjusted by and the choice of judge model. 6 Limitation and Future Work 3D CoCa v2 still has several limitations. TTS increases inference-time latency and may incur additional costs due to best-of-N decoding and external judge queries, Table 8: Test-time efficiency. Wall-clock latency per scene (s) with batch size 1. Std. denotes TTS disabled; TTS uses best-of-N with =8 and the same summary and prompt as the main method. Method Setting Total Breakdown (s) Encode Extra (dec+judge) Overhead 3D CoCa [17] 3D CoCa v2 (Ours) Std. (TTS off) TTS (N =8) Scan2Cap [7] Detector+Caption Vote2Cap-DETR++ [11] Detector+Caption 3D-VLP [51] Retrieval-style 0.55 1.78 2.35 2.80 2.05 0.18 0.18 1.70 2.10 1.55 0.37 1.60 0.65 0.70 0. 1.00 3.24 4.27 5.09 3.73 which can be undesirable for strict real-time deployment; although and the judge choice provide controllable quality-efficiency trade-off, the overhead is not eliminated. Moreover, judge-guided selection depends on judge reliability and the fixed scoring prompt, and it can fail when the compact scene summary is incomplete or when the judge over-emphasizes fluency over grounding. Finally, our lightweight summary may miss fine-grained spatial relations or rare attributes, limiting its ability to penalize subtle hallucinations. Future work includes building more structured evidence representations, reducing TTS costs via adaptive and early stopping or learned lightweight judges, and extending the framework to broader 3D settings such as outdoor LiDAR, dynamic scenes, and embodied scenar12 Hao Tang, Ting Huang, and Zeyu Zhang ios where captioning is coupled with actions and longhorizon memory. tion network for multi-source visual recognition. Knowledge-Based Systems, 254:109632, 2022."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented 3D CoCa v2, unified contrastivegenerative framework for 3D captioning that extends the 3D CoCa backbone with an inference-only TTS module. TTS generates multiple diverse caption candidates and selects the best one via an external LLM judge conditioned on compact scene summary, improving caption specificity and faithfulness without any additional training or parameter updates. Extensive experiments on both in-domain indoor benchmarks and out-of-distribution outdoor scenes demonstrate that 3D CoCa v2 consistently enhances caption quality and robustness under distribution shift. We hope this work highlights inference-time search as practical, plugand-play direction for building more reliable 3D captioners and motivates future research on stronger scene summarization, efficient judging, and generalizable 3D vision-language modeling for downstream embodied applications. Acknowledgements. This work was supported by the Fundamental Research Funds for the Central Universities, Peking University. References 1. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. 16th European Conference on Computer Vision (ECCV), 2020. 2. Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. 3. Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: unified framework for joint dense captioning and visual grounding on 3d point clouds. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1644316452, 2022. 4. Ziyun Cai, Tengfei Zhang, Fumin Ma, and XiaoYuan Jing. Dual contrastive universal adapta5. Dave Zhenyu Chen et al. Scanrefer: 3d object localization in rgb-d scans using natural language. 16th European Conference on Computer Vision (ECCV), 2020. 6. Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and Angel Chang. D3net: speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans. arXiv preprint arXiv:2112.01551, 2021. 7. DaveZhenyu Chen et al. Scan2cap: Contextaware dense captioning in rgb-d scans. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2021. 8. DaveZhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Nießner, and AngelX. Chang. Unit3d: unified transformer for 3d dense captioning and visual grounding. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), page 1806318073, Oct 2023. 9. Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z. Chen, and Jian Wu. hierarchical graph network for 3d object detection on point clouds. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 389398, 2020. 10. Sijin Chen et al. End-to-end 3d dense captioning with vote2cap-detr. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 1112411133, Jun 2023. 11. Sijin Chen et al. Vote2cap-detr++: Decoupling localization and describing for end-to-end 3d dense captioning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(11):73317347, Nov 2024. 12. Xin Chen et al. Sportscap: Monocular 3d human motion capture and fine-grained understanding in challenging sports videos. International Journal of Computer Vision, page 28462864, Oct 2021. 13. Xin Chen et al. Tightcap: 3d human shape capture with clothing tightness field. ACM Transactions on Graphics (Presented at ACM SIGGRAPH), 2021. 14. Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 15. Ting Huang, Dongjian Li, Rui Yang, Zeyu Zhang, Zida Yang, and Hao Tang. Mobilevla-r1: Reinforcing vision-language-action for mobile robots. arXiv preprint arXiv:2511.17889, 2025. 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence 13 16. Ting Huang, Zeyu Zhang, and Hao Tang. 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding. arXiv preprint arXiv:2507.23478, 2025. 17. Ting Huang, Zeyu Zhang, Yemin Wang, and Hao Tang. 3d coca: Contrastive learners are 3d captioners. arXiv preprint arXiv:2504.09518, 2025. 18. Ting Huang, Zeyu Zhang, Ruicheng Zhang, and Dc-scene: Data-centric learning arXiv preprint Yang Zhao. for 3d scene understanding. arXiv:2505.15232, 2025. 19. Yuki Ichihara, Yuu Jinnai, Tetsuro Morimura, Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, and Eiji Uchibe. Evaluation of best-of-n sampling strategies for language model alignment. Transactions on Machine Learning Research, 2025. 20. Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. More: Multiorder relation mining for dense captioning in 3d scenes. In In Proceedings of the European conference on computer vision, page 528545, Jan 2022. 21. Bu Jin et al. Tod3cap: Towards 3d dense captioning in outdoor scenes. In In Proceedings of the European conference on computer vision, page 367384, Jan 2025. 22. Zhao Jin et al. Context-aware alignment and mutual masking for 3d-language pre-training. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 1098410994, Jun 2023. 23. Minjung Kim et al. See it all: Contextualized late aggregation for 3d dense captioning. In Findings of the Association for Computational Linguistics ACL 2024, page 33953405, Jan 2024. 24. Minjung Kim et al. Bi-directional contextual attention for 3d dense captioning. In In Proceedings of the European conference on computer vision, page 385401, Jan 2025. 25. Chin-Yew Lin. ROUGE: package for automatic In Text Summarization evaluation of summaries. Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. 26. Qingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang. Nav-r1: Reasoning and navigation in embodied scenes. arXiv preprint arXiv:2509.10884, 2025. 27. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. Ptuning: Prompt tuning can be comparable to finetuning across scales and tasks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 6168, Dublin, Ireland, May 2022. Association for Computational Linguistics. 28. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 25112522, Singapore, December 2023. Association for Computational Linguistics. 29. Zeting Liu, Zida Yang, Zeyu Zhang, and Hao Tang. Evovla: Self-evolving vision-language-action model. arXiv preprint arXiv:2511.16166, 2025. 30. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 31. Guofeng Mei, Xiaoshui Huang, Juan Liu, Jian Zhang, and Qiang Wu. Unsupervised point cloud pre-training via contrasting and clustering. In 2022 IEEE International Conference on Image Processing (ICIP), Oct 2022. 32. A. Neubeck et al. Efficient non-maximum suppression. In 18th International Conference on Pattern Recognition (ICPR06), volume 3, pages 850855, 2006. 33. Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part II, pages 604621. Springer, 2022. 34. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL 02, page 311318, USA, 2002. Association for Computational Linguistics. 35. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. 36. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 37. Zirui Song, Guangxian Ouyang, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Fu Yujie, Zeyu Zhang, Shiyu Jiang, Miao Fang, et al. Hazards in daily life? enabling robots to proactively detect and resolve anomalies. In Proceedings of the 2025 Con14 Hao Tang, Ting Huang, and Zeyu Zhang Unsupervised pre-training for 3d point cloud understanding. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision ECCV 2020, pages 574591, Cham, 2020. Springer International Publishing. 47. Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, and Zheng Zhu. Vla-r1: Enhancing reasoning in vision-language-action models. arXiv preprint arXiv:2510.01623, 2025. 48. Jiahui Yu et al. Coca: Contrastive captioners are image-text foundation models, 2022. 49. Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pretraining 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 50. Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin Li, Shuguang Cui, and Zhen Li. - trans2cap: Cross-modal knowledge transfer using transformer for 3d dense captioning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 85538563, Jun 2022. 51. Taolin Zhang, Sunan He, Tao Dai, Zhi Wang, Bin Chen, and Shu-Tao Xia. Vision-language pre-training with object learning Proceedings of for 3d scene understanding. the AAAI Conference on Artificial Intelligence, 38(7):72967304, Mar 2024. contrastive 52. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 53. Jin Zhuang, Xiao-Yuan Jing, and Xiaodong Jia. Mining negative samples on contrastive learning via curricular weighting strategy. Information Sciences, 668:120534, 2024. ference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 73997415, 2025. 38. Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, et al. Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulation with large visionlanguage models. arXiv preprint arXiv:2505.16517, 2025. 39. Hao Tang, Xiaojuan Qi, Guolei Sun, Dan Xu, Nicu Sebe, Radu Timofte, and Luc Van Gool. Edge guided gans with contrastive learning for semantic image synthesis. In ICLR, 2023. 40. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 45664575, 2015. 41. Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matt J. Kusner. Unsupervised point cloud pre-training via occlusion completion. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), Oct 2021. 42. Heng Wang, Chaoyi Zhang, Jianhui Yu, and Weidong Cai. Spatiality-guided transformer for 3d In Proceedings dense captioning on point clouds. of the Thirty-First International Joint Conference on Artificial Intelligence, page 13931400, Jul 2022. 43. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. 44. Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Jiwen Lu. Take-a-photo: 3d-to-2d generative pre-training of point cloud models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 56175627, 2023. 45. Yu Xia, Jingru Fan, Weize Chen, Siyu Yan, Xin Cong, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. AgentRM: Enhancing agent generalization with reward modeling. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1927719290, Vienna, Austria, July 2025. Association for Computational Linguistics. 46. Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas Guibas, and Or Litany. Pointcontrast:"
        }
    ],
    "affiliations": [
        "School of Computer Science, Peking University"
    ]
}