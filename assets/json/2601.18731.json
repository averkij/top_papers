{
    "paper_title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
    "authors": [
        "Hongru Cai",
        "Yongqi Li",
        "Tiezheng Yu",
        "Fengbin Zhu",
        "Wenjie Wang",
        "Fuli Feng",
        "Wenjie Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines."
        },
        {
            "title": "Start",
            "content": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment 2026-1-27 Hongru Cai1 Yongqi Li1 Tiezheng Yu2 Wenjie Wang4 Fuli Feng4 Wenjie Li1 Fengbin Zhu3 6 2 0 J 6 2 ] . [ 1 1 3 7 8 1 . 1 0 6 2 : r a"
        },
        {
            "title": "4 University of Science and Technology of China",
            "content": "{henry.hongrucai, liyongqi0, wenjiewang96}@gmail.com Abstract Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as metalearning problem. Specifically, we represent each users reward model as weighted combination of base reward functions, and optimize the initialization of these weights using Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines. Project Page: https://github.com/ModalityDance/MRM"
        },
        {
            "title": "1 Introduction",
            "content": "The goal of Large Language Model (LLM) alignment is to ensure that models behave consistently with human preferences [1]. Previous approaches typically assume uniform standard and optimize for generic preferences that are broadly acceptable [2]. However, human values are highly diverse. Preferences vary significantly across individuals, which makes single standard insufficient [3]. Therefore, recent work emphasizes personalized alignment to address the plurality of user intents and contexts [4]. By tailoring model behavior to individual needs, personalized alignment enables LLMs to respect unique user preferences rather than following single monolithic objective [5, 3]. The alignment of LLMs relies on human feedback to provide learning signals that guide model behavCorresponding authors. iors. Standard approaches, such as Reinforcement Learning from Human Feedback (RLHF) [6], optimize LLMs by comparing responses ranked by human preference [6, 7, 8, 9]. However, keeping humanin-the-loop to provide continuous feedback is impractical [2, 10]. This challenge is even greater in personalized alignment, as we cannot expect every single user to actively provide feedback to align the model. This drives the research on personalized reward models, which learn individual preferences and provide personalized feedback automatically [11, 12, 13]. While personalized reward models are essential for achieving personalized alignment, their development still faces unique challenges. 1) Scarcity of personalized feedback. General reward models typically rely on massive preference data to capture preference patterns [14], but gathering such extensive feedback from single user is impractical. Simply aggregating data from other users is ineffective, as their conflictOne Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment entirely on the quality of the input context. This fails under sparse feedback, as the limited context cannot distinguish unique user intents, forcing the model into coarse-grained approximation [17] instead of fine-grained personalization. 2) Personalized parameter methods (see Figure 1 (b)) allocate user-specific parameters, such as LoRA adapters or separate models [12, 18, 19]. Although this allows for fine-grained alignment, it struggles to adapt to unseen users. Training user-specific parameters from scratch is infeasible in the few-shot scenario, as sparse feedback inevitably leads to overfitting and poor generalization. Consequently, neither paradigm can simultaneously efficiently adapt to unseen users while maintaining fine-grained personalization under limited user feedback. This dilemma implies that current optimization paradigms are inadequate and calls for fundamenIn this work, we reframe tal shift in perspective. personalized reward modeling through the lens of meta-learning, treating each individuals preference modeling as distinct learning task. Instead of relying on fitting data to learn each individual preference, we expect that the model could learn the process of preference adaptation (i.e., learning to learn). Specifically, we implement this idea by learning highly adaptable model initialization from users (see Figure 1 (c)). We employ bi-level optimization framework: the inner loop mimics the adaptation to individual users using sparse feedback, while the outer loop updates the initialization to achieve faster adaptation across users. By learning from this adaptation process, the model captures intrinsic preference commonalities across diverse users and thus serves as starting point that can rapidly converge to any users intent. Building on this insight, we propose Meta Reward Modeling (MRM). To enable lightweight per-user adaptation, we model each user reward as lowdimensional weight combination over shared basis reward functions [20, 15] and employ ModelAgnostic Meta-Learning (MAML) [21]-style framework to optimize the initialization of the weights. However, given the diversity of human values, some users are naturally harder to model than others. Standard meta-learning treats all users equally, aiming to maximize the average performance. ConseFigure 1: Comparison of personalized reward modeling methods: (a) Personalized input incorporates user contexts; (b) Personalized parameter assigns user-specific parameters; (c) Meta Reward Modeling formulates personalization as meta-learning problem by learning an adaptable initialization. ing preferences would confuse the model. Thus, the personalized feedback available for each user is inherently sparse, making it challenging to model their unique intent. 2) Adaptation to unseen users. It is impractical to collect data from every potential user in advance. As result, personalized reward models must handle unseen users1 whose feedback was not pre-collected, and quickly adapt to their unique preferences using only few demonstrations [15]. Both the scarcity of individual feedback and the necessity of adapting to unseen users pose significant challenges to personalized reward modeling. Existing approaches to personalized reward modeling typically rely on either personalized inputs or personalized parameters, but both face limitations. 1) Personalized input methods (see Figure 1 (a)) achieve personalization by explicitly incorporating user context into one shared model, such as persona descriptions [16, 13], prior preferences [17], or learned embeddings [11]. However, since the model parameters are fixed, personalization relies 1In this work, unseen users refer to users who are not included in the training phase but provide few preference examples during the test phase for personalization. 2 One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment quently, this uniform approach often neglects users with unique or complex needs, as the model prioritizes the majority to reduce the overall error. To address this, we introduce the Robust Personalization Objective (RPO). RPO dynamically gives more weight to these hard-to-learn users during training, identified by their meta-training losses. This prevents the model from ignoring distinct preferences, ensuring consistent and robust performance for all users. Together, our framework enables learning from sparse feedback and efficient adaptation to unseen users, while maintaining robustness across diverse preferences. The key contributions of this work are as follows: We introduce new formulation of personalized reward modeling from meta-learning perspective, where each user is treated as distinct task. This formulation explicitly targets rapid adaptation from sparse feedback and generalization to unseen users, addressing fundamental limitations of existing paradigms. We propose Meta Reward Modeling, which learns shared and highly adaptable initialization over set of base reward functions for efficient few-shot personalization. In addition, we design Robust Personalization Objective that emphasizes hard-tolearn users during meta-optimization, improving robustness under diverse user preferences. We demonstrate through extensive experiments that MRM consistently outperforms baselines in adaptation and robustness, thereby advancing the frontier of personalized LLM alignment."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we revisit prior studies on reward models, personalized LLMs, and meta-learning for personalization. Reward Models. Reward models are key component in aligning LLMs, as they translate human preferences into feedback signals that guide optimization [22]. Their design can be characterized along two dimensions. 1) At the type level, there are three categories. Discriminative models [23, 24, 25, 26] output scalar score through prediction head. Generative models [27, 28, 29, 30] produce natural language judgments before mapping them into score. Implicit models [31, 32, 33, 34] define rewards through generation probabilities to directly optimize on preference pairs. 2) At the granularity level, outcome-based models [35, 25, 36] assign score to the entire response, while process-based models [37, 38, 39, 40, 41] provide supervision at each intermediate step. However, these approaches generally assume single standard of human preference, overlooking the personalized preferences of users. Personalized reward models aim to capture individual differences in user preferences rather than assuming single universal standard [4, 3]. Existing methods can be broadly divided into two categories. 1) Personalized input approaches condition the model on user-specific signals derived from user history, such as persona descriptions [16, 13], prior preferences [17], or learned embeddings [11]. While these approaches avoid training separate models, they rely entirely on the quality of the input context. Under sparse feedback, they struggle to capture unique user intents, often resulting in coarse-grained approximations [17] rather than fine-grained personalization. 2) Personalized parameter approaches assign user-specific modules or parameters to capture individual preferences [12, 18, 19, 15, 20]. Although allowing for more flexible alignment, training these user-specific parameters from scratch is infeasible in few-shot scenarios. This paradigm is prone to overfitting when feedback is scarce and fails to generalize efficiently to unseen users. In contrast to these static fitting paradigms, our approach reformulates personalized reward modeling through the lens of meta-learning. By learning highly adaptable initialization and shared preference patterns, our method enables rapid and robust adaptation to unseen users using only minimal feedback. Personalized LLMs. Personalized LLMs are designed to meet the individualized needs of distinct users [42]. Research can be categorized into two main areas. 1) Personalized content generation focuses on generating personalized content. They have used openly available user data on Reddit [43], Twitter [44], and other blogging websites [45] to pretrain LLMs. Key tasks include stance classification, demographic inference [46], and personalized senti3 One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment ment prediction [47]. 2) Applications in real-world scenarios starting with personalized dialogue systems [48]. Studies have built datasets based on specific personas [49], and by extracting user attributes from Reddit [50] and Weibo [51]. Applications also include fields like healthcare [52], education [53], and robotics [54]. While these studies mainly operate at the policy or generation level, our work instead focuses on personalized reward modeling, providing an alternative path that enables few-shot personalization. Meta-Learning for Personalization. Metalearning focuses on training models that can rapidly adapt to new tasks with only few samples. There are three categories: model-based methods [55] that design architectures with fast adaptation capabilities, metric-based methods [56] that learn similarity measures for generalization, and optimization-based methods [21] that learn effective initialization. Our work is based on the third paradigm of MAML [21], which learn good initializations for rapid adaptation. Meta-learning has been applied in many domains including image classification [55, 21, 57], language modeling [58, 59], and reinforcement learning [60, 61]. In personalization, users are often treated as tasks within the meta-learning framework [62, 63]. Works in recommendation [64, 63, 65, 66, 67] showed that meta-learning can help to adapt to new users and thus alleviate cold-start problems. Similar ideas have been applied to LLMs [68, 69]to help adapt models to diverse preferences. FSPO [70] combines metalearning with DPO [10] to learn personalized preferences at the policy level, directly optimizing the models generation behavior. In contrast, our work focuses on personalized reward modeling, learning user-specific reward functions that capture diverse user preferences."
        },
        {
            "title": "3 Preliminaries",
            "content": "Before presenting our proposed method, we first review two key components that help to understand our method: Model-Agnostic Meta-Learning [21], and the training of reward models."
        },
        {
            "title": "3.1 Model-Agnostic Meta-Learning",
            "content": "The core idea of MAML is to learn set of initial parameters that can be efficiently adapted to new tasks using only few examples. As shown in Algorithm 1, let the model be fθ with parameters θ. Each task Ti is drawn from task distribution p(T ) and consists used for task-specific of two parts: support set Ds adaptation, and query set Dq used for evaluation and optimization. Inner loop task adaptation. In this phase, the model adapts to task Ti by updating θ with few gradient steps on the support set: ( fθ, Ds ), (1) θi = θ αθLTi where α is the inner learning rate, and LTi is the taskspecific loss (e.g., cross-entropy [71] or MSE [72]). represents the personalized paramAfter this step, θi eters for task Ti , adapted from shared initialization. Outer loop meta optimization. In this phase, the adapted parameters θi are evaluated on the query set, and the shared initialization θ is updated to improve generalization across tasks: θ θ βθ"
        },
        {
            "title": "LTi",
            "content": "( fθi , Dq ), (2) where β is the meta learning rate. This update requires differentiating through the inner loop with respect to the initialization parameters. Through this two-level optimization, MAML learns an initialization θ that can be rapidly personalized to new tasks with only few examples. This ability to support fast adaptation and few-shot learning makes MAML especially well-suited for personalized reward modeling, where limited feedback is available for each user."
        },
        {
            "title": "3.2 Reward Model Training",
            "content": "A central step in RLHF [6] is learning reward function that encodes human preferences. The reward model is trained from pairwise comparisons: for the same prompt, annotators indicate which response they prefer. Formally, the reward model rϕ(x, y) assigns scalar score to response given prompt x. For preference pair (x, y+, y), where y+ is preferred over y, the BradleyTerry model [73] defines: 4 One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment Algorithm 1 Model-Agnostic Meta-Learning [21]"
        },
        {
            "title": "4.1 Meta Weight Initialization",
            "content": "do Sample batch of tasks Ti p(T ) for all Ti Require: p(T ): distribution over tasks Require: α, β: step sizes for inner and outer updates Require: n: number of inner loop gradient updates 1: Randomly initialize θ 2: while not done do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end while Inner loop task adaptation: θi θ for all steps do Evaluate θi LTi θi θi αθi end for Outer loop meta optimization: Update θ θ βθ ( fθ, Ds LTi ) LTi ) ( fθ, Dq ( fθ, Ds end for"
        },
        {
            "title": "LTi",
            "content": "Ti ) i : Task-sepcific loss P(y+ x) = σ(rϕ(x, y+) rϕ(x, y)), (3) with σ(z) = 1 1+exp(z) . The training loss function is: LRM(ϕ) = log σ(rϕ(x, y+) rϕ(x, y)). (x,y+,y)D (4) This objective assigns higher scores to preferred responses and lower scores to rejected ones. In personalized scenarios, each user provides only few comparisons. Training separate model per user causes overfitting, while single shared model fails to capture individual preferences. We thus frame personalized reward learning as meta-learning problem that learns shared initialization while still enabling user-specific adaptation."
        },
        {
            "title": "4 Method",
            "content": "We introduce our proposed Meta Reward Modeling, which consists of two key components: Meta Weight Initialization. We formulate personalized reward modeling as meta-learning problem by learning shared initialization across users. This initialization enables fast personalization from scarce feedback and generalization to unseen users. Robust Personalization Objective. To improve robustness across user diversity, we introduce the Robust Personalization Objective, which reweights user-level query losses to emphasize hard-to-learn users in outer loop meta optimization. 5 As shown in Figure 2, the key idea is to represent the personalized reward model as weighted combination of base reward functions, and to meta-learn shared initialization of these weights, while keeping the base functions shared across users. Model structure. To capture diverse user preferences, following prior work [15], we represent the as weighted combination of mulreward model rwi tiple base reward functions. Let ϕk denote the set of base reward functions. For given user i, the personalized reward is defined as: k=1 rwi (x, y) = wi,k ϕk(x, y), (5) k=1 where wi = (wi,1, . . . , wi,K) are the user-specific determine the contribution weights. The weights wi of each base function to the overall personalized reward for user i. Task definition. Learning the user-specific weights is the core of personalized reward modeling. We treat each users weight learning as an independent metalearning task. Let Ti denote the task for user i, with containing pairwise preferences in the dataset Di form of triples (x, y+, y), where is prompt, y+ the preferred response, and the less preferred one. Following the meta-learning paradigm, we split Di provides into two disjoint subsets. The support set Ds limited feedback for inner-loop adaptation, while the query set Dq serves as held-out data to evaluate the adapted model and update the shared initialization in the outer loop. This separation prevents the model from overfitting to the limited feedback within each user and enables it to learn shared initialization that supports few-shot adaptation across users. Task loss. The training loss for each task is defined by the reward modeling loss introduced in 3.2. For , the reward funca preference pair (x, y+, y) Di tion rwi (x, y) assigns score to response given the prompt, and the BradleyTerry [73] model defines the preference probability as: P(y+ x, rwi ) = σ(rwi (x, y+) rwi (x, y)), (6) One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment Algorithm 2 Meta Reward Modeling 9: 10: 11: 12: 13: 14: and base functions {ϕk}K k=1 Sample batch of user tasks Ti p(T ) for all Ti Require: p(T ): distribution over users (tasks) Require: α, β: step sizes for inner and outer updates Require: n: number of inner loop gradient updates 1: Randomly initialize weights w0 2: while not done do 3: 4: 5: 6: 7: 8: Inner loop few-shot personalization: wi w0 for all steps do Evaluate wi LTi Loss [73] wi wi αwi LTi (wi, Ds ) (wi, Ds ) do end for L: Bradley-Terry end for Outer loop meta optimization: w0 w0 βw0 A({LTi (wi, Dq ϕk ϕk βϕk A({LTi (wi, Dq ) )}i ), )}i A(): Robust Personalization Objective = 1, . . . , 15: end while update both the shared initialization of the weights and the shared base reward functions. The initialization w0 is updated to provide better starting point for adaptation: w0 = w0 βw0 A({LTi (wi, Dq )}i ), (9) where β is the meta learning rate and A() denotes the robust personalization objective that reweights user-level query losses (see 4.2). Similarly, each base reward function is updated: ), k=1 )}i ϕk = ϕk βϕk A({LTi (wi, Dq = 1, . . . , K. (10) Thus, the reweighted query losses jointly update w0 , producing initializations that are both and ϕk adaptable and robust. MRM inference. At inference time, the learned initialization w0 enables efficient few-shot personalization. Given user with training data Du, the user-specific weights are adapted from w0 by performing one or few gradient steps on Du: wu = w0 αw0LTu (w0, Du). (11) This produces the personalized weights wu that define the user-specific reward function: rwu (x, y) = k=1 wu,k, ϕk(x, y). (12) 6 Figure 2: Overview of Meta Reward Modeling. The model employs base reward functions with shared weight initialization, adapts user-specific weights in the inner loop, and updates both initialization and base functions in the outer loop with the robust personalization objective. where σ(z) is the sigmoid function. The loss for user is then: LTi (wi) = log σ(rwi (x, y+) rwi (x, y)). (x,y+,y)Di (7) When restricted to the support set, this loss guides the adaptation of the weights for user i. When evaluated on the query set, it provides the signal to improve the shared initialization across users. Inner loop few-shot personalization. As shown in Algorithm 2, the inner loop starts from the shared initialization w0 and adapts only the user-specific weights wi . using its support set Ds This step personalizes the reward model for user with the limited feedback available. The procedure is written as: for each task Ti (8) wi = w0, wi = wi αwi LTi (wi, Ds ), where α is the inner learning rate, and LTi is the task loss defined earlier. In practice, this update can be repeated for few steps. After adaptation, wi represents the personalized weights that define the personalized reward function rwi Outer loop meta optimization. After adapting in the inner loop, we evaluuser-specific weights wi ate the personalized weights on the query sets Dq . The gradients from these evaluations are then used to for this user. One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment Once adapted, rwu can be directly used to evaluate or rank responses according to the users preferences. Benefiting from the meta-learned initialization, MRM achieves both few-shot adaptation within users and strong generalization across users."
        },
        {
            "title": "4.2 Robust Personalization Objective",
            "content": "While meta weight initialization enables fast personalization from limited data, it overlooks robustness across users. Some users are easy to adapt, while hard-to-learn users with unique or inconsistent feedback perform much worse when treated equally. To address this, we design the robust personalization objective that adaptively shifts attention toward hardto-learn users, improving overall robustness across the user distribution. User-level filtering. We identify hard-to-learn users as those with larger query losses. high query loss indicates that even after adaptation, the model still performs poorly on that user, reflecting greater difficulty in capturing the preference. To emphasize these users, we introduce filtering strategy. Given user query losses {LTi }i , we set threshold τ as the (1 ρ) quantile, where ρ (0, 1] specifies the fraction of hardest users to retain. The objective is Aρ({LTi }i) = LTi , LTi = { LTi , LTi > τ, LTi τ. 0, i=1 (13) Here, only the hardest ρn users contribute to the outer loop meta update, ensuring that optimization is guided by cases where the model struggles most, rather than by easy users it already fits well. Soft reweighting with smoothing. However, only keeping the hardest users can destabilize training and overemphasize the most difficult users at the cost of overall performance. To mitigate this, we introduce soft reweighting scheme that smoothly decreases the weight of users below the threshold, while keeping higher weight for those above it. Formally, each task loss is reweighted as LTi = σ ) ( LTi τ γ LTi , (14) where σ() is the sigmoid function and smoothing parameter γ > 0 controls the sharpness of the tran7 sition. The resulting objective is: Aρ,γ({LTi }i) = ( LTi τ γ σ ) LTi . (15) i=1 This soft reweighting preserves emphasis on hard-tolearn users while preventing easy ones from being completely discarded, yielding smoother gradients and more stable training."
        },
        {
            "title": "5 Experiments",
            "content": "We conduct experiments to answer five research questions: RQ1: How does our proposed MRM compare in performance to existing personalized reward modeling methods? RQ2: How does MRM perform in terms of robustness across users compared to previous methods? RQ3: How do the components of MRM affect the performance? RQ4: How does MRM adapt to unseen users under different numbers of few-shot examples compared to existing methods? RQ5: How does MRM compare to existing methods in terms of efficiency and scalability with respect to the number of users?"
        },
        {
            "title": "5.1.1 Datasets\nFollowing prior work [15], we evaluate on two\ndatasets with user-level preference annotations:\nPRISM [74] and Reddit TLDR [75]. PRISM contains\nfeedback from 1,500 participants across 8,011 con-\nversations with multiple LLMs. After filtering users\nwith fewer than 6 dialogues, we obtain 1,287 users,\neach with an average of 6 dialogues. Reddit TLDR\nconsists of posts, two candidate summaries, and a\npreference label, with annotator IDs serving as user\nidentifiers. Removing users with fewer than 50 an-\nnotations leaves 40 users, each contributing about\n3,750 labeled pairs.",
            "content": "To simulate realistic personalization, we split users into two equal groups: seen and unseen. Each users data is further equally divided into train and test sets. One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment The seen-train split is used for training, while the unseen-train split is reserved for few-shot adaptation. To reflect limited feedback per user in Reddit TLDR, we restrict the seen-train split to 100 or 150 pairs per user and the unseen-train split to 50 pairs per user, while keeping full test sets. We denote these two settings as Reddit TLDR (100 examples) and Reddit TLDR (150 examples). For PRISM, since each user has relatively few dialogues, we retain all available data for both train and test."
        },
        {
            "title": "5.1.2 Baselines",
            "content": "We compare MRM against representative methods from both personalized input and personalized parameter approaches. Here, we briefly introduce each baseline. Following prior work [15], most methods use the same embedding extracted from 1) Skyworka pretrained reward model [25]. Reward [25, 14] refers to series of open-source pretrained reward models (V1 and V2) based on Llama-3.1-8B-Instruct [76]. These models directly score promptresponse pairs. 2) BT trains single model with BradleyTerry loss [73], serve as nonpersonalized baseline. 3) GPO [17] learns group preferences by training transformer that predicts the current preference attending to embeddings of prior preference pairs. 4) VPL [11] encodes past user preferences into user-specific embedding, which conditions the reward model for personalization. 5) PAL [12] represents each user as weight distribution over finite set of preference prototypes, enabling the reward model to capture individual variation. 6) LoRe [15] applies low-rank decomposition of reward functions, representing individual preferences as weighted combinations of basis reward functions, with user-specific weights learned separately. 7) SynthesizeMe [13] builds user personas by reasoning over prior preferences and synthesizing prompts to guide LLM judgments. We evaluate both in-context learning (ICL) and fine-tuned (FT) variants based on the same backbone of Llama-3.1-8B-Instruct [76]. Since this method requires repeated generations and is slow on the full test set, we evaluate each user on randomly sampled quarter of their test instances for efficiency."
        },
        {
            "title": "5.1.3 Evaluation Settings\nWe evaluate methods using user-level accuracy on\ntest response pairs. For a user ui\ni =\n{(x, yc, yr)}, accuracy is:",
            "content": "with test set Dtest Acc(ui) = 1 Dtest 1(Rϕ(x, yc) > Rϕ(x, yr)), (x,yc,yr)Dtest (16) where Rϕ is the reward model and 1() equals 1 if the inequality holds and 0 otherwise. We report the average accuracy over all users U: Avg. Acc. = 1 U i= Acc(ui). (17) Each experiment is repeated 20 times with different seen and unseen splits, and we report the mean and standard deviation."
        },
        {
            "title": "5.1.4 Implementation Details\nConsistent with the baselines, MRM is implemented\nbased on embeddings from Skywork-Reward [25,\n14], with each user’s reward model represented as\na weighted combination of 2 base reward functions.\nExperiments are run on NVIDIA RTX A5000 GPU,\nand the number of training epochs is selected based\non held-out evaluation on each dataset. The inner\nloop performs one adaptation step per user, and meta-\noptimization uses a batch size of 2 users. Both loops\nare optimized with Adam [77]. For PRISM, the meta\nand inner learning rates are set to 1e-3; for Reddit\nTLDR, 5e-3. For each user, 10% of the training data\nis used as the support set and 90% as the query set.\nIn RPO, the proportion of hard-to-learn users ρ = 0.5,\nand the smoothing parameter γ = 0.5.\n5.2 Overall Performance (RQ1)\nWe compare MRM with baseline methods on person-\nalized preference learning. Results in Table 1 yield\nthe following observations:\n• Across all methods, the gap between seen and\nunseen users is small, and most personalized meth-\nods offer only limited improvements over BT. This\nsuggests that existing approaches struggle to extract\nstrong user-specific signals in the data-scarce setting.\nIn contrast, MRM consistently outperforms the BT\nbaseline. This suggests that MRM’s meta-learning",
            "content": "8 One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment Table 1: Performance comparison between our proposed MRM and baselines. The numbers report user-level accuracy (%) averaged over 20 random splits, with mean and standard deviation. Bold numbers indicate the best performance in each column, while the second-best are underlined. * implies the improvements over the best baselines are statistically significant (p-value < 0.05). % Improvement indicates the gain relative to the best-performing baseline. For SynthesizeMe, each user is evaluated on randomly sampled quarter of their test set due to the high cost of in-context inference and retries. Method Seen PRISM Unseen Reddit TLDR (100 examples) Reddit TLDR (150 examples) Overall Seen Unseen Overall Seen Unseen Overall Skywork-Reward V1 [25] 59.5 0. 59.5 0.4 59.5 0.5 62.6 0.8 62.6 0.8 62.6 0.8 62.6 0. 62.6 0.8 62.6 0.8 Skywork-Reward V2 [14] 60.5 0.4 60.1 0.5 60.3 0. 64.4 0.7 64.5 0.7 64.4 0.4 64.4 0.7 64.5 0.7 64.4 0. BT [73] GPO [17] VPL [11] PAL [12] LoRe [15] 64.3 0. 64.2 0.6 64.4 0.3 67.8 1.1 67.8 0.8 67.8 0.8 68.1 1. 68.2 0.8 68.1 0.8 64.2 1.1 64.2 1.0 64.2 1.0 68.0 1. 68.0 1.2 68.0 0.5 68.5 1.1 68.6 1.2 68.6 0.6 64.6 1. 64.0 1.2 64.3 1.1 67.6 1.1 67.7 1.0 67.6 1.0 67.7 1. 67.9 1.0 67.8 1.0 61.7 1.0 61.4 0.8 61.5 0.8 65.8 1. 65.9 1.1 64.5 0.7 66.3 1.2 66.7 0.9 66.5 0.7 63.0 0. 63.1 0.8 63.0 0.8 68.1 1.1 68.6 1.2 68.3 0.5 68.5 1. 68.8 1.1 68.6 1.0 SynthesizeMe (ICL) [13] 63.9 1.2 63.4 1.2 63.7 1. 66.6 2.2 66.3 2.2 66.5 2.0 66.7 2.2 66.3 2.3 66.5 2. SynthesizeMe (FT) [13] 64.3 1.0 64.5 1.1 64.4 1.0 68.2 1.3 68.0 1. 68.1 1.3 68.2 1.3 68.0 1.3 68.1 1.3 MRM (Skywork-Reward V1) 64.8 0. 64.9 0.4 64.9 0.2 68.7 1.1 69.0 0.8 68.8 0.4 69.0 1. 69.5 0.8 69.3 0.3 MRM (Skywork-Reward V2) 65.3 0.6* 65.2 0.5* 65.3 0.3* 69.6 0.9* 69.6 0.8* 69.6 0.3* 69.7 0.8* 69.8 0.9* 69.7 0.3* % Improvement 1.1% 1.1% 1.4% 2.1% 1.5% 1.9% 1.8% 1.5% 1.6% objective is uniquely capable of leveraging limited data for effective personalization On PRISM, personalized input methods (GPO, VPL, and SynthesizeMe) slightly outperform personalized parameter methods (PAL and LoRe). This pattern supports our earlier analysis regarding the limitations of personalized parameter approaches: learning userspecific parameters from scratch is brittle in few-shot scenarios, where sparse feedback inevitably leads to overfitting and poor generalization to unseen users. Meanwhile, even personalized input methods do not surpass the non-personalized BT baseline, consistent with prior findings [13] that personalization fails when per-user feedback is scarce. On Reddit TLDR, personalized input methods achieve clear gains over the BT model because the dataset provides richer per-user feedback. This suggests that personalized input methods benefit from richer feedback per user, which aligns with the limitations noted earlier. Among personalized parameter methods, LoRe also shows notable improvement and becomes more competitive, likely because the smaller number of users allows it to better balance shared structures with user-specific modules in training. Overall, MRM consistently achieves the strongest performance across datasets and settings, yielding relative improvements of around 1.5% over the bestperforming baselines. This advantage comes from 9 Figure 3: Performance of average accuracy on the worst 10%, 20%, and 50% of users for (a) PRISM and (b) Reddit TLDR with 100 examples. MRM consistently outperforms baselines on all proportions of worst users, showing stronger robustness. two key factors: 1) The meta weight initialization enables efficient few-shot personalization and strong generalization to unseen users. Unlike personalized input methods that rely on abundant per-user data or parameter-based methods that struggle with large user populations, MRM adapts effectively from only few examples (refer to 5.3.4 for empirical evidence). 2) By incorporating the RPO, MRM places greater emphasis on hard-to-learn users and thus achieves robust performance across diverse users (see 5.3.1 for detailed analysis). One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment"
        },
        {
            "title": "5.3.1 User Robustness (RQ2)\nMost prior methods optimize average performance\nacross users, whereas true personalization demands\nconsistent performance for every individual. To en-\nhance robustness across users, we introduce the RPO\nto explicitly emphasize hard-to-learn users during\nmeta optimization.",
            "content": "We evaluate robustness by measuring the average accuracy of the worst 10%, 20%, and 50% of users on PRISM and Reddit TLDR (100 examples). Personalized input methods are shown as squares, and Personalized parameter methods as triangles. The results in Figure 3 show that: 1) Personalized input methods and parameter-based methods both show sharp drops on the hardest users, with PRISM results even falling below random choice. SynthesizeMe (FT), relying on in-context learning, is more unstable and performs the worst. These results indicate that existing methods generally struggle to maintain performance for hard-to-learn users, as they are not explicitly designed to handle diverse or atypical preference patterns. 2) In contrast, MRM consistently outperforms all baselines across the hardest user subsets. This improvement arises from RPO, which dynamically reweights user losses to focus outer-loop updates on the most difficult users. By learning from these challenging cases, MRM avoids overfitting to easy users and achieves more robust performance across the entire user distribution. This explains why MRM adapts more reliably to diverse users and shows clear advantages in robustness, key factor behind its overall gains."
        },
        {
            "title": "5.3.2 Analysis of RPO (RQ2)\nWe analyze the impact of the robust personalization\nobjective by examining three parameters: (1) the\nthreshold ratio, specifying the fraction of the hard\nusers in each batch; (2) the meta batch size, con-\ntrolling the number of users per update; and (3) the\nsmoothing parameter, regulating the sharpness of\nreweighting. All experiments are based on PRISM.",
            "content": "Figure 4: Effect of threshold ratio on PRISM. (a) Overall accuracy with different threshold ratios (ρ = 0.1, 0.2, 0.5). (b) Accuracy on the worst k% of users (k = 10, 20, 50). Figure 5: Performance with respect to (a) meta batch size and (b) smoothing parameter γ. In (b), γ=0 denotes hard filtering. Effect of threshold ratio. We vary the threshold ratio across three representative values of 0.1, 0.2, and 0.5, with meta batch size of 8, and evaluate both overall accuracy and the accuracy of the worst 10%, 20%, and 50% of users. The results in Figure 4 reveal several key observations. 1) On overall performance, increasing the ratio from 0.1 to 0.5 yields monotonic improvement. This suggests that setting the threshold too small harms performance, likely because excessive emphasis on the hardest users shifts the optimization focus, weakening learning on other users that are also important. 2) On worst users, ratios of 0.2 and 0.5 perform comparably and outperform 0.1, while at 50% all settings converge. This shows that overemphasizing the hardest users does not actually improve them, but instead reduces stability and overall effectiveness. In summary, ratio of 0.5 provides the best trade-off between focusing on the hardest users and maintaining generalization, resulting in both robustness and overall stability. Effect of meta batch size. We vary the number of users sampled per outer loop update and evaluate both overall accuracy and accuracy on the worst 10% of users. As shown in Figure 5 (a), we have the following observations. 1) Overall accuracy decreases 10 One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment Figure 6: Performance of few-shot adaptation on unseen users. We vary the number of few-shot examples for each user (x-axis) and report accuracy on three settings. MRM consistently outperforms baselines and shows stronger gains with more examples. Table 2: Ablation study on key components of MRM."
        },
        {
            "title": "Unseen",
            "content": "Overall Worst 10% Worst 20% Worst 50% Overall Worst 10% Worst 20% Worst 50% (0): MRM 64.8 0.4 37.9 1.3 43.4 0.9 52.8 0.6 64.9 0.4 37.9 0.8 43.9 0.7 53.2 0.7 (1): MRM w/o meta-learning formulation 63.1 0.9 35.9 1.1 41.8 0.8 51.2 0.6 64.5 0.5 37.5 1.3 43.0 0.9 52.5 0.6 (2): MRM w/o RPO (3): MRM w/o basis combination 63.8 0.4 37.1 1.4 42.8 1.0 52.3 0. 63.1 0.8 36.1 0.9 41.9 0.8 51.4 0.7 64.5 0.6 37.2 0.9 43.5 0.7 52.7 0.7 63.8 0.6 37.2 0.9 42.8 0.7 52.5 0.6 steadily as batch size increases, indicating that optimizing over many users simultaneously becomes harder, and the reweighting in RPO further reduces training stability, leading to weaker performance. 2) For the worst 10%, accuracy improves as batch size grows from 2 to 8 but drops beyond that point. This suggests that moderate batch size helps RPO identify and emphasize harder users, since larger batches provide better approximation of globally hard users. However, overly large batches complicate optimization and reduce effectiveness. Overall, these results highlight that relatively small batch sizes offer better trade-off between stability and robustness. Effect of smoothing parameter. We compare hard filtering and soft reweighting under different smoothing parameters γ with meta batch size of 8 and evaluate both overall accuracy and accuracy on the worst 10% of users. We have the following observations from Figure 5 (b). 1) When γ is very small, performance is poor for both overall and worst users; as γ increases, accuracy improves and peaks around 0.5, then declines slightly at 1. This shows that too small γ destabilizes training by placing nearly all weight on the hardest users, while too large γ oversmooths the weighting and weakens the intended emphasis. 2) Hard filtering performs better than near-zero γ but still worse than γ=0.5. plausible explanation is that near-zero γ does not replicate hard filtering: instead of discarding easy users, it rescales their losses, altering gradient magnitudes and causing instability. By contrast, hard filtering drops users below the threshold without changing the scale of losses. Overall, smoothing of 0.5 achieves the best balance: it avoids the instability of sharp weighting and the over-smoothing caused by large γ, while also outperforming pure hard filtering."
        },
        {
            "title": "5.3.3 Ablation Study (RQ3)\nTo thoroughly investigate the contribution of each\ncomponent in MRM, we conduct an ablation study\nby removing each component individually. We\nevaluate the following variants: (0)“MRM”: our\nproposed method, (1) “MRM w/o meta-learning\nformulation”, which independently trains weights\nfor each user without meta-learning; (2) “MRM w/o\nRPO”, which removes the robust personalization ob-\njective in outer loop optimization; and (3) “MRM\nw/o basis combination”, which replaces the model\nwith a single MLP updated in both inner and outer\nloops. We evaluate all variants on both datasets and\nreport the accuracies of overall and worst users in Ta-\nble 2. We have the following observations. 1) Remov-\ning the meta-learning formulation causes the largest\ndrop, showing that meta-learned initialization is key\nfor few-shot personalization and adapting to unseen\nusers. 2) Removing RPO slightly lowers accuracy, in-\ndicating that handling hard-to-learn users improves\nrobustness, which in turn benefits not only the worst-\ncase users but also the overall performance. 3) Re-\nmoving basis combination also reduces performance,\nsuggesting that decomposing the reward model into",
            "content": "11 One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment Figure 7: Number of trainable parameters as the number of users increases for different methods. multiple basis reward functions does help to capture diverse preferences better."
        },
        {
            "title": "5.3.4 Few-shot Adaptation (RQ4)",
            "content": "This experiment evaluates how different methods adapt to unseen users when provided with different numbers of few-shot examples. For each unseen user, we vary the number of few-shot examples and report accuracy on their test data. Both SynthesizeMe variants are not included here because inserting all examples exceeds the input length limit. The results in Figure 6 lead to the following observations: 1) Accuracy improves steadily for all methods as the number of examples increases, showing that additional feedback consistently aids adaptation to unseen users. 2) Personalized input methods, such as GPO and VPL, achieve stronger accuracy across different numbers of examples. In contrast, personalized parameter methods perform worse in data-scarce settings, with LoRe showing noticeable gains only when more examples are available. This pattern suggests that personalized input methods can leverage additional examples more effectively from the input side, while personalized parameter approaches must learn new user-specific structures, which is harder with limited data. 3) MRM consistently outperforms baselines, especially with very limited feedback. This improvement arises because the meta-learned weight initialization extracts shared patterns across seen users, providing strong starting point that enables rapid adaptation to unseen users. As more feedback becomes available, MRM continues to refine user-specific parameters, leading to steady performance gains. This confirms MRMs strength in few-shot personalization and aligns with the overall findings."
        },
        {
            "title": "5.3.5 Efficiency and Scalability (RQ5)",
            "content": "To evaluate the efficiency and scalability of different methods, we analyze both parameter growth and computational cost with respect to user scale. We first examine how the number of trainable parameters changes as more users are introduced. For each method, we vary the number of users and report the total number of trainable parameters required. All methods are implemented following their original descriptions. For MRM, the parameter count includes both the shared base functions and the adapted weights stored for each user. The results in Figure 7 lead to the following observations: 1) Personalized parameter methods, including PAL and LoRe, show parameter growth that scales linearly with the number of users, reflecting the need to allocate user-specific modules for each individual. 2) Personalized input methods, such as GPO and VPL, maintain constant number of parameters regardless of user scale, since user preferences are encoded through extra input rather than separate structures. 3) MRM consistently requires the smallest number of trainable parameters across all user settings. Although its parameter count increases as the user grows, the growth remains limited due to lightweight shared base reward functions and userspecific weights, making MRM the most parameterefficient and scalable among all methods. In terms of computational efficiency, personalized reward modeling inevitably involves user-specific learning. Compared to personalized parameter methods that train and maintain separate user-specific modules, MRM concentrates computation on shared base reward functions and shared weight initialization. Although MRM adopts an MAML-style training procedure, the additional computation is limited, as meta-learning is applied only to the initialization of lightweight user-specific weights. At inference time, adaptation from the shared initialization is performed once per user, updating small set of weights that can be stored and reused."
        },
        {
            "title": "6 Ethical and Privacy Considerations",
            "content": "Personalized reward modeling offers finer-grained alignment with individual preferences, but it also introduces ethical and privacy challenges that must 12 One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment be carefully addressed. Privacy. To align with individual users, models must learn from specific feedback, which inevitably reflects personal values and habits. Even if we remove explicit user IDs, the model might still memorize or reveal sensitive private information through its responses. Therefore, strict data protection measures and clear user consent are essential for responsible deployment. Bias. Optimizing for individual preferences risks reinforcing user biases or narrowing exposure to alternative perspectives. reward model trained solely on users past feedback may overfit to prior judgments and amplify preference extremes. Designing personalization mechanisms that balance individual alignment with diversity and exploration is an important ethical consideration. Safety. While personalized alignment represents the most fine-grained level of the alignment hierarchy, it should remain consistent with broader societal norms and safety constraints. Personalized reward models must operate within shared ethical boundaries and should not optimize behaviors that conflict with collective values or established safety principles."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "In this work, we addressed the critical limitations of existing personalized reward modeling paradigms, which fail to simultaneously achieve efficient adaptation to unseen users while maintaining fine-grained personalization under limited user feedback. We argue that overcoming these challenges necessitates fundamental shift from fitting static user models to learning to learn adaptation. Guided by this perspective, we proposed Meta Reward Modeling. By optimizing highly adaptable initialization, MRM enables the model to rapidly adapt to unseen users using only few demonstrations. For each user, preference data is split into support set for inner loop personalization and query set for outer loop meta optimization of both the weight initialization and base functions. Furthermore, to ensure robustness under diverse user preferences, we introduced the Robust Personalization Objective, which dynamically emphasizes hard-to-learn users during meta-optimization. 13 Extensive experiments demonstrate that our framework effectively solves the few-shot personalization challenge, offering robust and scalable path for aligning LLMs with diverse human values. Building on this work, several promising directions remain open for future exploration. 1) Extending MRM from reward modeling to direct policy optimization (e.g., via DPO) would enable end-to-end personalized generation, effectively removing the reliance on intermediate reward proxies and streamlining the pipeline. 2) Current methods typically assume static preferences. Future work should address dynamic environments where user intents evolve over time, requiring continual learning mechanisms to handle temporal drift. 3) Instead of passively relying on sparse history, models could actively query users to resolve ambiguity, thereby significantly reducing the sample complexity required for effective adaptation. 4) Current methods rely heavily on explicit pairwise preferences, yet real-world signals are often implicit or noisy, investigating how to leverage implicit behavioral cues would be valuable. 5) Moving beyond lightweight adapters to full-model metalearning could unlock finer-grained personalization capacities, provided that the inherent challenges of computational scalability and training stability are adequately addressed."
        },
        {
            "title": "References",
            "content": "[1] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: survey, 2023. [2] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. survey of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. [3] Jian Guan, Junfei Wu, Jia-Nan Li, Chuanqi Cheng, and Wei Wu. survey on personalized Alignment The missing piece for large language models in realworld applications. In Findings of the Association for Computational Linguistics: ACL 2025, 2025. [4] Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. Position: roadmap to pluralistic alignment. In Proceedings of the 41st International Conference on Machine Learning, ICML24, 2024. One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment [5] Zhehao Zhang, Ryan A. Rossi, Branislav Kveton, Yijia Shao, Diyi Yang, Hamed Zamani, Franck Dernoncourt, Joe Barrow, Tong Yu, Sungchul Kim, et al. Personalization of large language models: survey. Transactions on Machine Learning Research, 2025. Survey Certification. [6] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, 2017. [7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. [8] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 2022. [9] Yongqi Li, Lu Yang, Jian Wang, Runyang You, Wenjie Li, and Liqiang Nie. Towards harmless multimodal assistants with blind preference optimization, 2025. [10] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your In language model is secretly reward model. Thirty-seventh Conference on Neural Information Processing Systems, 2023. [11] Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques. Personalizing reinforcement learning from human feedback with variational preference learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [12] Daiwei Chen, Yi Chen, Aniket Rege, Zhi Wang, and Ramya Korlakai Vinayak. PAL: Sample-efficient personalized reward modeling for pluralistic alignment. In The Thirteenth International Conference on Learning Representations, 2025. [13] Michael J. Ryan, Omar Shaikh, Aditri Bhagirath, Daniel Frees, William Held, and Diyi Yang. SynthesizeMe! inducing persona-guided prompts for personalized reward models in LLMs. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025. [14] Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, and Yahui Zhou. Skywork-reward-v2: Scaling preference data 14 curation via human-ai synergy, 2025. [15] Avinandan Bose, Zhihan Xiong, Yuejie Chi, Simon Shaolei Du, Lin Xiao, and Maryam Fazel. Lore: Personalizing llms via low-rank reward modeling, 2025. [16] Jiarui Zhang. Guided profile generation improves personalization with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, 2024. [17] Siyan Zhao, John Dang, and Aditya Grover. Group preference optimization: Few-shot alignment of large language models. In The Twelfth International Conference on Learning Representations, 2024. [18] Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. [19] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging, 2023. [20] Idan Shenfeld, Felix Faltings, Pulkit Agrawal, and Aldo Pacchiano. Language model personalization via reward factorization. In Second Conference on Language Modeling, 2025. [21] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, 2017. [22] Jialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, Yicheng Chen, Yang Zhang, Wei Zhou, Jinjie Gu, and Lei Zou. comprehensive survey of reward models: Taxonomy, applications, challenges, and future, 2025. [23] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, et al. Internlm2 technical report, 2024. [24] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Boji Shan, Zeyuan Liu, Jia Deng, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing LLM reasoning generalists with preference trees. In The Thirteenth International Conference on Learning Representations, 2025. [25] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms, 2024. [26] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. In Findings of the Association for Computational Linguistics: EMNLP 2024, 2024. [27] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-ajudge with mt-bench and chatbot arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, 2023. [28] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations, 2024. [29] Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. Compassjudger-1: All-in-one judge model helps model evaluation and evolution, 2024. [30] Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun LIU. LearnIn ing LLM-as-a-judge for preference alignment. The Thirteenth International Conference on Learning Representations, 2025. [31] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback, 2023. [32] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From $r$ to $q^*$: Your language model is secretly q-function. In First Conference on Language Modeling, 2024. [33] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, 2024. [34] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In Proceedings of the 41st International Conference on Machine Learning, ICML24, 2024. [35] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, and Jiantao Jiao. Starling-7b: Improving helpfulness and harmlessness with RLAIF. In First Conference on Language Modeling, 2024. [36] Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, 15 and Tong Zhang. Regularizing hidden states enables learning generalizable reward model for LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [37] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. [38] Jonathan Uesato, Nate Kushman, Ramana Kumar, H. Francis Song, Noah Yamamoto Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-based and outcome-based feedback, 2023. [39] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. [40] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-bystep without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. [41] Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. Multi-step problem solving through verifier: An empirical analysis on model-induced process supervision. In Findings of the Association for Computational Linguistics: EMNLP 2024, 2024. [42] Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-Lin Chen, Chao-Wei Huang, Yu Meng, and YunNung Chen. Two tales of persona in LLMs: survey of role-playing and personalization. In Findings of the Association for Computational Linguistics: EMNLP 2024, 2024. [43] Charles Welch, Chenxi Gu, Jonathan K. Kummerfeld, Veronica Perez-Rosas, and Rada Mihalcea. Leveraging similar users for personalized language modeling with limited data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022. [44] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), 2017. [45] Milton King and Paul Cook. Evaluating approaches to personalizing language models. In Proceedings of the Twelfth Language Resources and Evaluation Conference, 2020. One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment [46] Nikita Soni, Matthew Matero, Niranjan Balasubramanian, and H. Andrew Schwartz. Human language modeling. In Findings of the Association for Computational Linguistics: ACL 2022, 2022. [47] Fatemehsadat Mireshghallah, Vaishnavi Shrivastava, Milad Shokouhi, Taylor Berg-Kirkpatrick, Robert Sim, and Dimitrios Dimitriadis. UserIdentifier: Implicit user representations for simple and effective personalized sentiment analysis. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022. [48] Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, and Tat-Seng Chua. Large language models empowered personalized web agents. In Proceedings of the ACM on Web Conference 2025, WWW 25, 2025. [49] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: have dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018. [50] Pierre-Emmanuel Mazaré, Samuel Humeau, Martin Raison, and Antoine Bordes. Training millions of personalized dialogue agents. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018. [51] Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, and Ji-Rong Wen. Less is more: Learning to refine dialogue history for personalized dialogue generation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022. Health-llm: [52] Qinkai Yu, Mingyu Jin, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, and Yongfeng Zhang. Personalized retrievalaugmented disease prediction system, 2025. [53] Shady Shehata, David Santandreu Calonge, Philip Purnell, and Mark Thompson. Enhancing videobased learning using knowledge tracing: Personalizing students learning experience with ORBITS. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), 2023. [54] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: personalized robot assistance with large language models. Auton. Robots, 2023. [55] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML16, 2016. [56] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, 2017. [57] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. simple neural attentive metalearner. In International Conference on Learning Representations, 2018. [58] Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022. [59] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022. [60] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning, 2016. [61] Jane Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn, 2017. [62] Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and Pascale Fung. Personalizing dialogue agents via meta-learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [63] Ruofan Wang, Prakruthi Prabhakar, Gaurav Srivastava, Tianqi Wang, Zeinab S. Jalali, Varun Bharill, Yunbo Ouyang, Aastha Nigam, Divya Venugopalan, Aman Gupta, Fedor Borisyuk, Sathiya Keerthi, and Ajith Muralidharan. Limaml: Personalization of deep recommender models via meta learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, 2024. [64] Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, and Hugo Larochelle. meta-learning perspective on cold-start recommendations for items. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, 2017. [65] Zhenchao Wu and Xiao Zhou. M2eu: Meta learning for cold-start recommendation via enhancing 16 One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [75] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, 2020. [76] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. The llama 3 herd of models, 2024. [77] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2015. user preference estimation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 23, 2023. [66] Minchang Kim, Yongjin Yang, Jung Hyun Ryu, and Taesup Kim. Meta-learning with adaptive weighted loss for imbalanced cold-start recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 23, 2023. [67] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. Melu: Meta-learned user preference estimator for cold-start recommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 19, 2019. [68] Thomas Zollo, Andrew Wei Tung Siah, Naimeng Ye, Ang Li, and Hongseok Namkoong. PersonalLLM: Tailoring LLMs to individual preferences. In The Thirteenth International Conference on Learning Representations, 2025. [69] Yushang Zhao, Huijie Shen, Dannier Li, Lu Chang, Chengrui Zhou, and Yinuo Yang. Meta-learning for cold-start personalization in prompt-tuned llms, 2025. [70] Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, and Chelsea Finn. FSPO: Few-shot preference optimization of synthetic preference data elicits LLM personalization to real users. In 2nd Workshop on Models of Human Feedback for AI Alignment, 2025. [71] Sungyong Baik, Janghoon Choi, Heewon Kim, Dohee Cho, Jaesik Min, and Kyoung Mu Lee. Meta-learning with task-adaptive loss function for few-shot learning. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. [72] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta learning via learned loss. In International Conference on Pattern Recognition, ICPR, Italy, January 10-15, 2021, 2021. [73] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [74] Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Michael Bean, Katerina Margatina, Rafael Mosquera, Juan Manuel Ciro, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott A. Hale. The PRISM alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. In The Thirty-"
        }
    ],
    "affiliations": [
        "1Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China",
        "2Department of Computer Science, National University of Singapore, Singapore",
        "3School of Software, Tsinghua University, Beijing, China",
        "4School of Computing, National University of Singapore, Singapore"
    ]
}