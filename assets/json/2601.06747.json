{
    "paper_title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
    "authors": [
        "Glenn Matlin",
        "Akhil Theerthala",
        "Anant Gupta",
        "Anirudh JM",
        "Rayan Castilla",
        "Yi Mei Ng",
        "Sudheer Chava"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge."
        },
        {
            "title": "Start",
            "content": "FinForge: Semi-Synthetic Financial Benchmark Generation Glenn Matlin1 2 3, Akhil Theerthala1 *, Anant Gupta3 *, Anirudh Jaidev Mahesh2, Yi Mei Ng2, Rayan Castilla3, Sudheer Chava1 2 3 1Financial Services Innovation Lab, Georgia Institute of Technology 2College of Business, Georgia Institute of Technology 3College of Computing, Georgia Institute of Technology glenn@gatech.edu, akhiltvsn@gmail.com, agupta886@gatech.edu Abstract Evaluating Language Models (LMs) in specialized, highstakes domains such as finance remains significant challenge due to the scarcity of open, high-quality, and domainspecific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipelines efficacy, we produce FinForge-5k, snapshot *Equal Contribution Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. benchmark comprising over 5,000 human-validated questionanswer pairs across 11 finance subdomains, derived from curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the frameworks utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge. Introduction Language Models (LMs) are increasingly adopted for decision support in complex, high-stakes domains such as finance, law, and public policy (Bommasani et al. 2021; Nie et al. 2024). While recent advances in LMs have demonstrated strong performance on general knowledge benchmarks (Hendrycks et al. 2021) and professional exams (OpenAI 2023), reliably evaluating these systems in specialized, knowledge-intensive, and dynamic domains remains 6 2 0 2 1 1 ] A . [ 1 7 4 7 6 0 . 1 0 6 2 : r Figure 1: The FinForge Process - The framework ingests raw unstructured financial text (1) and identifies explicit causal triggers and outcomes. Crucially, the Reasoning Engine (2) applies domain-specific schemas to infer the implicit financial mechanism linking them. Finally, it synthesizes complex, multiple-choice question and answer (3). significant challenge. Existing evaluation sets offer broad subject coverage and serve as useful indicators of general knowledge. However, static benchmarks are limited by potential data leakage into LM training corpora (Deng et al. 2024), which can artificially inflate performance due to LM memorization. These challenges are exacerbated in dynamic domains like finance, where knowledge must be continually updated to reflect real-world developments. Empirical evidence suggests that top-performing general models do not necessarily excel at financial tasks requiring domain-specific nuance or quantitative reasoning (Chen et al. 2021; Islam et al. 2023). This highlights the necessity of dynamic benchmark generation to rigorously assess LM financial knowledge and reasoning robustness in realistic industry scenarios. instruments, Finance presents unique challenges for LM evaluation due to its multi-domain complexity, stochastic characteristics, and stringent regulatory requirements (Cao 2022; Vukovic, Dekpo-Adza, and Matovic 2025; Kamalov et al. 2024; Alzoubi, Alkhateeb et al. 2025). Effective financial analysis requires both comprehensive domain knowledge such as familiarity with financial regulations, and policiesand advanced quantitative problemsolving using real-world data, including asset valuations and risk projections. Additionally, the financial sector evolves rapidly, with new markets, regulations, and trends emerging continuously due to the dynamic nature of global economic systems. Consequently, maintaining up-to-date knowledge is essential for accurate financial reasoning. These challenges motivate three research questions: RQ1: Can semi-synthetic benchmark generation produce high-quality, contamination-free evaluation datasets for specialized domains like finance? RQ2: How do state-of-the-art language models perform across financial subdomains, and what patterns emerge? RQ3: What reasoning capabilitiesconceptual versus quantitativedo current models lack in financial contexts? To address these questions, this paper introduces the FinForge framework, methodology for generating semisynthetic benchmarks tailored to the financial domain. FinForge addresses the gap in the dynamic evaluation of language models financial knowledge by providing novel pipeline to generate diverse, challenging finance questions grounded in real-world content on demand. The methodology combines human-guided data curation with LM-driven question synthesis, thereby overcoming the limitations of prior static, easily memorizable benchmarks. By leveraging hybrid workflow, FinForge allows for the continuous creation of contamination-free evaluation sets that evolve alongside the dynamic financial landscape. The FinForge methodology curates relevant knowledge exclusively from authoritative sources, such as academic textbooks, institutional research, and domain experts, deliberately excluding user forums and trivial stock data. By leveraging verified knowledge, the pipeline increases the difficulty of AI evaluations by generating challenging scenarios that test domain-specific problem-solving capabilities. The process employs multi-stage language model workflow to analyze long-context documents, extract key information, and generate knowledge for creating questionanswer pairs. Each question is planned by identifying central concept or reasoning challenge within the source material, formulating complex question with embedded background information, and providing correct answer and distractors. more advanced language model serves as judge to validate the quality and financial relevance of each question. This controlled generation process produces difficult, selfcontained questions that require expert-level economic insight and multi-step reasoning. To demonstrate the frameworks utility, we generate FinForge-5k, snapshot benchmark comprising 5,000 expert-level finance questionanswer pairs. This benchmark is derived from high-quality corpus of up-to-date financial documentscurated via the pipelinetotaling 143M tokens across more than 100,000 verified articles spanning 11 subdomains, including personal finance, corporate finance, macroeconomics, and securitized investments. Notably, the underlying methodology is dynamic: the FinForge framework can be rerun and new documents incorporated to update the question set, allowing continual adaptation to emerging financial knowledge. This paper details the FinForge methodology and demonstrates its capabilities by benchmarking several state-of-the-art models on FinForge5k, thereby highlighting current strengths and weaknesses in financial reasoning. Related Works Existing public datasets for evaluating Question Answering (QA) in the financial domain are limited in scope, focus, and recency. While several benchmarks have made significant contributions to specific subtasks, their coverage remains narrow. For example, FinQA (Chen et al. 2021) and its conversational extension ConvFinQA (Chen et al. 2022) provide 8,200 QA pairs centered on numerical reasoning with plain-text representations of tabular financial data, and TAT-QA (Zhu et al. 2021) similarly targets QA on annual reports with tables and text. Although valuable, these datasets primarily address numeric reasoning and do not encompass broader conceptual finance knowledge. The recent FinanceBench (Islam et al. 2023) initiative sought to expand the range of finance QA but offers only 150 questionanswer pairs, with relatively simple questions that do not reflect real-world complexity. Similarly, FinTextQA, long-form QA dataset compiled from textbooks and policy documents, encourages explanatory answers and highlights the limitations of prior benchmarks that focused predominantly on stock data or basic calculations. In the industry, the scarcity of robust evaluation sets has prompted efforts such as S&P Global Kenshos S&P AI Benchmarks, which assembled 600 expert-verified questions across categories, including domain knowledge, quantity extraction, and quantitative reasoning. The emergence of these initiatives reflects the growing demand for more realistic and challenging evaluations of LMs in finance. central challenge in constructing dynamic benchmarks is ensuring that questions remain both novel and sufficiently Figure 2: The FinForge pipeline comprises two complementary stages. Data Curation (left): hybrid manualprogrammatic process that applies financial taxonomy to identify authoritative web domains, scrapes and filters content, and assembles high-quality Finance Corpus. Question Generation (right): An LM-driven five-stage workflow that analyzes documents to extract salient information, creates structured answer plans, generates self-contained questions with plausible distractors, assigns category labels, and applies rubric-based validation to ensure relevance, clarity, and factual accuracy. The validated outputs populate both Q/A Corpus for benchmarking and feed back into the Finance Corpus for iterative refinement. difficult. Since most language models are trained on extensive public internet data, static test suites risk being memorized during training, thereby compromising their evaluative value. To address this, researchers are increasingly exploring dynamic or semi-synthetic benchmarks that can be refreshed or generated as needed (Das et al. 2021; Guo et al. 2024). Previous studies have demonstrated that carefully controlled LM-based generation can yield high-quality, semi-synthetic data at scale, enabling contamination-free evaluation (Long et al. 2024). General Knowledge and QA Benchmarks Several benchmarks have been developed to measure broad knowledge and reasoning ability in LMs. MMLU (Hendrycks et al. 2021), which tests models on everything from elementary math to professional law and accounting, has become standard evaluation suite. Other efforts include Big-Bench (Srivastava et al. 2023) and the AI2 Reasoning Challenge (ARC) (Clark et al. 2018; Chollet et al. 2025), which target complex reasoning or scientific questions. These static benchmarks have driven progress, but are increasingly prone to contamination from training data as models ingest questions and answers from the web (Xu et al. 2024a,b). This has sparked interest in more adaptive evaluation methods. One line of work uses LM-based generation to create new test items, prompting LMs to rewrite or expand existing benchmark questions into novel variants. More holistically, LatestEval (Li, Guerin, and Lin 2024) constructs entirely new reading comprehension sets from real-time sources such as BBC News, using an LM to generate questions for up-to-date passages. Recently, multi-agent systems have been proposed for automated benchmark creation: BenchAgents (Butt et al. 2024) splits the task into planning, generation, and verification agents that collaborate (with humans in the loop) to produce high-quality evaluation data. Such approaches yield dynamically extendable benchmarks and help ensure test data novelty. In parallel, research on controllable question generation has introduced techniques to enforce difficulty and content constraints on generated questions. Notably, Li and Zhang (2024) propose Planning First, Question Second (PFQS) method in which an LM first outlines detailed answer plan (with target answer, relevant facts, and cognitive steps), and then another LM generates question conforming to this plan. This leads to more faithful, expert-aligned questions, as the model must adhere to blueprint for the desired reasoning. Our FinForge methodology draws inspiration from these advances in controllable question generation. Financial QA Datasets and Benchmarks Before our work, relatively few datasets existed for evaluating QA or reasoning in the financial domain, and each covered only slice of the domain. FinQA was one of the first, featuring questions that require numerical reasoning over company financial reports. It introduced the challenge of performing multi-step arithmetic operations on statements and tables, and showed that models lag far behind human experts on such tasks. TAT-QA (Table-and-Text QA) similarly targets reasoning with hybrid data (earnings tables + text) in financial reports. These datasets primarily evaluate the ability to do structured data reasoning (e.g., reading an annual report) and include annotated programs or formulas for interpretability. later extension, ConvFinQA, turned FinQA into multi-turn dialogue challenge. Beyond corporate reports, other benchmarks have been even more limited: the FiQA challenge (Macedo Maia et al. 2018) released small set of user-submitted questions and answers on personal finance topics. Most of these lack the complexity and diversity of knowledge needed to test an advanced AIs full financial acumen. For instance, FinQA and TAT-QA do not include conceptual questions on economics or open-ended advisory questions, focusing instead on factoid numeric problems. recent effort, FinanceBench (Islam et al. 2023), sought to compile wider range of financial QA pairs (covering banking, markets, accounting, etc.). Still, it contains only 150 questions in total too small to capture the breadth of finance or to reliably benchmark modern LMs. Moreover, FinanceBenchs question complexity remains limited, falling short of the complexity of real-world expert queries. Recognizing these gaps, Chen et al. introduced FinTextQA (Chen et al. 2024), long-form QA dataset drawn from finance textbooks and government agency documents. FinTextQAs 1,262 questions are designed to elicit paragraph-length answers, emphasizing explanatory responses over simple calculations. This provides valuable test of explanation and retrieval capabilities. However, the dataset is relatively small and requires generative answers rather than the multiplechoice format often used in benchmarking. Complementary to academic datasets, industry researchers have also developed proprietary benchmarks. Notably, Kenshos S&P BizBench (developed by Koncel-Kedziorski et al.) evaluates models on finance and business tasks across three main categories: domain knowledge (e.g., concept definitions or CFA exam questions), quantitative reasoning (multistep problems requiring math and finance formulas), and quantity extraction from financial documents. Recent studies have specifically evaluated LLMs on CFA examinations, revealing significant gaps in professional-level financial reasoning (Yao et al. 2024; Shetty et al. 2025). The benchmark consists of 600 expert-curated questions and includes eight tasks for financial reasoning, ranging from code generation for math problems to the FinKnow QA task for conceptual questions. public leaderboard shows that even toptier models (e.g., Claude 3.5, GPT-4) struggle with the hardest numerical reasoning questions (Koncel-Kedziorski et al. 2024). These efforts emphasize the growing importance of domain-specific evaluation. Our work differs in that we propose an open-source, automated pipeline to generate much larger set of finance QA pairs (5,000), blending the breadth of coverage of FinanceBench/FinTextQA (Islam et al. 2023) with the realism and difficulty seen in expert-written exams. FinForges use of real financial texts as the grounding for each question ensures that the content is current and verified, addressing both the dynamic knowledge aspect and the quality-control issue by providing source evidence for each answer. We view FinForge as complement to prior benchmarks pushing the envelope on scale and difficulty and hope it will enable more robust assessment and improvement of LMs for financial applications. Methodology To enable scalable and controlled evaluation of financial reasoning, we required robust corpus of finance-relevant documents encompassing both numerical and conceptual knowledge. Specifically, the corpus needed to capture (i) quantitative material such as financial calculations and analytical exercises, and (ii) qualitative content reflecting economic principles, market behavior, and institutional context. Given the scarcity of open-source datasets with verified, diverse, and high-quality financial text, we constructed semi-synthetic dataset using two-stage pipeline that integrates expert-guided data curation and LM-based question generation. In the first stage, we curated corpus of highquality financial documents drawn from authoritative web sources, leveraging hybrid manualprogrammatic pipeline that combines domain expertise with automated filtering and extraction. In the second stage, we employed frontier LMs (specifically, Gemini 2.5 Flash)(Team et al. 2025a) to generate diverse, high-quality questionanswer pairs from representative subset of this corpus. Together, these stages yield scalable, high-fidelity foundation for benchmarking and training models on financial reasoning tasks. Data Curation We structured our data curation methodology as hybrid manualprogrammatic pipeline that balances domain expertise with scalability. To minimize effort while maximizing domain coverage and quality, we first decomposed finance into structured hierarchy of 11 subdomains, including personal finance, corporate finance, investment theory, and macroeconomics. This taxonomy was guided in part by authoritative educational frameworks to ensure conceptual completeness and internal consistency. For each subdomain, humans identified authoritative web domains based on content rigor, institutional credibility, and topical relevance. Sources without clear editorial oversight or academic groundingsuch as discussion forums or informal opinion siteswere systematically excluded. This filtering ensured that only high-quality, verifiable content was included in the corpus. The pipeline then applies suite of open-source tools and heuristicsincluding domain whitelisting, keyword cooccurrence, sitemap traversal, and link structure analysis to automatically identify, filter, and rank candidate sites for extraction. Once candidate sites were finalized, we leveraged each sites structure to efficiently extract relevant financial text, avoiding the need for exhaustive manual traversal. For text extraction and parsing, we employed Trafilatura* and BeautifulSoup for HTML-based content, and *https://trafilatura.readthedocs.io/ https://beautiful-soup-4.readthedocs.io/ PyMuPDF4LLM for PDF documents, ensuring consistent text normalization and formatting across source types. The process cleanly separates filtering and extraction stages, enabling parallelized domain filtering and asynchronous content extraction at scale. Question Generation and Validation Recent works in question generation demonstrate the scalability and controllability of using tailored LM agents for this task (Li and Zhang 2024; Noorbakhsh et al. 2025). Our approach synthesizes insights from these methods and adapts them for controlled generation from domain-specific documents. FinForge employs an automated five-stage process, as illustrated in Figure 2. First, we perform deep analysis of the input documents to extract salient information. This analysis informs the generation of structured answer plan, which serves as blueprint for guiding question formulation. Using this plan, we then generate complex questions that remain strictly grounded in the source material. In the final stage, an LM-as-a-judge framework validates these questions against predefined rubric, filtering for relevance and quality. Synthetic Question Generation The preliminary document analysis phase aims to discern deep financial thinking patterns within articles to uncover opportunities for probing questions. We specifically focus on identifying four essential characteristics in document: causal relationships, prominent and competing hypotheses, necessary assumptions, and counterfactual possibilities. This breakdown of the document is crucial during the planning stage. In this stage, we translate the unstructured information into concrete blueprint for generation. This involves identifying specific, testable conceptual nucleus within the text that serves as the focus area. The agent also assesses the cognitive complexity of this concept, assigning difficulty rating on five-point scale ranging from basic recall (1) through multi-step reasoning (3) to expert-level synthesis requiring multiple constraints (5). Finally, it extracts the minimal set of relevant passages required to construct self-contained question. This plan ensures that the question is well-defined, appropriately challenging, and directly traceable to the source document. In the third stage, we use this blueprint to formulate complete questionanswer pair. Adhering to the principle of self-containment, all necessary context and data from the relevant passages are embedded directly into the questions premise. The language model for this stage is prompted to generate natural-language question, plausible distractors, and concise explanation for the correct answer, all while adhering to the domain-specific requirements provided in the inputs. In addition to the preceding processes, we implement supplementary labeling phase to categorize the artifacts based on the financial issue, perceived difficulty, and the targeted models finance-related capability. These labels help filter out irrelevant questions and provide valuable insights https://pymupdf.readthedocs.io/ into models performance across diverse settings. Each label is linked to its own case-specific rules that help the model adapt to the nature of the topic it must address. Validation and Filtering The primary issue of automated generation is that outputs frequently fail to meet benchmark standards. We frequently observed unclear inquiries, erroneous hypotheses, or entirely unrelated questions arising from ostensibly direct materials. Consequently, we implemented an extra question validation phase that uses language model to assess the questions validity for the specific use case. Each question is evaluated across multiple dimensionsfinancial relevance (domain appropriateness), self-sufficiency (answerable without external context), logical consistency (no contradictions), clarity (unambiguous wording), and complexity (appropriate difficulty). question passes validation only if it satisfies all five criteria; failure on any dimension results in rejection. Each iteration of the pipeline development has undergone rigorous validation through expert review, independent of the existing automated checks. Based on this expert feedback, we iteratively refined the generation and filtering logic to produce final set of higher-quality, more challenging questions. Results To demonstrate the pipelines efficiency, the complete corpus construction and question-generation workflow was executed over seven days, encompassing taxonomy design, domain filtering, and content extraction. For this snapshot generation, the data curation stage yielded over 100,000 highquality and verified financial documents spanning eleven well-defined subdomains. Collectively, these documents contained 143M tokens of domain-specific text, providing diverse foundation that integrates both quantitative and conceptual financial knowledge. From this corpus, we sampled 10,000 documents for question generation via stratified random sampling across the 11 subdomains, ensuring proportional representation using Gemini 2.5 Flash as the synthesis model. This process produced 10,000 initial questionanswer (Q/A) pairs. Subsequent automated filteringcombining rulebased quality control with LM-as-judge scoring for relevance, clarity, and factual accuracyresulted in the refined FinForge-5k benchmark set of 5,000 Q/A pairs. The primary rejection reasons were insufficient self-containment (questions requiring external context), ambiguous answer choices, and misalignment between question difficulty and source material complexity. stratified random sample of 500 Q/A pairs was additionally reviewed by domain experts (see Expert Evaluation). Model Benchmarking The validated FinForge-5k benchmark was used to evaluate range of both open-source and closed-source language models  (Table 1)  . Performance was measured using consistent multiple-choice setup, with accuracy defined as the proportion of correctly predicted answers across 5,000 questionanswer pairs. Models Qwen 3 235B (2025) DeepSeek V3 (2024) GPT-4o (2024) Qwen3-Next 80B (2025) Sonnet 4 (2025) Llama 3.3 70B (2024) OLMo 2 7B (2024) OLMo 2 32B (2024) Llama 4 Scout (2025) Overall Alt/RE Beh/Quant 0.852 0.776 0.820 0.705 0.762 0.717 0.803 0.720 0.713 0.756 0.779 0.709 0.730 0.626 0.721 0.516 0.590 0.413 0.771 0.739 0.734 0.732 0.726 0.725 0.608 0.567 0.465 Corp Fin & Val 0.739 0.698 0.704 0.700 0.693 0.690 0.568 0.516 0. FinTech FAR 0.950 0.783 0.950 0.743 0.875 0.743 0.925 0.739 0.875 0.770 0.875 0.686 0.750 0.566 0.750 0.527 0.600 0.593 Ethics & Gov 0.938 0.938 0.875 0.938 0.812 0.875 0.812 0.812 0.812 Mkt & Deriv 0.874 0.863 0.822 0.855 0.798 0.839 0.727 0.738 0.724 Reg & Portf Comp Mgmt 0.819 0.803 0.794 0.774 0.767 0.746 0.782 0.744 0.787 0.750 0.804 0.726 0.662 0.636 0.615 0.605 0.625 0.520 Wealth Mgmt 0.610 0.603 0.653 0.590 0.613 0.640 0.450 0.463 0.283 Pub/Intl Fin 0.815 0.818 0.818 0.793 0.771 0.799 0.702 0.650 0. Table 1: Model accuracies (proportion correct) on the FinForge benchmark (5k samples). Abbreviations: Alt/RE = Alternative Investments & Real Estate; Beh/Quant = Behavioral & Quant Finance; Corp Fin & Val = Corporate Finance & Valuation; FinTech = FinTech & Innovation; FAR = Financial Accounting & Reporting; Ethics & Gov = Financial Ethics & Governance; Mkt & Deriv = Markets & Derivatives; Reg & Comp = Regulation & Compliance; Portf Mgmt = Investment & Portfolio Management; Wealth Mgmt = Personal Finance & Wealth Management; Pub/Intl Fin = Public & International Finance. We organize the evaluation along two dimensions: model availability (proprietary vs. open-source) and scale (parameter count). Closed-source models such as GPT-4o and Claude Sonnet 4 achieve accuracies of 73.4% and 72.6%, respectively. Notably, open-source models of the same generation, such as Qwen-3-235B and DeepSeek v3.1, demonstrate superior performance on the benchmark. Additionally, we have categorized the evaluated models into three key groups to assess the influence of model scale on their performance against the benchmark. Surprisingly, mid-range models (32-110B) demonstrate performance comparable to large-scale proprietary models, with Qwen3-Next-80B exhibiting only 5% deficit relative to Qwen-3-235B and remaining within 1% of the DeepSeek, GPT-4o, and Sonnet models. This suggests that sheer model scale is not the sole determinant of success in domain-specific financial reasoning, underscoring the utility of FinForge-5k in diagnosing reasoning capabilities beyond generalist benchmarks. We hypothesize that observed performance differences may reflect variations in training data composition or domain-specific optimization strategies; however, confirming these hypotheses would require access to training corpora and implementation details that are not publicly disclosed. These results reveal key insight for financial evaluation: neither state-of-the-art generalist performance nor model scale alone guarantees superior financial reasoning capabilities. Expert Evaluation To validate the efficacy of the pipelines generation capability, three domain experts conducted qualitative assessment on 10% stratified sample of the FinForge-5k snapshot. This expert review entailed verifying the clarity, selfcontainment, plausibility, and real-world relevance of the generated items. The expert review validated the high quality and complexity of the generated questions, with 70% of the 500 samples deemed clear, accurate, and relevant. Crucially, the remaining 30% were typically not factually incorrect, but were flagged for ambiguity or missing contextual assumptions required for definitive answer. This validation rate stands in significant contrast to the 100% approval rate assigned by the automated LM-as-a-judge (Stage 5) for the identical sample. The 30-point discrepancy offers quantitative support for our conclusion regarding the limitations of the LM-asa-judge, indicating that it currently lacks the sophistication required to assess complex financial reasoning, demonstrating that expert oversight remains essential. Addressing Data Contamination and Circularity Data contaminationwhere test data appears in training corporaremains significant concern for LLM evaluation (Cheng, Chang, and Wu 2025). As methodological sanity check, we evaluated the generator model (Gemini 2.5 Flash) and model from the same family (Gemma 27B Team et al. (2025b)) on the resulting benchmark. They achieved scores of 79.3% and 74.0%, respectively. Due to the inherent risk of data contamination from this circular evaluation, we explicitly omit the Gemma and Gemini models from our primary benchmark  (Table 1)  to ensure fair and rigorous assessment of other models. Discussion The FinForge-5k snapshot reveals systematic weaknesses in current model capabilities. Beyond the accuracy scores in Table 1, we conduct proportional error analysis to identify specific reasoning deficiencies. Our analysis indicates that Personal Finance & Wealth Management and Corporate Finance & Valuation are notably challenging topics relative to others in the benchmark. Fundamentally, we characterize these domains as tasks requiring complex multi-constraint satisfactionsuch as optimizing for tax liabilities, liquidity needs, and risk simultaneouslyin contrast to traditional retrieval-based finance tasks. This contrasts with the models comparatively high performance on Markets and Derivatives and Portfolio Management. This disparity suggests that despite exposure to substantial publicly available financial data during pretraining (Wu et al. 2023), models struggle to transfer such knowledge to scenarios requiring the integration of regulatory constraints with fundamental financial principles. Conversely, across subjects, evidence indicates that models struggle most with quantitative questions, followed by counterfactual inquiries, aligning with the recent understanding of language model capabilities (Ahn et al. 2024; Mirzadeh et al. 2025). It is noteworthy that while the FinForge framework can generate multi-hop reasoning questions, for this snapshot they were frequently derived from distinct sections of single documents, representing specific tier of reasoning difficulty. Expert evaluation of incorrect quantitative answers in FinForge-5k identified two distinct failure modes. The first was conceptual: models applied incorrect financial methodologies, made flawed assumptions, or constructed inadequate logic (e.g., miscalculating depreciation). The second was arithmetic: models recognized the correct steps but erred in calculation. This distinction is essential. Arithmetic failures can be mitigated by external tools (Schick et al. 2023; Gao et al. 2023), but conceptual failures reveal fundamental deficiency: models are misinterpreting the intricate financial reasoning demanded by the prompt. Future studies should concentrate on addressing these conceptual misinterpretations. The findings possess substantial real-world implications. The significant shortcomings in Personal Finance & Wealth Management highlight that the average user cannot currently rely on these models for important financial decisions (Hean, Saha, and Saha 2025; Takayanagi et al. 2025). This represents significant gap that has previously been overlooked in benchmarks focused on conventional information retrieval. This work contributes in two ways. We first identify and analyze specific, high-impact weaknesses in contemporary LMs concerning financial reasoning and personalization. We demonstrate that the FinForge framework is an effective method for generating nuanced, domain-specific benchmarks. This outlines clear framework for guiding future advancements and focused enhancements to financial-sector models. Conclusion FinForge demonstrates that scalable, domain-grounded benchmark generation is achievable through principled combination of expert oversight and controlled LM synthesis. By integrating verified financial sources with structured question generation and multi-stage validation, FinForge produces high-quality datasets that accurately reflect the depth of reasoning and quantitative rigor demanded in economic analysis. Evaluations using the FinForge-5k snapshot reveal specific, high-impact weaknesses in state-of-theart models, particularly tendency to fail at conceptual reasoning rather than simple arithmetic. This validation gap reinforces the importance of expert oversight in complex domains. Beyond the immediate utility of the FinForge-5k snapshot, the framework offers generalizable methodology for creating transparent, extendable evaluation pipelines in other specialized domains. Future work will focus on expanding the corpus to additional subfields, integrating temporal and dynamic data to assess model recency, and establishing continual-learning benchmarks that evolve alongside financial markets. Ultimately, FinForge lays the groundwork for systematic, reproducible, and evolving evaluation of LMs in finance and other expert-driven disciplines. Limitations The primary limitation of the study is the reliance on Gemini 2.5 Flash for both question generation and evaluation. Manual verification revealed that while questions were challenging, they often lacked the contextual assumptions needed for definitive answer. The ambiguity, likely arising from the generators speed optimization, poses risk of leading assessed LMs to make incorrect assumptions, thereby compromising the reliability of assessments. Gemini 2.5 Flashs design, which prioritizes speed, results in capabilities mismatch when evaluating complex reasoning models. This likely led to the evaluator lacking the sophistication necessary to accurately assess advanced outputs, which may have distorted performance metrics. The boolean validator operated as black box. The system effectively filtered out irrelevant questions; however, its lack of transparency obstructed the analysis of failed questions, thereby impeding the improvement of the generation pipeline. Additionally, data contamination and circular dependency remain concerns when evaluating models from the same family as the generator. The present study omits the performance of Gemini and Gemma models from the primary results to mitigate this issue, but future work should develop more robust strategies for isolating generator influence from evaluation outcomes. References Ahn, J.; Verma, R.; Lou, R.; Liu, D.; Zhang, R.; and Yin, W. 2024. Large Language Models for Mathematical ReaIn Proceedings of the soning: Progresses and Challenges. 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, 225237. Association for Computational Linguistics. Alzoubi, M.; Alkhateeb, A.; et al. 2025. Artificial Intelligence and Finance: bibliometric review on the Trends, F1000Research, Influences, and Research Directions. 14(122). Anthropic. 2025. System Card: Claude Opus 4 & Claude Sonnet 4. Technical report, Anthropic. Bommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R. B.; Arora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.; Brunskill, E.; Brynjolfsson, E.; Buch, S.; Card, D.; Castellon, R.; Chatterji, N. S.; Chen, A. S.; Creel, K.; Davis, J. Q.; Demszky, D.; Donahue, C.; Doumbouya, M.; Durmus, E.; Ermon, S.; Etchemendy, J.; Ethayarajh, K.; FeiFei, L.; Finn, C.; Gale, T.; Gillespie, L. E.; Goel, K.; Goodman, N. D.; Grossman, S.; Guha, N.; Hashimoto, T.; Henderson, P.; Hewitt, J.; Ho, D. E.; Hong, J.; Hsu, K.; Huang, J.; Icard, T.; Jain, S.; Jurafsky, D.; Kalluri, P.; Karamcheti, S.; Keeling, G.; Khani, F.; Khattab, O.; Koh, P. W.; Krass, M. S.; Krishna, R.; Kuditipudi, R.; and et al. 2021. On the Opportunities and Risks of Foundation Models. CoRR, abs/2108.07258. Butt, S.; Chandrasekaran, D.; Moorthy, S. P.; Agrawal, R.; et al. 2024. BenchAgents: Multi-Agent Systems for Structured Benchmark Creation. arXiv:2410.22584. Cao, L. 2022. AI in Finance: Challenges, Techniques, and Opportunities. ACM Computing Surveys, 55(3): 138. Chen, J.; Zhou, P.; Hua, Y.; Xin, L.; Chen, K.; Li, Z.; Zhu, B.; and Liang, J. 2024. FinTextQA: Dataset for Longform Financial Question Answering. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Bangkok, Thailand: Association for Computational Linguistics. Chen, Z.; Chen, W.; Smiley, C.; Shah, S.; Borova, I.; Langdon, D.; Moussa, R.; Beane, M.; Huang, T.-H.; Routledge, B. R.; et al. 2021. FinQA: Dataset of Numerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 36973711. Chen, Z.; Li, S.; Smiley, C.; Ma, Z.; Shah, S.; and Wang, W. Y. 2022. ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question AnswerIn Proceedings of the 2022 Conference on Empiriing. cal Methods in Natural Language Processing, 62796292. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. Cheng, Y.; Chang, Y.; and Wu, Y. 2025. Survey on Data Contamination for Large Language Models. arXiv:2502.14425. Chollet, F.; Knoop, M.; Kamradt, G.; and Landers, B. 2025. ARC Prize 2024: Technical Report. arXiv:2412.04604. Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. Das, B.; Majumder, M.; Phadikar, S.; and Sekh, A. 2021. Automatic question generation and answer assessment: survey. Research and Practice in Technology Enhanced Learning, 16(5): 115. DeepSeek-AI. 2024. arXiv:2412.19437. Deng, C.; Zhao, Y.; Tang, X.; Gerstein, M.; and Cohan, A. 2024. Investigating Data Contamination in Modern Benchmarks for Large Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics. Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y.; Callan, J.; and Neubig, G. 2023. PAL: Program-aided Language Models. arXiv:2211.10435. Guo, S.; Liao, L.; Li, C.; and Chua, T.-S. 2024. Survey on Neural Question Generation: Methods, Applications, and Prospects. arXiv preprint arXiv:2402.18267. Hean, O.; Saha, U.; and Saha, B. 2025. Can AI help with your personal finances? Applied Economics, 19. DeepSeek-V3 Technical Report. Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive MulInternational Conference titask Language Understanding. on Learning Representations. Islam, P.; Kannappan, A.; Kiela, D.; Qian, R.; Scherrer, N.; and Vidgen, B. 2023. FinanceBench: New BencharXiv preprint mark for Financial Question Answering. arXiv:2311.11944. Kamalov, F.; Gurrib, I.; Elmazi, R.; and Moussa, S. 2024. Deep Learning in Finance: Survey of Applications and Techniques. AI, 5(4): 101. Koncel-Kedziorski, R.; et al. 2024. BizBench: Quantitative Reasoning Benchmark for Business and Finance. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. Li, K.; and Zhang, Y. 2024. Planning First, Question Second: An LLM-Guided Method for Controllable Question Generation. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Findings of the Association for Computational Linguistics: ACL 2024, 47154729. Bangkok, Thailand: Association for Computational Linguistics. Li, Y.; Guerin, F.; and Lin, C. 2024. LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction. In Proceedings of the AAAI Conference on Artificial Intelligence. Long, L.; et al. 2024. On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: Survey. arXiv:2406.15126. Macedo Maia, S.; Handschuh, S.; Freitas, A.; Davis, B.; McDermott, R.; Zarrouk, M.; and Balahur, A. 2018. WWW18 Open Challenge: Financial Opinion Mining and Question Answering. In WWW 18 Companion: The 2018 Web Conference Companion, 19411942. Lyon, France: ACM. ISBN 9781450356404. Meta AI. 2024. arXiv:2407.21783. Meta AI. 2025. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation. Mirzadeh, I.; Alizadeh, K.; Shahber, H.; Tuzel, O.; Benber, S.; and Farajtabar, M. 2025. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large LanIn International Conference on Learning guage Models. Representations. Nie, Y.; Kong, Y.; Dong, X.; Mulvey, J. M.; Poor, H. V.; Wen, Q.; and Zohren, S. 2024. Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. arXiv:2406.11903. Noorbakhsh, K.; Chandler, J.; Karimi, P.; Alizadeh, M.; Savaal: Scalable Conceptand Balakrishnan, H. 2025. Driven Question Generation to Enhance Human Learning. arXiv:2502.12477. OLMo, T.; et al. 2024. 2 OLMo 2 Furious. OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774. Schick, T.; Dwivedi-Yu, J.; Dess`Ä±, R.; Raileanu, R.; Lomeli, M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023. The Llama 3 Herd of Models. et 2025. Qwen3 al. 2024. GPT-4o System Card. Toolformer: Language Models Can Teach Themselves to Use Tools. arXiv:2302.04761. Shetty, P.; Upadhayaya, A.; Shah, P. M.; Jagabathula, S.; Nayak, S.; and Fee, A. J. 2025. Advanced Financial Reasoning at Scale: Comprehensive Evaluation of Large Language Models on CFA Level III. arXiv:2507.02954. Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid, A.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; et al. 2023. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. arXiv:2206.04615. Takayanagi, T.; Izumi, K.; Sanz-Cruzado, J.; McCreadie, R.; and Ounis, I. 2025. Are Generative AI Agents Effective Personalized Financial Advisors? arXiv:2504.05862. Team, G.; et al. 2025a. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. arXiv:2507.06261. Team, G.; et al. 2025b. Gemma 3 Technical Report. arXiv:2503.19786. team, O.; arXiv:2410.21276. Team, Q. arXiv:2505.09388. Vukovic, D. B.; Dekpo-Adza, S.; and Matovic, S. 2025. AI integration in financial services: systematic review of trends and regulatory challenges. Humanities and Social Sciences Communications, 12. Wu, S.; Irsoy, O.; Lu, S.; Dabravolski, V.; Dredze, M.; Gehrmann, S.; Kambadur, P.; Rosenberg, D.; and Mann, G. 2023. BloombergGPT: Large Language Model for Finance. arXiv:2303.17564. Xu, C.; Guan, S.; Greene, D.; and Kechadi, M.-T. 2024a. Benchmark Data Contamination of Large Language Models: Survey. arXiv:2406.04244. Xu, R.; Wang, Z.; Fan, R.-Z.; and Liu, P. 2024b. Benchmarking Benchmark Leakage in Large Language Models. arXiv:2404.18824. Yao, X.; Wang, Q.; Liu, X.; and Huang, K.-W. 2024. Evaluating Large Language Models for Financial Reasoning: CFA-Based Benchmark Study. arXiv:2509.04468. Zhu, F.; Lei, W.; Huang, Y.; Wang, C.; Zhang, S.; Lv, J.; Feng, F.; and Chua, T.-S. 2021. TAT-QA: Question Answering Benchmark on Hybrid of Tabular and Textual Content in Finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 32773287. Online: Association for Computational Linguistics. Technical Report."
        }
    ],
    "affiliations": [
        "College of Business, Georgia Institute of Technology",
        "College of Computing, Georgia Institute of Technology",
        "Financial Services Innovation Lab, Georgia Institute of Technology"
    ]
}