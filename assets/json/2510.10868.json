{
    "paper_title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding",
    "authors": [
        "Soroush Mehraban",
        "Andrea Iaboni",
        "Babak Taati"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer from high computational cost and complexity due to deep transformer architectures and redundant tokens. In this paper, we introduce two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM selectively merges transformer layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background tokens that contribute little to the final prediction. To further address the potential performance drop caused by merging, we propose a diffusion-based decoder that incorporates temporal context and leverages pose priors learned from large-scale motion capture datasets. Experiments across multiple benchmarks demonstrate that our method achieves up to 2.3x speed-up while slightly improving performance over the baseline."
        },
        {
            "title": "Start",
            "content": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding Soroush Mehraban1,2,3, Andrea Iaboni1,3, Babak Taati1,2,3 1University of Toronto 2Vector Institute 3KITE Research Institute, UHN Project Page: https://soroushmehraban.github.io/FastHMR/ 5 2 0 2 3 1 ] . [ 1 8 6 8 0 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer from high computational cost and complexity due to deep transformer architectures and redundant tokens. In this paper, we introduce two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM selectively merges transformer layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background tokens that contribute little to the final prediction. To further address the potential performance drop caused by merging, we propose diffusion-based decoder that incorporates temporal context and leverages pose priors learned from largescale motion capture datasets. Experiments across multiple benchmarks demonstrate that our method achieves up to 2.3 speed-up while slightly improving performance over the baseline. 1. Introduction Human Mesh Recovery (HMR) in 3D involves estimating human pose and shape from images or videos captured by monocular camera. Recent transformer-based methods [12, 14, 32, 48] have shown strong performance. However, these models require significant computational resources and memory, limiting their use in real-time applications and on resource-constrained hardware such as live VR/AR telepresence [15], on-device fitness and rehabilitation, and embedded robotics. key reason for the high computational demand is self-attentions quadratic complexity, which causes compute and memory to grow exponentially with sequence length and hinders real-time or resource-constrained HMR. We observe that two complementary forms of reduninterInter-layer redancy persist in transformer-based HMR models: layer redundancy and spatial redundancy. Figure 1. Throughput v.s. MPJPE error on EMDB benchmark. Throughput is evaluated on single RTX 3090 GPU. dundancy arises because consecutive transformer layers often learn highly correlated representations, making the full execution of all layers computationally inefficient. Spatial redundancy occurs because many tokens correspond to background regions that contribute little to the final pose or shape estimation. Motivated by these observations, we propose two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (MaskToMe). ECLM identifies and merges consecutive layers whose output differences fall below an error threshold, reducing model depth while maintaining accuracy. MaskToMe leverages coarse person-background segmentation to merge redundant background tokens in the first transformer blocks, thereby reducing the token count while preserving essential human-centric and context information. Together, these methods significantly reduce computational overhead, but at the cost of drop in accuracy. To recover from the errors introduced by layerand token-level merging, we append diffusion-based decoder conditioned on the sequence of per-frame features produced by the merged transformer backbone. This sequence-level 1 conditioning allows the decoder to exploit temporal context during denoising, suppressing flicker and producing smooth mesh trajectories. In addition, given that pose priors have proven effective in text-to-motion generation [5, 8], we inject such prior into an HMR pipeline. We achieve this by running the decoder in the latent space of Variational Autoencoder (VAE) trained on large-scale motion-capture data, which constrains the diffusion path to anatomically plausible regions. As shown in Fig. 1, this design not only restores the lost accuracy but also yields slight improvement, while running faster than the original baseline. In summary, our contributions are three-fold: Efficient architecture. We jointly apply errorconstrained layer merging and mask-guided token merging to accelerate inference in transformer-based HMR pipelines. Accuracy restoration. To offset the resulting accuracy drop, we introduce diffusion decoder that fuses VAEbased human-pose prior with frame-level temporal cues, yielding anatomically plausible and temporally smooth meshes. FastHMR framework. The resulting FastHMR framework delivers up to 2.3 faster inference than its transformer baseline while achieving slight improvement in estimation error. 2. Related Works Deterministic vs. probabilistic estimation. Traditional HMR pipelines output single mesh per frame. Optimization-based methods [3, 33, 49] iteratively align SMPL [27] to 2-D keypoints, but depend on careful initialization. Feed-forward networks fall into two groups: image-based models [12, 19, 23, 25] and video-based variants that add temporal encoders such as 1-D CNNs [20], GRUs [22], or recurrent ViTs [40]. To model uncertainty, recent work turns to diffusion and score-based sampling. HMDiff [13] starts from random Gaussian distribution in addition to conditioning the denoiser using the and, extracted image features, it proposes distribution alignment technique in the early diffusion steps that incorporates input-specific distribution information as prior knowledge to simplify the denoising process. ScoreHMR [42] uses an off-the-shelf 3D human mesh regressor to estimate SMPL parameters and refines the estimation by first adding noise through DDIM Inversion [41] and then denoising it using score guidance to align with the available observations, e.g., fitting with 2D keypoints, consistency with cross-view observations, or temporal consistency. ScoreHypo [48] proposes HypoNet network to leverage diffusion models to produce diverse set of plausible estimates, aligned with the input image, and ScoreNet to rank them based on the quality and identify the best ones. DiffMesh [52] is the first approach to attempt leveraging temporal information. However, it conditions diffusion on the output of MotionBERT [54], which takes 2D pose sequences as input and lacks the complete frame context needed for generating the human mesh. DPMesh [55] is the only approach that employs an explicit generative prior, coupling diffusion with VAE trained on RGB-image data. In contrast, our decoder diffuses in the latent space of VAE learned from largescale pose data, giving tighter kinematic prior that allows us to recover the accuracy lost to efficiency measures while maintaining rapid inference. Efficient Human Mesh Recovery. Research on efficient transformer-based Human Mesh Recovery (HMR) Token pruning techniques spans four main directions. such as TORE discard up to 70% of visual tokens before self-attention, cutting FLOPs while retaining accuracy [11]. Lightweight attention redesigns replace quadratic self-attention with cheaper variants: FastMETRO factorises METRO into lean encoderdecoder with disentangled cross-attention for 2 throughput boost [6], and POTTER couples pooling attention with high-resolution branch, shrinking parameters to 7% of METRO without accuracy loss [53]. Hardware co-design approaches such as VITA jointly tailor ViT layers and custom accelerators, reporting up-to-5 lower latency at the cost of specialized silicon [44]. Unlike methods that rely on custom blocks, alternate modalities, or dedicated hardware, FastHMR applies error-constrained layer merging and mask-guided token merging post-hoc to any transformer backbone and complements this compression with diffusion decoder that restores lost accuracy while enforcing temporal smoothness. This strategy yields up to 2.3 speed-ups and slight accuracy gains on standard GPUs, making FastHMR more broadly deployable and temporally consistent solution. 3. Method 3.1. Preliminaries Forward Diffusion. The forward diffusion process in latent diffusion [36] is gradually transforms the latent vectors Z0, in this context encoded pose parameters of SMPL, into noisy vectors ZT through series of steps: q(Z1:T Z0) := (cid:89) t= q(ZtZt1), q(ZtZt1) := (Zt; (cid:112)1 βtZt1, βtI), (1) (2) where {βt}T t=1 is the variance used in the diffusion scheduler, and is number of timesteps throughout training. Using the reparametrization trick and the Markov process [16], we can sample Zt in any arbitrary timestep in closed form: αtZ0 + 1 αtϵ, (3) q(ZtZ0) := where αt := 1 βt, αt := (cid:81)t s=1 αs, and ϵ (0, I). In our experiments, we use diffusion schedule with zero terminal SNR fix [26] to ensure that αT = 0. Backward Diffusion. In the backward diffusion process, noisy latent vectors ZT (0, I) are sampled from standard Gaussian distribution and gradually denoised over several steps. The denoising model ϵθ(Zt, t, c) takes noisy latent vectors Zt and, using the conditioning input c, predicts the original noise vectors ϵ that were added to the clean latent vectors Z0. In our case, contains features extracted from video frames. Algorithm 1 ECLM: Error-Constrained Layer Merging 1: Input: pretrained HMR model with layers, examFigure 2. CKA (Center Kernel Alignment) between pairs of Transformer layers in CameraHMR [32], and HMR2.0 [14]. ples , ground-truths G, threshold τ 2: Output: merged extractor model 3: MPJPEbase ExtractMPJPE(E, , G) 4: high 1, 5: while low 0 do 6: low high Etmp MergeLayers(E, low, high) MPJPEtmp ExtractMPJPE(Etmp, , G) if MPJPEtmp MPJPEbase < τ then low low 1 else if low + 1 = high then MergeLayers(E, low + 1, high) high low low high 1 else high high 1 low low 1 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end if 18: end if 19: 20: end while 21: return 3.2. Token and Layer Merging We aim to reduce the computational cost in human mesh recovery (HMR). We do this by merging layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), and by merging background tokens while still preserving essential person information. Error-Constrained Layer Merging. Fig. 2 shows the Centered Kernel Alignment (CKA) [10, 35] scores between transformer layers in CameraHMR [32], and HMR2.0 [14]. CKA is metric that quantifies the similarity between internal representations of neural networks. As in large language models (LLMs) [10], many layers in HMR2.0 exhibit high representational similarity. This indicates that merging such layers could reduce memory usage and inference time with minimal impact on performance. To achieve this, we propose Error-Constrained Layer Merging (ECLM). As shown 3 Figure 3. Overview of the Mask-ToMe strategy. Tokens are split into sets and B, and the most similar background token pairs are merged using similarity scores while masking out person tokens. The bold and underlined numbers represent the highest and second-highest similarity scores, respectively. The numbers shown are illustrative examples only. in Algorithm 1, it relies on MPJPE as threshold to iteratively merge layers from the last layer towards the beginning. For merging, we follow SLM [10] and merge layers {Li, Li+1, ..., Lj} with parameters {θi, θi+1, ..., θj} using: θ = θi + (θi+1 θi) + + (θj θi) = θi + ji (cid:88) k=1 (θi+k θi) (4) Mask-guided Token Merging. Since recent HMR models [14, 32] use cross-attention in the decoder, we can safely reduce the number of tokens without affecting the final output. Fig. 3 presents an overview of the proposed maskguided token merging (Mask-ToMe) strategy. The input to Figure 4. Diffusion Decoder Overview. (a) In first stage of training, VAE model is trained to learn human motion priors. (b) The second stage includes training of denoiser ϵθ to recover pose latents conditioned on per-frame encodings extracted from encoder M. the HMR models is person-cropped image, which is tokenized into tokens. Among these, tokens represents the person, and the remaining tokens correspond to the background. Within designated transformer layers, we partition the tokens into two equal-sized sets and based on even and odd indices. Each token in set is matched to its most similar token in set B, from which the top most similar pairs are selected for merging by averaging. To avoid merging tokens that correspond to the person, we use segmentation model to generate person mask, which is tokenized alongside the image. Tokens identified as the person are assigned similarity score of negative infinity, ensuring they are excluded from the merging process. Among the transformer layers, we apply token merging to the first layers, where nl tokens are merged at each layer. This merging follows the constraint that fixed number of tokens must be retained, with treated as hyperparameter. Notably, in the final merging layer l, fewer than nl tokens may be merged if necessary to satisfy this constraint. While one could use the exact number of person tokens as lower bound, its image-dependent variability would require sequential processing of video frames, which would significantly increase inference time. 3.3. Diffusion Decoder To compensate for the performance degradation introduced by mask-guided token merging, we design diffusion-based decoder that leverages (i) strong pose prior learned from large-scale motion capture datasets, and (ii) the temporal dynamics of joint movements. An overview of the training pipeline of the proposed decoder is shown in Fig. 4. It consists of motion VAE to encode human motion prior in lower-dimensional space and conditional latent diffusion model to recover human mesh given the frames of an RGB video. During inference, we use the segmentation model to track and segment person and the resulting batch of cropped images I1:F = {I (i)}F i=1 and corresponding segmentation masks M1:F = {M (i)}F i=1 are given to frame encoder H, with our masked-guided token merging, to extract features 1:F = {x(i)}F i=1, where denotes the number of frames. We rely on the frame encoder and its pretrained per-frame linear decoders [14, 32] to reconstruct the = { ˆβ(i)}F shape parameters ˆβ i=1 and camera parameters ˆC 1:F = {ˆc(i)}F i=1. We do not use its linear decoder for pose and, instead, recover the pose parameters using our diffusion decoder. The latent diffusion model starts from random Gaussian noise (0, I) and, conditioned on the extracted features from frame encoder H, outputs latent 1:F 4 0 }FZ vectors ˆZ0 = {ˆz(i) i=1, where FZ is the number of vectors in the latent space. Finally, the denoised latent vectors are passed to the VAE decoder to output pose parameters ˆP 1:F = D(Z0). Latent Motion Representation. As shown in Fig. 4 (a), the motion VAE takes as input sequence of poses 1:F = {p(i)}F i=1 and reconstructs it by generating lowdimensional latent space that retains high information content. Each pose p(i) = {J (i), Φ(i)} represents the pose parameters of SMPL [27] by decomposing it into joint locations RNJ 3 and twist rotations Φ RNϕ2 = {(cos(ϕi), sin(ϕi))}Nϕ i=1 based on HybrIK [24], where NJ , Nϕ, and ϕ denote the number of body joints, number of body parts, and 1-DoF twist rotation around the ith body part, respectively. The motion VAE, = {E, D}, employs transformerbased architecture [5, 34] and consists of transformer encoder and transformer decoder D. It is trained to reconstruct the pose sequence 1:F by minimizing the Mean Squared Error (MSE) loss and the Kullback-Leibler (KL) loss. Diffusion Network. The architecture of the proposed latent diffusion model is shown in Fig. 4 (b). Throughout training, the ground truth (or pseudo ground truth [31]) pose 1:F is given to the pretrained frozen VAE encoder to output the latent pose Z0 = E(P 1:F ). The forward diffusion follows Eq. (3) and constructs the noisy latent Zt. For the backward process, we use the encoded per-frame features of given video 1:F = H(I1:F ) as the condition to guide the denoising process towards recovering the human mesh in the observed video. We keep the linear decoders for the shape and camera translation parameters but replace the pose decoder with our proposed diffusion model. Our model decodes pose parameters by analyzing the entire video as input and uses the motion prior from the first stage (VAE pretraining) to produce more accurate poses. Note that we only refine the pose parameters as previous work [23, 42] shows that inferring the shape parameter β from single image is relatively easy and leveraging diffusion model does not lead to any performance improvement. The denoiser ϵθ is transformer-based denoising model with long skip connections [1, 5] that receives noisy latent vectors Zt RNZ DZ and is conditioned on per-frame features 1:F RF DF . To add the conditions as additional input, we concatenate the feature tokens 1:F with the noisy latent pose tokens Zt. Note that the embedding size of the features (DF ) may differ from the size of the latent tokens (DZ) and linear projection layer is used to match them. The denoiser iteratively denoises the latent Zt until we get the estimated ˆZ0 and decode it using ˆP 1:F = D( ˆZ0). To train the denoiser ϵθ, we rely on v-prediction [26, 37] objective as we empirically show that it is necessary for convergence and one-step denoising (details in Sec. 4.4). More specifically, we formulate the velocity as: vt = αtϵ 1 αtZ0, (5) to estimate velocity ˆvt = and change the denoiser ϵθ(Zt, t, ) instead of the original noise. Finally, we optimize the denoiser following hybrid loss: = λ1ˆvt vt2 2 + λ2ˆϵt ϵt2 2, (6) where ˆvt is the output of the denoiser, and ˆϵt is derived from Eq. (5). For simplicity, we set λ1 = λ2 = 1 in our experiments. 4. Experiments 4.1. Datasets and Metrics Datasets. We use AMASS [28] and the training split of Human3.6M [17] and MPI-INF-3DHP [30] to train our VAE and evaluate its reconstruction task on AMASS. For the human mesh recovery task, we train the diffusion model on Human3.6M, MPI-INF-3DHP, and BEDLAM [2] and evaluate it on test set of 3DPW, and EMDB (EMDB 1) [21] datasets. More details about the datasets are provided in the supplementary material. Evaluation Metrics. To evaluate the VAE reconstruction accuracy, we compute the MPJPE using the output pose parameters and ground-truth shape parameters. For the human mesh recovery task, we compute MPJPE, Procrustedaligned MPJPE (PA-MPJPE), and Per Vertex Error (PVE) using the pose parameter estimated by our diffusion model and shape parameter estimated from CameraHMR [32] or HMR 2.0 [14], averaged over all the frames in single video clip. All the errors are reported in millimeters. 4.2. Implementation Details ECLM. We apply ECLM to the EMDB benchmark using subset of nf = 1,360 frames. With threshold of τ = 0.1 mm, the algorithm merges 6 layers in CameraHMR and 4 layers in HMR2.0. Mask-ToMe. We retain = 90 tokens and merge nl = 40 tokens across layers. As result, the Mask-ToMe operation is applied to the first = 3 layers, with the third layer merging the remaining 10 tokens. We use YOLO11xseg [18] throughout training and YOLO11n-seg throughout inference time. Diffusion Decoder. Similar to previous work [29, 50, 51, 54], we use = 243 frames to process videos. For videos larger than 243 frames, we split them into clips and for shorter videos we resample the frames to be 243 and then resample back to the original length after processing the frames. The latent size of VAE is NZ DZ = 27 512, 5 Models TCMR [7] MPS-Net [46] VIBE [22] GLoT [39] POTTER [53] HMR 2.0 [14] CameraHMR [32] WHAM [40] ScoreHMR [42] HuManiFlow [38] (M =1) HMDiff(M =25) [13] DiffMesh [52] ScoreHypo(M =1) [48] ScoreHypo(M =100) [48] s m e c i a P Throughput 3DPW (14) EMDB (24) (frame/s) PA-MPJPE MPJPE PVE Accel PA-MPJPE MPJPE PVE Accel 7.1 22.6 34.4 11.5 83.3 65.2 54.5 4.6 23.4 3.6 25.9 18.3 2.2 52.7 52.1 51.9 50.6 44.8 44.4 38.5 35.9 51.1 53.9 44.5 40.1 44.5 37. 41.2 39.1 86.5 101.4 6.0 99.0 84.3 6.5 98.4 18.5 82.9 6.0 96.4 80.7 87.4 75.0 82.2 18.1 69.8 72.9 16.8 62.1 68.7 57.8 6.6 83.1 72.7 67.8 72.4 63.0 65.8 62.2 98.6 82.4 78.4 84.6 73.4 78.9 73. 6.3 5.4 5.4 79.8 81.4 81.6 79.1 62.1 44.4 50.4 76.4 76.4 77.9 58.5 53.5 46.7 127.7 150.2 5.3 123.3 143.9 6.2 126.1 149.9 26.5 119.9 140.8 5.4 100.7 122.8 20.7 85.2 16.4 73.3 5.3 94.4 79. 113.9 133.0 112.4 131.5 99.6 87.4 91.9 104.8 3.4 3.4 82.4 71.6 FastHMR-HMR2.0 (M =1) FastHMR-CameraHMR (M =1) 150.0 103.4 Table 1. Quantitative comparison of models evaluated on the 3DPW [45] and EMDB [21] datasets. The numbers in parentheses denote the number of body joints used to calculate MPJPE and PA-MPJPE, is the number of hypotheses, indicates models trained including the 3DPW training set, and denotes results are reproduced using the provided checkpoints. Bolded values highlight the best-performing methods for each column, while underlined values indicate the second-best. All the errors are in mm. Throughput is evaluated on single NVIDIA RTX3090 GPU. equivalent with compression ratio of 2.33:1, and the motion encoder [14] encodes each frame into DF = 1024 dimensional vector. We pretrain the VAE for 1600 epochs using the Adam optimizer, learning rate of 0.0001, and batch size of 32. we train the diffusion model using the AdamW optimizer for 400 epochs, batch size of 32, and learning rate of 0.0001. We also use time reversing and time warping as data augmentation with probability of 50% throughout the diffusion training, as we found marginal improvement by including them. All the training experiments are performed using two NVIDIA RTX 6000 GPUs, and during inference single RTX 3090 GPU is used to assess the throughput. More details about the hyperparameters are provided in supplementary material. 4.3. Performance Comparison Tab. 1 compares FastHMR with other HMR methods. Although WHAM achieves the lowest error on 3DPW, it operates at only 4.6 fps due to its reliance on several heavy submodules, including ViTPose [47] for 2D keypoints, the HMR2.0 [14] transformer backbone, DPVO [43] as the SLAM module, and two RNNs. In contrast, imagebased baselines such as HMR2.0 and CameraHMR achieve much higher throughput by processing frames independently. However, this per-frame inference leads to large acceleration errors, since these models lack temporal modeling and often produce jittery predictions. Probabilistic approaches involve iterative denoising and require sampling multiple noise vectors (M > 1) at inference time, selecting the best result for evaluation. This best-sample selection is unrealistic at deployment and, combined with the repeated sampling, results in slow runtimes (2.225.9 fps) that fall well below real-time. POTTER [53] is an efficient HMR model, achieving 83.3 fps; however, it falls behind the other models in estimation error. FastHMR removes this trade-off by merging backbone layers and spatial tokens once and introducing lightweight diffusion decoder. As result, FastHMR-HMR2.0 achieves 150.0 fps (2.3 faster than HMR2.0), and FastHMRCameraHMR reaches 103.4 fps (1.9 faster than CameraHMR), while reducing acceleration error by roughly threefold and slightly improving estimation errors. The larger speedup observed with HMR2.0 is due to the fact that CameraHMR also relies on HumanFoV CNN, which FastHMR does not optimize. Unlike probabilistic models that require multiple samples and iterative denoising, FastHMR relies on velocity prediction (v-prediction) instead of noise prediction, allowing it to converge to accurate estimates with only single denoising step and single sample. Overall, FastHMR is the only method that simultaneously surpasses the real-time threshold, reduces temporal jitter, and 6 Threshold τ (mm) MPJPE (mm) # layers Training data PA-MPJPE MPJPE MVE Baseline (CameraHMR) 1.0 0.5 0.3 0.2 0.1 73.3 74.7 73.9 73.7 73.6 73.1 32 27 26 28 27 26 w/o BEDLAM w/ BEDLAM 42.4 39.1 65.5 62. 77.9 73.9 Table 4. Ablation study on the effect of adding large-scale synthetic dataset (BEDLAM) on overall performance. All the evaluations are on FastHMR-CameraHMR and 3DPW dataset. Errors are in mm. Table 2. Ablation study on MPJPE threshold τ used in ECLM. MPJPE is evaluated on EMDB benchmark. Latent size Reconst. Err (mm) Recovery Err (mm) 9256 9512 91024 27512 6.6 4.9 4.3 3. 63.1 62.7 62.3 62.2 Table 3. Ablation study on the VAE latent size. Reconst. Err and Recovery Err denote VAE Reconstruction Error (VAE pretraining) and Human Mesh Recovery Error (Diffusion tuning) evaluated by MPJPE on AMASS [28] and 3DPW [45] datasets, respectively. Both ECLM and Mask-ToMe are applied during Human Mesh Recovery and CameraHMR is used as the baseline. preserves deterministic accuracy without relying on impractical multi-sample evaluation strategies. 4.4. Ablation Study Threshold τ in ECLM. The EMDB benchmark contains NF = 24,117 frames, but processing every frame through our ECLM pipeline is computationally expenrsive. To mitigate this, we uniformly sample 80 frames from each video clip in the EMDB dataset, resulting in reduced set of nf = 1,360 frames. As shown in Tab. 2, decreasing the MPJPE threshold τ in ECLM generally leads to lower MPJPE on the EMDB benchmark, which indicates that the selected nf frames are sufficiently representative of the full benchmark. Notably, tightening the threshold does not necessarily reduce the number of merged layers. This is because ECLM uses the threshold in sliding window fashion, meaning that the decision to merge layers depends on local structure and may vary across different parts of the model. Interestingly, when τ = 0.1 mm, the MPJPE slightly improves compared to the baseline, suggesting that stricter threshold can not only preserve but also enhance accuracy. VAE Latent Size. Tab. 3 investigates the impact of VAE latent dimensionality on both reconstruction quality and downstream mesh recovery performance. As the latent size increases, the VAE reconstruction error on AMASS [28] decreases significantly, from 6.6 mm for 9 256 latent to 3.6 mm for 27 512 latent, indicating that larger latent spaces enable the VAE to capture motion details more Training objective PA-MPJPE MPJPE Noise prediction v-prediction Both 97.0 39.1 39.1 181.8 62.5 62.2 Table 5. Ablation study on the effect of training objective on performance. All results are evaluated on FastHMR-CameraHMR using the 3DPW benchmark. Errors are reported in mm. precisely. However, the recovery error on 3DPW [45] remains relatively stable around 62 mm, with only marginal improvements at higher capacities. This suggests that the diffusion-based decoder does not fully benefit from the added expressiveness of larger latents. Training on Synthetic Data. Adding the large-scale synthetic BEDLAM dataset during training markedly improves FastHMR performance. As shown in Tab. 4, compared with the model trained without BEDLAM. These consistent gains across all three metrics indicate that synthetic motion diversity complements limited real footage and significantly reduces the error without any architectural changes. Tab. 5 shows that adopting vTraining Objective. prediction as the training objective dramatically outperforms pure noise prediction, reducing MPJPE from 181.8 mm to 62.5 mm and PA-MPJPE from 97.0 mm to 39.1 mm. Combining the two objectives, shown in Eq. (6), yields nearly identical PA-MPJPE to v-prediction alone while pushing MPJPE slightly lower to 62.2 mm. These results confirm that v-prediction drives the major accuracy gains, and the mixed objective retains those benefits without degradation. Qualitative Comparison. Fig. 5 compares the baseline CameraHMR method with three components: ECLM, Mask-ToMe, and our diffusion decoder. ECLM has minimal impact on the estimated output mesh while improving inference speed and reducing memory usage. Mask-ToMe merges background tokens, which can sometimes contain information relevant to the human pose, and may introduce local shape errors. The diffusion decoder corrects these errors by leveraging its learned pose prior and temporal information from neighboring frames. More specifically, in the first row, Mask-ToMe distorts the lower legs, but the diffusion decoder restores realistic shin orientation and cor7 Method Throughput (fps) Memory (MB) MPJPE (mm) Baseline (CameraHMR) 2D Bbox Detection Camera Intrinsics Estimation Backbone CameraHMR (full pipeline) FastHMR Segmentation model Camera Intrinsics Estimation Backbone + ECLM Backbone + ECLM + Mask-ToMe Diffusion Decoder FastHMR-CameraHMR 1500.0 333.3 68.2 54.5 1000.0 333.3 83.3 176.5 8211.0 103.4 10 260 2,628 2,898 11 260 2,165 351 2,787 73.3 73.3 73.1 84.0 71.6 71. Table 6. Comparison of throughput (in frames per second), memory usage (in MB), and MPJPE (in mm) evaluated on EMDB for individual components and complete pipelines. mation. While CameraHMR benefits from global context in the background, FastHMR sees only weak body features and fails to localize the orientation. (iii) Crowded scenes with limited token capacity: the third row includes multiple people in close proximity. Since we retain only = 90 tokens, and the mask assigns many tokens to person regions, the model is forced to merge similar tokens. Some of the merged tokens belong to the subject of interest, resulting an increase in estimation error. Although this issue can be mitigated by applying refinement step to isolate the central person throughout segmentation, we omit such heuristics for simplicity. Per-Component Analysis. Tab. 6 shows that FastHMR improves both speed and accuracy compared to the baseline CameraHMR. Specifically, it increases the overall throughput from 54.6 fps to 103.4 fps and reduces the MPJPE from 73.3 mm to 71.6 mm. This efficiency gain, however, does not come with reduced memory usage. The total memory consumption of FastHMR (2,787 MB) is comparable to that of CameraHMR (2,898 MB), indicating that while FastHMR is faster and more accurate, it does not significantly reduce memory requirements. 5. Conclusion We have presented FastHMR, framework for accelerating transformer-based human mesh recovery without compromising accuracy. FastHMR employs ECLM to merge layers that have the least effect on the output and Mask-ToMe to merge spatial tokens outside the person region. To recover fine-grained detail lost during compression, one-step diffusion decoder operating in learned latent space refines initial predictions using its temporal and pose prior. Extensive evaluation on 3DPW, EMDB shows that FastHMR achieves up to 2.3 speed-up on single GPU while reducing both joint position and acceleration errors. Remaining challenges include degraded performance when segmentation masks are unreliable future work will exor background cues dominate; plore adaptive merging and joint segmentation training. Figure 5. Qualitative comparison of mesh reconstructions across FastHMR pipeline stages. Figure 6. Failure cases of FastHMR. rects the elbow position, since elbows rarely rest between the feet in seated pose. In the second row, excessive token merging pulls the left wrist too close to the torso; the diffusion decoder moves the hand back to its correct position by using its temporal context. In the third row, CameraHMR misplaces the self-occluded left hand behind the hip, whereas the diffusion decoder infers its forward swing from adjacent frames and aligns it much more closely with the ground truth. Failure Cases. Fig. 6 shows failure cases of the proposed FastHMR. (i) Background tokens with semantic cues: in the first row, although the segmentation mask correctly excludes the chair, it causes the model to classify the chair region as background. During Mask-ToMe, background tokens are aggressively merged, including the chair, which contains strong contextual cues about the human pose. As result, the model fails to reconstruct plausible seated pose. (ii) Low-light conditions: in the second row, the segmentation mask is accurate, but due to poor lighting, masking out the background removes much of the usable image infor-"
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: ViT backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 5 [2] Michael Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. BEDLAM: synthetic dataset of bodies exIn Proceedings hibiting detailed lifelike animated motion. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87268737, 2023. 5, 1 [3] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from single image. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 561578. Springer, 2016. 2 [4] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. ToarXiv preprint ken merging: Your ViT but arXiv:2210.09461, 2022. 2 faster. [5] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. 2, [6] Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh. Crossattention of disentangled modalities for 3D human mesh recovery with transformers. In European Conference on Computer Vision, pages 342359. Springer, 2022. 2 [7] Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. Beyond static features for temporally consistent 3d human pose and shape from video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19641973, 2021. 6 [8] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. MotionLCM: Real-time controllable motion generation via latent consistency model. In European Conference on Computer Vision, pages 390408. Springer, 2024. 2 [9] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. 2 [10] Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Angelica Aviles-Rivero, Chuanlong Xie, and Yao Zhu. sliding layer merging arXiv method for efficient depth-wise pruning in llms. preprint arXiv:2502.19159, 2025. 3 [11] Zhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao, Qiangqiang Wu, Weilin Wan, Taku Komura, and Wenping Wang. TORE: Token reduction for efficient human mesh recovery with transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15143 15155, 2023. [12] Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, and Michael Black. TokenHMR: Advancing human mesh recovery with tokenized pose representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13231333, 2024. 1, 2 [13] Lin Geng Foo, Jia Gong, Hossein Rahmani, and Jun Liu. Distribution-aligned diffusion for human mesh recovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 92219232, 2023. 2, 6 [14] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: ReIn constructing and tracking humans with transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1478314794, 2023. 1, 3, 4, 5, 6 [15] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. LiveCap: Real-time human performance capture from monocular video. ACM Transactions On Graphics (TOG), 38(2):117, 2019. 1 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [17] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):13251339, 2013. 5, 1 [18] Glenn Jocher, Ayush Chaurasia, Jing Qiu, et al. Ultralytics YOLO11. https://docs.ultralytics.com/ models/yolo11/, 2024. Accessed: 2025-05-25. 5 [19] Angjoo Kanazawa, Michael Black, David Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 71227131, 2018. [20] Angjoo Kanazawa, Jason Zhang, Panna Felsen, and Jitendra Malik. Learning 3D human dynamics from video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 56145623, 2019. 2 [21] Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Jose Zarate, and Otmar Hilliges. EMDB: The electromagnetic database of global In Proceedings of 3D human pose and shape in the wild. the IEEE/CVF International Conference on Computer Vision, pages 1463214643, 2023. 5, 6, 1 [22] Muhammed Kocabas, Nikos Athanasiou, and Michael Black. VIBE: Video inference for human body pose and In Proceedings of the IEEE/CVF conshape estimation. ference on computer vision and pattern recognition, pages 52535263, 2020. 2, 6 [23] Nikos Kolotouros, Georgios Pavlakos, Michael Black, and Kostas Daniilidis. Learning to reconstruct 3D human pose and shape via model-fitting in the loop. In Proceedings of the IEEE/CVF international conference on computer vision, pages 22522261, 2019. 2, 5 [24] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. HybrIK: hybrid analytical-neural inverse kinematics solution for 3D human pose and shape estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 33833393, 2021. 5, 1 9 [25] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan. CLIFF: Carrying location information in full frames into human pose and shape estimation. In European Conference on Computer Vision, pages 590606. Springer, 2022. [26] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 54045411, 2024. 3, 5 [27] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. SMPL: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. 2, 5, 1 [28] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. AMASS: Archive In Proceedings of of motion capture as surface shapes. the IEEE/CVF international conference on computer vision, pages 54425451, 2019. 5, 7, 1 [29] Soroush Mehraban, Vida Adeli, and Babak Taati. MotionAGFormer: Enhancing 3D human pose estimation with In Proceedings of the transformer-gcnformer network. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 69206930, 2024. 5 [30] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. In 2017 international conference on 3D vision (3DV), pages 506516. IEEE, 2017. 5, 1 [31] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. NeuralAnnot: Neural annotator for 3d human mesh training sets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22992307, 2022. 5 [32] Priyanka Patel and Michael Black. Camerahmr: Aligning people with perspective. arXiv preprint arXiv:2411.08128, 2024. 1, 3, 4, 5, [33] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Expressive body capture: 3d hands, Michael Black. In Proceedings of face, and body from single image. the IEEE/CVF conference on computer vision and pattern recognition, pages 1097510985, 2019. 2 [34] Mathis Petrovich, Michael Black, and Gul Varol. Actionconditioned 3d human motion synthesis with transformer In Proceedings of the IEEE/CVF International Convae. ference on Computer Vision, pages 1098510995, 2021. 5 [35] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? Advances in neural information processing systems, 34:1211612128, 2021. 3 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [37] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. [38] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. HuManiFlow: Ancestor-conditioned normalising flows on so (3) manifolds for human pose and shape distribution estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47794789, 2023. 6 [39] Xiaolong Shen, Zongxin Yang, Xiaohan Wang, Jianxin Ma, Chang Zhou, and Yi Yang. Global-to-local modeling for video-based 3D human pose and shape estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88878896, 2023. 6 [40] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. WHAM: Reconstructing world-grounded humans with acIn Proceedings of the IEEE/CVF Concurate 3D motion. ference on Computer Vision and Pattern Recognition, pages 20702080, 2024. 2, 6 [41] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 and Stefano Ermon. arXiv preprint [42] Anastasis Stathopoulos, Ligong Han, and Dimitris Metaxas. Score-guided diffusion for 3D human recovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 906915, 2024. 2, 5, 6 [43] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. Advances in Neural Information Processing Systems, 36:3903339051, 2023. 6 [44] Shilin Tian, Chase Szafranski, Ce Zheng, Fan Yao, Ahmed Louri, Chen Chen, and Hao Zheng. VITA: Vit acceleration for efficient 3D human mesh recovery via hardwarealgorithm co-design. In Proceedings of the 61st ACM/IEEE Design Automation Conference, pages 16, 2024. 2 [45] Timo Von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3D human pose in the wild using imus and moving camera. In Proceedings of the European conference on computer vision (ECCV), pages 601617, 2018. 6, 7, 1 [46] Wen-Li Wei, Jen-Chun Lin, Tyng-Luh Liu, and HongYuan Mark Liao. Capturing humans in motion: Temporalattentive 3D human pose and shape estimation from monocIn Proceedings of the IEEE/CVF Conference ular video. on Computer Vision and Pattern Recognition, pages 13211 13220, 2022. 6 [47] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. ViTPose: Simple vision transformer baselines for human pose estimation. Advances in neural information processing systems, 35:3857138584, 2022. [48] Yuan Xu, Xiaoxuan Ma, Jiajun Su, Wentao Zhu, Yu Qiao, and Yizhou Wang. ScoreHypo: Probabilistic human mesh In Proceedings of the estimation with hypothesis scoring. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 979989, 2024. 1, 2, 6 [49] Fengyuan Yang, Kerui Gu, and Angela Yao. KITRO: Refining human mesh by 2d clues and kinematic-tree rotation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10521061, 2024. 2 10 [50] Jinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, and Junsong Yuan. Mixste: Seq2seq mixed spatio-temporal encoder for 3D human pose estimation in video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1323213242, 2022. 5 [51] Qitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, and Chen Chen. PoseformerV2: Exploring frequency domain for efficient and robust 3D human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88778886, 2023. 5 [52] Ce Zheng, Xianpeng Liu, Mengyuan Liu, Tianfu Wu, GuoJun Qi, and Chen Chen. DiffMesh: motion-aware diffusion-like framework for human mesh recovery from videos. arXiv preprint arXiv:2303.13397, 2023. 2, 6 [53] Ce Zheng, Xianpeng Liu, Guo-Jun Qi, and Chen Chen. POTTER: Pooling attention transformer for efficient human mesh recovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1611 1620, 2023. 2, [54] Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne Wu, and Yizhou Wang. MotionBERT: unified perspective on learning human motion representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1508515099, 2023. 2, 5 [55] Yixuan Zhu, Ao Li, Yansong Tang, Wenliang Zhao, Jie Zhou, and Jiwen Lu. DPMesh: Exploiting diffusion prior In Proceedings of for occluded human mesh recovery. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11011110, 2024. 2 11 FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Datasets 3DPW [45] is an outdoor in-the-wild dataset containing challenging scenarios, such as walking in the city and going upstairs, with ground-truth SMPL annotations. We use the test split of 3DPW for evaluation of our model. EMDB [21] is recently-captured dataset that recoded 10 participants in 81 indoor and outdoor environments using body-worn electromagnetic (EM) sensors and provides ground-truth SMPL parameters. It has two test splits, EMDB 1 for evaluation of 3D pose and shape in camera coordinates, and EMDB 2 for global trajectory estimation. We evaluate our model on EMDB 1 test split. BEDLAM [2] is large-scale synthetic dataset containing 1 million video frames with ground-truth SMPL/SMPL-X parameters. We use the train split of BEDLAM to train our network. Human3.6M [17] is large-scale motion capture dataset captured in indoor environment. it contains 3.6 million video frames of 11 subjects performing 15 distinct actions recorded using single motion-capture system and 4 calibrated video cameras. Our network is trained using motion capture data from five subjects (S1, S5, S6, S7, and S8), with the data downsampled to 25 fps. MPI-INF-3DHP[30] is markerless dataset that spans both indoor and outdoor environments, featuring variety of camera viewpoints, clothing styles, and human poses, along with ground-truth 3D keypoint annotations. The training dataset consists of 8 subjects, each captured in 16 videos. AMASS[28] is large motion-capture dataset that unifies 15 different optical marker-based mocap datasets and represent them all using SMPL [27] parametrization. It includes over 40 hours of motion data, covering more than 300 subjects and over 11,000 distinct motions. B. Additional Ablation Studies Effect of swing-twist decomposition. HybrIK [24] introduces an inverse kinematics approach to decompose SMPL pose parameters into joint locations and twist rotations. Tab. 7 shows that this decomposition reduces MPJPE by 33 mm, demonstrating notable improvement in model performance. This enhancement can be attributed to two key factors. First, representing the pose parameters in the SO(3) space makes it challenging for the diffusion denoiser to deIn SMPLs hierarchical structure, each noise effectively. joint rotation is defined relative to its parent joint. Consequently, when denoising joint rotation like the wrist, the Data representation PA-MPJPE MPJPE MVE w/o HybrIK w/ HybrIK 58.1 37.0 95.1 62.1 109.2 72.6 Table 7. Ablation study on the effect decomposing pose parameter into joint location and twist rotation using HybrIK [24] method. All the evaluations are with single-step diffusion model on 3DPW dataset. Errors are in mm. Method MPJPE (3DPW) MPJPE (EMDB) w/ merging w/o merging 62.2 60.3 71.6 72.6 Table 8. Impact of merging (Mask-ToMe + ECLM) on model performance after introducing the diffusion decoder. The MPJPE errors are reported in mm. denoiser may also adjust the parent joint, such as the elbow, to reduce error. However, this can unintentionally alter the elbows position from the true location. Second, since the pose parameters are defined in the SO(3) space, attributes like bone length depend on the shape parameters, which we estimate using HMR 2.0 [14] and may contain errors. Bone length, however, is critical for accurately determining joint positions. By decomposing the pose parameter into joint location and wrist rotation, the model reduces its reliance on the shape parameter, allowing it to be used for other attributes, such as body mass. Using diffusion alone. We propose diffusion decoder as replacement of the naive MLP decoder used in transformerbased HMR models to enhance the robustness of model against merging methods proposed to enhance the throughput. Tab. 8 shows that removing the merging methods from FastHMR and using only the diffusion decoder reduces MPJPE by 1.9 mm on the 3DPW dataset (used for hyperparameter tuning), but increases it by 1 mm on the EMDB benchmark (an external evaluation set). This indicates that while Mask-ToMe alone slightly worsens performance likely due to the MLP being less robust to token changes. the diffusion decoder can recover this loss. Moreover, Mask-ToMe supports the diffusion decoder by merging background tokens, which improves the models generalizability. Effect of Segmentor. Tab. 9 shows the effect of using different segmentors in Mask-ToMe on throughput and estimation error. Although YOLO11x-seg slightly reduces the es1 Segmentor Throughput (fps) MPJPE (mm) Setting PA-MPJPE MPJPE MVE YOLO11n-seg YOLO11x-seg 1500 300 62.2 61.7 Table 9. Performance comparison when using different segmentors. nl Baseline 10 20 40 60 80 100 MPJPE (3DPW) MPJPE (EMDB) 62.1 74.8 74.7 73.6 73.1 74.2 80.2 73.3 85.3 85.0 84.0 83.8 85.3 93.5 Table 10. The effect of merging ratio nl in Mask-ToMe, evaluated on CameraHMR model. Figure 7. Attention map visualization of CameraHMR. timation error, it requires significantly more computational resources. As result, we use YOLO11n-seg during inference in the proposed FastHMR framework. Effect of masking ratio. In the Mask-ToMe method, we merge nl background tokens at each layer. As shown in Tab. 10, the choice of nl directly affects MPJPE on the 3DPW and EMDB benchmarks. In general, larger merging ratios increase the error. For example, nl = 100 produces the worst results. Interestingly, even smaller ratios such as 10 or 20 also lead to higher errors than expected. To understand the underlying reason, we analyzed the attention maps  (Fig. 7)  . They reveal that corner background tokens, which carry little useful information, are repurposed by the model as register tokens [9]. As the layers progress, the model gradually shifts important information into these tokens. When the merging ratio is too low, we risk merg2 w/o person mask + diffusion decoder w/ person mask + diffusion decoder 67.2 51.0 54.8 46. 98.1 75.0 84.0 71.6 112.9 84.2 96.7 82.4 Table 11. Ablation of token merging with and without person mask, and the added benefit of the diffusion decoder. Method Parameters (M) MACs (G) HMR2.0 Baseline Baseline + ECLM Baseline + ECLM + Mask-ToMe CameraHMR Baseline Baseline + ECLM Baseline + ECLM + Mask-ToMe Diffusion Decoder 670.2 591.5 591.5 737.1 619.0 619.0 29.5 122.6 107.5 52. 144.0 121.3 70.7 6.8 Table 12. Effect of proposed methods on model parameters (M) and MACs (G). ing them after they begin storing meaningful information, which could potentially reduce the accuracy of the final estimation. Excluding person mask in Mask-ToMe We use person mask to merge only the background tokens while preserving the person tokens for human mesh recovery. An alternative method, proposed in ToMe [4], merges tokens based on similarity. Table 11 shows that removing the mask increases the MPJPE by 14.1 mm. Moreover, replacing the MLP decoder with diffusion decoder does not recover this loss. These results indicate that excluding the person mask risks merging person tokens, which removes critical information for mesh recovery. Once that information is lost, even strong decoder cannot fully compensate. Parameters and MACs. Table 12 compares the impact of ECLM, Mask-ToMe, and the diffusion decoder on model complexity. For both HMR2.0 and CameraHMR, incorporating ECLM reduces parameters and multiplyaccumulate operations (MACs), and combining it with Mask-ToMe yields substantial computational savings. The diffusion decoder, shown separately, is lightweight module with relatively few parameters and MACs, highlighting its efficiency compared to the backbone models. In-the-wild Evaluation. Fig. 8 shows qualitative results of FastHMR on challenging in-the-wild examples, including dynamic actions (jumping, weightlifting, climbing) and complex body articulations. Across diverse settings, our method reconstructs plausible and temporally consistent human meshes, even under fast motion, self-occlusion, and Figure 9. Qualitative comparison between CameraHMR and FastHMR-CameraHMR. Hyper-parameter Value Type condition dim embedding dim flip sin to cos # frames # encoded framed frequency shift # heads Feedforward dim Dropout Activation Normalize before # Layers Transformer Encoder 1024 512 True 243 27 0 4 1024 0.001 GELU False 5 Table 14. Denoiser Hyperparameters. Hyper-parameter Value input dim latent dim feedforward dim # layers # heads dropout activation positional embedding 243 133 27 512 1024 9 4 0.1 GELU learned Figure 8. In-the-wild video evaluation of FastHMR Hyper-parameter Value # steps βstart βend sheduler clip sample variance type 1000 0.00085 0.012 scaled linear False fixed small Table 13. Diffusion Hyperparameters. cluttered backgrounds, highlighting its robustness beyond standard benchmarks. Table 15. VAE Hyperparameters. D. Additional Qualitative Comparison C. Additional Hyperparameters Tables 13, 14, and 15 denote the hyperparameters used during diffusion training, denoiser configuration, and VAE pretraining, respectively. Fig. 9 compares CameraHMR and FastHMR-CameraHMR across four different frames. Although the results may appear similar depending on the camera viewpoint, CameraHMR is more likely to produce erroneous estimates for occluded body parts, whereas FastHMR demonstrates greater robustness due to its temporal awareness."
        }
    ],
    "affiliations": [
        "KITE Research Institute, UHN",
        "University of Toronto",
        "Vector Institute"
    ]
}