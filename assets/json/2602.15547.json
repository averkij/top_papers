{
    "paper_title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
    "authors": [
        "Mohammad Kalim Akram",
        "Saba Sturua",
        "Nastia Havriushenko",
        "Quentin Herreros",
        "Michael Günther",
        "Maximilian Werk",
        "Han Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development."
        },
        {
            "title": "Start",
            "content": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation Mohammad Kalim Akram, Saba Sturua, Nastia Havriushenko, Quentin Herreros, Michael Günther, Maximilian Werk, Han Xiao Jina by Elastic research@jina.ai 6 2 0 2 7 1 ] . [ 1 7 4 5 5 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with singleor multi-stage processes using contrastive loss functions. We introduce novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, highperformance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of jina-embeddings-v5-text similar size. models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development."
        },
        {
            "title": "Introduction",
            "content": "Information retrieval (IR) systems increasingly rely on text embedding models as first-stage retrievers, replacing or augmenting traditional methods. These models map queries and documents into shared dense vector space, enabling efficient retrieval via nearest-neighbor search. These dense embeddings see use in wide array of IR applications, including web search, question-answering, and retrievalaugmented generation, as well as other purposes like recommendation systems, clustering, classification and quantification of semantic similarity. The prevailing architecture for embedding models is transformer architecture augmented with pooling layer, first introduced for SentenceBERT [Reimers and Gurevych, 2019]. Recent *Equal contribution. models, like Qwen3Embeddings [Zhang et al., 2025a] and Embedding-Gemma [Vera et al., 2025], are trained using contrastive learning. Alternatively, knowledge distillation provides an efficient mechanism for training small models to mimic the behavior of one or more teacher models, as exemplified by the Jasper model [Zhang et al., 2024a]. This work combines model distillation with taskspecific contrastive loss training, demonstrating that (1) distillation outperforms naive contrastive training, (2) our combined approach leads to further improvements compared to pure distillation-based approach, and (3) the resulting models perform on the MTEB benchmarks [Enevoldsen et al.] on-par with or better than recent models with comparable sizes. Specifically, this works contributions are: Training Method: We introduce new training method that combines distillation with task-specific, specialized training objectives Empirical Analysis of Distillation Methods: We present comparative analysis of different distillation methods for embedding models. Model Release: We have released the resulting model weights to the public1 in order to foster advances in the field."
        },
        {
            "title": "2 Related Work",
            "content": "Related work spans work about distilling language models in general, research into distillation specifically for embedding models, and contrastive multi-task learning."
        },
        {
            "title": "2.1 Language Model Distillation",
            "content": "Model distillation is an approach to creating compact language models that has been used to create models like DistilBERT [Sanh et al., 2019]. 1https://huggingface.co/collections/jinaai/ jina-embeddings-v5-text Distillation uses specialized loss functions to align student model with teacher. For DistilBERT, this means one function to align their outputs, and one to align the hidden layers using cosine loss. Alternatively, MiniLM models [Wang et al., 2020] are distilled by mimicking the self-attention behavior of the parent model. TinyBERT [Jiao et al., 2020] uses pre-trained version of BERT during pre-training and fine-tuned version for fine-tuning. Chen et al. [2021] follow up on this work by developing reranker model using the same technique with additional labeled data."
        },
        {
            "title": "2.2 Embedding Model Distillation",
            "content": "Early approaches [Hofstätter et al., 2020, Menon et al., 2022] to the distillation of embedding models focused on aligning new models with the similarity scores of teacher models. Kim et al. [2023] employ projection layer to align teacher and student embedding spaces and perform distillation on the embeddings directly. Yang et al. [2024], Musacchio et al. [2025] train cross-lingual dense retrieval models using machine translation. Zhang et al. [2024a] introduce techniques for multi-teacher distillation, using both embedding alignment and score-based distillation methods, applied over multiple training stages. Formont et al. add Gaussian kernel-based loss component for multi-teacher distillation. This appears to improve performance for embedding-based distillation with projection layer setup. Also, Zhang et al. [2025b] recently proposed an approach that consists of distillation and contrastive training stage. Unlike our method, it only fine-tunes an existing embedding model and does not address differences in optimization methods for different task types."
        },
        {
            "title": "2.3 Task-Specific Embedding Training",
            "content": "Researchers have also proposed variety of techniques to train embedding models to jointly optimize for different tasks and thereby resolve task conflicts. Joint optimization to support multiple target domains commonly involves combinations of loss functions [Wang et al., 2014, Chen et al., 2024] or varying the training objective during training [Mohr et al., 2024]. Additionally, generating multiple models via task-specific fine-tuning and then merging their weights using model soup methods has proven productive [Vera et al., 2025]. Instruction tuning has been proposed to resolve task conflicts in both text [Su et al., 2023] and Figure 1: Architecture of jina-embeddings-v5-text. Table 1: Attributes of the Base Models and the Resulting Embedding Models Model Name Parameters RoPE Max. Emb. Base LoRA θ Tokens Dim. j-v5-text-small 596M 420.2M 3.5M 32K 1024 j-v5-text-nano 212M 46.7M 1M 32K Base Models Qwen3-0.6B EuroBERT-210M 600M 210M 1M 32K 1024 768 8K 250K image [Zhang et al., 2024b] retrieval models. Instructions enable fine-grained manual adjustments to improve embedding performance for specific domains and task types. However, achieving strong performance with hand-crafted instructions requires additional labeling effort from practitioners. Alternatively, LoRA adapters allow task-specific adaptations to be trained independently and have also been shown to resolve task conflicts effectively [Sturua et al., 2025]."
        },
        {
            "title": "3 Model Architecture",
            "content": "Figure 1 displays the model architecture. It is transformer model that closely follows the schema of other pre-trained language models [Boizard et al., 2025, Yang et al., 2025]. The model translates text input into single embedding via last-token pooling, i.e., it uses the embedding of the end-of-sequence token produced by the transformer layers. Following the approach of Sturua et al. [2025], the model includes LoRA adapters to support multiple tasks that are difficult to optimize for retrieval, semantic jointly. similarity, clustering, and classification. Adapters are loaded together with the model weights, and users select the appropriate one at inference time. These tasks are: To asymmetric retrieval, support jina-embeddings-v5-text distinguishes between query and document inputs by pre-pending prefix to the input text either \"Query:\" or \"Document:\". Other tasks use single \"Document:\" prefix. Embeddings can also be truncated for downstream efficiency, enabled by using Matryoshka Representation Learning during training [Kusupati et al., 2022]. Table 1 summarizes the attributes of both embedding models and their underlying backbone models."
        },
        {
            "title": "4 Training",
            "content": "For training our embedding models we use the pretrained language models EuroBERT-210m [Boizard et al., 2025] for jina-embeddings-v5-text-nano and Qwen3-0.6B-Base [Yang et al., 2025] for jina-embeddings-v5-text-small (see Table 1). Both models are multilingual.2 Our training method consists of two main stages: much larger, Embedding Distillation: We use distillation to transfer knowledge from Qwen3-Embedding-4B model3, trained embedding model [Zhang et al., 2025a]. The goal is to enable small model to approximate the performance of the larger model without requiring instruction-style prompts or other prompt engineering for embedding generation. Task-Specific Adapter Training: In this stage, we freeze the model weights and train LoRA adapters for better performance in broad task categories: retrieval, semantic similarity, clustering, and classification."
        },
        {
            "title": "4.1 First-Stage: Embedding Distillation",
            "content": "Distillation requires student model, teacher model, and training data for both to process. Our training data consists of text pairs (q, d) that consist of text that functions as query and one that functions as document to retrieve d, e.g., title-abstract and question-answer pairs. The Qwen3 teacher model has been trained to follow instructions when generating embeddings, enabling users to provide relevant extra-textual 2EuroBERTs training focuses on 15 major European and global languages: English, French, German, Spanish, Chinese, Italian, Russian, Polish, Portuguese, Japanese, Vietnamese, Dutch, Arabic, Turkish, and Hindi. It also includes some materials in other languages. Qwen3-0.6B-Base lists 119 languages: https://qwen.ai/blog?id=qwen3 (Last Access: 01/27/2026). 3https://huggingface.co/Qwen/Qwen3-Embedding4B (Last Access: 01/27/2026) information, like whether an embedding is to be used as query or document, or domain-relevant information like that text is scientific abstract or encyclopedia entry. This enables the model to position the embedding better in its semantic space and improves task performance. However, it leads to ambiguity when we do not know what instructions are empirically most useful and makes it harder for us to transfer knowledge through distillation. Therefore, we make only minimal use of instructions during distillation. For the student, we only provide generic query/document prefixes (described in Section 4.2.1), and for the teacher, the general instruction: Given web search query, retrieve relevant passages that answer the query, which is provided as default in its sentence transformer configuration."
        },
        {
            "title": "4.1.1 Positional Information",
            "content": "We use rotary positional embeddings (RoPE) [Su et al., 2024] to inject positional information during attention calculation. This technique uses rotation matrices and parameter θ, which controls the rotation frequencies. Using higher θ at inference time and lower one during training has been shown to improve performance on texts that are longer than those seen during training [Zhang et al., 2024c, Liu et al., 2024]. Since our training data consists of relatively short texts, but we want the models to perform well on long ones, we train with much smaller θ values, as seen in Table A1, than the ones we use at inference time, as shown in Table 1."
        },
        {
            "title": "4.1.2 Loss Function",
            "content": "At each training step, we apply the student/teacher model to batch of pairs (q, d), resulting in two batches of embeddings: BS = {(xS ,yS )}B i=1, xS ,yS Rn and BT = {(xT ,yT )}B i=1, xT ,yT Rm The dimensionality of the teacher embeddings is higher than the dimensionality of the student embeddings n. We use linear projection layer ψ : Rn Rm, ψ(z) = z+b to project the student embeddings into the teachers embedding space, enabling us to use cosine similarity ϕ to determine similarity 4https://huggingface.co/Qwen/Qwen3-Embedding4B/blob/main/config_sentence_transformers.json (Last Access: 02/13/2026) scores. Our distillation loss Ldistill is sum of cosine distances between the two sets of embeddings: Ldistill = (cid:32) (cid:88) (cid:88) i=1 z{x,y} (cid:2)1ϕ(cid:0)ψ(zS ), zT (cid:1)(cid:3) (cid:33) (1) Theoretically, it is possible to project the teacher embeddings to the dimensionality of the student embeddings instead. However, we found that this is less effective, as shown in Section 5.3.2."
        },
        {
            "title": "4.1.3 Training Procedure",
            "content": "Distillation proceeds in two phases: General-Purpose Training: First, we performed training using large, diverse collection of text pairs, drawn from over 300 datasets in over 30 languages. Training is conducted for 50,000 steps with the hyperparameters documented in Table A1. Long Context Training: General-purpose training for jina-embeddings-v5-text-small produced unsatisfactory performance on long documents, as shown in Table A18, and we undertook further training on that model to improve long sequence embeddings quality. This training incorporated curated collection of materials, including synthetic documents designed to retrieve documents based on specific contents embedded in long, highdensity, noisy texts. It also contained natural long texts, such as book chapters and long-form articles, paired with LLM-generated queries. This dataset includes multilingual document-query pairs with texts of 1,000 to 4096 tokens, ensuring that long document performance is robust across languages. We also lowered the θ parameter of the positional embeddings and increased the maximum sequence length. That facilitates smoother interpolation of frequencies across the extended context window, leading to better performance on long texts. Detailed hyperparameter configurations are stated in Table A1."
        },
        {
            "title": "4.2 Second-Stage: Task-Specific Adapters",
            "content": "We froze the weights in the distillation-trained model to train the LoRA adapters for specific tasks. For each task category, we have separate adapter. This avoids problems with conflicting optimization objectives. In this second stage of training, we used different loss functions and training data for each adapter. We also re-used the projection layer weights trained in the first stage."
        },
        {
            "title": "4.2.1 Asymmetric Retrieval Adapter\nAsymmetric retrieval is based on the insight that\nqueries and retrieval targets are usually very dif-\nferent from each other. Queries are almost always\nmuch shorter than the document they’re matched\nto, and are often worded differently, or use different\nsyntax, like question answering. Consequently,\nencoding queries and documents differently can\nyield large improvements in retrieval.",
            "content": "We implement this asymmetry with prefixes, specifically by pre-pending \"Query:\" to inputs intended to be used as queries and \"Document:\" to texts intended to be retrieval targets (see Section 3). Training data for this adapter consists of triplet datasets containing queries, relevant documents, and hard negatives, as well as the long-context datasets described in Section 4.1.3. For texts in the long-context datasets, the maximum sequence length and batch size were adjusted dynamically in each training step, depending on which dataset was sampled. Detailed hyperparameter values are provided in Appendix A1. We also use combination of three loss functions: i=1 Contrastive Loss: We use InfoNCE loss [Oord et al., 2018] with hard negatives [Karpukhin et al., 2020]. Given batch of size B, let = {xi}B denote the query embeddings and = {yi}B i=1 their corresponding relevant document embeddings. For each query embedding xi, we define negative set Nxi consisting of all non-matching in-batch document embeddings and additional mined hard negatives, i.e., semantically related but incorrect documents. Based on the the temperature-scaled exponential cosine similarity S(x,y) = exp(ϕ(x,y)/τ ), the contrastive loss is defined as follows: Lqd NCE ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) i=1 log S(xi,yi) S(xi,yi)+ (cid:80) nNxi S(xi,n) (2) where τ is learnable temperature parameter. Distillation Loss: We retain the same knowledge distillation loss used during the first stage of training (Equation (1)), ensuring that the retrieval adapter preserves the general-purpose embedding quality established by the base model. Spread-Out Regularizer Following Vera et al. [2025], we apply global orthogonal regularizer (GOR) [Zhang et al., 2017] that encourages embeddings to be distributed more uniformly improving their across the embedding space, expressive capacity. This also improves robustness to quantization and enables more efficient retrieval under approximate nearest neighbor (ANN) search. The GOR loss is defined as: LGOR ="
        },
        {
            "title": "1\nB(B −1)",
            "content": "(q qj)2 (cid:88) i,jB i=j +"
        },
        {
            "title": "1\nB(B −1)",
            "content": "(cid:88) i,jB i=j (p+ p+ )2 STS data is very limited in volume, so we supplemented the training data with text pairs drawn from parallel translations and paired paraphrases of texts. CoSENT Ranking Loss: For batch {(xi,yi,si)}B i=1 of training triplets, where xi,yi Rd are embeddings of two text inputs and si is their ground-truth semantic similarity score. we optimize the following ranking-based objective: (3) (cid:34) Lco = ln 1+ (cid:88) i,j{1,...,B} si>sj eϕ(xj ,yj ) eϕ(xi,yi) τ (cid:35) (5) where qi and p+ ument embeddings, respectively. denote the query and positive docThis loss penalizes high pairwise similarity between non-matching embeddings, driving them to behave as if uniformly sampled from the unit sphere. Combined Objective: The final training objective for the retrieval adapter is linear combination of the three loss functions: This loss function ensures that embedding pairs with higher ground-truth similarity tend to receive higher similarity scores than less ground-truth similarity. By aggregating ranking constraints across the batch, it performs listwise optimization that aligns model-predicted similarities with the ground-truth ordering indicated by human-provided scores. The temperature parameter τ > 0 controls the smoothness of the objective. Lretrieval = λNCELqd NCE +λDLdistill +λSLGOR (4) where λNCE, λD, and λS are scalar weights balancing the three objectives. The final LoRA adapter averages the weights of the last training checkpoint with an earlier checkpoint, employing model averaging to improve performance and robustness."
        },
        {
            "title": "4.2.2 Text Matching (STS) Adapter",
            "content": "We designed the text-matching adapter for semantic text similarity (STS) tasks, i.e., tasks where both text inputs are treated symmetrically, unlike asymmetric retrieval. This makes the adapter ideal for use cases like duplicate detection, paraphrase identification, or quantifying the similarity of documents in general. To achieve better symmetric encoding, this adapter uses only the \"Document:\" prefix during training and inference. Accurately capturing semantic similarity requires training data with graded annotations, for which we used STS12 [Agirre et al., 2012], SICK [Marelli et al., 2014], and similar datasets. Our training data is multilingual, including English, German, Spanish, French, and Japanese, among others. For less-resourced languages, we have relied on machine-translated versions of existing graded annotated datasets. High-quality human-annotated Combined Objective and Distillation: To optimize the adapter, we employ hybrid strategy. During each training step, batch is sampled that either contains annotated from dataset similarity scores or pairs or triplets without scores. If scores are available, we use the CoSENT loss Lco described above. If the dataset contains unscored pairs and triplets, we use combination of InfoNCE loss Lqd NCE and the knowledge distillation loss Ldistill as described in Section 4.1.2: Lsts = Lco, λNCELqd NCE +λDLdistill, if has scores otherwise (6) For unranked pairs or triplets, we set the weight ratio λnce : λd to 1 : 2. This makes sure that the adapter preserves the high-quality semantic features of the teacher model while learning to do symmetric matching. For parallel datasets lacking explicit negatives, we use in-batch negatives. This switching logic allows the model to benefit from the precision of human-annotated scores while remaining robust through large-scale distillation and contrastive learning."
        },
        {
            "title": "4.2.3 Clustering Adapter\nWhile retrieval tasks require distinguishing doc-\numents that are relevant from documents that are",
            "content": "only related to query, clustering tasks require an embedding model to group related documents near each other. This use is different enough to merit separate adapter for this task. As documented in Section 4.1, the initial distillation training stage uses generic instruction for the teacher model. We found this to be distinctly suboptimal for clustering tasks. (See Table A15). To solve this problem, we did new distillation training, following the approach in Section 4.1 and using the distillation loss in Equation (1), but with clustering-specific instruction for the teacher model: Identify the topic or theme of the given document:. We trained on pairs of texts derived from sources that are typically used for clustering tasks, e.g., titles and descriptions of news articles. All texts receive the prefix Document: when presented to the the student model. We detail the hyperparameters in Table A1."
        },
        {
            "title": "4.2.4 Classification Adapter",
            "content": "Classification is common use case for embeddings, encompassing document categorization, sentiment analysis, intent recognition, and recommendation systems. This can involve embeddings that encode fine-grained semantic information. Our training data comprises standard classification datasets, including multilabel data, which we converted to single-label format. All datasets consist of text-label pairs, which we transformed into triplet format: each sample includes one \"anchor\", one \"positive\" item that shares the same label as the anchor, and seven \"negative\" items with different labels. Random selection determined which items from the labeled dataset were deemed anchors, positives, and negatives. For both jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, we used the contrastive loss from Equation (2). To adapt it for supervised learning, we use pairs (q,p) of an anchor text and randomly selected target with the same label. We optimize with bi-directional loss function that aligns the representations: = Lqd NCE +Ldq NCE (7) For Lqd NCE , the set Nxi includes all other positives NCE uses and negatives in the batch. In contrast, Ldq only in-batch negatives. We also added relational knowledge distillation regularizer [Park et al., 2019] Lr to prevent feature collapse and enhance the classifier adapters zero-shot abilities. The teacher model for this regularization is the base model without the adapter. Lr = (cid:88) i,j="
        },
        {
            "title": "1\nM 2",
            "content": "(cid:18) 1ϕ(si,sj) µS 1ϕ(ti,tj) µT (cid:19)2 (8) where s, are embeddings from the set of all anchors, positives, and negatives; is the total number of embeddings (batch size 9); and µ is the scalar mean values of the student and teacher distance matrices. The loss and the regularizer were respectively scaled by weights λNCE and λR. Hyperparameters are described in Table A1."
        },
        {
            "title": "5 Evaluation",
            "content": "To evaluate our two new models, we apply variety of embedding evaluation benchmarks to our models, as well as to selection of comparable models, in order to provide baseline for comparison. Where evaluation results for those models are reported elsewhere, we took those values instead of redoing all benchmarks. For general embedding evaluation, we relied on the English MTEB benchmark [Muennighoff et al., 2023] and its multilingual version [Enevoldsen et al.], with results summarized in Section 5.1. We also conducted more extensive evaluation of retrieval performance with additional benchmarks outlined in Section 5.2. To investigate the effects of our novel design choices during the training, we performed ablation studies described in Section 5.3, and we tested the robustness of embeddings under truncation in Section 5.4. For comparison, we primarily focus on state-ofthe-art multilingual models with similar parameter counts to our models, specifically: jina-embeddings-v3 (jina-v3) [Sturua et al., 2025] snowflake-arctic-embed-l-v2 (snowflake-l-v2) [Yu et al., 2024] multilingual-e5-large-instruct (mult.-e5-l-instr.) [Wang et al., 2024] KaLM-embedding-multilingual-miniinstruct-v2.5 (KaLM-mini-v2.5) et al., 2025] [Zhao voyage4-nano [Voyage AI, 2026] embeddinggemma-300m (Gemma-300M) [Vera et al., 2025] Qwen3-Embedding-0.6B [Zhang et al., 2025a] (Qwen3-0.6B) that"
        },
        {
            "title": "Note",
            "content": "Qwen3-Embedding-0.6B has been trained on the same backbone model as jina-embeddings-v5-text-small. To determine the influence of instructiontuning on the performance, we distinguish between Qwen3-0.6B (instr.) and Qwen3-0.6B (generic). The generic version of the model uses one prefix for each category only, i.e. retrieval, clustering, etc., while the instruction version has an individualized instruction for each dataset. We also provide comparisons reference our teacher to two much larger models: (Qwen3-4B) Qwen3-Embedding-4B model [Zhang et al., 2025a], and our previous model jina-embeddings-v4 (jina-v4) [Günther et al., 2025]. Scores published here come from the relevant MTEB learderboards5 or our own evaluation if not published elsewhere. retrieval tasks were evaluated using nDCG@10, except for Passkey and Needle, which used nDCG@1. For semantic textual similarity (STS) and summarization tasks, we calculated the Spearman correlation coefficient. For clustering tasks, we used the V-measure6 to evaluate the quality of the embeddings. Classification and reranking tasks were evaluated using accuracy and precision metrics."
        },
        {
            "title": "5.1 Performance on MTEB Benchmarks",
            "content": "results on (MMTEB) the multibenchmark jina-embeddings-v5-text-small Table shows 2 lingual MTEB for (j-v5-text-small), jina-embeddings-v5-text-nano (j-v5-text-nano) and other multilingual models. Scores for individual tasks appear in Appendix A.3. both jina-embeddings-v5-text models achieve the highest average scores in their size category. The Qwen3-4B model, which we used as the teacher model, still significantly outperforms our models, but it has more than five times as many paramjina-embeddings-v5-text-small eters"
        },
        {
            "title": "Compared to other",
            "content": "small models, as 5https://huggingface.co/spaces/mteb/ leaderboard (Last Access: 02/09/2026) 6Specifically, the scikit-learn implementation [Pedregosa et al., 2011]: the harmonic mean of homogeneity and completeness, = 2hc h+c . Homogeneity measures cluster purity (each cluster contains mostly one true class), while completeness measures class concentration (each true class is mostly assigned to single cluster). Figure 2: Performance of j-v5-text-small on different languages on MMTEB compared to other models as as times many sixteen and jina-embeddings-v5-text-nano. KaLM-mini-v2.5 achieves results on clustering tasks than our models, and Voyage-4-nano has been narrowly trained to focus on retrieval, and has slightly higher benchmark performance than jina-embeddings-v5-text-nano in that one category. slightly better Qwen3-0.6B and Gemma-300M also have generally good average MMTEB scores. Our evaluation of Qwen3-0.6B (generic) with only one instruction defined individually for each task category shows that performance is generally higher when task-level instructions are used, with the exception of reranking tasks. The differences are most pronounced for classification tasks and less significant for other task categories. Note that for STS, pair classification, and bitext mining, Qwen does not define task-specific instructions at the individual task level, accordingly, the scores are identical. Table 2 does not provide insight into languagespecific differences in performance, so we conducted separate analyses, calculating average scores for individual languages, for five small multilingual models with published scores for all MMTEB tasks: Gemma-300M Qwen3-0.6B (instr.) BGE-M3 [Chen et al., 2024] jina-embeddings-v5-text-small jina-embeddings-v5-text-nano Table 2: MTEB (Multilingual, v2) Evaluation Results Model Qwen3-4B jina-v4 Params Dim Avg Tasks Avg Type BM Cls Clu IR MLC Pair RR Ret STS 4B 2560 3.8B 2048 69.5 58.17 60.9 51.55 79.4 72.3 57.1 11.6 26.8 85.1 69.6 80.9 62.4 55.2 44.0 0.7 19.3 79.3 62.20 66.4 74.4 65. Qwen3-0.6B (instr.) 596M 1024 Qwen3-0.6B (generic) 596M 1024 572M 1024 jina-v3 568M 1024 snowflake-l-v2 560M 1024 mult.-e5-l-instr. j-v5-text-small 677M 1024 KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano 494M 896 480M 2048 308M 768 239M 768 64.3 61.1 58.4 57.0 63.2 67.0 60.1 58.9 61.1 65.5 56.0 54.3 50.7 50.0 55.1 58. 52.4 52.0 54.3 57.7 5.1 61.4 24.6 80.8 72.2 66.8 52.3 64.7 76.2 72.2 58.4 49.8 3.8 21.1 80.8 62.2 64.2 76.2 55.8 77.1 65.3 58.8 45.6 -1.3 18.4 79.3 58.4 70.1 64.1 57.4 42.8 -2.5 18.9 76.7 80.1 64.9 50.8 -0.4 22.9 80.9 57.1 76.8 64.9 78.9 42.0 82.9 69.7 71.3 53.4 57.1 63.7 62.6 65. 1.3 65.0 61.2 53.8 -0.6 21.0 79.1 62.4 57.9 71.9 64.1 58.6 45.4 3.5 20.1 76.3 63.1 63.6 73.0 62.5 74.7 64.4 60.9 51.2 63.3 78.2 67.7 69.2 52.7 24.8 81.4 41.3 81.9 63.2 64.6 5.6 0.0 Task Abbreviations: Avg Tasks: Average (Task), Avg Type: Average (Task Type), BM: Bitext Mining, Cls: Classification, Clu: Clustering, IR: Instruction Reranking, MLC: Multilabel Classification, Pair: Pair Classification, RR: Reranking, Ret: Retrieval, STS: Semantic Textual Similarity (partially) self-evaluated Figure 2 presents the average scores per language for jina-embeddings-v5-text-small as heat map, with colors them based on its performance compared to the other four models.7 Appendix A.6 contains heat maps for all five models. Table 3 presents the English MTEB benchmark results for all included models. For results on individual tasks, see Appendix A.2. Here, jina-embeddings-v5-text-small achieves the highest average score among the small multilingual models, but lower score than Qwen3-4B. When examining specific task categories, Qwen3-0.6B achieves slightly better retrieval performance when used with instructions, and multilingual-e5-large-instruct obtains the best results on pair classification tasks. Using Qwen3-0.6B without individual instructions for each task leads to similar loss of performance for English benchmarks as was observed for MMTEB. achieves"
        },
        {
            "title": "Among models with fewer",
            "content": "than 500M KaLM-mini-v2.5 parameters, the highest average scores, only slightly better than jina-embeddings-v5-text-nano, despite having more than twice as many parameters. jina-embeddings-v5-text-nano achieves higher performance than all other models under 0.5B parameters in retrieval, reranking, and STS tasks. We note that Gemma-300M has the highest overall performance on summarization."
        },
        {
            "title": "5.2 Performance",
            "content": "on Various Retrieval Benchmarks To provide more global view of model performance, we used three additional benchmarks: RTEB (Multilingual)8 [Liu et al., 2025], BeIR [Thakur et al.], and LongEmbed [Zhu et al., 2024]. We summarize the results together with the retrieval scores on the MTEB benchmarks from Section 5.1 in Table 4. Detailed results for individual datasets are presented in Appendix A.4 In contrast to the MTEB retrieval benchmarks, BeIR contains very large English datasets, demonstrating the models performance on million document-scale corpora. LongEmbed contains tests on relatively long documents when most benchmarks only contain passages. RTEBs tests emphasize model performance on enterprise use cases. task-level average across all jina-embeddings-v5-text-small achieves rethe highest trieval benchmarks among the models tested, outperforming comparably-sized Qwen3-0.6B on three out of five benchmarks. Qwen3-0.6B enjoys stronger scores on MTEB English and LongEmbed, suggesting that it has an advantage on English and long-document retrieval tasks. Both jina-embeddings-v5-text models substantially outperform jina-v3, snowflake-L-v2, e5-large-instruct. and Among models with under 500M paramemultilingual 7Specifically, the color space is mapped to the interval µ3σ for each individual language. 8This benchmark contains mixture of publicly-available tasks and additional private tasks. These scores here refer to only the public tasks because we do not have access to the private ones. Table 3: MTEB(eng, v2) Evaluation Results Model Qwen3-4B jina-v Params Dim Avg Tasks Avg Type Cls Clu Pair RR Ret STS Sum 4B 2560 3.8B 2048 74.6 65.09 68.1 60.68 89.8 57.5 87.0 68.5 88.7 34.4 50.8 74.1 45.5 83.1 48.04 56.2 85.9 32.0 Qwen3-0.6B (instr.) 596M 1024 Qwen3-0.6B (generic) 596M 1024 572M 1024 jina-v3 568M 1024 snowflake-l-v2 560M 1024 mult.-e5-l-instr. j-v5-text-small 677M KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano 494M 896 480M 2048 308M 768 239M 768 70.5 67.0 65.7 63.6 65.5 71.7 71.3 63.3 69.7 71.0 64.7 62.0 62.6 59.8 61.2 65.6 65.3 58.8 65.1 65. 61.8 86.6 33.4 84.6 54.1 84.4 48.2 72.0 51.8 84.4 46.2 59.8 86.6 33.4 54.3 85.8 32.9 47.9 85.8 47.4 84.0 58.6 78.1 33.8 47.5 73.4 44.4 83.0 75.5 49.9 86.2 53.5 84.7 29.9 48.7 60.1 88.1 31.8 49.4 90.4 54.7 85.0 90.5 58.1 86.6 58.5 84.8 31.2 47.4 73.9 46.9 83.0 47.7 52.3 81.6 26.2 55.7 83.6 37.6 47.4 87.6 56.6 87.3 58.8 88.3 31.9 49.2 89.7 53.5 84.7 Task Abbreviations: Avg Tasks: Average (Task), Avg Type: Average (Task Type), Cls: Classification, Clu: Clustering, Pair: Pair Classification, RR: Reranking, Ret: Retrieval, STS: Semantic Textual Similarity, Sum: Summarization (partially) self-evaluated Table 4: Retrieval Benchmark Results Model Qwen3-4B jina-v Qwen3-0.6B jina-v3 snowflake-l-v2 mult.-e5-l-instr. j-v5-text-small KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano Params Dim Avg Tasks MTEB-M MTEB-E RTEB BEIR Long 4B 3.8B 2560 2048 596M 1024 572M 1024 568M 1024 560M 1024 677M 494M 896 480M 2048 308M 768 239M 768 67.95 63.62 61.87 56.11 57.59 54.22 63.28 56.58 61.48 59.66 61.43 69.60 66.43 64.65 55.76 58.36 57.12 64. 57.90 63.58 62.49 63.26 68.46 56.15 61.83 54.29 58.56 53.47 60.07 58.45 52.30 55.69 58.80 70.77 61.58 78.82 66.52 53.97 69.88 64.21 55.52 72.20 54.58 53.17 55.67 63.74 55.22 53.95 41.76 52.74 54.78 56.67 66.84 66. 56.51 55.00 43.35 70.36 49.93 74.93 63.75 53.69 55.29 56.06 63.65 64.08 Task Abbreviations: Avg Tasks: Task-level mean across benchmarks, MTEB-M: MTEB Multilingual v2, MTEB-E: MTEB English v2, RTEB: RTEB (Multilingual, Public), BEIR: BEIR Retrieval, Long: LongEmbed (partially) self-evaluated voyage-4-nano is ters, jina-embeddings-v5-text-nano achieves the best BEIR and MTEB English scores while being the smallest model tested. Voyage-4-nano has slightly higher task-level average than jina-embeddings-v5-text-nanoand significantly higher scores on RTEB and LongEmbed. roughly twice However, the size of j-v5-text-nano and has an embedding dimensionality of 2048 compared jina-embeddings-v5-text-nanos to 768. Gemma-300M and KaLM-mini-v2.5 also achieve competitive results on individual benchmarks but fall behind for the overall average across benchmarks. The Qwen3-4B teacher model unsurprisingly achieves the best results across all benchmarks by considerable margin."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "We analyzed the effect of key design choices in our training setup through ablation testing. We focused on several factors that directly influence retrieval performance. Section 5.3.1 describes empirical studies on different distillation strategies and Section 5.3.2 studies the role of student and teacher projections for aligning embedding spaces during distillation. Furthermore, in Section 5.3.3, we investigate the influence of the three loss components used to train the retrieval adapter, and in Section 5.3.4 how GOR regularization makes the model more robust towards binary quantization."
        },
        {
            "title": "5.3.1 Comparison of Training Objectives",
            "content": "We studied the impact of different training objectives on retrieval performance by comparing Figure 3: Performance comparison of different training objectives. Average nDCG@10 on the MTEB (English, v2) benchmark for S2ORC (left) and the full training data mixture (right). Figure 4: Comparison of projection configurations on S2ORC. Performance is measured by average nDCG@10 on MTEB (English, v2). three distinct loss functions: InfoNCE Lqd NCE (see Equation (2)), embedding-based distillation Ldistill (see Equation (1)), and score-based distillation Lscore. All models are evaluated on the MTEB English v2 retrieval benchmark, with nDCG@10 reported across training steps. Score-based distillation loss: As an alternative to direct embedding alignment with Ldistill, we evaluated score-based distillation loss that aims to match the distribution of pairwise similarities produced by the teacher and student models. Specifically, we compute the Mean Squared Error (MSE) between the softmax-normalized similarity matrices: Lscore = (cid:88) z{x,y}"
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) (cid:88) (cid:16) i=1 j=1 i,j(z)pT pS i,j(z) (cid:17)2 where the probability distributions pα model (S) and teacher model (T ) are defined as: (9) i,j for student pα i,j(z) = exp(cid:0)ϕ(zα ,zα k=1exp(cid:0)ϕ(zα )/τ (cid:1) ,zα (cid:80)B )/τ (cid:1) , α {S,T }. (10) Here, ϕ denotes the cosine similarity and τ is temperature hyperparameter. To emphasize the importance of higher similarity scores compared to lower similarity scores, we use temperature-scaled softmax values with τ = 0.02. We conducted these experiments under two data regimes: filtered version of the S2ORC dataset9 and 9https://huggingface.co/datasets/sentencetransformers/s2orc (Last Access: 02/09/2026) the full data mixture used during the first stage of our training. Detailed hyperparameter configurations for each objective and extended results at different learning rates are provided in Appendix A.5. Figure 3 illustrates training progress for all three loss functions on the MTEB English v2 retrieval benchmark at nDCG@10. We observe clear differences in both convergence speed and final performance. While Lscore and LNCE provide significantly faster initial increase in scores, they plateau relatively early, with score-based distillation showing very limited progress in later stages. In contrast, embedding-based distillation (Ldistill) converges more slowly at the beginning, yet improves steadily and ultimately achieves the highest final retrieval performance in both data regimes. This suggests that while score-level matching is efficient for early alignment, directly aligning student and teacher embeddings provides stronger and more sustained supervisory signal for long-term refinement."
        },
        {
            "title": "5.3.2 Projection Layer",
            "content": "We study the effect of projection head placement in embedding-based distillation when aligning models with mismatched embedding dimensions. All experiments use embedding-based distillation and the S2ORC dataset with the same hyperparameters used for the experiments in Section 5.3.1. We consider two projection strategies to align the embedding spaces: student projection, where the students embeddings are projected into the teachers embedding space before computing the distillation loss, and teacher projection, where the teachers embeddings are projected into the Table 5: Evaluation of retrieval adapter training losses on MTEB v2 Retrieval subset and public RTEB tasks. Loss Configuration MTEB RTEB LNCE +Ldistill +LGOR LNCE +Ldistill LNCE +LGOR Ldistill +LGOR LNCE Ldistill 64.50 64.21 64.11 63.49 63.38 63. 66.45 66.16 66.11 65.05 65.14 64.37 Table 6: Impact of GOR loss on quantization robustness, evaluated on MTEB v2 Retrieval subset and public RTEB tasks. MTEB RTEB Configuration BF16 Binary BF16 Binary Full (w/ GOR) 64.50 62.60 (-1.90) 66.45 63.94 (-2.51) 64.21 61.13 (-3.08) 66.16 62.24 (-3.92) w/o GOR In both cases, we students embedding space. evaluated configurations with randomly initialized projections, and with the heads frozen and unfrozen, resulting in four experimental settings. As shown in Figure 4, we observe that while teacher projection without freezing simply does not work10, all three other configurations perform comparably well. Freezing the student projection leads to faster convergence, and leaving it unfrozen yields the best final results."
        },
        {
            "title": "5.3.3 Retrieval Loss Components\nTable 5 presents an ablation study on the compo-\nnents of our retrieval adapter training loss. We\nsystematically remove individual losses from the\nfull combination (Equation 4) to assess their individ-\nual contributions. The results show that combining\nall three losses yields the best performance across\nboth benchmarks.",
            "content": "Notably, we show that relying solely on embedding distillation is insufficient, as Ldistill alone has the lowest scores (63.16 on MTEB, 64.37 on RTEB) of our tested combinations. This validates our two-stage training approach. While Ldistill distillation provides strong initialization in stage 1 training, the addition of task-specific losses (LNCE and LGOR) in stage 2 is critical for maximizing retrieval performance."
        },
        {
            "title": "5.3.4 GOR Loss and Quantization Robustness\nIn Table 6, we present the results of training the\nmodel with and without the GOR loss component of",
            "content": "10The training probably collapses into trivial solution. Figure 5: Average MMTEB score across reduced embedding dimensions. Equation 4, both at full-precision (BF16) and binary quantization. At full precision, LGOR contributes only modestly to performance, improving MTEB scores from 64.21 to 64.50 and RTEB from 66.16 to 66.45. However, its benefit becomes evident under quantization. Without LGOR, performance degrades over 50% more on both MTEB and RTEB benchmarks, from -1.90 to -3.08 on MTEB and from -2.51 to -3.92 on RTEB. This robustness was the goal of GOR regularization: Ensuring fuller use of the available dimensions in embedding space, making the resulting representations less sensitive to information loss."
        },
        {
            "title": "5.4 Truncation Robustness of Embeddings",
            "content": "truncated We evaluated the performance of embeddings, the result of using Matryoshka Representation Learning [Kusupati et al., 2022]. We progressively reduced to smaller dimensions, in order to assess the efficiency and adaptability of the models latent space. Figure 5 shows scores on MMTEBs retrieval benchmarks for embeddings of varying sizes, providing us with systematic and quantitative analysis of the trade-off between retrieval accuracy and computational efficiency. Our results show sizable decline in retrieval performance when the embedding dimensions fall below 256. This aligns with expectations from the Johnson-Lindenstrauss Lemma [Johnson and Lindenstrauss, 1984], which establishes theoretical limits on dimensionality reduction while maintaining pairwise distances between data points."
        },
        {
            "title": "We\nhave\nmultilingual",
            "content": "introduced two embedding compact models jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, and novel training method for them that combines distillationbased and task-specific training. We demonstrate through extensive ablation studies that this approach outperforms existing alternatives. Our models achieve state-of-the-art performance among comparable multilingual embedding models and remain robust under truncation and binary quantization, with only minimal performance degradation in response to large increases in storage and computational efficiency. To support reproducibility and accelerate future research, we have released the models publicly along with out-of-the-box integration with Sentence Transformers [Reimers and Gurevych, 2019] and vLLM [Kwon et al., 2023], in addition to multiple quantized variants for llama.cpp [ggml-org and contributors, 2026]."
        },
        {
            "title": "References",
            "content": "Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, 2019. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025a. Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, et al. Embeddinggemma: Powerful and lightweight text representations. arXiv preprint arXiv:2509.20354, 2025. Dun Zhang, Jiacheng Li, Ziyang Zeng, and Fulong Wang. Jasper and stella: distillation of sota embedding models. arXiv preprint arXiv:2412.19048, 2024a. Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, et al. Mmteb: Massive multilingual text embedding benchmark. In The Thirteenth International Conference on Learning Representations. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: deep self-attention distillation for task-agnostic compression of preIn Proceedings of the 34th trained transformers. International Conference on Neural Information Processing Systems, pages 57765788, 2020. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the association for computational linguistics: EMNLP 2020, pages 41634174, 2020. Xuanang Chen, Ben He, Kai Hui, Le Sun, and Yingfei Sun. Simplified tinybert: Knowledge distillation for document retrieval. In European Conference on Information Retrieval, pages 241248. Springer, 2021. Sebastian Hofstätter, Sophia Althammer, Michael and Allan Hanbury. Schröder, Mete Sertkan, Improving efficient neural ranking models with arXiv cross-architecture knowledge distillation. preprint arXiv:2010.02666, 2020. Aditya Menon, Sadeep Jayasumana, Ankit Singh Rawat, Seungyeon Kim, Sashank Reddi, and Sanjiv Kumar. In defense of dual-encoders for neural ranking. In International Conference on Machine Learning, pages 1537615400. PMLR, 2022. Seungyeon Kim, Ankit Singh Rawat, Manzil Zaheer, Sadeep Jayasumana, Veeranjaneyulu Sadhanala, Wittawat Jitkrittum, Aditya Krishna Menon, Rob Fergus, and Sanjiv Kumar. Embeddistill: geometric knowledge distillation for information retrieval. arXiv preprint arXiv:2301.12005, 2023. Eugene Yang, Dawn Lawrie, James Mayfield, Douglas Oard, and Scott Miller. Translate-distill: learning cross-language dense retrieval by translaIn European Conference on tion and distillation. Information Retrieval, pages 5065. Springer, 2024. Elio Musacchio, Lucia Siciliani, Pierpaolo Basile, and Giovanni Semeraro. xvlm2vec: Adapting lvlm-based embedding models to multilinguality using self-knowledge distillation. arXiv preprint arXiv:2503.09313, 2025. Philippe Formont, Maxime DARRIN, Banafsheh Karimian, Eric Granger, Jackie CK Cheung, Ismail Ben Ayed, Mohammadhadi Shateri, and Pablo Piantanida. Learning task-agnostic representations through multiteacher distillation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Dun Zhang, Ziyang Zeng, Yudong Zhou, and Shuyang Lu. Jasper-token-compression-600m technical report. arXiv preprint arXiv:2511.14405, 2025b. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly embedding. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 15911601, 2014. Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-embedding: Multilinguality, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 23182335, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Isabelle Mohr, Markus Krimmel, Saba Sturua, Mohammad Kalim Akram, Andreas Koukounas, Michael Günther, Georgios Mastrapas, Vinit Ravishankar, Joan Fontanals Martínez, Feng Wang, et al. Multi-task contrastive learning for 8192-token bilingual text embeddings. arXiv preprint arXiv:2402.17016, 2024. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. In Findings of the Association for Computational Linguistics: ACL 2023, pages 11021121, 2023. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: self-supervised image retrieval with open-ended instructions. In Proceedings of the 41st International Conference on Machine Learning, pages 5940359420, 2024b. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, et al. Jina embeddings v3: Multilingual text encoder with low-rank adaptations. In European Conference on Information Retrieval, pages 123129. Springer, 2025. Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Duarte Alves, André Martins, Ayoub Hammal, Caio Corro, Céline Hudelot, Emmanuel Malherbe, Etienne Malaboeuf, Fanny Jourdan, et al. Eurobert: scaling multilingual encoders for european languages. arXiv preprint arXiv:2503.05500, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Aditya Kusupati, Gantavya Bhatt, et al. Matryoshka Representation Learning. In Advances in Neural Information Processing Systems (NeurIPS 2022), 2022. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, et al. mgte: Generalized long-context representation and reranking models for multilingual text retrieval. In Proceedings text of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 13931412, 2024c. Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of roPE-based extrapolation. In The Twelfth International Conference on Learning Representations, 2024. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for In EMNLP (1), open-domain question answering. pages 67696781, 2020. Xu Zhang, Felix Yu, Sanjiv Kumar, and Shih-Fu Chang. Learning spread-out local feature descriptors. In Proceedings of the IEEE international conference on computer vision, pages 45954603, 2017. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 Task 6: Pilot on Semantic Textual Similarity. In SEM 2012: 1st Joint Conference on Lexical and Computational Semantics (SemEval), 2012. Marco Marelli, Stefano Menini, et al. SICK cure for the evaluation of compositional distributional semanIn Ninth International Conference on tic models. Language Resources and Evaluation (LREC), 2014. Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. arXiv preprint arXiv:1904.05068, 2019. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, 2023. Puxuan Yu, Luke Merrick, Gaurav Nuti, and Daniel Campos. Arctic-embed 2.0: Multilingual retrieval without compromise. arXiv preprint arXiv:2412.04506, 2024. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Multilingual e5 text embeddings: technical report. arXiv preprint arXiv:2402.05672, 2024. Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Xin Zhang, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, et al. Kalmembedding-v2: Superior training techniques and data inspire versatile embedding model. arXiv preprint arXiv:2506.20923, 2025. Voyage AI. Voyage-4-nano. https:// huggingface.co/voyageai/voyage-4-nano, 2026. State-of-the-art text embedding model with 32,000 token context length. Michael Günther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Bo Wang, Sedigheh Eslami, Scott Martens, Maximilian Werk, Nan Wang, et al. jina-embeddings-v4: Universal embeddings for multimodal multilingual retrieval. In Proceedings of the 5th Workshop on Multilingual Representation Learning (MRL 2025), pages 531550, 2025. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:28252830, 2011. Frank Liu, Kenneth C. Enevoldsen, Solomatin Roman Samoed, Isaac Chung, Tom Aarsen, and Zoltán Fodi. Introducing rteb: new standard for retrieval evaluation, October 2025. Accessed: 2026-02-11. Information Processing Conference on Neural Systems Datasets and Benchmarks Track (Round 2). Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Longembed: Extending embedding models for long context retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 802816, 2024. William Johnson and Joram Lindenstrauss. Extensions of lipschitz maps into hilbert space. Contemporary Mathematics, 26:189206, 1984. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: heterogeneous benchmark for zero-shot evaluation In Thirty-fifth of information retrieval models. ggml-org and contributors. llama.cpp: Llm inference in c/c++. https://github.com/ggml-org/ llama.cpp, 2026. GitHub repository, Accessed: 2026-02-16."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Hyperparameters The following table outlines all hyperparameters used during the various training phases. For all the LoRA adapters we use rank of 32 and an alpha value of 32. Table A1: Hyperparameters for the different models and training stages. Stage Model Steps Devices & Batch size Max. Tokens (Seq. Length) LR θ Others First Stage Long Context Training Asym. Retrieval Text-Matching Clustering Classification j-v5-text-small j-v5-text-nano 50,000 50,000 8512 81024 512 512 1104 1104 1M 250K j-v5-text-small 6,500 264 4096 1104 500K j-v5-text-small j-v5-text-nano j-v5-text-small j-v5-text-nano j-v5-text-small j-v5-text-nano j-v5-text-small j-v5-text-nano 8,000 8,000 20000 20000 20,000 20, 30,000 30,000 2(256 / 64) 2(384 / 96) 384 / 4096 384 / 4096 1256 1256 1512 11024 464 4 384 384 512 512 512 512 2105 2105 5105 5105 1105 110 4104 4104 1M τ = 0.02,λD = 2, 250K λNCE = λS = 1 1M τ = 0.02,τ = 0.05, 250K λNCE = 1,λD = 2 100K 25K 3.5M τ = 0.02, 1M λNCE = 1,λR = 20 A.2 English MTEB Benchmarks The following evaluations are computed using the default metrics on the MTEB(eng, v2) benchmark. We either report results that are stated on the MTEB leaderboard11 or self-evaluate them using the mteb package12. Model Qwen3-4B jina-v4 Table A2: Evaluation Results on MTEB Retrieval Tasks (nDCG@10 [%]) AVG Arg CQG CQU CFHN FEV FiQA HPQA SCI TREC TOU 68.46 75.64 71.51 59.60 56.15 66.96 57.59 42. 61.83 70.97 64.14 51.49 Qwen3-0.6B (instr.) Qwen3-0.6B (generic) 59.77 67.48 63.83 49.80 54.29 43.29 58.02 43.52 jina-v3 58.56 59.11 63.18 46.57 snowflake-l-v2 53.47 58.48 63.96 44.73 mult.-e5-l-instr. j-v5-text-small 60.07 65.07 62.16 49.61 KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano v5-small stage 1 v5-nano stage 1 58.45 60.15 65.52 48.87 52.30 58.63 60.96 46.15 55.69 71.54 59.53 41.52 58.80 65.70 61.60 47.40 58.52 64.05 62.00 48.42 58.29 65.60 60.86 46.79 48.48 34. 43.62 37.61 43.14 42.83 23.83 41.75 35.06 22.41 26.71 40.03 38.19 36.68 92.47 62.65 87.69 48.49 88.94 46.61 86.66 46.83 89.90 47.35 92.21 45.35 75.76 48.43 90.46 49.63 88.23 47.10 68.14 50.99 80.75 47.74 89.82 47. 86.46 47.53 87.58 46.20 75.22 69.01 67.69 66.90 64.70 68.40 64.53 69.94 71.79 63.25 71.48 69.28 68.45 68.70 31.44 21. 24.41 22.77 19.92 20.28 19.24 23.04 21.62 21.28 18.43 22.60 22.28 22.23 92.92 80.36 90.52 88.80 77.74 83.63 82.51 78.49 82.98 77.89 80.35 77. 79.48 79.39 74.65 52.41 69.90 67.01 55.28 64.05 53.26 70.59 63.23 53.83 58.90 66.12 68.36 68.88 Tasks: Avg: Average over all tasks, Arg: ArguAna, CQG: CQADupstackGamingRetrieval, CQU: CQADupstackUnixRetrieval, CFHN: ClimateFEVERHardNegatives, FEV: FEVERHardNegatives, FiQA: FiQA2018, HPQA: HotpotQAHardNegatives, SCI: SCIDOCS, TREC: TRECCOVID, TOU: Touche2020Retrieval.v 11https://huggingface.co/spaces/mteb/leaderboard (Last Access: 02/02/2026) 12https://github.com/embeddings-benchmark/mteb (Last Access: 02/02/2026) Table A3: Evaluation Results on MTEB Reranking Tasks (MAP@1000 [%]). Avg MindSmallReranking AskUbuntuDupQuestions Model Qwen3-4B jina-v4 50.76 48. Qwen3-0.6B (instr.) 48.18 Qwen3-0.6B (generic) 46.17 47.94 jina-v3 47.47 snowflake-l-v2 48.74 mult.-e5-l-instr. j-v5-text-small 49.39 KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano v5-small stage 1 v5-nano stage 1 47.42 47.68 47.43 49.23 47.07 47.64 32.71 32. 31.23 31.40 30.83 31.59 33.07 32.69 32.45 32.19 31.90 32.72 32.24 32.43 68.81 63.56 65.13 60.94 65.04 63.35 64.41 66.08 62.39 63.18 62.96 65. 61.91 62.85 Table A4: Evaluation Results on MTEB Semantic Textual Similarity Tasks (Spearman correlation [%]). Avg BIO SICK-R STS12 STS13 STS14 STS15 STS17 STS22 STSB Model Qwen3-4B jina-v4 88.7 82.9 85.9 89. Qwen3-0.6B (instr.) 86.6 85.5 Qwen3-0.6B (generic) 86.6 85.5 85.8 88.7 jina-v3 78.0 87.2 snowflake-l-v2 84.7 87.5 mult.-e5-l-instr. j-v5-text-small 88.1 85.2 KaLM-mini-v2.5 voyage-nano Gemma-300m j-v5-text-nano v5-small stage 1 v5-nano stage 1 84.8 84.0 81.6 86.6 83.6 86.4 88.3 87.5 82.3 87.3 83.6 87.5 88.1 89. 84.8 84.8 89.6 74.0 81.7 91.6 83.2 77.9 81.4 92.0 78.2 81.0 86.6 83.5 83.0 83.0 82.4 71.2 82.5 85.1 81.9 76.0 79.3 85. 77.3 80.3 94.4 88.6 91.8 91.8 89.5 80.4 88.1 88.7 89.5 87.4 86.4 89.5 84.3 86.7 90.9 84. 87.1 87.1 85.0 75.1 84.8 88.5 86.0 80.2 83.7 88.9 80.1 81.6 93.8 89.7 91.4 91.5 89.3 82.9 91.0 92.6 90.3 86.2 89.3 92. 88.2 88.9 95.1 88.7 93.3 93.2 90.0 84.5 90.3 94.9 92.3 88.9 90.3 93.7 84.9 90.1 73.1 70. 71.1 71.1 68.4 67.9 68.2 71.8 67.2 67.0 67.5 70.4 72.1 68.6 93.7 88.6 91.1 91.1 89.4 79.2 88.4 94.8 88.9 84.5 88.2 94. 88.0 87.8 Tasks: Avg: Average over all tasks, BIO: BIOSSES, STS22: STS22v2, STSB: STSBenchmark Table A5: Evaluation Results on MTEB Pair Classification Tasks (Max Average Precision [%]). Avg SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus Model Qwen3-4B jina-v 87.0 83.1 Qwen3-0.6B (instr.) 84.4 Qwen3-0.6B (generic) 84.4 84.0 jina-v3 83.0 snowflake-l-v2 86.2 mult.-e5-l-instr. j-v5-text-small 85.0 KaLM-mini-v2.5 voyage-nano Gemma-300m j-v5-text-nano v5-small stage 1 v5-nano stage 1 86.6 83.0 87.3 84.7 83.4 83. Avg: Average over all tasks 96.1 91.4 94.1 94.1 97.0 96.5 92.2 96.6 96.1 93.2 97.0 95.5 96.4 96.9 77.8 71. 72.3 72.3 70.9 67.0 79.8 72.3 77.2 70.5 77.9 73.1 68.4 67.6 87.2 86.4 86.8 86.8 84.1 85.4 86.7 86.0 86.7 86.7 86.9 85. 85.5 85.6 Table A6: Evaluation Results on MTEB Classification Tasks (Accuracy [%]). Avg AmzCF Bnk77 IMDB MTOP M-Int M-Scn Tox TwSent Model Qwen3-4B jina-v4 89.8 74. Qwen3-0.6B (instr.) 84.6 Qwen3-0.6B (generic) 72.0 85.8 jina-v3 73.4 snowflake-l-v2 75.5 mult.-e5-l-instr. j-v5-text-small 90.4 KaLM-mini-v2.5 voyage-nano Gemma-300M j-v5-text-nano v5-small stage 1 v5-nano stage 1 90.5 73.9 87.6 89.7 75.3 75.0 93.7 91.5 70.1 90.9 65.6 69.7 94.3 94.7 66.1 90.1 93.7 75.5 72.7 86.3 78.6 81.0 71.6 84.1 81.8 78.0 91.5 90.3 83.3 91.5 90. 84.6 84.3 97.2 81.0 95.4 89.6 91.9 72.8 94.6 95.6 95.9 88.2 92.9 94.7 82.0 80.7 97.8 93. 96.0 91.0 97.5 93.5 91.2 98.9 98.7 93.3 99.1 98.6 94.0 95.0 85.0 70.5 80.4 50.0 75.2 71.5 70.9 85.3 83.2 57.9 85.8 84. 56.7 56.8 88.8 91.4 72.6 64.5 74.1 82.1 74.4 68.0 84.1 91.3 76.2 65.9 73.9 66.8 92.1 93.7 89.3 80.1 76.2 64.7 91.5 82.9 91.8 92.8 77.5 69.9 79.1 68.1 78.4 60. 76.0 61.5 71.4 59.6 59.2 72.1 77.2 61.4 66.6 71.5 62.2 63.1 Tasks: Avg: Average over all tasks, AmzCF: Amazon Counterfactual Classification, Bnk77: Banking77, IMDB: IMDB, MTOP: MTOP Domain Classification, M-Int: MASSIVE Intent Classification, M-Scn: MASSIVE Scenario Classification, Tox: Toxic Conversations Classification, TwSent: Tweet Sentiment Extraction. Table A7: Evaluation Results on MTEB Clustering Tasks (V-measure [%] - see footnote in Section 5.1). Avg AXP AXS BioP MedP MedS SEx SExP 20News Model Qwen3-4B jina-v4 57.5 64.8 65.2 50.9 45.5 58.2 57.3 38.4 54.1 63.7 63.8 47.3 Qwen3-0.6B (instr.) Qwen3-0.6B (generic) 51.8 63.2 61.8 45.1 47.4 58.9 55.9 40.0 jina-v3 44.4 57.2 53.1 37.2 snowflake-l-v2 49.9 62.5 61.3 42.7 mult.-e5-l-instr. j-v5-text-small 54.7 65.8 62.7 47.7 KaLM-mini-v2.5 voyage-nano Gemma-300M j-v5-text-nano 58.1 63.5 61.2 50.7 46.9 58.2 56.4 41.2 56.6 63.6 59.6 52.1 53.5 65.0 61.1 46. v5-small stage 1 v5-nano stage 1 46.5 58.2 57.0 41.8 47.9 58.4 57.7 41.2 45.3 35.0 42.2 40.7 38.2 35.0 38.1 43.5 45.6 38.3 44.1 42.6 37.8 38. 43.8 34.8 40.4 39.8 37.2 32.6 37.7 41.4 43.5 37.1 41.9 40.3 36.7 37.9 77.6 53.5 53.6 39.8 71.2 52.1 66.2 45.1 56.7 40.9 56.3 41.2 60.0 46.1 70.1 52. 75.8 51.6 52.3 43.1 90.9 48.9 69.4 50.6 55.4 41.5 58.4 41.9 59.0 47.0 51.7 52.4 51.5 42.4 50.7 54.3 73.1 48.5 51.3 52.6 43.8 49. Tasks: Avg: Average over all tasks, AXP: ArXivHierarchicalClusteringP2P, AXS: ArXivHierarchicalClusteringS2S, BioP: BiorxivClusteringP2P.v2, MedP: MedrxivClusteringP2P.v2, MedS: MedrxivClusteringS2S.v2, SEx: StackExchangeClustering.v2, SExP: StackExchangeClusteringP2P.v2, 20News: TwentyNewsgroupsClustering.v2. A.3 Multilingual MTEB (MMTEB) Benchmarks The following evaluations are computed using the default metrics on the MTEB(Multilingual, v2) benchmark. Also here we report results that are stated on the MTEB leaderboard and self-evaluate missing scores. Table A8: Evaluation Results on MMTEB Retrieval Tasks (nDCG@10%) Model Qwen3-4B jina-v Avg AI Arg Bel Cov Hag PK LB MIR ML SD SQA SO STC TC TR TW Wiki WG 69.6 81.2 75.6 81.2 87.4 98.8 84.3 95.4 69.5 81.9 31.4 20.2 94.3 42.3 92.9 1.2 72.6 91.2 51.5 66.4 50.1 67.0 74.3 80.2 98.8 69.8 94.8 62.9 74.9 21.5 30.2 91.4 58.1 80.4 1.3 84.4 88.5 67.3 64.6 79.0 71.0 68.7 84.8 98.8 84.8 94.5 61.2 72.8 24.4 10.6 90.0 33.6 90.5 1.0 60.0 87.1 50.8 Qwen3-0.6B (instr.) Qwen3-0.6B (generic) 64.2 74.0 67.5 68.6 83.0 98.8 95.3 94.0 61.4 71.2 22.8 8.3 89.3 32.2 88.8 1.1 61.1 86.8 51.2 55.8 32.8 43.3 73.4 78.5 98.7 38.0 93.4 62.6 73.4 19.9 0.7 90.8 39.2 77.7 0.6 73.0 89.1 18.6 jina-v3 58.4 22.8 59.1 74.0 78.5 98.7 77.3 93.8 66.5 73.1 20.3 5.7 86.9 19.1 83.6 1.0 44.5 90.5 55.0 snowflake-l-v2 57.1 29.7 58.5 80.9 75.8 98.7 37.8 94.3 57.7 76.2 19.2 13.5 85.8 33.7 82.5 1.2 36.9 91.6 54.3 mult.-e5-l-instr. j-v5-text-small 64.9 53.3 65.1 77.5 80.1 98.8 80.5 94.0 66.6 77.6 23.0 11.5 93.4 46.3 78.5 0.9 71.6 90.6 58.7 KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano 57.9 39.8 59.6 69.7 83.8 98.7 38.5 94.5 63.0 72.6 17.6 5.0 91.2 37.8 83.2 1.9 58.1 86.6 40.5 63.6 48.7 58.6 80.9 79.8 99.0 87.3 94.8 58.7 78.6 21.3 9.7 94.3 40.1 77.9 1.7 80.5 92.8 39.9 62.5 37.4 71.5 72.4 78.9 98.9 60.8 95.1 66.2 79.0 18.4 10.7 86.5 46.3 80.4 1.0 72.0 90.0 59.4 63.3 51.5 65.7 75.3 78.0 98.8 81.5 94.5 65.8 77.0 22.6 6.3 92.3 33.1 77.6 1.2 74.0 90.0 53.4 v5-small stage 1 v5-nano stage 63.5 49.5 64.1 77.6 79.3 98.8 83.0 94.0 64.6 78.1 22.3 10.0 91.6 40.9 79.5 0.7 70.8 90.6 48.5 62.1 47.0 65.6 75.5 77.6 98.8 83.0 94.5 63.9 77.7 22.2 6.4 89.5 31.9 79.4 1.0 71.1 89.5 44.0 Tasks: Avg: Average over all tasks, AI: AILAStatutes, Arg: ArguAna, Bel: BelebeleRetrieval, Cov: CovidRetrieval, Hag: HagridRetrieval, PK: LEMBPasskeyRetrieval, LB: LegalBenchCorporateLobbying, MIR: MIRACLRetrievalHardNegatives, ML: MLQARetrieval, SD: SCIDOCS, SQA: SpartQA, SO: StackOverflowQA, TC: TREC-COVID, STC: StatcanDialogueDatasetRetrieval, TR: TempReasonL1, TW: TwitterHjerneRetrieval, Wiki: WikipediaRetrievalMultilingual, WG: WinoGrande Table A9: Evaluation Results on MMTEB Reranking Tasks (MAP@1000 [%]). Avg Alloprof RuBQ T2R Voyage WebLINX Wiki-Multi Model Qwen3-4B jina-v 65.08 62.20 Qwen3-0.6B (instr.) 61.41 Qwen3-0.6B (generic) 62.25 57.09 jina-v3 63.67 snowflake-l-v2 62.61 mult.-e5-l-instr. j-v5-text-small 65.66 KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano v5-small stage 1 v5-nano stage 1 62.36 63.15 63.25 64.63 64.72 63. 85.13 78.24 80.38 79.52 72.93 75.84 74.68 81.39 75.99 78.36 79.69 79.67 80.60 79.85 72.28 67.27 70.95 64.86 65.67 67.15 69.66 66.96 65.56 65.61 73.75 67.57 71.66 67.12 74.89 68. 73.59 67.45 73.38 65.19 71.26 67.54 73.68 67.63 73.19 67.69 72.81 67.34 65.61 58.86 57.66 61.38 50.76 66.63 62.48 68.84 62.10 59.24 61.00 67.02 66.36 64. 11.30 14.00 11.60 10.13 9.84 9.05 8.71 11.33 9.71 10.63 10.16 10.95 11.01 10.83 88.89 86.28 85.99 85.83 77.81 89.18 91.03 89. 85.31 92.07 89.88 88.83 89.45 88.37 Tasks: Avg: Average over all tasks, Alloprof: AlloprofReranking, RuBQ: RuBQReranking, T2R: T2Reranking, Voyage: VoyageMMarcoReranking, WebLINX: WebLINXCandidatesReranking, Wiki-Multi: WikipediaRerankingMultilingual. Table A10: Evaluation Results on MMTEB Semantic Textual Similarity Tasks (Spearman correlation [%]). Avg Faro FinP GSTS Indic JSICK SICK-R S12 S13 S14 S15 S17 S22 STSB STSBm STSES SRel Model Qwen3-4B jina-v4 80.9 85.8 34.0 74.4 72.3 15.1 Qwen3-0.6B (instr.) 76.2 74.3 26.3 Qwen3-0.6B (generic) 76.2 74.3 26.3 77.1 80.8 22.4 jina-v3 70.1 70.9 22.1 snowflake-l-v2 76.8 80.4 25.6 mult.-e5-l-instr. j-v5-text-small 78.9 76.9 33.1 KaLM-mini-v2.5 voyage-nano Gemma-300m j-v5-text-nano v5-small stage 1 v5-nano stage 1 71.8 64.4 22.1 73.0 73.7 20.1 74.7 65.3 25.2 78.2 71.4 35. 74.3 75.0 18.7 74.5 72.0 20.7 90.0 88.2 84.9 84.9 87.9 77.0 85.9 91.0 83.9 81.1 84.7 90.7 83.6 84.2 60.8 35. 39.0 39.0 54.7 47.2 53.7 47.8 15.0 46.3 43.1 41.4 44.6 40.3 88.8 80.3 86.6 86.6 78.2 81.6 82.5 81.3 79.6 81.6 84.4 81. 86.1 86.0 88.1 89.2 84.8 84.8 89.6 74.0 81.7 91.6 83.2 77.9 81.4 92.0 78.2 81.0 86.6 94.4 90.9 93.8 91.8 73.0 86.1 83.5 88.6 84.8 89.7 85.0 71.8 86. 83.0 91.8 87.1 91.4 85.5 71.8 84.6 83.0 91.8 87.1 91.4 85.5 71.8 84.6 82.4 89.5 85.0 89.3 85.9 71.1 85.4 71.2 80.4 75.1 82.9 74.4 68.7 72.0 82.5 88.1 84.8 91.0 86.0 69.0 83.1 85.1 88.7 88.5 92.6 87.2 71.0 89.9 81.9 89.5 86.0 90.3 81.3 73.2 82.9 76.0 87.4 80.2 86.2 79.1 70.8 79.2 79.3 86.4 83.7 89.3 84.4 71.2 81.6 85.3 89.5 88.9 92.8 86.3 69.6 89.8 77.3 84.3 80.1 88.2 84.9 72.1 82.2 80.3 86.7 81.6 88.9 84.3 71.8 82.5 93.7 88.6 91.1 91.1 89.4 79.2 88.4 94.8 88.9 84.5 88.2 94. 88.0 87.8 72.8 75.3 76.9 76.9 77.9 78.7 77.1 81.2 73.1 79.3 82.3 80.1 80.3 79.8 63.2 56. 59.4 59.4 64.6 66.3 69.2 60.9 54.2 65.3 65.2 61.9 64.6 63.7 Tasks: Avg: Average over all tasks, Faro: FaroeseSTS, FinP: FinParaSTS, GSTS: GermanSTSBenchmark, Indic: IndicCrosslingualSTS, S12: STS12, S13: STS13, S14: STS14, S15: STS15, S17: STS17, S22: STS22.v2, STSB: STSB, STSBm: STSBenchmark, SRel: SemRel24STS Table A11: Evaluation Results on MMTEB Pair Classification Tasks (Max Average Precision [%]). Avg ArmPC CTK Opus PawsX Ppc RTE3 Sprint TERRa TwURL XNLI IndoNLI Model Qwen3-4B jina-v4 85.1 79.3 Qwen3-0.6B (instr.) 80.8 Qwen3-0.6B (generic) 80.8 79.3 jina-v3 76.7 snowflake-l-v2 80.9 mult.-e5-l-instr. j-v5-text-small 82.9 KaLM-mini-v2.5 voyage-nano Gemma-300m j-v5-text-nano v5-small stage 1 v5-nano stage 79.1 76.3 81.4 81.9 77.9 78.0 96.3 94.4 93.1 93.1 95.8 95.9 96.0 94.5 92.5 95.0 92.7 93.5 93.9 93. 86.2 95.8 79.6 94.1 68.9 95.3 90.8 61.9 91.8 88.3 79.2 92.9 79.2 92.9 79.2 94.6 74.5 92.7 82.6 95.5 82.7 94.1 74.8 92.6 77.0 92.0 79.3 93.3 81.6 94.0 62.2 90.5 89.0 62.2 90.5 89.0 54.4 91.4 88.1 56.6 87.1 86.2 55.6 93.5 87.9 65.7 93.3 89.7 65.0 88.0 88.2 56.0 83.9 88.0 57.7 90.9 89.7 62.2 93.9 89. 75.2 93.6 74.6 93.7 59.3 87.9 86.8 58.7 89.1 86.6 96.1 91.4 94.1 94.1 97.0 96.5 92.2 96.6 96.1 93.1 97.0 95.5 96.4 96. 66.6 57.4 60.7 60.7 59.2 53.8 63.9 67.3 57.5 52.6 65.1 64.6 57.3 58.1 87.2 86.4 86.7 86.7 84.1 85.4 86.7 85. 86.7 85.3 86.9 85.6 85.5 85.6 87.2 71.9 81.7 81.7 73.7 64.3 79.5 82.4 74.3 65.3 81.7 81.4 69.3 69. 65.3 54.6 59.0 59.0 54.4 50.7 56.2 60.0 54.8 51.0 61.0 59.6 51.9 51.9 Tasks: Avg: Average over all tasks, ArmPC: ArmenianParaphrasePC, CTK: CTKFactsNLI, Opus: OpusparcusPC, PawsX: PawsXPairClassification, Ppc: PpcPC, RTE3: RTE3, Sprint: SprintDuplicateQuestions, TERRa: TERRa, TwURL: TwitterURLCorpus, XNLI: XNLI, IndoNLI: indonli Table A12: Evaluation Results on MMTEB Bitext Mining Tasks (F1 Score [%]). Avg BUCC Bible Bornh DiaBl Flores IN22 Indic NTX Nolly Norw NusaT NusaX Tato Model Qwen3-4B jina-v4 79.4 98.9 62.4 98.5 Qwen3-0.6B (instr.) 72.2 98.4 Qwen3-0.6B (generic) 72.2 98.4 65.3 98.4 jina-v3 64.1 98.1 snowflake-l-v2 80.1 99.0 mult.-e5-l-instr. j-v5-text-small 69.7 98.7 KaLM-mini-v2.5 voyage-nano Gemma-300m j-v5-text-nano v5-small stage 1 v5-nano stage 1 65.0 98.5 64.1 97.9 64.4 98.7 67.7 98.6 69.1 98.8 67.3 98.7 25.9 11.3 21.2 21.2 10.5 10.3 22.0 13.6 14.1 11.7 12.7 12. 15.0 14.4 71.3 34.0 54.2 54.2 37.3 42.0 55.4 78.0 43.6 35.7 34.6 77.1 57.1 56.9 87.1 84. 80.9 80.9 85.1 80.5 87.3 84.8 82.5 74.8 83.9 83.9 82.1 81.0 74.1 82.0 93.7 87.8 63.6 93.1 53.5 68.0 79.3 71.7 32.1 93.1 62.8 75.8 90.4 79.5 61.3 92.7 62.8 75.8 90.4 79.5 61.3 92.7 55.3 72.8 87.0 78.3 33.2 92.6 54.7 71.4 84.7 77.4 33.6 92.5 86.0 78.9 91.1 93.7 80.7 93.5 58.0 74.9 88.5 76.6 38.8 94.1 58.4 63.3 78.8 74.6 51.5 92.7 60.9 76.3 87.3 77.7 38.3 93.3 55.3 74.4 87.1 73.9 41.3 90.8 53.4 71.7 86.2 72.8 37.1 94. 60.7 76.5 89.5 78.4 39.1 94.3 57.0 74.2 87.4 75.7 38.7 93.3 91.1 63.4 85.4 85.4 62.3 62.8 85.1 70.1 73.0 61.3 66.1 69.6 74.8 72.1 86.9 76.3 64.6 57. 78.4 58.1 78.4 58.1 64.2 71.1 58.2 66.9 85.3 83.7 69.2 61.1 65.2 49.2 53.9 64.6 67.1 51.4 65.8 56.5 70.5 60.9 69.0 56.7 Tasks: Avg: Average over all tasks, BUCC: BUCC.v2, Bible: BibleNLP, Bornh: Bornholm, DiaBl: DiaBla, Flores: FLORES, IN22: IN22 General, Indic: IndicGenBench FLORES, NTX: NTREX; Nolly: NollySenti, Norw: Norwegian Courts, NusaT: NusaTranslation, NusaX: NusaX, Tato: Tatoeba. Table A13: Evaluation Results on MMTEB Classification Tasks (Accuracy [%]). Model Qwen3-4B jina-v4 Afr ACF BgS CSF Cat Cyr CzP DBP Dal Est Fil Fin Grk Guj Ind Idn Zul Ita Kor Kur Mac Mas 50.9 92.5 74.9 58.7 52.6 95.0 68.3 99.3 50.7 59.6 47.8 93.9 51.9 92.0 95.1 64.3 23.2 66.7 61.1 81.9 75.5 84.3 40.9 72.0 61.1 31.6 49.5 34.2 52.3 81.0 49.9 39.8 30.7 78.2 26.0 82.7 20.2 60.0 22.5 58.7 57.0 65.7 53.5 72.4 Qwen3-0.6B (instr.) 46.0 90.4 73.4 44.1 49.1 90.2 63.8 98.8 50.0 41.1 40.8 90.6 39.7 85.8 93.1 64.8 25.6 61.7 55.9 80.1 61.0 80.5 Qwen3-0.6B (gen.) 40.0 70.1 65.4 32.3 47.9 74.3 50.9 96.8 50.0 34.6 28.6 74.6 39.3 81.3 36.5 59.0 23.6 70.1 58.1 71.1 47.2 77.0 42.8 92.2 76.0 39.1 45.1 43.3 61.9 76.1 50.1 48.8 38.5 79.5 11.6 86.2 18.0 58.1 20.8 62.8 56.2 57.6 61.2 71.5 jina-v3 44.1 64.7 59.3 31.0 49.1 57.7 49.7 89.6 50.2 40.7 31.8 72.0 35.7 84.4 33.0 60.9 23.9 68.1 58.4 61.7 61.3 74.3 snowflake-l-v2 45.4 68.6 78.6 43.0 51.0 81.1 60.7 95.5 50.0 49.1 40.8 84.4 32.3 87.5 86.3 61.3 37.3 62.9 58.2 80.9 66.7 80.5 mult.-e5-l-instr. j-v5-text-small 36.7 94.3 74.8 46.7 65.9 97.6 63.6 98.3 49.9 56.1 41.9 63.4 65.0 93.0 98.0 57.7 19.9 91.0 55.2 93.4 67.7 86.9 KaLM-mini-v2.5 voyage-nano Gemma-300M j-v5-text-nano 42.6 95.5 63.3 33.1 50.1 80.7 52.0 95.3 50.2 37.9 32.6 84.0 28.1 74.0 75.5 59.0 26.9 72.0 57.6 60.0 52.3 76.4 42.7 66.1 66.2 36.3 51.1 39.2 52.4 90.5 49.9 40.0 31.4 79.3 38.3 84.9 18.7 57.6 29.3 66.1 57.2 74.9 53.9 77.2 44.5 84.2 71.3 34.5 51.2 58.6 58.6 94.3 50.3 38.3 40.5 86.4 29.0 82.8 46.6 60.9 26.4 70.4 58.1 60.0 45.3 74.9 36.8 93.7 74.0 40.6 65.8 97.9 62.7 97.8 50.4 51.0 40.5 48.6 59.0 91.0 97.6 54.3 17.8 87.4 53.7 93.3 64.9 82. v5-small stage 1 v5-nano stage 1 42.2 75.5 56.1 31.8 49.1 65.2 52.0 91.4 50.1 35.9 30.0 77.8 45.8 78.4 41.4 59.0 26.3 71.5 56.5 67.8 49.7 76.3 41.9 72.7 54.7 30.7 49.4 72.9 50.8 91.2 49.9 36.8 28.4 77.1 44.1 76.1 57.2 58.9 25.6 67.5 56.1 67.9 50.4 72.9 Model Qwen3-4B jina-v4 MI MH Nep Nor NE NX Odi PAC Poe PE Pun Sca Hin Sin Sis Slk Swa CH Tox Tsw TT Avg 76.5 77.5 97.3 90.8 60.4 79.7 93.8 69.7 72.2 77.1 81.9 51.3 76.4 75.5 57.3 93.3 67.2 62.6 91.4 36.4 81.7 72.3 70.5 59.5 93.4 46.5 43.3 63.7 70.1 64.3 55.3 24.6 80.5 50.0 54.2 58.8 51.1 68.9 58.9 55.3 64.5 29.5 71.9 55. Qwen3-0.6B (instr.) 61.4 64.3 95.6 84.3 47.8 71.3 84.2 69.5 73.3 71.5 83.5 50.5 74.3 59.6 58.0 86.3 58.4 55.6 82.1 34.9 80.8 66.8 Qwen3-0.6B (gen.) 50.0 56.6 95.0 65.1 41.7 63.1 78.6 67.4 56.9 35.1 82.4 50.4 56.5 57.7 57.0 72.6 61.6 54.8 68.0 35.4 77.9 58.4 68.5 60.3 93.4 40.9 42.2 66.0 81.4 69.2 59.1 59.7 77.5 50.2 65.7 74.3 51.4 84.3 56.2 55.1 91.3 25.9 57.1 58.8 jina-v3 63.0 59.1 91.2 49.1 42.7 65.5 82.9 66.9 51.2 34.5 81.1 50.6 56.0 73.3 57.9 66.2 60.9 57.1 65.9 25.4 65.6 57.4 snowflake-l-v2 62.7 64.7 97.0 76.6 44.1 71.0 84.5 65.7 57.1 57.0 84.4 50.4 59.6 78.0 47.5 87.5 59.1 55.5 66.8 46.4 74.9 64.9 mult.-e5-l-instr. j-v5-text-small 85.3 58.3 99.5 88.1 75.8 79.7 95.8 87.0 81.9 73.9 88.6 50.2 44.3 52.5 46.6 87.7 54.6 66.1 93.7 53.8 86.3 71.3 KaLM-mini-v2.5 voyage-nano Gemma-300M j-v5-text-nano 83.2 62.6 95.3 62.1 39.2 65.3 64.8 65.1 57.7 42.4 76.7 50.1 55.7 60.3 55.1 72.8 57.7 53.7 91.7 42.5 76.9 61.2 57.9 58.3 94.9 42.5 44.7 65.5 84.7 65.9 52.3 47.9 81.1 50.2 54.6 76.2 64.4 73.2 63.1 59.3 64.7 41.5 72.4 58.6 62.7 61.0 95.5 65.6 44.2 69.8 57.9 67.9 58.9 62.8 82.4 50.8 65.5 65.7 57.0 73.3 66.0 57.7 82.9 31.1 73.0 60.9 84.1 57.0 98.6 88.2 73.4 76.1 94.5 86.3 75.6 66.9 80.4 50.1 55.4 51.7 48.5 85.1 50.7 60.5 92.8 52.6 84.7 69.2 v5-small stage 1 v5-nano stage 1 56.7 58.2 92.7 61.2 45.0 65.0 74.9 64.4 54.2 32.4 80.1 50.1 55.9 66.3 61.5 65.8 63.1 55.9 69.9 35.3 70.9 58.4 56.8 57.7 93.8 65.4 44.2 65.3 72.6 64.5 54.1 36.1 68.7 50.0 56.6 62.2 59.4 65.3 61.3 56.4 68.1 32.7 72.5 58.1 Tasks: Avg: Average over all tasks, Afr: AfriSentiClassification, ACF: AmazonCounterfactualClassification, BgS: BulgarianStoreReviewSentimentClassification, CSF: CzechCSFDClassification, Cat: CataloniaTweetsClassification, Cyr: CyprusTurkishTweetsSentiment, CzP: CzechProductReviewsClassification, DBP: DBPediaClassification, Dal: DalajClassification, Est: EstonianValenceClassification, Fil: FilipinoHateSpeechClassification, Fin: FinnishClassification, Grk: GreekLegalClassification, Guj: GujaratiNewsClassification, Ind: IndicSentimentClassification, Idn: IndonesianClassification, Zul: IsiZuluSentimentClassification, Ita: ItalianClassification, Kor: KoreanSarcasmClassification, Kur: KurdishSentimentClassification, Mac: MacedonianClassification, Mas: MasakhaNEWSClassification, MI: MassiveIntentClassification, MH: MultilingualHateSpeechClassification, Nep: NepaliClassification, Nor: NordicSentimentClassification, NE: NusaXEmotionClassification, NX: NusaXClassification, Odi: OdiaNewsClassification, PAC: PAWSXClassification, Poe: PoemSentimentClassification, PE: PolishEmotionClassification, Pun: PunjabiClassification, Sca: ScalaSentimentClassification, Hin: HindiClassification, Sin: SinhalaClassification, Sis: SiswatiClassification, Slk: SlovakClassification, Swa: SwahiliClassification, CH: SwissJudgementClassification, Tox: ToxicConversationsClassification, Tsw: XitsongaClassification, TT: TwitterTopicClassification Table A14: Evaluation Results on MMTEB Multi-Label Classification Tasks (Accuracy [%]). Avg. BrazilianToxic CEDR KorHate MalteseNews MultiEURLEX Model Qwen3-4B jina-v4 26.8 19.3 Qwen3-0.6B (instr.) 24.6 Qwen3-0.6B (generic) 21.1 18.4 jina-v3 18.9 snowflake-l-v2 22.9 mult.-e5-l-instr. j-v5-text-small 42. KaLM-mini-v2.5 voyage-nano Gemma-300M j-v5-text-nano v5-small stage 1 v5-nano stage 1 21.0 20.1 24.8 41.3 20.3 19.9 20.6 17.6 22.6 23.7 19.7 22.5 19.8 21. 22.9 17.3 22.3 20.7 18.5 18.2 51.0 40.8 49.9 38.7 47.4 38.5 50.0 65.0 40.6 41.6 52.8 65.2 41.4 40. 14.9 7.8 9.7 8.8 10.6 10.5 10.2 60.1 7.6 8.8 11.6 58.8 8.9 8.5 42.1 26.7 36.1 29.7 12.7 18.7 29.0 57. 29.2 29.1 33.1 56.2 28.1 27.7 5.1 3.8 4.6 4.4 1.5 4.6 5.5 6.6 4.6 3.6 4.3 5.5 4.5 4. Tasks: Avg: Average over all tasks, BrazilianToxic: BrazilianToxicTweetsClassification, CEDR: CEDRClassification, KorHate: KorHateSpeechMLClassification, MalteseNews: MalteseNewsClassification, MultiEURLEX: MultiEURLEXMultilabelClassification. Table A15: Evaluation Results on MMTEB Clustering Tasks (V-measure [%] - see footnote in Section 5.1) Avg Allo AXP AXS BigPat BioP CLSP HalS MNC MedP PlscP Rom S200 SEx SCP Cities WikiP Model Qwen3-4B jina-v4 57.2 59.5 64.8 65.2 44.0 44.5 57.9 57. 52.3 54.0 63.7 63.8 Qwen3-0.6B (instr.) Qwen3-0.6B (generic) 49.8 53.8 63.2 61.8 45.7 44.7 58.9 55.9 jina-v3 42.8 45.7 57.2 53.1 snowflake-l-v2 50.8 56.5 62.5 61.3 mult.-e5-l-instr. j-v5-text-small 53.4 53.0 65.8 62.7 KaLM-mini-v2.5 voyage-nano Gemma-300m j-v5-text-nano 53.8 52.0 63.5 61.2 45.4 49.0 58.2 56.4 51.2 52.8 63.6 59.6 52.7 56.4 65.0 61.1 v5-small stage 1 v5-nano stage 1 44.7 41.1 58.2 57.0 45.6 45.5 58.4 57.7 43.2 28. 32.5 32.3 37.1 34.1 43.2 42.6 43.3 31.0 41.6 43.5 32.5 33.8 50.9 73.2 30.5 56.2 38.4 37.4 25.1 40.5 47.3 62.0 29.0 53.2 45.1 48.0 28.8 50.4 40.0 39.4 29.3 46.2 37.2 34.2 24.9 42.9 42.7 42.4 30.1 59.2 47.7 50.7 31.7 53.2 50.7 68.3 29.2 54.1 41.2 38.5 26.7 42.0 52.1 41.5 29.3 43.5 46.4 48.9 29.2 52. 41.8 40.4 24.4 37.2 41.2 39.9 25.2 40.1 45.3 35.2 42.2 40.7 38.2 35.0 38.1 43.5 45.6 38.3 44.1 42.6 37.8 38.1 75.1 44.3 41.3 77.6 62.1 93.6 69.6 41.1 27.4 54.0 27.3 92. 74.2 40.3 34.1 71.2 53.4 86.8 72.8 39.7 34.8 66.2 49.8 80.9 71.5 40.6 32.0 56.7 26.8 85.3 71.1 39.6 25.4 56.3 26.0 74.2 72.7 40.9 47.2 60.0 47.6 76.2 74.4 40.4 40.5 70.1 57.6 89.8 73.9 40.3 37.5 75.8 49.9 85.8 71.5 42.1 32.8 52.3 33.6 86.0 72.1 41.9 26.5 90.9 40.0 92.0 74.4 40.6 38.9 69.4 56.0 88.7 71.2 42.5 27.7 55.4 31.7 88.8 72.2 43.1 28.7 58.4 33.4 85.6 31.6 26.8 29.7 28.6 27.8 27.1 31.5 30.8 30.5 26.9 27.0 30. 27.5 27.5 Tasks: Avg: Average over all tasks, Allo: AlloProfClusteringS2S.v2, AXP: ArXivHierarchicalClusteringP2P, AXS: ArXivHierarchicalClusteringS2S, BigPat: BigPatentClustering.v2, BioP: BiorxivClusteringP2P.v2, CLSP: CLSClusteringP2P.v2, HalS: HALClusteringS2S.v2, MNC: MasakhaNEWSClusteringS2S, MedP: MedrxivClusteringP2P.v2, PlscP: PlscClusteringP2P.v2, Rom: RomaniBibleClustering, S200: SIB200ClusteringS2S, SEx: StackExchangeClustering.v2, SCP: SwednClusteringP2P, Cities: WikiCitiesClustering, WikiP: WikiClusteringP2P.v2. Table A16: Evaluation Results on Instruction Reranking Tasks (p-MRR [%]). Model Qwen3-4B jina-v4 Qwen3-0.6B (instr.) Qwen3-0.6B (generic) jina-v3 snowflake-l-v2 mult.-e5-l-instr. j-v5-text-small KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano v5-small stage 1 v5-nano stage 1 Avg Core17 News21 Robust04 11.56 0.71 13.53 3.24 8.93 5.09 3.84 6.30 1.34 0.06 2.47 0.43 0.40 1.82 2.40 1. 0.56 5.61 3.49 0.05 0.32 1.31 0.66 6.32 5.13 1.78 1.58 0.63 8.71 0.34 3.61 2.59 2.36 0.24 1.50 3. 0.46 11.45 4.60 1.28 0.00 0.18 12.44 1.46 2.75 2.63 6.31 7.60 4.52 1.61 2.80 0.94 0.73 2.92 2.53 4. Tasks: Avg: Average over all tasks, Core17: Core17InstructionRetrieval, News21: News21InstructionRetrieval, Robust04: Robust04InstructionRetrieval. A.4 Other Retrieval Benchmark Table A17: Retrieval performance on BeIR (nDCG@10 [%]). Avg Arg CQA CF DB FEV FiQA HPQA MSM NFC NQ Quora SCD SCF TREC TOU Model Qwen3-4B jina-v 61.6 75.6 50.3 47.4 48.2 91.6 62.7 54.0 67.0 43.7 35.1 43.9 87.9 48.5 55.5 71.0 46.0 42.4 39.5 88.1 46.6 Qwen3-0.6B 53.2 43.3 42.6 42.4 41.0 89.1 47.4 jina-v3 55.2 59.1 45.9 41.8 43.4 91.5 45.4 snowflake-l-v2 mult.-e5-l-instr. 52.7 58.5 44.3 29.9 38.4 78.0 48.4 j-v5-text-small 56.7 65.1 46.7 41.5 44.4 90.0 49.6 KaLM-mini-v2.5 55.0 60.2 47.2 34.5 42.6 87.9 47.1 49.9 58.6 44.3 22.2 39.5 68.4 51.0 voyage-4-nano 53.7 66.0 42.1 26.7 44.4 81.1 47.4 Gemma-300M j-v5-text-nano 56.1 65.7 44.6 39.6 45.3 89.5 47.9 v5-small stage 1 v5-nano stage 1 54.9 64.0 46.0 38.0 43.1 86.2 47.5 54.9 65.6 44.5 36.3 43.6 87.3 46.2 74.7 68. 65.7 64.7 68.2 69.3 69.8 71.8 62.0 70.1 69.1 68.0 68.1 42.7 41.1 63.1 88.1 31.4 78.3 92.9 38.1 34.4 61.7 78.6 21.5 76.1 80.4 38.0 36.7 53.5 87.8 24.4 69.7 90.5 40.8 36.6 64.2 89.2 19.9 72.5 77.7 44.9 35.1 63.7 88.8 20.3 70.9 83.6 40.4 36.3 57.8 89.2 19.2 71.6 82.5 42.1 39.8 64.0 89.1 23.0 76.5 78.5 40.6 37.1 58.6 89.6 21.6 74.4 83.0 31.5 39.6 49.2 86.1 21.3 75.2 77.9 38.6 39.2 63.5 86.6 19.5 78.7 76.4 41.6 38.7 63.4 88.9 22.6 75.8 77. 38.9 38.3 57.3 88.6 22.3 74.5 79.5 40.5 38.0 57.1 88.1 22.2 73.1 79.4 35.4 24.1 33.2 26.3 25.9 27.4 29.9 28.9 22.2 25.1 30.7 30.6 33.8 Tasks: Avg: Average over all tasks, Arg: ArguAna, CQA: CQADupstackRetrieval, CF: ClimateFEVER, DB: DBPedia, FEV: FEVER, FiQA: FiQA2018, HPQA: HotpotQA, MSM: MSMARCO, NFC: NFCorpus, NQ: Natural Questions, Quora: QuoraRetrieval, SCD: SCIDOCS, SCF: SciFact, TREC: TRECCOVID, TOU: Touche Table A18: Retrieval performance on LongEmbed (nDCG@10 [%]) Model Qwen3-4B jina-v4 Qwen3-0.6B jina-v3 snowflake-l-v2 mult.-e5-l-instr. j-v5-text-small KaLM-mini-v2.5 voyage-4-nano Gemma-300M j-v5-text-nano v5-small stage 1 v5-nano stage 1 v5-small pre-long-ctx** Avg NaQA Needle* Passkey* QMSum SummScreen Wikim 78.82 69.88 72.20 55.67 63.74 41.76 66.39 43.35 74.93 55.29 63. 68.36 63.48 44.54 68.94 58.71 63.25 34.30 43.63 26.71 52.95 29.32 63.71 28.83 52.17 56.85 46.17 18.36 75.50 60. 50.75 64.00 50.25 29.50 44.50 31.50 61.00 41.25 59.75 46.00 64.00 38.25 84.25 69.75 84.75 38.00 77.25 37.75 80.50 38.25 87.25 61.00 81. 83.00 83.00 47.25 52.35 46.06 47.70 39.34 40.08 26.08 43.80 27.06 51.44 37.62 31.83 46.39 34.40 27.78 97.96 96. 96.72 92.33 96.38 72.75 96.88 74.38 98.46 91.19 81.80 97.16 82.73 74.26 93.92 87.02 90.00 66.02 74.84 57.79 79.68 59.61 87.72 71.88 74. 80.76 70.57 61.33 Tasks: Avg: Average over all tasks, NaQA: LEMBNarrativeQARetrieval, Needle: LEMBNeedleRetrieval, Passkey: LEMBPasskeyRetrieval, QMSum: LEMBQMSumRetrieval, SummScreen: LEMBSummScreenFDRetrieval, Wikim: LEMBWikimQARetrieval * Scores are in nDCG@1 ** 1st stage checkpoint before long context training was applied Table A19: Retrieval performance on RTEB (Public) (nDCG@10 [%]). Avg ACD AST LS LQA FinB HC3 FQA HuE MBPP MIR Apps DS1K WSQL CDR CURE Fsh Model Qwen3-4B jina-v4 70.8 39.4 81.2 66.4 66.7 77.5 68.9 63.4 98.4 91.4 69.5 89.2 64.1 66.5 45.2 50.1 59.9 63.9 79.9 63.4 68.4 97.2 89.9 62.9 78.3 64.1 64.2 36.1 79.0 63.6 53.4 74.8 54.5 56.3 92.3 87.0 61.2 75.3 59.7 Qwen3-0.6B 54.6 34.8 32.8 59.2 59.4 72.2 61.5 39.2 80.6 83.2 62.6 29.0 50.1 jina-v3 54.0 34.0 22.8 66.2 65.5 75.6 54.4 56.6 71.5 80.2 66.5 9.7 42.7 snowflake-l-v2 54.8 33.3 29.7 68.1 51.2 79.7 51.2 45.1 86.3 83.6 57.7 34.9 49.4 mult.-e5-l-instr. j-v5-text-small 66.8 43.9 53.3 64.9 63.6 80.6 62.1 55.7 96.1 90.5 66.6 73.3 61.4 KaLM-mini-v2.5 56.5 34.4 36.5 64.4 42.9 77.7 61.9 48.1 89.7 85.0 53.8 35.9 55.9 70.4 42.8 48.7 72.3 70.2 90.9 63.1 82.5 98.8 91.4 58.7 81.0 63.9 voyage-4-nano 63.8 32.9 30.7 68.4 60.1 77.8 59.8 54.7 99.0 88.4 64.6 84.1 57.3 Gemma-300M j-v5-text-nano 64.1 39.7 51.5 65.8 57.8 78.8 57.8 57.8 92.1 86.7 65.8 58.4 54.9 v5-small stage 1 v5-nano stage 1 64.1 40.9 49.5 68.7 62.2 80.2 58.0 57.8 94.1 88.7 64.6 63.0 60.0 61.1 39.1 47.0 66.9 55.5 76.5 54.7 56.8 90.6 85.5 63.9 45.1 51. 84.7 96.1 86.8 68.0 69.7 80.7 93.7 78.6 98.0 91.5 97.7 79.4 88.8 71.5 64.1 62.5 64.2 60.7 55.2 71. 67.2 67.0 63.4 69.4 67.9 67.2 56.8 43.2 53.2 27.5 47.0 37.9 46.3 30.5 54.7 32.3 42.8 27.6 53.6 39.2 42.6 29.6 53.5 43.1 57.1 30.5 52.8 38.3 52.8 38.4 51.2 38. Tasks: Avg: Average over all tasks, ACD: AILACasedocs, AST: AILAStatutes, LS: LegalSummarization, LQA: LegalQuAD, FinB: FinanceBenchRetrieval, HC3: HC3FinanceRetrieval, FQA: FinQARetrieval, HuE: HumanEvalRetrieval, MBPP: MBPPRetrieval, MIR: MIRACLRetrievalHardNegatives, Apps: AppsRetrieval, DS1K: DS1000Retrieval, WSQL: WikiSQLRetrieval, CDR: ChatDoctorRetrieval, CURE: CUREv1, Fsh: FreshStackRetrieval A.5 Learning Rate Ablation In this section, we detail the experimental setup used to determine the optimal hyperparameters for the training objectives discussed in Section 5.3.1. All experiments in this ablation were conducted using the S2ORC dataset with fixed student-side trainable projection. The models were trained using two GPUs with total batch size of 512 and maximum sequence length of 512. We investigated the sensitivity of InfoNCE (LNCE), feature-based distillation (Ldistill), and score-based distillation (Lscore) to two different learning rates: 1 104 and 1 105. The results, visualized in Figure 6, reveal distinct optimization behaviors: Feature-based Distillation (Ldistill): This objective is highly robust and performs significantly better with higher learning rate of 1 104. At 1 105, convergence is considerably slower, and the model fails to reach the same performance ceiling within the same number of training steps. Figure 6: Learning rate sensitivity across different optimization objectives. We report the average nDCG@10 on the MTEB (English, v2) benchmark using the S2ORC dataset. The plots compare 1104 (blue) and 1105 (orange) learning rates for embedding-based distillation (Ldistill), InfoNCE (Lqd NCE ), and score-based distillation (Lscore), all utilizing trainable student projection. InfoNCE (Lqd NCE): In contrast, the contrastive objective is more sensitive to larger gradients. While learning rate of 1104 shows faster start, the performance eventually degrades or plateaus lower than the more stable 1105 run, which maintains better long-term consistency. Score-based Distillation (Lscore): Similar to InfoNCE, score-based distillation benefits from the lower learning rate. The 1104 configuration exhibits unstable behavior, with performance dropping sharply after an initial peak, whereas 1105 results in steady, sustained improvement. Based on these observations, we selected 1 104 for all Ldistill experiments and 1 105 for LNCE and Lscore in our main results to ensure each method is evaluated at its respective peak potential. A.6 Evaluation of MMTEB Performance Across Languages Figure 7: Performance of Models on different languages on MMTEB compared to average performance To visualize language-specific performance, we compute the mean and standard deviation of the per-language average scores on MMTEB. We map the interval µ 3σ determined by the models performance for each language to color scale. Figure 7 shows the resulting heatmaps."
        }
    ],
    "affiliations": [
        "Jina by Elastic"
    ]
}