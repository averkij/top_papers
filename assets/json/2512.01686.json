{
    "paper_title": "DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models",
    "authors": [
        "Patrick Kwon",
        "Chen Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 6 8 6 1 0 . 2 1 5 2 : r : Story Visualization Pipeline via Subject and"
        },
        {
            "title": "Layout Customized Generation using Video Models",
            "content": "Patrick Kwon, Chen Chen Center for Research in Computer Vision, University of Central Florida, USA yo564250@ucf.edu, chen.chen@crcv.ucf.edu Figure 1. (left) Overview of DreamingComics, story visualization framework for multi subject and layout control. The dialogues are post-edited by humans. (right) Examples from DreamingComics, which is capable of generating layout-controlled stories with diverse art styles such as pencil illustrations, Disney-style animation, digital line-art, live-action drama, and cel animation."
        },
        {
            "title": "Abstract",
            "content": "Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, layout-aware story visualization framework. We build upon pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce masked condition loss to further constrain each subjects visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present comprehensive evaluation of our approach, showing 29.2% increase in character consistency and 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126. github.io/dreamingcomics/ 1. Introduction Story visualization [11, 33, 65, 79], the task of generating coherent visual sequences from textual narratives and character identities, has gained increasing attention with the advancement of generation models [2, 14, 40, 43] and controllable image customization methods [34, 35, 63, 69, 74]. However, despite rapid improvements, existing approaches still lack the visual control necessary for storytelling tasks. Firstly, controlling the position of multiple characters is crucial for scene design, yet text prompts alone lack the pixel-level precision needed to specify spatial layout and character identity. Although recent image customization methods for diffusion transformer (DiT) models [2, 14, 38] can control who appears in the image [35, 61, 63] or where objects are placed [6, 72], they struggle to support both simultaneously, resulting in images with overlapping characters or incorrect appearances. Moreover, preserving consistent artistic styles, especially in cartoon and flat-shaded illustrations, remains difficult, as image generative models are typically biased toward photorealistic rendering, often overriding stylistic intent. In addition, the lack of paired datasets with both subject identity and position signals Method Layout Control Story-Adapter [33] StoryDiffusion [79] TheaterGen [11] DiffSensei [62] UNO [63] DreamO [35] Eligen [72] RealGeneral [28] DRA-Ctrl [3] DreamingComics (Ours) Multi-Subject Interaction DiT architecture LLM-based Layout Table 1. Comparison of DreamingComics with prior works across various capabilities. hinders the development of layout-aware customization. These challenges are most apparent in the comics domain, where consecutive image panels must preserve character identity and style while conforming to diverse layout structures unique to comics. In this paper, we present DreamingComics, story visualization framework that simultaneously supports multi-subject identity/style preservation and layout control. We decompose the task into two modules: an LLM-based layout generator and layout-aware customization model, Dream-Illustrator. The layout generator produces comic-style layouts consisting of panels and character bounding boxes from given storyline, while Dream-Illustrator renders consistent characters within those regions. This enables image customization to accurately position characters instead of solely relying on text, and enhance usability by reducing the burden on users of designing intricate layouts. We build Dream-Illustrator on pretrained video DiT backbone [23, 73], which, unlike image-specific generators, offers weaker perceptual quality but strong spatiotemporal priors that enhance visual consistency. Through FramePack [73], we repurpose video model for image customization, enabling robust style generalization while keeping an image-level computation bottleneck. To control subject position by layout, we introduce RegionalRoPE, regional 3D Rotary Position Embedding (RoPE) scheme that aligns each subject with its spatial region. To further improve layout fidelity, we employ masked condition loss that penalizes attention beyond the assigned regions. Together, these mechanisms allow our model to generate identity-preserving images aligned with layouts. Lastly, we construct an image-layout-paired dataset for training DreamIllustrator by annotating high-quality video samples. In parallel, we reformulate comic datasets into text-layout pairs to train our layout generator. We evaluate our framework on benchmarks such as ViStoryBench [80] and DreamBench [39]. Extensive experiments demonstrate that DreamingComics can generate multi-subject customized images that closely align with target layouts while generalizing across broad spectrum of artistic styles (Fig. 1 (b)-(e)). In short, our main contributions are as follows. DreamingComics, layout-aware story visualization framework that supports multi-subject identity and style customization with spatial layout. An image customization model built on pretrained video DiT model, leveraging spatiotemporal priors to improve style and identity consistency. Layout-aware conditioning strategies: RegionalRoPE, masked condition loss, and identity-layout-paired dataset generation that jointly enable accurate spatial control and reduce identity leakage. An LLM-guided layout generation module, fine-tuned on structured comic layout data, for prompt-based layout planning with minimal user input. 2. Related Work Image Customization. Early methods for subject customization were based on finetuning the model for each specific subject [16, 18, 24, 46], achieving strong consistency but requiring training for each instance. Other tuning-free methods [25, 60, 69, 75] focused on injecting encoded results from an image encoder into U-Net [44] diffusion model, balancing efficiency with consistency. With the advent of new Diffusion Transformer (DiT) models [2, 14, 38], many image customization works [19, 35, 52, 61, 63, 64, 76] directly reason over reference and noise latents in unified attention space, improving subject fidelity without per-instance tuning. However, these approaches tend to rely on implicitly learned spatial cues, limiting spatial positioning. There have been several studies on spatial position control for both training-based [26, 56, 58, 59, 78] and training-free methods [1, 11, 32] based on pretrained U-Net diffusion model. Recently, methods such as Regional Prompting [6] and Eligen [72] have proposed regional masking over the full-attention structure to control DiT models. However, explicit multi-subject spatial layout control remains underexplored, particularly within the DiT framework. Although TheaterGen [11] supports multi-subject layout control, it generates each subject individually and merges them with ControlNet [74], omitting any interactions between subjects. Our work is in line with previous studies that employed video generative models for image-level tasks. For example, Frame2Frame [45] uses an image-to-video diffusion model [68] for image editing by generating sequence of frames, while FramePainter [77] and Object-Mover [70] finetune video model to perform object-level image editing tasks. Several works [3, 9, 28] have also used video models for controllable image generation, demonstrating their capabilities to unify diverse image tasks. However, neither of them targeted multisubject-driven generation aligned with spatial layout. Our work uses novel strategies to incorporate layout Figure 2. Overview of our image customization pipeline. The input reference images are encoded as token sequences c1:n along with the noise latent zt and the text latent zp, passed to the stream of diffusion transformer blocks. We calculate custom regional RoPE from the layout condition and apply it to the encoded references. During training, we calculate Masked Condition Loss between the cross-attention map and the given layout condition, encouraging the model to position references within the layout. control into video model, utilizing multiple subject reference images with spatial layout. Story Visualization. Recent advances in generative models and LLMs have resulted in significant progress in story visualization. Previous works on image-based story visualization [10, 30, 37, 57, 67, 79] focused on improving cross-frame alignment for images generated by U-Net-based image generator. Comic generation [8, 62] is also notable subtask within story visualization. Although previous works such as DiffSensei [62] were restricted to monochrome manga aesthetics, our approach supports wider range of artistic styles, from stylized cartoon art to color-heavy illustrations, allowing diverse and culturally agnostic storytelling. Several recent works use large language models to reason about scene layouts from text [11, 27]. For instance, TheaterGen [11] and AutoStudio [10] employ training-free LLM agents to predict image-level layouts. In contrast, our module is fine-tuned on curated comic data, enabling the generation of both panel-wise and page-level layoutsa granularity not addressed by prior work. While DiffSensei [62] also incorporates an MLLM for its inference, its usage is limited to an identity adapter and does not support layout generation. 3. Methodology In Sec. 3.1, we formulate our objectives and introduce our base model, HunyuanVideo-I2V [23]. In Sec. 3.2, we introduce our LLM-based layout generator that outputs multi-panel layouts from text input. In Sec. 3.3, we present Dream-Illustrator, an image customization model based on DiT video generator, adapted for layout-aware next-frame prediction through regional RoPE scheme and masked condition loss. In Sec. 3.4, we report the dataset generation pipeline for both comic layout and image customization dataset. 3.1. Problem Statement and Preliminaries Our objective is to visualize stories through comicstyle narrative. Given script = {T1, T2, ..., Tn}, the traditional goal of story visualization is to generate sequence of corresponding images {p1, p2, ..., pn} of the same size. Comics, however, are composed of image panels that have different resolutions and locations within single page [54]. Although several works use LLMs to predict layouts for single image [11, 27], they do not predict layouts for multiple panels, making them insufficient for comic generation. In contrast, we adopt comic-centric representation, where panel i, consisting of characters, is represented as tuple (Ti, Di, {BOXi,1, ..., BOXi,n}), (1) where Ti is the input text, Di R4 is the panels bounding box, and BOXi,j R4 are subject-level bounding boxes for each character j. This structure enables us to reason about both the positioning of panels within page and the positioning of characters within panels, reflecting the unique narrative of comics. To utilize its rich spatiotemporal priors for visual consistency, we base our image customization method on HunyuanVideo-I2V [23], video generation model composed of causal 3D Variational Autoencoder [22] (3DVAE, E), multimodal large language model (MLLM) encoder, and diffusion transformer with unified full attention mechanism across spatial and temporal tokens. Given an input video R(4T +1)316H16W , the VAE encodes it into latent representation R(T +1)162H2W , which is then patchified into sequence of visual tokens. The textual prompts Tp are then encoded as tokens using the MLLM encoder and concatenated with the visual tokens to form single sequence z, and passed into transformer with 3D Rotary Position Embeddings (RoPE) [51] to model positional relationships. For I2V generation, HunyuanVideo-I2V replaces the tokens of the first frame with the conditioning image tokens. The model is Figure 3. Given list of textual descriptions for each panel, the finetuned LLM outputs spatial layout for each panel and characters as set of bounding boxes. Note that our layout, compared to the layout generated from the same prompt using GPT-4 [36], occupies most of the panel region, correctly orders the panel (top-to-bottom, right-to-left), and draws plausible character boxes, constituting good comic layout. then optimized using flow matching loss [29] = vθ(yt, t, CI , TP ) (ϵ y)2, (2) where ϵ (0, I), yt = (1 t)y + tϵ, and vθ represent the parametric neural network. 3.2. Layout Generation Pipeline To generate page-level layouts from given set of text descriptions, we fine-tune large language model (LLM) [66] using supervised fine-tuning (SFT) based on newly crafted comics layout dataset. Given an input script , the model is trained to predict structured layout of panel regions and associated character positions, which are parsed as (Di, (BOXi,1, ..., BOXi,n)). These representations are later used as layout conditions in the image customization stage, alongside their corresponding character images. visualization of this layout generation process is shown in Fig. 3. By training an LLM with strong spatial reasoning over domain-specific data and making it understand the sparse visual cues within comics, we reduce the burden on users to manually specify intricate layouts while enabling the model to follow complex, prompt-driven visual instructions. 3.3. Dream-Illustrator Using FramePack as next-frame predictor. Since video generation models handle sequences of semantically consistent frames, they capture rich cross-frame context and complex spatiotemporal priors during training, which motivated us to use them for image customization; specifically, by using the reference image as the first frame to generate subsequent target frame. Previous approaches in using video models for image tasks [3, 28, 45] required generating multiple frames for single image output. At the same time, treating the problem as simple two-frame video generation problem (generating the frame at = 1 conditioned on = 0) results in rigid copy-pasting artifacts, with limited variation and poor prompt adherence. (a) Reference Image with original RoPE (b) Generated Image (c) Reference Image with regional RoPE (d) Generated Image Figure 4. For illustration, we visualize the original positional indices for RoPE at (a) and the new indices at (c). The blue square indicates the intended layout region, and the red square indicates the actual generated region. The original RoPE restricts the reference content to the top-left corner, while ours can correctly position it according to the given layout. To address this, we build upon FramePack [73], finetuned video DiT model that generates video frames in progressive manner. Originally designed for efficient long-range video generation, FramePack compresses sequence of input frames (F RT hwc) to sample output frames (X RShwc). Crucially, FramePack is capable of generating frames that are temporally distant from the inputfor example, by producing an output at = 9 from reference at = 0. We leverage this property to reformulate FramePack as an image customization module: given target timestep and set of reference images (F = F1, F2, ..., Fn) at = 0, our goal is to generate single target frame that preserves subject identities while aligned with the input prompt. In practice, we encode each reference image Fi into the latent space using VAE (ci = E(Fi)), and concatenate the patched latents with the latent noise zt and the latent text zp. Compared to prior approaches [3, 9, 28], our method leverages video model priors while generating single frame, maintaining relatively low computation bottleneck. As result, we can infer 1280 720 image within 17 seconds, over 3 faster than DRA-Ctrl [3]. Detailed comparisons of inference time and memory are provided in the Supplementary. Regional RoPE for layout control. The default RoPE assigns the same starting coordinates (0, 0) to all reference frames, which makes the model perceive all subjects as if they are originating from the same region, leading to spatial entanglement and identity collapse  (Fig. 4)  . To enable precise layout-driven spatial control, we introduce RegionalRoPE, deterministic mapFigure 5. Comparison between the CAMs of our model trained without the masked condition loss (top) and with the masked condition loss (bottom). Using our new loss helps to position the attention around the target layout, which is evident in the third column (Transformer layer = 2), naturally inducing the model during training to position the character. ping of each references RoPE indices to its target layout box. Unlike previous works that modify RoPE for spatial decorrelation [52, 53, 63], we use RegionalRoPE for explicit spatial grounding. As mentioned in Sec. 3.1, we have region BOXi = [wstart, hstart, wend, hend] for each reference latent ci Rhiwid. For the region size (Wbox, Hbox), to preserve the latent aspect ratio of the reference while fitting within the region, we compute scaling factor = min and define the adjusted RoPE grid size as (W , ) = (s wi, hi). The RoPE grid is then positioned within the layout region with ranges (cid:16) Wbox wi , Hbox hi (cid:17) start = wstart + start = hstart + (Hbox ), h end = end = start + start + (4) (3) , Wbox where [0, 1] controls the vertical alignment (a=0 for top-aligned, a=0.5 for center-aligned). The indices for each latent pixel (i, j) are finally mapped as (cid:16) (cid:17) (5) i, 0, start + start + hi (t, i, j) = wi Each reference latent is encoded independently with its own coordinates, and the resulting sequences are concatenated into the input stream along with the target tokens. Previous works like UNO [63] and Ominicontrol [52, 53] modify RoPE to decorrelate spatial cues and reduce reference copying, but they do not enforce explicit spatial alignment. In contrast, we assign RoPE offsets for each reference based on its region, enabling precise location mapping. Unlike other works [3, 28] which resize inputs to fixed frame sizes, we operate on native-resolution cropped latents, preserving subject fidelity and improving efficiency. Masked condition loss. While RegionalRoPE aligns latents without extra training, its naive application can still lead to identity distortion and copy-paste artifacts. To address this, we introduce masked condition loss that supervises each subjects spatial attention. During diffusion, we extract the cross-attention map (CAM) between the reference and the generated result as t,blockj Qci,t,blockj CAMci,t,blockj = (6) where Qci,t,blocki are the tokens of the i-th reference image and Kt,blockj are the tokens of the noisy latent at timestep t, DiT layer j. Averaging these across the timestep and normalizing to [0, 1] yields CAMci,blockj , visualizing each subjects attention region. To ensure that these spatial attentions respect their layout boundaries, we introduce ReLU-based loss that operates on each CAM and the subject-wise binary spatial mask MASKi. Here, MASKi is binary mask from the bounding box BOXi, with in-layout pixels represented as 1 and others as 0. The masked condition loss, with respect to the DiT layer 2, is then defined as Lmask = 1 nc i= nc(cid:88) ReLU(CAMci,block2 MASKi) (7) where nc refers to the number of conditions. Here, the ReLU penalizes only the attention leakage beyond the defined layout region while preventing unnecessary suppression of valid in-region focus. The final loss combines this term with the diffusion objective = Ldiff + λmaskLmask (8) This constraint guides the model to respect spatial boundaries and maintain subject-specific attention, mitigating identity bleeding and visual artifacts. The effects of our new training loss are illustrated in Fig. 5. Additionally, motivated by previous works [3, 28] for multiple subjects, we apply attention masking such that each reference latent should not be visible to one another to avoid information leakage. More details on this feature are provided in the Supplementary. 3.4. Dataset Generation Pipeline Comics layout dataset. To train our layout generator, we compile and annotate data from three comic datasets: Method Base Model CIDS (Char) CSD (Style) OCCM Layout Precision Self Cross Cross Score Self Inception Aesthetics Score Score Copy-Paste Baseline Story-Adapter [33] StoryDiffusion [79] DiffSensei [62] Eligen [72] UNO [63] DreamO [35] Vlogger [81] RealGeneral [28] DRA-Ctrl [3] Ours - SDXL [40] SDXL [40] SDXL [40] FLUX [2] FLUX [2] FLUX [2] Vlogger [81] CogVideoX [68] HunyuanVid [23] FramePack [73] 94.1 35.3 30.1 47.5 35.9 46.2 51.6 33.9 37.5 36.2 66.6 98.7 57.4 46.9 62.5 56.0 61.1 63.1 54.2 54.2 50.2 68.1 72.8 29.6 31.8 31.5 29.7 39.3 38.3 28.3 38.6 39.0 53. 71.5 63.0 52.2 65.2 61.7 59.3 58.8 47.2 52.7 58.0 58.0 99.7 78.8 67.7 85.9 78.5 83.8 85.7 76.6 78.1 74.9 86.7 - - - 42.0 39.7 - - - - - 61.6 6.72 9.79 7.36 5.53 8.49 8.42 7.86 7.46 7.82 8.77 6.81 4.48 5.55 4.96 4.47 5.81 5.20 5.51 4.25 5.08 5.03 4.54 Table 2. Quantitative evaluation results on ViStoryBench. The bold value is the best, and the underlined value is the second best. COMICS [20], Manga109 [15], and PopManga [47], capturing varied styles and eras in comics. For datasets lacking annotations, we apply the MagiV2 panel detector [48] to extract panel and character bounding boxes. All data are standardized, with inconsistent samples filtered out. We generate panel and page-wise descriptions using Qwen2.5-VL [66]. Paired subject dataset. Public datasets rarely provide paired samples with references and layout conditions, which are critical for training Dream-Illustrator. To address this, we design novel data framework that samples image pairs from curated video sources. Specifically, we build on OpenS2V-Nexus [71], large-scale dataset with structured annotations. We select videos with at least one consistently framed human subject. Using the segmentation map for the first frame, we extract its human bounding boxes as target layout conditions. The source frame is then chosen from distant timestamp, ensuring subject continuity through facial presence. Samples are retained only if both the target (TopIQ) and the source face (TopIQ-Face) [7] scores exceed the quality thresholds. To cover diverse artistic styles, we apply similar procedure to Anime-Shooter [41], reference-guided multi-shot animation dataset. Additionally, we processed the high-quality subset of Subject200K [52] following DreamO [35], using LISA to predict subject masks for the construction of paired data. All datasets are publicly released with their usage restricted to the research community, and we have fully adhered to their usage guidelines. In total, we compiled 55K single-subject and 20K multi-subject paired samples, with further details and ethical considerations provided in the Supplementary. 4. Experiments 4.1. Implementation Details Layout Generation Model Training. We finetune Qwen2.5-VL (7B) [66], which has outperformed alternatives on comic layout understanding tasks [55], on 25K annotated comic layouts using supervised finetuning. Layouts are represented as dictionaries of normalized bounding boxes over fixed panel counts. We apply LoRA with rank 8, α = 16, dropout=0.05, and train using the AdamW optimizer (lr=5e-4). Dream-Illustrator Training. We finetuned HunyuanVideo-I2V [23] attached with the FramePack [73] LoRA weights. To support multi-subject conditioning and reduce copy-paste artifacts, we remove Training is image projection module. the original performed using LoRA [17] with rank 32, AdamW [31] optimizer (lr=2e-4), batch size 8, and mixed precision on 2NVIDIA H100 GPUs. The λmask is set to 0.05. We train for 6K steps on single-subject samples, followed by 3K steps on multi-subject samples. 4.2. Image Customization Evaluation We evaluate DreamingComics using ViStoryBench [80], the first comprehensive story visualization benchmark. The benchmark primarily measures Character Similarity (CIDS) [4, 12, 21, 42] and Style Similarity (CSD) [50] between reference and generated frames (cross) and between generated frames (self). The benchmark also defines Onstage Character Count Matching (OCCM), Inception Score (IS) [49], and Aesthetic Score [13]. To measure subject-layout fidelity, we add layout precision metric for methods that support layout-based customization [62, 72]. We also evaluate our method on DreamBench++ [39] for single-subject driven generation against other image customization methods. Further details on the evaluation procedure and metrics for quantitative evaluations are provided in Supplementary. As shown in Table 2, DreamingComics achieves substantial improvements in both character and style preserIt achieves the highest CIDS-cross score of vation. 66.6, surpassing the next best method (DreamO [35], 51.5) by 29.2%, and achieves CSD-cross score of 53.6, highlighting our ability to maintain both character identity and artistic style. Our model also achieves an OCCM score of 86.7 and layout precision score of 61.6, confirming correct layout-wise character generation. Although our Aesthetic Score and Inception Figure 6. Image level qualitative comparison against other methods, along with their respective scores. Our method shows enhanced visual consistency with respect to the reference images, which is reflected in the similar aesthetic scores. Method DINO CLIP-I CLIP-T DreamBooth [46] UNO [63] DreamO [35] Ours 53.34 56.24 58.59 60. 75.30 77.96 79.35 79.50 32.26 33.23 32.96 32.59 Table 3. Quantitative results on DreamBench. Score are comparable to the copy-paste baseline, this reflects our deliberate preservation of stylized, nonphotorealistic appearancesan area where video models, despite lower perceptual sharpness, excel in maintaining stylistic consistency (e.g., Fig. 6). We also present the results for DreamBench++ in Table 3, which shows that our model achieves the highest DINO [5] and CLIP-I [42] scores, with CLIP-T [42] scores that are comparable to other methods. Fig. 6 displays an image-level comparison with other methods [35, 63]. Unlike other methods, our method utilizes layouts to designate each characters position, enhancing controllability while preventing superfluous or erroneous character creation (e.g., DreamO [35], second and third rows). Also, the results from other methods produce higher aesthetic scores, even when their output style largely deviates from that of the references (e.g., first row). This is because aesthetic scoring tends to penalize images that do not look photorealistic rather than evaluating for diverse styles. However, our method can faithfully adhere to the given reference style, which is indicated by the similarity in aesthetic scores. We also present comic-level comparison in Fig. 7, where we first generate layouts from the input panelwise captions and use them along with the reference images and captions to synthesize full-length comicstyle story. Layout-based methods like DiffSensei [62] and Eligen [72] fail to preserve the aesthetics of images, Method Panel Coverage Count Ratio Panel Valid Ordering Characters Character Count Ours GPT-4 [36] 100.0 87.67 79.14 29. 99.00 66.67 100.0 57.67 86.30 76.47 Table 4. Evaluation results on our layout generator. either because they do not allow image-level inputs or fail to preserve the input style. Image-based customization methods such as RealGeneral [28], UNO [63], and DreamO [35] cannot utilize layout conditions, which creates confusion in placing characters. For instance, DreamO places the gray-colored Domestic Donkey on the right side and the brown-colored Wild Donkey on the left, which is not intended in its caption. In contrast, our method produces accurate and consistent story visualizations in terms of identity consistency, style preservation, prompt adherence, and layout composition. 4.3. Layout Generator Evaluation We compare our layout generator with GPT-4 [36] to evaluate the validity of our method. We use detailed prompt with in-context examples to guide GPT-4 to generate layouts as list of bounding boxes, and evaluate the results according to the following categories: Panel Count: correct number of panels should be present per page. Page coverage: Panels should collectively cover the page without large empty regions. Panel ordering: Panels should follow the conventional comic order (top-to-bottom, right-to-left). Valid Character: The character bounding box should cover reasonable area within the panel. Character Count: correct number of character bounding boxes should be present per panel. We present the results in Table 4, which shows our models improvements in understanding and generating Figure 7. Story-level qualitative comparison of DreamingComics. Methods marked with use the layout condition generated from our layout generator. Methods marked with use the reference images. Better viewed with zoom-in. Regional RoPE Masked Condition Loss CIDS (Cross/Self) CSD (Cross/Self) 38.7 / 50.3 56.6 / 61.2 58.5 / 63.0 41.0 / 52.9 50.5 / 56.3 52.9 / 62.4 Method Preference (%) Character Identity Character Style Story Consistency Layout Plausibility Ours vs. Baseline 80.0 83. 86.2 69.2 Table 5. Ablation study on the impact of RegionalRoPE and masked condition loss Our final model achieves the highest identity and style preservation performance without relying on regional attention masking. Name CIDS (Cross/Self) CSD (Cross/Self) Timestamp (t = 1) Timestamp (t = 5) Timestamp (t = 9) Ours (t = 3) 55.3 / 58.8 55.7 / 58.9 54.7 / 58.2 58.5 / 63.0 50.3 / 56.1 50.2 / 54.3 49.7 / 56.1 52.9 / 62.4 Table 6. Ablation study on the impact of choosing the right timestamp for the target image. comic-structured layouts. 4.4. Ablation Study We perform an ablation study to assess the contribution of our main components: RegionalRoPE and masked condition loss. As shown in Table 5, removing RegionalRoPE leads to the most significant performance drop, highlighting its role in spatial disentanglement. Excluding the masked condition loss also degrades layout fidelity. Overall, the combination of both components yields the best performance, validating our use of layout conditioning over rigid attention masking [6, 72]. Additionally, Table 6 evaluates the effect of choosing the target timestamp for FramePack [73]. We observe that = 3 achieves the best trade-off between identity preservation and stylistic variation, as lower values produce rigid results, while higher values introduce undesirable drift. All models were trained for 6K steps on the single-subject samples and evaluated on ViStoryBench. We also include ablation studies on choosing λmask values in Supplementary. Table 7. User study results: percentage of participants who preferred our method over the baseline across four questions. 4.5. User Study To evaluate the subjective quality of our framework, we conducted user study with 26 participants who were shown image pairs for four evaluation criteria: character identity, character style, story consistency, and layout plausibility. Each participant was presented with 20 comparison questions and was asked to select the preferred output. For the first 15 questions (three categories), the participants compared our results with those of DreamO [35] and UNO [63]. The remaining questions compare our layout generator with GPT-4. The user study questionnaire is included in Supplementary. As shown in Table 7, our work demonstrates strong advantage in both image quality and layout control. 5. Conclusion We present DreamingComics, novel framework for subjectand layout-controllable story visualization. The problem is decomposed into layout generation and layout-based image customization. Our image customization module builds on DiT video generation model, leveraging its spatiotemporal priors to ensure visual consistency across diverse artistic styles. For intuitive and semantically rich layout creation, we use finetuned LLM-based layout generator. To enable layoutbased control with multiple subjects, we introduce RegionalRoPE and masked condition loss. Quantitative results and user studies demonstrate that DreamingComics produces faithful layouts and stylistically coherent visual narratives, establishing strong foundation for future work in controllable story generation."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: fusing diffusion paths for controlled image generation. In Proceedings of the 40th International Conference on Machine Learning, 2023. 2 [2] Black Forest Labs. FLUX: High-Fidelity Image Generation Model. https://github.com/blackforestlabs /flux ? tab = readmeovfile, 2025. Accessed: 2025-08-01. 1, 2, 6 [3] Hengyuan Cao, Yutong Feng, Biao Gong, Yijing Tian, Yunhong Lu, Chuang Liu, and Bin Wang. DimensionVideo Generative Models are Reduction Attack! ArXiv, Experts on Controllable Image Synthesis. abs/2505.23325, 2025. 2, 4, 5, 6 [4] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. Vggface2: dataset for recognising faces across pose and age. 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), pages 6774, 2017. 6 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 96309640, 2021. 7 [6] Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang. Training-free regional prompting for diffusion transformers. arXiv preprint arXiv:2411.02395, 2024. 1, 2, 8 [7] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 33:24042418, 2024. [8] Siyu Chen, Dengjie Li, Zenghao Bao, Yao Zhou, Lingfeng Tan, Yujie Zhong, and Zheng Zhao. Manga generation via layout-controllable diffusion, 2024. 3 [9] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe L. Lin, and Hengshuang Zhao. Unireal: Universal image generation 2025 and editing via learning real-world dynamics. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1250112511, 2025. 2, 4 [10] Junhao Cheng, Xi Lu, Hanhui Li, Khun Loun Zai, Baiqiao Yin, Yuhao Cheng, Yiqiang Yan, and Xiaodan Liang. Autostudio: Crafting consistent subjects in multi-turn interactive image generation. ArXiv, abs/2406.01388, 2024. 3 [11] Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, et al. Theatergen: Character management with llm for consistent multi-turn image generation. arXiv preprint arXiv:2404.18919, 2024. 1, 2, 3 [12] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 46854694, 2019. 6 [13] discus0434. Aesthetic predictor v2.5. https : / / github . com / discus0434 / aesthetic - predictor-v2-5, 2024. Accessed: 2025-08-01. 6 [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the 41st International Conference on Machine Learning, 2024. 1, 2 [15] Azuma Fujimoto, Toru Ogawa, Kazuyoshi Yamamoto, Yusuke Matsui, T. Yamasaki, and Kiyoharu Aizawa. Manga109 dataset and creation of metadata. Proceedings of the 1st International Workshop on coMics ANalysis, Processing and Understanding, 2016. 6 [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing textArXiv, to-image generation using textual inversion. abs/2208.01618, 2022. 2 [17] J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021. 6 [18] Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, and Qian He. Dreamtuner: Single image is enough for subject-driven generation, 2023. [19] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arxiv:2410.23775, 2024. 2 [20] Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Lee Boyd-Graber, Hal Daume, and Larry S. Davis. The amazing mysteries of the gutter: Drawing inferences between panels in comic book narratives. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 64786487, 2016. 6 [21] Minchul Kim, Anil K. Jain, and Xiaoming Liu. Adaface: In 2022 Quality adaptive margin for face recognition. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1872918738, 2022. 6 [22] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. 3 [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jia-Liang Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fan Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Peng-Yu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhen Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Z. Xu, Yang-Dan Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. ArXiv, abs/2412.03603, 2024. 2, 3, 6 [24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19311941, 2022. 2 [25] Dongxu Li, Junnan Li, and Steven C.H. Hoi. Blipdiffusion: pre-trained subject representation for controllable text-to-image generation and editing. In Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023. 2 [26] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. GLIGEN: Open-Set Grounded Text-to-Image GenIn 2023 IEEE/CVF Conference on Computer eration . Vision and Pattern Recognition (CVPR), pages 22511 22521, Los Alamitos, CA, USA, 2023. IEEE Computer Society. 2 [27] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. Trans. Mach. Learn. Res., 2024, 2023. [28] Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong Mao. Realgeneral: Unifying visual generation via temporal in-context learning with video models, 2025. 2, 4, 5, 6, 7 [29] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. ArXiv, abs/2210.02747, 2022. 4 [30] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm - openended visual storytelling via latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6190 6200, 2024. 3 [31] Ilya Loshchilov and Frank Hutter. Decoupled weight In International Conference on decay regularization. Learning Representations, 2017. 6 [32] Wan-Duo Kurt Ma, J. P. Lewis, Avisek Lahiri, Thomas Leung, and W. Bastiaan Kleijn. Directed diffusion: Direct control of object placement through attention guidance. 2023. [33] Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, and Yuyin Zhou. Story-Adapter: Training-free Iterative Framework for Long Story Visualization, 2024. 1, 2, 6 [34] Chong Mou, Xintao Wang, Liangbin Xie, Jing Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In AAAI Conference on Artificial Intelligence, 2023. 1 [35] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, Mengtian Li, Songtao Zhao, Jian Zhang, Qian He, and Xinglong Wu. Dreamo: unified framework for image customization. ArXiV, abs/2504.16915, 2025. 1, 2, 6, 7, 8 [36] OpenAI. Gpt-4 technical report. 2023. 4, 7 [37] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. Synthesizing coherent story with auto2024 IEEE/CVF regressive latent diffusion models. Winter Conference on Applications of Computer Vision (WACV), pages 29082918, 2022. 3 [38] William Peebles and Saining Xie. Scalable diffusion In 2023 IEEE/CVF Intermodels with transformers. national Conference on Computer Vision (ICCV), pages 41724182, 2023. 1, [39] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: humanaligned benchmark for personalized image generation. In The Thirteenth International Conference on Learning Representations, 2025. 2, 6 [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1, 6 [41] Lu Qiu, Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, and Xihui Liu. Animeshooter: multi-shot animation dataset for reference-guided video generation, 2025. 6 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual In Internamodels from natural language supervision. tional Conference on Machine Learning, 2021. 6, 7 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings synthesis with latent diffusion models. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 1 [44] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and ComputerAssisted Intervention MICCAI 2015, pages 234241, Cham, 2015. Springer International Publishing. 2 [45] Noam Rotstein, G. Yona, Daniel Silver, Roy Velich, David Bensaid, and Ron Kimmel. Pathways on the image manifold: Image editing via video generation. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 78577866, 2024. 2, 4 [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subjectdriven generation. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2250022510, 2022. 2, 7 [47] Ragav Sachdeva and Andrew Zisserman. The manga whisperer: Automatically generating transcriptions for comics. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12967 12976, 2024. [48] Ragav Sachdeva, Gyungin Shin, and Andrew Zisserman. Tails tell tales: Chapter-wide manga transcriptions with [49] Tim Salimans, character names. In Proceedings of the Asian Conference on Computer Vision (ACCV), pages 20532069, 2024. 6 Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Proceedings of the 30th International Conference on Neural Information Processing Systems, page 22342242, Red Hook, NY, USA, 2016. Curran Associates Inc. 6 [50] Gowthami Somepalli, Anubhav Gupta, Kamal K. Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. ArXiv, abs/2404.01292, 2024. 6 [51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomput., 568(C), 2024. 3 [52] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. 2025. 2, 5, 6 [53] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Efficient conditioning for diffusion transformers. ArXiv, abs/2503.08280, 2025. Ominicontrol2: [54] Emanuele Vivoli, Andrey Barsky, Mohamed Ali Souibgui, Artemis LLabres, Marco Bertini, and Dimosthenis Karatzas. One missing piece in vision and lanArXiv, guage: survey on comics understanding. abs/2409.09502, 2024. 3 [55] Emanuele Vivoli, Artemis Llabres, Mohamed Ali Souibgui, Marco Bertini, Ernest Valveny Llobet, and Dimosthenis Karatzas. Comicspap: Understanding comic strips by picking the correct panel. In Document Analysis and Recognition ICDAR 2025, pages 337350, Cham, 2025. Springer Nature Switzerland. 6 [56] Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. Compositional text-to-image synthesis with attention map control of diffusion models. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence. AAAI Press, 2024. 2 [57] Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen. Autostory: Generating diverse storytelling images with minimal human effort. ArXiv, abs/2311.11243, 2023. 3 [58] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Instancediffusion: Rohit Girdhar, and Ishan Misra. Instance-level control for image generation, 2024. 2 [59] X. Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot ArXiv, image personalization with layout guidance. abs/2406.07209, 2024. 2 [60] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized textto-image generation. arXiv preprint arXiv:2302.13848, 2023. 2 [61] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. ArXiV, abs/2506.18871, 2025. 1, 2 [62] Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, Xiangtai Li, and Yunhai Tong. Diffsensei: Bridging multi-modal llms and diffusion models for customized In Proceedings of the IEEE/CVF manga generation. Conference on Computer Vision and Pattern Recognition (CVPR), pages 2868428693, 2025. 2, 3, 6, 7 [63] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. 2025 IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 1, 2, 5, 6, 7, 8 [64] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1329413304, 2024. [65] Xuenan Xu, Jiahao Mei, Chenliang Li, Yuning Wu, Ming Yan, Shaopeng Lai, Ji Zhang, and Mengyue Wu. Mmstoryagent: Immersive narrated storybook video generation with multi-agent paradigm across text, image and audio. arXiv preprint arXiv:2503.05242, 2025. 1 [66] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. 4, 6 [67] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. 3 [68] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 6 [69] Hu Ye, Jun Zhang, Siyi Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. ArXiv, abs/2308.06721, 2023. 1, 2 [70] Xin Yu, Tianyu Wang, Soo Ye Kim, Paul Guerrero, Xi Chen, Qing Liu, Zhe L. Lin, and Xiaojuan Qi. Objectmover: Generative object movement with video prior. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1768217691, 2025. 2 [71] Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, and Li Yuan. Opens2vnexus: detailed benchmark and million-scale dataset for subject-to-video generation. ArXiv, abs/2505.20292, 2025. [72] Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, and Yu Zhang. Eligen: Entity-level controlled image generation with regional attention. arXiv preprint arXiv:2501.01097, 2025. 1, 2, 6, 7, 8 [73] Lvmin Zhang and Maneesh Agrawala. Packing input frame contexts in next-frame prediction models for video generation. Arxiv, 2025. 2, 4, 6, 8 [74] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 38133824, 2023. 1, 2 [75] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. 2 [76] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. 2 [77] Yabo Zhang, Xinpeng Zhou, Yihan Zeng, Hang Xu, Hui Li, and Wangmeng Zuo. Framepainter: Endowing interactive image editing with video diffusion priors. 2025 IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [78] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68186828, 2024. 2 [79] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. NeurIPS 2024, 2024. 1, 2, 3, 6 [80] Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Zhewei Huang, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, Xuanyang Zhang, Xianfang Zeng, Gang Yu, and Xiaoyan Zhang. Vistorybench: Comprehensive benchmark suite for story visualization. ArXiV, abs/2505.24862, 2025. 2, 6 [81] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. VlogIn Proceedings of the ger: Make your dream vlog. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 88068817, 2024."
        }
    ],
    "affiliations": [
        "Center for Research in Computer Vision, University of Central Florida, USA"
    ]
}