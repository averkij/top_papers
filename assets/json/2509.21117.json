{
    "paper_title": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them",
    "authors": [
        "Yidong Wang",
        "Yunze Song",
        "Tingyuan Zhu",
        "Xuanwang Zhang",
        "Zhuohao Yu",
        "Hao Chen",
        "Chiyu Song",
        "Qiufeng Wang",
        "Cunxiang Wang",
        "Zhen Wu",
        "Xinyu Dai",
        "Yue Zhang",
        "Wei Ye",
        "Shikun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A>B>C>A) and equivalence contradictions (A=B=C\\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudge's components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at https://github.com/TrustJudge/TrustJudge."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 7 1 1 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TRUSTJUDGE: JUDGE AND HOW TO ALLEVIATE THEM INCONSISTENCIES OF LLM-AS-AYidong Wang1 Yunze Song2 Tingyuan Zhu3 Xuanwang Zhang4 Zhuohao Yu1 Hao Chen5 Chiyu Song6 Qiufeng Wang7 Cunxiang Wang6 Zhen Wu4 Xinyu Dai4 Yue Zhang6 Wei Ye1 Shikun Zhang1 1 Peking University 2 National University of Singapore 3 Institute of Science Tokyo 4 Nanjing University 5 Google DeepMind 6 Westlake University 7 Southeast University"
        },
        {
            "title": "ABSTRACT",
            "content": "The adoption of Large Language Models (LLMs) as automated evaluators (LLMas-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A > > > A) and equivalence contradictions (A = = = A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudges components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at https://github.com/TrustJudge/TrustJudge."
        },
        {
            "title": "INTRODUCTION",
            "content": "The widespread adoption of LLM-as-a-judge approaches has offered scalable and effective alternative to costly human assessments Chang et al. (2024); Fu et al. (2023); Lin & Chen (2023); Sottana et al. (2023); Huang et al. (2024); Koutcheme et al. (2024); Song et al. (2024); Zhu et al. (2023). Beyond evaluation, this paradigm also actively contributes to model improvement, enabling self-refinement through iterative feedback Yuan et al.; Wu et al. (2024); Wang et al. (2025c) and collaborative progress via mutual assessment Wang et al.; Li et al.. These LLM-as-a-judge frameworks typically implement two evaluation protocols Li et al. (2023); Chen et al. (2024a); Li et al. Equal contribution. Correspondence: wye@pku.edu.cn, zhangsk@pku.edu.cn."
        },
        {
            "title": "Preprint",
            "content": "(2025); Chen et al. (2024b); Tan et al. (2024); Thakur et al. (2024); Szymanski et al. (2025); Raju et al. (2024)n: single-score assessment, where judge LLM (either general-purpose or specifically fine-tuned for evaluation) assigns integer numerical ratings to model outputs Zheng et al. (2023); Wang et al. (2024), and pairwise comparison, where the judge evaluates competing responses in direct comparison (with the order of responses swapped in two separate evaluations to eliminate position bias) to produce preference judgments Dubois et al. (2024); Wang et al.; Li et al. (2024). However, our research identifies two critical inconsistencies in these LLM-as-a-judge evaluation frameworks for LLMs: (1) Score-Comparison Inconsistency between single-score and pairwise comparison assessment, where LLMs with lower absolute scores may outperform higher-scored counterparts in pairwise comparisons (Rx Ry despite score(Rx) < score(Ry))1; and (2) Pairwise Transitivity Inconsistency in pairwise comparison evaluation, where judgments show non-transitive cycles (Rx Ry Rz Rx) and equivalence contradictions (Rx Ry Rz = Rx), violating rational preference principles. While prior work addresses pairwise inconsistencies through complex mathematical modeling Xu et al.; Zhang et al., such continual training risks compromising model generalizability Luo et al. (2023); Lin et al. (2024) without resolving score-comparison conflicts. To the best of our knowledge, this is the first work to systematically expose both inconsistencies as foundational weaknesses in LLM-as-a-judge frameworks and to provide unified resolution. Figure 1: Left: Average entropy of Llama-3 Grattafiori et al. (2024) models single-score outputs over six rounds on 1,200 instructions, by scoring strategy. Right: Breakdown of circularvs. inequalitytransitivity errors in pairwise-comparison tests. To address these inconsistencies, we introduce TrustJudge, probabilistic evaluation framework that preserves judgment entropy while resolving both (a) score-comparison conflicts via distributionsensitive scoring and (b) transitivity violations through likelihood-aware aggregation. As shown in Figure 1, we argue that the score-comparison inconsistency primarily stems from information loss in the integer scoring systemthe coarse-grained 5-point scale compresses nuanced quality differences into identical scores (e.g., different quality responses both receiving 4 points), resulting in low entropy judgments that fail to discriminate actual performance gaps. We propose two effective solutions: (1) increasing scoring granularity (510100 points) to preserve more discriminative information, and (2) probabilistic scoring that maintains the full entropy of model judgments. For pairwise transitivity inconsistency, we find most of inconsistencies originate from tie judgments (equivalence contradictions). We propose breaking ambiguous ties by either (1) preferring responses whose entire sentence exhibits lower perplexity, or (2) deciding preference based on the judges tokenlevel confidence for win, tie and lose. These approaches significantly reduce inconsistency rates while maintaining the scalability and effectiveness of LLM-as-a-judge frameworks. While recent works like Liu et al. (2023); Wang et al. (2025b) adopt probabilistic scoring to enhance human alignment, our motivation differs fundamentally in addressing fundamental inconsistencies of evaluation frameworks rather than improving human-judge agreement. Our probabilistic scoring serves as an entropypreserving mechanism with granularity enhancement to resolve score-comparison conflicts. Our theoretical analysis shows that discrete scoring systems suffer from information loss by showing that distinct response distributions can yield identical scores despite different entropies, whereas TrustJudges distribution-sensitive scoring preserves and distinguishes these differences, and further 1We use Rx, Ry, Rz to represent distinct LLM responses, : strictly preferred; : strictly worse; : preferred or equivalent; : worse or equivalent; : equivalent; =: not equivalent"
        },
        {
            "title": "Preprint",
            "content": "demonstrates that its PPL-based confidence distribution reduces uncertainty in ambiguous cases by leveraging perplexity to create lower-entropy signal for decision-making. Extensive experimental results across multiple model families (Llama-3, GPT, Qwen, Gemma) and scales (3B to 70B parameters) demonstrate TrustJudges effectiveness. Our framework reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%) when using Llama-3.1-70B-Instruct as judge. These improvements are achieved while maintaining or improving evaluation accuracy, with exact match rates increasing by 1.19-6.85% across different model sizes. Our ablation studies confirm that both distribution-sensitive scoring and likelihood-aware aggregation contribute significantly to these improvements, with the full framework achieving the best performance across all tested scenarios. In conclusion, we present the first systematic analysis of fundamental inconsistencies in LLM-asa-judge evaluation frameworks, identifying two critical limitations: (1) information loss in discrete scoring systems causing Score-Comparison Inconsistency, and (2) ambiguous tie judgments leading to Pairwise Transitivity Inconsistency. TrustJudge addresses these through distribution-sensitive scoring that preserves judgment entropy and likelihood-aware aggregation to break ambiguous ties. Our experiments demonstrate significant inconsistency reductions while maintaining evaluation accuracy across diverse models and tasks. This work provides both insights into LLM evaluation limitations and practical solutions for more reliable automated assessment, enabling more trustworthy deployment of LLM-as-a-judge paradigms in research and applications. In addition, we provide more detailed review of related work in Appendix A."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "Our framework addresses two fundamental inconsistencies in LLM-as-a-judge systems through formal definitions and quantitative metrics. We first establish mathematical characterizations of these inconsistencies, then present our TrustJudge algorithm. 2.1 INCONSISTENCY DEFINITIONS AND METRICS Definition 2.1 (Score-Comparison Inconsistency). For responses Rx, Ry with scores Sx, Sy (e.g., 1-5 scale) and pairwise comparison = C(Rx, Ry) {1, 0, 1} (1: Rx preferred, -1: Ry preferred, 0: tie), inconsistency occurs when: (Sx > Sy 0) (Sx < Sy 0) (Sx = Sy = 0). (1) (cid:80)n The Conflict Ratio CR = 1 numbers in the test set and I[] is the indicator function. Definition 2.2 (Pairwise Transitivity Inconsistency). For set of responses Rn = {R1, . . . , Rn} and its subsets Rk of size 3, three responses Rx, Ry, Rz Rk satisfy one of the following transitivity violations: I[inconsistent pair i] measures prevalence, where is total pair i=1 Circular inconsistency: C(Rx, Ry) = 1 C(Ry, Rz) = 1 C(Rz, Rx) = 1. (2) (forming preference cycle Rx Ry Rz Rx) Inequality inconsistency: C(Rx, Ry) = 0 C(Ry, Rz) = 0 C(Rx, Rz) = 0. (3) (violating transitivity of indifference) The Non-Transitivity Ratio is defined as NTRk = Vk (n k) subsets exhibiting either inconsistency type and (cid:0)n possible k-size subsets from elements. , where Vk denotes the number of k-size (cid:1) represents the binomial coefficient counting all 2.2 TRUSTJUDGE As shown in Algorithm 1, the TrustJudge framework is probabilistic evaluation approach that preserves judgment entropy while resolving score-comparison conflicts and transitivity violations. The framework operates differently for single-score evaluation and pairwise comparison tasks, maintaining consistency between these two evaluation protocols."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 TrustJudge Evaluation Framework Require: Responses Rx, Ry (pairwise) or (single), expanded scores Θ (range [s min, max]), original range [smin, smax] Ensure: Score or comparison C(Rx, Ry) 1: if Single-Score Evaluation then 2: 3: 4: jR) Softmax(Po(s jP (s (s (cid:80)s max j=s smaxsmin return maxs min min 5: 6: else {Pairwise Comparison} 7: Option A: PPL-Based 8: 9: jR)) {Normalize probabilities} jR) {Expected expanded score} {Scale to original range} PPL1 PPL(M, Rx, Ry) {Perplexity of Rx followed by Ry} PPL2 PPL(M, Ry, Rx) {Perplexity of reverse ordering} 10: C(Rx, Ry) (cid:26)Corder1 Corder2 if PPL1 < PPL2 otherwise {Determine by comparing PPL} 11: Option B: Likelihood-aware Aggregation p1 Prob(M, Rx, Ry) {Probabilities for Rx vs Ry} 12: p2 Prob(M, Ry, Rx) {Probabilities for reverse comparison} 13: 14: m[k] p1[k] + p2[k] for {1, 1, 0} {Aggregate both directions} 15: 16: end if return arg maxk m[k] {Select most probable outcome} For single-score evaluation, TrustJudge employs distribution-sensitive scoring mechanism. Given response to be assessed, the framework first prompts the LLM to score on more fine-grained scale than original (e.g., 100-point scale when the original scale is 5-point). It then transforms the resulting discrete probability distribution Po(s R) over the expanded score set Θ = {s max} into logits ℓj for each candidate score j. These logits are then processed by softmax function which normalize the logits into valid probability distribution (s jR). Unlike other approaches such as G-Eval Liu et al. (2023), whose generated probabilities can violate (cid:80) R) = 1 because non-score tokens also influence the output, our method ensures properly normalized distribution. The final score is computed as the expected value, scaled back to the original range [smin, smax]: min, . . . , P (s = max(cid:88) j=s min exp(Po(s (cid:80) exp(Po(s jR)) kR)) smax smin max s min , (4) jR) represents the original probability for score where (s j. This approach preserves the full entropy of the judges assessment while producing continuous scores that maintain fine-grained distinctions between response qualities. For pairwise comparison tasks, TrustJudge offers likelihood-aware aggregation methods to resolve transitivity inconsistencies. The first approach (Option A) leverages perplexity-based (PPL-based) method to break ties when the judge exhibits ambiguity. Given two responses Rx and Ry, the framework computes the perplexity of both possible orderings (Rx followed by Ry and vice versa) under the judge model M. The comparison result C(Rx, Ry) is determined by selecting the ordering with lower perplexity: C(Rx, Ry) = (cid:26)Corder1, Corder2, if PPL(M, Rx, Ry) < PPL(M, Ry, Rx), otherwise. (5) The second approach (Option B) employs bidirectional probability based method that combines preference probabilities from both orderings to reduce position bias. For each possible outcome {1, 1, 0} (representing Rx preferred, Ry preferred, or tie), the framework aggregates the probabilities from both orderings:"
        },
        {
            "title": "Preprint",
            "content": "m[k] = porder1 [k] + porder2[k]. (6) where porder1 and porder2 are the probability vectors for the two orderings. The final comparison result is determined by selecting the outcome with maximum aggregated probability = arg maxk m[k]. This probabilistic approach significantly reduces circular and inequality transitivity violations while maintaining the scalability of pairwise comparisons. By producing nearly continuous score distributions, probabilistic judge makes exact equality between two responses much less likely than traditional discrete grading. To relax the tie criterion, we can introduce tolerance hyper-parameter δ 0. Whenever the discrepancy between two responsesabsolute score difference, PPL gap, or probability margindoes not exceed δ, the pair is declared tie, allowing users to tune the granularity of the final ranking without retraining the model. Although δ is set to 0 by default, we have conducted thorough hyper-parameter sweep that confirms TrustJudges reliability across range of δ values; the results recommend small positive δ, becauseeven with δ = 0the framework already produces noticeable number of ties."
        },
        {
            "title": "3 THEORETICAL ANALYSIS",
            "content": "In this section, we formalize the theoretical weaknesses of current LLM-as-a-judge frameworks and prove how TrustJudges components address them. The detailed analysis and derivation can be found at Appendix H. We start by proving that discrete scoring systems suffer from information loss. Theorem 3.1 (Information Loss of Discrete Scoring and Preservation in Expectation). Let pR1 and pR2 be two distinct probability distributions over the score set Θ representing the judge models assessment of two different responses, R1 and R2 (i.e., pR1 = pR2). The discrete scoring function fDiscrete can fail to distinguish between these two assessments, whereas the distribution-sensitive scoring function fDS provides mechanism for their discrimination. Specifically: 1. (Information Loss): There exist pR1 = pR2 with different conditional entropies, H(SR1) = H(SR2), such that their discrete scores are identical: fDiscrete(pR1) = fDiscrete(pR2 ). 2. (Information Preservation): For the same distributions pR1 and pR2 constructed in (1), their distribution-sensitive scores are distinct: fDS(pR1 ) = fDS(pR2). For pairwise comparisons 2, we formalize how the PPL-based method reduces the uncertainty caused by ambiguity, the proof of which is deferred to the Appendix H. Proposition 3.2 (Uncertainty Reduction via PPL-based Method). Let H(Cπ) be the Shannon entropy of the judge models outcome distribution. In an ambiguous regime, this entropy is maximized, H(Cπ) log C. We define confidence distribution pconf based on the perplexity of the rationale Jk for each outcome k: pconf(k) exp(γ PPL(Jk)), for scaling constant γ > 0. (7) If the rationale perplexities are not all equal, then pconf is non-uniform. By the properties of Shannon entropy, this implies its entropy is strictly less than the maximum: Thus, the PPL-based method makes its decision using more certain (lower-entropy) signal. H(pconf) < log C. (8)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Setup Our dataset combines both the 80 questions from MT-Bench Zheng et al. (2023) and the 500 challenges from ArenaHard Li et al. (2024). MT-Bench provides broad coverage of diverse instructions across eight categories including writing, roleplay, and reasoning, while ArenaHard offers particularly challenging queries drawn from real-world user interactions. For each question, we sample candidate responses from diverse LLMs. Under the single-comparison inconsistency protocol, we construct dataset of 10.8k instances, where each instance corresponds to pair of 2Please see more analysis of the bidirectional probability based method in Appendix H."
        },
        {
            "title": "Preprint",
            "content": "responses annotated with their single scores and the induced pairwise preference. Under the pairwise transitivity inconsistency protocol, we collect 43.2k pairwise relations for = 4 and 50.4k for = 5, each derived from the corresponding k-response sets to evaluate transitivity. For each question, we collected responses from diverse set of large language models with varying capabilities. All gold-standard scores and pairwise comparison results for these responses are verified through human review. The final dataset is carefully balanced, ensuring uniform score distributions across every rating level for both single-score and pairwise-comparison scenarios. The detailed model information and inference hyperparameters used in our test data creation are listed in Appendix D, and the detailed category distribution across our datasets is provided in Appendix G. Beyond the core framework, we also extend our approach to multi-dimensional evaluation, as detailed in Appendix F. We evaluate both the inconsistencies and accuracies. For inconsistencies, we use: (1) the Conflict Ratio (CR, Definition 2.1) and (2) the Non-Transitivity Ratio (NTR, Definition 2.2). For accuracies, we employ (1) Win Rate for single-score evaluation: the fraction of instances whose score sides with the reference, which highlights protocol differences more sharply than MSE or MAE. (2) Exact Match for pairwise comparison: given the ground-truth order B, only verbatim output of counts; any reversal or tie is wrongan all-or-nothing metric. Parameter (Def. 2.2) governs the subset size for pairwise transitivity checks. Larger captures higher-order cycles at cost (cid:0)n (cid:1); = 3 yields too few triples to discriminate models, so we report = 4, 5. Baselines We establish two fundamental baseline approaches for comparison with TrustJudge. For single-score evaluation, we implement: (1) the standard raw scoring method that directly outputs absolute scores (1-5 scale), as used in MT-Bench; and (2) G-Eval-style probability summation that calculates the total probability mass across possible scores without applying softmax normalization. Following previous work Wang et al., pairwise baseline mitigates position bias by evaluating each response pair twice (reversed order) and record differing outcomes as ties. All baselines use the identical judge model and the same detailed prompt (see Appendix C) as TrustJudge, enabling direct comparison of inconsistency metrics (CR and NTR) and accuracy metrics (Win Rate and Exact Match) under identical conditions. Evaluated LLMs Our experiments comprehensively evaluate TrustJudge across diverse set of popular LLMs, covering both open-source and proprietary families with varying parameter sizes. Specifically, we include: Llama-3.2-3B, Llama-3.1-8B, and Llama-3.1-70B Grattafiori et al. (2024), three instruction-tuned models from the Llama-3 series, representing small, medium, and large-scale open-source LLMs, respectively. We also evaluate GPT-3.5-Turbo OpenAI (2023) and GPT-4o OpenAI (2024), two widely-used proprietary models from OpenAI, included for their strong performance in both general and evaluation-specific benchmarks. Additionally, we assess the Qwen2.5-7B, Qwen2.5-14B, Qwen2.5-32B Yang et al. (2024), Gemma-2-2b, Gemma-2-9B, and Gemma-2-27B Riviere et al. (2024) to demonstrate TrustJudges generalization across model types and sizes. For all evaluations, we use the instruction-tuned or SFT variants of each model, consistent with their intended use as judge LLMs. All judge models are applied with identical evaluation prompts and configurations to ensure fair comparison. Main Results The experimental results comparing TrustJudge with baseline approaches across multiple model families and sizes are summarized in Table 1 and Figure 2. The key findings are: TrustJudge significantly reduces evaluation inconsistencies across all model sizes. Our experiments demonstrate that TrustJudge achieves superior consistency compared to both direct scoring baselines and G-Eval approaches. The proposed method achieves substantial reductions in Conflict Ratios, delivering absolute improvements of 4.78%8.43% over the baseline approaches. Moreover, Trustjudge consistently surpasses G-eval by approximately 12% across every experimental setting. More importantly, TrustJudge dramatically lowers transitivity violations in pairwise comparisons, with NTRk=5 violations reduced by 10.82%-36.93% absolute. For instance, Llama-3.2-3B shows the most substantial improvement, decreasing NTRk=5 from 54.69% to just 17.76% with TrustJudge. These consistency improvements are particularly notable because they are achieved without requiring additional training or fine-tuning of the base models. TrustJudge maintains and often improves evaluation accuracy while reducing inconsistencies. TrustJudge demonstrates that both consistency and accuracy can be achieved simultaneously. TrustJudge improves exact match rates by 1.19%-6.85% across different model sizes compared to baseline"
        },
        {
            "title": "Preprint",
            "content": "(a) scoring), G-Eval, Ours (distribution-sensitive Baseline. (b) (likelihood-aware aggregation), Ours (PPL-based method), Baseline. Ours Figure 2: Results for single-comparison inconsistency (left) and pairwise transitivity inconsistency (right) across tolerance δ: Llama-3.1-70B (blue), Llama-3.1-8B (red), Llama-3.2-3B (pink), and GPT-4o (green). Colors correspond to different judge models, while markers distinguish evaluation methods as described in each subfigure. For single-score in the experiment on the left, δ is tolerance proportion on the original rating scale; For pairwise (PPL-based) in the experiment on the right, δ is the threshold on the difference in perplexity between the two presentation orders; For pairwise (Likelihood-aware aggregation) in the experiment on the right, δ is the threshold on the confidence gap between the top two aggregated outcomes. Table 1: Results for two experiments: (1) Score-Comparison Inconsistency (CR) comparing raw-score baseline, G-Eval probability-summation, and TrustJudges distribution-sensitive scoring; (2) Pairwise Transitivity Inconsistency (NTRk=4,5) comparing two-pass swap-order baseline versus TrustJudges likelihood-aware aggregation. Win rate quantifies scoring precision by measuring the proportion of test instances where methods score is nearest the ground truth with results presented on both 5-point and 100-point scales. Exact match quantifies comparison consistency by measuring the proportion of pairwise method outcomes that perfectly align with dataset annotations. Model CR (%) NTRk=4 (%) NTRk=5 (%) Ours vs Baseline Ours vs G-Eval Pairwise Exact Match Baseline G-Eval Ours Baseline Ours Baseline Ours 5-scale 100-scale 5-scale 100-scale Baseline TrustJudge Llama-3.2-3B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct GPT-4o 36.65 29.73 23.32 27. 29.50 25.31 15.77 23.18 29.15 23.75 14.89 22.60 32.42 20.26 7.23 11.70 8.07 3.79 1.94 2.83 54.69 37.03 15.22 24.33 17.76 8.46 4.40 6. 45.41 56.84 51.77 50.31 54.66 51.88 54.53 55.60 62.21 59.61 64.22 65.11 51.03 51.24 55.27 53.43 72.06 75.67 80.42 78.67 78.91 81.68 81.61 81. approaches, with the most significant gains observed for smaller models (6.85% improvement for Llama-3.2-3B). In pairwise evaluations, TrustJudge achieves win rates of 45.41%-65.11% against both baseline methods and G-Eval approaches. The method performs particularly well on fine-grained 100-point scoring and maintains strong performance on 5-point scales. This accuracy preservation is crucial for practical applications where both reliable and precise evaluations are required. TrustJudge exhibits robust tolerance-aware gains across judge families and evaluation protocols. fine-tuned tolerance (δ) often yields superior outcomes, as smaller tolerance reduces ambiguity. Conversely, larger tolerance introduces greater uncertainty. Its important to note that even with tolerance of zero, ties can still occur. The TrustJudge scoring and aggregation method effectively mitigates inconsistencies. Notably, its benefits are evident across various tolerance settings, demonstrating its robustness and effectiveness. Table 2: Ablation study where \"L\" refers to LLaMA and \"G\" to GPT. Single Score Components report CR and Pairwise Comparison Components report NTRk=4. Components L-3.1-8B L-3.1-70B G-3.5-Turbo G-4o Single Score Components 29.73 5-scale Baseline 26.10 + Softmax 24.54 + 100-scale 23.32 17.08 17.94 24.35 24.03 22.10 27.95 25.50 24.01 Pairwise Comparison Components Baseline + Likelihood + PPL-Based 20.26 3.79 6.56 7.23 1.94 2. 14.01 6.26 4.80 11.70 2.83 4.48 Ablation Study To evaluate the contribution of different components in TrustJudge, we conduct an ablation study by systematically removing key elements: (1) the softmax normalization, (2) the 100-point granularity enhancement for"
        },
        {
            "title": "Preprint",
            "content": "single-score evaluation, and (3) the pairwise comparison strategies (likelihood-aware aggregation and PPL-based methods). We also examine performance variations across different judge LLMs to demonstrate TrustJudges model-agnostic properties. Table 2 reveals several key findings. For single score components, the 5-scale baseline shows the highest inconsistency rates across all models (39.73% for Llama-3.1-8B, 27.5% for GPT-4o), indicating the importance of TrustJudges enhancements. Adding softmax normalization reduces inconsistency by 0.32%-6.24% absolute across models, while incorporating 100-scale granularity yields improvements (up to 5.19% reduction from 5-scale). In pairwise comparison components, the baseline shows moderate performance (20.26% inconsistency for Llama-3.1-8B). The likelihoodaware aggregation strategy achieves the best results overall, reducing inconsistency to as low as 1.94% for Llama-3.1-70B and 2.83% for GPT-4o. The PPL-based comparison shows substantial gains over baseline (16.47% absolute improvement for Llama-3.1-8B) while offering practical advantages in implementation, as it operates directly on sequence probabilities without requiring explicit win/tie/lose position identification . The consistent performance patterns across model architectures (from 8B to 70B parameters) demonstrate that TrustJudges benefits are not model-specific but derive from its methodological innovations. Larger models generally achieve better absolute performance, with Llama-3.1-70B and GPT-4o showing particularly strong results when using TrustJudge. Figure 3: Effect of scoring granularity on Conflict Ratio across judge models. We measure the Conflict Ratio (CR) under three scoring scales: 5-point, 10-point, and 100-point. refers to LLaMA models and to GPT models; denotes baseline scoring, while represents TrustJudge. Increasing Score granularity reduces inconsistency. As shown in Figure 3, increasing the scoring scale from 5 to 100 points consistently reduces the Conflict Ratios. Furthermore, TrustJudge (T) achieves lower inconsistency than the baseline (B) under all granularities, demonstrating its effectiveness in preserving scoring fidelity. The benefit is especially pronounced for larger models such as Llama-3.1-70B and GPT-4o. Generalization Experiment To systematically validate TrustJudges cross-architectural adaptability and practical value for alignment training, we evaluate the framework across 12 model variants spanning four major architectures (Qwen, Gemma, Llama, GPT) with various parameter sizes. The experiments cover both single-response scoring and pairwise comparison scenarios. Note that we set = 4 for pairwise comparison. Figure 4: Performance of TrustJuduge with LLMs of different Sizes and Structures. Note that Qwen-2.5 is denoted as and Gemma-2 as G."
        },
        {
            "title": "Preprint",
            "content": "Figure 4 demonstrates three key findings through comprehensive architectural comparisons: Architecture-agnostic consistency improvement. The distribution-sensitive scoring achieves consistent reductions in single-instance conflict ratios across all tested architectures. Moreover, inconsistency varies markedly across architectures: Gemma consistently outperforms Qwen of comparable size. Transitivity violation reversal. The proposed likelihood-aware aggregation strategy substantially mitigates non-transitivity patterns across model variants. Remarkably, this approach enables midsized models to surpass the transitivity performance of significantly larger baseline models under controlled evaluation settings. Size-performance decoupling. While model capacity naturally correlates with lower inconsistency rates, TrustJudge effectively narrows the performance disparity between small and large models. This capability highlights the frameworks potential to enhance the practical utility of resource-efficient models for alignment tasks. Notably, bigger is not always better: the 9B Gemma actually exhibits lower inconsistency than its 27B sibling. Table 3: Performance of TrustJuduge for Llama-3.1-8B and DeepSeek-R1-Distill-Llama-8B. CR (%) NTRk=4 (%) NTRk=5 (%) Model Baseline -Eval Ours Baseline Likelihood PPL-Based Baseline Likelihood PPL-Based Llama-3.1-8B 29.73 DeepSeek-R1-Distill-Llama-8B 58.75 25.31 23.75 53.63 49.28 20.26 44. 3.79 11.43 6.80 25.16 37.03 63.98 8.46 18.50 16.20 41.78 Reasoning model results As shown in Table 3, The reasoning models significantly higher inconsistency rates suggest potential catastrophic forgetting of judge capabilities due to reinforcement training on mathematical data Guo et al. (2025). This finding is noteworthy as it highlights the challenges that arise when models are trained on specialized tasks, such as mathematical reasoning, which can inadvertently lead to the degradation of their performance in other critical areas like judging. Despite this, TrustJudge remains effective in improving judge performance, demonstrating its robustness and adaptability in enhancing the models capabilities across different domains. Table 4: DPO results on Llama-3.1-8B and Qwen2.5-7B. The experimental setup for DPO is provided in Appendix E. Using Trustjudge for Rewarding Models Table 4 shows TrustJudges DPO enhancement. We trained SFT models on sampled 6K IFT/EFT examples (Open Assistant Köpf et al. (2023) + UltraFeedback Cui et al. (2024)), then performed DPO on 5K questions from the same sources. Diverse LLMs answered these questions before preference judgments. AlpacaEval2 (GPT-4o judge) shows TrustJudges 100-point scoring improves win rates by 16.21% (Llama3-8B) and 1.94% 10.69% (Qwen2.5-7B) over 5-point baselines, measured across 805 questions with both standard and LC win rates. This confirms TrustJudges dual utility for evaluation and preference optimization. Llama-3.1-8B-SFT Llama-3.1-8B-SFT-5-Scale-Baseline Llama-3.1-8B-SFT-100-Scale-Softmax Qwen2.5-7B-SFT Qwen2.5-7B-SFT-5-Scale-Baseline Qwen2.5-7B-SFT-100-Scale-Softmax Data Selection Strategy LC Win Rate Win Rate 11.17 19.13 20.52 11.92 16.82 18. 7.95 20.93 24.16 8.07 15.09 18.76 The results establish TrustJudges robust generalizability across: (1) different model families and scales, maintaining consistent inconsistency reduction regardless of architecture; and (2) diverse applications including direct evaluation and reward modeling for DPO training. This versatility stems from TrustJudges architecture-agnostic probabilistic design and fine-grained scoring approach."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We presented TrustJudge, novel probabilistic evaluation framework designed to address fundamental inconsistencies in current LLM-as-a-judge paradigms. Through systematic analysis, we identified two critical issues: Score-Comparison Inconsistency due to information loss in discrete scoring systems, and Pairwise Transitivity Inconsistency stemming from ambiguous tie judgments. TrustJudge introduces distribution-sensitive probabilistic scoring, preserving judgment entropy, and likelihoodaware aggregation strategies to effectively mitigate these inconsistencies."
        },
        {
            "title": "Preprint",
            "content": "Empirical results demonstrate that TrustJudge significantly reduces Score-Comparison inconsistency and Pairwise Transitivity inconsistency across various LLM architectures and scales. Crucially, these improvements do not compromise evaluation accuracy, achieving enhancements in exact match rates and win rates compared to established baselines. Our ablation and generalization studies confirm the robustness and model-agnostic applicability of TrustJudge. TrustJudge offers both theoretical insights and practical solutions for enhancing the reliability and credibility of automated LLM evaluations, contributing towards more trustworthy and consistent use of large language models in research and applications. We also discuss the limitations of our approach in Appendix B."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. The claude 3 model family: Opus, sonnet, haiku model card. https://www. anthropic.com/claude-3-model-card, 2024. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-examiner. Advances in Neural Information Processing Systems, 36:7814278167, 2023. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):145, 2024. Chavinlo. alpaca-13b (huggingface model card). https://huggingface.co/chavinlo/ alpaca-13b, 2023. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024a. Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? study on judgement biases. arXiv preprint arXiv:2402.10669, 2024b. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, and et al. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality. arXiv:2304.01196, 2023. URL https://arxiv.org/pdf/ 2304.01196. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, et al. ULTRAFEEDBACK: Boosting language models with scaled AI feedback. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 97229744. PMLR, 2127 Jul 2024. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André FT Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. arXiv preprint arXiv:2308.07286, 2023. Evan Frick, Peter Jin, Tianle Li, Karthik Ganesan, Jian Zhang, Jiantao Jiao, and Banghua Zhu. Athene70b: Redefining the boundaries of post-training for open models. https://nexusflow.ai/ blogs/athene, 2024. See also HF model card: Nexusflow/Athene-70B. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025."
        },
        {
            "title": "Preprint",
            "content": "Hui Huang, Yingqi Qu, Xingyuan Bu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge model is not general substitute for gpt-4. arXiv preprint arXiv:2403.02839, 2024. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, et al. Openassistant conversations - democratizing large language model alignment. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4766947681. Curran Associates, Inc., 2023. https://proceedings.neurips.cc/paper_files/paper/2023/file/ URL 949f0f8f32267d297c2d4e3ee10a2e7e-Paper-Datasets_and_Benchmarks. pdf. Charles Koutcheme, Nicola Dainese, Sami Sarsa, Arto Hellas, Juho Leinonen, and Paul Denny. Open source language models can provide feedback: Evaluating llms ability to help students using gpt-4-as-a-judge. In Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1, pp. 5258. 2024. Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu. Preference leakage: contamination problem in llm-as-a-judge. arXiv preprint arXiv:2502.01534, 2025. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Pengfei Liu, et al. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Yen-Ting Lin and Yun-Nung Chen. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. arXiv preprint arXiv:2305.13711, 2023. Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, et al. Mitigating the alignment tax of rlhf. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 580606, 2024. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 25112522, 2023. LMSYS. Vicuna-7b v1.5 (huggingface model card). https://huggingface.co/lmsys/ vicuna-7b-v1.5, 2023. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023. OpenAI. Gpt-3.5-turbo. https://platform.openai.com/docs/models/gpt-3-5, 2023. OpenAI. Gpt-4o. https://openai.com/index/gpt-4o, 2024. Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, and Urmish Thakker. Constructing domain-specific evaluation sets for llm-as-a-judge. arXiv preprint arXiv:2408.08808, 2024. Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024."
        },
        {
            "title": "Preprint",
            "content": "Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. arXiv preprint arXiv:2310.15123, 2023. Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, and Saab Mansour. FineSurE: Fine-grained In Proceedings of the 62nd Annual Meeting of the summarization evaluation using LLMs. Association for Computational Linguistics (Volume 1: Long Papers), pp. 906922, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. acl-long.51. URL https://aclanthology.org/2024.acl-long.51/. Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. Evaluation metrics in the era of gpt-4: Reliably evaluating large language models on sequence to sequence tasks. arXiv preprint arXiv:2310.13800, 2023. Annalisa Szymanski, Noah Ziems, Heather Eicher-Miller, Toby Jia-Jun Li, Meng Jiang, and Ronald Metoyer. Limitations of the llm-as-a-judge approach for evaluating llm outputs in expert knowledge tasks. In Proceedings of the 30th International Conference on Intelligent User Interfaces, pp. 952966, 2025. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. Judgebench: benchmark for evaluating llm-based judges. arXiv preprint arXiv:2410.12784, 2024. Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. arXiv preprint arXiv:2406.12624, 2024. Prapti Trivedi, Aditya Gulati, Oliver Molenschot, Meghana Arakkal Rajeev, Rajkumar Ramamurthy, Keith Stevens, Tanveesh Singh Chaudhery, Jahnavi Jambholkar, James Zou, and Nazneen Rajani. Self-rationalization improves llm as fine-grained judge. arXiv preprint arXiv:2410.05495, 2024. Ruiqi Wang, Jiyu Guo, Cuiyun Gao, Guodong Fan, Chun Yong Chong, and Xin Xia. Can llms replace human evaluators? an empirical study of llm-as-a-judge in software engineering. arXiv preprint arXiv:2502.06193, 2025a. Victor Wang, Michael J. Q. Zhang, and Eunsol Choi. Improving llm-as-a-judge inference with the judgment distribution, 2025b. URL https://arxiv.org/abs/2503.03064. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. In The Twelfth International Conference on Learning Representations. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, et al. Autosurvey: Large language models can automatically write surveys. Advances in Neural Information Processing Systems, 37:115119115145, 2024. Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, et al. Temporal self-rewarding language models: Decoupling chosen-rejected via past-future. arXiv preprint arXiv:2508.06026, 2025c. WizardLM Team. Wizardlm-13b v1.2 (huggingface model card). https://huggingface.co/ WizardLMTeam/WizardLM-13B-V1.2, 2023. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. URL https://arxiv.org/abs/2304.12244. Yi Xu, Laura Ruis, Tim Rocktäschel, and Robert Kirk. Investigating non-transitivity in llm-as-a-judge. In Forty-second International Conference on Machine Learning."
        },
        {
            "title": "Preprint",
            "content": "An Yang, Baosong Yang, Zhang, et al. Qwen2.5 technical report. 2024. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928, 2023. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. In Forty-first International Conference on Machine Learning. Yifan Zhang, Ge Zhang, Yue Wu, Kangping Xu, and Quanquan Gu. Beyond bradley-terry models: general preference model for language model alignment. In ICLR 2025 Workshop on Bidirectional Human-AI Alignment. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges. 2023."
        },
        {
            "title": "A RELATED WORK",
            "content": "Traditional Discrete Evaluation Protocols LLM-as-a-judge frameworks have become widely adopted for their scalability and cost-efficiency in evaluating large language models. Early works predominantly relied on discrete evaluation protocols, including coarse single-score ratings and pairwise preference comparisons. MT-Bench and Chatbot Arena Zheng et al. (2023) demonstrated the feasibility of using powerful LLMs such as GPT-4 as judges, achieving high agreement with human preferences, while also noting issues such as verbosity and position bias. ArenaHard Li et al. (2024) proposed an automated benchmark construction pipeline and introduced Arena-Hard-Auto, challenging benchmark curated without human-in-the-loop, which relies on LLMs to produce and evaluate responses. AlpacaEval Dubois et al. (2024) highlighted persistent biases in LLM-based evaluation such as preference for longer outputs. Their proposed length-controlled regression analysis mitigated this issue and improved correlation with human preferences. PandaLM Wang et al. introduced pairwise judgment protocol that incorporates subjective dimensions like clarity, formality, and instruction adherence. Its judge model outperformed even GPT-4 in certain domains and was used to tune instruction-following models. Other works such as Trivedi et al. (2024); Saha et al. (2023); Que et al. (2024); Ye et al. (2023); Fernandes et al. (2023); Bai et al. (2023); Wang et al. (2025a) developed various discrete evaluation techniques, including majority voting, scalar ratings, skill-wise decomposition, and output-based scoring. While these methods brought interpretability and practical value, they were still constrained by coarse-grained annotations and did not fully resolve contradictions between scoring types or internal inconsistencies. Notably, Fernandes et al. (2023) proposed AUTOMQM for machine translation, which incorporated structured error labeling but remained within the paradigm of fixedscore prompting. Bai et al. (2023) proposed language model examiner framework combining scoring and ranking, but without entropy-aware mechanisms. Similarly, Wang et al. (2025a) employed outputbased scoring for software engineering tasks, emphasizing alignment with human evaluation but without probabilistic modeling. Additionally, Trivedi et al. (2024) used iterative self-rationalization for enhancing model rationales but still within discrete scoring. Probabilistic and Fine-Grained Evaluation Methods To overcome the limitations of discrete judgments, recent research has explored probabilistic evaluation strategies. G-EVAL Liu et al. (2023) introduced softmax-normalized score prediction over fine-grained rating scale using chain-ofthought prompting and form-filling, improving alignment with human preferences. Wang et al. (2025b) further examined extracting fine-grained preferences by leveraging the distributional output of judge models, demonstrating that methods incorporating distributional judgments significantly outperform traditional greedy decoding across various evaluation scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Example of pairwise evaluation prompt. Our work builds upon and extends this direction by proposing TrustJudge, probabilistic evaluation framework that preserves judgment entropy and explicitly resolves both score-comparison and pairwise transitivity inconsistencies in LLM-as-a-judge paradigms."
        },
        {
            "title": "B LIMITATIONS",
            "content": "Despite the demonstrated efficacy of TrustJudge, our approach still has some inherent limitations. Firstly, the performance of TrustJudge is fundamentally dependent on the instruction-following capabilities of the employed evaluation models. Smaller-scale language models often exhibit weaker instruction comprehension and execution capabilities, which could result in failure to yield valid scores or comparisons. Consequently, the quality and reliability of TrustJudge evaluations are directly tied to the underlying judge models competence, emphasizing the importance of model ability."
        },
        {
            "title": "C PROMPT EXAMPLES",
            "content": "The following figures 5 and 6 provide examples of evaluation prompts used to assess responses. The first figure shows pairwise comparison prompt, where two responses are compared and one is selected as better. The second figure illustrates single-score evaluation prompt with the 5-point scale, where response is rated based on quality metrics such as helpfulness and relevance. These examples are intended to support clarity and consistency in LLM-as-a-judge evaluation tasks."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Example of single-score evaluation prompt with the 5-point scale."
        },
        {
            "title": "D INFERENCE SETTINGS",
            "content": "Specifically, we included strong open-source models such as Llama-3-Athene-70B Frick et al. (2024), Llama-3-70B-Instruct Grattafiori et al. (2024), and Llama-3-8B-Instruct Grattafiori et al. (2024); strong closed-source models such as GPT-4o OpenAI (2024), GPT-4-Turbo OpenAI (2024), and Claude 3 Sonnet Anthropic (2024); weak open-source models including WizardLM-13B-v1.2 Xu et al. (2023); WizardLM Team (2023), Vicuna-7B Chiang et al. (2023); LMSYS (2023), and Alpaca-13B Chavinlo (2023); and weak closed-source models such as Claude 3 Haiku Anthropic (2024) and GPT-3.5-Turbo OpenAI (2023). We use batched inference of vLLM to accelerate the generation and judging process, setting the temperature to 1.0, the maximum number of tokens to 2048, and providing the top 20 log probabilities for each generated token."
        },
        {
            "title": "E DPO TRAINING SETTINGS",
            "content": "In DPO training, models are trained for one epoch with learning rate of 5.0 107. The temperature parameter β is set at 0.1. global batch size of 32 is used, with 4 samples per device across 8 GPUs. The training process employs cosine learning rate schedule, incorporating warmup phase that accounts for 10% of the total training steps. The maximum sequence length is maintained at 2048 tokens, while the maximum prompt length is limited to 512 tokens. EXTENSION TO MULTI-DIMENSIONAL EVALUATION Setup. To assess whether TrustJudge can be extended to multi-dimensional evaluation, we evaluate three sub-dimensions: factuality, coherence, and helpfulness. We randomly sample 120 questions from Arena Hard dataset. For each question, candidate responses and judgements are generated by models from the Llama, Qwen, Gemma, and GPT families. Each sub-dimension uses dimensionspecific prompt, which we show in Figures 7 and 8 respectively."
        },
        {
            "title": "Preprint",
            "content": "For each sub-dimension, we independently compute two degrees of inconsistencies: (i) ScoreComparison Inconsistency reported as CR and Pairwise Transitivity Inconsistency reported as Rk for {3, 4}. For brevity, Table 5 present the averages of these metrics across the three sub-dimensions, while all metrics are computed per dimension as specified above. Figure 7: Example of single-score prompts for multi-dimension evaluation. Figure 8: Example of pairwise prompts for multi-dimension evaluation. Results and analysis. Extending the judge to three axesfactuality, coherence, and helpfulnessyields clear reduction in inconsistency. With dimension-specific prompts and per-dimension computation, we observe drops on every model and on both of the metrics: CR decreases by roughly 5.13%11.03%, while NTR3 and NTR4 fall more sharply, on average by 11.23%24.99%. The pattern is most visible with Llama-3.1-70B, where NTR4 contracts from 44.65% to 16.21% and NTR3 from 18.29% to 5.48%, alongside CR decline from 52.20% to 41.47%. Qwen2.532B and Gemma-2-27B-Instruct show the same direction of change; even where CR narrows more"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Results for multi-dimensional evaluation across three sub-dimensionsfactuality, coherence, and helpfulness. For each sub-dimension, CR and NTRk are computed independently; tables report the mean across sub-dimensions. Model CR (%) NTRk=4 (%) NTRk=3 (%) Baseline Ours Baseline Ours Baseline Ours Gemma-2-27b-it Qwen2.5-32B-Instruct Llama-3.1-70B-Instruct 49.43 45.73 52. 44.30 37.87 41.47 19.60 17.38 18.29 8.20 7.89 5.48 48.76 42.55 44.65 22.41 22.36 16.21 modestly (e.g., Gemma 49.43% 44.30%), pairwise non-transitivity is still more than halved (48.76% 22.41%). Taken together, the improvements persist when quality is decomposed into orthogonal components rather than measured as single undifferentiated score. Mechanistically, the scalar channel benefits from distribution-sensitive scoring, which smooths discretization artifacts and reduces clashes between numeric scores and pairwise preferences, lowering CR. The pairwise channel benefits from likelihood-aware aggregation with calibrated tie handling, which suppresses position bias, lowering NTR. Because these effects arise within each dimension before averaging, the evidence indicates genuine generalization of TrustJudge to multi-dimensional evaluation."
        },
        {
            "title": "G GENERALIZATION ACROSS DATASET CATEGORIES",
            "content": "Setup. To assess whether our observations generalize across task types, we used 120 prompts from MT-Bench and Arena Hard as the main experiment; for each prompt we independently collected ten model responses so as to obtain quality-diverse distribution of outputs, yielding total of 1,200 responses. The 120 prompts were assigned to eight predefined MT-Bench categories as shown in Figure 9. Evaluation was performed with three judges Qwen2.5-7B-Instruct, Llama-3.1-8BInstruct, and Gemma-2-9b-it which each assessed all 1,200 responses using both (i) single-score assessment on 5-point scale and (ii) pairwise comparisons between responses. Per category we report the Conflict Ratio under the 5-point and the Non-Transitivity Ratio with = 4. Results are presented both aggregated across categories and broken down by the eight MT-Bench categories to illustrate the variation in inconsistency patterns across task types. Figure 9: Category distribution across the eight dataset categories. Results and analysis. As shown in Table 6, across eight MT-Bench categories and three judges, the clearest pattern is in pairwise transitivity consistency: non-transitivity ratio collapses from 18.74% under the two-pass baseline to 4.40% with likelihood-aware aggregation method and 5.64% with the PPL-based method (averaged over all 24 categoryjudge cells). That reduction is uniformalmost every category and every judge shows single-digit NTR after applying our pairwise aggregation, with extremes such as LlamaSTEM reaching 0.00%, and large cuts in difficult regimes like QwenMath (32.85% 4.46%). In short, once responses are compared bidirectionally with likelihood-aware tie handling, residual inconsistencies are rare regardless of task type. Score-Comparison Conflicts show more nuanced, category-dependent story. Averaged over all cells, CR drops from 23.32% to 20.63% with distribution-sensitive scoring. However, looking category-wise, our method is the best (or tied best) in three of eight groups that emphasize openended generationCoding (Ours 21.78% vs. G-Eval 22.13%), Reasoning (Ours 20.72% vs. G-Eval"
        },
        {
            "title": "Preprint",
            "content": "21.17%), and Writing (Ours 23.93% vs. G-Eval 24.09%)while G-Eval leads in STEM, Humanities, Roleplay, and Extraction. Math is the lone case where the raw baseline edges out both methods by small margin (Baseline 19.41% vs. Ours 19.55%/ G-Eval 20.10%). These contrasts suggest that when responses span wider stylistic or pragmatic range, TrustJudge that preserves rating entropy tends to reduce score-comparison inconsistency; when the signal is more templated or tightly factual, G-Eval probability summation can be slightly better calibrated. Table 6: Results for two category-wise inconsistencies. Left block (ScoreComparison Inconsistency): Baseline, G-Eval probability-summation, and TrustJudges distribution-sensitive scoring on 5-point scale. Right block (Pairwise Transitivity Inconsistency): two-pass swap-order Baseline, TrustJudges likelihood-aware aggregation (Option in 1), and PPL-based method (Option in 1). Judges are Llama-3.1-8B-Instruct (Llama), Qwen2.5-7B-Instruct (Qwen), and gemma-2-9b-it (Gemma). Category Model ScoreComparison (CR, %) Pairwise Transitivity (NTRk=4, %) Baseline G-Eval Coding Reasoning Math STEM Humanities Writing Roleplay Extraction Llama Qwen Gemma Llama Qwen Gemma Llama Qwen Gemma Llama Qwen Gemma Llama Qwen Gemma Llama Qwen Gemma Llama Qwen Gemma Llama Qwen Gemma 31.19 26.14 18. 31.18 27.53 10.23 24.24 26.63 7.35 25.62 29.35 9.52 27.08 23.88 12.24 38.71 20.95 18.92 35.04 28.49 16. 40.63 30.12 0.00 27.74 25.33 13.33 25.79 29.35 8.37 25.25 30.65 4.41 17.77 26.75 4.76 21.67 21.49 2. 30.97 30.48 10.81 29.91 26.16 5.36 34.38 30.12 0.00 Ours 27.59 23.69 14.07 25.90 26.48 9. 24.24 28.54 5.88 19.42 26.23 4.76 21.67 20.30 6.12 30.97 30.00 10.81 27.35 28.49 14.29 35.94 32.53 0. Baseline Likelihood PPL-based 22.07 19.86 16.76 22.08 23.93 14.13 23.26 32.85 16. 9.03 23.07 9.47 19.14 20.38 7.81 23.10 26.19 11.43 12.50 24.69 10.71 18.87 28.85 12.62 3.72 4.95 3. 5.01 5.56 2.52 4.86 4.46 4.29 1.94 3.68 1.11 4.29 3.86 2.12 2.07 10.71 3.62 5.47 6.76 4. 3.77 4.23 8.10 7.80 6.19 5.91 6.87 9.69 5.71 5.21 9.64 6.48 0.00 5.70 3.23 4.00 3.41 1. 14.83 5.06 1.90 1.56 7.70 6.43 3.77 7.69 5.00 Practically, the category study shows the generalization of TrustJudge. The likelihood-aware aggregation and PPL-based method are robust to task type, driving down inconsistencies nearly everywhere. The distribution-sensitive scoring is competitive overall and tends to be strongest where outputs are diverse and rubric-driven (coding, reasoning, writing)."
        },
        {
            "title": "H THEORETICAL DERIVATION",
            "content": "H.1 THEORETICAL ANALYSIS OF DISTRIBUTION-SENSITIVE SCORING In the LLM-as-a-Judge paradigm, judge model assesses given response R. The models internal assessment can be conceptualized as conditional probability distribution over discrete set of possible scores Θ = {s1, . . . , sk}. We denote this probability mass function (PMF) as pR(s) PM (S = sR), where is random variable representing the score. The uncertainty or"
        },
        {
            "title": "Preprint",
            "content": "ambiguity in this assessment is captured by the conditional entropy: H(SR) = (cid:88) sΘ pR(s) log pR(s) (9) Traditional discrete scoring protocols extract single score by taking the mode of this distribution. We define the discrete scoring function fDiscrete as: fDiscrete : k1 Θ, fDiscrete(pR) = arg max sΘ pR(s) where k1 is the (k 1)-simplex representing all possible probability distributions over the scores. This function maps probability distribution to single point estimate. The core issue with this approach is that the arg max operator is non-injective; it discards all information about the distributions shape and uncertainty (entropy), mapping distinct belief states to the same output score. This information loss is primary source of score-comparison inconsistencies. In contrast, our proposed distribution-sensitive scoring function, fDS, computes the expected value of the score distribution: fDS : k1 R, fDS(pR) = ESpR [S] = (cid:88) sΘ pR(s) This function maps the entire probability distribution to continuous scalar value, preserving more information about the underlying assessment. The following theorem formalizes the information preservation property of fDS compared to the information loss inherent in fDiscrete. Theorem H.1 (Information Loss of Discrete Scoring and Preservation in Expectation). Let pR1 and pR2 be two distinct probability distributions over the score set Θ representing the judge models assessment of two different responses, R1 and R2 (i.e., pR1 = pR2). The discrete scoring function fDiscrete can fail to distinguish between these two assessments, whereas the distribution-sensitive scoring function fDS provides mechanism for their discrimination. Specifically: 1. (Information Loss): There exist pR1 = pR2 with different conditional entropies, H(SR1) = H(SR2), such that their discrete scores are identical: fDiscrete(pR1) = fDiscrete(pR2 ). 2. (Information Preservation): For the same distributions pR1 and pR2 constructed in (1), their distribution-sensitive scores are distinct: fDS(pR1 ) = fDS(pR2). Proof. We will prove the theorem by formal symbolic construction. Let the score set be Θ. Let us choose two distinct scores sm, sa Θ such that sm = sa. Let sm be the intended mode of our distributions. Further, let us choose two distinct real numbers ϵ1 and ϵ2 such that 0 < ϵ1, ϵ2 < 0.5 and ϵ1 = ϵ2. The condition ϵ < 0.5 ensures that 1 ϵ > ϵ, which will be necessary to establish sm as the unique mode. The condition ϵ1 = ϵ2 ensures the resulting distributions are distinct. Consider two responses, R1 and R2, which elicit two different internal belief distributions from the judge model, defined as follows: 1. Let pR1 be probability mass function (PMF) where the probability mass is concentrated on sm and sa: pR1(s) = 1 ϵ1 ϵ1 0 if = sm if = sa otherwise 2. Let pR2 be second, distinct PMF, also concentrated on sm and sa but with different balance: pR2(s) = 1 ϵ2 ϵ2 0 if = sm if = sa otherwise Since ϵ1 = ϵ2, we have pR1 = pR2 ."
        },
        {
            "title": "Preprint",
            "content": "PART 1: PROVING INFORMATION LOSS IN fDISCRETE We apply the discrete scoring function fDiscrete to both distributions. By our choice of ϵ1, ϵ2 (0, 0.5), we have 1 ϵ1 > ϵ1 and 1 ϵ2 > ϵ2. Therefore, the mode for both distributions is uniquely sm: fDiscrete(pR1) = arg max sΘ pR1 (s) = sm fDiscrete(pR2) = arg max sΘ pR2 (s) = sm Thus, we have shown that for two distinct distributions pR1 and pR2 , it is possible that fDiscrete(pR1 ) = fDiscrete(pR2). Now, we consider their conditional entropies. The entropy of these distributions is function of ϵ: H(SR1) = ((1 ϵ1) log(1 ϵ1) + ϵ1 log ϵ1) H(SR2) = ((1 ϵ2) log(1 ϵ2) + ϵ2 log ϵ2) The binary entropy function H(p) = log (1 p) log(1 p) is strictly increasing on the interval (0, 0.5). Since we chose ϵ1 = ϵ2 within this interval, it follows that H(SR1) = H(SR2). This confirms that fDiscrete maps distributions with different levels of uncertainty to the same output, thereby losing information. This proves the first part of the theorem. PART 2: PROVING INFORMATION PRESERVATION IN fDS Next, we apply the distribution-sensitive scoring function fDS to the same distributions pR1 and pR2: fDS(pR1) = E[SR1] = fDS(pR2) = E[SR2] = (cid:88) sΘ (cid:88) sΘ pR1(s) = sm(1 ϵ1) + sa(ϵ1) pR2(s) = sm(1 ϵ2) + sa(ϵ2) To demonstrate that their scores are distinct, let us assume for contradiction that they are equal: fDS(pR1) = fDS(pR2 ) sm(1 ϵ1) + sa(ϵ1) = sm(1 ϵ2) + sa(ϵ2) sm smϵ1 + saϵ1 = sm smϵ2 + saϵ2 ϵ1(sa sm) = ϵ2(sa sm) (ϵ1 ϵ2)(sa sm) = 0 This equality can only hold if ϵ1 ϵ2 = 0 or sa sm = 0. However, by our initial construction, we chose ϵ1 = ϵ2 (so ϵ1 ϵ2 = 0) and sa = sm (so sa sm = 0). This leads to contradiction. Therefore, our assumption must be false, and it must be that fDS(pR1) = fDS(pR2). The distributionsensitive scoring function successfully distinguishes between these two belief states, preserving the discriminative information lost by fDiscrete. This proves the second part of the theorem. H.2 THEORETICAL ANALYSIS OF LIKELIHOOD-AWARE AGGREGATION The PPL-based estimator is designed to resolve ambiguity. From an information-theoretic perspective, ambiguity in discrete choice problem corresponds to high-entropy probability distribution over the possible outcomes. The PPL-based method leverages an alternative signalthe generative likelihood of the rationaleto induce more confident (lower-entropy) posterior belief for decision-making. The following proposition formalizes this concept. Proposition H.2 (Uncertainty Reduction via PPL-based Method). Let = {1, 1, 0} be the set of outcomes. Let p(Cπ) be the original outcome distribution from the judge model, and H(Cπ) its Shannon entropy. In an ambiguous regime, this distribution approaches uniformity, causing H(Cπ) log C. Let new \"confidence\" distribution pconf be derived from the rationales Jk for each outcome C: pconf(k) = exp(γ PPL(Jk)) iC exp(γ PPL(Ji)) (cid:80)"
        },
        {
            "title": "Preprint",
            "content": "where γ > 0 is scaling constant. If there exists at least one outcome whose rationale has strictly lower perplexity than another (i.e., k1, k2 s.t. PPL(Jk1 ) < PPL(Jk2)), then there exists γ such that the entropy of the confidence distribution is strictly lower than the maximum possible entropy: H(pconf) < log This demonstrates that ˆCP makes decision based on more certain signal, reducing the judgment uncertainty present in the original ambiguous distribution. Proof. The Shannon entropy function, H(p) = (cid:80) pi log pi, is strictly concave function over the probability simplex. Its unique maximum is achieved when the distribution is uniform, i.e., pi = 1/C for all i. In this case, H(p) = log C. In an ambiguous regime, the original outcome distribution p(Cπ) is, by definition, near-uniform. Consequently, its entropy H(Cπ) is close to its maximum possible value, log C. Now, consider the confidence distribution pconf. The condition k1, k2 s.t. PPL(Jk1 ) < PPL(Jk2) implies that the values exp(γ PPL(Jk)) are not all equal. As result, after normalization, the distribution pconf is not uniform. Because the Shannon entropy functions maximum is uniquely attained by the uniform distribution, any non-uniform distribution must have strictly lower entropy. Therefore, H(pconf) < max H(p) = log Since H(Cπ) log C, it follows that H(pconf) < H(Cπ). The parameter γ controls the \"peakedness\" of pconf. As γ , pconf approaches Kronecker delta function centered at the outcome with the minimum PPL, and its entropy approaches zero. Thus, for any non-trivial difference in PPLs, we can always choose γ to make the decision signal arbitrarily certain. This proves that the PPL-based method transforms high-entropy (ambiguous) belief state into lower-entropy (more certain) one, thereby providing more discriminative signal for making final judgment. desirable property of any comparison function (Rx, Ry) is symmetry, meaning that swapping the inputs should simply invert the outcome, i.e., (Ry, Rx) = (Rx, Ry). Single-pass estimators often violate this property due to positional bias. The following proposition proves that our bidirectional estimator is inherently stable and symmetric by construction. Proposition H.3 (Symmetry and Stability of the Bidirectional Estimator). Let the single-pass greedy estimator be ˆCSP (Rx, Ry) = arg maxk p(k(Rx, Ry), M). Due to positional bias, this estimator is not generally symmetric, meaning there exist pairs (Rx, Ry) for which ˆCSP (Rx, Ry) = ˆCSP (Ry, Rx). In contrast, the bidirectional estimator ˆCB is perfectly symmetric for all inputs: This property makes ˆCB stable estimator with respect to the input ordering. ˆCB(Rx, Ry) = ˆCB(Ry, Rx) Proof. Lets define the aggregated score function for ˆCB(Rx, Ry) as m(k; Rx, Ry) = p(k(Rx, Ry)) + p(k(Ry, Rx)). The decision is arg maxk m(k; Rx, Ry). Now consider the estimator for the swapped input, ˆCB(Ry, Rx). Its score function is m(k; Ry, Rx) = p(k(Ry, Rx)) + p(k(Rx, Ry)). Lets compare the score for outcome = 1 in the first case with the score for outcome = 1 in the second case: m(1; Rx, Ry) = p(1(Rx, Ry)) + p(1(Ry, Rx)) m(1; Ry, Rx) = p(1(Ry, Rx)) + p((1)(Rx, Ry)) = p(1(Ry, Rx)) + p(1(Rx, Ry))"
        },
        {
            "title": "Preprint",
            "content": "We see that m(1; Rx, Ry) = m(1; Ry, Rx). By the same logic, m(1; Rx, Ry) = m(1; Ry, Rx) and m(0; Rx, Ry) = m(0; Ry, Rx). This means that the score assigned to preference \"Rx Ry\" in the first ordering is identical to the score assigned to preference \"Rx Ry\" (which is outcome 1) in the second ordering. Therefore, if the maximum score in the first case is for outcome k, the maximum score in the second case must be for outcome k. This proves ˆCB(Rx, Ry) = ˆCB(Ry, Rx)."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Institute of Science Tokyo",
        "Nanjing University",
        "National University of Singapore",
        "Peking University",
        "Southeast University",
        "Westlake University"
    ]
}