{
    "paper_title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement",
    "authors": [
        "Jian Zhang",
        "Zhangqi Wang",
        "Zhiyuan Wang",
        "Weiping Fu",
        "Yu He",
        "Haiping Zhu",
        "Qika Lin",
        "Jun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare."
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1731 Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu, Qika Lin, Member, IEEE, and Jun Liu, Senior Member, IEEE AbstractLinguistic expressions of emotions, including depression, anxiety, and trauma-related states, are widespread in clinical notes, counseling dialogues, and online mental health communities. Accurate recognition of these emotions is crucial for clinical triage, risk assessment, and timely intervention in mental health related applications. Despite recent advances showing that large language models (LLMs) can generalize well to various emotion analysis tasks, their diagnostic reliability in high-stakes and context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing approaches are challenged by two major issues: emotional comorbidity, where multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these issues, we propose APOLO (Automated Prompt Optimization for Linguistic emOtion diagnosis), framework that systematically explores broader and finer-grained prompt space to enhance diagnostic efficiency and robustness. APOLO models instruction refinement as Partially Observable Markov Decision Process (POMDP) and introduces multi-agent collaboration mechanism comprising the PlannerTeacherCriticStudentTarget roles. This closed-loop design enables continuous optimization of prompt generation, evaluation, and evolution. After the Planner agent formulates high-level optimization trajectory within the POMDP framework, the TeacherCriticStudent agents collaboratively refine the prompts along this trajectory, iteratively enhancing the stability and effectiveness of the reasoning process. Finally, the Target agent determines whether to continue optimization or terminate the search based on performance evaluation. Experimental results demonstrate that APOLO improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, providing generalizable and scalable paradigm for trustworthy LLM applications in mental healthcare. Index TermsLinguistic Emotion Diagnosis; Emotional Comorbidity; Inefficient Exploration; Automated Prompt Optimization; Multi-Agent Collaboration; Medical Language Processing; Trustworthy Artificial Intelligence"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Emotion diagnosis for mental health plays an essential role in understanding mental health conditions and tracking the progression of related disorders [1]. Emotions associated with illnesses, including depression, anxiety, and posttraumatic stress, frequently appear in clinical notes, counseling conversations, and online mental-health communities [2], [3]. These texts capture patients cognitive and affective states and play vital role in clinical triage, risk assessment, and intervention decisions [4]. Traditional emotion recognition methods, often based on shallow features or lexicons, perform reasonably well on generic sentiment tasks but struggle in medical contexts due to semantic ambiguity, implicit emotional expressions, and domain variability [5], [6], [7]. Recently, large language models (LLMs) have demonstrated remarkable capabilities in affective and psycholinguistic reasoning, achieving strong few-shot or zero-shot performance on mental healthrelated text analysis [8], * Corresponding author: Haiping Zhu and Qika Lin. Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu and Jun Liu are with the School of Computer Science and Technology, Xian Jiaotong University, Shaanxi, China, 710049. E-mail: {zhangjian062422, Asteria wzq, wang zy, fuweiping, heyucs}@stu.xjtu.edu.cn, {zhuhaiping, liukeen}@xjtu.edu.cn. Qika Lin is with Saw Swee Hock School of Public Health, National University of Singapore, Singapore 119077. E-mail: qikalin@foxmail.com. Fig. 1. Examples of disease-related emotion diagnosis under three prompting strategies: zero-shot, CoT, and APOLO. [9], [10]. However, in high-stakes clinical scenarios, LLM outputs are highly sensitive to the phrasing and structure of prompts. Minor variations in instruction design or reasoning path can lead to drastically different diagnostic outcomes [11]. For instance, cant sleep lately and dont want to see anyone implies both anxiety and depression, yet poorly designed prompt may capture only one. Handcrafted prompts, although guided by expert intuition, fail to systematically cover diverse semantic cues and implicit intentions, resulting in limited consistency and transferabil6 2 0 2 0 2 ] A . [ 1 1 8 4 3 1 . 1 0 6 2 : r IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1732 ity [11], [12]. Automated Prompt Optimization (APO) provides principled way to overcome these limitations by systematically exploring broader and finer-grained prompt space [13]. Through iterative search and evaluation, APO enables adaptive discovery of optimal task-specific prompts [11]. Nevertheless, generic APO methods remain inadequate for emotion diagnosis, where complex high-context semantics and safety constraints prevail [14]. As shown in Figure 1, we compare three prompting strategies for disease-related emotion recognition. The zero-shot prompt identifies only one emotion, while the Chain-of-Thought (CoT) prompt captures two, but both yield incomplete emotion diagnoses. This observation highlights that conventional prompting strategies fail to uncover the full spectrum of disease-related emotions, motivating the need for more systematic and adaptive approach to prompt optimization [15]. Such incompleteness arises from two fundamental issues: emotional comorbidity and inefficient exploration. Emotional comorbidity. Disease-related emotions often appear in intertwined and co-occurring forms, such as anxiety with depression, trauma with guilt, or anhedonia with self-blame. single utterance may convey multiple affective states that interact hierarchically or causally. Fixedtemplate prompts or single-label formulations fail to capture such dependencies [1]. For instance, the sentence feel guilty and afraid that might break down simultaneously expresses guilt and fear; without hierarchical reasoning, the model tends to detect only the dominant emotion. Existing prompt designs typically rely on flat labels or keyword cues, lacking mechanisms to model emotional co-occurrence and semantic overlap. As shown in Figure 1, both zero-shot and CoT prompts fail to capture the full set of coexisting emotions, whereas our framework dynamically adjusts prompt structure to mitigate label ambiguity and missing detection caused by comorbidity. Inefficient exploration. Most existing APO methods rely on two main strategies, namely generationsearch and meta-prompt, as illustrated in Figure 2. Generationsearch methods [16], [17], [18] generate candidate prompts and refine them through local search [13], but they typically explore only limited region of the prompt space and are prone to premature convergence [12]. Meta-prompt approaches [19], [20] design high-level instruction templates to guide optimization, but these templates are rigid and fail to adapt dynamically to diverse task contexts. Both paradigms lack global planning, uncertainty modeling, and interactive feedback, which are essential for effective search in high-dimensional prompt spaces. Such inefficiency not only constrains optimization performance but also increases instability, which is particularly critical in safety-sensitive medical applications. To address the above issues, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), which formulates prompt optimization as Partially Observable Markov Decision Process (POMDP) and introduces multi-agent collaboration mechanism consisting of the PlannerTeacherCriticStudentTarget roles. As illustrated in Figures 1 and 2, APOLO dynamically explores the prompt space through adaptive reasoning and collaborative feedback, enabling the model to capture more complete set of emotions with greater interpretability and stability. Specifically, the Planner designs task-aware search trajectories under risk and cost constraints; the TeacherCriticStudent trio refines reasoning chains via Socratic-style dialogue, providing interpretable pseudo-gradient feedback; and the Target agent evaluates performance and safety metrics to complete the optimization loop. Our main contributions are summarized as follows: APOLO framework. We introduce POMDP-based multi-agent architecture that enables dynamic and interpretable instruction refinement for linguistic emotion diagnosis. Task-specific optimization. We incorporate riskaware and cost-constrained planning to alleviate emotiondiagnosisspecific challenges such as emotional comorbidity and uncertain inference. Comprehensive validation. Extensive experiments demonstrate the effectiveness, robustness, and scalability of the modified model across diverse linguistic and clinical scenarios. This paper is an extension of our previous conference work [21]. Compared with that publication, we have made the following improvements: (1) this study extends the MARS framework to the Linguistic Emotion Diagnosis task, providing detailed analysis of its unique issues and corresponding solutions; (2) in Section 3, we introduce riskaware and cost-constrained planning module tailored to mitigate emotion-diagnosisspecific issues; and (3) more comprehensive experiments are conducted to verify the effectiveness and robustness of the modified model across various datasets."
        },
        {
            "title": "2 RELATED WORK",
            "content": "This section aims to review two research areas closely related to this paper: Linguistic Emotion Diagnosis and APO. We review and comment on the core research in these two fields, respectively, to lay the theoretical and practical foundation for the methods proposed in this paper. 2.1 Linguistic Emotion Diagnosis Linguistic emotion diagnosis is core subfield of affective computing that aims to identify and interpret human emotions from textual signals. Distinct from broader sentiment analysis (which often emphasizes polarity), emotion diagnosis typically addresses finer-grained emotion categories or dimensions (e.g., anger, joy, sadness, fear) and the temporal dynamics of affective states. Early methods relied on lexicons and rule-based approaches, followed by classical supervised machine-learning models such as naive Bayes and SVMs [22], [23], [24]; these approaches achieved reasonable performance on certain benchmarks but commonly required extensive feature engineering and showed limited cross-domain transferability. To better capture temporal dependencies in text, sequence models based on deep learning, such as RNNs, LSTMs, and their variants, are applied to emotion recognition [25], [26], [27], [28], delivering advantages for sentencelevel and discourse-level dynamics modeling. Some works extend CNN-LSTM architectures to multimodal settings IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 Fig. 2. Comparison of APO strategies. Top: generationsearch methods generate and locally refine candidate prompts, leading to limited coverage. Middle: meta-prompt approaches rely on fixed optimization templates with low adaptability. Bottom: our APOLO framework introduces multi-agent adaptive reasoning, enabling dynamic and collaborative prompt exploration. Right: performance comparison across three datasets based on m-F1 score. (e.g., combining text with speech or facial cues), which enhance emotion recognition by integrating prosodic and visual signals; such studies should be distinguished from pure textual methods. More recently, pre-trained language models (PLMs) built on the Transformer architecture (e.g., BERT) have substantially advanced text-based emotion detection by providing contextualized representations learned from large unlabeled corpora. Reviews of BERT-based methods [29], [30] report significant gains in capturing implicit emotion and contextual relationships, while also noting limitations in fewshot scenarios, long-tail emotion classes, and cross-domain generalization. Beyond static classification, recent research has focused on emotion dynamics, the patterns of how an individuals emotions evolve over time, and explored their potential as linguistic biosocial markers for mental health. Empirical studies, such as those by Vishnubhotla and Mohammad [31], demonstrate that statistics such as mean valence, variability, and recovery rate derived from textual sequences correlate significantly with conditions such as depression and PTSD , suggesting avenues for early, non-invasive mental-health screening via language signals. Nonetheless, challenges remain: annotation subjectivity and inter-annotator agreement for fine-grained emotions, cross-lingual and cross-cultural generalization, class imbalance and rare emotions, and the high cost of reliable evaluation. Recent directions include leveraging PLM transfer and fine-tuning, weakly-/distantly-supervised and fewshot learning to reduce annotation needs, and integrating temporal dynamics modeling to produce more robust individual affective profiles. In this work we build on these trends and emphasize sample-efficient, semantically sensitive approaches (e.g., structured prompts combined with task-adaptive strategies) to improve fine-grained emotion discrimination in linguistically subtle contexts. 2.2 APO APO seeks to enhance the downstream performance of LLMs by automatically refining prompts. Early APO work can be broadly categorized into two types: one focusing on discrete hard prompts (token-level) such as AutoPrompt [32], and another focusing on continuous soft prompts (vector-level) such as Prefix-Tuning [33] [34]. The former replaces tokens within predefined templates via gradient or enumeration methods to trigger latent model knowledge, while the latter prepends trainable vectors to input sequences, keeping the model body frozen for task adaptation. Although these approaches achieved notable results in some settings, they are often highly dependent on specific tasks or prompt templates, and tend to suffer from local-optima or limited transferability in the context of large-scale models, semantically rich tasks, or high evaluation cost. With the proliferation of powerful LLMs, research emphasis has shifted toward semantic-level prompt optimization. The APE (Automatic Prompt Engineer) framework exemplifies this shift: it uses an LLM to generate candidate natural-language instructions, then filters and selects optimal prompts via scoring and search [16], [35]. Building on this, prompt optimization research has coalesced into two major strands: first, the generate-and-search paradigm, which generates multiple candidate prompts and then applies search, evolutionary or Monte Carlo mechanisms for ranking, filtering, or iterative refinement (e.g., PromptAgent [18]); second, the meta-prompting strategy, which designs higher-order prompts that generate or improve other prompts [36]. Meta-prompts provide structured scaffolds or templates, enabling LLMs to fill in task-specific details and hence generate efficient prompts, thereby improving generality and design efficiency. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1734 Fig. 3. The overall architecture of the APOLO model, designed to support medical emotion diagnosis tasks. It consists of five LLM agents. The Planner agent that autonomously generates task-specific optimization trajectories, and Teacher -Critic-Student Socratic dialogue mechanism that iteratively refines prompts, with the evaluation and iterative refinement process guided by feedback from the Target agent. More recent work casts prompt optimization as decision or control problem: for instance, RLPrompt formulates discrete prompt generation as reinforcement-learning agent process, optimizing prompt policies with taskspecific reward signals [37]; moreover, multi-role/multiagent frameworks (generatorevaluatorcritic) [38] explore iterative conversational loops across prompt generation, evaluation and rewriting stages to evolve high-quality prompts [21]. These trends suggest that APO is transitioning from token-level search toward higher-level, structured, self-reflective optimization mechanisms. Given that linguistic emotion-diagnosis tasks involve subtle affective expressions, strong context dependence, and expensive annotation/assessment, this paper builds on that lineage to propose an APO method combining structured meta-prompts with task-adaptive optimization for this domain."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "This section describes the overall APOLO framework, consisting of: (1) the task formulation, (2) risk-aware and costconstrained hierarchical trajectory planning, (3) Socraticstyle joint policy refinement, (4) evaluation and convergence procedures, and (5) theoretical insights together with the full optimization algorithm. 3.1 Task Formulation In linguistic emotion diagnosis, the target model πtar receives text instance and must identify set of cooccurring emotional states. Given an initial instruction p0, APOLO aims to construct an optimized instruction that yields reliable multi-label predictions across emotion categories. Each sample (x, ) consists of clinical or psychological text and multi-label emotion set reflecting possible emotional comorbidity. To quantify optimization performance, we define the objective: where () denotes multi-label evaluation metrics such as Micro F1 or Jaccard similarity. This objective captures the goal of improving both completeness and consistency in emotion detection. In line with the iterative nature of clinical assessment, APOLO models the multi-agent refinement process as partially observable decision system (S, A, , O, R), where reflects latent diagnostic cues and uncertainty derived from the patients narrative, corresponds to clinically motivated interventions generated by the TeacherCritic agents (e.g., probing for comorbid signals or clarifying ambiguous affect), updates the internal diagnostic hypothesis, expresses this latent state as an actionable instruction for the target model, and = (πtar(x O(s)), ) quantifies diagnostic utility. This formulation enables APOLO to mirror real-world clinical reasoning: prompts evolve through uncertainty-aware state transitions rather than direct text editing, allowing the system to account for subtle emotional co-occurrence and risk-related linguistic cues. 3.2 Riskand Cost-Aware Trajectory Planning The Planner is responsible for generating sequence of intermediate sub-goals that guides the downstream multi-agent refinement. Given task goal g, input x, and initial prompt p0, it outputs trajectory ST = [st1, . . . , stn] = πplan(g, x, p0), where each sti specifies localized reasoning focus (e.g., identifying candidate emotions, probing uncertainty, or clarifying clinical details). Conventional trajectory planner selects ST by maximizing its likelihood under latent semantic context drawn from q(z g, x): STbase = arg max ST Ezq(zg,x) (cid:2) log (ST z, p0)(cid:3). (2) This objective encourages trajectories that are semantically plausible and aligned with contextual cues, but it lacks explicit mechanisms for handling clinical risk or computational efficiency. = arg max pP E(x,Y )Dtest (cid:2)f (πtar(x p), ) (cid:3), (1) In emotion diagnosis, trajectory may be semantically likely yet clinically unsafe (e.g., overlooking suicidal IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1735 ideation) or unnecessarily expensive (e.g., requiring many model calls). To mitigate such issues, we introduce two additional penalty terms: risk term Risk(ST) and cost term Cost(ST). The overall risk of trajectory is defined as the aggregated risk across all sub-goals: Risk(ST) = 1 (cid:88) i=1 (cid:2)Remo(sti; x, g) + Rsafety(sti; x, g)(cid:3), (3) where Remo reflects diagnostic risks such as missing comorbid emotions or producing ambiguous boundaries, while Rsafety captures safety-related concerns such as ignoring potential self-harm cues. The cost of trajectory is modeled as weighted combination of its length, expected number of model calls, and computational latency: Cost(ST) = αℓ + αcall Ccall(ST) + αtime Ctime(ST), (4) where Ccall(ST) denotes the estimated LLM call count and Ctime(ST) approximates wall-clock delay. The coefficients αℓ, αcall, αtime 0 control the trade-off among trajectory length, computational budget, and execution time. For simplicity, we set αℓ = αcall = αtime = 1 3 in all experiments. By combining the likelihood term with the risk and cost penalties defined in Eqs. (3)(4), the planner optimizes regularized objective that balances semantic plausibility, clinical safety, and computational efficiency: πplan(g, x, p0) = arg max ST (cid:104) log γr Risk γc Cost (cid:105) , (5) where γr, γc 0 are high-level trade-off coefficients that govern the planners sensitivity to safety-related risks and computational constraints. This unified objective generalizes the conventional planner in Eq. (2). When γr = γc = 0, the planner collapses to pure likelihood maximization, favoring semantically likely but potentially unsafe or computationally expensive trajectories. Larger γr encourages the planner to avoid unsafe or clinically risky sub-goals, promoting conservative exploration and more reliable emotion coverage; larger γc enforces compact trajectories that reduce model invocations and latency. Intuitively, Eq. (5) provides structured mechanism for controlling exploration behaviors: the likelihood term promotes semantic coherence in the latent space; the risk penalty restricts unsafe reasoning paths (e.g., skipping comorbid emotions or ignoring critical cues); and the cost term prevents unnecessary branching or excessive refinement. This yields trajectories that are not only linguistically and semantically sound, but also clinically reliable and computationally efficient, ensuring stable downstream refinement by the multi-agent system. Planning procedure. Operationally, APOLO implements this objective in three steps: 1) For each (g, x), sample or approximate q(z g, x) using the underlying language model, capturing task semantics and risk signals. 2) Conditioned on (z, p0), generate small set of cank=1 using the Planners didate trajectories {ST(k)}K policy (ST z, p0). 3) For each candidate, compute (ST(k)), as in Eq. (5) and select ST = arg maxk (ST(k)). The selected trajectory ST is then passed to the TeacherCriticStudent triad as high-level optimization path. In this way, the overall refinement process is guided by planner that is explicitly aware of both clinical risk and computational cost, rather than optimizing prompts in an unconstrained or purely heuristic manner. 3.3 Joint Policy Optimization Given the risk-aware and cost-constrained trajectory ST generated by the Planner, APOLO performs structured multi-step refinement through TeacherCriticStudent triad. This refinement is essential for linguistic emotion diagnosis, where emotional expressions are often implicit, multilabel, and safety-sensitive. The triad collaboratively resolves ambiguity, strengthens diagnostic reasoning, and produces increasingly consistent and clinically aligned prompts. At refinement step i, the Teacher proposes an emotionaware probing query qi based on the sub-goal sti and the previous prompt pi1. The Critic evaluates the proposal along three dimensions: clarity, diagnostic relevance, and safety sensitivity, and returns structured feedback ci. The Student incorporates the pair (qi, ci) and updates the working prompt pi. This interaction process follows qi = πt(sti, pi1), ci = πc(qi), pi = πs((qi, ci), pi1), and latent-state transition si (si1, (qi, ci)), with oi = pi. Because clinical emotion expression frequently spans multiple interacting symptoms, such as depression cooccurring with anxiety or guilt, isolated refinement steps may lose semantic continuity. To maintain coherence, each agent conditions not only on (sti, pi1) but also on the historical interaction sequence H<i = {(qj, cj, pj)}j<i. The corresponding context-augmented update is: qi = πt(sti, pi1, H<i), ci = πc(qi, H<i), pi = πs((qi, ci), pi1, H<i). (6) (7) This formulation aligns with the POMDP transition dynamics: each refinement step updates the latent dialogue state and progressively reduces uncertainty across interdependent emotional dimensions. Joint policy optimization. Collectively, the Teacher, Critic, and Student form joint policy Π = {πt, πc, πs} operating in the latent state space. APOLO learns Π to maximize diagnostic performance while ensuring consistency with the planners trajectory. The optimization objective is: (cid:34) max Π E(x,Y )D R(Π) λ"
        },
        {
            "title": "Lalign",
            "content": "(cid:0)(qi, ci), sti (cid:1) (cid:35) , (8) (cid:88) i=1 where R(Π) measures multi-label diagnostic accuracy and safety compliance, and Lalign penalizes deviations from the sub-goals planned in Section 3.2. The coefficient λ controls the balance between adaptive exploration and goalconsistent refinement. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1736 3.4 Evaluation and Iterative Convergence After completing the refinement steps, the final prompt pℓ = pn is evaluated by the Target agent on the heldout test set. The evaluation provides an external performance signal that measures how well the entire PlannerTeacherCriticStudent pipeline improves diagnostic accuracy. Formally, at outer iteration the reward is: R(t) = (cid:88) (cid:0)πtar(x; p(t) ℓ ), (cid:1), (9) (x,Y )Dtest where () denotes task-specific metric (e.g., micro-F1 or Jaccard for multi-label emotion diagnosis). To ensure computational efficiency and prevent unnecessary refinement, APOLO employs an adaptive earlystopping rule based on marginal reward gain: R(t) = R(t) R(t1). (10) The optimization halts when R(t) δ or I, where δ is minimum improvement threshold and is the maximum number of outer iterations. This evaluationiteration mechanism closes the optimization loop, ensuring that APOLO converges to prompt that achieves strong diagnostic performance without excessive computation or over-refinement. 3.5 Theoretical Insights The performance gain achieved by APOLO integrates two complementary factors: (i) the risk-aware and costconstrained planner, which restricts unsafe or inefficient exploration paths, and (ii) the multi-agent Socratic refinement mechanism, which provides structured pseudo-gradient updates to progressively improve prompt quality. Together, these components produce stable and interpretable optimization dynamics within the POMDP framework. Unified improvement bound. Let (qi, ci) denote the TeacherCritic action at refinement step i, with expected advantage Ai and bounded variance σ2. Let Br and Bc provide upper bounds for the cumulative risk and cost penalties incurred along the planned trajectory. Then, after refinement steps, the expected improvement satisfies: E[R(pn)] R(p0) (cid:88) (cid:18) i=1 Ai (cid:19) σ2 2λ γrBr γcBc. (11) The first term reflects progressive improvement contributed by Socratic pseudo-gradients, while the last two terms account for explicit penalties from the riskand costaware planning objective. Equation (11) shows that APOLO improves expected reward monotonically up to explicit, bounded offsets introduced for safety and computational efficiency. Proof sketch. We view each refinement step as KLregularized policy update for the joint policy Π. Standard analysis of KL-constrained policy improvement shows that the expected reward gain at step satisfies Ri = E[R(pi)] E[R(pi1)] Ai σ2 2λ , Algorithm 1 APOLO Optimization Procedure 1: Input: Dataset D, initial prompt p0, thresholds (δ, γrisk, γcost), maximum iterations 2: Output: Optimized prompt 3: Planner: Generate risk-aware and cost-constrained trajectory ST = 0 p0, R(0) 0 for = 1 to do qi πt(sti, p(t) repeat i1) ci πc(qi) Teacher revises qi if necessary until quality and safety are satisfied πs((qi, ci), p(t) p(t) end for p(t) ℓ p(t) R(t) = (cid:80) if R(t) R(t1) δ then (πtar(x; p(t) ℓ ), ) (x,Y )Dtest i1) {st1, . . . , stn} 4: Initialize p(1) 5: for = 1 to do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end for 20: return p(t) ℓ end if break // Socratic refinement (clarity, relevance, safety) // Final prompt where Ai is the expected advantage of taking action (qi, ci) under the current policy and σ2 bounds the variance of the corresponding return. Summing over = 1, . . . , yields (cid:88) (cid:88) (cid:18) (cid:19) E[R(pn)] R(p0) = Ri Ai . σ2 2λ i=1 i= The planner further introduces explicit penalties on risk and cost along the trajectory. By assumption, the accumulated risk and cost terms are bounded by Br and Bc, and the corresponding contributions to the objective are scaled by γr and γc, respectively. Therefore, incorporating these penalties subtracts at most γrBr + γcBc from the achievable improvement, which leads directly to the bound in Eq. (11). The complete optimization procedure is summarized in Algorithm 1."
        },
        {
            "title": "4 EXPERIMENTS\nIn this section, we conduct comprehensive experiments\nto evaluate the proposed APOLO framework. Section 4.1\nintroduces the experimental setup, including datasets, im-\nplementation details, evaluation metrics, and baselines. Sec-\ntion 4.2 presents the main results, demonstrating APOLO’s\nsuperior performance across diverse tasks and models. Sec-\ntion 4.3 further analyzes its computational efficiency, show-\ning that APOLO is both effective and practical.",
            "content": "4.1 Experiment Setup Datasets. To ensure comprehensive and rigorous evaluation of APOLOs capabilities, our experiments are conducted on carefully curated selection of six public datasets. This collection spans wide spectrum of conversational emotion analysis tasks, systematically testing the frameworks adaptability and effectiveness. We include foundational multi-turn dialogue datasets like DailyDialog [39] for general conversational scenarios and EmoryNLP [40] to challenge the model with longer contextual dependencies. To assess the frameworks ability to handle more IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1737 TABLE 1 Comprehensive comparison of APOLO and baseline methods on six emotion diagnosis benchmarks. For single-label datasets, we report both Macro F1(M-F1) and Micro F1(m-F1) scores (%). For the multi-label DepressionEmo dataset, we additionally present Exact Match Ratio (EMR, %) and Partial Match Accuracy (PMA, %). The best and second-best results are highlighted in bold and underlined, respectively. Method Origin CoT(ZS) CoT(FS) APE ProTeGi OPRO PE2 DailyDialog EmoryNLP PELD RECCON EmotionX DepressionEmo Avg. M-F1 m-F1 M-F1 m-F1 M-F1 m-F1 M-F1 m-F1 M-F1 m-F1 M-F1 m-F1 EMR PMA M-F1 m-F1 GPT-5-mini 27.37 28.15 29.02 29.56 30.11 31.25 30.78 75.26 75.98 76.85 77.42 77.98 79.12 78.68 41.51 42.33 43.10 43.65 44.21 45.15 45.42 46.16 47.06 48.19 48.95 49.70 50.83 51.20 46.09 46.95 47.88 48.45 49.03 50.23 49.75 60.73 61.87 63.00 63.78 64.55 66.00 65. 37.71 38.54 39.42 39.99 40.57 41.77 41.28 43.01 44.29 45.65 46.43 47.21 48.75 48.12 32.84 33.67 34.51 35.02 35.54 36.63 36.18 46.38 47.52 48.79 49.54 50.33 51.89 51.27 58.26 59.98 61.85 63.01 64.22 66.45 65.59 69.32 70.97 73.07 74.50 75.94 78.70 77. 11.15 13.36 16.00 18.10 20.42 24.28 24.61 58.61 62.36 68.65 73.40 78.15 86.75 83.00 40.63 41.60 42.63 43.28 43.95 45.25 44.83 56.81 57.95 59.26 60.10 60.95 62.55 62.03 APOLO (Ours) 34. 82.34 48.01 54.22 53.15 69.23 44. 51.89 39.21 55.02 70.11 82.45 28. 91.17 48.19 65.86 Origin CoT(ZS) CoT(FS) APE ProTeGi OPRO PE2 23.47 24.55 25.68 26.34 27.01 28.31 27.82 36.16 39.87 43.71 46.21 48.81 53.19 51. 39.88 40.76 41.67 42.24 42.82 43.95 43.51 43.15 44.95 46.84 47.89 48.95 50.83 50.08 45.54 46.63 47.75 48.43 49.12 50.35 50.55 53.83 55.02 56.24 57.01 57.80 59.13 59.41 DeepSeek-V3 45.92 46.91 47.95 48.60 49.26 50.59 50. 47.82 49.33 50.91 51.88 52.87 54.72 53.99 31.81 32.75 33.72 34.33 34.95 36.21 35.69 36.64 38.01 39.41 43.09 41.24 42.96 42.28 65.04 66.82 68.75 70.01 71.30 73.69 72.76 75.06 76.93 79.03 80.46 81.90 84.66 83.55 10.82 12.91 15.23 17.11 19.21 22.96 21. 86.31 88.08 89.96 90.95 92.05 94.04 93.38 41.94 43.07 44.25 44.99 45.74 47.18 46.73 48.78 50.69 52.69 54.42 55.26 57.58 56.81 APOLO (Ours) 31.02 57. 46.55 54.89 53.48 62.50 53.61 58. 38.84 45.99 77.23 88.08 27.15 96. 50.12 61.12 Qwen3-32B Origin CoT(ZS) CoT(FS) APE ProTeGi OPRO PE2 28.11 29.02 29.98 30.55 31.13 32.33 31.84 76.01 76.89 77.80 78.40 79.01 80.21 79. 42.43 43.29 44.18 44.76 45.35 46.52 46.06 47.21 48.12 49.25 50.00 50.83 52.33 51.73 47.02 47.99 48.98 49.58 50.19 51.46 50.94 61.86 63.02 64.24 65.03 65.84 67.37 66.76 38.65 39.51 40.45 41.05 41.66 42.71 42.95 44.13 45.44 46.85 47.67 48.48 49.88 50. 33.76 34.62 35.51 36.05 36.60 37.75 37.28 47.49 48.66 49.93 50.75 51.56 53.19 52.54 59.33 61.11 63.05 64.25 65.49 67.82 66.91 70.42 72.19 74.28 75.83 77.26 80.02 78.92 11.92 14.24 16.89 19.21 21.41 25.61 23.95 60.04 63.91 70.42 75.28 80.13 89.85 90. 41.55 42.59 43.69 44.37 45.07 46.43 46.00 57.85 59.05 60.39 61.28 62.16 63.83 63.30 APOLO (Ours) 35.24 83.51 49. 55.50 54.41 70.62 45.72 53.18 40. 56.42 71.55 83.89 30.02 93.38 49. 67.19 complex inference, we incorporate PELD [41] and RECCON [42], both of which introduce the task of identifying emotion-cause pairs. Furthermore, to test the robustness of our method in noisy, real-world settings, we utilize the EmotionX [43] dataset, composed of multi-party dialogues from diverse social media platforms. Finally, to evaluate performance on specialized, fine-grained tasks, we include DepressionEmo [44], which uniquely presents the challenge of multi-label classification of psychological distress signals. Collectively, this diverse suite of datasets allows us to validate the generalizability of APOLO across various complexities, domains, and annotation schemes inherent in linguistic emotion diagnosis. Implementation Details. To demonstrate its versatility and robustness, we conduct parallel experiments utilizing two distinct large language models as the backbone for all agents: GPT-5-mini [45], DeepSeek-V3 [46] and Qwen-32B [47]. Within any single experimental run, all agents (Planner, Teacher, Student, Critic, and Target) are powered by the same backbone model to ensure internal consistency. For all generative inferences, the decoding temperature is uniformly set to 0.6 to balance creativity and factual coherence. The automated optimization process is configured for maximum of = 10 iterations. To enhance efficiency and prevent overfitting on minor gains, an early stopping mechanism is implemented. The process terminates if the accuracy improvement on the validation set between two consecutive iterations falls below threshold of δ = 0.01. Furthermore, to streamline the optimization path, each assess-adjust cycle within an iteration is constrained to single revision per step. Following each optimization cycle, the performance of the generated prompt is rigorously evaluated by the Target agent on the entire test set of the respective dataset to track the optimization trajectory comprehensively. Evaluation Metrics. The selection of evaluation metrics is carefully aligned with the specific characteristics of each task to ensure fair and comprehensive assessment of model performance. For the multi-class classification tasks, which constitute the majority of our datasets (e.g., DailyDialog, EmoryNLP), the Macro F1-score is adopted as the primary evaluation metric. This measure is particularly suitable for emotion recognition as it alleviates potential bias from class imbalance by computing the F1-score independently for each emotion category and then averaging them. Formally, for set of classes C, where F1c is the F1-score for IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1738 class c: Macro F1 = 1 (cid:88) cC F1c (12) This approach assigns equal importance to both frequent and rare emotions. To provide complementary perspective on overall performance, we also report the Micro F1score, which aggregates the contributions of all samples to compute single metric. It is calculated from the global sum of true positives (TPµ), false positives (FPµ), and false negatives (FNµ): Micro F1 = 2 TPµ 2 TPµ + FPµ + FNµ (13) For the multi-label classification task in the DepressionEmo dataset, where each sample may be associated with multiple emotional states, multi-faceted evaluation strategy was employed. For dataset of samples, let Yi be the set of true labels and Zi be the set of predicted labels for the i-th sample. The Exact Match Ratio (EMR) provides strict measure by quantifying the percentage of instances whose predicted label sets exactly match the ground truth: EMR ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) I(Yi = Zi) (14) i=1 where I() is the indicator function. To complement this stringent criterion and account for partially correct predictions, the Partial Match Accuracy (PMA) was introduced, which deems prediction correct if the model successfully identifies more than half of the true labels: PMA ="
        },
        {
            "title": "1\nN",
            "content": "(cid:18) (cid:88) i=1 Yi Zi > (cid:19) Yi 2 (15) Furthermore, to ensure comprehensive assessment of per-label performance, we also report both the label-based Macro F1-score (calculated as in Eq. 12) for balanced view of performance across all labels, and the label-based Micro F1-score (calculated as in Eq. 13) for an aggregate measure of correctness. Collectively, this comprehensive suite of metrics provides robust and nuanced evaluation framework for prompt performance across both single-label and multilabel diagnostic scenarios. Baselines. We compare APOLO against three categories of baselines: original prompts, CoT prompts, and several state-of-the-art(SOTA) APO approaches. (1) Original prompts refer to the default task instructions provided within each dataset, which typically contain minimal task-specific guidance and therefore serve as straightforward reference point for performance evaluation. (2) For the CoT (Zero-Shot) baseline, we prepend the phrase Lets think step by step to each task instruction to encourage explicit intermediate reasoning. Building upon this, the CoT (Few-Shot) baseline incorporates an additional in-context example that illustrates the reasoning process, thereby enhancing the models ability to generalize from limited demonstrations. (3) In addition, we include several representative APO methods from recent literature, namely APE [16], Prompt Optimization with Textual Gradients (ProTeGi) [48], Optimization by PROmpting (OPRO) [19], and Prompt Engineer 2 (PE2) [20]. APE formulates prompt design as search problem, where LLM autonomously generates, evaluates, and selects optimal prompt candidates based on self-assessment. ProTeGi refines prompts through gradientinspired textual feedback derived from model prediction errors, enabling more directed optimization. OPRO conceptualizes prompt optimization as an iterative meta-learning process, in which the LLM itself functions as an optimizer that continually improves prompt formulations. Building upon this idea, PE2 introduces structured meta-prompting framework that performs multi-round refinement, yielding more robust and generalizable prompts across diverse tasks. 4.2 Main Results APOLO achieves an overall improvement across all benchmarks, setting new SOTA in APO for emotion diagnosis. Across six benchmark datasets, as summarized in Table 1, APOLO achieves the highest overall performance with an average Macro F1 of 49.25% and Micro F1 of 64.72%, surpassing the best baseline (OPRO) by 2.96% and 3.40%, respectively. The improvement is not limited to aggregated scores: APOLO also increases the Exact Match Ratio (EMR) to 28.70% and the Partial Match Accuracy (PMA) to 93.78%, indicating stronger consistency and precision in handling complex, multi-label emotional data. Together, these results demonstrate that APOLOs optimization framework substantially enhances both the accuracy and reliability of emotion understanding across diverse diagnostic scenarios. APOLO delivers consistent and substantial improvements across both single-label and multi-label emotion diagnosis tasks. For single-label datasets such as DailyDialog and EmotionX, APOLO achieves remarkable gains in Macro F1, suggesting its superior capability in identifying minority or subtle emotional categories that are often underrepresented during training. This improvement reflects APOLOs enhanced ability to capture fine-grained affective nuances and mitigate the effects of class imbalance. In multilabel settings, particularly on the DepressionEmo dataset, APOLO substantially boosts the Exact Match Ratio (EMR) and Partial Match Accuracy (PMA), with increases of up to +4.41% and +3.57%, respectively. These gains indicate that APOLO more effectively models complex emotional cooccurrences and overlapping affective states, such as anxiety coexisting with fatigue or sadness. Moreover, the consistently larger relative improvement in Macro F1 over Micro F1 demonstrates that APOLO not only enhances overall predictive performance but also strengthens its sensitivity to infrequent yet clinically meaningful emotions. APOLO demonstrates consistent advantages across heterogeneous foundational models, reflecting its robustness, adaptability, and strong generalization capability. When applied to three representative LLM backbones, namely GPT-5-mini (closed-source), DeepSeek-V3, and Qwen3-32B (both open-source), APOLO consistently achieves superior results under all experimental configurations. Across these models, it delivers average Macro F1 improvements of +2.94%, +2.94%, and +3.00% over the strongest baseline, respectively, together with comparable gains in Micro F1, indicating stable and comprehensive performance enhancement. Notably, APOLOs improvements generalize well beyond specific architectures or vendor IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 computational overhead and less efficient utilization of inference resources under emotion diagnosis settings. These findings clearly highlight APOLOs superior capability to balance effectiveness and efficiency through its structured optimization design. By first conducting highlevel task decomposition and then applying step-wise Socratic refinement tailored to emotion diagnosis reasoning patterns, APOLO facilitates hierarchical reasoning and adaptive resource allocation, allowing the model to focus computational effort where it is most impactful. Such strategy substantially reduces redundant generation, mitigates unnecessary reasoning steps, and ultimately ensures both robust performance gains and sustainable computational cost throughout the entire APO pipeline."
        },
        {
            "title": "5 SUPPLEMENT ANALYSIS",
            "content": "In this section, we present supplementary analyses to further assess the robustness of our approach. We first conduct an ablation study to evaluate the contribution of each component in Section 5.1, followed by convergence analysis to examine optimization stability in Section 5.2. We then analyze the effect of different initial prompts P0 in Section 5.3, and the influence of sample size on performance in Section 5.4. 5.1 Ablation Study To better understand the internal mechanisms of the APOLO framework and evaluate the contribution of its core components, we conduct systematic ablation study. We remove three essential modules, namely the Planner, the Socratic module, and the Critic, and examine their effects on performance across three representative LLM backbones. The results present in Table 2 show that each module plays vital role in maintaining APOLOs effectiveness, with clear hierarchy of importance among them. The Socratic module, which consists of the Teacher, Student, and Critic agents, proves to be the most crucial component in emotion diagnosis task. Removing it, referred to as w/oSoc, leads to the most significant performance degradation. On average, Macro F1 decreases by 7.19%, Micro F1 by 8.69%, EMR by 11.22%, and PMA by 12.07%. This module acts as the optimization core of APOLO, driving iterative reflection and improvement through multiround questionanswer interactions. Without this process, APOLO loses its ability to iteratively refine prompts, reverting to simple single-pass generator guided only by initial planning. The sharp decline in all metrics demonstrates that the dialog-based iterative refinement is not an auxiliary enhancement but the fundamental mechanism enabling APOLO to achieve its superior performance. The removal of the Planner leads to the second most significant performance decline. Specifically, w/oPlan results in average decreases of 3.75% in Macro F1, 4.90% in Micro F1, 6.66% in EMR, and 6.73% in PMA. The Planner decomposes complex optimization tasks into manageable subgoals, allowing the Socratic module to focus on improving specific aspects of the prompt. In its absence, the model must optimize the entire prompt at once, which is less efficient, less focused, and more prone to suboptimal Fig. 4. Inference-time scaling law for APOLO and baseline methods, evaluated on datasets related to medical emotion diagnosis, illustrating the relationship between Micro F1-score and total token consumption during optimization. ecosystems. It achieves robust gains on both proprietary systems such as GPT-5 and open-source counterparts including DeepSeek and Qwen, suggesting that its Socratic optimization mechanism effectively enhances emotional reasoning independent of model origin, architecture, or training pipeline. Moreover, the relative performance hierarchy among baseline methods remains consistent across all backbones, reinforcing the stability and reproducibility of APOLOs optimization dynamics. 4.3 Efficiency Analysis APOLO Consistently Achieves the Highest Computational Efficiency for emotion diagnosis. The balance between resource consumption and performance improvement serves as fundamental criterion for evaluating the practical effectiveness of APO systems in emotion diagnosis [49]. As illustrated in Figure 4, APOLO consistently surpasses all baseline methods in terms of computational efficiency, demonstrating more favorable inference-time scaling trend across wide range of APO tasks related to emotion diagnosis. This indicates that APOLO not only achieves faster convergence but also maintains stable performance even as task complexity or model scale increases. As shown in Figure 4, the red APOLO curve is located in the upper-left region of the plot, indicating its superior efficiency in achieving higher accuracy with fewer generated tokens. Specifically, when the number of generated tokens is fixed at approximately 24.4 105, APOLO achieves about 7% higher average Micro F1-score than OPRO on emotion diagnosis benchmarks (as highlighted by the red dashed annotation). Conversely, to reach the same performance level as APOLO, OPRO must generate roughly 7 105 more tokens (as indicated by the blue dashed arrow), further emphasizing APOLOs advantage in balancing performance and computational cost. In contrast, competing methods generally require significantly larger number of generated tokens to reach comparable results, which implies greater IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1740 TABLE 2 Ablation study of APOLOs key components, conducted on medical emotion diagnosis tasks using three distinct backbone models. We systematically remove the Planner (w/oPlan), the entire Socratic module (w/oSoc), and the Critic (w/oCri). The performance drop () relative to the full APOLO framework is shown in green. Method DailyDialog EmoryNLP PELD RECCON EmotionX DepressionEmo Avg. M-F1 m-F1 M-F1 m-F1 M-F1 m-F1 M-F1 m-F1 M-F1 m-F1 M-F1 m-F1 EMR PMA M-F1 m-F1 GPT-5-mini APOLO (Ours) 34.11 82.34 48.01 54.22 53. 69.23 44.52 51.89 39.21 55.02 70. 82.45 28.92 91.17 48.19 65.86 w/oPlan w/oSoc w/oCri 31.18 -2.93 28.55 -5.56 32.54 -1.57 79.01 -3. 75.88 -6.46 80.52 -1.82 44.53 -3.48 40.91 -7.10 46.18 -1.83 50.08 -4. 46.54 -7.68 52.33 -1.89 49.25 -3.90 45.81 -7.34 51.08 -2.07 65.18 -4. 61.98 -7.25 67.01 -2.22 41.05 -3.47 38.12 -6.40 42.88 -1.64 48.02 -3. 44.25 -7.64 50.11 -1.78 36.15 -3.06 33.04 -6.17 37.55 -1.66 51.34 -3. 48.18 -6.84 53.09 -1.93 64.98 -5.13 60.15 -9.96 67.53 -2.58 78.15 -4. 73.07 -9.38 80.02 -2.43 23.62 -5.30 18.87 -10.05 26.05 -2.87 84.99 -6. 79.25 -11.92 88.63 -2.54 44.52 -3.66 41.10 -7.09 46.29 -1.89 61.96 -3. 58.32 -7.54 63.85 -2.01 DeepSeek-V3 APOLO (Ours) 31.02 57. 46.55 54.89 53.48 62.50 53.61 58. 38.84 45.99 77.23 88.08 27.15 96. 50.12 61.12 w/oPlan w/oSoc w/oCri 28.15 -2. 25.41 -5.61 29.45 -1.57 49.88 -7.33 46.55 -10.66 51.36 -5.85 43.12 -3. 40.03 -6.52 44.89 -1.66 46.99 -7.90 43.30 -11.59 49.10 -5.79 49.85 -3. 46.19 -7.29 51.64 -1.84 55.24 -7.26 51.07 -11.43 57.28 -5.22 49.95 -3. 46.58 -7.03 51.87 -1.74 54.12 -3.91 50.33 -7.70 56.15 -1.88 35.77 -3. 32.54 -6.30 37.12 -1.72 39.54 -6.45 36.19 -9.80 41.24 -4.75 72.08 -5. 67.81 -9.42 75.02 -2.21 80.13 -7.95 75.94 -12.14 82.78 -5.30 18.10 -9. 14.13 -13.02 20.86 -6.29 88.63 -8.17 83.22 -13.58 91.83 -4.97 46.49 -3. 43.09 -7.03 48.33 -1.79 54.32 -6.80 50.56 -10.55 56.32 -4.80 Qwen-32B APOLO (Ours) 35.24 83.51 49.21 55.50 54. 70.62 45.72 53.18 40.42 56.42 71. 83.89 30.02 93.38 49.43 67.19 w/oPlan w/oSoc w/oCri 31.21 -4.03 28.05 -7.19 33.14 -2.10 79.43 -4. 76.12 -7.39 81.21 -2.30 45.23 -3.98 41.91 -7.30 46.85 -2.36 51.73 -3. 47.29 -8.21 53.39 -2.11 50.02 -4.39 46.88 -7.53 52.04 -2.37 66.18 -4. 62.02 -8.60 68.19 -2.43 42.38 -3.34 38.91 -6.81 43.72 -2.00 49.77 -3. 45.43 -7.75 51.62 -1.56 37.14 -3.28 33.63 -6.79 38.42 -2.00 52.05 -4. 48.63 -7.79 53.75 -2.67 66.81 -4.74 62.40 -9.15 68.52 -3.03 79.91 -3. 75.72 -8.17 81.90 -1.99 24.39 -5.63 19.43 -10.59 26.93 -3.09 87.53 -5. 82.67 -10.71 89.74 -3.64 45.47 -3.96 41.96 -7.46 47.12 -2.31 63.18 -4. 59.20 -7.99 65.01 -2.18 reasoning paths. These results emphasize that the Planner plays critical role in structuring the reasoning process and enabling efficient multi-stage refinement. The Critic contributes the least to performance decline among all components. Removing this agent (w/oCri) leads to smaller but consistent performance declines, including 2.00% in Macro F1, 3.00% in Micro F1, 4.08% in EMR, and 3.72% in PMA. The Critic monitors the interaction between the Teacher and Student, ensuring that questions remain focused, logical, and open-ended. Without it, the dialogue may deviate from the intended principles, reducing the precision and consistency of the refinement process. These findings indicate that the Critic operates as metaoptimization layer that ensures stable and principled reasoning throughout the iterative process. 5.2 Converagence Analysis APOLO demonstrates rapid and stable convergence in emotion diagnosis tasks, effectively balancing optimization efficiency and computational cost. Beyond achieving state-of-the-art performance, an essential property of an automated optimization framework for emotional reasoning lies in its efficiency. practical method should not only discover high-quality diagnostic prompts but also do so with minimal time and computational overhead. To this end, we analyze APOLOs optimization trajectory to examine its convergence behavior and efficiency characteristics. As shown in Figure 5, which plots validation performance (m-F1) against optimization iterations across six emotion-related datasets (DailyDialog, EmoryNLP, PELD, RECCON, EmotionX, and DepressionEmo), APOLO consistently exhibits clear pattern of rapid early improvement followed by smooth stabilization. During the initial iterations (typically 14), APOLO achieves sharp increase in m-F1, substantially outperforming all baselines as it rapidly corrects major emotional reasoning and alignment deficiencies in the initial prompt. This phase accounts for the majority of the total improvement, with early-stage gains ranging from approximately 5% to 10%. After this surge, the performance gradually plateaus between iterations 610, reflecting transition from coarse structural optimization to fine-grained affective refinement. In this stage, improvements become smaller yet more stable, indicating that the optimization focuses on subtle behavioral adjustments rather than large-scale modifications. Compared to OPRO, which improves more slowly and exhibits mild fluctuations, APOLO maintains smoother and more monotonic convergence curve, particularly on RECCON and EmotionX where emotional convergence stability is typically harder to achieve. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1741 Fig. 5. Convergence analysis of APOLOs optimization process on six emotion diagnosis datasets. The plots track the Micro F1-score (m-F1) over 10 iterations. APOLOs trajectory is compared against the iterative baseline OPRO and three static baselines (Original, CoT, APE). TABLE 3 Final performance (Micro F1-scores) of APOLO on the RECCON dataset (using GPT-5-mini) when initialized with six different starting prompts (P0). The table shows the initial performance of each prompt and the final performance after optimization by APOLO. Method Prompt 1 Prompt 2 Prompt 3 Prompt Prompt 5 Prompt 6 P0 Choose the single most appropriate label for the utterance from the options. What is the speakers emotion? Choose from the options. You are an AI assistant. Your task is to identify the emotion. Your task is to output single emotion for the utterance. Based on the conversation, identify the emotion. Please tell me the emotion of the last sentence. Origin APOLO 43. 51.89 42.22 50.95 42.03 51.09 42. 50.11 42.54 50.99 41.05 49.88 5.3 Different Initial Prompts P0 APOLO consistently converges to high-performing solutions in emotion diagnosis tasks regardless of the quality of the initial prompt, demonstrating the robustness of its optimization trajectory. The starting point (P0) can influence the path of optimization, so truly robust framework should be able to find an effective solution even if the initial prompt is suboptimal. We test APOLOs resilience by initializing it with six distinct prompts of varying styles and quality on the RECCON dataset. The prompts range from simple, direct command to question-based format and basic role-playing instruction. As shown in Table 3, although the different starting prompts would lead to different initial performance levels, APOLO consistently guides the optimization towards state of high performance. The final Micro F1-scores are all within very narrow range, with standard deviation of only 0.6647, indicating that the framework is not highly sensitive to the quality of the initialization. This highlights APOLOs reliability: it can effectively adapt and refine its strategy to discover superior prompt, even from less-than-ideal starting point. This makes it highly dependable tool for emotion-related applications for users, regardless of their initial prompt engineering expertise. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1742 TABLE 4 Performance (Micro F1-scores) comparison of different sampling strategies on the evaluation metric across DailyDialog (D.D.), EmoryNLP (E.N.), PELD (PE.), RECCON (RE.), EmotionX (E.X), and DepressionEmo (D.E.), respectively. The experiments are conducted using the GPT-5-mini model. Train means the training data. Tasks Train D.D. E.N. PE. RE. E.X D.E. APE ProTeGi OPRO PE2 APOLO APOLO APOLO 100 20 50 0 1 3 77.42 48.95 63.78 46.43 49.54 74.50 77.98 49.70 64.55 47.21 50.33 75.94 79.12 50.83 66.00 48.75 51.89 78.70 78.68 51.20 65.45 48.12 51.27 77.48 80.76 52.28 67.09 50.83 54.35 80.94 82.34 54.22 69.23 51.89 55.02 82.45 82.57 54.69 69.30 51.24 55.96 83.35 5.4 Sample Size Analysis critical challenge in developing robust models for linguistic emotion diagnosis is the inherent cost and scarcity of high-quality, expert-annotated data, particularly for nuanced tasks like those in the DepressionEmo dataset. practical framework must therefore be highly data-efficient. To rigorously evaluate APOLOs efficiency, we analyze its performance under zero-shot, one-shot, and three-shot settings, benchmarking it against baseline methods that rely on substantially larger sample sizes for their optimization processes. The results, presented in Table 4, are unequivocal: APOLO, when provided with just single exemplary dialogue (1-shot), consistently outperforms all baseline methods, even those that leverage up to 100 samples to guide their search. This is particularly evident on the complex DepressionEmo dataset, where APOLO (1-shot) achieves Micro F1-score of 82.45%, surpassing the strongest baseline, OPRO (which uses 50 samples). This demonstrates that APOLO can achieve state-of-the-art performance with fraction of the data required by other leading frameworks. Furthermore, the results show that increasing the sample size from one to three shots yields only marginal improvements across all six emotion diagnosis tasks. This indicates that single, well-chosen example is sufficient for APOLO to capture the essential principles for effective prompting, making the one-shot configuration the best trade-off between performance and efficiency. This efficiency highlights the strength of APOLOs Socratic refinement mechanism, which extracts deep and generalizable insights from minimal data. Consequently, APOLO emerges as both highperforming and cost-effective framework for real-world emotion diagnosis, where labeled data is often scarce."
        },
        {
            "title": "6 DETAILS OF DATASETS\nPROMPTS",
            "content": "AND OPTIMIZED In this section, we first introduce the public datasets used in our experiments on linguistic emotion diagnosis in Section 6.1. We then describe the system prompts that define the roles and behaviors of each agent in Section 6.2. Finally, we present case study illustrating one complete optimization iteration in Section 6.3. 6.1 Datasets DepressionEmo [44]. The DepressionEmo dataset is multilabel corpus that captures broad range of depressive emotions expressed through naturalistic online text. It comprises user-generated posts from online communities, annotated with multiple emotional dimensions such as sadness, hopelessness, loneliness, worthlessness, and emptiness. Unlike conventional emotion classification datasets that focus on basic affective states, DepressionEmo provides fine-grained distinctions among depressive manifestations, making it suitable for tasks that involve psychological or clinical language patterns. In this work, it serves as an essential benchmark to evaluate whether the proposed prompt optimization framework can effectively diagnose and disentangle overlapping emotional cues in long, unstructured text typical of mental health discourse. DailyDialog [39]. The DailyDialog dataset contains multi-turn dialogues representing various everyday conversational situations, each labeled with both emotion and communicative intent. The test split used in this study includes 1,000 dialogues, encompassing diverse set of topics and interpersonal scenarios. This dataset provides high-quality human-written language with well-formed sentences and clear emotion shifts across turns, such as transitions between happiness, anger, sadness, and surprise. We employ it to examine how well our optimized prompts generalize to open-domain, emotionally varied conversations. Its structured dialogues also allow for consistent evaluation of linguistic sensitivity, ensuring that improvements in emotion diagnosis stem from the prompt design rather than noise or lexical imbalance. EmoryNLP [40]. For the EmoryNLP collection we specifically employ the Emotion Detection subdataset, released in May 2017. This subdataset provides categorical emotion labels for utterances drawn from multiparty scripted dialogues, preserving speaker identity and conversational context that are essential for analyzing emotion flow in multi-participant interactions. In our experiments we use the official test split of this subdataset, which contains 1,328 utterances annotated across seven categorical emotion labels. Because the material is dialogic and speakeranchored, it is particularly useful for assessing whether our prompt optimization method can capture speaker-specific affective patterns and fine-grained contextual emotion cues in multiparty exchanges. PELD (Personality EmotionLines Dataset) [41]. The PELD dataset extends the standard emotion recognition setting by integrating personality factors with emotional annotations in dialogue. It comprises over six thousand dialogue triples, each annotated with both the speakers personality profile, based on the Big Five dimensions, and the expressed emotion of the response. By combining personality and emotion, PELD captures how individual differences shape linguistic manifestations of affect. This dataset allows us to evaluate the adaptability of our prompt optimization framework to personality-driven variation in emotion expression, providing more realistic and psychologically grounded testing environment for linguistic emotion diagnosis. RECCON [42]. The RECCON dataset, which aims at Recognizing Emotion Cause in CONversations, frames conIEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1743 TABLE 5 Prompts for all Agents in APOLO framework. The examples in the table are from the Emotion Diagnosis Task of the DepressionEmo dataset. Planner You are medical emotion diagnosis planning assistant. Your role is to generate an efficient and structured plan to solve the next problem related to medical emotion diagnosis (for example, identifying emotional states from patient text, integrating physiological data, or improving diagnostic accuracy). Your plan should be goal-oriented, concise, and executable, reflecting logical reasoning and domain awareness. Your response must strictly follow the format below: Total steps: [number] Step 1: [description] Step 2: [description] ... Step N: [description] For example: Total steps: 2 Step 1: Review the emotional indicators described in the problem. Step 2: Select the appropriate diagnostic model or analytical method. Please follow this format strictly. Teacher You are teacher who asks questions in the Socratic manner, focusing on medical emotion diagnosis tasks. Your goal is to help students think critically about identifying, interpreting, or improving emotional diagnosis in medical contexts. Please ask total of two questions: The first one is for the problem that appeared in the prompt given by the students in the last round. The second one is an optimization solution based on the current steps of the task. Please include only questions in your output and do not make answers for your students. Student You are prompt generator specialized in medical emotion diagnosis. Your task is to iterate over existing prompts and refine them so they better address tasks related to identifying, interpreting, and reasoning about emotional states in medical contexts (such as analyzing patient narratives, clinical notes, or multimodal emotional indicators). Focus on improving the prompts clarity, diagnostic relevance, and reasoning efficiency, ensuring that the resulting prompt effectively guides agents to generate insights or actions beneficial for medical emotion diagnosis. Note: Output only the newly generated prompt and nothing else. Critic You are an evaluator specialized in medical emotion diagnosis, responsible for judging the correctness and clinical relevance of given task or response related to identifying, analyzing, or reasoning about emotional states in medical contexts. Your output must strictly follow these rules: 1. If the task is judged as correct, output only: [True] 2. If the task is judged as incorrect, output: [False] [suggestion: reason for the incorrect judgment] Replace reason for the incorrect judgment with clear and concise explanation of why the task is incorrect. Do not include any additional text, comments, or explanations beyond the specified format. Target Analyze the given users post carefully and identify all relevant psychological emotion labels that accurately reflect the mental health state expressed in the text. Follow these steps: 1. **Read the post thoroughly**: Pay close attention to both the title and main content to understand the full context. 2. **Identify key emotional cues**: Look for words, phrases, or tones that indicate specific emotions (e.g., feel like waste of space suggests worthlessness). 3. **Match cues to emotion labels**: Compare the identified cues with the provided emotion labels to determine which ones fit best. 4. **Consider overlapping emotions**: Some posts may express multiple emotionsensure all applicable labels are selected. 5. **Justify your selections**: Briefly explain why each chosen label is appropriate based on the text. Example Output Format: **Emotions**: [worthlessness, hopelessness, sadness] **Reasoning**: The post expresses feelings of inadequacy (waste of space), lack of motivation (hard to find motivation), and persistent sadness (dislike everything about yourself). Now, analyze the following post and provide the most accurate emotion labels: [Insert Post Here] Options: [anger, brain dysfunction (forget), emptiness, hopelessness, loneliness, sadness, suicide intent, worthlessness] Conversation History Current Utterance to Analyze Your output MUST be single word representing the emotion. Do not provide any explanation or other text. versational emotion analysis as causality-aware task: beyond labeling the emotion of target utterance, it requires identification or validation of the antecedent contextual elements that plausibly triggered that emotion. This dual focus, emotion recognition plus cause identification, elevates the task from surface classification to context-sensitive interpretation, demanding reasoning over discourse structure and pragmatic cues. We include RECCON to probe whether prompt optimization can improve not only categorical emotion recognition but also the models ability to associate emotional states with their linguistic antecedents, thereby supporting more interpretable and discourse-aware linguistic emotion diagnosis. EmotionX (EmotionX-2019) [43]. For EmotionX we utilize the 2018 EmotionX subdataset, specifically the test partitions drawn from two complementary sources: the Friends transcripts and the EmotionPush chat logs. In our setup we evaluate on the Friends test portion comprising 2,764 sentences and the EmotionPush test portion comprising 2,807 sentences. Each utterance in these partitions is labeled with one of seven basic emotion categories (neutral, joy, sadness, fear, anger, surprise, disgust). Because the two subcorpora differ in register, scripted televisual dialogue versus spontaneous chat, the combined evaluation enables IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1744 an analysis of how prompt optimization affects robustness across distinct dialogue domains, stylistic registers, and emotion expression modalities. 6.2 Prompts for Agents The behavior of the APOLO framework is primarily governed by the system prompts assigned to its agents. These prompts define each agents role, constraints, and communication protocols, ensuring structured and effective optimization process. Table 5 summarizes the exact metaprompts used for the Planner and the Socratic module (Teacher, Student, and Critic), along with an example of final optimized prompt produced for the Target agent. The Planner prompt emphasizes structure and reliability, enforcing strict template that decomposes the optimization task into clear and actionable steps, thereby ensuring consistent and machine-parsable outputs that effectively guide the subsequent Socratic refinement phase. The Socratic module, which consists of the Teacher, Student, and Critic, is designed to facilitate goal-oriented yet exploratory dialogue. Within this module, the Teacher poses probing and constructive questions, the Student iteratively refines its responses based on the feedback, and the Critic evaluates the exchange by providing binary judgments with concise justifications to preserve rigor and focus. Together, these components form the core mechanism that drives APOLOs iterative optimization process. In contrast, the Target agents prompt represents the final output of the entire APOLO process rather than predefined instruction. The example shown for the DepressionEmo dataset illustrates the end result of APOLOs refinement: structured, multi-step instruction that integrates explicit role guidance (Analyze the given users post carefully), clear Chain-of-Thought reasoning sequence (Steps 15), and precise output formatting. This transformation from simple initial command to sophisticated, well-structured instruction captures the essence of APOLOs methodology and explains its superior empirical performance. 6.3 Case Study To provide concrete illustration of the APOLO frameworks inner workings, we present step-by-step case study of one complete optimization iteration. Figure 6 visualizes this entire collaborative process, from the initial prompt (P0) to the refined prompt generated at the end of the Socratic dialogue, using the challenging multi-label Emotion Diagnosis Task from the DepressionEmo dataset. The process begins with simple and somewhat ambiguous initial prompt, P0: Choose the single most appropriate label for the utterance from the options. Recognizing the complexity of the task, the Planner agent first decomposes the optimization goal into structured, six-step plan. This plan includes crucial sub-goals such as defining the output expectations (Step 2) and ensuring the prompt handles multiple emotions (Step 2), which immediately addresses the shortcomings of the initial prompt. The framework then proceeds to execute this plan stepby-step. For Step 1, Understand the task requirements, the Teacher poses question to the Student aimed at improving the clarity of the instructions. However, the Critic intervenes, judging the Teachers initial question as [False] because it lacks sufficient depth and fails to adhere to Socratic principles. This feedback loop is critical; it forces the Teacher to re-evaluate and ask more profound, openended question: How could the prompt more clearly emphasize the importance of analyzing both the title and main content...? In response to this improved Socratic inquiry, the Student generates significantly more sophisticated prompt. It now includes explicit role-playing (You are an expert in clinical psychology...) and detailed guidelines, such as integrating title and content analysis and focusing on contextual semantic understanding. This process repeats for subsequent steps. For Step 2, Define the output expectations, the Teachers question (approved by the Critic with [True]) prompts the Student to further refine the prompt to handle multiple coexisting emotions. This iterative, multi-agent dialogue continues until all of the Planners steps are completed. The final output, denoted as , is vastly improved prompt that incorporates Chain-of-Thought structure, role-playing, and precise output constraints. This walkthrough clearly demonstrates how the structured planning, Socratic refinement, and critical feedback mechanisms in APOLO collaborate to systematically transform naive initial prompt into highly effective, nuanced instruction."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we address two fundamental challenges in disease-related emotion recognition: emotional comorbidity and inefficient exploration. To tackle these issues, we propose APOLO, POMDP-based multi-agent framework that enables dynamic, interpretable, and risk-aware prompt optimization. By incorporating the collaborative roles of PlannerTeacherCriticStudentTarget, APOLO systematically explores the prompt space through adaptive reasoning and feedback alignment, achieving more comprehensive emotion coverage with enhanced stability and interpretability. The framework mitigates emotional comorbidity via adaptive semantic reasoning and improves exploration efficiency through uncertainty modeling and multi-agent collaboration. Extensive experiments on multilingual and risk-sensitive benchmarks demonstrate that APOLO consistently outperforms existing methods in diagnostic accuracy, calibration, and robustness. This work provides scalable and trustworthy paradigm for leveraging large language models in mental health assessment and clinical decision support."
        },
        {
            "title": "REFERENCES",
            "content": "[1] P. Kumar, A. Vedernikov, Y. Chen, W. Zheng, and X. Li, Computational analysis of stress, depression and engagement in mental health: survey, arXiv preprint arXiv:2403.08824, 2024. [2] Z. Ge, N. Hu, D. Li, Y. Wang, S. Qi, Y. Xu, H. Shi, and J. Zhang, survey of large language models in mental health disorder detection on social media, in 2025 IEEE 41st International Conference on Data Engineering Workshops (ICDEW). IEEE, 2025, pp. 164176. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1745 Fig. 6. complete example of the collaborative output from all agents in single iteration, using the Emotion Diagnosis Task of the DepressionEmo dataset. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1746 [3] X. Zhang, Y. Dong, Y. Wu, J. Huang, C. Jia, B. Fernando, M. Z. Shou, L. Zhang, and J. Liu, PhysReason: comprehensive benchmark towards physics-based reasoning, in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2025, pp. 16 59316 615. [Online]. Available: https: //aclanthology.org/2025.acl-long.811/ [4] A.-M. Bucur, M. Zampieri, T. Ranasinghe, and F. Crestani, survey on multilingual mental disorders detection from social media data, arXiv preprint arXiv:2505.15556, 2025. [5] T. Zhang, K. Yang, S. Ji, and S. Ananiadou, Emotion fusion for mental illness detection from social media: survey, Information Fusion, vol. 92, pp. 231246, 2023. [6] W. Fu, B. Wei, J. Hao, Y. Zhang, J. Zhang, J. Wang, B. Li, Y. He, L. Zhang, and J. Liu, Erreval: Error-aware evaluation for question generation through explicit diagnostics, arXiv preprint arXiv:2601.10406, 2026. [7] H. Yan, F. Xu, R. Xu, Y. Li, J. Zhang, H. Luo, X. Wu, L. A. Tuan, H. Zhao, Q. Lin et al., Mur: Momentum uncertainty guided reasoning for large language models, arXiv preprint arXiv:2507.14958, 2025. [8] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha, systematic survey of prompt engineering in large language models: Techniques and applications, arXiv preprint arXiv:2402.07927, 2024. [9] F. Xu, Q. Lin, J. Han, T. Zhao, J. Liu, and E. Cambria, Are large language models really good logical reasoners? comprehensive evaluation and beyond, IEEE Transactions on Knowledge and Data Engineering, 2025. [10] Q. Lin, Y. Zhu, X. Mei, L. Huang, J. Ma, K. He, Z. Peng, E. Cambria, and M. Feng, Has multimodal learning delivered universal intelligence in healthcare? comprehensive survey, Information Fusion, vol. 116, p. 102795, 2025. [11] W. Li, X. Wang, W. Li, and B. Jin, survey of automatic prompt engineering: An optimization perspective, arXiv preprint arXiv:2502.11560, 2025. [12] W. Cui, J. Zhang, Z. Li, H. Sun, D. Lopez, K. Das, B. A. Malin, and S. Kumar, Automatic prompt optimization via heuristic search: survey, arXiv preprint arXiv:2502.18746, 2025. [13] K. Ramnath, K. Zhou, S. Guan, S. S. Mishra, X. Qi, Z. Shen, S. Wang, S. Woo, S. Jeoung, Y. Wang et al., systematic survey of automatic prompt optimization techniques, arXiv preprint arXiv:2502.16923, 2025. [14] K. Chang, S. Xu, C. Wang, Y. Luo, X. Liu, T. Xiao, and J. Zhu, Efficient prompting methods for large language models: survey, arXiv preprint arXiv:2404.01077, 2024. [15] K. Hegde and H. Jayalath, Emotions in the loop: survey of affective computing for emotional support, arXiv preprint arXiv:2505.01542, 2025. [16] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, Large language models are human-level prompt engineers, in The Eleventh International Conference on Learning Representations (ICLR), 2023. [17] W. Xu, A. Banburski-Fahey, and N. Jojic, Reprompting: Automated chain-of-thought prompt inference through gibbs sampling, arXiv preprint arXiv:2305.09993, 2023. [18] X. Wang, C. Li, Z. Wang, F. Bai, H. Luo, J. Zhang, N. Jojic, E. P. Xing, and Z. Hu, Promptagent: Strategic planning with language models enables expert-level prompt optimization, arXiv preprint arXiv:2310.16427, 2023. [19] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen, Large language models as optimizers, in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [Online]. Available: https://openreview.net/forum?id=Bb4VGOWELI [20] Q. Ye, M. Axmed, R. Pryzant, and F. Khani, Prompt engineering prompt engineer, arXiv preprint arXiv:2311.05661, 2023. [21] J. Zhang, Z. Wang, H. Zhu, J. Liu, Q. Lin, and E. Cambria, Mars: multi-agent framework incorporating socratic guidance for automated prompt optimization, in Proceedings of the AAAI Conference on Artificial Intelligence, 2026. [22] N. Singh and U. C. Jaiswal, detailed sentiment analysis survey based on machine learning techniques, ADC-AIJ: Advances in Distributed Computing and Artificial Intelligence Journal, vol. 12, no. 2, pp. 201216, 2023. [23] A. Hassan and M. R. Islam, comprehensive survey on sentiment analysis techniques, International Journal of Computers and Applications, vol. 185, no. 12, pp. 111, 2023. [24] K. He, R. Mao, T. Gong, E. Cambria, and C. Li, Jcbie: joint continual learning neural network for biomedical information extraction, BMC bioinformatics, vol. 23, no. 1, p. 549, 2022. [25] X. Zhu, T. Liu, Y. Wu, and Z. Wang, Enhancing facial emotion recognition through deep learning: Integrating CNN and RNNLSTM models, in Proceedings of the 2nd International Conference on Machine Learning and Gerontechnology, 2024, pp. 16. [26] V. Bhat, R. Shah, and N. Mehendale, CNN-LSTM based deep neural networks for facial emotion detection in videos, International Journal of Creative Research Thoughts (IJCRT), vol. 9, no. 11, pp. d327d332, 2021. [27] P. Mishra and M. P, Speech emotion recognition using LSTM and RNN, Journal of Electrical Engineering, vol. 20, no. 3, pp. 16, 2020. [28] X. Lan, F. Wu, K. He, Q. Zhao, S. Hong, and M. Feng, Gem: Empowering mllm for grounded ecg understanding with time series and images, arXiv preprint arXiv:2503.06073, 2025. [29] F. A. Acheampong, H. Nunoo-Mensah, and W. Chen, Transformer models for text-based emotion detection: review of BERTbased approaches, Artificial Intelligence Review, vol. 54, no. 8, pp. 57895829, 2021. [30] H. Bao, K. He, X. Yin, X. Li, X. Bao, H. Zhang, J. Wu, and Z. Gao, Bert-based meta-learning approach with looking back for sentiment analysis of literary book reviews, in CCF International Conference on Natural Language Processing and Chinese Computing. Springer, 2021, pp. 235247. [31] K. Vishnubhotla and S. M. Mohammad, Language and mental health: Measures of emotion dynamics from text as linguistic biosocial markers, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2023, pp. 31173133. [32] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, AutoPrompt: Eliciting knowledge from language models with automatically generated prompts, in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2020, pp. 42224235. [33] X. L. Li and P. Liang, Prefix-Tuning: Optimizing continuous prompts for generation, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2021, pp. 45824597. [34] J. Zhang, S. Qi, Y. Dong, L. Yuan, T. Shen, W. Fu, B. Wei, H. Zhu, and J. Liu, Gkg-llm: unified framework for generalized knowledge graph construction, Information Fusion, p. 103956, 2025. [35] J. Zhang, Z. Wang, Z. Wang, Y. He, H. Luo, L. Zhang, R. Mao, Q. Lin, J. Liu et al., Maxs: Meta-adaptive exploration with llm agents, arXiv preprint arXiv:2601.09259, 2026. [36] Y. Zhang, Y. Yuan, and A. C.-C. Yao, Meta prompting for ai systems, 2025. [Online]. Available: https://arxiv.org/abs/2311. [37] M. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E. P. Xing, and Z. Hu, RLPrompt: Optimizing discrete text prompts with reinforcement learning, in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2022, pp. 75767593. [38] J. Zhang, Z. Wang, Z. Wang, X. Zhang, F. Xu, Q. Lin, R. Mao, E. Cambria, and J. Liu, Maps: multi-agent framework based on big seven personality and socratic guidance for multimodal scientific problem solving, arXiv preprint arXiv:2503.16905, 2025. [39] Y. Li, H. Su, X. Shen, W. Li, Z. Cao, and S. Niu, Dailydialog: manually labelled multi-turn dialogue dataset, arXiv preprint arXiv:1710.03957, 2017. [40] S. M. Zahiri and J. D. Choi, Emotion detection on tv show transcripts with sequence-based convolutional neural networks. in AAAI Workshops, vol. 18, 2018, pp. 4452. [41] Z. Wen, J. Cao, R. Yang, S. Liu, and J. Shen, Automatically select emotion for response via personality-affected emotion transition, in Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, 2021, pp. 50105020. [42] S. Poria, N. Majumder, D. Hazarika, D. Ghosal, R. Bhardwaj, S. Y. B. Jian, P. Hong, R. Ghosh, A. Roy, N. Chhaya et al., Recognizing emotion cause in conversations, Cognitive Computation, vol. 13, no. 5, pp. 13171332, 2021. IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 1747 [43] B. Shmueli and L.-W. Ku, Socialnlp emotionx 2019 challenge overview: Predicting emotions in spoken dialogues and chats, 2019. [Online]. Available: https://arxiv.org/abs/1909.07734 [44] A. B. S. Rahman, H.-T. Ta, L. Najjar, A. Azadmanesh, and A. S. onul, Depressionemo: novel dataset for multilabel classification of depression emotions, Journal of Affective Disorders, vol. 366, pp. 445458, 2024. [45] OpenAI. (2025) Introducing gpt-5. Accessed: Aug. 2025. [Online]. Available: https://openai.com/index/introducing-gpt-5/ [46] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. [47] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025. [48] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng, Automatic prompt optimization with gradient descent and beam search, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, (EMNLP), 2023, pp. 79577968. [Online]. Available: https://doi.org/10.18653/v1/2023.emnlp-main.494 [49] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin, and X. Hu, Harnessing the power of llms in practice: survey on chatgpt and beyond, ACM Transactions on Knowledge Discovery from Data, vol. 18, no. 6, pp. 132, 2024. [50] M. A. Al-Taei and S. M. Al-Taei, survey of sentiment analysis: Approaches, datasets, and future research, Applied Sciences, vol. 13, no. 12, p. 7091, 2023. [51] Z. Li, Y. Liu, Y. Su, and N. Collier, Prompt compression for large language models: survey, in Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025, pp. 71827195."
        }
    ],
    "affiliations": [
        "Saw Swee Hock School of Public Health, National University of Singapore, Singapore 119077",
        "School of Computer Science and Technology, Xi'an Jiaotong University, Shaanxi, China, 710049"
    ]
}