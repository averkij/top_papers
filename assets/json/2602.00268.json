{
    "paper_title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation",
    "authors": [
        "Ariel Shaulov",
        "Eitan Shaar",
        "Amit Edenzon",
        "Lior Wolf"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space."
        },
        {
            "title": "Start",
            "content": "TOKENTRIM: INFERENCE-TIME TOKEN PRUNING FOR AUTOREGRESSIVE LONG VIDEO GENERATION Ariel Shaulov School of Computer Science Tel Aviv University, Israel arielshaulov@mail.tau.ac.il Eitan Shaar Independent Researcher shaarei@biu.ac.il Amit Edenzon School of Mathematics Bar-Ilan University, Israel amit.edenzon@live.biu.ac.il Lior Wolf School of Computer Science Tel Aviv University, Israel wolf@cs.tau.ac.il"
        },
        {
            "title": "TokenTrim Project Page",
            "content": "6 2 0 2 0 3 ] . [ 1 8 6 2 0 0 . 2 0 6 2 : r Figure 1: Text-to-video results before and after applying TokenTrim on Rolling Forcing [18] and Self Forcing [19]."
        },
        {
            "title": "ABSTRACT",
            "content": "Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse PREPRINT - of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space."
        },
        {
            "title": "Introduction",
            "content": "Video diffusion models have rapidly advanced, enabling text-conditioned generation of high-quality videos with realistic appearance and expressive motion [37, 11, 9, 38, 12, 30, 17]. Despite this progress, generating long videos that remain temporally coherent is still major open challenge [32, 19, 18, 36]. The dominant recipe for long-horizon synthesis is chunk-wise autoregressive generation: the model produces short clip, then extends the sequence by generating the next clip while conditioning on latent representations of previously generated frames [32, 35, 19, 18]. While effective for extending duration, this autoregressive loop often exhibits temporal drift (error accumulation): small artifacts and inconsistencies introduced early can compound across chunks, leading to identity changes, structural degradation, and loss of global coherence [19, 18, 36]. Recent work has proposed to mitigate drift by strengthening long-range conditioning and stabilizing the autoregressive mechanism, e.g., through temporal key-value (KV) caching [19, 35], anchor/sink tokens [18, 36], and cache refresh or management strategies during training [36, 35]. However, long rollouts remain brittle at inference time [18, 32]. key reason is that the conditioning context itself degrades: once region of the latent state becomes corrupted, it is repeatedly reused in subsequent steps and can dominate attention, effectively propagating errors forward [18]. This suggests that temporal drift is not only modeling or data issue, but also an inference-time information propagation problem: the system lacks mechanism to assess which cached latent tokens are trustworthy. In this work, we introduce TokenTrim, an inference-time method for identifying unstable latent tokens before reuse and removing them from the conditioning context. TokenTrim operates entirely in latent space, requires no architectural changes or retraining, and adds only negligible overhead. At each autoregressive step, our method estimates per-token drift by comparing compact summary of the previous chunks latent state to the current chunk at its first denoising iteration. Tokens with high drift are hard-pruned from the cached context, preventing corrupted regions from influencing future generations. TokenTrim is compatible with autoregressive video diffusion frameworks that rely on self-attention with KV caching, including Self Forcing [19] and Rolling Forcing [18]. It is also compatible with inference-time regularization techniques such as FlowMo [20], which can be applied independently as an additional motion-consistency term. By pruning high-drift cached tokens and retaining only reliable context for reuse, TokenTrim suppresses error amplification across autoregressive steps and improves long-horizon temporal consistency. Our results show that controlling the conditioning context at inference time alone can substantially reduce temporal drift, highlighting an underexplored mechanism for long video generation [18, 32]. In summary, we propose TokenTrim, fully inference-time drift detection and hard-pruning mechanism that operates in latent space. We further show that combining motion-stabilized initialization with selective latent pruning significantly improves long-video coherence (see Fig. 1), without retraining or modifying the underlying model."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Auto-regressive Long Video Generation The transition from short-clip synthesis [9, 11, 12, 10, 30, 44, 45] to long-horizon video generation [17, 14, 31, 13, 16, 41, 7, 4, 46] has necessitated shift from holistic spatiotemporal modeling to causal, auto-regressive architectures that factorize generation into sequential chunks. Early approaches like NUWA-XL [7] employed hierarchical \"coarse-tofine\" strategy, generating global key frames before filling local gaps. However, as this precludes real-time streaming, contemporary state-of-the-art methods favor continuous auto-regressive modeling, although often struggling with the train-test discrepancy or exposure bias. Self Forcing [19] addresses this by training on self-generated roll-outs with stochastic gradient truncation, ensuring the model learns to recover from its own inference artifacts. To bypass the computational cost of bidirectional attention in streaming contexts, CausVid [6] distills bidirectional teacher into 2 PREPRINT - Figure 2: TokenTrim overview at autoregressive step t. (a) Given the candidate batch Xt and the previous batch Xt1, we encode each frame and form latent summaries Zt and Zt1 by averaging latents over the frames in each batch. We compute per-token drift di = Zt(i) Zt1(i)2 and select the top-pN largest drifts to form the unstable set St, from which we compute the drift severity Dt. (b) We compare Dt to the adaptive threshold µt + λσt. If Dt µt + λσt, the KV cache (K, ) is left unchanged and the batch is accepted. Otherwise, we mask the selected token positions in the temporal KV cache to obtain ( K, ) and regenerate the current batch conditioned on the pruned cache. Running statistics and the cache are updated using the accepted batch . block-causal student, enabling efficient frame-by-frame generation. Addressing the stability of infinite generation, Rolling Forcing [18] introduces \"relaxed causality\" via rolling-window joint denoising and an attention sink mechanism to anchor identity. Similarly, LongLive [36] incorporates KV re-cache mechanism to handle interactive prompt changes without breaking temporal continuity. These training-based methods often require substantial computational resources to realign the models internal distribution with the auto-regressive task. However, these training-based methods often suffer from substantial computational costs to realign the models distribution. More critically, they remain inherently vulnerable to snowballing errors. Even with extensive tuning, minor artifacts inevitably persist in the context and compound into severe temporal drift. In contrast, our approach overcomes this by actively detecting and pruning unstable latent tokens, offering the distinct additional benefit of suppressing error propagation at the source to sustain long video consistency without the need for expensive retraining. 2.2 Inference-time guidance Inference-time guidance offers training-free paradigm to enhance generation quality by modifying the sampling dynamics or context. To improve temporal consistency, FreeInit [3] and FreeLong [8] leverage spectral analysis, iteratively refining the initial noise distribution to align low-frequency components with the training manifold, thereby stabilizing global structure. For motion coherence, Shaulov et al. [20] introduces latent-optimization approach, calculating the patch-wise temporal variance across generated frames and applying gradient updates to minimize incoherent motion trajectories. Other methods rely on explicit conditioning anchors. For instance ConsistI2V [25] modifies spatiotemporal attention to attend to the high-frequency details of the first frame, preventing identity degradation. More recently, focus has shifted to managing the Key-Value cache during inference. While TeaCache [34] and TaoCache [5] prune tokens primarily for acceleration. This demonstrates that selective context management can serve as powerful form of guidance, direction our work advances by pruning based on latent instability. 2.3 Error-Accumulation In Long Video Generation pervasive failure mode in auto-regressive video generation is temporal drift [41], where minor errors in early frames accumulate linearly or exponentially, leading to \"error-accumulation\" and semantic collapse. Theoretical analysis attributes this to exposure bias, i.e., the distributional shift between the ground truth history seen during training and the imperfect self-generated history at inference [18, 19, 40]. Empirical studies typically quantify this drift via metrics PREPRINT - Figure 3: Qualitative results. Text-to-video results before and after applying TokenTrim on Rolling Forcing [18] and Self Forcing [19]. TokenTrim mitigates degradation over time, e.g., color shifts (c - background and girl, - background and bear), artifacts (b - light lens flare) and unnatural motion (a - Pikatchu). For additional qualitative results see App. and App. E. 4 PREPRINT - such as Fréchet Video Distance (FVD) [4, 1, 31] which measures The overall visual quality and temporal coherence of the generated videos or the Quality Drift introduced in Rolling Forcing [18]. To mitigate drift, architectures like StreamingT2V [42] employ specialized long-term memory modules (Appearance Preservation Module) to reinject features from an anchor frame. Bagger [43] proposes self-supervised training scheme that aggregates backward trajectories to correct drift. However, these methods often rely on rigid anchoring or extensive retraining. Recent findings in FreeLong [8] suggest that drift manifests non-uniformly across frequency bands, with high-frequency details degrading faster than low-frequency structure. However, rather than implicitly balancing frequency domains, we propose to explicitly intercept error accumulation at the token level."
        },
        {
            "title": "3 Preliminaries: Self-Attention in Autoregressive Text-to-Video Models",
            "content": "Modern text-to-video (T2V) generators [39, 19, 35, 36] often produce long videos by iteratively synthesizing the output in chunks (batches of frames), while conditioning each new chunk on the prompt and on representations of previously generated content [32, 35, 36]. Many such systems employ diffusion backbone for within-chunk generation, often implemented with diffusion transformers (DiTs) [29, 31, 30], together with recurrent/iterative mechanism across chunks to extend temporal horizon [32]. At the core of these models are latent spatiotemporal representations and self-attention. Each generated chunk is represented as set of latent tokens (e.g., patchified latent features over space and time). When generating subsequent chunks, the model conditions on text features as well as latent tokens from earlier chunks, enabling long-range temporal dependencies [31, 30]. Self-Attention and Temporal Key-Value Caching Self-attention [23] allows each token in the current chunk to aggregate information from other tokens in its context, spanning spatial (within-frame) and temporal (across-frame) dimensions. In video DiTs, this mechanism is repeatedly applied inside the denoising network across diffusion steps [29, 31]. To support long-horizon context efficiently, many chunk-wise T2V architectures use temporal key-value (KV) cache that stores key/value projections from previously generated chunks, so current queries can attend to both current tokens and cached tokens without recomputing past projections [35, 36]. Related inference-time caching methods for video diffusion further accelerate generation by selectively reusing computations across denoising steps [33, 34]. Formally, let be the queries for the current tokens, and let (Kcache, Vcache) denote cached keys/values from earlier chunks. The attention operation is with Attention(Q, K, V) = softmax (cid:19) (cid:18) QK V, = [Kcurr; Kcache], = [Vcurr; Vcache], (1) (2) where [; ] denotes concatenation along the token dimension and is the attention head dimension. Temporal Drift While temporal KV caching enables long-range dependencies, it can also amplify temporal drift: imperfections in earlier latent tokens may be repeatedly attended to and propagated as the cache grows. Over long rollouts, this can accumulate and manifest as identity changes, structural inconsistency, or degraded motion coherence [32, 36]. This mechanism motivates inference-time interventions that control how information is retrieved from the cache and how errors propagate in recurrent T2V generation."
        },
        {
            "title": "4 Method",
            "content": "We propose an inference-time framework for stabilizing long-horizon auto-regressive video diffusion via latent-domain token pruning. Our method is designed to operate on auto-regressive video diffusion models that condition future generation on previously generated frames via causal self-attention and temporal key-value (KV) cache, such as Self Forcing [19] and Rolling Forcing [18]. The core mechanism of the proposed method focuses on identifying and removing unstable latent conditioning tokens during auto-regressive inference. Algorithm 1 outlines single TokenTrim step. 4.1 Motion-Stabilized Initialization The quality of the first batch of generated frames is critical in auto-regressive video generation, as errors introduced at this stage propagate and amplify over time. To reduce early-stage corruption, we generate the first batch of frames 5 PREPRINT - Figure 4: Text-to-video results from FlowMo [20] and TokenTrim. For additional qualitative results see App. F. using FlowMo [20], which introduces motion-aware variance guidance at inference time. It encourages temporally coherent motion and reduces initial artifacts by guiding the denoising trajectory toward stable motion patterns. In our framework, FlowMo is applied only to the first batch of frames, producing stable latent anchor. All subsequent batches are generated using the base auto-regressive model. 4.2 Latent Summary Construction Let Xt1 = {x(f ) x(f ) t1 RHW 3 denotes single video frame. Each frame is encoded into set of spatial latent tokens =1 denote the previously generated batch of frames at auto-regressive step 1, where t1}F PREPRINT - (3) where E() denotes the encoder, is the number of spatial tokens, and is the latent dimension. We construct latent summary frame by averaging across the temporal dimension (frame index): Z(f ) t1 = E(x(f ) t1) RN D, Zt1 ="
        },
        {
            "title": "1\nF",
            "content": "F (cid:88) =1 Z(f ) t1 RN D. (4) After generating candidate current batch Xt, we treat it in the same manner as Xt1 and compute corresponding latent summary Zt RN D. Averaging across frames ensures that Zt1 and Zt share identical spatial token structure, enabling direct token-wise comparison. 4.3 Per-Token Latent Drift Estimation We estimate instability in the auto-regressive context by computing per-token drift score using latentspace subtraction. For each spatial token index {1, . . . , }, the drift score is defined as di = Zt(i) Zt1(i)2 . This measure captures deviations in semantic and structural features at the patch level and serves as an indicator of latent instability between consecutive autoregressive steps. (5) This comparison is performed between latent summaries that share the same tokenization grid and are computed at the same spatial resolution; moderate camera motion is therefore reflected as smooth, coherent changes across neighboring tokens rather than isolated high-magnitude drift, while corrupted or unstable regions induce localized spikes in di. 4.4 Drift Severity and Trigger Criterion Rather than pruning unconditionally, we first assess whether the current batch exhibits abnormal drift. Let (0, 1) denote the pruning fraction, and let be the number of spatial latent tokens. For each autoregressive step t, we rank the per-token drift values {di}N i=1 and define St {1, . . . , }, St = , di (cid:80) i=1, pN ) =1 E(x(f ) and updated KV cache. t1) RN ) RN Algorithm 1 Single TokenTrim Step Input: Previous batch frames Xt1, candidate current batch frames Xt, encoder E, temporal KV cache (Kcache, Vcache), running drift statistics (µt, σt), pruning fraction p, sensitivity λ, warm-up length Twarm. Output: Accepted current batch (cid:80)F 1: Zt1 1 (cid:80)F =1 E(x(f ) 2: Zt 1 3: di Zt(i) Zt1(i)2 {1, . . . , } 4: St TopIndices({di}N 5: Dt 1 iSt St 6: if Twarm then 7: 8: 9: 10: end if 11: if Dt µt + λσt then Xt 12: 13: 14: 15: else 16: 17: 18: 19: 20: 21: end if mt(i) [i / St] Kcache[mt], Vcache[mt] GENERATEBATCH( K, V) UpdateStatsAndCache(X , K, V) Return (X UpdateStatsAndCache(X Return (X UpdateStatsAndCache(X Return (X , Kcache, Vcache) , Kcache, Vcache) , Kcache, Vcache) , Kcache, Vcache) , K, V) Xt as the set of indices corresponding to the top spatial tokens with the largest drift scores. We define scalar drift severity score Dt as the mean drift over these tokens: Dt = 1 St (cid:88) iSt di, di R0. (6) To obtain an adaptive threshold, we maintain running statistics over previously accepted batches. batch Xτ is considered accepted if it is finalized (completed tokentrim step, see Alg. 1) and appended to the auto-regressive context. Let At = {τ < Xτ was accepted} denote the index set of such accepted auto-regressive steps prior to t, and let At be its cardinality. We define the running mean µt and standard deviation σt R0 of the drift severity as µt = 1 At (cid:88) τ At Dτ , σt = (cid:115) 1 At (cid:88) τ At (Dτ µt)2. (7) PREPRINT - Figure 5: Human preference study conducted on Rolling Forcing [18] (left) and Self Forcing [19] (right) using VideoJAM-bench [28]. TokenTrim is consistently preferred in terms of drift reduction, motion quality, and overall visual quality, while preserving textvideo alignment. Error bars indicate 95% confidence intervals computed via Dirichlet sampling with Laplace smoothing. We trigger pruning intervention when the current drift severity exceeds the adaptive threshold: where λ > 0 is sensitivity hyperparameter controlling the strictness of the trigger (we use λ = 2.0). During the first Twarm auto-regressive steps, the statistics µt and σt may be unreliable due to limited history. We therefore disable pruning during this warm-up phase and only accumulate drift statistics. Dt > µt + λσt, (8) 4.5 Hard Pruning and Regeneration When the drift criterion is exceeded, we regenerate the current batch with hard pruning applied to the auto-regressive context. Specifically, we remove from the temporal KV cache all token positions whose spatial indices belong to St. Let mt {0, 1}N denote the spatial pruning mask: mt(i) = (cid:26)0, 1, St, otherwise. We apply this mask to the cached keys and values (conceptually along the token dimension): = Kcache[mt], = Vcache[mt], (9) (10) where Kcache and Vcache denote the cached keys and values corresponding to previously generated frames. The pruned tokens ( K, V) are used to condition second generation attempt of the current batch. If the drift criterion is not exceeded, the generated batch is accepted without pruning, see Fig 2. Formally, the current batch Xt is accepted as the final output for step and appended to the auto-regressive context without any pruning or regeneration. For stability and efficiency, we limit the procedure to at most regeneration attempts per batch (we use = 1). That is, when pruning is triggered, the batch is regenerated at most once using the pruned KV cache; if the regenerated batch still violates the drift criterion, it is accepted as-is to avoid unbounded regeneration loops. 4. Integration with Self Forcing and Rolling Forcing In Self Forcing [19], which employs rolling KV cache during inference, latent drift estimation and hard pruning are applied before appending new KV entries to the cache. In Rolling Forcing [18], which separates global anchor cache (derived from initial frames) from recent temporal context cache, hard pruning is applied exclusively to the recent context tokens. Overall, our method addresses temporal drift by operating entirely in the latent space; it requires no additional training or supervision, and it introduces almost no computational overhead. 8 PREPRINT -"
        },
        {
            "title": "5 Experiments",
            "content": "We present comprehensive evaluation of TokenTrim in the setting of long-horizon auto-regressive text-to-video generation. Our experiments examine whether selectively pruning unstable latent tokens at inference time can mitigate error accumulation across generation steps, while preserving visual quality, motion realism, and semantic alignment. We evaluate TokenTrim both quantitatively, using VBench metrics, and qualitatively, via visual comparisons and human preference studies. All evaluations are conducted under the same experimental conditions and include comparisons against both baseline auto-regressive methods and inference-time methods. Implementation details. We evaluate TokenTrim on Rolling Forcing[18] and Self Forcing[19], two auto-regressive inference strategies built on the Wan2.1-1.3B text-to-video model [21]. TokenTrim operates purely at inference time and introduces no changes to the underlying model weights or denoising dynamics. TokenTrim monitors latent drift between consecutive generated batches and triggers hard pruning when abnormal drift is detected. Unless otherwise stated, we use pruning ratio of p=0.1, drift threshold parameter λ = 2.0, and warm-up period of Twarm = 2 batches. These parameters are specific to TokenTrim and are independent of the underlying generation model or inference strategy. When pruning is triggered, the current batch is regenerated once using the pruned temporal KV cache. In the FlowMo-adapted setup, FlowMo[20] is applied during the generation of all the batches using its default inferencetime settings. In contrast, when combined with TokenTrim, FlowMo is applied only to the first batch to produce stable initialization, as described in Sec. 4.1, and is disabled for subsequent batches. All experiments are conducted on single NVIDIA H100 GPU. Rolling Forcing and Self Forcing generate 30-second videos at 16 FPS and resolution of 832 480. 5.1 Quantitative Results We evaluate TokenTrim using the VBench benchmark [22], which provides comprehensive suite of motion, quality, and semantic metrics for text-to-video generation. We report both per-dimension scores and aggregated metrics, including Semantic Score, Quality Score, and the overall Final Score. Comparisons are made against the baseline auto-regressive methods, Rolling Forcing [18] and Self Forcing[19] as well as FlowMo [20], using identical prompts and generation settings. Automatic metrics. Tab. 1 reports aggregated VBench results comparing TokenTrim against the baseline auto-regressive methods (Rolling Forcing and Self Forcing) as well as the inference-time method FlowMo, under identical generation settings. We enclose the aggregated metrics, which constitute an average of all the benchmark dimensions, and measure the overall quality of the generations. full breakdown of all metrics is provided in App A. Table 1: VBench evaluation results. Comparison of baseline auto-regressive inference, FlowMo, and TokenTrim across aggregated VBench scores. Model Semantic Quality Final Rolling Forcing + FlowMo + TokenTrim 68.52% 81.72% 69.53% 82.09% 72.05% 87.30% 79.67% (+4.55%) 75.12% 75.81% When applied on top of Rolling Forcing, TokenTrim yields substantial improvements across all aggregated VBench metrics. The Final Score increases from 75.12% to 79.67% (+4.55%), supported by gains in Quality Score (+5.58%) and Semantic Score (+3.53%). These improvements indicate that selectively pruning unstable latent tokens significantly enhances both perceptual quality and semantic consistency over long auto-regressive rollouts. 68.98% 82.89% 68.25% 83.85% 73.89% 89.79% 81.84% (+5.91%) Self Forcing + FlowMo + TokenTrim 75.93% 76.05% similar trend is observed under Self Forcing. TokenTrim improves the Final Score from 75.93% to 81.84% (+5.91%), accompanied by strong gains in Quality Score (+6.90%) and Semantic Score (+4.91%). Notably, the magnitude of these improvements is even larger than under Rolling Forcing, highlighting TokenTrims effectiveness across different auto-regressive inference strategies. TokenTrim also consistently outperforms FlowMo at the aggregate level. Under Self Forcing, TokenTrim raises the Final Score to 81.84% (+5.91%), whereas FlowMo yields only marginal increase to 76.05% (+0.12%). This gap is further reflected in the Semantic Score, which improves substantially with TokenTrim (+4.91%) but slightly degrades with FlowMo (-0.73%), as well as in the Quality Score, where TokenTrim achieves +6.90% gain compared to FlowMos +0.96%. 9 PREPRINT - Overall, these aggregated results demonstrate that TokenTrim provides significantly stronger and more reliable improvement in semantic fidelity and visual quality than both baseline auto-regressive methods and inference-time motion guidance, leading to higher overall generation quality in long-horizon video synthesis. Inference-Time Overhead. Averaged over 128 generated 30s videos, TokenTrim increases wall-clock runtime by 1.08 relative to the Rolling Forcing baseline. In contrast, applying the FlowMo-adapted setup incurs substantially larger cost, resulting in 2.18 slowdown over Rolling Forcing. 5.2 Qualitative Results We qualitatively compare TokenTrim against the baseline auto-regressive methods and FlowMo using long-horizon generations exceeding one minute. Representative examples are shown in Fig. 1 and Fig. 3. Across all examples, TokenTrim maintains stable object identities, colors, and structure over time, whereas the baseline methods and FlowMo exhibit progressive degradation. Common failure modes include color shifts, structural distortions, background corruption, and identity drift. For example, in Fig. 3(a), the baseline Rolling Forcing model produces Pikachu character with missing or duplicated limbs over time. In Fig. 1(a,b), baseline generations show noticeable color drift and texture degradation, while TokenTrim preserves consistent appearance. Similarly, artifacts such as lens flare accumulation and background warping are visible in Fig. 3(bd) for the baseline and FlowMo, but are largely absent when using TokenTrim. Additional qualitative comparisons between TokenTrim and Rolling Forcing are provided in App. E, with corresponding results for Self Forcing reported in App. D. Fig. 4 directly compares TokenTrim with FlowMo under identical settings. While FlowMo often produces plausible short-term motion, it exhibits gradual structural drift over long horizons. For example, in Fig. 4(a), the balloon animal dog generated with FlowMo progressively deforms, most noticeably in the snout and head shape, whereas TokenTrim preserves consistent geometry and proportions. In Fig. 4(b), FlowMo introduces subtle distortions in the dragons wings and head during the rollout, while TokenTrim maintains coherent anatomy and smooth articulation. Similar effects are observed in Fig. 4(c,d), where FlowMo suffers from background warping and implied motion drift, whereas TokenTrim maintains stable silhouettes and coherent global motion. Additional qualitative comparisons between TokenTrim and FlowMo are provided in App. F. These examples illustrate that TokenTrim stabilizes longhorizon generation by preventing corrupted latent tokens from repeatedly influencing future steps, resulting in videos that remain visually coherent throughout extended rollouts. Figure 6: Human preference study on VideoJAM-Bench [28]. Win rates comparing TokenTrim against FlowMo [20] under Rolling Forcing. Error bars: 95% CIs via Dirichlet sampling with Laplace smoothing. 5.3 User Study We conduct human preference study using prompts from the VideoJAM benchmark [28]. For each prompt, participants are shown paired videos generated under identical settings, differing only in the inference-time method (baseline, FlowMo, or TokenTrim). Video order is randomized to avoid positional bias. Each pair is evaluated by five independent annotators, resulting in 640 responses per baseline. Participants evaluate videos along four criteria: textvideo alignment, aesthetic quality, motion coherence, and temporal drift (see App. B). Figure 5 reports human preference results comparing TokenTrim against the baseline under both Rolling Forcing (left) and Self Forcing (right). Under Rolling Forcing, TokenTrim achieves preference rate of 15.2% in textvideo alignment, compared to 9.9% for the baseline, indicating that pruning unstable tokens does not harm semantic consistency. For motion coherence, TokenTrim is preferred in 30.3% of cases, nearly three times higher than the baseline (10.2%), demonstrating clear improvement in temporally plausible motion. similar trend is observed for aesthetic quality, where TokenTrim reaches 40.3% preference versus 12.3% for the baseline. The largest improvement is observed in temporal drift, with TokenTrim attaining 41.7% preference compared to 15.2% for the baseline, highlighting its effectiveness in maintaining stable visual characteristics over long rollouts. consistent pattern emerges under Self Forcing. TokenTrim improves textvideo alignment to 15.5%, compared to 7.6% for the baseline. For motion coherence, TokenTrim achieves preference rate of 26.2%, substantially higher than the baseline (9.4%). In aesthetic quality, 10 PREPRINT - TokenTrim is preferred in 43.7% of comparisons, compared to 11.0% for the baseline. Finally, for temporal drift, TokenTrim reaches 43.3% preference, significantly outperforming the baseline (13.2%), confirming that the benefits of TokenTrim generalize across auto-regressive inference strategies. Fig. 6 reports direct human preference comparison between TokenTrim and FlowMo. Across all evaluated criteria, TokenTrim consistently receives more favorable votes than FlowMo. For textvideo alignment, TokenTrim is preferred in 14.9% of comparisons, compared to 10.6% for FlowMo, indicating that TokenTrim provides stronger semantic alignment between the generated videos and the input prompts. In terms of motion coherence, TokenTrim achieves higher preference rate of 27.8%, surpassing FlowMos 15.1%, demonstrating that TokenTrim exceeds FlowMos motion quality despite not explicitly optimizing motion dynamics. TokenTrim also shows substantial advantage in aesthetic quality, receiving 41.7% of preferences compared to 13.1% for FlowMo, reflecting consistently stronger visual appearance. The largest gap is observed in temporal drift, where TokenTrim attains preference rate of 49.7%, dramatically outperforming FlowMos 10.2%. This result highlights TokenTrims superior ability to preserve stable structure, appearance, and identity over long generation horizons. Overall, the user study confirms that TokenTrim produces videos that are perceived as more stable, coherent, and visually consistent over long horizons. Table 2: TokenTrim Ablation on Rolling Forcing. Ablation study of TokenTrim (TT) under different pruning ratios and initialization settings. The full TokenTrim method uses 10% pruning with FlowMo initialization. Columns correspond to the aggregated Semantic Score (Semantic), Quality Score (Quality), and the overall Final Score (Final) as defined by VBench. Model Semantic Quality Full TT Method 72.05% 87.30% Final 79.67% TT 5% Pruning TT 20% Pruning TT w/o FlowMo Init 70.78% 64.22% 71.17% 85.92% 78.35% (-1.32%) 72.29% 68.25% (-11.87%) 83.49% 77.33% (-2.34%) 5.4 Ablation Study We conduct an ablation study to analyze the contribution of each component in TokenTrim and to evaluate the effect of different pruning strategies. Tab. 2 reports aggregated VBench results for set of TokenTrim variants evaluated on top of Rolling Forcing. In particular, we analyze alternative pruning strategies, including variant that disables the FlowMobased motion-stabilized initialization, and examine sensitivity to the pruning ratio by testing fixed pruning rates of 5% and 20%, instead of the 10% we use throughout all other experiments. Pruning 5% of the most unstable tokens results in moderate degradation over the baseline TokenTrim method, achieving final score of 78.35% (1.32% compared to the baseline TokenTrim method), while aggressive pruning at 20% severely degrades performance, reducing the final score to 68.25% (11.87%). These results highlight that excessive removal of contextual tokens disrupts semantic continuity and visual quality. In another ablation, the FlowMo is removed from the initialization of the first batch (FlowMo is not used in the subsequent batches). This leads to final score of 77.33% which is 2.34% less than the full TokenTrim method, demonstrating that motion-stabilized initialization provides complementary benefit by improving early coherence. Interestingly, the degradation that occures in this ablation, is greater than the benefit of FlowMo to the baseline Rolling Forcing method (see Tab. 1), demonstrating that TokenTrim leads to better utilization of FlowMo than the baseline Rolling Forcing. full breakdown of all metrics is provided in App. C."
        },
        {
            "title": "6 Limitations & Future Work",
            "content": "TokenTrim operates purely at inference time and leaves model parameters unchanged; therefore, its gains are ultimately bounded by the capabilities and biases of the underlying video diffusion backbone and its training data. When the base model persistently struggles to represent an object, preserve identity, or produce stable motion, TokenTrim can primarily attenuate error propagation rather than fully correct the generation. Moreover, our current implementation uses fixed hard-pruning budget (e.g., pruning constant fraction or top-k tokens per step). Although this design is lightweight and adds negligible overhead, single global pruning setting may be suboptimal across prompts, content types, and rollout lengths. An important direction for future work is to make pruning adaptive, i.e., dynamically choosing the pruning rate and structure at each step using drift statistics or uncertainty estimates so that difficult sequences receive stronger suppression while stable sequences retain richer context, especially over long rollouts. 11 PREPRINT -"
        },
        {
            "title": "7 Conclusions",
            "content": "Can we sustain long-video generation by simply knowing what to forget? In this work, we address the pervasive issue of temporal drift not by adding more capacity or retraining, but by pruning tokens at inference time. Since the uncontrolled accumulation of corrupted tokens in the auto-regressive context is primary driver of semantic collapse, TokenTrim computes drift score for the models latent representation and distinguishes between stable context and hallucinated artifacts. We demonstrate that pruning these unstable tokens allows the model to maintain its connection to reliable anchors, effectively breaking the feedback loop of error accumulation. This approach transforms the KV-cache from passive history log into an active self-correcting memory mechanism. Our results suggest that future progress in infinite video synthesis lies not only in how much context model can remember, but also in its ability to selectively discard the noise that threatens to distort reality."
        },
        {
            "title": "8 Acknowledgments",
            "content": "This work was supported by grant from the Tel Aviv University Center for AI and Data Science (TAD)."
        },
        {
            "title": "References",
            "content": "[1] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. MagicVideo: Efficient Video Generation with Latent Diffusion Models. arXiv preprint arXiv:2211.11018, 2023. [2] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, and Bin Cui. VideoTetris: Towards Compositional Text-to-Video Generation. arXiv preprint arXiv:2406.04277, 2024. [3] Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, and Ziwei Liu. FreeInit: Bridging Initialization Gap in Video Diffusion Models. arXiv preprint arXiv:2312.07537, 2024. [4] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating Minute-level Long Videos with Autoregressive Language Models. arXiv preprint arXiv:2410.02757, 2025. [5] Zhentao Fan, Zongzuo Wang, and Weiwei Zhang. TaoCache: Structure-Maintained Video Generation Acceleration. arXiv preprint arXiv:2508.08978, 2025. [6] Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From Slow Bidirectional to Fast Autoregressive Video Diffusion Models. arXiv preprint arXiv:2412.07772, 2025. [7] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, and Nan Duan. NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation. arXiv preprint arXiv:2303.12346, 2023. [8] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention. arXiv preprint arXiv:2407.19918, 2024. [9] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-A-Video: Text-to-Video Generation without Text-Video Data. arXiv preprint arXiv:2209.14792, 2022. [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen Video: High Definition Video Generation with Diffusion Models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [12] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. arXiv preprint arXiv:2311.15127, 2023. [13] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, Tanghui Jia, Junwu Zhang, Zhenyu Tang, Yatian Pang, Bin She, Cen Yan, Zhiheng Hu, Xiaoyi Dong, Lin Chen, Zhang Pan, Xing Zhou, Shaoling Dong, and Yonghong Tian. Open-Sora Plan: Open-Source Large Video Generation Model. arXiv preprint arXiv:2412.00131, 2024. 12 PREPRINT - [14] Kling Team. Kling: Video Generation Model with Spatiotemporal Attention. 2024. Available at https: //klingai.com/. [15] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal Flow Matching for Efficient Video Generative Modeling. arXiv preprint arXiv:2410.05954, 2025. [16] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. LTX-Video: Realtime Video Latent Diffusion. arXiv preprint arXiv:2501.00103, 2024. [17] OpenAI. Video Generation Models as World Simulators. 2024. Available at https://openai.com/research/ video-generation-models-as-world-simulators. [18] Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, and Shijian Lu. Rolling Forcing: Autoregressive Long Video Diffusion in Real Time. arXiv preprint arXiv:2509.25161, 2025. [19] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion. arXiv preprint arXiv:2506.08009, 2025. [20] Ariel Shaulov, Itay Hazan, Lior Wolf, and Hila Chefer. FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation. arXiv preprint arXiv:2506.01144, 2025. [21] Wan Team, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, and others. Wan: Open and Advanced Large-Scale Video Generative Models. arXiv preprint arXiv:2503.20314, 2025. [22] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, and others. VBench: Comprehensive Benchmark Suite for Video Generative Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2180721818, 2024. [23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017. [24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-Scale Pretraining for Text-to-Video Generation via Transformers. arXiv preprint arXiv:2205.15868, 2022. [25] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2V: Enhancing Visual Consistency for Image-to-Video Generation. arXiv preprint arXiv:2402.04324, 2024. [26] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. ID-Animator: Zero-Shot Identity-Preserving Human Video Generation. arXiv preprint arXiv:2404.15275, 2024. [27] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming AudioDriven Portrait Avatar with Long-Term Motion Dependency. arXiv preprint arXiv:2409.02634, 2024. [28] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models. arXiv preprint arXiv:2502.02492, 2025. [29] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [30] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: Space-Time Diffusion Model for Video Generation. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. [31] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. CogVideoX: Text-to-Video Diffusion Models with an Expert Transformer. arXiv preprint arXiv:2408.06072, 2024. [32] Siyang Zhang and Ser-Nam Lim. Towards Chunk-Wise Generation for Long Videos. arXiv preprint arXiv:2411.18668, 2024. [33] Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael S. Ryoo, and Tian Xie. Adaptive Caching for Faster Video Generation with Diffusion Transformers. arXiv preprint arXiv:2411.02397, 2024. 13 PREPRINT - [34] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. TeaCache: Timestep Embedding Tells: Its Time to Cache for Video Diffusion Model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [35] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, and Long Chen. Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing. arXiv preprint arXiv:2411.16375, 2024. [36] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, and Yukang Chen. LongLive: Real-Time Interactive Long Video Generation. arXiv preprint arXiv:2509.22622, 2025. [37] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video Diffusion Models. arXiv preprint arXiv:2204.03458, 2022. [38] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. ModelScope Text-toVideo Technical Report. arXiv preprint arXiv:2308.06571, 2023. [39] Ruben Villegas, Jiahui Liu, Kimin Lee, Zhuowen Tu, Bo Dai, and James Hays. Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [40] Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei, Zhenheng Yang, Weilin Huang, and Dahua Lin. End-to-End Training for Autoregressive Video Diffusion via Self-Resampling. arXiv preprint arXiv:2512.15702, 2025. [41] Marco Pasini, Javier Nistal, Stefan Lattner, and George Fazekas. Continuous Autoregressive Models with Noise Augmentation Avoid Error Accumulation. arXiv preprint arXiv:2411.18447, 2024. [42] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text. arXiv preprint arXiv:2403.14773, 2025. [43] Ryan Po, Eric Ryan Chan, Changan Chen, and Gordon Wetzstein. BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models. arXiv preprint arXiv:2512.12080, 2025. [44] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. arXiv preprint VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models. arXiv:2401.09047, 2024. [45] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve Your Own Correlation: Noise Prior for Video Diffusion Models. arXiv preprint arXiv:2305.10474, 2024. [46] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei A. Efros, and Tero Karras. Generating Long Videos of Dynamic Scenes. arXiv preprint arXiv:2206.03429, 2022. 14 PREPRINT - Table 3: VBench evaluation across all dimensions. Comparison of Rolling Forcing and Self Forcing with TokenTrim and FlowMo. Dimension Rolling Forcing + TokenTrim + FlowMo Self Forcing + TokenTrim + FlowMo Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality Object Class Multiple Objects Human Action Color Spatial Relationship Scene Appearance Style Temporal Style Overall Consistency Semantic Score Quality Score Final Score 91.92% 92.87% 96.77% 97.21% 59.57% 64.19% 71.70% 88.97% 68.31% 75.81% 87.98% 79.26% 29.22% 21.72% 23.26% 24.92% 68.52% 81.72% 75.12% 90.12% 93.54% 98.89% 99.02% 62.11% 63.95% 71.91% 89.63% 65.29% 80.00% 85.53% 75.09% 32.20% 21.40% 25.49% 28.37% 72.05% 87.30% 79.67% (+4.55%) 91.94% 91.27% 95.21% 98.03% 56.07% 63.05% 68.62% 89.33% 67.28% 74.89% 86.20% 80.53% 30.85% 19.56% 25.12% 26.98% 69.53% 82.09% 75.81% 88.29% 89.53% 98.90% 97.63% 67.75% 61.06% 68.92% 81.24% 63.98% 83.72% 81.21% 74.76% 32.59% 23.49% 20.55% 25.12% 68.98% 82.89% 75.93% 85.54% 89.72% 98.93% 98.64% 68.97% 63.13% 69.17% 80.30% 60.37% 83.93% 81.28% 76.02% 30.94% 25.56% 20.39% 25.76% 73.89% 89.79% 81.84% (+5.91%) 87.99% 87.12% 98.91% 97.95% 65.46% 62.15% 66.99% 82.24% 61.03% 82.09% 79.83% 76.59% 29.25% 24.15% 20.02% 25.10% 68.25% 83.85% 76.05%"
        },
        {
            "title": "A VBench Metrics Breakdown",
            "content": "We evaluate the impact of TokenTrim on both Rolling Forcing [18] and Self Forcing [19] using the VBench benchmark, with full per-dimension results reported in Tab. 3. The VBench metrics [22] consistently demonstrate that TokenTrim yields substantial improvements across wide range of dimensions for both auto-regressive baselines. Notably, TokenTrim improves performance in nearly all motionand stability-related metrics, indicating effective mitigation of error accumulation across auto-regressive steps rather than localized, within-chunk refinement. For Rolling Forcing, TokenTrim provides clear gains in key motion coherence metrics, improving Temporal Flickering by +2.12% (from 96.77% to 98.89%), Motion Smoothness by +1.81%, and Dynamic Degree by +2.54%. These improvements indicate that TokenTrim enhances temporal stability while preserving motion complexity, rather than suppressing dynamics. TokenTrim further improves higher-level consistency metrics such as Human Action (+4.19%), Scene (+2.98%), and Overall Consistency (+3.45%), reflecting stronger long-horizon semantic coherence. similar pattern is observed for Self Forcing. TokenTrim improves Temporal Flickering (+0.03%), Motion Smoothness (+1.01%), and Dynamic Degree (+1.22%), while also yielding consistent gains in Human Action, Appearance Style (+2.07%), and Overall Consistency (+0.64%). Importantly, these improvements arise despite Self Forcing already employing stronger temporal anchoring, highlighting that TokenTrim addresses complementary failure modenamely, the repeated reuse of corrupted latent tokens across auto-regressive steps. In the aggregated metrics, TokenTrim delivers strong and consistent improvements for both baselines. For Rolling Forcing, TokenTrim increases the Semantic Score by +3.53% and the Quality Score by +5.58%, resulting in Final Score improvement of +4.55%. For Self Forcing, the gains are even larger, with +4.91% in Semantic Score, +6.90% in Quality Score, and Final Score increase of +5.91%. These consistent improvements across both architectures confirm that TokenTrim robustly mitigates long-horizon drift by controlling context reuse, leading to higher-quality and more stable video generation over extended rollouts. We also compare TokenTrim against FlowMo using the VBench benchmark, with detailed per-dimension results reported in Tab. 3. The VBench metrics [22] clearly demonstrate that TokenTrim provides stronger and more consistent improvements than FlowMo across nearly all dimensions, particularly those related to long-horizon stability. Across the full set of VBench dimensions, TokenTrim achieves the best score in the majority of categories, whereas FlowMo only attains the highest score in small subset and often trails both TokenTrim and the baseline. Focusing first on motion-related metrics, TokenTrim yields substantial gains in Temporal Flickering (+2.12% over baseline, from 96.77% to 98.89%) and Motion Smoothness (+1.81%), outperforming FlowMo in both dimensions (95.21% and 98.03%, respectively). Importantly, TokenTrim also improves the Dynamic Degree (+2.54%), while FlowMo significantly reduces it (-3.50%), indicating that FlowMos apparent motion coherence may partially arise 15 PREPRINT - Dimension Rolling Forcing + Full TT Method + TT 5% Pruning + TT 20% Pruning + TT w/o FlowMo init Table 4: VBench ablation across all dimensions. Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality Object Class Multiple Objects Human Action Color Spatial Relationship Scene Appearance Style Temporal Style Overall Consistency Semantic Score Quality Score Final Score 91.92% 92.87% 96.77% 97.21% 59.57% 64.19% 71.70% 88.97% 68.31% 75.81% 87.98% 79.26% 29.22% 21.72% 23.26% 24.92% 68.52% 81.72% 75.12% 90.12% 93.54% 98.89% 99.02% 62.11% 63.95% 71.91% 89.63% 65.29% 80.00% 85.53% 75.09% 32.20% 21.40% 25.49% 28.37% 90.59% 93.07% 97.01% 97.65% 61.48% 64.32% 72.81% 87.66% 64.13% 77.29% 87.51% 76.42% 30.63% 21.98% 22.78% 25.65% 85.31% 78.02% 89.83% 85.52% 55.97% 59.82% 62.68% 72.11% 62.83% 69.18% 64.32% 68.04% 29.75% 11.86% 15.29% 19.21% 90.10% 93.28% 97.25% 97.56% 60.11% 62.54% 70.62% 88.03% 64.42% 78.95% 86.06% 77.36% 28.74% 20.91% 25.41% 26.31% 72.05% 87.30% 79.67% (+4.55%) 70.78% 85.92% 78.35% (+3.23%) 59.22% 67.29% 63.25% (-11.87%) 71.17% 83.49% 77.33% (+2.21%) from suppressing motion complexity rather than preserving it. This contrast suggests that TokenTrim maintains realistic motion dynamics while reducing drift, rather than converging to overly static trajectories. TokenTrim further improves higher-level semantic and structural consistency. It achieves the strongest gains in Human Action (+4.19%), Scene (+2.98%), and Overall Consistency (+3.45%), consistently outperforming FlowMo, which provides smaller or inconsistent improvements in these categories. Notably, FlowMo only surpasses TokenTrim in Subject Consistency and Spatial Relationship, but these gains do not translate into improved aggregate performance. In the aggregated metrics, TokenTrim clearly dominates. It improves the Semantic Score by +3.53% and the Quality Score by +5.58%, resulting in Final Score of 79.67% (+4.55% over baseline). In contrast, FlowMo yields only marginal improvements, achieving Final Score of 75.81% (+0.69%). These results indicate that while FlowMo offers limited local benefits, it does not effectively mitigate the accumulation of errors across auto-regressive steps. TokenTrims explicit control over context reuse enables significantly better long-horizon coherence and overall video quality. In addition, closer inspection of FlowMos per-dimension performance reveals that its improvements are uneven and largely confined to small subset of metrics. Under Rolling Forcing, FlowMo provides modest gains in Motion Smoothness (+0.82%, from 97.21% to 98.03%) and Spatial Relationship (+1.27%), but these come at the expense of reduced motion diversity, as reflected by substantial drop in Dynamic Degree (-3.50%, from 59.57% to 56.07%). FlowMo also degrades several stability-sensitive metrics, including Temporal Flickering (-1.56%) and Overall Consistency (from 24.92% to 26.98%, still well below TokenTrims 28.37%). similar pattern is observed under Self Forcing. While FlowMo yields small improvement in Quality Score (+0.96%), it reduces the Semantic Score (-0.73%) and lowers the Dynamic Degree from 67.75% to 65.46%. Across both baselines, FlowMo attains the best score in only few isolated dimensions (e.g., Subject Consistency and Spatial Relationship), but these gains do not translate into consistent improvements in long-horizon stability or aggregated performance. User Study: Instructions Provided to Participants As part of the evaluations we performed on our method, we conducted user study, as described in 5.1. The study was designed to assess human preferences on videos generated with and without FlowMo, using the videoJAM benchmark [28], which focuses on motion coherence. The study was conducted using Google Forms. For each prompt, participants were shown pair of videosone with FlowMo and one withoutgenerated with the same random seed (1024). The order of the videos was randomized to avoid positional bias. Each pair was evaluated by five different participants, resulting in 640 responses per baseline. Participants were asked to evaluate the videos based on three criteria: text alignment, aesthetic quality, motion coherence, and (No) Drift. The instructions provided to the annotators are reproduced below, together with screenshot of the annotation interface (see Fig. 7). 16 PREPRINT - Annotator Instructions. Participants were first asked to carefully read the given text prompt and then watch two generated videos. After viewing both videos, they were instructed to answer the following questions: Text alignment: Which video better matches the given caption? Quality: From an aesthetic perspective, which video looks better overall? Motion: Which video exhibits more coherent and physically plausible motion? Do Note: It is OK if the quality is less impressive as long as the motion looks better (No) Drift: Which video better maintains consistent visual characteristics throughout the entire sequence, including stable colors, characters or objects, shapes and identities, and an unchanged environment or background, without noticeable visual corruption or changes over time? Figure 7: Screenshot of the Google Form used in the user study. Ablation Study - VBench Metrics Breakdown We present an ablation study in Tab. 4 to analyze the contribution of each component of TokenTrim and to examine its sensitivity to pruning strength and initialization quality. All variants are evaluated on Rolling Forcing to isolate the effect of inference-time context management. First, we observe that the full TokenTrim configuration consistently achieves the strongest performance across most motionand stability-related dimensions, confirming that both adaptive pruning and drift-based triggering are essential. Compared to the baseline, TokenTrim improves Temporal Flickering by +2.12%, Motion Smoothness by +1.81%, Dynamic Degree by +2.54%, and Overall Consistency by +3.45%, indicating effective mitigation of long-horizon error accumulation without suppressing motion. Next, we study the effect of pruning strength. conservative pruning rate of 5% already yields noticeable improvements over the baseline, increasing the Final Score by +3.23%. However, its gains are consistently smaller than those achieved by the full TokenTrim configuration, particularly in Human Action (+1.48% vs. +4.19%) and Overall Consistency (+0.73% vs. +3.45%). In contrast, aggressive pruning (20%) leads to severe degradation across nearly all dimensions, including large drops in Motion Smoothness (-11.69%), Dynamic Degree (-3.60%), and Quality Score (-14.43%), resulting in substantial Final Score decrease of -11.87%. This clearly demonstrates that excessive token removal harms semantic and visual fidelity, underscoring the necessity of adaptive, drift-aware pruning rather than fixed-rate pruning. Finally, removing FlowMo from the initialization stage (w/o FlowMo) degrades performance relative to the full TokenTrim pipeline, reducing the Final Score by 2.22%. While this variant still outperforms the baseline, it exhibits weaker gains in Dynamic Degree, Human Action, and Overall Consistency, highlighting the importance of stable initial latent anchor for preventing early corruption that later propagates through the auto-regressive context. Overall, this ablation study confirms that TokenTrims effectiveness arises from the combination of (i) drift-triggered pruning, (ii) moderate, adaptive pruning ratios, and (iii) stable initialization. Removing or weakening any of these components leads to measurable degradation, while overly aggressive pruning causes catastrophic loss of video quality. 17 PREPRINT - Figure 8: Additional Qualitative Results comparison between TokenTrim and Self Forcing [19]. Additional Qualitative Experiments: TokenTrim vs Self Forcing Fig. 8 compares TokenTrim with the Self Forcing baseline under identical long-horizon generation settings. Although Self Forcing improves short-term temporal anchoring, it still exhibits gradual degradation when errors accumulate across extended rollouts. In Fig.8(a), depicting translucent jellyfish drifting through deep blue water, Self Forcing introduces subtle shape distortions and loss of translucency over time, while TokenTrim preserves stable body structure and consistent appearance. In Fig.8(b), where cloud of white smoke drifts horizontally across dark background, Self 18 PREPRINT - Forcing suffers from spatial warping and uneven density, whereas TokenTrim maintains coherent global motion and uniform texture. Similar behavior is observed in Fig.8(c), where swarm of birds generated with Self Forcing gradually loses collective structure, while TokenTrim preserves synchronized motion and stable silhouettes. Finally, in Fig.8(d), robotic spider walking toward the camera exhibits accumulated structural drift under Self Forcing, whereas TokenTrim maintains consistent limb geometry and forward motion. Overall, these examples demonstrate that TokenTrim more effectively suppresses long-horizon error accumulation than Self Forcing, resulting in improved structural stability and temporal coherence across diverse motion patterns. Additional Qualitative Experiments: TokenTrim vs Rolling Forcing Fig. 9 compares long-horizon generations produced by Rolling Forcing with and without TokenTrim under identical settings. While Rolling Forcing generates plausible content in early frames, it exhibits progressive temporal degradation as the rollout advances. In Fig.3(a), depicting helicopter flying over forest, the baseline gradually introduces background distortion and structural inconsistencies in the helicopter, whereas TokenTrim preserves stable geometry and consistent background appearance throughout the sequence. In Fig.3(b), where humanoid emerges from smoke and reforms, Rolling Forcing suffers from identity drift and shape instability during the reformation process, while TokenTrim maintains coherent structure and consistent appearance across frames. Fig.3(c) shows glowing wireframe animal filling itself with flesh while running: the baseline exhibits color bleeding and deformation over time, whereas TokenTrim preserves smooth transitions and stable anatomy. Finally, in Fig.3(d), Rolling Forcing introduces cloth distortion and background warping during the walking motion, while TokenTrim maintains consistent cloth dynamics and stable scene layout. Overall, these examples demonstrate that TokenTrim effectively suppresses long-horizon drift by preventing corrupted latent tokens from propagating through the auto-regressive context, resulting in significantly improved temporal stability compared to Rolling Forcing alone. Additional Qualitative Experiments: TokenTrim vs FlowMo Fig.10 presents qualitative comparison between TokenTrim and FlowMo under identical long-horizon generation settings. While FlowMo often produces plausible motion in early frames, it exhibits progressive structural and semantic drift as the rollout proceeds. In Fig.10(a), showing humanoid constructed from stacked books walking forward, FlowMo gradually introduces misalignment and deformation in the book stack, leading to inconsistent body proportions, whereas TokenTrim preserves stable silhouette and coherent limb structure throughout the sequence. In Fig.10(b), depicting wooden mannequin walking under soft studio lighting, FlowMo suffers from subtle identity drift and joint instability over time, while TokenTrim maintains consistent articulation and appearance. Fig.10(c) shows dragon towering over burning battlefield: FlowMo introduces background warping and deformation in the wings and body across frames, whereas TokenTrim preserves coherent anatomy and stable environmental structure. Finally, in Fig. 10(d), FlowMo exhibits temporal inconsistency in the arrangement of background figures and lighting, while TokenTrim maintains stable composition and consistent scene layout. 19 PREPRINT - Figure 9: Additional Qualitative Results comparison between TokenTrim and Rolling Forcing [18]. 20 PREPRINT - Figure 10: Additional Qualitative Results comparison between TokenTrim and FlowMo [20]."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "School of Computer Science Tel Aviv University, Israel",
        "School of Mathematics Bar-Ilan University, Israel"
    ]
}