{
    "paper_title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
    "authors": [
        "Yunze Tong",
        "Mushui Liu",
        "Canyu Zhao",
        "Wanggui He",
        "Shiyi Zhang",
        "Hongwei Zhang",
        "Peng Zhang",
        "Jinlong Liu",
        "Ju Huang",
        "Jiamang Wang",
        "Hao Jiang",
        "Pipei Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO."
        },
        {
            "title": "Start",
            "content": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Yunze Tong 1 * Mushui Liu 1 2 * Canyu Zhao 1 Wanggui He 2 Shiyi Zhang 3 Hongwei Zhang 1 Peng Zhang 2 Jinlong Liu 2 Ju Huang 2 Jiamang Wang 2 Hao Jiang 2 Pipei Huang 2 6 2 0 2 6 ] . [ 1 2 2 4 6 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TPGRPO), GRPO framework that alleviates stepwise reward sparsity and explicitly models longterm effects within the denoising trajectory. TPGRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing dense, step-aware learning signal that better isolates each denoising actions pure effect, and (ii) it identifies turning pointssteps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trendand assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameterfree. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github. com/YunzeTong/TurningPoint-GRPO. 1. Introduction Flow Matching (FM) (Lipman et al., 2022; Liu et al., 2022) models can transport simple prior to complex target dis- *Equal contribution 1Zhejiang University, Hangzhou, China 2Alibaba Group, Hangzhou, China 3Tsinghua University, Beijing, China. Correspondence to: Hao Jiang <aoshu.jh@taobao.com>. Preprint. February 9, 2026. tribution via the learned velocity field, and have been widely adopted for text-to-image generation. Motivated by recent progress in large language models, researchers have applied Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025) to FM models. Representative methods such as Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025) evaluate reward on the final generated image and assign this same terminal reward to each preceding denoising step produced by Stochastic Differential Equation (SDE)-based sampler (Song et al., 2021). Advantages computed from these per-step rewards enable effective post-RL fine-tuning and can improve overall performance. However, this design does not accurately model step-level reward assignment. It leads to two issues: (1) The reward reflects the aggregate effect of the entire denoising trajectory and is identically assigned to every step, without distinguishing the contribution of different steps, which induces reward sparsity. (2) Existing methods mainly leverage trajectorylevel ranking across sampled trajectories, while neglecting within-trajectory interactions among steps, even though such dependencies are crucial for composing coherent sample. We illustrate the first issue by sampling several trajectories and visualizing their step-wise reward evolution in Figure 1. For latent at the intermediate step, we perform Ordinary Differential Equation (ODE) sampling for the remaining steps to obtain clean image on which the reward can be evaluated. This procedure is motivated by the fact that, compared to SDE sampling, ODE sampling removes stochasticity while preserving the same marginal distributions (Song et al., 2021); thus, the ODE completion can be interpreted as an average over possible SDE outcomes from the same intermediate state. With this estimator, Figure 1 shows that the reward can oscillate frequently during denoising. In contrast, Flow-GRPO assigns only the reward of clean images to all preceding steps, which captures the relative ordering of complete trajectories but may conflict with local progress. For example, from = 6 to = 5, the orange and green trajectories exhibit local reward decrease; yet because their full SDE-based samples achieve higher terminal rewards at = 0, they receive larger advantages for this step, incorrectly reinforcing an action that locally degrades reward. Overall, this outcome-based reward allocation cannot isolate Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO sampling at that step makes the subsequent reward evolution consistent with the overall trend. For these turning-point actions, we assign an aggregated long-term reward as feedback. This encourages directions that are likely to improve future reward trends and discourages those leading to overall reward drops. Crucially, turning points are identified solely by sign changes in incremental rewards, not their magnitudes, making our method efficient and hyperparameter-free. We summarize our contributions as follows: We identify reward sparsity and step-level misalignment caused by propagating an outcome-based reward to intermediate denoising steps. We address this by using step-wise reward differences to capture the incremental effect of each SDE update, yielding better estimate of single steps pure gain. Based on this fine-grained signal, we uncover turning pointssteps that flip the local reward trend to match the overall trajectory. We provide strict sign-based criterion to identify them and assign aggregated longterm rewards to model their delayed impact. To our knowledge, this is the first work to explicitly model such implicit interaction in Flow-based GRPO. Building on these insights, we propose TurningPointGRPO, which mitigates reward sparsity and improves delayed credit assignment. Extensive experiments further demonstrate the effectiveness of our method. 2. Related Work Common RL Techniques. Reinforcement learning (RL) has become central tool for aligning large language models (LLMs) with human preferences and downstream objectives (Guo et al., 2025; Jaech et al., 2024). standard pipeline first trains reward model from human preference data, and then optimizes the policy using proximal policy optimization (PPO) (Schulman et al., 2017). To better exploit multiple candidates per prompt, group-based ranking objectives have been proposed. In particular, GRPO (Shao et al., 2024) and its variants (Zheng et al., 2025; Yu et al., 2025) have been widely adopted for LLM post-training. RL for Diffusion and Flow Models. Motivated by these advances in LLM alignment, recent work has explored RL for diffusion and flow-matching models. Several methods apply Direct Preference Optimization (DPO) (Yang et al., 2024; Wallace et al., 2024) or PPO-style algorithms (Black et al., 2023; Fan et al., 2023; Miao et al., 2024) to modify the denoising trajectories. More recently, Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025) adapt GRPOstyle objectives for fine-tuning flow models. This line of work (He et al., 2025; Li et al., 2025; Wang et al., 2025a; Deng et al., 2026) introduces stochasticity via SDE-based Figure 1. Rewards of several sampled trajectories. Each dot at is obtained by (10 t) steps of SDE sampling followed by steps of ODE sampling. The leftmost point corresponds to full ODE sampling, and the rightmost to full SDE sampling (i.e., standard Flow-GRPO outputs). the pure gain of each denoising step. It therefore induces step-level reward sparsity and globallocal misalignment, which can limit the effectiveness of RL fine-tuning. The second issue stems from the sequential nature of FM generation. Each denoising action affects not only the immediate next latent, but also subsequent denoising behavior and thus the future trajectory. An intermediate state xt depends implicitly on earlier states xt+2, . . . , xT , since different upstream trajectories may lead to different xt+1 and thereby change the starting point of the current update. We refer to this delayed dependence as implicit interaction. This effect is also reflected in the reward dynamics: in Figure 1, the local reward trend can temporarily deviate from the overall trend from noise to the final image, and later be restored by critical step that flips the trend. We call such steps turning points, which are common in practice (e.g., = 9 on the blue trajectory, = 6 on the green trajectory, and = 7 on the purple trajectory). Their impact is not limited to the immediate step reward, but also shapes later reward evolution until the end of denoising. However, GRPO forms groups by aligning steps at the same t, which mainly supports crosstrajectory ranking but ignores within-trajectory dynamics induced by implicit interaction, leaving the rewards of such key turning points under-modeled. To address these issues, we propose TurningPoint-GRPO (TP-GRPO), framework that aligns the nature of FM and enhances Flow-based GRPO. TP-GRPO has two folds. First, to reduce the misalignment between local steps and the overall trajectory, we introduce an incremental-effect-based step-wise reward that replaces the sparse outcome-based reward. Concretely, our step-aware reward is defined as the difference between rewards obtained after and before single SDE sampling, thereby more faithfully reflecting the relative quality of individual denoising actions within each group. Second, to explicitly account for the delayed effects of key stepsan aspect not modeled in prior work-we further design an aggregation-based implicit interaction modeling mechanism. We formally define turning points as steps that flip the local reward trend, such that taking SDE 2 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO sampling during denoising to form candidate groups and encourage exploration, leading to improved performance."
        },
        {
            "title": "With ri",
            "content": "t(θ) = pθ(xi pθold (xi t1xi t1xi t,c) t,c) , the computation becomes: 3. Preliminary 3.1. Flow Matching Flow Matching (Lipman et al., 2022) learns timedependent model vθ(xt, t) that approximates the velocity field transporting prior p0 to the target distribution p1. Sampling is performed by integrating the deterministic ODE dxt = vθ(xt, t) dt. (1) Recent works (Liu et al., 2025; Xue et al., 2025) convert this ODE sampler into an equivalent SDE sampler. For rectified flow (Liu et al., 2022), the SDE-based update rule is xt+t = xt + (cid:20) vθ(xt, t) + + σt ϵ, (cid:0)xt + (1 t)vθ(xt, t)(cid:1) (cid:21) σ2 2t (cid:113) (2) 1t and ϵ (0, I). α is scalar hyperwhere σt = α parameter for noise level control. Previous works (Song et al., 2021; Karras et al., 2022) have also shown that an SDE sampler can induce the same marginal distributions at each noise level as its corresponding ODE sampler. The key difference is that stochasticity in the ODE sampler arises solely from the random initialization, whereas in the SDE sampler it comes from both the initial noise and the diffusion term along the trajectory. 3.2. Adopting SDE-sampler in FM for GRPO GRPO is an online RL method that leverages the contrast among self-sampled trajectories to guide policy exploration. key requirement for applying GRPO is to obtain diverse rollouts conditioned on the same context or input. To deploy it on flow matching, current methods (Liu et al., 2025) adopt the SDE sampling in Eq. 2 to inject stochasticity into the denoising process, thereby producing diverse trajectories and corresponding final images. Taking Flow-GRPO as an example, given prompt c, the flow model pθ samples group of individual images {xi i=1 and the corresponding reversetime trajectories {(xi 1, , xi i=1. Then, the advantage of the i-th image is computed by normalizing the group-level rewards: 0}G , xi 0)}G ˆAi = R(xi 0, c) mean({R(xi 0, c)}G std({R(xi 0, c)}G i=1) i=1) . (3) The policy model is then optimized by maximizing: JFlow-GRPO(θ) = cC,{xi}G i=1πθold (c)f (r, ˆA, θ, ε, β). (4) 3 ( r, ˆA, θ, ε, β) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i="
        },
        {
            "title": "1\nT",
            "content": "(cid:32) 1 (cid:88) t=0 (cid:16) min t(θ) ˆAi ri t, (cid:16) clip ri t(θ), 1 ε, 1 + ε (cid:17) ˆAi (cid:17) βDKL(πθπref) (cid:33) . (5) 4. Analysis on Limitations of Terminal"
        },
        {
            "title": "Reward Assignment",
            "content": "4.1. Reward Sparsity and Misaligned Per-Step Reward Allocation In this subsection, we identify the problem of reward sparsity, which leads to inaccurate stepwise credit assignment. For each trajectory in group, the reward is computed only once from the final clean image xi 0, and this single scalar is then assigned uniformly to all denoising steps along the trajectory. Consequently, once group is fixed, the relative ordering of advantages computed in Eq. 3 is constant at each timestep. This reward design naturally induces sparsity, since Flow-GRPO optimizes all timesteps under SDE sampling while receiving supervision only from the final outcome. The terminal reward reflects cumulative feedback over the entire trajectory and is per-timestep agnostic, with no direct dependency on the individual denoising actions at each step. Moreover, using the final reward and its normalized advantage as surrogate for all intermediate steps yields inaccurate contribution attribution. Prior works have shown that denoising steps at different timesteps contribute unequally to the final generation (Karras et al., 2024; Kynkaanniemi et al., 2024). Uniformly assigning the same reward to every timestep implicitly assumes identical contribution from each denoising update, ignoring the heterogeneity of denoising operations and consequently overestimating or underestimating the influence of certain timesteps in the overall generation process. 4.2. Turning-Point Effects and Insufficient Implicit Cross-Step Interaction Modeling In this subsection, we show that, due to the iterative denoising nature of FM, there exists an implicit interaction across steps. This interaction reflects the aggregated effect of subset of denoising steps and influences the final image through an indirect mechanism, which should be taken into account when assigning rewards to different timesteps in GRPO, but is not explicitly modeled in prior methods. In flow matching, the generative denoising process in discrete time can be viewed as Markov Decision Process-like procedure where the state is the latent representation and the action is denoising step. Since each updated latent serves Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO using only SDE, and R(x(T ) ) denotes the reward of the image obtained with only ODE. st measures whether the local sampling action at timestep is aligned with the overall reward trend of the SDE-based trajectory. Based on this, we define the concept of turning point as follows. (a) Normal point that does not satisfy the local flip condition. (b) Normal point that does not align with the overall trend. (c) Good turning point that changes decreasing trend to an increasing one. (d) Bad turning point that changes an increasing trend to decreasing one. Figure 2. Some cases that are or are not identified as turning points. The first row shows cases that do not satisfy our turning-point definition and are optimized with rt. The second row shows cases that do satisfy it and are optimized with ragg . as the input to the next step, the effect of early denoising actions propagates and accumulates over time, thereby influencing subsequent updates and the final generated image. We use Figure 1 to illustrate the implicit interaction. In Figure 1, each line represents 10-step SDE-sampling-based denoising trajectory, and the dots at = 0 indicate the rewards of the final clean images. To obtain rewards for all intermediate states, for each intermediate latent in trajectory, we complete the remaining steps using ODE sampling to obtain corresponding image. The reward of this image reflects the cumulative gain of all preceding SDE steps. Following Section 3.1, ODE sampling is deterministic and preserves the same marginal distribution as SDE sampling. For latent xt, we use xODE(k) to denote the latent obtained by running ODE sampling steps after the existing (T t) SDE sampling steps. When = t, xODE(k) becomes fully denoised clean image. Hence, xODE(1) can be viewed as the statistical average of all possible SDE-sampled {xt1}, except that xt1 is obtained via SDE sampling from to 1, whereas xODE(1) uses ODE sampling for that step. t In Figure 1, we observe that, for most trajectories, the rewards of intermediate latents oscillate and are nonFor clarity, we use R(x(t) ) to denote monotonic. R(xODE(t) , c). To explain the implicit interaction, we first define the consistency between single SDE sampling step at and the overall trajectory as st = sign(cid:0)R(x(t1) t1 )R(x(t) )(cid:1)sign(cid:0)R(x(0) )(cid:1), (6) 0 ) denotes the reward of the image sampled 0 )R(x(T ) where R(x(0)"
        },
        {
            "title": "For",
            "content": "4.1. SDE-based Definition trajectory an {xT , xT 1, . . . , x1, x0} with the corresponding intermediate images {xODE(T ) }, timestep is turning point if and only if st+1 < 0, st > 0, (cid:0)sign(cid:0)R(x(t1) )(cid:1)(cid:1) > 0 and 1 1. )(cid:1) sign(cid:0)R(x(0) t1 ) R(x(t) , . . . , xODE(1) 0 ) R(x(t) , xODE(T 1) 1 , xODE(0) 0 Intuitively, turning point is the step at which the local reward trend flips so as to become consistent with the overall trend. Figures 2a and 2b show examples that are not treated as turning points, as they violate the conditions of local trend reversal and alignment with the global trend, respectively. Taking Figure 2c as an example, the overall trend of this trajectory improves as more SDE steps are applied. However, the sampling from xt+1 to xt decrease the reward. The actual change in trend is induced by the sampling at t: the step just before it causes degradation, while this step is the one that reverses the trend and makes it consistent with the overall trajectory. Afterward, most subsequent steps improve the reward rather than harming it. In this sense, corresponds to an implicit interaction that affects subsequent sampling and contributes to the final reward. The benefit of this step is not only immediate but also propagates forward and accumulates through later steps influenced by it. In GRPO, such twisting actions that flip the trend deserve more positive (or negative) rewards to reflect their implicit long-term impact, thereby receiving stronger preference (or rejection) during the group normalization in Eq. 3. However, to the best of our knowledge, existing methods do not explicitly model this type of implicit interaction. This motivates us to explicitly integrate interaction-aware information into the reward assignment to achieve better performance. 5. Methodology 5.1. Increment-Based Step-wise Reward Computation Section 4.1 has introduced the reward sparsity problem in standard GRPO-based FM. The core idea of this family of approaches is to aggregate the contribution of all sampling steps and define the reward solely in terms of the resulting global effect. However, this design is fundamentally misaligned with the need for step-wise feedback, and thus inherently induces reward sparsity. The relative ordering of advantages based on the final image reward is deterministic and may be inconsistent with the true ordering of certain local steps. Motivated by recent works (He et al., 2025; Deng et al., 2026), we instead consider replacing this global 4 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Figure 3. Overview of our method. For each trajectory, we compute stepwise rewards as the pure incremental effect of the current SDE sampling. We then identify the orange turning point that satisfies Definition 4.1 or Remark 5.2. Next, we assign cumulative rewards to capture their implicit impact on reversing the reward trend. Finally, we apply group normalization independently at each timestep. effect with the incremental change at each step and using this increment as the step-wise reward. Building on the properties in Section 4.2, we represent rt, the reward of the sampling action from to 1, as the difference in gain before and after this step. Specifically, we cache the intermediate latents xt and xt1, then apply ODE sampling for and 1 steps respectively to obtain xODE(t) and xODE(t1) . Both latents undergo full -step process, t1 so the reward model can directly evaluate them, and they differ only in the sampling operation at timestep t. We then compute the effective increment rt = R(cid:0)xODE(t1) (cid:1) R(cid:0)xODE(t) (cid:1). (7) (cid:1) and R(cid:0)xODE(t) (cid:1) share the same Note that R(cid:0)xODE(t1) (T t) SDE sampling; they begin to differ at the t-th step. As discussed in Section 3.1, compared to SDE sampling, ODE sampling preserves the same marginal distribution and only removes stochasticity. Thus, the ODE-sampled outcome can be viewed as statistical average and serves as good proxy for the baseline gain of appending the sampling at this step. By replacing the rewards in Eq. 3 with rt, we can obtain more accurate step-wise rewards for intermediate sampling steps. 5.2. Aggregation-Based Implicit Interaction Modeling In Section 4.2, we identify the problem of reward oscillation when deploying SDE sampling at different timesteps. We also show that turning points exert an implicit influence on subsequent sampling, which is not modeled by existing methods. We address this gap with the following design. Specifically, for timesteps that satisfy the definition of turning points, we replace the local stepwise reward with an aggregated multi-step reward, making the final advantage 5 computation implicit-interaction-aware. For an SDE-based trajectory {xT , xT 1, . . . , x1, x0} with the corresponding images {xODE(T ) , x0}, the aggregated reward is defined as , . . . , xODE(1) 1 , xODE(T 1) 1 = R(cid:0)x0 ragg (cid:1) R(cid:0)xODE(t) (cid:1), (8) (cid:1) denotes the reward of the final image sampled where R(cid:0)x0 with all SDE. Compared to rt, ragg captures the cumulative effect from the turning point to the end of denoising. By replacing rt with ragg at turning points, we also encode the implicit reward induced by their downstream impact. We provide two examples in Figure 3 to illustrate the benefit of using ragg at turning points. (1) For step in trajectory j, the reward trend switches from decreasing to increasing, yet the local gain is very small, failing to reflect the importance of the denoising action at this step. In contrast, ragg takes larger value and more accurately reflects its contribution. (2) While for step in the trajectory i, the local reward exhibits large drop, while the final reward remains close to the full ODE-based result. In this case, computing the advantage with rt would overemphasize the effect of this step, whereas using ragg yields more moderate contribution. As explained above, for certain turning points identified in Definition 4.1, ragg may have smaller absolute value than rt. Building on this, we design an alternative strategy that retains only those turning points satisfying ragg > rt. This leads to stricter notion of turning points. For 5.1. SDE-based Definition trajectory an {xT , xT 1, . . . , x1, x0} with the corresponding intermediate images {xODE(T ) , xODE(T 1) }, 1 timestep is consistent turning point if and only if st+1 < 0, st > 0, (cid:0)sign(cid:0)R(x(t1) )(cid:1) sign(cid:0)R(x(0) t1 ) R(x(t) t1 )(cid:1)(cid:1) > 0, and 1 1. 0 ) R(x(t1) , . . . , xODE(1) 1 , xODE(0) 0 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO The steps selected by Definition 5.1 form subset of those selected by Definition 4.1. We provide some lemmas and proofs in Appendix C. The benefit of Definition 5.1 is that it filters out purer implicit interactions, where the direction of the implicit interaction is aligned with the local update. Since both rt and ragg are defined as differences of rewards, i.e., as incremental quantities, they can be substituted for each other without introducing scale mismatch in the reward values. Moreover, the selected turning points may either improve or degrade the reward trend. Thus, we explicitly incorporate both preference-inducing and rejection-inducing operations when computing group advantages, thereby providing more reliable optimization signals for the RL process. To sum up, the motivation for using ragg as the reward at turning points is to model and incorporate the implicit longterm impact of key SDE-based denoising actions. With this design, we expect the model to perform sampling while accounting for its potential future impact: it learns to prefer actions whose downstream trajectory is likely to enhance the final reward, and to avoid actions whose later effects would ultimately degrade it. 5.3. Implicit Long-Term Effect Modeling for the Initial Sampling Step In this subsection, we propose practical scheme for selecting the initial sampling step to model its implicit long-term influence. This design is motivated by the observation that, under our definition of turning points, the first denoising step is naturally excluded from aggregation-based effect propagation. The proposed constraint closes this gap and yields more comprehensive treatment of long-term effects. Recall that our turning-point criterion relies on the auxiliary judgment based on the last denoising action before the current state (st+1 < 0). Directly applying it omits the first denoising step (t 1). Consequently, all initial sampling steps at = in each trajectory receive only the local reward rt and cannot benefit from the aggregated reward ragg . However, some early steps can strongly influence the overall trajectory and their implicit effects, if modeled, can further benefit the RL process. To address this, we add constraint that extends long-term effect detection to the first step. Concretely, we identify the initial step as eligible for using ragg Remark 5.2. The sampling action at the first denoising step )(cid:1) is selected if and only if (cid:0)sign(cid:0)R(x(T 1) sign(cid:0)R(x(0) 1 ) R(x(T ) as follows. 0 ) R(x(T ) )(cid:1)(cid:1) > 0. This condition selects early steps whose local reward change aligns with the overall trend, thereby capturing the impact of early decisions. In this case, initial action that strongly steers the trajectory can be assigned more representative cumulative reward, better leveraging these indicative points. 6 6. Experiments 6.1. Experimental Setting Our experimental setup follows Flow-GRPO. We train models on three tasks: (1) Compositional Image Generation with Geneval (Ghosh et al., 2023) rewards; (2) Human Preference Alignment with PickScore (Kirstain et al., 2023) rewards; and (3) Visual Text Rendering with rule-based OCR accuracy reward (Chen et al., 2023; Gong et al., 2025). We adopt SD3.5-M (Esser et al., 2024) as the base model and apply LoRA (Hu et al., 2022) for efficient fine-tuning. Key hyperparameters are aligned with Flow-GRPO: training with sampling timesteps = 10, inference with = 40, group size = 24, and image resolution as 512. All results reported in our tables and figures are obtained from our own re-implementation under controlled and consistent configuration on our hardware, rather than directly copied from the original Flow-GRPO paper, ensuring fair comparison. More experimental details are provided in Appendix B. 6.2. Main Results We present the quantitative comparison results in Table 1. The two TP-GRPO variants differ in whether they use Definition 4.1 or Definition 5.1 for turning-point selection. With the step-level reward computation in Section 5.1 and turningpoint detection in Section 5.2, TP-GRPO consistently outperforms Flow-GRPO across all three tasks (columns 2 4), while largely preserving generalization performance (columns 59). The qualitative comparison is provided in Figure 5. We observe that both variants of our proposed TP-GRPO improve generation quality by producing more accurate counts, better text rendering, and enhanced aesthetics and content alignment. We also find no indication of reward hacking in the generated images. For more qualitative results, please refer to Appendix F. In addition to the best-evaluation results reported in the table, we present the training curves in Figure 4. These curves are obtained from experiments where we remove the KL penalty in Eq. 4 to assess the exploratory capability of our method. In this unconstrained setting, TP-GRPO again achieves superior performance on all three tasks. The gain is especially notable on PickScore, where the unbounded, nonrule-based reward allows our method to better exploit the optimization signal. Our checkpoint at step 700 attains reward comparable to Flow-GRPO at step 2300, demonstrating faster convergence and higher performance. 6.3. Further Analysis We further analyze the key components that affect our methods optimization behavior in GRPO. In this section, we remove the KL penalty term in Eq. 4 during training. This setup isolates how the examined factors shape the pure Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Table 1. Results on Compositional Image Generation, Visual Text Rendering, and Human Preference benchmarks, evaluated by task scores on test prompts, and by image quality and preference scores on DrawBench prompts. All results are from our own re-implementation under consistent setup, not directly taken from the Flow-GRPO paper. ImgRwd: ImageReward; UniRwd: UnifiedReward Model SD3.5-M Task Metric Image Quality Preference Score GenEval OCR Acc. PickScore Aesthetic DeQA ImgRwd PickScore UniRwd 0.6029 0.4548 21.44 5.380 3. 0.6039 21.95 3.200 Flow-GRPO TP-GRPO (w/o constraint) TP-GRPO (w constraint) 0.9673 0.9714 0.9725 5.223 5.352 5.246 Compositional Image Generation Flow-GRPO TP-GRPO (w/o constraint) TP-GRPO (w constraint) Flow-GRPO TP-GRPO (w/o constraint) TP-GRPO (w constraint) Visual Text Rendering 0.9579 0.9718 0.9651 5.091 5.092 5.111 Human Preference Alignment 24.02 24.73 24.67 6.231 6.293 6.321 3.861 3.963 3.977 3.459 3.251 3.480 3.966 3.961 3. 0.8792 0.9267 0.8710 0.6906 0.6433 0.7378 1.3875 1.3714 1.4419 22.20 22.28 22.20 21.91 21.93 22.06 24.10 24.46 24. 3.459 3.491 3.488 3.088 3.160 3.260 3.605 3.600 3.640 (a) Compositional Image Generation (b) Visual Text Rendering (c) Human Preference Alignment Figure 4. Training curves on three evaluation tasks. The two TP-GRPO variants differ in whether they apply the consistency constraint in Definition 5.1. policy optimization dynamics with respect to the reward signal, without confounding effects from regularization. Window Size in SDE sampling. Because we assign rewards at each timestep, the number of steps on which we apply SDE sampling (i.e., the sampling-window size during training) directly affects how the model updates. Figure 6 shows the performance over 2400 training steps as we vary this window. The red curve corresponds to the default configuration, which applies SDE sampling to all steps1. Reducing the window size lowers the cost of intermediate sampling and thus shortens training time for fixed number of epochs. Interestingly, moderately shrinking the window (e.g., to size of 8) also improves performance. This is consistent with the fact that the final image is largely determined by earlier denoising steps, while the last 12 steps have negligible influence and rarely contain turning points. However, when the window is reduced too aggressively (e.g., to 4 steps), performance drops sharply. We attribute this degradation to skipping optimization on later steps and consequently 1Flow-GRPO default does not apply SDE at the final step. failing to capture turning points that occur in those steps. Noise-Scale Choice in the SDE Sampler. The coefficient α in Eq. 2 scales the stochastic term in the SDE sampler and thereby controls the variability of the denoising trajectory. Larger α produces more diverse intermediate states. FlowGRPO uses α = 0.7 as its default, and we take this value as the reference in Figure 7, where we report our results under different α. Small deviations around 0.7 do not substantially change the learning trend. When α is too small (e.g., 0.4), the stochasticity becomes insufficient: the curve exhibits oscillation around 1750 steps and stays mostly below that of α = 0.7. When α is too large (e.g., 1.0), the intermediate latents become overly diverse, the optimization direction becomes unstable, and performance clearly degrades. These results suggest that GRPO requires balanced level of stochasticity: both too little and too much noise harm learning. Across all tested values, however, our method consistently outperforms the Flow-GRPO baseline, indicating robustness to this hyperparameter. 7 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Figure 5. Qualitative comparison across three tasks. Compositional Image Generation, Visual Text Rendering, and Human Preference Alignment, respectively, assess color/counting, text rendering, and content alignment (including aesthetics). Figure 6. Comparison across different numbers of SDE-sampling steps. Performance is reported over 2400 training steps with corresponding training time. Figure 7. Comparison across different noise level α. Larger α indicates more stochastic SDE sampling. Within suitable range, our method consistently surpasses standard Flow-GRPO. 7. Conclusion In this paper, we identified two limitations of existing Flowbased GRPO methods. First, terminal rewards are uniformly propagated to all denoising steps. This causes step-wise reward sparsity and local misalignment. Second, group-wise ranking compares trajectories only at matched timesteps. It ignores within-trajectory dependencies and delayed effects. To address these issues, we proposed TurningPoint-GRPO (TP-GRPO). TP-GRPO uses step-level incremental rewards, which accurately models each denoising actions local effect. Our method also identifies turning points, which are steps that flip the local reward trend. We assign these turningpoint actions an aggregated long-term reward to capture their delayed impact. Turning points are detected via sign changes of incremental rewards. This keeps the method efficient and hyperparameter-free. Extensive experiments show that TP-GRPO could effectively improve generations. 8 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Black, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Chen, J., Huang, Y., Lv, T., Cui, L., Chen, Q., and Wei, F. Textdiffuser: Diffusion models as text painters. In NeurIPS, pp. 93539387, 2023. Cui, C., Sun, T., Lin, M., Gao, T., Zhang, Y., Liu, J., Wang, X., Zhang, Z., Zhou, C., Liu, H., et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. Deng, H., Yan, K., Mao, C., Wang, X., Liu, Y., Gao, C., and Sang, N. Densegrpo: From sparse to dense reward for flow matching model alignment. arXiv preprint arXiv:2601.20218, 2026. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Fan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P., Ghavamzadeh, M., Lee, K., and Lee, K. Reinforcement learning for fine-tuning text-to-image diffusion models. In NeurIPS, 2023. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, pp. 5213252152, 2023. Gong, L., Hou, X., Li, F., Li, L., Lian, X., Liu, F., Liu, L., Liu, W., Lu, W., Shi, Y., et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, X., Fu, S., Zhao, Y., Li, W., Yang, J., Yin, D., Rao, F., and Zhang, B. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In NeurIPS, pp. 2656526577, 2022. Karras, T., Aittala, M., Kynkaanniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself, 2024. URL https://arxiv.org/ abs/2406.02507. Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., and Levy, O. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NeurIPS, pp. 3665236663, 2023. Kynkaanniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T., and Lehtinen, J. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In NeurIPS, pp. 122458122483, 2024. Labs, B. F. Flux. black-forest-labs/flux, 2024. https://github.com/ Li, J., Cui, Y., Huang, T., Ma, Y., Fan, C., Yang, M., and Zhong, Z. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, J., Liu, G., Liang, J., Li, Y., Liu, J., Wang, X., Wan, P., Zhang, D., and Ouyang, W. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Miao, Z., Wang, J., Wang, Z., Yang, Z., Wang, L., Qiu, Q., and Liu, Z. Training diffusion models towards diverse image generation with reinforcement learning. In CVPR, pp. 1084410853, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763. PmLR, 2021. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 9 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. Wallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Purushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik, N. Diffusion model alignment using direct preference optimization. In CVPR, pp. 82288238, 2024. Wang, J., Liang, J., Liu, J., Liu, H., Liu, G., Zheng, J., Pang, W., Ma, A., Xie, Z., Wang, X., et al. Grpo-guard: Mitigating implicit over-optimization in flow matching via regulated clipping. arXiv preprint arXiv:2510.22319, 2025a. Wang, Y., Zang, Y., Li, H., Jin, C., and Wang, J. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025b. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: Learning and evaluating human preferences for text-to-image generation. In NeurIPS, pp. 1590315935, 2023. Xue, Z., Wu, J., Gao, Y., Kong, F., Zhu, L., Chen, M., Liu, Z., Liu, W., Guo, Q., Huang, W., et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. Yang, K., Tao, J., Lyu, J., Ge, C., Chen, J., Shen, W., Zhu, X., and Li, X. Using human feedback to fine-tune diffusion models without any reward model. In CVPR, pp. 8941 8951, 2024. You, Z., Cai, X., Gu, J., Xue, T., and Dong, C. Teaching large language models to regress accurate image quality In CVPR, pp. 14483 scores using score distribution. 14494, 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 10 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO The Appendix is structured as follows: Appendix presents the experimental results on FLUX.1-dev. Appendix provides the details of our experimental configuration. Appendix provides some theoretical discussion on our sign-based turning-point criteria, including the guarantee of sign consistency as well as the value range of rewards. Appendix details our balancing operations on aggregation-based rewards to further stabilize and improve optimization. Appendix offers the pseudocode of our TP-GRPO. Appendix provides more qualitative comparisons to demonstrate the superiority of our method. A. Experimental Results on FLUX.1-dev To further verify the robustness of our method across different architectures, we adopt FLUX.1-dev (Labs, 2024) as the base model and use PickScore (Kirstain et al., 2023) as the reward model. Following the Flow-GRPO configuration, we set α to 0.8, use = 6 sampling steps during training and = 28 during inference, and choose group size of = 24. The classifier-free guidance scale is fixed to 3.5. The corresponding training curves are shown in Figure 8. Our method consistently outperforms Flow-GRPO under this setup, demonstrating the effectiveness of the proposed step-level reward design and our mechanism for capturing turning points. B. Details of the Experimental Configuration Our experiments are conducted based on the Flow-GRPO codebase (Liu et al., 2025). We train all models using 32 NVIDIA H20 GPUs. To maximize performance, we compute advantages on per-prompt basis2. Following the practice of DanceGRPO (Xue et al., 2025), we set the KL penalty coefficient β to balance fast convergence and the prevention of reward hacking. Specifically, we use β = 0.0004 for Compositional Image Generation and Visual Text Rendering, and β = 0.0001 for Human Preference Alignment. For both training and evaluation, in some cases we adopt newer reward model checkpoints that exhibit stronger performance than the defaults used in Flow-GRPO. The exact reward models and their versions are summarized in Table 2. Figure 8. Training curves with FLUX.1-dev as base model Table 2. Used reward models and their links. Models Links Aesthetic Score (Radford et al., 2021) https://github.com/LAION-AI/aesthetic-predictor PickScore (Kirstain et al., 2023) DeQA Score (You et al., 2025) ImageReward (Xu et al., 2023) UnifiedReward (Wang et al., 2025b) OCR Accuracy (Cui et al., 2025) GenEval Score (Ghosh et al., 2023) https://huggingface.co/yuvalkirstain/PickScore_v1 https://huggingface.co/zhiyuanyou/DeQA-Score-Mix3 https://huggingface.co/THUDM/ImageReward https://huggingface.co/CodeGoat24/UnifiedReward-qwen-7b https://github.com/PaddlePaddle/PaddleOCR/tree/release/3.2 https://mmdetection.readthedocs.io/en/v2.28.2/ 2https://github.com/yifan123/flow_grpo/issues/ 11 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO C. Theoretical Analysis of Sign-based Turning-Point Criteria C.1. Sign Consistency of Rewards Under Definition 4.1 Lemma C.1. For any turning point selected by Definition 4.1, the sign of its local reward and aggregated long-term reward is the same, i.e., rt ragg > 0. Proof. By definition, rt = R(cid:0)xODE(t1) t1 (cid:1) R(cid:0)xODE(t) (cid:1), = R(cid:0)x0 ragg (cid:1) R(cid:0)xODE(t) (cid:1). Definition 4.1 states that is turning point only if (cid:16) sign R(x(t1) t1 ) R(x(t) ) (cid:17) (cid:16) sign R(x(0) 0 ) R(x(t) ) (cid:17) > 0. Identifying the terms, we see that sign(cid:0)rt (cid:1) sign(cid:0)ragg (cid:1) > 0. Therefore, rt and ragg have the same nonzero sign, which implies rt ragg > 0. C.2. Sign Consistency and Magnitude of Rewards Under Definition 5.1 C.2.1. SIGN CONSISTENCY OF LOCAL AND AGGREGATED REWARDS Lemma C.2. For any turning point selected by Definition 5.1, the sign of its local reward and aggregated long-term reward is the same, i.e., rt ragg > 0. Proof. Recall the definitions of our computed reward (Eq. 7 and 8) rt = R(cid:0)x(t1) = R(cid:0)x(0) ragg (cid:1) R(cid:0)x(t) (cid:1). (cid:1) R(cid:0)x(t) t1 t 0 (cid:1), From Definition 5.1, the only condition we need for this lemma is sign(cid:0)R(x(t1) t1 ) R(x(t) )(cid:1) sign(cid:0)R(x(0) 0 ) R(x(t1) t1 )(cid:1) > 0. (9) The other constraints in the definition (st+1 < 0, st > 0, and 1 1) are not used in this particular argument and are omitted here for clarity. Define the shorthand Then := R(x(t1) t1 ), := R(x(t) ), := R(x(0) 0 ). rt = b, ragg = b. In terms of a, b, c, the key consistency condition Eq. 9 becomes sign(a b) sign(c a) > 0. (10) 12 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO From Eq. 10, the product of the two signs is strictly positive, so they must be equal: sign(a b) = sign(c a)."
        },
        {
            "title": "Hence",
            "content": "sign(rt) = sign(a b) = sign(c a). (11) We now show that sign(c b) = sign(c a), which will imply sign(rt) = sign(ragg ). Consider two cases: Case 1: sign(a b) > 0. Then > b, and by Eq. 10 we also have sign(c a) > 0, so > a. Consequently, Thus sign(c b) > 0 = sign(c a), and, together with Eq. 11, we obtain > > > 0. sign(rt) = sign(ragg ). Case 2: sign(a b) < 0. Then < b, and by Eq. 10 we have sign(c a) < 0, so < a. Consequently, Thus sign(c b) < 0 = sign(c a), and again by Eq. 11, < < < 0. In both cases, we conclude that which implies sign(rt) = sign(ragg ). sign(rt) = sign(ragg ), rt ragg > 0, since Eq. 10 enforces strict inequalities and excludes the degenerate zero case. This completes the proof. C.2.2. COMPARISON OF ABSOLUTE VALUES OF LOCAL AND AGGREGATED REWARDS Lemma C.3. Under the same notation and condition Eq. 9 as in the proof of Lemma C.2, if rt ragg > 0 holds, then Proof. We use the same shorthand as before: (cid:12) (cid:12)ragg (cid:12) > (cid:12) (cid:12) (cid:12)rt (cid:12) (cid:12). := R(x(t1) t1 ), := R(x(t) ), := R(x(0) 0 ). so that rt = b, ragg = b. From the previous lemma, the condition Eq. 9 implies have the same sign, and therefore rt ragg > 0. sign(rt) = sign(ragg ), > rt, we analyze the two possible sign cases. i.e., rt and ragg To show ragg 13 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Case 1: rt > 0 and ragg > 0. In this case, > 0 and > 0, which means > and > b. The consistency condition Eq. 9 gives sign(a b) sign(c a) > 0. Since > 0, we must have > 0, and hence Therefore, > > b. > ragg > rt > 0. Taking absolute values preserves the inequality: Case 2: rt < 0 and ragg < 0. In this case, ragg = ragg > rt = rt. < and < 0, which means < and < b. Again, from Eq. 9, Since < 0, we must have < 0, and hence sign(a b) sign(c a) > 0. Therefore, < < b. < < 0 ragg < rt < 0. Taking absolute values reverses the inequality: In both cases, we obtain which proves the claim. ragg = (c b) > (a b) = rt. (cid:12) (cid:12)ragg (cid:12) > (cid:12) (cid:12) (cid:12)rt (cid:12) (cid:12), D. Balancing Operations for Improved Optimization with TP-GRPO When deploying TP-GRPO, we also introduce balancing strategy to control the frequency of reward replacement within each batch. Since ragg can be positive or negative, batch dominated by negative values may cause the policy to reject most actions, while batch dominated by positive values may lead to overly conservative updates and insufficient exploration. To prevent either partition from dominating the optimization, we compute the number of positive and negative ragg in each batch and enforce balanced selection. Specifically, we only keep an equal number of positive and negative samples for replacing rt. The remaining samples are discarded by ranking ragg and dropping those with the smallest magnitudes. Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO E. Pseudocode Algorithm 1 TP-GRPO Require: Policy (score) model pθ, reward model R(), SDE noise level α, KL regularization coefficient β, group size G, number of sampling steps , number of training iterations K, number of samples per iteration . for = 0 to 1 do // Stage 1: Sample trajectories and collect intermediate rewards for = 0 to 1 do Sample terminal noise xT,i and corresponding prompt ci. Run SDE sampling (Eq. 2) from = to = 0 to obtain noisy trajectory {xt,i}0 For each t, run ODE steps starting from xt,i to obtain the corresponding clean sample x(t) t,i . Compute intermediate rewards {R(x(t) t=T . t,i )}0 t=T . end for // Stage 2: Optimize pθ with TP-GRPO for = to 2 do for each group {xt,g}G g=1 do for each {1, . . . , G} do Initialize flag use aggregated effect False. if = and select good starting point then // First step: apply Remark 5.2 if sign(cid:0)R(x(T 1) 1,g) R(x(T ) use aggregated effect True T,g)(cid:1) sign(cid:0)R(x(0) 0,g) R(x(T ) T,g)(cid:1) > 0 then end if else // Later steps: select turning points via Definition 5.1 Compute st and st+1 according to Eq. 6. if st+1 < 0 and st > 0 and sign(cid:0)R(x(t1) use aggregated effect True t1,g) R(x(t) t,g)(cid:1) sign(cid:0)R(x(0) 0,g) R(x(t) t,g)(cid:1) > 0 then end if end if if use aggregated effect then Compute aggregated reward ragg t,g using Eq. 8. else Compute stepwise reward rt,g using Eq. 7. end if end for end for Compute advantages using rt and ragg Update model parameters θ using the GRPO objective in Eq. 4. (when applicable), according to Eq. 3. end for end for 15 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO F. More Qualitative Comparisons In this section, we present additional qualitative comparisons on three tasks. Figure 9, Figure 10, and Figure 11 show results on Compositional Image Generation, Visual Text Rendering, and Human Preference Alignment, respectively. In Compositional Image Generation, we observe that Flow-GRPO sometimes fails to generate accurate images. For example, it produces an unnecessary sandwich for the prompt in the second column, and some generations lose fine details, making them hard to discern (Columns 3 and 5). We attribute this to insufficient reward signals: since the task is rule-driven, the outcome-based reward is inherently sparse, and when the policy model cannot obtain informative process rewards, generation may fail. In contrast, our method provides step-wise rewards, enabling more stable optimization. For Visual Text Rendering, most samples are rendered well. However, Flow-GRPO occasionally omits short words (e.g., the in the fourth column), and some characters overlap, leading to an inaccurate appearance. Our method performs consistently well on this task while preserving the overall aesthetics of the images. For Human Preference Alignment, our method exhibits strong capability in capturing details and aligning with the prompt (e.g., the webs in the fourth column and the digital art style in the fifth column). The layout is also more reasonable: for instance, in the second column, our generation separates the city landscape from the planet as implicitly indicated by the prompt, whereas Flow-GRPOs generation confounds these two concepts. Figure 9. Additional qualitative comparison on the Compositional Image Generation task. 16 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Figure 10. Additional qualitative comparison on the Visual Text Rendering task. 17 Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO Figure 11. Additional qualitative comparison on the Human Preference Alignment task."
        }
    ],
    "affiliations": [
        "Alibaba Group, Hangzhou, China",
        "Tsinghua University, Beijing, China",
        "Zhejiang University, Hangzhou, China"
    ]
}