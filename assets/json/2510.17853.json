{
    "paper_title": "CiteGuard: Faithful Citation Attribution for LLMs via Retrieval-Augmented Validation",
    "authors": [
        "Yee Man Choi",
        "Xuehang Guo",
        "Yi R.",
        "Fung",
        "Qingyun Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have emerged as promising assistants for scientific writing. However, there have been concerns regarding the quality and reliability of the generated text, one of which is the citation accuracy and faithfulness. While most recent work relies on methods such as LLM-as-a-Judge, the reliability of LLM-as-a-Judge alone is also in doubt. In this work, we reframe citation evaluation as a problem of citation attribution alignment, which is assessing whether LLM-generated citations match those a human author would include for the same text. We propose CiteGuard, a retrieval-aware agent framework designed to provide more faithful grounding for citation validation. CiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4% accuracy on the CiteME benchmark, on par with human-level performance (69.7%). It also enables the identification of alternative but valid citations."
        },
        {
            "title": "Start",
            "content": "CiteGuard: Faithful Citation Attribution for LLMs via Retrieval-Augmented Validation Yee Man Choi1, Xuehang Guo2, Yi R. (May) Fung3, Qingyun Wang4, 1University of Waterloo, 2University of Illinois at Urbana-Champaign, 3Hong Kong University of Science and Technology, 4College of William and Mary ymchoi@uwaterloo.ca xuehangg@illinois.edu yrfung@ust.hk qwang16@wm.edu 5 2 0 2 5 1 ] . [ 1 3 5 8 7 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have emerged as promising assistants for scientific writing. However, there have been concerns regarding the quality and reliability of the generated text, one of which is the citation accuracy and faithfulness. While most recent work relies on methods such as LLM-as-a-Judge, the reliability of LLM-as-a-Judge alone is also in doubt. In this work, we reframe citation evaluation as problem of citation attribution alignment, which is assessing whether LLM-generated citations match those human author would include for the same text. We propose CiteGuard, retrieval-aware agent framework designed to provide more faithful grounding for citation validation. CiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4% accuracy on the CiteME benchmark, on par with humanlevel performance (69.7%). It also enables the identification of alternative but valid citations1."
        },
        {
            "title": "Introduction",
            "content": "If have seen further than others, it is by standing upon the shoulders of giants Isaac Newton. Scientific research often progresses by building on the foundation of prior knowledge. Therefore, thorough and faithful literature review and citation attribution of claims are essential to understand the history and scope of subject area, and ensure that new findings are properly contextualized (Salton and Bergmark, 1979; Snyder, 2019; Chigbu et al., 2023). However, conducting such practices has been increasingly difficult due to the rapid growth in number of scientific publications (Larsen and Von Ins, 2010; Bornmann and Mutz, 2015). Large Language Models (LLMs) and LLM agents have emerged as potentially useful tools to alleviate the burden of researchers and support scientific writing Lu et al. (2024); Yamada et al. 1Our code will be released publicly upon publication. 1 Figure 1: CiteGuard succeeds through expanded retrieval actions, where CiteAgent (Press et al., 2024) fails due to OpenPDF access error. (2025); Asai et al. (2024a); Wang et al. (2025). One of the main concerns is hallucinations in LLM (Ji et al., 2023; Huang et al., 2025). For instance, LLMs can generate up to 78-90% fabricated citations (Asai et al., 2024a) and misattribute findings to incorrect sources (Walters and Wilder, 2023). Retrieval-augmented generation (Lewis et al., 2020; Fan et al., 2025) has been proposed to mitigate hallucinations in LLM by retrieving external knowledge to validate the generated text during training data preparation or at inference time (Wang et al., 2024b; Asai et al., 2024a; Wang et al., 2024a, 2025). LLM-as-a-Judge is often used to prepare training data (Asai et al., 2024a,b) or to evaluate generated text (Asai et al., 2024a; Wang et al., 2024b) as it is more scalable in practice, despite the risk of bias and overdependence on LLMs capabilities (Ye et al., 2024; Thakur et al., 2024). LLMas-a-Judge often assumes that the retrieved knowledge used for the generation is available, limiting the use case to evaluating retrieval-augmented output. Furthermore, it does not account for situations where the evaluation requires grounding (Krumdick Figure 2: CiteGuard takes an excerpt and performs searches to find paper that best matches the missing citation. et al., 2025), such as broader textual understanding, cross-referencing multiple sources, or interpreting ambiguous claims)2. Method Precision Recall F1 Few-shot abstract Few-shot full text 1.0 1. 0.16 0.38 0.27 0.55 Table 1: ChatGPT-4o accuracy on citation attribution in the CiteME benchmark. We conduct an evaluation of the reliability of LLM-as-a-Judge for citation attribution of humanwritten scientific claims and their references. Although LLMs can recognize apparently incorrect citations, they often reject correct citations due to the lack of context in the field, resulting in recall as low as 16-17%  (Table 5)  . This could potentially lead to incorrect evaluation of existing methods and limit the performance of trained LLMs when the training data are filtered using LLM-as-a-Judge. We propose CiteGuard, an agent that provides more faithful, and generalizable citation attribution through retrieval-augmented validation. Prior work, CiteAgent (Press et al., 2024) aims to accurately cite scientific claims, although achieving accuracy higher than direct prompting, CiteAgents accuracy (35.3%), is still not on par with human. We propose additional tools (i.e. to search for the context of the scientific claim and to perform more robust search for paper content) and result in +12.3% accuracy over CiteAgent under the same settings. 2We present detailed related work section in App. A. When paired with Deepseek-R1 (Guo et al., 2025), CiteGuard can achieve performance (65.4%) which matches that of human (69.7%). Human evaluation indicates that CiteGuard can suggest additional citations that were missed by the original benchmark. Our contributions are threefold: We propose CiteGuard, an agent that provides faithful citation attribution by suggesting multiple appropriate references. We conduct detailed analysis on CiteME, and human annotations of alternative citations that is not captured by the current benchmark. We conduct experiments to show that CiteGuard significantly improves accuracy in finding the correct reference and that CiteGuard can suggest relevant alternative citations."
        },
        {
            "title": "2.1 Problem Formulation",
            "content": "We formulate the task of finding reference(s) for excerpts x1, x2, ..., xN given pool of possible reference candidates r1, r2, ...rn. We have ground-truth labeling function y(xi) that can map any excerpt xi to ground-truth reference : y(xi) = r. We also have another labeling function ˆy(xi) from human annotations that can map any excerpt xi to set of ground truth references ˆr = ˆr1 k: ˆy(xi) = ˆr. This is different from the CiteME (Press et al., 2024) setting where there is only one ground truth reference. The goal of CiteGuard is to find mapping function fθ such that fθ(xi) y(xi), = 1, ..., . , ..., ˆr 2 The accuracy is defined as: Acc(fθ) = 1 N (cid:88) i=1 1[fθ(xi) = y(xi)] (1) The agreement is defined as: Agree(fθ) = 1 (cid:88) i=1 1[fθ(xi) ˆy(xi) = ] (2) 2.2 Reference Retrieval then sort To obtain fθ, CiteGuard introduces new actions in addition to CiteAgent (Press et al., 2024). We provide the set of actions below (examples and prompts used can be found in App. B). These actions are executed in paper database (i.e., Semantic Scholar), which we can query using search query q, and the search result will be appended to R. Each paper contains title and abstract content , and body content, with text snippets denoted as pi P, i. The source paper that contains the excerpt is S. 1. (search_)citation_count/relevance (adopted): Search for query in the title and abstract fields, results by citation the count/relevance, defined as Searchc(q, D) = argsortP D(count(q, t)) and Searchr(q, D) = argsortP D(rel(q, t)) 2. select (adopted): Select paper from the search results, defined as Select(P R) 3. find_in_text: Search for query string within the full text of specified paper, defined as Searcht(q, ) = argsortpP (rel(q, p)) 4. ask_for_more_context: Retrieve the context for an excerpt from the source paper, defined as Searchcont(qi, S) = {qi3, ..., qi+3}, qi 5. string in the full Searchsni(q, D) = argsortpP,P D(rel(q, p)) search_text_snippet: Search for query text of papers, defined as Instead of finding only one reference, CiteGuard can suggest multiple references when appropriate to provide better understanding of the current literature and facilitate comparative analysis. Every run of CiteGuard suggests one appropriate reference, with subsequent runs searching for new appropriate reference. researcher using this agent can manually audit this iterative process and decide when to stop or allow the agent to make the decision."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate CiteGuard on CiteME (Press et al., 2024), which contains 130 excerpts collected from 3 human-written manuscripts in different Computer Science domains (i.e. computer vision, natural language processing, algorithms, theory), where each excerpt contains exactly one missing citation. The task is for the LLM agent to suggest an appropriate paper to fill in the missing citation. We follow the same hyperparameter settings (e.g., temperature) as CiteAgent (Press et al., 2024). We evaluate CiteGuard on both closedand open-source models, including non-reasoning (GPT-4o (Hurst et al., 2024), Kimi-K2 (Team et al., 2025), Qwen3 (Yang et al., 2025)) and reasoning models , taking the single run results. Using the single-run results obtained, we label the samples with difficulty levels, as detailed in App. C. 3.1 CiteGuard Accurately Grounds Scientific Claims Through Enhanced Actions Results in Table 2 demonstrate that CiteGuard substantially outperforms CiteAgent, improving the accuracy of retrieving the oracle citation by 12.3% on CiteME when both are powered by GPT-4o. When backed by open-source models DeepSeek-R1 and Kimi-K2, CiteGuard achieves up to 65.4% accuracy, approaching the 69.7% human performance reported in CiteME (Press et al., 2024). This improvement is driven by CiteGuards extended retrieval actions ( 2.2), which makes citation search more flexible and robust. As illustrated in Fig. 1, while CiteAgent relies heavily on the read action that assumes reliable PDF access, CiteGuard succeeds through introducing two key new actions: (1) ask_for_more_context that enables the agent to proactively query for additional claim context when the initial snippet is insufficient, and (2) search_text_snippet that allows searching directly within paper contents, thereby reducing reliance on PDF availability. This stepby-step reasoning allows CiteGuard to accurately identify the oracle citation where CiteAgent fails, improving the accuracy and robustness of scientific claim grounding, particularly in real-world citation retrieval with complex long-range contexts."
        },
        {
            "title": "Alternative Citations",
            "content": "Through manual assessment (App. E), CiteGuard showcases its ability to generate high-quality alternative citations beyond the original reference  (Table 2)  . Concretely, by using aggregated human annotations as new oracle, Table 2 computes the agreement between CiteGuards sugEasy(%) Medium(%) Med-Hard(%) Hard(%) All(%) Agree(%) CiteAgent+GPT-4o CiteGuard+GPT-4o CiteGuard+DeepSeek-R1 CiteGuard+Gemini CiteGuard+Kimi-K2 CiteGuard+Qwen3 Human - 100.0 100.0 100.0 100.0 100.0 - - 76.1 87.0 43.5 89.1 65.2 - - 12.8 59.0 15.4 38.5 30.8 - - 0.0 0.0 0.0 0.0 0.0 - 35.3* 47.7 65.4 36.9 60.0 49.2 69.7* - 55.2 66.7 40.6 68.8 62.5 - Table 2: CiteGuard accuracy in the CiteME benchmark, Agree denotes percentage of CiteGuard suggested citations that human annotations agree are relevant. * denotes the number reported by CiteAgent (Press et al., 2024). Top 1 Top 5 Top Method Accuracy (%) Avg # of Tokens AI2 Paper Finder Ours+Gemini Ours+DeepSeek-R1 38. 36.9 65.4 55.4 60.0 - - - - Table 3: AI2 Paper Finder (Ai2, 2025)s accuracy (%) on CiteME compared to CiteGuard. gested citations and human judgments. Across models, CiteGuard achieved substantial alignment with human evaluations, demonstrating its potential to identify relevant alternative literature. Notably, this ability is model-agnostic: both proprietary models like GPT-4o and open-source models like Qwen3 can effectively identify relevant alternatives.Fig. 15 demonstrates CiteGuards backward reasoning ability based on the excerpt. Fig. 10 further shows the lateral reasoning capacity of CiteGuard, where CiteGuard effectively identifies highly related work as the oracle reference suggested."
        },
        {
            "title": "4.1 Retrieval vs Long-Context",
            "content": "To demonstrate the effect of retrieving only relevant parts of the paper versus providing the full paper text, we run the CiteGuard+Kimi-K2 agent, replacing the \"find_in_text\" action with the \"read\" action and present the results in Table 4. With the \"read\" action, the accuracy increased by 3.07%, at the cost of 2 more tokens. The number of tokens can be as large as 4 as shown in App.H.2. Although reading the full paper content in context can provide some benefits, it is at the cost of significantly more tokens. When using CiteGuard, users would need to determine whether to use retrieval or long-context based on the token budget."
        },
        {
            "title": "4.2 Reasoning vs Non-Reasoning Models",
            "content": "Table 2 shows that the difference of open-sourced reasoning (DeepSeek-R1) and non-reasoning read find_in_text 60.0 63.07 33,544.68 15,451.43 Table 4: CiteGuard+Kimi-K2 accuracy difference on the CiteME benchmark when using different actions to get information from the paper content. model (Kimi-K2) in overall performance is small (5.4%). As demonstrated in the example  (Fig. 16)  , reasoning model tends to question itself (\"But note:...However,...\") and consider other available actions in the reasoning phase, while nonreasoning model would be more confident in its action (\"I can still be confident that...\"). Although the agent backed by both models eventually arrived at different citations, both are correct through human assessment, demonstrating that CiteGuard is not dependent on reasoning ability."
        },
        {
            "title": "4.3 CiteGuard vs Paper Finders",
            "content": "An alternative to finding potential references using CiteGuard is to use paper finder. We run Ai2 Paper Finder (Ai2, 2025) on CiteME and present the results in Table 3, with details in App. H.4. We argue that CiteGuard matches Paper Finder in terms of accuracy, if not surpasses it. In particular, the top 10 accuracy is 5.4 percentage points below the top 1 accuracy of CiteGuard+DeepSeek-R1, demonstrating that CiteGuard is more reliable, which is likely because it incorporates the context of the excerpt."
        },
        {
            "title": "5 Conclusion",
            "content": "We observe the limitation in using LLM-as-a-Judge for citation attribution of scientific writing and propose CiteGuard agent to provide more faithful citation attribution through retrieval-augmented validation. We show the reliability of CiteGuard in finding correct citations to be on par with humans, and the alternative citations suggested by CiteGuard are deemed relevant by human annotators."
        },
        {
            "title": "Limitations",
            "content": "Currently, the implementation of CiteGuard is based on the Semantic Scholar API, which causes CiteGuards performance to be limited by the coverage of the database and the ability of the retrieval pipeline of the API. One future direction of CiteGuard is enabling the use of other research literature database and retrieval pipelines. Although we have shown that CiteGuard agent works well with both open-sourced and closed-sourced, both reasoning and nonreasoning models, we have not yet explored its performance on smaller open-sourced models (i.e. models with less than 1B parameters) due to the limitation of time. We plan to conduct such an analysis and evaluate how much CiteGuard depends on the models size."
        },
        {
            "title": "Ethical considerations",
            "content": "Our work aims to promote more faithful citation attribution for scientific writing, regardless of machine-generated or human-generated. The framework relies on Large Language Models, which may exhibit systemic biases in research communities, such as geographic and linguistic biases. Although our method is model-agnostic, we acknowledge that mitigating these biases is still an open challenge. Future work includes better representation of under-cited or non-English sources. Our framework uses Semantic Scholar, which is an open-access research tool for scientific literature, through its API. We have not use any private or sensitive data. All human annotators (including the authors) participated in voluntary manner, with their identify kept anonymous during the analysis."
        },
        {
            "title": "References",
            "content": "Ai2. 2025. Introducing ai2 paper finder. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, and 1 others. 2024a. Openscholar: Synthesizing scientific literature with retrieval-augmented lms. arXiv preprint arXiv:2411.14199. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024b. Self-rag: Learning to retrieve, generate, and critique through selfreflection. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, and 1 others. 2022. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR. Lutz Bornmann and Rüdiger Mutz. 2015. Growth rates of modern science: bibliometric analysis based on the number of publications and cited references. Journal of the association for information science and technology, 66(11):22152222. Uchendu Eugene Chigbu, Sulaiman Olusegun Atiku, and Cherley Du Plessis. 2023. The science of literature reviews: Searching, identifying, selecting, and synthesising. Publications, 11(1):2. Travis Ebesu and Yi Fang. 2017. Neural citation network for context-aware citation recommendation. In Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, pages 10931096. Zhiyuan Fan, Longfei Yun, Ming Yan, Yumeng Wang, Dadi Guo, Brian Mak, James Kwok, and Yi R. Fung. 2025. End-to-end optimization for multimodal retrieval-augmented generation via reward backpropagation. In Findings of the Association for Computational Linguistics: EMNLP 2025. Michael Färber and Ashwath Sampath. 2020. Hybridcite: hybrid model for context-aware citation recIn Proceedings of the ACM/IEEE ommendation. joint conference on digital libraries in 2020, pages 117126. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1). Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, and 1 others. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and 1 others. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. 5 Chanwoo Jeong, Sion Jang, Eunjeong Park, and Sungchul Choi. 2020. context-aware citation recommendation model with bert and graph convolutional networks. Scientometrics, 124(3):19071922. William Walters and Esther Isabelle Wilder. 2023. Fabrication and errors in the bibliographic citaScientific Reports, tions generated by chatgpt. 13(1):14045. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM computing surveys, 55(12):138. Michael Krumdick, Charles Lovering, Varshini Reddy, Seth Ebner, and Chris Tanner. 2025. No free labels: Limitations of llm-as-a-judge without human grounding. arXiv preprint arXiv:2503.05061. Peder Larsen and Markus Von Ins. 2010. The rate of growth in scientific publication and the decline in coverage provided by science citation index. Scientometrics, 84(3):575603. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459 9474. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292. Jiacheng Miao, Joe Davis, Jonathan Pritchard, and James Zou. 2025. Paper2agent: Reimagining research papers as interactive and reliable ai agents. arXiv preprint arXiv:2509.06917. Ori Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, Ofir Press, and Matthias Bethge. 2024. Citeme: Can language models accurately cite scientific claims? Advances in Neural Information Processing Systems, 37:78477877. Gerard Salton and Donna Bergmark. 1979. citation study of computer science literature. IEEE Transactions on Professional Communication, (3):146158. Hannah Snyder. 2019. Literature review as research methodology: An overview and guidelines. Journal of business research, 104:333339. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, and 1 others. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. arXiv preprint arXiv:2406.12624. Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2024a. SciMON: Scientific inspiration machines optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279299, Bangkok, Thailand. Association for Computational Linguistics. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, and 1 others. 2024b. Autosurvey: Large language models can automatically write surveys. Advances in neural information processing systems, 37:115119115145. Yubo Wang, Xueguang Ma, Ping Nie, Huaye Zeng, Zhiheng Lyu, Yuxuan Zhang, Benjamin Schneider, Yi Lu, Xiang Yue, and Wenhu Chen. 2025. Scholarcopilot: Training large language models for academic writing with accurate citations. arXiv preprint arXiv:2504.00824. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. 2025. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Libin Yang, Yu Zheng, Xiaoyan Cai, Hang Dai, Dejun Mu, Lantian Guo, and Tao Dai. 2018. lstm based model for personalized context-aware citation recommendation. IEEE access, 6:5961859627. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, and 1 others. 2024. Justice or prejudice? quantifying biases in llm-as-ajudge. arXiv preprint arXiv:2410.02736. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, 6 be carried out by humans. Collecting human annotations are costly and not scalable. To overcome this issue, LLM-as-a-Judge was introduced to automate the evaluation process (Zheng et al., 2023). Due to the improved scalability, LLM-as-a-Judge has been widely used to evaluate LLMs generated scientific writing. For instance, OpenScholar (Asai et al., 2024a) uses LLM-as-a-Judge to filter and refine LLM-synthesized training data. However, LLM-as-a-Judge exhibits bias (Ye et al., 2024; Gu et al., 2024) or sensitivity to prompt (Thakur et al., 2024). Moreover, LLM-as-a-Judge often requires text snippet of the citation under review, which limits its use case for scenarios where the text snippets used during generation are not available. In this work, we explore expanding LLM-as-a-Judge to include RAG to alleviate biases and provide more robust evaluation in cases where relevant text snippets are not directly available. similar idea is Agent-as-a-Judge (Zhuge et al., 2024) targeting the task of automated code generation for AI development. Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, and 1 others. 2024. Agent-as-ajudge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Retrieval-Augmented Generation and LLMs for Scientific Research Retrieval Augmented Generation (RAG) models were first introduced as models that can combine parametric and non-parametric memory (Lewis et al., 2020). Recently, RAG has shown to be promising direction toward mitigating hallucinations and other challenges in knowledge-intensive tasks for LLMs (Borgeaud et al., 2022; Gao et al., 2023). One application is the use of LLMs/LLM Agents to assist human researchers, such as knowledge discovery, proposing ideas, carrying out experiments, scientific writing, conducting reviews, or even transforming paper into interactive agents (Lu et al., 2024; Yamada et al., 2025; Miao et al., 2025). As part of the effort to mitigate the hallucination issue in LLMs for scientific writing, RAG-aware finetuned LLMs for literature summaries have been introduced (Asai et al., 2024a; Wang et al., 2025). A.2 Citation Suggestion There has been different approaches to citation recommendation before the era of LLMs, such efforts include information retrieval (Färber and Sampath, 2020), and neural networks (Ebesu and Fang, 2017; Yang et al., 2018; Jeong et al., 2020). These methods require re-training and do not account of the In light of this, rapid updating paper database. LLM agentic workflow (Press et al., 2024) has been proposed to enable acccess to real-time paper database. In this work, we adopted the CiteAgent (Press et al., 2024) framework, where retrieval is performed through tool calls to the Semantic Scholar API, which we treat as black box for retrieval. This approach would benefit from further improvements in the retrieval pipeline of the API. The framework is built to enable multiple rounds of retrieval and reading, with the choice of action dependent on the agents own decision following its thought, similar to the ReAct approach (Yao et al., 2023). CiteGuard uses RAG to provide better evaluation for LLM-generated literature summaries. A.3 LLM-as-a-Judge Evaluation of LLM-generated text has traditionally"
        },
        {
            "title": "B CiteGuard",
            "content": "B.1 Actions examples Figure 3: Retrieving Actions. We define six retrieval actions to ensure the efficiency and accuracy of CiteGuard. B.2 Prompts The system prompt  (Fig. 4)  and examples provided in the prompt for each newly added actions (Fig.5, Fig.6 and Fig.7) are presented below. You are given an excerpt from paper, where citation was deleted. Im trying to find the citation (ignore the word [CITATION], thats just where the citation was deleted from. You will be asked to help me find the paper from which the citation was deleted. You are equipped with the following tools that will help you in your task: you can search, you can select to find keyword in paper from the search results, or you can select paper as your final answer. <FORMAT_INSTRUCTIONS> Your responses have to include one of the actions above. Before you take any action, provide your thoughts for doing so. Do not include anything other than your thoughts and an action in your responses. You must include exactly one action in your responses. Keep in mind that you can only find keyword in papers or select papers after you search. You can always search, and then search again. You can also find different keywords in few papers consecutively, without searching again (as long as the papers appeared in your last search). If your search does not return any relevant results, please try the following: 1. Adjust your query to focus on individual parts of the claim separately, rather than the entire sentence. It is likely that the citation supports only the immediate preceding concept, not the full claim. Use simpler, more general search queries with fewer words (around 3). 2. Avoid overly specific or plural termsuse base forms of key concepts to improve match quality. For example, We apply contrastive learning to improve the representation quality of ResNet-based encoder [CITATION].\", the citation is most likely for ResNet instead of covering both contrastive learning and ResNet. 3. Change to search by text snippets. You should respond with the following, and use the paper title to do subsequent search. Figure 4: CiteGuard System Prompt 8 The excerpt is from paper title \"Learning Deep Features for Discriminative Localization\": In this section, we evaluate the localization ability of CAM when trained on the ILSVRC 2014 benchmark dataset [CITATION] You would respond with: { \"reason\": \"The excerpt is not sufficient to determine whether the citation refers to the ILSVRC 2014 benchmark dataset or the CAMs localization ability. Lets ask for more context to clarify the scope of the citation.\", \"action\": { \"name\": \"ask_for_more_context\", \"query\": \"In this section, we evaluate the localization ability of CAM when trained on the ILSVRC 2014 benchmark dataset [CITATION]\" \"paper_title\": \"Learning Deep Features for Discriminative Localization\" } } You will then be given the following input: 3. Weakly-supervised Object Localization In this section, we evaluate the localization ability of CAM when trained on the ILSVRC 2014 benchmark dataset [20]. We first describe the experimental setup and the various CNNs used in Sec. 3.1. Then, in Sec. 3.2 we verify that our technique does not adversely impact the classification performance when learning to localize and provide detailed results on weakly-supervised object localization. You would respond with: { \"reason\": \"With the additional context, it is now clear that the citation refers specifically to the ILSVRC 2014 benchmark dataset. The cited paper seems to be the ImageNet dataset paper. Well do search to make sure.\", \"action\": {...} } Figure 5: CiteGuard ask_for_more_context Prompt You would then reply: { \"reason\": \"Paper cbb19236820a96038d000dc629225d36e0b6294a seems to directly mention the ILSVRC 2014 benchmark in the abstract. Though we can use ILSVRC 2014 as the query, but fewer words is better, so we only use ILSVRC as the query.\", \"action\": { \"name\": \"find_in_text\", \"paper_id\": \"cbb19236820a96038d000dc629225d36e0b6294a\" \"query\": \"ILSVRC\" }} You would then be given the following input: Based on this work, we attended the competition of ILSVRC 2014 [26] Figure 6: CiteGuard find_in_text Prompt You should respond with the following, and use the paper title to do subsequent search: { \"reason\": \"Due to the unsuccessful paper search, lets search by text snippets instead\", \"action\": { \"name\": \"search_text_snippet\", \"query\": \"ILSVRC 2014\" } } You will then be given the following input: - Title: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Section: Abstract Snippet: In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. Figure 7: CiteGuard search_text_snippet Prompt"
        },
        {
            "title": "C Difficulty Level Labels",
            "content": "We label the sample with difficulty levels using the following criteria: Easy (22 excerpts): Correct for all models Medium (46 excerpts): Correct for more than three out of five models Medium-Hard (39 excerpts): Correct for no more than two out of five models Hard (23 excerpt): Incorrect for all models task. We manually select some successful short trajectories which are more likely to be an indication of contamination and put the examples in Fig.8 and 9. Although these successful trajectories are short, we have not found any instances where the agent knows the ground-truth citation in advance and directly searches for the target citation. Instead, in these trajectories, both agents compose generic search query and identify the appropriate references from the list of search results."
        },
        {
            "title": "E Human Assessment On CiteGuard",
            "content": "Examples of excerpts in different difficulty levels:"
        },
        {
            "title": "Alternative Citations",
            "content": "Easy: Several studies demonstrate the fragility of convolutional networks on simple corruptions. For example, [CITATION] apply impulse noise to break Googles Cloud Vision API. (Ground-Truth: Googles cloud vision api is not robust to noise) Medium: To address this, [CITATION] introduced Adversarial Filtering (AF). An overview is shown in Figure 2. The key idea is to produce dataset which is adversarial for any arbitrary split of (D_train), (D_test). (Ground-Truth: Swag: Large-Scale Adversarial Dataset for Grounded Commonsense Inference) Medium-Hard: Even if we assume fixed filters using combination of the above, our probabilistic formulation still allows learning the parameters of the GSM experts from data as outlined below. Consequently, we do not need to tune the trade-off weights between the brightness and gradient constancy terms by hand as in [CITATION]. (Ground-Truth: High Accuracy Optical Flow Estimation Based on Theory for Warping) Hard: RCA [CITATION] is intermediate between PCA and LDA in its use of labeled data. Specifically, RCA makes use of so-called chunklet information, or subclass membership assignments. (Ground-Truth: Adjustment learning and relevant component analysis)"
        },
        {
            "title": "We evaluate the risk of contamination which means\nthe models are aware of the citation beforehand\nand does not use search tools to accomplish the",
            "content": "10 To evaluation the quality of alternative citations suggested by CiteGuard, we manually inspect the results (see Fig. 15 and Fig. 10 for examples). Concretely, for each sampled claim, at least two expert annotators with backgrounds in computer science and scientific writing independently judge whether the list of suggested citations produced by CiteGuard, backed by different LLMs, are appropriate alternatives. We defined an alternative citation as appropriate if it provides equivalent or stronger evidence for the scientific claim compared to the original reference. The annotators evaluated suggested citations along two axes: Relevance: Whether the cited paper genuinely supports the claim. Sufficiency: Whether the suggested citation can reasonably replace the original in scholarly writing. Inter-annotator agreement reports 72.7%, indicating high consistency among human annotators. Examples in Fig. 1, 10, and 15, suggest that CiteGuards extended retrieval actions and strategies not only improve the accuracy of original citation retrieval, but also expand the searching capacity to identify functionally equivalent references, supporting richer scholarly grounding with enhanced accuracy and robustness. Importantly, our manual analysis (Table 2 and Figures 10 & 15) reveals that CiteGuard is capable of both lateral reasoning  (Fig. 10)  and backward reasoning  (Fig. 15)  , behaviors that traditional citation retrieval systems typically lack. Backward Reasoning: While focusing on more recent publications, CiteGuard is capable of identifying citations of previous years written by the same author  (Fig. 15)  . system: ... user: You are now given an excerpt. Find me the paper cited in the excerpt, using the tools described above. Please make sure that the paper you select really corresponds to the excerpt: there will be details mentioned in the excerpt that should appear in the paper. If you read an abstract and it seems like it could be the paper were looking for, read the paper to make sure. Also: sometimes youll read paper that cites the paper were looking for. In such cases, please go to the references in order to find the full name of the paper were looking for, and search for it, and then select it. In the spirit of transductive bandits [CITATION] we consider more general setting where answers are sets of arms. The set of actions and the set of answers can be different. assistant: {\"reason\": \"The excerpt mentions transductive bandits as the key concept that the citation refers to. Since this is specific term, Ill start by searching for papers on transductive bandits to find the foundational work.\", \"action\": { \"name\": \"search_relevance\", \"query\": \"transductive bandits\" }} user: ... assistant: {\"reason\": \"The excerpt introduces transductive bandits as an existing concept and extends it to setting where answers are sets of arms. Among the search results, Paper ID 4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df directly introduces the transductive linear bandit problem, explicitly defining the scenario where measurement vectors (actions) and items (answers) can be different sets. This matches the excerpts description of distinct action/answer sets. Other papers either apply transductive learning in different domains (e.g., text classification, video segmentation) or focus on variants like federated bandits without defining the core concept.\", \"action\": { \"name\": \"select\", \"paper_id\": \"4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df\" }} Figure 8: CiteGuard+Deepseek Short trajectory (history length: 5) Lateral Reasoning: CiteGuard suggests peer or related work along with its identification of best-match citations  (Fig. 10)  , providing effective citation finding and alternative suggestions. LLM-As-A-Judge Failure Evaluation Prompt. For the evaluation of OpenScholar citation attribution, we guide the LLM judge through the prompt in Fig.11. Failure Example We show how LLM judge can fail in its evaluation of assessment as result of missing terminology nuances  (Fig.12)  ."
        },
        {
            "title": "G LLM Generation Failure",
            "content": "Method Precision Recall F1 Zero-shot abstract Few-shot abstract Zero-shot full text Few-shot full text 1.0 1.0 1.0 1. 0.17 0.16 0.36 0.38 0.29 0. 0.53 0.55 Table 5: ChatGPT-4o accuracy on citation attribution in the CiteME benchmark. generation failure as result of missing alternative citations."
        },
        {
            "title": "H Examples of CiteGuard",
            "content": "H.1 Suggestion On Alternatives. By examining LLM-generated outputs, we also observe failures due to their lack of important elements. For example, Fig.13 illustrates an LLM CiteGuard is capable of suggesting meaningful alternatives (Fig.10, and Fig.15). An example of case where alternative citations are not appropriate 11 system: ... user: You are now given an excerpt. Find me the paper cited in the excerpt, using the tools described above. Please make sure that the paper you select really corresponds to the excerpt: there will be details mentioned in the excerpt that should appear in the paper. If you read an abstract and it seems like it could be the paper were looking for, read the paper to make sure. Also: sometimes youll read paper that cites the paper were looking for. In such cases, please go to the references in order to find the full name of the paper were looking for, and search for it, and then select it. Our evaluation follows the protocol of [CITATION], recent inpainting model that introduces specialized architecture relying on Fast Fourier Convolutions [8] assistant: Okay, now need to locate the paper that (a) is specifically an image-inpainting model and (b) centres its method on Fast Fourier Convolutions. Because the excerpt does not specify the title, Ill start by issuing highly-targeted search that includes both key phrases.{\"reason\": \"The excerpt identifies the missing citation as recent inpainting model that introduces specialised architecture relying on Fast Fourier Convolutions [8]. The query must combine both aspects inpainting task and Fast Fourier Convolutions to find the exact paper.\", \"action\": {\"name\": \"search_relevance\", \"query\": \"Fast Fourier Convolution inpainting model\"}} user: ... assistant: {\"reason\": \"The excerpt calls for the recent inpainting model whose key novelty is Fast Fourier Convolution, and abstract fdf7012ebe9d4c4d2d93004613e7a19f69a83a93 directly fits that description: it introduces network that uses fast Fourier convolutions (FFCs) to obtain image-wide receptive fields for large-hole inpainting.\", \"action\": { \"name\": \"select\", \"paper_id\": \"fdf7012ebe9d4c4d2d93004613e7a19f69a83a93\" }} Figure 9: CiteGuard+Kimi-K2 Short trajectory (history length: 5) Excerpt: For comprehensive evaluation, we test on 19 datasets spanning wide range of object, scene and fine-grained categories: ImageNet [18], StanfordCars [43], UCF101 [68], Caltech101 [25], Caltech256 [32], Flowers102 [56], OxfordPets [CITATION], Food101 [7], SUN397 [75], DTD [14], EuroSAT [37], FGVCAircraft [51], Country211 [61], CIFAR-10 [44], CIFAR-100 [44], Birdsnap [5], CUB [72], ImageNet-Sketch [73] and ImageNet-R [38].\" Oracle Reference: Cats and dogs (2012) CiteGuard Suggestion: The truth about cats and dogs (2011) Figure 10: Example of CiteGuard Suggested Alternative Citations is as follows. Zephyr-7B-Beta [CITATION] an instruction-tuned version of Mistral-7B. (Ground-Truth: Zephyr: Direct Distillation of LM Alignment) is \"find_in_text\" action in Fig.14, where the number of tokens can be as large as 4x. This is due to the additional tokens required when reading multiple full papers in context. H.3 Reasoning vs Non-Reasoning Models H.2 Retrieval vs Long-Context. We present an example of the CiteGuard+GPT-4o agent when using the \"read\" action instead of the CiteGuard demonstrates effective citation generation when supported by models equipped either with or without reasoning capabilities  (Fig. 16)  . As an Attribution Validator, your task is to verify whether given reference can support the given claim. claim can be either plain sentence or question followed by its answer. Specifically, your response should clearly indicate the relationship: Attributable, Contradictory or Extrapolatory. contradictory error occurs when you can infer that the answer contradicts the fact presented in the context, while an extrapolatory error means that you cannot infer the correctness of the answer based on the information provided in the context. Claim: claim Reference: reference Figure 11: OpenScholar citation attribution evaluation prompt to LLM Claim: This technique of lower bounding mutual information is known as Variational Information Maximization [CITATION]. The IM Algorithm: Reference: variational approach to Information Maximization. . . . LLM judgement: Extrapolatory. Cannot infer the correctness of the answer based on the information provided in the context. Figure 12: LLM mistakenly judges correct citation correct due to the slight difference in terminology However, LLMs . . . may lack comparative analysis, organizational structure, and key elements (Li et al. 2024). To address these challenges, . . . , ChatCite, . . . (Li et al. 2024). Figure 13: Example of an issue in LLM-generated text: missing alternative citations (multiple papers other than ChatCite also address comparative analysis)"
        },
        {
            "title": "We provide the average number of tokens and the\nAPI cost per sample for each model we evaluated",
            "content": "13 Excerpt: second consideration is how to treat the image itself: the raw image could be fed directly into the reinforcement learning algorithm through series of convolutions[CITATION]. CiteGuard-Read: Total Input Tokens(75900) Select: Reward learning from human preferences and demonstrations in Atari CiteGuard-Find-In-Text: Total Input Tokens(19182) Select: Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing Figure 14: CiteGuard example when using \"read\" vs \"find_in_text\" Excerpt: We learn π using behavioral cloning [CITATION], which optimizes π by minimizing the negative log-likelihood of actions given the images and language instructions. Oracle reference: Alvinn: An autonomous land vehicle in neural network (1988) CiteGuard suggestion: Framework for Behavioural Cloning (2001) Figure 15: Alternative citation suggested by CiteGuard, both are relevant according to human annotations. Excerpt: second consideration is how the raw image to treat the image itself: could be fed directly into the reinforcement learning algorithm through series of convolutions[CITATION]. DeepSeek-R1: ... But note: the excerpt says \"the raw image could be fed directly into the reinforcement learning algorithm through series of convolutions\". This is exactly what DQN did. However, lets break it down: - The citation is likely for the method of using convolutions to process raw images in RL, not necessarily the entire RL algorithm... Kimi-K2: ...I can still be confident that its abstract already declares it is the first deep learning model to successfully learn control policies directly from high-dimensional sensory input, where input is raw pixels... Figure 16: CiteGuard thought example when backed by reasoning model (DeepSeek-R1) and non-reasoning model (Kimi-K2) H.4 CiteGuard vs Paper Finder Ai2 Paper Finder searches and ranks the documents, which can result in long list of papers, while CiteGuard operates in setting that only produces one suggestion at time. Therefore, we report Ai2 Paper Finders accuracy by taking the top ranked documents, respectively. with below: GPT-4o: 17931.8, avg_input_tokens: avg_output_tokens: 1705.83, avg_api_cost: $0.12(OpenAI) DeepSeek-R1(671B parameters, with 37B activated): 15004.85, avg_input_tokens: avg_output_tokens: 1771.35, avg_api_cost: $0.005 (DeepSeek platform) Gemini-2.0-Flash: , avg_output_tokens: , avg_api_cost: $0 (we use the free tier) avg_input_tokens: Kimi-K2(1T parameters, with 30B ac15451.43, avg_input_tokens: tivated): avg_output_tokens: 826.66, avg_api_cost: $0.017 (Together AI) Qwen3(235B parameters, with 22B activated): 14598.78, avg_input_tokens: avg_output_tokens: 936.81, avg_api_cost: $0.003 (Together AI)"
        },
        {
            "title": "J Human Annotators",
            "content": "All human annotators are graduate students pursuing masters or doctoral computer science degrees at universities where English is the primary language of instruction. Each human annotators are informed that the data collected will be used for paper submission. The instruction given to the human annotators is as follows: Please review each excerpt below and: 1. Select all papers that would be suitable for use as citations in the given excerpt context 2. If none of the papers are suitable, please choose \"None of the above\""
        },
        {
            "title": "K Use of AI Assistants",
            "content": "The AI assistant (i.e., Grammarly) is used for the writing of this manuscript. All content was critically reviewed and revised by human authors to ensure scientific accuracy and originality."
        }
    ],
    "affiliations": [
        "College of William and Mary",
        "Hong Kong University of Science and Technology",
        "University of Illinois at Urbana-Champaign",
        "University of Waterloo"
    ]
}