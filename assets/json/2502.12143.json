{
    "paper_title": "Small Models Struggle to Learn from Strong Reasoners",
    "authors": [
        "Yuetai Li",
        "Xiang Yue",
        "Zhangchen Xu",
        "Fengqing Jiang",
        "Luyao Niu",
        "Bill Yuchen Lin",
        "Bhaskar Ramasubramanian",
        "Radha Poovendran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models ($\\leq$3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer."
        },
        {
            "title": "Start",
            "content": "Yuetai Li Xiang Yue Bill Yuchen Lin Zhangchen Xu Fengqing Jiang Luyao Niu Bhaskar Ramasubramanian Radha Poovendran University of Washington Carnegie Mellon University Western Washington University {yuetaili,zxu9,fqjiang,luyaoniu,byuchenl,rp3}@uw.edu, xyue2@andrew.cmu.edu, ramasub@wwu.edu 5 2 0 2 7 1 ] . [ 1 3 4 1 2 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models (3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Anthropic, 2023; Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a) have demonstrated remarkable performance in complex reasoning tasks, enabling advancements in mathematical problem-solving, logical inference, and structured decision-making (Cobbe et al., 2021; Shao et al., 2024; Yang et al., 2024). key advancement in improving LLM complex reasoning capability is the chain-of-thought (CoT) prompting. This technique decomposes complex problems into intermediate reasoning steps, enhancing both performance and interpretability. (Wei et al., 2023). However, the high computational cost of LLMs hinders their deployment on resource-constrained devices, motivating the development of smaller models that offer similar capabilities at reduced 1 Figure 1: Small student models (3B parameters) do not consistently benefit from long CoT reasoning or distillation from large teacher models. Instead, they perform better when fine-tuned on shorter CoT reasoning or distilled from smaller teachers, which better matches their intrinsic learning capacity. We term this phenomenon the Small Model Learnability Gap. cost. widely adopted strategy to achieve this is distillation (Agarwal et al., 2024; Hinton et al., 2015; Kim et al., 2024a), where CoT sequences generated by strong teacher model are used to fine-tune weaker student model. Naturally, one might expect that distilling CoT sequences from stronger models would consistently improve small models complex reasoning capabilities (Agarwal et al., 2024; DeepSeek-AI et al., 2024; Min et al., 2024; Tunstall et al., 2023). However, we reveal an interesting phenomenon, which we term the Small Model Learnability Gap  (Fig. 1)  : small models do not consistently benefit from the complex reasoning sequences provided by strong teachers, such as long CoT reasoning or distillation from large models. In our experiments, we observe that when small models are exposed to long and intricate reasoning traces, they struggle to internalize the multi-step logic due to their constrained ability. Instead, small models perform better when fine-tuned on shorter, simpler reasoning chains that align more closely with their intrinsic learning capacity. This suggests that small models struggle to process overly elaborate reasoning traces or adapt to the distribution shifts introduced by stronger teachers, ultimately limiting their ability to generalize effectively. To address the challenge described above, we propose Mix Distillation, simple yet effective approach that balances reasoning complexity by blending different types of reasoning traces. Specifically, our method comprises two configurations: (1) Mix-Long combination of long and short CoT examples, ensuring that small models are exposed to both detailed and concise reasoning steps. (2) Mix-Large mixture of responses from both larger and smaller models, allowing small models to learn from reasoning chains that are better suited to their capacity. Our experiments demonstrate that Mix Distillation consistently improves small model reasoning performance compared to standard distillation. For instance, Qwen2.5-3B-Instruct improves by more than 8 points on MATH and AMC using MixLong, compared to direct training on long CoT data. Qwen2.5-3B-Instruct gains more than 7 points on MATH, AIME and AMC using Mix-Large compared with training on large teacher CoT data. These findings highlight fundamental limitation of direct strong model distillation and emphasize the importance of adapting reasoning complexity for effective knowledge transfer. By carefully designing distillation strategies, we provide new insights into overcoming the constraints of small model learning, making them more effective at reasoning-intensive tasks."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Notation Let = (x1, x2, . . . , xn) represent an input sequence (e.g., prompt), and = (y1, y2, . . . , ym) be the corresponding output sequence. We consider LLM parameterized by θ, which predicts the next token following conditional distribution (cid:1). We denote by CoT(y) the πθ subset of tokens in the generated output that encodes chain-of-thought, often serving as reasoning trace or explanatory sequence. (cid:0)ytx, y1:t1 Throughout this work, we use the term short CoT, to describe concise reasoning paths to arrive at solutions (Min et al., 2024; Yeo et al., 2025) and long CoT to describe an extended reasoning sequence that is not only longer but also demonstrates more complex reflective thoughts (Qwen, 2024b; Yeo et al., 2025). Additionally, we use the term large teacher CoT to refer to the reasoning trace generated by larger teacher model, and the term small teacher CoT for the reasoning steps produced by smaller teacher model. Please see Appendix for more examples. 2.2 Supervised Fine-Tuning (SFT) Supervised fine-tuning (SFT) is widely adopted to enhance reasoning capabilities of LLMs on dataset = {(xi, yi)}N i=1, where yi can be short CoT, long CoT, strong model CoT or weak model CoT sequences. The SFT process updates the parameters θ of language model by minimization the negative log-likelihood loss over the instruction dataset D."
        },
        {
            "title": "3 Small Model Learnability Gap",
            "content": "In this section, we fine-tune student models using different CoT data. We then reveal the small model learnability gap given the performance of fine-tuned models. 3.1 Experiment Setup Datasets. We use the 7,500 prompt set of MATH (Hendrycks et al., 2021). This dataset encompasses seven math topics such as advanced calculus, geometry, and linear algebra. Student models. Our study considers ten student models from the Qwen (Qwen, 2024a) and Llama (Meta, 2024a,b) model families of varying sizes. These models include the Instruct version of Qwen2.5-0.5B, Qwen2.5-3B, Qwen2.5-1.5B, Qwen2.5-7B, Qwen2.5-14B, and Qwen2.5-32B, Instruct version of Llama3.2-1B, and the Llama3.2-3B, Llama3.1-8B, and Llama3.3-70B. comprehensive overview of the student models is presented in Table 4 of Appendix A. Teacher models. To compare long CoT with short CoT, we use QwQ-32B-Preview (Qwen, 2024b) to generate long CoT sequences and Qwen2.5-32B-Instruct as the response generator for short CoT. Within each model family, we designate the larger scale model as the large teacher and the smaller scale model as the small teacher. This includes Qwen2.5-72B-Instruct vs Qwen2.5-3B-Instruct, Llama3.1-70B-Instruct vs Llama3.1-8B-Instruct, and Gemma2-27B-it vs Gemma2-9B-it. Evaluation Benchmarks. We evaluate the reasoning capability of fine-tuned student models on set of commonly used benchmarks, including 2 Figure 2: Long CoT Gap (Long = PLong PShort) of student models with different models sizes for (a) Qwen family (b) Llama family. For teacher models, QwQ-preview-32B is chosen to generate long CoT responses, while Qwen2.5-32B-Instruct is chosen to generate short CoT responses. Negative (Positive) Long indicates that long CoT is worse (better) than short CoT. Our results demonstrate that short CoT is better for smaller student models (indicated by Long < 0), while long CoT is better for larger student models (indicated by Long > 0). Figure 3: Large model CoT Gap (Large = PLarge PSmall) of student models with different models sizes for (a) Qwen family (b) Llama family. For teacher models, Qwen2.5-72B-Instruct is chosen as the large teacher to generate responses, while Qwen2.5-3B-Instruct is chosen as the small teacher to generate responses. Negative (positive) Large indicates that large teacher CoT is worse (better) than small teacher CoT. Our results demonstrate that small teacher CoT is better for smaller student models (indicated by Large < 0), while large model CoT is better for larger student models (indicated by Large > 0). MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), AMC 2023, AIME 2024, and the English math subset of OlympiadBench (He et al., 2024). These benchmarks span wide range of challenge levels, from elementary mathematics to advanced competition problems. We define the student model performance as the average score on five benchmarks. Unless otherwise specified, all fine-tuned models are evaluated in zero-shot setting using greedy decoding. We set the maximum generation tokens as 16k. Please see Appendix for detailed experimental setup. We define the following performance scores: PLong: Performance score of student model fine-tuned on long CoT data. PShort: Performance score of student model fine-tuned on short CoT data. PLarge: Performance score of student model fine-tuned on CoT from larger teacher. PSmall: Performance score of student model fine-tuned on CoT from smaller teacher. Training Setup. Teacher models generate responses by rejection sampling (Dong et al., 2023; Gulcehre et al., 2023; Tong et al., 2024; Yuan et al., 2023; Yue et al., 2023; Zelikman et al., 2022) By default, teacher models employ greedy decoding. By combining the math problem instructions with corresponding solutions generated by teacher models, we construct problem-solution pairs to finetune student models. We train the models using the 3 LLaMA-Factory framework (Zheng et al., 2024). For student models of scale less than 14B, we use full-parameter SFT and implement cosine learning rate schedule with maximum learning rate of 105 to fine-tune student models for two epochs (Touvron et al., 2023b). For student models larger than 14B, we adopt LoRA fine-tuning with learning rate of 104 for two epochs. Detailed hyperparameters and information about the experimental platform are provided in Appendix A. 3.2 Long CoT Gap This section evaluates the reasoning capabilities of student models fine-tuned over long CoT data and short CoT data. We quantify the performance difference between long and short CoT data using long CoT gap Long, defined as: Long = PLong PShort. Figure 2 provides comprehensive overview of the long CoT gap Long across different student models. The detailed benchmark scores on MATH, GSM8K, AIME, AMC, and OlympiadBench are deferred to Table 7 in Appendix B. We report the following key takeaways. Takeaway 1: Long CoT Gap Small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT. We observe that long CoT is more effective for larger models, consistently leading to improved performance across most math benchmarks. For example, the student model Qwen2.5-32B-Instruct improves about 15 points across all math metrics on average. However, long CoT data is not effective for smaller models, yielding significantly less improvement compared to short CoT. On the MATH and AMC benchmarks, student model Qwen2.5-1.5B-Instruct performs over 10 points lower when fine-tuned with long CoT data. This shows that smaller models may not be able to effectively learn and utilize the long CoT paradigm. Please see more attribution analysis in Section 3.4. 3.3 Large Teacher CoT Gap We investigate how effective small models may learn from large teacher and small teachers. We define large teacher CoT gap as: Large = PLarge PSmall. 4 Student Model PLong PShort Long Better? Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B Llama-3.3-70B 14.8 27.0 40.3 48.9 59.2 73.0 15.8 32.5 35.2 58.2 19.5 34.2 43.4 47.2 54.3 59.3 19.5 33.1 31.5 54. -4.7 -7.1 -3.1 1.7 4.9 13.7 -3.7 -0.6 3.7 3.8 Short Short Short Long Long Long Short Short Long Long Table 1: Comparison of the average performance between fine-tuning with long CoT (PLong) and short CoT (PShort). We find that small student models may struggle to learn from long CoT data. Student Model PLarge PSmall Large Better? Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Llama-3.2-1B Llama-3.2-3B Llama-3.2-8B Llama-3.2-70B 16.9 32.2 39.7 48.9 52.9 59. 16.5 32.8 25.6 57.6 20.4 33.0 39.4 42.3 49.9 53.0 18.5 31.2 25.1 53.3 -3.5 -0.8 0.3 6.6 3.0 6.5 -1.9 1.6 0.5 4.3 Weak Weak Strong Strong Strong Strong Weak Strong Strong Strong Table 2: Comparison of average performance between fine-tuning with large teacher CoT (PLong) and small teacher CoT (PSmall). We find that small student models may struggle to learn from large teacher CoT data. Figure 3 provides comprehensive comparison of the Large incurred by all student models. The detailed benchmark scores of MATH, GSM8K, AIME, AMC and OlympiadBench are deferred to Table 8 in Appendix B. More experimental results of different teacher models, including Llama3.1-70B vs Llama3.1-8B and Gemma2-27B vs Gemma2-9B are in Table 9 of Appendix B. that larger student example, We observe Qwen2.5-7B-Instruct student models learn effectively from large teacher CoT. For and Qwen2.5-32B-Instruct models improve over 5 points on average, with Qwen2.5-32B-Instruct achieving more than 15 point increase on the AIMC benchmark. However, learn efsmaller models do not fectively from large teacher models such as Qwen2.5-72B-Instruct. Instead, small teacher models such as Qwen2.5-3B-Instruct may serve as better teacher models for small student models. For instance, the performance of Qwen2.5-0.5B-Instruct degrades by more than Student Model Distillation Method MATH AMC GSM8k Olympiad Bench AIME Average Qwen2.5-3B Llama3.2-3B Long CoT Short CoT Strong Model CoT Weak Model CoT Deepseek-R1-32B (Long CoT) Ours Mix-Long Mix-Large Long CoT Short CoT Strong Model CoT Weak Model CoT Deepseek-R1-32B (Long CoT) Ours Mix-Long Mix-Large 56.2 61.0 57.5 60.3 50.7 64.7 65.8 48.7 50.9 47.4 47.9 48.5 53.0 51. 37.5 37.5 35.0 27.5 20.0 45.0 42.5 17.5 15.0 25.0 17.5 17.5 22.5 25.0 80.0 82.0 80.0 79.5 81.2 81.4 81. 75.1 77.5 71.2 74.1 77.7 79.4 76.3 24.4 26.4 25.9 26.4 15.7 28.6 29.0 17.6 18.7 16.9 16.4 16.1 17.2 17. 3.3 10.0 0.0 3.3 0.0 10.0 10.0 3.3 3.3 3.3 3.3 6.7 3.3 3.3 40.3 43.4 39.7 39.4 33.5 45.9 45. 32.5 33.1 32.8 31.2 33.3 35.1 34.7 Table 3: Mix Distillation outperforms the baseline models across most metrics. We use Llama3.2-3B-Instruct and Qwen2.5-3B-Instruct as the student model and 7.5k samples in MATH dataset as the training set. We distill different teacher models to generate responses as the baseline. Our proposed Mix-Long combines long CoT data and normal CoT data in 1:4 ratio, while Mix-Large combines strong model response and weak model response with the same proportion. Experimental results demonstrate that both Mix-Long and Mix-Large surpass baselines in most evaluation metrics. The highest score is bolded, and the second highest score is underlined. 10 points on the AMC benchmark. Note that prior studies (Kim et al., 2024b) also demonstrated that stronger models are not necessarily stronger teachers, emphasizing response generator and teacher-side factors. Our work differs in that we attribute this phenomenon primarily to the size of the student model. Takeaway 2: Large Teacher CoT Gap Small student models tend to learn better from small teachers, while large student models benefit more from large teachers. 3.4 Analysis of Small Model Learnability Gap Domain knowledge affects learnability gap. We observe that math expert models, in spite of small model size, exhibit smaller learnability gap for both long CoT and large teacher CoT data compared to general models in Figure 4. Specifically, we compare the learnability gaps between the student models Qwen2.5-Math-1.5B-Instruct and Qwen2.5-1.5B-Instruct. Our findings show that the long CoT gap of the small math expert model is significantly smaller than that of general small models. Furthermore, the performance improvement of Qwen2.5-Math-1.5B when fined-tuned with large teacher CoT exceeds that of Qwen2.5-1.5B, suggesting that math expert models benefit more substantially from large teacher CoT. We conjecture that key factor leading to the small model learnability gap is the limited in-domain knowledge of small student models. We summarize this observation in the following takeaway. Takeaway 3: Effect of Domain Knowledge Limited domain knowledge of small models may hinder their learning from strong reasoning teachers. Base models exhibit more significant learnability gap. We observe that base models generally exhibit more significant learnability gap than Instruct models in Figure 5. This suggests that it is more challenging for small base models to effectively learn from long CoT data or large teacher CoT. Takeaway 4: Base vs Instruct Small base models experience more significant learnability gap than Instruct models. Speaking styles shift. We adopt the method from (Lin et al., 2023) to evaluate the rank shift of each token before and after fine-tuning on long CoT and Large teacher CoT data. This allows us to 5 Figure 4: Math expert models usually have less significant Learnability Gap than the general models. positive Gap means long CoT or large teacher CoT is better while negative means worse. This indicates that the math expert model could more easily learn from long CoT data or large teacher CoT. Figure 5: Base models generally exhibit more significant learnability gap than Instruct models. positive gap indicates that long CoT data or large teacher CoT enhance performance, whereas negative gap suggests they have the opposite effect. This implies that it is more challenging for small base models to effectively learn from long CoT data or large teacher CoT. compare the token distribution shifts induced by the fine-tuning process. We then annotate the tokens that exhibit the largest rank shifts as the most shifted tokens. Our analysis reveals that these tokens are predominantly associated with expressive and stylistic elements, such as wait, But, and Let. Please see Appendix for more details. Takeaway 5: Speaking Styles Shift Long CoT and large teacher CoT primarily shift the student models distribution of tokens associated with speaking styles."
        },
        {
            "title": "Learnability Gap",
            "content": "This section presents our Mix Distillation approach to bridge the small model learnability gap. 4.1 Mix Distillation We propose Mix Distillation to address the learnability gap observed in small models. This approach blends easier-to-learn data with more challenging data for small models, thereby leveraging the strengths of both. Our insight is that small models tend to perform better on data that closely matches their inherent distribution (such as short CoT or small teacher CoT), while they struggle with data that exhibits greater distribution shifts. The token distribution of the mixed long CoT and large teacher CoT data may become closer to that of small models inherent distribution, thereby enabling them to learn more effectively from challenging datasets. We propose Mix-Long, which combines long CoT and short CoT data with weight of long CoT α and short CoT 1α. Similarly, we proposed MixLarge, which combines large teacher CoT with weight of α and small teacher CoT with weight of 1 α. 4.2 Experiment Results We use Qwen2.5-3B-Instruct as the student model and MATH (7.5k) as the training set. We distill different teacher models to generate responses as the baseline. They include QwQ-32B (long CoT), Qwen2.5-32B (short CoT), Qwen2.5-72B (large teacher CoT), Qwen2.5-3B (small teacher CoT). We add Deepseek-R1-32B (DeepSeek-AI, 2025) as the teacher model to generate another set of long 6 Figure 6: The average performance varies with the mix weight of long CoT or large teacher CoT data. Qwen2.5-3B-Instruct is chosen as the student model. At weight of 0.2, mix distillation achieves the highest average performance. CoT data as baseline. We set α = 0.2 in both configurations of Mix-Long and Mix-Large. Experimental results demonstrate that both MixLong and Mix-Large surpass baselines in most evaluation metrics. We show that the small student model could achieve improved performance by Mix Distillation compared to training on single dataset. For instance, Qwen2.5-3B-Instruct improves by more than 8 points on MATH and AMC using MixLong, compared to direct training on long CoT data. It also shows more than 7-point gain on MATH, AIME and AMC for Qwen2.5-3B-Instruct by Mix-Large compared with training on large teacher CoT data. This implies that it is easier for small student models to learn from datasets generated by Mix Distillation. Takeaway 6: Mix Distillation Bridges Gap large By mixing long CoT data (resp. teacher CoTs) and short CoT data (resp. small teacher CoT), the small student model could achieve better performance compared to training on either data alone. Figure 7: Case Study of Mix-Long. Models fine-tuned on long CoT tended to overthink, while those trained on short CoT produced incorrect answers. In contrast, Mix-Long, incorporating branching elements (e.g., Alternatively), achieved balanced reasoning process and arrived at the correct answer. served that the small student model fine-tuned on long CoT data becomes overwhelmed by repeated thoughts and fails to stop, whereas the model finetuned on short CoT data produces incorrect answers. In contrast, our proposed Mix-Long, which incorporates branching elements (e.g., the use of Alternatively), delivers the correct answer. Additionally, the average token lengths of responses generated by long CoT, short CoT, and Mix-Long are 3384.7, 575.7, and 1248.9, respectively. We suggest that mixing long CoT and short CoT data is practical approach to achieving balanced CoT length, thereby enhancing the reasoning capabilities of small student models. Figure 6 shows the average performance when taking different mix weight α of long CoT data or large teacher CoT. We choose Qwen2.5-3B-Instruct as the student model and find that weight α of 0.2 achieves the highest average performance across five benchmarks for both Mix-Long and Mix-Large. Interestingly, we find that after mixing long CoT and short CoT data, the small student models output incorporates characteristics of long CoT, such as branching process, while maintaining reduced token length and avoiding overly elaborate thinking. This is illustrated in Figure 7. We ob-"
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Chain-of-Thought Early research on CoT primarily focused on short CoT, where models produce succinct reasoning paths to reach solution (Lambert et al., 2025; Longpre et al., 2023; Wei et al., 2023; Yu et al., 2024). Recently, researchers have turned to long CoT prompting, which encourages the generation of extended and detailed reasoning chains (DeepSeek-AI, 2025; Hou et al., 2025; Kimi Team, 2025; NovaSky, 2025; OpenAI, 2024; Pan et al., 2025; Zeng et al., 2025). The model systemati7 cally explores multiple paths (branching) and reverts to earlier points if particular path proves wrong (backtracking). Although several studies have investigated methods such as distillation and reinforcement learning to integrate long CoT capabilities into LLMs, these efforts have predominantly concentrated on large models. In contrast, our work specifically targets the challenges associated with training smaller models. 5.2 Synthetic Reasoning Data Although human-crafted reasoning datasets have been used to enhance LLM reasoning capabilities (Hendrycks et al., 2021; LI et al., 2024), their development is both time-consuming and labor-intensive. Recent advancements have streamlined this process by generating instructions or responses directly from LLMs (Hui et al., 2024; Toshniwal et al., 2024; Xu et al., 2024; Yue et al., 2023; Zhang et al., 2025) or extracting data directly from web (Paster et al., 2023; Yue et al., 2024), yielding more detailed and diverse chain-of-thought reasoning pathways. Recent study has investigated the impact of various response generators (Kim et al., 2024b), suggesting that in the domains of instruction following and reasoning, responses from stronger teacher models do not necessarily produce the most effective learning effects for student models. However, these investigations have not recognized student model size as critical factor influencing this phenomenon, nor have they performed the more attribution and mitigation analyses as in this paper."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this paper, we show that long CoT data and large model responses were not uniformly beneficial for small student models. We found that small models may perform better when fine-tuned with short CoT and small model CoT. We termed this challenge as the Small Model Learnability Gap. The reason behind it may be that small student models excel on data that closely match their inherent distribution but struggle with significant distribution shifts. To bridge the gap, we introduced Mix Distillation, including Mix-Long, which combined long CoT and short CoT data in ratio, and MixLarge, which integrated large and small teacher CoT. Experimental results showed that both MixLong and Mix-Large outperform baselines across most evaluation metrics, which implied mix distillation outperforms training on single data distribution. This paper provided practical insights for optimizing post-training strategies to enhance small language model reasoning capability. We will explore several promising directions as future work. First, we will refine mix distillation by optimally combining diverse data sources and proposing more fine-grained mixing algorithms to boost reasoning capabilities. Second, we propose to study how strong reasoning teachers can generate data that is better suited for tuning small student models, thereby facilitating more effective knowledge transfer. Third, we will conduct further theoretical and model interpolability studies on the small model learnability gap. Lastly, we will investigate which SFT methods yield the best initial policies for subsequent RL procedure, ultimately enhancing overall model performance."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is partially supported by the Air Force Office of Scientific Research (AFOSR) under grant FA9550-23-1-0208, the Office of Naval Research (ONR) under grant N0014-23-1-2386, and the National Science Foundation (NSF) AI Institute for Agent-based Cyber Threat Intelligence and Operation (ACTION) under grant IIS 2229876. This work is supported in part by funds provided by the National Science Foundation, Department of Homeland Security, and IBM. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF or its federal agency and industry partners."
        },
        {
            "title": "Limitations",
            "content": "While our study provides valuable insights into the understanding of small model learnability gap in math reasoning, we acknowledge that our research primarily focuses on this specific domain and does not evaluate other crucial skills such as instruction following, code generation, or multi-modal understanding. We also did not investigate the impact of fine-grained variations in pre-training data composition on the small model learnability gap. more detailed analysis of how different pre-training data sources and their proportions affect learning outcomes could offer valuable insights into optimizing data selection strategies for mitigating this gap."
        },
        {
            "title": "Ethical Statement",
            "content": "This paper focuses on the evaluation and enhancement of reasoning capabilities in small language models through distillation techniques. The dataset and benchmarks used in our experiments are publicly available. We do not introduce or endorse any applications that could cause harm or be misused. This paper does not present any ethical concerns."
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2024. On-policy distillation of language models: Learning from self-generated mistakes. Preprint, arXiv:2306.13649. Anthropic. 2023. Introducing claude. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Preprint, arXiv:2005.14165. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, and et al. 2024. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. Preprint, arXiv:2304.06767. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. 2023. Reinforced self-training (rest) for language modeling. Preprint, arXiv:2308.08998. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Preprint, arXiv:2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. Preprint, arXiv:2103.03874. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in neural network. Preprint, arXiv:1503.02531. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. 2025. Advancing language model reasoning through reinforcement learning and inference scaling. Preprint, arXiv:2501.11651. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint, arXiv:2106.09685. Tingfeng Hui, Lulu Zhao, Guanting Dong, Yaqi Zhang, Hua Zhou, and Sen Su. 2024. Smaller language models are better instruction evolvers. Preprint, arXiv:2412.11231. Gyeongman Kim, Doohyuk Jang, and Eunho Yang. 2024a. Promptkd: Distilling student-friendly knowledge for generative language models via prompt tuning. Preprint, arXiv:2402.12842. Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee, Yizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, and Evaluating language Graham Neubig. 2024b. Preprint, models as synthetic data generators. arXiv:2412.03679. Kimi Team. 2025. Kimi k1.5: Scaling reinforcement learning with llms. Preprint, arXiv:2501.12599. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing frontiers in open language model post-training. Preprint, arXiv:2411.15124. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numina- [https://github.com/project-numina/ math. 9 aimo-progress-prize](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf). Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023. The unlocking spell on base llms: Rethinking alignment via in-context learning. Preprint, arXiv:2312.01552. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. Preprint, arXiv:2301.13688. Meta. 2024a. Llama-3.2-3b. https://huggingface. co/meta-llama/Llama-3.2-3B. Meta. 2024b. Meet llama 3.1. https://llama.meta. com. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. 2024. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. Preprint, arXiv:2412.09413. NovaSky. 2025. Sky-T1: Train your own o1 preview model within $450. Accessed: 2025-01-09. OpenAI. 2023. Gpt-4 technical report. OpenAI. 2024. Learning to reason with llms. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Tinyzero. Hao Peng, and Alane Suhr. 2025. https://github.com/Jiayi-Pan/TinyZero. Accessed: 2025-01-24. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2023. Openwebmath: An open dataset of high-quality mathematical web text. Preprint, arXiv:2310.06786. Qwen. 2024a. Qwen2.5: party of foundation models. Qwen. 2024b. Qwq: Reflect deeply on the boundaries of the unknown. 2024b. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2024. Beyond human data: Scaling self-training for problem-solving with language models. Preprint, arXiv:2312.06585. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. 2024. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. Preprint, arXiv:2407.13690. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. 2024. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. Preprint, arXiv:2410.01560. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct distillation of lm alignment. Preprint, arXiv:2310.16944. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. Preprint, arXiv:2406.08464. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. Preprint, arXiv:2409.12122. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying 10 long chain-of-thought reasoning in llms. Preprint, arXiv:2502.03373. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. Metamath: Bootstrap your own mathematical questions for large language models. Preprint, arXiv:2309.12284. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. Preprint, arXiv:2308.01825. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. Preprint, arXiv:2309.05653. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. 2024. Mammoth2: Scaling instructions from the web. Preprint, arXiv:2405.03548. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Preprint, arXiv:2203.14465. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 2025. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/ simplerl-reason. Notion Blog. Dylan Zhang, Qirun Dai, and Hao Peng. 2025. The best instruction-tuning data are those that fit. Preprint, arXiv:2502.04194. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Detailed Experimental Setups",
            "content": "Category Models Teacher Models Long CoT vs ShortCoT QwQ-32B-Preview vs Qwen2.5-32B-Instruct Large Teacher vs Small Teacher Qwen Family Llama Family Gemma Family Qwen2.5-72B-Instruct vs Qwen2.5-3B-Instruct Llama3.1-70B-Instruct vs Llama3.1-8B-Instruct Gemma2-27B-it vs Gemma2-9B-it Qwen Family Llama Family Student Models Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, Qwen2.5-32B-Instruct Llama3.2-1B-Instruct, Llama3.2-3B-Instruct, Llama3.1-8B-Instruct, Llama3.3-70B-Instruct Table 4: Overview of Teacher and Student Models A.1 Models Table 4 presents comprehensive overview of student and teacher models used in our paper. A.2 Training Setup Our model training is conducted using LLaMAFactory (Zheng et al., 2024), on server with four NVIDIA A100-SXM4-80GB GPUs, an AMD EPYC 7763 64-Core Processor, and 512 GB of RAM. We use full parameter fine-tuning on student models less than 14B parameters. When the student model is larger than 14B, we use LoRA fine-tuning (Hu et al., 2021). Table 5 and Table 6 list hyper-parameters for full parameter fine-tuning and LoRA fine-tuning respectively. Hyper-parameter Value Learning Rate Number of Epochs Number of Devices Per-device Batch Size Optimizer Learning Rate Scheduler Max Sequence Length 1 105 2 4 2 Adamw cosine 16384 Table 5: This table shows the hyper-parameters for full parameter fine-tuning. Hyper-parameter Value Learning Rate Number of Epochs Number of Devices Per-device Batch Size Lora Target Learning Rate Scheduler Warmup Ratio Max Sequence Length 1 104 2 4 1 full cosine 0.03 16384 Table 6: This table shows the hyper-parameters for LoRA fine-tuning. Teacher models generate responses by rejection sampling (Zelikman et al., 2022; Tong et al., 2024; Yue et al., 2023; Singh et al., 2024; Gulcehre et al., 2023; Yuan et al., 2023; Dong et al., 2023). By default, teacher models employ greedy decoding. By combining the math problem instructions with corresponding solutions generated by teacher models, we construct problem-solution pairs to fine-tune student models. We perform pairwise comparisons of solutions generated by different teacher models and filter out problem-solution pairs that are correct for both models to fine-tune student models. A.3 Evaluation Setup We evaluate the reasoning capability of fine-tuned student models on set of commonly used benchmarks, including MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), AMC 2023, AIME 2024, and the English math subset of OlympiadBench (He et al., 2024). Unless otherwise specified, all fine-tuned models are evaluated in zero-shot setting using greedy decoding. We set the maximum generation tokens as 16k. The evaluation prompt is shown below. Prompt Solve the following math problem and present the final answer in the format: Final Answer: {your answer} Problem: {problem} Answer: After extracting the final answer of the evaluated model, we first employ exact matching to determine the correctness of the answer. If the answer is incorrect, we use Qwen-32B-Instruct as judge to compare the extracted final answers against that of the ground truth. The prompt is shown below. MATH GSM8K AIME AMC Olympiad Average Long Model PLong PShort Long PLong PShort Long PLong PShort Long PLong PShort Long PLong PShort Long Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B Llama-3.3-70B Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B 28.6 48.7 50.0 75.3 23.0 41.6 56.2 68.2 78.3 84. 33.4 50.9 44.6 74.9 31.5 52.3 61.0 67.8 76.2 82.3 -4.78 -2.14 5.36 0.340 -8.44 -10.7 -4.84 0.460 2.04 2.44 42.3 75.1 81.4 92.7 39.5 63.8 80.0 86.2 93.3 94. 49.2 77.5 75.5 91.2 45.3 71.7 82.0 85.7 92.5 94.3 -6.90 -2.42 5.84 1.44 -5.84 -7.89 -1.98 0.560 0.760 0.610 0.00 3.33 0.00 26.7 0.00 0.00 3.33 13.3 20.0 40. 0.00 3.33 0.00 13.3 0.00 0.00 10.0 6.67 6.67 10.0 0.00 0.00 0.00 13.3 0.00 0.00 -6.67 6.67 13.3 30.0 2.50 17.5 27.5 55.0 7.50 17.5 37.5 40.0 60.0 85. 7.50 15.0 22.5 52.5 15.0 27.5 37.5 40.0 55.0 62.5 -5.00 2.50 5.00 2.50 -7.50 -10.0 0.00 0.00 5.00 22.5 5.48 17.6 17.3 41.3 4.00 12.3 24.4 36.6 44.4 60. 7.40 18.7 14.8 39.7 5.93 19.4 26.4 35.7 40.9 47.3 -1.92 -1.04 2.52 1.63 -1.93 -7.11 -1.93 0.889 3.56 13.2 -3.72 -0.619 3.74 3.85 -4.74 -7.13 -3.08 1.72 4.94 13. Table 7: This table summarizes the performance of models in Llama and Qwen families fine-tuned with long CoT and short CoT data. They are evaluated on MATH, GSM8K, AIME, AMC, and OlympiadBench. QwQ-32B-Preview is chosen to generate long CoT and awhile Qwen-2.5-32B-Instruct is chosen to generate short CoT. We observe that small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT. Prompt Given math problem, its correct final answer, and the models generated final answer, determine if the models answer is correct. Respond with True if the it is correct and False if it is incorrect. Problem: {problem} Correct Final Answer: {ground truth} Models Generated Final Answer: {resp answer} Your Judgement: model sizes. Qwen-2.5-72B-Instruct is chosen as the large teacher while Qwen-2.5-3B-Instruct is chosen as the small teacher. The results are shown in Table 8. Our findings indicate that small student models may experience degraded performance when distilled from large teacher compared to small teacher, whereas larger student models benefit more from distilling large teacher. results different model Gemma-27B-it vs Llama3.1-72B-Instruct for families, Gemma-9B-it vs Llama3.1-8B-Instruct. teacher models including and Table 9 shows more experiment in"
        },
        {
            "title": "C Examples of Speaking Style Shift",
            "content": "In this section we present additional experiment results of long CoT gap and large teacher CoT gap. B.1 Long CoT Gap: Additional Results Table 7 shows the detailed performance scores and gap of each benchmark for different student models fine-tuned on long CoT and short CoT. QwQ-32B-Preview is chosen to generate long CoT and awhile Qwen-2.5-32B-Instruct is chosen to generate short CoT. We observe that small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT. B.2 Large Teacher CoT Gap: Additional Results Table 8 shows the detailed performance scores and gap of each benchmark for different student models distilled from large teacher and small teacher. We summarize the performance of 10 student models from the Llama and Qwen families across various We adopt the method from (Lin et al., 2023) to evaluate the most shifted tokens after fine-tuning on long CoT and Large teacher CoT data. Figure 8 shows the calculation process. This allows us to compare the token distribution shifts induced by the fine-tuning process. We annotate the tokens that exhibit the largest rank shifts as the most shifted tokens. We choose Qwen2.5-3B-Instruct as the student model. We put the results of most shifted tokens after fine-tuning on long CoT data in Figure 9 and 10. The results of most shifted tokens after fine-tuning on large teacher CoT data are shown in Figure 11. Our analysis reveals that these tokens are predominantly associated with expressive and stylistic elements, such as wait, But, and Let."
        },
        {
            "title": "D Examples of Various CoT Data",
            "content": "This section demonstrates examples of different CoT data including long CoT, short CoT, large teacher CoT and small teacher CoT. See the following examples. 13 MATH GSM8k AIME AMC Olympiad Model PStrong PWeak Strong PStrong PWeak Strong PStrong PWeak Strong PStrong PWeak Strong PStrong PWeak Strong Llama-3.2-1B Llama-3.2-3B Llama-3.2-8B Llama-3.2-70B Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B 29.8 47.4 37.6 74. 30.0 50.3 57.5 71.3 76.4 80.5 29.6 47.9 37.6 72.2 31.0 50.7 60.3 63.6 72.8 76.8 0.160 -0.500 -0.040 2.28 -0.920 -0.440 -2.82 7.66 3.66 3.72 44.4 71.2 67.0 92. 43.1 70.6 79.9 87.8 93.1 92.2 47.5 74.1 69.2 92.2 45.4 71.0 79.5 84.1 89.6 92.7 -3.18 -2.88 -2.20 -0.152 -2.35 -0.455 0.379 3.72 3.49 -0.531 0.00 3.33 6.67 16. 0.00 0.00 0.00 6.67 6.67 20.0 0.00 0.00 0.00 16.7 0.00 3.33 3.33 0.00 3.33 3.33 0.00 3.33 6.67 0.00 0.00 -3.33 -3.33 6.67 3.33 16.7 2.50 25.0 7.50 67. 5.00 22.5 35.0 40.0 47.5 57.5 7.50 17.5 7.50 50.0 17.5 20.0 27.5 35.0 45.0 50.0 -5.00 7.50 0.00 17.5 -12.5 2.50 7.50 5.00 2.50 7.50 6.07 16.9 9.19 37. 6.52 17.8 25.9 38.8 41.0 47.4 7.70 16.4 11.0 35.7 8.30 20.0 26.4 29.0 39.0 42.4 -1.63 0.445 -1.78 1.63 -1.78 -2.22 -0.444 9.78 2.07 5.04 Average Strong -1.93 1.58 0.530 4.25 -3.51 -0.790 0.256 6.56 3.01 6.48 Table 8: This table summarizes the performance of models in Llama and Qwen families fine-tuned with large teacher CoT and small teacher CoT when evaluated on MATH, GSM8K, AIME, AMC, and OlympiadBench. Qwen-2.5-72B-Instruct is chosen as the large teacher while Qwen-2.5-3B-Instruct is chosen as the small teacher. We observe that small student models may experience degraded performance when distilled from large teacher compared to small teacher, whereas larger student models benefit more from the distilling large teacher. Gemma2-9B vs Gemma2-27B Llama3.1-8B vs Llama3.1-70B Model MATH AMC Olympiad AIME GSM8k Average MATH AMC Olympiad AIME GSM8k Average Llama3.2-1B Llama3.2-3B Llama3.1-8B Llama3.1-70B Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B -1.42 2.08 0.56 0.02 -4.56 -1.20 0.44 0.22 1.32 0.10 -7.50 -7.50 0.00 7. 0.00 2.50 5.00 5.00 2.50 2.50 0.00 -0.888 0.078 -0.741 0.741 -1.19 1.78 1.04 -0.148 1.48 0.00 0.00 0.00 10.0 0.00 0.00 0.00 -3.33 0.00 3.44 -0.227 1.67 -0.516 0. 0.592 -0.986 -0.758 3.94 -0.986 1.36 -1.83 -0.928 0.0243 3.39 -0.645 -0.174 1.29 1.37 0.537 1.78 -1.42 -0.14 -2.18 2.72 -1.88 -1.48 -1.26 3.68 2.18 2.72 -5.00 10.0 7.50 17. 0.00 5.00 5.00 20.0 0.00 -2.50 -0.296 -0.593 2.67 5.48 0.185 -0.148 -0.741 4.15 0.445 5.63 3.33 3.33 0.00 6.67 0.00 3.33 -3.33 3.33 3.33 3.33 0.152 1.06 -1.29 0. -1.74 -1.14 -1.29 2.81 -0.303 0.834 -0.646 2.73 1.34 6.67 -0.688 1.11 -0.325 6.79 1.13 2.00 Table 9: This table presents the performance of student models distilled from different teacher models, including Gemma-27B-it vs Gemma-9B-it and Llama3.1-72B-Instruct vs Llama3.1-8B-Instruct. We observe that small student models may experience degraded performance when distilled from large teacher compared to small teacher, whereas larger student models benefit more from the distilling large teacher. 14 Figure 8: The process of calculating most shifted tokens. We decode each token generated by the fine-tuned LLM in the student model before fine-tuning. Then we calculate the rank shift in the student model for each token generated by the fine-tuned model. We annotate the tokens that exhibit the largest rank shifts as the most shifted tokens. We found that these tokens are predominantly associated with expressive and stylistic elements, such as But and Let. Problem Let (x, y) be an ordered pair of real numbers that satisfies the equation x2 + y2 = 14x + 48y. What is the maximum value of y? 15 Example of most shifted tokens after fine-tuning on long CoT data. (1/2) So Ive got this math problem here. It says: Let (x, y) be an ordered pair of real numbers that satisfies the equation x2 + y2 = 14x + 48y. What is the maximum value of y? Okay, lets see how to approach this. First, need to understand what the equation is telling me. Its an equation involving and y, and its set equal to zero: x2 + y2 14x 48y = 0. need to find the maximum value of for real numbers and that satisfy this equation. Hmm, maybe can rearrange this equation to make it easier to work with. Lets see: x2 14x + y2 48y = 0. can complete the square for both and terms. For x: x2 14x, to complete the square, take half of 14, which is 7, and square it to get 49. So, x2 14x becomes (x 7)2 49. For y: y2 48y, half of 48 is 24, squared is 576. So, y2 48y becomes (y 24)2 576. Now, plugging these back into the equation: Simplify: (x 7)2 49 + (y 24)2 576 = 0. (x 7)2 + (y 24)2 625 = 0, So, (x 7)2 + (y 24)2 = 625. This looks like the equation of circle with center at (7, 24) and radius 25. Wait, but the problem is asking for the maximum value of y, and have circle equation. So, the maximum would be the y-coordinate of the highest point on the circle, which is the center plus the radius. Wait, but the center is at (7,24), and the radius is 25, so the highest point would be at (7,24 + 25) = (7,49). But, let me double-check if this makes sense. The equation is and Ive transformed it into x2 + y2 = 14x + 48y, (x 7)2 + (y 24)2 = 625, which is indeed circle with center (7, 24) and radius 25. So, the maximum would be 24 + 25 = 49. But, wait, the problem says the maximum value of y, and Ive found that to be 49. But let me consider if there are any other constraints or if Ive missed something. Let me think about the original equation: can rearrange it to: x2 + y2 = 14x + 48y. x2 14x + y2 48y = 0. Figure 9: Example of most shifted tokens after fine-tuning on long CoT data. All the most shifted tokens are labeled as red. 16 Example of most shifted tokens after fine-tuning on long CoT data (2/2) Then, completing the square: (x 7)2 49 + (y 24)2 576 = 0, (x 7)2 + (y 24)2 = 625. Yes, thats circle with center (7, 24) and radius 25. So, the maximum is indeed 24 + 25 = 49. But, just to be thorough, lets consider if there are any points on the circle where is greater than 49. The center is at (7, 24), and the radius is 25, so the highest point is at (7, 49), and the lowest point would be at (7, 1). But, the problem is asking for the maximum value of y, and Ive found that to be 49. Therefore, the final answer is 49. Final Answer: Figure 10: Most shifted tokens after fine-tuning on long CoT data (2/2). All the most shifted tokens are labeled as red. 17 Example of Most Shifted Tokens after fine-tuning on large model CoT To solve the problem, we start with the given equation: First, we rearrange the equation to bring all terms to oneside: x2 + y2=14x + 48y x2 14x + y2 48y = 0 Next, we complete the square for both and y. For x: x2 14x To complete the square, we add and subtract (cid:0) 14 2 (cid:1)2 = 49: For y: x2 14x + 49 49 (x 7)2 49 y2 48y To complete the square, we add and subtract (cid:0) 48 2 (cid:1)2 = 576: y2 48y + 576 (y 24)2 576 Now, substitute these completed squares back into the equation: (x 7)2 49 + (y 24)2 576 = 0 (x 7)2 + (y 24)2 625 = 0 (x 7)2 + (y 24)2 = 625 This is the equation of circle with center (7, 24) and radius 25. To find the maximum value of y, we need to find the highest point on the circle. The highest point on the circle is directly above the center, which is at = 24 + 25 = 49. Thus, the maximum value of is: = 49 Final Answer: 49 Figure 11: Example of most shifted tokens after fine-tuning on large teacher CoT. All the most shifted tokens are labeled as red. Problem What is the sum of the positive odd divisors of 60? Long CoT Example To find the sum of the positive odd divisors of 60, first need to understand what the divisors of 60 are and then identify which of those are odd. First, lets find all the positive divisors of 60. To do this, can start by factoring 60 into its prime factors. 60 = 22 31 51 Now, to find all the positive divisors, can take all possible products of these prime factors raised to their possible exponents. For 2, the exponents can be 0, 1, or 2; for 3, 0 or 1; and for 5, 0 or 1. Listing them out: 1 = 20 30 50, 2 = 21 30 50, 4 = 22 30 50, 3 = 20 31 50, 6 = 21 31 50, 12 = 22 31 50, 5 = 20 30 51, 10 = 21 30 51, 20 = 22 30 51, 15 = 20 31 51, 30 = 21 31 51, 60 = 22 31 51. So, the positive divisors of 60 are: 1, 2, 3, 4, 5, 6, 10, 12, 15, 20, 30, 60. Now, need to identify which of these are odd. Odd numbers are not divisible by 2, so can eliminate any divisors that are even. The odd divisors are: 1, 3, 5, 15. Now, need to find the sum of these odd divisors: 1 + 3 + 5 + 15 = 24. Alternatively, using the prime factorization approach: Since the odd divisors are those that do not include the factor 2, can ignore the 2s in the prime factorization. The remaining odd part is: The sum of the divisors of this odd part is: 31 51. (1 + 3)(1 + 5) = 4 6 = 24. This matches the previous result. Therefore, for 60 = 22 31 51, the sum of the odd divisors is: 24 . Figure 12: Long CoT Example 19 Short CoT Example To find the sum of the positive odd divisors of 60, we first need to factorize 60 into its prime factors. The prime factorization of 60 is: 60 = 22 3 5 Since we are only interested in the odd divisors, we can ignore the factor of 2. Therefore, we only consider the factors 3 and 5. The odd divisors of 60 are the divisors of 3 5 = 15. The divisors of 15 are 1, 3, 5, and 15. Next, we sum these divisors: Therefore, the sum of the positive odd divisors of 60 is: 1 + 3 + 5 + 15 = 24 Figure 13: Short CoT example Problem Find the greatest such that 7(cid:112)(2a)2 + (1)2 4a2 1 1 + 4a2 + 3 = 2. 20 Large Teacher CoT Example (1/2) To solve the equation square root in the numerator. The equation becomes: (2a)2+124a21 1+4a2+3 = 2, we start by simplifying the expression inside the 7 4a2 + 1 4a2 1 1 + 4a2 + 3 = 2 1 + 4a2. Then the equation transforms into: Lets denote = 7x 4a2 1 + 3 = 2 Next, we clear the fraction by multiplying both sides by + 3: 7x 4a2 1 = 2(x + 3) Expanding and simplifying the right side, we get: Rearranging terms to isolate and a2, we have: 7x 4a2 1 = 2x + 6 7x 2x 4a2 1 6 = 0 = 5x 4a2 7 = 0 = 5x = 4a2 + 7 = = 4a2 + 7 5 Recall that = 1 + 4a2. Therefore, we have: To eliminate the square root, we square both sides: (cid:112) 1 + 4a2 = 4a2 + 7 1 + 4a2 = (cid:19)2 (cid:18) 4a2 + 7 5 Expanding the right side, we get: 1 + 4a2 = (4a2 + 7)2 = 16a4 + 56a2 + 49 25 Multiplying both sides by 25 to clear the denominator, we obtain: 25 + 100a2 = 16a4 + 56a2 + 49 21 Large Teacher CoT Example (2/2) Rearranging all terms to one side, we get standard polynomial equation: 16a4 + 56a2 + 49 100a2 25 = 0 = 16a4 44a2 + 24 = 0 Let = a2. Then the equation becomes quadratic in y: 16y2 44y + 24 = 0 We solve this quadratic equation using the quadratic formula = = 44, and = 24: b24ac 2a , where = 16, = 44 (cid:112)(44)2 4 16 24 2 16 = 44 1936 1536 32 400 44 32 = = 44 20 32 This gives us two solutions for y: = 64 32 = 2 and = 3 4 4 . The corresponding values for are = 24 32 = 2 and Since = a2, we have a2 = 2 or a2 = 3 3 = 2 . The greatest value of is Thus, the greatest that satisfies the equation is: 2. 2 22 Small Teacher CoT Example (1/2) 7 To solve the equation square root and the equation itself. First, simplify the expression under the square root in the numerator: (2a)2+(1)24a21 1+4a2+ = 2, we start by simplifying the expression inside the (2a)2 + 12 = 4a2 + 1 7 4a2 + 1 4a2 1 4a2 + 1 + = 2 So the equation becomes: Next, let = 4a2 + 1. Then the equation transforms into: 7x 4a2 1 + = 2 Since = 4a2 + 1, we can express 4a2 in terms of x: 4a2 = x2 1 Substitute 4a2 = x2 1 into the equation: Simplify the numerator: So the equation becomes: 7x (x2 1) 1 + 3 = 2 7x x2 + 1 1 = 7x x2 7x x2 + = 2 Multiply both sides by + 3 to clear the denominator: Expand and simplify: Rearrange all terms to one side: 7x x2 = 2(x + 3) 7x x2 = 2x + x2 + 7x 2x 6 = 0 x2 + 5x 6 = 0 Multiply through by -1 to make the leading coefficient positive: x2 5x + 6 = 0 23 Small Teacher CoT Example (2/2) Factor the quadratic equation: So, the solutions are: (x 2)(x 3) = 0 = 2 or = 3 4a2 + 1. We need to check both solutions: Recall that = 1. For = 2: Square both sides: 2. For = 3: Square both sides: (cid:112) 4a2 + 1 = 4a2 + 1 = 4 4a2 = 3 3 4 a2 = = 3 2 (cid:112) 4a2 + 1 = 3 4a2 + 1 = 9 4a2 = 8 The greatest value of among the solutions is Thus, the final answer is: a2 = 2 = 2. 2 2 Problem What is the greatest common divisor of 128, 144 and 480? 24 Large Teacher CoT Example To find the greatest common divisor (GCD) of 128, 144, and 480, we can use the prime factorization method. 1. Prime Factorization: 128 = 27, 144 = 24 32, 480 = 25 3 5. 2. Identify the common prime factors: The common prime factor among 128, 144, and 480 is 2. 3. Determine the lowest power of the common prime factor: The lowest power of 2 in the factorizations is 24. 4. Calculate the GCD: The GCD is: 24 = 16. Therefore, the greatest common divisor of 128, 144, and 480 is: 16 25 Small Teacher CoT Example To find the greatest common divisor (GCD) of 128, 144, and 480, we start by finding the prime factorization of each number. The prime factorization of 128 is: Thus, The prime factorization of 144 is: Thus, The prime factorization of 480 is: 128 2 = 64 64 2 = 32 32 2 = 16 2 = 8 8 2 = 4 4 2 = 2 2 2 = 1 128 = 27. 144 2 = 72 2 = 36 36 2 = 18 18 2 = 9 9 3 = 3 3 3 = 1 144 = 24 32. 480 2 = 240 240 2 = 120 120 2 = 60 60 2 = 30 30 2 = 15 15 3 = 5 5 = 1 Thus, 480 = 25 3 5. Now, we identify the common prime factors. The factorizations are: 128 = 27, 144 = 24 32, 480 = 25 3 5. The only common prime factor among all three numbers is 2. Next, we determine the smallest power of the common prime factor. The lowest power of 2 among these factorizations is 24. Therefore, the GCD of 128, 144, and 480 is: 16 26 Problem Rounded to 2 decimal places, what is 7 9 ? Large Teacher CoT Example To solve the problem, we need to divide 7 by 9 and then round the result to two decimal places. Rounded to two decimal places, this becomes 0.78. Final Answer: 0.78 7 9 = 0.777 . . . Small Teacher CoT Example To solve the problem of finding the value of 7 steps: 9 rounded to 2 decimal places, we can follow these 1. Perform the division: Calculate 7 9 . 2. Convert to decimal: Perform the division to get the decimal equivalent. 3. Round to 2 decimal places: Round the decimal to 2 decimal places. Lets start with the division: 7 9 0.7777 . . . Next, we need to round this decimal to 2 decimal places. The digit in the third decimal place is 7, which is greater than or equal to 5. Therefore, we round the second decimal place up from 7 to 8. So, 0.7777 . . . rounded to 2 decimal places is 0.78. The final answer is: 0."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "University of Washington",
        "Western Washington University"
    ]
}