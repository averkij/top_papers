{
    "paper_title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "authors": [
        "Shuhaib Mehri",
        "Priyanka Kargupta",
        "Tal August",
        "Dilek Hakkani-Tür"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings."
        },
        {
            "title": "Start",
            "content": "Learning User Preferences Through Interaction for Long-Term Collaboration"
        },
        {
            "title": "Shuhaib Mehri",
            "content": "Priyanka Kargupta Tal August Dilek Hakkani-Tür University of Illinois Urbana-Champaign {mehri2, pk36, taugust, dilek}@illinois.edu 6 2 0 2 6 ] . [ 1 2 0 7 2 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MULTISESSIONCOLLAB, benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MULTISESSIONCOLLAB to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct human user study that demonstrates that memory helps improve user experience in real-world settings."
        },
        {
            "title": "Introduction",
            "content": "Throughout repeated interactions, humans naturally develop interpersonal relationships and adapt their behaviors to each other across various dimensions, including matching communication styles, establishing common ground, and refining mutual understanding (Burgoon et al., 1995; Niederhoffer and Pennebaker, 2002; Wilkes-Gibbs and Clark, 1992; Clark and Brennan, 1991; Clark and Wilkes-Gibbs, 1986). This type of adaptation is particularly critical in human-AI collaboration, where users have interaction preferences (e.g., having key takeaways 1Code available at https://github.com/Shuhaibm/ multisessioncollab 1 at the end of responses or receiving multiple options and their tradeoffs before making decision) that influence how effectively they can understand and engage with agent responses (Sweller, 1988; Shi et al., 2025). Rather than placing the cognitive burden on the user to repeatedly specify their preferences, conversational agents must be able to continuously learn and adhere to them over time (Bickmore and Picard, 2005; Shi et al., 2025). Recent works have introduced memory mechanisms to overcome the finite context window of Large Language Models (LLMs) and enable agents to dynamically store and retrieve user-specific information (Chhikara et al., 2025; Wang and Chen, 2025; Yan et al., 2025). Evaluations of these systems generally focus on how accurately agents can recall information and answer questions about past interactions (Xu et al., 2022a; Maharana et al., 2024; Wu et al., 2025a). While such recall abilities are necessary prerequisite, they do not capture all abilities needed for the intended purpose of these systems, which is to improve long-term interaction with users. In particular, it remains unclear whether agents can recognize what information is valuable for future interactions and leverage it effectively. To address this gap, we introduce MULTISESSIONCOLLAB, benchmark designed to evaluate how well conversational agents can learn user preference information and leverage it to meaningfully improve interactions during multi-session collaboration. The benchmark features user simulators that collaborate with the agent on problem-solving tasks across multiple sessions. Each user has distinct personas and interaction preferences that are derived from psychology, cognitive science, and human-computer interaction research, reflecting realistic human behaviors in collaborative settings (see taxonomy in Appendix A) (Wood and Neal, 2007; Sweller, 2006; Schwartz et al., 2002). The benchmark is designed so that users progress most effectively when agent responses align with their Figure 1: The MULTISESSIONCOLLAB benchmark with our long-term collaborative agent. Each session involves user seeking help for problem. The user maintains draft answer that represents what they have learned from the interaction. They update their draft answer when the agents responses are both helpful and preferencealigned. When responses violate preferences, the user enforces them, as indicated by the red text boxes. After each session, the agent reflects on the interaction to identify user preference information that will be useful for future interactions and update their memory accordingly. We measure collaboration quality in each session with task success, conversation length, and user effort. preferences, incentivizing agents to continuously learn and adhere to preferences. An overview of the benchmark is presented in Figure 1. Unlike prior memory systems that optimize for information recall, we develop long-term collaborative agents equipped with memory that learns user preferences to improve collaboration quality over time. As shown in Figure 1, agents reflect on interactions to identify user preference information that will be valuable for future interactions and update their memory accordingly. During subsequent sessions, the full memory is provided to the agent, and relevant parts are dynamically retrieved throughout the interaction to guide agent behavior towards being more preference-aligned. We also present reinforcement learning (RL) framework that derives learning signals from user simulator behavior in MULTISESSIONCOLLAB to train agents to update memory more effectively. We use Group Relative Policy Optimization (GRPO) (Shao et al., 2024) with an LLM-judge that rewards reflections for comprehensively capturing user preferences revealed during interaction. Extensive experiments across several models and five problem-solving tasks demonstrate that memory enables agents to continuously learn user interaction preferences and improve collaboration quality, yielding higher task success rates, more efficient interactions, and reduced user effort. Analyses of performance across sessions reveal continued improvement throughout the sessions, with the steepest gains occurring in early sessions before gradually stabilizing. Notably, agents that learn preferences through interaction are competitive with those given direct access to ground-truth user preferences, showing how memory captures richer information about preferences. Finally, we conduct human user study with 19 participants who engage in three consecutive collaborative sessions with an agent across coding, writing, and problem-solving tasks. Results align with our experimental findings and demonstrate that equipping agents with memory improves collaboration quality across sessions. Participants described these agents as more personalized and proactive, while also identifying challenges in cross-domain preference generalization. The contributions of our work are: We introduce MULTISESSIONCOLLAB to evaluate how well conversational agents can learn user preferences and leverage them to improve interactions during multi-session collaboration. We develop long-term collaborative agents equipped with memory that enables them to learn and leverage user preferences over time. We present an RL framework that trains agents 2 to generate more comprehensive reflections and update memory more effectively by using rewards derived from user behavior signals. Through extensive experiments and human user study, we demonstrate that memory has positive impact on multi-session collaboration."
        },
        {
            "title": "2 Related Work",
            "content": "Multi-Session Conversation Evaluation. Early works that evaluate LLMs in multi-session interactions focus on how well models can generate responses that are consistent with past interactions (Xu et al., 2022a,b). More recently, benchmarks have shifted towards question-answering style evaluations that assess how well LLMs can remember information from past interactions (Packer et al., 2023; Maharana et al., 2024; Wu et al., 2025a; Hu et al., 2025; Jiang et al., 2025; Zhao et al., 2025). In contrast, our work evaluates the downstream impact of memory, incorporating abilities such as identifying what information will be useful in future interactions and leveraging it to improve collaboration. Memory. Providing LLMs with large contexts is computationally inefficient and can degrade performance, since LLMs struggle to effectively handle large amounts of information (Shi et al., 2023; Liu et al., 2024). While one line of works try to improve LLM abilities to handle larger contexts (Liu et al., 2025a), another line of works introduces memory to enable agents to store information from past experiences, and retrieve it when useful in future interactions (Shinn et al., 2023; Packer et al., 2023; Zhong et al., 2024; Suzgun et al., 2025; Wang et al., 2025c; Ho et al., 2025; Chhikara et al., 2025). More recent works introduce more sophisticated memory architectures, including those that leverage multi-agent systems (Wang and Chen, 2025) or temporal-aware knowledge graphs (Rasmussen et al., 2025). While these works demonstrate the benefit of memory for question-answering tasks, we demonstrate that memory can improve downstream performance in multi-session collaboration settings. RL for Memory. RL has been used to train agents to manage and utilize memory more effectively. Existing approaches use rewards based on question-answering correctness (Zhou et al., 2025b; Yan et al., 2025; Wang et al., 2025b; Yu et al., 2025). In contrast, we present an RL framework that derives rewards from user behavior during interaction. Specifically, we train agents to recognize what user preference information revealed during an interaction will be valuable for future sessions and update memory more effectively. Human-AI Collaboration. Recent work has demonstrated that asking clarifying questions can enhance multi-turn interactions by helping agents better understand tasks, user intent, and preferences (Zhang et al., 2025; Li et al., 2025c,a; Andukuri et al., 2024; Wan et al., 2025; Li et al., 2025b), with several works showing that this behavior is helpful for human-AI collaboration (Wu et al., 2025b; Zhou et al., 2025a; Wang et al., 2025a). These approaches focus on single-session interactions, which is appropriate for cold-start scenarios. However, as users increasingly engage with agents over multiple sessions, repeatedly asking the similar questions and eliciting preferences can become tedious and place unnecessary cognitive burden on users. Our work addresses this limitation by enabling agents remember user preference information across multiple sessions."
        },
        {
            "title": "3 MULTISESSIONCOLLAB",
            "content": "We introduce MULTISESSIONCOLLAB for evaluating conversational agents ability to learn user preference information and leverage it to meaningfully improve collaboration quality over time. The benchmark takes place in multi-session collaborative problem-solving setting with diverse user simulators, and can be instantiated with any problemsolving task. An overview of MULTISESSIONCOLLAB is presented in Figure 1."
        },
        {
            "title": "3.1 Collaborative Problem-Solving Session",
            "content": "In each session, user simulator seeks to solve problem with assistance from conversational agent. Following Wu et al. (2025b); Zhou et al. (2025a), only the user has access to the problem statement, and they start the conversation by providing an initial description of their problem. Over multiple turns, the agent asks clarifying questions to better understand the users task and provides explanations to help the user develop solution. Conversations last until the user is satisfied and decides to terminate, or when the maximum conversation length of 10 user-agent turns has been reached. This design mirrors realistic collaborative scenarios, such as tutoring or debugging, and results in natural multi-turn interactions. 3 In such collaborative settings, agents must not only provide correct information, but also communicate it in ways that users can understand, apply, and learn from (Shi et al., 2025). We capture this by having users maintain draft answer for their solution to the problem. The draft answer is never visible to the agent. It is initially empty, and is progressively updated as users receive assistance. Crucially, users update their draft answer only when the agent provides helpful information in manner that aligns with their interaction preferences. Information delivered in way that violates the users preferences (e.g. using overly technical language when the user prefers simple explanations) hinders their ability to effectively understand and apply the information (Shi et al., 2025; Sweller, 1988). In such cases, users explicitly communicate their preferences to the agent rather than updating their draft answer. We note that real-world users may be able to extract useful information even when responses do not fully align with their preferences. Our design choice of not updating the draft answer in such cases helps isolate preference adherence, enabling more direct evaluation of how well agents can learn and adapt to preferences across sessions."
        },
        {
            "title": "3.2 User Simulator Design",
            "content": "A central component of our benchmark is designing user simulators with realistic user profiles that exhibit diverse behaviors and interaction preferences. First, we instantiate each user profile with randomly selected persona from Persona Hub, largescale persona collection (Ge et al., 2024). Each persona exhibits unique knowledge, experiences, interests, personalities and professions, which translates to varying perspectives and diverse behaviors. Each user profile is also assigned random set of three interaction preferences that describe how they expect the agent to behave during collaboration. The preferences may specify specific communication styles (Miehle et al., 2020), learning approaches (Chi et al., 1989), or proactivity levels (Horvitz, 1999). For instance, user may prefer concise responses with key takeaways highlighted at the end, or expect detailed step-by-step explanations. Each preference is grounded in studies from psychology, cognitive science, and humancomputer interaction, and reflects realistic human behavior in problem-solving settings. complete taxonomy of the preferences and their sources are provided in Appendix A. Finally, the effectiveness of our benchmark as an evaluation framework depends on user simulators that can reliably adhere to their user profiles and consistently enforce their preferences (Mehri et al., 2025). To ensure this, we employ structured reasoning process that explicitly considers whether (a) each preference has been satisfied and if any needs to be enforced, (b) the draft answer should be updated, and (c) the conversation should be terminated. This structure yields interpretable user behavior signals: we track which utterances enforce preferences, using this both to quantify user effort (Section 3.3) and to derive learning signals for our RL framework (Section 4.2). The system prompt is provided in Appendix B."
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "For each session, we evaluate collaboration quality across three dimensions: Task Success: the accuracy of the users final draft answer, which represents what the user learned from the interaction. Task success is standard metric for collaboration (Wu et al., 2025b; Wang et al., 2025a). User Effort: the number of times the user enforced their preferences. Each enforcement represents preference adherence failure, placing cognitive burden on the user to correct the agent behavior rather than focus on the task. Conversation Length: the total number of turns in the conversation. Agents that adhere to preferences reduce friction and help users complete the task in fewer turns. These metrics represent collaboration quality: higher task success, lower user effort, and shorter conversations indicate more effective collaboration."
        },
        {
            "title": "3.4 Multi-Session Setting",
            "content": "For each user, we sequentially run multiple collaborative problem-solving sessions, where each session involves different problem. The agent initially has no knowledge of user preferences, and is expected to learn and adapt to preferences throughout the sessions. We report metrics averaged across all sessions per user, and then across all users."
        },
        {
            "title": "4.1 Long-Term Collaborative Agents",
            "content": "Conversational agents start with no knowledge about user preferences and inevitably generate misaligned responses during early interactions. But 4 specifically, we use learning signals derived from user simulator behavior to design rewards for the GRPO algorithm. Problem Formulation. For collaborative session, let = {(u1, a1), ..., (un, an)} denote conversation consisting of turns, where (ui, ai) are the user and agent utterances at turn i. Let ε = {e1, ..., ek} {u1, ..., un} denote the subset of utterances where the user enforces their preferences. Given conversation C, the agent must generate reflection that captures all preference information revealed during the interaction. Reward Design. session-level reflection is considered successful if it comprehensively captures the user preferences revealed during the interaction and provides actionable guidance for future sessions. ε specifies all information that reflection should capture. We therefore use an LLM judge to evaluate covers all preferences in ε without any hallucinations, and get an objective scalar reward Rcoverage(r, ε). We also include format reward Rf ormat(r) that encourages well-structured outputs containing both reasoning trace and the reflection. The total reward is: R(r, ε) = Rcoverage(r, ε) + Rf ormat(r) (1) LLM judge prompts are provided in Appendix B. Using this reward, we employ GRPO to train policy πθ to generate more comprehensive sessionlevel reflections that have no hallucinations. As result, the agent is able to make more meaningful memory updates that helps them be more preference-aligned during future interactions."
        },
        {
            "title": "5 Experimental Setup",
            "content": "To rigorously evaluate conversational agents with MULTISESSIONCOLLAB, we instantiate the benchmark with 100 user profiles and five problemsolving benchmarks that cover diverse domains: MATH-500 (Hendrycks et al., 2021b), MATHHard (Hendrycks et al., 2021b), LogiQA (Liu et al., 2007), MMLU (Hendrycks et al., 2021a), and MedQA (Jin et al., 2021). We use Llama-3.3-70BInstruct (Dubey et al., 2024) for our user simulator and LLM-judge. Each user collaborates with the agent to solve 20 randomly sampled problems, with one problem per session and maximum of 10 conversational turns per session. Across all benchmarks, this totals 10,000 collaborative sessions per agent, ensuring robust evaluation of agent performance during multi-session collaboration. Figure 2: RL framework for improving session-level reflections. The policy model generates reflection rollouts for conversation. The judge model evaluates each reflection against ε (the subset of user utterances that enforce preferences) and assigns rewards. Advantages are computed and the policy is updated via GRPO. as interaction experience accumulates, users reveal their preferences, and long-term collaborative agents must be able to learn from these signals to reduce cognitive burden on users and improve collaboration quality over time. To build such agents, we equip them with memory that persists and refines user preference information across sessions. It is illustrated in Figure 1 and all prompts are provided in Appendix B. The memory architecture comprises two components: 1. Session-Level Reflection: the agent analyzes the interaction and identifies preference information that would be useful for future interactions, which can include the specific preferences themselves, the contexts in which they apply, and details about which actions or approaches satisfy each preference. The agent then updates their existing memory with this new information. 2. Persistent Memory: the memory is provided at the start of each session as part of the agents system prompt, enabling them to adapt their behavior according to learned preferences without requiring users to repeatedly specify them. Additionally, at each turn, we use an LLM to analyze the conversation history and retrieve parts of the memory that are directly relevant to the conversational context and provide them to the agent."
        },
        {
            "title": "MULTISESSIONCOLLAB",
            "content": "We present an RL framework that uses the MULTISESSIONCOLLAB environment to train agents to generate more comprehensive session-level reflections that effectively capture the user preferences revealed during interactions (Figure 2). More 5 Environment Validation. To validate the reliability of MULTISESSIONCOLLAB and understand the contribution of different components, we conduct series of ablation studies that isolate the impact of various dimensions. We observe drop in performance when moving from direct problemsolving to multi-turn interaction, which aligns with prior work showing that LLMs struggle to ask clarifying questions or maintain coherence across turns (Wu et al., 2025b; Laban et al., 2025; Zhou et al., 2025a; Mehri et al., 2025). Performance decreases further when introducing user preferences, since users correct agent behavior during preference violations rather than focus on the task. Our complete analysis is provided in Appendix C. We also present user study that confirms our findings generalize to real users in Section 7. Training. Training data is constructed using different set of user profiles and problems, with no overlap with evaluation data. Similar to evaluation, we have 100 user profiles and select 20 problems from each problem-solving benchmark. Using Llama-3.3-70B-Instruct as both the user simulator and the agent, we generate 10,000 collaborative sessions along with reflections. We train Llama-3.1-8B-Instruct (Dubey et al., 2024) and Qwen-2.5-7B-Instruct (Qwen et al., 2025) to generate session-level reflections given conversation. The models are first initialized with supervised fine-tuning (SFT) as cold start, which helps stabilizes RL optimization and leads to more effective performance (Guo et al., 2025). Then, we conduct GRPO as described in Section 4.2. Details on hyperparameters and issues such as catastrophic forgetting are provided in Appendix D. We evaluate several LLMs in MULTISESSIONCOLLAB, including Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, gpt-oss-20b, and Llama-3.3-70BInstruct (Qwen et al., 2025; OpenAI, 2025; Dubey et al., 2024). Models are evaluated as standard conversational agents without memory, and also when equipped with memory. For Llama-3.1-8B-Instruct and Qwen-2.5-7B-Instruct models, we additionally report results after applying GRPO training to improve session-level reflections."
        },
        {
            "title": "6 Results",
            "content": "Table 1 presents our experimental results on the MULTISESSIONCOLLAB benchmark. Equipping conversational agents with memory enables them to learn preferences as they accumulate experience 6 and proactively adhere to them without requiring users to repeatedly enforce them, reducing friction and allowing users to focus on the task. This generally results in improvements in collaboration quality: interactions become more efficient, require less user effort, and yield higher task success rates. We observe even further improvements when using our RL framework to train agents to capture more meaningful user preference information in sessionlevel reflections and update memory effectively. Analysis across different models. Different models exhibit varying baseline performance, as shown in the first row for each model that reports the results for standard conversational agent without memory. For instance, the Qwen family tends to achieve the highest performance on mathematical reasoning tasks. Regardless of the baseline performance, memory helps improve collaboration quality. How effectively model can leverage memory depends on their abilities to capture information that will be valuable for future interactions through session-level reflections and leverage them during actual interactions to improve user experience. RL for Improving Session-Level Reflections. We apply our RL framework to train Qwen-2.57B-Instruct and Llama-3.1-8B-Instruct to generate session-level reflections that effectively capture the preferences revealed during interactions. After training, both models are able to leverage the memory more effectively, with notable improvements across all metrics. In particular, while Qwen-2.57B-Instruct initially exhibited 1.03% decrease in task success when equipped with memory, after training, the same model achieves 3.43% improvement in task success. This demonstrates that the memory architectures effectiveness depends largely on reflection quality. Additionally, these results suggest that the way our RL framework uses learning signals derived from user simulator behavior offers promising direction for improving agent memory management abilities. Performance Across Sessions. Next, we analyze our agents performance as they accumulate interaction experience throughout sessions. For each session {1, ..., 20}, we compute the agents average performance across all users and tasks for each metric. Then, we calculate the deltas , , where each delta denotes the difi ference between the agent with memory and the agent without memory at session for the respec- , and Ef MATH-500 MATH-Hard LogiQA TS (%) UE Len TS (%) UE Len TS (%) UE Len Qwen-2.5-7B + memory + GRPO Llama-3.1-8B + memory + GRPO 51.35 51.750.40 56.805. 44.55 44.930.38 48.684.13 2.95 2.800.15 2.160.79 2.73 2.200.53 1.850.88 14.05 13.460.59 12.851.20 14.83 13.731.10 12.851.98 26.5 26.210.29 27.751. 11.50 12.791.29 13.592.09 2.93 2.860.07 2.270.66 2.84 2.330.51 2.010.83 14.62 14.400.22 13.780.84 16.15 15.280.87 14.571.58 19.65 18.611.04 23.203. 24.45 26.502.05 26.552.10 2.40 2.350.05 1.740.66 2.44 1.920.52 1.630.81 15.61 15.570.04 15.010.60 16.50 15.930.57 15.570.93 gpt-oss-20b + memory 67.60 70.352.75 2.90 2.340.56 13.95 12.721.23 41.45 45.073.62 3.46 2.830.63 15.73 14.331. 20.31 25.194.88 2.90 2.320.58 16.56 14.891.67 Llama-3.3-70B + memory 59.29 63.854.56 3.00 1.991. 14.98 13.281.70 25.81 30.504.69 3.27 2.251.02 17.03 15.171.86 26.69 30.253.56 2.96 1.891. 17.08 15.851.23 MMLU MedQA Overall TS (%) UE Len TS (%) UE Len TS (%) UE Len Qwen-2.5-7B + memory + GRPO Llama-3.1-8B + memory + GRPO 47.85 45.172.68 51.153.30 50.50 51.501.00 54.003.50 2.51 2.460.05 1.910. 2.42 2.000.42 1.680.42 13.81 13.720.09 12.880.93 14.12 13.620.50 12.921.20 35.70 34.161.54 39.303.60 39.20 41.852.65 40.951.75 2.58 2.650.07 1.970. 2.74 2.100.64 2.100.64 15.96 16.330.37 15.610.35 16.66 16.000.66 15.890.77 36.21 35.181.03 39.643.43 34.04 35.511.47 36.752.71 2.67 2.620.05 2.010. 2.63 2.110.52 1.850.78 14.81 14.700.11 14.020.79 15.65 14.910.74 14.360.45 gpt-oss-20b + memory 49.62 55.055.43 2.46 2.00. 13.81 12.231.58 32.75 31.681.07 3.26 3.760.50 17.06 16.700.36 42.35 45.473.12 3.00 2.650. 15.42 14.171.25 Llama-3.3-70B + memory 51.26 57.356.09 2.76 1.741.02 14.34 12.991.35 45.84 49.954. 2.91 1.920.99 16.38 15.351.03 41.78 46.384.60 2.98 1.961.02 15.96 14.531.43 Table 1: Conversational agent performance on MULTISESSIONCOLLAB across five problem-solving tasks. We report task success (TS), user effort (UE), and conversation length (Len). For each model, the first row is the baseline conversational agent without memory, +memory is the agent equipped with our memory architecture, and +GRPO is the agent trained to generate session-level reflections. Subscripts denote change relative to the baseline. Overall performance is also reporting by averaging performance across all tasks. We use the instruct variant for all models. tive metric. These deltas isolate the impact of memory at each session. Figure 3 plots these deltas across sessions for Llama-3.3-70B-Instruct, with additional plots for other models presented in Ap- , and decreasing pendix E. Increasing and Ef indicate that the agent is learning to collaboration more effectively. i We observe consistent trends of improvement across all deltas. has an upward trajectory, particularly after smoothing. However, there are also some points in the graph where the delta is close to 0 and the memory does not seem to help improve performance for these problems. and Ef show consistent downward trends, with the steepest improvements occurring within the first five sessions, and gradually stabilizing towards the last 10 sessions. This demonstrates how agents with memory continuously learn and refine their knowledge as interaction experience accumulates, without plateauing in early sessions. Some variance in the results comes from certain preferences that make problem-solving more difficult, reflecting limitation in the agents ability to adhere to preferences during interactions while maintaining task performance. 7 Figure 3: Performance across sessions for Llama-3.370B-Instruct. Each graph plots the delta between agents with memory and agents without memory across 20 sessions for Task Success (T ) , User Effort (U ) , and Conversation Length (Len ) . i Value of Learning Through Interaction. In Appendix C, we report further ablations that help us better understand the memory architecture. One provides agents with direct access to user preferences, same as what is provided to the user simulator. This was intended to be an oracle to establish an upper bound on performance. Surprisingly, agents equipped with memory achieve competitive performance with this oracle, and sometimes outperform it, despite the fact that agents with memory begin with no user preference information. Our qualitative analysis indicates that memory helps capture richer preference information from interactions, such as the contexts in which the preferences apply and specific strategies for adhering to them. Moreover, this information is continually refined across sessions as agents accumulate interaction experience. These results demonstrate the effectiveness of using memory to learn through interaction."
        },
        {
            "title": "7 Real-World User Study",
            "content": "Setup. To complement our main experiments, we conduct user study with 19 participants to understand the impact of memory in more realistic collaboration settings. The study lasted approximately 1.5 hours per participant, and was declared exempt after being reviewed by our Institutional Review Board (IRB). The complete details of our study design and results are provided in Appendix F. We adopt similar setup to MULTISESSIONCOLLAB and design two study types: singledomain (coding only) and mixed-domain (writing, math problem-solving, and coding). In each study, participants complete three sequential sessions, solving one problem per session, with both standard agent and an agent equipped with memory. Participants adopt set of interaction preferences that they ensure the agents adhere to throughout the sessions. These preferences are parallel to those from Appendix A. After each session, participants complete survey where they rate the intrinsics aspects of the collaboration on 5-point Likert scale: preference adherence, preference retention, confidence, and overall satisfaction. Results. Our user study results align with the trends observed in our simulated experiments. Across both single-domain and mixed-domain settings, agents with memory demonstrated consistent improvements over sessions: conversations required fewer turns, preference adherence and retention improved, and confidence and satisfaction increased (see Figure 10). For instance, in the first session conversations with memory had median of 8 (coding) and 6 (mixed) turns, and those without memory had median of 8 turns. By the third session, agents with memory dropped to 6 and 4 turns, compared to 10 and 8 turns without memory. Participants also provided free-form feedback after each session and shared overall thoughts at the end of the study. They described the agents with memory as noticeably more personalized, highlighting their ability to proactively adhere to preferences and handle vague queries. However, participants also noted limitations: personalization felt less effective in mixed-domain settings, and the agents learned preferences were not as effective as when explicitly restated within session. These findings suggest that while memory substantially improves multi-session collaboration, challenges remain in cross-domain generalization and fully capturing user preferences from interaction."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduce MULTISESSIONCOLLAB, benchmark for evaluating conversational agents ability to learn user preferences and leverage them to meaningfully improve collaboration across multiple sessions. We develop long-term collaborative agents equipped with memory that learn preferences to improve interaction quality over time. We further demonstrate that MULTISESSIONCOLLAB can serve as an effective RL environment, where we use learning signals derived from user behavior to encourage more comprehensive reflections, leading to more meaningful memory updates. Extensive experiments and user studies show that equipping agents with memory improves collaboration quality across sessions, yielding higher task success rates, more efficient interactions, and reduced user effort. Our work provides foundation for multisession collaboration, with many promising directions for future research. The user simulators in MULTISESSIONCOLLAB can be extended to have more complex preferences, such as contextdependent preferences that vary across domains, or evolving preferences that develop over time, enabling the evaluation of more sophisticated adaptation abilities. As user simulation advances, simulators can also express preferences through more diverse means, for example through implicit behavioral cues. Finally, improving memory management abilities and addressing ensuring agents 8 do not forget useful information as they update memories remains an important direction."
        },
        {
            "title": "9 Limitations",
            "content": "We use Llama-3.3-70B-Instruct as our user simulator. While it is highly capable model, user simulation requires reliably following complex instructions, such as consistently enforcing preferences throughout interactions, and our evaluation metrics depend on this behavior. We found that the simulator struggled to exhibit expected behaviors during more complex tasks such as coding, which we consequently excluded from our benchmark. Additionally, user simulators may not be realistic or fully capture all kinds of human behavior, for example real users can express preferences implicitly or adapt to the agent over time. Although we conduct human user study to validate our findings across realistic tasks and real users, we acknowledge these limitations remain and merit further investigation."
        },
        {
            "title": "References",
            "content": "Chinmaya Andukuri, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah Goodman. 2024. STar-GATE: Teaching language models to ask clarifying questions. In First Conference on Language Modeling. Timothy Bickmore and Rosalind Picard. 2005. Establishing and maintaining long-term humanACM Transactions on computer relationships. Computer-Human Interaction (TOCHI), 12(2):293 327. Penelope Brown. 1987. Politeness: Some universals in language usage, volume 4. Cambridge university press. Judee Burgoon, Lesa Stern, and Leesa Dillman. 1995. Interpersonal adaptation: Dyadic interaction patterns. Cambridge University Press. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413. Michelene TH Chi, Miriam Bassok, Matthew Lewis, Peter Reimann, and Robert Glaser. 1989. Selfexplanations: How students study and use examples in learning to solve problems. Cognitive science, 13(2):145182. Herbert Clark and Susan Brennan. 1991. Grounding in communication. Herbert Clark and Deanna Wilkes-Gibbs. 1986. Referring as collaborative process. Cognition, 22(1):1 39. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094. William Gudykunst, Stella Ting-Toomey, and Elizabeth Chua. 1988. Culture and interpersonal communication. Sage Publications, Inc. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. In International Conference on Learning Representations. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Yichi Yang, Zhijian Liu, Zhiting Hu, and Lianhui Qin. 2025. Arcmemo: Abstract reasoning composition with lifelong llm memory. arXiv preprint arXiv:2509.04439. Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 159 166. Yuanzhe Hu, Yu Wang, and Julian McAuley. 2025. Evaluating memory in llm agents via incremental multiturn interactions. arXiv preprint arXiv:2507.05257. Bowen Jiang, Zhuoqun Hao, Young-Min Cho, Bryan Li, Yuan Yuan, Sihao Chen, Lyle Ungar, Camillo Taylor, and Dan Roth. 2025. Know me, respond to me: Benchmarking llms for dynamic user profiling and personalized responses at scale. arXiv preprint arXiv:2504.14225. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. 9 Slava Kalyuga. 2009. The expertise reversal effect. In Managing cognitive load in adaptive multimedia learning, pages 5880. IGI Global Scientific Publishing. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. 2025. Llms get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120. Belinda Z. Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. 2025a. Eliciting human preferences with language models. In The Thirteenth International Conference on Learning Representations. Shuyue Stella Li, Avinandan Bose, Faeze Brahman, Simon Shaolei Du, Pang Wei Koh, Maryam Fazel, and Yulia Tsvetkov. 2025b. Personalized reasoning: Just-in-time personalization and why llms fail at it. arXiv preprint arXiv:2510.00177. Shuyue Stella Li, Jimin Mun, Faeze Brahman, Pedram Hosseini, Bryceton G. Thomas, Jessica M. Sin, Bing Ren, Jonathan S. Ilgen, Yulia Tsvetkov, and Maarten Sap. 2025c. ALFA: Aligning LLMs to ask good questions case study in clinical reasoning. In Second Conference on Language Modeling. Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. 2025a. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2007. Logiqa: challenge dataset for machine reading comprehension with logical reasoning, 2020. URL https://arxiv. org/abs. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025b. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of llm agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851 13870. Shuhaib Mehri, Xiaocheng Yang, Takyoung Kim, Gokhan Tur, Shikib Mehri, and Dilek Hakkaniin llm-based user Tür. 2025. Goal alignment arXiv preprint simulators for conversational ai. arXiv:2507.20152. Juliana Miehle, Isabel Feustel, Julia Hornauer, Wolfgang Minker, and Stefan Ultes. 2020. Estimating user communication styles for spoken dialogue systems. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 540548. Kate Niederhoffer and James Pennebaker. 2002. Linguistic style matching in social interaction. Journal of language and social psychology, 21(4):337 360. OpenAI. 2025. gpt-oss-120b & gpt-oss-20b model card. Preprint, arXiv:2508.10925. Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. 2023. Memgpt: Towards llms as operating systems. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. 2025. Zep: temporal knowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956. Barry Schwartz, Andrew Ward, John Monterosso, Sonja Lyubomirsky, Katherine White, and Darrin Lehman. 2002. Maximizing versus satisficing: happiness is matter of choice. Journal of personality and social psychology, 83(5):1178. Susanne Scott and Reginald Bruce. 1995. Decisionmaking style: The development and assessment of new measure. Educational and psychological measurement, 55(5):818831. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv. org/abs/2402.03300, 2(3):5. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 3121031227. PMLR. 10 Quan Shi, Carlos Jimenez, Shunyu Yao, Nick Haber, Diyi Yang, and Karthik Narasimhan. 2025. When models know more than they can explain: Quantifying knowledge transfer in human-ai collaboration. arXiv preprint arXiv:2506.05579. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. 2025. Dynamic cheatsheet: Test-time learning with adaptive memory. arXiv preprint arXiv:2504.07952. John Sweller. 1988. Cognitive load during problem solving: Effects on learning. Cognitive science, 12(2):257285. John Sweller. 2006. The worked example effect and human cognition. Learning and instruction. Yaacov Trope and Nira Liberman. 2012. Construal level theory. Handbook of theories of social psychology, 1:118134. Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, and Natasha Jaques. 2025. Enhancing personalized multi-turn dialogue with curiosity reward. arXiv preprint arXiv:2504.03206. Ante Wang, Yujie Lin, Jingyao Liu, Suhang Wu, Hao Liu, Xinyan Xiao, and Jinsong Su. 2025a. Beyond passive critical thinking: Fostering proactive questioning to enhance human-ai collaboration. arXiv preprint arXiv:2507.23407. Yu Wang and Xi Chen. 2025. Mirix: Multi-agent memory system for llm-based agents. arXiv preprint arXiv:2507.07957. Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, and Xiaojian Wu. 2025b. Mem-{alpha}: Learning memory construction via reinforcement learning. arXiv preprint arXiv:2509.25911. Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2025c. Agent workflow memory. Deanna Wilkes-Gibbs and Herbert Clark. 1992. Coordinating beliefs in conversation. Journal of memory and language, 31(2):183194. Wendy Wood and David Neal. 2007. new look at habits and the habit-goal interface. Psychological review, 114(4):843. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, KaiWei Chang, and Dong Yu. 2025a. Longmemeval: Benchmarking chat assistants on long-term interactive memory. In The Thirteenth International Conference on Learning Representations. Shirley Wu, Michel Galley, Baolin Peng, Hao Cheng, Gavin Li, Yao Dou, Weixin Cai, James Zou, Jure Leskovec, and Jianfeng Gao. 2025b. CollabLLM: From passive responders to active collaborators. In Forty-second International Conference on Machine Learning. Jing Xu, Arthur Szlam, and Jason Weston. 2022a. Beyond goldfish memory: Long-term open-domain conversation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51805197, Dublin, Ireland. Association for Computational Linguistics. Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. 2022b. Long time no see! open-domain conversation with long-term persona memory. In Findings of the Association for Computational Linguistics: ACL 2022, pages 26392650, Dublin, Ireland. Association for Computational Linguistics. Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Kristian Kersting, Jeff Pan, Hinrich Schütze, et al. 2025. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828. Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, WeiYing Ma, Jingjing Liu, Mingxuan Wang, et al. 2025. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259. Michael JQ Zhang, W. Bradley Knox, and Eunsol Choi. 2025. Modeling future conversation turns to teach LLMs to ask clarifying questions. In The Thirteenth International Conference on Learning Representations. Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, and Kaixiang Lin. 2025. Do LLMs recognize your preferences? evaluating personalized preference following in LLMs. In The Thirteenth International Conference on Learning Representations. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1972419731. Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. 2025a. Sweet-rl: Training multi-turn llm agents 11 on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478. Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. 2025b. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841."
        },
        {
            "title": "A User Interaction Preferences",
            "content": "In human-AI collaborative settings, users often have specific interaction preferences that describe how they expect agents to behave. When agents adhere to these preferences, users may learn more effectively, communicate more efficiently, or experience smoother, more personalized interactions (Shi et al., 2025; Sweller, 1988). To ensure our benchmark captures realistic preferences, we draw inspiration from studies in psychology, cognitive science, and human-computer interaction. We manually curate these preferences to be well-suited for our collaborative problem-solving setting. Table 2 provides the complete taxonomy of interaction preferences used in the MULTISESSIONCOLLAB benchmark. Table 2: Taxonomy of user interaction preferences in the MULTISESSIONCOLLAB Benchmark."
        },
        {
            "title": "Elaborateness and Directness",
            "content": "You prefer agent responses to avoid any unnecessary narration and get directly to the point. Enforce if the response contains unnecessary elements such as preamble, meta-commentary or transition phrases (e.g. Thats great question! or Let me walk you through this...). You prefer agent responses to be narrative, conversational, and engaging. The response should acknowledge previous context (e.g. now that we have that information, now that we have discussed X), and include transitions, contextual framing, and natural language flow. Enforce if the response is overly terse or choppy (e.g. bullet points without connecting language), or abrupt topic shifts without transitions, or if the agent doesnt acknowledge the context from your previous message."
        },
        {
            "title": "Politeness",
            "content": "You prefer the agent responses to be respectful, considerate, and friendly. Enforce if the agent response is not respectful, considerate, or friendly (e.g. lacks courtesy markers, lacks pleasantries, is purely transactional like The answer is or Do this then that). You prefer the agent responses to be blunt without pleasantries, apologetic language, or courtesy markers. Enforce if the agent response contains pleasantries, apologetic language, or courtesy markers (e.g. Id be absolutely delighted to help!, Im so sorry, but..., or Thank you so much for this wonderful question!). Analytic vs. Intuitive When the agent is solving problem or explaining concept, you prefer responses that state relevant assumptions, show step-by-step reasoning, and justify conclusions. Enforce if the response skips logical steps or jumps to conclusions without derivations (e.g. The answer is instead of Assuming Y, we can derive because [step 1], [step 2], therefore X). When the agent is solving problem or explaining concept, you prefer responses that start with high-level intuition and generalizable principles before diving into the specific solution. Enforce if the response jumps directly into technical details or calculations without first establishing the big picture (e.g. starting with First, calculate using formula Y... instead of The key principle here is Z, which applies whenever you see W. Lets apply it to your problem...)."
        },
        {
            "title": "Guidance",
            "content": "When working on multi-step problem you prefer that each agent response covers only single small increment of the problem, and asks for confirmation before proceeding to the next increment. Enforce if the agent provides the complete solution in single response instead of breaking them down into smaller increments (e.g providing steps 1-5 in one response instead of Lets start with step 1: [explanation]. Does this make sense before we continue?). When working on problem, you prefer holistic responses that address the full solution. Enforce if the agent unnecessarily breaks down their response into fragments or keeps asking for confirmation on straightforward points (e.g. Let me explain just part 1 first. [Brief explanation]. Should continue? when you could handle the complete answer)."
        },
        {
            "title": "Source",
            "content": "Gudykunst et al. (1988) Brown (1987) Trope and Liberman (2012) Kalyuga (2009) Continued on next page..."
        },
        {
            "title": "Source",
            "content": "Horvitz (1999) Chi et al. (1989) Schwartz et al. (2002) Scott and Bruce (1995) Sweller (2006) Wood and Neal (2007)"
        },
        {
            "title": "Proactivity",
            "content": "You prefer the agent to always end responses with proactive suggestion or next step. Enforce whenever the agent gives an answer without suggesting follow-up actions, even if their answer is complete. (e.g. answering your question but not suggesting You might also want to consider or Next, you could do Y). You prefer the agent to respond to only your request, and does not provide unsolicited suggestions or next steps. Enforce if the agent adds suggestions or next steps (e.g. after answering, adding You might also want to consider or Here are some next steps: ...). You prefer the agent to offer key takeaways or summary for future reference. Enforce if the agent does not mention key takeaways or summary when the conversation is winding down (e.g. not suggesting Let me summarize the key points we covered). Habitual Strategies - Takeaways Habitual Strategies - Maximizer When given an answer, you prefer the agent to provide multiple viable approaches along with the tradeoffs for each. Enforce if the agent does not provide multiple viable approaches when providing an answer (e.g. suggesting Use library instead of You could use library (easier to learn but less performant) or library (steeper learning curve but faster)). Habitual Strategies - Planner When the agent is providing an answer, you prefer the response to start out with an outline of what will be covered. Enforce if the agent dives into response without first outlining the plan (e.g. starting 5-step tutorial immediately instead of Heres what well cover: step 1, step 2, step 3). When the agent provides an explanation, you prefer responses that include examples, analogies or metaphors. Enforce if explanations do not contain examples. Habitual Strategies - Worked Examples Habitual Strategies - General You prefer the agent responses to be no longer than three sentences. Enforce if the agent response exceeds three sentences. You prefer the agent responses structured in bullet-point format. Consider this satisfied if the response contains at least one list item using -, * or numbered items (1., 2., ...). Mixed prose + bullets is acceptable. Enforce only if the response contains no bullets at all. You prefer the agent structured responses that have numbered steps. Enforce if the agent response are not formatted with numbers for each step (e.g. 1. X, 2. Y, 3. instead of X, Y, Z). You prefer structured agent responses that have headings for each section. Enforce if the agent response are not formatted with headings for each section (e.g. X, ## Y, ### instead of X, Y, Z). You prefer the agent responses that have one-line TL;DR at the end. Enforce if the agent response do not include one-line summary at the end (e.g. TL;DR: instead of X). When the agent provides an answer, you prefer the agent responses that contain confidence estimates. Enforce if the agent response contain an answer but do not include confidence estimate (e.g. Im 90% confident that X, instead of X)."
        },
        {
            "title": "B Agent Prompts",
            "content": "Session-Level Reflection Prompt You are collaborative AI agent learning to better help user with problem - solving tasks across multi - session interactions . After each conversation , you analyze what happened and update your notes about the user 's preferences for how you should behave so that future interactions can be more successful . # Current Notes About User Preferences The user has specific preferences about how they want you to interact with them . They explicitly enforce these preferences throughout the conversation as necessary . Here are your current notes about the user 's preferences from previous conversations : { agent_notes } # Conversation to Analyze { conversation_str } # Notes Updating Task Analyze the conversation above to identify the user 's preferences and how you can best satisfy them . Your goal is to create actionable notes that help you satisfy these preferences for future conversations . Keep your notes concise and actionable , without adding unnecessary details . Consider : - When did the user explicitly ask you to adjust your response ? What specifically did they want changed ? - What specific actions , formats , or approaches satisfy each preference ? What should you keep in mind for future conversations ? As new situations arise , you may refine , combine , or split preferences to better reflect the user 's needs . When updating the notes , do not lose any useful information from past interactions . Make sure to add information about the user preferences that you are sure about , and do not hallucinate preferences . # Output Format : {{ \" user_preferences_reasoning \": str , # Reasoning about the user preferences and how to satisfy them \" agent_notes \": str , # Updated notes . Provide description of the user preferences , how to satisfy them , and any additional notes . This will be provided to you in future conversations with this user . Ensure that you provide structured response that is clear and easy to understand . }} For each response , output valid JSON object using the exact format above , do not include any text before or after the JSON object . Long-Term Collaborative Agent System Prompt You are collaborative AI agent helping users solve writing , question answering , math , and coding problems . # User Preferences The user has set of preferences for how you should behave . If you do not follow these preferences , the user will be unable to learn from your response and you will need to adjust your response to adhere to these preferences ( so it is best to follow them initially ). Based on your past interactions with the user , you have maintained set of notes about the users preferences for how you should behave : { agent_notes } # Conversation Guidelines : - If the user 's message is unclear , lacks details , or is ambiguous (e.g. length of an essay , format requirements , specific constraints ) , do not make assumptions . Ask for clarification and ensure you have enough information before providing an answer . - Your goal is to help the user solve their problem . Adhere to their preferences and do your best to help them solve their problem . # Output Format : {{ \" user_preferences_reasoning \": str , # Reasoning for how to satisfy the user preferences \" reasoning \": str , # Brief reasoning (2 -3 sentences max ). Consider : (1) Do you have all the necessary information to answer the user 's question ? If not , should you ask any clarifying questions ? (2) Which user preferences are relevant and how do you satisfy them ? \" response \": str , # Response to the user . }} For each response , output valid JSON object using the exact format above . Use double quotes (\") , escape any double quotes within strings using backslashes (\") , escape newlines as , and do not include any text before or after the JSON object . IMPORTANT : Your output must be within { max_new_tokens } tokens to avoid being cut off ."
        },
        {
            "title": "Memory Retrieval Prompt",
            "content": "You are preprocessing agent that identifies relevant user preferences for an AI assistant . # Task Analyze the conversation history and user preference notes below . Extract the notes that are directly relevant to the user 's current request and will help the main agent generate better response . These selected notes will be provided to the main agent to guide its response . # Conversation History { conversation_history } # User Preference Notes { complete_agent_notes } # Output Format {{ \" reasoning \": str , # Provide your reasoning for which user notes are relevant and why . \" relevant_notes \": str , # The extracted relevant notes . }} Output valid JSON object using the exact format above , and do not include any text before or after the JSON object ."
        },
        {
            "title": "User Simulator System Prompt",
            "content": "You are user simulator collaborating with an agent to solve problem . You will be provided with problem description , and you must get the agent to help you solve it . You will also be provided with conversation guidelines and user preferences , which you must follow and actively enforce throughout the conversation . # Problem Description { user_task_description } { problem } Note : the agent cannot see this problem description . # User Persona { user_persona } # User Preferences { user_preferences } These preferences are NON - NEGOTIABLE that define how you prefer the agent to behave . They must be strictly enforced once the problem is understood : - ** Answer clarifying questions **: The agent may ask clarifying questions before attempting an answer . Answer such questions , and do not enforce preferences about answer format or content while the agent is clarifying . - ** Enforce immediately **: Every agent response must satisfy your preferences before you can proceed . Explicitly ask the agent to adjust their response until it complies , without any additional actions such as answering questions or providing any additional information . - ** Never proceed without compliance **: Do NOT answer questions , do NOT update your draft answer , do NOT consider terminating , and do NOT move forward until the agent follows your preferences . Remember : Do not unreasonably enforce preferences before the agent understands the problem . # Draft Answer Management - ** Maintain working draft **: You will maintain draft answer to your problem throughout the conversation . Start with an empty draft (e.g., \"I don 't know \") . Update your draft answer based on what you learn from agent responses . - ** Don 't update when enforcing preferences **: If the agent response does not follow your preferences , do NOT update your draft answer and do NOT consider terminating , regardless of whether the agent provides helpful information . Wait until they adjust their approach and satisfy your preferences . # Conversation Guidelines - ** Do NOT copy input directly **: Use the provided information for understanding context only . Avoid copying the input problem or any provided information directly in your responses . - ** Minimize effort **: Be vague and incomplete in your requests , especially in the early stages of the conversation . Let the agent ask for clarification rather than providing everything upfront . - ** Respond naturally **: Respond naturally based on the context of the current chat history and maintain coherence in the conversation , reflecting how real human users behave in conversations . # Conversation Termination Before generating your response , determine if you should terminate the conversation : - Do you feel like your draft answer is good answer to the problem ? - Do you feel like the agent cannot help further ? If the agent reponse does not follow your preferences , you must NOT terminate - instaed , enforce the preferences . When ready to terminate , respond with \"{ termination_signal }\". # Output Format : {{ \" preference_1_satisfied \": str , # Reasoning for if the agent satisfies preference 1 \" preference_2_satisfied \": str , # Reasoning for if the agent satisfies preference 2 \" preference_3_satisfied \": str , # Reasoning for if the agent satisfies preference 3 \" enforce_preferences \": bool , # Whether you have to enforce any of your preferences ? \" reasoning \": str , # Brief reasoning (2 -3 sentences max ). Does the agent response follow all of your preferences ? If no , you must enforce them and not proceed . If yes , how should you update your draft answer ? Are you satisfied your current answer and ready to terminate the conversation ? \" draft_answer \": str , # Your current working draft answer to the problem . Start with \"I don 't know \"."
        },
        {
            "title": "Only update it if the agent provides helpful information AND follows your preferences",
            "content": "\" should_terminate \": bool , # Should you terminate the conversation \" response \": str , # Your response to the agent }} For each response , output valid JSON object using the exact format above . Use double quotes (\") , escape any double quotes within strings using backslashes (\") , escape newlines as , and do not include any text before or after the JSON object . 17 Session-Level Reflection Reward Prompt You are an expert evaluator analyzing conversational agent 's reflection of conversation , where they analyze the conversation to identify the user 's preferences and create actionable notes to help them satisfy these preferences in future conversations . Throughout the conversation , the user explicitly enforces their preferences whenever necessary . The agent analyzes the conversation to identify the user 's preferences and create actionable notes to help them satisfy these preferences in future conversations . # Your Task : Evaluate whether the agent 's reflection succesfully captures the user 's preferences and provides actionable notes to help them satisfy these preferences in future conversations . # Agent ' Reflection : { completion_text } # User Messages Where They Enforce Their Preferences : { user_messages_where_they_enforce_preferences } # Gold Reflection : Here is gold reflection for the same conversation . Use this as reference to evaluate the agent 's reflection . { gold_response } # Evaluation Criteria : Assess the reflection on four dimensions : - ** Coverage ( Completeness ) :** Does the agent 's reflection capture all of the user 's preferences ? - ** Actionability ( Quality ) :** Does the agent 's reflection provide actionable notes and details that help the agent satisfy these preferences in future conversations ? - ** Accuracy ( No Hallucination ) :** Are all points grounded in actual user statements ? Does the reflection avoid inventing preferences or misrepresenting user statements ? - ** Clarity :** Is the reflection well - organized and clearly formatted ? Does the reflection avoid redundancy , with each preference stated once without repetitive or overlapping notes ? You will output score from 0 -3 , where : - 0: Does not effectively capture user preferences : gaps in converage , or significant hallucinations - 1: Captures some preferences with limited actionable notes , may hallucinate some preferences - 2: Captures most preferences with actionable notes , may have some slight hallucinations - 3: Comprehensively captures all preferences with highly actionable notes and no hallucinations # Output Format : {{ \" reasoning \": # Brief explanation of your decision \" reflection_score \": # 0 -3 }} Output properly formatted JSON response , as specified by the Output Format ."
        },
        {
            "title": "C Environment Validation",
            "content": "MULTISESSIONCOLLAB presents complex evaluation environment that encompasses several dimensions of agent abilities: problem-solving abilities such as reasoning and domain knowledge, multi-turn interaction including eliciting information through clarification questions, and personalization through learning and adapting to user preferences. To ensure the reliability of our environment and better understand the different dimensions, we conduct series of ablation studies where we incrementally introduce each component and examine its effect on agent performance. We evaluate Llama-3.3-70B-Instruct across all settings, using the experimental setup described in Section 5. Ablation Studies. We define the following settings for our ablation studies: Direct Problem-Solving (S1): The agent receives the problem statement directly and generates solution without any user interaction. This setting establishes baseline task performance and isolates the agent problem-solving abilities, before introducing the complexity of user interaction. Multi-Turn Interaction (S2): The problem statement is provided to user simulator (with no interaction preferences) who engages in conversation with the agent. The agent must ask clarification questions to fully understand the problem statement, then provide an answer to the problem. This mirrors prior works such as Wu et al. (2025b), Laban et al. (2025), Zhou et al. (2025a). This setting isolates the effect of multi-turn interaction, introducing challenges such as underspecification and incorporating user input, and reveals how performance changes when agents must interact with users rather than solving problems directly. 18 Users with Preferences (S3): Building upon S2, user simulators are assigned interaction preferences. This setting corresponds to the standard MULTISESSIONCOLLAB environment, where agents must learn and adhere to user preferences to ensure smooth collaboration. Agent with Oracle Preferences (S4): We use the same environment as S3, but provide the agent with the users preferences at the start of each session. This oracle setting establishes what we would expect to be an upper bound on performance, since agents are given explicit knowledge of user preferences, though as we discuss below, this assumption proves incomplete. Agent with Memory (S5): We use the same environment as S3, but equip the agent with our memory architecture. This enables agents to learn preferences through interaction and retain them across sessions. Discussion. The results from our ablation studies are presented in Table 3: MATH-500 MATH-Hard"
        },
        {
            "title": "MedQA",
            "content": "TS UE Len TS UE Len TS UE Len TS UE Len TS UE Len S1 68. S2 67.85 - - 2.00 50.00 11.99 37. - - 2.00 55.00 13.70 48.85 - - 2.00 75.00 13.09 76.95 - - 2.00 85.00 10.75 80.45 - - 2. 12.43 S3 59.29 3.00 14.98 25.81 3.27 17.03 26. 2.96 17.08 51.26 2.76 14.34 45.84 2.91 16. S4 64.31 1.63 12.92 28.51 1.83 15.29 30.43 1. 15.67 57.43 1.39 12.42 50.91 1.51 14.68 S5 63. 1.99 13.28 30.50 2.25 15.17 30.25 1.89 15.85 57. 1.74 12.99 49.95 1.92 15.35 Table 3: Results for ablation studies on the MULTISESSIONCOLLAB environment. Each row represents different setting: S1: Direct Problem-Solving, S2: Multi-Turn Interaction, S3: Users with Preference, S4: Agent with Oracle Preferences, S5: Agent with Memory. The transition from S1 to S2 reveals notable drop in performance when moving from direct problemsolving to multi-turn interaction. This result aligns with findings from prior works (Wu et al., 2025b; Laban et al., 2025; Zhou et al., 2025a; Mehri et al., 2025), and demonstrates how LLMs struggle with challenges in multi-turn interactions such as handling underspecified problem descriptions. Introducing user preferences in S3 leads to even further decrease in performance. When agents fail to adhere to user preferences, users must expend additional effort to communicate their needs, introducing friction that negatively impacts task progression and collaboration quality. The drop in performance can either come from the agent struggling to effectively provide information to the user because they have to adhere to preferences while doing so, or from the user struggling to digest the information. In S4, agent performance improves when provided with descriptions of user preferences, confirming that knowledge about preferences can meaningfully enhance collaboration quality. However, the gap in performance between S4 and S2 suggests that effective preference adherence requires more than simply knowing what users prefer. Most notably, agents with memory (S5) achieve performance competitive with agents with oracle preferences (S4), despite beginning with no prior knowledge and learning preferences entirely through interaction across sessions. This result can be explained by the nature of the information about the user preferences in each setting. In S4, agents have few sentences that describe the user preferences. This information is the same as what the user receives. On the other hand, in S5, the agents have detailed notes about the preferences. This includes more user-specific information, such as the contexts in which they apply and specific strategies for how to best satisfy them. These findings demonstrate the effectiveness of leveraging memory and continually learning across sessions."
        },
        {
            "title": "D Training Hyperparameters",
            "content": "When training conversational agents to generate session-level reflections, we observed that models were prone to catastrophic forgetting and demonstrated degraded performance on other tasks such as response 19 generation. We also observed that models generated excessively long responses, known optimization bias in GRPO (Liu et al., 2025b), and also compounded by the tendency of LLM judges to favor longer outputs (Dubois et al., 2024). To mitigate such issues and preserve general capabilities, careful hyperparameter selection was necessary. We use LLaMa-Factory (Zheng et al., 2024) for SFT and VERL (Sheng et al., 2025) for GRPO. In Table 4, we summarize the hyperparameters that we used for both training stages. The hyperparameters that are not mentioned here are set to their default values."
        },
        {
            "title": "GRPO",
            "content": "Cutoff length Max prompt length Max response length Batch size Learning rate Epochs / Steps Rollouts KL coefficient 32768 - - 64 1 106 4 epochs - 2048 1024 64 1 106 200 steps 8 0.003 Table 4: Training hyperparameters for SFT GRPO."
        },
        {
            "title": "E Performance Across Sessions",
            "content": "Figure 4: Performance across sessions for Qwen-2.5-7B-Instruct (after GRPO). Each graph plots the delta between agents with memory and agents without memory across 20 sessions for Task Success (T ), User Effort (U ), and Conversation Length (Len ). Figure 5: Performance across sessions for Llama-3.1-8B-Instruct (after GRPO). Each graph plots the delta between agents with memory and agents without memory across 20 sessions for Task Success (T ), User Effort (U ), and Conversation Length (Len ). Figure 6: Performance across sessions for gpt-oss-20b. Each graph plots the delta between agents with memory and agents without memory across 20 sessions for Task Success (T ), and Conversation Length (Len ), User Effort (U ). 21 Real-World User Study In this section, we provide further details into our user study, which is designed to evaluate the intrinsic impact of our agent on real users multi-session collaboration experience, complimenting the evaluation we do in Section 5. This study was reviewed by our Institutional Review Board (IRB) and declared exempt. The study consisted of 19 volunteer participants recruited from the university, who had diverse computer science and electrical engineering backgrounds, including software engineers, undergraduate students, and graduate students. Real-world, long-term collaboration typically requires working across multiple problem types and often spans different domains. As result, human interaction preferences can range from highly domain-specific (e.g., variable names should be written in camelCase) to more abstract and domain-agnostic (e.g., always present the high-level reasoning before producing the concrete answer). The goal of our study design is to emulate these real-world characteristics. Preferences. For each study, participants are instructed to adopt set of fixed preferences that span four categories and three granularities: analytical intuitiveness (high-level), habitual strategies (mid-level), proactivity (low-level), and stylistic conventions (low-level). These categories parallel those used for our user simulator (Appendix A). Their specificity varies by study type: Single-domain (coding): Preferences are explicitly tied to programming practice. For example, starting with pseudocode, comparing algorithmic alternatives (e.g., recursion vs. dynamic programming), justifying library dependencies, and using camelCase. Mixed-domain (writing, math, and coding): We assign only the highand mid-level preferences and generalize them into domain-agnostic expectations: participants first receive high-level plan (e.g., an editing strategy or mathematical decomposition) and brief comparison of viable solution strategies before the agent produces detailed output. Participants must enforce these preferences across all three sessions and are also free to introduce any of their own preferences, enabling us to assess how well the agent can learn and adhere to preferences across sessions, and also how well it can generalize preferences to different scenarios. Session Problem Types. To capture the breadth of real-world collaboration, we curated set of problems spanning varying degrees of structure and designed to elicit all assigned preferences. In the single-domain, coding-only studies, participants solve debugging (mid-structured troubleshooting), implementation (well-structured rule-using), and object-oriented design problem (ill-structured design). In the mixeddomain studies, participants complete writing, math, and coding tasks that similarly vary in structure: adding plot-twist to paragraph (ill-structured with many plausible narrative directions); solving word problem (mid-structured logical), and code implementation. This variation allows us to examine whether the agent maintains consistent collaborative behaviors as domain and problem structure shift across sessions. Study Length & Agent Conditions. Each participant completes four studies, each consisting of three sequential sessions. The four studies correspond to (1) single-domain without memory, (2) single-domain with memory, (3) mixed-domain without memory, and (4) mixed-domain with memory. This design allows us to compare how memory affects preference retention and collaborative experience across domains and over time. After each session, participants complete post-session survey designed to measure the intrinsic aspects of collaboration with the agent. The survey includes four key dimensions: 1. Preference adherence. Participants rate the extent to which the agent adhered to the assigned and user-introduced preferences. This assesses real-time alignment and responsiveness. 22 2. Preference retention across sessions. Based on the current session, participants evaluate how well the agent remembered preferences from prior sessions. This dimension is critical for assessing long-term collaborative stability and the effectiveness of the memory mechanism. 3. Impact on problem-solving experience. Participants report whether the agents adherence (or lack thereof) positively or negatively influenced their collaborative experience. This measures subjective satisfaction and perceived usefulness beyond task correctness. 4. Confidence in future collaboration. Participants indicate how confident they are that the agent will continue to improve in remembering and upholding preferences over time. This captures trust-building and expectations about long-term reliability. Below are example screenshots of our user study interface: Figure 7: User study interface initial instructions. 23 Figure 8: User study interface problem-solving session example. 24 Figure 9: User study interface post session survey. We report the quantitative results from our human study in Table 5, which includes the average conversation lengths, preference adherence scores, preference memory scores, confidence scores, and overall satisfaction scores for each session, across all study types. We also analyze the performance across sessions by plotting the deltas across the sessions for each metric, the way we did in Section 6. Each delta denotes the difference between the agent with memory and the agent without memory at session for the respective metric. In Figure 10, we present the plots for each metric: conversation length (CL ), preference adherence (P ), and overall satisfaction(O ). Decreasing CL over the sessions indicate that the agent is learning to collaboration more effectively as interaction experience accumulates. ), preference memory (P , i , and increasing ), confidence (C , and , i i The results for our user study align with results from our simulated evaluations, while providing insights into the experience of real users when collaborating with our agents. Across both coding and mixed-domain settings, agents with memory demonstrate consistent trends of improvements relative to the agent without memory over the three sessions. CL decreases across sessions, indicating that conversations require fewer turns to reach point where the user is satisfied with the agents answer. In the first session, conversations required median of 8 turns regardless of whether or not they had memory. However, by the third session, agents with memory required only 6 turns (coding) and 4 turns (mixed), compared to 10 and 8 turns without memory. shows an increasing trend for agents with memory, where the first session for all settings has median score of 3 and the third session goes up to 5. Similarly we also see an increasing trend for , which starts out with median score of 1 for all settings, and increases all the way up to 5. Similarly, shows an increasing trend, indicating that the users are gaining more trust in the agent to be able to develop long-term, collaborative relationships. Finally, the users overall satisfaction, , increases throughout all the sessions for agents with memory. These trends generalize across both single-domain and mixed-domain studies, and across all types of preferences, demonstrating that memory enables agents to effectively improve their ability to user preferences, which benefits for real-world, long-term collaboration. We additionally conduct qualitative analysis of how memory evolves across sessions as well as participant feedback. Examining how memory developed across sessions, we observed that reflections became increasingly detailed and captured nuanced, user-specific information over time. Beyond the assigned preferences, the agent also captured preferences that participants naturally introduced during collaboration, most often related to readability and presentation style. Participants consistently described their experience with the agent equipped with memory as noticeably more personalized and effective compared to the standard agent without memory. Participants highlighted that the agent addressed their queries appropriately even when it was vague, and that it proactively adhered to their preferences. However, participants also identified limitations. First, some noted that responses felt less personalized in mixed-domain settings, suggesting that generalizing preferences across domains is relatively challenging. Second, participants observed the agents learned personalization, while beneficial, was less effective than when they explicitly restated their preferences within session. This indicates that the memory mechanism does not yet capture complete representation of user preferences from their interactions so far."
        },
        {
            "title": "Condition",
            "content": "Session 1 Session 2 Session"
        },
        {
            "title": "Overall Satisfaction",
            "content": "Coding, w/ memory Coding, w/o memory Mixed, w/ memory Mixed, w/o memory Coding, w/ memory Coding, w/o memory Mixed, w/ memory Mixed, w/o memory Coding, w/ memory Coding, w/o memory Mixed, w/ memory Mixed, w/o memory Coding, w/ memory Coding, w/o memory Mixed, w/ memory Mixed, w/o memory 7.953.78 / 8 9.684.33 / 8 7.683.67 / 6 8.683.20 / 8 3.000.47 / 3 2.950.62 / 3 2.740.65 / 3 2.790.79 / 1.371.01 / 1 1.160.69 / 1 1.210.63 / 1 1.110.46 / 1 2.741.10 / 3 3.051.13 / 3 3.001.00 / 3 2.740.93 / 3 Coding, w/ memory Coding, w/o memory Mixed, w/ memory Mixed, w/o memory 3.420.96 / 3 3.791.08 / 4 3.630.90 / 4 3.371.01 / 3 5.792.30 / 6 9.794.71 / 10 5.792.57 / 6 5.683.07 / 6 6.263.56 / 6 10.053.98 / 10 4.682.31 / 4 7.053.42 / 4.050.71 / 4 2.470.70 / 3 3.740.99 / 4 2.840.96 / 3 3.950.97 / 4 1.320.58 / 1 3.321.49 / 3 2.001.33 / 1 3.790.71 / 4 1.790.85 / 2 3.371.30 / 4 2.111.10 / 2 4.110.81 / 4 2.741.05 / 3 3.791.03 / 4 3.211.08 / 3 3.890.94 / 4 2.630.68 / 3 4.470.84 / 5 2.740.73 / 3 3.631.16 / 4 1.320.58 / 1 3.951.51 / 5 1.160.37 / 3.741.19 / 4 1.530.77 / 1 4.211.08 / 5 1.370.60 / 1 3.891.05 / 4 2.630.96 / 3 4.260.99 / 5 2.531.02 / 3 Table 5: Quantitative results from our user study on multi-session collaboration. We report the meanstandard deviation / median for conversation length and four survey metrics rated on 5-point Likert scale. 27 (a) Coding domain (b) Mixed domain Figure 10: Performance across sessions for our user study. Each graph plots the average delta between agents with memory and agents without memory across 3 sessions for Conversation Length (CL ), Preference Adherence ), Confidence (C (P i ), and Overall Satisfaction (O ). ), Preference Memory (P i"
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}