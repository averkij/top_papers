{
    "paper_title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "authors": [
        "Qiuyang Mang",
        "Wenhao Chai",
        "Zhifei Li",
        "Huanzhi Mao",
        "Shang Zhou",
        "Alexander Du",
        "Hanchen Li",
        "Shu Liu",
        "Edwin Chen",
        "Yichuan Wang",
        "Xieting Chu",
        "Zerui Cheng",
        "Yuan Xu",
        "Tian Xia",
        "Zirui Wang",
        "Tianneng Shi",
        "Jianzhu Yao",
        "Yilong Zhao",
        "Qizheng Zhang",
        "Charlie Ruan",
        "Zeyu Shen",
        "Kaiyuan Liu",
        "Runyuan He",
        "Dong Xing",
        "Zerui Li",
        "Zirong Zeng",
        "Yige Jiang",
        "Lufeng Cheng",
        "Ziyi Zhao",
        "Youran Sun",
        "Wesley Zheng",
        "Meiyuwang Zhang",
        "Ruyi Ji",
        "Xuechang Tu",
        "Zihan Zheng",
        "Zexing Chen",
        "Kangyang Zhou",
        "Zhaozi Wang",
        "Jingbang Chen",
        "Aleksandra Korolova",
        "Peter Henderson",
        "Pramod Viswanath",
        "Vijay Ganesh",
        "Saining Xie",
        "Zhuang Liu",
        "Dawn Song",
        "Sewon Min",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jingbo Shang",
        "Alvin Cheung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs."
        },
        {
            "title": "Start",
            "content": "2025-12-18 FrontierCS: Evolving Challenges for Evolving Intelligence Contributors ( equal contribution ) Qiuyang Mang1,, Wenhao Chai2,, Zhifei Li1,, Huanzhi Mao1,, Shang Zhou3,, Alexander Du1,4,, Hanchen Li1,, Shu Liu1,, Edwin Chen5, Yichuan Wang1, Xieting Chu6, Zerui Cheng2, Yuan Xu4, Tian Xia1, Zirui Wang1, Tianneng Shi1, Jianzhu Yao2, Yilong Zhao1, Qizheng Zhang7, Charlie Ruan1, Zeyu Shen2, Kaiyuan Liu8, Runyuan He1, Dong Xing4, Zerui Li4, Zirong Zeng1, Yige Jiang9, Lufeng Cheng10, Ziyi Zhao11, Youran Sun1, Wesley Zheng1, Meiyuwang Zhang5, Ruyi Ji12, Xuechang Tu6, Zihan Zheng13, Zexing Chen3, Kangyang Zhou14, Zhaozi Wang13, Jingbang Chen5 Advisors ( equal advising ) Aleksandra Korolova2, Peter Henderson2, Pramod Viswanath2, Vijay Ganesh6, Saining Xie13, Zhuang Liu2, Dawn Song1, Sewon Min1, Ion Stoica1, Joseph E. Gonzalez1,, Jingbo Shang3,, Alvin Cheung1, Affiliations 1UC Berkeley 7Stanford University 10University of Toronto 2Princeton University 3UCSD 4X-camp Academy 5Independent 6Georgia Tech 8University of Washington 11UIUC 12University of Michigan 9Nanyang Technological University 13New York University 14MIT Abstract: We introduce FrontierCS, benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs. Project page: www.frontier-cs.org Code and Data: Frontier-CS GitHub 5 2 0 2 7 ] . [ 1 9 9 6 5 1 . 2 1 5 2 : r a"
        },
        {
            "title": "FrontierCS",
            "content": "Figure 1: FrontierCS, an unsolved, open-ended, verifiable, and diverse benchmark for computer science tasks. The Polyomino Packing example, where both human experts and LLMs produce valid but non-optimal solutions that differ substantially in density. This reflects the benchmarks core design choice: problems are unsolved, admit many solution strategies, and are evaluated via deterministic scoring rather than pass-or-fail. 1."
        },
        {
            "title": "Introduction",
            "content": "The rapid progress of large language models (LLMs) is evident on numerous code and reasoning benchmarks [9, 16, 18, 22, 27, 30, 31, 36, 4143, 46, 4951, 54, 56], which largely comprise closedform tasks with single optimal answer and pass-or-fail criterion. Yet many frontier problems in computer science are intrinsically open-ended, requiring nuanced trade-offs among quality, efficiency, and robustness [6, 7, 17, 25, 32, 37]. While recent work has begun using LLMs to address unsolved or open-ended CS problems [1, 10, 33, 35, 38], these efforts naturally evaluate models only within their specific application domains or on small number of representative cases, leaving the field without comprehensive, cross-domain benchmark. In this paper, we introduce FrontierCS, coding benchmark that evaluates LLMs on solving openended computer science problems, where no known closed-form or deterministic optimal solution exists in practice. Unlike math or reasoning benchmarks that require direct answer (e.g., AIME), FrontierCS requires models to implement executable programs to solve the problem (e.g., LiveCodeBench). We focus on tasks where global optimum is either unknown or practically unattainable, yet any proposed solution can be deterministically checked for validity and assigned score by an automatic evaluator [21, 26, 47]. This design measures the ability of models to implement effective and efficient algorithms rather than algorithms that exhaustively solve the problem. FrontierCS includes optimization tasks for which no known optimal solver exists in practical time, each task includes an expert reference solution and deterministic automatic evaluator to facilitate objective comparisons and ensure reproducibility. Each FrontierCS task can be solved by submitting executable code: the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage). The model is prompted with the problem specification (and any required I/O or API stubs) and must produce self-contained solver program. As an example of the type of tasks included in FrontierCS, consider the Polyomino Packing task"
        },
        {
            "title": "FrontierCS",
            "content": "in Figure 1, packing set of polyominos (block shapes) into the smallest possible grid without overlapping to maximize density, i.e., the area fraction occupied by the polyominoes. Even though the optimal packing is unknown, solutions can still be compared objectively using packing density. The evaluator first runs the LLM-generated program to obtain packing arrangement, then checks whether the arrangement is valid, i.e., if any polyominoes overlap or extend beyond the grid boundaries. If valid, the score is computed as the packing density, defined as the total area covered by all placed polyominoes divided by the area of the grid. For this task, both the human expert and GPT-5 produce valid packings, but the human achieves 87% density while GPT-5 achieves only 47%. By varying n, this single specification yields infinitely many task instances, each with its own difficulty and unknown optimum. This example illustrates the kind of tasks targeted by FrontierCS: problems with many valid solutions whose quality spans continuous spectrum rather than simple pass-or-fail outcome. Formally, we define an open-ended optimization problem without known polynomial time solutions as one that satisfies the following: Unsolved or Intractable Optimum: The global optimum is unknown to compute over all problem instances, requiring progress to come from creative algorithms, heuristics, search, or optimization. Deterministically Verifiable and Quantitatively Scored: Solutions with runtime limit can be automatically checked for validity and assigned numeric score that reflects the quality of the solution, rather than simple pass-or-fail. Parametric Problem Generator: The task specification induces large, variable-difficulty space of instances, enabling fresh, unseen test cases to prevent leak and overfitting. FrontierCS focuses on two tracks: algorithmic optimization problems, and tasks more closely tied to real-world CS research. Both tracks naturally exhibit open-endedness, stress-testing models ability to perform deep open-ended reasoning and discover nontrivial optimization strategies. The design of FrontierCS encourages iterative improvement in an open-ended landscape rather than aiming for deterministic optimal solution, since none of its problems have known practical optima. FrontierCS, with its dynamic task scaling and objective quantitative feedback, offers an adaptive framework for continuous progress in LLM reasoning and creativity. Moreover, given that solutions are automatically verifiable and reward signals are available, FrontierCS is well-suited not only for evaluation but also for training and ablation studies. The scoring functions can effectively drive reinforcement learning or even self-play [8, 19, 29, 34, 44, 45, 53, 55, 57] as reward model. Empirically, we find that even the strongest frontier reasoning models remain far behind human experts on both the algorithmic and research tracks of FrontierCS. Simply scaling up context length or reasoning budgets yields diminishing returns on the hardest problems, and models frequently converge to locally workable but clearly suboptimal algorithms. These observations suggest that current large reasoning models are still missing key ingredients for truly open-ended computer-science problem solving, motivating FrontierCS as challenging benchmark for future progress. 2. Related Work Closed-form benchmarks for code and reasoning. wealth of benchmarks have evaluated LLMs on coding and math problems with known solutions. In the programming domain, tasks like HumanEval [9], MBPP [5], SWE-bench [24], BFCL [42], and LiveCodeBench [22] present coding challenges"
        },
        {
            "title": "FrontierCS",
            "content": "s b # 20 15 10 0 Research Problems (Total 49) 19 8 6 5 4 OS"
        },
        {
            "title": "Security",
            "content": "Algorithmic Problems (Total 107) 51 29 27 l P # 40"
        },
        {
            "title": "Constructive",
            "content": "Interactive Figure 2: Categories distribution of the 156 problems in FrontierCS. Left: Algorithmic Problems, covering Optimization tasks, Constructive tasks, and Interactive tasks, adapted from programming contests but rewritten into open-ended, partially scored variants. Right: Research Problems, spanning six major CS domains: OS (Operating Systems), HPC (High-Performance Computing), AI (Artificial Intelligence research tasks), DB (Databases), PL (Programming Languages), and Security (cybersecurity and vulnerability analysis). These research problems are sourced from real research workflows and each comes with deterministic evaluator. with unit tests as pass-or-fail criterion. These were useful for early code models, but top LLMs now approach saturation on them. Similarly, in math and logical reasoning, datasets such as MATH [28], GSM8K [11], and AIME were designed to test step-by-step reasoning. State-of-the-art models have achieved near-perfect scores on these high-school-level benchmarks, indicating that they no longer discriminate well at the frontier of ability. In the domain of mathematics, FrontierMath [20] collects hundreds of new, expert-crafted math challenges spanning areas from algebraic geometry to number theory, each designed to require substantial creativity and insight. Crucially, every problem has reference answer and an automated solution checker, allowing objective scoring even if the solution process is complex. Recently, LiveCodeBench Pro [54] has increased the difficulty of its competitive programming benchmark and updates it quarterly. On the hardest split, only few models are able to solve even single problem. Even so, LiveCodeBench Pro still focuses only on problems that have known optimal solutions, whereas in many real-world scenarios, finding the optimal solution is tied to NP-completeness. Open-ended and partially-scored benchmarks. Beyond strictly one-answer tasks, number of benchmarks embrace open-ended problem solving or partial-score evaluation. ALE-Bench [21] introduces optimization-style tasks from AtCoder Heuristic Contests, replacing binary correctness with score-based evaluation. Spanning routing, packing, scheduling, and stochastic search, it challenges models to design heuristics that achieve high objective scores under strict runtime constraints. UQ [37] curates set of unsolved tasks from collection of over 500 problems on forums like Stack Exchange, showing that even top models pass only around 15% after human validation. MLR-Bench [7] evaluates models on research tasks taken from top ML conferences, using an automated judge and agent. MLRBench finds that models are capable of writing coherent papers but still struggle with experimentation. Additionally, their automated judge aligns closely with human reviewers, revealing the possibility of automated evaluation for research tasks. NP-Engine [26] introduces benchmark and training framework for 10 classic NP-hard tasks, including Subset Sum, the Traveling Salesman Problem, and"
        },
        {
            "title": "FrontierCS",
            "content": "others. model trained on their framework achieves state-of-the-art performance on these tasks and shows strong out-of-domain generalization. KernelBench [41] evaluates LLMs on writing efficient GPU kernels for variety of computational tasks, using performance metrics like correctness and runtime for scoring. Existing benchmarks generally adopt one of three evaluation paradigms: binary pass-or-fail testing (e.g., LiveCodeBench), performanceor runtime-based scoring (e.g., KernelBench), or open-ended tasks that require human or LLM-as-judge evaluation (e.g., UQ). Although ALE-Bench and the AtCoder Heuristic Contests fall outside these categories by using optimization-style scoring, they still cover narrow slice of problem types, and their tasks are authored by only small number of recurring problem setters, limiting their diversity. FrontierCS addresses these gaps by providing large, diverse, and continually expanding collection of real-world reasoning problems sourced from multiple platforms. Each problem is unsolved yet automatically verifiable, enabling objective evaluation without relying on binary correctness or human judgment. Instead of checking only for optimality or execution performance, FrontierCS supports continuous scoring based on solution quality, allowing models to be credited for partial progress on research-style tasks where optimal solutions are unknown or computationally prohibitive. Together, these properties establish FrontierCS as new baseline for evaluating frontier model reasoning in open-ended, real-world problem settings. 3. Problem Collection As shown in Figure 2, FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems adapted from programming contests, covering three categories: Optimization, Constructive, and Interactive problems. The Research Problems track contains 49 problems sourced from real-world computer science research questions, spanning six domains: Operating Systems, High-Performance Computing, Artificial Intelligence, Databases, Programming Languages, and Security. Each problem is carefully curated through multi-stage process involving proposal, implementation, and review to ensure quality and relevance, which will be detailed in the following sections. Taxonomy. To organize the algorithmic problems in FrontierCS, we adopt taxonomy that reflects the dominant reasoning mode each problem requires. (1) Constructive problems center on synthesizing valid structured object (e.g., graph and packing) under global constraints; even when tasks ask for minimal or smallest solution, the difficulty lies in producing coherent structure rather than tuning explicit parameters (e.g., Problem 1, Problem 4 and Problem 5 in Section 6). (2) Optimization problems, in contrast, require explicit search over parameterized space to minimize or maximize quantitative objective, often involving accuracy-latency or cost-capacity trade-offs (e.g., Problem 2 in Section 6). (3) Interactive problems involve solving hidden-instance task through query-response protocol, where each action depends on previous replies, and performance typically depends not only on correctness but also on interaction efficiency such as minimizing the number of steps (e.g., Problem 3 and Problem 10 in Section 6). Together, these three categories capture the principal forms of algoritmic and open-ended reasoning that arise in real computer science tasks. For problems in the Research Problems track, we categorize them based on their respective computer science domains, including Operating Systems (OS), High-Performance Computing (HPC), Artificial Intelligence (AI), Databases (DB), Programming Languages (PL), and Security. Specifically, when"
        },
        {
            "title": "FrontierCS",
            "content": "research topic spans multiple domains, we assign it to the domain that the models solution is primarily meant to address. This domain-based taxonomy reflects the diverse challenges and methodologies inherent in different areas of computer science research. 3.1. Algorithmic Problems Our algorithmic problems largely originate from programming contests (e.g., Codeforces, AtCoder, ICPC, IOI) and classical CS problem settings (e.g., knapsack), where tasks are solved under time and memory limits and are automatically judged. However, most contest problems admit single optimal solution and are scored with binary pass-or-fail system, which does not meet our requirement for open-ended problems without known optimum. To address this mismatch, we introduce structured curation pipeline, i.e., Proposal, Implementation, and Review, to construct genuinely variants. Proposal. This stage is led by experts with qualifications equivalent to ICPC World Finalists, who are responsible for submitting candidate problems, complete with links to their original sources and description of intended modifications. The proposal is reviewed by experts against: (i) openness and multiplicity of viable solutions; (ii) discriminative strength of the scoring scheme; (iii) clarity and completeness of the problem statement and data ranges. Implementation. This stage: (i) converts each problem into an open-ended variant by changing single-optimum objectives and introducing partial scoring system; (ii) standardizes input/output formats or, when appropriate, provides an interaction library; (iii) implements deterministic verifier to formally assess the validity of candidate solutions; and (iv) delivers human-written reference solution with significant advantage over the best LLMs; (v) provides the necessary configuration and test data for evaluation. Review. After implementation, each problem is reviewed by another algorithmic-problem expert to make sure that: (1) the problem has no known optimal solution or nearly optimal solution that leaves limited room for improvement; (2) the scoring policy is objective and can meaningfully reflect progress; (3) the human reference solution is significantly stronger than the best models performance; and (4) the evaluator is implemented correctly and the test data can comprehensively evaluate solutions. In total, FrontierCS contains 107 high-quality algorithmic problems adapted from programming contests, including 29 optimization problems, 27 constructive problems and 51 interactive problems. Each problem contains: well-defined statement with formal constraints; generators for test cases; deterministic verifier with partial scoring system; an expert-authored reference solution; suite of baseline implementations; reproducible model evaluation harness with associated scripts and metrics; and thorough documentation on provenance and data decontamination. Scoring policy design. We primarily evaluate solutions using problem-specific quality metrics such as cost, density, or number of queries, rather than computational efficiency. This reflects the openended nature of our tasks, where the main challenge is designing effective algorithms or strategies. Like traditional competitive programming tasks, each problem includes strict time and memory limits, and any solution that exceeds these limits is considered invalid and receives no score. In this way, runtime and memory act as feasibility constraints rather than components of the scoring metric. Unless explicitly specified otherwise, runtime is never part of the scoring, and together with the resource"
        },
        {
            "title": "FrontierCS",
            "content": "Figure 3: Evaluation pipeline of FrontierCS research problem using SkyPilot [52] limits, this ensures that higher scores reflect genuinely better strategies rather than solutions that rely on excessive compute. In programming contests such as ICPC and IOI, subtasks On contest subtasks vs. our scoring. partition tests into groups with extra constraints; points are awarded only if all cases in subtask pass, yielding discrete, all-or-nothing partial credit. Moreover, these subtasks are typically not strongly related to the essence of the problem, such as simplifying the problems constraints or addressing the input data size. Our benchmark instead uses task-specific, objective metrics (e.g., density, cost, number of queries) with continuous or piecewise-continuous mappings to scores, measured relative to trivial baseline and strong human reference. This design rewards incremental improvements and enables fair comparison even when no single optimal solution is known. 3.2. Research Problems Proposal. For FrontierCS research problems, we ask CS PhD students to set problems based on their unsolved research questions and to implement the evaluation container. Some of the research problems we select are drawn from recent AI-Driven Research for Systems (ADRS) work [10], including multi-region spot-instance scheduling, cloud transfer path optimization, and LLM-guided SQL query reordering. Similarly to the algorithmic problems, we use process of proposal and implementation, with review stages in between. Implementation. Unlike problems sourced from programming contests that can be judged in unified framework, each research problem needs specialized environment. This stage ensures full reproducibility and objective evaluation. Candidate research problems contains resources, set up env.sh, evaluate.sh, and README file. The README describes the problem statement, VM or Docker requirements, the data layout prepared by set up env.sh, and the exact input/output contract of the participants solution. The environment can be utilized by the LLM or agent to create its solution for the problem. Examples include training set for image classification problems. Last but not least, the problem must support automated scoring via evaluate.sh, which gives standardized score between 0 and 100. The evaluation has to be"
        },
        {
            "title": "FrontierCS",
            "content": "deterministic and must not use an LLM as judge. An overview of the architecture of the research evaluation platform is shown in Figure 3. Infrastructure. To facilitate extensive experimentation, we integrate SkyPilot [52] to manage the compute infrastructure. This abstraction allows the evaluation to scale up seamlessly from single node to distributed cloud clusters, handling the heterogeneous hardware requirements of our research track. Crucially, it optimizes for economic viability by leveraging spot instances and region arbitrage, enabling researchers to evaluate agents on the full benchmark at minimal costs. Review. Each submission is reviewed by CS researchers according to the following criteria: (i) the problem should not have single optimal solution; (ii) partial scoring system should meaningfully reflect progress; (iii) all scripts must run unattended in fresh VM; (iv) the environment must be deterministic, isolated, and free from external dependencies. Scoring policy design. Unlike algorithmic problems, which are scored primarily on solution quality, research problems often involve multiple objectives such as accuracy, latency, memory usage, and cost due to their real-world nature. For example, task may require designing vector database index that minimizes query latency while maintaining at least 95% accuracy (Problem 7 in Section 6). Nevertheless, we still impose strict resource limits for each problem to prevent excessive computation, and any solution exceeding the limit is considered invalid and receives no score. Variants. For each research problem, we provide multiple variants with different resource constraints and objective targets to reflect real-world research scenarios. For example, variant may impose stricter memory limit but turning off accuracy requirement, changing the hardware setting from CPU to GPU, or adjusting the objective from latency minimization to throughput maximization. Note that, we treat every variant as an independent problem in the reported totals, because differing resource constraints or objectives lead to distinct solution strategies and reflect how related tasks are separately evaluated in real computer science research practice. In total, FrontierCS contains 49 research problems across diverse domains such as symbolic regression, vector database design, and kernel optimization. Each accepted research problem includes: (i) detailed problem description and background motivation; (ii) reproducible environment with pinned dependencies or Docker image; (iii) deterministic evaluator with partial scoring and diagnostics; (iv) an expert-authored reference solution; (v) the most trivial baseline implementations for comparison. 3.3. Update Policy core design principle of FrontierCS is to enable measurable progress on open-ended tasks. Rather than using binary judgments, our evaluators provide score of 0 100 using human performance as the baseline, allowing for measurable feedback on incremental improvements. FrontierCS is designed to remain relevant as models improve by supporting three complementary forms of task evolution. (1) Adding new tasks. When expanding the scope of the benchmark or introducing fundamentally new problem categories, we may add new tasks. This is the traditional route for extending benchmark, but it is not the only mechanism FrontierCS provides."
        },
        {
            "title": "FrontierCS",
            "content": "(2) Increasing the difficulty of existing tasks without changing the problem statement. key feature of FrontierCS is the separation between the written problem description and the environment in which the task is instantiated. This decoupling allows us to preserve the original question text while making the task more challenging. Difficulty can be increased by tightening constraints (e.g., time or memory budgets, feasibility requirements), modifying workloads or datasets (e.g., larger or more adversarial instances), or adjusting optimization objectives (e.g., stricter accuracy or performance targets). These updates retain task continuity while ensuring that the benchmark keeps pace with advancing model capabilities. (3) Refining human reference solutions and evaluation thresholds. When models approach or surpass strong human baselines, we can refine the human reference solution, scoring rubric, or evaluation thresholds to provide finer-grained separation between capable models. This method increases difficulty without modifying either the task description or the environment, enabling continued measurable progress. Taken together, these three mechanisms provide flexible, scalable, and principled update policy that allows FrontierCS to evolve over time without repeatedly constructing entirely new problem descriptions, while still preserving comparability across versions. 4. Evaluation Results In this section, we report the metric design and the performance of the frontier models on FrontierCS. 4.1. Setup In our evaluation, we tested 9 frontier models: GPT 5 Thinking [40], Gemini 2.5 Pro [13], Gemini 3.0 Pro [14], Grok 4 [48], Claude Opus 4.1 [2], Claude Sonnet 4.5 [4], Claude Opus 4.5 [3], DeepSeek V3.2 Thinking [15]. We compared their performance against human experts on both the algorithmic problems track and the research problems track. We set time-out for each LLM request as 20 minutes. The other configurations for the models are: GPT-5 / GPT-5.1 Thinking, Grok 4, Claude Opus 4.5: set reasoning effort = high. Claude Opus 4.1 / Sonnet 4.5: set max tokens = 32,000 and reasoning budget = 20,000. Gemini 2.5 Pro / Gemini 3.0 Pro: set thinking budget = -1. During evaluation, models are tested in single-round setting: once they produce solution, that output is final. They do not have the opportunity to run their code, inspect unit-test results, or iterate based on feedback. They also do not have access to code editor, Python environment, or any external tools. All inputs are provided purely as text, i.e., no diagrams or visual components are involved and each problem can be fully described in text alone. We leave the evaluation of multi-round, agentic, tool-call-assisted, and evolve-style frameworks to future work."
        },
        {
            "title": "FrontierCS",
            "content": "4.2. Metrics In FrontierCS, no problem has universally correct solution, so we cannot use accuracy as an evaluation metric. Instead, we introduce grading system that scores each solution relative to two reference points: the reference solution written by human experts and trivial baseline solution. Meanwhile, for some problems that have nontrivial bounds on achievable performance, we also use those bounds for reference. detailed example can be found in Section 6. Specifically, for test case, if solution fails to reach the level of the trivial baseline, it receives score of zero; if it surpasses the threshold defined by human expert reference solution or the nontrivial bound, it receives full score. Scores between these two extremes are assigned based on the nature of the problem, ensuring that each task has its own fair and tailored grading system. In all evaluations, we do not use any agentic framework and require the model to directly output code. However, we still encourage submissions that utilize agentic frameworks for comparison and future extensions. We report both Score@1, Avg@5, and Score@5, as we observe that the model exhibits stochasticity and notable improvement across multiple attempts. Here, we define Score@k as the maximum score among the model trials, and Avg@k as the average score among those trials. We also report Pass@1 and Pass@5: the fraction of problems where the model achieves non-zero score, i.e., produces valid solution that exceeds the the most trivial baseline. 4.3. Algorithmic Problems Model Score@1 Avg@5 Score@5 Pass@1 Pass@5 Human Experts Gemini 3.0 Pro Claude Opus 4.5 Grok 4 GPT 5 Thinking GPT 5.1 Thinking DeepSeek 3.2 Gemini 2.5 Pro Claude Opus 4.1 Claude Sonnet 4.5 95.41 29.37 14.95 13.67 10.87 11.80 12.65 12.53 6.91 5.84 - 29.51 13.83 13.25 12.03 11.90 11.80 11.13 5.84 6.72 - 52.06 25.83 26.42 24.35 21.59 24.20 24.60 13.31 15. - - 65.42% 83.18% 54.21% 69.16% 14.95% 46.73% 38.32% 63.55% 34.58% 54.21% 46.73% 69.16% 38.32% 64.49% 30.84% 48.60% 28.04% 55.14% Table 1: Algorithmic problems results. We define score@k as the highest score achieved across runs, and avg@k as the average score over those runs. Pass@k denotes the proportion of runs whose results exceed the most trivial baseline, not exceed human expert solution. As shown in Table 1, frontier models still lag significantly behind human experts on the algorithmic problems track, where the 8 frontier models achieve Score@1 of 10.87 (GPT 5 Thinking), 11.80 (GPT 5.1 Thinking), 12.53 (Gemini 2.5 Pro), 29.37 (Gemini 3.0 Pro), 13.67 (Grok 4), 14.95 (Claude Opus 4.5), 6.91 (Claude Opus 4.1), and 5.84 (Claude sonnet 4.5) respectively, compared to human experts 95.41. These results highlight the substantial gap that remains between current AI capabilities and human expertise in tackling open-ended algorithmic challenges. As expected, we also observe that all 8 models demonstrate stronger performance when increasing sampling attempts from one to five,"
        },
        {
            "title": "FrontierCS",
            "content": "indicating that they can generate diverse solutions and benefit from multiple trials. For instance, Score@5 improves over Score@1 by 6.40 22.69 points across models. However, they still lag far behind human experts on challenging open-ended problems. 4.4. Research Problems"
        },
        {
            "title": "Model",
            "content": "Score@1 Avg@5 Score@5 Pass@1 Pass@5 Claude Opus 4.5 GPT 5.1 Thinking Gemini 3.0 Pro GPT 5 Thinking Claude Sonnet 4.5 Gemini 2.5 Pro Grok 4 Fast Claude Opus 4.1 DeepSeek 3.2 29.40 28.39 26.95 23.74 24.65 21.04 18.01 13.10 14.65 32.31 29.32 29.18 27.12 25.64 21.50 13.60 13.23 10.89 44.47 47.21 47.20 44.78 38.34 39.07 28.81 30.14 21. 57.14% 75.51% 57.14% 83.67% 57.14% 81.63% 46.94% 77.55% 57.14% 71.43% 40.82% 73.47% 36.73% 55.10% 32.65% 63.27% 26.53% 42.86% Table 2: Research problems results. We define each metric as the same as in Table 1. For the research track, Table 2 reports Score@1, Avg@5, Score@5, Pass@1, and Pass@5 across 8 frontier models. Claude Opus 4.5 attains the best one-shot results (Score@1 29.40), while GPT 5.1 Thinking leads under multi-sample evaluation (Score@5 47.21). Across models, additional sampling yields sizable gains, i.e., Score@5 improves over Score@1 by 6.72 21.04 points. Pass rates remain substantially higher than raw scores, indicating that models often produce workable but under-optimized solutions, mirroring the trend observed on the algorithmic track. 5. Discussion 5.1."
        },
        {
            "title": "Improving Reasoning Effort Does Not Yield Further Gains",
            "content": "We further analyze the relationship between reasoning effort and model performance on open-ended algorithmic problems. As shown in Figure 4, we plot GPT 5 Thinkings score against the number of reasoning tokens it consumes per attempt by setting different reasoning effort levels, i.e., low, medium, and high. As expected, we observe clear positive correlation between reasoning effort when comparing low and medium reasoning levels. However, increasing the reasoning effort from medium to high does not yield further gains; in fact, performance drops from 15.336 to 12.626, suggesting diminishing returns at higher reasoning budgets. This indicates that while increased reasoning effort generally aids performance on open-ended problems, there may be an upper limit beyond which additional effort yields limited benefits for current LLMs. Future work could explore more effective ways to leverage high reasoning effort for complex open-ended problem solving."
        },
        {
            "title": "FrontierCS",
            "content": "Effort Avg tokens Avg score GPT 5 low GPT 5 medium GPT 5 high GPT 5.1 high 4,389 11,554 19,763 20,402 7.903 15.336 12.626 12.508 Figure 4: Reasoning tokens vs. Score: We setting GPT 5 Thinking with different reasoning efforts. Top: scatter plot; Each point is one attempt (3 attempts per problem). Marks in the middle denote group averages. Bottom: average tokens and scores by reasoning effort. 5.2. Misleading Micro-Optimization Trap During evaluation, we identify recurring failure pattern in LLM behavior: the model often fixates on small, low-impact optimizations while overlooking the core algorithmic choices required for substantial performance gains. This is especially evident in Polyomino Packing (Problem 5 in Section 6), which asks models to pack numerous polyominoes into minimum-area rectangle and output list of transformations for each piece. In practice, GPT 5-Thinking frequently adopts this transformation list as its internal data structure. Although memory-efficient and aligned with the final output format, this choice is conceptual pitfall. Relying solely on transformation lists renders overlap detection and free-space search both cumbersome and error-prone. Consequently, the model produces invalid code in about 30% of attempts, and in the remaining 70% achieves only low scores, i.e., 20 70 due to ineffective search strategies. In contrast, minimal prompt adjustment dramatically changes the outcome. Adding single instruction like Please use 2D array to maintain the rectangle state, and convert to the required format only at the end reliably shifts the model toward structurally sound internal representation. With this modification, the zero-score rate drops to about 10%, and in nearly 80% of cases the model successfully implements an efficient search strategy, achieving high scores in the 80 85 range, consistently surpassing prior best solutions. This case study highlights fundamental limitation of current LLMs: they do not inherently recognize which optimizations are algorithmically meaningful, often becoming trapped in superficially appealing but strategically irrelevant micro-optimizations."
        },
        {
            "title": "FrontierCS",
            "content": "5.3. The ResearchEngineering Dilemma of Claude In Table 1 and Table 2, we observe clear disparity in Claude models scores, whereas the other models perform similarly across both tracks. After examining its submissions, we find that Claude Sonnet 4.5 often produces workable solutions that generate basic outputs within resource limits and avoid compilation or runtime errors, yet these solutions frequently lack the optimization strategies needed for competitive performance. Consequently, most of its workable solutions fall below the trivial baseline in the algorithmic track and thus get zero score, leading to lower overall score compared to models that generate fewer but higher-quality solutions. In contrast, research problems require not only problem-specific optimization but also system-level reasoning and awareness of real research project environments; under these broader requirements, Claude Opus 4.5s workable solutions achieve relatively strong performance. For example, in Symbolic Regression Problems (Problem 6 in Section 6), workable solutions must correctly invoke and configure the PySR package [12] to search over expression spaces, tune evolutionary parameters, and validate candidate formulas against the provided datasets. Although its optimization is limited, these workable solutions still earn around 50 points. Interestingly, the high rate of workable solutions also aligns with Claude models state-of-the-art performance on SWE-bench Verified [39]. This suggests that models tuned for closed-form software engineering tasks may still struggle to produce high-performance solutions for open-ended algorithmic problems. Overall, these results highlight that (1) merely producing workable solutions is insufficient for open-ended tasks, especially for algorithmic problems, and (2) success on research problems requires both engineering skills (e.g., knowing how to use existing research tools) to produce workable solutions and the ability to effectively optimize them."
        },
        {
            "title": "FrontierCS",
            "content": "6. Example Problems Example (1) The first problem is adapted from the International Olympiad in Informatics (IOI) 2025, where FrontierCSs adaptation uses an open-ended grading policy to encourage more compact solutions, i.e., smaller grid sizes. The problem statement is as follows: Problem 1: World Map Problem Description. You are given countries and set of unordered pairs indicating which countries must be adjacent. Construct grid (for some integer K) that assigns each cell country label from {1, . . . , N} such that: 1. For every {a, b} E, there exists at least one pair of orthogonally adjacent cells labeled and b; 2. Whenever two orthogonally adjacent cells have distinct labels and b, then {a, b} E. Adjacency is by shared edge only (no diagonals). The goal is to minimize K. Grade Policy. Otherwise, if the output grid has size K, let = expert solutions. The score is computed as If the output grid does not satisfy the adjacency requirements, the score is 0. , and let be the same ratio from human score = 100 clamp (cid:18) 6 6 , 0, (cid:19) For this problem, the only known solution achieves = 1.5 across all possible inputs from an IOI 2025 submission. It remains unclear whether better solutions exist or what the optimal ratio is for different patterns of adjacency constraints. Nevertheless, solutions can still be objectively graded by their achieved ratios, and solution validity can be easily verified by checking each adjacency requirement. Here, we provide visualized solutions generated by GPT 5 and human experts for comparison, as shown in Figure 5. The illustrated input test case has = 5 countries and = 6 adjacency requirements. In this example, the LLMs solution results in significantly larger grid size (K = 15) compared to the human experts compact solution (K = 7). 1 5 4 3 (a) Adjacency graph E. (b) Human expert (K = 7). (c) GPT 5 (K = 15). Figure 5: Problem 1: Inputoutput illustration. (a) The input adjacency graph for = 5; (b) and (c) show valid outputs from human experts algorithm and GPT 5 generated algorithm with respective grid sizes K."
        },
        {
            "title": "FrontierCS",
            "content": "Example (2) The second problem, variant of the knapsack problem, is adapted from the 2024 ICPC North America Championship (NAC) NSA Challenge. The problem statement is as follows: Problem 2: Treasure Packing Problem Description. You are given = 12 treasure categories. Each category has per-item value vc, mass mc, volume ℓc, and an upper bound qc on how many you may take. The bag has two capacities: mass and volume L. Choose nonnegative integer counts xc qc to maximize the total value vcxc while keeping total mass mcxc at most and total volume ℓcxc at most L. Formally, you need to solve the following optimization problem: c=1 max xZC 0 vcxc s.t. c=1 mcxc M, c=1 ℓcxc L, 0 xc qc . For all test cases, 1 qc 104, 1 vc 106, 1 mc 20 106, and 1 ℓc 25 106. Grade Policy. The solution must return valid result within 1-second time limit and 1024 MB memory limit. If the output is invalid or exceeds resource limits, the score is 0. Otherwise, the score is computed as score = 100 clamp (cid:18) value value base value ref value base (cid:19) , 0, 1 , where value is the total value of the submitted solution, value base is the value of baseline solution provided, and value ref is the value of reference solution from human experts. Currently, the baseline solution is greedy naive algorithm, and the reference solution is enhanced from the champion solution of the challenge. Note that within the time and memory limits, exact solutions are generally infeasible for large test cases. However, approximate solutions can still be objectively graded based on their total value, allowing for open-ended algorithm discovery and improvement. In this problem, the strategies used by the human expert and the LLM (GPT 5) are as follows. The human expert solution uses combination of greedy and randomized selection that incrementally improves its result over the entire time limit. The LLM solution tries something similar, using an initial greedy pass augmented by branch-andbound algorithm that recursively explores and then fixes item counts. The LLM obtains points on every test case for total score of 74 points, which is good but still far from the human score of 100."
        },
        {
            "title": "FrontierCS",
            "content": "Example (3) FrontierCS also includes interactive problems that require adaptive querying. Problem 3: Permutation Guess Problem Description. There is hidden permutation π of [n] = {1, 2, . . . , n} with = 1000. Your task is to identify π using the fewest possible queries. The problem is interactive: in each query, you submit length-n integer sequence = (a1, . . . , an) with each ai [1..n]. The judge returns single integer (a) = (cid:12) (cid:12){ 1 : ai = πi }(cid:12) (cid:12), i.e., the number of positions where your sequence matches the hidden permutation exactly. Interaction. Query: submit any integer sequence of length n, [1..n]n. Response: deterministic integer (a) {0, 1, . . . , n}. Goal: after some number of queries, output final sequence ˆπ; the submission is accepted if ˆπ = π, otherwise it is rejected. Grade Policy. The score is based on the number of queries used to correctly identify π. If the submission is rejected, the score is 0. Otherwise, the score is computed as score = 100 clamp (cid:18) Qbase Qbase Qref (cid:19) , 0, 1 , where Qbase is the number of queries used by naive binary search strategy (approximately 10,000), and Qref is the number of queries used by divide-and-conquer solution from human experts (approximately 6,000). In this problem, the LLM strategy contains redundant steps and is highly inefficient compared to the human expert, e.g., 12 steps vs. 5 steps for the instance shown in Figure 6. Step Query (a) Inference Step Query (a) Inference 1 2 3 4 5 1122 3344 1411 submit 2 0 2 0 NA 1 {1, 2}, 2 {3, 4} 3 {1, 2}, 4 {3, 4} 1 at pos 2, 4 at pos 3 2 at pos 1, 3 at pos 4 answer: π = 1432 1 2 3 4 5 6 7 8 9 10 11 12 2322 3222 2233 3233 3232 3233 3233 3332 3222 2322 4222 submit 1 1 1 1 2 1 1 2 1 1 1 NA 2 {2, 4}, 3 {1, 3} 2 at pos 4 3 not at pos 1 3 not at pos 2, at pos 3 4 not at pos 1, at pos 2 answer: π = 1432 (a) Human expert: divide-and-conquer. (b) LLM-generated solution (GPT 5). Figure 6: Problem 3, toy instance (n = 4; hidden π = 1432): side-by-side strategies with their query transcripts."
        },
        {
            "title": "FrontierCS",
            "content": "Example (4) Outside of more traditional competition problems, FrontierCS also includes classical open problems that require incremental optimization. Problem 4: Square Packing Problem Description. Given an integer 1 10,000, you need to place unit squares inside an axis-aligned square container of side length such that: 1. Every unit square lies entirely inside the container, squares can be rotated by an arbitrary angle. 2. Any pair of unit squares share no common interior points, corners or edges touching is allowed. Your goal is to give valid output while minimizing L. Grade Policy. The score is based on the size of the square container the solution code outputs. If the packing is invalid, the score is 0. Otherwise, supposing the solution has size L, we define LB = as the naive upper bound. We also define the size of reference solution s(n). The final score is computed as as the absolute lower bound, and L0 = Score = 100 95 + 5 min (cid:16) (cid:16) 94 min 0 (cid:17) 1.0, 1.1 sL sLB (cid:17) 1.0, 1.1 L0L L0s + 1 if = LB if LB < if < < L0 if L0 For this problem, best human solutions are available for 100 (some proven optimal). We set the human benchmark to 95 points to leave space for better solutions; validity is easy to check. For 100, we set s(n) as the best human solution to date. For > 100, we recursively define s(n) = 2 s(n/4). This is based on the divide-and-conquer strategy of packing four existing solutions of size n/4 into one square container. Figure 7 shows the = 10 case. The human solution achieves minimal with valid packing; the LLM uses naive packing with = n. Note that for > 10, most of the minimal is still open and unknown to human. (a) Human expert (L = 3.707) (b) Gemini 2.5 Pro (L = 4) Figure 7: Problem 4: Shows valid outputs for = 10 square packing from human expert and Gemini 2.5 Pro generated solution with respective square size"
        },
        {
            "title": "FrontierCS",
            "content": "Example (5) more challenging packing problem is Polyomino Packing (Figure 1) Problem 5: Polyomino Packing Problem Description. Given an integer n, the instance contains every distinct polyomino shape of size for all = 1, . . . , n. You need to place all these polyominoes into grid using the following allowed moves for each piece: integer translation ti Z2, rotation by 0/90/180/270 (Ri {0, 1, 2, 3}), optional mirror across the y-axis (Fi {0, 1}). Valid placement: all transformed cells lie inside grid, no two pieces occupy the same grid cell (edge/corner touching is allowed). Goal: minimize area (equivalently, maximize density ρ = packed cells /(W H)). Figure 8: Examples of polyominoes. Grade Policy. Validity: any out-of-bounds cell, overlap, or disallowed move gives score 0. Scoring: let ρ be your density. Score scales linearly from 0 at the baseline density ρbase to at the reference density ρref (both provided per test set; ρref 1). Figure 9 shows sample instance. The human expert achieves noticeably tighter packing (higher density), while GPT 5 leaves more whitespace between pieces. (a) Human expert: 87% density. (b) GPT 5-Thinking: 47% density. Figure 9: Problem 5: Polyomino Packing valid outputs from human expert and GPT 5 for the same instance, illustrating density differences."
        },
        {
            "title": "FrontierCS",
            "content": "Example (6) FrontierCS also includes research problems such as symbolic regression. Problem 6: Symbolic Regression Problem Description. Given supervised dataset with features x1, . . . , xd and target y, find closed-form expression (x1, . . . , xd) that predicts while keeping as simple as possible. The allowed grammar is: S+S SS SS S/S exp(S) log(S) sin(S) cos(S) (S) xj, {1, . . . , d}. Expressions must evaluate to finite real numbers on all rows of (no division by zero, log() > 0, etc.); otherwise the submission is invalid and receives score 0. Complexity. The expression complexity is defined as = 2 (# binary ops) + 1 (# unary ops), which corresponds to the node count of the expression tree. Grade Policy. For feature matrix Rnd and target Rn, let ˆy = (X) and MSE = 1 i=1 (yi ˆyi)2. Let mbase be the MSE of the best linear predictor (OLS) on (X, y), and mref the MSE of the provided reference expression. The score is Score = 100 clamp (cid:18) mbase MSE mbase mref (cid:19) , 0, 1 0.99max(CCref, 0), where is the complexity of and Cref that of the reference. If mbase = mref, set the score to 100 when MSE mref and 0 otherwise. Figure 10 shows the symbolic regression task for the McCormick function. Figure 10a shows the direct plot of the data. The human expert, with the help of symbolic regression tools, obtains the function shown in Figure 10b with complexity 12, and GPT 5 produces the result in Figure 10c with complexity 19. (a) Data plot. (b) Human expert (complexity 12). (c) GPT 5 (complexity 19). Figure 10: Problem 6: Symbolic Regression data plot compare with functions discovered by human expert and GPT 5, with noted expression complexities."
        },
        {
            "title": "FrontierCS",
            "content": "Example (7) Another problem included in FrontierCS is vector database system design. Problem 7: Vector Database Design RecallLatency Tradeoff Problem Description. You need to build an approximate nearest-neighbor (ANN) index and evaluate on SIFT1M [23] with 1M base vectors (128D), 10K queries (128D), L2 metric, k=1. Report Recall@1 and average per-query latency in ms (search only). End-to-end run (build+search) must finish within 10 hours. Grade Policy. Validity requirements: 1. Implements the required API and runs within 10 hours; 2. Returns finite distances and valid integer indices; 3. Meets rmin and tthr; otherwise score = 0. Scoring (if tthr): Score = 100 clamp (cid:16) rmin rbase rmin (cid:17) . , 0, Here rmin is the minimum acceptable recall and rbase is the reference recall of strong humandesigned index under the same latency budget tthr. In practice, vector databases rely on approximate nearest neighbor (ANN) search, which inherently induces latency-accuracy tradeoff: longer search budgets typically yield higher recall. In this benchmark, we construct several task variants that explicitly reflect this property, e.g., Recall80, where the goal is to minimize latency while achieving at least 80% recall. Figure 11 compares GPT 5 Thinking with human experts across multiple such variants. As shown, GPT 5 Thinking substantially underperforms humans in these tasks. Human results are obtained by tuning standard ANN index parameters in classic structures, e.g., adjusting nprobe in IVF or efSearch in HNSW. RecallLatency Tradeoff on SIFT1M (k = 1) ) % ( 1 @ c 100 95 90 80 Human Pareto Frontier recall95 recall80 0.01 0. Better recall95 balanced recall80 0.1 1 Average Query Latency (ms) 0.5 Human Experts GPT-5 Thinking 2 5 Figure 11: Problem 7: Performance comparison of human expert and GPT 5-thinking in multiple VectorDB designing variants (k = 1)."
        },
        {
            "title": "FrontierCS",
            "content": "Example (8) FrontierCS also includes research problems in cybersecurity. Problem 8: Minimal PoC Generation Problem Description. Given codebase and target vulnerability description, generate proof-of-concept (PoC) test to reproduce the vulnerability. Make the PoC as short as possible. Input. Each problem provides the following data: The entire pre-patch codebase of real-world open-source project. The description of the target vulnerability. Because the entire codebase may be too large for the current LLM context windows, we ask the LLM to generate program that can process the codebase using the provided description. In addition, solution may incorporate the LLM into an agentic workflow: it incrementally identifies relevant files, extracts the necessary inputs, and generates the target PoC. Grade Policy. The submitted PoC must successfully trigger the target vulnerability by meeting the following criteria: It triggers sanitizer crash in the pre-patch codebase; It does not produce any sanitizer crash in the post-patch codebase; The correctness of the PoC is evaluated using the CyberGym [47]. Scoring: Score = (cid:40) Lg , 60 + 40 2 0, it triggers the target vulnerability, it cannot trigger the target vulnerability, where denotes the length of the submitted PoC, and Lg represents the length of the groundtruth PoC. Figure 12 shows valid PoC inputs that trigger heap-use-after-free vulnerability in the PHP interpreter. fuzzing program written by human expert produces 79-byte PoC, whereas GPT 5 generates longer PoC of 577 bytes. ff24 2201 6179 6972 6261 656c 2022 3c3d 703f 7068 6620 726f 2820 8024 ffff 3bff 2424 2e68 243d 6824 3d2f 6924 322d 2e30 3b30 6924 2b2b 2e29 3730 242d dfdf dfe0 695f 3f31 0a3e 9e3f 0a24 3053 0a0a 003f 3f3c 6870 0a70 2f2f 4620 726f 6563 6520 7261 796c 642d 7365 7274 6375 6974 6e6f 5520 4641 7620 6169 6320 6d6f 6f70 6e75 2064 6964 6976 6564 612d 7373 6769 206e 7962 7a20 7265 2e6f 2f0a 202f 484c 2053 ... 200a 2020 6520 6863 206f 7573 7362 7274 2428 2c62 3020 202c 2931 0a3b 0a7d 3e3f 000a (a) Human expert (79 bytes, hexadecimal dump). (b) GPT 5 (577 bytes, hexadecimal dump). Figure 12: Problem 8: Minimal PoC Generation - valid PoC of heap-use-after-free vulnerability."
        },
        {
            "title": "FrontierCS",
            "content": "Example (9) FrontierCS also includes kernel code optimization problems that mirror real-world systems workflows. There are two types of problems: (A) rewrite PyTorch fused implementation into Triton kernel; (B) further optimize the Triton kernel using warp specialization. Problem 9: Kernel Rewrite & Warp Specialization for GDPA Problem Description. Given query (Q), key (K), value (V) tensors and per-element gates (GQ, GK), compute gated dot-product attention α = 1/ d, = σ(GK), = head dimension, = σ(GQ), = α K, = V. = softmax(S) (row-wise over keys), Shapes follow Transformer convention. For batch size B, heads H, query length M, key/value length N, head dim d: Q, K, RBH()d, GQ RBHMd, GK RBHNd, RBHMd. Inputs are float16; accumulations must be in float32; outputs are cast back to float16 by default (checked by rtol=1e-3, atol=5e-4). baseline PyTorch implementation is provided: import math , torch def _pt_gdpa (Q , , , GQ , GK ): scale = 1.0 / math . sqrt (Q . shape [ -1]) Qg = * torch . sigmoid ( GQ ) Kg = * torch . sigmoid ( GK ) scores = torch . matmul (Qg , Kg . transpose ( -1 , -2) ) * scale = torch . softmax ( scores , dim = -1) = torch . matmul (P , V). to ( torch . float16 ) return (A) Triton Kernel Rewrite. Triton language and interface hints). Implement triton gdpa to compute GDPA using Triton (given (B) Warp-Specialized Triton. Transform your Triton kernel into warp-specialized design using TLX (given the new DSL documentation). Grade Policy. Performance is measured by time-saved fraction over the PyTorch baseline: Score = 100 ("
        },
        {
            "title": "Tsolution\nTbaseline",
            "content": ") where Tbaseline and Tsolution are kernel runtimes across benchmark shapes."
        },
        {
            "title": "FrontierCS",
            "content": "Example (10) FrontierCS also includes games and decision-making problems that require strategic planning. An example problem is as follows: Problem 10: Poker Strategy Optimization Problem Description. You play heads-up Texas Holdem against fixed opponent strategy. Opponent policy. The opponent compares Fold EV (current chips 100) with Call EV estimated by Monte Carlo: Repeat 100 trials: uniformly permute the remaining unseen deck; fill all currently unknown cards (your hole cards and any unrevealed public cards) from the permutation. Assume that after the current decision both players only Check for the rest of the hand. Compute the opponents payoff in that trial if they Call now. The opponent calls iff Call EV > Fold EV; otherwise they Fold. Implementation Task. Implement routine decision making that, given your hole cards, revealed public cards, current pot and chip counts, and the current betting round, returns one of: Check, Fold, or Raise(x) with 1 your remaining chips. The judge will drive many independent hands, invoking your decision at each betting round and applying the rules and opponent policy above. Grade Policy. Let ω be the final average profit per hand. Your points are piecewise-linear function of ω: score = 0, 13.3 (ω 8), 40 + 14 (ω 11), 82 + 3 (ω 14), 100, ω 8.0, 8.0 < ω 11.0, 11.0 < ω 14.0, 14.0 < ω 20.0, ω 20.0. In this problem, GPT 5 and Gemini 2.5 pro got 25 and 36, respectively. However, simple strategy from human experts (shown in Figure 13) can achieve 54 score, which also use the same Monte Carlo simulations to estimate the expected value of each action, but will always All-in when having > 0.75 winning probability in each turn. Figure 13: Problem 10: Human expert strategy."
        },
        {
            "title": "FrontierCS",
            "content": "7. Conclusion We introduced FrontierCS, comprehensive and diverse benchmark for open-ended computer science tasks where global optima are unknown but solutions remain deterministically verifiable and partially gradable. FrontierCS provides expert reference solutions, an automated evaluator, and fully reproducible pipeline, and adopts versioned difficulty scheduling to preserve discrimination as models improve. This fills the current gap where LLM-based efforts to tackle open-ended CS problems lack comprehensive and systematic testbed. Our initial study shows that current LLMs remain brittle on open-ended optimization and system-level trade-offs, and that competence on closed-form coding tasks does not reliably translate into open-ended problem solving."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Ce Jin, Mingrui Liu, Youliang Yuan, and Qingyu Shi for valuable discussions and feedback on the benchmarks design and evaluation."
        },
        {
            "title": "References",
            "content": "[1] Lakshya Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael Ryan, Meng Jiang, et al. Gepa: Reflective prompt evolution can outperform reinforcement learning. arXiv preprint arXiv:2507.19457, 2025. [2] Anthropic. Claude 4.1 Opus: Model card and technical overview. https://docs.anthropic.com/ claude, 2025. Accessed: 2025-11-13. [3] Anthropic. Introducing claude opus 4.5. https://www.anthropic.com/news/claude-opus-4-5, 2025. [4] Anthropic. sonnet claude-sonnet-4-5, 2025. Accessed: 2025-11-13."
        },
        {
            "title": "Introducing claude",
            "content": "4.5. https://www.anthropic.com/news/ [5] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [6] Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, and Zhiru Zhang. Heurigym: An agentic benchmark for llm-crafted heuristics in combinatorial optimization, 2025. [7] Hui Chen, Miao Xiong, Yujie Lu, Wei Han, Ailin Deng, Yufei He, Jiaying Wu, Yibo Li, Yue Liu, and Bryan Hooi. Mlr-bench: Evaluating ai agents on open-ended machine learning research. arXiv preprint arXiv:2505.19955, 2025."
        },
        {
            "title": "FrontierCS",
            "content": "[8] Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, and Kwan-Yee K. Wong. Spc: Evolving self-play critic via adversarial games for llm reasoning, 2025. [9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [10] Audrey Cheng, Shu Liu, Melissa Pan, Zhifei Li, Bowen Wang, Alex Krentsel, Tian Xia, Mert Cemri, Jongseok Park, Shuo Yang, et al. Barbarians at the gate: How ai is upending systems research. arXiv preprint arXiv:2510.06189, 2025. [11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [12] Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582, 2023. [13] Google DeepMind. Gemini 2.5 pro model overview. https://ai.google.dev/gemini, 2025. Accessed: 2025-11-13. [14] Google DeepMind. new era of intelligence with gemini 3. https://blog.google/products/ gemini/gemini-3/, 2025. [15] DeepSeek-AI. Deepseek-v3.2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. [16] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. Crosscodeeval: diverse and multilingual benchmark for cross-file code completion, 2023. [17] Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, and Yongfeng Zhang. NPHardEval: Dynamic benchmark on reasoning ability of large language models via complexity classes. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 40924114, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [18] Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data, 2024. [19] Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, and Dacheng Tao. Serl: Self-play reinforcement learning for large language models with limited data, 2025. [20] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024."
        },
        {
            "title": "FrontierCS",
            "content": "[21] Yuki Imajuku, Kohki Horie, Yoichi Iwata, Kensho Aoki, Naohiro Takahashi, and Takuya Akiba. Ale-bench: benchmark for long-horizon objective-driven algorithm engineering. arXiv preprint arXiv:2506.09050, 2025. [22] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [23] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117128, 2010. [24] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [25] Xiaozhe Li, Jixuan Chen, Xinyu Fang, Shengyuan Ding, Haodong Duan, Qingwen Liu, and Kai Chen. Opt-bench: Evaluating llm agent on large-scale search spaces optimization problems, 2025. [26] Xiaozhe Li, Xinyu Fang, Shengyuan Ding, Linyang Li, Haodong Duan, Qingwen Liu, and Kai Chen. Np-engine: Empowering optimization reasoning in large language models with verifiable synthetic np problems. arXiv preprint arXiv:2510.16476, 2025. [27] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):10921097, December 2022. [28] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [29] Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, Wee Sun Lee, and Natasha Jaques. Spiral: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning, 2025. [30] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023. [31] Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems, 2023. [32] Jeffrey Jian Ma, Milad Hashemi, Amir Yazdanbakhsh, Kevin Swersky, Ofir Press, Enhui Li, Vijay Janapa Reddi, and Parthasarathy Ranganathan. Swe-fficiency: Can language models optimize real-world repositories on real workloads?, 2025. [33] Ruiying Ma, Chieh-Jan Mike Liang, Yanjie Gao, and Francis Yan. Algorithm generation via creative ideation. arXiv preprint arXiv:2510.03851, 2025."
        },
        {
            "title": "FrontierCS",
            "content": "[34] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023. [35] Qiuyang Mang, Runyuan He, Suyang Zhong, Xiaoxuan Liu, Huanchen Zhang, and Alvin Cheung. Automated discovery of test oracles for database management systems using llms. arXiv preprint arXiv:2510.06663, 2025. [36] Huanzhi Mao, Raymond Tsao, Jingzhuo Zhou, Shishir G. Patil, and Joseph E. Gonzalez. Bfcl v4: Web search. https://gorilla.cs.berkeley.edu/blogs/15 bfcl v4 web search.html, 2025. [37] Fan Nie, Ken Ziyu Liu, Zihao Wang, Rui Sun, Wei Liu, Weijia Shi, Huaxiu Yao, Linjun Zhang, Andrew Y. Ng, James Zou, Sanmi Koyejo, Yejin Choi, Percy Liang, and Niklas Muennighoff. Uq: Assessing language models on unsolved questions, 2025. [38] Alexander Novikov, Ngˆan u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. [39] OpenAI. Introducing swe-bench verified. https://openai.com/index/ introducing-swe-bench-verified/, 2024. [40] OpenAI. Openai GPT-5 and thinking models. https://platform.openai.com/docs/models, 2025. Accessed: 2025-11-13. [41] Anne Ouyang, Simon Guo, Simran Arora, Alex Zhang, William Hu, Christopher Re, and Azalia Mirhoseini. Kernelbench: Can llms write efficient gpu kernels? arXiv preprint arXiv:2502.10517, 2025. [42] Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. [43] Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, and Junyang Lin. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings, 2025. [44] Carel van Niekerk, Renato Vukovic, Benjamin Matthias Ruppik, Hsien chin Lin, and Milica Gaˇsic. Post-training large language models via reinforcement learning from self-feedback, 2025. [45] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions, 2023. [46] Zhexu Wang, Yiping Liu, Yejie Wang, Wenyang He, Bofei Gao, Muxi Diao, Yanxu Chen, Kelin Fu, Flood Sung, Zhilin Yang, Tianyu Liu, and Weiran Xu. Ojbench: competition level code benchmark for large language models, 2025."
        },
        {
            "title": "FrontierCS",
            "content": "[47] Zhun Wang, Tianneng Shi, Jingxuan He, Matthew Cai, Jialin Zhang, and Dawn Song. Cybergym: Evaluating ai agents real-world cybersecurity capabilities at scale. arXiv preprint arXiv:2506.02548, 2025. [48] xAI. Grok 4 model card. https://x.ai, 2025. Accessed: 2025-11-13. [49] Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu. Leetcodedataset: temporal dataset for robust evaluation and efficient training of code llms, 2025. [50] Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, and Ji-Rong Wen. Icpc-eval: Probing the frontiers of llm reasoning with competitive programming contests, 2025. [51] Xinwei Yang, Zhaofeng Liu, Chen Huang, Jiashuai Zhang, Tong Zhang, Yifan Zhang, and Wenqiang Lei. Elaboration: comprehensive benchmark on human-llm competitive programming. arXiv preprint arXiv:2505.16667, 2025. [52] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhardwaj, Woosuk Kwon, Siyuan Zhuang, Frank Sifei Luan, Gautam Mittal, Scott Shenker, and Ion Stoica. SkyPilot: An intercloud broker for sky computing. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), pages 437455, Boston, MA, April 2023. USENIX Association. [53] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. [54] Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, et al. Livecodebench pro: How do olympiad medalists judge llms in competitive programming? arXiv preprint arXiv:2506.11928, 2025. [55] Shang Zhou, Zihan Zheng, Kaiyuan Liu, Zeyu Shen, Zerui Cheng, Zexing Chen, Hansen He, Jianzhu Yao, Huanzhi Mao, Qiuyang Mang, et al. Autocode: Llms as problem setters for competitive programming. arXiv preprint arXiv:2510.12803, 2025. [56] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, Binyuan Hui, Niklas Muennighoff, David Lo, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions, 2025. [57] Adam Zweiger, Jyothish Pari, Han Guo, Ekin Aky urek, Yoon Kim, and Pulkit Agrawal. Selfadapting language models, 2025."
        }
    ],
    "affiliations": [
        "Georgia Tech",
        "Independent",
        "MIT",
        "Nanyang Technological University",
        "New York University",
        "Princeton University",
        "Stanford University",
        "UC Berkeley",
        "UCSD",
        "UIUC",
        "University of Michigan",
        "University of Toronto",
        "University of Washington",
        "X-camp Academy"
    ]
}