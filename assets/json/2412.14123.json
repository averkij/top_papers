{
    "paper_title": "AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities",
    "authors": [
        "Guillaume Astruc",
        "Nicolas Gonthier",
        "Clement Mallet",
        "Loic Landrieu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and resolution-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of $5$ multimodal datasets with varying characteristics and $11$ distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned, we achieve better or near state-of-the-art results on the datasets of GeoPlex and $4$ additional ones for $5$ environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, and flood segmentation. The code and models are available at https://github.com/gastruc/AnySat."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 3 2 1 4 1 . 2 1 4 2 : r AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities Guillaume Astruc 1,3, Nicolas Gonthier 1,2 Clement Mallet 1 Loic Landrieu1,4 1 LASTIG, Univ Gustave Eiffel, IGN, ENSG, France 2 IGN, France 3 CNES, France 4 LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France"
        },
        {
            "title": "Abstract",
            "content": "PASTIS-HD 1280m TSAI-TS 60m S2NAIP 640m FLAIR 102.4m PLANTED 120m Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, multimodal model based on joint embedding predictive architecture (JEPA) and scale-adaptive spatial encoders, allowing us to train single model on highly heterogeneous data in self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, collection of 5 multimodal datasets with varying characteristics and 11 distinct sensors. We then train single powerful model on these diverse datasets simultaneously. Once finetuned, we achieve better or near state-of-the-art results on the datasets of GeoPlex and 4 additional ones for 5 environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, and flood segmentation. The code and models are available at https://github.com/gastruc/AnySat. 1. Introduction From remote sensing perspective, the natural images of computer vision are remarkably uniform: they are captured by nearly identical sensors (standard cameras) with the same RGB channels and are often taken from similar perspectives. This consistency allows the creation of large composite image datasets from various sources [25, 52, 59], which are key for image foundation models to learn powerful, generalpurpose features [8]. In contrast, Earth observation (EO) data displays significant variability in modalities, scales, and spatial, temporal, and spectral resolutions. Existing EO foundation models are generally trained on single dataset with specific format [11, 32, 44, 71], and cannot be applied to datasets with different input types without retraining from scratchdefeating the purpose of foundation models. EO foundation models should be able to seamlessly integrate new datasets for training and prediction, regardless of their resolution, scale, Aerial, SPOT6, NAIP Sentinel Sentinel1, ALOS-2 LandSat 7/8/9 SSL pre-training fine-tuning / linear probing AnySat (cid:10) Picea (cid:139) Alnus (cid:11) Rutaceae land cover mapping tree species identification crop type classification deforestation detection flood segmentation Figure 1. Multi-Dataset Training. For the first time, single model can be pretrained simultaneously on collection of Earth Observation datasets with heterogeneous resolutions, scales, and modalities. The resulting model can be fine-tuned to achieve stateof-the-art results for wide variety of data types and tasks. and modalities. As recent efforts provide more flexibility in terms of modalities [7, 39], scale [54], or spectral resolutions [75], none fully leverage the diversity of EO sensors. We introduce AnySat, novel EO model using the spatial alignment of multiple modalities as source of selfsupervision. Indeed, while multiple observations of the same area from distinct sensors capture different information, they share the same underlying semantics. Therefore, we can expect the learned representations to be consistent across modalities. Moreover, we should be able to reconstruct missing modalities from available ones, encouraging the use of cross-modal masked auto-encoding techniques [7, 35]. However, EO data are subject to complex disruptors such as weather conditions, acquisition angles, and variations in time of day or year. To overcome this issue, we design new multimodal Joint Embedding Predictive Architecture (JEPA) [6] to learn representations that are consistent in feature space. key advantage of our JEPA model is that it eliminates the need for modality-specific decoders, allowing us to handle wide variety of sensors seamlessly. Combined with our scale-adaptive patch encoders, this approach enables us to train single model on highly heterogeneous collections of multimodal EO datasets. Notably, over 75% of the learnable parameters in our model are shared across all modalities and resolutions, and thus benefit from large and varied source of data for self-supervision. To evaluate our approach, we compile GeoPlex, collection of 5 multimodal datasets including 11 distinct modalities, with aerial images and satellite time series, radar and optical sensors. GeoPlex spans various spatial resolutions (from 0.2 to 250 per pixel), revisit times (from single images to weekly time series), channel counts (3 to 11), and spatial extent (samples ranging from 0.4 to 160 hectares). To showcase the versatility of AnySat, we also consider 4 external evaluation datasets with diverse characteristics. After fine-tuning, AnySat achieves state-of-the-art performance on 7 downstream tasks, including classification, segmentation, and change segmentation across domains such as land cover mapping, crop type classification, tree species identification, and deforestation detection. Our contributions are as follows: We present AnySat, versatile architecture capable of learning from multiple EO sources with heterogeneous resolutions, scales, and modalities. We introduce the first application of JEPA for multimodal EO data, enabling large-scale and efficient selfsupervised learning. We demonstrate that, when pretrained on curated collection of EO datasets, AnySat can be fine-tuned to achieve state-of-the-art performance across diverse array of tasks and datasets. Thanks to its versatility, our powerful pretrained model can be applied to wide variety of applications and datasets from unimodal data to any combination of the 11 sensors featured in GeoPlex, and scales from single forest plots to large urban areas spanning several hectares. 2. Related Work In this section, we review the dynamic field of selfsupervised learning in geospatial models, highlighting recent efforts to enhance their adaptability to diverse inputs. Finally, we present the feature-predictive paradigm, which is instrumental to improve the versatility of EO models. Self-Supervised Geospatial Models. The abundance of raw EO data makes it particularly suitable for self-supervised learning approaches [9, 14, 46, 67]. Generative models leverage the unique properties of EO data with adapted strategies such as spectral [19], temporal [22, 23], and spatio-temporal [39, 77], and hybrid [66] masking. Other approaches predict rotated [42] or rescaled [49, 54, 63] versions of the input data, or predict missing modalities from available ones [7, 24]. However, these models are often trained on specific combinations of modalities and are limited to those modalities during inference, which hinders their applicability as foundation models expected to adapt to diverse scenarios. Versatile EO Models. Several approaches have been proposed to improve the generalizability of EO models. Some models address variability in spatial resolutions by training on images of different resolutions and generalizing to coarser scales [54], while others manage spectral variability by training on sensors with different spectral bands [75]. Temporal adaptability is achieved in models capable of handling both single-date images and image time series [7, 11, 32]. Attempts have also been made to generalize across modalities by training on data from different sensors [37, 39] or and even text or audio [58]. Despite these efforts, many models are still trained with single scale and expect the input to have certain shape, typically 224 224 pixels. They resize other inputs to fit the model architecture, leading to inefficiencies for smaller inputs [66, Tab 5]. key obstacle preventing the creation of truly versatile generative self-supervised models is the requirement for multiple encoders, decoders, and augmentations to handle different configurations. In this paper, we explore feature-predictive architectures as promising solution to this challenge. Feature-Predictive Architectures. Self-supervised learning methods have achieved significant success in image analysis [17, 34, 52]. These approaches learn without labels using pretext tasks, which can be discriminative [30, 50], contrastive [16, 17, 31, 34], or generative, where the model predicts degraded version of its input [35, 70]. Recent works have proposed performing reconstruction in feature space rather than input space (e.g., pixel space) [10, 76]. Among feature-predictive architectures, the Joint Embedding Predictive Architecture (JEPA) has shown particular promise [6] by learning to predict the features of masked parts of an input image. Feature space reconstruction based model can also be combined with contrastive objectives for improved stability and representation quality [10]. Because it bypasses the need for complex data augmentations or decoder networks, JEPA is particularly well-suited for massively multimodal applications such as Earth observation. SAR-JEPA [41] introduces the first implementation of JEPA concepts for EO, focusing exclusively on SAR data. In this paper, we combine JEPA with versatile spatial encoder architecture, allowing single model to handle diverse data (P/Rm) (P/Rm) Tm Cm (P/(Rmδm))2 (δ2 Tm Cm) (P/(Rmδm))2 xm split in sub-patches size : δm pixels apply to sub-patches merge sub-patches ϕproj ϕtrans m Figure 2. Scale-Adaptive Patch Encoding. We propose an architecture that can encode patches of different sizes. We consider patch xm of size (in m) and resolution Rm (in m). We first split xm into sub-patches of size δm pixels, which are processed by modality-specific projector ϕproj . Then, shared transformer module ϕtrans combines the sub-patches into vector of size E. The sub-patch size δm is fixed, and only influences the number of input tokens to ϕtrans, allowing us to use the same configuration for different patch sizes . scales, resolutions, and modalities. 3. Method We first detail our proposed architecture (Sec. 3.1) and self-supervised training procedure (Sec. 3.2). We then explain how to fine-tune the model for downstream tasks (Sec. 3.3). key motivation of this work is multi-dataset self-supervised training. For simplicity, we first explain how our model operates with single multimodal dataset and then generalize it to multiple datasets. 3.1. Architecture Our model processes multimodal tiles by first partitioning them into spatially aligned patches. Unlike classical Vision Transformers [21], our model can handle patches of different sizes, allowing us to adapt to the significant scale variations in EO datasets. Each patch is embedded using scale-adaptive patch encoder, and combiner network merges the representations from multiple modalities into unified representation for each spatial patch. We consider tile of size SS meters observed through set of modalities M. Each modality is characterized by its resolution Rm (size of pixel in meters), the number of temporal observations Tm (with Tm = 1 for single-date modalities), and the number of channels Cm (e.g., spectral channels or polarization ratios). We denote by xm the observation of tile in modality m, represented as tensor of size (S/Rm) (S/Rm) Tm Cm. Spatially Consistent Patching. We divide the tile into set of non-overlapping patches of size meters. For each patch and modality M, we define an input token xm , which corresponds to the observation of patch in modality m. This results in (S/P )2 tokens per modality and total of (S/P )2 tokens. Since the patch size is consistent across all modalities, all input tokens {xm }mM correspond to the same spatial region. However, the dimensions of each token xm given by (P/Rm) (P/Rm) Tm Cmmay vary between modalities due to differences in spatial, temporal, and spectral resolutions. Patch Encoding. Handling datasets with tiles of varying extents requires adapting our patch size: the 6 6 Sentinel-2 tiles of TreeSatAI [3] can be processed with 1 1 patches, but this would be impractical for the 128 128 Sentinel-2 tiles of the PASTIS dataset [28]. We propose patch encoder into fixed-size vector ϕpatch that embeds each patch xm of dimension E, regardless of its resolution Rm and size . As our patches can have varying temporal dimensions and spatial extents that can reach hundreds of pixels, we do not use FlexiVits [12] weight rescaling. In the spirit of the Perceiver [38], we adopt instead 2 stage embedding scheme. We first divide each token xm into sub-patches of fixed size δm pixels, resulting in (P/(Rmδm))2 sub-patches per token. Each sub-patch has dimensions δm δm Tm Cm. As illustrated in Fig. 2, we map each sub-patch to dimenm , defined as follows: sion with the function ϕproj (i) If Tm > 1, we apply to each pixel Lightweight Temporal Attention Encoder (LTAE) ϕtemp : Tm Cm (cid:55) [27] to collapse the temporal dimension. If = 1, we use linear layer Cm (cid:55) E. (ii) We flatten the pixel dimensions of each sub-patch to mE; obtain vector of size δ2 (iii) We apply Multi-Layer Perceptron (MLP) δ2 mE (cid:55) to each sub-patch; (iv) We add absolute positional encodings based on the ground sampling distance, as in ScaleMAE [54]; (v) shared transformer network ϕtrans with blocks comp of size bines the sub-patches into single vector with CLS-like token. The patch encoder ϕpatch thus consists of modalityspecific projectors ϕproj and shared transformer ϕtrans for all modalities. Using sub-patches of fixed sizes δm allows ϕpatch to process patches of different patch sizes without rescaling the input data. Indeed, changes in only influence the number of input token processed by ϕtrans, which has no incidence on the size of the class token. Modality-Combiner Network. We merge the embeddings for tokens of different modalities into single multimodal vector an absolute positional encoding pos(p) (the same one used for for each patch. We first add to each STUDENT TEACHER"
        },
        {
            "title": "LJEPA",
            "content": "dropped patches mask token mask multimodal token drop token drop"
        },
        {
            "title": "GEOPLEX",
            "content": "xm p,S ϕpatch S"
        },
        {
            "title": "Lcon",
            "content": "ϕcomb p,S p,pred ϕpred"
        },
        {
            "title": "EMA",
            "content": "f p,T ϕpatch p,T ϕcomb positional encoding EMA exponential moving average Patches of size Figure 3. Architecture of AnySat. We begin each iteration by randomly selecting dataset among GeoPlex and sampling tile. Each available modality is divided into spatially aligned patches of size . The student networks patch encoder ϕpatch embeds each patch and we apply contrastive loss to encourage spatial consistency across modalities. We then apply dropping and masking : some patches have all modalities removed (dropping), while others have only random modalities removed (masking). The remaining patches are merged in the modality combiner ϕcomb then reconstructs the embeddings of the dropped patches. Finally, the student networks output is compared to the teachers, whose weights are an Exponential Moving Average (EMA) of the students weights and which processes the complete set of patches without masking or dropping. for the non-dropped patches. The predictor ϕpred to form multimodal representations sub-patches). The combiner network ϕcomb summarizes the resulting token embeddings into multimodal representation for each patch P. The combiner network follows the architecture proposed by OmniSat [7, 3.1]: (i) The tokens go through sequence of self-attention blocks; (ii) We add one token per patch, with fixed (and learned) value and add positional encoding; and (iii) We compute the crossattention between these tokens and the embeddings of the last self-attention block. The resulting values (one per patch) define multimodal embeddings . 3.2. Training Student-Teacher Architecture. We consider two versions of the network: student and teacher. The student network consists of patch encoder ϕpatch , modality combiner ϕcomb , and predictor network ϕpred . The teacher network includes patch encoder ϕpatch . The weights of the teacher network are updated as an Exponential Moving Average (EMA) of the students weights [34]. The students predictor network ϕpred is composed of residual positional encoding and three residual self-attention blocks, and the teacher does not have predictor. and modality combiner ϕcomb T The student network embeds all input tokens xm into vectors of size using the patch encoder: p,S = ϕpatch ) . (xm (1) We adapt the Joint Embedding Predictive Architecture (JEPA) framework [6] to multimodal EO, allowing AnySat to be pretrained in self-supervised manner on datasets with varying properties and modalities, without labels. The general idea is that student network operates on heavily masked inputs spatially, temporally, and modality-wise while the teacher network sees all the input information. The students goal is to combine all its available information to predict the teachers embeddings. We combine this model with contrastive loss for improved stability. We now present the two loss functions used to train the student model: contrastive loss and JEPA loss. Contrastive Loss. For fixed patch p, the observations xm for capture different aspects of the same spatial region but share the same underlying semantics: the content of p. Therefore, we expect the representations m,S to be consistent across modalities. We enforce this intuition with contrastive loss inspired by OmniSat [7]. Specifically, we use modified InfoNCE loss [51], where each token (p, m) is positively matched with tokens of the same patch but different modalities, and negatively matched with tokens corresponding to different patches and modalities. Tokens of the same modality but different patches are neither counted as positive nor negative, as patches of the same tile can have similar content. The contrastive loss is defined as: (cid:88) Lcon = log PM (p,m)PM (cid:88) n=m (cid:88) n=m q=p exp (cid:0)f p,S , p,S /τ (cid:1) exp (cid:0)f p,S , q,S /τ (cid:1) , (2) where τ is temperature parameter, and , denotes the cosine similarity between embeddings. Joint Embedding Predictive Architecture. We adapt the JEPA self-supervised learning framework [6] to the context of multimodal Earth Observation. Avoiding reconstruction in pixel space is particularly beneficial for EO data, which can be heavily influenced by factors such as weather, time of day, or acquisition angle. Reconstructing in latent space allows us to learn more consistent and semantically meaningful features. The training process proceeds as follows: Patch Dropping. We apply JEPAs multi-block masking strategy by randomly selecting five rectangular regions on the tile. Let be the set of patches intersected by these rectangles, and = the remaining patches. We drop all the students tokens p,S for patches K. Modality & Temporal Masking: We randomly mask subset of the remaining tokens, ensuring that at least one modality per patch remains unmasked. Masked token embeddings are replaced with fixed value mask RE, which is learned as parameter of the network. We also randomly mask 50% of the timestamps of all time series. Combiner: We add positional encodings to all tokens , producand input them to the students combiner ϕcomb ing multimodal embeddings p,S for K: p,S = ϕcomb({f p,S }(p,m)L {f mask}(p,m)L) . (3) Predictor: We replace each dropped patch with fixed value drop RE. We add positional encodings to all tokens (including the dropped ones) and input them to the predictor ϕpred p,pred for all patches P: , yielding embeddings p,pred = ϕpred ({f p,S }p {f drop}pK) . (4) Teacher Encoding: The teacher network receives all inp , embeds them using ϕpatch , and combines put tokens xm GeoPlex S2NAIP Planted FLAIR TSAITS PASTIS External datasets SICKLE TS2Crop BraDD-S1TS Sen1Flood11 Figure 4. Datasets Considered. GeoPlex is composed of 5 diverse dataset spanning the entire world, with higher concentration in Europe and the US where open-data are more abundant. We also consider external evaluation datasets with more diverse spread. them with ϕcomb without any dropping, masking, or temporal dropout. The teacher outputs patch embeddings p,T for all P. Loss Function: The training objective is the L2 distance between the student predictions and the teachers multimodal embeddings for the dropped patches: LJEPA = 1 (cid:88) pK (cid:13) (cid:13)f p,pred p,T (cid:13) 2 (cid:13) 2 . (5) After training, we use the teacher network for downstream tasks and discard the student. Note that all modules are shared across all modalities except for the projection layers ϕproj in the patch encoder ϕpatch. Training with Multiple Datasets. The flexibility of AnySat enables us to train single model simultaneously on several datasets of various sizes and scales with the same weights and without rescaling. We consider set of such datasets. Each dataset is characterized by the subset Md of its available modalities and Sd the size of its tiles. We also consider batch size Bd and set Pd of acceptable patch sizes, which depend on the nature of the data, the available resolution, and the tile size. We use the following procedure: 1. Randomly select dataset in D. 2. Randomly select patch size in Pd. 3. Randomly sample Bd tiles in d. 4. Process the tiles and backpropagate the loss. 3.3. Downstream Tasks After pretraining our model, we fine-tune it for various downstream tasks such as classification, semantic segmentation, and panoptic segmentation. Classification. In this task, we predict label or set of labels for each tile. We add [CLS] token in the crossTreeSatAI-TS PASTIS-HD PLANTED classif (weighted F1) classif (macro F1) classif (macro F1) PASTIS-HD semseg (mIoU) FLAIR semseg (mIoU)"
        },
        {
            "title": "Test sets of datasets from GeoPlex",
            "content": "75 70 nySat niSat A PS + LT SICKLE semseg (IoU) 6.1K 90 80 70 nySat 3 etM 3 et-S2 51.4 E 70 60 55 nySat niSat A D BRADD-S1TS chgdet (IoU) 6.1K 80 60 65 60 55 66 ViViT-S2 nySat ViViT SM"
        },
        {
            "title": "External datasets",
            "content": "A nySat EM ViT E TS2Crop classif (OA) Sen1Flood11 semseg (mIoU) 58 56 54 36. & nySat + et + E 90 80 70 14K 92 90 6.1K 47M 39M 33M from scratch fine-tuned linear probing UperNet probing nySat E 3 et onvL A nySat S-C A T A nySat O Prithvi SatlasN et 6.1K free parameters models trained on private external data + access to additional context Figure 5. Quantitative Evaluation. We evaluate AnySat across 9 open-access datasets and for four tasks: multilabel classification (classif), semantic segmentation (semseg), pixel-wise change detection (chgdet), and pixel-wise regression (regression). For clarity, we only visualize the four best performance per plot. We report the number of trainable parameters for probing evaluation with frozen network. attention module of the combiner network, allowing it to output tile-wise representation alongside the multimodal patch tokens. linear layer then maps this representation to vector of logits corresponding to the classification labels. Semantic Segmentation. Here, we predict label for each pixel at specific spatial resolution. We first select modality m, usually one with resolution close to that of the annotations. We create feature map at the subpatch resolution δm by concatenating the subpatch embeddings (output of ϕproj ) with the multimodal embedding of their corresponding patch (output of ϕcomb). An MLP is then applied to each subpatch to map the concatenated to vector of logits of size δm δm , where is the number of semantic classes. We reshapeand, if necessary, resample using bilinear interpolationto obtain map of pixel logits at the label resolution. Note that this approach allows us to perform linear probing for segmentation, without having to use complex segmentation head such as UPerNet [74], as is usually done in this setting [47]. 4. Experiments 4.1. Datasets and Evaluation We present the datasets used for training and evaluation, as well as our evaluation protocol. GeoPlex. As argued by Roscher et al. [56], EO models benefit from high-quality, diverse, and curated data rather than extensive but uniform acquisitions. We follow this intuition by compiling collection of five multimodal datasets, each featuring different combinations of modalities, scales, and resolutions. Unless specified otherwise, GeoPlex comprises the training sets of the following datasets: TreeSatAI-TS [3, 7]: forest-centric dataset in Germany with Sentinel-1 & 2 time series and Very High Resolution (VHR) images at 0.2 resolution. FLAIR [26]: French land cover dataset with Sentinel2 time series and VHR images with elevation data (0.2 m). In order to form multimodal patches, we crop the Sentinel-2 time series to match the extent of the VHR images (discarding 93.5% of pixels). PLANTED [53]: global forest dataset comprising time series from multiple sensors, including Sentinel1/2, Landsat-7, ALOS-2, and MODIS. Only 1.3 of the 2.3M images used in the paper are publicly available. S2NAIP-URBAN [11]: An urban dataset in the continental US with VHR images (1.25m) and time series from Sentinel-1/2 and Landsat-8/9. PASTIS-HD [7, 28]: French crop mapping dataset with VHR images (1.5m) and Sentinel-1 & 2 time series. As PASTIS is evaluated in 5-fold cross-validation, there are no dedicated train and test sets. We include the entire dataset (without labels) in GeoPlex. As illustrated in Fig. 4, GeoPlex spans 249K km2 across five continents and 171 billion pixels. The sampled tiles range in size from 0.36 to 164 hectares. GeoPlex includes 11 distinct modalities with resolutions ranging from 0.2 to 250 m, with both VHR images and time series data: Very High Resolution Images: Aerial: RGB+NIR (near-infrared) at 0.2 Aerial+NMS: RGB+NIR+Elevation at 0.2 NAIP: RGB+NIR at 1.25 SPOT6: RGB+NIR at 1.5 m. Time Series Data: Sentinel-1: 3 channels (VV/VH polarization + ratio) at 10 m, Ascending & Descending Orbits Sentinel-2: 10 channels at 10 ALOS-2: 3 channels (polarization) at 30 Landsat-7: 6 channels at 30 Landsat-8/9: 11 channels at 30 MODIS: 7 channels at 250 m. We select the possible patch size per dataset, while we set the sub-patch size per modality 1 pixel for very high-resolution images and 10 pixels for time series data. See the Appendix for the complete characteristics of all datasets. The official repository contains scripts to compile GeoPlex and add new datasets. External Datasets. To showcase AnySats flexibility, we also consider 4 datasets not included in GeoPlex. AnySat can be directly fine-tuned or linearly probed on new datasets, even if their exact modality combination is not featured in GeoPlex. We consider the following datasets: SICKLE [57]: multimodal crop mapping dataset in India featuring Sentinel-1, Sentinel-2, and Landsat-8 time series. As the test set has not been released, we use the validation set. BraDD-S1TS [40]: change detection dataset comprising Sentinel-1 time series of the Amazon rainforest, aiming to segment deforested areas. TimeSen2Crop [72]: crop mapping dataset in Slovenia consisting of single-pixel Sentinel-2 time series, modality not present in GeoPlex. Table 1. External Datasets. We evaluate our pretrained model on 4 external datasets, in the fine-tuning or linear probing settings. } stands for single-date observations. We report the number of trainable parameters for probing experiments. SICKLE [57] L8 AnySat (fine-tune) AnySat (linear 6.1K) Unet3d [45, 57] UTAE [28, 57] BraDD-S1TS [40] AnySat (fine-tune) AnySat (linear 6.1K) UTAE [28] 3D-UNet [45] Conv-LSTM [61] TimeSen2Crop [72] AnySat (fine-tune) AnySat (linear 14K) OS-CNN [64, 70] MLP+TAE [27, 69] W.LSTM [15, 60] Transformer [68] MSResNet [20] Sen1Floods11 [13] AnySat (linear 6.1K) CROMA [24] (UperNet 47M) Prithvi [39] (UperNet 39M) SatlasNet [11] (UperNet 33M) S1 S1 } } } } S2 } } } } mIoU 89.3 82.0 82.1 51.4 mIoU 80.9 78.9 70.7 68.1 63.7 OA 92.2 70.3 81.2 80.9 78.2 78.1 76.3 mIoU 91.1 90.9 90.4 90.3 pixel annotations and single-date Sentinel-1 and 2 observations, modality not present in GeoPlex. Each tile covers 2600 hectares. Note that SICKLEs LandSat8 requires 3 bands not present in S2NAIPs LandSat8. Conversely, TimeSen2Crop only provide 9 of the 10 bands used to train our Sentinel-2 projector network. In all cases, we use our pretrained projectors and pad the missing channels with learned value shared across all datasets, sensors, and channels. Evaluation. We evaluate our model on the annotated datasets of GeoPlex (excluding S2NAIP-URBAN) and the 4 external datasets, across three tasks: (i) Classification: TSAIT-TS, PASTIS-HD, PLANTED, TimseSenCrop; (ii) Semantic Segmentation: PASTIS-HD, FLAIR, SICKLE, Sen1Flood11; and (iii) Binary pixel-wise change detection: BraDD-S1TS. Sen11Flood1 [13]: global flood mapping dataset with We use three evaluation setting to evaluate the models: From Scratch. The model is trained directly on the labeled training set in supervised manner. The model Fine-tuning. is pretrained in selfsupervised manner, then fine-tuned on the training set. Linear Probing. The model is initially pretrained in self-supervised manner, and linear layer is fitted with the training set. This experiment is reserved for external datasets. Note that most foundation models pretrained on external data cannot be directly applied to target datasets with different input configurations. For example, the ScaleMAE and SatMAE models trained on the Functional Map of the World [18] and are limited to RGB bands, while CROMA is trained on single-date Sentinel-2 data. Since these specific modalities are not present in any of our evaluation datasets, we cannot directly evaluate these pretrained models. Instead, we modify the input layers of these models to match the target number of spectral bands and pre-train them from scratch on each training set separately. Competing Methods. We compare AnySat against stateof-the-art Earth Observation models. Since most models are not designed to handle multimodal or multitemporal inputs, we adapt them following the protocol established by Astruc et al. [7]: (i) we set up independent copies of the model process each modality, followed by feature fusion scheme; (ii) time series are converted into single images either by concatenating seasonal medians or selecting predetermined date, depending on which yields the best results; and (iii) smaller backbones (e.g., ViT-L to ViT-S) are used when they improve performance on smaller datasets. We report for Sen1flood11 the performance of models trained with frozen encoder and fine-tuned UperNet [74] as given by the PANGAEA benchmark [48]. In contrast, AnySat can be linearly probed for semantic segmentation, which requires up 7,000 times fewer free parameters. 4.2. Results and Analysis We evaluate our model on different datasets from and outside of GeoPlex with fine-tuning and linear probing. Performance on GeoPlex Test Sets. We evaluate AnySat on the test sets of the GeoPlex datasets, as shown in Fig. 5, with detailed results provided in the Appendix. Despite using single pretrained model, AnySat sets new state-ofthe-art results for TreeSatAI-TS (+0.9 weighted F1 score) and PASTIS-HD (+2.8 mIoU in classification and +0.2 in segmentation). AnySat also achieves near state-of-the-art performance on PLANTED [53], even though the ViViT models [5] were trained on withheld dataset with nearly 80% more data of the same type. Similarly, our model performs close to the state-of-the-art on FLAIR, despite having access to only 6.5% of the extent of the Sentinel-2 tiles used by UT&T [26]. Pretraining on GeoPlex consistently improves performance, indicating that training on collection of datasets with varied modalities leads to richer and more robust representations. The improvement is more pronounced for smaller datasets like TreeSatAI-TS and in classification tasks rather than segmentation. We attribute this to the amount of supervision available in larger datasets and dense annotations, which make pretraining less beneficial. Performance on External Datasets. As shown in Tab. 1, AnySat pretrained on GeoPlex significantly surpasses the state-of-the-art on the three external datasets: +3.6 mIoU for SICKLE, +10.2% mIoU for BraDD-S1TS, and +11.0 OA for TimeSen2Crop. This shows the strong generalization capabilities of our model despite differences in sensor configurations, such as missing channels. Moreover, while GeoPlex mostly spans the northern hemisphere, SICKLE and BraDDS1TS are in India and Brazil, respectivly. Furthermore, AnySat is the first EO model that can be effectively linearly probed for semantic segmentation and achieves competitive results. Indeed, AnySat outperforms all dedicated approaches on BraDD-S1TS with linear probing. Moreover, AnySat achieve in linear probing better performance on Sen1flood11 than foundation models whith finetuned UperNet segmentation heads, with up to 7000 times more free parameters. These results highlight the expressiveness of the features learned through our self-supervised training scheme. Our pretrained AnySat model can thus be adapted to new tasks and data with minimal training cost and yield competitive results. Ablation Study. We evaluate the impact of several key design choices and report the results in Tab. 2. All results are presented for the Fold 5 of PASTIS-HD and for the classification and semantic segmentation tasks. We do not pretrain on the entire GeoPlex but use Fold 1 to 4 of PASTISHD in self-supervised fashion. Random Token Dropping. We replaced JEPAs block masking strategy with purely random token dropping for the student network. This modification decreased classification performance but slightly improved segmentation results. In order to use single model configuration for all tasks, we maintained unified approach. Interestingly, block masking does not appear to be as critical for EO data than for natural images (see Table 6 in [6]). No Contrastive Loss. We remove the contrastive loss and retain only the reconstruction loss LJEPA. This substantially reduces the classification performance (4.3 F1) but only moderate decrease in segmentation performance (0.2 mIoU). These findings suggest that the contrastive loss can help the feature-predictive approach Table 2. Ablation. We evaluate the impact for several critical design choices of our model on the Fold 1 of PASTIS-HD."
        },
        {
            "title": "Experiment",
            "content": "classification macro F1 segmentation mIoU best configuration random token dropping no contrastive naive semseg 72.0 71.3 67.7 - 63.6 64.1 63.4 61.2 learn more discriminative features, particularly benefiting classification tasks. Naive Semantic Segmentation. We predict pixel-wise logits directly from the patch embeddings without utilizing subpatch features. This results in decrease in segmentation performance by 2.4 mIoU, highlighting the importance of subpatches in providing fine-grained spatial information. Inference and Training Times. Our model was pretrained on GeoPlex using 1760 GPU-hours on an NVIDIA H100 GPU. Fine-tuning takes between 10 and 40 hours, depending on the dataset size. Linear probing takes approximately 2 hours on BraDD-S1TS. In terms of inference speed, AnySat processes one monodate tile from TreeSatAI [3] in 3ms on average, which is faster than ScaleMAE [54] (10ms) and comparable to DOFA [75] (3ms) and OmniSat [7] (2ms). 5. Conclusion We have presented AnySat, versatile architecture designed to address the diversity of EO data in terms of resolutions, scales, and modalities. By leveraging joint embedding predictive architecture and scale-adaptive spatial encoders, AnySat can be trained in self-supervised manner on highly heterogeneous datasets. Pretrained on GeoPlex, comprehensive collection of multimodal datasets with varying characteristics, our model achieved state-of-the-art performance across multiple datasets, tasks, and modalities. key advantage of AnySat is its ability to be applied and fine-tuned on wide array of combinations of data types and scales with single model. Moreover, new datasets can be easily incorporated into GeoPlex for self-supervised pretraining. Our goal is to generalize this approach to develop versatile foundation model for environmental monitoring on global scale."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was granted access to the HPC resources of IDRIS under the allocations AD011014719 and AD011014286R1 made by GENCI. We thank Jordi Inglada, Antoine Labatie, Dimitris Samaras, Yohann Perron, Vincent Lepetit for inspiring discussions and valuable feedback."
        },
        {
            "title": "References",
            "content": "LinearWarmupCosineAnneal- [1] Lightning: https : / / lightning - flash . ingLR. readthedocs . io / en / stable / api / generated / flash . core . optimizers . LinearWarmupCosineAnnealingLR . html. Accessed: 2024-11-20. 17 [2] PyTorch: ReduceLROnPlateau. org / docs / stable / generated / torch . optim . lr _ scheduler . ReduceLROnPlateau . html # torch . optim . lr _ scheduler . ReduceLROnPlateau. 2024-0229. 17 Accessed: [3] Steve Ahlswede, Christian Schulz, Christiano Gava, Patrick Helber, Benjamin Bischke, Michael Forster, Florencia Arias, Jorn Hees, Begum Demir, and Birgit Kleinschmit. TreeSatAI Benchmark Archive: multi-sensor, multi-label dataset for tree species classification in remote sensing. Earth System Science Data Discussions, 2022. 3, 6, 9, 16, 17 [4] allenai.org. AI2-S2-NAIP. https://huggingface.co/datasets/allenai/s2-naip, 2024. [Online; accessed 01-Sept-2024]. 16, 17 [5] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇcic, and Cordelia Schmid. ViViT: video vision transformer. In CVPR, 2021. 8, 15 [6] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In CVPR, 2023. 2, 4, 5, 8 [7] Guillaume Astruc, Nicolas Gonthier, Clement Mallet, and Loic Landrieu. Omnisat: Self-supervised modality fusion for earth observation. In ECCV, 2024. 1, 2, 4, 6, 7, 8, 9, 14, 15, 16, 17 [8] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundational models defining new era in vision: survey and outlook. arXiv preprint arXiv:2307.13721, 2023. 1 [9] Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, and Stefano Ermon. Geography-aware self-supervised learning. In ICCV, 2021. [10] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: general framework for self-supervised learning in speech, vision and language. In ICML, 2022. 2, 15 [11] Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. SatlasPretrain: large-scale dataset for remote sensing image understanding. In ICCV, 2023. 1, 2, 7 [12] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch sizes. In CVPR, 2023. 3 [13] Derrick Bonafilia, Beth Tellman, Tyler Anderson, and Erica Issenberg. Sen1Floods11: georeferenced dataset to train and test deep learning flood algorithms for Sentinel-1. In CVPR Workshop EarthVision, 2020. 7, 14, 16, 18 [14] Jules Bourcier, Gohar Dashyan, Karteek Alahari, and Jocelyn Chanussot. Learning representations of satellite images from metadata supervision. In ECCV, 2024. 2 [15] Lorenzo Bruzzone and Sebastiano Serpico. Classification of imbalanced remote-sensing data by neural networks. Pattern recognition letters, 1997. [16] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 2 [17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. 2 [18] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In CVPR, 2018. 8 [19] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall Burke, David Lobell, and Stefano Ermon. SatMAE: Pre-training transformers for temporal and multi-spectral satellite imagery. In NeurIPS, 2022. 2, 15 [20] Bo Dang and Yansheng Li. MSResNet: Multiscale residual network via self-supervised learning for waterbody detection in remote sensing imagery. Remote Sensing, 2021. 7 [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020. [22] Iris Dumeur, Silvia Valero, and Jordi Inglada. Paving the way toward foundation models for irregular and unaligned satellite image time series. arXiv preprint arXiv:2407.08448, 2024. 2 [23] Iris Dumeur, Silvia Valero, and Jordi Inglada. Selfsupervised spatio-temporal representation learning of satellite image time series. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2024. 2 [24] Anthony Fuller, Koreen Millard, and James Green. CROMA: Remote sensing representations with contrastive radar-optical masked autoencoders. In NeurIPS, 2023. 2, 7, 15 [25] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS, 2024. 1 [26] Anatol Garioud, Nicolas Gonthier, Loic Landrieu, Apolline De Wit, Marion Valette, Marc Poupee, Sebastien Giordano, and Boris Wattrelos. FLAIR: country-scale land cover semantic segmentation dataset from multi-source optical imagery. In NeurIPS Dataset and Benchmark, 2023. 6, 8, 14, 15, 16, [27] Vivien Sainte Fare Garnot and Loic Landrieu. Lightweight temporal self-attention for classifying satellite images time series. In Advanced Analytics and Learning on Temporal Data: ECML PKDD Workshop, 2020. 3, 7, 15 [28] Vivien Sainte Fare Garnot and Loic Landrieu. Panoptic segmentation of satellite image time series with convolutional temporal attention networks. In ICCV, 2021. 3, 7, 14, 15, 16 [29] Vivien Sainte Fare Garnot, Loic Landrieu, and Nesrine Chehata. Multi-modal temporal attention models for crop mapping from satellite time series. ISPRS Journal of Photogrammetry and Remote Sensing, 2022. 15, 17 [30] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. 2 [31] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2 [32] Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, et al. Skysense: multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery. In CVPR, 2024. 1, 2, 15 [33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [34] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 2, 4, 15 [35] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 1, 2 [36] Danfeng Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, Jing Yao, Pedram Ghamisi, Naoto Yokoya, Hao Li, Xiuping Jia, Antonio Plaza, Paolo Gamba, Jon Atli Benediktsson, and Jocelyn Chanussot. SpectralGPT: Spectral remote sensing foundation model. TPAMI, 2024. 15 [37] Chia-Yu Hsu, Wenwen Li, and Sizhe Wang. Geospatial foundation models for image analysis: Evaluating and enhancing NASA-IBM Prithvis domain adaptability. International Journal of Geographical Information Science, 2024. 2 [38] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In ICML. PMLR, 2021. 3 [39] Johannes Jakubik, Roy, CE Phillips, Fraccaro, Godwin, Zadrozny, Szwarcman, Gomes, Nyirjesy, Edwards, et al. Foundation models for generalist geospatial artificial intelligence. URL https://arxiv. org/abs/2310.18660. 1, 2, 7, 15 [40] Kaan Karaman, Sainte Fare Garnot, and Jan Dirk Wegner. Deforestation detection in the Amazon with Sentinel-1 SAR image time series. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2023. 7, 14, 16, [41] Weijie Li, Wei Yang, Tianpeng Liu, Yuenan Hou, Yuxuan Li, Zhen Liu, Yongxiang Liu, and Li Liu. Predicting gradient is better: Exploring self-supervised learning for sar atr with joint-embedding predictive architecture. ISPRS Journal of Photogrammetry and Remote Sensing, 2024. 2 [42] Zhihao Li, Biao Hou, Siteng Ma, Zitong Wu, Xianpeng Guo, Bo Ren, and Licheng Jiao. Masked angle-aware autoencoder for remote sensing images. arXiv preprint arXiv:2408.01946, 2024. 2 [43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR. 17 [44] Siqi Lu, Junlin Guo, James Zimmer-Dauphinee, Jordan Nieusma, Xiao Wang, Parker VanValkenburgh, Steven Wernke, and Yuankai Huo. AI foundation models in remote sensing: survey. arXiv preprint arXiv:2408.03464, 2024. 1 [45] Rose Rustowicz, Robin Cheong, Lijing Wang, Stefano Ermon, Marshall Burke, and David Lobell. Semantic segmentation of crop type in Africa: novel dataset and analysis of deep learning methods. In CVPR Workshop EarthVision, 2019. [46] Oscar Manas, Alexandre Lacoste, Xavier Giro-i Nieto, David Vazquez, and Pau Rodriguez. Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data. In ICCV, 2021. 2 [47] Valerio Marsocci and Nicolas Audebert. Cross-sensor self-supervised training and alignment for remote sensing. arXiv preprint arXiv:2405.09922, 2024. 6 [48] Valerio Marsocci, Yuru Jia, Georges Le Bellier, David Kerekes, Liang Zeng, Sebastian Hafner, Sebastian Gerard, Eric Brune, Ritu Yadav, Ali Shibli, et al. PANGAEA: global and inclusive benchmark for geospatial foundation models. arXiv preprint arXiv:2412.04204, 2024. 8 [49] Mubashir Noman, Muzammal Naseer, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, and Fahad Shahbaz Khan. Rethinking transformers pretraining for multi-spectral satellite imagery. In CVPR, 2024. 2 [50] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. 2 [51] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 5 [52] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. TLMR, 2023. 1, 2, 15 [53] Luis Miguel Pazos-Outon, Cristina Nader Vasconcelos, Anton Raichuk, Anurag Arnab, Dan Morris, and Maxim Neumann. Planted: dataset for planted forest identification from multi-satellite time series. International Geoscience and Remote Sensing Symposium, 2024. 6, 8, 15, 16, [54] Colorado Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp, Kurt Keutzer, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-MAE: scale-aware masked autoencoder for multiscale geospatial representation learning. In ICCV, 2023. 1, 2, 3, 9, 15, 17 [55] Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara, Benjamin Recht, and Solomon Hsiang. generalizable and accessible approach to machine learning with global satellite imagery. Nature communications, 2021. 15 [56] Ribana Roscher, Marc Russwurm, Caroline Gevaert, Michael Kampffmeyer, Jefersson A. Dos Santos, Maria Vakalopoulou, Ronny Hansch, Stine Hansen, Keiller Nogueira, Jonathan Prexl, and Devis Tuia. Better, not just more: Data-centric machine learning for Earth observation. IEEE Geoscience and Remote Sensing Magazine, 2024. 6 [57] Depanshu Sani, Sandeep Mahato, Sourabh Saini, Harsh Kumar Agarwal, Charu Chandra Devshali, Saket Anand, Gaurav Arora, and Thiagarajan Jayaraman. SICKLE: multi-sensor satellite imagery dataset annotated with multiple key cropping parameters. In WACV, 2024. 7, 14, 16, 18 [58] Srikumar Sastry, Subash Khanal, Aayush Dhakal, Adeel Ahmad, and Nathan Jacobs. TaxaBind: unified embedding space for ecological applications. In WACV, 2025. [59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS Dataset and benchmark, 2022. 1 [60] Hochreiter Sepp and Schmidhuber Jurgen. Long shortterm memory. Supervised sequence labelling with recurrent neural networks, 2012. 7 [61] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-Chun Woo. Convolutional LSTM network: machine learning approach for precipitation nowcasting. In NeurIPS, 2015. 7 [62] Adam Stewart, Nils Lehmann, Isaac Corley, Yi Wang, Yi-Chia Chang, Nassim Ait Ait Ali Braham, Shradha Sehgal, Caleb Robinson, and Arindam Banerjee. SSSL4EO-l: Datasets and foundation models for Landsat imagery. NeurIPS, 36, 2024. 15 [63] Maofeng Tang, Andrei Cozma, Konstantinos Georgiou, and Hairong Qi. Cross-scale mae: tale of multiscale exploitation in remote sensing. In NeurIPS, 2024. 2 [64] Wensi Tang, Guodong Long, Lu Liu, Tianyi Zhou, Michael Blumenstein, and Jing Jiang. Omni-scale CNNs: simple and effective kernel size configuration for time series classification. In ICLR, 2021. 7 [65] Michail Tarasiou, Erik Chavez, and Stefanos Zafeiriou. ViTs for SITS: Vision transformers for satellite image time series. In CVPR, 2023. 15 [66] Gabriel Tseng, Ivan Zvonkov, Mirali Purohit, David Rolnick, and Hannah Kerner. Lightweight, pre-trained transformers for remote sensing timeseries. arXiv preprint arXiv:2304.14065, 2023. 2, [67] Wei-Hsin Tseng, Ho`angˆAn Lˆe, Alexandre Boulch, Sebastien Lef`evre, and Dirk Tiede. CROCO: Crossmodal contrastive learning for localization of Earth observation data. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2022. 2 [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 7 [69] Elliot Vincent, Jean Ponce, and Mathieu Aubry. Satellite image time series semantic change detection: Novel arXiv architecture and analysis of domain shift. preprint arXiv:2407.07616, 2024. 7 [70] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. 2, 7 [71] Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad Albrecht, and Xiao Xiang Zhu. SSL4EO-S12: large-scale multi-modal, multitemporal dataset for self-supervised learning in Earth observation. IEEE Geoscience and Remote Sensing Magazine, 2023. 1 [72] Giulio Weikmann, Claudia Paris, and Lorenzo Bruzzone. Timesen2crop: million labeled samples dataset of Sentinel 2 image time series for crop-type classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2021. 7, 16, 18 [73] Piper Wolters, Favyen Bastani, and Aniruddha Kembhavi. Zooming out on zooming in: Advancing superresolution for remote sensing, 2023. 16, [74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018. 6, 8 [75] Zhitong Xiong, Yi Wang, Fahong Zhang, Adam Stewart, Joelle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, and Xiao Xiang Zhu. Neural plasticity-inspired foundation model for observing the Earth crossing modalities. arXiv preprint arXiv:2403.15356, 2024. 1, 2, 9, 15 [76] Kun Yi, Yixiao Ge, Xiaotong Li, Shusheng Yang, Dian Li, Jianping Wu, Ying Shan, and Xiaohu Qie. Masked In ICLR, image modeling with denoising contrast. 2023. 2 [77] Yuan Yuan, Lei Lin, Qingshan Liu, Renlong Hang, and Zeng-Guang Zhou. SITS-Former: pre-trained spatiospectral-temporal representation model for sentinel2 time series classification. International Journal of Applied Earth Observation and Geoinformation, 2022. 2 AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities"
        },
        {
            "title": "Supplementary Material",
            "content": "P I - ( 1 ) 5 0 s i PASTIS-HD semseg(mIoU) 70 SOTA AnySat (mIoU) PLANTED semseg 65 L ) m ( e s 4 0 65 60 7 0 6 5 5 7 4 7 6 8 91 75 8 5 S 9 0 s g ( E ) 90 BRADD-SITS chgdet(IoU) 1 0 0 e 2 p ( ) s l a s i ( - 7 5 1 ) s ( 1 ) t I - 92 (mIoU) Sen1Floods11 LP semseg Figure A. Overall Performance. We underline external datasets. LP stands for Linear Probing. In this appendix, we provide detailed results in Sec. A, an extended ablation study in Sec. B, and provide implementation details in Sec. C. Finally, we provide more details on the datasets and experiments of the main paper in Sec. A. Detailed Results We provide qualitative illustrations of our predictions and detailed quantitative results for the test sets of GeoPlex. Qualitative Results. We present qualitative illustrations in Fig. for four segmentation tasks: PASTIS, FLAIR, SICKLE, and BraDD-S1TS. AnySat predicts precise segmentations that closely follow the extents of buildings, trees, and parcels. Notably, the predictions do not display grid artifacts despite our segmentation head being simple linear layer applied to each subpatch. This suggests that using subpatches of small sizes (e.g., 4 4 pixels for PASTIS and 10 10 pixels for FLAIR), combined with larger context through patch embeddings, is an effective strategy for producing smooth and consistent segmentation maps. TreeSatAI-TS, PASTIS in classification), but this effect is more limited for segmentation datasets (FLAIR, PASTIS in segmentation) or larger ones like PLANTED. We hypothesize that this is due to the quantity of available supervision; for instance, FLAIR has over 20 billion individual labels. In the case of FLAIR, the pretrained model is 0.5 points behind training from scratch, which we attribute to stochastic noise, as our performance on the validation set is on par with training from scratch: 54.7 for pretrained vs. 54.8 from scratch. B. Additional Ablation We propose an additional experiment to evaluate the impact of one of our design choices. No Modality or Temporal Masking. In this experiment, we remove the modality and temporal masking for the student encoder during pretraining. This modification results in slight increase in segmentation performance by +0.4 mIoU but decrease in classification performance by 0.6 F1 score. These ambiguous results are similar to the effects we observed with naive patch dropping. An advantage of including modality and temporal masking is that it reduces the memory requirements during training by up to 30%. Since our goal is to train single model on several datasets aimed to be fine-tuned for multiple tasks, we keep unique configuration and adopt this masking strategy. C. Implementation Details GeoPlex. See Tab. for more details on the composition of GeoPlex. GeoPlex is composed of five distinct datasetsTSAI-TS, PASTIS-HD, FLAIR, PLANTED, and S2NAIP-URBANwhich collectively offer rich combination of data types, including images, time series, and various modalities. These datasets span extensive geographical areas, ranging from 180 km² to over 211,000 km², and provide wide array of spatial resolutions (from 0.2m to 250m), temporal resolutions (from 1 to 140 time steps), and spectral resolutions (from 3 to 10 bands). The inclusion of multiple satellite and aerial platforms, such as Sentinel-1/2, Landsat 7/8/9, SPOT6/7, and NAIP, ensures robust and varied training set. Quantitative Results. We provide in Tab. the detailed performance of AnySat, with and without pretraining, and an extensive comparison with recent EO models. Pretraining on GeoPlex improves performance for smaller datasets (e.g., Network Architecture. AnySats architecture follows the Vision Transformer (ViT) template and has 125M learnable parameters, of which 73.6% are modality-agnostic and resolution-adaptive. The components of the model are: PASTID-HD [7, 28] FLAIR [26] SICKLE [57] BraDD-S1TS [40] Sen1Floods11[13] S1-TS S1-TS S1-TS, first date S1 monodate S2-TS S2-TS S2-TS S1-TS, last date S2 monodate VHR 1.5 VHR 0.2 LandSat8-TS prediction 1280 102.4 320 ground truth 480 5120 Figure B. Illustration of Results. We represent the inputs, predictions, and ground truth for tiles from four datasets. The colormaps are taken directly from the papers. TS: time series, single date has been chosen. S1/2 stands for Sentinel-1/2. For PASTIS-HD, white parcels are not annotated (void label). Table A. Model Performance on the Test Sets of GeoPlex. For time series, we denote by } when single date has been selected, and (cid:129) when seasonal medians have been concatenated in the channel dimension. AL stands for ALOS-2 and MO for MODIS. LP stands for linear probing Model Pre-training Modalities PASTIS-HD - multilabel classif. VHR S1 S2 maF TSAI-TS - multilabel classif. VHR S1 S2 AnySat (ours) AnySat (ours) GeoPlex None OmniSat [7] TSAI-TS DOFA DOFA [75] PSE+LTAE [27] None PSE + ResNet [7] None TSAI ScaleMAE [54] TSAI SatMAE [19] TSAI CROMA [24] ImageNet UT&T [26] TSAI MOSAIKS[55] PRESTO PRESTO [66] } } } } } } } } } wF1 75.1 72.7 74.2 71.6 71.2 68.1 62.5 61.5 61.0 56.7 56.0 46.3 Model Pre-training Modalities PLANTED - classif. S1 S2 LS AL MO maF1 AnySat (ours) AnySat (ours) GeoPlex 61.5 61.2 None ViViT [5, 53] ViViT [5, 53] None None 62.2 59.3 FLAIR - semantic seg VHR AnySat (ours) AnySat (ours) UT&T [26] UNet [33] UTAE [28] GeoPlex None ImageNet ImageNet None mIoU 55.1 55.6 56.9 54.7 36. AnySat (ours) AnySat (ours) GeoPlex None OmniSat [7] CROMA [24] DOFA [75] UT&T [26] UTAE [28] ScaleMAE [54] PASTIS-HD (cid:129) (cid:129) PASTIS-HD (cid:129) (cid:129) DOFA ImageNet None (cid:129) PASTIS-HD 72.8 65. 69.9 60.1 55.7 53.5 46.9 42.2 PASTIS-HD - semantic seg VHR S1 S2 OA mIoU AnySat (ours) AnySat (ours) GeoPlex None 85.0 84. SkySense [32] UTAE-MM [29] None None TSViT [65] None UTAE [28] SkySense 85.9 84.2 83.4 - 66.5 66.3 - 66.3 65.4 63.1 PASTIS-HD - semseg LP VHR S2 mIoU AnySat LP (ours) GeoPlex S12-DINO LP [52, 62] foundation S12-MoCo LP [34, 62] foundation foundation S12-D2V LP [10, 62] foundation SpectralGPT [36] foundation Prithvi [39] 42.7 36.2 34.5 34.3 35.4 33. Table B. Considered Datasets. We present the detailed composition of GeoPlex, the collection of datasets used for self-supervised training, and our external evaluation datasets. img: img, t.s.: time series: t.s. S1/2: Sentinel-1/2. upsampled from original acquisition resolution. Dataset Extent Sample Size (S) Patch Size (P) GeoPlex TSAI-TS [3, 7] 50k (1 img + 2 t.s.) 180 km² - 4.7 GPix = 60m {10, 20, 30}m PASTIS-HD [7, 28] 2433 (1 img + 2 t.s.) 3986 km² - 7.5 GPix = 1280m {40, 80, 160}m Modalities Aerial VHR S1 S2 SPOT6/7 S1 S2 FLAIR [26] 78k (1 img + 1 t.s.) 815 km² - 24 GPix = 102.4m {10, 20, 50}m Aerial VHR S2 Planted [53] 1.3M (5 t.s.) 33,120 km² - 3.0 GPix = 120m {30, 60}m S2NAIPURBAN [4, 73] 515k (1 img + 3 t.s.) 211,063 km² - 136 GPix = 640m {40, 80, 160}m BraDD-S1TS [40] Sickle [57] TimeSen2Crop [72] 13k (1 t.s.) 2,995 km² - 1.2 GPix 35k (2 t.s.) 3,584 km² - 3.6 GPix 1.2M (1 t.s.) 120 km² - 35 MPix Sen1floods11 [13] 4.8k (2 img) 125,829 km² - 2.6 GPix External datasets = 480m = 10 = 320m = 10m = 10m = 10m = 5120m = 80m S2 S1 Landsat 7 ALOS-2 MODIS NAIP S2 S1 Landsat 8/9 S1 S2 Landsat 8/ S2 S2 S1 Resolution Spatial (R) Temporal (T) Spectral (C) 0.20m 10m 10m 1m 10m 10m 0.2m 10m 10m 10m 30m 30m 250m 1.25m 10m 10m 10m 10m 10m 10m 10m 10m 10m 1 10-70 10-70 1 140 38-61 1 208 8 20 4 60 1 16-32 2-8 4 20-66 13-148 8-34 29 1 4 3 10 4 3 10 5 10 10 3 3 3 7 4 10 3 8 10 8 10 10 3 Modality Projectors ϕproj (33M parameters for 11 projectors). These modules are MLPs responsible for projecting the input data of each modality into common feature space. Spatial Transformer ϕtrans (45M parameters). Composed of three self-attention transformer blocks, this module captures the spatial relationships between subpatches for each modality and patch. Modality Combiner ϕcomb (49M parameters). This module consists of three self-attention blocks followed by cross-attention block, and merges the representations from different modalities into unified feature vector for each patch. Predictor ϕpred (29M parameters). Exclusive to the student, this module is single self-attention block and predicts the teachers embeddings for the dropped patches. Handling MODIS data. In the Planted dataset [53], MODIS observations are included, but their resolution (250 meters) is larger than the entire observed tile (120 meters). We treat these observations as context tokens: we concatenate their ϕpatch embeddings to the (S/P )2 tokens from all other modalities. We do not add positional encoding, and this token is not included in the contrastive loss. Optimization Parameters. To better manage our memory usage, we adapt the batch size to the size of the samples of each dataset: TreesatAI-TD: 384, PASTIS-HD: 8, FLAIR: 96, PLANTED: 2048, S2NAIP: 16. We use 8 NVIDIA H100 for experiments on GeoPlex, PLANTED and Pastis-HD , and smaller cluster of 3 A600 for TreeSatAI-TS and FLAIR. Beyond the changes above, all optimization parameters are shared across all datasets. We used the AdamW [43] optimizer with learning rate of 5 105 for all our experiments (pretraining and fine-tuning). We used LinearWarmupCosineAnnealingLR [1] for classification and ReduceLROnPlateau [2] scheduler for pretraining and segmentation. We set he contrastive temperature γ to 0.1 to Eq. X. We used an EMA decay of 0.996. All other hyperparameters are shared with original JEPA implementation. Position Encodings. We describe here our scale-adaptive positional encoding which allows us to use the same encoders for different resolutions, scales, and patch size. The input tokens to the modality combiner ϕcomb correspond to patches of size meters, while those to the spatial transformer ϕtrans represent subpatches of size (Rmδm)(Rmδm) meters. Here, Rm varies per sensor modality m, and is randomly chosen for each batch during training. To train single scale-aware model capable of handling varying resolutions, we employ scale-adaptive positional encoding inspired by Scale-MAE [54]. We use the same positional encodings in ϕcomb and ϕtrans. We first describe the positional encoding of token by ϕcomb. We denote by posx the index of the tokens patch within its tile along the x-axis; similarly, posy along the y-axis. If the embeddings of the token have dimension D, the positional encodings µx(posx, i) and equivalently µy(posy, i) are of size D/2. For [0, D/2[ we have: µx(posx, i) = sin (cid:18) posx 10000 E + π 2 (cid:19) mod(i, 2) , (A) where = is the size in meter of the patch considered unit: patch of size for ϕcomb, and is reference length that we set to one meter. We compute µy(posy, i) similarly, and the positional encoding is the channelwise concatenation of both vectors. The positional encoding is directly added to the embeddings. For ϕtrans, we define the positional encoding of each subpatch within its patch with the same formula, but set to = Rmδm, the size of the subpatch in meter. D. Datasets and Tasks Here, we provide more details about the datasets used to train and evaluate AnySat and their associated tasks. See Tab. for an overview of the datasets used in GeoPlex. TreeSatAI-TS [3, 7]: This multimodal dataset is designed for tree species identification and consists of 50,381 tiles, each covering an area of 6060 meters, with multi-label annotations across 20 classes. All data were collected in Germany. The dataset includes Very High Resolution (VHR) images at 0.2 with NIR band, Sentinel-2 time series, and Sentinel-1 time series. PASTIS-HD [7, 29]: This crop mapping dataset supports classification, semantic segmentation, and panoptic segmentation. Each agricultural parcel is delineated at resolution of 10 and annotated across 18 crop types. The dataset contains 2,433 tiles with an extent of 1,2801,280 m, including Sentinel-2 time series, Sentinel-1 time series (we use only the ascending orbit), and SPOT6 VHR imagery at 1.5 resolution. FLAIR [26]: This dataset combines VHR aerial imagery at 0.2 resolution with Sentinel-2 time series data and comprises 77,762 tiles acquired across metropolitan France. The VHR images include five channels: RGB, near-infrared, and normalized digital surface model derived by photogrammetry. Each VHR pixel is annotated with one of 13 land cover classes. PLANTED [53]: The PLANTED dataset is specifically designed for tree species identification and features 1,346,662 tiles of planted forest across the world. Each tile is associated with one of 40 distinct classes. This dataset integrates imagery from five different satellites with various resolutions: Sentinel-2 (10 m), Landsat-7 (30 m), MODIS (250 m), as well as radar time series from Sentinel-1 (10 m) and ALOS-2 (30 m). The time series are temporally aggregated at various intervalsseasonally, monthly, or yearly. S2Naip-Urban [4, 73]: This dataset includes images captured at the same locations as the S2NAIP-Urban superresolution dataset [73], which is subset of the extensive S2NAIP [4] dataset focused on urban areas. This split comprises 515,270 tiles, featuring imagery from NAIP at 1.25 resolution, Sentinel-2 and Sentinel-1 time series, and Landsat-8/9 data rescaled to 10 resolution. We use this dataset for pretraining only because there are no official labels and evaluations. BraDD-S1TS [40]: BraDD-S1TS (Brazilian Deforestation Detection) is change detection dataset comprising Sentinel-1 time series of the Amazon rainforest, aiming to segment deforested areas. It includes 13,234 tiles covering regions with varying deforestation rates, providing pixelwise binary annotations for deforestation events occurring between the time series first and last radar image. Sickle [57]: SICKLE is multimodal crop mapping dataset from India containing 34,848 tiles with Sentinel1, Sentinel-2, and Landsat-8 time series. We use the paddy / non-paddy culture binary semantic segmentation task. As the test set has not been released by the authors, we perform our experiments on the validation set. TimeSen2Crop [72]: TimeSen2Crop is crop mapping dataset consisting of 1,212,224 single-pixel Sentinel-2 time series, configuration not present in GeoPlex. It includes data from Slovenia with annotations for 16 different crop types. Sen1floods11 [13]: Sen1Floods11 is flood segmentation dataset featuring 4,831 pairs of Sentinel-1 and Sentinel-2 images, each annotated with dense flooded/not-flooded labels. The dataset spans diverse global regions, with each tile covering 5120 5120 area ( 2600 hectares) and containing single acquisition date per sensor."
        }
    ],
    "affiliations": [
        "CNES, France",
        "IGN, France",
        "LASTIG, Univ Gustave Eiffel, IGN, ENSG, France",
        "LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France"
    ]
}