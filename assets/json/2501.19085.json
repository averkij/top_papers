{
    "paper_title": "Enhancing Code Generation for Low-Resource Languages: No Silver Bullet",
    "authors": [
        "Alessandro Giagnorio",
        "Alberto Martin-Lopez",
        "Gabriele Bavota"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights."
        },
        {
            "title": "Start",
            "content": "Enhancing Code Generation for Low-Resource Languages: No Silver Bullet Alessandro Giagnorio, Alberto Martin-Lopez, Gabriele Bavota Software Institute USI Universit`a della Svizzera italiana, Switzerland 5 2 0 2 1 3 ] . [ 1 5 8 0 9 1 . 1 0 5 2 : r AbstractThe advent of Large Language Models (LLMs) has signiﬁcantly advanced the ﬁeld of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs performance on low-resource languages, namely: (i) classic ﬁne-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) pre-training objective teaching the model how to translate between highand low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our ﬁndings reveal that ﬁne-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even small dataset is sufﬁcient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when ﬁne-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights. Index TermsCode Generation, Low-Resource Languages I. INTRODUCTION Large Language Models (LLMs) have substantially pushed the boundaries of automated code generation, namely the implementation of given code starting from its textual description. Tools such as GitHub Copilot are nowadays used by thousands of companies and millions of developers worldwide [1]. To achieve this level of automation, LLMs are trained on open source code mined from millions of public open source repositories. For very popular programming languages (e.g., Python, Java), this training process exposes the LLM to vast and diverse set of code examples spanning across different domains and exploiting variety of technical solutions (e.g., frameworks, libraries). We refer to these languages as highresource languages. On the opposite side of the spectrum there are instead niche languages, used only in speciﬁc domains and characterized by scarcity of publicly available data (i.e., low-resource languages). The limited availability of such data hampers the LLMs ability to generalize effectively, struggling to handle unique constructs and edge cases that are instead well-covered in high-resource languages. With no surprise, this results in substantial gap in performance between highand low-resource languages in the context of LLM-based code generation [2]. To close this performance gap, researchers recently proposed solutions aimed at bolstering LLMs performance for low-resource languages (or even languages completely unseen by the model), including ﬁne-tuning [2][4] and few-shot learning [5]. The former consists of performing an additional training of the model on the low-resource language of interest, possibly speciﬁcally targeting the code generation problem (i.e., providing the model with pairs hdescription, codei showcasing the task). Clearly, such training is capped by the availability of training data, which remains scarce. The latter, instead, consists of including in the prompt input to the model (i.e., the instructions provided for the code generation) few examples demonstrating successful executions of the task on the low-resource language. Positive results have been reported across the board, with all proposed solutions substantially helping in improving performance on low-resource languages, even though those are often not yet on par with high-resource languages. Nevertheless, comparative study across these techniques is missing, as well as wider exploration of their effectiveness when varying the models size. Indeed, LLMs can feature substantially different number of trainable parameters, likely impacting their ability to (i) correctly interpret complex prompts featuring examples and additional instructions, and (ii) ﬁt to the training data during ﬁne-tuning, with larger models possibly lacking training data to effectively update their parameters. We present an extensive study comparing several approaches to tackle low-resource languages which have been either previously proposed in the literature [2], [5] or that are presented for the ﬁrst time in this work. We focus on six different LLMs, including both open and closed source models: DeepSeek Coder 1B, 7B, and 33B [6], Code Llama 7B, and 13B [7], and GitHub Copilot [1]. For all models, we exploit their instruct version, meaning that the models are able to understand complex prompts in natural language thanks to speciﬁc instruction ﬁne-tuning they underwent. This is necessary to experiment with the in-context learning techniques considered in our study. We start by assessing the performance of the six LLMs for code generation on six programming languages, two clearly being high-resource (i.e., Python and Java), and four which have been considered as low-resource in previous work [2], [8] (i.e., Julia, Lua, R, and Racket). We assess the capabilities of the models on benchmark of code generation problems [2] featuring code description and tests to verify the correctness of the generated solution. We found that the gap in performance between the highand the low-resource languages is quite strong for and Racket, while it is more limited for Julia and Lua. For example, Code Llama 13B is able to successfully implement 44.5% of Python functions, against the 33.4% of Julia, 32.6% of Lua, 15.6% of R, and 15.2% of Racket. For this reason, we decided to focus the attention on and Racket, experimenting on them the techniques aimed at boosting performance on low-resource languages. We experiment with three in-context learning techniques, including few-shot learning [5] as well as two prompts we devised which provide the model with information on how to map given code in high-resource language into the same code written in the low-resource language. The fourth strategy is classic ﬁne-tuning of the models performed using the publicly available datasets released by Cassano et al. [2], featuring 37,592 functions for and 40,489 for Racket. Finally, we devised pre-training task specializing the model for the task of code translation (highto low-resource) before ﬁne-tuning it on the target low-resource. Note that strategies relying on ﬁne-tuning have not been evaluated with Copilot, since this is not technically possible. Our ﬁndings show that for the smallest model in our study (DeepSeek Coder 1B), ﬁne-tuning (with/without pretraining) helps in substantially boosting performance, while the model seems to not beneﬁt from in-context learning (despite not worsening its generation capabilities), probably due to its limited ability to interpret complex prompts. With the increase in size of the models (7B and 13B models), results are mixed, with better results achieved with in-context learning for and ﬁne-tuning for Racket. DeepSeek Coder 33B, instead, worsens its performance on both languages after ﬁne-tuning, likely due to the scarcity of data which is insufﬁcient to meaningfully update its parameters. On the other hand, it substantially beneﬁts from in-context learning strategies, which are also successful with Copilot. In summary, while ﬁne-tuning may be beneﬁcial for some (smaller) LLMs, in-context learning represents safe bet for all models, since it is cheap and it usually improves performance. II. RELATED WORK Several deep learning (DL)-based techniques have been proposed in the literature to tackle code generation [6], [9][15]. These approaches mostly differ for the exploited DL model, and for the training task and data adopted. The evaluation of these approaches is nowadays based on publicly available code benchmarks [5], [8], [16][20], being collections of well-documented programming tasks paired with ad-hoc test suites. For example, HumanEval [16] features 164 handcrafted Python prompts, each including function signature, docstring, correct solution, and set of speciﬁc unit tests. In our work, we use MultiPL-E [19] as benchmark for assessing the capabilities of the experimented models on both highand low-resource languages. The latter has been presented by Cassano et al. [19], who translated two existing benchmarks (i.e., HumanEval [16] and Mostly Basic Python Problems [17]) into 18 additional languages, including low-resource ones. The most relevant works are however those proposing solutions to tackle low-resource languages. Ahmed and Devanbu [21] show that code identiﬁers affect the model training more than code syntax and that multilingual ﬁne-tuning can be beneﬁcial for tasks such as code summarization and retrieval. While not directly related to code generation, the idea of multilingual ﬁne-tuning can be seen as transfer learning helping the model to exploit the knowledge acquired in highresource languages also in the low-resource ones. On similar thread, Chen et al. [3] propose to ﬁne-tune pre-trained models on languages that share similar semantics and textual features with the target one. They found that such strategy can help to enhance the effectiveness of these models, beneﬁtting lowresource languages like Ruby. In our study, we exploit these ﬁndings by starting from DL models which have been already pre-trained on multi lingual dataset, thus possibly beneﬁtting of the boost in performance documented in [3], [21]. Orlanski et al. [8] treat the low-resource problem as data distribution issue, showing that training model on balanced distribution of 14 languages helps in reducing disparities in performance among languages. Since we do not have control on the training datasets used for the six LLMs subject of our study, such strategy is not considered in our work. Athiwaratkun et al. [5] study the performance of monoand multi-lingual language models on code translation and generation. They show that few-shot learning can improve the generation accuracy of model, especially for languages for which the model has not been trained at all. We consider fewshot as one of the experimented techniques. Van Dam et al. [4] and Cassano et al. [2] proposed instead ﬁne-tuning as solution for low-resource languages. The former explicitly target functional programming languages such as Haskell, showing that, while ﬁne-tuning helps, the obtained performance is still far from that obtained for highresource languages (e.g., Python, Java). The latter, instead, recently released MultiPL-T, framework explicitly targeting the improvement of code generation models for low-resource languages. The idea is to build ﬁne-tuning datasets for lowresource languages by exploiting automated code translation. In particular, they use the StarCoder 15B model [13] to generate test cases for set of Python functions having highquality documentation. Through quality assurance pipeline, they remove invalid tests and, they use suite of compilers to translate the Python tests into more than 20 languages, including low-resource ones. Finally, they use again StarCoder 15B to translate the Python function to target language, verifying its correctness through the translated tests. They use the built datasets to ﬁne-tune LLMs, reporting major beneﬁts on the code generation performance for low-resource languages. In our work, we exploit their datasets to ﬁne-tune the subject models, considering this approach as one of the state-of-the-art techniques. then, Finally, recent survey by Joel et al. [22] highlights the scarcity of benchmarks for niche programming languages, as well as the importance of experimenting with more advanced techniques. We provide an overview of the code generation performance of several LLMs on low-resource programming languages, and experiment different strategies proposed in the literature to improve their accuracy. III. ON THE GAP IN PERFORMANCE FOR LLM-BASED CODE GENERATION IN HIGHAND LOW-RESOURCE LANGUAGES Before moving to the core of our work (i.e., investigating techniques which can boost LLMs performance in code generation for low-resource languages), we measure the gap in performance for LLM-based code generation in highand low-resource languages. In particular, we aim at answering the following research question (RQ): RQ1 What is the gap in performance of state-of-the-art LLMs when generating code in highand low-resource programming languages? Note that evidence in the literature already suggests the existence of such performance gap (see e.g., [2], [3]). However, LLM-based code generation is fast-evolving ﬁeld and, for example, we found no previous work studying this phenomenon on GitHub Copilot [1], arguably the most popular code completion/generation tool in terms of developers adoption. In the following we describe the context of our study, as well as the study procedure and data analysis. A. Context Selection: Programming Languages To answer RQ1 we need both highand low-resource languages. The former will be represented by Java and Python which count, at the date of writing, 135k and 371k public repositories, respectively, having at least 10 stars on GitHub. We count only repositories having at least 10 stars in an attempt to remove toy/personal projects which may inﬂate the counting, while actually resulting in little training material (e.g., projects composed by single HelloWorld ﬁle). As for low-resource languages, we focus on Julia, Lua, R, and Racket. All these languages have been already considered as low-resource in previous studies [2], [3], [19]. Also, their prevalence on GitHub is substantially lower than that of Java and Python, with 6k 10+ stars GitHub repositories written in Julia, 16k in Lua, 16k in R, and 1k in Racket. B. Context Selection: LLMs As subject LLMs, we selected two (families of) open source models and the commercial tool Copilot [1]. As for the open source models, we use DeepSeek Coder [6] and Code Llama [7]. Both these families of models achieved state-ofthe-art performance across code-related benchmarks [6], [7]. DeepSeek Coder is family of deep learning models trained on 2 trillion tokens including code written in the six programming languages considered in our study. We experiment with all three DeepSeek Coder variants, featuring 1B, 7B, and 33B trainable parameters. Code Llama is based on Llama 2 [23], and has been obtained by further training the Llama 2 checkpoint on coderelated dataset. Although the complete list of programming languages used for its training is not publicly available, it is known to feature at least Java, Python, Julia, and Lua.1 We consider the 7B and 13B variants of Code Llama. While larger Code Llama versions exist (up to 70B), their inference is extremely expensive. Also, the combination of the considered variants of DeepSeek Coder and Code Llama provides us with good distribution of models sizes, including 1B, 7B, 13B, and 33B models. For the purpose of our study, we consider the instruct versions of these models, which support code generation based on natural language prompts providing speciﬁc instructions. In addition to open source models, we also consider GitHub Copilot [1], which is built on top of the Codex model [24]. For the sake of simplicity, we may use the term model throughout the paper to refer to both open source models and Copilot. C. Models Evaluation To evaluate the code generation capabilities of the six models on the six languages we resort to the MultiPL-E benchmark proposed by Cassano et al. [19]. MultiPL-E consists of two benchmarks of Python programs with documentation and unit tests, namely HumanEval (164 programs) and Mostly Basic Python Problems (974 programs)MBPP. Both benchmarks have been translated to 18 programming languages, including all those considered in our study. For our experiments, we consider only the HumanEval dataset with its translations, since it features less simplistic programs compared to MBPP. The MultiPL-E benchmark features 161 out of the 164 original HumanEval programs, since three of them are too Pythonspeciﬁc and could not be translated into other languages. Also, only 157 out of the 161 programs are common to all languages, since some of them could not be translated to speciﬁc languages (e.g., 3 for Java and 2 for Julia). For this reason, in our study, we only consider this subset of 157 programs. Each program represents speciﬁc function to implement, as described in its documentation. The benchmark is designed to evaluate the correctness of the code generated by model by checking whether it passes the provided unit tests. The reference metric for evaluation and comparison is pass@k, where indicates the number of generations (attempts) model is allowed to make. If any of the generations pass all unit tests, then pass@k = 1, otherwise pass@k = 0. For the purpose of statistical signiﬁcance, the generations are repeated times, and then averages are computed. We compute pass@1 with = 50 repetitions. This is in line with the work of Cassano et al. [19], who use pass@1 as their main evaluation metric and state that this rate appears to stabilize at = 20. Similarly to them, we set the models temperature to 0.2 when generating predictions. The temperature is used to control the randomness of the models predictions, with 0 being the lowest and 1 the highest. 1https://github.com/meta-llama/codellama/issues/53 The input prompt used for code generation is composed of the function description and its signature. The models, once triggered, are expected to ﬁnalize the implementation. We set the maximum input and output length for all open source models to 1,024 tokens. To generate the predictions with GitHub Copilot, we develop an AppleScript program that automatically opens Visual Studio Code, accesses the ﬁle containing the function to complete, triggers Copilot and saves the generated code. The process is repeated for = 50 independent sessions, so as to compute the pass@1 rate. D. Data Analysis For RQ1, we are interested in comparing the performance of the same model across different languages, speciﬁcally to observe gaps in performance between highand low-resource languages. To this aim, we report the pass@1 score achieved by each model on each language. We also statistically compare these distributions, namely the correct/wrong predictions generated by each model on each run for different languages. This is possible since the code generation problems are the same across all languages. To make concrete example, when contrasting the performance of DeepSeek Coder 1B on Java and R, we consider two distributions composed of 157 programs 50 repetitions = 7,850 pass@1 values. We use the McNemars test [25], which is suitable to do pairwise comparisons of dichotomous results of two different treatments. We adjust p-values using the Benjamini-Hochberg procedure [26] to account for multiple comparisons (e.g., the performance of Copilot on Julia is compared against what the same model achieves on both Java and Python). We complement the McNemars test with the Odds Ratio (OR) effect size in order to better quantify the magnitude of the differences between the treatments. E. Results Discussion Table shows the average pass@1 score achieved across the 50 repetitions by each model (rows) on each language (columns). The color schema adopted in Table assigns black background to the language on which each model performs best, and white background to the language on which the model performs the worst. For example, Copilot achieves the best performance on Python (61.7% of pass@1) and the worst one on Racket (24.7%). In general, the darker cell, the better the performance of model on that language as compared to the other languages. Table II reports the ORs output of the McNemars tests comparing the performance of the different models (columns) for speciﬁc pairs of languages (rows). We do not report the test results for all possible pairs of languages (e.g., Java vs. Python is not shown), since we are mainly interested in comparing high-resource vs. low-resource languages. Other tests shown in Table II (e.g., Julia vs. R) are motivated by our ﬁndings, discussed in the following. While there is no clear deﬁnition of low-resource language, the results in Table basically show three clusters of languages. The ones that are clearly high-resource (i.e., Python and Java) are those on which, usually, all models exhibit their best performance, with very few exceptions (e.g., Copilot works slightly better on Lua as compared to Java). The second cluster features Julia and Lua, two languages considered as low-resource in previous work but on which models are usually able to provide performance close to the ones observed for Python and Java (pending the exception of DeepSeek Coder 1B on Julia). Finally, the third set of languages features and Racket, which are the worst supported ones by all models. These results also suggest that the number of public repositories on GitHub written in speciﬁc language is not good discriminator to identify low-resource languages possibly being problematic. For example, we found that all models work better on Julia as compared to R, despite the availability of more than Julia repositories on GitHub, with still both languages being poorly represented. This may be due to several factors, such as the similarity between these languages and other high-resource languages, or the average amount of training material within each repository (e.g., Julia repositories may be signiﬁcantly larger than ones). For and Racket, pass@1 ranges between 7% and 33.1%, while for Julia and Lua it stays between 19.2% and 61.4%. For reference, pass@1 for Java and Python ranges between 30.6% and 74.9%. Overall, the average performance gap between Java/Python and each of the low-resource languages is 10.9%/20.8% for Julia, 3.6%/13.5% for Lua, 24.3%/34.2% for and 28.7%/38.6% for Racket. The differences observed between highand low-resource languages, as well as the differences between Julia/Lua and R/Racket, are all statistically signiﬁcant (p-value < 0.05) for all models evaluated, except for the difference between Python and Lua for Copilot. The ORs, reported in Table II, help in quantifying the differences in performance achieved by the models between languages. For instance, in the comparison between Java and Julia for DeepSeek Coder 1B, the OR is 5.93, meaning that the odds of generating correct program in Java are about 5 times higher than in Julia. All ORs reported are higher than 1 (except for the comparison between Java and Lua for Code Llama 7B and Copilot), indicating that high-resource languages achieve greater performance than low-resource languages, and that Julia and Lua perform better than and Racket. Indeed, the ORs between Java/Python and Julia/Lua are closer to 1 than those between Java/Python and R/Racket: in the former case, ORs range between 0.72 and 19.34, while in the latter case, ORs range between 4.05 and 251.59. The performance gap between Julia/Lua and R/Racket is also signiﬁcant, with ORs ranging from 1.74 to 14.16. We analyzed sample of the generated programs to understand the reasons behind the performance gap for each low-resource language. In some cases, we observed syntactical errors in the form of patterns, constructs or APIs not supported by the low-resource language but resembling those of highresource languages. For instance, some predictions in Julia contained references to push function, although the correct API is push!. TABLE PASS@1 RATES OF THE MODELS ON THE DIFFERENT LANGUAGES. BLACK CELLS INDICATE BEST PERFORMANCE PER MODEL, WHITE CELLS INDICATE WORST PERFORMANCE. PERFORMANCE IN BETWEEN IS GRAY-SCALED. Model DeepSeek Coder - Instruct DeepSeek Coder - Instruct DeepSeek Coder - Instruct Code Llama - Instruct Code Llama - Instruct GitHub Copilot Average Size 1B 7B 33B 7B 13B Unknown Julia 19.2 41.2 43.3 28.4 33.4 53.5 36.5 Lua 31.3 51.6 53.8 32.3 32.6 61.4 43.8 14.3 30.4 30.9 14.2 15.6 32.9 23.1 Racket 7.0 20.9 33.1 11.5 15.2 24.7 18.7 Java 41.9 58.1 57.8 30.6 38.7 57.2 47. Python 61.3 74.9 67.5 33.7 44.5 61.7 57.3 Ì"
        },
        {
            "title": "Python generation",
            "content": "Ì"
        },
        {
            "title": "Lua generation",
            "content": "ê"
        },
        {
            "title": "R generation",
            "content": "from typing import List def filter_by_substring( strings: List[str], substring: str) -> List[str]: \"\"\" Filter an input list of strings only for ones that contain given substring >>> filter_by_substring([], a) [] >>> filter_by_substring([abc, bacd, cde, array], a) [abc, bacd, array] \"\"\" return [s for in strings if substring in s] -- Filter an input table of strings -- only for ones that contain given -- substring -- >>> filter_by_substring({}, a) -- ... local function filter_by_substring( strings, substring) local result = {} for _, in ipairs(strings) do if string.find(s, substring) then table.insert(result, s) end end return result end # Filter an input list of strings # only for ones that contain given # substring # >>> filter_by_substring(c(), a) # list() # >>> filter_by_substring(c(abc, # bacd, cde, array), a) # list(abc, bacd, array) filter_by_substring <- function(strings, substring) { strings[ grepl(substring, strings) ] } Fig. 1. DeepSeek Coder 33B generations on HumanEval 7 ﬁlter by substring problem from the translated HumanEval dataset. TABLE II ODDS RATIOS COMPUTED BETWEEN PAIRS OF LANGUAGES FOR EACH MODEL EVALUATED (DS = DEEPSEEK CODER, CL = CODE LLAMA). VALUES IN BOLD ARE STATISTICALLY SIGNIFICANT (p-VALUE < 0.05). Languages Java vs. Julia Java vs. Lua Java vs. Java vs. Racket Python vs. Julia Python vs. Lua Python vs. Python vs. Racket Julia vs. Julia vs. Racket Lua vs. Lua vs. Racket DS-1B 5.93 2.04 8.32 22.88 19.34 9.31 25.61 251.59 1.74 4.98 4.80 14.16 DS-7B 2.69 1.50 4.31 12.08 8.66 4.27 11.56 42.97 2.09 4.39 3.90 8.13 DS-33B 2.61 1.28 4.05 4.33 5.40 2.47 7.21 8.06 2.10 1.80 3.63 2.91 CL-7B 1.27 0.85 4.16 11.21 1.79 1.13 5.72 11.88 3.98 9.92 4.96 9.69 CL-13B 1.65 1.75 4.97 9.17 2.59 2.69 10.93 16.56 4.37 5.93 4.02 6.24 Copilot 1.34 0.72 4.82 8.91 1.70 1.02 6.00 9.97 3.79 7.88 6.57 12. This error may be due to the prevalence of functions named push in other high-resource programming languages. We also speculate that the performance gap may be due to not only the amount of training data, but also the use cases of each language. For example, is widely used in data analysis and statistics, while not so much in other domains, which may explain low performance in the HumanEval dataset. Furthermore, languages adopting less common programming paradigms (e.g., functional programming) may be more challenging than others adopting more common ones (e.g., imperative and object-oriented programming), due to the fact that the training data may not be representative of the target usage scenarios. Fig. 1 shows an example of HumanEval problem (#7) generated by DeepSeek Coder 33B in three languages, namely Python, Lua, and R. This problem requires the implementation of function that accepts two arguments, i.e., list of strings and substring, and returns the list of strings that contain the substring. As illustrated, the Python and Lua generations are correct, passing all test cases from the benchmark. Conversely, the generation fails because: (i) when the substring is not found in any of the strings, the function should return an empty list, but it returns null object instead; and (ii) the function should always return list, but it returns vector object instead. This example reveals yet two more reasons for the performance gap between highand low-resource languages: (i) the inability to handle edge cases properly; and (ii) the inability to correctly understand requirements and translate them to the target lowresource language. Answer to RQ1: Our results show that modern LLMs are able to successfully deal with some low-resource languages, such as Julia and Lua, with limited loss in performance as compared to high-resource languages. For other low-resource languages (i.e., and Racket), instead, the gap in performance is major and requires the investigation of techniques to boost the LLMs performance. This will be the focus of our next study. IV. STUDYING TECHNIQUES TO BOOST CODE GENERATION ON LOW-RESOURCE LANGUAGES Given the ﬁndings of RQ1 showing major gap in LLMs code generation performance between highand low-resource languages (in particular, and Racket), we formulate our next research question: RQ2 Which techniques are best suited to improve the code generation capabilities of state-of-the-art LLMs in lowresource programming languages? As context of our study, we exploit the same LLMs studied in RQ1 (i.e., DeepSeek Coder 1B, 7B, and 33B [6], Code Llama 7B and 13B [7], and GitHub Copilot [1]). As lowresource languages we target and Racket, being those for which we observed major drop in performance. A. Experimented Techniques We present here the techniques we investigate. We can classify them into two main categories: in-context learningbased and ﬁne-tuning-based. The former aims at improving the generation capabilities of pre-trained model without updating its weights. This would be the cheapest solution in terms of time and resources for anyone wanting to improve model performance in niche programming languages. The latter, instead, requires further training of the model on supplementary data related to the target low-resource language. This approach is considerably more expensive and cannot even be applied for some commercial tools (e.g., Copilot). We explore ﬁve different techniques, three of which are based on in-context learning and two on ﬁne-tuning. 1) In-context Learning Translation Examples: The idea is to create prompt featuring examples of translation between high-resource language and the targeted low-resource language. In particular, we provide the model with two examples of translations, considering Python as the high-resource language due to its popularity and succinctness. Both aspects are important since (i) the basic assumption is that the model is likely to better understand high-resource languages, and (ii) the examples included as input should be concise, to take as little space as possible in the prompt. Indeed, the inference cost of the models increases with the increase in size of the prompt. Listing 1 illustrates the prompt we use for this technique. Listing 1: Prompt featuring translation examples. This is an example of Python function: <PYTHON_FUNCTION> This is the equivalent in <TARGET_LANGUAGE>: <TRANSLATED_FUNCTION> This is another example of Python function: <PYTHON_FUNCTION> This is the equivalent in <TARGET_LANGUAGE>: <TRANSLATED_FUNCTION> <CODE_GENERATION_PROMPT> For each target programming language, we show the model translation examples (from Python to the low-resource laninclude basic aspects of language, such as guage) that conditional expressions and variable assignments, as well as more complex features like data structures and function calls. The examples have been extracted from the previously discussed MultiPL-T dataset [2] (Section II). The goal is for the prompt to assist the model in transferring its expertise from well-known programming language like Python to less common one. The <CODE_GENERATION_PROMPT> shown at the end is the same used in RQ1, namely the description of the function to generate followed by its signature. 2) In-context Learning Translation Rules: This technique aims to instruct the model with pre-deﬁned set of mapping rules from high-resource language, i.e., Python, to the target low-resource language, e.g., R. Listing 2 shows some examples of mapping rules for transforming Python code into code. The translation rules have been deﬁned by the ﬁrst author by looking at the ofﬁcial documentation of the involved languages (Python, R, Racket), trying to map the main building blocks of the languages. Listing 2: Prompt featuring translation rules. Here are some general mapping rules to translate Python code into code: 1. Variable Declaration: Replace = with <-. Python: <VAR> = 1 R: <VAR> <- 1 ... 5. Array Declaration: Replace [] with c(). Python: <VAR> = [<ELEMENTS>] R: <VAR> <- c(<ELEMENTS>) ... Employ these rules to generate code, leveraging your Python knowledge. First try to understand the code and then convert it to R. <CODE_GENERATION_PROMPT> The translation rules involve both simple programming constructs, such as variable deﬁnitions and control ﬂow statements, as well as more advanced features like data structures and error handling. Unlike the previous approach, this technique does not require translation examples, which may be hard to ﬁnd for low-resource languages and challenging to manually craft. The used prompts are fully available in our replication package [27]. 3) In-context Learning Few-shot: Finally, we evaluate the in-context learning capabilities of the models by enriching the prompt with examples of the task to accomplish in the speciﬁc low-resource language of interest. In this case, an example (shot) represents description of function to implement and possible correct implementation. This technique has proved to increase performance for models not trained at all on the target language (i.e., out-of-domain languages) [5]. We decided to use 2-shot prompting, providing the models with two examples mirroring those presented in the translation examples prompt: Rather than showing the model with two pairs of Python code and its corresponding low-resource language translation, we only provide the low-resource code with its corresponding description. Using the same examples among the two techniques allows for better comparison between them. Indeed, if we use different examples and observe that one technique is superior to the other, it would be difﬁcult to conclude whether this is due to the prompt schema (i.e., translation vs. implementation examples) or to the showed code. 4) Fine-tuning Code Generation: Previous research has shown that ﬁne-tuning model can effectively improve its code generation performance on low-resource languages [2]. Fine-tuning model requires code datasets in the target lowresource languages. To this end, we reuse the datasets released by Cassano et al. [2], featuring 133,168 Python functions and their translations into multiple languages, including and Racket. The Python dataset is curated subset of The Stack [28] which features only functions having high-quality docstring and concrete body implementation that passes test cases having high statement coverage. The process adopted for the translation is the one we previously detailed in Section II. Cassano et al. managed to successfully translate 37,592 functions to and 40,489 to Racket. Each of these entries features: docstring (D), signature (S), and concrete body implementation (B). As ﬁne-tuning dataset for the task of code generation, we combine these elements to generate (i) an instruction, i.e., the prompt provided in input to the models, by combining the function docstring and the signature hD, Si; and (ii) an expected output, i.e., the body implementation hBi. 5) Pre-training and Fine-tuning Code Translation and Generation: Lastly, we evaluate the usefulness of tailored pre-training task which can further help the model in the code generation. The idea of pre-training is usually to provide the model with basic understanding of the language of interest, without specializing it for any task (which is the goal of the ﬁne-tuning). However, recent works from the NLP community [29] suggested that using pre-training objective that more closely resembles the downstream task leads to better ﬁnetuning performance. In our case, the downstream task is the code generation on the low-resource language. Thus, we start by pre-training the model for the task of code translation from high-resource language (Python) to the language of interest (i.e., or Racket). This requires pairs of Python functions (S) and their translation (T) into the target language. The dataset by Cassano et al. [19] that we used to build the previously described ﬁne-tuning datasets does not provide information about the original Python code from which each translated instance has been obtained. Thus, we adopt the following process to build the code translation pre-training dataset. For each entry in our ﬁne-tuning datasets (i.e., 37,592 functions and 40,489 Racket functions), we looked for the corresponding Python function in the original dataset [19] trying to match the function name and its docstring. The matching of the docstring was not trivial since, during the translation process adopted by Cassano et al., the authors performed some processing steps aimed at aligning the docstring to the target language. For example, the term dictionary in Python docstring was replaced by hash table in Racket [19]. Since our goal was to only collect high-quality translation pairs, whenever there were doubts about the matching of the original Python function (e.g., multiple functions were matched all having the same name) we just excluded the instance. This resulted in two code translation datasets (one for and one for Racket), featuring pairs of hS, that we can use to pre-train the model before ﬁne-tuning it. The dataset features 22,796 pairs, while the Racket one features 25,390. During pre-training, we provide the models with the prompt Translate the following Python function to [R/Racket]. It is important to note that while pre-training is usually performed to initialize the models weights, the code translation pretraining we perform is done on top of already trained models (i.e., the released versions of DeepSeek Coder and Code Llama). In this sense, this can be seen as ﬁrst ﬁne-tuning (code translation) followed by second ﬁne-tuning (code generation). However, we refer to it as pre-training just to make it clear that it comes before the code generation ﬁnetuning and aims at providing the model with highto lowresource translation examples which may become useful at code generation time. B. Training Procedure While in-context learning-based techniques do not require any additional training, ﬁne-tuning-based techniques require training the models on the above-described datasets. In addition, note that the ﬁne-tuning-based techniques have only been applied to the models of the DeepSeek Coder and Code Llama families, since it is not possible to ﬁne-tune Copilot. For all trainings, we adopt the default parameters of the models. In particular, we train the DeepSeek Coder models with DeepSpeed,2 deep learning optimization library for faster training. The learning rate is set at 2 105, with the AdamW optimizer [30] and cosine scheduler. Code Llama is trained with learning rate of 5105, AdamW optimizer [30] and linear decay scheduler. For all models the maximum sequence length is set to 2,048 tokens, to account for additional information introduced in the prompt (e.g., Python function to translate in the pre-training task). We use bfloat16 mixed precision for training as it is supported by our hardware infrastructure, being an HPC cluster featuring 8 NVIDIA A30, 32 NVIDIA A40 and 8 NVIDIA A100 GPUs. The batch size is adapted according to the resources available and the size of the trained model (1B, 7B, 13B or 33B parameters), with larger models using smaller batch size. All models have been trained for three epochs on the whole datasets, since we did not observe substantial beneﬁts in terms of loss when moving from the second to the third epoch. We acknowledge that training for longer epochs could result in slightly improved results. However, the training cost, especially for models such as DeepSeek Coder 33B, is extremely high. We further discuss this point in the threats to validity (Section V). 2https://github.com/microsoft/DeepSpeed C. Models Evaluation and Data Analysis Similarly to the evaluation procedure to answer RQ1, we again leverage the MultiPL-E benchmark [19] to evaluate the code generation capabilities of the models. More speciﬁcally, we evaluate each of the six models with each of the ﬁve techniques (except for Copilot, for which only in-context learning techniques are evaluated) for each of the two lowresource languages. In this case, however, we consider the full set of 161 programs provided in the MultiPL-E benchmark, since all translations are present for both and Racket. For ﬁne-tuned models, we evaluate each epoch on the MultiPL-E benchmark and only report the best models results. Indeed, depending on the model size, training on small dataset for many epochs may result in performance degradation. We provide the performance of our ﬁne-tuned models on all three epochs in our replication package. [27]. As previously stated, for ﬁne-tuning the maximum input length was set to 2,048 tokens, since this was sufﬁcient. However, for in-context learning-based techniques, we set it to 3,072 tokens to account for the additional, longer instructions introduced in the prompt (e.g., multiple code generation examples). As in our previous study, our evaluation metric is the pass@1 rate with = 50 repetitions. In this case, instead of performance across languages, we compare performance across techniques. We statistically analyze the results in the same way as in RQ1, using the McNemars test [25] and the Odds Ratio (OR) effect size to quantify the magnitude of the differences between the treatments. We compare the distributions of pass@1 rates obtained for each technique, namely 161 programs 50 repetitions = 8,050 pass@1 values. We adjust p-values using the Benjamini-Hochberg procedure [26] to account for multiple comparisons. For instance, the performance of DeepSeek Coder 1B ﬁne-tuned for code generation in is compared against the performance of the same model in the same language leveraging other techniques, including the baseline model. D. Results Discussion Table III reports the average pass@1 rates obtained per model, size, technique and language. The white rows represent the baseline values, namely the model used out of the box on the low-resource languages. Light gray rows correspond to in-context learning-based techniques, while dark gray rows correspond to ﬁne-tuning-based techniques. Best-performing techniques for each model, size, and language are highlighted in bold. The results of the statistical tests (adjusted p-value plus OR) are included in our replication package [27], as they are 162 tests in total (considering all pairs of techniques plus the baseline for each model, size, and language). All ORs reported in the text are statistically signiﬁcant unless otherwise noted. There are two observations that can be immediately made from the analysis of Table III. First, there is no silver bullet, in the sense that no speciﬁc technique results to be the best across all combinations of model, size, and language. Second, the size of the models seem to play role in the family of techniques performing the best. Indeed, for the smallest model considered in our study (i.e., DeepSeek Coder 1B), the ﬁne-tuning-based techniques are the best-performing ones, with both of them improving the baseline, especially for Racket (from 7.0% to 18.4% with pretraining & ﬁne-tuning). Both ﬁne-tuning-based techniques achieve pass@1 values being signiﬁcantly higher than all in-context learning-based approaches for the 1B model, with ORs going from 1.45 to 14.01. When scaling up to larger models, we observe that in-context learning-based techniques become more and more performant at the expense of lower effectiveness of ﬁnetuning-based approaches. Indeed, for the 7B (DeepSeek Coder and Code Llama) and 13B (Code Llama) models there is no clear trend, with the in-context learning-based approaches working better for R, and the ﬁne-tuning-based working better for Racket. Finally, for the largest model for which we can compare the two families of techniques (i.e., DeepSeek Coder 33B), the in-context learning approaches always work substantially better than the ﬁne-tuning ones. For example, when comparing the best performing in-context learning technique (i.e., few-shot for and translation examples for Racket) with the best among the ﬁne-tuning-based approaches (i.e., pretraining & ﬁne-tuning for and ﬁne-tuning only for Racket) the gap in pass@1 scores is +12.5% on and +8.3% on Racket. These differences are accompanied by ORs of 2.79 and 2.06, respectively. Essentially, it seems that small models can substantially beneﬁt from the (limited) ﬁne-tuning possible for low-resource languages, while they suffer when it comes to the interpretation of complex prompts such as those used with in-context learning. On the contrary, the larger the model the lower the beneﬁt brought by ﬁne-tuning, with the largest DeepSeek Coder model (33B) even observing decrease in performance over the baseline due to the ﬁne-tuning (ORs of 1.64 for and 1.42 for Racket). This may suggest that the limited amount of training data available for low-resource languages is not sufﬁcient to properly update the models weights, thus just resulting in deterioration of what the model learned during its original training (i.e., baseline). it is worth noting that While comparison with ﬁne-tuning techniques is not in-context learning helps possible, GitHub Copilot as well. The boost in performance obtained on is substantial, with few-shot prompting ensuring +8.4% of pass@1 over the baseline (OR=1.94). Considering the already good performance of this model and how cheap in-context learning is, this is notable result. In Racket, prompting with translation examples is the best technique, although with more limited improvement (+2.8%, with OR=1.33). Until now we mostly compared the two families of techniques, namely in-context learning-based and ﬁne-tuningbased. We now discuss which speciﬁc technique seems to work better within each family. Among the ﬁne-tuning ones, the pretraining task we devised (i.e., code translation) does not seem to provide any relevant boost. Indeed, ﬁne-tuning only works better than pre-training & ﬁne-tuning for 3 out of 5 models in and 4 out of 5 in Racket. 1B 7B"
        },
        {
            "title": "Size",
            "content": "DeepSeek Coder - Instruct DeepSeek Coder - Instruct DeepSeek Coder - Instruct TABLE III PASS@1 RATES BY MODEL, SIZE, AND TECHNIQUE. LIGHT GRAY ROWS ARE IN-CONTEXT LEARNING-BASED TECHNIQUES, DARK GRAY ROWS ARE FINE-TUNING-BASED TECHNIQUES. VALUES IN BOLD DEPICT THE BEST-PERFORMING TECHNIQUE PER MODEL, SIZE AND LANGUAGE. Racket Model 7.0 7.7 6.5 8.4 18.1 18.4 20.4 22.5 20.0 24.6 31.7 30.4 32.5 36.3 35.8 36.2 28.0 26.8 11.2 12.1 11.1 12.7 22.0 19.7 14.8 16.1 14.2 13.9 22.3 20.7 24.3 27.1 25.1 25.7 Technique Baseline In-context Learning Translation Examples In-context Learning Translation Rules In-context Learning Few-shot Examples Fine-tuning Code Generation Pre-training & Fine-tuning Code Translation and Generation Baseline In-context Learning Translation Examples In-context Learning Translation Rules In-context Learning Few-shot Examples Fine-tuning Code Generation Pre-training & Fine-tuning Code Translation and Generation Baseline In-context Learning Translation Examples In-context Learning Translation Rules In-context Learning Few-shot Examples Fine-tuning Code Generation Pre-training & Fine-tuning Code Translation and Generation Baseline In-context Learning Translation Examples In-context Learning Translation Rules In-context Learning Few-shot Examples Fine-tuning Code Generation Pre-training & Fine-tuning Code Translation and Generation Baseline In-context Learning Translation Examples In-context Learning Translation Rules In-context Learning Few-shot Examples Fine-tuning Code Generation Pre-training & Fine-tuning Code Translation and Generation Baseline In-context Learning Translation Examples In-context Learning Translation Rules In-context Learning Few-shot Examples 13.9 13.8 13.4 14.1 16.7 16.0 29.6 32.1 30.0 30.9 26.4 25.0 30.2 36.5 33.6 38.3 25.3 25.8 13.9 15.8 12.3 14.6 14.6 15.7 15.2 18.9 17.2 19.7 16.6 15.6 32.7 37.3 34.4 41. Code Llama - Instruct Code Llama - Instruct"
        },
        {
            "title": "Unknown",
            "content": "13B 33B 7B Also, in general, there are no huge gaps in performance among these two techniques. For example, the average pass@1 across all models when using ﬁne-tuning only on is 19.92% against the 19.62% obtained with the pre-training. As for the in-context learning-based, it is clear that translation rules is the one performing the worst, worsening the performance of the baseline for 2 out of 6 models in and 4 out of 6 in Racket. The other two prompts showcasing examples to the models show good performance instead. Few-shot is the best in-context learning-based technique for 4/3 models on R/Racket, while translation examples is the best for the remaining 2/3. When looking at the overall picture, however, there is one important observation to be made: In-context learning with translation examples represents safe bet to improve the code generation capabilities of state-of-the-art models in low-resource languages. Indeed, excluding the 1B model which, as said, is likely too small for such complex prompts, this technique is able to always improve the baseline (i.e., all models and languages), while few-shot learning worsened performance for Code Llama 13B in Racket. On the other hand, using translation examples, the deltas over the baseline go from +0.9% (Code Llama 7B on Racket) to +6.3% (DeepSeek Coder 33B on R). These deltas are statistically signiﬁcant, with ORs ranging between 1.28 and 2.27. Listing 3 depicts representative example of how in-context learning with translation examples can help code generation. The model was asked to implement an function that rescales list of numbers to the unit interval. The baseline model (DeepSeek Coder 33B), while able to understand the logic behind the function, fails to return list objectwhich is explicitly required by the prompt (lines 23 and 25)and returns vector object instead. When augmenting the prompt with translation examples, the model successfully leverages those to address this limitation, by reusing (not copying) part of the code from both examples, including the creation of list object (lines 16 42) and the calculation of the rescaled values in for loop (lines 7-9 43-45). Both ﬁne-tuned versions of the model fail to correctly implement the function. Answer to RQ2: While smaller LLMs may beneﬁt from ﬁne-tuning on low-resource languages, in-context learning showcasing translation examples is safe bet, being cheap and always boosting performance. Listing 3: Generation in by the baseline and using incontext learning with translation examples. Prompt prefix: This is an example of Python function: <PYTHON_FUNCTION> This is the equivalent in R: dot_product <- function(vector_one, vector_two) { total <- 0 for (i in 1:length(vector_one)) { total <- total + vector_one[i] * vector_two[i] } return(total) } This is another example of Python function: <PYTHON_FUNCTION> This is the equivalent in R: convert_sv_types <- function(field_dict) { new_dict <- list() for (key in names(field_dict)) { value <- field_dict[key] new_dict[[key]] <- list() ... Prompt: # Given list of numbers (of at least two # elements), apply linear transform to that # list, such that the smallest number will # become 0 and the largest will become 1 # >>> rescale_to_unit(c(1.0, 2.0, 3.0, 4.0, 5.0)) # list(0.0, 0.25, 0.5, 0.75, 1.0) rescale_to_unit <- function(numbers) { Baseline generation: min_num <- min(numbers) max_num <- max(numbers) range_num <- max_num - min_num (numbers - min_num) / range_num } In-context learning with transl. examples generation: min_num <- min(numbers) max_num <- max(numbers) range_num <- max_num - min_num rescaled <- list() for (i in 1:length(numbers)) { rescaled[[i]] <- (numbers[i] - min_num) / range_num } return(rescaled) } 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 35 36 37 38 39 41 42 43 44 45 47 V. THREATS TO VALIDITY Construct validity. possible threat in this regard is how we assess the code generation capabilities of the models, for which we rely on the pass@k metric. The pass@k metric may be subject to bias, e.g., due to the randomness of the models generations or if the programs from the benchmark do not include thorough test cases. To mitigate this threat, we adopted the same strategy as previous work [19], setting = 1 with low temperature value for the models generations (0.2) and high number of repetitions (n 20). Internal validity. For the models evaluated, we did not perform hyperparameter tuning as this would have required signiﬁcant amount of time and computational resources. We used the default conﬁgurations suggested by the authors of the models. For in-context learning-based techniques, the prompt has signiﬁcant impact on the performance of the models. While we may have used suboptimal prompts, we partially alleviated this threat by experimenting with three different incontext learning techniques. We acknowledge that better results may be obtained with further prompt-tuning. Also, when ﬁne-tuning we limited the number of epochs to 3, which may have capped the performance in some cases. As explained, we never observed substantial improvements when moving from 2 to 3 epochs and, as also shown in the literature [19], longer training on these small datasets may actually decrease performance. External validity. We decided to focus our study on four low-resource languages (Julia, Lua, and Racket), one closed-source tool (GitHub Copilot), two open source models (DeepSeek Coder and Code Llama) and four model sizes (1B, 7B, 13B and 33B parameters). Our ﬁndings may not generalize to other settings, although they entail representative sample as illustrated by the differences in performance observed across languages and models. VI. CONCLUSION AND FUTURE WORK Programming languages such as Julia, Lua, and Racket are considered low-resource languages, as they lack the amount of publicly available data that high-resource languages like Java and Python have. This scarcity may affect the performance of LLMs in code generation tasks. Our work showed that modern LLMs may be able to deal with some of these languages (i.e., Julia and Lua), possibly due to their resemblance to the high-resource ones. Indeed, we found that LLMs such as DeepSeek Coder and GitHub Copilot generate correct code over 50% of the times in these languages, similarly to what achieved for Java and Python. This suggests that the amount of training data is only one of the factors impacting the performance in code generation, and that languages being considered problematic in the past, may not pose any peculiar challenge nowadays with modern LLMs for the task of code generation. Languages such as and Racket, instead, are still lagging behind. This entails an obstacle for the adoption of LLMs to support automated code generation. We therefore investigated ﬁve strategies to boost LLMs code generation performance on these languages, three of them based on in-context learning and two on ﬁnetuning. Our main ﬁndings show that ﬁne-tuning may be the best alternative for small models (1B parameters), which may be unable to leverage complex prompts and instructions. On the other hand, in-context learning remains safe and cheap bet in all cases (i.e., it always improves performance), especially for larger models (e.g., over 30B parameters). Future research should pursue larger studies involving more low-resource languages (e.g., [31] and Haskell [32]) and LLMs (e.g., Phi [33] and CodeGemma [34]), as well as further strategies to boost performance such as retrieval-augmented generation [35], chain-of-thought prompting [36], and agentbased solutions. VII. ACKNOWLEDGMENTS We acknowledge the ﬁnancial support of the Swiss National Science Foundation for the PARSED project (SNF Project No. 219294). Giagnorio also thanks CHOOSE for sponsoring his trip to the conference."
        },
        {
            "title": "REFERENCES",
            "content": "[1] GitHub"
        },
        {
            "title": "Copilot",
            "content": ""
        },
        {
            "title": "Your",
            "content": "AI pair programmer, https://github.com/features/copilot/, accessed: 2024-03-10. [2] F. Cassano, J. Gouwar, F. Lucchetti, C. Schlesinger, A. Freeman, C. J. Anderson, M. Q. Feldman, M. Greenberg, A. Jangda, and A. Guha, Knowledge transfer from high-resource to low-resource programming languages for code llms, Proceedings of the ACM on Programming Languages, vol. 8, no. OOPSLA2, pp. 677708, 2024. [3] F. Chen, F. Fard, D. Lo, and T. Bryksin, On the transferability of pretrained language models for low-resource programming languages, in 30th IEEE/ACM International Conference on Program Comprehension, ICPC, 2022, pp. 401412. [4] T. van Dam, F. van der Heijden, P. de Bekker, B. Nieuwschepen, M. Otten, and M. Izadi, Investigating the performance of language models for completing code in functional programming languages: haskell case study, arXiv preprint arXiv:2403.15185, 2024. [5] B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian, M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, S. K. Gonugondla, H. Ding, V. Kumar, N. Fulton, A. Farahani, S. Jain, R. Giaquinto, H. Qian, M. K. Ramanathan, and R. Nallapati, Multi-lingual evaluation of code generation models, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [6] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li et al., Deepseek-coder: When the large language model meets programmingthe rise of code intelligence, arXiv preprint arXiv:2401.14196, 2024. [7] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950, 2023. [8] G. Orlanski, K. Xiao, X. Garcia, J. Hui, J. Howland, J. Malmaud, J. Austin, R. Singh, and M. Catasta, Measuring the impact of programming language distribution, in International Conference on Machine Learning. PMLR, 2023, pp. 26 61926 645. [9] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, Codet5: Identiﬁer-aware uniﬁed pre-trained encoder-decoder models for code understanding and generation, arXiv preprint arXiv:2109.00859, 2021. [10] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [11] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, Palm: Scaling language modeling with pathways, J. Mach. Learn. Res., vol. 24, pp. 240:1240:113, 2023. [12] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. H. Hoi, Codet5+: Open code large language models for code understanding and generation, arXiv preprint, 2023. [13] R. Li, L. B. allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. LI, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, J. Lamy-Poirier, J. Monteiro, N. Gontier, M.-H. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. Murthy, J. T. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, U. Bhattacharyya, W. Yu, S. Luccioni, P. Villegas, F. Zhdanov, T. Lee, N. Timor, J. Ding, C. S. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. V. Werra, and H. de Vries, Starcoder: may the source be with you! Transactions on Machine Learning Research, 2023, reproducibility Certiﬁcation. [Online]. Available: https://openreview.net/forum?id=KoFOg41haE [14] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [15] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang, Wizardcoder: Empowering code large language models with evol-instruct, in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=UnUwSIgK5W [16] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., Evaluating large language models trained on code, arXiv preprint arXiv:2107.03374, 2021. [17] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., Program synthesis with large language models, arXiv preprint arXiv:2108.07732, 2021. [18] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li et al., Codegeex: pre-trained model for code generation with multilingual evaluations on humaneval-x, arXiv preprint arXiv:2303.17568, 2023. [19] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M.-H. Yee, Y. Zi, C. J. Anderson, M. Q. Feldman et al., Multipl-e: scalable and polyglot approach to benchmarking neural code generation, IEEE Transactions on Software Engineering, 2023. [20] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, Advances in Neural Information Processing Systems, vol. 36, 2024. [21] T. Ahmed and P. T. Devanbu, Multilingual training for software engineering, in 44th IEEE/ACM International Conference on Software Engineering, ICSE, 2022, pp. 14431455. [22] S. Joel, J. J. Wu, and F. H. Fard, survey on llm-based code generation for low-resource and domain-speciﬁc programming languages, arXiv preprint arXiv:2410.03981, 2024. [23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and ﬁne-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [24] Codex website, https://openai.com/blog/openai-codex, [n.d.], accessed: 2023-10-08. [25] Q. McNemar, Note on the sampling error of the difference between correlated proportions or percentages, Psychometrika, vol. 12, no. 2, pp. 153157, 1947. [26] B. Yoav and H. Yosef, Controlling the false discovery rate: practical and powerful approach to multiple testing, Journal of the Royal Statistical Society. Series (Methodological), vol. 57, no. 1, pp. 289 300, 1995. [27] Replication package, https://doi.org/10.5281/zenodo.13128630, [n.d.]. [28] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf et al., The stack: 3 tb of permissively licensed source code, arXiv preprint arXiv:2211.15533, 2022. [29] J. Zhang, Y. Zhao, M. Saleh, and P. J. Liu, PEGASUS: pre-training with extracted gap-sentences for abstractive summarization, in 37th International Conference on Machine Learning, ICML, 2020, pp. 11 328 11 339. [30] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in 7th International Conference on Learning Representations, ICLR, 2019. [31] programming language, https://dlang.org/, [n.d.], accessed: 202407-24. [32] Haskell language, https://www.haskell.org/, [n.d.], accessed: 2024-0724. [33] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl et al., Phi3 technical report: highly capable language model locally on your phone, arXiv preprint arXiv:2404.14219, 2024. [34] C. Team, Codegemma: Open code models based on gemma, arXiv preprint arXiv:2406.11409, 2024. [35] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang, Retrieval-augmented generation for large language models: survey, arXiv preprint arXiv:2312.10997, 2023. [36] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in neural information processing systems, vol. 35, pp. 24 82424 837, 2022."
        }
    ],
    "affiliations": [
        "Software Institute USI Università della Svizzera italiana, Switzerland"
    ]
}