{
    "paper_title": "Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts",
    "authors": [
        "Marta Skreta",
        "Tara Akhound-Sadegh",
        "Viktor Ohanesian",
        "Roberto Bondesan",
        "Alán Aspuru-Guzik",
        "Arnaud Doucet",
        "Rob Brekelmans",
        "Alexander Tong",
        "Kirill Neklyudov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional 'corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion."
        },
        {
            "title": "Start",
            "content": "Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Marta Skreta * 1 2 Tara Akhound-Sadegh * 3 4 Viktor Ohanesian * 5 Roberto Bondesan 5 Alán Aspuru-Guzik 1 2 Arnaud Doucet 6 Rob Brekelmans 2 Alexander Tong 7 4 Kirill Neklyudov 7 4 5 2 0 2 4 ] . [ 1 9 1 8 2 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inferencetime behavior in principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional corrector steps. In this work, we provide an efficient and principled method for sampling from sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive weighted simulation scheme which we call FEYNMAN-KAC CORRECTORS (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifierfree guidance for text-to-image generation. Our code is available at https://github.com/ martaskrt/fkc-diffusion. 1. Introduction Score-based generative models, also known as diffusion models, have emerged as the model of choice across diverse generative tasks such as image generation, natural language, *Equal contribution 1University of Toronto 2Vector Institute 3McGill University 4Mila - Quebec AI Institute 5Imperial College London 6Google DeepMind 7Université de Montréal. Correspondence to: Kirill Neklyudov <k.necludov@gmail.com>. 1 Figure 1. FEYNMAN-KAC CORRECTOR Inference for annealed pt,β(x) qt(x)β=10 and product pt(x) q1 (x) densities. (x)q2 and protein simulation (Saharia et al., 2022; Sahoo et al., 2024; Abramson et al., 2024). These models leverage the ability to estimate scores of the sequence of noise-corrupted distributions and then use the learned scores to reverse the corruption process enabling high-quality generation. Thus, diffusion models aim to produce new samples from the same distribution as the training data. However, the classical paradigm of generative modeling as the problem of reproducing the training data distribution becomes less relevant for many applications including drug discovery and text-to-image generation. In practice, generative models demonstrate the best performance when tailored to specific needs at inference time. For instance, linear combinations of scores allow for concept composition (Liu et al., 2022) or for increasing image-prompt consistency as in classifier-free guidance (CFG) (Ho & Salimans, 2021). However, by modifying the scores, one loses control over the marginal distributions of the generated samples. Various approaches from the Monte Carlo sampling literature have been adapted to correct samples along trajectory to more closely match the prescribed intermediate distributions. Assuming access to an exact score, additional Langevin corrector steps with the desired invariant distribution can be applied with additional simulation steps as the only Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts practical overhead (Song et al., 2021; Bradley & Nakkiran, 2024). However, these corrector schemes are only exact in the limit of infinite intermediate steps. Accept-reject or Sequential Monte Carlo techniques may be used when the score is parameterized through scalar energy function (Du et al., 2023; Phillips et al., 2024), although these parameterizations require extra computation during training and may sacrifice expressivity in practice (Salimans & Ho, 2021; Thornton et al., 2025). While methods for sampling from mixtures or equiprobable regions of diffusion models have been proposed (Skreta et al., 2024), general solutions to accurately sample from combinations or temperings of flexibly-parameterized diffusion models with limited computational overhead remain elusive. To address these challenges, we introduce FEYNMAN-KAC CORRECTOR (FKCs), which enable efficient and principled sampling from sequence of annealed, geometric-averaged, or product distributions derived from pretrained diffusion models. To develop FEYNMAN-KAC CORRECTORS and test their efficacy, we make the following contributions: We propose flexible recipe for constructing weighted stochastic differential equations (SDEs), which account for additional terms appearing when manipulating the distribution of generated samples. As our primary examples, we derive the correction terms for multiple heuristic schemes commonly used to approximate annealed, product, or geometric averaged distributions, including CFG (Sec. 3). To simulate these weighted SDEs, we propose family of Sequential Monte Carlo (SMC) resampling schemes, which correct batch of simulated samples to closely approximate the intermediate target distributions (Sec. 4). For the problem of sampling from an unnormalized density, we demonstrate that FKC allows for sampling from variety of temperatures without retraining (Sec. 5.2). Moreover, we demonstrate that high-temperature learning, low-temperature inference scheme can be more efficient than the notoriously difficult task of directly training sampler at lower temperature. For pretrained diffusion models we demonstrate that adding FKC terms enhances compositional generation of molecules with multiple properties (Sec. 5.3) and classifier-free guidance for image generation (Sec. 5.1). 2. Background 2.1. Diffusion Models Generative modeling via diffusion models can be formulated as the simulation of the Stochastic Differential Equation (SDE) corresponding to the reverse-time process. In particular, during training, one gradually destroys samples from the data-distribution pdata(x) by simulating the following noising SDE: dxτ = fτ (xτ )dτ + στ dW τ , xτ =0 pdata(x) , (1) where fτ (xτ ) is usually some linear drift function fτ (xτ ) = ατ xτ , στ defines the scale of noise through time, and dW τ is the standard Wiener process. The drift fτ and the diffusion coefficient στ are chosen so the final density is close to the standard normal distribution pτ =1 (0, Id). The generation process then can be defined as the family of denoising SDEs in the opposite time direction (t = 1 τ ), dxt = (cid:0)ft(xt) + σ2 log pt(xt)(cid:1)dt + σtdWt , (2) where pt = p1τ is the density of the marginals induced by the noising process in Eq. (1); hence, the process starts with x0 (x 0, Id). By training model of the score functions log pt(), one can generate new samples from pdata(x) using Eq. (2) (Song et al., 2021). 2.2. Feynman-Kac PDEs While Eq. (2) describes procedure for simulating individual particles, we can also derive Partial Differential Equations (PDEs) which describe the time-evolution of the density of samples pt(x) under this SDE. We begin by describing the relevant equations for the standard SDE case. (1) Continuity Equation, which describes how the density changes when the samples move in space according to flow or ODE with drift vt dxt = vt(xt)dt = pode (x) (cid:68) = , pode (x)vt(x) (cid:69) . (3) where pode indicates the evolution only according to flow. (2) Diffusion Equation, which describes the change of the density for the pure Brownian motion with coefficient σt, pdiff (x) dxt = σtdWt = (x) . pdiff σ2 2 (4) = where pdiff denotes evolution due to the diffusion term only. The SDE in Eq. (2) can be viewed as the composition of flow and diffusion terms, where the corresponding Fokker-Planck PDE describes the combined evolution psde (x) = (cid:10), psde (x)vt(x)(cid:11) + σ2 2 psde (x). (5) However, our main focus in this work will be to study third type of PDE, which will yield weighted SDEs that we eventually use to simulate sequence of marginals other those the forward noising process p1τ (Sec. 3). (3) Reweighting Equation, which describes the change of density when samples have time-dependent log-weights wt which are updated based on the positions of samples xt, pw (x) = gt(x)pw (x) , gt(x)pw (x)dx (6) (cid:90) dwt = gt(xt)dt = where gt(x) = gt(x) 2 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts where the last equation guarantees the conservation of the normalization constant, i.e. (cid:82) dx gt(x)pw Feynman-Kac Formula We now focus on the combination of all three components to describe the Feynman-Kac PDE, (x) = 0. samples (without transport), = (cid:10), pt(x)vt(x)(cid:11) = pt(x) = dwt = ((cid:10), vt(xt)(cid:11) (cid:10) log pt(xt), vt(xt)(cid:11))dt (cid:10), pt(x)vt(x)(cid:11) (cid:18) 1 pt(x) (cid:19) pt(x) (12) (x) pFK = (cid:10), pFK (x)vt(x)(cid:11) + (x) , σ2 2 pFK (x)+ (7) + gt(x)pFK where to sample from pFK the following SDE (x), one first has to sample xt via dxt = vt(xt)dt + σtdWt , dwt = gt(xt)dt , (8) and then reweight the obtained samples using wt. Thus, (x) reflects the density of weighted samples, which difpFK fers from the density psde (x) obtained via the Fokker-Planck PDE in Eq. (5) due to the addition of reweighting terms. In practice, we account for this difference by sampling (cid:40) Categorical exp(wk ) j=1 exp(wj ) (cid:80)K (cid:41)K k=1 , (9) and returning the index x(i) as an approximate sample from pT . We discuss more refined resampling techniques in Sec. 4. For estimating the expectation of test functions ϕ, we account for the weights reweighting collection of particles, i.e., EpT [ϕ(x)] (cid:88) k=1 exp(wk ) exp(wj ) (cid:80) ϕ(xk ) . (10) This expression corresponds to Self-Normalized Importance Sampling (SNIS) estimation, which converges to exact expectation estimators when (e.g. Naesseth et al. (2019)). For justification of the validity of this weighting scheme for Feynman-Kac PDEs, see App. A. 2.3. Flexibility of Simulation for Given Marginals Given PDE describing the time-evolution of particular density pt(x), there may exist multiple simulation methods. For instance, it is well-known that the diffusion equation (4) can be simulated using an ODE (Song et al., 2021). Diffusion Continuity Through simple manipulations, we can rewrite the diffusion equation using continuity equation and change the simulation scheme accordingly (cid:19)(cid:29) (cid:28) (cid:18) = pt(x) = , pt(x) log pt(x) pt(x) σ2 2 σ2 2 = dxt = σ2 2 log pt(xt)dt . (11) The reweighting equation adds an extra dimension to the interplay between different simulation schemes. Continuity Reweighting We first recast the continuity equation in terms of reweighting, in which case the simulation changes the density solely by adjusting the weights of 3 Diffusion Reweighting We further observe that diffusion terms may be captured in the weights using pt(x) = σ2 = dwt = pt(x) = σ2 2 pt(x)(cid:0) log pt(x) + log pt(x)2(cid:1) σ2 2 ( log pt(xt) + log pt(xt)2) dt (13) In particular, using Eqs. (12) and (13) we now have an approach for translating arbitrary flow vt or diffusion σt terms into the reweighting factors, assuming access to an exact score function log pt. Such manipulations will play key role in deriving our proposed methods in Sec. 3. 3. Modifying Diffusion Inference using Feynman-Kac Correctors i=1 following (cid:0) ft + σ In this section, we propose new sampling tools for combining or modifying diffusion models at inference time using the Feynman-Kac PDEs in Sec. 2.2. To this end, consider several different pretrained diffusion models with marginals {qi (cid:1)(cid:11) + qi , t(xt)(cid:1)dt + σtdWt , t}M qi = (cid:10), qi dxt = (cid:0)ft(xt) + σ2 log qi which is the denoising SDE from Eq. (2). Note that qi may arise from training on different datasets or correspond to conditional models with different conditioning. Throughout this work, we assume access to an exact score model t(x; θi) = log qi si t(x), in part to facilitate the conversion rules introduced in Sec. 2.3 and summarized in Table 1. log qi σ2 2 (14b) (14a) At inference time, we would like to sample from modified target distribution involving these given models. While other variants are possible, we focus on the following examples: qt(x)β Annealed: panneal t,β (x) = Product: pprod (x) = Geometric Avg: pgeo t,β(x) = 1 Zt(β) 1 Zt 1 Zt(β) (x)q2 q1 (x) (15) (x)1βq2 q1 (x)β. common heuristic for sampling from the distributions in the form of Eq. (15) is to simulate according to the score function of the target density. For example, in classifier-free guidance (Ho & Salimans, 2021) we use the score of the geometric average log pgeo + β log q2 to simulate the following SDE t,β = (1 β) log q1 dxt = (ft(xt) + σ2 log pgeo t,β(xt))dt + σtdWt . (16) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts However, despite the similarity to Eq. (2), this heuristic does not sample from the prescribed marginals (including the final distribution), except in special cases. We proceed by using the pgeo t,β example to illustrate our approach. 3.1. Outline of Our Approach To remedy this, we inspect the PDE corresponding to pgeo t,β, and q2 which can be written in terms of the evolution of q1 pgeo t,β(x) = t"
        },
        {
            "title": "1\nZt(β)",
            "content": "t (x)(1β)q2 q1 (x)β. (17) Expanding and using our expressions for the Fokker-Planck equation of qi in (14), we proceed to locate terms corresponding to the simulation of an SDE with the drift log pgeo vt(xt) = ft(xt) + σ2 t,β(xt). Collecting all remaining terms of PDE (17) into weights gt(xt) we obtain the following Feynman-Kac PDE, which can be simulated using the weighted SDE in Eq. (8), along with the resampling schemes described in Sec. 4 conditional or unconditional model q2 (Karras et al., 2024). For our purposes, we will view CFG as it is usually presented an attempt to sample from the geometric average distributions pgeo (x)β. Using the conversion rules in Table 1, we derive the reweighting terms which facilitate consistent sampling along the trajectory. t,β(x) q1 (x)1βq2 Proposition 3.1 (Classifier-Free Guidance + FKC). Consider two diffusion models q1 (x) defined via (14). The weighted SDE corresponding to the geometric average of the marginals pgeo (x), q2 (x)1βq2 (x)β is (xt) + β log q2 (xt))dt t,β(x) q1 ((1 β) log q1 ft(xt)dt + σtdWt , σ2 2 β(β 1)(cid:13) (cid:13) log q1 dxt = σ2 dwt = (xt) log t (xt)(cid:13) 2 (cid:13) dt . (19) See proof in Prop. D.3. As further example, we combine CFG with product of experts in Prop. D.4. pgeo t,β (cid:68) = , pgeo t,β vt (cid:69) + σ2 pgeo t,β + pgeo t,β gt . (18) 3.3. Annealed Distribution Conversion Rules To facilitate the construction of Feynman-Kac PDEs corresponding to existing simulation schemes, in Table 1 we present the conversion rules that describe how the corresponding PDEs change for the annealed densities and the product of densities. We use these rules as building blocks when deriving our practical schemes. Computational Considerations Our recipe above can yield many different weighted PDEs for given sequence of target distributions. In practice, we would like our simulation scheme to closely approximate the intermediate targets distributions to limit the need for correction. On the other hand, for computational efficiency, we hope to obtain weights which avoid expensive divergence (cid:10), vt(x)(cid:11) or Laplacian terms (cid:10), log qi t(xt)(cid:11). Remarkably, for linear drift functions ft(x) commonly used in diffusion models (Song et al., 2021), we find that simulating according to the common heuristic in Eq. (16) yields Feynman-Kac PDE whose weights can be estimated with no additional overhead. We focus on these schemes in our examples. 3.2. Classifier-Free Guidance (CFG) CFG (Ho & Salimans, 2021) is widely-used procedure that simulates an SDE combining the scores of conditional and unconditional models with guidance weight β, (x ) + β log q2 log pt,β(x) = (1 β) log q1 (x c) In practice, q1 (x) may represent an unconditional model (or model with an empty prompt) whereas q2 (xc) is conditioned on text prompt, class, or other random variables (Ho & Salimans, 2021). Alternatively, in autoguidance techniques, q1 may be an undertrained version of stronger Next, we consider single diffusion model with the learned score log qt(x), which we use to sample from the annealed or tempered density (20) t,β (x) = qt(x)β/Zt(β) . panneal For β > 1, this can be used to generate samples from modes or high-probability regions of given models (Karczewski et al., 2024), while in Sec. 5.2 we explore the use of annealed inference in learning diffusion samplers from Boltzmann densities. The annealed target can be shown to admit the following Feynman-Kac weighted simulation scheme. Proposition 3.2 (Annealed SDE + FKC). Consider diffusion model qt(x) defined via (14). Sampling from the annealed marginals panneal (x) qt(x)β , β > 0 can be performed by simulating the following weighted SDE dxt = (ft(xt) + ησ2 log qt(xt))dt + ζσtdWt , t,β (cid:18) dwt = (β 1) (cid:10), ft(xt)(cid:11) + β log qt(xt)2 (cid:19) dt , σ2 with the coefficients (for [0, 1/2]) η = β + (1 β)a , ζ = (cid:112)(β + (1 β)2a)/β . (21) See Prop. D.1 for proof, and note that linear drifts ft(x) will lead to constant divergence terms which cancel upon reweighting in (9) and (10). We detail two choices of a. Target Score Simulation For = 0, we have η = β and ζ = 1, which yields the target score SDE whose drift corresponds to the score of the annealed target, dxt = (ft(xt) + βσ log qt(xt))dt + σtdWt . (22) 4 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Original FK-PDE Original wSDE (cid:10), qtvt (cid:11) vt(xt)dt Annealed PDE (cid:10), pt,βvt (cid:10), pt,ββvt (cid:11) (cid:11) σtdWt dwt = gtdt σ2 2 pt,β σ2 2β pt,β βgtpt,β σ2 2 qt gtqt Annealed SDE dxt = vt(xt)dt βvt(xt)dt σtdWt σt β dWt FK Corrector dwt += (β 1)(cid:10), vt (cid:11)dt β(β 1)(cid:10) log qt, vt β(β 1) σ2 (β 1) σ2 t 2 log qt2dt (cid:11)dt 2 log qtdt βgtdt time-dependent annealing: β βt βt log qtdt Proof Prop. C.1 Prop. C.2 Prop. C.3 Prop. C. Prop. C.5 Prop. C.6 Original FK-PDE Original wSDE Product PDE Product SDE dxt = FK Corrector dwt += (cid:68) , qtv1,2 σ2 2 q1,2 q1,2 g1,2 (cid:69) v1,2 dt σtdWt dwt = g1,2 dt + v2 )(cid:11) (cid:10), pt(v1 σ2 2 pt + (g1 )pt (v1 + v2 )dt σtdWt σ2 ((cid:10) log q1 (cid:11) + (cid:10) log q2 , v2 (cid:10) log q1 , log q2 (g1 + g2 )dt , v1 (cid:11)dt (cid:11))dt Prop. C. Prop. C.8 Prop. C.9 Table 1. Conversion rules for different terms of the original Feynman-Kac PDEs (FK-PDEs) and the corresponding weighted SDE (wSDE). For every term term corresponding to the original densities qt (first two columns), we present the terms corresponding to the annealed marginals pt,β(x) qt(x)β (top part) and the terms corresponding to the product of marginals pt(x) q1 (x) (bottom part). Importantly, the correctors are additive in the weight space, e.g. when transforming the Fokker-Planck equation, we transform both the continuity & diffusion equation terms and sum the corresponding correctors. References to proofs are provided in the right-most column. (x)q2 Tempered Noise Simulation For = 1/2, we have η = β). We refer to this as an SDE with (1 + β)/2, ζ = 1/ tempered noise, namely dxt = (ft(xt) + β + 1 2 σ2 log qt(xt))dt + σt β dWt . (23) We focus on these two choices of a, but note that for different β, we found that either target score or tempered-noise simulation could perform better in practice (Sec. 5). 3.4. Product of Experts (PoE) Intuitively, samples from the product of densities correspond to the generations that have high likelihood values under both models. The product can also be interpreted as unanimous vote of experts, since sample is not accepted if one of the densities is zero. Formally, consider the density pprod (x) = q1 (x)q (x)/Zt . (24) For conditional generative models, the product of densities can describe samples satisfying several conditions. For example, in image generation, we could use q(x horse)q(x sandy beach) to generate images of horse on sandy beach (Du et al., 2023). In Sec. 5.3, we demonstrate that the PoE target can be used to improve molecule generations which satisfy multiple conditions simultaneously. Again, natural heuristic is to use the score of the target product density in the reverse-time SDE (2), (xt) + log q2 (x) = log q1 log pprod (xt) , (25) In the following proposition, we further combine these rules with the annealing procedure to present the weighted SDE that samples from the marginals pprod t,β (x) (q1 (x)q (x))β. Proposition 3.3 (Product of Experts + FKC). Consider two diffusion models q1 (x) defined via (14). The weighted SDE corresponding to the product of the marginals pprod (x)q2 (x), q2 (x))β , with β > 0 is (xt)(cid:1)dt (xt) + log q2 dxt = σ t,β (x) (q1 η(cid:0) log q1 ft(xt)dt + ζσtdWt , σ2 2 (cid:10) log q1 (cid:13) (cid:13) log q1 dwt = β(β 1) + βσ2 + (2β 1)(cid:10), ft(xt)(cid:11)dt , with the coefficients (for [0, 1/2]) (xt), log q2 (xt) + log q2 (xt)(cid:11)dt (26) (xt)(cid:13) 2 (cid:13) dt (27) η = β + (1 β)a , ζ = (cid:112)(β + (1 β)2a)/β . (28) See proof in Prop. D.2. Again, note that for linear drifts, the divergence term (cid:10), ft(x)(cid:11) is constant and can be ignored. Further, for β = 1, the first term in the weight evolution vanishes to leave only the inner product of score vectors. Similarly to Eqs. (22) and (23) for annealing, we have the target score SDE (a = 0, η = β, ζ = 1) and the tempered noise SDE (a = 1/2, η = (β + 1)/2, ζ = 1/ β). 4. Resampling Methods In this section, we describe several options for utilizing the weights to improve sampling with batch of particles. While the simplest technique would be to simulate the weighted SDE in Eq. (8) for independent particles across the full time interval [0, 1] and reweight using SNIS in Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts (10), we expect these full-trajectory weights to have high variance in practice due to error accumulation. Sequential Monte Carlo Since our weights provide proper weighting scheme for all intermediate distributions ((Naesseth et al., 2019), App. A), we can leverage SMC techniques which reweight particles along our trajectories. In practice, we find that resampling only over an active interval [tmin, tmax] is useful for improving sample quality and preserving diversity, and set weights to zero outside of this interval. Within the active interval, we resample at each step based on the increment w(k) = gt(x(k) )dt, ust ing systematic sampling proportional to exp{w(k) } (Douc & Cappé, 2005). For small discretizations dt, we expect relatively low-variance weights. From this perspective, systematic resampling is an attractive selection mechanism as all particles are preserved in the case of uniform weights. Jump Process Interpretation of Reweighting Finally, by reframing the reweighting equation in terms of Markov jump process (Ethier & Kurtz (2009, Ch. 4.2)), variety of further simulation algorithms for Feynman-Kac PDEs are possible (Del Moral (2013, Ch. 1.2.2, 5); Rousset & Stoltz (2006); Angeli (2020)). Markov jump process is determined by rate function λt(x), which governs the frequency of jump events, and Markov transition kernel Jt(yx), which is used to sample the next state when jump occurs. The forward Kolmogorov equation for jump process is given by pjump (x) = (cid:18)(cid:90) λt(y)Jt(xy)pt(y)dy pt(x)λt(x) (cid:19) where the two terms can intuitively be seen to measure the inflow and outflow of probability due to jumps. Our goal is to find λt(x), Jt(yx) such that pjump matches the evolution of pw in Eq. (6) for given choice of gt. In fact, there are many possible jump processes which satisfy this property (Del Moral (2013, Ch. 5); Angeli et al. (2019)) We present particular choice here, with proof in App. B.2. Proposition 4.1. For given gt in Eq. (6), define the jump process rate and transition as λt(x) = (cid:0)gt(x) Ept [gt](cid:1) (29a) Jt(yx) = (cid:0)gt(y) Ept[gt](cid:1)+ (cid:82) (cid:0)gt(z) Ept [gt](cid:1)+ where (u) := max(0, u) and (u)+ := max(0, u). Then, pt(z)dz pt(y) (29b) (x) pjump pw (x) which matches Eq. (6). = = pt(x)(cid:0)gt(x) Ept[gt](cid:1) (30) In continuous time and the mean-field limit, this jump process formulation of reweighting corresponds to simulating xt+dt = (cid:40) xt Jt(yxt) w.p. 1 λt(xt)dt + o(dt) w.p. λt(xt)dt + o(dt). (31) We expect this process to improve the sample population in efficient fashion (Angeli et al., 2019), since jump events are triggered only in states where (gt(x) Ept[gt]) 0 = gt(x) Ept[gt], and transitions are more likely to jump to states with high excess weight (gt(y) Ept [gt])+ > 0. In practice, we use an empirical approximation pK (z) = 1 k=1 δz(x(k)) to approximate the jump rate λt(x) and transition Jt(yx). Instead of simulating Eq. (31) directly, one can also adopt an implementation based on birth-death exponential clocks (BDC, Del Moral (2013, Ch. 5.3-4)). (cid:80)K 5. Empirical Study Throughout this section, we compare our Feynman-Kac corrector (FKC) resampling schemes against their corresponding SDEs without resampling. We consider both target score and tempered noise SDEs. While we show results for BDC sampling in App. F.2 Table A1, we proceed with systematic resampling throughout the remainder of our experiments. 5.1. Image Generation with Stable Diffusion XL We apply CFG from Prop. 3.1 and study the effect of FKC on generating images with Stable Diffusion XL (SDXL). For the visual quality, we find that FKC performs the best with the guidance scale β = 5.5 and we compare it to CFG with the default scale β = 7.5. However, in practice, we observe that applying the FK correction as is results in identical images with only minor differences across the batch, even for large batch sizes. Therefore, we consider two regimes for application FKC: as is and clipped. When running FKC as is, we take smaller batch size and select single image in the end of the generation process. Hence, to generate another image we have to restart the process from scratch for another batch of initial samples. We found that the low diversity of samples is due to the high variance of the integrated weights, which is common phenomenon in high dimensions. To increase the diversity of the generated samples we divide the weights by constant and perform the resampling at the clipped time-interval (the variance is especially high when close to the data distribution), which we call the clipped version of FKC. This allows for diverse generations within single batch, and we use it with larger batch size. For all experiments, we integrate variancepreserving SDE using 100 discretization steps. In Table 3, we compare the performance of our algorithm with CFG. To quantitatively evaluate the generated images, we consider three metrics: CLIP Score (Radford et al., 2021), ImageReward (Xu et al., 2024), and GenEval (Ghosh et al., 2023). CLIP Score measures the cosine similarity between the image embeddings and text prompt embeddings, Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Figure 2. Samples from Mixture of 40 Gaussians. Table 3. Comparison of our method with classifier-free guidance (CFG) for image generation using SDXL. For all metrics, we report average metric values over 8 different prompts and 16 different generations. Method ImageReward GenEval CLIP FKC β CFG Ours (as is) Ours (clipped) 7.5 5.5 5.5 36.71 36.72 37.85 0.87 0.94 1. 0.16 0.27 0.28 to the learning (qt) and sampling target (pt,β) distributions, with β = TL/TS in the notation of Sec. 3.3. Mixture of 40 Gaussians with Ground-Truth qβ To verify our tools in tractable setting, we consider highly multimodal distribution where we can calculate the optimal qt and log qt for (small) integer TL. We show qualitative results in Fig. 2. We find that target score + FKC performs best, while tempered noise has tendency to drop modes. We also find that FKC outperforms SDE-only simulation in both tempered noise and target score settings. This is further supported by quantitative results in Table A1. Sampling LJ-13 To demonstrate the utility of first learning sampler at high temperature then annealing to lower temperature vs. directly learning at lower temperature, we consider Lennard-Jones (LJ) system of 13 particles at base temperature TL = 2. We train Denoising Energy Matching (DEM) model (Akhound-Sadegh et al., 2024) at this base temperature and perform temperature-annealed inference to lower temperatures. In Table 2 and A2 we compare the performance of DEM model trained at lower temperature against DEM model trained at higher temperature and annealed to the lower temperature using various SDEs. We evaluate methods using the 2-Wasserstein metric between distance distributions, and the 1and 2-Wasserstein metrics between energy histograms to reference distribution (App. F.3). We note that we exclude samples with energy > 100 for all methods and metrics, but in practice, this only affects Target Score and Tempered Noise SDEs without FKC and DEM trained at lower temperatures. This excludes roughly 2-3% of samples for those models, which helps these baselines. See App. for more details. We find that tempered noise+FKC performs best at higher target temperatures. However, at lower temperatures, the target score SDE+FKC performs best. Both methods outperform DEM directly trained at the lower temperature. We find DEM is Figure 3. Samples with CFG (top), FKC (ours as is, bottom). whereas ImageReward assigns score that reflects human preferences including aesthetic quality and prompt adherence. GenEval uses object detection and color classification models to verify if text-to-image model follows the prompt. For the baseline (CFG) we generate 16 images per prompt. We run FKC as is with the batch size 64 to generate single image and repeat this process 16 times to produce 16 images per prompt. For the clipped version of FKC, we run single batch of size 16 64 to generate images, approximately 16 of which are distinct. We evaluate all the methods on 8 different prompts. Prompts were generated with GenEval for one of the most challenging tasks for text-to-image models: rendering two different objects with two different colors. Qualitative results in Fig. 3 suggest that FKC as is results in better prompt adherence compared to CFG. However, the quantitative comparison in Table 3 favors the clipped version of FKC both in terms of ImageReward and CLIP due to the increased diversity of generations. We provide more generated images and additional details on the hyperparameter search in App. F.6. 5.2. Samplers from the Boltzmann Density As described in the Sec. 1, our FKC inference techniques suggest flexible schemes for learning diffusion samplers at given temperature and sampling according to different temperature. Since we are given an energy function in these settings, we are not restricted to learning with temperature 1 for for our base model qt. Thus, we use (TL, TS) to refer 7 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Table 2. LJ-13 sampling task with various SDEs, with performance measured by mean standard deviation over 3 seeds. The starting temperature is TL = 2, annealed to target temperatures TS = 0.8 and TS = 1.5. The DEM samples are generated with model trained at those corresponding target temperatures. Target Temp. SDE Type FKC Distance-W2 Energy-W1 Energy-W2 0.8 (β = 2.5) 1.5 (β = 1.33) Target Score Tempered Noise DEM Target Score Tempered Noise DEM 0.189 0.002 0.048 0.019 0.108 0.007 0.047 0.006 0.103 0.001 0.168 0.009 0.083 0.003 0.095 0.006 0.066 0.002 0.268 0.005 14.730 0.029 6.252 2.710 6.487 0.056 7.016 0.538 9.794 0.100 15.556 0.045 6.356 2.673 8.501 0.283 7.111 0.535 9.804 0.101 5.340 0.054 3.366 0.083 2.154 0.048 0.765 0.156 4.471 0. 6.210 0.254 3.386 0.090 3.920 0.258 0.939 0.171 5.211 0.017 qualitatively easier to learn at higher temperatures requiring much less tuning compared to lower temperatures (Fig. A2). This makes the train-then-anneal approach attractive in this setting. We find that FKC in this setting is able to successfully sample from temperatures TS [2.0, 0.8]  (Fig. 4)  . This is attractive as, with FKC, practitioners can train single amortized model, and then sample at variety of temperatures post-hoc. For extended results and discussion see App. F. 5.3. Multi-Target Structure-Based Drug Design We apply FKC to the setting of structure-based drug design (SBDD), where the goal is to design molecules (or ligands) using the three-dimensional structure of biological targettypically proteinas guide (Anderson, 2003). The ligands are then evaluated based on how well they fit into the proteins binding site. We focus on dual-target drug design, where molecule should interact with two proteins simultaneously. Dual-target drug design has become increasingly investigated for targeting complex disease pathways such as in various cancers and neurodegeneration (Ramsay et al., 2018), as well as for diminishing drug resistance mechanisms (Yang et al., 2024). Our goal is to generate ligands that are predicted to bind simultaneously to pair of proteins. Zhou et al. (2024) introduced dataset of biologically relevant protein target pairs derived from drug synergism. Following their methodology, we align the target pockets in 3D space and generate sample coordinates for each pocket using an SE(3)-equivariant graph neural network over 1000 integration steps. We then use our PoE scheme in Prop. 3.3 to guide ligand generation by taking the product of the sample distributions for the individual protein targets. We investigate the performance of PoE using both target score and tempered noise SDEs at various β, with and without FKC. Ligand performance is determined by docking scores to each protein target, which was done using AutoDock Vina (Eberhardt et al., 2021). We evaluate 14 protein pairs. Note that the PoE weight computation in Figure 4. 2-Wasserstein between energy distributions of MCMC samples from the annealed target distribution and our methods at different temperatures. Note the training temperature TL = 2. Eq. (27) necessitates equal sample dimensionality, otherwise resampling would be skewed to favor samples of higher dimensions. This requires the molecules within batch to have the same number of atoms. To this end, we sampled 5 molecule sizes from the original training set from Guan et al. (2023): {15, 19, 23, 27, 35}. For each molecule size for each protein pair, we generated 32 molecules. We showcase our best results in Table 4 and the full ablation in App. F.4. We evaluate the generated molecules on their docking scores to protein pair, P1 and P2. We report the average of docking score products for each target, as well as the average maximum docking score for pair. Lower docking scores are better, and so lower maximum docking scores indicate the molecule is better at binding to both targets. We compute the percentage of molecules that have better docking scores than known binders, as well as the number of valid and unique molecules generated, their diversity, and the quality of the molecules (Lee et al., 2025b), which is computed as the fraction of molecules that are unique, valid, have druglikeness (QED (Bickerton et al., 2012)) 0.6 and synthetic accessibility (SA (Ertl & Schuffenhauer, 2009)) 4.0. Our baseline is the target score SDE with β = 0.5, which is equivalent to DualDiff from Zhou et al. (2024) and also corresponds to an averaging of scores (Liu et al., 2022). We also generate molecules conditioned on single protein pocket using TargetDiff from Guan et al. (2023), but dock the molecules to both targets in protein pair to understand the need for conditioning on two pockets simultaneously. We find that the target noise SDE at β > 0.5 generates molecules with better average docking scores for each of the target proteins compared with both DualDiff and TargetDiff. When we incorporate FKC, the average docking scores improve further. There is slight sacrifice in terms of diversity and uniqueness when resampling with FKC, although this common trade-off for an increase in quality. Notably, our method achieves the lowest maximum docking score, meaning that generated ligands are able to better bind to both proteins on average. Our method also generates the highest fraction of molecules that are better than known Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Table 4. Docking scores of generated ligands for 14 protein target pairs (P1, P2). We generate 32 ligands for 5 molecule lengths for each protein pair using the Target Score SDE. Lower docking scores are better. Values are reported as averages over all generated molecules in each run, except for top-1. \"Better than ref.\" is the percentage of ligands with better docking scores than known reference molecules for both targets (the mean docking score for the reference molecules is 8.2551.849). We also report the diversity, validity & uniqueness, and quality. 1TargetDiff from Guan et al. (2023), 2DualDiff from Zhou et al. (2024). (P1 * P2) () max(P1, P2) () P1 () P2 () P1 top-1 () P2 top-1 () Better than ref. () Div. () Val. & Uniq. () Qual. () P1 only1 62.77023.741 7.3011.902 8.3841.513 7.4411.934 12.7171.846 10.8220.996 0.3210. 0.8890.011 0.9460.067 0.1610.160 β FKC (P1 * P2) () max(P1, P2) () P1 () P2 () P1 top-1 () P2 top-1 () Better than ref. () Div. () Val. & Uniq. () Qual. () 0.5 1.0 2. 2 64.34521.535 64.05631.212 69.03121.614 69.82932.702 7.1412.117 6.8583.259 7.5411.738 7.3992.932 7.9031.994 7.8922. 8.2351.712 8.5141.816 7.9601.665 7.9232.419 8.2981.531 8.2712.877 10.7491.310 11.2221.589 11.0321.311 10.9781.667 11.1651.484 11.0961.195 11.3171.432 11.1901. 68.11518.557 11.3031.080 8.2111.663 75.53523.261 7.9131.619 8.6011.623 8.6641.552 11.4061.361 7.3962.025 8.1131.617 10.7761.088 11.1281.305 0.2470.339 0.2820.371 0.2850.361 0.3270. 0.2790.358 0.3410.429 0.8860.008 0.8770.015 0.8860.009 0.8470.023 0.8820.014 0.8080.047 0.8900.211 0.9470.106 0.8980.186 0.9220. 0.9440.160 0.8760.091 0.2360.202 0.1980.185 0.2390.199 0.2230.210 0.2710.216 0.2340.232 Figure 5. Molecules generated from our method (target score SDE with β = 2.0 and FKC resampling) and baselines in the binding pockets of two proteins: GRM5 (top row, UniProt ID P41594) and RRM1 (bottom row, UniProt ID P23921). Docking scores for each molecule and target are above each image; lower docking scores are better. Here, we display molecules with the best docking scores that have QED 0.4; more generations are in App. F.4. The binding pocket is shaded in light green. binders (reference molecules), which could motivate using our model in de novo drug design settings (the mean docking score of reference molecules is 8.2551.849). We visualize ligands for sample target pair in Fig. 5 and Fig. A5. In App. F.5, we further investigate the utility of PoE in generating molecule SMILES using latent diffusion model, and show that FKC resampling improves generation for small molecules satisfying multiple functional properties. 6. Related Work Sequential Monte Carlo methods have proven useful across wide range of tasks involving diffusion models, including for reward-guided generation (Uehara et al., 2024; 2025; Singhal et al., 2025; Kim et al., 2025), conditional generation (Wu et al., 2024), or inverse problems (Dou & Song, 2024; Cardoso et al., 2024), with recent extensions to discrete diffusion models (Singhal et al., 2025; Li et al., 2024; Lee et al., 2025a; Uehara et al., 2025). For compositional generation, Du et al. (2023) learn an energy-based score function and use the energy within MCMC procedures. Thornton et al. (2025) improve training of the energy-based score function by distilling an unconditional score model, where the resulting energy can be used for SMC resampling from tempered or product densities. Within the context of diffusion samplers from Boltzmann densities, Phillips et al. (2024) consider SMC for energybased score parameterizations. Chen et al. (2025); Albergo & Vanden-Eijnden (2024) consider SMC resampling along trajectories with respect to prescribed geometric annealing path, where Albergo & Vanden-Eijnden (2024) is presented through the Feynman-Kac perspective. The approaches in (Vargas et al., 2024; Albergo & Vanden-Eijnden, 2024) correspond to the escorted Jarynski equality (Vaikuntanathan & Jarzynski, 2008; 2011), where additional transport terms are learned to more closely match the evolution of given density path (Arbel et al., 2021; Chemseddine et al., 2024; Máté & Fleuret, 2023; Tian et al., 2024; Fan et al., 2024; Maurais & Marzouk, 2024; Vargas et al., 2024). Indeed, the celebrated Jarzynski equality (Jarzynski, 1997; Crooks, 1999) and its variants admit an elegant proof using the Feynman-Kac formula (Lelièvre et al. (2010, Ch. 4),Vaikuntanathan & Jarzynski (2008)). Predictor-corrector simulation (Song et al., 2021) performs additional Langevin steps to promote matching the intermediate marginals of pt of diffusion model. These schemes can be adapted for annealed or product targets, although Du et al. (2023) found best performance using Metropolis corrections. Finally, Bradley & Nakkiran (2024) interpret 9 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts standard CFG SDE simulation (19) as predictor-corrector where the corrector targets different guidance or geometric mixture weight β = 1 2 (1 + β). Our resampling correctors are instead tailored to the original guidance weight β. 7. Conclusion In this work, we proposed FEYNMAN-KAC CORRECTORS, an array of tools allowing for fine control over the sample distributions of diffusion processes. These target distributions may arise in compositional generative modeling (Du & Kaelbling, 2024), where we seek to combine specialist models capturing various chemical properties of molecules or different aspects of complex prompt. Geometric averaging appears in widely-used CFG techniques while, via annealing, we demonstrate that an approach of first learning an amortized sampler at higher temperature and then annealing using FKCs down to lower temperature opens up new dimension for the construction of amortized samplers. Finally, our framework allows for the use of reward models (see Prop. D.5), and for time-dependent annealing schedule βt (Prop. C.6), where the log-density terms which appear in the resulting weights can be efficiently estimated using techniques from (Skreta et al., 2024). 8. Acknowledgments This project was partially sponsored by Google through the Google & Mila projects program. The authors acknowledge funding from UNIQUE, CIFAR, NSERC, Intel, and Samsung. The research was enabled in part by computational resources provided by the Digital Research Alliance of Canada (https://alliancecan. ca), Mila (https://mila.quebec), the Acceleration Consortium (https://acceleration.utoronto. ca/), and NVIDIA. KN was supported by IVADO and Institut Courtois. MS thanks Ella Rajaonson for assistance with docking visualizations, as well as Austin Cheng and Cher-Tian Ser for providing feedback on molecule generation. References Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., Ronneberger, O., Willmore, L., Ballard, A. J., Bambrick, J., et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pp. 13, 2024. Akhound-Sadegh, T., Rector-Brooks, J., Bose, J., Mittal, S., Lemos, P., Liu, C.-H., Sendera, M., Ravanbakhsh, S., Gidel, G., Bengio, Y., et al. Iterated denoising energy matching for sampling from Boltzmann densities. In Forty-first International Conference on Machine Learning, 2024. 10 Albergo, M. S. and Vanden-Eijnden, E. non-equilibrium transport sampler. arXiv:2410.02711, 2024. Nets: arXiv preprint Anderson, A. C. The process of structure-based drug design. Chemistry & biology, 10(9):787797, 2003. Angeli, L. Interacting particle approximations of FeynmanKac measures for continuous-time jump processes. PhD thesis, University of Warwick, 2020. Angeli, L., Grosskinsky, S., Johansen, A. M., and Pizzoferrato, A. Rare event simulation for stochastic dynamics in continuous time. Journal of Statistical Physics, 176(5): 11851210, 2019. Arbel, M., Matthews, A., and Doucet, A. Annealed flow transport Monte Carlo. In International Conference on Machine Learning, 2021. Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S., and Hopkins, A. L. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):9098, 2012. Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F., Pradhan, N., Karaletsos, T., Singh, R., Szerlip, P., Horsfall, P., and Goodman, N. D. Pyro: Deep universal probabilistic programming. arXiv preprint arXiv:1810.09538, 2018. Bortoli, V. D., Hutchinson, M., Wirnsberger, P., and arXiv preprint Doucet, A. Target score matching. arXiv:2402.08667, 2024. Bradley, A. and Nakkiran, P. Classifier-free guidance is predictor-corrector. arXiv preprint arXiv:2408.09000, 2024. Cardoso, G. V., El Idrissi, Y. J., Le Corff, S., and Moulines, E. Monte Carlo guided diffusion for Bayesian linear inverse problems. In International Conference on Learning Representations, 2024. Chang, J. and Ye, J. C. Ldmol: Text-conditioned molecule diffusion model leveraging chemically informative latent space. arXiv preprint arXiv:2405.17829, 2024. Chemseddine, J., Wald, C., Duong, R., and Steidl, G. Neural sampling from Boltzmann densities: Fisher-Rao arXiv preprint curves in the Wasserstein geometry. arXiv:2410.03282, 2024. Chen, J., Richter, L., Berner, J., Blessing, D., Neumann, G., and Anandkumar, A. Sequential controlled Langevin diffusions. International Conference on Machine Learning, 2025. Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Chizat, L., Peyré, G., Schmitzer, B., and Vialard, F.-X. An interpolating distance between optimal transport and FisherRao metrics. Foundations of Computational Mathematics, 18:144, 2018. Guan, J., Qian, W. W., Peng, X., Su, Y., Peng, J., and Ma, J. 3d equivariant diffusion for target-aware arXiv molecule generation and affinity prediction. preprint arXiv:2303.03543, 2023. Crooks, G. E. Excursions in Statistical Dynamics. University of California, Berkeley, 1999. Davis, M. H. Piecewise-deterministic Markov processes: general class of non-diffusion stochastic models. Journal of the Royal Statistical Society: Series (Methodological), 46(3):353376, 1984. Del Moral, P. Mean Field Simulation for Monte Carlo Integration. Chapman and Hall, CRC press, 2013. Dou, Z. and Song, Y. Diffusion posterior sampling for linear inverse problem solving: filtering perspective. In The Twelfth International Conference on Learning Representations, 2024. Douc, R. and Cappé, O. Comparison of resampling schemes for particle filtering. In ISPA 2005. Proceedings of the 4th International Symposium on Image and Signal Processing and Analysis, pp. 6469, 2005. Du, Y. and Kaelbling, L. Compositional generative modeling: single model is not all you need. arXiv preprint arXiv:2402.01103, 2024. Du, Y., Durkan, C., Strudel, R., Tenenbaum, J. B., Dieleman, S., Fergus, R., Sohl-Dickstein, J., Doucet, A., and Grathwohl, W. S. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International conference on machine learning, pp. 84898510. PMLR, 2023. Eberhardt, J., Santos-Martins, D., Tillack, A. F., and Forli, S. Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings. Journal of chemical information and modeling, 61(8):38913898, 2021. Ertl, P. and Schuffenhauer, A. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of Cheminformatics, 1:111, 2009. Ethier, S. N. and Kurtz, T. G. Markov Processes: Characterization and Convergence. John Wiley & Sons, 2009. Fan, M., Zhou, R., Tian, C., and Qian, X. Path-guided particle-based sampling. International Conference on Machine Learning, 2024. Gardiner, C. Stochastic Methods, volume 4. 2009. Ho, J. and Salimans, T. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. Hoffman, M. D. and Gelman, A. The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. arXiv preprint arXiv:1111.4246, 2011. Holderrieth, P., Havasi, M., Yim, J., Shaul, N., Gat, I., Jaakkola, T., Karrer, B., Chen, R. T., and Lipman, Y. Generator matching: Generative modeling with arbitrary Markov processes. arXiv preprint arXiv:2410.20587, 2024. Huang, K., Fu, T., Gao, W., Zhao, Y., Roohani, Y., Leskovec, J., Coley, C. W., Xiao, C., Sun, J., and Zitnik, M. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. Proceedings of Neural Information Processing Systems, NeurIPS Datasets and Benchmarks, 2021. Jarzynski, C. Equilibrium free-energy differences from nonequilibrium measurements: master-equation approach. Physical Review E, 56(5):5018, 1997. Karczewski, R., Heinonen, M., and Garg, V. Diffusion models as cartoonists! the curious case of high density regions. arXiv preprint arXiv:2411.01293, 2024. Karras, T., Aittala, M., Kynkäänniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. Kim, S., Kim, M., and Park, D. Alignment without overoptimization: Training-free solution for diffusion models. arXiv preprint arXiv:2501.05803, 2025. Köhler, J., Klein, L., and Noé, F. Equivariant flows: exact likelihood generative learning for symmetric densities. In International Conference on Machine Learning, 2020. Kondratyev, S., Monsaingeon, L., and Vorotnikov, D. new optimal transport distance on the space of finite Radon measures. arXiv preprint arXiv:1505.07746, 2015. Lee, C. K., Jeha, P., Frellsen, J., Lio, P., Albergo, M. S., and Vargas, F. Debiasing guidance for discrete diffusion with sequential Monte Carlo. arXiv preprint arXiv:2502.06079, 2025a. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Lee, S., Kreis, K., Veccham, S. P., Liu, M., Reidenbach, D., Peng, Y., Paliwal, S., Nie, W., and Vahdat, A. Genmol: drug discovery generalist with discrete diffusion. arXiv preprint arXiv:2501.06158, 2025b. 11 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Lelièvre, T., Rousset, M., and Stoltz, G. Free Energy Computations: Mathematical Perspective. World Scientific, 2010. Li, X., Zhao, Y., Wang, C., Scalia, G., Eraslan, G., Nair, S., Biancalani, T., Ji, S., Regev, A., Levine, S., et al. Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding. arXiv preprint arXiv:2408.08252, 2024. Liero, M., Mielke, A., and Savaré, G. Optimal entropytransport problems and new HellingerKantorovich distance between positive measures. Inventiones mathematicae, 211(3):9691117, 2018. Liu, N., Li, S., Du, Y., Torralba, A., and Tenenbaum, J. B. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pp. 423439. Springer, 2022. Lu, Y., Lu, J., and Nolen, J. Accelerating Langevin sampling with birth-death. arXiv preprint arXiv:1905.09863, 2019. Máté, B. and Fleuret, F. Learning interpolations between Boltzmann densities. Transactions on Machine Learning Research, 2023. Maurais, A. and Marzouk, Y. Sampling in unit time with kernel Fisher-Rao flow. In Forty-first International Conference on Machine Learning, 2024. Midgley, L. I., Stimper, V., Simm, G. N., Schölkopf, B., and Hernández-Lobato, J. M. Flow annealed importance sampling bootstrap. International Conference on Learning Representations, 2023. Naesseth, C. A., Lindsten, F., Schön, T. B., et al. Elements of sequential Monte Carlo. Foundations and Trends in Machine Learning, 12(3):307392, 2019. Neal, R. M. Annealed importance sampling. Statistics and Computing, 11:125139, 2001. OuYang, R., Qiang, B., and Hernández-Lobato, J. M. Bnem: boltzmann sampler based on bootstrapped noised energy matching. arXiv preprint arXiv:2409.09787, 2024. Ramsay, R. R., Popovic-Nikolic, M. R., Nikolic, K., Uliassi, E., and Bolognesi, M. L. perspective on multi-target drug discovery and design for complex diseases. Clinical and translational medicine, 7:114, 2018. Richter, L. and Berner, J. Improved sampling via learned diffusions. In The Twelfth International Conference on Learning Representations, 2024. Rogers, D. and Hahn, M. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50 (5):742754, 2010. Rousset, M. On the control of an interacting particle estimation of Schrödinger ground states. SIAM journal on mathematical analysis, 38(3):824844, 2006. Rousset, M. and Stoltz, G. Equilibrium sampling from nonequilibrium dynamics. Journal of Statistical Physics, 123:12511272, 2006. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: 3647936494, 2022. Sahoo, S. S., Arriola, M., Gokaslan, A., Marroquin, E. M., Rush, A. M., Schiff, Y., Chiu, J. T., and Kuleshov, V. Simple and effective masked diffusion language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Salimans, T. and Ho, J. Should EBMs model the energy or the score? In Energy Based Models Workshop-ICLR 2021, 2021. Singhal, R., Horvitz, Z., Teehan, R., Ren, M., Yu, Z., McKeown, K., and Ranganath, R. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv:2501.06848, 2025. Skreta, M., Atanackovic, L., Bose, A. J., Tong, A., and Neklyudov, K. The superposition of diffusion modarXiv preprint els using the Itô density estimator. arXiv:2412.17762, 2024. Phillips, A., Dau, H.-D., Hutchinson, M. J., De Bortoli, V., Deligiannidis, G., and Doucet, A. Particle denoising diffusion sampler. In Forty-first International Conference on Machine Learning, 2024. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Thornton, J., Béthune, L., ZHANG, R., Bradley, A., Nakkiran, P., and Zhai, S. Controlled generation with distilled diffusion energy models and sequential Monte Carlo. In The 28th International Conference on Artificial Intelligence and Statistics, 2025. 12 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Tian, Y., Panda, N., and Lin, Y. T. Liouville flow importance sampler. International Conference on Machine Learning, 2024. Zhang, Q. and Chen, Y. Path integral sampler: stochastic control approach for sampling. In International Conference on Learning Representations, 2022. Uehara, M., Zhao, Y., Biancalani, T., and Levine, S. Understanding reinforcement learning-based fine-tuning of diffusion models: tutorial and review. arXiv preprint arXiv:2407.13734, 2024. Zhou, X., Guan, J., Zhang, Y., Peng, X., Wang, L., and Ma, J. Reprogramming pretrained target-specific diffusion models for dual-target drug design. arXiv preprint arXiv:2410.20688, 2024. Uehara, M., Zhao, Y., Wang, C., Li, X., Regev, A., Levine, S., and Biancalani, T. Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review. arXiv preprint arXiv:2501.09685, 2025. Vaikuntanathan, S. and Jarzynski, C. Escorted free energy simulations: Improving convergence by reducing dissipation. Physical Review Letters, 100(19):190601, 2008. Vaikuntanathan, S. and Jarzynski, C. Escorted free energy simulations. The Journal of chemical physics, 134(5), 2011. Vargas, F., Grathwohl, W. S., and Doucet, A. Denoising diffusion samplers. In The Eleventh International Conference on Learning Representations, 2023. Vargas, F., Padhy, S., Blessing, D., and Nusken, N. Transport meets variational inference: Controlled Monte Carlo diffusions. In The Twelfth International Conference on Learning Representations: ICLR 2024, 2024. Wang, H., Skreta, M., Ser, C.-T., Gao, W., Kong, L., StriethKalthoff, F., Duan, C., Zhuang, Y., Yu, Y., Zhu, Y., et al. Efficient evolutionary search over chemical space with large language models. arXiv preprint arXiv:2406.16976, 2024. Woo, D. and Ahn, S. Iterated energy-based flow matching for sampling from Boltzmann densities. arXiv preprint arXiv:2408.16249, 2024. Wu, L., Trippe, B., Naesseth, C., Blei, D., and Cunningham, J. P. Practical and asymptotically exact conditional sampling in diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. Yang, Y., Mou, Y., Wan, L.-X., Zhu, S., Wang, G., Gao, H., and Liu, B. Rethinking therapeutic strategies of dual-target drugs: An update on pharmacological smallmolecule compounds in cancer. Medicinal Research Reviews, 44(6):26002623, 2024. 13 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts A. Expectation Estimation under Feynman-Kac PDEs We proceed in two steps, first finding Kolmogorov backward equation corresponding to evolution under weighted Feynman-Kac SDE. We then use this identity to derive the expectation estimator in Eq. (10). Throughout, we consider the evolution of density pt defined via the following Feynman-Kac PDE, pt(xt) = (cid:10), pt(xt)vt(xt)(cid:11) + pt(xt) + pt(xt) gt(xt) (cid:18) (cid:90) σ2 2 gt(xt)pt(xt)dxt (32) (cid:19) Our proof follows similar derivations as in Lelièvre et al. (2010, Prop 4.1, Ch. 4.1.4.3) (see also (Vaikuntanathan & Jarzynski, 2008; 2011) and references therein), where the authors are interested in sampling from sequence of unnormalized distributions pt specified via time-varying energy or Hamiltonian. The proofs often rely on Langevin dynamics that leave pt invariant. We adopt similar proof technique, but focus directly on simulation with arbitrary vt, gt derived via our methods in Sec. 3. Proposition A.1. For bounded test function ϕ : and pt satisfying Eq. (32), we have EpT (xT )[ϕ(xT )] = (cid:104) E"
        },
        {
            "title": "1\nZT",
            "content": "(cid:82) 0 gs(xs)dsϕ(xT ) (cid:105) where dxt = vt(xt)dt + σtdWt, x0 p0 (33) where ZT is normalization constant independent of x. Eq. (33) which suggests that the self-normalized importance sampling approximation in Eq. (10) is consistent as . Proof. The proof proceeds in three steps, delineated with bold paragraph headers. We first derive the backward Kolmogorov equation for appropriate functions, then specify the evolution of the Feynman-Kac PDE for the unnormalized density, before combining these results to prove the result in Prop. A.1. Backward PDE: For given test function ϕ(x), consider defining the following function (cid:104) ΦT (x, t) = (cid:82) gs(xs)dsϕ(xT ) xt = (cid:105) , ΦT (x, ) = ϕ(x) where expectations are taken under the evolution of the SDE dxt = vt(xt)dt + σtdWt. In particular, for τ > t, we have (cid:104) ΦT (x, t) = (cid:82) τ gs(xs)dse (cid:82) τ gs(xs)dsϕ(xT ) xt = (cid:105) (cid:104) = (cid:82) τ gs(xs)dsΦT (xτ , τ ) xt = (cid:105) (34) (35) We will leverage this identity to derive PDE which ΦT (x, t) must satisfy. Note, to link ΦT (x, t) and (the expected value (cid:82) τ gs(xs)ds. Thus, we apply Itos product rule and Itos lemma to capture of) ΦT (xτ , τ ), we should account for the weights (cid:82) τ gs(xs)dsΦT (xτ , τ ) evolves with τ , how (cid:16) (cid:82) τ gs(xs)dsdΦT (xτ , τ ) + ΦT (xτ , τ )de (cid:82) τ gs(xs)ds + dΦT (xτ , τ ), (cid:17) (cid:82) τ gs(xs)dsΦT (xτ , τ ) (cid:82) τ g(xs)ds = (36) In the final term, vanishes. We can use Itos lemma to expand dΦT (xτ , τ ) and simple differentiation for de (cid:82) τ gs(xs)ds is non-stochastic and, assuming it has finite variation, the term dΦT (x, t), (cid:82) τ gs(xs)ds, (cid:82) τ gs(xs)ds (cid:16) (cid:17) (cid:82) τ gs(xs)dsΦT (xτ , τ ) (cid:82) τ gs(xs)ds = (cid:18) ΦT (xτ , τ ) τ + (cid:10)vτ (xτ ), ΦT (xτ , τ )(cid:11) + ΦT (xτ , τ ) (cid:19) dτ + (cid:82) τ gs(xs)dsσt (cid:10)ΦT (xτ , τ ), dWt (cid:11) + ΦT (xτ , τ )e (cid:82) τ (cid:17) gτ (xτ ) dτ (37) σ2 τ 2 gs(xs)ds(cid:16) (cid:82) τ gs(xs)ds = (cid:18) ΦT (xτ , τ ) (cid:10)ΦT (xτ , τ ), dWt + (cid:82) τ gs(xs)dsσt + (cid:10)vt(x), ΦT (xτ , τ )(cid:11) + ΦT (xτ , τ ) + ΦT (xτ , τ )gt(x) dt (cid:19) σ2 2 (cid:11) (38) Integrating Eq. (38) τ = to τ = and taking expectations under the simulated process from initial point xt = x, the 14 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts stochastic term vanishes and we obtain (cid:104) = (cid:82) gs(xs)dsΦT (xT , ) xt = (cid:34)(cid:90) (cid:82) τ gs(xs)ds τ =t (cid:18) ΦT (xτ , τ ) τ + (cid:10)vτ (x), ΦT (xτ , τ )(cid:11) + σ2 τ 2 (cid:105) (cid:104) (cid:82) gs(xs)dsΦT (x, t) xt = (cid:105) ΦT (xτ , τ ) + ΦT (xτ , τ )gτ (x) dτ (39) (cid:19) (cid:35) Finally, we simplify the first line in Eq. (39). Considering the definition and endpoint condition in Eq. (34), we have (cid:104) (cid:82) gs(xs)dsΦT (xT , ) xt = (cid:105) (cid:104) = (cid:82) gs(xs)dsϕ(xT ) xt = (cid:105) (cid:104) (cid:82) gs(xs)dsΦT (x, t) xt = (cid:105) ΦT (x, t) = (40) (41) by definition in Eq. (34). Since for any τ . Thus, we obtain backward PDE which is often used directly in the statement of the Feynman-Kac formula, (cid:82) τ gs(xs)ds > 0, this implies that the integrand in the second line of Eq. (39) should be zero ΦT (xτ , τ ) τ + (cid:10)vτ (x), ΦT (xτ , τ )(cid:11) + σ2 τ 2 ΦT (xτ , τ ) + ΦT (xτ , τ )gτ (x) = 0 (42) Evolution of Unnormalized Density In practice, we cannot exactly calculate (cid:82) gt(xt)pt(xt)dxt, which appears in the reweighting equation in Eq. (6) (or Eq. (45) below) to ensure normalization. Eventually, we will account for normalization using SNIS as in Eq. (10). For now, consider the evolution of unnormalized density pt(x) = pt(x)Zt for particular vt, σt, gt and some normalization constant Zt. With foresight, we define pt(xt) = (cid:10), pt(xt)vt(xt)(cid:11) + σ2 2 pt(xt) + pt(xt)gt(xt) which we justify by noting that only the reweighting term does not preserve normalization. In particular, let (cid:90) log Zt := pt(x)gt(x)dx. (43) (44) which seems to be natural candidate from inspecting general, (x)(cid:0)gt(x) (cid:82) pw (x)gt(x)dx(cid:1), which implies log pw pw log pw consistent with the original Feynman-Kac PDE, (x) = (x)gt(x)dx. Defining terms such that (x) log Zt yields Eq. (44). We finally confirm that the definitions in Eq. (43) and Eq. (44) are (x) = gt(x) (cid:82) pw reweighting-only evolution tpw (x) = log pw Namely, since pt(xt) = pt(xt)Z 1 pt(xt) = (cid:10), pt(xt)vt(xt)(cid:11) + , the definitions in Eq. (43)-(44) should satisfy pt(xt) + pt(xt) gt(xt) (cid:18) (cid:90) σ2 2 gt(xt)pt(xt)dxt (45) (cid:19) pt(xt) = (cid:0)pt(xt)Z 1 (cid:1) = 1 = 1 = 1 pt(xt) + pt(xt)Z t (cid:18) (cid:10), pt(xt)vt(xt)(cid:11) + pt(xt) pt(xt)Z 1 ) (46a) (46b) (46c) log(Z 1 log Zt pt(xt) + pt(xt)gt(xt) (cid:19) pt(xt)Z 1 (cid:90) σ2 2 pt(xt)gt(xt)dx (46d) Noting that xtZt = 0, we can pull 1 (cid:29) (cid:28) , = pt(xt) Zt = (cid:10), pt(xt)vt(xt)(cid:11) + vt(xt) as desired. σ2 2 inside differential operators to obtain + σ2 2 pt(xt) Zt + pt(xt) Zt (cid:18) gt(xt) (cid:90) pt(xt) Zt pt(xt)gt(xt)dx (cid:90) (cid:19) pt(xt)gt(xt)dx (46e) (46f) pt(xt) + pt(xt) gt(xt) 15 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Expectation Estimation: Now, we use Eq. (42) to write the total derivative of the following integral under the unnormalized density pt(x), (cid:18)(cid:90) dt ΦT (x, t)pt(x)dx = (cid:19) (cid:90) (cid:18) ΦT (x, t) (cid:19) pt(x)dx + (cid:90) ΦT (x, t) (cid:18) pt(x) (cid:19) dx (47a) Using Eq. (42) and Eq. (43), we have (cid:90) (cid:18) = (cid:10)vt(x), ΦT (x, t)(cid:11) (cid:18) (cid:90) + ΦT (x, t) (cid:10), pt(x)vt(x)(cid:11) + ΦT (x, t) ΦT (x, t)gτ (x) pt(x)dx (47b) (cid:19) σ2 τ 2 pt(x) + pt(x)gt(x) dx (cid:19) σ2 2 Integrating by parts in the second line, we have (cid:90) (cid:18) = + = (cid:10)vt(x), ΦT (x, t)(cid:11) (cid:90) (cid:18) (cid:10)vt(x), ΦT (x, t)(cid:11) + ΦT (x, t) ΦT (x, t)gτ (x) pt(x)dx (47c) (cid:19) σ2 τ 2 ΦT (x, t) + ΦT (x, t)gt(x) pt(x)dx (cid:19) σ2 2 (47d) Integrating on the interval = 0 to = , we obtain (cid:90) ΦT (xT , )pT (xT )dxT (cid:90) ΦT (x0, 0)p0(x0)dx0 = (cid:18)(cid:90) (cid:90) 0 dt ΦT (x, t)pt(x)dx dt = 0 (48) (cid:19) Thus, we can set these two quantities equal to each other. Using the identity pt(x) = pt(x)Zt and assuming we initialize simulation with normalized p0(x) = p0(x) with Z0 = 1, we can finally use the definitions in Eq. (34) (namely Φt(xT , ) = ϕ(xT )) to write (cid:90) ΦT (x0, 0)p0(x0)dx0 = (cid:90) ΦT (xT , )pT (xT )dxT (cid:90) (cid:16) Z0 E[e (cid:17) (cid:82) 0 gs(xs)dsϕ(xT ) x0] p0(x0)dx0 = ZT (cid:90) ϕ(xT )pT (xT )dxT (cid:104) (cid:105) (cid:82) 0 gs(xs)dsϕ(xT ) = EpT (xT )[ϕ(xT )] 1 ZT which is the desired identity. (cid:82) (cid:80)K E[e 0 gs(xs)dsϕ(xT )] 1 In practice, we could estimate ZT 1 k=1 ew(k) ϕ(x(k) ), which yields Eq. (10). (cid:80)K k=1 (cid:82) 0 gs(x(k) )ds = 1 (cid:80)K k=1 ew(k) (49) (50) (51) and Note that our choice of upper limit in ΦT was arbitrary, suggesting that we could repurpose the same reasoning for estimating expectations at intermediate from initialization at time 0. This suggests that our samples are properly weighted for estimating expectations and normalization constants Zt for intermediate pt (Naesseth et al., 2019). Similarly, changing the lower limit of integration from = 0 to intermediate t, the analogue of Eq. (50) suggests estimating expectations using ZtEpt[ΦT (xt, t)] = ZT EpT [ϕ(xT )]. Given properly-weighted particle approximations of pt, Zt, we can continue calculating the appropriate weights along the trajectory to estimate ZT or terminal expectations under pT . These arguments can be similarly adapted to justify SMC resampling at intermediate steps, as we do in practice (Sec. 4). B. Feynman-Kac Processes B.1. Markov Generators for Feynman-Kac Processes In Sec. 2, we described the adjoint generators L(v) [pt], L(σ) [pt] corresponding to flows with vector field vt, diffusions with coefficient σt, and reweighting with respect to gt. In particular, the Kolmogorov forward equation pt (x) = [pt](x) corresponds to our PDEs presented in Eqs. (3), (5) and (6). In the lemma below, we recall the generators which are adjoint to those in Sec. 2 and operate over smooth, bounded test functions with compact support, e.g. L(v) [pt], L(g) [ϕ]. Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Lemma B.1 (Adjoint Generators). Using the identity (cid:82) ϕ(x) [pt](x) dx = (cid:82) Lt[ϕ](x) pt(x) dx Flow: L(v) [ϕ](x) = ϕ(x), vt(x) σ2 [ϕ](x) = ϕ(x) L(σ) Diffusion: Reweighting: L(g,p) [ϕ](x) = ϕt(x) gt(x) (cid:18) (cid:90) (cid:19) gt(x) pt(x) dx L(g) [pt](x) = pt(x) gt(x) L(v) [pt](x) = , pt(x) vt(x) L(σ) [pt](x) = σ2 2 pt(x) (cid:18) (52) (cid:19) gt(x) pt(x)dx (cid:90) Proof. The proofs for flows and diffusions follow using integration by parts, with proofs found in, for example Holderrieth et al. (2024, Sec. A.5). For the reweighting generator, we have (cid:90) ϕ(x)L(g) [pt](x)dx = = =: (cid:90) (cid:90) (cid:90) (cid:18) (cid:18) ϕ(x) pt(x) gt(x) (cid:18) (cid:18) pt(x) ϕ(x) gt(x) (cid:90) (cid:90) (cid:19)(cid:19) gt(y) pt(y)dy dx (cid:19)(cid:19) gt(y) pt(y)dy dx pt(x) L(g,p) [ϕ](x) dx Note that the weights gt are often chosen in relation to the unnormalized density of pt (Lelièvre et al. (2010, Sec. 4)), and our attention will be focused on the pair of generator actions L(g) [ϕ] for possibly time-dependent ϕ. [pt], L(g,p) B.2. Jump Process Interpretation of Reweighting One way to perform simulation of the reweighting equation will be to rewrite it in terms of jump process. We first recall the definition of the Markov generator of jump process (Ethier & Kurtz (2009, 4.2), Del Moral (2013, 1.1), Holderrieth et al. (2024, A.5.3)) and derive its adjoint generator. Lemma B.2 (Jump Process Generators). Using the definition of (cid:82) ϕ(x) [pt](x) dx = (cid:82) Jt[ϕ](x) pt(x) dx. Letting Wt(x, y) = λt(x)Jt(yx) for normalized Jt(yx), the jump process generator and the identity Jump Process: (W ) [ϕ](x) := (cid:90) (cid:16) ϕ(y) ϕ(x) (cid:17) λt(x)Jt(yx)dy (W ) [pt](x) = (cid:18)(cid:90) λt(y)Jt(xy)pt(y)dy pt(x)λt(x) (cid:19) (53a) (53b) Proof. Through simple manipulations and changing the variables of integration, we obtain (cid:90) ϕ(x) [pt](x) dx = (cid:90) Jt[ϕ](x) pt(x) dx (cid:90) (cid:18)(cid:90) (cid:16) ϕ(y) ϕ(x) (cid:17) λt(x)Jt(yx)dy pt(x) dx (cid:19) (cid:90) (cid:90) (cid:90) (cid:90) ϕ(y)λt(x)Jt(yx)pt(x) dydx ϕ(x)λt(y)Jt(xy)pt(y) dxdy (cid:90) (cid:90) (cid:90) (cid:90) ϕ(x)λt(x)Jt(yx)pt(x) dydx ϕ(x)λt(x)Jt(yx)pt(x) dydx (cid:90) (cid:18)(cid:18)(cid:90) ϕ(x) λt(y)Jt(xy)pt(y)dy (cid:19) (cid:18)(cid:90) pt(x)λt(x) (cid:19)(cid:19) Jt(yx)dy dx = = = = = [pt](x) = (cid:18)(cid:90) λt(y)Jt(xy)pt(y)dy pt(x)λt(x) (cid:19) using the assumption that Jt(yx) is normalized. Reweighting Jump Process Our goal is to derive jump process such that the adjoint generators are equivalent (W ) While Del Moral (2013); Angeli (2020) emphasize the freedom of choice in such generators,1 Sec. 4 of (Angeli et al., 2019) [pt](x) for given reweighting generator with weights gt (Eq. (52)). [pt](x) = L(g) 1For example, see Rousset (2006); Rousset & Stoltz (2006) for particular instantiation combining separate birth and death processes. 17 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts argues for particular choice to reduce the expected number of resampling events. To define this process, consider the following thresholding operations, (u) := max(0, u) (u)+ := max(0, u), which satisfy: (u)+ (u) = u."
        },
        {
            "title": "We can now define the Markov generator using",
            "content": "Wt(x, y) = λt(x)Jt(yx) λt(x) := (cid:16) gt(x) Ept[gt] (cid:17) Jt(yx) := (gt(y) Ept[gt])+pt(y) (cid:82) (gt(z) Ept[gt])+pt(z)dz (54) (55) Since jump events are triggered based on λt(xt) = (gt(x) Ept[gt]) and are more likely to transition to events with high excess weight (gt(y) Ept[gt])+pt(y), we expect this process to improve the sample population in efficient fashion (Angeli et al., 2019). Proposition B.3. For given weighting function gt and the adjoint generator L(g) [pt](x) = L(g) using in Eq. (55) satisfies (W ) [pt](x). More explicitly, we have t , the adjoint generator (W ) derived (cid:18) pt(x) gt(x) (cid:90) L(g) [pt](x) = (W ) (cid:32)(cid:90) (cid:16) (cid:19) gt(x) pt(x)dx = [pt](x) (56) gt(y) Ept [gt] (cid:17) (gt(x) Ept [gt])+pt(x) (cid:82) (gt(z) Ept [gt])+pt(z)dz (cid:33) pt(y)dy pt(x) (cid:16) gt(x) Ept [gt] (cid:17) . Proof. We start by expanding the definition of (W ) [pt](x) (W ) [pt](x) = (cid:18)(cid:90) λt(y)Jt(xy)pt(y)dy pt(x)λt(x) (cid:19) = = = (cid:32)(cid:90) (cid:16) gt(y) Ept[gt] (cid:17) (gt(x) Ept[gt])+pt(x) (cid:82) (gt(z) Ept[gt])+pt(z)dz (cid:33) (cid:16) pt(x) pt(y)dy gt(x) Ept[gt] (cid:17) (cid:18)(cid:90) (cid:16) gt(y) Ept [gt] (cid:17) pt(y)dy (cid:19)(cid:32) (cid:32) (cid:82) (gt(y) Ept[gt])pt(y)dy (cid:82) (gt(z) Ept[gt])+pt(z)dz (cid:33) (gt(x) Ept[gt])+pt(x) (cid:82) (gt(z) Ept[gt])+pt(z)dz (cid:16) (cid:17)+ pt(x) gt(x) Ept[gt] (cid:33) (cid:16) pt(x) gt(x) Ept[gt] (cid:17) (57c) pt(x) (cid:16) gt(x) Ept[gt] (cid:17) (57d) (57a) (57b) Using Eq. (54), note that (cid:90) (cid:16) gt(z) Ept[gt] (cid:17)+ pt(z)dz (cid:90) (cid:16) dpt(z) gt(z) Ept[gt] (cid:17) (cid:90) = (gt(z) Ept[gt])pt(z)dz = 0 (58) which implies (cid:82) (gt(z) Ept[gt])+pt(z)dz = (cid:82) (gt(z) Ept[gt])pt(z)dz. We proceed in two cases, handling separately the trivial case where the denominator in Eq. (57d) is zero. Case 1 (λt(x) = 0 supp(pt)): Note that (cid:82) (cid:0)gt(z) Ept[gt](cid:1) (u) 0. In this case, the generators become trivial and we can confirm pt(z)dz = 0 if and only if gt(z) = Ept[gt], z, since [pt](x) = pt(x) gt(x) (cid:18) (cid:90) (cid:19) gt(x) pt(x)dx L(g) = pt(x)(Ept [gt] Ept [gt]) = 0 (W ) [pt](x) = (cid:90) 0 0 pt(y)dy pt(x) 0 = 0 and thus Eq. (56) holds, as desired. Case 2 (x supp(pt) s.t. λt(x) > 0): Under the assumption, supp(µt) s.t. (cid:0)gt(x) Ept[gt](cid:1) (cid:82) (cid:0)gt(z) Ept [gt](cid:1) pt(z)dz = (cid:82) (cid:0)gt(z) Ept[gt](cid:1)+ In this case, we can conclude using Eq. (58) that pt(z)dz > 0. (cid:82) dpt(z)(cid:0)gt(z)Ept [gt](cid:1) (cid:82) dpt(z)(cid:0)gt(z)Ept [gt](cid:1)+ = 1. (59) > 0. This implies Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Continuing from Eq. (57d) (W ) [pt](x) = pt(x) (cid:16) gt(x) Ept[gt] (cid:17)+ (cid:16) pt(x) gt(x) Ept[gt] (cid:17) (cid:32) (cid:82) (gt(y) Ept[gt])pt(y)dy (cid:82) (gt(z) Ept[gt])+pt(z)dz (cid:16) (cid:17)+ (cid:16)(cid:16) (cid:33) gt(x) Ept[gt] = pt(x) = pt(x)(gt(x) Ept[gt]) = L(g) [pt](x) gt(x) Ept[gt] (cid:17)(cid:17) (60a) (60b) (60c) (60d) as desired. Note that, in the second to last line, we used the identity in Eq. (54) that (u)+ (u) = u. B.3. Simulation Schemes In practice, we use an empirical mean over particles with as an approximation to the expectation Ept[gt], with (cid:16) gt(x(k)) Ept[gt] (cid:17) (cid:16) gt(x(k))"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 See Del Moral (2013, Sec. 5.4) for discussion. (cid:17) gt(x(i)) (cid:16) , gt(x(k)) Ept[gt] (cid:17)+ (cid:16) gt(x(k))"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 (cid:17)+ gt(x(i)) Discretization of the Continuous-Time Jump Process To simulate jump process with generator (J,p) [ϕ], we can consider the following infinitesimal sampling procedure (Gardiner (2009, Ch. 12); Davis (1984); Holderrieth et al. (2024)). With rate λt(x) = gt(x) Ept[gt] (cid:16) (cid:17) , the particle jumps to new configuration, xt+dt = xt (cid:40) (cid:16) yt+dt Categorical gt(x(k)) 1 (cid:80)K (cid:16) (cid:80)K j=1 gt(x(j)) 1 i=1 gt(x(i)) (cid:80)K i=1 gt(x(i)) (cid:17)+ with probability 1 dt λt(xt) + o(dt) (cid:17)+ (cid:41)K with probability dt λt(xt) + o(dt) k= The new configuration is sampled according to an empirical approximation of Jt(yx) using pK where the outer 1 Note that the jump rate is zero for particles with gt(x) Ept[gt]. Resampling new particle proportional to (gt(x(k)) 1 gt(x(j)))+ thus promotes the replacement of low importance-weight samples with more promising samples. factor cancels. k=1 δy(x(k)), (y) = 1 (cid:80) (cid:80)K Interacting Particle System Following Del Moral (2013, Sec 5.4), the process may also be simulated using exponential clocks. In particular, we sample an exponential random variable with rate 1, τ (k) exponential(1) as the time when the next jump event will occur (see Gardiner (2009, Ch. 12)). We record artificial time by accumulating the rate function λtlast:s = (cid:80)s tlast:s τ (k), we sample transition according the empirical approximaton of Jt(yx) in Eq. (61). We report results using this scheme in App. F.2 Table A1, but found it to underperform relative to systematic resampling in these initial experiments. λt(xt)dt for samples xt along our simulated diffusion. Upon exceeding the threshold time λ(k) t=tlast C. Proofs for Table 1 C.1. Annealing Proposition C.1 (Annealed Continuity Equation). Consider the marginals generated by the continuity equation qt(x) = (cid:10), qt(x)vt(x)(cid:11) . The marginals pt,β(x) qβ (x) satisfy the following PDE pt,β(x) = (cid:10), pt,β(x)vt(x)(cid:11) + pt,β(x)(cid:2)gt(x) Ept,β gt(x)(cid:3) , gt(x) = (1 β)(cid:10), vt(x)(cid:11) . (61) (62) (63) 19 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Proof. We want to find the partial derivative of the annealed density pt,β(x) = qt(x)β (cid:82) dx qt(x)β , pt,β(x) =?"
        },
        {
            "title": "By the straightforward calculations we have",
            "content": "t log pt,β = β (cid:90) log qt = β(cid:10), vt dx pt,ββ (cid:11) β(cid:10) log qt, vt log qt (cid:90) (cid:11) dx pt,β (cid:2)β(cid:10), vt (cid:11) β(cid:10) log qt, vt (cid:11)(cid:3) = (cid:10), vt (cid:11) (cid:10) log pt,β, vt (cid:11) + (1 β)(cid:10), vt (cid:11) = (cid:10), vt (cid:11) (cid:10) log pt,β, vt (cid:11) + (1 β)(cid:10), vt (cid:11) (cid:90) (cid:90) dx pt,β (cid:2)β(cid:10), vt (cid:11) (cid:10) log pt,β, vt (cid:11)(cid:3) dx pt,β (cid:2)(1 β)(cid:10), vt (cid:11)(cid:3) . Thus, we have pt,β(x) = (cid:10), pt,β(x)vt(x)(cid:11) + pt,β(x)(cid:2)(1 β)(cid:10), vt(x)(cid:11) Ept,β (1 β)(cid:10), vt(x)(cid:11)(cid:3) , which can be simulated as dxt = vt(xt)dt , dwt = (β 1)(cid:10), vt(xt)(cid:11)dt . (64) (65) (66) (67) (68) (69) (70) (71) Proposition C.2 (Scaled Annealed Continuity Equation). Consider the marginals generated by the continuity equation qt(x) = (cid:10), qt(x)vt(x)(cid:11) . The marginals pt,β(x) qβ (x) satisfy the following PDE pt,β(x) = (cid:10), pt,β(x)βvt(x)(cid:11) + pt,β(x)(cid:2)gt(x) Ept,β gt(x)(cid:3) , gt(x) = (1 β)(cid:10) log pt,β(x), vt(x)(cid:11) . Proof. We want to find the partial derivative of the annealed density pt,β(x) = qt(x)β (cid:82) dx qt(x)β , pt,β(x) =? By the straightforward calculations we have log pt,β = β (cid:90) log qt = β(cid:10), vt dx pt,ββ (cid:11) β(cid:10) log qt, vt log qt (cid:90) (cid:90) = (cid:10), βvt (cid:11) (cid:10) log pt,β, βvt Thus, we have (cid:11) dx pt,β (cid:2)β(cid:10), vt (cid:11) β(cid:10) log qt, vt (cid:11)(cid:3) = (cid:10), βvt (cid:11) (cid:10) log pt,β, vt (cid:11) dx pt,β (cid:2)β(cid:10), vt (cid:11) (cid:10) log pt,β, vt (cid:90) (cid:11)(cid:3) (cid:11) (1 β)(cid:10) log pt,β, vt (cid:11) dx pt,β (cid:2)(1 β)(cid:10) log pt,β, vt pt,β(x) = (cid:10), pt,β(x)βvt(x)(cid:11) + pt,β(x)(cid:2)gt(x) Ept,β gt(x)(cid:3) , gt(x) = (1 β)(cid:10) log pt,β, vt (cid:11) , 20 (72) (73) (74) (75) (76) (77) (78) (cid:11)(cid:3) . (79) (80) (81) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts which can be simulated as dxt = βvt(xt)dt , dwt = β(β 1)(cid:10) log qt(xt), vt(xt)(cid:11)dt . Proposition C.3 (Annealed Diffusion Equation). Consider the marginals generated by the diffusion equation The marginals pt,β(x) qβ (x) satisfy the following PDE qt(x) = σ2 2 qt(x) . pt,β(x) = σ2 2 pt,β(x) + pt,β(x)(cid:2)gt(x) Ept,β gt(x)(cid:3) , gt(x) = β(β 1) σ2 log qt(x)2 . Proof. We want to find the partial derivative of the annealed density pt,β(x) = qt(x)β (cid:82) dx qt(x)β , pt,β(x) =? By the straightforward calculations we have log pt,β = β (cid:90) log qt dx pt,ββ log qt σ2 2 σ2 2 σ2 2 = = = β log qt + β dx pt,β log qt + β log qt2 (cid:90) (cid:90) (cid:18) (cid:20) β σ2 2 (cid:20) σ2 2 (cid:19) σ2 2 1 β (cid:21) log pt,β + log pt,β2 dx pt,β log pt,β + log pt,β + log pt,β2 1 log pt,β2 dx pt,β 1 log pt,β2 . (cid:20) (cid:19) σ2 2 1 β σ2 2 σ2 2β σ2 2 (cid:18) (cid:21) log qt (cid:21) log pt,β2 σ2 2 σ2 2β (cid:90) Thus, we have which can be simulated as pt,β(x) = σ2 2 pt,β(x) + pt,β(x)(cid:2)gt(x) Ept,β gt(x)(cid:3) , gt(x) = β(β 1) σ2 log qt(x)2 , dxt = σtdWt , dwt = β(β 1) σ2 2 log qt(xt)2dt . (82) (83) (84) (85) (86) (87) (88) (89) (90) (91) (92) (93) (94) (95) (96) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Proposition C.4 (Scaled Annealed Diffusion Equation). Consider the marginals generated by the diffusion equation The marginals pt,β(x) qβ (x) satisfy the following PDE qt(x) = σ2 2 qt(x) . pt,β(x) = σ2 2β pt,β(x) + pt,β(x)(cid:2)gt(x) Ept,β gt(x)(cid:3) , gt(x) = (β 1) σ2 2 log qt(x) . Proof. We want to find the partial derivative of the annealed density pt,β(x) = qt(x)β (cid:82) dx qt(x)β , pt,β(x) =? By the straightforward calculations we have log pt,β = β (cid:90) log qt dx pt,ββ log qt = β log qt + β dx pt,β log qt + β log qt2 (cid:90) σ2 2 σ2 2 σ2 2β = = σ2 2 σ2 2β σ2 2β log pt,β + log pt,β2 dx pt,β log pt,β + log pt,β + log pt,β2 + log pt,β (cid:20) β σ2 2 (cid:20) σ2 2 (cid:19) σ2 2 (cid:90) (cid:18) 1 β 1 (cid:21) (cid:90) (cid:20)(cid:18) dx pt,β 1 (cid:19) σ2 2 1 β Thus, we have log pt,β . (cid:21) log qt (cid:21) log pt,β2 σ2 2 σ2 2β which can be simulated as pt,β(x) = σ2 2β pt,β(x) + pt,β(x)(cid:2)gt(x) Ept,β gt(x)(cid:3) , gt(x) = (β 1) σ2 2 log qt(x) , dxt = σt β dWt , dwt = (β 1) σ2 2 log qt(xt)dt . Proposition C.5 (Annealed Re-weighting). Consider the marginals generated by the re-weighting equation The marginals pt,β(x) qβ (x) satisfy the following PDE qt(x) = qt(x)(cid:0)gt(x) Eqt(x)gt(x)(cid:1) . pt,β(x) = pt,β (cid:2)βgt(x) Ept,β βgt(x)(cid:3) . 22 (97) (98) (99) (100) (101) (102) (103) (104) (105) (106) (107) (108) (109) (110) (111) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Proof. We want to find the partial derivative of the annealed density pt,β(x) = qt(x)β (cid:82) dx qt(x)β , pt,β(x) =?"
        },
        {
            "title": "By the straightforward calculations we have",
            "content": "t log pt,β = β (cid:90) log qt dx pt,ββ (cid:90) = β(cid:0)gt(x) Eqt(x)gt(x)(cid:1) (cid:90) = βgt(x) dx pt,ββgt(x) . log qt dx pt,β (cid:2)β(cid:0)gt(x) Eqt(x)gt(x)(cid:1)(cid:3) Thus, we have which can be simulated as pt,β(x) = pt,β (cid:2)βgt(x) Ept,β βgt(x)(cid:3) , dxt = 0 , dwt = βgt(xt) . (112) (113) (114) (115) (116) (117) (118) Proposition C.6 (Time-dependent annealing). Consider the annealed marginals pt,β(x) qt(x)β following some Then, for the time-dependent schedule βt, we have dxt = vt,β(xt) + σt,βdWt , dwt = gt,β(xt) . dxt = vt,βt(xt) + σt,βtdWt , dwt = gt,βt(xt) + βt log qt(xt) , sampling from pt,βt(x) qt(x)βt. Proof. First, lets note that for the annealed marginals pt,β(x) qt(x)β with constant β, we have log pt,β = β log qt (cid:90) (cid:20) dx pt,β β (cid:21) log qt = 1 pt,β (cid:10), pt,βvt,β (cid:11) + 1 pt,β σ2 t,β 2 pt,β + (cid:0)gt,β Ept,β gt,β (cid:1) . Thus, for the time-dependent βt, we have log pt,βt = βt log qt + (cid:90) log qt βt dx pt,βt (cid:20) βt = 1 pt,βt (cid:10), pt,βtvt,βt (cid:11) + 1 pt,βt σ2 t,βt 2 From which we have the statement of the proposition. (cid:21) log qt βt log qt + (cid:20)(cid:18) pt,βt + gt,βt + 23 (cid:19) log qt βt Ept,βt (cid:18) gt,βt + βt (cid:19)(cid:21) log qt . (126) (119) (120) (121) (122) (123) (124) (125) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts C.2. Product Proposition C.7 (Product of Continuity Equations). Consider marginals q1,2 equations (x) generated by two different continuity q1 (x) = (cid:10), q1 (x)v1 (x)(cid:11) , q2 (x) = (cid:10), t (x)v2 (x)(cid:11) . The product of densities pt(x) q1(x)q2(x) satisfies the following PDE pt(x) = (cid:10), pt(x)(cid:0)v1 gt(x) = (cid:10) log q1 (x), v2 (x) + v2 (x)(cid:11) + (cid:10) log t (x)(cid:1)(cid:11) + pt(x)(cid:0)gt(x) Ept(x)gt(x)(cid:1) , (x)(cid:11) . (x), v1 Proof. For the continuity equations q1,2 (x) = (cid:68) , q1,2 (x)v1,2 (cid:69) (x) , we want to find the partial derivative of the annealed density q1 (x)q2 (cid:82) dx pt(x) = (x) (x)q2 (x) , pt(x) =? By the straightforward calculations we have log pt = log q1 = (cid:10), v1 (cid:90) + + v2 dx pt = (cid:10), v1 (cid:90) (cid:90) dx pt log t (cid:11) (cid:10) log q1 (cid:20) log q1 + (cid:11) (cid:10) log q2 , v1 (cid:11) (cid:10) log q1 (cid:21) log q2 , v2 (cid:11) (cid:10) log q2 (cid:11) (cid:2)(cid:10), v1 + v2 (cid:11) (cid:10) log pt, v1 + v2 t , v1 (cid:11) + (cid:10) log q1 , v2 (cid:11)(cid:3) , v2 (cid:11) + (cid:10) log q2 , v1 + v2 (cid:11) + (cid:10) log t , v1 (cid:11)(cid:3) . dx pt (cid:2)(cid:10) log q1 , v2 Thus, we have pt(x) = (cid:10), pt(x)(cid:0)v1 gt(x) = (cid:10) log q1 (x), v2 (x) + v2 (x)(cid:11) + (cid:10) log q2 (x)(cid:1)(cid:11) + pt(x)(cid:0)gt(x) Ept(x)gt(x)(cid:1) , (x)(cid:11) , (x), which can be simulated as dxt = (cid:0)v1 dwt = (cid:2)(cid:10) log q1 (xt) + v2 (xt)(cid:1)dt , (xt), v2 (xt)(cid:11) + (cid:10) log t (xt), v1 (xt)(cid:11)(cid:3)dt . (127) (128) (129) (130) (131) (132) (133) (134) (135) (136) (137) (138) (139) (140) (cid:11) Proposition C.8 (Product of Diffusion Equations). Consider marginals q1,2 equations (x) generated by two different diffusion The product of densities pt(x) q1(x)q2(x) satisfies the following PDE q1 (x) = σ2 q1 (x) , q2 (x) = σ2 2 t (x) . pt(x) = σ2 2 gt(x) = σ2 pt(x) + pt(x)(cid:0)gt(x) Ept(x)gt(x)(cid:1) , (cid:10) log q1 (x), log q2 (x)(cid:11) . 24 (141) (142) (143) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Proof. We want to find the partial derivative of the annealed density pt(x) = q1 (x)q2 (cid:82) dx q1 (x) (x)q (x) , pt(x) =? By the straightforward calculations we have (cid:90) log pt = log q1 + log q2 dx pt log t + log q2 (cid:13) (cid:13) log q1 (cid:13) 2 (cid:13) + log (cid:13) (cid:13) log q2 (cid:13) 2 (cid:13) dx pt log q1 + σ2 2 (cid:13) (cid:13) log q1 (cid:13) 2 (cid:13) log q2 + (cid:13) (cid:13) log q2 2(cid:21) (cid:13) (cid:13) σ2 2 (cid:20) σ2 2 (cid:21) σ2 2 + σ2 + = = Thus, we have σ2 σ2 2 log q1 (cid:90) σ2 + (cid:20) σ2 2 σ2 2 which can be simulated as gt(x) = σ2 pt(x) = σ2 pt(x) + pt(x)(cid:0)gt(x) Ept(x)gt(x)(cid:1) , (cid:10) log q1 (x), log q2 (x)(cid:11) , dxt = σtdWt , dwt = (cid:2)σ2 (cid:10) log q1 (xt), log q2 (xt)(cid:11)(cid:3)dt . log pt + log pt2 σ2 (cid:10) log t , log q2 (cid:11) (cid:90) dx pt (cid:2)σ2 (cid:10) log q1 , log q2 (cid:11)(cid:3) . (148) Proposition C.9 (Product of Re-weightings). Consider marginals q1,2 (x) generated by two different diffusion equations q1 (x) (cid:16) = (x) g1 q1 (cid:17) g1 (x) q1 (x) , (cid:16) = (x) g2 q2 g2 (x) (cid:17) q2 (x) . q2 (x) The product of densities pt(x) q1(x)q2(x) satisfies the following PDE pt(x) = pt(x)(cid:0)gt(x) Ept(x)gt(x)(cid:1) , gt(x) = g1 (x) + g2 (x) , Proof. We want to find the partial derivative of the annealed density By the straightforward calculations we have pt(x) = (x)q2 q1 (cid:82) dx q1 (x) (x)q2 (x) , pt(x) =? log pt = = (cid:16) log q1 + log q2 (x) g1 (cid:90) q1 (cid:104)(cid:16) dx pt g1 (x) (cid:17) (cid:16) + (x) g1 (cid:90) (cid:90) dx pt (cid:20) (x) g2 (cid:17) q2 (cid:16) g1 (x) q1 + log q1 + (cid:21) log q2 g2 (x) (cid:17) (x) g2 q2 (cid:17)(cid:105) g2 (x) = g1 (x) + g2 (x) dx pt (cid:2)g (x) + g2 (x)(cid:3) . 25 (144) (145) (146) (147) (149) (150) (151) (152) (153) (154) (155) (156) (157) (158) (159) (160) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Thus, we have which can be simulated as pt(x) = pt(x)(cid:0)gt(x) Ept(x)gt(x)(cid:1) , gt(x) = t (x) + g2 (x) , dxt = 0 , dwt = g1 (xt) + g2 (xt) . D. Proofs of Propositions Proposition D.1 (Annealed SDE). Consider the SDE then the samples from the annealed marginals pt,β(x) qt(x)β can be obtained via the following family of SDEs dxt = (cid:0)ft(xt) + σ2 log qt(xt)(cid:1)dt + σtdWt , dxt = (cid:0)ft(xt) + (β + (1 β)a)σ2 log qt(xt)(cid:1)dt + (cid:115) σ2 (β + (1 β)2a) β dWt , dwt = (cid:20) (β 1)(cid:10), ft(xt)(cid:11) + 1 β(β 1) log qt(xt)2 σ2 (cid:21) dt , where the parameter [0, 1/2]. Proof. For the following SDE lets consider everything but the drift ft. Thus, we can write the following PDE dxt = (cid:0)ft(xt) + σ2 log qt(xt)(cid:1)dt + σtdWt , qt = (cid:10), qt (cid:2)(1 a)σ2 log qt(xt) + aσ log qt(xt)(cid:3)(cid:11) + (1 b) σ2 2 qt + σ2 2 qt . (161) (162) (163) (164) (165) (166) (167) (168) (169) We apply Prop. C.2, Prop. C.1, Prop. C.4, Prop. C.3 (rules from Table 1) to the corresponding terms of the PDE above. Hence, the formulas for the weights are gt(x) = (1 a)σ2 β(β 1) log qt(x)2 aσ2 (β 1) log qt(x)+ + (β 1) (1 b)σ2 2 log qt(xt) β(β 1) bσ2 2 log qt(xt)2 . Lets cancel out the term with the Laplacians, hence, we have 2a = 1 (hence, [0, 1/2]) and gt(x) = (1 b/2)σ2 β(β 1) log qt(x)2 = 1 2 β(β 1) log qt(x)2 . σ2 (170) (171) (172) The PDE for the density is pt,β = (cid:10), pt,β (cid:0)ft + (β(1 a) + a)σ2 log qt (cid:1)(cid:11) + = (cid:10), pt,β (cid:0)ft + (β + (1 β)a)σ2 log qt (cid:1)(cid:11) + + (cid:18) 1 β (cid:19) σ2 2 β + (1 β)2a β σ2 2 pt,β + pt,β (cid:0)gt Ept,β gt (cid:1) (173) pt,β + pt,β (cid:0)gt Ept,β gt (cid:1) (174) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts This corresponds to the following family of SDEs (a [0, 1/2]) dxt = (cid:0)ft(xt) + (β + (1 β)a)σ2 log qt(xt)(cid:1)dt + (cid:115) σ2 (β + (1 β)2a) β dWt , dwt = (cid:20) (β 1)(cid:10), ft(xt)(cid:11) + 1 2 β(β 1) log qt(xt)2 σ2 (cid:21) dt . (175) (176) Proposition D.2 (Product of Experts). Consider two PDEs corresponding to the following SDEs dxt = (ft(xt) + σ2 log q1, (xt))dt + σtdWt , (177) which marginals we denote as q1 product of the marginals pt,β(x) (q1 (xt) and q2 (x)q2 (xt). The following family of SDEs (for all [0, 1/2]) corresponds to the (x))β dxt = (cid:0)ft(xt) + σ2 (β + (1 β)a)( log q1 (xt) + log q2 (xt))(cid:1)dt + (cid:115) σ2 (β + (1 β)2a) β dWt , (178) dwt = (cid:20) βσ2 (cid:10) log q1 (xt), log t (xt)(cid:11) + β(β 1) σ2 2 (cid:13) (cid:13) log q1 (xt) + log q2 (xt)(cid:13) 2 (cid:13) + (2β 1)(cid:10), ft(xt)(cid:11) (cid:21) dt . (179) Proof. First, according to Table 1, we have the following PDE for the product density pt(x) q1 (x)q2 (x) is pt(x) = (cid:10), pt(x)(cid:0)2ft(x) + σ2 + pt(x)(gt(x) Eptgt(x)) , ( log q1 (x) + log q2 (x))(cid:1)(cid:11) + σ2 pt(x)+ gt(x) = (cid:10) log q1 (x), ft(x) + σ2 (x), log q2 log q2 (x)(cid:11) (cid:10) log q1 σ2 t (x)(cid:11) + (cid:10) log q2 (x), ft(x) + σ2 log q1 (x)(cid:11) = σ2 (cid:10) log t (x), log q2 (x)(cid:11) (cid:10)ft(x), log q1 (x) + log q2 (x)(cid:11) . Now, combining Prop. D.1 and Prop. C.5, for the annealed density pt,β pt(x)β we have pt,β(x) = (cid:10), pt,β(x)(cid:0)2ft(x) + σ2 (β + (1 β)a)( log q1 (x) + log q2 (x))(cid:1)(cid:11)+ + β + (1 β)2a β σ2 2 (x), log q2 + (β 1)(cid:10), 2ft(x)(cid:11) + β(β 1) gt(x) = βσ2 (x)(cid:11) β(cid:10)ft(x), log q1 σ2 2 The last step is interpreting (cid:10), pt,β(x)ft(x)(cid:11) as the weight term, i.e. (cid:13) (cid:13) log q1 (cid:10) log q1 pt,β(x) + pt,β(x)(cid:0)gt(x) Ept,β gt(x)(cid:1) , (x) + log q2 (x) + log q2 (x)(cid:11)+ (x)(cid:13) 2 (cid:13) . pt,β(x) = (cid:10), pt,β(x)(cid:0)ft(x) + σ (β + (1 β)a)( log q1 (x) + log q2 (x))(cid:1)(cid:11)+ + β + (1 β)2a β σ2 pt,β(x) + pt,β(x)(cid:0)gt(x) Ept,β gt(x)(cid:1) , gt(x) = βσ2 (cid:10) log q1 (x), log q2 + (2β 1)(cid:10), ft(x)(cid:11) . (x)(cid:11) + β(β 1) σ2 2 (cid:13) (cid:13) log t (x) + log q2 (x)(cid:13) 2 (cid:13) + 27 (180) (181) (182) (183) (184) (185) (186) (187) (188) (189) (190) (191) (192) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Thus, we get the following family of SDEs (for all [0, 1/2]) dxt = (cid:0)ft(xt) + σ2 (β + (1 β)a)( log q1 (xt) + log q2 (xt))(cid:1)dt + (cid:115) σ2 (β + (1 β)2a) β dWt , dwt = (cid:20) βσ2 (cid:10) log q1 (xt), log t (xt)(cid:11) + β(β 1) σ2 2 (cid:13) (cid:13) log q1 (xt) + log q2 (xt)(cid:13) 2 (cid:13) + (2β 1)(cid:10), ft(xt)(cid:11) (193) (cid:21) dt . (194) Proposition D.3 (Classifier-free Guidance). Consider two PDEs corresponding to the following SDEs dxt = (ft(xt) + σ log q1,2 (xt))dt + σtdWt , (195) (xt) and q2 (xt). The SDE corresponding to the geometric average of the marginals which marginals we denote as q1 (x)1βq2 pt,β(x) q1 (x)β is dxt = (cid:0)ft(xt) + σ2 β(β 1)(cid:13) σ2 dwt = 1 2 ((1 β) log q1 (cid:13) log t (xt) log q2 (xt) + β log q2 (xt)(cid:13) 2 (cid:13) . (xt))(cid:1)dt + σtdWt , (196) (197) Proof. First, according to Prop. D.1, we perform annealing p1 t,1β(x) q1 (x)1β and p2 t,β(x) q2 (x)β, i.e. t,1β(x) = (cid:10), p1 t,1β(x)(cid:0)ft(x) + σ2 (1 β a1) log q1 (x)(cid:1)(cid:11) + 1 β 2a1 1 β σ2 2 p1 t,1β(x)+ (198) + p1 t,1β(x) (cid:16) (cid:17) gt(x) , p1 t,1β gt(x) 1 2 gt(x) = β(cid:10), ft(x)(cid:11) + β(β 1)(cid:13) σ2 (cid:13) log q1 (xt)(cid:13) 2 (cid:13) , and p2 t,β(x) = (cid:10), p2 t,β(x)(cid:0)ft(x) + σ2 (cid:16) gt(x) (cid:17) gt(x) , + p2 t,β(x) (β + (1 β)a2) log t (x)(cid:1)(cid:11) + gt(x) = (β 1)(cid:10), ft(x)(cid:11) + β(β 1)(cid:13) σ2 (cid:13) log q2 (xt)(cid:13) 2 (cid:13) , p2 t,β 1 2 β + (1 β)2a2 β σ2 2 p2 t,β(x)+ (199) (200) (201) (202) (203) Now, according to Table 1, for the product density pt,β p1 coefficient t,1β(x)p2 t,β(x). However, first, we have to match the diffusion 1 β 2a1 1 β = β + (1 β)2a2 β = (1 2a1)β β2 = β β2 + (1 β)22a a1β + (1 β)2a2 = 0 = a2 := , a1 = a(1 β)2 β . (204) (205) However, we see that the only possible solution that have a1 [0, 1/2] and a2 [0, 1/2] for positive β is a1 = a2 = 0. 28 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Thus, we have pt,β(x) = (cid:10), pt,β(x)(cid:0)2ft(x) + σ2 + pt,β(x)(cid:0)gt(x) Ept,β gt(x)(cid:1) , (1 β) log t (x) + β log q2 (x))(cid:1)(cid:11) + gt(x) = β(cid:10), ft(x)(cid:11) + 1 2 (cid:13) log q1 β(β 1)(cid:13) σ2 1 β(β 1)(cid:13) σ2 (x), ft(x) + σ2 (x), ft(x) + σ2 + (β 1)(cid:10), ft(x)(cid:11) + + (1 β)(cid:10) log q1 + β(cid:10) log q2 σ2 1 2 (cid:10), ft(x)(cid:11) (cid:10)(1 β) log q1 β(1 β)(cid:10) log q1 β(β 1)(cid:13) (cid:13) log q1 σ2 (x), log (cid:13) log q2 β log q2 (1 β) log q1 (x)(cid:11) + (x)(cid:13) 2 (cid:13) (x)(cid:13) 2 + (cid:13) (x)(cid:11)+ (x)(cid:11) = (x)(cid:13) 2 (cid:13) (x) + β log q2 Finally, we re-interpret (cid:10), pt,β(x)ft(x)(cid:11) as the weighting term, and get (x) log (x), ft(x)(cid:11) . pt,β(x) = (cid:10), pt,β(x)(cid:0)ft(x) + σ2 ((1 β) log q1 (x) + β log t (x))(cid:1)(cid:11) + σ2 2 pt,β(x) σ2 2 pt,β(x) + pt,β(x)(cid:0)gt(x) Ept,β gt(x)(cid:1) , 1 β(β 1)(cid:13) σ2 (cid:13) log q1 (x) log q2 (x)(cid:13) 2 (cid:13) . gt(x) = Thus, we have dxt = (cid:0)ft(xt) + σ2 β(β 1)(cid:13) σ2 dwt = 1 2 ((1 β) log q1 (cid:13) log t (xt) log q2 (xt) + β log q2 (xt)(cid:13) 2 (cid:13) . (xt))(cid:1)dt + σtdWt , Proposition D.4 (PoE + CFG). Consider two PDEs corresponding to the following SDEs dxt = (ft(xt) + σ2 dxt = (ft(xt) + σ2 log qt(xt))dt + σtdWt , log q1, (xt))dt + σtdWt , (206) (207) (208) (209) (210) (211) (212) (213) (214) (215) (216) (217) (218) (219) (220) (221) (xt) and q2 (xt). The SDE corresponding to the product of the marginals with corresponding marginals qt(xt), q1 pt,β(x) qt(x)2(1β)(q1 (x)q2 (x))β is dxt = (cid:0)ft(xt) + σ2 (v1 (cid:16)(cid:13) (cid:13) log qt(xt) log q1 (xt)(cid:11) + (cid:10), ft(xt)(cid:11) , σ2 β(β 1) (cid:10)v (xt))(cid:1)dt + σtdWt , (xt)(cid:13) 2 (cid:13) 1 2 + σ2 (xt) + v2 (xt), v2 dwt = where we denote v1, (x) = (1 β) log qt(x) + β log q1,2 (x). 29 + (cid:13) (cid:13) log qt(xt) log q2 (xt)(cid:13) (cid:13) 2(cid:17) + (222) (223) (224) Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Proof. Using Prop. D.3, we start from the SDEs simulating the product qt(x)(1β)q1 (x)β and qt(x)(1β)q2 )(cid:1)dt + σtdWt , (xt) (cid:125) (x)β, i.e. dxt = (cid:0) ft(xt) + σ 1 2 β(β 1)(cid:13) σ2 dwt = dxt = (cid:0) ft(xt) + σ2 ((1 β) log qt(xt) + β log q1 (cid:123)(cid:122) v1 (xt) (cid:124) (xt)(cid:13) 2 (cid:13) (cid:13) log qt(xt) log q1 ((1 β) log qt(xt) + β log q2 (cid:123)(cid:122) v2 (xt) (cid:124) , )(cid:1)dt + σtdWt , (xt) (cid:125) dwt = 1 2 β(β 1)(cid:13) σ (cid:13) log qt(xt) log q2 (xt)(cid:13) 2 (cid:13) . Then we consider the product of these SDEs, i.e. pt,β(x) gt(x) = (v1 (x) + v2 (x))(cid:1)(cid:11) + + (cid:13) = (cid:10), pt,β(x)(cid:0)2ft(x) + σ2 (cid:16)(cid:13) (cid:13) log qt(x) log q1 (x)(cid:11) + (cid:10)v2 (x), ft(x) + σ2 (cid:16)(cid:13) (cid:13) log qt(x) log q1 (x)(cid:11) (cid:10)ft(x), v1 1 σ2 β(β 1) 2 + (cid:10)v1 1 2 + σ2 σ2 β(β 1) (cid:10)v (x)(cid:13) 2 (cid:13) (x), ft(x) + σ2 (x)(cid:13) + (cid:13) 2 (cid:13) (x)(cid:11) . (x) + v2 (x), v2 v2 = σ2 2 pt,β(x) + pt,β(x)(cid:0)gt(x) Ept,β gt(x)(cid:1) , (x)(cid:13) + (cid:13) (x)(cid:11) (x), v2 2(cid:17) (x)(cid:13) (cid:13) (cid:13) log qt(x) log q2 (cid:10)v1 (x)(cid:11) σ2 (cid:13) log qt(x) log q2 v1 2(cid:17) + Re-interpreting (cid:10), pt,β(x)ft(x)(cid:11), we get pt,β(x) gt(x) = = (cid:10), pt,β(x)(cid:0)ft(x) + σ2 (v1 (x) + v2 1 2 + σ2 σ2 β(β 1) (cid:10)v1 (x), v2 (cid:16)(cid:13) (cid:13) log qt(x) log q1 (x)(cid:11) + (cid:10), ft(x)(cid:11) , σ2 2 (x))(cid:1)(cid:11) + + (cid:13) (x)(cid:13) 2 (cid:13) pt,β(x) + pt,β(x)(cid:0)gt(x) Ept,β gt(x)(cid:1) , (x)(cid:13) (cid:13) (cid:13) log qt(x) log q2 2(cid:17) + which corresponds to (xt) + v2 dxt = (cid:0)ft(xt) + σ2 (v1 (cid:16)(cid:13) (cid:13) log qt(xt) log q1 (xt)(cid:11) + (cid:10), ft(xt)(cid:11) . σ2 β(β 1) (cid:10)v1 (xt))(cid:1)dt + σtdWt , (xt)(cid:13) 2 (cid:13) 1 2 + σ2 (xt), v2 dwt = + (cid:13) (cid:13) log qt(xt) log q2 (xt)(cid:13) (cid:13) 2(cid:17) + (225) (226) (227) (228) (229) (230) (231) (232) (233) (234) (235) (236) (237) (238) (239) Proposition D.5 (Reward-tilted SDE). Consider the following SDE which samples from the marginals qt(x). The samples from the marginals pt(x) qt(x) exp(βtr(x)) can be simulated according to the following SDE dxt = vt(x)dt + σtdWt , (240) dxt = vt(xt)dt + σtdWt , dwt = (cid:20)(cid:28) βtr(xt), vt(xt) σ2 log qt(xt) σ2 2 (cid:29) βtr(xt) βt σ2 2 r(xt) + (cid:21) r(xt) dt . βt For the reverse SDE, it is dxt = (ft(xt) + σ2 (cid:20)(cid:28) dwt = βtr(xt), ft(xt) log qt(xt))dt + σtdWt , (cid:29) σ2 2 βtr(xt) βt σ2 2 r(xt) + (cid:21) r(xt) dt βt (241) (242) (243) (244) 30 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Proof. First, consider the density qt(x) that follows the PDE We want to find the PDE for the reward-tilted density qt(x) = (cid:10), qt(x)vt(x)(cid:11) + σ2 2 qt(x) . pt(x) = qt(x) exp(βtr(x)) (cid:82) dx qt(x) exp(βtr(x)) . Straightforwardly, we get log pt(x) = log qt(x) + (cid:90) r(x) βt dx pt(x) (cid:20) log qt(x) + (cid:21) r(x) βt For the first term, we have log qt(x) = (cid:10), vt(x)(cid:11) (cid:10) log qt(x), vt(x)(cid:11) + = (cid:10), vt(x)(cid:11) (cid:10) log pt(x), vt(x)(cid:11) + (cid:28) + βtr(x), vt(x) σ2 log qt(x) σ2 2 σ2 2 σ2 2 log qt(x) + log pt(x) + (cid:29) βtr(x) σ2 2 σ2 2 σ2 2 βt r(x) . log qt(x) log pt(x)2+ Thus, we have pt(x) gt(x) = This can be simulated as = (cid:10), pt(x)vt(x)(cid:11) + pt(x) + pt(x)(cid:0)gt(x) Ept(x)gt(x)(cid:1) σ2 2 (cid:28) βtr(x), vt(x) σ2 log qt(x) σ2 (cid:29) βtr(x) βt σ2 2 r(x) + βt r(x) . dxt = vt(xt)dt + σtdWt , dwt = (cid:20)(cid:28) βtr(xt), vt(xt) σ2 log qt(xt) σ2 2 (cid:29) βtr(xt) βt σ2 2 r(xt) + (cid:21) r(xt) dt βt (245) (246) (247) (248) (249) (250) (251) (252) (253) (254) E. Additional Related Work Amortized Sampling Recently, there has been renewed interested in learning amortized samplers, and particularly diffusion-based amortized samplers particularly towards molecular systems. Midgley et al. (2023) explored learning normalizing flow using an α-divergence trained with samples using annealed importance sampling (Neal, 2001). Zhang & Chen (2022); Vargas et al. (2023; 2024); Richter & Berner (2024); Akhound-Sadegh et al. (2024); Albergo & Vanden-Eijnden (2024); Bortoli et al. (2024) learn diffusion annealed bridges between distributions using various methods. While we use DEM in this work as it achieves state of the art results for our LJ-13 setting, there are several works that build upon DEM using bootstrapping (OuYang et al., 2024) and learning the energy function instead of the score (Woo & Ahn, 2024). We note that our FKC sampler applies to any diffusion based sampler. (Wasserstein)-Fisher-Rao Gradient Flows The reweighting portion of our Feynman-Kac weighted SDEs corresponds to non-parametric Fisher-Rao gradient flow of linear functional G[pt] = (cid:82) gt ptdx, whereas gradient flows in the Wasserstein Fisher-Rao metric (Kondratyev et al., 2015; Chizat et al., 2018; Liero et al., 2018) have form similar to our weighted PDEs (Lu et al., 2019) for an appropriate ODE simulation term vt = gt. In sampling applications, Chemseddine et al. (2024) study the problem of when given tangent direction in the Fisher-Rao space can be simulated using transport via tangent direction in the Wasserstein space. Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts F. Additional Experimental Details and Results F.1. Sampling Metrics We use number of metrics to asses the quality of generated samples. These metrics capture different aspects of the distribution. Before computing metrics we filter out samples with energy > 100. This only affects non-resampled metrics and prevents numerical. We find this filters out no samples for DEM or with FKC, and filters less than 3% of samples with target score SDE or tempered noise SDE sampling. We justify this as it is easy to set these filters for generated samples of very poor quality. Distance-W2 For the LJ-13 task we compute the 2-Wasserstein distance between pairwise distance histograms. For this metric we take all pairwise distances for all samples and flatten them into single distribution. For sample of 10,000 points this leads to distributions of size 700,000 as there are 70 pairwise distances for 13 particle system. This is useful metric as it is equivariant and measures the global fidelity of the generated samples. It however is not useful for assessing fine grained details of the generated samples. For that we turn to the Energy-W1/2 distances. Energy-W1/2 The Energy-W1 and Energy-W2 measures the deviation in the energy value distribution of samples from the reference distribution and the generated distribution. We find this metric is useful to assess the overall fit of model, although it cannot assess whether sampler drops modes well. model that has reasonably small Energy Wasserstein distance may still have missed mode of similar energy value. Maximum Mean Discrepancy (MMD) We use radial-basis function MMD with multiple scales to assess distribution fit. This measures how well the reference distribution matches the generated distribution locally. Total Variation distance For low dimensional sampling problems, it is useful to consider the total variation distance between empirical distributions that are discretized into grid. This measures fit in terms of density, ignoring the underlying metric, and is less sensitive to global reweighting of modes. 1-Wasserstein and 2-Wasserstein distances (W1 / W2) On 40 GMM we also measure the 1-Wasserstein and 2-Wasserstein distances between the generated and reference distributions with respect to the Euclidean metric. We note that while this is possible to measure in the LJ-13 case, it is not as useful as particles in the LJ-13 setting are SE(3) equivariant, and therefore the Euclidean distance is not suitable ground metric. F.2. Mixture of 40 Gaussians The mixture of 40 Gaussians setting is 2D energy function with 40 randomly initialized modes with equal standard deviation. This serves as useful experimental setting where we are able to calculate true densities and scores efficiently without modelling error. F.2.1. ADDITIONAL RESULTS We include quantitative results for the tractable GMM example in Sec. 5.2, where we start at temperature TL = 3 and anneal to target temperature TS = 1/3. We used geometric noise schedule with σmin = 0.01 and σmax = 500. We sample 10k samples with 1000 integration steps, with dt = 0.001. We observe that Target Score sampling (a = 0) from Eq. (22) with systematic resampling performs best in more metrics. We also use this example as an ablation study for the impact of the resampling scheme, where we find that systematic resampling appears to outperform the birth-death exponential clocks implementation of the jump process resampling. See Sec. 4 and App. B.2. On ground truth qβ is not mixture of π Gaussians, but rather πTL Gaussians for integer TL. This means that we are restricted to small integer TL. We use TL = 3 for all experiments in the 40 Gaussians setting. Note that we reserve β = TS/TL for the ratio of learning and sampling/target temperatures. subtle point to note is that qTL F.3. LJ-13 Sampling Task The Lennard-Jones Potential. The Lennard-Jones (LJ) potential is an intermolecular potential, modelling interactions of non-bonding particles. This system is studied to evaluate the performance of various neural samplers. The energy for the Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Figure A1. Comparison between the energy distribution of the MCMC dataset, samples generated using DEM model trained at the target temperature, and samples generated using temperature annealing from model trained at starting distribution = 2. Left: the target temperature is 1.5 and Right: the target temperature is 0.8. Figure A2. Comparison between the distribution of the interatomic distances of the particles in the MCMC dataset, samples generated using DEM model trained at the target temperature, and samples generated using temperature annealing from model trained at starting distribution = 2. Left: the target temperature is 1.5 and Right: the target temperature is 0.8. 33 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Table A1. Mixture of 40 Gaussians. Sampling from an annealed distribution with inverse temperature β = 3. Metrics are calculated over 5 runs with 10k samples. SDE Type FKC Energy-W2 MMD Total Var W2 Target Score Tempered Noise Target Score Tempered Noise Target Score Tempered Noise 0.943 0.026 1.032 0.012 1.064 0.369 1.228 0.401 1.098 0.418 0.926 0.248 0.020 0.001 0.058 0.001 0.010 0.004 0.056 0.029 0.007 0.005 0.027 0.011 0.487 0.007 0.638 0.002 0.402 0.029 0.572 0.055 0.372 0.020 0.512 0.017 11.304 0.296 16.051 0.123 7.797 3.990 12.598 4.155 6.256 3.960 9.974 1. 15.671 0.269 19.627 0.101 12.451 5.417 17.679 4.178 11.265 5.629 14.045 1.308 BDC BDC systematic systematic Figure A3. Left: 1-Wasserstein between energy distributions and Right: 2-Wasserstein between distributions of interatomic distances of MCMC samples from the annealed distribution and generated samples. system is based on the interatomic distance between the particles is given by: (cid:18) rm dij (cid:32)(cid:18) rm dij LJ(x) = ε 2τ (cid:88) (cid:19)6 (cid:19)12(cid:33) ij (255) where we denote the Euclidean distance between two particles and by dij = xi xj2 and rm, τ , ϵ and are physical constants. As in Köhler et al. (2020), we also add harmonic potential to the energy so that LJsystem = LJ(x)+cE osc(x) The harmonic potential is given by: osc(x) = 1 2 (cid:88) xi xCOM2 (256) where xCOM refers to the center of mass of the system. We set rm = 1, τ = 1, ε = 2.0 and = 1.0. Training details. All DEM models are trained for 166 epochs on 4 NVIDIA A100 80GB GPUs. For all models, the best checkpoint with the lowest energy-W2 is used for inference. The model is an EGNN with the same architecture as in Akhound-Sadegh et al. (2024). Similar to Akhound-Sadegh et al. (2024), we use geometric noise schedule for all experiments. We set σmin = 0.01 and σmax = 4.0. We clip the score to maximum norm of 1000 (per particle). For sampling, we use 1000 integration steps with dt = 0.001. For inference with FKC, we assume Gaussian distribution at time tstart = 0.99 and start integration with the annealed SDE and resampling at that time. We found that this helps significantly to reduce the variance of the results over different runs. For visualizations in Fig. A2, we selected the best run for all methods for consistency. In line with previous work, we find the DEM scores are noisy at high times, based on the score of the energy. This can be seen from the score estimator in DEM, which depends on the average gradient direction from normal distribution sampled around xt. The variance of this estimate grows with both time and gradient of the energy. This makes DEM style objective significantly easier to train on smooth energies, as quantified by norm of the score of the energy. Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Table A2. Additional results for LJ-13 at different target temperatures. The model is trained at starting temperature TL = 2.0 and metrics are computed over 3 runs. DEM is run for one seed only as the standard-deviation over seeds is negligible. Target Temp."
        },
        {
            "title": "SDE Type",
            "content": "FKC distance-W2 Energy-W1 Energy-W2 0.9 (β=2.2)"
        },
        {
            "title": "Tempered Noise",
            "content": "1.0 (β=2.0)"
        },
        {
            "title": "Tempered Noise",
            "content": "1.2 (β=1.67)"
        },
        {
            "title": "Tempered Noise",
            "content": "DEM 0.215 0.001 0.042 0.009 0.110 0.016 0.042 0.004 0.168 0.221 0.001 0.039 0.008 0.094 0.002 0.053 0.008 0.127 0.234 0.004 0.026 0.001 0.098 0.002 0.076 0.006 0.143 13.886 0.040 6.218 0.896 5.633 0.090 4.384 0.135 14.516 12.915 0.054 2.629 0.665 5.215 0.095 3.205 0.462 1.352 10.414 0.036 2.831 0.155 4.258 0.069 1.017 0.494 9.669 14.893 0.012 6.259 0.873 7.682 0.585 4.530 0.167 14.606 13.558 0.112 2.876 0.548 6.560 0.000 3.538 0.468 2.050 10.910 0.110 2.915 0.074 5.564 0.095 1.300 0.433 9.736 Figure A4. Energy distributions of samples generated with temperature annealing compared to the MCMC samples (in dark green), at different target temperatures. The starting temperature is TL = 2.0. 35 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Sampling Reference distributions To generate reference distributions from the Lennard-Jones-13 potential we use Pyro (Bingham et al., 2018) and No-U-Turn sampler (Hoffman & Gelman, 2011) with default arguments. We use 20k warmup steps and collect 20k samples from the 10 chains for each temperature. Additional results In Table A2 we can see additional results extending Table 2 for intermediate temperatures. Here we can see that generally the same patterns hold with one exceptionDEM on target temperature of 1.0 is better than FKC with β = 2.0 on Energy-W1 and Energy-W2 metrics. This means that DEM has on this temperature has better local fidelity but slightly worse global fidelity. This is quite interesting as we know DEM was originally developed and therefore tuned with temperature of 1.0 on this dataset. Our hypothesis is that some hyperparameters are specifically tailored to this setting. It is quite interesting then that FKC can still perform better than DEM on global metrics for this temperature. In Fig. A3 we can see the Energy-W1 and Interatomic Distance W2 metrics plotted against the target temperature using model trained at temperature 2.0. Here we see that FKC performs well across all temperatures for our global metric of interatomic distances and across energy W1 distances, although at low target temperatures the Energy-W1 metric gets worse for all methods. We note that this is after excluding roughly 2-3% of samples with unacceptably bad energy from the Target Score SDE and Tempered Noise SDE. Therefore even though the lines are close here, we still prefer the FKC samplers. F.4. Multi-Target Structure-Based Drug Design SDE Component Analysis In Table A3, we show the performance of varying the following SDE settings for dual-target drug design: SDE type, β, and the presence/absece of FKC resampling. Here, we report metrics for generating molecules on single protein pocket pair (UniProt IDs P23786/P05023) as the validation set. We generate molecules batch 32 molecules for 5 different molecule lengths, which were sampled from the original training distribution (Guan et al., 2023): {15, 19, 23, 27, 35}. We study the impact of the following changing the following settings: Inverse temperature (β) We find that as we increase β from 0.5 to 2.0 the product of the docking scores of the protein pair increases, though the delta increase is larger at smaller βs. FKC. Next, we try turning FKC on at fixed β. We find that performance improves at both β = 1.5 and β = 2.0, although the improvement at β = 2.0 is larger. However, this comes at cost of diversity and the uniqueness of molecules generated. tmax Given that resampling is helpful in terms of improving the quality of the final molecules but decreases molecular diversity, we investigate setting tmax for our best β settings, where we resample only when τ <= tmax. We find that setting tmax to value in [0.5, 0.7] generates molecules that are higher in quality compared to always resampling or no resampling for β = 1.5. For β = 2.0, the performance slightly decreases, but the diversity and uniqueness of the molecules is much higher at the end. Setting tmax to 0.6 gives good tradeoff in terms of generating molecules that perform well vs. maintaining diversity, and so we proceed with β = 2.0 and tmax = 0.6 for the final experiments. SDE Type Finally, we try using different types of SDEs. We find that at lower β, the Tempered Noise SDE performs better with and without FKC. At higher β, however, using the Tempered Noise SDE does not significantly change performance or decreases performance. Thus, for the main experiments, we proceed with the Target Score SDE. Visualizing docked molecules GRM5/RRM1 (UniProt IDs P41594 and P23921, respectively) at each molecule size. In Fig. A5, we visualize the molecules with the highest docking scores to the protein pair F.5. Molecule SMILES generation using latent diffusion models We also investigate generating molecular SMILES strings conditioned on functional properties, which describe the desired function that the molecule should have. Molecules often need to possess multiple properties (e.g. bind to protein and be non-toxic) (Wang et al., 2024). By controlling for these properties during the molecular generation process (as opposed to post-hoc filtering), we aim to increase the probability of discovering molecules that exhibit all desired characteristics, thereby improving the efficiency of hit identification. Model We select LDMol (Chang & Ye, 2024) to generate molecules, which is latent diffusion model conditioned on natural language descriptions of molecule properties; this gives flexibility of generating molecules with wide range of properties. 36 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Figure A5. Molecules generated from our method (target score SDE with β = 2.0 and FKC resampling) and baselines in the binding pockets of two proteins: GRM5 (UniProt ID P41594) and RRM1 (UniProt ID P23921) for all 5 molecule sizes considered ({15, 19, 23, 27, 35} atoms). Docking scores for each molecule and target are above each image; lower docking scores are better. The QED of the molecule is above each model name. The binding pocket is shaded in light green. 37 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Table A3. Performance of generated molecules with different SDE settings. We generate 32 molecules for 5 molecule sizes for one protein pair for each setting. Lower docking scores are better. Values are reported as averages over all generated molecules in each run. \"Better than ref.\" is the percentage of ligands with better docking scores than known reference molecules for both targets (the mean docking score for the reference molecules is 8.2551.849). We also report the diversity, validity & uniqueness, and quality, which refers to the percentage of molecules that are valid, unique, have QED 0.6 and SA 4.0 (Lee et al., 2025b). Bolded values are the best metrics within each set of midlines. For β = 1, target score and tempering noise match (Prop. 3.3). β FKC tmax SDE Type (P1 * P2) () max(P1, P2) () P1 () P2 () Better than ref. () Div. () Val. & Uniq. () Qual. () 0.5 1.0 1.5 2. 1.5 1.5 2.0 2.0 1.5 1.5 1.5 1.5 1.5 2.0 2.0 2.0 2.0 2.0 0.5 0.5 0.5 0. 2.0 2.0 2.0 2.0 2.0 2.0 0.4 0.5 0.6 0.7 1.0 0.4 0.5 0.6 0.7 1.0 Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Target Score Tempered Noise 0.6 0.6 Target Score Tempered Noise 7.9860.948 67.65711.985 8.1740.989 73.36614.423 75.21315.779 8.2581.024 75.55116.345 8.0890.899 8.9660.884 8.3091. 7.6670.687 7.9290.763 8.0850.856 8.3770.661 8.8430.899 8.9800.935 75.21315.779 75.79832.984 8.0850.856 8.9800.935 8.2581.024 8.5823.200 8.8291.193 7.4382.507 75.55116.345 8.3091.112 91.84528.421 8.9771.433 9.9841.533 8.9781.434 8.0890. 8.9660.884 7.9610.785 8.3650.905 8.3221.083 74.55814.361 80.42116.567 8.5391.077 83.40519.024 8.5301.070 9.5161.083 8.6461.098 9.4201.229 83.10019.354 8.7231.227 8.5823.200 8.8291.193 75.79832.984 8.4340.912 7.4382.507 8.8830.885 9.3140.882 8.4671.133 9.2830.968 79.73417.631 9.5291.018 8.7961.220 84.94919.056 9.6191.101 8.9881.442 87.98322.856 8.8271.452 9.5141.028 85.16821.258 91.84528.421 8.9771.433 9.9841.533 8.9781. 8.3310.981 8.5690.978 8.7901.231 8.5741.097 67.65711.985 7.9860.948 71.60615.139 7.8380.872 8.7270.878 8.0851.088 7.6670.687 8.3770.661 77.10016.533 8.3371.045 78.50115.383 8.3230.919 9.1270.770 8.4961.100 8.2430. 9.1120.898 Target Score Tempered Noise 75.55116.345 75.86816.154 8.0890.899 8.0450.909 8.9660.884 8.3091.112 8.9770.978 8.3521. 0.6 0.6 0.7 0.7 Target Score Tempered Noise 87.98322.856 8.7901.231 9.6191.101 8.9881.442 8.6811.481 79.69618.087 8.2290.997 9.1040. Target Score Tempered Noise 85.16821.258 8.5741.097 8.5310.980 84.96918.906 9.5141.028 8.8271.452 9.8091.170 8.5450.991 0.2510.199 0.3780.311 0.4020.339 0.3910.331 0.4020.339 0.4460.454 0.3910.331 0.6710. 0.3720.328 0.4940.378 0.4850.441 0.5030.441 0.4460.454 0.4980.388 0.5140.423 0.5690.468 0.5930.484 0.6710.419 0.2510.199 0.3620.274 0.4170.337 0.4960.308 0.3910.331 0.4600.390 0.5690.468 0.4640. 0.5930.484 0.5890.427 0.8860.006 0.8840.008 0.8800.012 0.8810.011 0.8800.012 0.6510.102 0.8810.011 0.6170.049 0.8830.021 0.8670.014 0.8200.024 0.7990.030 0.6510.102 0.8760.012 0.8510.025 0.8180.011 0.7860.034 0.6170. 0.8860.006 0.8870.007 0.8770.008 0.8790.016 0.8810.011 0.8740.008 0.8180.011 0.7960.038 0.7860.034 0.7960.035 0.9690.062 0.9620.023 0.9880.015 0.9940. 0.9880.015 0.4750.169 0.9940.012 0.4750.132 0.9810.038 0.9940.012 0.9250.078 0.8880.094 0.4750.169 0.9880.015 0.9380.059 0.8880.073 0.8060.096 0.4750.132 0.9690.062 0.9440.098 0.9750.023 0.9310. 0.9940.012 0.9940.012 0.8880.073 0.8380.109 0.8060.096 0.8500.121 0.2440.161 0.2310.170 0.2500.159 0.2880.179 0.2500.159 0.1000.157 0.2880.179 0.0440. 0.2620.222 0.2880.233 0.2440.196 0.1620.205 0.1000.157 0.3060.234 0.2440.160 0.2120.223 0.2310.224 0.0440.073 0.2440.161 0.3000.212 0.2120.155 0.2500.163 0.2880.179 0.2620.186 0.2120.223 0.3310. 0.2310.224 0.2620.154 Experiment setup: TDC oracles We consider three proteins oracles from Therpeutic Data Commons (TDC) (Huang et al., 2021): JNK3, GSK3β, DRD2, which predicts whether or not molecule binds to protein. Note that while this task is similar in nature to the objective of SBDD, we are generating molecules conditioned on functional text description instead of 3D protein pocket. However, we could also consider other functional property descriptions, such as molecular solubility, toxicity, etc. Prompts To generate molecules that inhibit specific protein, we prompt the model with This molecule inhibits {protein_name}\", following Wang et al. (2024). In addition to reporting the top-performing molecules, we report the percent of molecules that are valid and Metrics unique, as well as their diversity (evaluated using Tanimoto distance on Morgan fingerprints (Rogers & Hahn, 2010)) and quality, which is the set of unique and valid molecules that also have quantitative estimate of drug-likeness (QED) 0.6 and synthetic accessibility (SA) 4.0. This metric was taken from Lee et al. (2025b). Results: TDC oracles We aim to generate molecules that satisfy the function of binding each protein when taking all combinations of the protein pairs. In Table A4, we show the best performance for each set of molecules and in Table A6 we ablate different SDE components. We find that the tempered noise SDE at higher β generates molecules that have higher fitness for binding to each pair of proteins. When we incorporate FKC, the average performance of the molecules further increases. We also note that PoE+FKC tends to generate more molecules that are unique, valid and are higher drug-like quality, although their diversity decreases slightly, which is common tradeoff. In practice, we find that the FKC weights with the latent diffusion model have large variance during molecule generation. This is problematic, as large number of samples are thrown away. Furthermore, we noted that the score was not always well-conditioned. To ameliorate this, for all experiments using LDMol, we divided the weights by set temperature term (T = 100) to reduce their variance before resampling, clipped the top 20% to account for any score instabilities, and did early-stopping (only resampled for 70% of the timesteps). Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Table A4. Multi-property molecule generation results (PoE). For set of two target properties (P1 and P2), we take the set of the top-10 best performing molecules from batch-size of 512 as the molecules with the highest P1*P2 scores. We report averages of the top-10 molecules from 5 runs and the top-1 molecule overall. We also report the diversity, validity & uniqueness, and quality of all molecules. P1 / P2 SDE Type JNK3 GSK3β JNK3 DRD GSK3β DRD2 Target Score Tempered Noise Target Score Tempered Noise Target Score Tempered Noise β 0.5 1.5 0.5 1. 0.5 1.5 FKC P1 top-10 () P2 top-10 () (P1, P2) top-1 () Div. () Val. & Uniq. () Qual. () 0.2120.016 0.3410.039 0.3420.012 0.0900.018 0.1320.032 0.1410.020 0.1460.034 0.2280.016 0.2660.061 0.3560.046 0.4680.041 0.5020.034 0.4340.065 0.5500.036 0.6170.040 0.5280.077 0.6490.084 0.6380.036 (0.500, 0.580) (0.590, 0.560) (0.500, 0.720) (0.150, 0.472) (0.280, 0.469) (0.360, 0.655) (0.051, 0.908) (0.550, 0.655) (0.520, 0.796) 0.9100.000 0.8810.002 0.8820.002 0.9150.001 0.8840.001 0.8840.005 0.9140.001 0.8840.002 0.8850.002 0.7130.027 0.8130.025 0.8320.021 0.6710.022 0.6500.021 0.6610.018 0.7090.021 0.7740.015 0.7740. 0.1270.015 0.3520.012 0.3600.021 0.2280.011 0.2580.020 0.2520.014 0.2030.015 0.3030.012 0.3070.012 Table A5. Docking scores of generated molecules to P1=ATP1A1 and P2=CPT2. We used the Tempered Noise SDE with β = 1.5 and generated 32 molecules. FKC (P1, P2) top-10 () (P1, P2) top-1 () Div. () 6.651.05, 7.360.854 (7.490.71, 8.310.94) (8.87, 8.13) (8.41, 9.73) 0.921 0.895 Experiment setup: protein docking Finally, we consider more challenging setting of protein-ligand docking, where we generate molecules using LDMol based on text-based prompts of binding to the proteins ATP1A1 (UniProt ID P05023) and CPT2 (UniProt ID P23786), and then evaluate them using docking. The protein pockets were obtained from Zhou et al. (2024) and the final generated molecules were docked using AutoDock Vina (Eberhardt et al., 2021). Results: protein docking Table A5 shows the docking scores of molecules, and we find that incorporating FKC generates molecules with better scores. While ligands are typically generated using SBDD, we find it interesting that text-prompt generation is able to produce molecules that have reasonably good docking scores; known binders to ATP1A1 and CPT2 have docking scores of -8.168 and -9.174, respectively (Zhou et al., 2024). We visualize the top molecules in Fig. A6. F.6. SDXL: Additional images and hyperparameter search We show additional images generated by our method and vanilla SDXL in Fig. A7. In Fig. A8, we motivate our selection of β = 5.5 for our experiments by plotting ImageReward and CLIP Score as function of β and selecting the value that gives the highest scores. Figure A6. Molecules with best docking scores for binding to ATPA1 (P1) and CPT2 (P2) from PoE with FKC (left) and without (right). 39 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Table A6. Multi-property molecule generation results. For set of two target properties (P1 and P2), we take the set of the top-10 best performing molecules as the molecules with the highest P1*P2 scores. We report the average properties of the top-10 molecules over five runs and the top-1 molecule overall. We also report the diversity, validity & uniqueness, and quality of all generated molecules, where quality is the percent of molecules that are valid, unique, have QED 0.6 and SA < 0.4. For β = 1, target score and tempering noise match (Prop. 3.3). P1 P2 JNK3 GSK3β JNK3 DRD2 GSK3β DRD2 SDE Type β FKC P1 top-10 () P2 top-10 () (P1, P2) top-1 () Div. () Val. & Uniq. () Qual. () Target Score Tempered Noise Target Score Target Score Tempered Noise Tempered Noise Target Score Tempered Score Target Score Target Score Tempered Noise Tempered Noise Target Score Tempered Score Target Score Target Score Tempered Score Tempered Score 0.5 1.0 1.5 0.5 1.0 1. 0.5 1.0 1.5 0.2120.016 0.2250.028 0.2890.022 0.3420.029 0.3360.031 0.3510.0340 0.3410.039 0.3420.012 0.0900.018 0.0660.015 0.0870.028 0.0940.024 0.1360.046 0.1020.031 0.1320.032 0.1410.020 0.1460.034 0.1620.025 0.2020.023 0.1900.022 0.2400.030 0.2220.036 0.2280.016 0.2660.061 0.3560.046 0.3850.042 0.4290.018 0.4420.051 0.4840.052 0.4470.026 0.4680.041 0.5020.034 0.4340.065 0.5710.187 0.6240.094 0.6350.067 0.5820.067 0.6200.148 0.5500.036 0.6170.040 0.5280.077 0.5430.063 0.6200.057 0.6660.093 0.6360.066 0.5840.068 0.6490.084 0.6380.036 (0.500, 0.580) (0.440, 0.690) (0.470, 0.580) (0.600, 0.650) (0.480, 0.780) (0.590, 0.780) (0.590, 0.560) (0.500, 0.720) (0.150, 0.472) (0.110, 0.943) (0.100, 0.978) (0.413, 0.550) (0.490, 0.640) (0.320, 0.541) (0.280, 0.469) (0.360, 0.655) (0.051, 0.908) (0.430, 0.965) (0.660, 0.726) (0.240, 0.986) (0.350, 0.804) (0.630, 0.580) (0.550, 0.655) (0.520, 0.796) 0.9100.000 0.9090.001 0.8980.002 0.8970.002 0.8860.003 0.8860.003 0.8810.002 0.8820.002 0.9150.001 0.9140.002 0.9030.001 0.8990.002 0.8860.003 0.8850.006 0.8840.001 0.8840.005 0.9140.001 0.9140.001 0.9080.002 0.9070.002 0.8940.002 0.8910.003 0.8840.002 0.8850.002 0.7130.027 0.7230.016 0.8110.008 0.8040.015 0.8160.013 0.8230.024 0.8130.025 0.8320.021 0.6710.022 0.6780.0187 0.6750.022 0.6860.025 0.6390.019 0.6590.022 0.6500.021 0.6610.018 0.7090.021 0.6970.013 0.7730.021 0.7840.010 0.7590.015 0.7400.027 0.7740.015 0.7740.017 0.1270.015 0.1340.006 0.2050.011 0.2050.015 0.3360.022 0.3560.037 0.3520.012 0.3600.021 0.2280.011 0.2360.020 0.2410.010 0.2630.023 0.2410.017 0.2740.028 0.2580.020 0.2520.014 0.2030.015 0.1980.017 0.2380.021 0.2540.019 0.2900.016 0.2830.020 0.3030.012 0.3070.012 40 Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts Figure A7. Samples from SDXL using vanilla CFG (left) or our method of CFG + FKC (right). Figure A8. Parameter search for guidance scale. We plot CLIP Score (orange) and ImageReward (green) for different β. Underneath, we show sample images for the prompt \"a photo of an orange bench and black refrigerator\" at different β."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Imperial College London",
        "McGill University",
        "Mila - Quebec AI Institute",
        "University of Toronto",
        "Université de Montréal",
        "Vector Institute"
    ]
}