{
    "paper_title": "Improving Inference-Time Optimisation for Vocal Effects Style Transfer with a Gaussian Prior",
    "authors": [
        "Chin-Yun Yu",
        "Marco A. Martínez-Ramírez",
        "Junghyun Koo",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji",
        "György Fazekas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach for transferring the applied effects of a reference audio to a raw audio track. It optimises the effect parameters to minimise the distance between the style embeddings of the processed audio and the reference. However, this method treats all possible configurations equally and relies solely on the embedding space, which can lead to unrealistic or biased results. We address this pitfall by introducing a Gaussian prior derived from a vocal preset dataset, DiffVox, over the parameter space. The resulting optimisation is equivalent to maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the MedleyDB dataset show significant improvements across metrics compared to baselines, including a blind audio effects estimator, nearest-neighbour approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter mean squared error by up to 33% and matches the reference style better. Subjective evaluations with 16 participants confirm our method's superiority, especially in limited data regimes. This work demonstrates how incorporating prior knowledge in inference time enhances audio effects transfer, paving the way for more effective and realistic audio processing systems."
        },
        {
            "title": "Start",
            "content": "Improving Inference-Time Optimisation for Vocal Effects Style Transfer with Gaussian Prior Chin-Yun Yu1, Marco A. Martınez-Ramırez2, Junghyun Koo2, Wei-Hsiang Liao2, Yuki Mitsufuji2,3, Gyorgy Fazekas1 1Centre for Digital Music, Queen Mary University of London, London, UK 2Sony AI, Tokyo, Japan 3Sony Group Corporation, Tokyo, Japan 5 2 0 2 6 1 ] . [ 1 5 1 3 1 1 . 5 0 5 2 : r AbstractStyle Transfer with Inference-Time Optimisation (ST-ITO) is recent approach for transferring the applied effects of reference audio to raw audio track. It optimises the effect parameters to minimise the distance between the style embeddings of the processed audio and the reference. However, this method treats all possible configurations equally and relies solely on the embedding space, which can lead to unrealistic or biased results. We address this pitfall by introducing Gaussian prior derived from vocal preset dataset, DiffVox, over the parameter space. The resulting optimisation is equivalent to maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the MedleyDB dataset show significant improvements across metrics compared to baselines, including blind audio effects estimator, nearest-neighbour approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter mean squared error by up to 33% and matches the reference style better. Subjective evaluations with 16 participants confirm our methods superiority, especially in limited data regimes. This work demonstrates how incorporating prior knowledge in inference time enhances audio effects transfer, paving the way for more effective and realistic audio processing systems. 1. INTRODUCTION Audio production is complex task that needs years of practice and experience to master. Audio engineers need to know their tools and understand their working context, such as the instruments characteristics, the recording environment, and the artists intention. For the latter, it is common to use reference track provided by the client to guide the process [1]. This user-friendly paradigm has also been applied to automatic music mixing with deep learning [2], [3]. The systems can either be transformation model that directly manipulates the audio [2], [4] or predict the control parameters of audio effects [3], [5], [6], where the latter is more preferable since it is more controllable and generalisable to real-world workflow. At the same time, the high degree of freedom of the former could introduce artefacts. Nevertheless, these parameter-based systems use fixed effects and track routings, which hinders their flexibility. 1.1. Inference-time optimisation (ITO) More recent works explore the idea of processing the audio during inference time, which is free from training conditions and can generalise to arbitrary effects and routings. Text2Fx [7] leverages the joint text and audio embedding space of CLAP [8] to optimise the parameters of an equaliser and reverb so the processed audio has an embedding close to the text prompt. Steinmetz et al. [9] proposed Style Transfer with Inference Time Optimisation (ST-ITO), which includes self-supervised method to construct mixing style encoder and non-differentiable way to find suitable parameters that have the minimum embedding distance to the reference audio. With their proposed encoder AFx-Rep, ST-ITO achieves state-of-the-art performance on this task. Koo et al. [10] proposed ITO-Master, which uses training-free audio features from [6] as the style encoder. Our work extends ITO by addressing the pitfall of solely minimising the This research is supported jointly by UKRI (grant number EP/S022694/1) and QMUL. This research utilised Queen Marys Apocrita HPC facility, supported by QMUL Research-IT. doi: 10.5281/zenodo.438045 Fig. 1: Overview of the proposed calibration for inference-time vocal effects style transfer. embedding distance, which neglects the prior information of the effect parameters. 1.2. Problem statement Given reference track RN , raw tracks RCN , and content-invariant style encoder : RN SD1 which maps the audio into D-dimensional unit vector, we would like to process the tracks in way that the resulting has the same effects applied as y. Here, raw track means no or very few effects applied. natural criterion is to make the two style embeddings = g(y) and = g(y) as close as possible [7], [9]. We further assume that the reference is the result of applying an effects model : RCN RM RN with parameters θ RM , to its corresponding raw tracks RCN , which we do not have access to. Assuming the effects configuration encompasses what mixing style is. Transferring the reference style becomes problem of estimating the parameters θ given and x, i.e., the posterior distribution p(θ z, x). Once the parameters are estimated, we can apply the effects model to to obtain the desired output = (x, θ). In ST-ITO, the style distance is the cosine distance D(z, z) = 1 zz. The authors find the optimal parameters by θ = arg minθ D(z, z). We could view this as maximising the likelihood p(z θ, x). Nevertheless, according to Bayes rule, this is only proportional to the posterior p(θ z, x) p(z θ, x)p(θ x) when the prior p(θ x) is uniform over the parameter space. Or, when the likelihood function is Dirac delta function, the prior does not affect the posterior. This could happen if is perfect encoder, i.e., injective to style (a.k.a θ) and fully represents the entire parameter space, which is, unfortunately, not the case in practice. Thus, reasonable prior is needed to guide the search for the parameters θ. Such prior tells us that some configurations are more likely to happen than others, thus reducing the risk of arriving at an unrealistic solution. This prior can come from heuristics, such as the knowledge of the audio engineer, or from data-driven methods, which is the focus of this work. 1.3. Related works The concept of utilising external guidance during inference time is not new and has been extensively studied in the field of generative modelling [11], especially using diffusion models [12][14]. Diffusion models learn the gradients of the log-probability density θ log p(θ) that can be used to draw samples from the distribution p(θ). (We omit the condition variable representing diffusion time steps for demonstration purposes.) Here, θ could be any targeted variable. Using the equivalence of θ log p(θ z) = θ log p(z θ) + θ log p(θ), one can design likelihood function p(z θ) on given condition thereby turning the unconditional generative model into conditional one [13], [15]. This is known as posterior sampling. This idea has been successfully applied in solving inverse problems such as image and audio restoration [16][19], and source separation [20], [21]. Recently, similar approach has been used to control music generation models known as Diffusion Inference-Time T-Optimisation (DITTO) [22], [23]. The difference is that DITTO treats the whole sampling process as function and backpropagates the gradients through it to optimise the initial latent. This, however, deviates from posterior sampling and is more similar to the ITO methods we mentioned in Section 1.1, in which the sampling process is like and the initial latent is like θ. (This can be corrected by regularising the initial latent to its initial distribution p(θ) = (0, I) during the optimisation, similar to our approach proposed in Section 2.1.) Posterior sampling has strong prior that guarantees correctness, which the previous ITO-based style transfer works do not have. The reason is apparent: large-scale effect parameter datasets are needed to capture their distribution, which are rare and expensive to collect. Our work does not address this issue but instead focuses on using limited amount of data to showcase that missing the prior information can lead to sub-optimal results and draw more attention to the importance of audio effects prior. Our contributions are as follows: 1) converting effects style transfer into maximum-a-posteriori estimation problem, 2) improvements in vocal effects style transfer, outperforming baselines objectively and subjectively using Gaussian prior and differentiable effects model (DiffVox), 3) exploration of multiple encoders for style representation, and 4) scalability to limited paired data regimes. We also open-source our implementation and experiments on GitHub 1. 2. METHODOLOGY 2.1. The maximum-a-posteriori estimation To correct the biased posterior of ST-ITO, we propose calibrating it by introducing non-uniform prior p(θ x) over the parameter space. We further assume that the prior is independent of the raw vocal x, so we can write it as p(θ x) = p(θ) to simplify the problem. The optimal parameters θ thus equals the following maximum-a-posteriori (MAP) estimation: θ = arg max log p(z θ, x) + α log p(θ) (1) θ where α is hyperparameter that controls the prior strength. Figure 1 gives an intuitive illustration of (1) in 2-dimensional parameter space. To define the likelihood p(z θ, x), we first convert the distance measure : RD RD [0, 2] into probability density function as: p(z z) = exp 1 2πσ (cid:18) arccos (1 D(z, z))2 2σ2 (cid:19) (2) where σ is hyperparameter that controls the variance of the distribution. We measure the embedding distance using arccos since embeddings lie on (D-1)-sphere and the distance on the surface equals the angle between the two vectors. We parametrise the likelihood to be p(z θ, x) = p(z = (g )(x, θ)). 2.2. The effects model and Gaussian prior Due to the lack of multi-track effects datasets, we demonstrate our approach on the simpler case of processing single track. In this case, we use the DiffVox dataset [24], which contains 365 vocal presets derived from professionally mixed songs, providing highquality effect presets to build the prior. The differentiable effects model : R1N RM R2N converts mono vocal track into stereo track. consists of six-band parametric equaliser (PEQ), dynamic range controller (DRC), ping-pong delay, feedback delay network (FDN) reverb, and panner. The routing of the effects is shown in Fig. 2. The total number of parameters is = 130."
        },
        {
            "title": "Reverb",
            "content": "Fig. 2: The effects chain in DiffVox. The vocal presets Θ R365130 were retrieved by fitting to paired vocal tracks picked from private multi-track dataset containing mainstream Western music sampled at 44.1 kHz. Each track is split into overlapping long chunks and silent chunks are dropped. The effects are trained on each track for 2k steps using the Adam optimiser [25] with 0.01 learning rate and maximum batch size of 35. The training loss is combination of multi-scale STFT (MSS) losses [26] and mean absolute error (MAE) loss on microdynamics [27] in two different scales (MLDR) on the left, right, mid, and side channels. The MSS is the sum of spectral convergence and the MAE of log-magnitude spectrograms computed using auraloss2. The spectrograms for MSS are computed with FFT sizes of {128, 512, 2048} and 75 % overlap. We first tried fitting diffusion model to the presets, but due to data scarcity, the model collapsed and failed to generate meaningful presets. Thus, we propose simple Gaussian prior p(θ) = ( θ, Σθ) (cid:80)365 instead, using the sample mean θ = 1 i=1 Θi and the sample 365 i=1(Θi θ)(Θi θ) as parameters of covariance Σθ = 1 364 the prior. Although the statistical tests in [24] show that the presets are not multivariate normal, we argue that this weak model is still beneficial since it captures the non-uniformity to some extent. (cid:80)365 2.3. Working with multiple vocals and channels The effects model of DiffVox produces stereo vocals due to the spatial effects. Let us denote its two-channel output as [yl, yr] = (x, θ). Following ST-ITO [9], we compute four style embeddings zm, zs, zm, and zs from the mid and side channels of the vocals as [ym, ys] = [yl + yr, yl yr] and [ym, ys] = [yl + yr, yl yr]. We assume each channels likelihood is independent of each other, thus p(zm, zs θ, x) = p(zm = zm)p(zs = zs). In addition, we may 1Will be pushed to github.com/SonyResearch/diffvox. 2github.com/csteinmetz1/auraloss have multiple reference embeddings = {(zm1 , zs1 ), (zm2 , zs2 ), . . .} and multiple raw vocals = {x1, x2, . . .}. In this case, we replace log p(z θ, x) in (1) with its expectation, which is Z, [log p(zm, zs θ, x)] = 1 Z (cid:88) (cid:88) (zm,zs) x log p(zm, zs θ, x). (3) We set σ2 the likelihood on the mid channel: to be the average of the squared term in the exponent of σ2 = 1 Zm Zm (cid:88) (cid:88) Zm Zm arccos (1 (z, z))2 (4) [28]. Zm and Zm so (3) is always maximised with respected to σ2 are the style embeddings for the mid channel. The same applies to the side channel with embeddings Zs and Zs to calculate σ2 . 3. EXPERIMENTAL DETAILS We evaluate the proposed method on 70 vocal tracks of the MedleyDB dataset [29], [30], with their presets derived in the same way as the DiffVox dataset mentioned in Section 2.2, denoted as oracle. Oracle defines the upper bound of modelling ability. We test three different encoders g: the AFx-Rep encoder from ST-ITO [9] and two signal processing-based encoders. The AFx-Rep encoder is based on the PANNs architecture [31] and was trained to identify the applied effects and their configurations from randomly processed audio. The second encoder is 25 Mel-frequency cepstral coefficients (MFCCs) computed from 128 Mel bands. The last encoder is based on standard MIR features, which consist of root-mean-square (RMS) energy, crest factor, dynamic spread, spectral centroid, spectral flatness, and spectral bandwidth [32], [33]. The MFCCs and MIR features are computed using 2048 frame length with 50 % overlap, and the Hanning window is used for the spectrograms. Finally, the features are reduced to one vector by computing the mean, standard deviation, skewness, and kurtosis along the time axis. We apply normalisation (cid:55) x/x2 to all the embeddings. To objectively evaluate performance, we design the following experiment similar to the self-supervised training process in [3], [6], [10]. We split the vocal tracks into non-overlapping elevensecond segments and select active segments in which at least 50 % of the processed audio is over 60 dB. We then randomly split the segments equally into two sets, denoted as and B. The embeddings of As processed audio and Bs raw vocal are used as and x, respectively. After estimating the parameter θ, we directly compare the resulting with the processed audio of B. We drop five tracks due to the insufficient number of segments. We minimise the negative log-posterior (1) using Adam [25] for 1k steps with learning rate of 0.01 to reach sufficient convergence, similar to the variational posterior sampling [34]. These parameters were decided based on early trial runs. The parameters are initialised to the mean of the presets θ. We compare the proposed method with the following baselines: Mean: The presets mean θ, which is the mode of the prior θ = arg maxθ p(θ). Regression: neural network that performs blind estimation of the parameters (equals the posterior p(θ y)), trained with mean squared error (MSE) loss on the parameters. Nearest neighbour (NN): It finds the nearest preset from the training set as the parameters. We examine four different spaces to measure the distance: the parameter space (θ) and the embedding space of the three encoders. We pick the preset nearest to the oracle parameters when using the parameter space. To train the regression baseline, we split vocals into non-overlapping ten-second segments, and we select active segments in which at least 80 % of the audio is over 60 dB. We pick 17 out of 365 tracks for validation and the rest for training. The input features include RMS, crest factor, dynamic spread, and Mel-spectrogram with 80 Mel bands. We compute the features with hop size of 256 on the left, right, mid, and side channels and concatenate them along the channel dimension. The model has five convolutional layers with channel sizes [512, 512, 768, 1024, 1024] and kernel size of 5. Average pooling with stride of 3, batch normalisation, and ReLU activation are sequentially applied between the layers. Finally, we use global max-pooling followed by fully connected layer that predicts the parameters. The model is trained using AdamW [35] with learning rate of 0.001 and batch size of 64. Due to the small amount of data (roughly 4177 segments), the model overfits soon after 6.5k steps. We initially tried processing random raw vocals with random parameters to augment the training data, similar to [36]. However, the performance on the validation set is worse, likely due to the intrinsic difference between the DiffVox effects and the actual processing of the data. Thus, we only use the original processed vocals for training. When multiple references are given during inference, we average the predictions. 4. RESULTS Table 1: Median scores of the proposed methods and baselines. The best scores in their respective categories are highlighted in bold. Method Oracle Mean Regression NN-θ NN-AFx-Rep NN-MFCC NN-MIR Encoder α AFx-Rep MFCC MIR 0.0 0.01 0.1 1.0 0.0 0.01 0.1 1.0 0.0 0.01 0.1 1.0 MSS l/r m/s MLDR l/r m/s PMSE 0.775 1.012 0. 0.383 0.0 +0.354 +0.281 +0.381 +0.321 +0.274 +0.424 +0.435 +0.221 +0.211 +0.318 +0.761 +0.507 +0.333 +0. +0.782 +0.598 +0.490 +0.363 +0.836 +0.574 +0.675 +0.672 +0.464 +0.803 +0.570 +0.606 +0.513 +0.795 +0.897 +0.531 +0.469 +0.563 +1.105 +1.505 +0.807 +0. +0.503 +0.480 +0.518 +0.320 +0.424 +0.561 +0.343 +0.249 +0.321 +0.427 +1.047 +0.720 +0.514 +0.459 +0.873 +0.856 +0.778 +0.508 +0.692 +0. +0.629 +0.504 +0.559 +0.706 +0.424 +0.402 +0.445 +0.629 +0.977 +0.765 +0.621 +0.547 +0.797 +0.854 +0.778 +0.695 +5.310 +5.002 +4.145 +9.463 +8.374 +10. +7.756 +5.924 +5.168 +5.339 +9.255 +6.706 +5.661 +5.250 +7.103 +5.622 +5.359 +5.319 4.1. Objective evaluation The evaluation metrics are the same MSS and MLDR losses used to fit the DiffVox dataset and the MSE loss on the parameters (PMSE). We use oracle as the target to calculate PMSE since there is no better ground-truth. The median scores across 65 tracks are shown in Table 1. We report the median due to extreme outliers when evaluating on few tracks mid and side channels. Firstly, looking at the nearest neighbour methods (denoted as NNfollowed by the embedding name), we can see that NN-θ performs the best in terms of PMSE, but not for the other metrics. This implies that having close parameters does not mean their audio features are closer. NN-MFCC is better for MSS, which is expected since the MFCCs encode the spectral information mainly, while the NN-AFx-Rep is better for MLDR, e.g., the dynamics. Both encoders PMSE are way larger than NN-θ, which could be the misalignment of the parameter and audio feature spaces or the quasi/approximate symmetry in the effects [37] (i.e., different configurations but similar results). We test the proposed method with α {0.0, 0.01, 0.1, 1.0}. The proposed prior improves the original ST-ITO (α = 0.0) in all metrics, especially in PMSE, where the best ones are comparable to the mean baseline. AFx-Rep with α = 0.1 performs the best and surpasses its NN counterpart in all metrics. MFCC with α = 1.0 performs comparably to its NN counterpart and has lower PMSE. Both methods perform better than the regression model in MSS and MLDR, showing that the calibration is beneficial when there is limited paired data and the effectiveness of the Gaussian prior. 4.2. Subjective listening test Based on the objective evaluation, we select the oracle and the four best-performing methods from each category: regression, NN-MFCC, AFx-Rep (α = 0.1), and MFCC (α = 1.0), to carry out MUSHRAlike listening test [38]. We select one track and its loudest part for 7 to 11 seconds from each singer in MedleyDB. We drop tracks that do not have long enough singing phrases, have minimal effects applied, such as classical singing, or have very different singing style from the rest. We then randomly pair two tracks together as one trial. In each trial, the processed audio of one track is used as and the raw vocal of the other track is used as x. The reference and raw vocals do not have the same singer/recording environment and the available samples are also limited ( = = 1), thus making it more challenging than the experiment in Section 4.1. If the pitch ranges in pair are too different, we find another part of the same track with similar pitches. We include the raw vocal as hidden low anchor. Participants are asked to compare of the five methods and the anchor with the reference and rate how close the applied effects are to the reference on scale from 0 to 100. We instruct participants to identify the low anchor and rate it below 20. We construct the evaluation website using webMUSHRA [39]. We divided 19 trials into two sets equally and randomly assigned them to the participants to reduce fatigue. The sets are divided so that each track appears only once in each set, and if it is used as reference in one set, it can only be used as the raw vocal in the other set. The orders of the trials and the methods are randomised. We posted the link through research communities and social media and have received 26 responses as of this writing. We examined the histogram of accuracies of picking the low anchor to exclude outliers. It shows bimodal distribution with modes at both ends. Therefore, we use 40 % as the threshold, which perfectly filter out the low-performance group with eight responses. Then, we check the ratings variation across trials for each participant and exclude two more participants with variations apparently larger than the rest. Finally, we average the ratings across trials for each participant. The distributions of average ratings are shown in Fig. 3. The oracle is not rated close to 100, which is expected due to the difference between the fitted effects, the references actual effects, and the singers timbre. The regression model is rated the lowest and AFxRep is rated the highest on average, though we cannot conclude significant difference between AFx-Rep and NN-MFCC using the Wilcoxon signed-rank test [40] (p = .495). In summary, the subjective evaluation confirms again that the proposed method with the AFx-Rep encoder can better transfer the reference style than the regression model when there is limited data. Picking presets from DiffVox using the MFCC distance is also good alternative. Fig. 3: Violin plot of the average ratings sorted based on the mean. The white dot is the median, and the black lines are the interquartile range. 5. DISCUSSIONS Although the inferior results of the regression p(θ y) are best explained by insufficient data, we also identify few possible reasons. Our simple regressor is trained on parameter loss only and will likely be beneficial by adding audio loss like MSS during training [36], [41] to match the content better. In addition, averaging the predictions does not guarantee lower MSS and MLDR than the individual predictions; thus, its evaluation in Table 1 could be sub-optimal. Our proposed method could be improved using the conditional prior p(θ x) instead of the unconditional one. However, the dependency between the parameters and the raw vocal is apparently less than the one between the parameters and the processed audio. Alternatively, we could even use p(θ y) as stronger prior, which might substantially improve the performance. Our work is limited by the assumption of fixed and ordered dimensionality to model the prior, which implies that the effects and routings are fixed. The actual prior must have variable dimensionality [42] and lots of equivariance in the parameters [37], [43], e.g., the order of applying the effects does not matter, which is non-trivial to model. In addition, our method requires differentiable effects, but the general idea should apply to non-differentiable ones [9], as long as the prior information is injected during the optimisation. 6. CONCLUSION In this paper, we proposed calibrated ST-ITO by introducing non-uniform Gaussian prior over the parameter space. This calibration addresses the limitations of the original ST-ITO, which neglects prior information and leads to sub-optimal results. We demonstrated the effectiveness of the proposed method on vocal effects style transfer, showing significant improvements in objective evaluations compared to baseline methods, including blind effects estimator, nearest-neighbour approaches, and the original ST-ITO. Our subjective evaluation further confirmed the superiority of the proposed method with the AFx-Rep encoder. The results highlight the importance of incorporating prior knowledge into inference-time optimisation for better performance. Future work will extend the proposed method to handle more complex effects configurations with variable dimensionality and equivariance in the parameter space. Additionally, we aim to explore stronger priors, such as conditional priors dependent on raw vocals or reference tracks, and investigate the methods application to nondifferentiable effects."
        },
        {
            "title": "REFERENCES",
            "content": "[25] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, in ICLR, 2015. [26] X. Wang, S. Takaki, and J. Yamagishi, Neural Source-filter-based Waveform Model for Statistical Parametric Speech Synthesis, in Proc. ICASSP. IEEE, 2019, pp. 59165920. [27] S. Nercessian, R. McClellan, and A. Lukin, direct microdynamics adjusting processor with matching paradigm and differentiable implementation, in Proc. DAFx, 2022, pp. 248255. [28] A. Asperti and M. Trentin, Balancing reconstruction error and kullbackleibler divergence in variational autoencoders, IEEE Access, vol. 8, pp. 199 440199 448, 2020. [29] R. M. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam, and J. P. Bello, MedleyDB: multitrack dataset for annotation-intensive mir research. in Proc. ISMIR, 2014, pp. 155160. [30] R. M. Bittner, J. Wilkins, H. Yip, and J. P. Bello, MedleyDB 2.0: New data and system for sustainable data collection, ISMIR Late Breaking and Demo Papers, 2016. [31] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, Panns: Large-scale pretrained audio neural networks for audio pattern recognition, IEEE Trans. Audio, Speech, Lang. Process., vol. 28, pp. 28802894, 2020. [32] B. D. Man, B. Leonard, R. L. King, and J. D. Reiss, An analysis and evaluation of audio features for multitrack music mixtures. in Proc. ISMIR. ISMIR, Oct. 2014, pp. 137142. [33] Z. Ma, B. De Man, P. D. Pestana, D. A. Black, and J. D. Reiss, Intelligent multitrack dynamic range compression, J. Audio Eng. Soc., vol. 63, no. 6, pp. 412426, 2015. [34] M. Mardani, J. Song, J. Kautz, and A. Vahdat, variational perspective on solving inverse problems with diffusion models, in Proc. ICLR, 2024. [35] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in Proc. ICLR, 2019. [36] C. Peladeau and G. Peeters, Blind estimation of audio effects using an auto-encoder approach and differentiable digital signal processing, in Proc. ICASSP, 2024, pp. 856860. [37] Anonymous, Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching, in Under review, 2025. [38] B. Series, Method for the subjective assessment of intermediate quality level of audio systems, International Telecommunication Union Radiocommunication Assembly, vol. 2, 2014. [39] M. Schoeffler, E. Schubert, and J. D. Reiss, webMUSHRA comprehensive framework for web-based listening tests, Journal of Open Research Software, vol. 6, no. 1, p. 8, 2018. [40] F. Wilcoxon, Individual comparisons by ranking methods, Biometrics Bulletin, vol. 1, no. 6, pp. 8083, 1945. [41] H. Han, V. Lostanlen, and M. Lagrange, Learning to solve inverse problems for perceptual sound matching, IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 32, p. 26052615, Apr. 2024. [42] S. Lee, J. Park, S. Paik, and K. Lee, Blind estimation of audio processing graph, in Proc. ICASSP. IEEE, 2023, pp. 15. [43] B. Hayes, C. Saitis, and G. Fazekas, The responsibility problem in neural networks with unordered targets, in Tiny Papers @ ICLR, K. Maughan, R. Liu, and T. F. Burns, Eds. OpenReview.net, 2023. [1] S. S. Vanka, M. Safi, J.-B. Rolland, and G. Fazekas, The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers, J. Audio Eng. Soc., vol. 72, pp. 515, 2024. [2] M. A. Martinez Ramirez, W. Liao, C. Nagashima, G. Fabbro, S. Uhlich, and Y. Mitsufuji, Automatic music mixing with deep learning and out-of-domain data, in Proc. ISMIR, 2022. [3] C. J. Steinmetz, N. J. Bryan, and J. D. Reiss, Style transfer of audio effects with differentiable signal processing, JAES, vol. 70, no. 9, pp. 708721, 2022. [4] J. Koo, M. A. Martınez-Ramırez, W.-H. Liao, S. Uhlich, K. Lee, and Y. Mitsufuji, Music mixing style transfer: contrastive learning approach to disentangle audio effects, in Proc. ICASSP, 2023, pp. 15. [5] C. J. Steinmetz, J. Pons, S. Pascual, and J. Serr`a, Automatic multitrack mixing with differentiable mixing console of neural audio effects, in Proc. ICASSP, 2021, pp. 7175. [6] S. S. Vanka, C. J. Steinmetz, J.-B. Rolland, J. D. Reiss, and G. Fazekas, ISMIR, Diff-mst: Differentiable mixing style transfer, in Proc. ISMIR. Nov. 2024, pp. 563570. [7] A. Chu, P. OReilly, J. Barnett, and B. Pardo, Text2fx: Harnessing clap IEEE, embeddings for text-guided audio effects, in Proc. ICASSP. 2025, pp. 15. [8] B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, Clap learning audio concepts from natural language supervision, in ICASSP 20232023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 15. [9] C. J. Steinmetz, S. Singh, M. Comunit`a, I. Ibnyahya, S. Yuan, E. Benetos, and J. D. Reiss, ST-ITO: Controlling audio effects for style transfer with inference-time optimization, in ISMIR, Nov. 2024, pp. 661668. [10] J. Koo, M. A. Martınez-Ramırez, W.-H. Liao, G. Fabbro, M. Mancusi, and Y. Mitsufuji, ITO-Master: Inference-time optimization for music mastering style transfer, iSMIR Late Breaking Demo. [11] E. Manilow, P. OReilly, P. Seetharaman, and B. Pardo, Source separation by steering pretrained music models, in Proc. ICASSP, 2022, pp. 126 130. [12] J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli, Deep unsupervised learning using nonequilibrium thermodynamics, in Proc. ICML, ser. ICML15. JMLR.org, 2015, p. 22562265. [13] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, Score-based generative modeling through stochastic differential equations, in Proc. ICLR, 2021. [14] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Proc. NeurIPS, vol. 33, pp. 68406851, 2020. [15] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye, Diffusion posterior sampling for general noisy inverse problems, in Proc. ICLR, 2023. [16] B. Kawar, M. Elad, S. Ermon, and J. Song, Denoising diffusion restoration models, Proc. NeurIPS, vol. 35, pp. 23 59323 606, 2022. [17] C.-Y. Yu, S.-L. Yeh, G. Fazekas, and H. Tang, Conditioning and sampling in variational diffusion models for speech super-resolution, in Proc. ICASSP. IEEE, 2023, pp. 15. [18] C. Hernandez-Olivan, K. Saito, N. Murata, C.-H. Lai, M. A. MartınezRamırez, W.-H. Liao, and Y. Mitsufuji, VRDMG: Vocal restoration via diffusion posterior sampling with multiple guidance, in Proc. ICASSP, 2024, pp. 596600. [19] J.-M. Lemercier, J. Richter, S. Welker, E. Moliner, V. Valimaki, and T. Gerkmann, Diffusion models for audio restoration: review, IEEE Signal Process. Mag., vol. 41, no. 6, pp. 7284, 2024. [20] G. Mariani, I. Tallini, E. Postolache, M. Mancusi, L. Cosmo, and E. Rodol`a, Multi-source diffusion models for simultaneous music generation and separation, in Proc. ICLR, 2024. [21] M. Hirano, K. Shimada, Y. Koyama, S. Takahashi, and Y. Mitsufuji, Diffusion-based Signal Refiner for Speech Separation, May 2023, arXiv:2305.05857 [cs, eess]. [22] Z. Novack, J. McAuley, T. Berg-Kirkpatrick, and N. J. Bryan, DITTO: Diffusion inference-time t-optimization for music generation, in Proc. ICML, 2024. [23] , DITTO-2: Distilled diffusion inference-time t-optimization for music generation, in Proc. ISMIR, Nov. 2024, pp. 874881. [24] C.-Y. Yu, M. A. Martınez-Ramırez, J. Koo, B. Hayes, W.-H. Liao, G. Fazekas, and Y. Mitsufuji, DiffVox: differentiable model for capturing and analysing professional effects distributions, 2025. [Online]. Available: https://arxiv.org/abs/2504."
        }
    ],
    "affiliations": [
        "Centre for Digital Music, Queen Mary University of London, London, UK",
        "Sony AI, Tokyo, Japan",
        "Sony Group Corporation, Tokyo, Japan"
    ]
}