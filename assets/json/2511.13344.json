{
    "paper_title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
    "authors": [
        "Ori Meiraz",
        "Sharon Shalev",
        "Avishai Weizman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model."
        },
        {
            "title": "Start",
            "content": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection Ori Meiraz* Faculty of Data and Decision Sciences Technion, Israel Institute of Technology Haifa, Israel Email: ori.meiraz@campus.technion.ac.il ORCID: 0009-0001-1469-5582 Sharon Shalev Independent Researcher Email: shalev1310@gmail.com ORCID: 0009-0003-5054-230X Avishai Weizman* School of Electrical and Computer Engineering Ben-Gurion University of the Negev Beersheba, Israel Email: wavishay@post.bgu.ac.il ORCID: 0009-0004-1182-8601 AbstractThis paper presents novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to single YOLOv9-T model. Index TermsDeep Learning, Object Detection, Mixture-ofExperts, YOLOv9. I. INTRODUCTION Object detection is one of the most fundamental and challenging problems in computer vision. It aims to identify and localize instances of objects within images or video frames and has received great attention in recent years [1][3]. The introduction of convolutional neural networks (CNNs) revolutionized the field, giving rise to detectors such as RCNN [4], single-shot detector (SSD) [5], and YOLO [6][8], which achieved remarkable improvements in both accuracy and speed. Among them, YOLOv9 has shown superior performance compared to other models in the YOLO series [7]. In parallel with these developments, the Mixture-of-Experts (MoE) [9] paradigm introduces multiple specialized experts and routing network that dynamically assigns responsibility for each input. The outputs are combined either softly, as weighted mixtures [10], or through hard expert selection [11]. In [12], MoE module is integrated into the YOLOv5 [13] backbone to process intermediate feature maps. The MoE framework has also proven effective in tasks such as nexttoken prediction [11] and deepfake detection [14]. In many of these applications, the outputs correspond to class probabilities or logits, which can be combined straightforwardly as: (cid:88) = αeze (1) e=1 where ze denotes the logits produced by expert e, and αe represents the weight assigned by the router, is the total number of experts, and is the fused output logits. Building on these advances, introduces an MoE architecture based on YOLOv9-T (the tiny version of this paper *These authors contributed equally to this work. YOLOv9) [6], designed to improve detection performance and robustness. II. PROPOSED MOE-BASED YOLOV9 ARCHITECTURE In the YOLOv9 framework, each input image is processed through = 3 feature maps Fi RBCiHiWi corresponding to spatial resolutions {8, 16, 32}, where denotes the batch size, Ci the number of channels, and (Hi, Wi) the spatial dimensions of the i-th level. Each map is responsible for detecting objects at corresponding scale, and instead of directly regressing the bounding box coordinates, the model predicts the distances from each cell center to the four sides of the bounding box (left, top, bottom, right). These distances are discretized into bins (typically 16) and treated as classification targets, with coordinates obtained by the expectation over predicted probabilities. This discrete formulation enables consistent probabilistic fusion across experts (YOLOv9 models). In this framework, each feature level produces prediction map containing both bounding box and class logits. Specifthe map outputs ybox R4N for bounding box ically, regression and ycls Rnc for object classification, where denotes the number of discrete distance classes used in bounding box regression, and nc denotes the number of object classes. Our design follows the Enhanced MoE method proposed in [14], where the routing decision is based on features extracted by individual experts rather than on the raw input image. Building on this multi-scale structure of YOLOv9, the resulting feature representations at each level serve as inputs to the MoE routing mechanism. Each expert processes its corresponding feature map Fi,e RBhHiWi and extracts intermediate features from layer preceding the output head, where denotes the hidden dimension. The router Ri at level operates at its corresponding resolution and receives as input both the outputs of all experts at that level and their concatenated representations, combined through reweighted Hadamard fusion. The Hadamard fusion captures cross-expert interactions and is defined as: 5 2 0 2 1 2 ] . [ 3 4 4 3 3 1 . 1 1 5 2 : r Fig. 1. Visualization of MoE within the YOLOv9 architecture, multiple experts process the input image to produce multi-scale feature maps and outputs (class and bounding box logits). Routers at different resolutions (88, 1616, 3232) generate adaptive routing weights that fuse expert outputs into final detections. The loss is computed between model outputs before Non-maximum suppression (NMS) [6], [15] and the ground truth, ensuring end-to-end differentiability. Ki = (Fi,1 Fi,2 Fi,E) Wi (2) where denotes element-wise multiplication, and Wi Rh11 is learnable weighting parameter. The combined input to the router is defined as: Mi = Concat (cid:2)Fi,1, Fi,2, . . . , Fi,E, Ki (cid:3) (3) COCO where Mi RB(E+1)hHiWi represents the fused feature map at level i, obtained by concatenating all inputs along the channel dimension. The resulting fused map Mi is processed by the router Ri, implemented as lightweight CNN with downsampling followed by fully connected layer, to produce αi RBE, representing the routing weights assigned to each expert. softmax function is applied along the expert dimension to produce normalized routing weights at each level, ensuring that (cid:80)E e=1 αi,e = 1. These weights adaptively modulate the outputs of the experts for both classification and bounding box prediction, as defined in Equation 1. To prevent the router from collapsing onto single expert, load balancing loss inspired by [11] is incorporated. The total loss is defined as: = Ldet + λlbLlb, (4) where Ldet denotes the standard YOLOv9 detection loss and Llb is the load balancing term weighted by λlb. The load balancing loss encourages uniform expert selection and is computed as: Llb ="
        },
        {
            "title": "1\nI",
            "content": "I (cid:88) (cid:88) i=1 e=1 fi,e Pi,e, (5) where fi,e denotes the fraction of samples routed to expert at resolution i, and Pi,e represents its corresponding mean routing probability. III. EXPERIMENTS AND RESULTS We conducted experiments on the COCO [16] and VisDrone [17] datasets, which represent everyday scenes and aerial imagery, respectively, focusing on the Person, Vehicle, Bicycle, and Motorcycle classes (the classes shared by both datasets). The models were trained for 50 epochs using the default hyperparameters of the YOLOv9 framework [6], and our TABLE PERFORMANCE COMPARISON BETWEEN OUR PROPOSED MODEL AND YOLOV9-T VARIANTS ON THE COCO AND VISDRONE (VIS.) DATASETS, EVALUATED ON THE CLASSES Person, Vehicle, Bicycle, AND Motorcycle Test Set Vis. Model YOLOv9-T YOLOv9-T Train Set COCO COCO + Vis. COCO + Vis. MoE-T (Ours) Vis. COCO + Vis. COCO + Vis. MoE-T (Ours) YOLOv9-T YOLOv9-T mAP@0.5:0.95 34.5 34.1 37.5 18.3 15.5 20.0 AR 46.7 49.2 50.0 34.7 30.3 36. architecture employed two experts based on YOLOv9-T, with λlb = 0.5. As in [14], the experts were initialized from weights pretrained on their datasets (COCO or VisDrone), enabling dataset-specific specialization, while the routing modules were trained from scratch. The best model was selected according to the mean Average Precision (mAP) metric. The evaluation was conducted using mAP and Average Recall (AR) metrics. As shown in Table I, our model achieved better performance compared to the other YOLOv9-T models on both datasets. The improvements appear both when each dataset is trained separately and when using single model trained on the combined sets, showing consistent gains in both mAP and AR. These results demonstrate the effectiveness of incorporating the MoE mechanism in the feature map space by allowing experts to specialize in different visual features so that the router dynamically selects the most relevant expert for each image region. IV. CONCLUSIONS AND FUTURE WORK This paper presents hybrid object detection framework that combines the MoE paradigm within the YOLOv9 architecture. By introducing adaptive routing between multiple YOLOv9-T experts, the proposed approach enables dynamic specialization in feature map space and improves detection performance and robustness compared to single model. Experimental results on the COCO and VisDrone datasets demonstrate the potential of this design. For future work, we plan to extend the study to additional YOLO variants of different sizes (e.g., YOLOv9L), explore more efficient routing mechanisms, and adapt the framework to temporal data for video object detection and multi-modal inputs."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Z. Zou, K. Chen, Z. Shi, Y. Guo, and J. Ye, Object detection in 20 years: survey, Proceedings of the IEEE, vol. 111, no. 3, pp. 257276, 2023. [2] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, Object detection with deep learning: review, IEEE transactions on neural networks and learning systems, vol. 30, no. 11, pp. 32123232, 2019. [3] X. Zou, review of object detection techniques, in 2019 International IEEE, conference on smart grid and electrical automation (ICSGEA). 2019, pp. 251254. [4] R. Girshick, J. Donahue, T. Darrell, and J. Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, in 2014 IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 580587. [5] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, Ssd: Single shot multibox detector, in European conference on computer vision. Springer, 2016, pp. 2137. [6] C.-Y. Wang, I.-H. Yeh, and H.-Y. Mark Liao, Yolov9: Learning what you want to learn using programmable gradient information, in European conference on computer vision. Springer, 2024, pp. 121. [7] T. Jiang and Y. Zhong, Odverse33: Is the new yolo version always better? multi domain benchmark from yolo v5 to v11, 2025. [Online]. Available: https://arxiv.org/abs/2502.14314 [8] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, You only look once: Unified, real-time object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779 788. [9] S. Mu and S. Lin, comprehensive survey of mixture-of-experts: [Online]. Available: theory, and applications, 2025. Algorithms, https://arxiv.org/abs/2503.07137 [10] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, in International Conference on Learning Representations, 2017. [11] W. Fedus, B. Zoph, and N. Shazeer, Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, Journal of Machine Learning Research, vol. 23, no. 120, pp. 139, 2022. [12] X. Chen, M. Li, G. Chen, Q. Li, S. Chen, Y. Chen, J. Li, L. Pan, and D. Liang, Yolo-moe: Multi-scenario cigarette defect detection with mixture of experts, in 2025 IEEE 5th International Conference on Software Engineering and Artificial Intelligence (SEAI). IEEE, 2025, pp. 5864. [13] S. K. Jaiswal and R. Agrawal, comprehensive review of yolov5: advances in real-time object detection, Int. J. Innov. Res. Comput. Sci. Technol, vol. 12, no. 3, pp. 7580, 2024. [14] V. Negroni, D. Salvi, A. I. Mezza, P. Bestagini, and S. Tubaro, Leveraging mixture of experts for improved speech deepfake detection, in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [15] J. H. Hosang, R. Benenson, and B. Schiele, Learning non-maximum suppression, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 64696477, 2017. [Online]. Available: https://api.semanticscholar.org/CorpusID: [16] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, Microsoft coco: Common objects in context, in European conference on computer vision. Springer, 2014, pp. 740755. [17] P. Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, and H. Ling, Detection and tracking meet drones challenge, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PP, pp. 11, 2020."
        }
    ],
    "affiliations": [
        "Ben-Gurion University of the Negev",
        "Technion, Israel Institute of Technology"
    ]
}