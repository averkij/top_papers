{
    "paper_title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting",
    "authors": [
        "Haoyu Zhao",
        "Cheng Zeng",
        "Linghao Zhuang",
        "Yaxi Zhao",
        "Shengke Xue",
        "Hao Wang",
        "Xingyue Zhao",
        "Zhongyu Li",
        "Kehan Li",
        "Siteng Huang",
        "Mingxiu Chen",
        "Xin Li",
        "Deli Zhao",
        "Hua Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The scalability of robotic learning is fundamentally bottlenecked by the significant cost and labor of real-world data collection. While simulated data offers a scalable alternative, it often fails to generalize to the real world due to significant gaps in visual appearance, physical properties, and object interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real framework that converts multi-view real-world images into scalable, high-fidelity, and physically interactive simulation environments for robotic manipulation. Our approach reconstructs scenes using a hybrid representation: 3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the environment, while mesh primitives for interactive objects ensure accurate physics simulation. Crucially, we pioneer the use of a Multi-modal Large Language Model (MLLM) to automate the creation of physically plausible, articulated assets. The MLLM analyzes visual data to infer not only physical properties (e.g., density, stiffness) but also complex kinematic structures (e.g., hinges, sliding rails) of objects. We demonstrate that policies trained entirely on data generated by RoboSimGS achieve successful zero-shot sim-to-real transfer across a diverse set of real-world manipulation tasks. Furthermore, data from RoboSimGS significantly enhances the performance and generalization capabilities of SOTA methods. Our results validate RoboSimGS as a powerful and scalable solution for bridging the sim-to-real gap."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 7 3 6 0 1 . 0 1 5 2 : r High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting Haoyu Zhao1,2, Cheng Zeng5, Linghao Zhuang1, Yaxi Zhao2,3, Shengke Xue2,3, Hao Wang6, Xingyue Zhao2, Zhongyu Li4, Kehan Li2,3, Siteng Huang2,3,7, Mingxiu Chen2,3, Xin Li2,3, Deli Zhao2,3, Hua Zou 1Wuhan University 2DAMO Academy, Alibaba Group 3Hupan Lab 4The Chinese University of Hong Kong 5Tsinghua University 6Huazhong University of Science and Technology 7Zhejiang University Equal contribution, Corresponding author The scalability of robotic learning is fundamentally bottlenecked by the significant cost and labor of real-world data collection. While simulated data offers scalable alternative, it often fails to generalize to the real world due to significant gaps in visual appearance, physical properties, and object interactions. To address this, we propose RoboSimGS, novel Real2Sim2Real framework that converts multi-view real-world images into scalable, highfidelity, and physically interactive simulation environments for robotic manipulation. Our approach reconstructs scenes using hybrid representation: 3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the environment, while mesh primitives for interactive objects ensure accurate physics simulation. Crucially, we pioneer the use of Multi-modal Large Language Model (MLLM) to automate the creation of physically plausible, articulated assets. The MLLM analyzes visual data to infer not only physical properties (e.g., density, stiffness) but also complex kinematic structures (e.g., hinges, sliding rails) of objects. We demonstrate that policies trained entirely on data generated by RoboSimGS achieve successful zero-shot Sim2Real transfer across diverse set of real-world manipulation tasks. Furthermore, data from RoboSimGS significantly enhances the performance and generalization capabilities of SOTA methods. Our results validate RoboSimGS as powerful and scalable solution for bridging the Sim2Real gap. Project page: https://robosimgs.github.io/ Date: October 14,"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "The pursuit of generalist robot policies capable of open-world manipulation (Black et al., 2024; Bjorck et al., 2025; Team et al., 2025; Kim et al., 2024) represents grand ambition in Embodied AI and robotics. These end-to-end models learn directly from raw sensory input and promise capabilities like language instruction following, task transfer, and in-context learning. However, this ambition is fundamentally constrained by the data acquisition bottleneck. While teleoperation systems (Li et al., 2025; Zhao et al., 2025a, 2023; Whitney et al., 2018) offer partial solution for gathering expert demonstrations, they fails to overcome the fundamental bottleneck of high human effort (Yu et al., 2025), thus limiting the scalability of models that promise language-guided control and generalization. In contrast, generating the synthetic data under the simulated environment offers the advantage of exponential scalability with computational resources, making it an appealing and renewable alternative for training robotic policies. Those Sim2Real approaches suffer from the large domain gaps in geometric representation, visual appearance, and physical material behaviors between simulated and real-world environments, making transfer the learned policy into the real world remains challenging (Huber et al., 2024; Tobin et al., 2017). Substantial efforts have been made to bridge this gap through techniques like domain randomization (Sadeghi and Levine, 2016; Tobin et al., 2017) and system identification (Chebotar et al., 2019; Tan et al., 2018). They enhance the agents robustness by simulating real-world noises and aligning the agent dynamic model with the real-world settings. While these methods have yielded certain advancements, their effectiveness is fundamentally limited by the simulators capabilities. Traditional simulators often fail to provide visually realistic observations, dynamic interactions, and diverse environmental variations, limiting the agents capacity to generalize effectively beyond the simulation environments. To directly tackle the source of the reality gap, the Real2Sim2Real (R2S2R) paradigm has recently emerged as transformative approach (Li et al., 2024; Yang et al., 2025; Han et al., 2025; Wu et al., 2024; Yu et al., 2025). The core insight is to perform Real2Sim reconstruction via radiance field methods, such as NeRF (Mildenhall et al., 2021) and 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023), and insert learned photorealistic representations into the simulator to drastically reduce the domain gap. Pioneering works like Robo-GS (Lou et al., 2024) have demonstrated the potential of this R2S2R pipeline, introducing hybrid representation to generate digital assets enabling high-fidelity simulation. However, despite achieving promising visual fidelity, these frameworks fall short in critical dimension: physical interaction. Their focus on photorealism results in world that is largely static and non-interactive, limiting their application to observational tasks and preventing the training of policies for complex, contact-rich manipulation. To tackle these issues, we propose RoboSimGS, novel R2S2R framework designed to bridge this crucial gap between photorealism and physical interactivity. At its core, RoboSimGS constructs hybrid scene representation, synergizing the photorealism of 3DGS for static backgrounds with the explicit geometry of mesh primitives for interactive objects, which allows for both high-fidelity visuals and corresponding physical collision. Crucially, to bring these objects to life, we pioneer the use of Multi-modal Large Language Model (MLLM) to directly infer an objects physical properties (e.g., density and stiffness) and kinematic structure (e.g., hinges, drawers) from multi-view images, transforming static scene into dynamic, interactive sandbox. We further enhance policy robustness through holistic scene augmentation that randomizes object properties, camera views, lighting, and trajectories. Extensive experiments demonstrate that policies trained exclusively on our simulated data achieve remarkable zero-shot transfer to the real world across variety of manipulation tasks. Furthermore, augmenting limited real-world data with our generated data significantly boosts the performance and generalization capabilities of state-of-the-art visuomotor policies, highlighting RoboSimGSs value as scalable and effective data generation solution. We summarize our contributions as follows: We introduce novel R2S2R framework that generates photorealistic and physically interactive simulation environments from real-world scenes, built upon hybrid (3DGS + mesh) representation. We pioneer the use of MLLM to automatically create physically-plausible and articulated assets. Extensive experiments demonstrate that our simulated data not only enables remarkable zero-shot Sim2Real transfer, but also significantly boosts the performance and generalization of existing SOTA models."
        },
        {
            "title": "2.1 Data Generation for Robotics",
            "content": "Addressing data scarcity is critical challenge in robotics. primary strategy is computational generation of new data from limited source demonstrations. These approaches can be broadly categorized into two main branches: trajectory-level generation and observation-level augmentation. One prominent line of work focuses on generating new action trajectories (Jiang et al., 2025; Xue et al., 2025; Mandlekar et al., 2023). These methods primarily operate on principle of relative-pose transformation. At their core, they exploit SE(3)-equivariance to replay transformed versions of source trajectory under new object poses. While effective for pose variation, their reliance on rigid, object-level formulation means they fundamentally struggle to generalize to novel object geometries, failure point for tasks requiring precise manipulation. Another line of work focuses on augmenting the visual appearance of observations to improve the robustness of visuomotor policies (Hansen and Wang, 2021; Laskin et al., 2020; Yu et al., 2023). These methods are achieved by employing generative models, such as for inpainting robot embodiments or other scene elements into 2D images. While 2D image-space augmentations are computationally efficient and easy to implement, their inherent lack of 3D spatial awareness is critical drawback. Operating directly on pixels without geometric context, these methods can generate unrealistic augmentationssuch as objects pasted at physically impossible locations or textures that ignore object boundarieswhich limits their effectiveness for tasks grounded in 3D interaction. We propose RoboSimGS, Real2Sim2Real framework that addresses data scarcity by creating photorealistic and physically interactive simulator. Using hybrid 3D representation (3DGS + mesh), RoboSimGS generates rich, physically-plausible synthetic data at scale. This approach overcomes the core limitations of augmenting 2D observations or trajectories, providing inherent 3D spatial awareness and robust generalization to novel object geometries and interactions. Figure 1 Pipeline of RoboSimGS. Starting from multi-view images, we first perform Scene Reconstruction to create hybrid representation with photorealistic 3DGS background and interactive mesh objects. key step involves using Multi-modal Large Language Model (MLLM) for automatic Physics Estimation and Articulation Inference. The scene is then aligned with the simulator with Sim2Real Environment Alignment. Finally, we apply Holistic Scene Augmentation to generate diverse simulated data. Policies trained on this data can be deployed directly to the real world."
        },
        {
            "title": "2.2 Sim2Real Transfer in Robotics",
            "content": "To improve the feasibility of deploying models in the real world, many researchers strive to bridge the gap between simulation and reality with Sim2Real methods (Mouret and Chatzilygeroudis, 2017). Previous Sim2Real methods can be broadly classified into two categories: domain randomization (Huber et al., 2024), domain adaptation (Bousmalis et al., 2018). Domain randomization methods are designed to expand the operational envelope of robot in simulator by introducing randomness, which involves varying task-related parameters in the simulation to cover broad range of real-world conditions (Exarchos et al., 2021; Huber et al., 2024; Andrychowicz et al., 2020). Domain adaptation approaches aim to extract unified feature space of simulated and real environments, facilitating the training and migration within the unified feature space (Bousmalis et al., 2017; Long et al., 2015). These methods introduce the disturbances into the simulated environment, in which the policy of robots is learned. It develops the capacity to operate effectively in the real world with noise and unpredictability (Wang et al., 2020). To tackle the reality gap at its source, the Real2Sim2Real (R2S2R) paradigm has emerged, aiming to automatically reconstruct high-fidelity digital twins from real-world scene to build simulated environments for policy learning (Geng et al., 2025; Jiang et al., 2025). However, these digital twins often fail to provide fully realistic observations, plausible dynamic interactions. For example, DexMimicGen (Jiang et al., 2025) uses fixed simulation assets. RoboVerse (Geng et al., 2025) supports only rigid objects. More recently, another line of work has adopted the R2S2R pipeline, utilizing 3D Gaussian Splatting (3DGS) to reconstruct photorealistic scenes (Qureshi et al., 2024; Li et al., 2024; Yang et al., 2025; Han et al., 2025; Lou et al., 2024). For instance, SplatSim (Qureshi et al., 2024) can create visually stunning, near-indistinguishable reconstructions of real scenes. However, despite their visual fidelity, these state-of-the-art R2S2R methods face new, critical limitation: Interactions are typically limited to pre-defined, rigid assets, precluding the simulation of complex dynamics, articulations (like drawers or hinges), or non-rigid objects. In essence, they produce photorealistic but static digital snapshots, not interactive robotic playgrounds. To bridge the physical interactivity gap, our method, RoboSimGS, creates interactive simulators through two key innovations. First, it uses hybrid representation (3DGS + mesh) to separate static backgrounds from interactable objects. Second, and more critically, it pioneers the use of Multi-modal Large Language Model (MLLM) to automatically infer material properties and kinematic constraints (e.g., hinges, sliding rails) directly from visual data. This automated generation of articulated assets enables rich, dynamic data creation at scale, facilitating zero-shot sim-to-real transfer for complex manipulation tasks."
        },
        {
            "title": "3 Methodlogy",
            "content": "To bridge the Sim2Real gap, we introduce RoboSimGS, method that generates realistic and physically interactive simulation environments from multi-view images. Our approach is two-stage pipeline designed to create high-fidelity digital twin of real-world scene. The first stage, Scene Reconstruction in Section. 3.1, builds hybrid representation with 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) capturing the photorealistic appearance of the static environment, while explicit mesh for interactive objects. In the second stage, Sim2Real Environment Alignment in Section. 3.2, these components are seamlessly imported into physics simulator, where the 3DGS model serves as the visual background and the meshes become dynamic, physics-enabled assets. We further enrich this environment via Holistic Scene Augmentation in Section. 3.3 to randomize objects, cameras, lighting, and trajectory to create diverse simulated data, as shown in Fig. 1. policy trained exclusively on this augmented synthetic data can achieve remarkable zero-shot transfer to real-world scenarios."
        },
        {
            "title": "3.1 Scene Reconstruction",
            "content": "While physics-based simulators offer high-fidelity physics, they struggle to render scenes with high-fidelity visual appearance, leaving large gap from real world. Recent advancements in 3D reconstruction (Kerbl et al., 2023) provide high-quality rendering while keeping real-time rendering, promising to bridge these gaps. However, 3DGS (Kerbl et al., 2023) are not directly compatible with physics engines, as robustly converting them into interactive mesh formats remains major research challenge (Guédon and Lepetit, 2024). Therefore, we adopt decoupled reconstruction approach. We use 3DGS for the static background to maximize visual fidelity and explicit meshes for interactive objects to ensure physical plausibility. The robot itself is excluded from this process, as it is readily available in the standard robot description file (URDF). Our process for each task-specific scene requires approximately 10 minutes of manual scanning to capture the necessary multi-view data. While physics-based simulators offer high-fidelity physics, they struggle with photorealistic rendering, leaving large visual gap from the real world. Recent advancements in 3D reconstruction like 3DGS (Kerbl et al., 2023) promise to bridge this gap with high-quality, real-time rendering. However, 3DGS is not directly compatible with physics engines, as robustly converting it into interactive mesh formats remains major research challenge (Guédon and Lepetit, 2024). To address this, our process begins with manual scan of each task-specific scene, requiring approximately 10 minutes to capture the necessary multi-view data. We then adopt decoupled reconstruction approach, using 3DGS for the static background to maximize visual fidelity and explicit meshes for interactive objects to ensure physical plausibility. The robot itself is excluded from this process, as it is readily available in standard robot description file (URDF)."
        },
        {
            "title": "3.1.1 Background Reconstruction",
            "content": "Background reconstruction contains most parts of the scenes and applies more photorealistic rendering using 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023). The scene is modeled by set of 3D Gaussians, each defined by position, opacity, color (via Spherical Harmonics), and covariance matrix Σ. To render novel view, these Gaussians are sorted by depth and projected onto the image plane. The final color for each pixel is then synthesized via alpha-blending according to the volume rendering formula. This approach provides high-quality, real-time rendering. To enable precise spatial alignment between our reconstructed scene and the scene in the simulator, we append learnable semantic feature vector fi Rd to each Gaussian (Qiu et al., 2024; Zhao et al., 2024b). The training of these semantic features is guided by language-driven supervision (Radford et al., 2021). We first define set of text prompts corresponding to all semantic classes of interest (e.g., \"a robot arm\", \"a red block\", \"a banana\"). These prompts are then encoded into target text embeddings using the frozen CLIP text encoder. The dimension of our features is set to match that of the CLIP embeddings. For given camera view, we render 2D feature map ˆF and supervise it using multi-class 2D ground-truth semantic mask. Specifically, we minimize contrastive loss between the rendered feature for each pixel and the corresponding target text embedding. Figure 2 Qualitative comparison between simulated data from RoboSimGS (left) and real-world data (right). The entire learning process is underpinned by the differentiable nature of 3DGS. The rendering process begins by culling outside the camera frustum (Kerbl et al., 2023). The remaining Gaussians are then projected onto the 2D image plane using the projection matrix of the camera. This projection induces the following transformation on the covariance matrix Σ: Σ = Σ WJ , (1) where is the Jacobian of the projection matrix W. We can then render both the color and the visual features with the splatting algorithm: { ˆF, ˆC} = (cid:88) iN {fi, ci} αi i1 (cid:89) (1 αj) , j=1 (2) where αi is the opacity of the Gaussian conditioned on Σ by their distance to the camera origin. and the indices are in the ascending order determined"
        },
        {
            "title": "3.1.2 Object Reconstruction",
            "content": "For foreground objects, we employ ARCode (Code, 2022), which can automatically segment the object, balancing usability and quality. However, the resulting mesh is inherently static, limiting its use in physical simulations. To support more complex manipulation tasks, we introduce pipeline to automatically endow these static meshes with physically-plausible, articulated structures. MLLM-driven Articulation Inference. Our process begins by employing Multi-modal Large Language Model (MLLM), such as GPT-4o (Achiam et al., 2023), to infer the objects kinematic structure. By providing the MLLM with multi-view renderings of the reconstructed mesh, it identifies the objects category and proposes its potential articulation, including the joint type (e.g., prismatic or revolute) and the semantic labels of the parts to be separated, such as \"drawer body\" and \"main cabinet\". To partition the mesh according to the MLLMs proposal, we directly employ the open-vocabulary segmentation method from AffordDex (Zhao et al., 2025b). This allows us to segment the object using the generated textual labels as direct prompts. Finally, with the mesh partitioned into static base and mobile part, the MLLM is again prompted to determine the precise joint parameters, such as its axis and motion limits. This information is used to automatically define URDF-compatible joint, completing the creation of fully interactive, articulated object for our simulation environment. MLLM-driven Physics Estimation. Inferring an objects physical properties from its visual appearance is longstanding challenge, as materials with similar aesthetics can exhibit vastly different physical behaviors. While humans can perform this task through contextual reasoning, recent advances in Multi-modal Large Language Models (MLLMs) have demonstrated analogous capabilities in complex reasoning (Driess et al., 2023; Zhao et al., 2024a). Inspired by this progress, we introduce physics expert agent, powered by GPT-4o (Achiam et al., 2023), to automate the estimation of material properties. Our agent processes set of four orthographic views of 3D asset to estimate its fundamental physical parameters: density (ρ in kg/m3), Youngs modulus (E in Pa), and Poissons ratio (ν, dimensionless). These parameters are critical for high-fidelity physical simulation, as they govern the objects mass distribution, stiffness, and deformation response under external forces. Figure 3 Task illustration. We design eight manipulation tasks for real-world evaluation: Stack Cubes, PickPlace, Deformable PickPlace, Upright Bottle, Move Bottle, Drawer Close, Box Close, Wiping, whose details are shown in Section. 4.1.4."
        },
        {
            "title": "3.2.1 World Coordinate Frame Alignment",
            "content": "This step aligns the coordinate system of the 3DGS reconstruction described in Section. 3.1.1 with that of the simulator, using the robots geometry as common anchor. For this process, the robots in both scenes are fixed at default joint configuration. The alignment thus reduces to finding the rigid transformation between the robots URDF base frame Rurdf and the 3DGS world frame Rgs. To compute this transformation matrix, we first generate point cloud of the robots geometry from both the URDF model and the 3DGS reconstruction. We then apply the Iterative Closest Point (ICP) (Besl and McKay, 1992) algorithm, which finds the optimal rigid transformation Tscene by solving the following minimization problem: Tscene = arg min RSO(3),tR3 (cid:88) (Rpgs + t) qurdf 2, (3) } and {qurdf where {pgs } are the point clouds from the 3DGS reconstruction and the URDF, respectively, and the transformation consists of rotation SO(3) and translation R3. By applying the resulting matrix Tscene, we map all elements from the 3DGS scene into the simulators coordinate system to get aligned environment."
        },
        {
            "title": "3.2.2 Camera Pose Alignment",
            "content": "To precisely align the simulated camera with its real-world counterpart, we formulate the problem as an optimization task that minimizes the photometric error between rendered and real images. Given real-world reference image Ireal, our objective is to solve for the optimal camera pose Tcam, which comprises its position and rotation. We minimize the following loss function: Lcam = R(Tcam) Ireal. (4) where R() is our differentiable rendering function that synthesizes an image from given camera pose, and denote the L1 norm. Starting from rough initial pose init cam , we leverage the differentiability of the 3DGS renderer to compute the gradient of the loss with respect to the camera parameters. The camera pose is then iteratively refined using gradient descent until convergencee, resulting in simulated camera pose that is precisely aligned with the real-world view. With both the world coordinate frame and camera pose aligned, our hybrid representation is now fully integrated into the simulator, creating high-fidelity digital twin ready for large-scale data generation, as shown in Fig. 2."
        },
        {
            "title": "3.3 Holistic Scene Augmentation",
            "content": "While the previous steps create visually and physically faithful digital twin, static environment is insufficient for training robust policy that can handle real-world variability. To bridge this gap, we introduce Holistic Scene Augmentation, process that systematically randomizes multiple facets of the simulated environment to create diverse dataset."
        },
        {
            "title": "3.3.1 Object-level Augmentation",
            "content": "To enhance the policys robustness to variations in object placement and size, we perform object-level augmentation. We randomize the 6-DoF pose (position and orientation) and uniform scale of interactive objects, each sampled from predefined range. This process ensures the model is exposed to diverse set of object configurations during training, thereby improving its ability to generalize to unstructured real-world environments."
        },
        {
            "title": "3.3.2 Camera View Augmentation",
            "content": "A key advantage of 3D Gaussian Splatting (3DGS) is its inherent capability for high-fidelity novel view synthesis. To generate diverse yet realistic camera perspectives, we anchor our augmentation to the deployment environment. We begin with the camera extrinsics that were optimized to match the real-world setup, as detailed in Section 3.2.2. We then apply random perturbations to this base pose by introducing random translations and rotations. This creates distribution of views centered on the deployment perspective, training the policy to be robust against minor shifts in camera placement."
        },
        {
            "title": "3.3.3 Lighting Condition Augmentation",
            "content": "Discrepancies in lighting conditions between training and deployment environments pose significant challenge to policy generalization. To address this, we directly augment the visual attributes of 3D Gaussians through combination of random scaling, offset, and noise to simulate global changes in color contrast and overall brightness. In contrast, random Gaussian noise is sampled independently for each Gaussian to mimic the sensor noise characteristic of real-world cameras. This approach creates robust training data that enables the policy to operate effectively under wide variety of previously unseen lighting conditions."
        },
        {
            "title": "3.3.4 Trajectory Augmentation",
            "content": "To further diversify the demonstration data and prevent the policy from overfitting to single kinematic path, we introduce trajectory-level augmentation. Instead of generating motion to target, the robots end-effector is first commanded to randomized intermediate via-point using an inverse kinematics (IK) solver. This via-point is generated by applying small positional offset to the final goal. This two-stage motion strategy breaks trajectory determinism and exposes the policy to richer distribution of paths, enhancing its robustness and ability to recover from minor execution errors."
        },
        {
            "title": "4.1.1 Hardware platform",
            "content": "We conduct our experiments across both real-world and simulated environments. For real-world data collection and policy evaluation, we utilize the LeRobot (Cadene et al., 2024) framework with two cameras providing RGB observations which captures RGB observations at 640x480 resolution. Meanwhile, all simulation data is generated on separate workstation configured with an Intel Core i5-14400F CPU and an NVIDIA RTX 5060 Ti GPU."
        },
        {
            "title": "4.1.2 Policy and VLA model training details",
            "content": "To validate our simulated data fidelity, we evaluate several state-of-the-art policies: Single-task methods, such as Diffusion Policy (Chi et al., 2023) across diverse tasks, and Generalist VLA model: π0 (Black et al., 2024). Policy inference is carried out on an NVIDIA H20 GPU. Table 1 Performance improvement from augmenting real data with simulated data. Adding just 50 synthetic demonstrations from RoboSimGS provides substantial performance boost across all evaluated methods. Method Data Source Stack Cubes PickPlace Deformable PickPlace Upright Bottle DP (Chi et al., 2023) DP (Chi et al., 2023) DP (Chi et al., 2023) DP (Chi et al., 2023) π0 (Black et al., 2024) π0 (Black et al., 2024) π0 (Black et al., 2024) π0 (Black et al., 2024) DP (Chi et al., 2023) DP (Chi et al., 2023) Method DP (Chi et al., 2023) DP (Chi et al., 2023) DP (Chi et al., 2023) DP (Chi et al., 2023) π0 (Black et al., 2024) π0 (Black et al., 2024) π0 (Black et al., 2024) π0 (Black et al., 2024) DP (Chi et al., 2023) DP (Chi et al., 2023) 50 Real 50 RoboSimGS 100 RoboSimGS 50 Real + 50 RoboSimGS 50 Real 50 RoboSimGS 100 RoboSimGS 50 Real + 50 RoboSimGS 50 RoboSimGS w/o Physics Estimation 50 RoboSimGS w/o Holistic Scene Augmentation 0.60 0.54 0.57 0.69 0.40 0.34 0.54 0.54 0.54 0.37 0.71 0.60 0.83 0.83 0.94 0.80 0.91 0.94 0.60 0.46 0.77 0.69 0.86 0.86 0.97 0.83 0.94 0.94 0.54 0.51 0.86 0.66 0.82 0.91 0.63 0.57 0.69 0.74 0.57 0.43 Data Source Move Bottle Drawer Close Box Close Wiping 50 Real 50 RoboSimGS 100 RoboSimGS 50 Real + 50 RoboSimGS 50 Real 50 RoboSimGS 100 RoboSimGS 50 Real + 50 RoboSimGS 50 RoboSimGS w/o Physics Estimation 50 RoboSimGS w/o Holistic Scene Augmentation 0.89 0.80 0.91 0.91 0.37 0.51 0.54 0.57 0.77 0.54 0.51 0.43 0.49 0.60 0.49 0.57 0.60 0.60 0.46 0. 0.57 0.54 0.63 0.66 0.54 0.54 0.63 0.69 0.49 0.34 0.91 0.85 0.91 0.94 0.00 0.69 0.88 0.86 0.51 0.71 Table 2 Performance when facing changing settings. We report the success rate of DP (Chi et al., 2023) under various settings. The policies trained with demonstrations generated by RoboSimGS significantly boosts the policys generalization capabilities. Data Source Lighting Condition Object Size Scene Clutter Camera Pose Desktop Appearance 50 Real 50 RoboSimGS 50 Real + 50 RoboSimGS 0.00 0.46 0.54 0.06 0.63 0. 0.00 0.51 0.60 0.23 0.57 0.71 0.00 0.49 0."
        },
        {
            "title": "4.1.3 Metric",
            "content": "The primary metric for evaluation is the Success Rate, defined as the percentage of successful task completions over 35 consecutive trials in the real world. trial is deemed successful if the robot correctly finish the task in under 20 seconds."
        },
        {
            "title": "4.1.4 Robot tasks",
            "content": "To demonstrate the generalization ability of our pipeline across diverse manipulation challenges, we evaluate our method on eight distinct tasks, as shown in Fig. 3. These tasks cover spectrum of skills, including stacking, articulated object interaction, deformable object handling, and tool use. The specific tasks are as follows: Stack Cubes: Stacking 3.5 cm red cube onto 3.5 cm blue cube. Pick & Place: Picking banana and placing it into box. Deformable Pick & Place: variation of the previous task, where the rigid banana is replaced with soft toy to test deformable object manipulation. Upright Bottle: Restoring an overturned bottle to its stable, upright orientation. Move Bottle: Grasping and dragging bottle across the workspace. Drawer Close: long-horizon task requiring the robot to place 2 cm cube into drawer and then close it. Box Close: similar long-horizon task involving placing 3.5 cm red cube into box and closing its lid. Wiping: Grasping towel and wiping designated area on the table surface. To ensure robust evaluation of generalization, we apply significant randomization to the initial state of the environment. For all tasks, the 6-DoF pose of primary objects (e.g., cubes, banana) and containers (e.g., box, drawer) is randomized. Specifically, their initial positions are sampled uniformly within an annular region 28 to 35 cm from the robots base. Concurrently, their initial orientations around the vertical z-axis are randomized within intervals such as [0, 2π], [0, π], and [π/8, π/8]. Figure 4 Visualization of policy performance under four challenging generalization settings designed to test robustness."
        },
        {
            "title": "4.2.1 Zero-shot Sim2Real Transfer",
            "content": "To validate RoboSimGSs ability to achieve zero-shot Sim2Real transfer, we conducted key experiment where the policy is trained exclusively on our simulated data and then deployed directly in the real world without any fine-tuning. We specifically analyze the performance of policy trained on 100 demonstrations generated by RoboSimGS (100 RoboSimGS) and compare it to policy trained on 50 real-world demonstrations (50 Real), as shown in Tab. 1."
        },
        {
            "title": "4.2.2 Synergistic Effect of Real and Simulated Data",
            "content": "To evaluate the benefit of combining real and simulated data, we compare policies trained on 50 real-world demonstrations (50 Real) against policies trained on hybrid dataset (50 Real + 50 RoboSimGS). The results, presented in Table 1, reveal substantial and consistent performance boost across all evaluated methods. This highlights powerful synergistic effect, where our high-fidelity synthetic data significantly enhances the utility of limited real-world data."
        },
        {
            "title": "4.2.3 Data Scaling and Efficiency Analysis",
            "content": "We conduct data scaling analysis to quantify the quality and sample efficiency of our simulated data. Using Diffusion Policy (Chi et al., 2023), we compare performance of policies trained on varying amounts of real data (50, 100 demonstrations) versus those trained exclusively on our simulated data (50 to 500 demonstrations). As shown in Fig. 5, the policy trained on just 200 of demonstrations by RoboSimGS achieves performance comparable to one trained on 100 real-world demonstrations."
        },
        {
            "title": "4.2.4 Generalization to Challenging Settings",
            "content": "A robust policy must generalize to unforeseen variations. We test this by evaluating performance in challenging real-world conditions, as shown in Fig. 4. We compare three policies: one trained on 50 real demonstrations, second on 50 simulated demonstrations with our holistic augmentations, and third on their combination. As shown in Tab. 2, the policy trained with our augmented synthetic data significantly outperforms the one trained on real data alone, demonstrating superior generalization. Combining both data sources yields the best results, confirming that our augmentations effectively prepare the policy for real-world uncertainties. Figure 5 Data scaling analysis for Diffusion Policy (Chi et al., 2023) on the Stack Cubes task. The plot compares the success rate of policies trained on varying amounts of real-world data versus purely simulated data generated by RoboSimGS. Notably, the policy trained on 200 simulated demonstrations achieves success rate comparable to one trained on 100 real-world demonstrations, highlighting the high quality and data efficiency of our generated data. Table 3 Performance of simulated data generation. We report the data collection success rate (Succ) and average generation time in seconds (Time) per sample, measured on single NVIDIA RTX 5060 Ti GPU."
        },
        {
            "title": "Stack Cubes\nPickPlace\nDeformable PickPlace\nUpright Bottle",
            "content": "0.94 0.98 0.97 0.97 8.9 8.6 8.7 8."
        },
        {
            "title": "Move Bottle\nDrawer Close\nBox Close\nWiping",
            "content": "0.95 0.93 0.95 1.00 6.7 9.8 10.3 6."
        },
        {
            "title": "4.2.5 Sim2Real Transfer Fidelity",
            "content": "To assess the core transfer capability of RoboSimGS, we conduct comprehensive cross-domain evaluation  (Fig. 6)  . The key finding is that policies trained exclusively in our simulator (Sim-to-Real) achieve success rates nearly identical to those of policies trained and evaluated entirely in the real world (Real-to-Real). This result validates our central claim of achieving true zero-shot Sim2Real transfer without any real-world fine-tuning. Furthermore, the strong performance in the Real-to-Sim scenario confirms that our simulation environment serves as high-fidelity twin of reality, capable of reliably evaluating policies trained on real data."
        },
        {
            "title": "4.2.6 Data Generation Efficiency and Scalability",
            "content": "In addition to demonstrating high success rates in simulation, our framework offers significant advantage in data generation efficiency. As shown in Tab. 3, RoboSimGS generates over 10,000 demonstrations per day on single NVIDIA RTX 5060 Ti GPU. In contrast, full-time human operator can typically collect around 1,000 demonstrations per day via teleoperation. Our method thus offers more than 10-fold increase in data collection throughput, providing scalable and cost-effective alternative to manual data collection."
        },
        {
            "title": "4.3.1 Effect of Physics Estimation",
            "content": "To isolate the contribution of our MLLM-driven Physics Estimation in Section. 3.1.2, we trained policy on 50 demonstrations generated with default, physically implausible properties. As shown in Table 1, removing physics estimation significantly degrades real-world performance, especially in contact-rich tasks. For instance, the success rate for Wiping plummets from 0.85 to 0.51, and for Deformable PickPlace, from 0.69 to 0.54. This occurs because Figure 6 Policy Performance Analysis Across Training and Evaluation Domains. This chart compares the success rates of policies on eight tasks. We evaluate four distinct scenarios to analyze the domain gaps: policies are trained on data from our RoboSimGS (Sim) or from real-world demonstrations (Real), and then evaluated in both domains. the policy learns invalid interaction dynamics. This result underscores that accurate physics simulation is as critical as visual fidelity."
        },
        {
            "title": "4.3.2 Effect of Holistic Scene Augmentation",
            "content": "To demonstrate that simple domain randomization is insufficient, we ablate our holistic augmentation strategy. We trained policy on 50 demonstrations where only the 6-DoF pose of objects is randomized. As shown in Table 1 (50 RoboSimGS w/o Holistic Scene Augmentation), this partial augmentation still results in severe performance collapse. The policy overfits to the static camera view, lighting conditions, and deterministic action sequences. Therefore, our holistic approach, which randomizes the entire scene context, is indispensable for learning truly robust and deployable policy."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "This work presents RoboSimGS, novel Real2Sim2Real framework tackling the core challenges in robot learningthe high cost of real-world data collection and the significant gap between simulation and reality. At its core is hybrid scene representation merging the photorealism of 3DGS for static environments with the physical fidelity of mesh primitives for dynamic objects. We pioneer the use of MLLM to deduce objects physical properties and kinematic structures from vision, creating fully articulated assets automatically. Policies trained solely on our generated data achieve high success rates in diverse, real-world manipulation tasks, realizing zero-shot Sim2Real transfer. Furthermore, our method serves as powerful data augmentation tool, substantially improving SOTA visuomotor policies when supplementing limited real-world data. Limitation. primary limitation of RoboSimGS is the time-consuming and complex scene reconstruction pipeline. Although this manual step is one-time effort per scene, it poses significant bottleneck for large-scale deployment of our framework to new environments. Future work will focus on faster 3D reconstruction methods to streamline this setup."
        },
        {
            "title": "References",
            "content": "Josh Achiam et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. OpenAI: Marcin Andrychowicz et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39 (1):320, 2020. Paul Besl and Neil McKay. Method for registration of 3-D shapes. In Sensor Fusion IV: Control Paradigms and Data Structures, volume 1611, pages 586606, 1992. Johan Bjorck et al. GR00T N1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black et al. π0: vision-language-action flow model for general robot control, 2024. Konstantinos Bousmalis et al. Unsupervised pixel-level domain adaptation with generative adversarial networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 37223731, 2017. Konstantinos Bousmalis et al. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. In International Conference on Robotics and Automation, pages 42434250, 2018. Remi Cadene et al. Lerobot: State-of-the-art machine learning for real-world robotics in pytorch, 2024. Yevgen Chebotar et al. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In International Conference on Robotics and Automation, pages 89738979, 2019. Cheng Chi et al. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 2023. A. Code. Ar code, 2022. https://ar-code.com/. Software tool for algorithmic rendering. Danny Driess et al. PaLM-E: An embodied multimodal language model. In International Conference on Machine Learning, 2023. Ioannis Exarchos, Yifeng Jiang, Wenhao Yu, and Karen Liu. Policy transfer via kinematic domain randomization and adaptation. In International Conference on Robotics and Automation, pages 4551, 2021. Haoran Geng et al. RoboVerse: Towards unified platform, dataset and benchmark for scalable and generalizable robot learning. arXiv preprint arXiv:2504.18904, 2025. Antoine Guédon and Vincent Lepetit. SuGaR: Surface-aligned gaussian splatting for efficient 3D mesh reconstruction and high-quality mesh rendering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53545363, 2024. Xiaoshen Han et al. Re3Sim: Generating high-fidelity simulation data via 3D-photorealistic real-to-sim for robotic manipulation. arXiv preprint arXiv:2502.08645, 2025. Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmentation. In International Conference on Robotics and Automation, pages 1361113617, 2021. Johann Huber et al. Domain randomization for sim2real transfer of automatically generated grasping datasets. In IEEE International Conference on Robotics and Automation, pages 41124118, 2024. Zhenyu Jiang et al. DexMimicGen: Automated data generation for bimanual dexterous manipulation via imitation learning. In International Conference on Robotics and Automation, pages 1692316930, 2025. Bernhard Kerbl et al. 3D gaussian splatting for real-time radiance field rendering. 42(4):1391, 2023. Moo Jin Kim et al. OpenVLA: An open-source vision-llanguage-action model. arXiv preprint arXiv:2406.09246, 2024. Misha Laskin et al. Reinforcement learning with augmented data. Advances in neural information processing systems, 33: 1988419895, 2020. Hangyu Li et al. TeleOpBench: simulator-centric benchmark for dual-arm dexterous teleoperation. arXiv preprint arXiv:2505.12748, 2025. Xinhai Li et al. RoboGSim: real2sim2real robotic gaussian splatting simulator. arXiv preprint arXiv:2411.11839, 2024. Mingsheng Long et al. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pages 97105, 2015. Haozhe Lou et al. Robo-GS: physics consistent spatial-temporal model for robotic arm with hybrid representation. arXiv preprint arXiv:2408.14873, 2024. Ajay Mandlekar et al. Mimicgen: data generation system for scalable robot learning using human demonstrations. arXiv preprint arXiv:2310.17596, 2023. Ben Mildenhall et al. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1): 99106, 2021. Jean-Baptiste Mouret and Konstantinos Chatzilygeroudis. 20 years of reality gap: few thoughts about simulators in evolutionary robotics. In genetic and evolutionary computation conference companion, pages 11211124, 2017. Ri-Zhao Qiu, Ge Yang, Weijia Zeng, and Xiaolong Wang. Feature splatting: Language-driven physics-based scene synthesis and editing. arXiv preprint arXiv:2404.01223, 2024. Mohammad Nomaan Qureshi et al. SplatSim: Zero-shot sim2real transfer of rgb manipulation policies using gaussian splatting. arXiv preprint arXiv:2409.10161, 2024. Alec Radford et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763, 2021. Fereshteh Sadeghi and Sergey Levine. CAD2RL: Real single-image flight without single real image. arXiv preprint arXiv:1611.04201, 2016. Jie Tan et al. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018. Gemini Robotics Team, Saminda Abeyruwan, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. Josh Tobin et al. Domain randomization for transferring deep neural networks from simulation to the real world. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 2330, 2017. Jingkang Wang et al. Reinforcement learning with perturbed rewards. In AAAI Conference on Artificial Intelligence, volume 34, pages 62026209, 2020. David Whitney et al. ROS reality: virtual reality framework using consumer-grade hardware for ROS-enabled robots. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 19, 2018. Yuxuan Wu et al. RL-GSBridge: 3D gaussian splatting based real2sim2real method for robotic manipulation learning. arXiv preprint arXiv:2409.20291, 2024. Zhengrong Xue et al. DemoGen: Synthetic demonstration generation for data-efficient visuomotor policy learning. arXiv preprint arXiv:2502.16932, 2025. Sizhe Yang et al. Novel demonstration generation with gaussian splatting enables robust one-shot manipulation. Robotics: Science and Systems, 2025. Justin Yu et al. Real2Render2Real: Scaling robot data without dynamics simulation or robot hardware. arXiv preprint arXiv:2505.09601, 2025. Tianhe Yu et al. Scaling robot learning with semantically imagined experience. arXiv preprint arXiv:2302.11550, 2023. Haoyu Zhao et al. Automated 3D physical simulation of open-world scene with gaussian splatting. arXiv preprint arXiv:2411.12789, 2024a. Haoyu Zhao et al. SG-GS: Photo-realistic animatable human avatars with semantically-guided gaussian splatting. arXiv preprint arXiv:2408.09665, 2024b. Haoyu Zhao et al. SMAP: Self-supervised motion adaptation for physically plausible humanoid whole-body control. arXiv preprint arXiv:2505.19463, 2025a. Haoyu Zhao et al. Towards affordance-aware robotic dexterous grasping with human-like priors. arXiv preprint arXiv:2508.08896, 2025b. Tony Zhao et al. Learning fine-grained bimanual manipulation with low-cost hardware. Robotics: Science and Systems, 2023."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Huazhong University of Science and Technology",
        "Hupan Lab",
        "The Chinese University of Hong Kong",
        "Tsinghua University",
        "Wuhan University",
        "Zhejiang University"
    ]
}