{
    "paper_title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "authors": [
        "Ruoxuan Zhang",
        "Bin Wen",
        "Hongxia Xie",
        "Yi Yao",
        "Songhan Zuo",
        "Jian-Yu Jiang-Lin",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 2 0 4 5 3 0 . 2 1 5 2 : r CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation Ruoxuan Zhang zhangrx22@mails.jlu.edu.cn Jilin University Changchun, China Yi Yao leo81005.ee10@nycu.edu.tw National Yang Ming Chiao Tung University Hsinchu, Taiwan Bin Wen wenbin2122@mails.jlu.edu.cn Jilin University Changchun, China Songhan Zuo zuosh2122@mails.jlu.edu.cn Jilin University Changchun, China Hongxia Xie hongxiaxie@jlu.edu.cn Jilin University Changchun, China Jian-Yu Jiang-Lin jianyu@cmlab.csie.ntu.edu.tw National Taiwan University Taipei, Taiwan Hong-Han Shuai hhshuai@nycu.edu.tw National Yang Ming Chiao Tung University Hsinchu, Taiwan Wen-Huang Cheng wenhuang@csie.ntu.edu.tw National Taiwan University Taipei, Taiwan Figure 1: Demonstration of our CookAnything model generating multi-step cooking instructions in single pass. Each example shows the users prompt (left) and the corresponding series of dish images (right), from initial preparation steps through the final plated result (Details of the complete recipe text can be found in the Supplementary A.6.). Corresponding Author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. MM 25, Dublin, Ireland 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2035-2/2025/10 https://doi.org/10.1145/3746027.3755174 Abstract Cooking is sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, flexible and consistent diffusion-based framework that generates coherent, MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within single denoising process; (2) Flexible RoPE, step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in trainingbased and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation. More details are at https://github.com/zhangdaxia22/CookAnything. CCS Concepts Computing methodologies Computer vision tasks. Keywords Recipe image generation, procedural sequence generation, food computing ACM Reference Format: Ruoxuan Zhang, Bin Wen, Hongxia Xie, Yi Yao, Songhan Zuo, Jian-Yu JiangLin, Hong-Han Shuai, and Wen-Huang Cheng. 2025. CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation. In Proceedings of the 33rd ACM International Conference on Multimedia (MM 25), October 2731, 2025, Dublin, Ireland. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3746027."
        },
        {
            "title": "1 Introduction\nCooking is a richly visual and sequential activity: from chopping\nonions to garnishing a dish, each step not only involves semantic\ntransitions but also yields observable visual transformations [16,\n21, 43]. Accurately illustrating these processes from textual instruc-\ntions holds significant value for applications in culinary education,\nassistive technology, and multimodal content generation, enabling\nusers to better understand, follow, and interact with complex pro-\ncedures in an intuitive visual manner.",
            "content": "As textual recipes abstract the cooking process into language, recipe illustration aspires to reverse this abstraction, generating coherent image sequences that visually narrate each procedural step [8, 14, 18, 23]. Compared to single-image generation, this task introduces unique challenges: it requires maintaining temporal progression, preserving ingredient consistency, and capturing subtle visual distinctions between stages. StackedDiffusion [18] pioneered the task of illustrated recipe instructions by generating one image per recipe step. However, its design assumes fixed number of steps, ignoring the inherent variability across recipes, resulting in both under-generation and over-generation in real-world settings. While recent advances in text-to-image synthesis, particularly diffusion-based models such as FLUX [11] have achieved remarkable success in high-fidelity image generation, these models are predominantly designed for single-image outputs. This limits their applicability in structured, multi-step domains such as cooking recipes, where each visual output must correspond to distinct semantic step and together form coherent sequence. Attempts to extend these models, such as In-Context LoRA [9], adopt simple concatenation of step prompts to jointly synthesize multi-step outputs. However, this design leads to semantic entanglement, where visual features bleed across steps, producing indistinguishable or incoherent images that undermine the narrative flow. Motivated by these challenges, we investigate the key question: How can we achieve flexible, coherent, and semantically disentangled multi-step recipe image generation in unified framework? We propose CookAnything, diffusion-based framework for generating step-by-step illustrated recipes with variable length and high visual consistency (as shown in Fig. 1). It introduces three key components: (1) Step-wise Regional Control (SRC) mechanism that assigns each instruction to distinct latent region, ensuring semantic separation and global coherence; (2) new positional encoding method, Flexible Rotary Position Embedding (RoPE), which resets coordinate indices per step to support diverse layouts; and (3) Cross-Step Consistency Control (CSCC) module that preserves the visual continuity of fine-grained ingredients across steps. These innovations enable structured, coherent, and flexible multi-image generation for sequential visual synthesis. Our contributions are summarized as follows: We propose CookAnything, the first diffusion-based framework for illustrated recipe generation with arbitrary-length, step-wise image sequences, flexibly adapting to diverse realworld structures. We introduce SRC and Flexible RoPE to address position misalignment via step-aware spatial encoding and region binding, and CSCC to ensure visual consistency of recurring ingredients across steps. Experiments show that our method achieves state-of-theart results in both training-based and training-free settings, with broad potential in instructional and procedural content generation."
        },
        {
            "title": "2 Related Work\n2.1 Recipe Analysis Task\nThe study of food-centric multimedia has garnered increasing at-\ntention within the multimedia research community due to its sig-\nnificant relevance to human survival, nutrition, health, and sensory\nenjoyment [22, 36, 38, 40, 42, 47, 49, 50]. The growing availabil-\nity of large-scale food datasets such as Recipe1M+ [17], Food-101\n[2], VireoFood-172 [5], and Nutrition5k [37] has fueled research\nin a wide range of fine-grained food analysis tasks. These tasks\ninclude food and ingredient classification [10, 19, 20, 44, 45], food\ninstance segmentation [12, 39], and nutrition or weight estimation\n[7], among others.",
            "content": "In this work, we focus on the task of step-wise recipe image generation, which aims to synthesize sequence of visual illustrations corresponding to each step in cooking recipe. Early efforts, such as CookGAN [8], generated dish images based on latent representations of ingredient lists, while ChefGAN [23] employed recipe instructions as input. ML-CookGAN [14] further combined both ingredients and steps to generate the final dish image. However, these methods are inherently limited to producing only single CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland image corresponding to the completed dish, thus failing to capture the procedural nature of cooking. To address this limitation, StackedDiffusion [18] introduced the novel task of illustrated recipe instructions, where an image is generated for each individual step in the recipe. However, it falls short in adapting to the natural variability of recipe lengths, producing fixed number of step images regardless of the actual recipe structure. In this work, we propose flexible framework, CookAnything, that dynamically adapts to the varying number of steps in different recipes while ensuring accurate visual-semantic alignment, procedural coherence, and ingredient consistency across the generated sequence."
        },
        {
            "title": "2.2 Procedural Sequence Generation Model\nIn the context of text-to-image synthesis, a Procedural Sequence\nGeneration Model decomposes the generation pipeline into discrete\nstages or operations, such as layout planning, object placement, at-\ntribute assignment, and appearance refinement. Recently, diffusion\nmodels have revolutionized text-to-image generation, producing\nhigh-fidelity visuals through iterative denoising [6, 9, 11, 27]. Latent\nDiffusion Models (LDM) [30] and Vision Transformer-based Diffu-\nsion Transformers (DiTs) [11, 24] balance generation quality with\nscalability, capturing global semantics via attention mechanisms.\nHowever, most existing work focuses on single-image synthesis,\noverlooking structured, multi-step image generation.",
            "content": "Emerging efforts in story visualization [15] hint at the potential of sequential visual generation but fall short in domains like recipes, which demand procedural consistency, spatial coherence, and semantic disentanglement across steps. Crucially, ingredient continuity must be preserved, ensuring logical visual evolution throughout the cooking process. To our knowledge, we are the first to introduce diffusion-based model for step-wise recipe illustration. Building on the DiT backbone, our framework explicitly models semantic grounding, temporal alignment, and ingredient consistency across visual sequences, offering structured and extensible solution for procedural image generation."
        },
        {
            "title": "3.1 Preliminary\nFlux.1-dev. Flux.1-dev is a text-to-image model that generates a\nsingle high-quality image from a text prompt [11]. It replaces the U-\nNet [33] in Stable Diffusion (SD) [31] with a Diffusion Transformer\n(DiT)[24] for better representation learning. For text encoding, it\ncombines T5 [29] and CLIP [28] to improve text-image alignment.\nDiT performs joint self-attention over concatenated text and latent\ntokens, processing noisy latent tokens z âˆˆ Rğ‘ Ã—ğ‘‘ and text condition\ntokens Cğ‘‡ âˆˆ Rğ‘€ Ã—ğ‘‘ , where ğ‘‘ is the embedding dimension, and ğ‘ ,\nğ‘€ are the numbers of image and text tokens.\nRoPE. In the Flux.1-dev model, Rotary Position Embedding (RoPE)\nis employed to encode positional information for latent tokens.\nGiven noisy latent tokens ğ‘§, the positional encoding process can be\nmathematically expressed as:",
            "content": "(cid:164)ğ‘§ğ‘–,ğ‘— = ğ‘§ğ‘–,ğ‘— ğ‘…(ğ‘–, ğ‘—), (1) where ğ‘…(ğ‘–, ğ‘—) is the rotation matrix corresponding to the position (ğ‘–, ğ‘—), effectively encoding the spatial location of each token within the image. This approach enhances the models ability to capture spatial relationships and dependencies inherent in visual data. Joint Attention. The joint attention mechanism maps positionencoded tokens into Query ğ‘„, Key ğ¾, and Value ğ‘‰ . Additionally, it concatenates the text tokens for attention calculation. The attention operation can be expressed as: Attn( [ğ¶ğ‘‡ ; (cid:164)ğ‘§ğ‘–,ğ‘— ]) = SoftMax (cid:19) (cid:18) ğ‘„ğ¾ ğ‘‘ ğ‘‰ , (2) where ğ‘„, ğ¾, and ğ‘‰ are the queries, keys, and values derived from the token embeddings, and ğ‘‘ represents the dimensionality of the embeddings. The concatenation of the image and text tokens, denoted as [ğ¶ğ‘‡ ; (cid:164)ğ‘§ğ‘–,ğ‘— ], facilitates the multi-modal attention mechanism, enabling joint attention across both modalities."
        },
        {
            "title": "3.2 Step-wise Regional Control\nLimitations of Flux.1-dev. Flux.1-dev is designed for generating\na single image from a text prompt, making it inherently unsuitable\nfor tasks that require a coherent sequence of step-wise imagesâ€”such\nas recipe visualization. This application demands accurate per-step\ngeneration, global consistency, and visual diversity across steps. To\nadapt Flux.1-dev for multi-image generation, In-Context LoRA [9]\nproposes a simple strategy: concatenating all step-level prompts and\njointly generating the full sequence of images in a single pass. While\nthis method allows simultaneous generation, it lacks explicit step-\nlevel separation, resulting in severe semantic entanglement across\nsteps. Visual features from one instruction often leak into others,\nproducing highly similar and indistinct images across steps.",
            "content": "We quantify this limitation using the Cross-Step Consistency (CSC) metric. The In-Context LoRA baseline yields CSC score of 44.129.03 points lower than the ground truth score of 53.15and the lowest among all evaluated methods (more details can be found in Tab. 2). This significant drop clearly indicates that without explicit step-level control, the model fails to maintain step-wise distinctiveness and coherence. Step-wise Regional Control (SRC). To address the entanglement and lack of step-wise control in Flux.1-dev and its simple concatenation-based variants, we propose SRC-a novel mechanism MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. Figure 2: Overall structure of our CookAnything model, illustrated with 3-step vegetable pancake recipe. The Cooking Agent reformats the raw recipe into context-tagged steps, supplementing missing ingredient details. Each step is encoded by T5 Encoder in two ways: (1) all steps are concatenated to capture global context and produce contextual step tokens, and (2) each step is encoded independently to preserve local semantics and generate step tokens. These two types of tokens are fused via weighted averaging. Meanwhile, noisy latent tokens, processed by Flexible RoPE, are fed into DiT. Step-wise Regional Attention Mask is applied during DiTs self-attention to constrain attention within each step, ensuring step-wise focus and visual consistency. In the illustration, purple, green, and pink tokens represent Steps 1, 2, and 3, respectively that enables the model to synthesize coherent sequence of semantically distinct step images within single denoising process. SRC introduces architectural changes that explicitly bind each textual step instruction to designated image region while preserving global coherence across the entire image. This allows for both localized control and smooth visual transitions, bridging the gap between single-image generation and structured multi-image synthesis. Step-wise Encoding and Integration. SRC modifies the decoding pipeline of Flux.1-dev by introducing Step-wise Encoding Mechanism and Step-wise Regional Attention Mask. Specifically, each recipe step is first independently encoded using shared text encoder, and the resulting step tokens are concatenated before the latent tokens: ğ¶input = (cid:2)ğ¶ (1) ; ğ¶ (2) ; . . . ; ğ¶ (ğ‘ ) (cid:3) , (3) ğ‘‹ input = (cid:2)ğ¶input; ğ‘§input(cid:3) , (4) where ğ¶ (ğ‘›) denotes the encoded tokens of the ğ‘›-th step and ğ‘§input denotes the noisy latent tokens. Step-wise Regional Attention Mask. To prevent semantic leakage between steps and ensure localized step-to-region alignment, we design Step-wise Regional Attention Mask ğ‘€ R2ğ‘ 2ğ‘ restricting attention within each step-region pair. Formally, it is defined as: ğ‘€ğ‘–,ğ‘— = if ğ‘– = ğ‘— or ğ‘– ğ‘— = ğ‘ , 1 0 otherwise. Here, ğ‘– and ğ‘— represent the ğ‘–-th and ğ‘—-th step/image token sets. The attention operation within DiT then becomes: (cid:18) ğ‘„ğ¾ğ‘‡ ğ‘‘ğ‘˜ Attn(ğ‘„, ğ¾, ğ‘‰ , ğ‘€) = Softmax ğ‘€ ğ‘‰ , (6) (5) (cid:19) (cid:40) ensuring each step attends only to its paired visual region and associated instruction tokens. The updated regional latent representation at timestep ğ‘¡ 1 is computed as: (cid:16) ğ‘§ğ‘¡ 1 region = ğœ“ Attn(ğ‘„ğ‘¡ 1 where ğœ“ represents the DiT block from Flux.1-dev. , ğ‘‰ ğ‘¡ 1 region , ğ¾ğ‘¡ 1 region region , ğ‘€region) (cid:17) , (7) Whole-Description Control for Global Coherence. To complement regional specificity with global visual consistency, we incorporate Whole-Description Control Mechanism that processes the full recipe description in parallel: ğ‘§ğ‘¡ 1 base = ğœ“ (Attn(ğ‘„ğ‘¡ 1 , ğ‘‰ ğ‘¡ 1 base , ğ‘€base)). , ğ¾ğ‘¡ 1 base (8) base Finally, we fuse the regional and global latent representations through weighted interpolation: ğ‘§ğ‘¡ = ğ›¼ ğ‘§base (9) where ğ›¼ [0, 1] controls the trade-off between global context and regional detail. ğ‘¡ 1 + (1 ğ›¼) ğ‘§region ğ‘¡ 1 , By preserving global structure while controlling step-wise semantics and region-level generation, SRC overcomes the limitations of prior methods and enables high-quality procedural image synthesis."
        },
        {
            "title": "3.3 Flexible RoPE\nLimitation of the Origin RoPE. When generating a flexible num-\nber of step-wise images, Flux.1-dev, which adopts the original Ro-\ntary Position Embedding (RoPE), encounters two key limitations,\nas illustrated in Fig. 3 using a Lamb Pilaf example. The first issue is\nthe Misaligned Positional Embedding. When generating multi-\nimages in a single forward pass using FLUX, the use of original",
            "content": "CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland Figure 4: Examples before and after applying Cross-Step Consistency Control (CSCC). Left: Stir-Fried Carrot with Dried Tofu. Without CSCC, the carrot changes from cubes to strips in Step 4. Visualization of contextual tokens (using Flux.1dev) shows shape continuity is preserved, so CSCC helps maintain consistent appearance. Right: Steamed Chicken Wings with Taro. In Step 5, taro should appear beneath the wings but disappears without CSCC. Since contextual tokens confirm its presence, CSCC successfully preserves it. Specifically, for each image region ğ‘›, we apply separate RoPE encoding: (cid:164)ğ‘§ (ğ‘›) ğ‘–,ğ‘— = ğ‘§ (ğ‘›) ğ‘–,ğ‘— ğ‘… (ğ‘›) (ğ‘–, ğ‘—), (10) where ğ‘… (ğ‘›) (ğ‘–, ğ‘—) is the region-specific rotation matrix for the ğ‘›-th image, applied to token ğ‘§ (ğ‘›) ğ‘–,ğ‘— at position (ğ‘–, ğ‘—). This design ensures that each region learns positionally independent patterns, effectively preventing the model from inheriting noise or structural artifacts from adjacent steps."
        },
        {
            "title": "We then concatenate all positionally encoded tokens as input to",
            "content": "the model: ğ‘§input = (cid:2) (cid:164)ğ‘§ (1) ; (cid:164)ğ‘§ (2) ; . . . ; (cid:164)ğ‘§ (ğ‘ ) (cid:3) , where ğ‘ denotes the number of step-wise images. This forms joint input sequence where each region maintains its own positional integrity while enabling global attention across regions. (11) In Fig.3, Flexible RoPE leads to significantly improved spatial alignment and visual consistency across steps, especially in long recipes, demonstrating its effectiveness and scalability."
        },
        {
            "title": "3.4 Cross-Step Consistency Control\nWhile generating all step-wise images in a single denoising pass\nenhances stylistic and background consistency, it struggles with\nrecipes involving numerous small ingredientsâ€”especially in stir-fry\ndishes where chopped ingredients are scattered and repeatedly ap-\npear in small visual regions. In such cases, simultaneous denoising\nfails to preserve crucial visual attributes such as shape, color, and\ntexture, and may even omit ingredients. We refer to this as the Tiny\nIngredient Continuity Problem (see Fig. 4).",
            "content": "To address this challenge, we propose Cross-Step Consistency Control (CSCC), lightweight yet effective solution that promotes the visual continuity of fine-grained ingredients across steps, while maintaining the independence of each steps image content. Our Figure 3: The example from Original RoPE. Visualization comparison between original RoPE and our proposed Flexible RoPE using the example of Lamb Pilaf. With original RoPE, repeated step images appear as early as Step 2. Steps 3 and 6 exhibit positional misalignment, and Step 9 suffers from noticeable blurring. In contrast, Flexible RoPE maintains clear step-wise differentiation, stable spatial alignment, and improved visual sharpness throughout the cooking process. RoPE causes the positional encoding across steps to remain entangled in global coordinate frame. RoPE tends to overemphasize absolute positional alignment, which is suboptimal for tasks requiring local step-wise independence. As result, each step image in our generation process is decoded from similar latent spatial origin, leading to visual redundancy and layout collapse. In Fig. 3, Step 2 is visually repeated, and the semantic boundary between steps becomes ambiguous. The second issue is the Attenuation of Long-range Dependencies. As step count grows, the model struggles to maintain semantic consistency, causing blurry or collapsed outputs in later stepsclearly seen at Step 9 in Fig. 3. To address this, we propose step-aware positional encoding that explicitly re-initializes positional indices for each step, enabling the model to better capture both step-wise independence and inter-step coherence. Flexible RoPE. Unlike standard RoPE, which uses globally continuous encoding across all image regions and leads to entangled positional dependencies, our proposed Flexible RoPE assigns an independent positional encoding to each image region. This disentanglement allows the model to clearly differentiate the position and semantics of each step, reducing cross-region interference and preserving generation fidelity even when scaling to many steps. MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. approach consists of two stages: (1) Contextual Step Token Extraction. We encode the entire recipe by concatenating all step instructions and feeding them into T5 encoder, generating unified representation. Since recurring ingredients appear in multiple steps, their token representations inherently share semantic similarities. We then segment the token sequence according to step lengths to extract context-aware tokens for each stepretaining both global context and local specificity. Many recipe steps contain vague descriptions, such as \"Pour the batter in the pan,\" without specifying the exact ingredients (e.g., carrot and zucchini strips). To address this, we introduce the Cooking Agent, based on GPT-4o [1], which supplements missing ingredient details for each step. The Cooking Agent fills in any implicit ingredient information not explicitly mentioned in the recipe text, while ensuring that descriptionssuch as shape and colorremain consistent across steps. This consistency is crucial for maintaining ingredient coherence across images, enabling greater consistency in Contextual Step Token Extraction. As shown in Fig.4 and Supplementary A.3, these step-specific tokens effectively capture ingredient-level consistency, allowing for coherent representation of shared ingredients across steps. (2) ContextAware Fusion via Weighted Averaging. To reinforce ingredient continuity across steps, we combine the globally informed step tokens with the tokens generated from individually encoding each step from SRC. This combination is achieved through weighted averaging approach, which strikes balance between maintaining the unique details of each step and ensuring consistency in ingredient appearance. As result, the model preserves fine-grained detailssuch as the color and shape of ingredientsacross all generated images, while keeping the independence of each steps content intact. The process can be represented as: (cid:164)ğ¶ (ğ‘›) [0 : ğ‘¡ (ğ‘›) ] = ğ¶ (ğ‘›) [0 : ğ‘¡ (ğ‘›) ] + ğœ† ğ¶ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ğ‘’ [ğ‘ (ğ‘›) : ğ‘ (ğ‘›) + ğ‘¡ (ğ‘›) ], (12) where ğ¶ (ğ‘›) represents the tokens obtained by individually decoding the ğ‘›-th step and ğ¶ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ğ‘’ represents the tokens obtained by decoding the entire recipe together. ğ‘ (ğ‘›) denotes the starting position of the ğ‘›-th step in the token sequence ğ¶ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ğ‘’ . ğ‘¡ (ğ‘›) represents the length of the ğ‘›-th step. ğœ† is weight factor that balances the contribution of the recipe-wide context and the individual step decoding."
        },
        {
            "title": "4 Experiment\n4.1 Experiment Settings\nDatasets. We conduct experiments on RecipeGen [46] and VGSI [48].\nDetails on how the two datasets are used can be found in Supple-\nmentary A.1.",
            "content": "Implementation Details. We evaluate our model in both trainingfree and training-based settings. Detail is in Supplementary A.2. Evaluation metrics. We evaluate CookAnything on two datasets: RecipeGen [46] and VGSI-Recipe [41], using the following metrics in Tab.1."
        },
        {
            "title": "4.2 Quantitative Evaluation\nWe evaluate our model against a wide range of baselines, including\nUNet-based methods: StoryDiffusion [51], Stable Diffusion XL\n(SDXL) [26], and StackedDiffusion (SKD) [18]. We also compare\nwith DiT-based models: Flux.1-dev , In-Context LoRA (IC-LoRA)",
            "content": "Table 1: Evaluation metrics for CookAnything. Metric Description Step Flexibility Joint Generation Goal Faithfulness Step Faithfulness Cross-Step Consistency Ingredient Accuracy Usability Indicates whether the model can generate variable number of step images in single pass. Indicates whether the model can generate all step images simultaneously to ensure consistency and efficiency. CLIP [28] similarity between the final image and last-step caption, measuring alignment with the overall goal. Assesses each images alignment with its step caption using CLIP and contextual consistency with the recipe via GPT-4o [1]. Detailed prompt can be found in the Supplementary A.4. Based on StackedDiffusion [18] and DINOv2 [3], uses ğ‘™2 distance and step count difference to assess visual and numerical consistency. Uses GPT-4o and manual inspection to verify whether the expected ingredients are visually present in each step and to check for any omissions. Detailed prompt is in Supplementary A.4. Evaluates spatial alignment in jointly generated images to avoid layout collapse or misalignment, using GPT-4o and human inspection. GPT-4o scores usability across five aspects: size consistency, step clarity, content duplication, process completeness, and step count reasonableness. Full results in Tab. 2, details in Supplementary A.4. [9], Stable Diffusion 3.5 (SD3.5) [35] and Regional Prompt Flux (RPF) [4]. Additionally, we include layout-aware methods such as GLIGEN [13] and Attention Refocusing (A-R) [25]. As shown in Tab. 2, our CookingAnything model achieves state-of-the-art results on the RecipeGen dataset, surpassing all baselines across key metrics. It excels in Goal Faithfulness (GF) and Step Faithfulness (SF), indicating precise visual-step alignment, and achieves the lowest Cross-Step Consistency (CSC), reflecting strong procedural coherence. Additionally, it leads in Ingredient Accuracy (IA) and Usability (UB) under both GPT-based (G) and Human Evaluation (H), demonstrating its ability to capture fine-grained details and deliver user-preferred outputs. We further validate our model on on the VGSI-Recipe dataset using both Training-Free (TF) and Training-Based (TB) settings. As shown in Tab. 3, our method consistently achieves the best performance across all metrics. These results highlight the effectiveness of our approach in producing accurate, and coherent recipe visualizations across diverse scenarios."
        },
        {
            "title": "4.3 Ablation Study\nOur main experiments are conducted in the Training-Based Mode,\nwhile the ablation study on the Training-Free Mode is provided in\nSupplementary Section A.7.",
            "content": "Effectiveness of Cross-Step Consistent Control. We propose the Cross-Step Consistent Control (CSCC) to ensure ingredient CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland Figure 5: Qualitative comparisons. SKD refers to StackedDiffusion, and SD3.5 refers to Stable Diffusion 3.5. Both SD3.5 Flux.1-dev and SKD exhibit issues with ingredient accuracy, discontinuous ingredient shapes, and the generation of incorrect ingredients. In contrast, our model excels in maintaining the shape and continuity of ingredients. consistency across steps. As shown in Tab. 4, removing CSCC leads to notable drop in Cross-Step Consistency (CSC), demonstrating the effectiveness of our design. We also examine the effect of the hyperparameter ğœ† in Equation 5, balancing the regional and contextenhanced prompts. Testing ğœ† values from 0, 0.2, 0.4, 0.6, 0.8, 1 (Fig. ?? (d)-(f)) reveals ğœ† = 0.2 as optimal, so we set ğœ† = 0.2 in this paper. Effectiveness of Flexible RoPE. We evaluate Flexible RoPE by replacing it with the original RoPE. As shown in Tab.4, removing Flexible RoPE causes notable drop in Goal Faithfulness (1.93) and Step Faithfulness (0.98), and introduces visual inconsistencies across steps. Specifically, consecutive images exhibit blurred transitions, making step boundaries harder to distinguish  (Fig.3)  . Effectiveness of Cooking Agent. We remove the Cooking Agent and test on diverse recipe steps, including those with vague or implicit ingredient descriptions. As shown in Tab.4, CookingAnything still outperforms all other models in Tab.2, showing its ability to understand recipe text and perform well even without explicit instructions. Evaluation on Variable-Length Recipes. We evaluate the performance of our model, as well as In-Context LoRA, Regional Prompting FLUX, and Flux.1-dev, on recipes ranging from 3 to 10 steps. Detailed results can be found in Supplementary Section A.7."
        },
        {
            "title": "4.5 User Study on Perceptual Quality\nTo evaluate the perceptual quality of our generated images, we\nconducted a user study involving 53 participants on 75 questions,",
            "content": "MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. Table 2: Comparison with Other Models in RecipeGen. SF means Step Faithfulness, IA means Ingredient Accuracy and UB means Usability. (C) means CLIP score, (G) means GPT-Score and (H) is Human evaluation. The UB metric is applicable only to methods capable of Joint Generation. TF means Traning-Free and TB means Traning-Based. Category Method Step Flexibility Joint Generation Goal Faithfulness Cross-Step Consistency UNet-based DiT-based Layout-aware DiT-based StoryDiffusion [51] SDXL [26] SKD [18] SD3.5 [35] Flux.1-dev [11] IC-LoRA [9] RPF [4] GLIGEN [13] A-R [25] Ours (TF) Ours (TB) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) 17.51 27.46 26.62 27.42 26.47 26.07 27.19 26.99 26.31 30.12 30.59 6.01 2.98 0.7 2.97 3.47 9.03 8.73 2.17 2.46 0. 0.19 SF IA UB (C) (G) (G) (H) (G) (H) 25.54 29.37 28.53 28.77 28.31 26.58 25.99 26.72 27. 5.30 6.79 4.57 6.73 5.31 4.03 4.45 5.17 4.58 7.25 7.51 6.67 7.58 5.93 5.50 7.05 6.16 5.46 3.41 3.71 2.59 3.97 5.71 3.91 3.02 5.28 5. 6.43 5.34 3.89 3.59 4.45 4.24 29.80 30.45 8.52 8. 9.12 9.27 6.92 7.15 9.89 9.70 7.66 8.48 These results highlight the superior visual fidelity and user preference of our approach. Further details are provided in Supplementary A.9. Table 5: Human Evaluation on Perceptual Quality. Method GF SF(C) CSC AQ OA SD3.5 Flux.1-dev 12.08 17. 15.60 16.23 12.70 17.61 13.58 26.34 13.46 16.86 Ours 70. 68.18 69.69 60.38 69."
        },
        {
            "title": "5 Conclusion\nIn this work, we present the CookAnything, a novel framework\nfor flexible, high-fidelity illustrated recipe generation from step-\nwise textual instructions. By integrating Step-wise Regional Control,\nFlexible RoPE, and Cross-Step Consistency Control, our approach\naddresses key limitations of prior methods, achieving accurate\nsemantic alignment, step-wise visual disentanglement, and fine-\ngrained ingredient continuity within a unified generation process.\nExtensive evaluations demonstrate that CookAnything not only\nproduces visually coherent and semantically diverse step images,\nbut also scales effectively to variable-length recipes under both\nTraining-Based and Training-Free settings.",
            "content": "Table 3: Comparison with Other Models in VGSI-Recipe. SF (C) and SF (F) denote CLIP-based and GPT-based Step Faithfulness, respectively. Method GF SF(C) SF(G) CSC SD1.5 [32] SD2.1 [34] SDXL [26] SD3.5 [35] Flux.1-dev [11] IC-LoRA [9] SKD [18] RPF [4] GLIGEN [13] A-R [25] 27.03 27.03 27.78 26.12 25.82 26.18 28.25 28.40 29.70 28.87 26.63 26.63 28.32 27.03 27.77 28.25 28.26 26.54 29.48 28.34 Ours (TF) Ours (TB) 31.22 29.88 29.61 29. 3.41 3.32 3.60 3.43 2.99 3.77 4.22 3.25 5.01 4.32 7.12 6.63 12.28 11.94 11.7 5.27 5.97 4.88 3.14 7.12 4.08 3.94 1.67 2.26 IA 5.32 5.01 5.87 5.17 4.55 5.74 6.35 6.03 6.93 6. UB 6.06 5.75 3.96 8.42 8.07 9.06 7.72 Table 4: Ablation Study. F-RoPE refers to the Flexible RoPE we proposed, CSCC stands for Cross-Step Consistency Control, and C-Agent refers to the Cooking Agent. SF (C) and SF (F) denote CLIP-based and GPT-based Step Faithfulness, respectively. Method GF SF(C) SF(G) CSC w/o F-RoPE w/o CSCC w/o C-Agent 28.66 30.57 29.00 29.33 30.28 29. Ours 30.59 30.45 7.93 8.67 7.84 8.69 0.23 0.29 3. 0.19 assessing five key aspects: Cross-Step Consistency (CSC), Step Faithfulness (SF), Goal Faithfulness (GF), Aesthetic Quality (AQ), and Overall Appeal (OA). As summarized in Tab. 5, our method consistently surpasses Stable Diffusion 3.5 and Flux.1-dev across all metrics, achieving significantly higher Aesthetic Quality score of 60.38. CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland Acknowledgments This work was funded by the National Natural Science Foundation of China (Grant No. 62406126) and the Scientific Research Project of the Education Department of Jilin Province (Grant No. JJKH20250119KJ). References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101mining discriminative components with random forests. In Proceedings of the European Conference on Computer Vision. Springer, 446461. [3] Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision. 96509660. [4] Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang. 2024. Training-free regional prompting for diffusion transformers. arXiv preprint arXiv:2411.02395 (2024). [5] Jingjing Chen and Chong-Wah Ngo. 2016. Deep-based ingredient recognition for cooking recipe retrieval. In Proceedings of the 24th ACM international conference on Multimedia. 3241. [6] Mengmeng Ge, Xu Jia, Takashi Isobe, Xiaomin Li, Qinghe Wang, Jing Mu, Dong Zhou, Li Wang, Huchuan Lu, Lu Tian, et al. 2024. Customizing text-to-image generation with inverted interaction. In Proceedings of the 32nd ACM International Conference on Multimedia. 1090110909. [7] Yinxuan Gui, Bin Zhu, Jingjing Chen, Chong Wah Ngo, and Yu-Gang Jiang. 2024. Navigating weight prediction with diet diary. In Proceedings of the 32nd ACM International Conference on Multimedia. 127136. [8] Fangda Han, Ricardo Guerrero, and Vladimir Pavlovic. 2020. CookGAN: Meal image synthesis from ingredients. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 14501458. [9] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. 2024. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775 (2024). [10] Shuqiang Jiang, Weiqing Min, Linhu Liu, and Zhengdong Luo. 2020. Multi-Scale Multi-View Deep Feature Aggregation for Food Recognition. IEEE Transactions on Image Processing 29 (2020), 265276. doi:10.1109/TIP.2019.2929447 [11] Black Forest Labs. 2024. Flux.1 AI. https://flux1ai.com/. Accessed: 2025-04-04. [12] Xing Lan, Jiayi Lyu, Hanyu Jiang, Kun Dong, Zehai Niu, Yi Zhang, and Jian Xue. 2023. Foodsam: Any food segmentation. IEEE Transactions on Multimedia (2023). [13] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2023. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2251122521. [14] Zhiming Liu, Kai Niu, and Zhiqiang He. 2023. ML-CookGAN: Multi-label generative adversarial network for food image generation. ACM Transactions on Multimedia Computing, Communications and Applications 19, 2s (2023), 121. [15] Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2022. Storydall-e: Adapting pretrained text-to-image transformers for story continuation. In European conference on computer vision. Springer, 7087. [16] Jonathan Malmaud, Earl Wagner, Nancy Chang, and Kevin Murphy. 2014. Cooking with semantics. In Proceedings of the ACL 2014 Workshop on Semantic Parsing. 3338. [17] Javier MarÄ±n, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Antonio Torralba. 2021. Recipe1m+: dataset for learning cross-modal embeddings for cooking recipes and food images. IEEE Transactions on Pattern Analysis and Machine Intelligence 43, 1 (2021), 187203. [18] Sachit Menon, Ishan Misra, and Rohit Girdhar. 2024. Generating Illustrated Instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 62746284. [19] Weiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei, Xiaolin Wei, and Shuqiang Jiang. 2020. Isia food-500: dataset for large-scale food recognition via stacked global-local attention network. In Proceedings of the 28th ACM International Conference on Multimedia. 393401. [20] Weiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo, Liping Kang, Xiaoming Wei, Xiaolin Wei, and Shuqiang Jiang. 2023. Large scale visual food recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 8 (2023), 9932 9949. [21] Liang-Ming Pan, Jingjing Chen, Jianlong Wu, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Yugang Jiang, and Tat-Seng Chua. 2020. Multi-modal cooking workflow construction for food recipes. In Proceedings of the 28th ACM International Conference on Multimedia. 11321141. [22] Liang-Ming Pan, Jingjing Chen, Jianlong Wu, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Yugang Jiang, and Tat-Seng Chua. 2020. Multi-modal cooking workflow construction for food recipes. In Proceedings of the 28th ACM International Conference on Multimedia. 11321141. [23] Siyuan Pan, Ling Dai, Xuhong Hou, Huating Li, and Bin Sheng. 2020. ChefGAN: Food image generation from recipes. In Proceedings of the 28th ACM International Conference on Multimedia. 42444252. [24] William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision. 41954205. [25] Quynh Phung, Songwei Ge, and Jia-Bin Huang. 2024. Grounded text-to-image synthesis with attention refocusing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 79327942. [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023). [27] Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-Seng Chua. 2023. Layoutllm-t2i: Eliciting layout guidance from llm for text-to-image generation. In Proceedings of the 31st ACM International Conference on Multimedia. 643654. [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 87488763. [29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 167. [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer, 234241. [34] Stability AI. 2022. Stable Diffusion v2.1 Model Card. https://huggingface.co/ stabilityai/stable-diffusion-2-1. Accessed: 2025-04-11. [35] Stability AI. 2024. Stable Diffusion 3.5 Large Model Card. https://huggingface. co/stabilityai/stable-diffusion-3.5-large. Accessed: 2025-04-11. [36] Yu Sugiyama and Keiji Yanai. 2021. Cross-modal recipe embeddings by disentangling recipe contents and dish styles. In Proceedings of the 29th ACM International Conference on Multimedia. 25012509. [37] Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait, Tobias Weyand, and Jack Sim. 2021. Nutrition5k: Towards automatic nutritional understanding of generic food. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 89038911. [38] Liangyu Wang, Yoko Yamakata, Ryoma Maeda, and Kiyoharu Aizawa. 2024. Measure and Improve Your Food: Ingredient Estimation Based Nutrition Calculator. In Proceedings of the 32nd ACM International Conference on Multimedia. 1127311275. [39] Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven CH Hoi, and Qianru Sun. 2021. large-scale benchmark for food image segmentation. In Proceedings of the 29th ACM international conference on multimedia. 506515. [40] Yoko Yamakata, Akihisa Ishino, Akiko Sunto, Sosuke Amano, and Kiyoharu Aizawa. 2022. Recipe-oriented food logging for nutritional management. In Proceedings of the 30th ACM International Conference on Multimedia. 68986904. [41] Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. 2021. Visual Goal-Step Inference using wikiHow. arXiv preprint arXiv:2104.05845 (2021). [42] Yuehao Yin, Huiyan Qi, Bin Zhu, Jingjing Chen, Yu-Gang Jiang, and Chong-Wah Ngo. 2023. Foodlmm: versatile food assistant using large multi-modal model. arXiv preprint arXiv:2312.14991 (2023). [43] Takuya Yonezawa, Yuanyuan Wang, Yukiko Kawai, and Kazutoshi Sumiya. 2019. cooking support system by extracting difficult scenes for cooking operations from recipe short videos. In Proceedings of the 27th ACM International Conference on Multimedia. 22252227. [44] Ruoxuan Zhang, Dantong Ouyang, Lili He, Lingjin Kuang, and Hongtao Bai. 2024. Recognize after early fusion: the Chinese food recognition based on the alignment of image and ingredients. Multimedia Systems 30, 2 (2024), 93. MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. [45] Ruoxuan Zhang, Dantong Ouyang, Ximing Li, Hongtao Bai, Chenming Zhang, and Lili He. 2025. Learning multi-scale features automatically from food and ingredients. Multimedia Systems 31, 3 (2025), 111. [46] Ruoxuan Zhang, Hongxia Xie, Yi Yao, Jian-Yu Jiang-Lin, Bin Wen, Ling Lo, HongHan Shuai, Yung-Hui Li, and Wen-Huang Cheng. 2025. RecipeGen: Benchmark for Real-World Recipe Image Generation. arXiv preprint arXiv:2503.05228 (2025). [47] Yixin Zhang, Yoko Yamakata, and Keishi Tajima. 2022. Miais: multimedia recipe dataset with ingredient annotation at each instructional step. In Proceedings of the 1st International Workshop on Multimedia for Cooking, Eating, and related APPlications. 4952. [48] Luowei Zhou, Chenliang Xu, and Jason Corso. 2018. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32. [49] Pengfei Zhou, Weiqing Min, Chaoran Fu, Ying Jin, Mingyu Huang, Xiangyang Li, Shuhuan Mei, and Shuqiang Jiang. 2024. FoodSky: Food-oriented Large Language Model that Passes the Chef and Dietetic Examination. arXiv preprint arXiv:2406.10261 (2024). [50] Pengfei Zhou, Weiqing Min, Yang Zhang, Jiajun Song, Ying Jin, and Shuqiang Jiang. 2023. SeeDS: Semantic separable diffusion synthesizer for zero-shot food detection. In Proceedings of the 31st ACM International Conference on Multimedia. 81578166. [51] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. 2024. Storydiffusion: Consistent self-attention for long-range image and video generation. Advances in Neural Information Processing Systems 37 (2024), 110315 110340. CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland"
        },
        {
            "title": "6.1 Datasets Details\nIn our experiments, we utilize two datasets: RecipeGen [46] and\nVGSI [48]. To the best of our knowledge, RecipeGen represents\nthe first and currently the only large-scale dataset specifically con-\nstructed for the task of recipe image generation. It spans a wide\nrange of cooking types and regional cuisines, including both liquid-\nbased recipes and solid dishes. The number of steps per recipe is\nwidely distributed (ranging from 2 to 15), which facilitates flexible\nmulti-step image generation. The dataset consists of 21,944 recipes.\nFor our experiments, we randomly sample 5,000 recipes from the\ntraining split for model training, and we use the official test split,\ncomprising 4,389 recipes, for evaluation.",
            "content": "VGSI is visual goal-step instruction dataset collected from WikiHow, where recipe-related samples represent only small subset of the overall data. Compared to RecipeGen, VGSI encompasses fewer cooking types and exhibits more limited visual diversity, with number of samples presented in comic style. To focus on reciperelated content, we filter VGSI by the keyword cook, resulting in 1,157 recipes with total of 6,417 images. We then apply an 85:15 split, randomly selecting 173 recipes as the test set."
        },
        {
            "title": "6.2 Implementation Details\nWe evaluate our model under two distinct settings: training-free\nand training-based. In the training-based framework, individual\nstep images are standardized to a resolution of 512Ã—512 pixels.\nThese images are then concatenated vertically to yield a complete\nmulti-step visual sequence, ensuring spatial consistency across the\nrecipeâ€™s steps.",
            "content": "For the textual input, we incorporate the entire recipe text. To improve the models comprehension, we generate concise recipe summary using GPT-4o, which is prepended to the full text. Moreover, each step description is explicitly marked with prefix in the format [step-i], denoting its corresponding step number. This results in final input structure consisting of the GPT-4o-produced summary followed by all step-wise instructions. Training is carried out on single A100 GPU for 20,000 training steps, using batch size of 2. We train on both the VGSI-recipe and RecipeGen datasets. Our implementation is built upon the Flux.1-dev variant, which consists of 19 DoubleStreamBlocks and 38 SingleStreamBlocks, aggregating to approximately 12 billion parameters. We apply LoRA with rank of 16 to adapt the model, and optimization is performed using the Adam optimizer with Table 6: Experiments in Contextual Tokens. Goal Faithfulness Step Faithfulness Cross-Step Consistency 29.91 28.70 1. Table 7: Detailed result in Usability. Method ISC CSR DIC PCL RNS IC-LoRA SKD RPF Ours (TF) Ours (TB) 1.98 2.00 1.75 1.99 1.97 0.86 1.07 0.52 1.98 1.96 0.92 1.29 0.63 1.97 1. 0.79 1.04 0.50 1.98 1.96 0.78 1.03 0.50 1.97 1.96 an initial learning rate of 0.0001. We set ğ›¼ = 0.1 and ğœ† = 0.1 in CookAnything."
        },
        {
            "title": "6.4 More details about Step Faithfulness,",
            "content": "Ingredient Accuracy, and Usability In addition to using CLIP to compute Step Faithfulness, we also employ GPT-4o to assess the semantic alignment between the generated image and the original recipe text. Specifically, we design five-level scoring system to evaluate whether the ingredient shapes, containers, and states depicted in the image accurately correspond to those described in the respective recipe step. The prompt for GPT-4o is in Fig.9. . Ingredient Accuracy evaluates whether each step includes the correct and relevant ingredients. Existing models often suffer from ingredient omission, confusion, or hallucination, especially in multi-step cooking processes where ingredients appear, transform, or disappear over time. To address this, we assess both the presence and correctness of visible ingredients at each step, using combination of GPT-based scoring and human verification. The prompt used for Ingredient Accuracy is in Fig.10. As shown in Fig.11, we evaluate the Usability based on five key aspects. First, Image Size Consistency (ISC) ensures that the dimensions of all sub-images are uniform, with no issues such as incorrect cropping or inconsistent sizes that could hinder understanding. Second, Clarity of Step Representation (CSR) assesses whether each sub-image clearly represents distinct cooking step and aligns with specific step in the recipe, making it easy for users to follow the process. Third, Duplication of Image Content (DIC) checks for repetition in image content, such as identical MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. perspectives or similar compositions, ensuring that the images remain diverse and informative. Fourth, Process Completeness and Logic (PCL) evaluates whether the image sequence accurately shows the entire recipe processfrom preparation to the finished productand whether the sequence logically matches the recipe text. Finally, Reasonableness of Number of Steps (RNS) verifies whether the number of sub-images aligns with the number of steps in the recipe, ensuring the image sequence is neither too sparse nor overloaded. These criteria together ensure that the generated images effectively represent the cooking process, are logically structured, and provide clear guidance for the user. The detailed result is in Tab.7. CookAnything performs the best on CSR, DIC, PCL, and RNS. However, the SKD model performs well in Image Size Consistency because it is fixed to output only six images, which allows for consistent cropping and uniform image sizes. Despite this, its output does not align with the number of steps in the text. Details about Human Evaluation in Ingredient Accuracy, and Usability. We conducted an evaluation on RecipeGen, where we selected 780 recipes and over 5000 images from 12 models for assessing ingredient accuracy. These were rated by 13 evaluators. Additionally, for usability, we chose 1300 recipes from four models to conduct comprehensive review. The evaluation focused on various aspects of usability, such as the clarity of step representation, image consistency, and the logical flow of the recipe process. This thorough testing across both ingredient accuracy and usability helps ensure that our models not only generate correct ingredient representations but also provide seamless user experience in terms of clarity and coherence."
        },
        {
            "title": "Paper",
            "content": "There are prompts for examples in Fig.1 of main paper: Example 1: <Goal>: Broccoli Egg Salad. <Step-1> Blanching broccoli: Boil water in pot, add some salt and cooking oil, and blanch the cleaned broccoli until cooked, then remove and set aside. <Step-2> Preparing ingredients: Add the cut pieces of boiled eggs. <Step-3> Final dish: Finished dish. Example 2: <Goal>: Hamburger. <Step-1>: Knead the dough: Mix all the ingredients except the butter and sesame seeds, and slowly add warm water while stirring with chopsticks into flocculent state, then knead by hand to form dough. Once it reaches the initial expansion stage, add the butter and continue kneading until the dough forms glove film. <Step-2> Treat the dough: Deflate the fermented dough, evenly divide it into 4 portions, cover with cling film, and let rest for 10 minutes. Round the rested dough, brush with water, coat with sesame seeds, and let it ferment in the oven for another 30 minutes. <Step-3> Bake the dough: Place the fermented dough into preheated oven at 160C upper tube and 140C lower tube, bake for about 16 minutes until done. The burger buns are now ready. <Step-4> Pan-fry patties and eggs: Shape the mixed chicken into patties, pan-fry until golden brown on both sides, then fry the eggs until done and set aside. <Step-5> Assemble the burger: Slice the burger bun horizontally in two, place lettuce leaf and chicken patty on one half, squeeze little ketchup on the patty, then add cheese, egg, bell pepper rings, and top with the other bun half. Example 3: <Goal>: Mexican Chicken Wrap. <Step-1>: Prepare the tortilla: used semi-finished tortillas, slightly fry them or microwave for one minute. <Step-2>: Prepare the vegetables: Get lettuce and carrots ready. <Step-3>: Fry chicken breast: Fry until golden brown (double fry for crispier texture). <Step-4> Lay out tortilla: Place lettuce leaves and carrots on one side of the tortilla. <Step-5>Add chicken breast: Add the fried chicken breast to the tortilla. <Step-6> Roll the tortilla: Roll up the tortilla. Cut the tortilla in half, and the Mexican chicken wrap is ready. Example 4: <Goal>: Tomato and Scrambled Egg Rice. <Step-1> Prepare ingredients: Rinse the rice and cook it in rice cooker; beat eggs with pinch of salt. <Step-2> Cook the eggs: Heat oil in pan, pour in the beaten eggs, quickly stir to scramble, and set aside. <Step-3> Handle tomatoes: Make cross cuts on tomatoes, blanch in hot water to remove the skins, and cut into pieces. <Step-4> SautÃ© garlic and scallions: Heat oil in pan, sautÃ© garlic slices and white part of scallions on low heat until fragrant. <Step-5> Cook tomatoes: Add tomatoes, stir-fry until juicy, then add sugar and salt, and stir evenly. <Step-6> Combine ingredients and reduce the sauce: Add the scrambled eggs and cook until the sauce thickens, adding bit of water if necessary. <Step-7> Finish the dish: Pour the cooked tomato and eggs mixture over the steamed rice, and sprinkle with chopped scallions. Example 5: <Goal>: Dried Fruit Pound Cake. <Step-1> Prepare ingredients: Prepare the ingredients. <Step-2> Dried fruits: Process and cut the dried fruits into small pieces. <Step-3> Melt and mix butter: Melt butter over water bath, beat with whisk at low speed until smooth. Add salt and powdered sugar, mix briefly, then beat at low speed until combined. <Step-4> Add egg mixture: Beat the eggs. Gradually add the beaten eggs to the butter in three portions, mixing well at low speed each time. <Step-5> Mix flour and dried fruits: Sift in the mixture of low-gluten flour, baking powder, and almond flour. Fold with spatula until its smooth. Then fold in the dried fruits. <Step-6> Prepare for baking: Line the mold with parchment paper, and preheat the oven to 135C. Pour the cake batter into the mold, smooth the surface, and tap the mold lightly to remove air bubbles. <Step-7> Bake the cake: Place the mold in the oven at 135C on the middle-lower rack for 30 minutes. Insert toothpick into the cake; if it comes out clean, the cake is done. <Step8> Cool and cut: Take the cake out of the oven, with beautiful golden color. Let it cool slightly and then cut into pieces. There are example for fig.10 of main paper: Example (a): <Goal>: How to Eat Kimchi. <Step-1> Eat kimchi out of the jar for an effortless snack.<Step-2> Serve individual CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland often pose challenges due to their fluid textures and fine-grained visual details."
        },
        {
            "title": "6.10 Future Work\nBeyond cooking, our method lays a foundation for structured visual\ngeneration in broader procedural domains such as instructional\nmanuals, scientific workflows, and educational storytelling. Future\nwork will explore extending our framework to multimodal video\ngeneration, interactive editing, and alignment with real-world cook-\ning data.",
            "content": "Table 8: Effect of ğœ† on Generation Performance."
        },
        {
            "title": "Goal\nFaithfulness",
            "content": "Step Faithfulness Cross-Step Consistency 29.99 30.12 29.99 29.78 29.39 29.18 29.79 29.80 29.70 29.43 29.01 28. 0.19 0.17 0.3 0.81 1.66 3.15 ğœ† 0 0.2 0.4 0.6 0.8 1.0 pieces of kimchi with toothpicks to easily share it. <Step-3> You can eat kimchi straight out of the fridge, or you can throw it in small skillet and heat it up with 1 US tbsp (15 mL) of vegetable oil. Example (b): <Goal>: How to Use Emu Oil for Health and Skin Benefits. <Step-1> Place small dab of the oil on the palm of your hand or on the affected area and rub it in until its clear. Within short amount of time, you should feel relief and notice swelling go down. You may apply emu oil when your back or neck feels swollen or sore. Emu oil can be purchased online or in your local pharmacy. Use the oil once or twice day. <Step-2> Emu oil has slight pain-killing effect when applied to the skin, so rub it onto scrape or bruise to reduce your pain once day. The antioxidants found in the oil can also help prevent additional damage or further infection. Seek medical help if you have large, deep cuts. <Step-3> Gently rub the area with the emu oil once day until its completely absorbed by your skin. The oil will reach deep into your skin and alleviate the pain quickly while the sunburn heals. Speak with your doctor or dermatologist to determine if emu oil is good option for you. Have friend help you get the oil on hard-to-reach areas such as your back. You can also use emu oil as natural sunscreen. Apply the oil as you would with regular sunscreen. Example (c): <Goal>:How can repair peeling for faux leather sofa with vinyl repair kit? <Step-1>: Mix paint colors until they match the sofa. <Step-2>: Brush paint onto the affected area. <Step3>: Apply texture relief paper to the paint, if desired. <Step-4>: Use heat tool with the paper for 2 minutes and finish.The sofa looks as good as new."
        },
        {
            "title": "6.8 More Visualization Result\nIn this section, we visualize results across three major food cate-\ngories. First, beyond the Asian and Western dishes already shown in\nFig.1, 3, 4, and 6 of the main paper, we present additional examples\nof diverse regional cuisines in Fig.6, demonstrating the modelâ€™s\nadaptability to various cultural food styles. Second, we showcase\nour modelâ€™s performance on liquid-based dishes in Fig.7, which",
            "content": "MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. Figure 6: Visualization of dishes from different regions. CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland Figure 7: Visualization about liquid. Figure 8: Prompt for refining recipe caption, with GPT-4o MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. Figure 9: Prompt to measure the Step Faithfulness of the generated image, with GPT-4o CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland Figure 10: Prompt to measure the Ingredient Accuracy of the generated image, with GPT-4o MM 25, October 2731, 2025, Dublin, Ireland Ruoxuan Zhang et al. Figure 11: Prompt to measure the Usability of the generated image, with GPT-4o CookAnything: Framework for Flexible and Consistent Multi-Step Recipe Image Generation MM 25, October 2731, 2025, Dublin, Ireland Figure 12: Template for Human Study."
        }
    ],
    "affiliations": [
        "Jilin University",
        "National Taiwan University",
        "National Yang Ming Chiao Tung University"
    ]
}