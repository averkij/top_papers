{
    "paper_title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks",
    "authors": [
        "Zonglin Wu",
        "Yule Xue",
        "Xin Wei",
        "Yiren Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 2 8 9 5 0 . 6 0 5 2 : r MCA-Bench: Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks Zonglin Wu1 Yule Xue1 Xin Wei1 Yiren Song2 1Southwest University, 2National University of Singapore Abstract As automated attack techniques rapidly advance, CAPTCHAs remain critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass diverse range of modalitiesfrom static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questionsyet the community still lacks unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into single evaluation protocol. Leveraging shared visionlanguage model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, andcruciallyoffers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets are available at https://www.kaggle.com/datasets/luffy798/mca-benchmultimodal-captchas. Code is released at https://github.com/noheadwuzonglin/MCA-Bench."
        },
        {
            "title": "Introduction",
            "content": "CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) has long played vital role in protecting online services from automated attacks. In recent years, growing research interest in AI security topics such as adversarial attacks [65, 61, 62] and digital watermarking [11, 10, 37, 72] has also drawn attention to the vulnerability of CAPTCHA systems. However, with rapid advances in deep learning, computer vision, and multimodal pretraining models, many CAPTCHA types once considered secure have now become susceptible to machine-learning-based attacks[7, 60, 73]. Techniques such as GANs, vision-language models (VLMs), and reinforcement learning have enabled attackers to mimic human behavior with increasing precision[22, 47, 34, 55]. As result, researchers have begun developing multimodal CAPTCHA datasets and evaluation frameworks to assess model performance across various CAPTCHA types[3, 4, 19]. This makes it vital to reassess CAPTCHAs real-world security to ensure trustworthy Internet service ecosystem[44, 19]. Despite the emergence of new interactive CAPTCHA forms, existing studies often focus on specific categories (e.g., text recognition, image selection) without offering comprehensive comparisons[7, 13, 25]. Moreover, there is no large-scale, multimodal benchmark for evaluating CAPTCHA designs against intelligent-agent attacks[27, 54, 12]. This gap limits the identification of vulnerabilities and Correspondence to: songyiren725@gmail.com. Preprint. Under review. Figure 1: Data samples from MCA-Bench includes four categories and 20 sub-clusters of Pointand-Click Localization, Static Visual Recognition, Textual Logic Q&A and Interactive Manipulation. the development of more robust CAPTCHA systems. Therefore, unified, standardized evaluation platform is urgently needed for security analysis across modalities and tasks. MCA-Bench is the first end-to-end CAPTCHA security benchmark spanning four modalitiesstatic visual recognition, point-and-click localization, interactive manipulation and textual logic Q&Aacross twenty real-world tasks. It provides over 180000 training samples and 4000item test set (200 examples per task), organized into four clusters that respectively evaluate OCR robustness to visual noise, target retrieval in complex scenes, human-like interaction behaviors, and multi-step language reasoning. Representative samples from the MCA-Bench dataset are shown in Figure 1. We use QwenVL-2.5-7B as the vision-language backbone, fine-tuned with LoRA adapters for each task. Training for static and logic CAPTCHAs is supervised with target labels, while for interactive tasks, human demonstration data is used for behavior cloning. specially designed JSON protocol facilitates large-scale evaluation and integration. The evaluation framework uses pass rates as the core metric. Experiments show that multimodal VLMs achieve over 96% accuracy on simple tasks, while more complex ones (requiring physical interaction or multi-step logic) still pose significant challenges, with success rates as low as 2.5%. This highlights the coupling of visual confusion, depth of interaction, and semantic complexity with attack difficulty, providing actionable insights for future CAPTCHA design. MCA-Bench is released open-source, enabling easy replication and fostering ongoing cycles of attack, defense, and benchmark iteration. The main contributions are as follows. MCA-Bench: the first large-scale, cross-modal CAPTCHA attack benchmark with 20 real tasks. Proposed unified evaluation pipeline with single pass-rate metric and open source scripts. First full-scale CAPTCHA security assessment with guidance for humanmachine verification."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 VLMs in Structured Image Tasks Visual Language Models (VLMs) bridge images and text through unified embeddings, with early methods like CLIP [47] and ALIGN [29] excelling in simple visual tasks but falling short in complex reasoning. Recent models integrate large language models (LLMs) with visual encoders to enhance cross-modal understandingBLIP-2 [34] uses frozen components for efficient training, while MiniGPT-4 [74] maps visual features to LLMs for stronger image-text QA. As research shifts toward structured tasks like tables and documents, which resemble CAPTCHA images with dense 2 Table 1: Overview of CAPTCHA datasets for various task types. The dataset includes multiple task categories, such as static visual recognition, point-and-click localization, interactive manipulation, and textual logic Q&A tasks, with varying image sizes and training samples to support diverse CAPTCHA research. Type Dataset Image size #Images #Train Original image Single image Static visual recognition Point-and-click localization Classic character CAPTCHA Distorted-word CAPTCHA Arithmetic-character CAPTCHA 33 grid selection Inverted-letter selection Geometric-shape recognition Brightness discrimination Hollow-pattern recognition Sequential-letter ordering Full-image grid selection Color discrimination Vowel selection Arithmetic-selection Rotated-letter selection 33 jigsaw-swap selection 16060 5000 800 5000 13048 5000 672672 224224 5500 5000 224 5898 224224 5377 224224 5000 300 4999 512512 5015 224224 224224 240240 224224 5000 5322 5065 5015 224 6505 Interactive manipulation Textual logic Q&A Sliding-block Rotation-block Alignment sliders 600360 600360 600360 Text-based arithmetic Commonsense reasoning 5012 5041 5006 1000 1000 4800 4800 4800 5300 5698 5177 4800 4799 4815 4800 5122 4865 6305 4812 4841 4806 800 800 text and noise, models such as Qwen-VL [68] improve encoder design and pretraining for better layout and multilingual comprehension. Architectures like LLaVA [35] and MiniGPT-4 [74] adopt unified visual projection and instruction-tuning framework, achieving robustness on distorted and occluded inputs. However, most VLMs lack CAPTCHA-specific adaptation, limiting their accuracy and generalization. 2.2 Evolution of Intelligent Agents Toward AGI An agent in AI refers to an autonomous entity that perceives its environment via sensors, makes decisions, and acts through actuators [43], characterized by autonomy, reactivity, and social interactivitykey traits driving progress toward Artificial General Intelligence (AGI) [57, 28]. Agent research has evolved from early symbolic agents with interpretable rule-based reasoning [41, 20], yet limited in handling uncertainty and scalability [50], to reactive agents offering fast perception-action loops [53], albeit with limited decision-making. The rise of reinforcement learning (RL) enabled agents like AlphaGo [58] to learn from interactions, though challenges like sample inefficiency and poor generalization remained [49, 31, 24, 1]. Transfer learning and meta-learning improved task adaptation via knowledge reuse and learning-to-learn paradigms [6, 45, 75, 14, 18, 23, 48, 16], despite issues in task diversity and pre-training costs [17]. Recently, LLM-based agents have transformed the field, exhibiting emergent reasoning and planning abilities[40, 2, 70, 39], multimodal understanding (e.g., BLIP-2 [33]), dynamic task decomposition (e.g., Voyager [66]), and tool usage (e.g., Toolformer [52]), 3 Table 2: Comparison of pass rates for various CAPTCHA types. This table presents the pass rates for different CAPTCHA task types, comparing the performance of the QwenVL-2.5-7B-Instruct model with human pass rates. CAPTCHA Type QwenVL-2.5-7B-Instruct Pass Rate Human Pass Rate 33 grid selection Inverted-letter selection Geometric-shape recognition Brightness discrimination Hollow-pattern recognition Sequential-letter ordering Full-image grid selection Color discrimination Vowel selection Arithmetic selection Rotated-letter selection 33 jigsaw-swap selection Classic character CAPTCHA Distorted-word CAPTCHA Arithmetic-character CAPTCHA Text-based arithmetic Commonsense reasoning Sliding-block Rotation-block Alignment sliders Point-and-Click Localization Static Visual Recognition Textual Logic Q&A Interactive Manipulation 0.96 0.52 0.96 0.665 0.995 0.925 0.35 0.99 0.975 0.025 0.335 0.96 0.32 0.985 0.695 0.985 0.97 0.36 0.28 0. 0.88 0.94 0.98 0.78 0.98 0.98 0.74 0.88 0.80 0.78 0.74 0.80 0.92 0.84 0.98 0.97 0.86 0.74 0.76 0.72 enabling general-purpose intelligence [46, 36, 63, 67]. Their capabilities in zero-shot generalization [69, 51, 9] and social collaboration[26] mark paradigm shift in agent research. 2.3 Advances and Challenges in CAPTCHA Security Early text CAPTCHAs used heavy distortion and noise, but segmentation flaws and CNN-based attacks soon achieved high recognition rateseven breaking reCAPTCHA under laboratory conditions [8, 59, 7]. Image-classification schemes such as ASIRRA shifted effort to visual cognition, yet SVMs trained on public datasets ultimately compromised them [15, 21]. Contemporary defences leverage deep object-detection networks alongside generative techniques (e.g., diffusion models and style-transfer) to create more diverse and intricate puzzles [42, 71, 30, 13, 22, 38]. Interactive CAPTCHAs, exemplified by reCAPTCHA v2, further blend visual tasks with user gestures such as drag-and-drop or click behaviour to raise the bar for automation [56]. Despite these advances, CAPTCHA schemes still struggle with adaptive difficulty tuning, behaviour consistency across devices, adversarial robustness, andcriticallythe absence of standardized, reproducible benchmarks for end-to-end evaluation[32, 5]."
        },
        {
            "title": "3 Dataset Construction Process\nThis section outlines the MCA Bench pipeline—from raw sample collection to final benchmark\nrelease—while detailing the data-processing and annotation strategies for its four task clusters and\ntwenty subtasks. The complete workflow appears in Figure 2.",
            "content": "3.1 Data Collection Sources and Compliance Strategies In developing the MCA-Bench benchmark suite, we implemented comprehensive data collection and compliance management system to uphold scientific integrity and ensure legal and ethical standards in evaluating CAPTCHA security. Our approach combines independently created data and reused public datasets. For the former, we designed diverse CAPTCHA dataset, including distorted text, obfuscated graphics, interactive tasks, and puzzles, based on thorough analysis of technology trends and security needs. These designs are original, validated, and proprietary to our team. For reused datasets, we carefully selected authorized academic, open-source, and industry resources, adhering to licensing agreements and privacy regulations. We ensured compliance through preprocessing steps like anonymization and mitigating privacy risks. Additionally, we established 4 Figure 2: Schematic overview of the MCA-Bench data-acquisition and annotation workflow. The four grey panelsfrom left to rightrepresent the dataset categories Static Visual Recognition, Interactive Manipulation, Point-and-Click Localization, and Textual Logic Q&A. The red labels (SVT, MIM-RU, CPTL, and LLM-GFP) denote the dedicated data-collection pipelines associated with each category. Each pipeline follows unified four-stage paradigm: (i) determining the raw input format; (ii) applying task-specific geometric transformations, coordinate projections, or prompt-template generation; (iii) separating fine-grained annotation types; and (iv) saving the annotations to text files. multi-level review system with legal and ethics oversight, encrypted storage, and tiered access control to guarantee ongoing compliance and data security for MCA-Bench. 3.2 Data Collection and Processing 3.2.1 Text-based Task Data Collection For text-based CAPTCHA tasks, we developed semi-automated LLM-driven pipeline to efficiently generate and filter math and commonsense questions, reducing manual effort while maintaining quality and diversity. By using adaptive prompts, we guided the Qwen LLM to create structured, semantically relevant question-answer pairs, incorporating knowledge constraints, task-guided sampling, and type-control parameters for difficulty and logic. The output was further refined through manual filtering with custom evaluation protocol, assessing grammar, reasoning, and ambiguity to ensure robustness and clarity for real-world use. 3.2.2 Click-based Coordinate Task Data Collection We propose CPTL (Click-based Positioning and Target Localization), multimodal benchmark designed to evaluate models spatial localization accuracy and image-language instruction alignment across varying scenario complexities. The framework consists of two main tasks: Free-form Pointing and Grid-based Discrete Pointing. In the Free-form Pointing task, we use highly controllable image generation process combining procedural background perturbations with public visual datasets, creating images layered with semantic content, based on Flickr scenic photos. The Grid-based Discrete Pointing task employs standardized 33 grid layout to assess model decision-making within constrained regions, dividing each image into 9 segments (0 to 8) to form discrete click space. The image content is derived from 64-class animal classification dataset[64], Flickr scenic backgrounds, Google ReCaptcha V2 challenges, and manually designed targets. CPTL supports systematic, adjustable evaluation of spatial complexity and instruction modalities, enabling isolated assessment of spatial parsing and language comprehension. Its key advantage lies in simulating real-world CAPTCHA click scenarios while maintaining robust model performance under noisy backgrounds, multi-target interference, and weak semantic prompts. Experimental results demonstrate that multimodal models fine-tuned with CPTL exhibit strong localization and semantic understanding, providing reliable evaluation baseline and optimization direction for future CAPTCHA security research. 5 3.2.3 Static Visual Recognition Task Data Collection We propose SVT (Static Visual Textual Understanding), framework designed to evaluate multimodal models ability to understand and reconstruct text from distorted images. SVT combines procedural image generation with language modeling constraints, allowing high controllability and complexity. It uses character-level geometric perturbations (e.g., affine transformations, warping, and perspective distortion) to generate challenging yet recognizable character patterns, while stochastic interference generator adds random noise and occlusions. SVT effectively tests models character-level attention and resilience to structural noise, providing comprehensive evaluation from visual perception to symbolic reasoning. Results show that SVT reveals bottlenecks in fine-grained character perception and serves as valuable benchmark for assessing the robustness of pretrained multimodal models. 3.2.4 Interactive Behavior Task Data Collection We present multimodal interaction modeling framework for interactive CAPTCHA tasks, based on real user operations, to enhance vision-language models dynamic understanding capabilities. Focusing on continuous motion interactionssuch as sliding alignment, rotation calibration, and trajectory restorationthe framework requires models to perceive spatial structures, motion directions, and behavioral patterns. Data is collected through large-scale manual sampling across devices, capturing thousands of diverse user trajectories with timestamps and velocity, enabling realistic spatiotemporal behavior modeling. Tasks are designed around target state restoration, formalized into standardized sub-tasks with defined start, goal, and intermediate states. Interaction trajectories are serialized into behavioral vector streams and jointly encoded with image content to guide visual-behavior learning. Fine-tuning with high-quality samples improves the models decision consistency and action alignment. To evaluate performance, we introduce metrics including Center Deviation Error, Angular Restoration Accuracy, Slide Path Alignment Rate, and Motion Variability Index, offering comprehensive assessment beyond traditional image recognition benchmarks. 3.3 Data Annotation Strategy To ensure consistent training and evaluation of multimodal CAPTCHA tasks, we developed taskdriven annotation framework with standardized labeling for four representative task types. 3.3.1 Text-based Tasks: Semantic Consistency Annotation Based on Q&A Structure We propose semi-automated, LLM-driven pipeline for generating and filtering math and commonsense questions in text-based CAPTCHA tasks, significantly reducing manual workload while maintaining question quality and diversity. An adaptive prompt template guides the Qwen LLM to produce well-structured and semantically coherent questionanswer pairs. Unlike traditional approaches relying on random sampling or manual composition, our method integrates knowledgeconstrained generation and task-guided sampling, with type-control parameters to regulate structural complexity, difficulty, and logical patterns. Following generation, manual filtering stage applies multi-dimensional evaluation protocolassessing grammatical correctness, reasoning consistency, and task ambiguityto ensure that questions are both discriminative and robust for real-world CAPTCHA evaluation. 3.3.2 Unified Intent Modeling for Coordinate Pointing and Grid Selection For free-form coordinate pointing tasks, we use box-to-center projection strategy. Annotators mark the top-left and bottom-right corners of the targets bounding box using absolute pixel coordinates, with (0, 0) at the top-left of the image. The geometric center of each box serves as the training supervision signal. During inference, prediction is considered correct if it falls within the corresponding bounding box, following an IoB-Gated Validation mechanism. This approach offers spatial tolerance, improves robustness to outliers, and enhances training convergence. Formally, the validation criterion between the predicted point and the bounding box = [ bmin, bmax ] is defined as: G(cid:0)p, b(cid:1) = (cid:16)(cid:13) (cid:13) D1(cid:0)p 1 2 (bmin + bmax)(cid:1)(cid:13) (cid:13) 2 (cid:17) (1) 6 Here, I() is the indicator function (1 if the condition holds, 0 otherwise), = diag( bmax bmin) is the diagonal matrix of bounding-box width and height, and is the ℓ norm (the maximum absolute component). Thus, the expression tests whether the predicted point lies within the normalized box region centered on the ground-truth box. We adopt Discrete Region Index Supervision scheme for grid-based pointing, where each image is divided into uniform 33 grid, yielding nine equal-sized cells indexed from 0 to 8 in row-major order. Annotators label the target region by selecting the corresponding grid index, which serves as discrete classification target during training. This Semantic-Aligned Index Encoding effectively aligns linguistic cues with local image regions, simplifies supervision under multi-object interference, and enhances model robustness and generalization in the face of linguistic ambiguity and similar visual distractors. 3.3.3 Static Visual Recognition Class Task Labeling Methods We developed refined labeling system for static visual recognition tasks to address task diversity and challenges. For character recognition, we use character-level text tagging, where each character in the image is directly labeled and aligned with the content for accurate learning. Labels are automatically generated and validated to prevent errors, with random sampling and manual review to ensure readability and consistency. For semantic tasks, such as mathematical expressions or symbolic combinations, we apply VisualSymbolic Abstraction Parsing, parsing the images semantic structure into \"final semantic solution\". The labels are deterministic and unique, supporting logical reasoning and computation. This system ensures clear, consistent, and task-aligned supervision, providing precise signals for multimodal models and reliable evaluation in complex scenarios. 3.3.4 Interactive Behavior Task Labeling Methods The annotation system for interaction-based CAPTCHAs focuses on modeling user behavior through detailed trajectory data, target operations, and motion features across common interaction modes such as sliding, rotation, and layer alignment. These tasks combine visual restoration (e.g., aligning puzzle pieces or rotating images) with behavioral trajectory monitoring, allowing the system to distinguish between natural user actions and scripted or emulated inputs. Real user behaviors show non-linearity, irregular speed, and realistic timing, unlike the overly uniform patterns of bots or automation tools. In slider CAPTCHAs, users drag puzzle piece into matching gap; in rotation-based ones, they adjust the image angle via slider; and in alignment CAPTCHAs, they slide misaligned layers back into position. During these actions, 10 monitoring points capture behavioral data including position, time, and event type. These are analyzed for trajectory naturalness, speed/acceleration profiles, and duration plausibility (data fields detailed in Appendix). For rotation tasks, there is mapping between the sliders horizontal displacement (x) and the counter-clockwise rotation angle (ϕ) of the image. This mapping incorporates affine transformations and periodic functions. Given maximum slider travel distance of = 250 pixels, the rotation angle ϕ can be computed as follows: (cid:18) (cid:18) ϕ(x) = arg exp 2π (cid:19)(cid:19) 360 2π (2) [0, L] is the sliders horizontal displacement in pixels (with = 250 px), i2 = 1 the imaginary unit; exp(cid:0)i 2πx/L(cid:1) maps this displacement onto the complex unit circle, arg() extracts its phase (in radians), and the factor 360 2π converts that phase into the rotation angle ϕ(x)."
        },
        {
            "title": "4 Agent Pipeline and Experiments",
            "content": "This section presents detailed analysis of the experimental results based on the MCA-Bench benchmark. Set up We use Qwen2.5-VL-7B-Instruct as the backbone and fine-tune LoRA adapters on 20 CAPTCHA tasks. Images are resized to 224224, with inputs paired to structured prompts and outputs. Training runs on 4 H20 GPUs with per-GPU batch size 8 and gradient accumulation of 4 (effective batch size 32). We optimize using AdamW with linearly decaying learning rate starting 7 Figure 3: The schematic diagram illustrates the data flow and key module configuration across the four stages of the end-to-end framework: unified interface access, gent fine-tuning loading, collaborative inference execution, structured result feedback. at 1 104. Checkpoints are saved every 100 steps, with early stopping triggered if validation loss fails to improve over 20 evaluation intervals. MCA-Bench is high-quality dataset designed to enhance models cracking capabilities We fine-tuned the QWen-2.5VL-7B-instruct model using MCA-Bench, significantly improving its performance. As shown in Figure 4, the fine-tuned model outperforms both the zero-shot baseline and the best closed-source model, ChatGPT-4o, on various CAPTCHA tasks. The dataset boosts the models adaptability in visual recognition, logical reasoning, and interaction tasks, while also improving its robustness against complex CAPTCHAs, proving its value as powerful resource for improving model performance. MCA-Bench Dataset Overview MCA-Bench is the first multimodal dataset offering wide range of CAPTCHA tasks for research. It covers visual recognition, point selection, textual reasoning, and interactive operations, with samples of varying image sizes and features. The training set includes large number of samples, supporting large-scale training and improving model generalization. The dataset also allows multi-image fine-tuning, optimizing the models performance across related tasks. Some tasks use publicly available resources, ensuring diversity and reproducibility in research. Table 1 summarizes key details of the dataset, including categories, image sizes, and training quantities. Agent System Pipeline The system uses an end-to-end pipeline to unify images, text prompts, and user interactions into single JSON payload. This payload is directly mapped to semantic space without intermediate parsing. Based on the task ID in the JSON header, the pipeline activates the appropriate LoRA-Agent, each applying fine-tuned parameters to shared frozen backbone for efficient multi-task operation with low memory overhead. During inference, visual and textual embeddings are routed to the corresponding agent, which produces outputs such as coordinates, text, mouse movements, or character recognition. These are reassembled into the original JSON format for downstream processing, enabling flexible handling of diverse CAPTCHA tasks, and the overall fine-tuning flow is shown in Figure 3. Fine-Tuned Model Performance in Visual Tasks For simple visual tasks with minimal background interference, the fine-tuned visual-language model outperforms humans in both speed and accuracy, especially in tasks like target recognition and geometric shape identification. However, as task 8 Figure 4: Violin plots of Passing-Rate (01) for Original Qwen-2.5 VL-7B-Instruct, Original ChatGPT-4-o, and Finetuned Qwen-2.5 VL-7B-Instruct on 20 CAPTCHA classes. Outlines show density; boxes, dots, and whiskers mark IQR, median, and range. complexity increasessuch as with distorted or rotated imagesthe models performance drops significantly, while humans maintain strong performance in these challenging scenarios. This highlights the limitations of visual-language models in handling complex visual transformations. Human vs. Model Performance in Reasoning and Interaction Tasks In tasks requiring reasoning or behavioral interaction, the fine-tuned model still lags behind humans. Interactive tasks, such as sliding puzzles and rotating tiles, require precision and coordination, which are challenging for the model. Similarly, in tasks demanding multi-step reasoning or common-sense judgment, the model struggles with complex tasks that involve deeper understanding and contextual reasoning. As shown in Table 2, despite the fine-tuned Qwen-Agents success in some standard visual recognition tasks, humans still outperform the model in complex tasks involving reasoning and interaction."
        },
        {
            "title": "5 Design Principles for Next-Generation Cognitive-Secure CAPTCHAs\nLarge-scale experiments on MCA-Bench demonstrate that fine-tuned multimodal models achieve\nover 96% success on visual or shallow-semantic CAPTCHAs but drop below 2.5% for tasks requiring\nphysical interaction or multi-step reasoning. This emphasizes the need for a more robust verification\napproach, as single-dimension obfuscation is no longer effective against vision–language model-\nbased attacks. To address this, we propose a framework that tightly integrates perception, action, and\nreasoning through three core principles: Deep Modality Coupling, Behavior-Anchored Validation,\nand Session-Specific Semantic Personalization.",
            "content": "Deep Modality Coupling involves continuous challenge pipeline that blends visual cue localization, logical inference, and interactive input. This approach dynamically adjusts task complexity based on user performance, enhancing both security and user experience. Behavior-Anchored Validation analyzes user interaction trajectories, granting verification based on similarity to genuine human data, which prevents scripted attacks without hindering user flow. Session-Specific Semantic Personalization embeds unique semantic seed into each CAPTCHA, ensuring that solutions rely on session-specific context, preventing automated solvers from precomputing answers. Together, these principles create cognitively rich verification system that is resilient to advanced attacks. Conclusion We present MCA-Bench, the first unified benchmark for evaluating CAPTCHA robustness across twenty real-world tasks spanning visual recognition, text comprehension, logical reasoning, and interactive manipulation. Through extensive experiments with LoRA-based attack agents on state-of9 the-art visionlanguage models, we identify structural vulnerabilities in visual and shallow semantic CAPTCHAs, while tasks involving physical interaction and multi-step reasoning demonstrate greater resilience. Based on these findings, we propose three practical design principlesDeep Modality Coupling, Behavior-Anchored Validation, and Session-Specific Semantic Personalizationto guide the development of more secure humanmachine verification systems. MCA-Bench establishes standardized foundation for adversarial evaluation and future research into adaptive difficulty, human behavior modeling, and usability-aware CAPTCHA design."
        },
        {
            "title": "References",
            "content": "[1] RUMMERY G. A. On-line q-learning using connectionist systems. Technical Report CUED/F-INFENF/TR, (166), 1994. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Alejandro Acien, Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez, and Ivan Bartolome. Becaptcha: Detecting human behavior in smartphone interaction using multiple inbuilt sensors. arXiv preprint arXiv:2002.00918, 2020. [4] Alejandro Acien, Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez, and Oscar Delgado-Mohatar. Becaptcha: Behavioral bot detection using touchscreen and mobile sensors benchmarked on humidb. Engineering Applications of Artificial Intelligence, 98:104058, 2021. [5] Suliman Alsuhibany. benchmark for designing usable and secure text-based captchas. International Journal of Network Security & Its Applications, 8(4):4154, 2016. [6] Tim Brys, Anna Harutyunyan, Matthew E. Taylor, and Ann Nowé. Policy transfer using reward shaping. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, AAMAS 15, page 181188, Richland, SC, 2015. International Foundation for Autonomous Agents and Multiagent Systems. [7] Elie Bursztein, Matthieu Martin, and John Mitchell. Text-based captcha strengths and weaknesses. In Proceedings of the 18th ACM conference on Computer and communications security, pages 125138, 2011. [8] Kumar Chellapilla, Kevin Larson, Patrice Simard, and Mary Czerwinski. Building segmentation based human-friendly human interaction proofs (hips). In International Workshop on Human Interactive Proofs, pages 126. Springer, 2005. [9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. [10] Hai Ci, Yiren Song, Pei Yang, Jinheng Xie, and Mike Zheng Shou. Wmadapter: Adding watermark control to latent diffusion models. arXiv preprint arXiv:2406.08337, 2024. [11] Hai Ci, Pei Yang, Yiren Song, and Mike Zheng Shou. Ringid: Rethinking tree-ring watermarking for enhanced multi-key identification. In European Conference on Computer Vision, pages 338354. Springer, 2024. [12] Gelei Deng, Haoran Ou, Yi Liu, Jie Zhang, Tianwei Zhang, and Yang Liu. Oedipus: Llm-enchanced reasoning captcha solver. arXiv preprint arXiv:2405.07496, 2024. [13] Ziqi Ding, Gelei Deng, Yi Liu, Junchen Ding, Jieshan Chen, Yulei Sui, and Yuekang Li. Illusioncaptcha: captcha based on visual illusion. In Proceedings of the ACM on Web Conference 2025, pages 36833691, 2025. [14] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. arXiv e-prints, page arXiv:1611.02779, November 2016. [15] Jeremy Elson, John Douceur, Jon Howell, and Jared Saul. Asirra: captcha that exploits interest-aligned manual image categorization. CCS, 7(366-374):15, 2007. [16] Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alex Smola. Meta-q-learning. 2020. [17] Jesse Farebrother, Marlos C. Machado, and Michael Bowling. Generalization and regularization in DQN, 2019. [18] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, page 11261135. JMLR.org, 2017. [19] Yipeng Gao, Haichang Gao, Sainan Luo, Yang Zi, Shudong Zhang, Wenjie Mao, Ping Wang, Yulong Shen, and Jeff Yan. Research on the security of visual reasoning {CAPTCHA}. In 30th USENIX security symposium (USENIX security 21), pages 32913308, 2021. [20] Matt Ginsberg. Essentials of Artificial Intelligence. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2012. [21] Philippe Golle. Machine learning attacks against the asirra captcha. In Proceedings of the 15th ACM conference on Computer and communications security, pages 535542, 2008. [22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [23] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-reinforcement learning of structured exploration strategies. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS18, page 53075316, Red Hook, NY, USA, 2018. Curran Associates Inc. [24] WATKINS C. J. C. H. Learning with delayed rewards. Ph. D. thesis, Cambridge University, 1989. [25] Carlos Javier Hernández-Castro, David Barrero, and María R-Moreno. Basecass: methodology for captchas security assurance. Journal of Information Security and Applications, 63:103018, 2021. [26] Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. Metagpt: Meta programming for multi-agent collaborative framework, 2024. [27] Imran Hossen and Xiali Hei. aaecaptcha: The design and implementation of audio adversarial captcha. In 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P), pages 430447. IEEE, 2022. [28] Marcus Hutter. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability, volume 1. Springer, 1st edition, 2005. [29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 49044916. PMLR, 1824 Jul 2021. [30] Ran Jiang, Sanfeng Zhang, Linfeng Liu, and Yanbing Peng. Diff-captcha: An image-based captcha with security enhanced by denoising diffusion model. arXiv preprint arXiv:2308.08367, 2023. [31] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement Learning: Survey. arXiv e-prints, page cs/9605103, April 1996. [32] Mohinder Kumar and Manish Kumar Jindal. Benchmarks for designing secure devanagari captcha. SN Computer Science, 2:116, 2021. 11 [33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3489234916. Curran Associates, Inc., 2023. [36] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models on simulated social interactions. arXiv preprint arXiv:2305.16960, 2023. [37] Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, and Yuheng Bu. Image watermarks are removable using controllable regeneration from clean noise. arXiv preprint arXiv:2410.05470, 2024. [38] Haochen Luo, Jindong Gu, Fengyuan Liu, and Philip Torr. An image is worth 1000 lies: Adversarial transferability across prompts on vision-language models. arXiv preprint arXiv:2403.09766, 2024. [39] Ben Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 1:3, 2020. [40] Lev McKinney and Yuchong Zhang. Training language models to follow instructions with human feedback. 2020. [41] Allen Newell and Herbert A. Simon. Computer science as empirical inquiry: symbols and search. Commun. ACM, 19(3):113126, March 1976. [42] Jiawei Nian, Ping Wang, Haichang Gao, and Xiaoyan Guo. deep learning-based attack on text captchas by using object detection techniques. IET Information Security, 16(2):97110, 2022. [43] Peter Norvig and Stuart Russell. Artificial Intelligence: Modern Approach. Pearson Education, USA, 2021. [44] Zahra Noury and Mahdi Rezaei. Deep-captcha: deep learning based captcha solver for vulnerability assessment. arXiv preprint arXiv:2006.08296, 2020. [45] Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning. arXiv e-prints, page arXiv:1511.06342, November 2015. [46] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [48] Kate Rakelly*, Aurick Zhou*, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy meta-reinforcement learning via probabilistic context variables, 2019. [49] C. Ribeiro. Reinforcement learning agents. Artif. Intell. Rev., 17(3):223250, May 2002. [50] Stuart Russell and Eric Wefald. Do the right thing: studies in limited rationality. MIT Press, Cambridge, MA, USA, 1991. [51] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021. 12 [52] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. [53] M. J. Schoppers. Universal plans for reactive robots in unpredictable environments. In Proceedings of the 10th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI87, page 10391046, San Francisco, CA, USA, 1987. Morgan Kaufmann Publishers Inc. [54] Andrew Searles, Yoshimichi Nakatsuka, Ercan Ozturk, Andrew Paverd, Gene Tsudik, and Ai Enkoji. An empirical study & evaluation of modern {CAPTCHAs}. In 32nd usenix security symposium (usenix security 23), pages 30813097, 2023. [55] Dhruv Shah, Błazej Osinski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In Conference on robot learning, pages 492504. PMLR, 2023. [56] Vinay Shet. Are you robot? introducing no captcha recaptcha. Google Security Blog, 3:12, 2014. [57] Yoav Shoham. Agent oriented programming: An overview of the framework and summary of recent research. In Michael Masuch and László Pólos, editors, Knowledge Representation and Reasoning Under Uncertainty, pages 123129, Berlin, Heidelberg, 1994. Springer Berlin Heidelberg. [58] David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484503, 2016. [59] Patrice Simard, David Steinkraus, and John Platt. Best practices for convolutional neural networks applied to visual document analysis. In Seventh International Conference on Document Analysis and Recognition (ICDAR 2003), volume 3, pages 958962. IEEE, 2003. [60] Suphannee Sivakorn, Iasonas Polakis, and Angelos Keromytis. am robot:(deep) learning to break semantic image captchas. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P), pages 388403. IEEE, 2016. [61] Yiren Song, Shengtao Lou, Xiaokang Liu, Hai Ci, Pei Yang, Jiaming Liu, and Mike Zheng Shou. Anti-reference: Universal and immediate defense against reference-based generation. arXiv preprint arXiv:2412.05980, 2024. [62] Yiren Song, Pei Yang, Hai Ci, and Mike Zheng Shou. Idprotector: An adversarial noise encoder to protect against id-preserving image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 30193028, 2025. [63] Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive architectures for language agents, 2024. [64] Anthony Therrien. Image classification - 64 classes - animal, 2024. [65] Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, and Anh Tran. Antidreambooth: Protecting users from personalized text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21162127, 2023. [66] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [67] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), March 2024. [68] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 13 [69] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [70] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [71] Wan Xing, Mohd Rizman Sultan Mohd, Juliana Johari, and Fazlina Ahmat Ruslan. review on textbased captcha breaking based on deep learning methods. In 2023 International Conference on Computer Engineering and Distance Learning (CEDL), pages 171175. IEEE, 2023. [72] Pei Yang, Hai Ci, Yiren Song, and Mike Zheng Shou. Can simple averaging defeat modern watermarks? Advances in Neural Information Processing Systems, 37:5664456673, 2024. [73] Guixin Ye, Zhanyong Tang, Dingyi Fang, Zhanxing Zhu, Yansong Feng, Pengfei Xu, Xiaojiang Chen, and Zheng Wang. Yet another text captcha solver: generative adversarial network based approach. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security, pages 332348, 2018. [74] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. In Proceedings of the 12th International Conference on Learning Representations (ICLR 2024), 2024. [75] Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain, and Jiayu Zhou. Transfer learning in deep reinforcement learning: survey. IEEE Trans. Pattern Anal. Mach. Intell., 45(11):1334413362, November 2023. 14 MCA-Bench: Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks"
        },
        {
            "title": "Outline",
            "content": "A More Details on MCA-Bench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.1 Overview of Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 Distribution Pattern and Training Balance Details for Each Type of Task . . . . . . . . . 16 A.3 Image Resolution Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Market Research on Mainstream CAPTCHA Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Comparative Analysis of User Experience and Security Performance . . . . . . . . . . . . 16 B.2 Evaluation of Mainstream CAPTCHA Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Application Trends and User Acceptance Analysis of Mainstream CAPTCHA Types 19 B.4 Evaluation and Analysis of Mainstream CAPTCHA Types Selection . . . . . . . . . . . . 19 B.5 Analysis of CAPTCHA Error-Proneness and User Self-Recovery . . . . . . . . . . . . . . . 20 B.6 Semiotic Analysis of CAPTCHA Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Extensive Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.1 Comparison of Model Stability and Human Performance Variability . . . . . . . . . . . . . 23 C.2 Error-Correction Capability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.3 Comparative Analysis of Micro-Decision Path Interpretability . . . . . . . . . . . . . . . . . . 24 Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D.1 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Future Work Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 More Details on MCA-Bench A.1 Overview of Data Sets To systematically compare the adaptability and data support of different CAPTCHA tasks in model training, this study categorizes them into four major types: static visual recognition, point-based localization, incremental interaction, and textual logical reasoning. We summarize seven key technical indicators for representative datasets, including image resolution, data volume, training set size, original annotation attributes, and whether single-image tasks are supported. Character-based CAPTCHAs typically feature large data volumes and low resolutions, providing abundant and lowcost training samples for deep learning models. In contrast, point-click and interaction-based tasks have relatively smaller datasets but higher annotation complexity, often utilizing high-resolution images that significantly enhance models fine-grained representation capabilities in multimodal reasoning tasks such as object localization and action prediction. Interactive CAPTCHAs rely on dynamic generation mechanisms, simulating temporal dependencies and adversarial human-machine interactions in real-world scenarios. For these interactive CAPTCHAs, we have specifically designed sampling field, as detailed in Table 3. 15 Table 3: Field specification of the CAPTCHA interaction dataset. We define 13 structured fields used to record user interactions with CAPTCHA tasks, including spatial position, timing, image paths, and movement characteristics. These fields are essential for modeling behavior patterns and evaluating automated solver performance. Name Type Length Decimal point Not null Virtual Key Explanatory note id data_id group_index type bg_img_path path event_type speed smallint int int varchar varchar varchar varchar smallint smallint smallint decimal acceleration decimal jitter_frequency decimal 5 10 10 10 50 50 10 5 5 5 10 10 5 5 1 Primary key ID (maximum 50,000) Group ID to which the data belongs (10,000) Group number CAPTCHA type Background image path Template image path (optional) Event type coordinate (10,000 px) coordinate (10,000 px) Time value (10,000 ms) Speed (px/ms), keep 5 decimal places Acceleration (px/ms2), keep 5 decimal places Jitter frequency (times/ms), keep 5 decimal places A.2 Distribution Pattern and Training Balance Details for Each Type of Task To ensure balanced model performance across diverse tasks, we conducted visual analysis of the overall dataset structure by subtask type, as presented in Appendix A. As illustrated in Figure 5, the sample distribution across the four task categoriesstatic visual recognition, point-based localization, interactive operation, and textual logical reasoningremains relatively balanced. This distribution is intended to prevent any single task from dominating the training process, thereby enabling more comprehensive multimodal learning environment. Moreover, the comparable level of sample support across tasks provides solid data foundation for subsequent task switching and joint optimization within the model. A.3 Image Resolution Hierarchy In terms of model input preprocessing strategies, Figure 6 illustrates the distribution of image resolutions across the entire dataset, serving as an important reference for multi-scale feature learning and computational resource allocation. As shown in Figure 6, the majority of image samples fall within medium-resolution range, while instances with extreme resolutions are relatively rare. This visualization supports the effectiveness of using unified resizing and cropping approach to accommodate the detail requirements of most samples efficiently. Additionally, by incorporating multi-resolution data augmentation strategies, model robustness to varying image scales can be further enhanced without significantly increasing computational overhead."
        },
        {
            "title": "B Market Research on Mainstream CAPTCHA Datasets",
            "content": "We conducted comprehensive survey of the mainstream CAPTCHAs currently available on the market, which provided clear requirements and technical references for the development of MCABench. B.1 Comparative Analysis of User Experience and Security Performance To systematically evaluate the performance of various mainstream CAPTCHAs in real-world scenarios, this study conducts comparative analysis across five dimensions: user completion time, misclick rate, visual fatigue, cracking cost, and typical application scenarios, as summarized in Table 16 Figure 5: Category distribution of CAPTCHA types in the dataset The dataset covers wide range of 3D-interactive, text-based, and visually complex CAPTCHA categories, with each type contributing approximately 5% of the total. The lowest-frequency categories (e.g., commonsense reasoning and text-based arithmetic) represent specialized reasoning-based challenges, while the most common types focus on perceptual and motor interactions. 4. The results reveal significant differences among CAPTCHA types in terms of usability and security. Color recognition and slider puzzles offer the fastest operation and lowest visual load, providing superior user experience overall. In contrast, math-based CAPTCHAs require additional cognitive processing and thus result in the longest completion times, though they have relatively low misclick rates. Traditional text-based CAPTCHAs are cognitively demanding, error-prone, and increasingly vulnerable due to advances in automated attack techniques. In terms of security robustness, slider puzzles are the most resilient, owing to their reliance on authentic user-generated trajectories, making them difficult to bypass using automated scripts. Conversely, traditional text and math CAPTCHAs are more susceptible to automated cracking. Regarding application scenarios, slider puzzles are best suited for high-security contexts such as financial logins; color recognition CAPTCHAs are ideal for systems prioritizing usability or targeting children and elderly users; and traditional text CAPTCHAs are only recommended for low-security environments where legacy compatibility is concern. B.2 Evaluation of Mainstream CAPTCHA Types To comprehensively understand the trade-offs between security and user experience in mainstream CAPTCHA types, this study systematically reviews various commonly used CAPTCHA forms in the market, summarizing their technical characteristics and user perspectives, as detailed in Table 5. Analysis across three key dimensions reveals clear trade-off between security and usability: behavioral CAPTCHAs, which leverage sliding trajectories and temporal patterns, offer the highest level of security. They are followed by dynamic graphics and shape-based CAPTCHAs, which rely on motion interference or spatial recognition. However, as the level of interaction complexity increases, users tend to experience greater fatigue. As result, CAPTCHAs such as nine-grid clicks and dynamic graphics rank lower in user preference. In contrast, color recognition and arithmetic CAPTCHAs, which are easier to operate, are favored for their simplicity but rank lowest in terms of security due to their weak protection capabilities. We find that the public tends to prefer CAPTCHA mechanisms that are intuitive and natural in their interaction. Behavioral and shape-based CAPTCHAs are generally perceived as offering balanced compromise between robust security and user-friendly design. 17 Figure 6: Distribution of image file sizes in the dataset Over 60% of the images are smaller than 100 kB, indicating strong skew toward low-resolution or compressed images. The dashed line shows the estimated probability density function. Percentage labels denote the relative frequency per bin. 18 Table 4: User experience and security performance across mainstream CAPTCHA types. This table compares various CAPTCHA mechanisms based on usability and security dimensions, highlighting trade-offs in effectiveness, user preference, and resistance to automation. CAPTCHA Type Slider Puzzle CAPTCHA Shape Recognition CAPTCHA Nine-grid Click CAPTCHA Mathematical Calculation CAPTCHA Color Recognition CAPTCHA Distorted Character CAPTCHA Traditional Character CAPTCHA Completion Time (s) Mistouch Rate Fatigue (15) Cracking Cost (%) Applicable Scenarios 58 712 610 1015 4 710 69 15% 18% 20% 10% 12% 18% 25% 2 3 3 2 3 4 80% 60% 40% 20% 40% 20% 20% Login for financial apps, enterprise operations Design platforms, graphical UIs E-commerce registration, social logins Educational systems, bank transfers Childrens apps, elderly systems Old systems, non-core entrances Test pages, low-priority verifications B.3 Application Trends and User Acceptance Analysis of Mainstream CAPTCHA Types This study presents comprehensive comparison of mainstream CAPTCHA types in terms of industry penetration, user preferences, cross-age acceptability, future growth potential, and cross-platform consistency, as summarized in Table 6. Overall, slider puzzle CAPTCHAs strike the best balance between security and usability, demonstrating broad device compatibility and promising development prospects. While image recognition and nine-grid click CAPTCHAs perform well in specific contexts, their reliance on graphic rendering and sensitivity to screen size limit their user-friendliness and scalability. Math-based and color recognition CAPTCHAs are better suited to older user groups and platforms with low graphical complexity or text-driven interfaces. Distorted and traditional text CAPTCHAs, owing to their high compatibility, continue to serve in scenarios with stringent environment adaptability requirements. B.4 Evaluation and Analysis of Mainstream CAPTCHA Types Selection To comprehensively evaluate the applicability of different CAPTCHA types in real-world scenarios, this study presents \"Website CAPTCHA Selection Decision Table,\" constructed based on six core dimensions: security requirements, user experience, development cost, device compatibility, accessibility support, and anti-cracking update frequency. Five mainstream CAPTCHA types are analyzed and quantitatively scored using five-point scale, where higher score indicates better performance in the corresponding dimension, as shown in Table 7. The sliding puzzle CAPTCHA stands out as the preferred choice for mainstream applications due to its high security, user-friendly interactive design, and strong cross-device compatibility. The nine-grid click CAPTCHA performs well in terms of interactivity and engagement but is somewhat limited by image loading speed and resolution, and shows weaknesses in accessibility support. The color recognition CAPTCHA is simple to implement, cost-effective, and highly adaptable, making it suitable for elderly users, though it lacks robustness against automated attacks. The mathbased CAPTCHA offers easy deployment and strong compatibility, making it ideal for streamlined interfaces such as those in educational or government services; however, its lack of graphical interference mechanisms results in lower security. The traditional text-based CAPTCHA, while the most economical and widely supported in terms of display, is gradually being phased out due to its relatively poor performance in both security and user experience. 19 Table 5: General public perspectives on commonly used CAPTCHA systems. We present comparative analysis of user preferences, perceived security rankings, and subjective commentary reflecting real-world usability and acceptance. CAPTCHA type Security strength User preference Standpoint Behavioral CAPTCHA Dynamic Graphic CAPTCHA Shape Judgment CAPTCHA Nine-grid Click CAPTCHA Calculation Problem CAPTCHA Color Recognition CAPTCHA Distorted Character CAPTCHA Traditional Character CAPTCHA NO.1 NO.2 NO. NO.4 NO.5 NO.6 NO.7 NO.8 The operation of sliding is intuitive, and it combines interactivity with relatively high security. Its dynamic feature makes it difficult for automated bots to precisely imitate human behavior By presenting changing visual elements, it increases the difficulty of recognition. However, it may cause visual fatigue, and advanced image analysis algorithms may be able to decipher its patterns over time It requires users to judge shapes according to certain rules. This type of CAPTCHA makes use of human spatial perception and shape recognition abilities. Although it has certain security level, sophisticated shape analysis algorithms may pose threat to its security It presents nine-grid layout with elements and requires users to click on elements that meet specific rules. It balances security and user-friendliness, but if the rules are repetitive, automated scripts may be able to bypass it It presents arithmetic or logical problems and requires users to calculate and input answers. It tests human cognitive abilities, but simple calculation patterns can be easily automated, reducing its security over time It asks users to recognize colors or select elements based on color-related rules. It is relatively easy for humans but can be challenge for bots. However, advanced color detection algorithms may undermine its security It displays characters in distorted forms, increasing the difficulty of optical character recognition. However, with the continuous progress of OCR technology and machine learning, its security is lower compared to more interactive types Composed of standard characters, it can be easily recognized by modern OCR software. It offers minimal security and has become obsolete due to its simplicity and vulnerability B.5 Analysis of CAPTCHA Error-Proneness and User Self-Recovery This study conducts comparative analysis of the human-computer interaction performance of six common CAPTCHA types, evaluating their error-proneness and users ability to self-correct mistakes in real-world scenarios. The aim is to provide data-driven insights and optimization recommendations for CAPTCHA design. Table 8 presents the average user error rate, user self-recovery rate, and common causes of errors for each CAPTCHA type. The sliding puzzle CAPTCHA is the most user-friendly option, featuring intuitive operation and smooth interaction that enables users to quickly identify and correct errors. The nine-grid click CAPTCHA, while offering high level of engagement, suffers from higher error rates due to small clickable areas and visual recognition challenges. The color recognition CAPTCHA introduces novel visual design but is less accessible for users with color vision deficiencies or visual fatigue. The math-based CAPTCHA imposes higher cognitive load, making error recovery more difficult for Table 6: Comparison of user experience and security performance across mainstream CAPTCHA types. This table evaluates key CAPTCHA variants based on industry adoption, user acceptance across demographics, future growth potential, and platform consistency, providing insights into usability and robustness. CAPTCHA Type Slider Puzzle CAPTCHA Shape Recognition CAPTCHA Nine-grid Click CAPTCHA Mathematical Calculation CAPTCHA Color Recognition CAPTCHA Distorted Character CAPTCHA Traditional Character CAPTCHA Industry Penetration (2024) Preference (Young Users) Acceptance (Middleaged/Elderly) Growth Potential (%) Cross-Platform Consistency 68% 10% 35% 12% 20% 5% 3% 52% 12% 28% 8% 15% 2% 1% 38% 6% 25% 15% 30% 5% 3% 80% 60% 40% 20% 60% 20% 20% High (Unified on Mobile/PC) Medium (Requires Graphic Rendering) Medium (Better on PC) High (Text-Based Compatibility) High (Simple Graphics Adaptability) Medium (Font Compatibility Issues) Highest (Static Text Consistency) users. The traditional text-based CAPTCHA is hindered by distortion and blurring effects, leading to recognition difficulties and limited interaction feedback. Lastly, the shape-matching CAPTCHA relies on spatial reasoning, which may not be suitable for all users due to its higher adaptation threshold. Table 7: Evaluation-based selection matrix for website CAPTCHA deployment. This table compares various CAPTCHA types across multiple evaluation dimensionsincluding security, user experience, development cost, device compatibility, accessibility, and maintenance needsto support informed decision-making for web application integration. Evaluation Dimension Business Requirement Priority Security Requirement User Experience Requirement Development Cost Device Compatibility Accessibility Support Anti-Cracking Tech Update Frequency High High Low Multi-device Adaptation Disabled User Compatibility High Slider Puzzle Nine-grid Click Color Recognition Calculation Problem Traditional Character 100% 80% 40% 80% 60% 80% 60% 60% 60% 60% 40% 60% 40% 100% 80% 100% 80% 40% 40% 40% 100% 100% 60% 40% 20% 40% 100% 100% 40% 20% B.6 Semiotic Analysis of CAPTCHA Types From semiotic perspective, this section presents systematic analysis of the symbolic dimensions of various CAPTCHA types, aiming to uncover their mechanisms of meaning-making, cognitive adaptability, and symbolic evolution within human-computer interaction. Table 9 compares four major types of CAPTCHAs across several dimensions, including semantic transparency, the relationship between signifier and signified, degree of cognitive schema alignment, anti-symbolic structural capacity, and their respective stages in symbolic evolution. This analysis sheds light on the ongoing transition of CAPTCHAs from static linguistic symbols to dynamic behavioral symbols. 21 Table 8: Comparative analysis of user error and self-repair rates across CAPTCHA types. This table highlights typical failure modes encountered by users, the average error rates per CAPTCHA type, and the proportion of users able to self-correct without external assistance, providing insights into usability and design robustness. CAPTCHA Type Average Error Rate User SelfRepair Rate Error Cause Analysis Slider Puzzle CAPTCHA Nine-grid Click CAPTCHA Color Recognition CAPTCHA Mathematical Calculation CAPTCHA Traditional Character CAPTCHA Graphic Shape Recognition CAPTCHA 8% 15% 12% 22% 25% 18% 75% 55% 68% 40% 32% 45% Inaccurate mobile touch operations (misclicking the slider area), delayed dynamic tile loading causing operation lag Low visibility of target graphics (confusion between similar elements), too-small click areas on mobile causing misclicks Color blindness/weakness (e.g., red-green confusion), visual fatigue from dynamic color changes Misunderstanding of operation order (e.g., priority in mixed operations), ambiguous wording Excessive character distortion (blurred fonts), confusion between cases/symbols (e.g., vs 0) Abstract pattern comprehension difficulty (e.g., incomplete shapes), visual interference from complex background textures Text-based CAPTCHAs exhibit the highest levels of semantic transparency and signifier-signified correspondence, with moderate cognitive load. However, their structurally regular nature makes them vulnerable to OCR-based attacks. Image-based CAPTCHAs, benefiting from intuitive visual mappings, demonstrate high schema compatibility across multilingual contexts, achieving trade-off between usability and security. Behavioral CAPTCHAs (such as sliding puzzles and nine-grid clicks), while lower in semantic transparency and symbolic clarity, require users to quickly learn interaction rules. Nevertheless, their reliance on dynamic trajectories and spatiotemporal uncertainty grants them significant advantages in resisting symbolic deconstruction. Notably, the nine-grid click CAPTCHAcombining visual targeting with spatial interactionis considered \"2.5D\" hybrid form. Table 9: Comparative semiotic analysis of CAPTCHA types. This table evaluates CAPTCHAs across key semiotic dimensions, including semantic transparency, signifier-signified relationships, cognitive schema alignment, and resistance to automated sign decoding, offering insight into their interpretability and robustness. CAPTCHA Type Semantic Transparency Signifier-Signified Relevance Cognitive Schema Match Anti-sign Deconstruction Ability Stage of Sign Evolution Traditional Character CAPTCHA High (direct text meaning) Graphic CAPTCHA Medium (representational images) Behavioral CAPTCHA (Slider/Nine-grid) Low (abstract actions) Nine-grid Click CAPTCHA Medium (spatial selection) Strong Medium Weak Medium Medium (language-dependent) Weak (vulnerable to OCR parsing) Linguistic signs (1D) High (cross-lingual universality) Medium (AI-recognizable features) Visual signs (2D) Low (requires rule-learning) Strong (unpredictable dynamic trajectories) Behavioral signs (3D) Medium (game-like interaction) Medium-high (spatial ambiguity defense) Hybrid visual-behavioral signs (2.5D) Figure 7: Comparison of Multi-round Task Performance Between Models and Humans Across Task Types Each subplot illustrates specific task type, with the left side displaying the distribution of multiple performances by human participants (scatter plots), and the right side showing the models stable outputs (box plots). It can be observed that the model exhibits smaller range of variability in most tasks, indicating higher execution stability."
        },
        {
            "title": "C More Extensive Experiments",
            "content": "C.1 Comparison of Model Stability and Human Performance Variability The stability of model outputs across diverse task scenarios is critical to the robustness and controllability of complex CAPTCHA systems. In contrast, human participants performance is more susceptible to extraneous factorssuch as fluctuations in attention and operator fatigueresulting in measurable variability over repeated trials. Accordingly, this study collected multiple executions from both models and human subjects across four task categoriespoint-and-click recognition, static visual identification, textual logical reasoning, and interactive operationsand compared their results. As illustrated in Figure 7, boxplots and scatter plots distinctly highlight the disparity in consistency between the two, providing empirical support for subsequent stability assessment and risk management during model deployment. C.2 Error-Correction Capability Analysis In real-world deployments, when recognition errors are inevitable, the systems self-correction mechanism becomes critical metric of model robustness. This study establishes multi-stage feedback experimental framework to systematically evaluate the probability of successful iterative correction following an initial misclassification. By conducting side-by-side comparison of model and human performance on secondary judgment tasks, we quantify the performance limits and optimization potential of existing error-correction strategies, as illustrated in Figure 8. 23 Figure 8: Comparison of Error Correction Capabilities Between Models and Humans Across Tasks The horizontal axis presents 20 types of interactive tasks (including representative scenarios such as Point and Click Location and State VR), while the vertical axis indicates the normalized correction capability index (ranging from 0 to 1.2). The light yellow bars represent the models error correction rate achieved through multi-step feedback mechanism, whereas the ochre bars denote human participants spontaneous correction performance under the same tasks. The data show that in 75% of the task scenarios, the human correction index exceeds 0.8, while the models performance is mainly concentrated in the 0.50.7 range. Notably, the gap is most pronounced in TL-Q&A (dialogue-based interaction) and IM (immediate feedback) tasks. Error bars indicate the standard deviation across three independent experiments. C.3 Comparative Analysis of Micro-Decision Path Interpretability The interpretability of models decision-making on complex multimodal CAPTCHAs is fundamental to assessing its trustworthiness in deployment. Leveraging real-world sampling data and two-dimensional metric of mean versus variance, this study constructs quantitative comparison framework to reveal systematic differences between model and human micro-decision processes. By concurrently analyzing decision consistency (mean) and fluctuation range (variance), we not only deconstruct the transparency of the models reasoning logic but also furnish empirical insights for enhancing its decision-making robustness, as shown in Figure 9."
        },
        {
            "title": "D Limitations and Future Work",
            "content": "D.1 Limitations Although this paper establishes the unified multimodal CAPTCHA evaluation benchmark MCABench and conducts systematic testing on LoRA-fine-tuned proxy of visual-language model (VLM), several limitations remain. First, owing to computational constraints, our experiments focus exclusively on the QwenVL-2.5-7B-Instruct architecture. This single-model choice restricts our ability to systematically compare performance across different VLM architectures and parameter scales, and it may underrepresent the true bounds of VLM-based CAPTCHA-breaking capabilities. Additionally, our current work emphasizes the offensive perspectivenamely, the cracking performance of fine-tuned modelswhile defensive mechanisms receive insufficient attention. Key questions such as CAPTCHA resilience against adversarial perturbations, forged inputs, and multidevice deployment scenarios have not been explored in depth. Future studies must close the loop 24 Figure 9: Quantitative Comparison of Model Decision Paths in Multimodal CAPTCHA Scenarios. The horizontal axis includes 20 representative model architectures (e.g., 3-3G, HPR, J-S CC), with the left vertical axis indicating the normalized decision consistency index (ranging from 0 to 1.2), and the right vertical axis showing the variance fluctuation coefficient (ranging from 0 to 0.3). The red line represents the alignment of each models decision path with the human baseline, while the cobalt blue line reflects the degree of decision variability. Error bars denote standard deviations across three independent samples. The HPR model (Human-Pattern Recognition) demonstrates near-human performance in both consistency index (1.05 0.07) and variance coefficient (0.12 0.03). In contrast, the CD model (Cascade Decision) exhibits the highest decision variance (0.28 0.05), revealing instability in decision logic. Notably, in the CCC (Cross-Channel Correlation) task, over 70% of models exceed the 0.2 threshold in variance coefficient. on the attack-defense ecology to elucidate the evolutionary dynamics of CAPTCHA systems under adversarial pressure. Moreover, although MCA-Bench covers four primary task categories (static recognition, image-click, interactive manipulation, and logical reasoning), it remains centered on mainstream CAPTCHA paradigms and lacks coverage of emerging verification scenarios, limiting its fidelity to the complexity of real-world human-machine challenges. Finally, our training regime relies on homogenously annotated dataset for supervised learning. In actual deployment, distribution shifts caused by device heterogeneity, varied user behaviors, and evolving task distributions may degrade model generalization and security. This issue is particularly pronounced in interactive tasks, where our simulated user trajectoriesgenerated according to preset sampling rulesfail to capture the full spectrum of individual variability and temporal uncertainty. Consequently, the robustness of automated attacks that emulate real user behaviors may be underestimated. Enhancing the realism of data collection and the granularity of behavior modeling is therefore an essential direction for improving the benchmarks reliability. D.2 Future Work Outlook MCA-Bench lays the groundwork for unified evaluation in multimodal CAPTCHA research, but its potential extends beyond attack benchmarking to the design of next-generation cognitive-security verification mechanisms. Guided by the three cognitive security design principles proposed hereindeep modality coupling, behavior-anchored verification, and semantic personalization embeddingfuture research should transition from attack evaluation toward defensive design. Specifically, building on modality coupling, one can devise multi-stage, multi-channel CAPTCHA workflows that inter25 twine visual recognition, logical reasoning, and physical interaction, thereby thwarting automation through single-path exploitation. By introducing dynamic challenge generation and adaptive difficulty adjustment based on individual user histories, systems can calibrate task complexity in real time to preserve usability while bolstering overall resistance to automated threats."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Southwest University"
    ]
}