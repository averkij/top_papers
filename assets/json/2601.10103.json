{
    "paper_title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "authors": [
        "Lizhen Wang",
        "Yongming Zhu",
        "Zhipeng Ge",
        "Youwei Zheng",
        "Longhao Zhang",
        "Tianshu Hu",
        "Shiyang Qin",
        "Mingshuang Luo",
        "Jiaxu Zhang",
        "Xin Chen",
        "Yulong Wang",
        "Zerong Zheng",
        "Jianwen Jiang",
        "Chao Liang",
        "Weifeng Chen",
        "Xing Wang",
        "Yuan Zhang",
        "Mingyuan Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles."
        },
        {
            "title": "Start",
            "content": "FlowAct-R1: Towards Interactive Humanoid Video Generation Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao Liang, Weifeng Chen, Xing Wang, Yuan Zhang, Mingyuan Gao"
        },
        {
            "title": "ByteDance Intelligent Creation",
            "content": "Core Contributors, Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, framework specifically designed for real-time interactive humanoid video generation. Built upon MMDiT architecture, FlowActR1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce chunkwise diffusion forcing strategy, complemented by novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves stable 25fps at 480p resolution with time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles. Date: January 16, 2026 Correspondence: Tianshu Hu at tianshu.hu@bytedance.com Project Page: https://grisoon.github.io/FlowAct-R1/ 6 2 0 2 5 1 ] . [ 1 3 0 1 0 1 . 1 0 6 2 : r Figure 1 We present FlowAct-R1, novel framework that enables lifelike, responsive, and high-fidelity humanoid video generation for seamless real-time interaction. 1 Method Neural Voice Puppetry [32] INFP [45] Omnihuman-1.5 [18] KlingAvatar 2.0 [31] LiveAvatar [16] FlowAct-R1 (ours) Stream Real-Time Full-body Control Generalization Vividness Table 1 Comparison of state-of-the-art humanoid video generation methods. FlowAct-R1 simultaneously achieves streaming, real-time generation with fully controllable, generalization, and lifelike video generation capacity."
        },
        {
            "title": "Introduction",
            "content": "Enabling visual humanoid agents to engage in real-time, natural interactions with humans is long-standing objective in the research community [3, 4, 6, 14, 16, 23, 33, 45]. In the task of interactive humanoid video generation, the model is required to synthesize naturalistic videos conditioned on conversational contexts (e.g., audio and text) from both the user and the agent. To realize this vision, several critical challenges must be addressed. First, the model must support streaming and real-time video generation to ensure low-latency and responsive interaction [5, 15, 19, 22, 29, 38, 42]. Furthermore, as multi-round interaction inherently involves extended durations, maintaining visual quality and temporal consistency in long-form video remains non-trivial task [17, 21, 24, 34, 35, 39, 41]. Finally, humanoid interaction involves variety of behavioral states such as speaking, listening, reflecting, and idling. The ability to seamlessly transition between these dynamic states while producing plausible behaviors is essential for achieving perceptual realism and lifelike engagement [3, 4, 14, 23, 45]. To this end, we introduce FlowAct-R1, framework specifically designed for interactive humanoid generation. Built upon MMDiT architecture [12, 28], our approach enables the streaming synthesis of video with arbitrary lengths. It achieves real-time performance while maintaining low-latency responsiveness. The framework facilitates fine-grained controllability over the generated humanoid videoencompassing lip-sync, facial expressions, body gestures, and object interactionsallowing it to adapt naturally to various behavioral states during interactions. Our method produces lifelike videos and demonstrates robust generalization across diverse characters. We believe this framework paves the way for applications such as live streaming, virtual companionship, and video conferencing. Early research on humanoid video generation has primarily centered on lip-synchronization [13, 25, 32, 40, 43, 44]. By conditioning on audio signals, these methods synthesize mouth movements precisely aligned with speech. While these techniques have reached commercial maturity [1, 2], their scope remains largely confined to the facial region, lacking fine-grained control over full-body gestures. This inherent limitation hinders the generation of highly expressive and naturalistic behaviors necessary for truly lifelike interaction. Recently, diffusion-based generative models have demonstrated significant breakthroughs in visual quality [7, 10, 18, 20, 31]. Although these approaches can precisely manipulate body dynamics to synthesize vivid motion, they often suffer from heavy computational overhead, leading to prohibitively slow inference speeds. Furthermore, most existing diffusion frameworks are restricted to short-clip generation and lack support for continuous streaming, which limits their deployment in real-time interactive scenarios. Meanwhile, several efforts have specifically targeted interactive tasks. While methods such as INFP [45] and ARIG [14] enable real-time streaming for long-form video, they are predominantly constrained to portrait-style cropping. Other approaches, such as TalkingMachines [23] and LiveAvatar [16], achieve real-time streaming performance through model distillation or engineering optimizations but still exhibit perceptual gap in terms of behavioral vividness and naturalness. In light of the limitations identified above, we introduce FlowAct-R1, framework designed expressly for interactive humanoid video generation. comprehensive comparison between our method and existing stateof-the-art approaches is presented in Table 1, highlighting FlowAct-R1s unique capability to simultaneously achieve streaming, real-time performance, and high-fidelity behavioral expressivity. Our main contributions are summarized as follows: 2 Streaming and Infinite-Length Generation: Leveraging chunkwise diffusion forcing strategy, we adapt MMDiT backbone [12, 28] for streaming synthesis. To alleviate error accumulation over extended sequences, we design self-forcing [15] variant to bridge the gap between training and inference, complemented by simple memory strategy to promote long-term temporal consistency. Real-time Inference with Low Latency: We optimize the model across both algorithmic and system-level dimensions. By employing efficient distillation techniques [27, 30, 36, 37], we reduce the denoising process to only 3 NFEs. Combined with operator-level optimizations and parallel computing, our framework achieves real-time 480p video generation at 25fps with time-to-first-frame (TTFF) of around 1.5s. Vividness and Generalization: Our method preserves the robust generalization of its foundational model, enabling high-fidelity synthesis from single reference image across various character styles. Furthermore, it supports holistic control via audio and text, allowing for natural transitions between diverse interactive states and significantly outperforming existing SOTA models in behavioral vividness."
        },
        {
            "title": "2 Approach",
            "content": "We propose FlowAct-R1, real-time streaming video generation framework architected upon Seedance[12, 28]. Serving as the backbone, Seedance is Multimodal Diffusion Transformer (MMDiT) featuring native crossmodal alignment capabilities. To achieve indefinite-length streaming, FlowAct-R1 employs chunkwise diffusion forcing strategy [5, 15, 19, 22, 34] augmented by structured memory bank. By integrating shortclip curriculum training with system-level inference optimizations, we realize low-latency, interactive video generation. Furthermore, through fine-grained annotation on behavior-rich human datasets, our method enables the generation of vivid, text-controllable human dynamics."
        },
        {
            "title": "2.1 Overall Pipeline",
            "content": "FlowAct-R1 inherits the foundational architecture of Seedance. The input video stream is compressed temporally and spatially into latent tokens via VAE, while text prompts are encoded into semantic tokens. For the audio branch, inspired by OmniHuman-1.5 [18], we utilize Whisper [26] to compress audio input into aligned acoustic tokens. These multimodal representations are fused within the MMDiT, where information extraction and exchange occur via cross-attention mechanisms. The efficient design of Seedancecharacterized by reduced parameters, shot-based temporal slicing, and window-based spatial attentionprovides the requisite computational speed for real-time interaction. The core of our streaming inference is fixed-size stream buffer tailored to the MMDiTs sequence length constraints. It comprises four key components: 1. Reference Latent: single input reference image used to anchor identity and temporal consistency. 2. Long-term Memory Queue: buffer of fully denoised latents (max size 3) from earlier chunks, preserving long-range action dependencies. 3. Short-term Memory Latent: The most recently fully denoised latent, utilized to enforce local motion smoothness. 4. Denoising Stream: queue organized as 3 chunks 3 latents per chunk, currently undergoing parallel gradient updates. This structured design guarantees continuous output of 0.5 seconds of video for every 0.5 seconds of wall-clock time (corresponding to one chunk), supporting 3-NFE (Number of Function Evaluations) inference cycle while maintaining temporal coherence."
        },
        {
            "title": "2.2 Training Procedure and Framework Design",
            "content": "Our training pipeline follows three-stage curriculum: (1) Autoregressive Adaptation, transforming the general-purpose MMDiT into streaming-compatible variant; (2) Joint Audio-Motion Training, integrating the speech module; and (3) Distillation, employing distillation to compress the inference process to 3 NFEs. 3 Figure 2 Overview of the FlowAct-R1 framework. It consists of training and inference stages: training integrates converting base full-attention DiT to streaming AR model via autoregressive adaptation, joint audio-motion finetuning for better lip-sync and body motion, multi-stage diffusion distillation; inference adopts structured memory bank (Reference/Long/Short-term Memory, Denoising Stream) with chunkwise autoregressive generation and memory refinement. Complemented by system-level optimizations, it achieves 25fps real-time 480p video generation (TTFF 1.5s) with vivid behavioral transitions. We utilize composite dataset of general video samples and conversation videos. All data is pre-processed into clips with dense annotations to facilitate granular streaming control. To ensure robustness in streaming, we adopt two distinct AR training paradigms: intra-prompt segment training to learn local dependencies, and cross-prompt segment training to model smooth transitions between segments with differing text prompts. Additionally, native Image-to-Video (I2V) capabilities are retained via weighted loss during training to guarantee coherent initialization of the first generated segment. The final distillation stage balances denoising quality with real-time performance without sacrificing core capabilities. Furthermore, we implement fake-causal attention mechanism during both training and inference. Specifically, the attention mask is designed such that the denoising stream has full visibility over the reference, memory, and itself, whereas the reference and memory components are restricted from attending to the denoising stream. This asymmetric design reduces computational overhead and ensures that the fully denoised information remains stable across deepening DiT layers, serving as robust and uncorrupted anchor for the generation process. Additionally, drawing on Self-Forcing++ [8], to align the cumulative memory errors between training and inference, we use an intermediate trained model to perform noise injection and denoising on groundtruth video latents to obtain generated-GT-latents. During training, we probabilistically select generated-GT-latents instead of GT-latents when sampling memory components to simulate inference-stage memory errors, which helps mitigate cumulative errors during inference."
        },
        {
            "title": "2.3 Multimodal Fusion",
            "content": "The MMDiT architecture of Seedance [12, 28] inherently supports text-visual fusion. For the audio branch, inspired by OmniHuman-1.5 [18], we adopt an IP-Adapter-style approach to correlate audio signals with fine-grained motions, such as lip synchronization and body dynamics, via cross-attention. During both training and inference, the video is processed at 25 FPS. Audio input (16kHz) is converted into 25 features per second using Whisper. These features are then aggregated into condition vectors with temporal overlap. Text prompts are specifically annotated to describe detailed human behaviors within short-range intervals and periodically updated during inference. Utilizing such short, action-dense clips better adapts to low-latency streaming requirements while ensuring accurate motion response and behavioral diversity."
        },
        {
            "title": "2.4 Model Acceleration",
            "content": "We design multi-stage distillation pipeline specifically tailored to FlowAct-R1s architecture and its unique training-inference paradigm. This pipeline progressively reduces the sampling cost to highly efficient 3 NFEs (chunk-size=3, micro-step=1, without CFG), achieving an 8 acceleration while maintaining competitive synthesis quality. Prior to step distillation, we first eliminate the overhead of classifier-free guidance (CFG). This is achieved by injecting an auxiliary CFG embedding layer and distilling outputs from various guidance scales into single, unified model. We then perform naive step distillation [27], where original NFEs are partitioned into three segments. Within each segment, constituent micro-steps are distilled into single step. Following step distillation, we apply few-step score distillation - DMD [36]. Crucially, both the student and fake models are initialized from the checkpoint obtained in the previous stage to ensure training stability and convergence. To better align the distilled model with streaming generation trajectories, we further modify DMD by chunking training videos and explicitly simulating FlowAct-R1s progressive rollout behavior during online generation and backward simulation. Beyond diffusion distillation, we implement comprehensive suite of optimizations to achieve consistent real-time generation performance. Specifically, we strategically employ FP8 quantization across selection of attention and linear layers, effectively boosting inference throughput with minimal impact on synthesis quality. To further streamline distributed inference, we transition from token-level sequence parallelism to frame-level hybrid-parallel strategy. This reconfiguration significantly reduces the all-to-all communication overhead and mitigates network bottlenecks. To minimize the data movement overhead between HBM and SRAM [9], frequent operators are fused into single kernels within each DiT block. Furthermore, we decouple DiT denoising and VAE decoding into an asynchronous pipeline to enable concurrent execution. Consequently, these optimizations enable FlowAct-R1 to achieve real-time 480p video synthesis at 25 fps on the NVIDIA A100 platform, while restricting the Time-to-First-Frame (TTFF) to approximately 1.5 seconds."
        },
        {
            "title": "2.5 Inference Optimization",
            "content": "To enhance consistency in long-duration videos and mitigate error accumulation during streaming, our empirical analysis reveals that the short-term memory exerts the most significant influence on the denoising stream, and consequently, cumulative errors manifest earliest within this component. To address this, we introduce Memory Refinement strategy. At regular intervals, we conduct noise injection and denoising repair operations on short-term memory frames. During the denoising phase, copy of reference and long-term memory are employed as stable guidance constraints. This process effectively rectifies artifacts accumulated from continuous streaming. Combined with our structured memory bank and chunked denoising framework, this optimization ensures that our long-video generation maintains sustained motion smoothness and identity consistency."
        },
        {
            "title": "2.6 Multimodal Action Planning",
            "content": "To enhance behavioral naturalness, we integrate Multimodal Large Language Model (MLLM) into the action planning process. At regular short intervals, the latest audio segment (speech content) and reference 5 image are fed into the MLLM, which predicts plausible subsequent actions aligned with contextual cues and visual constraints. These action priors guide the MMDiT backbone, ensuring smooth and natural transitions between interactive states."
        },
        {
            "title": "3 Experiments",
            "content": "We compare FlowAct-R1 with three SOTA methods: KlingAvatar 2.0 [31], LiveAvatar [16], and Omnihuman1.5 [18]. Omnihuman-1.5 shares similar network structure but only supports up to 30s video without real-time streaming; KlingAvatar 2.0 achieves 5min long-duration and high visual quality yet lacks streaming capability and suffers from motion repetition. LiveAvatar, based on Wan2.2-S2V-14B [11], enables real-time streaming but also faces motion repetition issues that reduce naturalness. We evaluate all methods via user study using the GSB (good-same-bad) metric. The key evaluation metrics mainly include motion naturalness, lip-sync accuracy, frame structure stability, and motion richness. For consistency, FlowAct-R1 and LiveAvatar use full-length audio, while Omnihuman-1.5s audio is truncated to 30s and KlingAvatar 2.0s to 5min (matching their maximum video durations). We invited 20 participants for this user study. As shown in Fig. 3, results show FlowAct-R1 outperforms competitors by simultaneously supporting long-duration streaming, real-time responsiveness (25fps at 480p, TTFF around 1.5s), and superior behavioral naturalnessattributed to MLLM-guided action planning and chunkwise diffusion forcing that mitigate motion repetition. Figure 3 Comparisons with KlingAvatar 2.0 [31], LiveAvatar [16], and Omnihuman-1.5 [18] via user study using the GSB (good-same-bad) metric. The orange segments indicate the percentage of user votes favoring FlowAct-R1 over other methods. Video demos are shown in our project page."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we presented FlowAct-R1, an interactive humanoid video generation framework aimed at synthesizing lifelike agents that can engage with humans through continuous and responsive video. Our framework enables streaming, arbitrary-duration video generation while maintaining superior temporal consistency and visual fidelity. Through synergistic model-level distillation and system-level optimizations, our method achieves stable real-time performance with low-latency responsiveness. Notably, the model delivers exceptional behavioral vividness and perceptual realism, capturing subtle human nuances for natural transitions across complex interactive states, while maintaining high-quality synthesis across diverse character styles from single reference image. Ethical Considerations. There is potential risk that our method could be misused to fabricate deceptive or harmful content. To mitigate this, we are committed to the responsible deployment of our technology and will implement rigorous access control policy for our core models, ensuring they are provided only to verified entities for legitimate and ethical use. Notably, all human images used in our demonstrations were generated by AI tools (e.g. Gemini or GPT-4o) to ensure privacy and copyright compliance."
        },
        {
            "title": "References",
            "content": "[1] Heygen. URL https://www.heygen.com/. [2] Synthesia. URL https://www.synthesia.io/. [3] Vasu Agrawal, Akinniyi Akinyemi, Kathryn Alvero, Morteza Behrooz, Julia Buffalini, Fabio Maria Carlucci, Joy Chen, Junming Chen, Zhang Chen, Shiyang Cheng, Praveen Chowdary, Joe Chuang, Antony DAvirro, Jon Daly, Ning Dong, Mark Duppenthaler, Cynthia Gao, Jeff Girard, Martin Gleize, Sahir Gomez, Hongyu Gong, Srivathsan Govindarajan, Brandon Han, Sen He, Denise Hernandez, Yordan Hristov, Rongjie Huang, Hirofumi Inaguma, Somya Jain, Raj Janardhan, Qingyao Jia, Christopher Klaiber, Dejan Kovachev, Moneish Kumar, Hang Li, Yilei Li, Pavel Litvin, Wei Liu, Guangyao Ma, Jing Ma, Martin Ma, Xutai Ma, Lucas Mantovani, Sagar Miglani, Sreyas Mohan, Louis-Philippe Morency, Evonne Ng, Kam-Woh Ng, Tu Anh Nguyen, Amia Oberai, Benjamin Peloquin, Juan Pino, Jovan Popovic, Omid Poursaeed, Fabian Prada, Alice Rakotoarison, Rakesh Ranjan, Alexander Richard, Christophe Ropers, Safiyyah Saleem, Vasu Sharma, Alex Shcherbyna, Jia Shen, Jie Shen, Anastasis Stathopoulos, Anna Sun, Paden Tomasello, Tuan Tran, Arina Turkatenko, Bo Wan, Chao Wang, Jeff Wang, Mary Williamson, Carleigh Wood, Tao Xiang, Yilin Yang, Julien Yao, Chen Zhang, Jiemin Zhang, Xinyue Zhang, Jason Zheng, Pavlo Zhyzheria, Jan Zikes, and Michael Zollhoefer. Seamless interaction: Dyadic audiovisual motion modeling and large-scale dataset, 2025. URL https://arxiv.org/abs/2506.22554. [4] Tenglong Ao. Body of her: preliminary study on end-to-end humanoid agent. arXiv, 2024. [5] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion, 2024. URL https://arxiv.org/abs/2407.01392. [6] Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, Xiaoqiang Liu, and Pengfei Wan. Midas: Multimodal interactive digital-human synthesis via real-time autoregressive video generation, 2025. URL https://arxiv.org/abs/2508.19320. [7] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer, 2024. [8] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Self-forcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. [9] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [10] Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, and Steven Hoi. Omniavatar: Efficient audio-driven avatar video generation with adaptive body animation, 2025. URL https://arxiv.org/abs/2506.18866. [11] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, and Lian Zhuo. Wan-s2v: Audio-driven cinematic video generation, 2025. URL https://arxiv.org/abs/2508.18621. [12] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, and Feilong Zuo. Seedance 1.0: Exploring the boundaries of video generation models, 2025. URL https://arxiv.org/abs/2506.09113. [13] Jiazhi Guan, Zhanwang Zhang, Hang Zhou, Tianshu HU, Kaisiyuan Wang, Dongliang He, Haocheng Feng, Jingtuo Liu, Errui Ding, Ziwei Liu, and Jingdong Wang. Stylesync: High-fidelity generalized and personalized lip sync in style-based generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [14] Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, and Xiaoming Wei. Arig: Autoregressive interactive head generation for real-time conversations, 2025. URL https://arxiv.org/abs/2507.00472. [15] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 7 [16] Yubo Huang, Hailong Guo, Fangtai Wu, Shifeng Zhang, Shijie Huang, Qijun Gan, Lin Liu, Sirui Zhao, Enhong Chen, Jiaming Liu, and Steven Hoi. Live avatar: Streaming real-time audio-driven avatar generation with infinite length, 2025. URL https://arxiv.org/abs/2512.04677. [17] Jack Parker-Holder and Shlomi Fruchter. Genie 3: new frontier for world models. Technical report, Google DeepMind, 2025. URL https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/. [18] Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, and Mingyuan Gao. Omnihuman-1.5: Instilling an active mind in avatars via cognitive simulation, 2025. URL https://arxiv.org/abs/2508.19209. [19] Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, and Yue Zhao. Streamdit: Real-time streaming text-to-video generation, 2025. URL https://arxiv.org/abs/2507.03745. [20] Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. [21] Wuyang Li, Wentao Pan, Po-Chien Luan, Yang Gao, and Alexandre Alahi. Stable video infinity: Infinite-length video generation with error recycling, 2025. URL https://arxiv.org/abs/2510.09212. [22] Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, and Shijian Lu. Rolling forcing: Autoregressive long video diffusion in real time, 2025. URL https://arxiv.org/abs/2509.25161. [23] Chetwin Low and Weimin Wang. Talkingmachines: Real-time audio-driven facetime-style video via autoregressive diffusion models, 2025. URL https://arxiv.org/abs/2506.03099. [24] Yuta Oshima, Yusuke Iwasawa, Masahiro Suzuki, Yutaka Matsuo, and Hiroki Furuta. Worldpack: Compressed memory improves spatial consistency in video world modeling, 2025. URL https://arxiv.org/abs/2512.02473. [25] Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C.V. Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia, MM 20, page 484492, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379885. doi: 10.1145/3394171.3413532. URL https://doi.org/10.1145/3394171.3413532. [26] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv.org/abs/2212.04356. [27] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022. URL https://arxiv.org/abs/2202.00512. [28] Team Seedance. Seedance 1.5 pro: native audio-visual joint generation foundation model, 2025. URL https://arxiv.org/abs/2512.13507. [29] Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Shechtman, and Xun Huang. Motionstream: Real-time video generation with interactive motion controls, 2025. URL https://arxiv.org/abs/ 2511.01266. [30] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. URL https: //arxiv.org/abs/2303.01469. [31] Kling Team, Jialu Chen, Yikang Ding, Zhixue Fang, Kun Gai, Yuan Gao, Kang He, Jingyun Hua, Boyuan Jiang, Mingming Lao, Xiaohan Li, Hui Liu, Jiwen Liu, Xiaoqiang Liu, Yuan Liu, Shun Lu, Yongsen Mao, Yingchao Shao, Huafeng Shi, Xiaoyu Shi, Peiqin Sun, Songlin Tang, Pengfei Wan, Chao Wang, Xuebo Wang, Haoxian Zhang, Yuanxing Zhang, and Yan Zhou. Klingavatar 2.0 technical report, 2025. URL https://arxiv.org/abs/ 2512.13313. [32] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias Nießner. Neural voice puppetry: Audio-driven facial reenactment. ECCV 2020, 2020. [33] You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Guoxian Song, Xiaochen Zhao, Chao Liang, Jianwen Jiang, Hongyi Xu, and Linjie Luo. X-streamer: Unified human world modeling with audiovisual interaction, 2025. URL https://arxiv.org/abs/2509.21574. [34] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, and Song Hanand Yukang Chen. Longlive: Real-time interactive long video generation. 2025. [35] Hongwei Yi, Tian Ye, Shitong Shao, Xuancheng Yang, Jiantong Zhao, Hanzhong Guo, Terrance Wang, Qingyu Yin, Zeke Xie, Lei Zhu, Wei Li, Michael Lingelbach, and Daquan Zhou. Magicinfinite: Generating infinite talking videos with your words and voice. to be updated, 2025. [36] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. [37] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. [38] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. [39] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval, 2025. URL https://arxiv.org/abs/2506.03141. [40] Longhao Zhang, Shuang Liang, Zhipeng Ge, and Tianshu Hu. Personatalk: Bring attention to your persona in visual dubbing. In SIGGRAPH Asia 2024 Conference Papers, pages 19, 2024. [41] Lvmin Zhang, Shengqu Cai, Muyang Li, Gordon Wetzstein, and Maneesh Agrawala. Frame context packing and drift prevention in next-frame-prediction video diffusion models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [42] Dingcheng Zhen, Shunshun Yin, Shiyang Qin, Hou Yi, Ziwei Zhang, Siyuan Liu, Gan Qi, and Ming Tao. Teller: Real-time streaming audio-driven portrait animation with autoregressive motion generation, 2025. URL https://arxiv.org/abs/2503.18429. [43] Weizhi Zhong, Chaowei Fang, Yinqi Cai, Pengxu Wei, Gangming Zhao, Liang Lin, and Guanbin Li. Identitypreserving talking face generation with landmark and appearance priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97299738, June 2023. [44] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. Pose-controllable talking face generation by implicitly modularized audio-visual representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [45] Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Hu, Shuang Liang, and Zhipeng Ge. Infp: Audio-driven interactive head generation in dyadic conversations. arXiv preprint arXiv:2412.04037, 2024."
        }
    ],
    "affiliations": [
        "Bytedance"
    ]
}