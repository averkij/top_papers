{
    "paper_title": "MASPRM: Multi-Agent System Process Reward Model",
    "authors": [
        "Milad Yazdani",
        "Mahdi Mostajabdaveh",
        "Zirui Zhou",
        "Ying Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to partial inter-agent transcripts and acts as an inference-time controller. MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts without requiring step-level human annotations, by propagating returns to local targets. At inference, MASPRM guides step-level beam search and MCTS, focusing computation on promising branches and pruning early. On GSM8K and MATH, MASPRM-guided decoding with an outcome reward model (ORM) applied to the final answer, improves exact match (EM) over a single straight-through MAS pass by $+30.7$ and $+22.9$ points, respectively. A MASPRM trained on GSM8K transfers zero-shot to MATH without retraining, adding $8.4$ EM points at the same budget. MASPRM is a plug-in value model that estimates per-agent progress and complements verifier-style decoders, enabling more reliable, compute-aware multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM"
        },
        {
            "title": "Start",
            "content": "MASPRM: Multi-Agent System Process Reward Model Milad Yazdani1*, Mahdi Mostajabdaveh2, Zirui Zhou2, Ying Xiong2 1Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC V6T1Z4, Canada 2Huawei Technologies Canada, Burnaby, BC V5C6S7, Canada Correspondence: mahdi.mostajabdaveh1@huawei.com"
        },
        {
            "title": "Abstract",
            "content": "Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to partial inter-agent transcripts and acts as an inferencetime controller. MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts without requiring step-level human annotations, by propagating returns to local targets. At inference, MASPRM guides steplevel beam search and MCTS, focusing computation on promising branches and pruning early. On GSM8K and MATH, MASPRMguided decoding with an outcome reward model (ORM) applied to the final answer, improves exact match (EM) over single straightthrough MAS pass by +30.7 and +22.9 points, respectively. MASPRM trained on GSM8K transfers zero-shot to MATH without retraining, adding 8.4 EM points at the same budget. MASPRM is plug-in value model that estimates per-agent progress and complements verifier-style decoders, enabling more reliable, compute-aware multi-agent reasoning. Code: https://github.com/milad1378yz/ MASPRM 5 2 0 2 8 2 ]"
        },
        {
            "title": "A\nM",
            "content": ". [ 1 3 0 8 4 2 . 0 1 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have made multistep problem solving increasingly practical. However, reliability still drops when single pass must explore alternatives, justify choices, and coordinate subtasks (Zhang et al., 2024). Multi-Agent Systems (MAS) offer different stance: multiple specialized agents converse over communication graph, cross-check one another, and decompose *Intern at Huawei Technologies Canada during this work Corresponding Author 1 Figure 1: Token-accuracy trade-off on GSM8K. Each point reports exact match (y-axis) versus average testtime tokens (x-axis; 103). MASPRM consistently shifts the frontier upward: it improves Maj@5 and stepwise beam search (SBS) under matched budgets, and when paired with MCTS and an ORM, it reaches 74.6% EM at 19k tokens, while Greedy sits near 43.9% EM at 1.6k tokens. work so that no single context or policy bears the entire burden. When paired with structured exploration and deliberation, such collaboration can outperform single-shot prompting by distributing computation across complementary roles (Wang et al., 2022; Yao et al., 2023). In parallel, the community has learned that how we spend test-time compute often matters more than how many parameters we have: procedures that expand, verify, and rank partial solutions yield substantial gains in correctness and calibration (Snell et al., 2024; Cobbe et al., 2021; Lightman et al., 2023). Despite this promise, the inference-time search in MAS remains unreliable for two main reasons. First, outcome-only evaluation (e.g., majority vote or final-answer verifiers) is too sparse to steer long Figure 2: Search-generated supervision. Left: Extracted rollouts yield edge-level estimates ˆQ(s, a); for each child = next(s, a) we set the regression target = ˆQ(s, a) and train the MASPRM Vϕ(s) accordingly. Right: Uses of MASPRM in multi-agent systems, including inference-time guidance (SBS and MCTS via Vϕ and ˆQ). inter-agent exchanges; it provides little information about which message, from which agent, moved the system closer to solution (Cobbe et al., 2021). Second, errors propagate across turns; without mechanism that scores intermediate states, MAS tends to keep extending unpromising branches simply because they are likely under the policy, not because they are helpful. Prior work in single-agent settings shows that progress-aware feedback can guide inference-time search to where improvement is most likely (Setlur et al., 2024; Lightman et al., 2023; Chen et al., 2024; Guan et al., 2025; Zeng et al., 2025). However, directly transplanting these ideas to MAS is non-trivial. We introduce MASPRM, Process Reward Model (PRM) for Multi-Agent Systems. MASPRM estimates the value of an intermediate inter-agent state. It serves as value head over agents outputs, producing per-action, per-agent scores that serve two roles: (i) an inference time controller that guides expansion and early stopping over agent rollouts, and (ii) per-agent progress signal for analysis and improving MAS. Conceptually, MASPRM transforms long, entangled dialogue into sequence of value-estimated actions at the agent-level, enabling MAS to allocate compute to promising branches rather than to long but unproductive ones. Developing PRMs for MAS presents challenges beyond traditional PRMs for single-agent reasoning chains. Granular steps and substeps. In single-agent chain-of-thought, step typically corresponds to small reasoning act. In MAS, single turn expands into multiple substeps (e.g., planning, routing, tool invocation, and cross-agent summarization); thus, scores should be defined at the level of inter-agent states rather than tokens. Scheduleand topology dependence. In MAS, progress is scheduleand topology-dependent: the value of an intermediate state depends on who acts next and what operation they perform (e.g., review, summarize, vote, aggregate). Even with identical agents, non-commutative operations make order matter; in single-agent reasoning chains, this dependence largely collapses because the next actor and operation are fixed. Heterogeneous agents. Beyond ordering, MAS agents can differ in role, tool access, system prompt, and even base model. Consequently, the same intermediate state can have different expected value depending on the next agents capabilities and interface. PRM must be robust to changing agent identity and I/O surface, whereas single-agent chains keep the policy fixed across steps. Partial observability. In MAS, messages propagate over communication graph, so each agent observes only subset of the global state (in contrast to single-agent reasoning, which typically has access to the full prior chain). We therefore score each inter-agent state using only the information available to the acting agent at that moment, its locally visible intermediate state, so the value function remains well-defined under partial observability induced by the communication topology. These design choices draw on lessons from structured exploration (Wang et al., 2022; Yao et al., 2023; Snell et al., 2024) and verifierguided decoding (Cobbe et al., 2021; Lightman et al., 2023), while remaining compatible with implicit process signals learned from outcome-only data (Cui et al., 2025). Contributions. We formulate MASPRM, per-action, peragent process reward model that estimates the value of intermediate inter-agent states and acts as an inference time controller for MAS under fixed budgets. We present search-generated supervision recipe that trains MASPRM from MASMCTS rollouts without manual step annotations while preserving progress sensitivity. We instantiate MASPRM-guided decoding via step-level beam search and value-guided MCTS over agent rollouts, improving decision quality and pruning unproductive intermediate inter-agent states early. Key Results Relative to single straight-through MAS pass (Greedy), our best MASPRM-guided configuration delivers large absolute accuracy gains; see Fig. 1 for the GSM8K compute/accuracy frontier. On GSM8K we observe +30.7 point gain, and on MATH +22.9 point gain."
        },
        {
            "title": "2 Related Work",
            "content": "MAS optimization and agent reinforcement learning (RL). Recent MAS research optimizes communication graphs, roles, and tool routing to improve coordination and efficiency (Zhang et al., 2024; Zhou et al., 2025). Multi-agent reinforcement fine-tuning exposes instability under sparse rewards, underscoring the credit-assignment challenge (Liao et al., 2025). To mitigate delayed rewards, SPA-RL redistributes final rewards into stepwise contributions via learned progress estimator and improves agent performance on interactive benchmarks (Wang et al., 2025). ARMAP automatically learns reward models from environment interaction (no human labels) and couples them with planning to evaluate trajectories and guide autonomous agents (Chen et al., 2025). Beyond these, LLM-as-judge methods enlist centralized language-model critic to assign per-agent credit, either by generating dense agent-specific rewards from natural-language task descriptions (Lin et al., 2025) or by numerically decomposing team rewards into individualized signals (Nagpal et al., 2025), but recurring LLM inference introduces overheads that limit scalability for long inter-agent dialogues. In contrast, MASPRM serves as an efficient inference time controller: it scores messages and tool calls across the communication graph to steer multi-agent rollouts and provides per-agent progress signals. PRM training and process supervision. PRMs provide dense, progress-aware feedback and have been used to guide search and prune low-promise branches (Setlur et al., 2024; Lightman et al., 2023). To curb labeling cost, search-generated supervision with MCTS constructs step-level signals (ALPHAMATH ALMOST ZERO) (Chen et al., 2024); RSTAR-MATH pairs PRMs with MCTS to reach state-of-the-art math results (Guan et al., 2025); VERSAPRM trains multi-domain PRMs usable at inference (Zeng et al., 2025); and outcome-only training can yield implicit process rewards (Cui et al., 2025). For agents specifically, Process Reward Models for LLM Agents proposes AGENTPRM/INVERSEPRM, practical framework for agent learning with process rewards (Choudhury, 2025); and automated process supervision (OMEGAPRM) scales PRM data collection for math (Luo et al., 2024). Unlike these, which largely target single-agent chains or domainspecific pipelines, we develop PRMs for MAS: our MASPRM scores inter-agent states to guide multi-agent search at inference and delivers dense, per-agent scores to stabilize collective optimization."
        },
        {
            "title": "3 Preliminaries",
            "content": "Problem and trajectories. Each instance consists of question and hidden ground-truth answer y. An episode is finite inter-agent dialogue that halts when special \"Final Answer\" message is routed to terminal node; correctness is evaluated only at termination. Multi-agent communication graph. We model the system as directed graph = (cid:0)V {q, sink}, E(cid:1), = {1, . . . , }, where every is an agent, is non-acting question node that holds x, and sink is non-acting terminal node with no outgoing edges. An edge (i j) denotes that may route message to j. Let the adjacency {0, 1}(N +2)(N +2) satisfy Aij = 1 iff (i j) E. For {q, sink} define Nin(u) = { : (v u) }, Nout(u) = { : (u w) }. Schedule and horizon. deterministic schedule σ : {1, . . . , } specifies the agent at depth t; the agent it = σ(t) produces the t-th response. The horizon upper-bounds the number of agent turns. 3 States, actions, and transitions. Let Ht1 be the transcript after t1 agent turns: an ordered list of triples (speaker, message, recipient) consistent with G. state is st = (cid:0)x, Ht1, it (cid:1), containing the question (x), the partial transcript, and the identity of the next agent. When it acts at st, it chooses an action at Ait(st), realized as textual response together with recipient rt Nout(it), producing the child state via the deterministic constructor st+1 = next(st, at), which appends (it, at, rt) to the transcript and sets the next agent to it+1 = σ(t+1) if rt = sink. The recipient rt determines which messages are routed and visible to later agents, but it does not affect the identity of the next agent. If rt = sink, the episode terminates. Terminal reward and return. Let sT be terminal and ˆy be the extracted final answer from the last message. Define the terminal reward R(sT ) = (cid:40) +1, 1, ˆy = y, ˆy = y, and R(s) = 0 for non-terminal s. For any state and admissible action a, (s) = E[R(sT ) s] , Q(s, a) = E[R(sT ) s, a] . Policies and candidate proposals. Each agent is paired with policy π(i)( s) that induces proposal distribution over textual responses and valid recipients. During search, finite candidate set Ct Ait(st) is obtained by sampling from π(it). likelihood (length-normalized logPolicy likelihood). To compare against policy-only guidance, we use the length-normalized loglikelihood (Wu et al., 2016; Yang et al., 2018; Koehn and Knowles, 2017) ψpol(a st) = 1 L (cid:88) ℓ=1 log pθ (cid:0)wℓ w<ℓ, st (cid:1), (1) where w1:L are the generated tokens for action a. This single scalar can rank candidates. Process value head. process value head Vϕ : [1, 1] maps any (intermediate) state to its predicted expected terminal reward. Because encodes the next agent, we interpret Vϕ as peragent, per-step local value: for an edge (st, at) with agent it, the local value of its decision is (cid:0)next(st, at)(cid:1). Vϕ When referring to an agent node acting at depth t, (cid:0)next(st, at)(cid:1), we write Vϕ(v) as shorthand for Vϕ i.e., the value of the post-action state immediately after vs message. MCTS bookkeeping. For each visited edge (s, a) we maintain visit count (s, a) and running value sum (s, a) with empirical estimate ˆQ(s, a) = (s, a) max{1, (s, a)} . Selection uses the upper confidence bound for trees (UCT) (Kocsis and Szepesvári, 2006) with constant cUCT > 0: (cid:40) = arg max aAit (st) ˆQ(st, a) + cUCT (cid:115) ln(cid:0)1 + (cid:80) (st, b)(cid:1) 1 + (st, a) (cid:41) . (2) All successors must respect and the schedule σ. ORM. An ORM Υ maps terminal pair (x, ˆy) to [1, 1] and is only defined on terminal states. It is not used on intermediate states. Notation for budgets. We use simulations per instance (expansion budget), the depth cap, and Cmax the per-node candidate cap."
        },
        {
            "title": "4 MASPRM",
            "content": "It Overview. MASPRM is process reward model that supplies per-step, per-agent value estimates via shared head Vϕ. is trained from search-generated supervision constructed by MASspecific MCTS; no manual annotations are required. The same UCT rule in Eq. (2) is used both during label generation (training) and for inference-time search. An overview of MASPRM is shown in Fig. 2. 4.1 MASPRM training MCTS phases (training). For each instance, MAS-MCTS proceeds with the standard four phases, all constrained by and σ: (i) Selection. Starting at the root state, repeatedly choose actions by UCT (2) until reaching frontier 4 state. state is frontier state if and only if it is terminal or not yet fully expanded under the current candidate cap. (ii) Expansion. If the frontier state st is nonterminal, sample candidate set Ct Ait(st) from π(it) and, for each Ct, create the child = next(st, a). We omit virtual visit initialization during training to avoid biasing Monte Carlo targets used for supervision. (iii) Evaluation (ground truth only). Continue selection/expansion until terminal leaf sT is reached, and set the leaf value vleaf (sT ) = R(sT ) {1, +1}, which corresponds to λ=1 in the unified rule of Eq. (3). (iv) Backpropagation. For every edge (s, a) on the selected path, update 4.2 Inference-time guidance Leaf initializer and terminal mixing. We use two standard knobs: (i) leaf initializer fINIT for first-visit child nodes, and (ii) single mixing coefficient λ [0, 1] applied only when terminal signal is available. On the first visit to frontier child = next(st, a) we may set virtual visit by (st, a) 1, (st, a) fINIT(st, a), ˆQ(st, a) fINIT(st, a), with fINIT(st, a) = Vϕ(s) (MASPRM initializer) or fINIT(st, a) = ψpol(a st) (mean token loglikelihood). Leaf evaluation uses single unified rule: (s, a) (s, a) + 1, (s, a) (s, a) + vleaf (sT ), (s, a) (s, a) ˆQ(s, a) = . vleaf (s) = R(s), training and terminal (we set λ=1), (1 λ) Vϕ(s) + λ Υ(cid:0)x, ˆy(s)(cid:1), inference with ORM and terminal, concise summary appears in Algorithm 1 (App. D). Edge to child conversion and regression targets. After completing rollouts, for each visited edge (s, a) with child = next(s, a), define the process-level target yproc(s) ˆQ(s, a) [1, 1], which is Monte Carlo estimate of Q(s, a) backed up from terminal rewards. Collect Dproc = {(s, yproc(s))} over all edges and fit the process value head Vϕ offline via min ϕ 1 Dproc (cid:88) ℓ(cid:0)Vϕ(s), y(cid:1), (s,y)Dproc with bounded convex loss ℓ (e.g., Huber). The head output is range-restricted to [1, 1]. Outcome-only dataset and objective. dently of Dproc, we extract terminal dataset Dout = (cid:8) (x, ˆy(sT ), r) : sT , = R(sT ) {1, +1} (cid:9). IndepenWe then fit the ORM Υθ by 1 Dout min θ (cid:88) (x,ˆy,r)Dout ℓ(cid:0)Υθ(x, ˆy), r(cid:1), again using bounded convex loss (e.g., Huber), with Υθ range-restricted to [1, 1]. At inference, the ORM is optionally used only at terminal leaves (see Eq. (3)). 5 Vϕ(s), otherwise (inference default λ=0). (3) (i) Selection. From MCTS phases (inference). the root, traverse by UCT using the current ˆQ estimates. (ii) Expansion. At the first visit to frontier nonterminal state st, sample Ct from π(it); for each Ct, create = next(st, a) and initialize virtual visit with fINIT(st, a) = Vϕ(s) unless otherwise stated. (iii) Evaluation. When simulation stops at (terminal), apply Eq. (3). We set λ=0 by default (pure bootstrap). (iv) Backpropagation. Update (N, W, ˆQ) along the traversed path exactly as in training: + 1, + vleaf , ˆQ = W/N . final transcript is decoded by repeatedly choosing, from the root downward, the action with maximal ˆQ among available children. We summarize the inference time procedure in Algorithm 2 (App. D). Step-level beam search (SBS). At depth the acting agent it expands each beam state Bt by sampling B2 candidates from π(it)( s) and scoring each successor by score(a s) = Vϕ (cid:0)next(s, a)(cid:1), (or by ψpol(a s) for policy-only guidance). We pool all Bt B2 successors and retain the top-B1 by score to form the next beam Bt+1, repeating until terminal enters the beam or = . We return the terminal path with the highest final state value. The algorithm 3 (App. D) summarizes the SBS we use."
        },
        {
            "title": "5 Experiments & Results",
            "content": "Research Questions (RQ). We design our experiments to answer the following questions: RQ1: MASPRM vs. policy-only at inference. Does MASPRM improve accuracy under matched compute? RQ2: MASPRM vs. ORM. How does MASPRM guidance compare to ORM, and are their gains complementary when combined? RQ3: Zero-shot out-of-distribution (OOD) generalization. Does MASPRM trained on one dataset transfer zero-shot to another and improve EM at matched compute? 5.1 Overall experimental setup In our experiments, we train MASPRM using the Qwen2.5-1.5B pretrained model. For the multiagent system, we adopt the architecture described in App. C, where each agent is instantiated with Qwen2.5-1.5B-Instruct model. We evaluate our method on GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) using exact match (EM) accuracy. Training supervision is obtained from MAS-MCTS rollouts with = 40 simulations and UCT constant cUCT = 4.0, with leaf mixing λ=1 (so vleaf (sℓ) = R(sℓ)), producing process-level value targets in [1, 1] without manual annotation. Throughout, when we say policy likelihood baselines, the baseline scores candidates by the length-normalized log-likelihood as defined in Eq. (1); we use ψpol as single scalar for ranking and (when applicable) initialization. The MASPRM uses tanh regression head and Huber loss, trained on separator tokens with 4-bit QLoRA (Dettmers et al., 2023) (rank 256, dropout 0.05) for 5 epochs, learning rate 1105, and gradient accumulation of 16. At inference, we compare settings at matched agent-call (AC) budgets; MASPRM calls and tokens are reported separately via MC and Tok, respectively: (i) Greedy decoding, (ii) Maj@5 (majority vote over k=5 independently sampled full MAS runs), (iii) SBS with either MASPRM scoring or policy likelihood, and (iv) MASPRM-guided MCTS with = 20 simulations and at most 3 children per node. Candidate generation respects the MAS graph; SBS ranks partial solutions with the chosen scorer; MCTS uses λ=0 (unless stated); decoding proceeds greedily from the root by repeatedly selecting the child with maximal ˆQ. Accounting and budget matching. We report: EM; AC = average number of agent calls (LLM generations) per problem; MC = average number of MASPRM calls per problem (i.e., the average number of times the MASPRM is invoked); and Tok = average number of generated tokens per problem. Let denote the scheduled depth (numIn our fixed ber of agents that act once each). 4-agent schedule, single straight-through MAS pass uses AC = = 4 (one call per agent). If MASPRM scoring is applied once per produced message, this pass also uses MC = 4. For SBS with global beam B1 and per-state samples B2, AC = B2 + (D1) B1 B2 (e.g., B2=5, B1=3, D=4 AC = 50), but the number of AC in MCTS rollouts is not deterministic. Evaluation protocol. The Verifier outputs Final Answer: <value> (see App. C). We extract the value with the regex Final Answer: (.*) and apply the normalization in App. B. We use random seed and report single run metrics.1 Baselines and variants. Maj@5 selects the plurality answer among 5 independent full MAS runs; ties are broken by highest aggregate lengthnormalized log-likelihood of the final answer. Maj@5 (+MASPRM step weights) weights each runs vote by = 1 t=1 Vϕ(st) (mean MASPRM score across the path). +ORM end appends learned ORM Υ (Qwen2.5-1.5B) that scores (q, final answer) (cid:55) [1, 1] after decoding (tanh head similar to MASPRM); selection is by top-Υ among candidates. (cid:80)T Sampling settings. Decoding and search hyperparameters are in App. (Decoding Settings) and App. (Search configurations). Hardware. Experiments run on 8 NVIDIA Tesla V100-PCIE-32GB (32 GB VRAM; driver 535.54.03; CUDA 12.2), an Intel Xeon Gold 6140 CPU @ 2.30 GHz, and 754 GB RAM. 1Max decode tokens per agent: Lmax = 1024; see App. C. 6 5.2 A. MASPRM improves MAS inference under matched budgets Evaluation. We evaluate whether process-level guidance improves multi-agent inference under fixed compute (Q1). All methods share the same MAS graph and schedule (Sec. 3) and are compared at matched AC budgets. We report EM, average AC per problem, average MC per problem, and average Tok per problem. Baselines include Greedy, Maj@5 (five independent full MAS runs; with our 4-agent schedule, this corresponds to 5D=20 agent calls), step-level beam search (SBS) with either policy likelihood scoring (length-normalized loglikelihood) or MASPRM scoring, and MCTS with either policy likelihood or MASPRM guidance. Table 1: MASPRM improves MAS inference at matched compute. Abbreviations: AC=average agent calls per problem; MC=average MASPRM calls per problem; Tok=average generated tokens per problem; Maj@5: majority vote over 5 independent full MAS runs (uses 5 = 20 agent calls with = 4). Notation B1/B2 indicates (global beam / per-state samples). Method Greedy Maj@5 SBS (policy likelihood) 1/5 SBS (MASPRM) 1/5 SBS (MASPRM) 3/5 MCTS (policy likelihood), =20 MCTS (MASPRM), =20 EM (%) AC MC Tok 1.6k 4 8.2k 20 8.1k 20 8.2k 20 17.3k 50 19.4k 96 18.7k 89 43.9 59.7 53.9 57.1 65.4 60.8 72.4 0 0 0 20 50 0 89 Table 2: Same setup as GSM8K. Abbreviations: AC=average agent calls per problem; MC=average MASPRM calls per problem; Tok=average generated tokens per problem; Maj@5: majority vote over 5 independent full MAS runs. Notation B1/B2 indicates (global beam / per-state samples). Method Greedy Maj@5 SBS (policy likelihood) 1/5 SBS (MASPRM) 1/5 SBS (MASPRM) 3/5 MCTS (policy likelihood), =20 MCTS (MASPRM), =20 EM (%) AC MC Tok 1.2k 4 5.6k 20 5.5k 20 5.8k 20 12.5k 50 16.4k 96 15.0k 96 25.1 36.0 34.8 39.7 41.1 35.8 47.3 0 0 0 20 50 0 96 Observation. On GSM8K  (Table 1)  , at matched AC = 20 MASPRM-guided SBS improves EM from 53.9% to 57.1% (+2.2) with similar tokens (8.1k vs. 8.2k). Increasing beam size (SBS 3/5) reaches 65.4% at AC = 50, surpassing Maj@5 (59.7%) by +5.7. With =20 simulations, MCTS (MASPRM) attains 72.4%, exceeding MCTS (policy likelihood) at 60.8% by +11.6 while using fewer agent calls (89 vs. 96) and fewer tokens (18.7k vs. 19.4k). On MATH  (Table 2)  , at matched AC = 20 MASPRM-guided SBS increases EM from 34.8% to 39.7% (+4.9) with comparable tokens (5.5k vs. 5.8k), and SBS 3/5 reaches 41.1% at AC = 50 (+5.1 over Maj@5 at 36.0%). For MCTS with =20 simulations, MASPRM yields 47.3% versus 35.8% for policy likelihood (+11.5) at the same AC (96) and fewer tokens (15.0k vs. 16.4k). 5.3 B. MASPRM guidance vs. ORM Evaluation. We compare process-level guidance during search (MCTS MASPRM), an outcome model used at leaves with backup (MCTS +ORM end; leaves are evaluated by Υ and backed up), and their combination under identical search budgets. For completeness, we also evaluate vote-based reranking variants. Table 3: MASPRM guidance vs. ORM (matched AC). ORM Υ details in App. B. Method MCTS (policy likelihood) MCTS (MASPRM) MCTS (+ORM end) MCTS (MASPRM) + ORM Maj@5 Maj@5 (+MASPRM step weights) Maj@5 (+ORM end) GSM8K EM (%) MATH EM (%) 60.8 72.4 65.3 74.6 59.7 63.8 61.4 35.8 47.3 38.3 48.0 36.0 38.4 37.6 Observation. On GSM8K, MCTS (MASPRM) + ORM achieves 74.6% EM, +30.7 point gain over Greedy (43.9%). Relative to MCTS(policy likelihood) at 60.8%, MASPRM-in-search adds +11.6 points (to 72.4%), and ORM-only yields 65.3%; the combination provides further +2.2 over MASPRM alone. On MATH, MCTS (MASPRM)+ORM reaches 48.0% EM, +22.9 point gain over Greedy (25.1%); MASPRM alone attains 47.3% and ORM-only 38.3%, with the combination adding +0.7 over MASPRM alone. 5.4 C. Zero-shot OOD transfer without retraining Evaluation. We test whether MASPRM trained on GSM8K transfers to MATH without retraining. We compare policy-only search baseline, the frozen OOD MASPRM, and an in-distribution MASPRM (reference). All runs use the same MAS graph, policy, and matched #AC. Observation. At closely matched budgets (AC 94-96; tokens 15k), MASPRM trained on GSM8K transfers zero-shot to MATH with 44.2% EM, which is +8.4 points over MCTS(policy likelihood) without MASPRM (35.8%) and within 3.1 points of the in-distribution MASPRM (47.3%). ing as drop-in controller for existing multi-agent workflows. Because many day-to-day MAS already coordinate planners, solvers, retrievers, tool routers, and verifiers (e.g., collaborative coding, customer support triage, and planning copilots), our approach can immediately make these systems more reliable and compute-aware without changing their base policies or requiring manual annotations. Looking ahead, we will (i) use MASPRM as dense, fine-grained signal for multi-agent RL, both online and from logged traces, with safeguards against reward hacking; (ii) move beyond fixed communication graph by using MASPRM to search for and/or learn the MAS topology and schedule, producing instance adaptive routing under explicit budget constraints; (iii) explore alternative objectives for the PRM beyond pointwise regression, including pairwise ranking, contrastive or advantage style (delta) targets, and distributional/uncertainty aware losses to improve calibration and robustness; and (iv) apply MASPRM across domains such as software engineering, retrieval augmented QA, robotics/planning, and multimodal agent teams, where process-level guidance can deliver practical gains under tight compute budgets. Table 4: Zero-shot transfer of GSM8K-trained MASPRM. Abbrev.: AC=average agent calls/problem; MC=average MASPRM calls/problem; Tok=average generated tokens/problem. The OOD MASPRM is frozen and not re-tuned. Guidance MCTS (policy likelihood), no MASPRM OOD MASPRM (frozen) In-distribution MASPRM (reference)"
        },
        {
            "title": "6 Discussion",
            "content": "EM (%) AC MC Tok 16.4k 96 15.3k 94 15.0k 96 35.8 44.2 47.3 0 94 96 Finding 1 MASPRM-guided inference beats policy-only at matched compute. Across GSM8K and MATH, MASPRM-guided inference (SBS MASPRM and MCTS MASPRM) reliably outperforms matched policy-only baselines (Greedy, Maj@5, SBS policy likelihood, MCTS policy likelihood) at the same #AC; see the Observation in Sec. 5.2 and Tables 1 and 2. This supports our central claim that process-level signals are better aligned with multi-step, multiagent decision making than token likelihoods alone. Reporting MC separately clarifies overhead: improvements are achieved under matched agent-call budgets, with MASPRM evaluations acting as lightweight controllers (wall-clock comparison is in App. B.9) over the same action budget. Finding 2 Process guidance and outcome scoring are complementary. The comparison in Sec. 5.3  (Table 3)  shows that using the MASPRM during search (process-level guidance) outperforms ORM when compute is matched. Moreover, combining the two (MASPRM in search + ORM at the end) yields the strongest accuracies among the variants considered. Finding 3 Progress signals transfer: OOD gains at matched compute. Sec. 5.4 demonstrates that MASPRM trained on GSM8K transfers zero-shot to MATH and improves EM over no MASPRM baseline at identical compute  (Table 4)  . While an in-distribution MASPRM remains the upper bound, the OOD MASPRM narrows the gap, indicating that progress-sensitive signals learned from MAS-MCTS supervision capture reusable structure (e.g., step validity, simplification quality, verifier alignment) beyond single dataset."
        },
        {
            "title": "7 Conclusion & Future Work",
            "content": "We presented MASPRM, per-action, per-agent process reward model that scores intermediate interagent states and steers inference-time search, improving accuracy at matched compute while act-"
        },
        {
            "title": "References",
            "content": "Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024. Alphamath almost zero: process supervision without process. Advances in Neural Information Processing Systems, 37:2768927724. Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, and Chuang Gan. 2025. Scaling autonomous agents via automatic reward modeling and planning. arXiv preprint arXiv:2502.12130. Sanjiban Choudhury. 2025. Process reward models for llm agents: Practical framework and directions. arXiv preprint arXiv:2502.10325. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, and 1 others. 2025. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Levente Kocsis and Csaba Szepesvári. 2006. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer. Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. arXiv preprint arXiv:1706.03872. Junwei Liao, Muning Wen, Jun Wang, and Weinan Zhang. 2025. Marft: Multi-agent reinforcement finetuning. arXiv preprint arXiv:2504.16129. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Muhan Lin, Shuyang Shi, Yue Guo, Vaishnav Tadiparthi, Behdad Chalaki, Ehsan Moradi Pari, Simon Stepputtis, Woojun Kim, Joseph Campbell, and Katia Sycara. 2025. Speaking the language of teamwork: Llmguided credit assignment in multi-agent reinforcement learning. arXiv preprint arXiv:2502.03723. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, and 1 others. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592. Kartik Nagpal, Dayi Dong, Jean-Baptiste Bouvier, and Negar Mehr. 2025. Leveraging large language models for effective and explainable multi-agent credit assignment. arXiv preprint arXiv:2502.16863. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Jonathan Berant, and Aviral Kumar. Agarwal, 2024. Rewarding progress: Scaling automated proarXiv preprint cess verifiers for llm reasoning. arXiv:2410.08146. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Hanlin Wang, Chak Tou Leong, Jiashuo Wang, Jian Wang, and Wenjie Li. 2025. Spa-rl: Reinforcing llm agents via stepwise progress attribution. arXiv preprint arXiv:2505.20732. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, and 1 others. 2016. Googles neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Yilin Yang, Liang Huang, and Mingbo Ma. 2018. Breaking the beam search curse: study of (re-) scoring methods and stopping criteria for neural machine translation. arXiv preprint arXiv:1808.09582. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, and 1 others. 2025. Versaprm: Multi-domain process reward model via synthetic reasoning data. arXiv preprint arXiv:2502.06737. 9 Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. 2024. G-designer: Architecting multi-agent communication topoloarXiv preprint gies via graph neural networks. arXiv:2410.11782. Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan Vulic, Anna Korhonen, and Sercan Ö Arık. 2025. Multi-agent design: Optimizing agents with better prompts and topologies. arXiv preprint arXiv:2502.02533."
        },
        {
            "title": "A Illustrative Examples",
            "content": "A.3 MCTS rollout with MASPRM values A.1 Example MAS and schedule The four-agent example  (Fig. 3)  is illustrative only and does not exactly match the experimental MAS; see App. for the configuration used in the results. Dashed directed edges indicate admissible message routing; one agent acts per depth according to σ, producing outputs o1, . . . , o4. Fig. 5 shows value-guided search over the unrolled tree: internal annotations (e.g., 0.5, 0.25) depict example mean values backed up from MASPRM leaf evaluations; white circles denote agent outputs; green/red leaves indicate terminal reward +1/ 1 (correct/incorrect). Selection uses UCT (Eq. 2). Figure 3: Example MAS (four agents) and schedule. Dashed directed edges define admissible routing and the order of agent outputs o1-o4; one agent acts per depth according to σ. Figure 5: MCTS over the unrolled tree (example). Numbers on nodes are illustrative mean values ; green/red leaves denote {+1, 1}. A.2 Stylized transcript for GSM8K-style problem We visualize  (Fig. 4)  one rollout on the ticket question: \"A group buys 14 tickets; adult $5, child $3; total $56. How much more was spent on adult tickets than on child tickets?\" The roles are Reader & Fact Extractor (o1), Planner & Variable Setter (o2), Arithmetic Worker (o3), and Verifier & Finalizer (o4). Two trajectories are shown: the green path solves a+c = 14 and 5a+3c = 56 to get a=c=7 and returns 5a3c = 3521 = 14; the red path makes an arithmetic error and outputs 15. Figure 4: Stylized transcript (example). 4-agent pipeline produces outputs o1-o4. Green is correct (a=c=7 5a3c = 14); red is incorrect (3521 = 15)."
        },
        {
            "title": "B Additional Experimental Details",
            "content": "B.1 Datasets and splits We use GSM8K and MATH. For each, validation is random 5% split of the training set. We use the provided test sets and ensure no training leakage. GSM8K: 7,099 train, 374 validation, 1,319 test MATH: 9,975 train, 525 validation, 2000 test B.2 Answer extraction and normalization Final answers are extracted from the last agent via the regex Final Answer: (.*). Fallback: last numeric span in the final message. Preprocessing: lowercase; strip commas/spaces; remove units in {USD, dollars, $}. Numeric tolerance δ = 103. Non-parsable outputs or timeouts are scored incorrect. B.3 MAS graph, prompts, and schedule Roles and prompts are listed in App. C; schedule length = 4. Max decode tokens per agent Lmax = 1024. B.4 MASPRM training data generation We run MAS-MCTS with = 40 simulations, cUCT = 4.0, Cmax = 3 children per expansion on training problems. Targets are clamped to [1+ϵ, 1ϵ] with ϵ = 104 and used directly with Huber loss on the tanh-bounded outputs. Leaf values are set to the terminal reward (λ=1 in Sec. 3). B.5 MASPRM model and optimization Backbone: Qwen2.5-1.5B with tanh head. QLoRA 4-bit, rank 256, dropout 0.05; batch size 8 (effective with grad accumulation 16); LR 1105; epochs 5; optimizer ADAMW; weight decay 0.01. Evaluation metrics: Pearson, RMSE, MAE, R2, bias. B.6 Search configurations SBS: \"1/5\" means (B2, B1) = (5, 1); \"3/5\" means (5, 3). Temperature = 0.7, top-p = 0.95. Tiebreaking by higher MASPRM score (if applicable), else by higher final-answer policy likelihood. MCTS: Selection uses UCT (Eq. 2); expansion samples up to Cmax = 3 children by top-p sampling from the current agent; leaf evaluation uses Vϕ (λ = 0); rollout depth capped at = 4; on dead ends, back up the current MASPRM leaf value. No policy priors are added to UCT unless otherwise stated. B.7 ORM Υ Training hyperparameters and optimizer mirror the MASPRM training, using sequence classification head. B.8 OOD transfer protocol The GSM8K-trained MASPRM is frozen for MATH (no re-tuning); we keep the same normalization/clamping. B.9 Wall-clock Comparison - name: Planner system_prompt: \"You are the Planner. Propose concise plan to solve the task.\" max_new_tokens: 1024 - name: Solver system_prompt: \"You are the Solver. Carry out the plan and compute results.\" max_new_tokens: 1024 - name: Verifier system_prompt: \"You are the Verifier. Double-check the result and output exactly: Final Answer: <value>\" max_new_tokens: 1024 C.2 Communication edges and schedule Edges are directed and 1 denotes the root (question). This replicates the connections used in all experiments: Edges and Schedule # Edges (src, dst). Use -1 for ROOT ( . . . Question). edges: - [-1, 0] - [-1, 1] - [ 0, 1] - [ 1, 2] - [-1, 3] - [ 2, 3] # Question -> Reader # Question -> Planner # Reader # Planner -> Solver # Question -> Verifier # Solver -> Verifier -> Planner # Fixed schedule (one turn per depth): schedule_order: [Reader, Planner, Solver, . . . Verifier] C.3 Decoding and search settings (for reproducibility) Unless specified otherwise in the main text, decoding and search use: Each AC takes 18.94 on average, while each MC takes 0.16 on average for GSM8K dataset. Decoding Settings MAS Graph, Prompts, and"
        },
        {
            "title": "Configuration",
            "content": "C.1 Agent roles, system prompts, and decoding The multi-agent system (MAS) uses four specialized agents and fixed schedule of one turn per agent (depth D=4): Reader Planner Solver Verifier. - name: Reader system_prompt: \"You are the Reader. Extract key facts and restate the question.\" max_new_tokens: 1024 temperature: 0.7 top_p: 0.95 max_new_tokens_per_agent: 1024 Algorithms (Brief Overview) This appendix summarizes the procedures used for training-time data generation, inference time MCTS, and step-level beam search. The descriptions are intentionally brief and omit implementation details not essential for understanding the method. 12 Algorithm 1 MAS-MCTS Training Data Generation (general) Input: Instance (x, y); communication graph G; schedule σ; simulations 1: Initialize empty search tree with statistics (s, a) = 0, (s, a) = 0 2: for = 1 to do 3: root(x); path [ ] while is expanded and non-terminal do (cid:110) ˆQ(s, b) + cUCT arg maxb Append (s, a) to path; next(s, a) (cid:113) ln(1+(cid:80) (s,u)) (cid:111) 1+N (s,b) Selection end while if non-terminal then Expansion (no virtual visits in training) Sample candidate set from the current agents policy π(it)( s) For each C, create child = next(s, a) Evaluation with ground truth (terminal reward) end if Continue selection/expansion until reaching terminal leaf sT R(sT ) {1, +1} for each (u, b) in path do (corresponds to λ=1 during training) (u, b) (u, b) + 1; (u, b) (u, b) + v; ˆQ(u, b) (u,b) (u,b) Backpropagation end for 16: 17: end for 18: Process targets: For every visited edge (s, a) with child = next(s, a), set yproc(s) ˆQ(s, a) and add (s, yproc(s)) to Dproc 19: Fit the process head: Train Vϕ on Dproc with bounded regression loss Algorithm 2 Inference-Time MCTS with MASPRM (and optional ORM) Input: Instance x; graph G; schedule σ; simulations ; optional ORM Υ 1: Initialize empty search tree with statistics (s, a) = 0, (s, a) = 0 2: for = 1 to do 3: root(x); path [ ] while is expanded and non-terminal do (cid:110) ˆQ(s, b) + cUCT arg maxb Append (s, a) to path; next(s, a) (cid:113) ln(1+(cid:80) (s,u)) (cid:111) 1+N (s,b) Selection end while if non-terminal then Expansion with virtual visit initialization from Vϕ Sample candidate set from π(it)( s) for each do next(s, a) (s, a) 1; (s, a) Vϕ(s); ˆQ(s, a) Vϕ(s) end for Leaf evaluation: bootstrap by default; use ORM at terminal if provided end if if is terminal and ORM is used then Υ(cid:0)x, ˆy(s)(cid:1) else Vϕ(s) end if for each (u, b) in path do (u, b) (u, b) + 1; (u, b) (u, b) + v; (effective λ=1 at terminal leaves) (default bootstrap; λ=0) Backpropagation ˆQ(u, b) (u,b) (u,b) end for 22: 23: end for 24: Decode: From the root, greedily follow at each depth the child with maximal ˆQ 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 13 Algorithm 3 Step-Level Beam Search (SBS) with MASPRM or Policy Score (B1/B2) Input: Instance x; graph G; schedule σ; global beam B1; per-state samples B2; scorer where (s) {Vϕ(s), ψpol(a s)} is the schedule length (one agent per depth); it = σ(t) 1: Initialize beam {root(x)} 2: for = 1 to do Call [ ] 3: for each do 4: Sample B2 candidate actions {a(b)}B2 for = 1 to B2 do b=1 π(it)( s) next(s, a(b)); Append (s, score(s)) to Call score(s) (s) end for 5: 6: 7: 8: 9: 10: 11: end for top-B1 states in Call by score Early stop: If any is terminal, optionally stop and return the best terminal by score 12: 13: end for 14: Select output: If no terminal in the final beam, select the highest-scoring state and finalize with the Global pool across parents (e.g., = Vϕ) Verifier"
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC V6T1Z4, Canada",
        "Huawei Technologies Canada, Burnaby, BC V5C6S7, Canada"
    ]
}