{
    "paper_title": "LIVE: Long-horizon Interactive Video World Modeling",
    "authors": [
        "Junchao Huang",
        "Ziyang Ye",
        "Xinting Hu",
        "Tianyu He",
        "Guiyu Zhang",
        "Shaoshuai Shi",
        "Jiang Bian",
        "Li Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths."
        },
        {
            "title": "Start",
            "content": "LIVE: Long-horizon Interactive Video World Modeling Junchao Huang1,2,3 Tianyu He3, Guiyu Zhang1 1The Chinese University of Hong Kong, Shenzhen Shaoshuai Shi5 Ziyang Ye1 Xinting Hu4 Jiang Bian3 2Shenzhen Loop Area Institute Li Jiang1,2, 3Microsoft Research 6 2 0 2 3 ] . [ 1 7 4 7 3 0 . 2 0 6 2 : r 4The University of Hong Kong Project Page: https://junchao-cs.github.io/LIVE-demo/ 5Voyager Research, Didi Chuxing Figure 1. LIVE achieves bounded error accumulation for stable long-horizon video world modeling. Top: Qualitative comparison with baselines and FID curves showing LIVE maintains stable quality while other methods degrade as rollout length increases. Bottom: Applications in real-world (RealEstate10K) and gaming environments (Minecraft, UE Engine)."
        },
        {
            "title": "Abstract",
            "content": "Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with longhorizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, Long-horizon Interactive Video world modEl that enforces bounded erTeam lead. Corresponding authors. ror accumulation via novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs forward rollout from ground-truth frames and then applies reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths. 1 Figure 2. Comparison of autoregressive training paradigms. Teacher Forcing (TF) uses ground truth context during training, causing traininference mismatch. Diffusion Forcing (DF) injects noise but fails to model real rollout errors. Self-Forcing (SF) employs sequence-level distillation with unbounded error accumulation. Our LIVE performs forward rollout then reverse recovery with frame-level diffusion loss, bounding errors through the cycle-consistency objective. sue by injecting stochastic noise into the conditioning context during training, thereby exposing the model to imperfect inputs. While this provides some degree of robustness for short sequences, the approach remains ineffective for long-horizon generation, as there remains substantial distributional gap between noised ground-truth data and genuine model rollouts with accumulated errors. To further mitigate the train-inference gap, Self-Forcing (SF) [28] has proposed training on rollouts generated by the model itself and distilling knowledge from pre-trained teacher via holistic sequence-level distribution matching. While effective, this paradigm suffers from several limitations. First, the reliance on pre-trained, interaction-capable teacher models incurs substantial computational overhead, particularly in domain-specific settings. Second, knowledge distillation inherently constrains the student model by the teachers capacity and can induce mode-seeking behavior that degrades output diversity. Third, SF applies distribution matching at the sequence level without explicitly bounding error accumulation, limiting its ability to control long-horizon error propagation. As result, the model is only exposed to errors within fixed training rollout length, and inference beyond this horizon leads to unseen error patterns and potential catastrophic collapse. To address these limitations, we propose LIVE, Long-horizon Interactive Video world modEl that enforces bounded error accumulation via novel cycle-consistency objective, thereby eliminating the reliance on teacher-based distillation. Instead of matching full sequence distributions [28], LIVE performs forward rollout from groundtruth frames followed by reverse generation process to reconstruct the initial state, on which the diffusion loss is computed. This formulation explicitly enforces cycle consistency: training the model to map its own imperfect rollouts back to the ground-truth manifold. Crucially, unlike sequence-level objectives that permit unbounded drift, Figure 3. Rollout from GT produces semantically diverse content, making direct supervision infeasible. LIVE addresses this by requiring the model to generate back toward the original GT, enabling valid supervision through the cycle-consistency objective. 1. Introduction Video world models aim to learn action-conditioned future video predictions for interactive agents, based on past observations and control inputs such as camera poses and keyboard commands. Different from bidirectional video diffusion models that generate the entire video frames at once [15, 33, 37, 48], effective world models require finegrained interactivity and real-time inference. To achieve this goal, approaches such as Teacher Forcing (TF) [14, 26, 29] and Diffusion Forcing (DF) [7] have introduced causal attention mechanisms into video diffusion models, enabling autoregressive video generation with real-time interactivity. Despite its empirical success [8, 51], autoregressive video world modeling is fundamentally limited by the temporal accumulation of generation errors. This issue arises from exposure bias, where the model is trained on groundtruth frames but must condition on its own predictions at inference time, leading to compounding distributional shift over long horizons. DF [7, 39] attempts to mitigate this isthe proposed design maintains distributional alignment between generated rollouts and supervision targets. By training with fixed-length windows while explicitly modeling error accumulation, LIVE learns to operate within controlled error bound, enabling stable generalization to longhorizon generation at inference time. In addition, we present unified view that encompasses TF, DF, and the proposed LIVE. Under this unified view, TF and DF emerge as special cases of LIVE by adjusting the proportion of ground-truth conditioning. Motivated by this observation, we introduce progressive training curriculum that explicitly controls error tolerance by parameterizing the ratio of ground-truth frames to model-generated rollouts within each training window. This curriculum facilitates stable optimization while preserving high-quality generation through end-to-end diffusion training. In summary, our contributions are threefold: We propose LIVE, long-horizon interactive video world model that enforces bounded error accumulation via cycle-consistency objective, eliminating the need for teacher-based distillation. We present unified view of TF, DF, and LIVE, and derive progressive training curriculum that controls error tolerance by adjusting the ratio of ground-truth to rollout frames, enabling stable end-to-end diffusion training. We demonstrate state-of-the-art performance on longhorizon interactive video benchmarks, with robust generalization to sequences far beyond the training horizon. 2. Related Work Video Diffusion Models. Early video diffusion methods extended image diffusion into temporal domains using UNet-based architectures [24, 18, 25]. The introduction of Diffusion Transformers (DiT) [20, 35] enabled better modeling of global spatiotemporal dependencies, leading to large-scale models like Sora [33], Seaweed [38], HunyuanVideo [31], and Wan [42]. These bidirectional approaches [1, 5] employ full-sequence attention where all frames interact simultaneously, achieving impressive temporal consistency and motion quality. However, they are constrained to fixed-length generation and lack frame-level interaction control, while facing computational complexity that scales quadratically with sequence length, making them unsuitable for real-time interactive world modeling. Autoregressive Video Generation. Autoregressive methods synthesize videos sequentially by conditioning on preceding context [21, 24, 32, 40, 46]. Approaches include discrete token-based autoregression [17, 30, 44] and diffusionbased frameworks [8, 16]. This paradigm naturally supports interactive world modeling where environments are simulated step-by-step [6, 13, 23, 34, 41, 52]. Several works adopt causal attention with sliding windows for real-time generation [9, 11, 27], while facing error accumulation challenges during long-horizon inference. Mitigating Exposure Bias. Teacher Forcing [14, 29, 51] conditions on ground truth during training but causes exposure bias at inference when models encounter their own imperfect rollouts. Diffusion Forcing [7, 8, 16, 39, 49] injects noise into ground truth context during training to approximate rollout distributions, yet noised ground truth still fundamentally differs from actual rollouts. Self-Forcing [28] and its extensions [10, 47] align the models rollout distribution with that of pre-trained bidirectional teacher during training, which slows down error accumulation but still suffers from degradation beyond training rollout lengths. Moreover, the reliance on pre-trained teachers complicates extension to interactive video generation models. Concurrent work [36] constructs corrective trajectories from the models rollouts to teach it to recover from its mistakes. Another approach [19] simulates rollouts via resampling ground truth, yet this still differs from genuine rollouts. 3. Preliminaries 3.1. Interactive Video World Modeling We consider video world modeling as learning the conditional distribution p(x1:T c1:T ), where x1:T = (x1, . . . , xT ) denotes sequence of video frames and c1:T = (c1, . . . , cT ) represents conditioning information for each frame (e.g., camera poses, actions). Video diffusion models learn to denoise Gaussian noise through an iterative process, where forward diffusion process gradually adds noise to the data: q(xtixti1 ) = (xti; (cid:112)1 βixti1 , βiI), (1) and reverse denoising process learns to predict the noise: ϵθ(xk , t, ck) ϵ, (2) where [t1, . . . , tN ] denotes the diffusion timestep for frame and ϵ (0, I) is the Gaussian noise. In video world modeling, the model generates frames sequentially conditioned on previous frames: p(x1:T c1:T ) = (cid:89) k=1 p(xkx<k, ck), (3) where each frame xk is generated conditioned on the context window x<k = (x1, . . . , xk1) and corresponding conditions ck = (c1, . . . , ck). For interactive world models requiring real-time inference, we employ sliding window approach where only the most recent frames are used as context: p(xkxkK:k1, ckK:k). (4) 3 Figure 4. LIVE training pipeline. Forward rollout (Left, frozen): Given prompt frames xi, the model generates the remaining frames xj via causal attention. Cycle-consistency objective (Right, trainable): The rollout is reversed and used as context to recover the original prompt frames via frame-level diffusion loss, employing reverse attention (right mask, shown for = 2). 3.2. Training Paradigms for AR Generation 4. Method Existing autoregressive video diffusion models typically employ one of three training strategies (Figure 2 (a)-(c)): Teacher Forcing (TF). During training, the model predicts noise conditioned on ground truth frames within sliding window: LTF = x1:K , ϵ,ti (cid:34) (cid:88) k= (cid:13) (cid:13)ϵk ϵθ(xk ti , x<k, ti, ck)(cid:13) 2 (cid:13) (cid:35) . (5) = αtixk +σtiϵk is the noised frame at timestep where xk ti ti, with ti independently sampled for each frame from the noise schedule [t1, . . . , tN ], and denotes the context window length. This creates train-inference discrepancy: at inference, the model must condition on its own imperfect rollouts rather than ground truth. Diffusion Forcing (DF). To bridge this gap, DF injects noise into the conditioning context during training: LDF = x1:K , ϵ,ti (cid:34) (cid:88) k=1 (cid:13) (cid:13)ϵk ϵθ(xk ti , ˆx<k, ti, ck)(cid:13) 2 (cid:13) (cid:35) , (6) where ˆxj = αtixj + σtiϵj represents noisy context frame with independently sampled timestep ti, and ϵj (0, I). However, the distribution of noised ground truth differs from genuine model rollouts with accumulated errors. Self-Forcing (SF). SF addresses this through knowledge distillation, where student model learns from its own rollouts under the supervision of teacher: LSF = DKL (cid:0)pteacher(x1:T )pstudent(x1:T )(cid:1) . (7) However, sequence-level distribution matching fails to constrain error accumulation within bounded ranges, leading to quality degradation that prevents generalization beyond training rollout lengths. We introduce LIVE, framework that enforces bounded error accumulation via cycle-consistency constraint. Specifically, LIVE performs forward rollout from ground-truth (GT) frames followed by reverse generation process to reconstruct the initial state, on which the diffusion loss is computed. This formulation explicitly enforces cycle consistency by training the model to map its own imperfect rollouts back to the GT manifold. 4.1. Bounded Error Accumulation an autoregressive diffusion model Consider video p(x1:T c1:T ) = that generates frames sequentially: (cid:81)T k=1 pθ(xkx<k, ck), where x<k = (x1, . . . , xk1) denotes the context frames and c1:T represents conditioning information (e.g., camera poses, actions). Problem Setup. During autoregressive generation with rollouts, we observe general tendency toward quality degradation in expectation: E[D(xk, xk)] E[D(xk+1, xk+1)], [1, 1], (8) where D(xk, xk) measures perceptual quality (e.g., FVD, FID). While this error accumulation pattern is empirically well-established, directly supervising rollouts to reduce D(xk, xk) faces fundamental obstacle: rollouts naturally produce semantically diverse content that diverges from GT trajectories (Figure 3). Since xk and xk represent different but equally valid future states, computing diffusion loss between them is infeasible. This limitation hinders extending SF to efficient parallel diffusion supervision and increases its dependence on pretrained teacher models. Cycle-consistency Objective. To address the above challenges, LIVE introduces cycle-consistency objective that enables valid frame-level supervision without requiring distributional alignment between rollouts and GT. The key insight is: instead of supervising rollouts xp+1:T (where de4 Algorithm 1 LIVE Training Pipeline Require: Window length , minimum pmin 1: for each epoch do 2: 3: Pre-training: Post-training: decrease gradually for each batch (x1:T , c1:T ) do xp+1:T pθ(xp+1:T x1:p, c1:T ) {Eq. 9} xp+1:T,rev (xT , . . . , xp+1) {Eq. 10} crev (cT , . . . , c1) {Eq. 10} Inject noise: xk,ϵ αt xk + σtηk {Eq. 11} Compute LLIVE {Eq. 13} θ θ αθLLIVE 4: 5: 6: 7: 8: 9: 10: 11: 12: end for end for notes the number of prompt frames used to initiate the rollout) directly against GT, we require them to be recoverable - the model must be able to reverse-generate the original GT prompt frames from the rollouts by reversing camera poses/actions. This creates valid training signal while accommodating distributional diversity. For video sequence x1:T : Step 1 (Forward Rollout): Given training window of frames with known camera/action conditions, we use the first frames as prompt frames and generate the remaining frames with gradients disabled. Unlike inference which requires frame-by-frame interaction, during training we can efficiently generate all frames simultaneously (initialized from pure noise) since we have access to all future camera/action conditions. This uses the same causal attention mask as inference (Figure 4), ensuring traininginference consistency while dramatically improving efficiency: (9) xp+1:T pθ(xp+1:T x1:p, c1:T ). Step 2 (Reverse Generation): Reverse the rollout temporally and reverse camera/action conditions, then attempt to recover the original prompt frames. Since forward rollouts satisfy D(xk, xk) D(xk+1, xk+1) (quality degrades monotonically), after reversal the context quality improves monotonically. Without intervention, the model could recover x1 by attending primarily to the highest-quality context frame x2,rev, trivially satisfying recoverability without constraining forward errors. To prevent this shortcut, we first reverse the rollout: xp+1:T,rev (xT , . . . , xp+1), crev (cT , . . . , c1), (10) then inject random noise per frame. For each [p+1, ], sample U([t1, . . . , tN ]) and ηk (0, I), then: xk,ϵ αt xk + σtηk, and finally recover the original prompts: ˆx1:p pθ(x1:p xp+1:T, rev, ϵ, crev). (11) (12) Step 3 (Frame-Level Supervision): To enable efficient parallel training like TF/DF, we extend the p-frame supervision to the full window length by repeating the GT frames and applying different noise timesteps to each position. This allows computing noise prediction loss on all frames in parallel: LLIVE = x1:T pdata tU ([t1,...,tN ]) ϵkN (0,I) (cid:34)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) k=1 (cid:13) (cid:13)ϵk ϵk θ (cid:13) 2 (cid:13) (cid:35) , (13) where each ϵk θ is predicted as: θ = ϵθ(xgt(k) ϵk , x<k,rev,ϵ, t, ck), (14) with independently sampled for each frame from the noise schedule, xgt(k) denoting the GT frame (repeated from the prompts), and x<k,rev,ϵ representing the reversed rollout context with injected noise. Specifically, x<k,rev,ϵ consists of the subset of frames from xp+1:T,rev,ϵ in Eq. 12 preceding position k. Implicit Error Bounding. The cycle-consistency objective creates an implicit incentive to bound forward distortion through the recovery objective. Define Dctx = D(xk, xk) as the distortion between rollout and GT at frame k, and Drec = 1 k=1 D(xk, ˆxk) as the average recovery distorp tion over the prompt frames. The training objective minimizes Drec, which encourages: (1) Maintaining forward distortion D(xk, xk) within bounded range to enable recovery from rollout context; (2) Optimizing ϵθ to recover GT frames from imperfect rollout context, directly reducing Drec via gradient descent. (cid:80)p Consequently, gradient optimization learns to maintain D(xk, xk) within bounded range, preventing the monotonic degradation that plagues AR diffusion inference. 4.2. Progressive Training Curriculum Unified Training Objective. LIVE unifies existing training paradigms by controlling the GT ratio [1, ], encompassing: (1) Teacher Forcing (p = , perfect GT context x<k); (2) Diffusion Forcing (p = , noisy GT context ˆx<k = x<k+ϵ); (3) LIVE (p < , imperfect rollout context x<k with accumulated errors). By controlling p, our framework supports both pre-training and post-training: during pre-training, the model uses = since it has not yet learned to generate rollouts; during post-training, as shown in Figure 6, we progressively decrease to adapt the model to increasing error levels. Error Tolerance. When is large, the context consists primarily of GT frames with small distortions, making recovery relatively easy; as decreases, more rollout frames enter the context, accumulating larger errors and making recovery increasingly difficult. Through gradually exposing the Figure 5. Post-training performance from converged DF checkpoint. Continued DF training stagnates with oscillating metrics, while LIVE achieves substantial improvements that amplify at longer horizons. LIVE converges to comparable FID across 128-frame and 200frame generation, demonstrating uniform quality regardless of rollout length. domly select one video per scene (12 videos total) as the test set. (3) Minecraft: We train on the WorldMem [45] dataset and collect 300 video-action pairs from MineDojo [12] for evaluation, testing long-horizon generation in interactive environments. We compare LIVE against multiple baselines including CameraCtrl [22], DFoT [39], GF (Geometry Forcing) [43], and NFD-TF/DF (Teacher Forcing/Diffusion Forcing) [9], assessing generation quality using PSNR, SSIM, LPIPS, and FID metrics. We focus our comparison on methods without interactive teacher model distillation. Training large-scale bidirectional teacher models [28] remains important future work beyond our current computational budget. 5.1. Main Results Error Accumulation Analysis. Figure 1 demonstrates LIVEs core advantage. Training models on RealEstate10K with TF (Teacher Forcing), DF (Diffusion Forcing), DFoT [39], GF (Geometry Forcing) [43], and LIVE (TF, DF, and LIVE use the same model architecture [9]), we evaluate FID at 32, 64, 128, and 200 frames. LIVE maintains stable FID around 10 across all lengths, while all baselines degrade dramatically beyond 64 frames, validating that our cycle-consistency objective successfully bounds error accumulation. Figure 5 shows post-training from converged DF checkpoint. Continued DF training stagnates with oscillating metrics, while LIVE achieves substantial gains that amplify at longer sequences. Critically, LIVE converges to comparable FID for both 128-frame and 200-frame generation, maintaining uniform quality across rollout horizons. Quantitative Results. Tables 1 and 2 show that LIVE achieves substantial improvements over all baselines across three benchmarks, with particularly large gains at longer rollout lengths. Our method shares identical architecture and inference procedures with NFD [9], isolating training strategy as the sole differentiator. While DF improves upon TF by injecting noise during training, it remains insufficient for long-horizon generation since noised ground truth fails Figure 6. Progressive training curriculum by increasing rollout ratio. From left to right, as decreases, more generated frames enter the context, increasing the models error tolerance while maintaining recoverability through the cycle-consistency objective. model to harder recovery tasks, this strengthens its ability to recover from imperfect contexts (Error Tolerance). This enhanced capability, in turn, produces better rollouts with reduced errors, enabling robust long-horizon generation. 5. Experiments Implementation Details. All experiments are conducted on cluster of 32 NVIDIA H100 GPUs with batch size of 64. Our model architecture follows the NFD [9] 774M configuration. For RealEstate10K, we train from scratch following DFoT [39] settings at 256 256 resolution with frame skip of 2. For UE Engine Videos datasets [50], we initialize from RealEstate10K pre-trained weights and apply the same frame skip of 2. Our model use fixed context window of 32 frames during both training and evaluation. Additional details are provided in Appendix 7.1. Datasets and Baselines. We evaluate on three diverse benchmarks: (1) RealEstate10K: large-scale dataset of real estate videos featuring diverse camera motions. We report results on the complete test set. (2) UE Engine Videos: Following Context-as-Memory [50], we use their dataset containing 100 videos of 7,601 frames across 12 scenes with camera pose annotations, collected from realistic game engine environments (UE engine). We ran6 Table 1. RealEstate10K full test set results across different rollout lengths. LIVE achieves state-of-the-art performance, with particularly large gains at longer sequences demonstrating superior long-horizon generation capability. Method Realtime CameraCtrl [22] DFoT [39] GF [43] NFD-TF [9] NFD-DF [9] LIVE 064 frames 0128 frames 0200 frames 256 frames PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM 14.09 15.65 16. 16.87 16.59 0.3829 0.3053 0.2450 0.2571 0.2558 0.4366 0.4989 0.5567 0.5503 0.5723 11.69 12.55 12. 13.59 13.82 0.5224 0.4601 0.4190 0.4302 0.3922 0.3651 0.3936 0.4534 0.4448 0.5015 10.25 10.86 10. 11.63 12.21 0.6115 0.5613 0.5400 0.5526 0.4956 0.3181 0.3287 0.3969 0.3724 0.4598 9.48 10.02 9. 10.58 11.51 0.6585 0.6128 0.5936 0.6222 0.5506 0.2886 0.2921 0.3796 0.3281 0.4397 18. 0.2215 0.5810 15.91 0.3298 0.5096 14. 0.4163 0.4630 13.89 0.4682 0.4400 Table 2. Results on interactive game environments. LIVE achieves consistent improvements over baselines on both realistic game engine videos (UE Engine) and interactive gameplay (Minecraft), demonstrating strong performance for interactive world modeling. 064 frames 0128 frames 0256 frames 400 frames Method PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM UE Engine (Realistic Game Engine) NFD-TF [9] NFD-DF [9] 17.16 17.15 0.3387 0.3357 0.4953 0.5062 14.95 14. 0.4597 0.4586 0.4245 0.4441 12.97 12.27 0.5702 0.5799 0.3625 0.3956 11.80 11. 0.6318 0.6456 0.3286 0.3760 LIVE 17.83 0.3145 0. 15.85 0.4210 0.4600 14.04 0.5214 0. 12.96 0.5794 0.3834 032 frames 064 frames 0128 frames 0200 frames Method PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM Minecraft (Interactive Gameplay) NFD-TF [9] NFD-DF [9] 16.09 17. 0.3474 0.2888 0.6224 0.6401 14.62 15.54 0.4067 0.3586 0.5930 0.6036 13.06 13. 0.4781 0.4469 0.5560 0.5594 12.10 12.34 0.5255 0.5091 0.5311 0.5332 LIVE 17.87 0.2698 0.6558 16.31 0.3271 0. 14.90 0.3877 0.6037 14.02 0.4299 0. Table 3. Ablation studies on RealEstate10K test set evaluating the impact of key components in LIVE. 064 frames 0200 frames Variant PSNR LPIPS SSIM PSNR LPIPS SSIM Effect of Cycle-consistency Objective 13.99 w/o Cycle 0.4041 0.4597 11.18 0.6024 0.3564 Effect of Context Noise Strategy 0.2310 0. No Noise Fixed Noise 17.76 17.48 0.5752 0.5551 13.83 14.09 0.4573 0.4444 0.4487 0. Effect of Progressive Training Curriculum Fixed = 1 0.5265 0.2747 16.78 13.58 0.4800 0. LIVE 18.11 0.2215 0.5810 14.57 0. 0.4630 Figure 7. Qualitative comparison on UE Engine dataset. We compare models with identical architecture trained using Teacher Forcing (TF), Diffusion Forcing (DF), and LIVE. to match the distribution of genuine rollouts with accumulated errors. LIVE addresses this limitation by training directly on imperfect rollouts with the cycle-consistency objective, achieving bounded error accumulation through endto-end diffusion optimization. Qualitative Results. Figures 7 and 8 present qualitative comparisons on UE Engine and RealEstate10K datasets. On UE Engine, we compare models with identical architecture trained using TF, DF, and LIVE, demonstrating LIVEs superior generation quality. On RealEstate10K, our method maintains consistent visual quality over extended rollouts across both indoor and outdoor scenes, while competing methods exhibit noticeable degradation. Full videos and additional examples are provided in Appendix 7.2. 7 Figure 8. Qualitative comparison on RealEstate10K dataset. We showcase indoor and outdoor scenes comparing various methods. LIVE demonstrates stable visual quality during rollouts. Full videos and additional examples are provided in Appendix 7.2. 5.2. Ablation Studies We conduct comprehensive ablation studies on the RealEstate10K test set to validate the key design choices in LIVE. Results are summarized in Table 3. Effect of Cycle-consistency Objective. Removing the reverse generation step leads to substantial performance degradation. This validates our core hypothesis illustrated in Figure 3: direct supervision on forward rollouts is infeasible due to semantic divergence between rollouts and ground truth. The cycle-consistency objective addresses this by requiring the model to generate back toward the original GT, creating valid training signal that accommodates distributional diversity while constraining error accumulation within recoverable limits. Effect of Context Noise Strategy. We compare three noise injection strategies for the rollout context: (1) no noise, (2) fixed-scale noise, and (3) random timestep sampling (LIVE). Without noise, the model shows acceptable shorthorizon performance but degrades at longer sequences. Fixed-scale noise provides marginal improvement, while our random timestep sampling achieves the best results. This validates the analysis in Sec 4.1 Step 2: after reversing, context quality improves monotonically, allowing trivial recovery by attending to higher-quality neighboring frame. Random per-frame noise breaks this pattern, forcing model to learn robust recovery from diverse error distributions. Effect of Progressive Training Curriculum. Directly setting = 1 throughout post-training underperforms our progressive curriculum that gradually decreases from to pmin. Abruptly exposing the model to the maximum rollout length creates an overly difficult task before sufficient error tolerance develops. Progressive rollout extension allows gradual capability building, starting from easy recovery with mostly GT context, then progressively increasing the rollout proportion to expose harder error patterns. This enhanced recovery capability produces better rollouts, ultimately enabling the models error tolerance to converge smoothly toward its recovery capacity. 6. Conclusion In this work, we introduce LIVE, long-horizon interactive video world model that addresses the fundamental challenge of error accumulation in autoregressive generation. By enforcing cycle-consistency objective through diffusion loss, LIVE explicitly bounds long-horizon error propagation without relying on teacher-based distillation. We further present unified perspective that connects TF, DF, and LIVE, and derived progressive training curriculum that stabilizes optimization while preserving generation quality. Extensive experiments demonstrate that LIVE achieves strong performance and robust generalization on long-horizon interactive video world modeling benchmarks, significantly extending the effective rollout horizon beyond the training window. In future work, we will further scale up LIVE on large-scale and diverse datasets."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. 3 [2] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, 2024. 3 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023. 3 [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 2024. 3 [6] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. 3 [7] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 2024. 2, [8] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Skyreels-v2: Zheng Chen, Chengcheng Ma, et al. arXiv preprint Infinite-length film generative model. arXiv:2504.13074, 2025. 2, 3 [9] Xinle Cheng, Tianyu He, Jiayi Xu, Junliang Guo, Di He, and Jiang Bian. Playing with transformer at 30+ fps via nextframe diffusion. arXiv preprint arXiv:2506.01380, 2025. 3, 6, 7, 11 [10] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Selfforcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 3 [11] Decart, Julian Quevedo, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer. 2024. 3 [12] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 6, 11 [13] Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568, 2024. [14] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, and Long Chen. Ca2-vdm: Efficient autoregressive video diffusion model with causal generation and cache sharing. arXiv preprint arXiv:2411.16375, 2024. 2, 3 [15] Google. Veo 3. https : / / deepmind . google / models/veo/, 2025. 2 [16] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Longcontext autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. 3 [17] Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, and Jiang Bian. Mineworld: real-time and open-source interactive world model on minecraft. arXiv preprint arXiv:2504.08388, 2025. [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [19] Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei, Zhenheng Yang, Weilin Huang, and Dahua Lin. Endto-end training for autoregressive video diffusion via selfresampling, 2025. 3 [20] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision. Springer, 2024. 3 [21] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. Advances in neural information processing systems, 2022. 3 [22] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 6, 7 [23] Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, et al. Matrix-game 2.0: An open-source realtime and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025. [24] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from In Proceedings of the Computer Vision and Pattern text. Recognition Conference, 2025. 3 9 [25] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3 [26] Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, and Maosong Sun. Acdit: Interpolating autoregressive conditional modeling and diffusion transformer. arXiv preprint arXiv:2412.07720, 2024. 2 [27] Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, and Li Jiang. Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft, 2025. 3 [28] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Self forcing: Bridging the trainand Eli Shechtman. test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2, 3, [29] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 2, 3 [30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 3 [31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [32] Zongyi Li, Shujie Hu, Shujie Liu, Long Zhou, Jeongsoo Choi, Lingwei Meng, Xun Guo, Jinyu Li, Hefei Ling, and Furu Wei. Arlon: Boosting diffusion transformers with arXiv autoregressive models for long video generation. preprint arXiv:2410.20502, 2024. 3 [33] OpenAI. Sora, 2024. 2, 3 [34] Parker-Holder, Ball, Bruce, Dasagi, Holsheimer, Kaplanis, Moufarek, Scully, Shar, Shi, et al. Genie 2: large-scale foundation world model. URL: https://deepmind. google/discover/blog/genie2-a-large-scale-foundation-world-model, 2024. 3 [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 2023. [36] Ryan Po, Eric Ryan Chan, Changan Chen, and Gordon Wetzstein. Bagger: Backwards aggregation for mitigating drift in autoregressive video diffusion models, 2025. 3 [37] Polyak, Zohar, Brown, Tjandra, Sinha, Lee, Vyas, Shi, CY Ma, CY Chuang, et al. Movie gen: cast of media foundation models. 2024a. arXiv preprint arXiv:2410.13720, 2024. 2 [38] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. 3 [39] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. 2, 3, 6, 7 [40] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 3 [41] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. 3 [42] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3 [43] Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, and Jiang Bian. Geometry forcing: Marrying video diffusion and 3d representation for consistent world modeling. arXiv preprint arXiv:2507.07982, 2025. 6, 7 [44] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. Advances in Neural Information Processing Systems, 2024. [45] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Longterm consistent world simulation with memory, 2025. 6, 11 [46] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2025. 3 [47] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 3 [48] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [49] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, 2025. 3 [50] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141, 2025. 6, 11 [51] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025. 2, 3 [52] Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Fei Kang, Biao Jiang, Zedong Gao, Eric Li, Yang Liu, et al. Matrix-game: Interactive world foundation model. arXiv preprint arXiv:2506.18701, 2025. 10 7. Appendix 7.1. Implementation Training Details 7.1.1. RealEstate10K Our model has 774M parameters, sharing identical architecture (DiTs) and inference procedures with NFD [9]. Specifically, we use 18-step ODE sampling during inference for all methods. The model operates at 256 256 resolution with frame skip of 2 during both training and inference, employing the same VAE as NFD for 16 spatial downsampling to the latent space. All experiments are conducted on cluster of 32 NVIDIA H100 GPUs with batch size of 64. We use the Adam optimizer with learning rate of 4 105. Both NFD-TF and NFD-DF are trained from scratch for over 200k iterations until convergence. Our method (LIVE) is initialized from the converged NFD-DF checkpoint (200k steps) and trained for an additional 20k iterations until convergence. We use fixed context window of 32 frames during both training and evaluation. The training set follows DFoT, containing approximately 50-60k videos. All metrics are reported on the complete RealEstate10K test set with over 7k videos. 7.1.2. UE Engine Videos For UE Engine Videos dataset [50], we use the same model configuration as RealEstate10K (774M parameters). We initialize from RealEstate10K (256256) pre-trained weights and fine-tune at 352640 resolution with frame skip of 2. The dataset contains 100 videos totaling 7,601 frames across 12 scenes with camera pose annotations. We randomly select 12 videos (one per scene) for testing and use the remaining 88 videos for training. For evaluation, we uniformly sample 50 starting frames from each test video, resulting in 600 test sequences in total. All experiments are conducted on 32 NVIDIA H100 GPUs with batch size of 64. We use the Adam optimizer with learning rate of 4105. NFD-TF and NFD-DF are initialized from their respective RealEstate10K checkpoints (TF and DF) and fine-tuned for 10k iterations. Our method (LIVE) is initialized from the RealEstate10K DF checkpoint, first fine-tuned with DF for 10k iterations, then further trained with LIVE for 6.5k iterations. We use the same VAE as RealEstate10K for 16 spatial downsampling to the latent space. 7.1.3. Minecraft For Minecraft, we use the same model configuration as RealEstate10K (774M parameters) and operate at 224384 resolution with frame skip of 1. The WorldMem [45] training dataset contains approximately 10k interactive gameplay videos of 1500 frames each, collected through MineDojo [12], where each frame is accompanied by 25-dimensional action vector. Since WorldMem does not provide an official test set, we collect 300 action trajectories from MineDojo for evaluation, with each trajectory representing randomly generated gameplay data. All experiments are conducted on 32 NVIDIA H100 GPUs with batch size of 64. We use the Adam optimizer with learning rate of 4105. NFD-DF is initialized from the original NFD checkpoint (200k steps) and fine-tuned on WorldMem for 30k iterations. NFD-TF is trained from scratch on WorldMem for 100k iterations. Our method (LIVE) is initialized from the converged NFD-DF checkpoint (after 30k iterations on WorldMem) and further trained with LIVE for 3k iterations. We use the same VAE as NFD with the decoder fine-tuned on Minecraft scenarios for 16 spatial downsampling to the latent space. 7.2. Additional Qualitative Results We provide additional qualitative examples across different datasets. Through these examples, we observe that different models exhibit distinct failure patterns during long rollouts. For instance, TF models tend to develop color distortion and semantic inconsistency, while DF models show exposure problems with overexposed or underexposed regions. Our method addresses these issues by training the model to recover from its own generated errors, thereby achieving stable generation quality even over extended sequences. The reversibility constraint ensures that the model learns to maintain quality within bounded range throughout the generation process. 11 12 14 15 16"
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Shenzhen Loop Area Institute",
        "The Chinese University of Hong Kong, Shenzhen",
        "The University of Hong Kong",
        "Voyager Research, Didi Chuxing"
    ]
}