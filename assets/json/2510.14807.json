{
    "paper_title": "SimKO: Simple Pass@K Policy Optimization",
    "authors": [
        "Ruotian Peng",
        "Yi Ren",
        "Zhouliang Yu",
        "Weiyang Liu",
        "Yandong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 7 0 8 4 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Technical Report",
            "content": "SIMKO: SIMPLE PASS@K POLICY OPTIMIZATION Ruotian Peng1,* Yi Ren2,* Zhouliang Yu3 Weiyang Liu3 Yandong Wen1, 1Westlake University 2University of British Columbia 3CUHK"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verifiedcorrect responses, it boosts the probabilities of the top-K candidates. For verifiedincorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating overconcentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for wide range of K, providing simple way to improve RLVRs exploration. Figure 1: SimKO improves pass@K performance on math tasks (AIME24/25, AMC, MATH500, Minerva, Olympiadbench) and logic tasks (Synlogic, BBH) compared to GRPO, as shown in the plots (left and middle). The figure on the right shows the k-th highest candidate probabilities averaged over the dataset. The SimKOtrained model exhibits less concentrated probability distribution compared to GRPO."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning with verifiable rewards (RLVR) offers simple recipe for improving LLM reasoning: the model generates responses, and updates itself by increasing the probability of the correct ones while decreasing that of the incorrect ones (Shao et al., 2024; Schulman et al., 2017; Hu, 2025). This coupled process induces systematic bias, whereby the model progressively collapses to narrow set of safe responses, implicitly prioritizing exploitation over exploration (Liang et al., 2025; He et al., 2025a). Such effect is evident in improved pass@1, which measures the expected quality of single reasoning path, but degraded pass@K, which measures the coverage of multiple reasoning paths (Yue et al., 2025; Wu et al., 2025). It has been observed that the reduced exploration ability limits the models reasoning potential and deteriorates its ability to generalize to novel or more challenging scenarios (Chen et al., 2025a; Song et al., 2025). *Equal contributions Corresponding author spherelab.ai/simko"
        },
        {
            "title": "Technical Report",
            "content": "Several approaches have been proposed to mitigate this exploration deficit. Data-centric methods focus on data augmentation, exposing the model to broader reasoning environments to promote response diversity. Techniques include using off-policy data (Dong et al., 2025; Li et al., 2025), generating more responses for challenging samples (Yang et al., 2025), and creating new task variations from the model itself (Liang et al., 2025). As complementary line of work, reward-centric methods derive group-wise rewards to evaluate the collective quality of multiple responses. These approaches provide an unbiased estimate of pass@K (Walder & Karkhanis, 2025; Chen et al., 2025b), which the model can then directly optimize. Despite their promise, these methods operate only at the input or output end, leaving the internal learning dynamics underexplored. More recently, entropy-based methods (Cui et al., 2025; Cheng et al., 2025) use output entropy as proxy for exploration control, preventing the model from collapsing. While these methods yield valuable insights, entropy remains coarse and incomplete measure that cannot capture fine-grained exploration behavior. In this work, we take an alternative perspective grounded in mechanism of next-token prediction. At each decoding step, an LLM outputs probability distribution over its entire vocabulary, offering direct and fine-grained view of its exploration behavior. How this probability mass is distributed across vocabulary candidates determines whether the model explores multiple reasoning paths or collapses into single deterministic trajectory. Unfortunately, capturing the full distribution is computationally prohibitive, as modern vocabularies often exceed 100K candidates. This practical constraint likely explains why prior work favored scalar measures like entropy over the full distribution. We seek practical solution to this constraint by revisiting the token-level probability distribution. Our empirical evidence shows that these distributions are highly skewed, as illustrated in Figure 1, with only few candidates carrying non-negligible probability mass. This finding justifies tractable approximation: by focusing on the top-K candidates, we can effectively characterize exploration behavior. Building on this view, we further analyze the training dynamics of RLVR algorithms and observe distinct pattern: probability mass gradually concentrates on the top-1 candidate, while other candidates are suppressed. This over-concentration pushes the model towards deterministic behavior and, more importantly, directly explains the degradation of pass@K performance. Motivated by this observation, we propose Simple Pass@K Optimization (SimKO), method designed to explicitly mitigate distribution collapse. The core idea is to redistribute and balance gradient updates across the top-K candidates rather than allowing the top-1 to dominate. For verifiedcorrect responses, SimKO shares positive gradients between the generated token and other highprobability candidates, reducing over-concentration on single choice. For incorrect responses, SimKO applies stronger penalties to the top-1 candidate, encouraging probability mass to flow into other alternative candidates. This asymmetric regularization proves especially effective when applied to semantic forking tokens in the reasoning path, where token-level entropy is high. We evaluate SimKO on several mathematical and logical reasoning benchmarks across multiple LLM backbones. SimKO consistently improves over vanilla GRPO and surpasses strong baselines across wide range of values (up to 256), as shown in Figure 1. Ablation studies further confirm the mechanism behind its gains, offering new evidence for the role of probability concentration in the exploration-exploitation trade-off. In summary, our work makes three key contributions: We introduce new perspective on understanding RLVR dynamics. Specifically, we adopt top-K posterior probabilities as tractable approximation of the full token-level distribution, providing direct insight into why existing RLVR methods often improve pass@1 at the cost of exploration. We propose simple, effective yet principled RLVR method, termed SimKO. SimKO mitigates probability collapse through asymmetric gradient redistribution. By redistributing probability mass from overconfident top-1 tokens to other highly confident candidates, SimKO can explicitly promote exploration, thereby improving the pass@K performance. SimKO consistenly improves the pass@K performance by considerable margin. In our experiments, we demonstrate SimKOs effectiveness on multiple challenging math and logic benchmarks, showing consistent improvements in pass@K without sacrificing pass@1."
        },
        {
            "title": "2 BACKGROUND AND PRELIMINARIES",
            "content": "Group relative policy optimization (GRPO), representative and widely used policy-based RLVR method (Shao et al., 2024), is variant of PPO (Schulman et al., 2017) tailored for LLM post-"
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: (a) The exploration behavior visualized according to the token-level posterior probability. (b) Comparison of two exploration strategy. (c) An example of two distributions with identical entropy but distinct probability distribution. training. Given question x, the model generates different responses {yi}G parameters according to the following objective function: i=1 and updates its JGRPO(θ; γi,l) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) l=1 [min (γi,lAi,l, clip(γi,l, 1 ϵ, 1 + ϵ) Ai,l) βDKL(πθπref)] where γi,l = πθ(yi,l si,l)/πref(yi,l si,l) is the likelihood ratio between the current policy πθ and the reference policy πref at the l-th token of response yi, with si,l = (x, yi,<l) denoting the decoding state. The term Ai,l represents the advantage of the l-th token in the i-th response, computed by normalizing rewards within the roll-out. GRPO can be analyzed in gradient space under the learning dynamics framework of Ren & Sutherland (2025). The dominant contribution to parameter updates comes from θAi,lγi,l. Here, the advantage Ai,l can be viewed as sequence-level adaptive learning rate that scales gradients from different responses yi. Ignoring the contributions of the KL regularization and clipping terms (both primarily introduced for stability), the main optimization direction of most GRPO-based methods is governed by: θAi,lγi,l = Ai,l πθ(yi,lsi,l) πref(yi,lsi,l) θ log πθ(yi,lsi,l) = Ai,l sg(γi,l) θ log πθ(yi,lsi,l), (1) where sg() is the stopping-gradient operator."
        },
        {
            "title": "3 OVER-CONCENTRATED DISTRIBUTION DEGRADES PASS@K SCORE",
            "content": "To understand the mechanism driving current RLVR methods to favor exploitation over exploration, we begin by examining the connection between the models token-level probability distribution and its pass@K performance. We consider the illustrative example in Figure 2-(a), where the model generates the l-th token given the context si,l = (x, yi,<l). Suppose two valid reasoning paths begin with y(1) i,l denotes the rank-k candidate under πθ( si,l). model with strong exploration ability will distribute probability more evenly between πθ(y(1) si,l), i,l as illustrated in the upper panel of Figure 2-(b). si,l) and πθ(y(2) i,l i,l , where y(k) i,l and y(2) To track the token probability distribution among top-K candidates, we propose to compute Λ(k) :="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) l=1 log πθ(y(k) i,l si,l), {1, . . . , K}. (2) This metric represents the average log-probability of the rank-k candidate during generation. Despite the large vocabulary size, Figure 1 shows the probability mass is highly concentrated in the top few candidates. Empirically, we find that top-3 candidates (i.e. k=3) is sufficient and efficient for approximating the distribution. In addition to the probability of the rank-k candidate, we track the evolution of the average logprobability of the token sequence that is actually sampled, which is denoted as Λ.1 Since updates 1Simply replace the y(k) i,<l term in Equation 2 by the sampled candidate."
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: (a)-(c) Training dynamics of average log probability Λ and top-K probabilities Λ(k) derived by GRPO, NSR, and PSR. (d) The corresponding pass@1 and pass@K results of the RLVL-trained models. Following the setups of Zhu et al. (2025), we train Llama3.2-3B-Instruct on mixture of GSM8K and MATH (Level 1) and train Qwen2.5-Math-7B on the MATH dataset. Complete training dynamics are shown in Figure 9. occur directly on these sampled tokens, examining which Λ(k) is closer to Λ allows us to infer which ranked candidate primarily dominates the models generation process. To better disentangle the roles of positive and negative gradients, central to our algorithm design, we also conduct two ablations where the model is updated using only one type of response. We refer to these as Positive Sample Reinforce (PSR) and Negative Sample Reinforce (NSR) (Zhu et al., 2025). We present the learning dynamics in Figure 3. The results in the first column reveal clear trend under GRPO training: the probability of the rank-1 candidate, Λ(1), approaches Λ and saturates near 1, while the probabilities of alternatives (Λ(2), Λ(3)) collapse to negligible values (108 to 1010 in Qwen2.5-Math-7B). This demonstrates that GRPO aggressively concentrates probability mass onto single candidate. While NSR partially mitigates this effect, PSR substantially exacerbates it. Crucially, we observe clear inverse relationship between the degree of probability concentration and pass@K performance. As shown in Figure 3-(d), as the probability gap between the rank-1 candidate and its alternatives increases, the models pass@K accuracy declines. This suggests that over-concentration of posterior probability on the top-1 candidate suppresses the exploration of other plausible reasoning paths. This observation naturally motivates key question: If we can mitigate this over-concentration, can we improve pass@K performance by encouraging broader exploration? Our algorithmic design is precisely driven by this intuition and presented in the next section."
        },
        {
            "title": "Summary of Findings",
            "content": "RLVR Induces Over-Concentration: Standard RLVR methods, such as GRPO, tend to aggressively concentrate the token probability distribution. Specifically, the probability of the rank-1 candidate saturates near 1, while the probabilities of other viable alternatives collapse to negligible values. Over-Concentration Inversely Correlates with Pass@K: We observe significant inverse correlation between the degree of probability concentration on the rank-1 candidate and the models pass@K performance. Specifically, as the disparity between the probability of the rank-1 candidate and its alternatives increases, the pass@K accuracy decreases."
        },
        {
            "title": "4 SIMKO: SIMPLE PASS@K OPTIMIZATION",
            "content": "In this section, we present the three key components of our SimKO method: (i) identifying forking tokens for reasoning, (ii) applying top-K label smoothing to redistribute positive gradients among the top-K candidates, and (iii) strengthening rank-1 candidate updates for negative gradients. These steps, illustrated in Figure 4, work together to improve reasoning diversity."
        },
        {
            "title": "Technical Report",
            "content": "Figure 4: Intuition of the proposed method. (a) We begin by identifying the forking tokens, which are high-entropy tokens, and diverge into multiple reasoning paths. (b) For positive samples, we redistribute the probability mass from the top-1 candidate to the top-K candidates, mitigating overconcentration. (c) For negative samples, we apply strong penalty to the top-1 candidate and weaker penalty to non-top-1 candidates to prevent the squeezing effect, thereby avoiding sharp distributions and facilitating model exploration. 4."
        },
        {
            "title": "IDENTIFYING INFORMATIVE TOKENS TO OPTIMIZE",
            "content": "While standard on-policy RLVR methods compute gradients over the entire generated sequence, not all tokens contribute equally to logical inference. Recent work (Wang et al., 2025) shows that only small subset of tokens drives the core reasoning process, whereas the majority (over 80%) primarily ensure grammatical fluency and formatting. Our analysis supports this finding by showing that key reasoning tokens, often marked by forking tokens, often exhibit higher entropy and shape the reasoning trajectory (Figure 4-(a)). Building on this insight, SimKO selectively operate on critical subset of tokens whose entropy is greater than threshold τ . In short, by replacing the γi,l in the vanilla GRPO loss with newly defined γi,l, the proposed method can be written as: JSimKO(θ) = JGRPO(θ; γi,l γi,l); γi,l := γpos i,l , γneg i,l , γi,l, if H(πθ(si,l)) > τ, Ai,l > 0, if H(πθ(si,l)) > τ, Ai,l < 0, if H(πθ(si,l)) τ, Ai,l, (3) where H(πθ(si,l)) = (cid:80) distribution at state si,l. Ai,l is the advantage calculated in vanilla GRPO. yi,lV πθ(yi,l si,l) log πθ(yi,l si,l) denotes the entropy of the policy"
        },
        {
            "title": "4.2 REDISTRIBUTING POSITIVE GRADIENTS AMONG TOP-K CANDIDATES",
            "content": "We now design regularization mechanisms for tokens with positive gradients, i.e., tokens from correct responses. To understand how the models predictions evolve after an update, we analyze its behavior in gradient space and denote the derivative of the loss with respect to the logits as G-term: log πθ(yi,lsi,l) := G(i, l) = πθ(si,l) eyi,l , (4) where eyi,l is the one-hot vector for the label yi,l. We now analyze the one-step effect of learning the l-th token in the i-th correct response. As shown Figure 4-(b), one-step update increases the probability of the yi,l-th candidate of πθ( si,l) while simultaneously decreasing all other candidates. When yi,l is the rank-1 candidate, which is highly likely under on-policy sampling, the distribution becomes sharper, and the probability gap between the rank-1 candidate and its alternatives grows. Continued training under this dynamic causes the rank-1 candidate to absorb nearly all probability mass, leaving the model unable to generate diverse yet correct responses that begin with rank-k candidates. This effect is empirically validated by our results in Figure 3. To address this issue, we need to design mechanism that can reallocate probability mass absorbed by the rank-1 candidate back to other plausible ones, thereby restoring diversity. This resonates with the classical solution for over-concentration: label smoothing (Muller et al., 2019), in which the one-hot target eyi,l is replaced by convex combination of the one-hot and uniform distribution: (1 α)eyi,l + α 1 u, where α [0, 1] controls the smoothing strength. However, directly applying vanilla label smoothing in LLM fine-tuning is problematic because the vocabulary size is extremely large. Spreading probability mass uniformly risks generating ungrammatical or irrelevant tokens, which can destabilize training. To address this problem, we take"
        },
        {
            "title": "Technical Report",
            "content": "inspiration from the design of top-K sampling (Fan et al., 2018), and propose top-K label smoothing, which redistributes probability only across several most plausible candidates. Concretely, by replacing the eyi,l in Eqn. 4, we propose the G-term: G(i, l) = πθ(si,l) etopK = πθ(si,l) ((1 α)eyi,l + α (cid:88) ek), kItopK (5) where ItopK denotes the indices of the top-K tokens under the current model distribution. Importantly, we retain this definition even if yi,l ItopK, ensuring consistent treatment of the target token. We now present equivalent loss function with the re-designed one-step gradient provided in Equation 5 as γpos i,l = (1 α) γi,l + α ItopK (cid:88) sg kItopK γi,l γ(k) i,l (cid:32) (cid:33) γ(k) i,l , α [0, 1], (6) i,l si,l si,t (cid:0)y(k) (cid:1)/πθref i,l = πθ (cid:0)y(k) (cid:1), which is the ratio of the models probability on the where γ(k) i,l rank-k candidate. The term sg() is designed to make sure that sg(γpos i,l ) = sg(γi,l), which is crucial for importance sampling in GRPO-based methods (see Equation 1). More design details can be found in Appendix B. Intuitively, optimizing with γpos increases the probabilities of the top-K i,l candidates, causing the output distribution to form plateau rather than sharp peak. This flatter distribution promotes exploration and enhances response diversity, as illustrated in Figure 4."
        },
        {
            "title": "4.3 RESTRAINING NEGATIVE GRADIENTS ON TOP-1 CANDIDATE",
            "content": "We design separate mechanism for negative responses because their gradients affect the probability distribution asymmetrically compared to positive gradients. As in the previous subsection, we analyze the one-step influence of negative gradients (Ai,l < 0) through their G-term. Expanding G(i, l) element-wise (Equation 4) leads to: [G(i, l)]yi,l = πθ(yi,lsi,l) 1 and [G(i, l)]others = πθ(yi,lsi,l). This formulation highlights two key effects. First, candidates with already high probabilities experience minimal push-down pressure when selected, since πθ(yi,l si,l)1 0. Second, the probability mass removed from the target candidate is redistributed proportionally across all other candidates based on their πθ(yi,l si,l). These mechanisms align with the squeezing effect described in Ren & Sutherland (2025), where the rank-1 candidate benefits the most during redistribution. This dual influence also clarifies why simply amplifying negative gradients, e.g., by multiplying big scalar to those Ai,l < 0, is ineffective in mitigating the decay of exploration. While uniformly stronger push-down pressure can suppress probability growth of the rank-1 candidate, the induced squeezing effect on non-rank-1 candidates paradoxically makes the distribution sharper, as demonstrated in the upper panel of Figure 4-(c). In summary, for negative gradients, we must control the relative strength between the negative gradients imposed on rank-1 and non-rank-1 candidates. Motivated by this, we propose to replace the original γi,l by λ γi,l when yi,l is the rank-1 candidate, where λ is hyper-parameter that is greater than one. The effect of applying this γneg i,l is demonstrated by the lower panel in Figure 4-(c). We present the pseudo-code in Figure 5. def compute_policy_loss(): ratio = torch.exp( log_prob - old_log_prob ) + + + + + + + + + # 1. Identify forking tokens = (entropy > percentile(entropy, τ )) # 2. Using top-K ratio topk_ratio = torch.exp(topk_log_probs - old_topk_log_probs) topk_ratio = ((ratio.detach() / topk_ratio.detach())*topk_ratio).sum(dim=-1) ratio = torch.where(advantage > 0, (1-α*w)*ratio + (α*w/K)*topk_ratio, ratio) # 3. Apply strong penalty to top1 negative tokens mask = (advantage < 0) & is_top1 & ratio[mask] *= λ pg_losses = -advantage * ratio # ...clip and compute loss Figure 5: The pseudo-code of the policy loss computation with SimKO. SimKO only requires modifying few lines from standard policy gradient implementation."
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Comparison of SimKO with GRPO, KL-Cov, and Entropy-Adv on Qwen2.5-Math-7B. SimKO effectively controls probability concentration on the Λ(1) while preserving diversity among Λ(2) and Λ(3). Figure 7: Token-level entropy distributions from the Qwen2.5-Math-7B backbone trained with SimKO, GRPO, KL-Cov, and Entropy-Adv, demonstrating SimKOs ability to maintain the entropy of the forking tokens."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Models and Datasets. We experiment with diverse set of models, including Qwen2.5-7B (Team, 2024), Qwen2.5-Math-7B (Yang et al., 2024), and Llama3.2-3B-Instruct (AI@Meta, 2024). Following the setups in Zhu et al. (2025); Zeng et al. (2025), the Qwen models are trained on the MATH dataset (Hendrycks et al., 2021), while the Llama model is trained on combined dataset of GSM8K (Cobbe et al., 2021) and MATH (Level 1). For logical reasoning tasks, we train Qwen2.5-7B on the Synlogic-easy dataset (training split). Training details and hypers in Appendix D.1. Evaluation Protocol. We compare SimKO against several competitive baselines, including GRPO, PSR, NSR, W-REINFORCE (Zhu et al., 2025), KL-Cov (Cui et al., 2025), P@k T. (Chen et al., 2025a) and Entropy-Adv (Cheng et al., 2025). Evaluations are conducted on variety of reasoning benchmarks: MATH-500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), Olympiad-Bench (He et al., 2024), AMC, AIME, Synlogic-easy (validation split) (Liu et al., 2025a), and BBH (Suzgun et al., 2022). To obtain comprehensive evaluation, we adopt the unbiased pass@K (cid:1)(cid:3), where denotes the metric with up to 256, computed as pass@K := ExD number of correct completions out of generated responses. To reduce evaluation variance on small datasets (e.g., AIME and AMC), we set = 300; for other math datasets, we use = 256, and for logic datasets, = 128. (cid:2)1 (cid:0)nc (cid:1)/(cid:0) K"
        },
        {
            "title": "5.2 EFFECTS OF SIMKO ON TRAINING DYNAMICS",
            "content": "We analyze the training dynamics of SimKO in comparison with GRPO, KL-Cov, and Entropy-Adv. Figure 6 presents the changes of top-K log-probabilities (Λ(1), Λ(2), and Λ(3)) across training steps. As can be seen, GRPO leads to severe over-concentration: Λ(1) GRPO and Λ(3) GRPO sharply drop below 108 and 1010, respectively. This indicates that nearly all probability mass collapses onto the top-1 token. KL-Cov exhibits moderate concentration effect due to the KL penalty, while Entropy-Adv collapses even more rapidly, likely because of its stronger emphasis on high-entropy tokens. In contrast, SimKO achieves the most effective deconcentration among all methods. This is evidenced by lower Λ(1) SimKO. These results suggest that SimKO effectively mitigates probability mass collapse and can potentially encourages exploration during training. GRPO increases to nearly 1, while Λ(2) SimKO and higher Λ(2) SimKO and Λ(3) To further validate this, we visualize the histogram of token-level entropy in Figure 7. GRPO drives most tokens toward near-zero entropy. SimKO, however, can preserve token entropy, particularly"
        },
        {
            "title": "Method",
            "content": "1 2 4 8 16 64 128 256 Qwen2.5-Math-7B Base Model GRPO PSR NSR W-REINFORCE KL-Cov P@k T. Entropy-Adv SimKO (SimKO-GRPO) Qwen2.5-7B Base Model GRPO PSR NSR W-REINFORCE SimKO (SimKO-GRPO) 25.8 41.7 39.5 39.5 41.5 42.5 39.8 42.1 43.4 +1.7 26.6 38.4 36.2 35.2 35.9 38.9 +0.5 Llama3.2-3B-Instruct Base Model GRPO PSR NSR W-REINFORCE SimKO (SimKO-GRPO) 14.2 23.3 20.6 22.5 22.4 24.0 +0.7 35.9 47.9 45.1 47.0 48.6 49.5 47.7 47.7 50.7 +2. 35.0 44.4 41.6 42.1 42.7 45.5 +1.1 20.7 29.4 26.1 29.1 28.8 30.3 +0.9 45.1 53.3 49.9 53.6 54.4 55.4 54.1 52.6 56.7 +3.4 42.7 49.8 46.5 48.2 48.8 50.8 +1.0 28.0 35.7 32.0 35.6 34.9 36.4 +0.7 52.8 58.1 54.3 59.2 59.5 60.4 59.5 56.7 61.4 +3. 49.5 54.5 51.2 53.7 54.0 55.2 +0.7 35.6 41.4 37.9 41.6 40.8 42.0 +0.6 59.1 62.6 58.5 64.0 64.1 64.8 64.2 60.3 65.6 +3.0 55.6 58.7 55.7 58.4 58.8 59.2 +0.5 43.1 46.9 43.9 47.4 46.4 47.6 +0.7 64.3 66.4 62.4 68.2 68.4 68.6 68.5 63.9 69.4 +3. 61.1 62.5 59.8 62.5 63.3 63.1 +0.6 50.2 52.4 49.8 53.1 52.0 53.3 +0.9 68.8 69.7 66.0 72.2 72.4 72.0 72.5 67.7 73.1 +3.4 66.2 66.0 63.4 66.3 67.3 67.0 +1.0 57.2 57.9 55.8 58.6 57.5 58.9 +1.0 72.7 72.9 69.3 76.1 76.3 75.5 76.3 71.9 76.7 +3. 71.3 69.3 66.9 69.8 71.1 70.8 +1.5 63.6 63.7 61.8 64.1 63.1 64.8 +1.2 76.4 76.1 72.5 80.3 80.2 79.0 80.1 76.1 80.5 +4.4 76.4 72.3 70.7 72.8 75.2 74.3 +2.0 68.9 69.5 67.8 69.7 68.1 70.8 +1.3 Table 1: Average pass@256 results for Qwen2.5-Math-7B, Qwen2.5-7B, and Llama3.2-3B-Instruct on MATH500, AIME 2024/25, Minerva math, Olympiadbench, and AMC23 Datasets. at semantic forks, where high entropy is desirable for exploration. This preservation of entropy further further confirms SimKOs role in promoting exploration."
        },
        {
            "title": "5.3 MAIN RESULTS ON MATH BENCHMARKS",
            "content": "We evaluate SimKO on six widely used math benchmarks across different model backbones. Table 1 reports average pass@K results for ranging from 1 to 256. Detailed results on separate benchmarks are provided in the Appendix Table 4. Compared to the base models, SimKO significantly improves the pass@1 score by 17.6% on Qwen2.5-Math-7B and 9.8% on Llama3.2-3B-Instruct, indicating improved exploitation. At the same time, it also boosts the pass@256 score by 4.1% and 1.9% on the same backbones respectively, demonstrating improved exploration and overall reasoning quality. Although the base model of Qwen2.5-7B achieves the highest pass@256 score (76.4% vs. 74.3% from SimKO), its pass@1 performance is notably low (26.6% vs. 38.9% from SimKO), indicating an imbalance between exploration and exploitation. Compared to GRPO and its variants (KL-Cov, Entropy-Adv and P@k T.), SimKO consistently outperforms them across all model backbones and values of K. More importantly, SimKO delivers these gains without sacrificing exploration (pass@256) and with even stronger exploitation (pass@1). Relative to GRPO, SimKO improves pass@256 by 4.4%, 2.0%, and 1.3% on Qwen2.5-Math-7B, Qwen2.5-7B, and Llama3.2-3B-Instruct, respectively, while also achieving higher pass@1. Al8 PSR NSR"
        },
        {
            "title": "BBH",
            "content": "1 2 4 8 16 64 128 1 2 4 16 32 64 128 Base Model 3. 4.9 7.5 11.1 15.4 20.0 24.5 28.7 42.4 59.3 74.4 84.6 89.9 92.4 93.6 94.2 GRPO 35.3 38.2 40.8 42.9 44.7 46.3 47.9 49.4 56.4 64.6 71.3 76.6 80.7 83.9 86.3 88.2 27.3 29.3 31.7 34.3 36.9 39.4 41.6 43.6 54.9 62.7 68.8 73.4 76.8 79.4 81.4 82.8 W-REINFORCE 0. 1.1 1.9 1.3 3.2 1.8 5. 2.4 8.5 12.5 17.2 21.6 26.1 41.6 59.3 74.9 85.3 90.5 92.7 93.5 2.9 3.4 3.8 3.9 15.4 21.9 27.8 32.3 35.4 37.2 38.3 38. SimKO 34.7 38.4 42.0 45.5 48.5 51.0 53.2 55.0 58.4 69.5 77.7 83.2 86.8 89.2 90.9 92.0 Table 2: Pass@K results for Qwen2.5-7B on Synlogic and BBH Datasets. though P@k T. achieves higher pass@256 (80.1%), its pass@1 performance drops to 39.8%, indicating that it fails to balance exploration and exploitation effectively. For NSR and W-REINFORCE, strong pass@256 performance is maintained, but often at the expense of much lower pass@1. In contrast, SimKO achieves better balance on most backbones. On Qwen2.5-Math-7B, SimKO reaches slightly higher pass@256 score (80.5% vs. 80.3% for NSR and 80.2% for W-REINFORCE) while clearly outperforming both in pass@1 (43.4% vs. 39.5% and 41.5%). similar trend is observed on Llama3.2-3B-Instruct, where SimKO improves both pass@256 (70.8% vs. 69.7% and 68.1%) and pass@1 (24.0% vs. 22.5% and 22.4%). For Qwen2.57B, however, the trade-offs differ: SimKO outperforms NSR on pass@256 (74.3% vs. 72.8%) but lags slightly behind W-REINFORCE (75.2%), and on pass@1 it significantly surpasses both baselines (35.2% vs. 35.9% for NSR and 38.9% for W-REINFORCE). These results support our hypothesis that alleviating probability over-concentration (Figure 6) improves pass@K performance, indicating better balance between exploitation and exploration."
        },
        {
            "title": "5.4 GENERALIZATION TO LOGICAL TASKS",
            "content": "We evaluate SimKOs generalization ability on two logic reasoning benchmarks, as shown in Table 2. These benchmarks cover two scenarios: (1) Synlogic, an in-distribution task where the base model performs poorly in pass@K on both the training and test datasets, which come from the same distribution (Synlogic-easy), and (2) BBH, an out-of-distribution task where the base model performs better in pass@K, but the test data differs from the training data distribution. On Synlogic, SimKO significantly outperforms the base model, with +31.6% gain in pass@1 and +26.3% at pass@128. Methods like GRPO and PSR show improvements but lag behind SimKO by 4.6% and 11.4% at pass@128. NSR and W-REINFORCE, however, fail to train effectively, with pass@1 scores of only 1.1% and 0.8%. Similar observations can also be found in BBH dataset. On BBH, SimKO boosts the base models pass@1 to 58.4% (+16.0%), and maintains stability at higher sampling rates, with just 2.2% decrease in pass@128. GRPO and PSR, by comparison, drop 6.0% and 11.4% at pass@128 compare to base model, showing difficulties in sustaining performance. NSR and W-REINFORCE perform poorly, achieving only 26.1% and 15.4% at pass@1. These results demonstrate that relying solely on negative samples is insufficient to improve Pass@K on challenging tasks. In contrast, SimKO exhibits strong generalization, effectively trains on difficult tasks, and improves Pass@K performance by mitigating probability overconcentration."
        },
        {
            "title": "5.5 ABLATION STUDIES",
            "content": "We conduct an in-depth analysis of how various parameters, such as τ , α, K, and the impacts of γpos and γneg, affect SimKO. The full ablation results are summarized in Table 3, and the performance variations with respect to τ , α and are shown in Figure 8. Specifically, we evaluate α values from 0 to 0.1, with α = 0 representing the performance of GRPO. Increasing α results in monotonic improvement in pass@256, with gains ranging from 3.3% to 4.4% compared to GRPO. In contrast, pass@1 performance peaks at α = 0.01 and then slightly degrades, though it remains superior to GRPO. For τ , we test values from 0 to 100, with τ = 100 corresponding to GRPO. Notably, SimKO outperforms GRPO across all τ values in pass@256. However, when τ = 0,"
        },
        {
            "title": "Method",
            "content": "AIME24 AIME"
        },
        {
            "title": "AMC",
            "content": "MATH500 Minerva Olympiad Avg. Base Model 13.2/66.0 5.4/51.8 38.2/98. 55.8/96.0 16.5/68.8 25.6/77.0 25.8/76.4 GRPO α=0. α=0.03 α=0.05 α=0.1 τ (0) τ (40) τ (60) = 1 = 2 = 4 = 5 w/o γneg w/o γpos"
        },
        {
            "title": "SimKO",
            "content": "28.1/72.3 11.5/52.1 61.2/97.1 76.6/96.2 33.4/64.0 39.1/74. 41.7/76.1 31.1/79.9 13.3/58.9 62.6/99.3 77.6/97.0 35.2/66. 39.4/76.9 43.2/79.7 30.9/72.7 12.4/64.6 62.4/97.5 77.2/97. 34.9/66.9 38.9/76.9 42.8/79.4 30.8/78.7 12.2/67.8 63.0/97. 77.2/96.8 34.8/65.1 39.3/76.3 42.9/80.4 29.1/75.5 12.4/65. 61.8/99.9 77.2/96.8 35.2/67.6 38.9/77.8 42.4/80.4 10.7/74. 6.2/54.0 51.7/96.7 70.5/95.8 32.2/67.3 33.5/74.5 34.1/77. 20.8/70.9 7.2/57.5 58.3/94.6 74.9/95.2 33.8/68.0 36.4/75. 38.6/76.9 27.5/77.6 10.0/56.0 62.2/99.9 76.5/96.8 35.3/68. 38.2/76.4 41.6/79.3 29.5/83.7 12.2/55.1 62.1/97.1 77.1/96. 35.1/65.8 39.5/75.9 42.6/79.1 30.3/78.4 12.2/57.9 62.5/99. 77.2/96.4 34.9/65.8 39.5/76.9 42.8/79.2 32.8/84.6 12.5/54. 62.6/97.5 77.7/97.2 35.3/68.4 39.2/78.4 43.4/80.1 31.3/78. 11.7/58.0 62.2/99.6 77.5/96.2 35.7/68.0 39.0/76.3 42.9/79. 31.5/80.7 11.5/57.9 62.7/97.4 77.1/96.2 34.1/65.8 39.0/75. 42.8/78.9 30.4/75.2 12.7/64.9 62.3/99.6 77.4/96.4 34.7/65. 39.5/77.3 42.8/79.9 32.8/78.0 12.9/64.6 62.4/97.5 77.6/96. 35.0/68.4 39.8/77.8 43.4/80.5 Table 3: Ablations on α, τ , k, γpos, and γneg. Pass@1/Pass@256 scores are evaluated using Qwen2.5-Math-7B. Figure 8: Ablations on α, τ and K. Pass@1 and pass@256 scores are evaluated using the Qwen2.5-Math-7B backbone on math benchmarks. where SimKO is applied to all tokens, pass@1 drops significantly by 9.3% compared to applied in forking tokens. This indicates that restricting SimKO to the semantic forks of the task is essential for maintaining optimal performance. For K, we test values from 1 to 5. As increases, both pass@256 and pass@1 show an initial increase followed by decrease. This trend suggests that restricting optimization to small subset of the most probable tokens is sufficient. Specifically, pass@256 fluctuates between 79.1% and 80.5%, while pass@1 fluctuates between 42.6% and 43.4%, both outperforming GRPO. Additionally, as shown in Table 3, applying SimKO exclusively to either correct or incorrect examples leads to drop in pass@K performance. This highlights the importance of asymmetric regularization, applied to both correct and incorrect examples, as it yields the best results."
        },
        {
            "title": "6 CONCLUDING REMARKS",
            "content": "This paper addresses key limitation in current RLVR methods, where pass@K performance drops due to reduced output diversity. Through analyzing token-level posterior probabilities, we find that RLVR training causes the model to overly concentrate probability mass on the top-1 candidate, leading to deterministic policy and limited exploration. To overcome this issue, we propose Simple Pass@K Optimization (SimKO), method that mitigates this effect by redistributing gradient updates across the top-K candidates. Extensive evaluations demonstrate that SimKO effectively preserves output diversity and consistently outperforms the GRPO baseline on both pass@1 and pass@256 metrics. These results highlight that SimKO achieves superior balance between exploitation and exploration, thereby enhancing the models overall reasoning capabilities."
        },
        {
            "title": "REFERENCES",
            "content": "Madhu Advani, Andrew Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428446, 2020. 15 AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md. 7 Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@k training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025a. 1, Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025b. 2, 15 Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. 2, 7, 15 Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. 15 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 7 Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. 2, 7, 15, 16 DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. 15 Wenlong Deng, Yi Ren, Muchen Li, Danica Sutherland, Xiaoxiao Li, and Christos Thrampoulidis. On the effect of negative gradient in group relative deep reinforcement optimization. arXiv preprint arXiv:2505.18830, 2025a. 16 Wenlong Deng, Yi Ren, Yushu Li, Boying Gong, Danica Sutherland, Xiaoxiao Li, and Christos Thrampoulidis. Token hidden reward: Steering exploration-exploitation in group relative deep reinforcement learning. arXiv preprint arXiv:2510.03669, 2025b. 15 Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, et al. Rl-plus: Countering capability boundary collapse of llms in reinforcement learning with hybrid-policy optimization. arXiv preprint arXiv:2508.00222, 2025. 2, 15 Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018. Andre He, Daniel Fried, and Sean Welleck. Rewarding the unlikely: Lifting grpo beyond distribution sharpening. arXiv preprint arXiv:2506.02355, 2025a. 1 Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. 7 Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025b."
        },
        {
            "title": "Technical Report",
            "content": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 7 Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. Advancing language model reasoning through reinforcement learning and inference scaling. arXiv preprint arXiv:2501.11651, 2025. 15 Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. 1 Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. URL https://arxiv.org/abs/2503.24290. 15 Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. 15 Yuxian Jiang, Yafu Li, Guanxu Chen, Dongrui Liu, Yu Cheng, and Jing Shao. Rethinking entropy regularization in large reasoning models. arXiv preprint arXiv:2509.25133, 2025. 15 Katie Kang, Amrith Setlur, Dibya Ghosh, Jacob Steinhardt, Claire Tomlin, Sergey Levine, and Aviral Kumar. What do learning dynamics reveal about generalization in llm reasoning? arXiv preprint arXiv:2411.07681, 2024. 15 Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. 7 Jiazheng Li, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Hongzhou Lin, Yi Wu, and Jingzhao Zhang. Questa: Expanding reasoning capacity in llms via question augmentation. arXiv preprint arXiv:2507.13266, 2025. 2, Xiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, and Weizhu Chen. Beyond pass@ 1: Self-play with variational problem synthesis sustains rlvr. arXiv preprint arXiv:2508.14029, 2025. 1, 2, 15 Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, et al. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond. arXiv preprint arXiv:2505.19641, 2025a. 7 Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025b. 15 Rafael Muller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? Advances in neural information processing systems, 32, 2019. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. arXiv preprint Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv:2402.13228, 2024. 15 Noam Razin, Sadhika Malladi, Adithya Bhaskar, Danqi Chen, Sanjeev Arora, and Boris Hanin. Unintentional unalignment: Likelihood displacement in direct preference optimization. In The Thirteenth International Conference on Learning Representations, 2025. 15 Yi Ren. Learning dynamics of deep learningforce analysis of deep neural networks. arXiv preprint arXiv:2509.19554, 2025. 15 Yi Ren and Danica J. Sutherland. Learning dynamics of LLM finetuning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=tPNHOoZFl9. 3, 6, 15,"
        },
        {
            "title": "Technical Report",
            "content": "Andrew Saxe, James McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. 15 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 1, 2 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 1, 2, 15 Yuda Song, Julia Kempe, and Remi Munos. Outcome-based exploration for llm reasoning. arXiv preprint arXiv:2509.06941, 2025. 1 Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. 7 Remi Tachet, Mohammad Pezeshki, Samira Shabanian, Aaron Courville, and Yoshua Bengio. On the learning dynamics of deep neural networks. arXiv preprint arXiv:1809.06848, 2018. 15 Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Christian Walder and Deep Karkhanis. Pass@ policy optimization: Solving harder reinforcement learning problems. arXiv preprint arXiv:2505.15201, 2025. 2, 15 Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. 5, 15 Fang Wu, Weihao Xuan, Ximing Lu, Zaid Harchaoui, and Yejin Choi. The invisible leash: Why rlvr may not escape its origin. arXiv preprint arXiv:2507.14843, 2025. 1, 15 An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, and Jing Tang. Depth-breadth synergy in rlvr: Unlocking llm reasoning gains with adaptive exploration. arXiv preprint arXiv:2508.13755, 2025. 2, 15 Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 15 Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 1, 15 Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. In Second Conference on Language Modeling, 2025. 7, 15 Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 15 Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025. 4, 7, 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 15 15 18 18 19 20"
        },
        {
            "title": "Appendix",
            "content": "A.1 Reinforcement Learning with Verifiable Rewards in LLMs . A.2 Effective Exploration for RLVR in LLMs . A.3 Analysis of Learning Dynamics in LLMs . . . . . . . . . . . . . . . . More Details about the Design of γpos i,l Additional Analysis of Distribution Changes During Training . . C.1 Over-Concentrated Distribution across multiple models . . C.2 Top-6 Candidates Probabilities Distribution . . . . . . Experimental Details . D.1 Training Detail D.2 More Experiment Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A RELATED WORK",
            "content": "A.1 REINFORCEMENT LEARNING WITH VERIFIABLE REWARDS IN LLMS Reinforcement learning with verifiable rewards (RLVR) from large language models (LLMs) has demonstrated significant potential (DeepSeek-AI, 2025; Hugging Face, 2025; Zeng et al., 2025; He et al., 2025b), especially when directly applied to base model using GRPO (Shao et al., 2024) for RL training. This approach has notably enhanced the base models performance, particularly in improving its reasoning abilities for mathematical and coding tasks. Subsequent works have focused on improving GRPO to further enhance the algorithms performance. For instance, DAPO (Yu et al., 2025) adjusts GRPOs clipping thresholds and removes KL regularization to encourage larger updates in correct answer. Dr.GRPO (Liu et al., 2025b) eliminates the normalization term when computing advantages to prevent length bias. GSPO (Zheng et al., 2025) modifies the importance sampling from the token level to the sequence level, which proves to be more stable in training mixture-of-experts (MoE) models. These modifications have contributed to improvements in the models pass@1 performance, but they have not specifically addressed pass@K performance, which relates to the models exploration ability. A.2 EFFECTIVE EXPLORATION FOR RLVR IN LLMS central challenge in RLVR tasks lies in moving beyond the exploitation of pretrained models implicit knowledge to actively exploring diverse reasoning paths. Current methods tend to converge on limited set of solutions, as evidenced by poor performance on the pass@K metric, which evaluates the coverage of multiple reasoning paths and thus reflects exploration effectiveness (Yue et al., 2025; Wu et al., 2025). To address this exploration deficit, the community has pursued several strategies. Data-centric methods aim to use data augmentation to enhance the models exposure to diverse reasoning environments, thereby encouraging the exploration of broader range of solution paths. One such approach involves using off-policy data from more capable models to expand the models knowledge and promote solution diversity (Dong et al., 2025; Li et al., 2025). Additional strategies include generating varied responses for challenging samples (Yang et al., 2025) or paraphrasing questions to stimulate different reasoning trajectories for the same problem (Liang et al., 2025). As complementary approach, Reward-centric methods redesign the objective function to directly incentivize diversity by calculating group-wise reward based on set of candidate solutions, providing an unbiased gradient for optimizing pass@K (Walder & Karkhanis, 2025; Chen et al., 2025b). While these methods are effective to some extent, both treat the model as black box, manipulating its inputs and final supervisory signals without understanding the internal mechanisms driving exploration. To address this limitation, more recent line of work has shifted focus inward. The Entropy-based methods use entropy as proxy for exploration controls (Cui et al., 2025; Cheng et al., 2025; Wang et al., 2025; Hou et al., 2025; Hu et al., 2025), but policy entropy is rough measure that does not provide fine-grained insights into models exploration behavior. More recently, (Jiang et al., 2025) improved the coarse entropy metric by considering only candidates with cumulative probabilities exceeding threshold p. However, it still lacks detailed analysis of how the models predicted candidates training dynamics affect exploration. This highlights the need for mechanism that directly monitors and explores changes in the distribution of next-token predictions during training, which is the central focus of our work. The most relevant work to ours is Token-Hidden Reward (THR) (Deng et al., 2025b), which restricts confidence increases for positive tokens to leave more probability mass for alternative reasoning paths. However, it only addresses the prevention of excessive confidence for correct tokens, overlooking the squeezing effect on negative samples. Additionally, it introduces the need for inner product calculations between token embeddings, which increases computational cost. A.3 ANALYSIS OF LEARNING DYNAMICS IN LLMS Analyzing the learning dynamics of deep neural networks provides valuable insight into how training shapes model behavior (Saxe et al., 2013; Tachet et al., 2018; Advani et al., 2020). This analytical perspective has recently been extended to Large Language Models (LLMs), where prior work has widely examined the dynamics of supervised fine-tuning (SFT) (Kang et al., 2024; Chu et al., 2025), off-policy preference optimization methods such as DPO (Razin et al., 2025; Pal et al., 2024), or both (Ren & Sutherland, 2025; Ren, 2025)."
        },
        {
            "title": "Technical Report",
            "content": "Several recent studies have begun exploring the learning dynamics of on-policy RL. Cui et al. (2025) adopt entropy-based metrics to track model changes during training. However, such metrics provide only an indirect signal by averaging over the entire vocabulary, thereby failing to capture meaningful shifts among high-probability candidates. In contrast, Deng et al. (2025a) examine probability shifts induced by individual gradient updates to analyze inter-sample effects. While these analyses offer valuable fine-grained insights into probability changes, they fail to capture the cumulative evolution of the models policy. To overcome these limitations, we propose top-K probability dynamics framework that directly tracks how probability mass redistributes among the most likely candidates throughout training. This approach provides scalable and interpretable lens for understanding how on-policy RL shapes model behavior."
        },
        {
            "title": "Technical Report",
            "content": "B MORE DETAILS ABOUT THE DESIGN OF γ POS i,l This appendix provides more details about how we design Equation-(6): γpos i,l = (1 α) γi,t + α ItopK (cid:88) kItopK (cid:33) (cid:32) sg γi,l γ(k) i,l γ(k) i,l , α [0, 1], Based on the smoothed G(i, l) term in Equation-(5): G(i, l) = πθ(si,l) etopK = πθ(si,l) (1 α)eyi,l + ek , α (cid:88) kItopK Following the AKG decomposition in Ren & Sutherland (2025), we know θγi,l = Ai,l sg(γi,l) (cid:125) (cid:124) (cid:123)(cid:122) Constant log πθ(yi,l si,l) (cid:124) (cid:123)(cid:122) (cid:125) Defined as G(i,l) = Ai,l sg(γi,l) θzG(i, l). Now, we replace G(i, l) term to G(i, l), the gradient part (Ai,l sg(γi,l) is constant w.r.t θ): θz G(i, l) = θz πθ(si,l) (1 α)eyi,l + ek α (cid:88) kItopK = θz (1 α)πθ(si,l) + απθ(si,l) (1 α)eyi,l + ek α (cid:88) kItopK = θz (1 α)(πθ(si,l) eyi,l ) + α (cid:88) kItopK = (1 α)θzG(i, y) + α (cid:88) kItopK θzG(i, k) (πθ(si,l) ek) (7) From the equation above, we know that if we change eyi,l in the original γ to etopK, the gradient of the new loss can be simplified to combination of the above. Note that Ai,l sg(γi,l) θzG(i, k) is exactly the decomposition of γ(k) (cid:1) (cid:1) , i.e., updating the model using y(k) i,l . In other i,l = πθ (cid:0)y(k) i,l si,t (cid:0)y(k) i,l si,l πθref words, our new loss might have form like (1 α)γi,l + α (cid:88) γ(k) i,l kItopK However, the combination above will make the new γ biased estimator (the RL theoretical guarantee needs correct importance sampling). We then use the following stop-gradient trick to fix this. to the second term in Equation-(7), we can ensure that Specifically, by multiplying sg sg(γpos i,l ) = sg(γi,l). This design is only one line of code, using the .detach() in Pytorch. γi,l/γ(k) i,l (cid:17) (cid:16)"
        },
        {
            "title": "C ADDITIONAL ANALYSIS OF DISTRIBUTION CHANGES DURING TRAINING",
            "content": "C.1 OVER-CONCENTRATED DISTRIBUTION ACROSS MULTIPLE MODELS In Figure 9, we present the training dynamics for all three models, which validate our findings in Section 3 across multiple models. Figure 9: (a)-(c) Training dynamics of average log probability Λ and top-K probabilities Λ(k) derived by GRPO, NSR, and PSR. (d) The corresponding pass@1 and pass@K results of the RLVL-trained models. Following the setups of Zhu et al. (2025), we train Llama3.2-3B-Instruct on mixture of GSM8K and MATH (Level 1) and train Qwen2.5-Math-7B/Qwen2.5-7B on the MATH dataset. Figure 10: The probability distribution of each top-K candidate. It shows that the probabilities of candidates from top-2 to top-6 largely fall within the lowest probability range, indicating that monitoring the top-K candidates probability distribution is sufficient."
        },
        {
            "title": "Technical Report",
            "content": "C.2 TOP-6 CANDIDATES PROBABILITIES DISTRIBUTION Figure 10 illustrates the probability distributions of the top-6 candidates. The rank-1 candidate distribution for all models is concentrated in the highest probability bin, with the GRPO-trained model showing more pronounced concentration, where over 90% of the candidates are focused on the highest probability. In contrast, the rank-2 to rank-6 candidates are concentrated in the lowest probability bin, with over 95% of the probability for the rank-6 candidate across all models being below 0.05. This concentration of probability mass in the top-K candidates suggests that their collective distribution serves as sufficient proxy for the full vocabulary distribution of the model."
        },
        {
            "title": "D EXPERIMENTAL DETAILS",
            "content": "D.1 TRAINING DETAIL All models are trained with learning rate of 106, batch size of 1024, and PPO mini-batch size of 256. Each input problem is sampled with 8 responses using temperature 1.0. We set α in SimKO as 0.01, and define the entropy threshold τ (q) as the q-quantile of the token-level entropy distribution, such that fraction of all tokens have entropy values lower than τ (q). Unless otherwise specified, we use τ (0.8) in our experiments. We also set λtop1 = 1.1, except for Qwen2.5-7B where we use λtop1 = 1.05. For the logic task, we apply warm-up of 50 steps and use smaller α = 0.005 along with λtop1 = 1.05. D.2 MORE EXPERIMENT RESULT In this section, we present more detailed experimental results, as shown in Table 4."
        },
        {
            "title": "Method",
            "content": "AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Avg. Qwen2.5-Math-7B 38.2/98.5 5.4/51.8 13.2/66.0 Base Model 61.2/97.1 11.5/52.1 28.1/72.3 GRPO 62.1/94.9 11.2/48.9 19.3/68.5 PSR 59.4/100.0 9.7/61.2 22.8/80.3 NSR 61.1/97.4 10.8/55.7 29.2/86.5 W-REINFORCE 62.2/97.4 11.7/55.2 30.9/81.2 KL-Cov 58.8/97.5 10.2/61.3 P@k T. 26.7/77.9 62.5/92.1 10.9/55.0 GRPO w/ Entropy-Adv 29.1/81.7 32.8/78.0 SimKO 62.4/97.5 12.9/64.6 (SimKO-GRPO) +4.7/+5.7 +1.4/+12.5 +1.2/+0.4 Qwen2.5-7B Base Model GRPO PSR NSR W-REINFORCE SimKO (SimKO-GRPO) Llama3.2-3B-Instruct Base Model GRPO PSR NSR W-REINFORCE SimKO (SimKO-GRPO) 3.6/48.8 7.4/64.0 8.8/55.0 15.6/59.0 9.3/54.2 14.3/61.8 7.1/52.3 9.9/48.9 6.1/54.2 11.2/57.5 16.3/58.4 9.4/55.2 +0.7/-0.6 +0.6/+0.2 3.4/51.7 12.7/55.1 7.8/57.4 11.1/53.7 13.3/51.7 13.8/54.6 +1.1/-0.5 0.7/46.7 1.1/44.1 1.0/35.1 1.5/47.4 1.1/42.1 1.0/45.4 -0.1/+1.3 36.1/99.6 56.0/92.5 51.6/96.7 49.8/97.1 54.3/99.6 57.3/97.1 1.3/+4. 20.3/94.9 32.5/96.7 27.2/98.8 30.3/94.6 31.4/96.3 35.2/98.8 +2.7/+2.1 25.8/76.4 55.8/96.0 41.7/76.1 76.6/96.2 39.5/72.5 74.0/91.4 39.5/80.3 74.6/97.0 41.5/80.2 76.4/96.4 42.5/79.0 76.5/97.0 39.8/80.1 73.3/96.8 42.1/76.1 77.1/95.0 77.6/96.8 43.4/80.5 +1.0/+0.6 +1.6/+4.4 +0.7/+3.1 +1.7/+4.4 25.6/77.0 39.1/74.7 37.6/67.7 37.8/78.4 38.1/77.6 39.2/76.9 36.6/78.2 39.7/71.9 39.8/77.8 16.5/68.8 33.4/64.0 32.8/63.6 32.9/65.1 33.4/67.6 34.4/66.2 33.2/68.8 33.5/60.7 35.0/68.4 61.4/97.2 75.7/95.2 73.6/92.0 73.9/95.4 73.9/95.8 76.7/94.8 1.0/-0.4 23.0/70.2 35.7/61.8 32.9/52.2 33.7/65.8 33.4/66.2 35.2/66.2 -0.5/+4. 26.6/76.4 28.1/78.5 38.4/72.3 38.8/70.4 36.2/70.7 35.6/67.0 35.2/72.8 36.6/77.0 35.9/75.2 36.6/77.9 38.7/74.4 38.9/74.3 -0.1/+4.0 +0.5/+2.0 14.2/68.9 37.8/93.6 23.3/69.5 53.1/91.6 20.6/67.8 50.3/91.0 22.5/69.7 53.3/94.0 22.4/68.1 52.4/92.8 54.6/93.4 24.0/70.8 +1.5/+1.8 +1.2/+0.7 +0.9/+2.6 +0.7/+1.3 10.1/59.2 17.3/62.5 18.5/61.0 19.0/60.3 16.7/59.9 18.5/63.2 12.7/67.1 20.1/67.0 18.9/63.7 20.0/68.0 19.6/65.8 21.0/69.6 Table 4: Pass@1 / Pass@256 Results for Qwen2.5-Math-7B, Qwen2.5-7B, and Llama3.2-3B-Instruct on MATH500, AIME 2024/25, Minerva math, Olympiadbench, and AMC23 Datasets."
        }
    ],
    "affiliations": [
        "CUHK",
        "University of British Columbia",
        "Westlake University"
    ]
}