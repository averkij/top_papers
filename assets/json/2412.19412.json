{
    "paper_title": "MINIMA: Modality Invariant Image Matching",
    "authors": [
        "Xingyu Jiang",
        "Jiangwei Ren",
        "Zizhuo Li",
        "Xin Zhou",
        "Dingkang Liang",
        "Xiang Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image matching for both cross-view and cross-modality plays a critical role in multimodal perception. In practice, the modality gap caused by different imaging systems/styles poses great challenges to the matching task. Existing works try to extract invariant features for specific modalities and train on limited datasets, showing poor generalization. In this paper, we present MINIMA, a unified image matching framework for multiple cross-modal cases. Without pursuing fancy modules, our MINIMA aims to enhance universal performance from the perspective of data scaling up. For such purpose, we propose a simple yet effective data engine that can freely produce a large dataset containing multiple modalities, rich scenarios, and accurate matching labels. Specifically, we scale up the modalities from cheap but rich RGB-only matching data, by means of generative models. Under this setting, the matching labels and rich diversity of the RGB dataset are well inherited by the generated multimodal data. Benefiting from this, we construct MD-syn, a new comprehensive dataset that fills the data gap for general multimodal image matching. With MD-syn, we can directly train any advanced matching pipeline on randomly selected modality pairs to obtain cross-modal ability. Extensive experiments on in-domain and zero-shot matching tasks, including $19$ cross-modal cases, demonstrate that our MINIMA can significantly outperform the baselines and even surpass modality-specific methods. The dataset and code are available at https://github.com/LSXI7/MINIMA ."
        },
        {
            "title": "Start",
            "content": "MINIMA: Modality Invariant Image Matching Xingyu Jiang1, Jiangwei Ren1, Zizhuo Li2, Xin Zhou1, Dingkang Liang1, Xiang Bai1 1 Huazhong University of Science and Technology, 2 Wuhan University {jiangxy998, jwren, dkliang, xbai}@hust.edu.cn 4 2 0 2 7 ] . [ 1 2 1 4 9 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Image matching for both cross-view and cross-modality plays critical role in multimodal perception. In practice, the modality gap caused by different imaging systems/styles poses great challenges to the matching task. Existing works try to extract invariant features for specific modalities and train on limited datasets, showing poor generalization. In this paper, we present MINIMA, unified image matching framework for multiple cross-modal cases. Without pursuing fancy modules, our MINIMA aims to enhance universal performance from the perspective of data scaling up. For such purpose, we propose simple yet effective data engine that can freely produce large dataset containing multiple modalities, rich scenarios, and accurate matching labels. Specifically, we scale up the modalities from cheap but rich RGB-only matching data, by means of generative models. Under this setting, the matching labels and rich diversity of the RGB dataset are well inherited by the generated multimodal data. Benefiting from this, we construct MD-syn, new comprehensive dataset that fills the data gap for general multimodal image matching. With MD-syn, we can directly train any advanced matching pipeline on randomly selected modality pairs to obtain cross-modal ability. Extensive experiments on indomain and zero-shot matching tasks, including 19 crossmodal cases, demonstrate that our MINIMA can significantly outperform the baselines and even surpass modalityspecific methods. The dataset and code are available at https://github.com/LSXI7/MINIMA. 1. Introduction Image matching refers to establishing pixel-wise correspondences from two-view images, which serves as prerequisite for wide range of visual applications [32]. Recently, matching two images of different imaging sysrole in multimodal perceptems/styles plays vital tions [21], including image fusion and enhancement [51, Corresponding author. Figure 1. Overall Image Matching Accuracy and Efficiency on Six Datasets of Real Cross-modal Image Pairs. AUC of the pose error (@10) or reprojection error (@10px) is used for accuracy evaluation, while Pairs Per Second is used for efficiency test. Left: AUCs on each dataset of representative methods are reported. Right: average performance is summarized, wherein different colors indicate matching pipelines of sparse, semi-dense and dense matching, while our MINIMA is marked as . Only with synthetic multimodal data created by our data engine, MINIMA can generalize to real cross-modal scenes with large improvements. 57], visual localization/navigation [1, 63], target detection/recognition/tracking [13, 49, 52, 61, 62, 64], etc. They benefit from gathering the advantages of different modalities by aligning them, thereby yielding more comprehensive representations for better accuracy and robustness. However, the cross-view and cross-modality nature makes the matching task much more challenging, particularly using single model for different modalities such as RGB-Infrared (IR), RGB-Depth, and RGB-Event. Existing studies focus more on RGB-only image matching due to the accessible training sets [5, 26], which have given birth to many advanced matching architectures [9, 10, 35, 41, 47]. By contrast, cross-modal matching dataset is weak in scale and scene coverage, as concluded in Tab. 1. The main reasons are as follows: i) It is laborious to capture large number of multimodal images of the same target/scene, and also hard to ensure rich scene coverage. Therefore, existing datasets are often captured from driving or fixed camera views [19, 43], and the number of modality types is merely two or three for each dataset. ii) Creating precise dense labels is also expensive. Researchers of1 Figure 2. Qualitative Results on Real Cross-modal Image Pairs. Our methods MINIMALG (sparse) and MINIMARoMa (dense) are compared with the sparse matching pipeline ReDFeat [7] and OmniGlue [20], and semi-dense matcher XoFTR [43]. ReDFeat and XoFTR are cross-modal methods, and OmniGlue is known for its generalization ability. Matches generated by each method are drawn, where the 4 or 3 pixels. Details are recorded in the top-left red lines indicate epipolar error (pose) or projection error (homography) beyond 5 of each image pair, including the geometric errors created by default RANSAC estimation and the (# correct match / # match). 10 ten manually label matched landmarks [19, 21, 29], or use camera calibration1 to produce approximate poses [43, 53]. These limited datasets can not support the training of general matching method well due to the imbalances among them, which make the models easily dominated by simple datasets. In addition, to enlarge the data scale, researchers often generate pseudo transformations from aligned image pairs [6, 58]. However, this approach is still limited by the original data, where the stimulated deformations are not consistent with practical viewpoint changes. That is why existing works can only extract matchable features for specific modalities and show poor generalization. In this paper, we try to develop unified matching framework for multiple cross-modal cases, by filling the data gap with an effective data engine. This engine helps us to freely scale up cheap RGB images to large multimodal dataset with rich scenarios and accurate labels. The introduced dataset can well support the training of any matching pipelines, and largely enhance the cross-modal performance and zero-shot ability. Our contributions are as follows: We are the first to develop unified matching framework 1Making two cameras as close as possible, thus considering they share common camera pose. MINIMA for any cross-view and cross-modality image pairs, and achieve amazing performance enhancement. We introduce simple yet effective data engine to freely build high-quality multimodal dataset for image matching. Based on this, we construct comprehensive dataset MD-syn with large scene coverages and precise labeling, which fills the data gap for the matching community. We conduct extensive experiments on in-domain and zero-shot matching tasks including 19 cross-modal cases, which demonstrate the high quality of our MD-syn and the promising generalization of our MINIMA. 2. Related Work 2.1. RGB-Only Image Matching Image matching is fundamental problem in computer vision, which has aroused numerous matching methods over the past decades. Conventional pipelines start with handcrafted designs of keypoints detection, description then matching, which are recently updated with deep learning [8, 28, 32, 35]. These detector-based methods can establish keypoint matches with high efficiency but often struggle in textureless regions. Recently, detector-free meth2 Table 1. Overview of Representative Datasets. It contains RGBonly and multimodal matching datasets, and our proposed MDsyn. The number (#) of Pairs, Scene (Type: Indoor or Outdoor), Modality type, and the forms of Match Label are summarized. our MINIMA can outperform modality-specific approaches and shows superior performance in zero-shot tasks, solely relying on high-quality synthetic training data. Dataset #Pairs # Scene (Type) #Modality Match Label 2.3. Existing Datasets RGB Matching MegaDepth [26] ScanNet [5] 40M 230M 196 (Out.) 1513 (In.) Multimodal Matching METU-VisTIR [43] M3FD [29] LLVIP [19] DIODE [53] MD-syn (ours) 6 (Out.) 15 (Out.) 26 (Out.) 2.5K 4.2K 15K 25K 20 (In. & Out.) 480M 196 (Out. ) 1 1 2 2 2 3 7 Depth, Pose Depth, Pose Pose Pre-aligned Pre-aligned Pre-aligned Depth, Pose ods [41, 47] have been introduced to produce semi-dense or dense [9, 10] pixel matches, and achieve dominant performance on RGB image matching in terms of match number and downstream applications. Since these methods regard each pixel as matchable points within the coarse and fine matching stage, they commonly produce huge computational burden. Driven by sufficient datasets, those deep methods enjoy great success in building more accurate point matches. Supported by our data engine, those advanced matching pipelines can be easily fine-tuned to multimodal cases with large enhancements. 2.2. Multimodal Image Matching Image matching for multi-modalities is more challenging, It often due to the domain gap between two images. shows variations in pixel intensity distributions (as shown in Fig. 4), making it difficult to search matchable cues. Existing studies still rely on handcrafted designs [17, 23, 55, 56], focusing on extracting matchable information such as shape, gradient, or phase. However, these low-level features are not consistently effective and time-consuming to extract. Data-driven methods exhibit powerful abilities to extract matchable features for multimodal images. They commonly utilized off-the-shelf matching pipelines [7, 31, 43] as the backbone, then adapted them to the target modalities with specific designs. For example, ReDFeat [7] recoupled independent constraints of detection and description of multimodal feature learning with mutual weighting strategy. It performs for three cross-modality cases, but is merely trained and tested on each dataset separately. Recently, XoFTR [43] utilizes two-stage training approach for RGB-IR image matching. It is first pre-trained on collected real dataset containing 95K aligned multi-spectral images, then fine-tuned on pseudo infrared images. XoFTR achieves large enhancement on RGB-IR image matching, by using abundant training data and tailored matching rule. In this paper, we contribute to filling the data gap of the general image matching task. We demonstrate that It is necessary to analyze the data gap between RGB-only image matching and the cross-modal cases. Specifically, multi-viewed RGB images of the same target/scene are extremely cheap and easy to collect, such as directly collecting from the internet [26] or capturing video frames [40]. Opensource tools like COLMAP [37, 38] are widely used to generate precise matching labels, such as depths and camera poses. These good datasets give birth to advanced models for RGB image matching [28, 32, 35]. However, capturing large number of multimodal images of the same scene is laborsome, since some imaging devices should be gathered for shooting together. This limits the scale of available image pairs. Moreover, the matching labels cannot be directly obtained by tools, which are often labeled manually [19, 21, 29], or approximated from camera calibrations [43, 53]. We conclude representative public datasets in Tab. 1. It shows that these multimodal datasets exhibit significant variability from each other, and are all limited by the scale and scene coverage. This impedes us from training unified matching model for multiple cross-modal cases. Recently, data scale-up has shown great success in general vision tasks [53, 54]. They typically enlarge the training set by generating pseudo labels from huge wild RGB images. In contrast, our challenges lie in getting numerous paired images of different modalities and rich scenarios. For such purposes, we propose data engine to generate multiple pseudo modalities from cheap RGB image pairs. On this basis, we can generate high-quality dataset for cross-view and cross-modality image matching, that may fill the data gap for the universal matching community and will encourage more excellent matching techniques. 3. Cross-Modal Generation with Data Engine In this paper, we contribute to exploring unified image matching framework for all possible image modalities by generating large multimodal matching dataset. To achieve this, several key challenges we will face: - How to obtain large scale of image pairs with viewpoint and modality changes, and ensure the rich diversity. - How to freely generate dense labels of matching for those image pairs, such as depths and camera poses. - How to ensure the balance of different modalities in terms of scale and scene coverage. Next, we will introduce data engine to alleviate these concerns. It mainly benefits from the powerful ability of recent generative methods [15, 16]. The proposed engine can freely produce various pseudo modalities from real RGB 3 Figure 3. Overview of the proposed MINIMA pipeline: single model for any cross-modal matching tasks. Wherein the Data Engine is to generate large multimodal image matching dataset, which supports the training of matching models to obtain cross-modal ability. image pairs, whose matching labels and scene diversity would be well inherited by the generated data. 3.1. Advantages of Cross-Modal Generation The ideal strategy is to capture real images of multiple modalities in the wild. But obviously, it is impractical to arrange multiple imaging systems together. Additionally, it is more troublesome to obtain dense labels for raw image pairs, such as depth and pose information. Another common strategy [6, 7, 58] involves augmenting existing aligned image pairs by randomly generating homography matrices to simulate geometric distortions. However, this is still limited by the small scale of the used dataset in diversity. The synthetic deformations are not consistent with real viewpoint changes, resulting in weak generalization of the trained model, i.e., it can only work for the test set separated from the same dataset as the training set [7, 58]. To this end, we try to generate pseudo modalities to obtain large-scale multimodal dataset, which may help to train unified matching model for multiple cross-modal cases. Cross-modal generation from multi-viewed RGB images has distinct advantages. a) Cheap: Those RGB images are easy to collect, such as capturing from the internet [26] or video frames [40]. This allows us to avoid capturing raw multimodal images in the wild. b) Flexible: We can obtain any pseudo modality we want by only giving some real image pairs as guidance. With cheap RGB images, we can freely define the scale and scene of the target modality to generate. This helps us to obtain sufficient multimodal image pairs and ensure the balance of scale and scene diversity among different modalities, preventing model bias toward specific modalities. c) High-quality: First, the generated images have the same resolution as RGB, breaking the limits of real sensors such as infrared or depth. Second, the matching labels of RGB images can be easily obtained by open-source tools [37, 38]. Those accurate and dense labels Figure 4. Pixel Intensity Statistic for Generated Modalities. The statistic differences reveal the excellent ability of our data engine to generate modality gaps. can be directly inherited by the generated data. 3.2. Scaling Up from MegaDepth There are many RGB image matching benchmarks, represented by MegaDepth (outdoor) [26] and ScanNet (indoor) [5]. These datasets contain millions of image pairs with depth and pose information, which are widely used 4 and have facilitated the development of advanced matching pipelines [9, 10, 28, 47]. Here we choose MegaDepth [26] as the basic dataset for the following reasons. i) Multimodal perception tasks are typically performed outdoors, and also, the corresponding datasets [21] are from outdoor scenes. ii) MegaDepth demonstrates strong generalization capabilities due to its rich scene coverage and accurate labeling, making it popular choice for training the models of existing methods [9, 10, 47] to test their generalization. iii) The synthetic MegaDepth makes it convenient to fine-tune those advanced matching methods for multimodal cases. Obviously, we can also generate the data from videos as GIM [40]. However, GIM uses several times the scale of images but merely achieves slight gains in outdoor performance. Considering the high computational cost of generative models, using long videos is not economical. 3.3. Details of Our Data Engine We subsequently introduce how to use our data engine to generate different modalities from the public MegaDepth dataset. Here we consider the target modalities as commonly used Infrared, Depth, Event, Normal, and two Artistic Styles. Each modality is combined with RGB to construct cross-modal pair. Actually, we can combine any two of these modalities to form matching pair if needed, and any other new modalities we want can also be added. As depicted in Fig. 3, our data engine consists of three parts: Source Data, Guidance Data, and Generative Models. The source data is multi-viewed RGB images that we want to scale up, i.e., MegaDepth. The guidance data is real image pairs of our target cross-modality, mainly for finetuning the generative models. Here, we use publicly aligned data introduced in Tab. 1. As for generative models, we first leverage existing models to directly obtain corresponding modalities for convenience, since recent generative methods have achieved great success in image style transfer [15, 50] and depth or normal generation [3, 54]. As for other modalities, such as infrared, we use the guidance data to fine-tune advanced generative models [15]. Fig. 3 gives the overall process, and the details are as follows: Infrared: Transferring RGB to infrared is challenging due to the significant variations in their imaging systems, making existing works hard to produce satisfying results [19]. To this end, we turn to diffusion-based model for help. We use StyleBooth [15] as the basis due to its remarkable performance in style transfer. StyleBooth was originally used to generate artistic styles controlled by an image or text description. In our study, we fine-tuned it using aligned RGB-IR image pairs from the LLVIP [19] and M3FD [29] datasets. We then implement the style tuner with LoRA [18] of rank 256 and standardize the resolution 2 as 1024 1024 2To meet the resolution, we upscale the longer side of each image to 1024 pixels, then the short side is padded with zero. for both input and output. We fine-tune it on 1 NVIDIA 3090 GPU for 210k steps with fixed learning rate 1104 and batch size 2. (More Details Are in Sec. A.1 ). Depth: We directly use DepthAnything V2 [54] of the official model (the large one) to generate high-quality depth images, due to the outstanding performance of monocular depth estimation and the zero-shot ability. Event: The imaging principle of an event camera is simple, which has independent pixels that respond to bright- . ness changes in their log photocurrent = log(I). Specif- . ically, an event ek = (xk, tk, pk) is triggered at pixel . = (xk, yk) and at time tk as soon as the brightness xk increment reaches temporal contrast threshold C, i.e., L(xk, tk) . = L(xk, tk) L(xk, tk tk), (1) with L(xk, tk) = pkC, where > 0, tk is the time elapsed since the last event at the same pixel, and the polarity pk is the sign of the brightness change [12, 27]. In our study, we randomly set [0.05, 0.5], pk = 1 as suggested in [14] to simulate varied sensors and give random slight motion to compute the event responses. Normal: The surface normal images are directly generated with DSINE [3], an advanced approach that utilizes the perpixel ray direction and recasts surface normal estimation as relative rotation estimation between pixels. Artistic: Our artistic styles include oil paint and sketch, which are implemented with open-source models, i.e., Paint Transformer [30] and Anime2Sketch [50], respectively. Each of them is selected for the stylistic specialization. i=1, = {Bi}K With the above settings, we can obtain our data engine {Fθi}K i=1 corresponding to above = 6 models. On this basis, and for pair of RGB images {A0, B0}, we will create two image sets = {Ai}K i=1 of modality types. Since our source data MegaDepth [26] contains 40M image pairs for image matching, we will create over 480M cross-modal image pairs in total, with {A0, Bi}K i=1. We term the new dataset as MD-syn. Notably, we can also create any modality pair, such as Infrared-Event, if needed. The training and testing sets are split similarly to the original MegaDepth. i=1 or {Ai, B0}K 4. Modality Invariant Image Matching Model After constructing MD-syn, the training of our Modality Invariant Image MAtching (MINIMA) is easy and clear. As shown in Fig. 3, it consists of the following two stages: Stage 1: Pre-train advanced matching models on multiview RGB data until they are converged. Stage 2: Fine-tune on randomly selected cross-modal image pairs with small learning rate. We adopt pre-training and then fine-tuning strategy for the following reasons. First, training from scratch on MDsyn is challenging due to the high variance among different 5 modalities. This requires extensive iterations for convergence. By contrast, training on the RGB dataset is easy. The pre-trained models can provide good matching priors for hard tasks like multimodal matching, making it converge rapidly, as verified in Fig. 5. In addition, the training on the RGB dataset is well studied [10, 28, 47], whose officially trained models can directly support our fine-tuning. Since MegaDepth has given birth to numerous matching methods with the taxonomy of sparse, semi-dense, and dense matching, we use three representative models from them as our basic models, i.e., LightGlue (LG) [28], LoFTR [41], and RoMa [10]. We will fine-tune them and release our three models, termed as MINIMALG, MINIMALoFTR, and MINIMARoMa. Those models will be evaluated with in-domain and zero-shot matching on synthetic and real cross-modal datasets. 5. Experiments 5.1. Implementation Details We directly use the official models of LightGlue [28], LoFTR [41] and RoMa [10] as the pre-trained models, and then fine-tune them with our MD-syn. All the models are trained on 4 RTX 3090 GPUs, with batch sizes being 32, 8, and 12, respectively. Based on the pre-trained models, we first individually fine-tune for each modality. The unified model is fine-tuned on randomly selected modality pairs in each iteration, which is used for the generalization ability test. Notably, we only use RGBIR, RGB-Depth, RGB-Normal modality pairs for training, which is sufficient to achieve satisfying performance. (Refer to Appendices for More Details). Datasets. The used datasets include our synthetic MDsyn and 5 multimodal datasets of real images, which contain 19 cross-modal cases in total. In particular, 1) MDsyn splits two scenes for test, which consist of 1500 image pairs for each cross-modal case following the setting of the original Megadepth. It contains 6 cross-modal cases: 3 of them (RGB-IR/-Depth/-Normal) are used for the in-domain test, while the rest are for zero-shot evaluation. 2) METUVisTIR [43] is real RGB-IR dataset containing 2590 real image pairs with camera poses attached. 3) DIODE [44] is real RGB-Depth/Noraml dataset, containing 27858 fully aligned image pairs. 4) DSEC [46] provides 60 sequences of RGB-Event videos. Three sequences are selected as our test set, which generates 100 RGB-Event pairs for testing. We rectify the frames following the instructions of the authors to obtain aligned image pairs. To test the generalization in 5) Remote Sensing and 6) Medical domains, we use MMIM datasets [21] for evaluation, where the ground truths are manually labeled matches for homography estimation. The Remote Sensing domain consists of 7 crossmodal cases such as Optical-SAR, Optical-Map, OpticalDepth, etc. The Medical domain consists of 6 cross-modal cases such as Retina, MRI-PET, CT-SPECT, etc. Evaluation Protocols. The used datasets exhibit different labels for matching, such as camera poses and 2-D homography matrices. For two-viewed datasets, such as our MDsyn and METU-VisTIR, the recovered poses by matches are evaluated to measure the matching accuracy. We report the area under the curve (AUC) of the pose error at thresholds {5, 10, 20}. As for homography, similar to [41], we collect the mean projection error of four corner points and report the AUC under thresholds {3px,5px,10px} for evaluation. Notably, for those aligned image pairs, we impose synthetic homography matrices on one image to imitate deformations, which are finished before evaluation to maintain fairness. Then, we try to recover the homography matrix. And we uniformly resize all images with their long dimension equal to 640. All the experiments are performed on single RTX 3090 GPU for accuracy and runtime tests. For all baselines, we employ the same RANSAC [11] settings as robust homography or pose estimator for fair comparison. Baselines. Following [47], we select representative methods from the matching pipelines of sparse, semi-dense, and dense matching. 1) For spare keypoint detection and matching methods, we choose SuperGlue [35], LightGlue (LG) [28], OmniGlue [20], and LG-based GIM [40] for comparison. All of them (including our MINIMALG) use SuperPoint as the keypoint detector (the maximum number of extracted keypoints is set as 2048). We also take ReDFeat [7] into account as it is deep method In addition, designed for multimodal image matching. three handcrafted multimodal matching methods, including RIFT [23], SRIT [25], LNIFT [24], are also used. However, we only test them (including OmniGlue) on the real RGB-IR dataset due to their poor accuracy and huge time costs. 2) Semi-dense matching methods includes LoFTR [41], ELoFTR [47], XoFTR [43], and GIMLoFTR, where XoFTR is tailored for RGB-IR image matching. 3) As for dense matching, DKM [9], GIMDKM [40] and recent SOTA matcher RoMa [10] are used for comparison. 5.2. Evaluate on Our MD-syn We first test the matching methods on MD-syn, multimodal image matching dataset synthesized by our data engine. Tab. 2 reports the qualitative results. It shows that our MINIMA can largely enhance the cross-modal ability of the baselines. However, we achieve weak advantages for RGBSketch and RGB-Paint since these two artistic modalities are more similar to RGB. As the table revealed, GIM shows poor generalization for multimodal cases, since it is overfitted on RGB videos. ReDFeat performs not well on new scenes and even fails in the event case. As for the LoFTR series, the original LoFTR and ELoFTR are worse than SuperGlue and LG. Because Edge or shape information is more Table 2. Full Results on Our Synthetic Dataset. The AUC of the pose error in percentage is reported. The best and second of each category are masked as Bold and Underline, respectively. Category Method RGB-IR RGB-Depth RGB-Normal RGB-Event RGB-Sketch RGB-Paint @5 @10 @20 @5 @10 @20 @5 @10 @20 @5 @10 @20 @5 @10 @20 @5 @10 @20 Sparse SuperGlue [35] 7.49 17.51 33.54 3.06 6.94 13.70 11.53 24.42 41.85 10.38 23.48 41.63 21.52 37.99 56.17 11.35 24.15 42.51 LightGlue (LG) [28] 7.64 17.73 32.86 1.19 2.87 6.42 12.32 24.93 41.86 10.11 22.40 39.33 26.77 44.47 62.00 13.93 27.99 46.16 2.75 8.56 20.90 2.20 6.36 15.25 2.56 7.25 17.79 0.00 0.00 0.00 5.26 13.91 29.01 2.73 7.32 17.83 ReDFeat [7] 8.40 18.88 33.20 0.00 0.00 0.12 12.03 23.93 38.53 6.75 14.19 23.81 28.80 46.82 63.94 13.18 26.84 43.45 GIMLG [40] 14.74 30.24 49.22 16.19 32.53 51.76 20.47 37.33 56.17 19.00 36.27 54.97 27.51 45.71 63.77 16.39 32.85 51.65 MINIMALG Semi-Dense Dense LoFTR [41] XoFTR [43] ELoFTR [47] GIMLoFTR [40] MINIMALoFTR DKM [9] GIMDKM [40] RoMa [10] MINIMARoMa 5.44 12.58 24.28 0.13 0.44 1.88 5.72 12.07 23.14 4.90 12.43 26.45 37.81 54.82 69.52 5.93 12.22 22.19 17.85 32.21 49.53 12.82 23.10 36.02 22.74 38.35 54.71 33.33 51.61 67.49 44.18 61.39 75.07 3.73 7.54 14.48 6.73 14.59 27.36 0.25 0.79 3.32 11.20 21.67 36.86 9.25 20.39 37.56 43.86 61.09 74.84 14.09 25.11 39.44 2.60 6.79 15.50 0.00 0.04 0.27 0.35 1.06 4.01 0.44 1.43 5.28 17.30 31.82 48.79 4.84 10.64 21.82 18.07 32.36 48.42 14.70 28.81 46.23 27.65 44.26 59.88 18.14 32.74 49.11 36.07 53.54 68.47 7.79 15.45 27.39 15.68 29.46 46.11 0.10 0.38 1.92 23.23 39.28 55.22 10.18 18.14 27.78 56.91 72.25 83.31 29.64 44.73 58.57 11.23 22.72 37.93 1.42 4.07 10.86 14.09 25.81 40.55 22.86 38.30 53.58 50.89 67.12 79.02 28.22 43.49 58.06 20.27 35.99 54.02 10.21 22.75 39.43 40.99 59.48 74.19 40.86 58.87 73.35 58.49 73.90 84.80 41.30 58.36 72.70 24.33 40.94 58.33 29.56 48.58 65.87 47.10 64.48 77.90 43.83 61.48 75.21 59.17 74.30 84.86 40.09 57.21 71.96 Table 3. Evaluation on Real RGB-IR Dataset (METU-VisTIR) [43] with Pose Estimation. The AUC of the pose error in percentage is reported. The average runtime is listed in the last column. Table 4. Evaluation on Real RGB-Depth Dataset (DIODE) [44] with Homography Estimation. The AUC of the projective error in percentage is reported. Category Method Pose estimation AUC @5 @10 @20 Time (ms) Category Method Homo. estimation AUC @3px @5px @10px 0.05 0.27 RIFT [23](TIP 19) 0.00 0.08 SRIT [25](ISPRS 23) 0.02 0.09 LNIFT [24](TGRS 22) 4.30 9.26 SuperGlue [35](CVPR 20) ReDFeat [7](TIP 23) 1.71 4.57 LightGlue (LG) [28](ICCV 23) 2.17 5.37 2.43 5.85 GIMLG [40](ICLR 24) OmniGlue [20](CVPR 24) 1.48 4.13 MINIMALG 19.14 37.17 LoFTR [41](CVPR 21) GIMLoFTR [40](ICLR 24) ELoFTR [47](CVPR 24) XoFTR [43](CVPR 24) MINIMALoFTR DKM [9](CVPR 23) GIMDKM [40](ICLR 24) RoMa [10](CVPR 24) MINIMARoMa 2.88 6.94 0.43 1.06 2.88 7.88 18.47 34.64 15.61 30.84 6.76 13.69 5.08 12.30 25.61 48.12 37.45 60.70 13k 0.90 1.9k 0.37 1.2k 0.43 17.21 86.1 10.85 235.8 57.7 11.21 42.9 10.58 3k 10.11 58.6 55.51 14.95 2.99 17.72 51.50 47.87 61.6 69.5 46.6 62.7 71.6 22.53 485.3 23.69 792.2 68.37 639.1 78.00 633. Sparse Semi-Dense Dense crucial for multimodal image matching, it is difficult for semi-dense methods to build matches among textureless areas. XoFTR achieves competitive results, as it is pre-trained on sufficient multi-spectral image pairs and equipped with many advanced designs. As for dense matching, DKM and GIMDKM perform poorly on four cross-modal cases due to the huge modality gaps among them. While original RoMa exhibits good generalization, mainly because of the use of DINOv2 [33] that has seen numerous types of images during pre-training. Our MINIMA still obtains significant enhancement over RoMa. Moreover, we also evalSparse Semi-Dense Dense SuperGlue [35](CVPR 20) ReDFeat [7](TIP 23) LightGlue (LG) [28](ICCV 23) GIMLG [40](ICLR 24) MINIMALG LoFTR [41](CVPR 21) GIMLoFTR [40](ICLR 24) ELoFTR [47](CVPR 24) XoFTR [43](CVPR 24) MINIMALoFTR DKM [9](CVPR 23) GIMDKM [40](ICLR 24) RoMa [10](CVPR 24) MINIMARoMa 1.77 1.01 0.79 0.30 8.71 0.97 0.00 0.82 11.03 5. 1.29 1.90 9.21 28.98 6.83 4.58 3.30 1.14 26.80 4.20 0.25 4.09 27.24 18.65 4.23 6.34 24.64 50.88 21.15 16.30 11.26 3.65 55.97 15.16 1.15 16.69 51.60 44. 11.78 17.96 49.31 72.54 uate those methods back to the original Megadepth-1500 (Refer to Sec. C.3 Tab. A5 for Details). 5.3. In Domain Image Matching We next conduct in-domain tests, i.e., training on synthetic data but testing on real data of the same modality. Two real datasets are used, including RGB-IR (METU-VisTIR [43]) for pose estimation and RGB-Depth (DIODE [44]) for homography estimation. The results are in Tab. 3 and Tab. 4. For the RGB-IR test, our MINIMALG enhances sparse matching, with AUC increasing over 400%. Mostly, it even beats the SOTA semi-dense method XoFTR. As for semi-dense matching, XoFTR achieves the best performance. This is attributed to its pre-training on sufficient 7 Table 5. Zero-shot Matching on Real Dataset with Homography Estimation. The AUC of the corner error in percentage is reported. The best and second of each category are masked as Bold and Underline, respectively. Category Method Medical Remote Sensing RGB-Event @3px @5px @10px @3px @5px @10px @3px @5px @10px Sparse Semi-Dense Dense SuperGlue [35] LightGlue (LG) [28] ReDFeat [7] GIMLG [40] MINIMALG LoFTR [41] XoFTR [43] ELoFTR [47] GIMLoFTR [40] MINIMALoFTR DKM [9] GIMDKM [40] RoMa [10] MINIMARoMa 30.72 35.47 38.55 24.32 37.95 38.42 39.67 34.57 39.51 39.67 39.43 37.78 39.62 39.17 36.18 42.37 44.26 27.88 44. 43.89 45.60 41.66 44.40 45.33 45.00 43.46 45.13 45.92 44.66 49.48 50.93 33.84 52.50 50.13 52.32 49.08 48.94 52.77 51.78 48.87 53.75 57.55 18.34 16.22 15.99 11.09 23. 24.13 27.35 16.45 17.96 23.32 26.44 21.19 29.24 32.55 27.47 27.51 23.95 17.44 38.40 33.80 39.58 29.65 27.41 35.18 35.82 30.28 40.50 44.68 45.59 44.62 43.72 27.18 58. 50.79 56.63 46.74 37.29 56.81 51.20 47.68 57.84 64.38 0.00 0.00 0.55 0.57 0.52 0.00 0.00 0.64 0.00 0.81 0.00 0.00 0.85 0.54 0.67 0.67 0.97 1.08 2. 0.00 1.37 1.34 0.55 2.49 0.00 0.66 1.69 3.51 8.00 7.02 6.07 5.54 12.82 3.59 12.64 7.78 1.19 11.75 0.00 7.04 10.71 17.07 multi-spectral image pairs, the use of an effective data augmentation strategy, and specific designs incorporated in both the training and matching stages. In dense matching, our MINIMA combined with RoMa outperforms all other pipelines consistently with large margins. The runtime of each method is also listed. The results show that sparse and semi-dense methods (except for handcrafted methods, ReDFeat, and OmniGlue) are often more efficient due to their fewer points to match. ELoFTR is faster than the sparse methods due to its efficient designs. This trend is consistent with existing works [10, 47]. The same trends are obtained in the RGB-Depth matching as in Tab. 4. To be specific, our semi-dense method is worse than XoFTR. That is because depth data is more challenging, and our MINIMA is based on LoFTR, representative but old model without any fancy designs. But we largely enhance the original LoFTR from 4.2 to 18.65 @5 px. The overall performance on all real cross-modal data is concluded in Fig. 1, which reveals the promising generalization of our MINIMA. 5.4. Zero-shot Matching for Unseen Modality We next extend to zero-shot matching. 1) Medical tasks consist of 6 modality pairs such as Retina, CT-SPECT, etc. 2) Remote Sensing tasks consist of 7 cases such as Optical-SAR, Optical-Map, etc. 3) RGB-Event case is from DSEC [46]. 1) and 2) are both from MMIM [21] datasets. The quantitative results are outlined in Tab. 5. For medical scenes, almost all the methods have close accuracy since the datasets are either too easy or too difficult. But our MINIMALG still exhibits few advantages. As for remote sensing cases, our MINIMA achieves large gains for sparse and dense matching. While the semi-dense matcher MINIMALoFTR falls behind XoFTR for the same reason. As for the RGB-Event matching, the task is extremely chalFigure 5. Training Loss and AUC@5 w.r.t. Epochs, using Scratch Training and Fine-tuning. The basic model is LoFTR. The test set is our synthetic RGB-IR of MD-syn. lenging due to the large modality gap. Despite this, our proposed MINIMA performs good capacity of matching them. Some qualitative results are shown in Fig. 2, and more results are in Sec. C.5, which demonstrate that our MINIMA can establish high number and ratio of correct matches for real cross-modal image pairs. 5.5. Scratch Training v.s. Fine-tuning We report the loss values and AUC@5 performance with respect to epochs, by using scratch training and fine-tuning. The test set is our synthetic RGB-IR data generated from MegaDepth-1500 [26]. We use LoFTR as the basic model, and the training set is our synthetic RGB-IR/Depth/Nomal. Statistic results are shown in Fig. 5, which reveal that the fine-tuning strategy can converge more rapidly since the pre-trained model can provide good matching priors for challenging cross-modal tasks. 5.6. Ablation Studies In this part, we conduct ablation studies to analyze the superiority of our MINIMA. The results on synthetic RGBIR, real RGB-IR, and real RGB-Depth data are reported in Tab. 6. We use LoFTR (LT) as the basic model, which 8 Table 6. Ablation Studies. Test on Synthetic RGB-IR, Real RGBIR, and Real RGB-Depth data, with different training settings. Training Strategy Syn RGB-IR Rel RGB-IR Rel RGB-D AUC@10 AUC@10px AUC@10 Basic Model: LoFTR (LT) [41] (1) Train from scratch on syn IR (2) LT + real IR (3) LT + syn IR (4) LT + syn Depth (5) LT + syn IR/Depth/Normal 12.58 23.63 6.28 29.43 17.30 32.36 6. 21.41 9.78 29.55 15.12 30.84 15.16 30.04 32.93 39.23 36.06 44.85 serves as the pre-trained model for (2)-(5). (1) Directly train LT on synthetic RGB-IR from scratch. (2) Fine-tune LT on real RGB-IR datasets (LLVIP and M3FD). (3) Fine-tune LT only with our synthetic RGB-IR. (4) Fine-tune LT only with our synthetic RGB-Depth. (5) Fine-tune LT on mixed data of our synthetic RGB-IR, RGB-Depth, and RGB-Normal. The results of (1) and (3) reveal that training from scratch is worse than fine-tuning. (2) and (3) demonstrate the advantages of our synthetic data against real datasets. (4) reveals that only fine-tuning on synthetic RGB-Depth can well generalize to other cross-modal cases, even better than test (2) on real RGB-IR data. (5) and (3) reveal that different synthetic data can cooperate for better performance. Our full model can largely enhance the generalization ability. More combinations of training datasets are evaluated in Tab. A2. 5.7. Discussion on Possible Limitations Our objective is to generate pseudo modalities to form large multimodal dataset, which would produce two possible limitations: i) The gap between real and pseudo modality. ii) The fake information during generation. Fortunately, these two possible limitations have little impact on our task. First, multimodal images intrinsically vary in pixel intensity distributions [51]. This property is well exhibited in our generated modalities (see Fig. 4), which plays an important role in training general matching model. Existing diffusion-based methods [15, 16] can generate high-quality images of the target modality, making the pseudo modality much closer to the real. Extensive experiments verify the high quality of our generated data. As for the generated fake information, it can well imitate the multimodal cases, e.g. the target is visible in infrared but not in RGB, which may help to enhance the robustness of the trained model. 6. Conclusion This paper presents unified matching framework, named MINIMA, for any cross-modal cases. It is achieved by filling the data gap using an effective data engine that freely scales up cheap RGB data into large multimodal one. The constructed MD-syn dataset contains rich scenarios and precise match labels, and supports the training of any advanced matching models, significantly improving cross-modal performance and zero-shot ability in unseen cross-modal cases."
        },
        {
            "title": "References",
            "content": "[1] NG Aditya, PB Dhruval, Jehan Shalabi, Shubhankar Jape, Xueji Wang, and Zubin Jacob. Thermal voyager: comparative study of rgb and thermal cameras for night-time auIn Proc. of the IEEE Int. Conf. on tonomous navigation. Robotics and Automation, pages 1411614122. IEEE, 2024. 1 [2] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly In Proc. of IEEE Intl. Conf. supervised place recognition. on Computer Vision and Pattern Recognition, pages 5297 5307, 2016. 15 [3] Gwangbin Bae and Andrew J. Davison. Rethinking inductive biases for surface normal estimation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024. 5 [4] Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to match features with seeded graph matching network. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 63016310, 2021. 14, 15 [5] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie10ner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 58285839, 2017. 1, 3, 4 [6] Xin Deng, Enpeng Liu, Chao Gao, Shengxi Li, Shuhang Gu, and Mai Xu. Crosshomo: Cross-modality and crossIEEE Transactions on resolution homography estimation. Pattern Analysis and Machine Intelligence, 2024. 2, 4 [7] Yuxin Deng and Jiayi Ma. Redfeat: Recoupling detection and description for multimodal feature learning. IEEE Transactions on Image Processing, 32:591602, 2022. 2, 3, 4, 6, 7, 8, 15 [8] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition Workshop, pages 224236, 2018. 2, 12, [9] Johan Edstedt, Ioannis Athanasiadis, Marten Wadenback, and Michael Felsberg. Dkm: Dense kernelized feature In Proc. of IEEE Intl. matching for geometry estimation. Conf. on Computer Vision and Pattern Recognition, pages 1776517775, 2023. 1, 3, 5, 6, 7, 8, 14 [10] Johan Edstedt, Qiyu Sun, Georg Bokman, Marten Wadenback, and Michael Felsberg. Roma: Robust dense feature matching. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1979019800, 2024. 1, 3, 5, 6, 7, 8, 13, 14 [11] Martin Fischler and Robert Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381395, 1981. 6 [12] Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Jorg Conradt, Kostas Daniilidis, et al. 9 Event-based vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):154180, 2020. 5 [13] Daniel Gehrig and Davide Scaramuzza. Low-latency automotive vision with event cameras. Nature, 629(8014):1034 1040, 2024. [14] Daniel Gehrig, Mathias Gehrig, Javier Hidalgo-Carrio, and Davide Scaramuzza. Video to events: Recycling video In Proc. of IEEE Intl. Conf. datasets for event cameras. on Computer Vision and Pattern Recognition, pages 3586 3595, 2020. 5 [15] Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, and Jingfeng Zhang. Stylebooth: Image style editing with multimodal instruction. arXiv preprint arXiv:2404.12154, 2024. 3, 5, 9, 12 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proc. of Advances in Neural Information Processing Systems, pages 68406851, 2020. 3, 9 [17] Zhuolu Hou, Yuxuan Liu, and Li Zhang. Pos-gift: geometric and intensity-invariant feature transformation for multimodal images. Information Fusion, 102:102027, 2024. 3 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In Proc. of Intl. Conf. on Learning Representations, 2022. 5 [19] Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli Zhou. Llvip: visible-infrared paired dataset for low-light In Porc. of IEEE Intl. Conf. on Computer Vision, vision. pages 34963504, 2021. 1, 2, 3, 5, 12 [20] Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, and Andre Araujo. Omniglue: Generalizable feature matching with foundation model guidance. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1986519875, 2024. 2, 6, 7, [21] Xingyu Jiang, Jiayi Ma, Guobao Xiao, Zhenfeng Shao, and Xiaojie Guo. review of multimodal image matching: Information Fusion, 73:2271, Methods and applications. 2021. 1, 2, 3, 5, 6, 8, 12, 15, 16 [22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. In Proc. of Advances in Neural Information Processing Systems, 2012. 12 [23] Jiayuan Li, Qingwu Hu, and Mingyao Ai. Rift: Multi-modal image matching based on radiation-variation insensitive feature transform. IEEE Transactions on Image Processing, 29: 32963310, 2019. 3, 6, 7 [24] Jiayuan Li, Wangyi Xu, Pengcheng Shi, Yongjun Zhang, and Qingwu Hu. Lnift: Locally normalized image for rotation invariant multimodal feature matching. IEEE Transactions on Geoscience and Remote Sensing, 60:114, 2022. 6, 7 [25] Jiayuan Li, Qingwu Hu, and Yongjun Zhang. Multimodal image matching: scale-invariant algorithm and an open dataset. ISPRS Photogramm, 204:7788, 2023. 6, 7 [26] Zhengqi Li and Noah Snavely. Megadepth: Learning singleIn Proc. of view depth prediction from internet photos. IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 20412050, 2018. 1, 3, 4, 5, 8, 14 [27] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck. 128 120db 15µs latency asynchronous temporal con128 trast vision sensor. IEEE Journal of Solid-State Circuits, 43 (2):566576, 2008. 5 [28] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. Lightglue: Local feature matching at light speed. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 17627 17638, 2023. 2, 3, 5, 6, 7, 8, 12, 14, 15 [29] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong, and Zhongxuan Luo. Target-aware dual adversarial learning and multi-scenario multi-modality benchmark to fuse infrared and visible for object detection. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 58025811, 2022. 2, 3, 5, 12 [30] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Ruifeng Deng, Xin Li, Errui Ding, and Hao Wang. Paint transformer: Feed forward neural painting with stroke prediction. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 65986607, 2021. 5 [31] Yuyan Liu, Wei He, and Hongyan Zhang. Grid: Guided refinement for detector-free multimodal image matching. IEEE Transactions on Image Processing, 2024. 3 [32] Jiayi Ma, Xingyu Jiang, Aoxiang Fan, Junjun Jiang, and Junchi Yan. Image matching from handcrafted to deep features: survey. International Journal of Computer Vision, 129(1):2379, 2021. 1, 2, 3 [33] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [34] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 12716 12725, 2019. 14 [35] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 49384947, 2020. 1, 2, 3, 6, 7, 8, 14, 15 [36] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, et al. Benchmarking 6dof outdoor visual localization in changing conditions. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 86018610, 2018. 14 [37] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2016. 3, 4, 14 [38] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In Proc. of European Conference on Computer Vision, 2016. 3, 4, 14 [39] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, 2020. Version 0.3.0. 12 [40] Xuelun Shen, Zhipeng Cai, Wei Yin, Matthias Muller, Zijun Li, Kaixuan Wang, Xiaozhi Chen, and Cheng Wang. Gim: Learning generalizable image matcher from internet videos. In Proc. of Intl. Conf. on Learning Representations, 2024. 3, 4, 5, 6, 7, 8, 14 [41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 89228931, 2021. 1, 3, 6, 7, 8, 9, 13, 14 [42] Linfeng Tang, Jiteng Yuan, Hao Zhang, Xingyu Jiang, and Jiayi Ma. Piafusion: progressive infrared and visible image fusion network based on illumination aware. Information Fusion, 2022. 12 Onder Tuzcuoglu, Aybora Koksal, Bugra Sofu, Sinan Kalkan, and Aydin Alatan. Xoftr: Cross-modal feature matching transformer. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 42754286, 2024. 1, 2, 3, 6, 7, 8, 12, 13, 14, 15 [43] [44] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. 6, 7, 15 [45] Di Wang, Jinyuan Liu, Xin Fan, and Risheng Liu. Unsupervised misaligned infrared and visible image fusion via cross-modality image generation and registration. In Proc. of Intl. Joint Conf. on Artificial Intelligence, pages 35083515, 2022. [46] Xiao Wang, Jianing Li, Lin Zhu, Zhipeng Zhang, Zhe Chen, Xin Li, Yaowei Wang, Yonghong Tian, and Feng Wu. Visevent: Reliable object tracking via collaboration of frame and event flows. IEEE Transactions on Cybernetics, 2023. 6, 8, 15, 16 [47] Yifan Wang, Xingyi He, Sida Peng, Dongli Tan, and Xiaowei Zhou. Efficient loftr: Semi-dense local feature matching with sparse-like speed. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 2166621675, 2024. 1, 3, 5, 6, 7, 8, 13, 14 [48] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. 12 [49] Zongwei Wu, Jilai Zheng, Xiangxuan Ren, Florin-Alexandru Vasluianu, Chao Ma, Danda Pani Paudel, Luc Van Gool, and Radu Timofte. Single-model and any-modality for video object tracking. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1915619166, 2024. 1 [50] Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, Xiaohui Shen, and Jan Allebach. Adversarial open domain adaptation for sketch-to-photo synthesis. In Proc. of IEEE Winter Conf. on Applications of Computer Vision, pages 14341444, 2022. 5 [51] Han Xu, Jiteng Yuan, and Jiayi Ma. Murf: Mutually reinforcing multi-modal image registration and fusion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):1214812166, 2023. 1, 9 [52] Bin Yang, Jun Chen, and Mang Ye. Towards grand unified representation learning for unsupervised visible-infrared person re-identification. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 1106911079, 2023. 1 [53] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: UnIn Proc. leashing the power of large-scale unlabeled data. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 2, 3 [54] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In Proc. of Advances in Neural Information Processing Systems, 2024. 3, 5 [55] Yongxiang Yao, Yongjun Zhang, Yi Wan, Xinyi Liu, Xiaohu Yan, and Jiayuan Li. Multi-modal remote sensing image IEEE Transacmatching considering co-occurrence filter. tions on Image Processing, 31:25842597, 2022. 3 [56] Yuanxin Ye, Lorenzo Bruzzone, Jie Shan, Francesca Bovolo, and Qing Zhu. Fast and robust matching for multimodal remote sensing image registration. IEEE Transactions on Geoscience and Remote Sensing, 57(11):90599070, 2019. 3 [57] Hao Zhang, Han Xu, Xin Tian, Junjun Jiang, and Jiayi Ma. Image fusion meets deep learning: survey and perspective. Information Fusion, 76:323336, 2021. 1 [58] Kaining Zhang and Jiayi Ma. Sparse-to-dense multimodal image registration via multi-task learning. In Proc. of Intl. Conf. on Machine Learning, 2024. 2, [59] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 586595, 2018. 12 [60] Shihua Zhang and Jiayi Ma. Convmatch: Rethinking netIEEE work design for two-view correspondence learning. Transactions on Pattern Analysis and Machine Intelligence, 2023. 14, 15 [61] Yukang Zhang and Hanzi Wang. Diverse embedding expansion network and low-light cross-modality benchmark for In Proc. of IEEE visible-infrared person re-identification. Intl. Conf. on Computer Vision and Pattern Recognition, pages 21532162, 2023. 1 [62] Yuxiang Zhang, Yang Zhao, Yanni Dong, and Bo Du. Selfsupervised pretraining via multimodality images with transIEEE Transactions on Geoformer for change detection. science and Remote Sensing, 61:111, 2023. 1 [63] Kaichen Zhou, Changhao Chen, Bing Wang, Muhamad Risqi Saputra, Niki Trigoni, and Andrew Markham. Vmloc: Variational fusion for learning-based multimodal camera localization. In Proc. of the AAAI Conf. on Artificial Intelligence, pages 61656173, 2021. 1 [64] Jiawen Zhu, Simiao Lai, Xin Chen, Dong Wang, and Huchuan Lu. Visual prompt multi-modal tracking. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 95169526, 2023."
        },
        {
            "title": "Appendices",
            "content": "We first provide more details of our data engine and the proposed MINIMA. Then we conduct additional experiments, including more ablation studies, more quantitative and qualitative matching results, and applying our MINIMA to the Visual Localization. A. Details of Our Data Generation A.1. Quality Verification of Modality Generation The generation models for several modalities, excluding infrared (IR), have achieved significant success in their respective domains. Therefore, we additionally evaluate the quality of our infrared generation model. We use diffusion-based method [15] for fine-tuning due to its significant performance in style transfer. The fine-tuning process utilizes 80% real RGB-IR pairs from LLVIP [19] and M3FD [29], while the rest 20% is used for the test. These two datasets provide over 10,000 real RGB-IR image pairs, which are fully aligned by manual labeling. In addition to LLVIP and M3FD, Evaluation Protocol. we additionally evaluate the generation performance on the MSRS dataset [42] by randomly selecting 120 RGBIR pairs. Specifically, we generate the pseudo-IR image for one RGB and then compare it with the corresponding real IR image. For comparison, we adopt XoFTR (CVPR 24) [43] and CPSTN (IJCAI 22) [45] as baseline methods. XoFTR used handcrafted method to transfer RGB to IR, while CPSTN is cycle-consistent perceptual network. We employ quantitative metrics including PSNR (Peak Signalto-Noise Ratio), SSIM (Structural Similarity Index Measure) [48], LPIPS (Learned Perceptual Image Patch Similarity) [59] with AlexNet [22], and PyTorch FID (Frechet Inception Distance) [39]. As for FID, we compute the dimensionality of features with sizes 2048 to serve as an evaluation indicator. The results are presented in Tab. A1 with visualizations provided in Fig. A1 and Fig. A2 Results Analysis. From both qualitative and quantitative results, we find our infrared generation achieves huge improvements. Specifically, our generated infrared images are closer to the real sensors, and the contents are clear and even better than ground truths. In addition, almost all the metrics demonstrate superiority to others by large margins. The promising performance helps lot for our data engine to generate high-quality cross-modal image pairs. It is also critical in training unified matching model, making our MINIMA obtain high generalization ability. A.2. Data Cleaning It is necessary to clean up the synthetic data to reduce the impacts of abnormal ones since we can not ensure the quality of the generated images. To this end, and for each RGB Figure A1. Visualization Results of Infrared generation on MSRS. The first two columns are real RGB and Infrared images. Figure A2. Visualization Results of Infrared Generation on M3FD. The first two columns are real RGB and Infrared images. Table A1. Quantitative Evaluation of Infrared Generation with Different Metrics. The test datasets are LLVIP [19], M3FD [29] and MSRS [42]. CPSTN (IJCAI 22) [45] and XoFTR (CVPR 24) [43] are used for comparison. Bold indicates the best. Data Method PSNR SSIM LPIPS FID-2048 LLVIP M3FD MSRS CPSTN XoFTR Ours CPSTN XoFTR Ours CPSTN XoFTR Ours 27.91 27.90 28.28 27.82 27.86 28.14 27.95 27.84 27.87 0.32 0.29 0. 0.37 0.33 0.53 0.15 0.16 0.19 0.66 0.71 0.42 0.56 0.59 0.46 0.74 0.77 0.73 303.55 204.44 145. 161.71 125.07 119.96 204.37 167.39 161.37 image and its corresponding pseudo modalities, we use our matching model (fine-tuned on the target modality) to recover the homographies (the ground truths are the identity matrix) for them. Any image pair with the mean projection error of corner points larger than 10 pixels is regarded as dirty data and dropped. Finally, 0.91% of the matching pairs are dropped in the training set. B. Details of MINIMA The details of our fine-tuning stage are as follows. i) LightGlue (LG) [28]. We use SuperPoint [8] to extract 2048 keypoints and freeze its parameters, then only fine-tune LightGlue. Because SuperPoint is verified to extract matchable features for cross-modal images [21]. Just fine-tuning Table A2. Ablation Studies with Different Training Data. The basic models are LG, LoFTR, and RoMa. The training sets are different combinations of our generated cross-modal data. We evaluated the fine-tuned models on real cross-modal cases. For each baseline, the model trained on the original MegaDepth is reported in the first row. The average performance is shown in the last column. Models Generated Modalities Rel IR Rel Depth Rel Event RS Medical Infrared Depth Normal Event Paint Sketch AUC@10 AUC@10px AUC@10px AUC@10px AUC@10px Average MINIMALG MINIMALoFTR MINIMARoMa 5.37 35.55 30.54 32.66 23.33 37.17 36.34 6.94 29.55 15.12 23.14 14.96 30.84 30.80 30.61 48.12 57.28 60.42 60.36 59.11 58.89 60.70 61.27 60.43 11.26 47.27 51.78 48.66 38.37 55.97 55.93 15.16 39.23 36.06 39.79 32.97 44.85 48.55 45. 49.31 57.49 72.63 72.51 69.11 72.88 72.54 73.80 72.83 7.02 13.39 12.08 10.44 10.01 12.82 12.74 5.91 11.12 5.32 10.28 12.19 11.38 12.44 11.83 10.71 10.49 11.00 10.89 11.71 12.36 17.07 11.02 12.98 44.62 55.12 57.73 58.15 55.72 58.74 58.41 50.79 48.79 53.64 54.73 45.77 56.81 56.04 55. 57.84 60.37 62.95 63.23 64.30 63.91 64.38 65.01 64.80 49.48 52.73 52.17 52.43 51.32 52.50 52.45 50.13 51.71 52.40 52.53 50.28 52.77 51.82 52.19 53.75 57.08 56.72 55.26 57.75 55.50 55.09 55.04 57.92 23.55 40.81 40.86 40.47 35.75 43.44 43.17 25.79 36.08 32.51 36.09 31.24 39.33 39.93 39. 43.95 48.54 52.74 52.45 52.40 52.71 53.96 53.23 53.79 LightGlue can achieve promising performance, as demonstrated by our MINIMALG. Here we directly adopted the initial learning rate, i.e., 1 104, in the fine-tuning stage. In practice, we fine-tune the LG model for 50 epochs as the authors suggested, which also shows converged performance in our study. ii) LoFTR [41] and RoMa [10]. We lower the learning rate to the 1/10 of the original, with the linear scaling rule to account for batch size differences. Specifically, the initial learning rate is set as 1 104 for LoFTR. And we set it as 1.5 105 for the RoMa decoder, and 7.5 107 for the RoMa encoder. Note that we maintain the default learning rate decay strategies for all methods during the fine-tuning. For LoFTR, we finetune for 30 epochs. In contrast, we fine-tune RoMa for only 4 epochs due to its inherent capabilities, which have already achieved amazing gains. For better understanding, we also fine-tune ELoFTR [47] and XoFTR [43], denoted as MINIMAELoFTR and MINIMAXoFTR. And they are fine-tuned for 20 epochs and 5 epochs, respectively. Their learning rates are similar to our MINIMALoFTR. The corresponding results are in Tab. A3, Tab. A4 and Tab. A5. C. Additional Experimental Results modal data. The obtained models are evaluated on different real scenes, and the results are reported in Tab. A2. For each baseline, we first report the AUCs of the official models (without any fine-tuning) in the first row. Then we finetune each model on single type of modality pair (RGB+X), which shows large enhancements compared with the basic models. Finally, we fine-tune the models on two or more modality types. The results demonstrate that different modalities can cooperate to train better model. Using RGB-IR/Depth/Normal can achieve the best overall performance, hence we use them as our final models. Additionally, using artistic data (Paint and Sketch) can not further enhance the performance because the artistic type has no physical property and is different from other modality types. C.2. More Results of Semi-dense Matching For better understanding of our MINIMA, we further fine-tune ELoFTR [47] and XoFTR [43] on the generated data, obtaining MINIMAELoFTR and MINIMAXoFTR. The corresponding results of semi-dense matching are reported in Tab. A3 and Tab. A4. The results reveal that with better pipelines, our MINIMA can achieve further enhancements of overall performance. C.1. More Studies on Different Training Data C.3. Results on Original MegaDepth Dataset To better understand our MINIMA, we fine-tune the basic models on different combinations of our generated crossIn this part, we will evaluate the performance degradation on RGB-only matching tasks for those cross-modal match13 Table A3. Semi-dense Matching Results on Our Synthetic Dataset. The AUC of the pose error in percentage is reported. The best and second are masked as Bold and Underline, respectively. Category Method RGB-IR RGB-Depth RGB-Normal RGB-Event RGB-Sketch RGB-Paint @5 @10 @20 @5 @10 @20 @5 @10 @20 @5 @10 @20 @5 @10 @20 @5 @10 @20 Semi-Dense 5.44 12.58 24.28 0.13 0.44 1.88 5.72 12.07 23.14 4.90 12.43 26.45 37.81 54.82 69.52 5.93 12.22 22.19 LoFTR [41] 17.85 32.21 49.53 12.82 23.10 36.02 22.74 38.35 54.71 33.33 51.61 67.49 44.18 61.39 75.07 3.73 7.54 14.48 XoFTR [43] 3.32 11.20 21.67 36.86 9.25 20.39 37.56 43.86 61.09 74.84 14.09 25.11 39.44 6.73 14.59 27.36 0.25 0.79 ELoFTR [47] GIMLoFTR [40] 4.01 0.44 1.43 5.28 17.30 31.82 48.79 4.84 10.64 21.82 0.27 0.35 1.06 2.60 6.79 15.50 0.00 0.04 MINIMALoFTR 18.07 32.36 48.42 14.70 28.81 46.23 27.65 44.26 59.88 18.14 32.74 49.11 36.07 53.54 68.47 7.79 15.45 27.39 MINIMAXoFTR 18.97 34.36 51.72 24.47 40.90 58.36 30.47 47.90 64.64 31.14 49.39 65.71 42.91 60.77 75.00 5.61 11.56 20.95 MINIMAELoFTR 13.14 26.36 43.63 16.59 32.26 50.37 29.72 47.47 63.72 15.66 30.72 48.73 41.64 59.63 73.73 15.02 27.02 41.62 Table A4. Semi-dense Matching Results on Real Dataset. The AUC of the pose error in percentage is reported. The best and second are masked as Bold and Underline, respectively. Category Method Real RGB-IR Real RGB-Depth Medical Remote Sensing Real RGB-Event @5 @10 @20 @3px @5px @10px @3px @5px @10px @3px @5px @10px @3px @5px @10px Semi-Dense 6.94 2.88 18.47 34.64 7.88 2.88 1.06 0.43 4.20 0.97 14.95 LoFTR [41] XoFTR [43] 11.03 27.24 51.5 4.09 0.82 17.72 ELoFTR [47] 0.25 0.00 GIMLoFTR [40] 2.99 MINIMALoFTR 15.61 30.84 47.87 18.65 5.35 MINIMAXoFTR 19.38 35.82 52.94 11.76 29.48 16.42 MINIMAELoFTR 12.11 28.07 47.25 3.96 15.16 51.60 16.69 1.15 44.85 55.05 44.03 38.42 43.89 39.67 45.60 34.57 41.66 39.51 44.40 39.67 45.33 39.33 44.92 39.12 44. 50.13 52.32 49.08 48.94 52.77 52.09 52.12 24.13 33.80 27.35 39.58 16.45 29.65 17.96 27.41 23.32 35.18 25.19 37.86 19.70 33.78 50.79 56.63 46.74 37.29 56.81 54.36 53.83 0.00 0.00 0.64 0.00 0.81 0.00 0.37 0.00 1.37 1.34 0.55 2.49 1.92 1.04 3.59 12.64 7.78 1.19 11.75 15.23 9. Table A5. Evaluation on Original Megadepth-1500 for Pose Estimation. The AUC of the pose error in percentage is reported. This mainly demonstrates that our MINIMA can well preserve the RGB-only matching performance except when using LoFTR. Category Method Sparse SuperGlue [35](CVPR 20) LightGlue (LG) [28](ICCV 23) GIMLG [40](ICLR 24) MINIMALG Semi-Dense Dense LoFTR [41](CVPR 21) GIMLoFTR [40](ICLR 24) ELoFTR [47](CVPR 24) XoFTR [43](CVPR 24) MINIMALoFTR MINIMAELoFTR MINIMAXoFTR DKM [9](CVPR 23) GIMDKM [40](ICLR 24) RoMa [10](CVPR 24) MINIMARoMa Pose estimation AUC @5 @10 @20 49.7 49.9 41.3 47. 53.6 51.3 56.4 45.8 29.9 51.0 44.5 60.4 60.7 62.6 61.7 67.1 67.0 60.7 65.0 69.9 68.5 72.2 61.7 45.3 68.1 60.0 74.9 75.5 76.7 76.5 80.6 80.1 75.9 78. 82.0 81.1 83.5 74.0 59.5 80.3 72.3 85.1 85.9 86.3 86.4 ing methods. To this end, we test these methods back to the original MegaDepth-1500 [26]. We use the same settings as described in [28, 41]. Following previous testing, the RANSAC threshold is still set to 0.5. For semi-dense and dense methods, the longest edge of the input images is resized to 1200 pixels, while for sparse methods, it is resized Table A6. Visual Localization on Aachen Day-Night V1.0 [36] Method Day Night (0.25m,2) / (0.5m,5) / (5m,10) MNN SuperGlue [35](CVPR 20) SGMNet [4](ICCV 21) LightGlue (LG) [28](ICCV 23) ConvMatch [60](TPAMI 23) MINIMALG 86.9 / 92.0 / 95.5 87.9 / 95.0 / 97.9 86.5 / 93.7 / 97.2 88.0 / 93.8 / 97.5 88.1 / 94.4 / 97.3 88.3 / 94.7 / 98.3 73.5 / 79.6 / 88.8 84.7 / 92.9 / 99.0 82.7 / 91.8 / 99.0 84.7 / 91.8 / 99.0 79.6 / 88.8 / 96.9 85.7 / 92.9 / 100.0 to 1600 pixels. The results are summarized in Tab. A5, revealing that our MINIMA can well maintain the ability of RGB-only matching, except for LoFTR. C.4. Apply to Visual Localization Vision localization (VL) is critical downstream task of image matching. The target is to recover the 6degreeof-freedom (6DOF) camera pose from query image related to known 3D scene model. We perform it on the Aachen v1.0 dataset, which is challenging large-scale outdoor dataset for localization with large-viewpoint and daynight illumination changes, making the localization largely rely on the robustness of matching methods. We adopt its full localization track for benchmarking. Following [28, 34], we integrate different matching methods into the official HLoc pipeline [34] to achieve localization. Specifically, with COLMAP [37, 38] toolbox, we first triangulate 3D point cloud for all reference im14 Figure A3. Qualitative Results on Real RGB-IR Image Pairs of METU-VisTIR [43]. The red lines indicate false matches. Figure A4. Qualitative results on real RGB-Depth image pairs of DIODE dataset [44]. The red lines indicate false matches. ages with known poses and calibration, then retrieve 20 reference images for each query image with NetVLAD [2] on Aachen Day-Night v1.0. Then, we match the query image and the retrieved images with image matching methods, where the feature points are extracted up to 4096 by SuperPoint [8]. Finally, the camera poses are estimated by RANSAC and Perspective-n-Point solver. We report the pose recall at different scales of distance and angular thresholds, i.e., (0.25m,2) / (0.5m,5) / (5m,10). The sparse matchers, including SuperGlue [35], SGMNet [4], LightGlue (LG) [28], ConvMatch [60] and our MINIMA finetuned with LightGLue, are used for comparison. We also report the raw results of SuperPoint directly with Mutual Nearest Neighbor (MNN) matching. The localization results are summarized in Tab. A6, which demonstrates the good ability of our MINIMA for downstream applications. Since our MINIMA is additionally trained on high-quality multimodal image pairs, it can be more robust to complex scenarios. C.5. More Visible Results on Real Datasets We also show more qualitative results, which are selected from real RGB-IR [43], RGB-Depth [44], RGBEvent [46] and Remote Sensing [21] (including OpticalSAR, optical-Map, and Day-Night) datasets. For each pair, we show the raw matching results before RANSAC. The red lines indicate false matches whose epipolar error (pose) or projection error (homography) is beyond 5 104 and 3 pixels, respectively. Visible results are shown in Fig. A3, Fig. A4, Fig. A5 and Fig. A6. Our methods MINIMALG (sparse) and MINIMARoMa (dense) are compared with the sparse pipeline ReDFeat [7] and OmniGlue [20], and semi-dense matcher XoFTR [43]. ReDFeat and XoFTR are cross-modal methods, and OmniGlue is known for its generalization. The results reveal that our MINIMA can produce high number and ratio of correct matches (green lines). Figure A5. Qualitative results on real RGB-Event image pairs of DSEC dataset [46]. The red lines indicate false matches. Figure A6. Qualitative results on real image pairs of cross-modal remote sensing dataset [21]. The red lines indicate false matches."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "Wuhan University"
    ]
}