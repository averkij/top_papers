{
    "paper_title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
    "authors": [
        "Xinyu Fang",
        "Zhijian Chen",
        "Kai Lan",
        "Shengyuan Ding",
        "Yingji Liang",
        "Xiangyu Zhao",
        "Farong Wen",
        "Zicheng Zhang",
        "Guofeng Zhang",
        "Haodong Duan",
        "Kai Chen",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 8 7 4 4 1 . 3 0 5 2 : r Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLMs Xinyu Fang1,2*, Zhijian Chen3*, Kai Lan3, Lixin Ma3, Shengyuan Ding2,4, Yingji Liang5, Xiangyu Zhao2,6, Farong Wen6, Zicheng Zhang2,6, Guofeng Zhang1, Haodong Duan2, Kai Chen2, Dahua Lin2,7 Zhejiang University1 Shanghai AI Laboratory2 Tongji University3 Nanjing University4 East China Normal University Shanghai Jiaotong University6 The Chinese University of Hong Kong7 Figure 1. Our Motivation for Creation-MMBench. The triarchic theory of intelligence divides intelligence into three forms. Current MLLM benchmarks have significant gaps in evaluating visual-creative intelligence compared to the other forms. Additionally, existing benchmarks feature simple questions that fail to assess model performance in real-life creative tasks. Therefore, we proposed CreationMMBench, which includes four categories, more creative and discriminative questions, and better evaluation of visual creative intelligence."
        },
        {
            "title": "Abstract",
            "content": "Creativity is fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce CreationMMBench, multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in realworld, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLMs creative abilities. Creation-MMBench pro1Equal Contribution. 2Corresponding Author. vides valuable insights for advancing MLLM creativity and establishes foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/opencompass/Creation-MMBench. 1. Introduction Creativity is the ability to generate novel and appropriate solutions to complex problems across various contexts[1, 17]. With the rapid advancement of Large Language Models (LLMs), numerous benchmarks have been proposed to assess their capabilities across different dimensions of intelligence, including comprehension, reasoning, and creativity [12, 18, 21, 22, 25]. These benchmarks have significantly contributed to deeper understanding of LLM intelligence and have played crucial role in driving their improvement. Meanwhile, Multimodal Large Language Models (MLLMs) [2, 4, 14] have also benefited from advancements in LLMs, achieving notable progress in perception, reasoning, and other cognitive abilities [3, 16, 32]. As well-established theory in psychology, the Triarchic Theory of Intelligence [23] comprises three subtheories: the 1 analytical subtheory, the contextual subtheory, and the creative subtheory. The analytical subtheory primarily focuses on information processing and problem-solving skills based on domain-specific knowledge and can be assessed through various knowledge and reasoning benchmarks [10, 32]. The contextual subtheory, on the other hand, emphasizes practical intelligence in real-world scenarios and is typically evaluated using agent-based or embodied AI benchmarks [28, 33]. Despite the significance of the creative subtheory in intelligence, evaluations of MLLMs creative capabilities remain highly inadequate and lag significantly behind those conducted for LLMs [8, 18]. Moreover, constructing benchmarks to assess visual creativity presents inherent challenges. Cognitive science research suggests that creativity arises from distributed cortical network involving the coordination of multiple brain regions. As illustrated in Fig. 2, creativity is closely associated with functions of the frontal lobe, such as concentration, planning, and problem-solving [11]. Within the context of MLLM evaluation, assessing creative capabilities requires benchmarks that encompass broader range of fundamental cognitive abilities compared to those needed for other types of intelligence assessment [15, 31]. To address this significant gap, we introduce CreationMMBench, novel benchmark specifically designed to assess the creative capabilities of MLLMs in image-based tasks across authentic real-world scenarios. The benchmark consists of 765 test cases spanning 51 fine-grained tasks, which are categorized into four major groups: Literary Writing, Common Functional Writing, Professional Functional Writing, and Creative Multimodal Understanding. Additionally, the benchmark is accompanied by rich context to facilitate comprehensive evaluation. In each task, an MLLM is provided with one or more images along with detailed context specifying the assigned role, necessary background information, and clear task instructions. The model then follow the instruction and leverage the visual input to accomplish various creative tasks, such as composing artwork-inspired prose, developing structured lesson plans, or interpreting the conceptual foundations of advertisements. The approach enables systematic assessment of MLLMs capacity to integrate visual perception with creative expression in contextually appropriate ways. Unlike ground-truth based evaluations, creative responses generated by models resist rule-based assessment methods. In our evaluation framework, we implement the widely adopted MLLM-as-a-Judge methodology, utilizing GPT-4o to assess the quality of model-generated responses. Given the diverse task types and stylistic variations across Creation-MMBench, single-criterion evaluation model cannot reliably assess all tasks. To this end, we define instance-level evaluation criteria for each test case, ensuring that responses are assessed based on their ability Figure 2. Brain regions related to creativity and their respective functions [6, 11]. to integrate contextual and visual information effectively. Using these tailored criteria, an MLLM-generated response is compared against reference answer, and preferences are assigned accordingly. In addition to the preference obtained through pairwise comparison, we introduce visual factuality score to evaluate whether the MLLMs response aligns with key facts present in the visual input. This factual score is determined through unitary evaluation conducted by the GPT-4o judge model. Both Unitary Scoring and Pairwise Comparison offer comprehensive assessment of creative quality and factual accuracy. Based on Creation-MMBench, we conduct comprehensive evaluation of mainstream MLLMs. The results indicate that current open-source MLLMs generally underperform compared to advanced proprietary models (e.g., Gemini2.0-Pro, GPT-4o) in terms of context-aware creativity. To further explore the impact of visual instruction tuning, we transformed Creation-MMBench into text-only variant, Creation-MMBench-TO, by replacing image inputs with corresponding textual descriptions. The results reveal negative effect of visual fine-tuning on the creative abilities of the base LLM, suggesting potential trade-offs introduced by multimodal adaptation. In summary, our main contributions are three-fold: Development of Creation-MMBench, multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs. The benchmark incorporates diverse set of image sources, spans wide range of topics and task types across real-world scenarios, and features highquality, original human-written instructions. Design of robust evaluation methodology that includes carefully crafted instance-specific criteria for each test case, enabling assessment of both general response quality and visual-factual alignment in model-generated content. comprehensive assessment of various MLLMs on Creation-MMBench, providing detailed insights into their performance. The results highlight the current limitations of MLLMs in context-aware creativity and vision-based language generation, offering valuable guidance for future research and development. 2 Figure 3. Overview of Creation-MMBench. Contains four task categories, each category consists of multiple tasks, and the types of images are diverse. Only few representative tasks of each category are shown here. Complete list of tasks is detailed in the Appendix A. 2. Related Work Evaluating Creative Capabilities of LLMs. To evaluate the creative writing capabilities of large language models (LLMs), several benchmark tests have been introduced. One example is the LLM Creative Story-Writing Benchmark [18], where 26 LLMs generate 500 short stories each, incorporating random elements, for total of 13,000 stories. Six models then assess these stories based on 16 criteria related to character development, plot, and narrative structure. Another test [26] challenges models and humans to create stories based on specific prompts. These benchmarks assess not only the writing quality but also the diversity and complexity of the generated content. In addition to creative writing tasks, psychological tests commonly used to assess human creativity have also been adapted for evaluating LLMs. The Alternative Uses Test (AUT) evaluates models ability to propose novel uses for everyday items within time limit, as demonstrated in the assessment of GPT-3s creativity [24]. Another benchmark introduces small-scale test with leaderboard to evaluate how four LLMs generate alternative uses for objects [20]. The Torrance Tests of Creative Thinking (TTCT) have also been applied to LLMs to assess fluency, flexibility, originality, and elaboration in creative tasks [9]. Brainstorming techniques, commonly used to boost creativity, have been applied to evaluate LLMs creative abilities. RPGBench [30] uses role-playing games to assess creativity, and LiveIdeaBench [22] evaluates scientific creativity using single-keyword prompts, focusing on novelty, feasibility, fluency, and flexibility. Other benchmarks like LLM-Evolve [29] test problem-solving and adaptability, while SimulBench [13] evaluates creative simulations like acting as Linux terminal. These benchmarks offer comprehensive evaluation of LLMs creative and simulation capabilities, inspiring further exploration of MLLMs creative potential. Advancing the Evaluation of Creative Intelligence in MLLMs. The advancement of MLLMs has led to the development of various benchmarks to evaluate their intelligence. MMBench [15] covers 20 distinct ability dimensions, focusing on MLLMs general capability. MMMU [32] evaluates advanced perception and reasoning with domain-specific knowledge, featuring 11,500 multimodal questions across 6 disciplines. These benchmarks mainly focus on the analytical intelligence of MLLMs. intelligence, agentFor assessing MLLMs contextual Benchmarks VisIT-Bench MLLM-Bench Touch-Stone AlignMMbench Creation-MMBench Num of Creative Questions Criteria Level multi-images task Specific Role for each Questions Visual Factuality Check 65 20 189 353 765 benchmark instance benchmark task instance Table 1. Comparison of Creation-MMBench with other partial-creation MLLM benchmarks. based or embodied AI benchmarks are commonly used. VLABench [33] provides 100 categories of tasks to evaluate robotics language-conditioned manipulation ability, while EmbodiedBench [28] offers comprehensive evaluation on models problem-solving ability with 1,128 tasks across 4 environments. While the evaluation of MLLMs analytical and contextual intelligence has become relatively mature, the assessment of their creative intelligence remains insufficient. Existing partial-creation benchmarks, such as MLLMBench [7] and AlignMMBench [27], lack systematic and comprehensive evaluation, often failing to assess models capabilities in complex, real-world scenarios. Furthermore, dedicated benchmark designed specifically to evaluate MLLMs creativity has yet to be developed. Therefore, there is pressing need for comprehensive and practical benchmark to bridge this gap. Creation-MMBench aims to establish dedicated benchmark for creative ability evaluation by incorporating diverse set of real-world tasks, offering novel perspective on evaluating MLLMs creative intelligence. 3. Creation-MMBench This section describes the construction process of CreationMMBench, covering aspects such as task design, data collection, annotation, quality control, and evaluation. As shown in Fig. 3, the dataset includes diverse categories, reflecting the complexity and breadth of the tasks involved. Additionally, we introduce the data format and the indicators used to assess model capabilities. 3.1. Benchmark construction Task Design. We began with brainstorming session to explore creative tasks in daily scenarios and designed prototype task set encompassing both routine (e.g., writing common emails) and professional tasks (e.g., designing teaching plans). Leveraging large language model, we then expanded this set to generate diverse range of candidate tasks. Finally, through manual refinement and integration, well-defined set of 51 tasks was established. Task Categorization. We divided the 51 tasks into four Figure 4. Evaluation Result of MLLMs w/o visual input. main categories: 1. Literary Writing: Focus on literary creation (poetry, dialogues, stories, etc.) 2. Common Functional Writing: Focus on functional writing in daily life (social media writing, daily affairs inquiry, etc.) 3. Professional Functional Writing: Focus on functional writing and creative problem-solving in professional domains (analyzing design, developing lesson plans, etc.) 4. Creative Multimodal Understanding: Focus on the integration of visual understanding and creativity (formatted visual content analysis, image appreciation, etc.) Data Composition. For each task, 15 carefully crafted test cases are collected. Each test case comprises two major components: Visual Content: One or more images that contain the necessary information required to accomplish the test case. Query: Include Role (the identity models need to play), Background (prior knowledge that is not duplicated by the visual content and is difficult to acquire, Instruction (operations that models need to perform), and Requirement (constraints or additional considerations). All queries are organized into complete format using unified template and sent to MLLMs with visual content. Instance-specified criteria are defined to make the evaluation more reasonable. The criteria can be mainly divided into two groups: General Subjective Criteria: Assess models expressive capability (structure, style, fluency), execution ability for queries (compliance with requirements, roles, and instructions), and deep reflection on visual content. Visual Factuality Criteria: Assess models ability to perceive objective visual content and utilize visual information effectively. Data Annotation and Quality Control. After task design and definition of data composition, we proceeded with data annotation (including questions and criteria) and quality control. To make the annotator easier to understand, we first built an example question for each task with detailed annotation, then asked volunteers to annotate 15 sam4 (a) Distribution of query lengths. (b) Roles in Creation-MMBench. (c) Example Case of Creation-MMBench. Figure 5. Statistics and Cases of Creation-MMBench. Compared to other widely used MLLM benchmarks, Creation-MMBench features more comprehensive query design to capture abundant creative contexts. Diverse roles are introduced into the queries to stimulate MLLMs utilization of disciplinary and prior knowledge. As an MLLM benchmark, Creation-MMBench includes rich variety of images to thoroughly evaluate multiple capabilities of MLLMs. ple questions for each task with the example and guideline provided below: 1. The visual content of questions should be semantic rich, and the query should not contain any explicit information in the visual content. 2. You are encouraged to formulate diverse queries within the task scope, like diverse roles and background settings, matching the visual content. 3. The ideal answer should be open-ended, creative, but the quality of the response can be assessed using criteria. 4. Ensure each requirement is clear and avoids redundancy. Keep the Visual Factuality Criteria concise and direct. After initial labeling, we conducted cross-verification among volunteers, followed by expert review to ensure data quality. Evaluation Strategy. We employ the MLLM-as-a-judge approach, which consists of two forms: Unitary Scoring and Pairwise Comparison. In Unitary Scoring, the judging model assigns score between 1 and 10 to the response of the evaluated model based on the Visual Factuality Criteria. The Visual Factuality Score is the average score across all questions. In Pairwise Comparison, the evaluated model is designated as model A, while the baseline model (GPT4o-1120) is designated as model B. The judging model assesses the responses based on General Subjective Criteria and visual content, selecting from the set {A>>B, A>B, A=B, A<B, A<<B}. To facilitate further computation, we assign numerical values to the pairwise comparison results: {A>>B = +2, A>B = +1, A=B = 0, A<B = -1, A<<B = -2}. For better interpretability, we multiply this average score by 50 and normalize it to the range of -100 to +100, forming metric as Reward. To mitigate the inherent position bias in the MLLM-as-a-judge approach, we conduct Dual Evaluation, swapping the response positions. The final result is obtained by averaging the outcomes of both evaluations. Detailed evaluation prompt is shown in Appendix B. 3.2. Dataset Statistics To better understand the composition of CreationMMBench, we conducted statistical analysis. Benchmark Comparison Tab. 1 shows the comparison result of Creation-MMBench and four widely used partialcreation MLLM benchmarks. As dedicated benchmark for evaluating creativity, Creation-MMBench features significantly richer set of creative questions and adopts multiimage format. Each question is designed with specific roles to stimulate MLLMs creative capabilities. Unlike other benchmarks that apply the same evaluation criteria across an entire benchmark or task, Creation-MMBench customizes assessment criteria for each question, taking into account both subjective creativity and visual factuality. This tailored approach enables more comprehensive evaluation of MLLMs creative abilities. Statistics and Cases Fig. 5 presents several statistics and cases of Creation-MMBench. As depicted in Fig. 5a, we analyzed the query length distributions of Creation5 Model Overall LW CFW PFW CMU VFS Reward VFS Reward VFS Reward VFS Reward VFS Reward OC Score Avg Tokens Gemini-2.0-pro-exp GPT-4o-1120[Baseline] Gemini-1.5-pro-002 GPT-4.5GPT-4o-mini Doubao-VL Claude-3.5-Sonnet Moonshot-v1-32k-vision Qwen2.5-VL-72B-Instruct InternVL2.5-78B-MPO InternVL2.5-8B-MPO InternVL2.5-78B Qwen2-VL-72B-instruct InternVL2.5-8B Qwen2.5-VL-7B-Instruct MiniCPM-o-2. DeepSeek-VL2 LLaVA-OneVision-72B LLaVA-OneVision-7B Qwen2-VL-7B-instruct 8.53 8. 8.41 8.54 8.07 8.38 7.96 7. 8.33 8.06 7.65 7.91 7.87 7. 7.55 7.49 7.24 7.16 6.75 7. 4.48 0.00 -5.49 -5.88 -13.56 -14. -15.46 -20.58 -5.82 -12.55 -15.10 -16. -22.45 -25.42 -29.80 -34.77 -38.52 -39. -43.49 -43.76 8.66 8.86 8.66 8. 8.30 8.28 8.44 7.30 8.04 8. 8.09 8.05 7.75 7.91 7.34 7. 7.58 7.26 7.36 6."
        },
        {
            "title": "Proprietary MLLMs",
            "content": "8.98 8.93 8.59 8.76 8.44 9. 7.45 8.20 12.71 0.00 -2.04 -8. -15.28 -3.33 -21.57 -8.80 Open-Source MLLMs 8. 8.60 8.30 8.45 8.17 7.95 8. 7.95 7.58 7.72 7.27 7.67 4. -5.00 -3.80 -7.69 -15.56 -15.83 -21. -27.31 -32.50 -30.61 -31.85 -36.30 8. 8.26 8.05 8.05 7.50 7.65 7. 6.91 7.68 7.45 6.80 7.26 7. 6.62 6.71 6.76 6.61 6.43 6. 6.57 -1.88 0.00 -6.04 -4.38 -4. -19.17 -16.46 -21.46 -10.83 -9.17 -16. -17.50 -24.58 -23.33 -39.38 -35.42 -33. -36.32 -43.54 -55.83 3.33 0.00 -4. -5.88 -16.05 -18.72 -11.14 -26.50 -11. -16.32 -23.95 -20.53 -26.84 -33.95 -33. -40.88 -44.02 -47.98 -50.53 -45.26 8. 9.38 8.75 9.29 8.40 8.77 8. 6.91 8.86 8.22 7.88 8.18 8. 7.45 7.78 8.08 7.81 7.62 6. 7.25 -8.06 0.00 -17.22 -0.56 -12. -25.00 -9.44 -36.11 -11.94 -27.78 -19. -28.33 -26.39 -30.00 -30.56 -36.94 -45. -46.37 -56.11 -45.28 73.4 72.0 72. / 64.1 / 70.6 / 76. 77.0 70.3 75.2 74.8 68.1 70. 70.2 66.4 68.0 60.2 67.1 497 444 394 436 516 485 553 461 548 473 500 510 389 440 315 456 Table 2. Evaluation Result of MLLMs on Creation-MMBench. VFS stands for Visual Factuality Score. LW, CFW, PFW, and CMU stand for four categories in Creation-MMBench: Literary Writing, Common Functional Writing, Professional Functional Writing, and Creative Multimodal Understanding. OC Score represents the average score of the OpenVLM Leaderboard and mainly demonstrates the objective performance of the model. The token number is calculated with tiktoken GPT-4o-1120 tokenizer. MMBench in comparison with two partial-creation benchmarks (MLLM-Bench, AlignMMBench) and two widely used general benchmarks (MM-Vet, MMBench). The results indicate that our benchmark features more comprehensive and complex query designs. The majority of queries exceed length of 500 tokens, which facilitates models in capturing richer creative contexts. Fig. 5b illustrates the diversity of roles present in the queries (e.g., writer, artist, Michelin chef, etc.), reflecting the richness of the questions. As an MLLM benchmark, our dataset contains total of 1,001 images spanning more than 25 different categories, with some questions incorporating up to 9 images. Fig. 5c displays the example cases in Creation-MMBench. Vision Indispensability To verify the necessity of visual content in Creation-MMBench, we selected three MLLMs with varying capability levels (Gemini-1.5-Pro002, Qwen2-VL-72B-instruct, and MiniCPM-o-2.6) and examined their performance after removing visual input. In Fig. 4, we observe that when the visual information is removed, the same models exhibit significant declines in Reward. This finding verifies the necessity of visual content in evaluating model performance. 4. Experiment Using Creation-MMBench, we evaluate various Multimodal Large Language Models (MLLMs), with focus on image-based MLLMs that support multiple image inputs. Additionally, we adapted our benchmark into text-only version (Creation-MMBench-TO) by replacing the visual inputs with corresponding textual descriptions and tested multiple Large Language Models (LLMs) to gain deeper insights into their creative capabilities. All evaluations were conducted based on VLMEvalKit [5], employing greedy decoding during inference with the maximum output tokens set to 4096. 4.1. Main Results We evaluated 20 current powerful MLLMs on CreationMMBench, results are shown on Tab. 2. Proprietary MLLMs. Gemini-2.0-Pro performs similarly to GPT-4o, particularly in common functional writing, where it excels in producing content with conversational tone and effectively integrates images. Its strong pre-existing knowledge also helps in professional functional writing tasks, but there is slight gap in perception,"
        },
        {
            "title": "VLM",
            "content": "GPT-4o-1120 Gemini-2.0-pro-exp Qwen2.5-VL-72B-Instruct Qwen2.5-VL-7B-Instruct MiniCPM-o-2.6 InternVL2.5-8B"
        },
        {
            "title": "Corresponding LLM",
            "content": "GPT-4o-1120 Gemini-2.0-pro-exp Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct InternLM2.5-7B-Chat Text Input w. LLM Reward VFS 6.96 8.71 4.08 8.49 0.82 8.55 -19.18 8.18 -19.18 8.18 -22.19 7.83 Text Input w. VLM VFS 8.71 8.49 8.51 7.97 7.78 7.92 Reward 6.96 4.08 -4.05 -27.50 -36.57 -28.73 Vision+Text Input w. VLM VFS 8.72 8.53 8.33 7.55 7.49 7.38 Reward 0.36 4.48 -5.82 -29.80 -34.77 -25. Table 3. LLM performance on Creation-MMBench-TO and Visual Instruction Tuning Impact on VLM creation capability. The image descriptions provided by GPT-4o are general. For the proprietary models, we point to themselves as corresponding LLM and report the performance with image descriptions and questions. especially in tasks like document and snapshot analysis. The smaller GPT-4o-mini outperforms proprietary models like Claude but struggles with professional functional writing due to its limited disciplinary knowledge. DoubaoVL stands out in common functional writing tasks, achieving the highest visual factuality score in this area. Open-Source MLLMs. Among open-source MLLMs, Qwen2.5-VL-72B stands out, performing similarly to advanced proprietary models like Gemini-1.5-Pro and outperforming GPT-4o-mini across all four major categories. This highlights the potential of open-source models in visual creation. The InternVL series also shows strong performance across different model sizes, indicating potential advantages in data and training strategies. The mixed preference optimized (MPO) model demonstrates impressive results in smaller models, with particular strengths in creative multimodal understanding, suggesting that MPO can effectively guide models to better align with human preferences. Category-level Evaluation Results. Across all four categories, professional functional writing shows relatively weaker performance, while common functional writing performs the best. This may be due to the greater difficulty of tasks in the former, which require extensive disciplinary knowledge and deeper understanding of image content. These tasks are more complex and demand higher cognitive abilities. In contrast, common functional writing typically involves simpler, everyday tasks that require less advanced image understanding, making them easier to complete. In the Multimodal Content Understanding and Creation category, while all models show basic content understanding, their ability to generate more creative content is limited. This highlights the gap between the models objective interpretation abilities and their human-aligned visual creativity, further qualitative cases are provided in Appendix G. Comparison of Model Performance on Objective Tasks and Creation-MMBench. To better compare the models objective performance with their visual creativity, we use the OC Score to represent the overall objective performance. As shown in Fig. 6, proprietary models perform well both in objective tasks and visual creativity. However, Figure 6. Comparing OC Score and Creation-MMBench Reward. This figure shows the model performance on the OpenVLM Leaderboard and Creation-MMBench, highlighting significant gap between objective performance and visual creativity in some open-source models. some open-source models, despite showing strong objective performance, struggle with open-ended visual creativity tasks. These models tend to excel in tasks with definitive answers but fall short in generating creative, contextually relevant content. This discrepancy emphasizes the need for more comprehensive evaluation approach, as traditional objective metrics alone may not fully capture models creative abilities in complex, real-world scenarios. 4.2. Evaluating LLMs on Creation-MMBench-TO Current creation benchmarks for Large Language Models mostly focus on specific topics (e.g., LiveIdeaBench [22]), but fail to reveal their creation capability in multiple daily scenarios. To investigate it, we build Creation-MMBenchTO and GPT-4o was used to make the image descriptions with the prompt shown in Appendix E. As shown in Tab. 3, proprietary LLMs showed slightly better contextual creativity than open-source LLMs, though the gap was smaller than that between MLLMs. Large-scale language models performed better at understanding context and expressing ideas compared to smaller models. Additionally, the visual factuality score improved because GPT-4os image descriptions helped LLMs better interpret the image in comparison to MLLMs. Surprisingly, GPT-4o performed better in 7 Judger MLLM Dual Eval Single Eval MAE Cons. MAE Cons. Gemini-2P Claude-3.5 GPT-4o Gemini Qwen Gemini Qwen 0.65 0.51 MiniCPM 0.61 0.56 0.46 MiniCPM 0.47 0.53 0.42 MiniCPM 0.53 Gemini Qwen 0.59 0. 0.50 82.83 91.00 86.14 89.90 92.00 89.90 92.08 96.08 88.24 86.67 90.60 92.13 0.78 0.67 0.69 0.61 0.59 0.57 0.57 0.46 0. 0.72 0.59 0.54 74.75 80.00 81.19 83.84 85.00 86.87 89.11 91.18 86.27 78.67 85. 88.85 Table 4. The Alignment Between Different Evaluation Strategies and Human Preference. visual creativity on Creation-MMBench-TO. This could be because the model can focus more on divergent thinking and creation with the help of descriptions, which may minimize the negative impact of the basic visual content on creativity. 4.3. Impact of Visual instruction tuning on creation capability of MLLM Existing research indicates that visual instruction tuning procedures may adversely affect the language encoders capacity to process and model text-only inputs. To further investigate this, we conducted three experiments under different settings, as shown in Tab. 3. The results indicate that the open-source MLLM, after visual instruction tuning, consistently performs worse compared to the corresponding LLM on Creation-MMBench-TO. This could be due to the instructions used during tuning being of similar length, which restricts the models ability to grasp detailed content in longer texts, resulting in lower visual factuality score. The lack of creative data that combines images further contributes to significant drop in the reward score. Although some proprietary models have shown stronger performance on Creation-MMBench, the performance gap of most MLLMs on Creation-MMBench-TO and CreationMMBench highlights the need for improvement in the perceptual capabilities of MLLMs. 4.4. Evaluation Strategy Selection The goal of MLLM-as-a-judge is always to achieve higher alignment with human preferences. Therefore, we randomly sampled subset of questions (51 tasks 2 questions) and recruited four volunteers to do the pairwise comparison. We selected three models (Gemini-1.5-pro-002, Qwen2-VL-72B, MiniCPM-o-2.6) as Model A, used the baseline model (GPT-4o-1120) as Model B, randomizing the responses position to avoid human biases. Details of the human evaluation process are provided in Appendix F. We then selected three advanced MLLMs (Gemini-2.0Pro, Claude-3.5-Sonnet, GPT-4o) as judging models, and used MAE and Consistency as metrics to reflect the alignment degree. Tab. 4 presents the alignment degree between different evaluation strategies and human preferences. The Figure 7. Qualitative study Case between InternVL-2.5-78B and Reference Answer (GPT4o-1120). results indicate that for all judging models, Dual Evaluation outperforms Single Evaluation, verifying the necessity of Dual Evaluation. Among all the judging models, GPT-4o achieves the best performance in terms of MAE and Consistency, exhibiting the highest alignment with human preferences. Finally, we selected Dual Evaluation, and GPT-4o as the evaluation strategy for Creation-MMBench. 4.5. Qualitative Study To further explore the differences between models on Creation-MMBench, we conducted detailed qualitative study by combining model responses with evaluations. As shown in Fig. 7, InternVL2.5 exhibited limitations in visual perception, particularly in accurately identifying characters due to insufficient latent knowledge. Additionally, InternVL2.5 showed certain weaknesses in the fluency and engagement of its language expression. In contrast, GPT4o was favored by the evaluation model, which provided more balanced assessment. This highlights that open-source models still have considerable space for improvement, particularly in visual creativity tasks. 5. Conclusion We present Creation-MMBench, novel benchmark designed to assess the creative capabilities of MLLMs in realworld scenarios. The benchmark consists of 765 cases across 51 detailed tasks. For each case, we develop instance-specific criteria to evaluate both the subjective 8 quality of responses and visual-factual alignment. Additionally, we create text-only version, Creation-MMBenchTO, by substituting image inputs with corresponding textual descriptions. Extensive experiments on both benchmarks enable thorough assessment of mainstream MLLMs creative abilities and allow us to examine the negative impact of visual instruction tuning."
        },
        {
            "title": "References",
            "content": "[1] Teresa Amabile. Creativity in context: Update to the social psychology of creativity. Routledge, 2018. 1 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 [3] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 1 [4] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1 [5] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. 6 [6] Zhenni Gao, Xiaojin Liu, Delong Zhang, Ming Liu, and Ning Hao. Subcortical structures and visual divergent thinking: resting-state functional mri analysis. Brain Structure and Function, 226(8):26172627, 2021. [7] Wentao Ge, Shunian Chen, Guiming Hardy Chen, Junying Chen, Zhihong Chen, Nuo Chen, Wenya Xie, Shuo Yan, Chenghao Zhu, Ziyue Lin, et al. Mllm-bench: evaluating multimodal llms with per-sample criteria. arXiv preprint arXiv:2311.13951, 2023. 4 [8] Sikun Guo, Amir Hassan Shariatmadari, Guangzhi Xiong, Albert Huang, Eric Xie, Stefan Bekiranov, and Aidong Zhang. Ideabench: Benchmarking large language models for research idea generation. arXiv preprint arXiv:2411.02429, 2024. 2 [9] Erik Guzik, Christian Byrge, and Christian Gilde. The originality of machines: Ai takes the torrance test. Journal of Creativity, 33(3):100065, 2023. 3 [10] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal arXiv preprint arXiv:2501.05444, reasoning benchmark. 2025. 2 [11] Kenneth Heilman. Possible brain mechanisms of creativity. Archives of Clinical Neuropsychology, 31(4):285296, 2016. 2 [12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 1 [13] Qi Jia, Xiang Yue, Tianyu Zheng, Jie Huang, and Bill Yuchen Lin. Simulbench: Evaluating language models with creative simulation tasks. arXiv preprint arXiv:2409.07641, 2024. 9 [28] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. 2, 4 [29] Jiaxuan You, Mingjie Liu, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Llmevolve: Evaluation for llms evolving capability on benchmarks. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 16937 16942, 2024. 3 [30] Pengfei Yu, Dongming Shen, Silin Meng, Jaewon Lee, Weisu Yin, Andrea Yaoyun Cui, Zhenlin Xu, Yi Zhu, Xingjian Shi, Mu Li, et al. Rpgbench: Evaluating large language models as role-playing game engines. arXiv preprint arXiv:2502.00595, 2025. 3 [31] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 2 [32] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 1, 2, 3 [33] Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, YuGang Jiang, et al. Vlabench: large-scale benchmark for language-conditioned robotics manipulation with longhorizon reasoning tasks. arXiv preprint arXiv:2412.18194, 2024. 2, [34] Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu, Xiongkuo Min, Haodong Duan, Kai Chen, and Guangtao Zhai. Redundancy principles for mllms benchmarks, 2025. 1 [14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1 [15] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 2, 3 [16] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 1 [17] Richard Mayer. Fifty years of creativity research. Handbook of creativity, pages 449460, 1999. [18] Lech Mazur. Llm creative story-writing benchmark. https : / / github . com / lechmazur / writing, 2025. 1, 2, 3 [19] Yuxuan Qiao, Haodong Duan, Xinyu Fang, Junming Yang, Lin Chen, Songyang Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Prism: framework for decoupling and assessing the capabilities of vlms. Advances in Neural Information Processing Systems, 37:111863111898, 2025. 2 [20] Abdullah Al Rabeyah, Fabrıcio Goes, Marco Volpe, and Talles Medeiros. Do llms agree on the creativity evaluation of alternative uses? arXiv preprint arXiv:2411.15560, 2024. 3 [21] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleIn First Conference on Language proof q&a benchmark. Modeling, 2024. 1 [22] Kai Ruan, Xuan Wang, Jixiang Hong, and Hao Sun. Liveideabench: Evaluating llms scientific creativity and arXiv preprint idea generation with minimal context. arXiv:2412.17596, 2024. 1, 3, [23] Robert Sternberg. The triarchic theory of intelligence. 1997. 1 [24] Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, and Han van der Maas. Putting gpt-3s creativity to the (alternative uses) test. arXiv preprint arXiv:2206.08932, 2022. 3 [25] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 1 [26] Paul Williams and Carlos Gomez-Rodrıguez. confederacy of models: comprehensive evaluation of llms on creative In UniSC Research Conference. University of the writing. Sunshine Coast, 2024. 3 [27] Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, and Yuxiao Dong. Alignmmbench: Evaluating chinese multimodal alignment in large vision-language models. arXiv preprint arXiv:2406.09295, 2024. 10 Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLMs"
        },
        {
            "title": "Supplementary Material",
            "content": "A Overview of Tasks in Creation-MMBench"
        },
        {
            "title": "C Main Experiment Analysis on Win Rate",
            "content": "Creation-MMBench consists of four main categories and 51 tasks, as shown in Fig. 8. The Literary Writing category includes 8 tasks, focusing on visual literary creation using images such as photographs, illustrations, and paintings. The Common Functional Writing category comprises 18 tasks, addressing everyday functional creation across various genres and image types. The Professional Functional Writing category contains 19 tasks, focusing on creation tasks that require specific domain knowledge. Finally, the Creative Multimodal Understanding category includes 6 tasks, which involve interpreting implied content from images with rich textual information. For each category, we provide four examples, as illustrated in Fig. 14 - Fig. 29. Query and Judge Prompt Template for Creation-MMBench B.1 Query Template For each test case, the query is formatted using the template shown in Fig. 30. In Creation-MMBench-TO, we replace visual content with generated descriptions, as no images are provided to the LLM. B.2 Judge Template For pairwise comparison, the General Subjective Criteria are essential for fair assessment. Visual content helps the judge model better understand the predictions and prevents arbitrary conclusions based solely on linguistic strengths. As shown in Fig. 33, the predictions from different models are presented side by side with the criteria to minimize position bias, with instructions also provided to the judging model. Although changing the hypothetical positions helps reduce positional bias, dual evaluation remains necessary. The format restrictions for evaluating model responses facilitate the extraction of the final verdict through regular matching methods. For Unitary Scoring, we provide Visual Factuality Criteria along with the models predictions, the reference answer, and the query, as outlined in Fig. 34. In test cases with corresponding GroundTruth, this is included to ensure accurate judgment. Each criterion includes several main points, which may be further subdivided into subpoints. The evaluation model scores based on the completeness of these points, with total score of 10. In Creation-MMBench, we adopt the MLLM-as-a-judge approach and introduce two metrics, Visual Factuality Score and Reward, to assess the creative capabilities of In this section, we propose new metric, MLLMs. Win Rate, to provide more comprehensive evaluation of MLLMs performance. C.1 Win Rate Definition Win Rate is defined as the proportion of instances in which the response generated by the evaluated model surpasses that of the baseline model in the Pairwise Comparison. C.2 Main results on Win Rate Tab. 5 presents the Win Rate and detailed judgment counts of MLLMs on Creation-MMBench. Among Proprietary MLLMs, Gemini-2.0-pro-exp demonstrates the best performance in terms of Win Rate, exhibiting the highest number of Much Better and Better cases. In contrast, GPT-4omini performs the worst, with only 53 Better cases. Among Open-Source MLLMs, Qwen2.5-VL-72B-Instruct achieves the best performance, with only 42 Much Worse cases. However, most models perform poorly, lacking any Much Better cases. noticeable performance gap remains between Open-Source and Proprietary MLLMs in terms of Win Rate. Advanced Analysis of Creation-MMBench D.1 Redundancy Analysis Following the [34], we compute the correlation coefficients between the model evaluation results of CreationMMBench and other representative objective benchmarks to investigate the redundancy of Creation-MMBench. Fig. 9 presents the Spearmans Rank Correlation Coefficient (SRCC) and the coefficient of determination (R²) between the benchmarks. As shown in the figure, CreationMMBench exhibits low correlation with MathVista, AI2D, and OCRBench in both SRCC and R². This is likely because these three benchmarks primarily assess objective capabilities such as mathematical reasoning, information extraction, and simple logical inference, with most queries presented in multiple-choice formatan evaluation focus that differs significantly from that of Creation-MMBench. In contrast, MMMU and MM-Vet show certain degree of correlation with Creation-MMBench. This may be attributed to the fact that both benchmarks incorporate portion of creativity-oriented testing, such as the Art & Design 1 Figure 8. Overview of Creation-MMBench Complete Task. Contains four task categories, each category consists of multiple tasks. section in MMMU-Val and the summarization task in MMVet. In general, Creation-MMBench shows low redundancy with existing MLLM Benchmarks, which reflects the novelty and uniqueness of our benchmark. D.2 Other Statistics Fig. 10 presents supplementary statistics for CreationMMBench. As shown in Fig. 10a, we compare the reference answer lengths of Creation-MMBench with four widely used MLLM benchmarks. It is evident that our benchmark exhibits significantly higher proportion of long answers exceeding 1,500 tokens, which reflects the greater complexity of our tasks. Fig. 10b illustrates the richness of instructions within Creation-MMBench, reflecting the diversity of tasks. The analysis of image categories in Fig. 10c demonstrates the rich visual content incorporated in our benchmark. This diversity ensures comprehensive evaluation of the models perceptual capabilities, further solidifying Creation-MMBench as rigorous MLLM benchmark. Query-Specific Experiments on CreationMMBench-TO For Creation-MMBench-TO, the instructions for visual content description are crucial as they are designed to fully stimulate the model to interpret the content of the image as detailed and rich as possible. To avoid the loss of some fine-grained content caused by generic visual descriptions, which could affect the performance of LLMs creative ability, we additionally used Query-Specific Instruction generated by GPT-4o to guide the visual description [19]. As shown in Fig. 31, Generic instruction is standardized, universal instruction aimed at extracting and describing the basic elements present in an image. Query-specific instruction is combination of generic instruction and incremental instruction that directs the VLM to provide detailed account of the visual information relevant to the question. The incremental instruction is crafted by the GPT-4o given the text-only question and the few-shot prompt template shown in Fig. 32. Results on Tab. 6 reveal that query-specific descriptions can help LLMs gain better understanding of visual content, resulting in higher Visual Factuality Score and Reward. However, GPT-4o exhibits an inverse trend, which may be because fine-grained descriptions can mislead the attention of the models and may generate too much detailed creative content that does not fully meet the criteria."
        },
        {
            "title": "Proprietary MLLMs",
            "content": "Gemini-2.0-pro-exp GPT-4o-1120 Gemini-1.5-pro-002 GPT-4.5-0227 GPT-4o-mini Doubao-VL Claude-3.5-Sonnet Moonshot-v1-32k-vision Qwen2.5-VL-72B-Instruct InternVL2.5-78B-MPO InternVL2.5-8B-MPO InternVL2.5-78B Qwen2-VL-72B-instruct InternVL2.5-8B Qwen2.5-VL-7B-Instruct MiniCPM-o-2.6 DeepSeek-VL2 LLaVA-OneVision-72B LLaVA-OneVision-7B Qwen2-VL-7B-instruct 8.53 8.72 8.41 8.54 8.07 8.38 7.96 7.43 8.33 8.06 7.65 7.91 7.87 7.38 7.55 7.49 7.24 7.16 6.75 7.12 - 4.48 0.00 -5.49 -5.88 -13.56 -14.09 -15.46 -20. 26.75% 9 - 11.37% 6 5.36% 7 3.79% 5 9.22% 4 12.55% 4 6.09% 1 Open-Source MLLMs -5.82 -12.55 -15.10 -16.43 -22.45 -25.42 -29.80 -34.77 -38.52 -39.87 -43.49 -43.76 13.2% 6 8.76% 6 10.33% 0 7.25% 4 4.64% 0 5.62% 2 4.25% 0 2.29% 2 1.77% 0 1.72% 0 1.96% 1 1.57% 0 400 - 168 75 53 137 188 92 196 128 158 107 71 84 65 33 27 26 29 24 898 - 1032 1186 1022 850 843 984 917 843 863 764 699 620 545 504 448 411 402 163 - 300 255 422 500 321 500 302 434 438 494 632 624 713 799 791 842 816 845 59 - 24 7 28 38 174 111 42 45 91 62 63 121 132 151 207 194 273 259 1 - 0 0 0 1 0 0 0 0 0 0 0 0 0 1 20 0 0 Table 5. Win Rate Result of MLLMs on Creation-MMBench. WR, MB, MW stands for Win Rate, Much Better and Much Worse"
        },
        {
            "title": "LLM",
            "content": "GPT-4o-1120 Qwen2.5-72B-Instruct InternLM2.5-7B-Chat"
        },
        {
            "title": "Generic",
            "content": "Query-Specific VFS Reward VFS Reward 8.71 8.55 7.83 3.33 4.80 -15.29 6.96 0.82 -22.19 8.88 8.82 8.33 Table 6. Comparison on Generic Descriptions and QuerySpecific Descriptions on Creation-MMBench-TO."
        },
        {
            "title": "F Human Alignment",
            "content": "In this section, we provide detailed examination of Human Alignment, covering the process of pairwise comparison conducted by human evaluators, the definition of the evaluation metrics, and the comprehensive results of ModelHuman and Human-Human alignment. 3 F.1 The process of Human Pairwise Comparison For human evaluation, We sampled two questions from each task in Creation-MMBench to construct set of 102 questions. Four volunteers were recruited to perform pairwise comparisons on this question set. Fig. 11 illustrates the user interface used by human evaluators for this task. To mitigate potential bias, we randomized both the order of the questions and the positions of Model (Gemini-1.5-pro002, Qwen2-VL-72B, MiniCPM-o-2.6) and Model (baseline, i.e. GPT-4o-1120)s responses. Evaluators were provided with the corresponding visual content, related questions, and assessment criteria to compare the quality of the responses presented on the left and right. Their selections were recorded to generate preference results. F.2 The Definition of MAE and Consistency Eq (1) and (2) present the metrics used to evaluate the degree of alignment, specifically MAE and Consistency. In (a) Distribution of query lengths. (b) Roles in Creation-MMBench. Figure 9. Redundancy Analysis of Creation-MMBench with other widely used MLLM Benchmarks. focuses on the category of Professional Functional Writing. It can be significantly observed that Qwen2.5-VL misjudged the swimlane diagram as data flow diagram due to insufficient understanding of the domain-specific knowledge, leading to subsequent errors in diagram analysis. In contrast, GPT-4o-1120 effectively avoided this mistake, and its overall language is more professional and structured, demonstrating more accurate and detailed explanation of the diagram, thus gaining the preference of the judge model. This example also reflects the important role of specific disciplinary knowledge and detailed understanding of image content in this category of tasks. For Creative Multimodal Understanding tasks, as shown in Fig. 13, both models gain full scores in visual factuality and exhibit similar performance in basic visual content understanding and information extraction. However, GPT-4o-1120 gives more comprehensive plan with clear scheduling and reasonable arrangement, thus winning the preference of the judging model. these equations, represents the pairwise comparison results from specific judging model or human evaluator, while denotes the corresponding reference value (average of human ratings). MAE = 1 (cid:88) i=1 Ji Pi Consistency = 1 (cid:40) (cid:88) i=1 if Ji Pi 1 1, 0, otherwise (1) (2) F.3 Full Results We conducted experiments to study both Model-Human Alignment and Human-Human Alignment. For the former, refers to the judging models comparison result, while represents the average human preference. For the latter, refers to the comparison result of an individual human, with being the average preference of the remaining humans. Tab. 7 presents the detailed alignment results. It can be observed that for all judging models, MLLMas-a-judge outperforms LLM-as-a-judge in terms of MAE and Consistency. This may be because the incorporation of visual content allows the judging models to conduct more comprehensive evaluation. Regarding Human-Human Alignment, human preferences are not highly consistent with one another, which reflects the subjective nature of our benchmark."
        },
        {
            "title": "G Category Qualitative Case Study",
            "content": "We conducted qualitative analysis of the common situations that occur in some task categories. Fig. 12 mainly 4 (a) Distribution of reference answers lengths. (b) Instructions in Creation-MMBench. (c) Top 15 Image Categories in Creation-MMBench. Figure 10. Other Statistics of Creation-MMBench. 5 Figure 11. The Process of Human Pairwise Comparison. 6 Judging Method Judging Model/Human MLLM Dual Evaluation Non-Dual Evaluation MAE Consistency MAE Consistency LLM-as-a-judge MLLM-as-a-judge Human-as-a-judge Gemini-2.0-Pro GPT-4o-mini Claude-3.5-Sonnet GPT-4o Gemini-2.0-Pro GPT-4o-mini Claude-3.5-Sonnet GPT-4o H2 H3 H4 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2.6 Gemini-1.5-pro-002 Qwen2-VL-72B MiniCPM-o-2. 0.67 0.59 0.61 0.67 0.59 0.52 0.63 0.46 0.46 0.56 0.46 0.51 0.65 0.51 0.61 0.64 0.53 0.49 0.56 0.46 0.47 0.53 0.42 0.53 / / / / / / / / / / / / 0.62 0.59 0. 0.51 0.59 0.55 0.50 0.50 / / / / 83.17 84.16 85.15 83.17 85.29 90.20 89.11 94.12 92.16 93.07 92.16 89.22 82.83 91.00 86.14 84.16 93.14 91.18 89.90 92.00 89.90 92.08 96.08 88.24 / / / / / / / / / / / / 84.16 86.23 91.80 91.48 86.67 89. 90.60 92.13 / / / / 0.75 0.65 0.67 0.79 0.67 0.66 0.73 0.58 0.58 0.56 0.54 0.58 0.78 0.67 0.69 0.71 0.65 0.61 0.61 0.59 0.57 0.57 0.46 0.59 0.65 0.60 0.66 0.82 0.72 0.73 0.74 0.62 0.72 0.64 0.61 0.65 0.69 0.71 0. 0.56 0.72 0.66 0.59 0.54 0. 0.75 0.68 0.63 77.23 78.22 82.18 74.26 76.47 81.37 78.22 82.35 85.29 90.10 87.25 85.29 74.75 80.00 81.19 76.24 82.35 82.35 83.84 85.00 86.87 89.11 91.18 86.27 84.16 90.20 87.25 74.26 82.35 79.41 76.24 80.39 90.20 87.13 89.22 87. 79.21 77.38 81.97 87.54 78.67 80. 85.23 88.85 87.21 78.69 82.30 87. Table 7. The Results of Model-Human Alignment and Human-Human Alignment. 7 Figure 12. Qualitative Case in Professional Functional Writing. This case comes from Software Engineering Diagram Explanation Task, Assistant is GPT-4o-1120, assistant is Qwen2.5-VL-72B. 8 Figure 13. Qualitative Case in Creative Multimodal Understanding. This case comes from Travel Itinerary Planning and Recommendations Task, Assistant is GPT-4o-1120, assistant is InternVL2.5-78B. Figure 14. Example Case of Literary Writing, from Task story continue. 10 Figure 15. Example Case of Literary Writing, from Task daily conversation creation. 11 Figure 16. Example Case of Literary Writing, from Task landscape to poem. Figure 17. Example Case of Literary Writing, from Task historical story creation. 12 Figure 18. Example Case of Common Functional Writing, from Task daily achievement show off. 13 Figure 19. Example Case of Common Functional Writing, from Task social media travel content. Figure 20. Example Case of Common Functional Writing, from Task daily affairs inquiries. Figure 21. Example Case of Common Functional Writing, from Task personal event summaries. 15 Figure 22. Example Case of Professional Functional Writing, from Task teaching plan. 16 Figure 23. Example Case of Professional Functional Writing, from Task product marketing strategy. Figure 24. Example Case of Professional Functional Writing, from Task nutritional formulation of recipe. 17 Figure 25. Example Case of Professional Functional Writing, from Task clothing match design. 18 Figure 26. Example Case of Creative Multimodal Understanding, from Task advertisement explanation. Figure 27. Example Case of Creative Multimodal Understanding, from Task document understanding. Figure 28. Example Case of Creative Multimodal Understanding, from Task snapshot analysis. 20 Figure 29. Example Case of Creative Multimodal Understanding, from Task travel itinerary planning and recommendations."
        },
        {
            "title": "Query",
            "content": "Creation-MMBench: Assume you are <Role> <Background> Please follow the requirements below to <Instruction>. <Requirement> Creation-MMBench-TO: Assume you are <Role> <Background> Please follow the requirements below to <Instruction>. <Requirement> This question does not provide images, only descriptions of images by large language model. Please answer based on the descriptions. Description of the image: <Image Description> Figure 30. Query Template of Creation-MMBench and Creation-MMBench-TO"
        },
        {
            "title": "Descrption",
            "content": "Generic Instruction: Please carefully describe the content of each incoming image, starting with the number of images. For each image, first provide general introduction to the content, then describe the image type, characters and objects, scene and atmosphere, the relationships between people and objects, and any text on the image. Query-Specific Instruction: Please carefully describe the content of each incoming image, starting with the number of images. For each image, first provide general introduction to the content, then describe the image type, characters and objects, scene and atmosphere, the relationships between people and objects, and any text on the image. Please pay special attention to the following aspects: <query-specific part>. Figure 31. Generic Instruction vs. Query-Specific Instruction of Image Description 22 Few-Shot Prompt Template Your task is to give concise instruction about what basic elements are needed to be described based on the given question. Ensure that your instructions do not cover the raw question, options, or thought process of answering the question. ##Question##: Assume you are an expert at parsing documents, extracting key points and core ideas from documents, and condensing long documents into one or two paragraphs to summarize them. come from the environmental protection department. What conclusions can draw from this report? Please provide key evidence from the document to support your answer, within 100 words. Please follow the requirements below to Answer my question. 1. Your answer should fit the content of the reference material. 2. Provide concise answer with sufficient thinking, removing unnecessary details. ##Contents to observe##: All text and related charts on the picture show the trend of changes ##Question##: Assume you are world traveler. You went to this particular country, Chile, and this is what you saw. Please follow the requirements below to write Reddit post with these pictures, titled Am the only one who feels Chile is extremely underrated as travel destination? 1. Combine the picture content to show the beautiful scenery of Chile. 2. Attract other users to reply ##Contents to observe##: The uniqueness of the main scenery in the picture and the emotions conveyed by the characters in the picture ##Question##: Assume you are an experienced Reddit post creator seeking help and guidance. There is problem while uninstalling software from the iPhone 12. Please follow the requirements below to identify what the problem is based on this image and write Reddit post asking for help. 1. The post needs concise title that clearly summarizes the problem 2. The post should include tags closely related to the problem to increase the likelihood of it being seen or answered 3. The post requires detailed body to provide more information for potential responders ##Contents to observe##: The situation on the interface and the main issues identified ##Question##: Assume you are experienced UI/UX designer, skilled in analyzing and optimizing interface designs to enhance usability and visual appeal. Please follow the requirements below to analyze and propose optimization suggestions for the current UI design based on the provided image. 1. The goal of the optimization is to improve the interface layout, user interaction flow, and overall visual aesthetics while ensuring seamless user experience. 2. After optimization, the application should have strong visual appeal, easy navigation, and provide users with an enjoyable experience, ultimately increasing user engagement and retention rates. ##Contents to observe##: Interface layout and related information on the interface ##Question##: <Question>. ##Contents to observe##: Figure 32. The Prompt Template for the GPT-4o to Generate the Query-Specific Part."
        },
        {
            "title": "Subjective Judge",
            "content": "<Image Content> Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt below, considering both the provided criteria and the image. Your task is to carefully assess each response based on how well it meets the evaluation criteria, incorporating the visual context from the image. The criteria should be the primary basis for your judgment, with the image serving to complement and inform your analysis. Steps for Evaluation: 1. Review Both Responses Independently: Carefully analyze Assistant As and Assistant Bs responses with the criteria and the image. Do not assume any response is better just because it is listed first. Each response should be independently assessed based on the criteria and aided by images to help understand the context. 2. Compare the Strengths and Weaknesses: After evaluating each response independently, compare the two. Consider both the quality of the content and how closely it aligns with the criteria and image. Identify the strengths and weaknesses of each response, and highlight the key differences. 3. Ensure Fairness: To avoid positional bias, swap the positions of Assistant and Assistant after the first evaluation (i.e., make Assistant become Assistant and vice versa) and repeat the analysis and comparison. This ensures that each response is evaluated impartially under the same criteria. 4. Provide Conclusion Based on Both Evaluations: After completing both evaluations (original and swapped positions), combine your analysis to provide final verdict. If the responses are similar, with only minimal differences, your judgment should reflect that and indicate tie. Possible Verdict Options: If Assistant is clearly better in both evaluations: [[A>>B]] If Assistant is slightly better in both evaluations: [[A>B]] If both responses are nearly identical, showing minimal differences and no clear advantage: [[A=B]] If Assistant is slightly better in both evaluations: [[B>A]] If Assistant is clearly better in both evaluations: [[B>>A]] Instructions to the AI Assistants: [INSTRUCTIONS] <instructions> [END INSTRUCTIONS] Assistant Response: [ASSISTANT A] <Reference Answer> [END ASSISTANT A] Evaluation Criteria: [CRITERIA] <Subjective Criteria> [END CRITERIA] Assistant Response: [ASSISTANT B] <Model Prediction> [END ASSISTANT B] Output Format: Your output should include: 1. Evaluation of Assistant As Response: Provide detailed qualitative evaluation, focusing on how well Assistant As response aligns with the criteria and the image. 2. Evaluation of Assistant Bs Response: Provide detailed qualitative evaluation, focusing on how well Assistant Bs response aligns with the criteria and the image. 3. Final Verdict: After considering both evaluations, select one of the following verdicts and justify it based on your analysis: Your output format should end like this: Assistant Evaluation: [qualitative comment] Assistant Evaluation: [qualitative comment] Final Verdict is: [[VERDICT]] Figure 33. Subjective Judge Prompt Template of Creation-MMBench"
        },
        {
            "title": "Visual Judge",
            "content": "With GroundTruth: Please act as an impartial judge and evaluate the Visual Factuality of the responses provided by two AI assistants to the user prompt displayed below. The responses were generated based on the provided instructions and visual input from images. There is provided ground truth for the instructions, but the ground truth was not given to the AI assistants when generating their responses. Take this context into account when making your judgment. Steps for Evaluation: 1. Evaluate visual factuality for both responses based on the provided ground truth and visual factuality criteria. If the visual factuality criteria consist of **X aspects**, each aspect is worth **10/X points**. For each aspect, there may be multiple small criteria. If there are **Y small criteria in one aspect**, each small criterion is worth **10/X/Y points**. 2. Assign total score out of 10 for each response. Instructions to the AI assistants: [INSTRUCTIONS] <instructions> [END INSTRUCTIONS] Assistant response: [ASSISTANT A] <Reference Answer> [END ASSISTANT A] Visual Factuality Criteria: [VISUAL FACTUALITY CRITERIA] <Visual Factuality Criteria> [END CRITERIA] Assistant response: [ASSISTANT B] <Model Prediction> [END ASSISTANT B] Ground truth: [GROUND TRUTH] <GroundTruth> [END GROUND TRUTH] Your output should evaluate visual factuality scores for each assistant and end like this: Response Visual Factuality Score: X/10 Response Visual Factuality Score: Y/10 Without GroundTruth: Please act as an impartial judge and evaluate the Visual Factuality of the responses provided by two AI assistants to the user prompt displayed below. The responses were generated based on the provided instructions and visual input from images. Take this context into account when making your judgment. Steps for Evaluation: 1. Evaluate visual factuality for both responses based on the visual factuality criteria. If the visual factuality criteria consist of **X aspects**, each aspect is worth **10/X points**. For each aspect, there may be multiple small criteria. If there are **Y small criteria in one aspect**, each small criterion is worth **10/X/Y points**. 2. Assign total score out of 10 for each response. Instructions to the AI assistants: [INSTRUCTIONS] <instructions> [END INSTRUCTIONS] Assistant response: [ASSISTANT A] <Reference Answer> [END ASSISTANT A] Visual Factuality Criteria: [VISUAL FACTUALITY CRITERIA] <Visual Factuality Criteria> [END CRITERIA] Assistant response: [ASSISTANT B] <Model Prediction> [END ASSISTANT B] Your output should evaluate visual factuality scores for each assistant and end like this: Response Visual Factuality Score: X/10 Response Visual Factuality Score: Y/10 Figure 34. Visual Factuality Judge Prompt Template of Creation-MMBench"
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Nanjing University",
        "Shanghai AI Laboratory",
        "Shanghai Jiaotong University",
        "The Chinese University of Hong Kong",
        "Tongji University",
        "Zhejiang University"
    ]
}