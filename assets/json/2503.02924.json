{
    "paper_title": "Diverse Controllable Diffusion Policy with Signal Temporal Logic",
    "authors": [
        "Yue Meng",
        "Chuchu fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature \"single-outcome\", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy."
        },
        {
            "title": "Start",
            "content": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY"
        },
        {
            "title": "Diverse Controllable Diffusion Policy with Signal\nTemporal Logic",
            "content": "Yue Meng1 and Chuchu Fan1, Member, IEEE 5 2 0 2 4 ] . [ 1 4 2 9 2 0 . 3 0 5 2 : r AbstractGenerating realistic simulations is critical for autonomous system applications such as self-driving and humanrobot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rulecompliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature single-outcome, making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy. Index TermsAutonomous Agents; Autonomous Vehicle Navigation; Machine Learning for Robot Control I. INTRODUCTION"
        },
        {
            "title": "R EALISTIC behavior modeling is vital for developing",
            "content": "simulators and studying intelligent systems such as autonomous driving and warehouse ground robots [1], [2]. To close the sim-to-real gap for the agents, is critical to model the uncertainty and rule adherence properties that naturally arise from human behaviors. For example, human drivers have different characteristics (aggressiveness, conservativeness), which affect their decision-making in challenging scenarios (e.g., going through roundabout with dense traffic). Besides, low-level driving commands (steering the wheel, accelerating, braking) are also driven by high-level maneuvers lane-changing) and traffic rules (e.g., speed (lane-keeping, it Manuscript received: March 26, 2024; Revised: June 22, 2024; Accepted: July 31, 2024. This paper was recommended for publication by Editor Jens Kober upon evaluation of the Associate Editor and Reviewers comments. This work was partly supported by the National Science Foundation (NSF) CAREER Award #CCF-2238030 and the MIT-Ford Alliance program. 1Yue Meng and Chuchu Fan are with the Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA (mengyue@mit.edu; chuchu@mit.edu) Digital Object Identifier (DOI): see top of this page. limit). Thus, it is of paramount importance to endow agent models with diversity, controllability, and rule-awareness. However, driving simulators up-to-date [3], [4] still struggle in delivering diverse and rule-compliance agent behaviors. They either use recorded trajectories or utilize rule-based or imitation-based methods to generate policy. Rule-based approaches (IDM [5], MOBIL [6]) directly encode rules into mathematical models thus can provide safety and goalreaching performance. Still, they assume simplified driving scenarios and require careful parameter tuning, lacking diversity and realisticness. Imitation-based methods [7], [8] learn from real-world driving data, being more akin to human behaviors, but are prone to violate the rules. Besides, since there is only one outcome (out of many possible future trajectories) per scene in the ground truth, only limited diversity is achieved by these imitation-based approaches [9]. Impeding the advancement of learning realistic behaviors are three challenges: (1) flexible rule representation, (2) the scarcity of multiple-outcome datasets, and (3) the trade-off between rule compliance and diversity. Our paper systematically addresses these problems by leveraging formal language termed Signal Temporal Logic (STL) [10], [11]. STL is known for modeling complicated rules [12], and there are increasing works recently studying controller synthesis under STL specifications via trajectory optimization [13], deep learning [14], [15] and reinforcement learning [16]. Inspired by these works and recent breakthroughs in diffusion models [17] for policy learning [18], we proposed parametric-STL approach to flexibly encode traffic rules, augment the dataset, and learn controllable diffusion policy to balance quality and diversity1. The whole pipeline is: We first specify rules via parameterSTL and use demonstrations to calibrate the parameters. The parameters involve both discrete and continuous values, adding the capacity to form multi-modal and diverse policy distributions. Based on the STL, the parameters, and the original data, we generate the multiple-outcome data via trajectory optimization. Next, we use Denoising Diffusion Probabilistic Model (DDPM [17]) to learn from the augmented data. Finally, different from other diffusion-based policies [18], [19], an additional neural network is designed to regulate the trajectories to be rule-compliant and diverse. We conduct experiments on NuScenes [20], large-scale autonomous driving dataset. We first label the dataset using our annotation tool and generate the augmented dataset. Then we train our approach on the augmented dataset and evaluate on the validation set and in closed-loop testing. Our approach 1Diversity refers to generate different trajectories for the same STL rule. IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY 2024 results in the highest STL satisfaction rate on the validation set and generates the most diverse trajectories compared to baselines. In closed-loop testing, our approach reaches the highest overall performance regarding diversity, STL satisfaction, collision, out-of-lane, and progress. We also show that with varied STL parameters, our approach can reflect different driver characteristics in challenging roundabout scenario, which is valuable for diverse behavior modeling in simulators. case study on human-robot encounter scenarios also shows similar performance compared to other baselines. To summarize, our contributions are: (1) We are the first to use parametric-STL formulation to augment the driving dataset for diverse and controllable policy generation (2) we propose an add-on module (RefineNet) for Diffusion Models to improve trajectory diversity and quality (3) we achieve leading performance in the open-loop evaluation and closed-loop test on NuScenes [20] (4) our algorithm, annotation tool and the augmented data will be available via open-source distribution. II. RELATED WORK Trajectory prediction. Decades of effort have been devoted to exploring trajectory prediction for autonomous systems [21]. Traditional methods include physics-based methods [22] and machine-learning approaches such as Gaussian Process [23]. More recent works use neural networks to conduct behavior cloning or imitation learning [24] on multi-modal largescale datasets (NuScenes [20], WOMD [25]), where the performance is improved by better scene representations (rasterization [7], polylines [26]), advanced architectures [27], and varied output types (sets [28], heatmaps [29] and distrajectories by fitting tributions [30]). Most works predict the dataset. Instead, we assume the rules are given, extract statistics from the data and learn diverse and rule-compliant trajectories conditional on these statistics. Diverse trajectory generation. Deep generative models (Variational Auto Encoder, Generative Adversarial Network, and Diffusion Models) are used to learn to produce diverse trajectories [27], [18], [19], [31], [32]. The diversity is either learned implicitly from data [18], [19], [31], [32] or guided by the Minimum over (MoN) loss [27]. To avoid the prediction concentrating merely around the major mode of the data, the work in [33] learns diversity sampling functions (DSF) in the latent space to generate diverse trajectories. In the autonomous driving domain, recent works use inductive heuristics from the scenes (such as drivable area [34] or lanes [35]) to further regulate the diverse behaviors to be reasonable in common sense. However, it is worth noting that the original dataset lacks diversity in essence (for each scene, there is just one ground trajectory). Unlike all methods that learn diversity from the original data, we first generate diverse data using trajectory optimization then learn the diverse policy. The closest works similar to ours are ForkingPath [9] and [36], where the former is for pedestrian prediction and requires heavy annotation, and the latter generates data using IDM which are less diverse. Realistic agent modeling. Realisticness is often achieved by augmenting imitation with common sense factors, such as collision-free [2], [37], map-consistency [38], attractor-repeller effect [30], driving patterns [39], and LLM-based designs [40]. Recently, Signal Temporal Logic (STL) [10] is widely used to specify rules for trajectory predictions [41], [18], for its expressiveness to encode rules [41] and differentiable policy learning [14], [15]. We follow this line of work and parametrize the STL to learn controllable behaviors. III. PRELIMINARIES A. Signal Temporal Logic (STL) signal = xt, xt+1, ..., xt+T is discrete-time finite sequence of states xi Rn. STL is formal language to specify signal properties via the following expressions [11]: ϕ ::= µ(x) 0 ϕ ϕ1 ϕ2 ϕ1U[a,b]ϕ2 (1) where the boolean-type (true / false) operators split by serve as building blocks to construct an STL formula. Here is true, µ denotes function Rn R, and , , U, [a, b] are not, and, until, and time interval from to b. Other operators are or: ϕ1 ϕ2 = (ϕ1 ϕ2), imply: ϕ1 ϕ2 = ϕ1 ϕ2, eventually: [a,b]ϕ = U[a,b]ϕ and always: [a,b]ϕ = [a,b]ϕ. Denote s, = ϕ if signal from time satisfies ϕ, i.e., ϕ returns true. It is easy to check satisfaction for , µ 0, , , and . As for temporal operators [42]: s, = [a,b]ϕ [t + a, + b] s, = ϕ and s, = [a,b]ϕ [t + a, + b] s, = ϕ. Robustness score ρ(s, t, ϕ) measures how well signal satisfies ϕ, where ρ 0 if and only if s, = ϕ. larger ρ reflects greater satisfaction margin. The calculation is [10]: ρ(s, t, ) = 1, ρ(s, t, µ) = µ(s(t)) ρ(s, t, ϕ) = ρ(s, t, ϕ) ρ(s, t, ϕ1 ϕ2) = min{ρ(s, t, ϕ1), ρ(s, t, ϕ2)} ρ(s, t, [a,b]ϕ) = sup r[a,b] ρ(s, t, [a,b]ϕ) = inf ρ(s, + r, ϕ) ρ(s, + r, ϕ) r[a,b] (2) In our paper, we adopt differentiable approximation for ρ proposed in [43] to provide gradient-based policy guidance. B. Denoising Diffusion Probabilistic Models (DDPM) Diffusion Models are powerful generative models that learn density distribution from the training data to generate samples that resemble these data. diffusion model consists of two procedures: forward process (diffusion) and inverse process (denoising). In paper [17], during the diffusion process, the data are iteratively fused with Gaussian noise until they are close to the white noise. neural network is trained to predict the noises added to these samples at different diffusion steps. To generate samples, the latent samples initialized from the white noise are recovered iteratively by removing the noise predicted by the network. in the denoising process, IV. TECHNICAL APPROACH A. Problem formulation Consider an autonomous system, where we denote the state of the agent at time as st Rn, the control ut MENG et al.: DIVERSE TEMPORAL LOGIC DIFFUSION POLICY 3 Fig. 1: Learning framework. The neural encoder embeds the scene to feature vector. The DDPM takes the feature vector, the STL parameters (indicating driving modes, speed limit, safe distance threshold, etc) and the Gaussian noise to generate trajectories. RefineNet takes the upstream trajectories and features and generates diverse and rule-compliant trajectories. Rm, the scene context Rp, and the STL template Ψ : Γ Φ which generates the STL formula ϕ Φ based on the STL parameters γ Γ and the scene provided (i.e., ϕ = Ψ(γ, c)). Assume known differentiable discretetime system dynamics: : S, where from s0 we could generate trajectory τ = (s0, s1, ..., sT ) R(T +1)n based on control sequence (u0, u1, ..., uT 1) . Assume known diversity measure : Ξ which is real-valued function over set of trajectories Ξ. Given set of demonstrations = {(ci, τ i}N i=1 and an STL template Ψ, the goal is first to generate set of STL parameters {γi}N i=1 such that τi, 0 = Ψ(γi, ci), = 1, 2, ...N , and secondly, learn policy π : Γ such that for given initial state s0, scene context state and STL parameters γ, the trajectories set Ξ generated by π(s0, c, γ) can both (1) satisfy the STL rule τ, 0 = Ψ(γ, c), τ Ξ, and (2) maximize the diversity measure J(Ξ) (we will use entropy to measure the diversity and the detailed computation for the entropy is shown in Sec. V-B). B. System modeling and STL rules for autonomous driving Ego car model. We use unicycle model for the dynamics: xt+1 = xt + vt cos(θt)t, yt+1 = yt + vt sin(θt)t, θt+1 = θt + wtt, vt+1 = vt + att where the state st = (xt, yt, θt, vt)T stands for ego cars xy coordinates, heading angle, and the velocity at time t, and the controls ut = (wt, at)T are angular velocity and acceleration. We denote the ego car width and length L. Scene model. The scene context consists of neighbor vehicles and lanes. We consider the Nn nearest neighbors within the R-radius disk centered at the ego vehicle and denote their states at time as = {I , yj j=1, where the binary indicator is one when the jth neighbor is valid and zero otherwise (this happens when there are less than Nn neighbor vehicles within the R-radius disk), and the rests are similar to the ego car state. We represent each lane as = (I, x1, y1, θ1, ..., xNp , yNp , θNp ) R3Np+1 with an indicator to show its validity and then sequence of Np centerline waypoints with 2D coordinates and directions. We , Lj, j}Nn , xj , vj , θj (3) denote the current lane, the left adjacent lane, and the right adjacent lane for the ego vehicle as Qc, Ql, and Qr. STL rules and parameters. We define common STL rules that are required in autonomous driving scenarios, ϕ0 = [0,T ] vmin Speed(s) vmax ϕ1 = [0,T ] Dist(s, ) dsafe ϕ2 = [0,T ] dmin Dist(s, Qc) dmax ϕ3 = [0,T ] Angle(s, Qc) θmax 4 = [0,T ][0,T ]dmin Dist(s, QH ) dmax ϕH 5 = [0,T ][0,T ]Angle(s, QH ) θmax ϕH where the variables in blue are STL parameters, ϕ0 is for the speed limit, ϕ1 specifies the distance to the neighbors should always be greater than the threshold. Formulas ϕ2 and ϕ3 restrict the vehicles distance and heading deviation and ϕH from the current are for the 5 lanes with {l, r} to restrict left and right adjacent the car to eventually keep the distance/angle deviation from the target lane within the limit 2. We consider the driving mode belongs to one of the high-level behaviors: lanekeeping (M = 0), left-lane-change (M = 1) and rightlane-change(M = 2). Denote the STL parameters γ = (M, vmin, vmax, dsaf e, dmin, dmax, θmax)T Γ R7. The STL template thus is defined as: lane, whereas ϕH Ψ(γ) = ((M = 0) (ϕ0 ϕ1 ϕ2 ϕ3)) 5)(cid:1) (cid:0)(M = 1) (ϕ0 ϕ1 ϕl ((M = 2) (ϕ0 ϕ1 ϕr 5)) 4 ϕl 4 ϕr (4) C. STL parameters calibration Given the STL template Ψ and the expert demonstrations D, we find STL parameters γi for each trajectory τi and the scene ci such that τi, 0 = Ψ(γi, ci). We develop an annotation tool to manually label the high-level behaviors for all the trajectories, then based on M, we calibrate the rest of the STL parameters by making the robustness score on the expert 2Since we aim to eventually always keep it in the limit, the Always operator is put inside the Eventually scope to realize this behavior. 4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY 2024 trajectories equal to zero. This is done by conducting min/max extraction on the existing measurements: e.g., to find dmin, we first check the high-level policy, then compute the minimum distance from the trajectory τ to the target lane. D. Diverse data augementation After obtaining the STL parameters for each scene, we augment the original demonstration by generating diverse behaviors using trajectory optimization. For each scene ci and parameter γi, we formulate the following optimization: minimize u0,u1,...,uT 1 σ+(ρ((s0, s1, ..., sT ), 0, Ψ(γi, ci))) subject to umin ut umax, = 0, 1, ..., 1 (5) st+1 = (st, ut), = 0, 1, ..., 1 where σ+() = max(, 0), means the vector is elementwise no larger than the vector b, and umin, umax are predefined control limits. The system dynamics and STL formula make Eq (5) nonlinear optimization, and we use gradient-based method to solve for the solution. To increase solutions diversity, we consider all three driving modes for M, and for each mode, we run gradient-descent from initial solutions uniformly sampled from the solution space . We denote our augmented dataset as = {(ci, γi, {τ j=1)}3N i=1 where ci = ci/3 and γi = γi/3. Here, denotes rounding float number to an integer index. }K E. Policy learning framework Given the new dataset D, we learn the diverse and rulecompliant behavior via the learning framework shown in Fig. 1. The encoder network embeds the ego state and the scene to feature vector. The DDPM network takes the feature vector, STL parameters, and Gaussian noise to produce trajectories that closely match the distribution in D. Finally, the RefineNet takes upstream features and trajectories to generate diverse and rule-compliant trajectories. Encoder Network. The ego state and the scene are first transformed to the ego frame, and = { Qc, Ql, Qr, } accordingly. The state is sent to fully connected network (FCN) to get ego feature: zego = gego(s) Rd. Similarly, the lanes are fed to lane FCN to generate feature: zlane = [glane( Qc), glane( Ql), glane( Qr))] R3d, where [, ...] is the vector concatenation. To make the neighbors feature not depend on the neighbor orders, we utilize permutation-invariant operators in [44] with neighbor FCN gnei to get: znei = gnei( Nj)] R3d, where Nj [max is the jth neighbor feature. The final merged embedding is: = [zego, zlane, znei] R7d. DDPM Network. Given the embedding z, the STL parameters γ, the sample τ (0) from D, the random noise ϵ, we generate the αtτ (0) + diffused samples: τ (t) = 1 αtϵ for uniformly sampled diffusion steps Uniform(1, Td) and pre-defined gnei( Nj), min gnei( Nj), (cid:80) coefficients αt and αt = αs. The DDPM network gd : R7d+7+2T +1 RT 2 takes z, γ, τ (t), as input and predicts (cid:81) s=1 the noise, guided by the diffusion loss in the first stage of the training: Ld = D,t (cid:20)(cid:12) (cid:12)ϵ gd(z, γ, τ (t), t) (cid:12) (cid:12) (cid:12) (cid:12) 2(cid:21) (6) (cid:17) (cid:16) gd(z, γ, τ (t), t) τ (t) 1αt 1 αt In inference, from the Gaussian noise τ (Td) (0, I), the trajectories are generated iteratively by the denoising step: τ (t1) = 1 + σtξt with αt ξt (0, I) and σ1 = 0 and σt = 1 for 2. We denote the trajectories generated by DDPM as τd. Refine Network. After DDPM is trained, we use RefineNet, fully-connected network gr to regulate the trajectories generated by the DDPM network to encourage rule-compliance and diversity. RefineNet takes as input the trajectories with the highest STL score from the last five denoising steps and outputs residual control sequence conditional on the violation of the STL rules, which is: : R7d+7+2T RT 2, τf inal = τd + 1{ρ(τd, 0, Ψ(γ, c)) < 0} gr(z, γ, τd) (7) If the DDPM produced trajectories already satisfy the STL rules, the RefineNet will not affect the final trajectories (i.e., τf inal = τd); otherwise, the RefineNet improves trajectories diversity and the rule satisfaction rate.3 It is hard to directly optimize for the diversity measure (entropy approximation requires state space discretization, which is non-differentiable). Instead, in the second stage, the RefineNet is updated by the following loss: Lr = (cid:16) (cid:104) tr (K({τf inal,j}Nd j=1) + I)1(cid:17)(cid:105) (8) where tr() is the matrix trace, and RNdNd is the Direct Point Process (DPP) kernel [33] over Nd samples: Kij = 1(ρ(τi) 0) exp(τi τj2) 1(ρ(τj) 0). (9) Minimizing Eq. (8) increases the trajectory cardinality and quality [33], thus increases the diversity and rule satisfaction. F. Guidance-based online policy refinement In evaluation, our learned policy might violate the STL rules in the unseen scenarios due to the generalization error. Similar to [18], we use the STL guidance to improve the sampling process. For new state s0, scene c, the embedding and parameter γ in testing, we replace the original denoising step to τ (t1) = 1 τ (t) + σtξt, where τ (t) is initialized αt as τ (t) 1αt gd(z, γ, τ (t), t) and is updated by minimiz1 αt ing ρ(τ (t), 0, Ψ(γ, c)) via gradient descent. The work [18] conducts multiple guidance steps at every denoising step. In contrast, we find it sufficient to just conduct the guidance step at the last several denoising steps in the diffusion model to accelerate the computation. Although nonlinear optimization does not guarantee optimality, in practice, this method can satisfy the rules with high probability, as shown below. V. EXPERIMENTS We first conduct experiments on NuScenes where our method generates the most diverse trajectories quantitatively 3Table shows that DDPM results in very low rule satisfaction rate, which reflects the great potential of using RefineNet for improvement. MENG et al.: DIVERSE TEMPORAL LOGIC DIFFUSION POLICY 5 TABLE I: Open-loop evaluation: The highest is shown in bold and the second best is shown in underline. Our data augmentation boosts the diversity for baselines VAE and DDPM. Ours+guidance generates the trajectories in the highest quality (Success and Compliance) and diversity (Valid area, Entropy), with the runtime 1/17X to the second best CTG [18]. Methods Augmentation Success Compliance Valid area Entropy Time (s) Traj. Opt. VAE VAE DDPM [17] DDPM [17] TrafficSim [2] CTG [18] Ours (w/o RefineNet) Ours (LST L) Ours Ours+guidance Yes - Yes - Yes Yes Yes Yes Yes Yes Yes 0.961 0.337 0.253 0.514 0.548 0.699 0.833 0.624 0.817 0.782 0.840 0.746 0.077 0.018 0.078 0.050 0.335 0. 0.078 0.573 0.442 0.544 49.130 0.618 0.627 1.760 3.444 6.798 14.933 5.899 17.032 20.284 33.530 2.124 0.162 0.200 0.455 0.557 1.059 1.384 0.780 1.152 1.411 1.735 36.205 0.036 0.039 0.081 0.081 0.037 13. 0.172 0.174 0.174 0.786 (a) Traj. Opt. (b) VAE (c) TrafficSim (d) CTG (e) Ours (f) Ours+guidance (g) Traj. Opt. (h) VAE (i) TrafficSim (j) CTG (k) Ours (l) Ours+guidance Fig. 2: Open-loop visualizations (Green: left-lane-change, red: right-lane-change and blue: lane-keeping). Our approach generates the closest to the Traj. Opt. solution and results in the largest trajectory coverage among all the learning methods. and visually compared to baselines, reaching the highest STL compliance rate. In closed-loop test we get the lowest collision rate and out-of-lane rate. Visualization shows how varied STL parameters affect the agent behavior under the same scene, indicating our approachs potential for diverse agent modeling. We also consider human-robot scenario, where we generate the most close-to-oracle distribution. A. Implementation details Dataset. NuScenes [20] is large-scale real-world driving dataset that comprises 5.5 hours of driving data from 1000 scenes (from Boston and Singapore). We use the trainval split of the dataset (850 scenes), densely sample from all valid time instants and randomly split the dataset with 70% for training (11763 samples) and 30% for validation (5042 samples). We developed an annotation tool and it took student four days to label high-level driving behaviors for the data. We mainly focus on vehicles and leave the other road participants (pedestrians and cyclists) for future research. Algorithm details. The planning horizon is = 20, the duration is = 0.5s, the number of neighbors is Nn = 8, perception radius is = 50m, each lane has Np = 15 waypoints, and the control limits are umax = umin = (0.5rad/s, 5.0m/s2)T . In data generation, the number of samples per scene is = 64. Similar to [18], the diffusion steps is Td = 100 and cosine variance schedule is used. The networks are FCN with 2 hidden layers, with 256 units for each layer and ReLU activation for the intermediate layers. For our method, we first generate the augmented dataset, then train the DDPM for 500 epochs using Eq. (6). Finally, we freeze the Encoder and DDPM and train the RefineNet for 500 epochs using Eq. (8). We use PyTorch with an ADAM [45] optimizer, learning rate 3 104 and batch size 128. The augmentation takes 5 hours, and training takes 8 hours on an RTX4090Ti GPU. B. Open-loop evaluation Baselines. The methods are Traj. Opt.: Trajectory optimization solution (treated as Oracle); VAE: Variational Auto-encoder; 6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY 2024 TABLE II: Closed-loop testing: The highest is shown in bold and the second best is shown in underline. Ours+guidance strikes in diversity and rule compliance with the least collision and out-of-lane rate within an acceptable computation budget. Methods Compliance Valid area Progress Collision Out-of-lane Time (s) VAE DDPM TrafficSim [2] CTG [18] Ours Ours+guidance 0.076 0.168 0.311 0.704 0.448 0. 1.403 5.250 3.363 16.838 12.908 21.577 71.435 83.493 71.113 92.181 74.189 88.638 0.385 0.115 0.269 0.115 0.192 0. 0.154 0.000 0.077 0.000 0.154 0.000 0.019 0.031 0.018 9.280 0.055 0.379 DDPM: Trained with the loss in Eq. (6); TrafficSim [2]: train VAE with an extra rule-violation loss; and CTG [18]: train the DDPM and test with guidance during sampling. And Ours: our method (Sec. IV-E); Ours+guidance: with guidance (Sec. IV-F). For ablations, Ours (w/o RefineNet): no RefineNet; and Ours (LST L): uses LST = ReLU(0.5ρ) to train the RefineNet. We implement baselines to accommodate for modality and STL rules, and also train VAE and DDPM on the original NuScenes to show gains from our augmentation. Metrics. We evaluate the trajectories by (1) Success: the ratio of the scenes that have at least one trajectory satisfying the STL rules, (2) Compliance: the ratio of generated trajectories satisfying the STL rules (valid trajectories), (3) Valid area: The 2d occupancy area of the valid trajectories averaged the scenes, (4) Entropy: At each time step, we over all compute the entropy for the normalized angular velocity and the acceleration from the valid trajectories respectively, and average over all time steps and all scenes, and (5) Time: measures the trajectory generation time. Quantitative results. As shown in Table I, both VAE and DDPM trained on our augmented dataset achieve higher diversity (valid area and entropy) than them trained on the original NuScenes data, indicating the value of our augmentation technique to generate diverse demonstrations. VAE and DDPMs low compliance rates (less than 10%) imply the need to use an advanced model. Compared to advanced baselines TrafficSim [2] and CTG [18], Ours strikes sharp rise in the quality and diversity: 3266% higher for rule compliance, 36198% larger valid area, and up to 33% increase in entropy. Moreover, Ours+guidance achieves the highest quality and diversity with 1/17X the time used by the best baseline CTG 4. Ablation studies. Ours (w/o RefineNet) is bit better than DDPM, where the gains result from the ensemble of DDPM outputs from the last five denoising steps. With RefineNet and the STL loss used, Ours (LST L) gets 47 188% increase in the diversity measure compared to Ours (w/o RefineNet). Further using the diversity loss, Ours achieves 19 22% increase in diversity measure, and Ours+guidance generates the most diverse trajectories but at the cost of 4X longer inference time. We can see that adding the RefineNet and using diversity loss greatly improves the diversity and rule compliance rate (though using the loss Lr will drop the compliance rate by 3%.) Visualizations. In Fig. 2 we 4Ours+guidance is much faster than CTG because we only use guidance at the last five denoising steps, whereas CTG uses guidance at every step. plot all the rule-compliant trajectories (generated by different methods) under specific scenes and color them based on highlevel driving modes (red for right-lane-change, blue for lane-keeping and green for left-lane-change). Ours and Ours+guidance generate close to Traj. Opt. distributions, with the largest area coverage among all learning baselines. C. Closed-loop testing Implementation. We select 26 challenging trials in NuScenes dataset (where the ego car needs to avoid cars on the street or to keep track of curvy lanes), and we set the STL parameters to the minimum/maximum values in the training data to represent the largest feasible range for parameter selection. We start the simulation from these trials and stop it if (1) it reaches the max simulation length or (2) collision happens or the ego car drives out-of-lane. Due to the fast-changing environment, we follow the MPC [15] rather than windowed-policy [19], [32] or other mechanism5 [31]. At every time step, out of the 64 generated trajectories, we choose the one with the highest robustness score and pick its first action to interact with the simulator. We measure: (1) Compliance: the ratio of generated trajectories satisfying the STL rules, (2) Valid Area: The 2d occupancy area of the valid trajectories, (3) Progress: the average driving distance of the ego car, (4) Collision: the ratio of trials ending in collisions, (5) Out-of-lane: the ratio of trials ending in driving out-of-lane, and (6) Time: the runtime at every step. Results. As shown in Table II, our approach without guidance already achieves high performances compared to VAE, DDPM, and TrafficSim [2], with slightly high computation time compared to these learning-based baselines - the overhead in the runtime mainly owes to the DDPM and STL evaluation. Given that the simulation = 0.5s, this overhead is still in reasonable range. With the guidance used, Ours+guidance surpasses all the baselines in quality and diversity metrics (except for CTGs progress), achieving the lowest collision rate and zero out-of-lane. Compared to the best baseline, CTG, our runtime is just 1/24X of CTGs. This shows our methods ability to provide diverse and high-quality trajectories with an acceptable time budget in tests. 5The works [19], [32], [31] are mainly for robot manipulation, where the task horizon is long and the environment is relatively static. One challenge in driving scenarios is that the environment can change suddenly in planning (a new neighbor vehicle emerges, lane changes, etc). Thus, the windowed policy might not react to these changes and mechanism to detect the change and trigger the replanning process is needed. We do not use the goal-conditioned mechanism in [31] as STL cannot be fully conveyed by few goal states. MENG et al.: DIVERSE TEMPORAL LOGIC DIFFUSION POLICY 7 (a) Speed 0 1m/s (b) Speed 0 4m/s (c) Speed 0 6m/s Fig. 3: Diverse behaviors due to varied STL parameters. When the speed limit is low, the agent waits until all vehicles pass the roundabout. When at the middle-speed limit, the agent joins the queue in the middle but yields to other vehicles at high speed. At the high-speed limit, the car joins the queue and keeps its place as traversing the roundabout. Diverse behaviors under different STL parameters. To show the controllability, we use our method with varied STL parameters to render agent behaviors in challenging scene shown in Fig. 3, where the ego car waits to join in roundabout with dense traffic on the right side. We assign three different maximum speed limits to our network, fix the minimum speed to 0m/s, and plot the scenario at t=22. In Fig. 3, the history of the ego car is in pink, and planned trajectories are in blue (for lane-keeping) and red (for rightlane-change). When the max speed limit is low (1m/s), the agent waits until all cars finish the roundabout. When at speed range [0m/s, 4m/s], the agent joins the queue in the middle but yields to other vehicles at high speed. When at wider interval ([0m/s, 6m/s]), the car joins the middle of the queue and keeps the place as moving in the roundabout. This finding shows we can model different agent characteristics, which is valuable for realistic agent modeling in simulators. D. Case study on diverse human behaviors generation We demonstrate how to generate diverse human behaviors in scenarios [46] where the STL rule for the human is to reach the goal while avoiding collision with the incoming robot. We choose collision thresholds as STL parameters and calibrate them on 1000 real-world trajectories collected by [46]. The dataset is further augmented as in Sec. IV-C. We train all methods on the augmented dataset for comparison. TABLE III: Open-loop evaluation in human-robot encounter scenarios [46]. Ours+guidance reaches the closest diversity to the oracle (Traj. Opt.), while using only 1/35X of its time. Methods Compliance Area Entropy Time (s) Traj. Opt. VAE DDPM [17] TrafficSim [2] CTG [18] Ours Ours+guidance 1.000 0.420 0.262 0.966 0.665 0.811 0.921 6.114 1.894 4.673 2.076 4.645 5.675 5.773 55.553 18.600 20.715 17.998 39. 44.715 48.959 66.125 0.002 0.023 0.002 16.153 0.040 1.850 (a) TrajOpt (b) VAE (c) T.S. [2] (d) CTG (e) Ours (f) OursG Fig. 4: Valid trajectories for the human-robot encounters. VAE and T.S. (TrafficSim) cannot capture diverse trajectories, whereas OursG (Ours+guidance) are close-to-oracle. In Table III, Ours+guidance is the highest in Area and Entropy, reaching the most close-to-oracle (Traj. Opt.) performance with 1/35X of its time. Although TrafficSim [2] reaches the highest compliance rate, its trajectories are less diverse (shown in Fig. 4c). Ours+guidance gets 24 38% improvement over CTG [18] in rule-compliance rate and diversity, being 7.7X faster in inference speed. Visualizations from Fig. 4 show that the trajectory distribution of OursG (Ours+guidance) is close to the oracle. These results show our advantage in generating diverse rule-compliant policy. VI. CONCLUSIONS We propose method to learn diverse and rule-compliant agent behavior via data augmentation and Diffusion Models. We model the rules as Signal Temporal Logic (STL), calibrate the STL parameters from the dataset, augment the data using trajectory optimization, and learn the diverse behavior via DDPM and RefineNet. In the NuScenes dataset, we produce the most diverse and rule-compliant trajectories, with 1/17X the runtime used by the second-best baseline [18]. In closedloop test, we achieve the highest safety and diversity, and with varied STL parameters we can generate distinct agent behaviors. case study on human-robot scenarios shows we can generate closed-to-oracle trajectories. The limitations are: high-level behavior labeling effort; longer runtime than VAE, DDPM, or TrafficSim [2]; and lack of guarantees. Besides, in rare cases if DDPM learns rule-compliance and non-diverse trajectories, our RefineNet cannot improve the diversity. We plan to address those and more complex rules in the future. ACKNOWLEDGEMENT This work was partly supported by the National Science Foundation (NSF) CAREER Award #CCF-2238030 and the MIT-Ford Alliance program. Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and dont reflect the views of the sponsors. REFERENCES [1] W. Jager and M. Janssen, The need for and development of behaviourally realistic agents, in International Workshop on Multi-Agent Systems and Agent-Based Simulation, pp. 3649, Springer, 2002. 8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY [2] S. Suo, S. Regalado, S. Casas, and R. Urtasun, Trafficsim: Learning to simulate realistic multi-agent behaviors, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1040010409, 2021. [3] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, Carla: An open urban driving simulator, in Conference on robot learning, pp. 116, PMLR, 2017. [4] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Flotterod, R. Hilbrich, L. Lucken, J. Rummel, P. Wagner, and E. Wießner, Microscopic traffic simulation using sumo, in 2018 21st international conference on intelligent transportation systems (ITSC), 2018. [5] A. KESTING and M. T. D. HELBING, Enhanced intelligent driver model to access the impact of driving strategies on traffic capacity, Phil. Trans. R. Soc. A, vol. 368, pp. 45854605, 2010. [6] A. Kesting, M. Treiber, and D. Helbing, General lane-changing model for car-following models, Transportation Research Record, mobil vol. 1999, no. 1, pp. 8694, 2007. [7] T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone, Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data, in European Conference on Computer Vision, 2020. [8] R. Bhattacharyya, B. Wulfe, D. J. Phillips, A. Kuefler, J. Morton, R. Senanayake, and M. J. Kochenderfer, Modeling human driving behavior through generative adversarial imitation learning, IEEE Transactions on Intelligent Transportation Systems, 2022. [9] J. Liang, L. Jiang, K. Murphy, T. Yu, and A. Hauptmann, The garden of forking paths: Towards multi-future trajectory prediction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1050810518, 2020. [10] A. Donze and O. Maler, Robust satisfaction of temporal logic over real-valued signals, Formal Modeling and Analysis of Timed Systems, p. 92, 2010. [11] A. Donze, T. Ferrere, and O. Maler, Efficient robust monitoring for stl, in Computer Aided Verification: 25th International Conference, CAV 2013, pp. 264279, Springer, 2013. [12] Y. E. Sahin, R. Quirynen, and S. Di Cairano, Autonomous vehicle decision-making and monitoring based on signal temporal logic and mixed-integer programming, in 2020 American Control Conference (ACC), pp. 454459, IEEE, 2020. [13] C. Dawson and C. Fan, Robust counterexample-guided optimization for planning from differentiable temporal logic, in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 72057212, IEEE, 2022. [14] K. Leung, N. Arechiga, and M. Pavone, Backpropagation through signal temporal logic specifications: Infusing logical structure into gradientbased methods, The International Journal of Robotics Research, vol. 42, no. 6, pp. 356370, 2023. [15] Y. Meng and C. Fan, Signal temporal logic neural predictive control, IEEE Robotics and Automation Letters, 2023. [16] X. Li, C.-I. Vasile, and C. Belta, Reinforcement learning with temporal logic rewards, in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 38343839, IEEE, 2017. [17] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, 2020. [18] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray, and M. Pavone, Guided conditional diffusion for controllable traffic simulation, in 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 35603566, IEEE, 2023. [19] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, arXiv preprint arXiv:2303.04137, 2023. [20] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, nuscenes: multimodal dataset for autonomous driving, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11621 11631, 2020. [21] F. Leon and M. Gavrilescu, review of tracking and trajectory prediction methods for autonomous driving, Mathematics, vol. 9, no. 6, p. 660, 2021. [22] S. Ammoun and F. Nashashibi, Real time trajectory prediction for collision risk estimation between vehicles, in 2009 IEEE 5Th international conference on intelligent computer communication and processing, pp. 417422, IEEE, 2009. [23] J. Joseph, F. Doshi-Velez, A. S. Huang, and N. Roy, bayesian nonparametric approach to modeling motion patterns, Autonomous Robots, vol. 31, pp. 383400, 2011. [24] A. O. Ly and M. Akhloufi, Learning to drive by imitation: An overview of deep behavior cloning methods, IEEE Transactions on Intelligent Vehicles, vol. 6, no. 2, pp. 195209, 2020. [25] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, et al., Scalability in perception for autonomous driving: Waymo open dataset, arXiv e-prints, 2019. [26] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C. Schmid, Vectornet: Encoding hd maps and agent dynamics from vectorized representation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152511533, 2020. [27] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, Social gan: Socially acceptable trajectories with generative adversarial networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 22552264, 2018. [28] T. Phan-Minh, E. C. Grigore, F. A. Boulton, O. Beijbom, and E. M. Wolff, Covernet: Multimodal behavior prediction using trajectory sets, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1407414083, 2020. [29] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde, Thomas: Trajectory heatmap output with learned multi-agent sampling, arXiv preprint arXiv:2110.06607, 2021. [30] C. Jiang, A. Cornman, C. Park, B. Sapp, Y. Zhou, D. Anguelov, et al., Motiondiffuser: Controllable multi-agent motion prediction using diffusion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 96449653, 2023. [31] M. Reuss, M. Li, X. Jia, and R. Lioutikov, Goal-conditioned imitation learning using score-based diffusion policies, arXiv preprint arXiv:2304.02532, 2023. [32] P. M. Scheikl, N. Schreiber, C. Haas, N. Freymuth, G. Neumann, R. Lioutikov, and F. Mathis-Ullrich, Movement primitive diffusion: Learning gentle robotic manipulation of deformable objects, IEEE Robotics and Automation Letters, 2024. [33] Y. Yuan and K. Kitani, Diverse trajectory forecasting with determinantal point processes, arXiv preprint arXiv:1907.04967, 2019. [34] Y. Xu, H. Cheng, and M. Sester, Controllable diverse sampling forecasting, arXiv preprint for diffusion based motion behavior arXiv:2402.03981, 2024. [35] S. Kim, H. Jeon, J. W. Choi, and D. Kum, Diverse multiple trajectory prediction using two-stage prediction network trained with lane loss, IEEE Robotics and Automation Letters, pp. 20382045, 2022. [36] M. Stoll, M. Mazzola, M. Dolgov, J. Mathes, and N. Moser, Scaling planning for automated driving using simplistic synthetic data, arXiv preprint arXiv:2305.18942, 2023. [37] Y. Meng, Z. Qin, and C. Fan, Reactive and safe road user simulations using neural barrier certificates, in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE, 2021. [38] S. Casas, C. Gulino, S. Suo, and R. Urtasun, The importance of prior knowledge in precise multimodal prediction, in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 22952302, IEEE, 2020. [39] R. P. Bhattacharyya, D. J. Phillips, C. Liu, J. K. Gupta, K. DriggsCampbell, and M. J. Kochenderfer, Simulating emergent properties of human driving behavior using multi-agent reward augmented imitation learning, in 2019 International Conference on Robotics and Automation (ICRA), pp. 789795, IEEE, 2019. [40] Z. Zhong, D. Rempe, Y. Chen, B. Ivanovic, Y. Cao, D. Xu, M. Pavone, and B. Ray, Language-guided traffic simulation via scene-level diffusion, arXiv preprint arXiv:2306.06344, 2023. [41] S. Maierhofer, P. Moosbrugger, and M. Althoff, Formalization of intersection traffic rules in temporal logic, in 2022 IEEE Intelligent Vehicles Symposium (IV), pp. 11351144, IEEE, 2022. [42] O. Maler and D. Nickovic, Monitoring temporal properties of continuous signals, Formal Techniques, ModellingandAnalysis of Timed and Fault-Tolerant Systems, p. 152, 2004. [43] Y. V. Pant, H. Abbas, and R. Mangharam, Smooth operator: Control using the smooth robustness of temporal logic, in 2017 IEEE Conference on Control Technology and Applications, IEEE, 2017. [44] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, Pointnet: Deep learning on point sets for 3d classification and segmentation, in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652 660, 2017. [45] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, arXiv preprint arXiv:1412.6980, 2014. [46] A. Linard, I. Torre, A. Steen, I. Leite, and J. Tumova, Formalizing trajectories in human-robot encounters via probabilistic stl inference, in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 98579862, IEEE, 2021."
        }
    ],
    "affiliations": [
        "Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA"
    ]
}