{
    "paper_title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "authors": [
        "Chi-Pin Huang",
        "Yunze Man",
        "Zhiding Yu",
        "Min-Hung Chen",
        "Jan Kautz",
        "Yu-Chiang Frank Wang",
        "Fu-En Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 1 ] . [ 1 8 0 7 9 0 . 1 0 6 2 : r Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Chi-Pin Huang1, Yunze Man2, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang1, Fu-En Yang NVIDIA 2026-1-"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from teacher, driven by preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery. Links: Project Page 1. Introduction Recent large vision-language models (VLMs) Liu et al. (2023); Comanici et al. (2025); Liu et al. (2024); Bai et al. (2025); Shi et al. (2024); Li et al. (2025); Chen et al. (2025); Wang et al. (2025); Xie et al. (2024) have achieved remarkable capabilities in visual-language understanding across diverse multimodal tasks. To extend these capabilities to embodied-centric tasks, recent works leverage large-scale robot demonstrations ONeill et al. (2024) to develop Vision-Language-Action (VLA) foundation models Brohan et al. (2022, 2023); Team et al. (2024); Bjorck et al. (2025); Li et al. (2025); Black et al. (2024); Yang et al. (2025); Kim et al. (2024). These VLA tasks require agents to perceive complex visual scenes, reason over spatial and temporal contexts, and execute adaptive actions within dynamic environments, demanding robust long-horizon planning and contextual adaptation. However, as these VLA models primarily rely on supervised training from action data, they excel at basic skills (e.g., pick-and-place) but struggle to generalize beyond training distributions, such as long-horizon planning, selfFigure 1: Overview of Fast-ThinkAct. Previous reasoning VLAs generate lengthy reasoning traces (250 tokens). Our approach learns compact continuous tokens (e.g., 6) (blue) and parallel spatial tokens (green) as internal reasoning. The bottom-right plot shows that we achieve 9.3 faster inference than ThinkAct-7B Huang et al. (2025), while delivering improved performance on the SimplerEnv-Google benchmark. Additional affiliations: 1 National Taiwan University. 2 University of Illinois Urbana-Champaign. 2026 NVIDIA. All rights reserved. Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning correction from failures, and adaptation to novel scenarios, due to the impracticality of collecting exhaustive robot demonstrations. Reasoning VLAs Zawalski et al. (2024); Zhao et al. (2025); Lee et al. (2025); Wu et al. (2025); Qu et al. (2025); Huang et al. (2025); Kim et al. (2025) address these limitations by incorporating intermediate thinking processes, improving generalization and task-solving capability. Supervised chain-of-thought (CoT) methods Zawalski et al. (2024); Zhao et al. (2025); Lee et al. (2025); Qu et al. (2025) address this by learning from intermediate reasoning annotations. These approaches can be categorized into textual reasoning methods that leverage off-the-shelf LLMs and VLMs to generate pseudo CoT labels Zawalski et al. (2024), and visual reasoning methods that generate structured visual reasoning representations such as sub-goal images, image depth, and 2D visual traces Zhao et al. (2025); Lee et al. (2025). However, these supervised approaches require substantial reasoning annotations and remain limited by training data coverage. To address this, ThinkAct Huang et al. (2025) employs RL-based reasoning Shao et al. (2024) to generate long textual CoTs guided by action-aligned visual rewards. While these reasoning methods effectively improve task generalization and planning capabilities, they require generating lengthy chain-of-thought steps that introduce substantial reasoning latency, which hampers embodied applications with real-time requirements. In embodied AI applications such as robotic manipulation and autonomous driving, agents must make rapid decisions at high frequencies (e.g., 1-15 Hz) Guan et al. (2025). However, generating lengthy reasoning traces can take several seconds per decision (e.g., 0.1 Hz) Huang et al. (2025); Lee et al. (2025), creating critical bottleneck that limits real-time performance Guan et al. (2025); Yu et al. (2025) and poses safety risks in time-critical scenarios Wang et al. (2025). To mitigate this efficiency bottleneck while preserving reasoning capabilities, very recent works Chen et al. (2025); Yu et al. (2025); Guan et al. (2025) have explored approaches to reduce inference latency in embodied reasoning. For instance, ECoT-Lite Chen et al. (2025) proposes reasoning dropout to accelerate inference, yet directly reducing textual reasoning length risks performance degradation due to critical information loss. How to preserve reasoning capability while enabling compact representations that properly capture essential spatial-temporal dynamics remains crucial challenge for reasoning VLA models. In this paper, we propose Fast-ThinkAct, an efficient embodied reasoning framework for Vision-Language-Action tasks that achieves compact yet expressive planning through verbalizable latent reasoning. As depicted in Figure 1, unlike prior reasoning VLAs that generate lengthy explicit textual CoT traces, we introduce rewardguided preference distillation with visual trajectory alignment to compress linguistic and visual planning into compact continuous latents that enable implicit internal reasoning. Our student VLM encodes reasoning into compact latents decodable by verbalizer, enabling preference-based optimization that leverages RL-derived reward signals to distill high-quality reasoning patterns from textual teacher VLM while suppressing lowquality ones. We further align trajectory latents between teacher and student to transfer visual planning capabilities essential for embodied control. Once trained, the student VLM enables reasoning-enhanced policy learning that bridges implicit multimodal planning with action execution, achieving significantly faster inference while outperforming existing reasoning VLAs. Our contributions can be summarized as follows: We propose Fast-ThinkAct, an efficient reasoning framework that compresses reasoning into verbalizable latent thoughts while maintaining expressive planning abilities. We introduce preference-guided distillation with manipulation trajectory alignment that compresses linguistic and visual planning into compact continuous latents. We bridge high-level visual planning to low-level action execution through reasoning-enhanced policy learning guided by manipulation trajectory latents. We achieve up to 89.3% inference latency reduction over state-of-the-art reasoning VLAs while maintaining strong performance across diverse embodied benchmarks. 2 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning 2. Related Works 2.1. Vision-Language-Action (VLA) Models Foundation VLAs. Vision-Language-Action (VLA) models Brohan et al. (2022, 2023); Team et al. (2024); Bjorck et al. (2025); Li et al. (2025); Black et al. (2024); Yang et al. (2025); Pertsch et al. (2025); Driess et al. (2025); Bu et al. (2025); Team et al. (2025); Wang et al. (2025) have recently emerged as promising paradigm for embodied AI by training vision-language backbones on large-scale robot demonstrations. Works such as OpenVLA Kim et al. (2024) and ğœ‹0 Black et al. (2024) achieve language-conditioned manipulation through end-to-end policy learning, while Magma Yang et al. (2025) co-trains on heterogeneous human and robot data. HAMSTER Li et al. (2025) and TraceVLA Zheng et al. (2024) further leverage 2D visual trajectories to boost spatial-action connections. Despite success on routine manipulation, these imitation-based approaches struggle with long-horizon planning and generalization to novel scenarios due to limited training data coverage. Reasoning VLAs. To overcome these limitations, recent works Zawalski et al. (2024); Zhao et al. (2025); Lee et al. (2025); Wu et al. (2025); Qu et al. (2025); Huang et al. (2025); Kim et al. (2025); Yuan et al. (2025); Abdolmaleki et al. (2025) integrate explicit reasoning mechanisms into VLA architectures. Supervised approaches Zawalski et al. (2024); Zhao et al. (2025); Lee et al. (2025); Qu et al. (2025) introduce intermediate reasoning through chain-of-thought annotations. Embodied CoT Zawalski et al. (2024) and Hi-Robot Shi et al. (2025) synthesize reasoning labels via pretrained foundation models. To perform vision-centric reasoning Man et al. (2025); Sarch et al. (2025) beyond pure text, CoT-VLA Zhao et al. (2025) employs visual goal generation and MolmoAct Lee et al. (2025) structures reasoning by spatial representations. Additionally, EO-1 Qu et al. (2025) introduces interleaved vision-language-action pre-training to bridge reasoning and interaction. Recent works Yuan et al. (2025); Huang et al. (2025) alternatively leverage reinforcement fine-tuning to generate reasoning chains with designed rewards. Despite improved generalization, these reasoning VLAs suffer from high inference latency and inevitably introduce extraneous information that degrades action quality. 2.2. Efficient Reasoning To address the inference latency of reasoning, recent LLM research explores various efficiency techniques Lee et al. (2025); Dai et al. (2025); Yuan et al. (2025); Xiang et al. (2025); Aggarwal and Welleck (2025); Lee et al. (2025). For example, RL-based approaches Dai et al. (2025); Yuan et al. (2025); Xiang et al. (2025); Aggarwal and Welleck (2025) introduce length penalties to encourage shorter reasoning chains, though such methods can suffer from training instability. Beyond length control, latent reasoning methods Hao et al. (2024); Shen et al. (2025); Zhang et al. (2025); Cheng and Van Durme (2024); Xu et al. (2025) enable reasoning in continuous spaces, such as Coconut Hao et al. (2024) using hidden states as continuous thoughts, CODI Shen et al. (2025) distilling explicit CoT into continuous space via teacher-student alignment, and Soft Thinking Zhang et al. (2025) generating weighted concept tokens. However, these LLM techniques cannot directly transfer to VLA tasks due to the need for spatial-temporal understanding and bridging semantic reasoning with embodied control. Recently, ECoT-Lite Chen et al. (2025) proposes reasoning dropout to accelerate embodied reasoning by skipping test-time reasoning traces. However, reasoning dropout can lead to inconsistent planning as it builds on supervised embodied CoT. Our proposed Fast-ThinkAct distills reasoning into compact latent representations that naturally encode multimodal information, enabling robust reasoning-enhanced policy learning. 3. Method 3.1. Problem Formulation We first define the setting and notations. At each timestep ğ‘¡, given language instruction ğ‘™, the model observes visual input ğ‘œğ‘¡ and generates an action chunk ğ‘ğ‘¡, represented as sequence of continuous robot control vectors (e.g., 7or 14-DOF for singleor bimanual robots, respectively). To address this problem, we propose Fast-ThinkAct, an efficient reasoning framework that bridges high-level Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Figure 2: Overview of Fast-ThinkAct. (a) Given observation ğ‘œğ‘¡ and instruction ğ‘™, the Textual Teacher VLM generates explicit reasoning chains. The Latent Student VLM â„±ğœƒ distills these into compact latent tokens â„± ğ‘‡ ğœƒ guided by reward preferences. Verbalizer LLM ğ’±ğœ“ decodes latents to text for preference-based learning via â„’verb, while â„’distill transfers visual planning capability from teacher, and spatial tokens enable parallel visual trajectory prediction via â„’ans, ensuring latents are verbalizable and grounded in visual planning. (b) Reasoning-Enhanced Policy Learning. The Action Model ğœ‹ğœ‘ is trained with â„’IL while freezing the latent student â„±ğœƒ and state encoder. planning with low-level action execution. Our approach employs VLM â„±ğœƒ to perform reasoning in continuous latent space, integrated with an action model ğœ‹ğœ‘ for executable action generation. Specifically, â„±ğœƒ processes observation-instruction pairs (ğ‘œğ‘¡, ğ‘™) through latent chain-of-thought (CoT) reasoning to produce compact visual plan latent ğ‘ğ‘¡ that encapsulates the intended trajectory in visual space (Sec. 3.2). This ğ‘ğ‘¡ subsequently guides ğœ‹ğœ‘ to predict executable actions ğ‘ğ‘¡ (Sec. 3.3). By distilling reasoning into continuous latent space rather than discrete text, Fast-ThinkAct achieves significantly improved inference efficiency while enhancing action performance through better preservation of spatial and visual information. 3.2. Efficient Embodied Reasoning To enable efficient embodied reasoning that meets the real-time requirements of embodied AI tasks, we aim to compress long textual CoTs into compact set of continuous latent representations. However, compressing reasoning traces into latents is challenging, as there is no direct supervision signal in the latent space to guide what reasoning patterns should be encoded. 3.2.1. Verbalizable Latent CoT by Reward Preferences To address this challenge, we propose to perform distillation in natural language space by introducing verbalizer LLM that decodes latents into verbalizable reasoning. This approach grounds latent learning in an interpretable textual form, ensuring that the learned latents faithfully preserve the underlying reasoning exhibit varying quality, we adopt structure. Since reasoning traces generated by the teacher model â„± ğ‘‡ ğœƒ preference-based learning framework that exploits reward signals from the teachers GRPO training to guide the latent student â„±ğœƒ toward high-quality reasoning patterns while suppressing low-quality ones. Specifically, we employ teacher-student framework where textual teacher model â„± ğ‘‡ ğœƒ reasoning through GRPO Shao et al. (2024) training by maximizing: first learns explicit ğ’¥GRPO(ğœƒ) = ğœ â„± ğ‘‡ ğœƒ [ min (ğ‘Ÿğœƒ(ğœ )ğ´(ğœ ), clip(ğ‘Ÿğœƒ(ğœ ), 1 ğœ–, 1 + ğœ–)ğ´(ğœ ))] , (1) where ğœ denotes reasoning trace and ğ‘Ÿğœƒ(ğœ ) = â„± ğ‘‡ â„± ğ‘‡ ğœƒ (ğœ ) old(ğœ ) is the probability ratio. The advantage function for group 4 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning rewards {ğ‘…ğ‘–}ğ‘–ğº(ğœ ) is represented as: ğ´(ğœ ) = ğ‘…ğœ mean({ğ‘…ğ‘–}ğ‘–ğº(ğœ )) std({ğ‘…ğ‘–}ğ‘–ğº(ğœ )) . (2) This training process produces textual CoTs with varying quality, where the advantage function ğ´(ğœ ) naturally serves as quality indicator. To construct preference pairs for distillation, we select the highest and lowest advantage traces from each rollout group: ğœ + = arg max ğœ ğº ğ´(ğœ ) and ğœ = arg min ğœ ğº ğ´(ğœ ). (3) Instead of generating textual tokens, the student model â„±ğœƒ performs latent reasoning by autoregressively generating ğ‘€ continuous latent vectors = {ğ‘§ğ‘š}ğ‘€ with ğ‘§ğ‘š Rğ‘‘, where ğ‘‘ is the hidden size. We then train the verbalizer LLM ğ’±ğœ“ to decode these latents into natural language. The training objective encourages the verbalizer to assign higher likelihood to decoding latents into high-quality reasoning ğœ + than low-quality reasoning ğœ . Inspired by DPO Rafailov et al. (2023), we formulate this as an optimization guided by the reward preferences: ğ‘š= [ â„’verb = log ğœ ( ğ›½( log ğ‘ğœ“(ğœ +z) ğ‘ref(ğœ +) log ğ‘ğœ“(ğœ z) ğ‘ref(ğœ ) ))] , (4) where ğ‘ref is the reference model (i.e., ğ’±ğœ“ without latent conditioning), ğœ is the sigmoid function, and ğ›½ = 0.1 controls preference strength. This encourages the student VLM â„±ğœƒ to encode latents that the verbalizer decodes into high-quality reasoning while suppressing low-quality patterns. 3.2.2. Action-Aligned Visual Plan Distillation While the verbalizer loss (Eq. 4) enables the student â„±ğœƒ to capture high-level reasoning patterns, it does not explicitly ensure that latent representations encode the visual planning capability crucial for embodied control. To address this, we introduce action-aligned visual plan distillation to transfer the teacher â„± ğ‘‡ spatial ğœƒ reasoning ability to the student â„±ğœƒ. We distill spatial reasoning from the teacher, which is trained with trajectory-level rewards (e.g., goal completion and trajectory alignment Huang et al. (2025)) for grounded visual planning. We align the trajectory-level representations by minimizing the L2 distance between hidden states of the <answer> token that encodes the visual plan: â„’distill = â„ğ‘‡ ğ‘¡ â„ğ‘¡2 2, (5) where â„ğ‘‡ ğ‘¡ and â„ğ‘¡ are the hidden states from teacher (corresponding to ğœ +) and student, respectively. To enable efficient parallel trajectory prediction, unlike the textual teacher that autoregressively generates with ğ‘ğ‘˜ [0, 1]2 (tokenized into 60-70 tokens when ğ¾ = 5), the verbose text sequences of waypoints {ğ‘ğ‘˜}ğ¾ ğ‘˜=1 student uses ğ¾ learnable spatial tokens {sğ‘–}ğ¾ appended to the reasoning latent sequence, with each output ğ‘–=1 hidden state simultaneously projected to waypoint via an MLP. The total objective for training â„±ğœƒ combines all three components: â„’student = â„’verb + â„’distill + â„’ans, where â„’ans = ğ¾ ğ‘–=1 ğ‘ğ‘– Ë†ğ‘ğ‘–2 2, with ğ‘ğ‘– = MLP(â„(sğ‘–)), (6) where â„(sğ‘–) denotes the output hidden state of the ğ‘–-th spatial token and Ë†ğ‘ğ‘– are ground-truth waypoints. Through this unified framework, the student model â„±ğœƒ performs compact yet expressive latent reasoning and generates visual trajectory plans efficiently. 5 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Figure 3: Evaluation of robot manipulation and reasoning efficiency. (a)-(e) Success rates on LIBERO Liu et al. (2023) and SimplerEnv Li et al. (2024) benchmarks compared with state-of-the-art 7B reasoning VLAs. (f) Latency comparison across 3B and 7B reasoning VLAs. Our approach achieves up to 89.3% inference latency reduction while maintaining superior task success rates. 3.3. Reasoning-Enhanced Policy Learning After the student VLM â„±ğœƒ performs compact latent reasoning and generates visual trajectory planning through spatial tokens, we leverage these representations to guide diffusion Transformer-based action model ğœ‹ğœ‘ (e.g., RDT Liu et al. (2024)) for action prediction. To bridge the high-level visual planning with low-level action generation, we connect the visual latent planning ğ‘ğ‘¡ encoded in the key-value cache corresponding to the spatial tokens to the action model. Specifically, we extract visual latent planning ğ‘ğ‘¡ from the KV cache of spatial tokens in earlier VLM layers (since â„±ğœƒ has more layers than ğœ‹ğœ‘) and concatenate with KV pairs from the action models state encoder. The action models cross-attention then attends to both the visual planning context and state observations. We post-train on action-annotated robot data by freezing â„±ğœƒ and the state encoder while updating only ğœ‹ğœ‘ with the imitation learning objective: â„’IL(ğœ‘) = â„“ (ğœ‹ğœ‘(ğ‘œğ‘¡, ğ‘™, ğ‘ğ‘¡), Ë†ğ‘ğ‘¡) , (7) where â„“ denotes the denoising objective for diffusion policy and Ë†ğ‘ğ‘¡ is the ground-truth action. Through this posttraining, the action model effectively translates visual planning from compact latent reasoning into low-level robot actions. 3.4. Learning Strategy and Inference and student â„±ğœƒ from the same checkpoint obtained through Training Strategy. We initialize both teacher â„± ğ‘‡ ğœƒ SFT and CoT-SFT on pre-trained VLM. The teacher is trained with GRPO using action-aligned rewards Huang et al. (2025), while the student is trained with â„’student to compress reasoning into compact latents. We then connect the trained â„±ğœƒ with action model ğœ‹ğœ‘ (initialized from Liu et al. (2024)) by freezing â„±ğœƒ and the state encoder while updating the latent projector and ğœ‹ğœ‘ with â„’IL on large-scale robotic data. For target environment adaptation (e.g., LIBERO Liu et al. (2023), RoboTwin2.0 Chen et al. (2025)), we fine-tune on environment-specific demonstrations. Inference. The â„±ğœƒ processes (ğ‘œğ‘¡, ğ‘™) by compact latent reasoning, generating visual trajectories via ğ¾ spatial tokens. The visual latent planning ğ‘ğ‘¡, extracted from the spatial tokens KV cache, conditions ğœ‹ğœ‘ to predict Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning actions ğ‘ğ‘¡. Inference requires only â„±ğœƒ and ğœ‹ğœ‘; the verbalizer ğ’±ğœ“ is used solely during training and optionally for interpretability. 4. Experiment 4.1. Experimental Setup Implementation Details. We use Qwen2.5-VL 3B Bai et al. (2025) as the VLM backbone. The SFT stage runs for 1 epoch with batch size 64 and learning rate 1e5, followed by CoT-SFT for 15K iterations with the same hyperparameters. For teacher-student training, both â„± ğ‘‡ and â„±ğœƒ are initialized from the CoT-SFT checkpoint and trained for 4,500 ğœƒ iterations with batch size 128 and learning rate 1e6. The teacher is optimized with GRPO Shao et al. (2024) using action-aligned visual rewards Huang et al. (2025) and QA-style rewards (detailed in supplementary material). For the first 3,000 iterations of the student training, we train the verbalizer ğ’±ğœ“ with standard language modeling loss, then switch to â„’verb for the remaining 1,500 iterations. For reasoning-enhanced policy learning, we initialize ğœ‹ğœ‘ from DiT-Policy Chi et al. (2023) pre-trained on OXE ONeill et al. (2024) for SimplerEnv, and from RDT Liu et al. (2024) for LIBERO and RoboTwin2.0. linear projection adapts the VLMs KV cache to the action model dimension (1,024 for DiT-Policy and 2,048 for RDT). Training runs for 20K iterations with batch size 256 and learning rate 1e4. All diffusion hyperparameters follow those of the respective action models. All experiments are conducted on 16 NVIDIA A100 GPUs with 80 GB memory. Training Datasets and Evaluation Benchmarks. For reasoning VLM training, we utilize single-arm visual trajectories labeled by Lee et al. (2025) and dual-arm visual trajectories from the AIST dataset Motoda et al. (2025), along with QA tasks from PixMo Deitke et al. (2024), RoboFAC Lu et al. (2025), RoboVQA Sermanet et al. (2024), ShareRobot Ji et al. (2025), EgoPlan Chen et al. (2023), and Video-R1 Feng et al. (2025). For reasoning-enhanced policy learning, we use action data from the OXE dataset ONeill et al. (2024) (following OpenVLA Kim et al. (2024)) when training with DiT-Policy, and augment with bimanual data from the static Aloha dataset Shi et al. (2023); Zhao et al. (2023) when training with RDT. We evaluate Fast-ThinkAct on four embodied reasoning benchmarks and three robot manipulation benchmarks. For embodied reasoning, we use EgoPlan-Bench2 Qiu et al. (2024) (accuracy on multiple-choice questions), RoboVQA Sermanet et al. (2024) (BLEU score Papineni et al. (2002)), OpenEQA Majumdar et al. (2024), and RoboFAC Lu et al. (2025) (both using LLM-based scoring). Notably, RoboVQA and RoboFAC contain videos captured from real robots. For robot manipulation, we evaluate on SimplerEnv Li et al. (2024), which demonstrates strong correlation with real-world performance, LIBERO Liu et al. (2023) covering diverse manipulation tasks including long-horizon scenarios, and RoboTwin2.0 Chen et al. (2025) for complex bimanual manipulation. All robot manipulation tasks use task success rate as the metric. Additional details are provided in the supplementary material. 4.2. Quantitative Evaluation Robot Manipulation. We evaluate Fast-ThinkAct on robotic manipulation using LIBERO Liu et al. (2023) and SimplerEnv Li et al. (2024) benchmarks. LIBERO covers diverse subtasks, including Spatial, Object, Goal, and Long, while SimplerEnv provides simulated benchmark with strong real-world correlation, featuring variations in lighting, object appearance, and camera viewpoints. As shown in Fig. 3(a)-(e), Fast-ThinkAct consistently outperforms all baselines, achieving the highest success rates across all LIBERO subtasks and SimplerEnv-Google. This includes substantial improvements over foundation VLAs such as OpenVLA Kim et al. (2024), and reasoning VLAs including CoT-VLA Zhao et al. (2025), ThinkAct Huang et al. (2025), and MolmoAct Lee et al. (2025). Moreover, as shown in Fig. 3(f), our compact latent reasoning achieves 89.3% and 88.0% latency reduction 7 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Table 1: Quantitative evaluation on RoboTwin2.0 Chen et al. (2025). and denote easy and hard settings (without/with domain randomization). Background colors indicate task length based on expert demonstrations: short (80-100) , medium (110-220) , long (270-470) steps. Model click alarm click bell turn switch adjust bottle beat block handover mic handover block hanging mug stack blocks two stack bowls three Average DP Chi et al. (2023) ACT Zhao et al. (2023) ğœ‹0 Black et al. (2024) RDT Liu et al. (2024) ThinkAct Huang et al. (2025) Fast-ThinkAct 61 32 63 61 64 5 4 11 12 13 17 54 58 44 80 84 0 3 3 9 11 12 36 5 27 35 40 1 2 23 15 19 21 97 97 90 81 94 0 23 56 75 70 72 42 56 43 77 79 0 3 21 37 33 33 53 85 98 90 92 0 0 13 31 40 42 10 42 45 45 56 0 0 8 14 15 15 8 7 11 23 31 0 0 3 16 18 22 7 25 42 21 30 0 0 1 2 5 5 63 48 66 51 54 0 0 24 17 23 25 43.1 45.5 52.9 56.4 62.4 0.6 3.5 16.3 22.8 24.7 65.7 26.4 Table 2: Quantitative evaluation on EgoPlan-Bench2 Qiu et al. (2024), RoboVQA Sermanet et al. (2024), and OpenEQA Majumdar et al. (2024) benchmarks for embodied reasoning. Method EgoPlan-Bench RoboVQA OpenEQA Overall Daily. Work. Rec. Hobbies Avg. BB-2 B-3 B-4 B-Avg. Score GPT-4V Achiam et al. (2023) Gemini-2.5-Flash Comanici et al. (2025) InternVL2.5-2B Chen et al. (2024) InternVL3-2B Zhu et al. (2025) NVILA-2B Liu et al. (2024) Qwen2.5-VL-3B Bai et al. (2025) Magma-8B Yang et al. (2025) RoboBrain2.0-3B Team et al. (2025) ThinkAct-3B Huang et al. (2025) 36.7 44.2 30.9 36.9 34.6 29.0 32.1 45.3 46.6 27.7 42.3 27.8 29.9 26.7 27.0 25.7 37.6 41.4 33.9 43. 28.6 35.6 33.3 30.2 34.4 45.9 45.9 32.5 39.1 33.1 31.5 31.6 28.9 29.3 39.7 42.5 32.6 42.4 30.1 33.4 31.4 28.5 29.8 41.8 44.0 32.2 39. 36.6 34.4 38.7 42.5 38.6 54.4 62.4 26.5 31.6 33.7 33.9 34.3 36.3 31.5 47.7 57.3 24.7 22.9 31.0 33.5 31.1 28.7 28.1 43.1 52.0 23.9 22. 29.4 33.3 29.2 31.8 26.7 41.0 49.6 26.8 28.9 32.7 33.8 33.3 34.8 31.2 46.5 55.3 49.6 45.3 47.1 48.8 47.0 43.4 49.1 50.1 48.9 Fast-ThinkAct-3B 50.3 44.3 46.4 43.2 46.4 70. 63.0 57.2 53.0 60.8 51.2 Avg. 36.4 38.9 36.6 38.7 37.2 35.6 36.7 46.1 49.4 52.8 compared to ThinkAct-7B Huang et al. (2025) and MolmoAct-7B Lee et al. (2025) respectively, and 7 faster inference than ThinkAct-3B, demonstrating substantial efficiency gains without sacrificing performance. To further validate Fast-ThinkAct on more complex scenarios, we evaluate on RoboTwin2.0 Chen et al. (2025), challenging bimanual manipulation benchmark requiring long-horizon planning. As shown in Tab. 1, FastThinkAct significantly outperforms previous VLAs including DP Chi et al. (2023), ACT Zhao et al. (2023), ğœ‹0 Black et al. (2024), RDT Liu et al. (2024), and ThinkAct Huang et al. (2025) across both easy and hard settings. Compared to RDT, Fast-ThinkAct achieves 9.3% and 3.6% higher success rates on easy and hard settings, respectively. Against the reasoning VLA ThinkAct, it improves success by 3.3% and 1.7% while maintaining substantially higher efficiency, as shown in Fig. 3(f). These results demonstrate that our compact reasoning design enables both superior accuracy and computational efficiency on complex bimanual manipulation tasks. Embodied Reasoning. In Tab. 2, we evaluate the reasoning capabilities of Fast-ThinkAct in embodied scenarios across three benchmarks: EgoPlan-Bench2 Qiu et al. (2024), RoboVQA Sermanet et al. (2024), and OpenEQA Majumdar et al. (2024). These benchmarks assess multi-step planning in egocentric everyday scenarios, long-horizon reasoning for robotic manipulation tasks, and zero-shot understanding of embodied scenes in diverse environments, respectively. We observed that, Fast-ThinkAct surpasses all comparison methods, including two proprietary models (i.e., GPT-4V Achiam et al. (2023) and Gemini-2.5-Flash Comanici et al. (2025)), exceeding the runner-up by 2.4% on EgoPlan-Bench2, 5.5 BLEU score on RoboVQA, and 1.1 points on OpenEQA. These results demonstrate that Fast-ThinkAct effectively handles complex planning sequences and extended reasoning horizons while generalizing to novel environments, showcasing robust capabilities for scene comprehension and multi-step task execution in embodied AI applications. 4.3. Analysis of Fast-ThinkAct Reasoning Enables Long-Horizon Planning. 8 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Figure 4: Visualization of predicted visual trajectories and action execution results on long-horizon tasks. Examples from (a) SimplerEnv-Google, (b) LIBERO-Long, and (c) RoboTwin2.0-Hard with long (278) steps. Yellow traces indicate single-arm/left gripper trajectories; red traces indicate right gripper trajectories for bimanual tasks. Figure 5: Failure recovery capability on RoboFAC Lu et al. (2025). Left: Qualitative examples (from both simulation and real robot) of corrective guidance for manipulation errors. Right: Quantitative evaluation on simulation (RoboFAC-Sim) and real-robot (RoboFAC-Real) settings. We analyze Fast-ThinkActs capability on long-horizon tasks in Tab. 1 and Fig. 4. We focus on longhorizon tasks (average length exceeding 270 steps) in RoboTwin2.0 Chen et al. (2025) that require multistep reasoning and extended planning horizons. As shown in Tab. 1, Fast-ThinkAct achieves average scores of 48.8 and 16.8 on easy and hard settings of longhorizon tasks, respectively, surpassing RDT (35.0/12.3) and ThinkAct (42.8/15.3). Fig. 4 visualizes predicted 2D visual traces and execution results on representative tasks from SimplerEnv-Google Li et al. (2024), LIBEROLong Liu et al. (2023), and RoboTwin2.0 Chen et al. (2025). For example, the LIBERO-Long task requires sequentially turning on the stove and placing moka pot on it, while the RoboTwin2.0 handover task requires bimanual coordination to transfer block between grippers. The visual traces successfully predict feasible solution paths, with their corresponding representations serving as visual planning guidance for successful execution. These results demonstrate that our compact latent reasoning effectively supports long-horizon Figure 6: Few-shot adaptation results on RoboTwin2.0 benchmark. We use 10 demonstrations per task for fine-tuning. 9 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning planning in complex manipulation scenarios. Reasoning Enables Failure Recovery. key advantage of reasoning-based VLAs Huang et al. (2025); Abdolmaleki et al. (2025) is their ability to identify runtime failures and provide corrective guidance for recovery. To evaluate this capability, we conduct experiments on RoboFAC Lu et al. (2025), benchmark specifically designed to assess failure identification and correction in embodied VLMs. As shown in Fig. 5, Fast-ThinkAct substantially outperforms the second-best baseline RoboFAC3B Lu et al. (2025) by 10.9 points on the simulation split and 16.4 points on the real-world split. The qualitative examples demonstrate Fast-ThinkActs ability to reason over manipulation videos, identify failures, and propose recovery steps. For instance, in the right example where the target object drops mid-execution, Fast-ThinkAct generates concrete recovery plan: first moving the arm backward to create space, then adjusting laterally to align with the target object, and finally lowering to the appropriate height for secure grasp. These results demonstrate that our latent reasoning supports both fast task execution and crucial failure analysis capabilities essential for robust robotic manipulation. Figure 7: Reasoning trace comparison on RoboVQA. (a) Teachers textual reasoning. (b) Students verbalized latent reasoning. Green: relevant content; orange: less relevant content. Table 3: Ablation study of training objectives and learning stages. Note that Fast-ThinkAct w/o â„’verb, â„’distill denotes the student VLM â„±ğœƒ trained without the corresponding loss components. Reasoning Enables Few-Shot Adaptation. To assess how reasoning capability improves few-shot adaptation, we conduct few-shot experiments on the RoboTwin2.0 benchmark Chen et al. (2025), fine-tuning models using only 10 demonstrations per task. As illustrated in Fig. 6, Fast-ThinkAct significantly enhances our adopted action model RDT Liu et al. (2024) and outperforms the state-of-theart VLAs, including ğœ‹0 Black et al. (2024) and ThinkAct Huang et al. (2025) on both medium and long-horizon tasks. Notably, our method achieves these gains while operating with significantly lower reasoning latency compared to ThinkAct, highlighting the advantage of efficient yet effective reasoning for few-shot action adaptation in complex robot manipulation scenarios. Textual Teacher â„± ğ‘‡ ğœƒ SFT + CoT-SFT SFT only EgoPlan RoboVQA OpenEQA Average w/o â„’verb w/o â„’verb, â„’distill 58.2 46.1 53.6 49.8 45.0 46.5 49.4 48.8 45.3 41.7 40.0 40.5 Fast-ThinkAct 53.8 52. 48.5 47.7 42.1 41.6 49.5 48.9 Method 60.8 52. 46.4 51.2 Visualization of Verbalizable Latent Reasoning. In Fig. 7, we compare the teachers textual reasoning with the students verbalized latent reasoning on RoboVQA. While both capture task-relevant information (green), the teacher generates verbose outputs with less directly relevant content (orange), whereas our student produces more concise and focused responses when verbalized. This demonstrates that our preference-guided distillation not only reduces computational cost but also distills concise reasoning patterns while filtering out redundant information. 10 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Ablation Study. In Tab. 3, we ablate training stages and loss components. Starting from the full Fast-ThinkAct, removing â„’verb causes performance drops as latent CoTs lack preference-based guidance to align with high-quality reasoning and suppress low-quality patterns. Further removing â„’distill leads to additional decline, indicating that aligning trajectory-level representations is crucial for transferring visual planning capabilities. Comparing training strategies, CoT-SFT underperforms SFT on EgoPlan-Bench2 and RoboVQA but improves on OpenEQA, suggesting naÃ¯ve chain-of-thought supervision benefits open-ended QA but introduces verbosity that hinders structured reasoning tasks. Our preference-guided approach distills high-quality reasoning while maintaining efficiency. This validates the necessity of our proposed distillation framework and visual trajectory alignment. We provide additional ablation studies in the supplementary material. 5. Conclusion We presented Fast-ThinkAct, an efficient reasoning framework for vision-language-action tasks that achieves compact yet expressive planning through verbalizable latent reasoning. By distilling lengthy textual reasoning into compact latent representations via preference-guided distillation and visual trajectory alignment, our approach bridges high-level embodied reasoning with low-level action execution through reasoning-enhanced policy learning. Extensive experiments across diverse robotic manipulation and embodied reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with significantly reduced inference latency while enabling effective long-horizon planning, few-shot adaptation, and failure recovery capabilities. Limitations and Future Works. As our verbalizer ğ’±ğœ“ is built upon pre-trained LLM, it inevitably inherits language model limitations, including hallucination, occasionally producing plausible but inaccurate descriptions. However, this does not affect action execution during inference, as the verbalizer serves only for interpretability while action prediction uses the grounded latent representations from visual plan distillation. To further improve the faithfulness of verbalized reasoning, we can consider incorporating grounding-aware objectives or hallucination suppression techniques in future work. 11 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning A. Additional Experimental Setup A.1. Algorithm Algorithm 1: Training Fast-ThinkAct (Sec. 3.2) Input: CoT-SFT checkpoint â„±ğœƒ0 , training data ğ’Ÿ, rollout size ğ‘ , latent reasoning steps ğ‘€ , number of waypoints ğ¾, total iterations ğ‘‡total ğœƒ â„±ğœƒ0 Output: Trained student model â„±ğœƒ // Initialize models ; â„± ğ‘‡ Initialize verbalizer ğ’±ğœ“ from pre-trained LLM; ğ‘¡ 0; while ğ‘¡ < ğ‘‡total do , â„±ğœƒ â„±ğœƒ Sample batch (ğ‘œ, ğ‘™, Ë†ğ‘) from ğ’Ÿ; // Suppose bs=1 for simplicity ğ‘–=1 from â„± ğ‘‡ ğœƒ (ğ‘œ, ğ‘™); ; // Teacher GRPO training Generate ğ‘ rollouts {ğœğ‘–}ğ‘ Compute trajectory rewards {ğ‘Ÿğ‘–}ğ‘ Compute group-wise advantages {ğ´ğ‘–}ğ‘ Update â„± ğ‘‡ with ğ’¥GRPO (Eq. 1); ğœƒ ğœ + arg maxğ‘– ğ´ğ‘–, ğœ arg minğ‘– ğ´ğ‘– (Eq. 3) ; ğ‘¡ hidden state of ğœ + at <answer> token from â„± ğ‘‡ â„ğ‘‡ ğœƒ ğ‘–=1 ğ‘–= ; ; // For student distillation // For distillation loss // Student latent distillation ğ‘š=1 â„±ğœƒ(ğ‘œ, ğ‘™) ; = {ğ‘§ğ‘š}ğ‘€ Compute â„’verb with z, ğ’±ğœ“, ğœ +, ğœ (Eq. 4); Forward ğ¾ spatial tokens from â„±ğœƒ(ğ‘œ, ğ‘™, z) to obtain â„ğ‘¡ and {â„(sğ‘–)}ğ¾ ğ‘–=1 Compute â„’distill with â„ğ‘‡ ğ‘¡ Compute â„’ans with {â„(sğ‘–)}ğ¾ ğ‘–=1 Update â„±ğœƒ with â„’student = â„’verb + â„’distill + â„’ans; ğ‘¡ ğ‘¡ + 1; , â„ğ‘¡ (Eq. 5); , Ë†ğ‘ (Eq. 6); ; // Perform auto-regressive latent reasoning end return â„±ğœƒ; Algorithm 1 presents the complete training procedure corresponding to Sec. 3.2. It shows how we jointly optimize the teacher model with GRPO and distill its reasoning into the students compact latent representations. A.2. Implementation Details Our implementation follows the setup described in Sec. 4.1 of the main paper. Here we provide additional details. The verbalizer ğ’±ğœ“ is initialized from small LLM, Qwen3-0.6B, with cross-attention layers inserted at each layer to condition on latent CoTs z. For the student model training, in the first 3,000 iterations, we replace verbalization loss â„’verb with language modeling loss using ğœ + as ground truth to warm up ğ’±ğœ“s alignment with the latent representations z. We then freeze ğ’±ğœ“ and use the â„’verb for the remaining 1,500 iterations. The student â„±ğœƒ is optimized throughout both phases. For waypoint prediction in Eq. 6, each ğ‘ğ‘– R6 encodes coordinates in the format [ğ‘¥single, ğ‘¦single, ğ‘¥left, ğ‘¦left, ğ‘¥right, ğ‘¦right], where the first two dimensions are for single-arm and the last four are for bimanual robot. For ground-truth Ë†ğ‘ğ‘–, we fill the corresponding dimensions based on robot type and mask out the unused dimensions when computing â„’ans. For GRPO training, we follow the configuration of ThinkAct Huang et al. (2025), using rollout size ğ‘ = 5. Following Lee et al. (2025), we set the number of waypoints in trajectory to ğ¾ = 5. We use ğ‘€ = 6 latent reasoning tokens, with ablation study provided in Fig. 8. Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning During reasoning-enhanced policy learning, for SimplerEnv Li et al. (2024) evaluation, to ensure fair comparison with previous works Kim et al. (2024); Lee et al. (2025), we initialize ğœ‹ğœ‘ from DiT-Policy Chi et al. (2023) pre-trained on the same OXE dataset ONeill et al. (2024); Kim et al. (2024) and conduct reasoning-enhanced policy learning (Sec. 3.3) using the same OXE data. For LIBERO Liu et al. (2023) and RoboTwin2.0 Chen et al. (2025) evaluations, we initialize ğœ‹ğœ‘ from RDT Liu et al. (2024), which has demonstrated strong performance on RoboTwin2.0, and conduct policy learning using OXE ONeill et al. (2024) and static ALOHA datasets Shi et al. (2023); Zhao et al. (2023). Our method further enhances RDTs manipulation capabilities on both benchmarks. The use of different action models also demonstrates that our approach is agnostic to the underlying action model choice. A.3. Training Data Details A.3.1. Dataset Sources 2D Visual Trace of Manipulation Tasks. For single-arm manipulation, we utilize 2D visual trajectories labeled by MolmoAct Lee et al. (2025) from the Open X-Embodiment (OXE) dataset ONeill et al. (2024), comprising approximately 1.3M trajectories. For bimanual manipulation, we extract dual-arm visual trajectories from the AIST dataset Motoda et al. (2025), resulting in approximately 92K trajectory samples. Specifically, we first use Molmo-72B Deitke et al. (2024) to detect left and right gripper positions (following Lee et al. (2025)) in the first frame, then apply CoTracker3 Karaev et al. (2025) to track and parse the manipulation trajectories throughout the video sequences. RoboFAC Lu et al. (2025). RoboFAC is robotic failure analysis dataset containing 9,440 erroneous manipulation trajectories across 16 tasks in both simulated and real-world environments. We utilize the training set with 64K QA pairs covering various failure types for developing failure identification and correction planning capabilities. RoboVQA Sermanet et al. (2024). RoboVQA contains robot manipulation videos with QA tasks covering task understanding. The dataset includes approximately 5K long-horizon and 92K medium-horizon video sequences from diverse robotic platforms, resulting in total 798K QA pairs. Videos are annotated with multiple questions probing spatial reasoning, action prediction, and task comprehension. ShareRobot Ji et al. (2025). ShareRobot is large-scale dataset collected by RoboBrain Ji et al. (2025), containing over 1M QA pairs covering task planning, object affordances, and manipulation strategies across diverse robot embodiments and scenes. The dataset features fine-grained annotations linking task descriptions to frame-level execution details, facilitating learning of transferable manipulation knowledge. EgoPlan-Bench Chen et al. (2023). EgoPlan-Bench features egocentric videos of daily activities annotated with task planning information including goals, execution history, and current states. The dataset contains approximately 53K video-text pairs for training long-horizon planning and progress tracking capabilities from egocentric view. Video-R1-CoT Feng et al. (2025). Video-R1 comprises 165K video question-answer pairs with chain-of-thought reasoning annotations generated by large-scale vision-language models. The dataset covers diverse reasoning domains including mathematical logic, spatial understanding, OCR, and visual analytics. All samples are quality-filtered to ensure annotation consistency and correctness. PixMo Deitke et al. (2024). PixMo is general-purpose vision-language dataset with diverse image captions and question-answer pairs. Following MolmoAct Lee et al. (2025), we incorporate PixMo dataset to preserve general visual understanding Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Table 4: Quantitative results with larger model size (7B or 8B) on embodied reasoning benchmarks. Method EgoPlan-Bench2 RoboVQA OpenEQA Overall Daily. Work. Rec. Hobbies Avg. B-1 B-2 BB-4 B-Avg. Score InternVL2.5-8B Chen et al. (2024) InternVL3-8B Zhu et al. (2025) NVILA-8B Liu et al. (2024) Qwen2.5-VL-7B Bai et al. (2025) Magma-8B Yang et al. (2025) RoboBrain2.0-7B Team et al. (2025) ThinkAct-7B Huang et al. (2025) Fast-ThinkAct-7B 36.2 38.5 35.8 31.4 32.1 39.4 50. 51.3 28.7 32.9 28.7 26.7 25.7 27.0 49.8 47.3 34.4 36.1 37.2 29.5 34.4 33.9 44.8 41.5 35.4 37.2 35.4 28.6 29.3 32.2 45. 45.9 33.5 36.2 33.7 29.1 29.8 33.2 48.2 40.5 44.3 42.7 47.8 38.6 44.9 69.1 33.3 36.5 39.7 41.2 31.5 38.2 61.8 29.6 31.6 37.6 36.2 28.1 34.7 56.0 27.5 28.9 36.1 33.7 26.7 33.5 52. 47.5 70.4 63.3 57.3 53.2 32.7 35.3 39.0 39.7 31.2 37.8 59. 61.1 54.4 55.5 54.0 50.8 49.1 51.1 56.2 59.0 Avg. 40.2 42.3 42.2 39.9 36.7 40.7 54.7 55. Table 5: Results on LIBERO and SimplerEnv benchmarks with additional ThinkAct-3B comparison. Method LIBERO SimplerEnv-Google Latency () OpenVLA-7B Kim et al. (2024) CoT-VLA-7B Zhao et al. (2025) ThinkAct-7B Huang et al. (2025) MolmoAct-7B Lee et al. (2025) ThinkAct-3B Huang et al. (2025) Fast-ThinkAct-3B 76.5 83.9 84.4 86.8 83.1 89.7 40.2 N/A 68.3 64.9 64.7 68.7 N/A N/A 7513 6723 5674 805 (7.0) and prevent catastrophic forgetting when training on embodied dataset. Specifically, we use approximately 726K samples from the ask_model_anything, cap, and cap-qa splits. A.3.2. Data Processing and Formatting Supervised Fine-Tuning (SFT). To enhance foundational embodied knowledge, we perform supervised fine-tuning on approximately 4M samples combining 2D visual trajectories from MolmoAct Lee et al. (2025) and AIST Motoda et al. (2025), along with QA data from PixMo Deitke et al. (2024), RoboFAC Lu et al. (2025), RoboVQA Sermanet et al. (2024), ShareRobot Ji et al. (2025), and EgoPlan Chen et al. (2023). This stage enables the model to acquire basic visual understanding, task comprehension, and manipulation knowledge across diverse embodiments and scenarios. Chain-of-Thought SFT (CoT-SFT). To develop reasoning capabilities while preserving embodied understanding, we sample 5% from the SFT data (approximately 200K samples) and augment with 165K samples from Video-R1-CoT Feng et al. (2025). For data with CoT annotations, we format prompts to elicit structured reasoning enclosed in <think> tags followed by answers in <answer> tags; for data without CoT annotations, we prompt for direct answers only. This enables the model to learn reasoning capabilities from CoT-annotated data and generalize them to embodied tasks. Teacher-Student Training. Building upon the CoT-SFT checkpoint, we curate balanced training set by sampling approximately 5,000 instances from each dataset and data type, totaling nearly 50K samples. We adopt the prompt formatting strategy from CoT-SFT for both teacher GRPO training and student latent distillation. We train both the teacher with GRPO and the student with latent distillation (as detailed in Sec. 3.2) on this data, efficiently transferring high-quality reasoning patterns into compact latent representations. Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Table 6: Comparison with efficient textual reasoning methods. Method Textual Teacher â„± ğ‘‡ ğœƒ â„± ğ‘‡ ğœƒ â„± ğ‘‡ ğœƒ â„± ğ‘‡ ğœƒ Fast-ThinkAct-3B Inference w/o thinking Inference w/ 6 textual tokens w/ RL Length-Penalty Arora and Zanette (2025) EgoPlanBench2 41.7 42.7 39.3 41. 46.4 RoboVQA OpenEQA Average 58.2 55.0 53.0 57.5 60.8 49.4 41.7 46.5 44.7 52. 49.8 46.5 46.3 47.8 53.3 Figure 8: Visualization of predicted visual trajectories and action execution results on RoboTwin2.0. Yellow traces indicate left gripper trajectories; red traces indicate right gripper trajectories for bimanual tasks. A.4. Evaluation Setup A.4.1. Embodied Reasoning Benchmarks We evaluate on three benchmarks assessing different aspects of embodied reasoning. EgoPlan-Bench2 Chen et al. (2023) tests egocentric task planning across 24 daily-life scenarios with 1,321 multiple-choice questions, measuring accuracy in predicting next steps given task goals and progress history. RoboVQA Sermanet et al. (2024) evaluates visual reasoning in manipulation contexts through 1,893 free-form QA pairs from robot and human demonstrations, assessed via BLEU score. OpenEQA Majumdar et al. (2024) assesses spatial and functional understanding through 1,600+ questions spanning 180+ real-world environments, evaluated using LLM-based scoring aligned with human preferences. These benchmarks comprehensively evaluate embodied reasoning capability across planning, manipulation, and spatial understanding. A.4.2. Robotic Manipulation Benchmarks We evaluate on three simulation benchmarks covering diverse manipulation scenarios. SimplerEnv Li et al. (2024) provides manipulation tasks with strong sim-to-real correlation, featuring diverse visual variations in lighting, textures, backgrounds, and camera poses. Following MolmoAct Lee et al. (2025), we evaluate on the Google Robot tasks using the standard protocol Kim et al. (2024); Lee et al. (2025) of directly evaluating on SimplerEnv after training on OXE. LIBERO Liu et al. (2023) targets different generalization challenges through four task suites: spatial layout variation (LIBERO-Spatial), object diversity (LIBERO-Object), goal variation (LIBERO-Goal), and long-horizon planning with mixed variations (LIBERO-Long). We evaluate each suite over 500 trials using 3 random seeds following prior works Kim et al. (2024); Lee et al. (2025). RoboTwin2.0 Chen et al. (2025) features challenging bimanual manipulation with easy and hard difficulty settings, where the hard setting introduces domain randomization including clutter, lighting variations, diverse textures, and height changes. Following the original protocol, we train on 50 clean expert demonstrations per task and evaluate with 100 rollouts under both settings. We assess 10 tasks categorized into short, medium, and long horizons based on demonstration lengths. 15 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Figure 9: Failure identification and analysis capabilities on RoboFAC Lu et al. (2025). Top row shows identification of failure types and execution stages. Bottom row demonstrates failure root cause analysis. B. Additional Experiment Results B.1. Additional Quantitative Results Results of Larger Model Size. To demonstrate the scalability of our approach, we apply Fast-ThinkAct to larger backbone, Qwen2.5-VL-7B, and evaluate its performance on embodied reasoning benchmarks. As shown in Tab. 4, Fast-ThinkAct consistently achieves strong performance across EgoPlan-Bench2 Qiu et al. (2024), RoboVQA Sermanet et al. (2024), and OpenEQA Majumdar et al. (2024), validating that our latent reasoning distillation method effectively scales to larger model backbones. Performance Comparison with ThinkAct-3B. Tab. 5 presents detailed numerical results corresponding to Fig. 3 with additional ThinkAct-3B results. At the same 3B model size, Fast-ThinkAct achieves notable performance gains (89.7 vs. 83.1 on LIBERO, 68.7 vs. 64.7 on SimplerEnv-Google) while dramatically improving efficiency with 7 faster inference (805ms vs. 5674ms). Comparison with Efficient Reasoning Baselines. Table 6 compares our method with efficient textual reasoning alternatives applied to the textual teacher â„± ğ‘‡ . ğœƒ We evaluate three baselines: removing reasoning during inference entirely (0 tokens), constraining the teacher to generate only 6 textual tokens during inference, and applying RL training with length penalty Arora and Zanette (2025) to encourage concise reasoning (50 tokens). These achieve 46.5, 46.3, and 47.8 respectively, all degrading from the teachers 49.8. In contrast, Fast-ThinkAct uses only 6 latent tokens and achieves 53.3, demonstrating superior efficiency and performance. 16 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Figure 10: Reasoning trace comparison on OpenEQA. (a) Teachers textual reasoning. (b) Students verbalized latent reasoning. Green: reasonable reasoning trace; red: incorrect trace. B.2. Additional Qualitative Results Qualitative Robot Execution. We provide qualitative robot execution comparisons between the base action model RDT Liu et al. (2024) and Fast-ThinkAct in the supplementary video Fast-ThinkAct.mp4. Our method shows substantial improvements on challenging robotic execution tasks, where reasoning capabilities provide better spatial understanding and coordination for successful manipulation. Bimanual Manipulation Results. In Fig. 8, we present visualized trajectories and execution results for hanging mug and handover mic tasks under easy and hard settings in RoboTwin2.0 Chen et al. (2025). The hard setting includes different backgrounds and distractor objects. These examples show successful bimanual coordination where predicted waypoints accurately guide both grippers through the manipulation sequence, demonstrating Fast-ThinkActs spatial reasoning ability across varied visual conditions. 17 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning Table 7: Additional ablation study of training objectives and learning stages on robot manipulation benchmarks. Method LIBERO SimplerEnv Google RoboTwin2.0 Average Fast-ThinkAct w/o â„’verb w/o â„’verb, â„’distill Textual Teacher SFT + CoT-SFT SFT only 89.7 88.6 86.3 88.5 87.2 86. 68.7 67.3 65.7 67.3 65.8 64.5 46.1 44.9 42.6 45.8 43.3 42.8 68.2 66.9 64.9 67.2 65.4 64. Table 8: Ablation of Latent Reasoning Steps ğ‘€ . Failure Identification and Recovery. In Fig. 9, we demonstrate Fast-ThinkActs failure identification and analysis capabilities, complementing the recovery planning shown in the main paper. The top row shows that Fast-ThinkAct identifies failure types (e.g., position deviation) and execution stages (e.g., reaching for the cube). The bottom row illustrates root cause analysis, for instance, in the bottom-right example, the model correctly infers that the failure to push the cube with an L-shaped tool stems from an improper initial grasp. These results demonstrate Fast-ThinkActs comprehensive understanding of manipulation failures beyond recovery planning. Verbalized Latent Reasoning. Fig. 10 visualizes teacher textual reasoning and student verbalized reasoning. While the student generates compact and correct (green) reasoning, the teachers lengthy output sometimes contains erroneous steps (red) that might degrade the performance. B.3. Additional Ablation Study and Analysis Additional Ablation Results on Manipulation Benchmarks. Table 7 shows ablation results on LIBERO Liu et al. (2023), SimplerEnv-Google Li et al. (2024), and RoboTwin2.0 Chen et al. (2025). Removing â„’verb or â„’distill progressively degrades performance, confirming their contributions. Our full model consistently outperforms the textual teacher and models without teacher-student training (CoT-SFT, SFT only), demonstrating the benefits of compact latent reasoning distillation. Ablation Study on Action Model Conditioning. In Sec. 3.3, we extract visual latent planning ğ‘ğ‘¡ from early-layer KV cache of spatial tokens to condition the action model. We compare this against using late-layer KV cache (last ğ‘ layers, where ğ‘ is the action model depth) and directly using spatial tokens output hidden states. Our approach achieves 89.7 on LIBERO, outperforming late-layer KV at 88.3 and output hidden states at 87.1, demonstrating that early-layer representations better capture visual planning information for action prediction. Therefore, we adopt early-layer KV conditioning as our default configuration. Ablation Study on Latent Reasoning Steps. In Fig. 8, we study the effect of latent reasoning steps ğ‘€ . We observe that too few steps (ğ‘€ = 1) limit reasoning capacity, while excessive steps (ğ‘€ = 30, 100) might introduce redundant or noisy information. Therefore, we adopt ğ‘€ = 6, which achieves optimal performance, as our default. 18 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning References [1] Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, et al. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer. arXiv preprint arXiv:2510.03342, 2025. 3, 10 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 8 [3] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. 3 [4] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. 15, 16 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 7, 8, 14 [6] Johan Bjorck, Fernando CastaÃ±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. 1, 3 [7] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. ğœ‹0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1, 3, 8, [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 1, 3 [9] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 1, 3 [10] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. 3 [11] Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, et al. Eagle 2.5: Boosting long-context post-training for frontier vision-language models. arXiv preprint arXiv:2504.15271, 2025. 1 [12] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. 6, 7, 8, 9, 10, 13, 15, 17, 18 [13] William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, and Sergey Levine. Training strategies for efficient embodied reasoning. arXiv preprint arXiv:2505.08243, 2025. 2, [14] Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv:2312.06722, 2023. 7, 13, 14, 15 19 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning [15] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 8, 14 [16] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. [17] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. 7, 8, 13 [18] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1, 8 [19] Muzhi Dai, Shixuan Liu, and Qingyi Si. Stable reinforcement learning for efficient reasoning. arXiv preprint arXiv:2505.18086, 2025. 3 [20] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pages arXiv2409, 2024. 7, 13, 14 [21] Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, et al. Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. [22] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 7, 13, 14 [23] Weifan Guan, Qinghao Hu, Aosheng Li, and Jian Cheng. Efficient vision-language-action models for embodied manipulation: systematic survey. arXiv preprint arXiv:2510.17111, 2025. 2 [24] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 3 [25] Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. Thinkact: Vision-language-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. 1, 2, 3, 5, 6, 7, 8, 10, 12, 14 [26] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17241734, 2025. 7, 13, [27] Nikita Karaev, Yuri Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 60136022, 2025. 13 [28] Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, and Younggyo Seo. Robot-r1: Reinforcement learning for enhanced embodied reasoning in robotics. arXiv preprint arXiv:2506.00070, 2025. 2, 3 20 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning [29] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1, 3, 7, 13, 14, 15 [30] Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, and Yueh-Hua Wu. Unified reinforcement and imitation learning for vision-language models. arXiv preprint arXiv:2510.19307, 2025. [31] Byung-Kwan Lee, Ryo Hachiuma, Yu-Chiang Frank Wang, Yong Man Ro, and Yueh-Hua Wu. Vlsi: In Proceedings of the Verbalized layers-to-interactions from large to small vision language models. Computer Vision and Pattern Recognition Conference, pages 2954529557, 2025. 3 [32] Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. 2, 3, 7, 8, 12, 13, 14, 15 [33] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 6, 7, 9, 13, 15, 18 [34] Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, et al. Hamster: Hierarchical action models for open-world robot manipulation. arXiv preprint arXiv:2502.05485, 2025. 1, 3 [35] Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, et al. Eagle 2: Building post-training data strategies from scratch for frontier vision-language models. arXiv preprint arXiv:2501.14818, 2025. [36] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. 6, 7, 9, 13, 15, 18 [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1 [38] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. 6, 7, 8, 10, 13, 17 [39] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. 1, 8, 14 [40] Weifeng Lu, Minghao Ye, Zewei Ye, Ruihan Tao, Shuo Yang, and Bo Zhao. Robofac: comprehensive framework for robotic failure analysis and correction. arXiv preprint arXiv:2505.12224, 2025. 7, 9, 10, 13, 14, [41] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In CVPR, pages 1648816498, 2024. 7, 8, 15, 16 21 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning [42] Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, and Zhiding Yu. Argus: Vision-centric reasoning with grounded chain-of-thought. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1426814280, 2025. 3 [43] Tomohiro Motoda, Masaki Murooka, Ryoichi Nakajo, Muhammad A. Muttaqien, Koshi Makihara, Hanbit Oh, Keisuke Shirai, Floris Erich, Ryo Hanai, and Yukiyasu Domae. Aist-bimanual manipulation, 2025. 7, 13, 14 [44] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. 1, 7, [45] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 7 [46] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. 3 [47] Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, and Xihui Liu. Egoplan-bench2: benchmark for multimodal large language model planning in real-world scenarios. arXiv preprint arXiv:2412.04447, 2024. 7, 8, 16 [48] Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, et al. Eo-1: Interleaved vision-text-action pretraining for general robot control. arXiv preprint arXiv:2508.21112, 2025. 2, 3 [49] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 5 [50] Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael Tarr, Aviral Kumar, and Katerina Fragkiadaki. Grounded reinforcement learning for visual reasoning. arXiv preprint arXiv:2505.23678, 2025. [51] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 645652. IEEE, 2024. 7, 8, 13, 14, 15, 16 [52] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 4, 7 [53] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. 3 [54] Lucy Xiaoyang Shi, Archit Sharma, Tony Zhao, and Chelsea Finn. Waypoint-based imitation learning for robotic manipulation. arXiv preprint arXiv:2307.14326, 2023. 7, [55] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, et al. Hi robot: Open-ended instruction following with hierarchical vision-language-action models. arXiv preprint arXiv:2502.19417, 2025. 3 22 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning [56] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 1 [57] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. 8, 14 [58] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [59] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. 1, 3 [60] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 1 [61] Yan Wang, Wenjie Luo, Junjie Bai, Yulong Cao, Tong Che, Ke Chen, Yuxiao Chen, Jenna Diamond, Yifan Ding, Wenhao Ding, et al. Alpamayo-r1: Bridging reasoning and action prediction for generalizable autonomous driving in the long tail. arXiv preprint arXiv:2511.00088, 2025. 2 [62] Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, et al. Vla-adapter: An effective paradigm for tiny-scale vision-language-action model. arXiv preprint arXiv:2509.09372, 2025. 3 [63] Yilin Wu, Anqi Li, Tucker Hermans, Fabio Ramos, Andrea Bajcsy, and Claudia Perez-DArpino. Do what you say: Steering vision-language-action models via runtime reasoning-action alignment verification. arXiv preprint arXiv:2510.16281, 2025. 2, 3 [64] Violet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, and Nick Haber. Just enough thinking: Efficient reasoning with adaptive length penalties reinforcement learning. arXiv preprint arXiv:2506.05256, 2025. [65] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1 [66] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134, 2025. 3 [67] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. arXiv preprint arXiv:2502.13130, 2025. 1, 3, 8, 14 [68] Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Nicu Sebe, and Heng Tao Shen. survey on efficient vision-language-action models. arXiv preprint arXiv:2510.24795, 2025. 2 [69] Danlong Yuan, Tian Xie, Shaohan Huang, Zhuocheng Gong, Huishuai Zhang, Chong Luo, Furu Wei, and Dongyan Zhao. Efficient rl training for reasoning models via length-aware optimization. arXiv preprint arXiv:2505.12284, 2025. 23 Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning [70] Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. arXiv preprint arXiv:2508.13998, 2025. 3 [71] MichaÅ‚ Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. 2, 3 [72] Zhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, and Xin Eric Wang. Soft thinking: Unlocking the reasoning potential of llms in continuous concept space. arXiv preprint arXiv:2505.15778, 2025. [73] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17021713, 2025. 2, 3, 7, 14 [74] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. 7, 8, 13 [75] Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal DaumÃ© III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. 3 [76] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 8,"
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}