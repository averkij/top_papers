{
    "paper_title": "ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?",
    "authors": [
        "Vy Tuong Dang",
        "An Vo",
        "Quang Tau",
        "Duc Dm",
        "Daeyoung Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, a benchmark containing 2,548 multimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieve 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human test-takers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Cross-lingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 0 8 6 3 1 . 8 0 5 2 : r ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions? Vy Tuong Dang*, An Vo*, Quang Tau, Duc Dm, Daeyoung Kim KAIST {vydang, anvo, quangtau, ducdm200158, kimd}@kaist.ac.kr Abstract VLMs fail to interpret traffic signals and rules Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, benchmark containing 2,548 mulimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieved 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human testtakers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Crosslingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Humanin-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io."
        },
        {
            "title": "Introduction",
            "content": "Vision Language Models (VLMs) have achieved remarkable success on English multimodal benchmarks (e.g., o3 achieving 82.9% on MMMU (Yue et al. 2024)). However, their performance on low-resource languages remains largely unexplored. Vietnamese is the worlds 10th most spoken language with over 100 million native speakers, making it critical test case for cross-lingual multimodal understanding. Educational contexts present particular challenges as they combine cultural knowledge, domain-specific terminology, and complex visual reasoning. While existing datasets like VNHSGE (Dao et al. 2023) and M3Exam (Zhang et al. 2023) include Vietnamese exam questions, they either lack genuinely multimodal elements or provide insufficient coverage of Vietnamese exam questions. Prior Vietnamese benchmarks have significant limitations. VNHSGE (Dao et al. 2023) contains 19,000 Vietnamese *These authors contributed equally. Question: In the intersection shown, which vehicles are obeying the traffic rules? 1. Only buses and motorbikes. 2. All of the above vehicles. 3. None of the vehicles are obeying the traffic rules. Prompt: Answer the multiple-choice question in the image. Put your choice in curly brackets, e.g., {2} Ground truth: 2 Gemini 2.5 Flash : {1} Sonnet 4.0 : {3} GPT 4.1 : {3} o3 : {3} Figure 1: VLMs fail despite the clear image, as they lack full understanding of Vietnamese traffic signs and road rules, and cannot accurately interpret vehicle movements at intersections. questions but strips away all visual content during preprocessing, making it text-only. M3Exam (Zhang et al. 2023) includes 1,817 Vietnamese questions, but only 116 contain actual images, the remaining questions are simply screenshots of text-only questions. In this work, we define multimodal questions as those containing images that integrate both textual and visual elements (e.g., charts, diagrams, illustrations, 5. VLMs perform better on text-only questions (70%) versus multimodal questions (61%). This consistent pattern across SOTA VLMs confirms that multimodal integration poses fundamental challenges (see Sec. 4.4). 6. Open-source VLMs achieve substantially lower performance than closed-source/SOTA VLMs (27.7% vs. 57%), confirming that Vietnamese multimodal understanding remains significant challenge for current open-source VLMs (see Sec. 4.6). 7. Cross-lingual prompting shows mixed results: improving open-source VLMs (+2.9%) while hurting SOTA VLMs (-1.0%), suggesting language-content mismatches affect VLMs differently (see Sec. 4.5). 8. Human-in-the-loop collaboration provides modest gains with OCR help (+0.48%) but substantial improvement with full text and image editing (+5.71%) (see Sec. 4.7)."
        },
        {
            "title": "2 Related work\nLLMs in Vietnamese tasks As Vietnamese remains a low-\nresource language, recent years have witnessed significant\nresearch efforts on LLMs/VLMs for Vietnamese tasks, in-\ncluding fact verification (Hoa et al. 2025), OCR on historical\nVietnamese texts (Do et al. 2025), knowledge and reason-\ning capabilities (ZaloAI and JAIST 2025; Nguyen, Le, and\nNguyen 2024), QA (Nguyen et al. 2023; Singh et al. 2025),\nand VQA (Tran et al. 2024). In our work, we focus on multi-\nmodal exam questions, an area that remains underexplored\nfor evaluating VLM capabilities in Vietnamese.",
            "content": "Table 1: ViExam is the largest multimodal Vietnamese exam benchmark with 2,548 questions, while existing benchmarks primarily focus on text-only evaluation or contain minimal multimodal content. Benchmark #VN Questions #VN Multimodal Questions EXAMS-V (Das et al. 2024) SeaExam (Liu et al. 2025) VNHSGE (Dao et al. 2023) VMLU (ZaloAI and JAIST 2025) M3Exam (Zhang et al. 2023) ViExam (Ours) 0 1,745 19,300 10,880 1, 2,548 0 0 0 0 116 2,548 Vietnamese exam benchmarks Several works have explored LLM abilities on Vietnamese exam questions, but with some limitations. VMLU (ZaloAI and JAIST 2025) and SeaExam (Liu et al. 2025) focus exclusively on textonly questions. VNHSGE (Dao et al. 2023) considers multimodal questions but converts all images to text-only format during preprocessing. EXAMS-V (Das et al. 2024) and M3Exam (Zhang et al. 2023) are multilingual exam benchmarks. However, EXAMS-V does not support Vietnamese, while M3Exam contains 1,817 Vietnamese questions but only 116 include actual visual elements (e.g., charts, tables, diagrams). The remaining questions in M3Exam are simply text-only screenshots. ViExam differs from these benchmarks by focusing on evaluating SOTA VLM performance on multimodal exam questions containing visual elements (e.g., Figure 2: Our ViExam benchmark is the first Vietnamese multimodal exam benchmark with 2,548 questions. tables; see Fig. 1) rather than text-only screenshots. To address this gap, we introduce ViExam (Figs. 2 and 3), the first comprehensive multimodal benchmark designed to evaluate VLMs reasoning capabilities in Vietnamese educational contexts. Besides, we propose semi-automated pipeline comprising various stages for data curation and validation (e.g., data collection, cleaning, data labeling, and quality validation). Unlike previous benchmarks that simplify visual content into text descriptions (Dao et al. 2023), all 2,548 questions in ViExam contain visual elements (e.g.,., charts, diagrams, illustrations, tables), spanning 7 domains: Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. To answer correctly, VLMs must simultaneously process Vietnamese text, interpret visual content, and perform complex reasoning across modalities. We test 4 state-of-the-art (SOTA) VLMs: Gemini 2.5 Flash (Comanici et al. 2025), Sonnet 4.0 (Anthropic 2025), GPT 4.1 (OpenAI 2025a), and o3 (OpenAI 2025b), along with 10 open-source models: Aya Vision 8B vs. Aya Vision 32B ( Ustun et al. 2024), Gemma 3 4B vs. Gemma 3 27B (Mesnard et al. 2024), Mistral Medium 3 (MistralAI 2025a) vs. Mistral Small 3.2 24B (MistralAI 2025b), Llama 4 Maverick vs. Llama 4 Scout (MetaAI 2025), and Qwen 2.5 VL 32B vs. Qwen 2.5 VL 72B (Bai et al. 2025). Our key findings are: 1. All 5 SOTA VLMs achieve strong OCR performance on Vietnamese text (i.e., 6% CER and 9% WER), confirming that poor performance on ViExam stems from multimodal reasoning challenges rather than basic text recognition failures (see Sec. 4.1). 2. SOTA VLMs achieve only 57% mean accuracy across 7 domains, with Geography most accessible (72%) and Physics most challenging (44%) (see Sec. 4.2). 3. The thinking VLM o3 substantially outperforms nonthinking VLMs (74% vs. 48-59% (Sec. 4.2). 4. VLMs exhibit significant bias toward option (31%) in multiple-choice questions, suggesting that failures are not purely due to reasoning limitations but may be partially attributable to bias in training data (see Sec. 4.3). Figure 3: Sample questions from ViExam spanning 7 domains. charts, diagrams, tables) rather than (screenshots of) textonly questions (see Tab. 1). We also focus on broad domains spanning from academic subjects to practical assessments (i.e., driving test, IQ test), providing comprehensive evaluation across diverse multimodal reasoning contexts."
        },
        {
            "title": "3 The ViExam Benchmark",
            "content": "ViExam evaluates VLMs performance on multimodal Vietnamese exam questions where visual elements and Vietnamese text are integrated within single images. This research tests whether VLMs can maintain their reasoning capabilities when confronted with simultaneous processing of visual content (e.g., diagrams, charts, tables) and Vietnamese text across diverse academic domains. Taxonomy ViExam spans 7 distinct domains representative of Vietnamese exams (Figs. 2 and 3): (1) Mathematics, (2) Physics, (3) Chemistry, (4) Biology, (5) Geography, (6) Driving Test, and (7) IQ Test. The benchmark contains total of 2,548 questions, including multiple-choice questions with 4 options (88%), multiple-answer questions (1%), and multiple-choice questions with number of options other than 4 (11%). Data Curation To ensure high-quality multimodal content, we implemented systematic collection and filtering pipeline (see Fig. 4). We collected examination tests in PDF format and converted each page to PNG images. The key challenge was distinguishing between image-rich and text-only questions. We employed Tesseract OCR library (Smith 2007) to identify question boundaries by detecting Vietnamese markers (e.g., Câu; Question). We then developed pipeline to automatically detect image-containing regions using 3 criteria: (1) contour area analysis to identify non-textual shapes exceeding minimum area thresholds, (2) geometric filtering to distinguish text-like rectangles from image elements based on aspect ratios and dimensions, and (3) morphological operations to detect complex visual structures. Images passing automated filters underwent manual review through webbased interface where 3 Vietnamese native speakers (i.e.,, co-authors) performed triple verification with binary accept/reject decisions to ensure content quality (see Appendix C)."
        },
        {
            "title": "3.1 Tasks 1-5: Academic subjects: Mathematics,\nPhysics, Chemistry, Biology, Geography",
            "content": "Vietnamese academic assessments require students to demonstrate comprehension across multiple disciplines through integrated visual-textual reasoning. These subjects demand understanding of complex diagrams, mathematical equations, scientific illustrations, geographical maps, etc. Following our hypothesis that VLMs trained predominantly on English data will struggle with Vietnamese academic content, we test VLMs across 5 core academic subjects. Images We systematically collected questions using Selenium web scraping from Vietnamese educational platforms1. Our dataset comprises questions from 3 primary sources: (1) Official Exam Tests from the Vietnamese National High School Graduation Examination, (2) Official Sample Tests issued by the Ministry of Education and Training (MOET), and (3) Mock Exam Tests from Provincial Departments of Education. ViExam comprises 456 Mathematics questions, 361 Physics questions, 302 Chemistry questions, 341 Biology questions, and 481 Geography questions."
        },
        {
            "title": "3.2 Task 6: Driving Test",
            "content": "Vietnamese driving license examinations represent standardized practical assessments where visual scene understanding directly impacts real-world safety outcomes. These questions 1https://thuvienhoclieu.com, https://vndoc.vn Figure 4: Three-stage data curation: (1) PDF conversion to images, (2) automated multimodal question detection, and (3) manual verification by native speakers. test traffic rule comprehension, hazard recognition, and situational judgment through scenario-based images requiring both visual processing and knowledge of Vietnamese traffic regulations. We hypothesize that VLMs will struggle with culture-specific traffic scenarios and Vietnamese regulatory context. Images The Vietnamese Ministry of Transport (MOT) maintains official question banks containing 250 questions for A1 licenses (motorcycles) and 600 questions for B2 licenses (automobiles). To achieve A1 level requires minimum of 21/25 correct answers (84%), while B2 level requires 32/35 correct answers (91.4%) and must not make errors on critical questions. From this broader collection of 850 total questions, we selected 367 multimodal multiple-choice questions that contain visual elements integrated with Vietnamese text. Each selected question presents realistic traffic scenarios through illustrations, accompanied by Vietnamese text describing the situation and 2-4 multiple-choice options. The questions assess understanding of traffic signs, road markings, vehicle positioning, and appropriate responses to various driving conditions within the Vietnamese regulatory framework."
        },
        {
            "title": "3.3 Task 7: IQ Test",
            "content": "Intelligence quotient (IQ) assessment through visual reasoning provides evaluation of pattern recognition and logical thinking capabilities. We test whether VLMs can maintain reasoning capabilities when instructions are presented in Vietnamese. Images We collected 240 IQ test questions from https://vndoc.com through manual screenshot capture. These questions span various cognitive domains, including spatial reasoning, pattern completion, logical sequences, numerical reasoning and abstract thinking. Each question presents 4-8 multiple-choice options embedded within the image. The questions maintain Vietnamese instructions while testing cognitive abilities through visual puzzles, making them suitable for evaluating VLM performance on reasoning tasks in Vietnamese context."
        },
        {
            "title": "4.1 Sanity check: VLMs achieve high OCR\naccuracy but struggle with ViExam",
            "content": "Here, we first verify that VLMs can effectively read Vietnamese text, establishing that poor performance on ViExam stems from multimodal reasoning challenges rather than basic text recognition failures. Prior work shows SOTA VLMs excel at OCR tasks (Do et al. 2025), so we test whether failures on multimodal Vietnamese exam questions result from inability to process Vietnamese text or from more complex multimodal understanding requirements. Experiments We evaluate 4 SOTA VLMs (Gemini 2.5 Flash , Sonnet 4.0 GPT 4.1 , o3 ) on OCR performance using subset of 210 questions from ViExam (see Appendix for more details). We extract text regions from the images and measure by widely used OCR metrics: F1 score, Character Error Rate (CER), and Word Error Rate (WER) (Appendix E). Results All VLMs achieve strong OCR performance, with mean F1 score of 0.94 (Tab. 3). CER and WER remain low at 6.68% and 9.32% respectively (Tab. 3). These results confirm that VLMs can effectively read Vietnamese text, indicating that challenges on ViExam must stem from multimodal reasoning rather than basic text recognition limitations."
        },
        {
            "title": "4.2 ViExam reveals that while most VLMs",
            "content": "underperform Vietnamese test-takers, o3 achieves superior results Experiments We replicate the experiments of Sec. 4.1 but evaluate on the complete ViExam benchmark containing 2,548 questions across 7 domains: Mathematics, Physics, Table 2: Closed-source VLMs (57.74%) underperform human average (64.54%) but significantly outperform open-source VLMs (27.70%).Human performance is based on official Vietnamese national high school graduation exam results. a. Math b. Physics c. Chemistry d. Biology e. Geography f. Driving g. IQ Mean Human 62.80 100.0 71.90 100. 66.54 99.60 Open-source VLMs Human (Average) Human (Best) Aya Vision 8B Aya Vision 32B Mistral Small 3.2 24B Mistral Medium 3 Llama 4 Scout Llama 4 Maverick Gemma 3 4B Gemma 3 27B Qwen 2.5 VL 32B Qwen 2.5 VL 72B"
        },
        {
            "title": "Mean",
            "content": "Gemini 2.5 Flash Sonnet 4.0 GPT 4.1 o3 Mean 64.50 98.00 5.92 7.46 20.83 25.88 33.55 27.41 27.41 39.69 26.75 37.94 25.29 48.46 50.66 41.01 85.09 56. 66.70 100.0 2.77 8.86 12.50 9.42 11.91 9.42 17.73 29.64 10.56 19.39 13.22 37.12 38.50 33.80 68.98 44.60 66.80 100. 2.98 12.91 20.86 20.53 23.51 18.87 21.19 38.41 17.88 40.40 21.75 2.93 17.01 27.86 26.98 31.09 21.41 21.99 30.21 32.26 37.83 24.96 2.29 18.71 27.86 31.19 52.18 39.50 27.23 47.61 51.35 60.08 35. Closed-source/SOTA VLMs 61.29 68.54 44.87 53.31 43.70 41.72 67.16 82.78 85.03 48.44 68.81 88.98 61.59 54.26 72.81 26.98 32.97 39.51 46.32 49.86 51.77 40.33 43.87 54.50 54. 44.03 71.12 58.04 66.21 74.66 67.51 12.08 7.99 23.75 17.38 30.42 25.69 31.25 27.37 33.75 33.69 24.17 27.51 21.67 25.37 35.42 37.83 33.33 32.38 42.50 41.77 28.83 27.70 47.50 59.87 44.17 48.28 45.83 48.73 50.83 74. 47.08 57.74 Table 3: SOTA VLMs achieve strong OCR performance on Vietnamese text extraction. Model F1 Score CER (%) WER (%) Gemini 2.5 Flash Sonnet 4.0 GPT 4.1 o3 Mean 0.90 0.95 0.95 0.97 0.94 14.46 4.24 4.10 3.90 6.68 17.57 7.52 7.04 5.16 9. Chemistry, Biology, Geography, Driving Test, and IQ Test (see Appendix for more details). Results SOTA VLMs achieve mean accuracy of 57.74% across all domains (Tab. 2), underperforming compared to typical Vietnamese test-taker results2 (i.e., normally >62% across subjects; Tab. 2). The thinking model o3 substantially outperforms non-thinking models (74.07% vs 48.28-59.87%; Tab. 2), suggesting that explicit reasoning capabilities help with complex Vietnamese multimodal exam questions. Qualitative results are in Appendix G."
        },
        {
            "title": "4.3 VLMs are biased toward option B in incorrect",
            "content": "responses When VLMs answer incorrectly, they often exhibit bias toward specific answer choices in multiple-choice questions (Pezeshkpour and Hruschka 2024). Understanding these biases is crucial for interpreting VLM performance on standardized tests like ViExam. Experiments We replicate the experiments of Sec. 4.2, but downsample the ViExam dataset to ensure uniform distribution, where exactly 25% of correct answers correspond to each option (A, B, C, D). We then analyze only the incorrect responses from 4 SOTA VLMs and calculate the percentage distribution of their chosen letters when answering incorrectly. Table 4: VLMs demonstrate significant bias toward option (31.09%). Model C Gemini 2.5 Flash 24.40 26.80 23.71 25.09 Sonnet 4.0 22.91 28.33 27.34 21.43 GPT 4.1 27.10 35.99 20.99 15.92 o3 20.16 33.24 21.53 25.07 Mean 23.64 31.09 23.39 21.88 2https://vnexpress.net/pho-diem-9-mon-thi-tot-nghiep-thpt2024-4770524.html Results All SOTA VLMs show significant bias toward option (31.09%; Tab. 4) of incorrect responses selecting this choice, substantially above the expected 25% for uniform distribution (see Tab. 4). This phenomenon across all VLMs shows that failures on ViExam are not purely due to reasoning limitations but may be partially attributable to bias in training data where option is disproportionately the correct answer. These results suggest that cross-lingual prompting may even hinder performance for SOTA VLMs by creating languagecontent mismatches. It can even generate answers mixing Vietnamese and English, which may confuse end users. More results are presented in Appendix F."
        },
        {
            "title": "4.4 Multimodal integration drives the difficulty\nTo isolate whether ViExam’s challenge stems from Viet-\nnamese language understanding or multimodal integration,\nwe compare VLM performance on questions containing only\ntext versus those requiring visual-textual integration.\nExperiments We select 210 multimodal questions from\nViExam and pair them with 210 text-only questions of com-\nparable difficulty using Vietnamese national test numbering\nsystems, where adjacent question numbers indicate similar\ndifficulty levels (Appendix D). We evaluate the same four\nSOTA VLMs from Sec. 4.1 on both question types.",
            "content": "Table 5: Performance gap between multimodal and text-only Vietnamese exam questions, revealing that VLMs do not understand multimodal content well, leading to this gap. Multimodal Text-only Gemini 2.5 Flash Sonnet 4.0 GPT 4.1 o3 Mean 62.86 50.48 53.81 77. 61.19 70.00 (+7.14) 58.57 (+8.09) 66.19 (+12.38) 87.62 (+10.00) 70.60 (+9.41) Results VLMs demonstrate substantially better performance on text-only questions compared to multimodal questions (+9.41; Tab. 5). The consistent pattern across different SOTA VLMs confirms that multimodal integration poses fundamental challenges beyond Vietnamese language comprehension."
        },
        {
            "title": "4.5 Cross-lingual prompting does not improve\nVietnamese content understanding",
            "content": "We investigate whether prompting VLMs in English while presenting Vietnamese visual content improves performance, testing the hypothesis that models might reason more effectively in their primary training language. Experiments We replicate the experiments of Sec. 4.2 but use English prompts while maintaining identical Vietnamese visual questions. The English prompts translate the original Vietnamese instructions while preserving the same question format and requirements (Appendix and Fig. 1). Results English prompting shows mixed results depending on model type (Tab. 6). While open-source VLMs benefit from English prompts with (+2.9)% improvement, SOTA closed-source models actually perform worse (-1.05)%. The improvement in open-source models demonstrates that English prompts help them leverage their predominantly English training foundation, thereby clarifying task structure. Conversely, the performance decline in SOTA models indicates that pure English prompts may interfere with their already complex multilingual fine-tuning processes, which have likely been optimized for handling Vietnamese content."
        },
        {
            "title": "4.6 Open-source VLMs significantly\nunderperform on ViExam",
            "content": "We evaluate open-source VLMs to understand the current capabilities of publicly available models on Vietnamese multimodal exam questions. This comparison reveals the performance gap between commercial and open-source VLMs on multimodal tasks on low-source languages. Experiments We replicate the experiments of Sec. 4.2, but now evaluate 10 open-source VLMs on ViExam: Aya Vision 8B vs. Aya Vision 32B ( Ustun et al. 2024), Gemma 3 4B vs. Gemma 3 27B (Mesnard et al. 2024), Mistral Medium 3 (MistralAI 2025a) vs. Mistral Small 3.2 24B (MistralAI 2025b), Llama 4 Maverick vs. Llama 4 Scout (MetaAI 2025), Qwen 2.5 VL 32B vs. Qwen 2.5 VL 72B (Bai et al. 2025). Table 6: Using English prompts to ask Vietnamese questions improves open-source VLM accuracy by (+2.90) but reduces closed-source model performance by (-1.05) on ViExam. This shows that open-source VLMs might be trained more on English data, so English prompts can trigger them to leverage their stronger English capabilities, yielding better performance. Vietnamese English Open-source VLMs Aya Vision 8B Aya Vision 32B Mistral Small 3.2 24B Mistral Medium 3 Llama 4 Scout Llama 4 Maverick Gemma 3 4B Gemma 3 27B Qwen 2.5 VL 32B Qwen 2.5 VL 72B Mean 7.99 17.38 25.69 27.37 33.69 27.51 25.37 37.83 32.38 41.77 27.70 13.36 (+5.37) 24.80 (+7.42) 26.67 (+0.98) 25.23 (-2.14) 36.79 (+3.10) 41.17 (+13.66) 26.38 (+1.01) 42.75 (+4.92) 30.89 (-1.49) 37.98 (-3.79) 30.60 (+2.90) Closed-source VLMs Gemini 2.5 Flash Sonnet 4.0 GPT 4.1 o3 Mean 59.87 48.28 48.73 74.07 57.74 54.77 (-5.10) 47.94 (-0.34) 48.44 (-0.29) 75.58 (+1.51) 56.68 (-1.05) Results Open-source VLMs achieve substantially lower performance than closed-source models (27.70% vs. 57.74%; Tab. 2). The best-performing open-source model, Qwen 2.5 VL 72B , reaches 41.77% but still falls significantly below the lowest-performing closed-source model (Sonnet Figure 5: Human-in-the-loop collaboration process. Humans manually crop image regions, edit OCR text output, and refine VLM-generated image descriptions before feeding the processed content to the VLM for final answer generation. 4.0 at 48.28%). Within model families, larger models consistently outperform smaller variants: Aya Vision 32B vs. Aya Vision 8B (17.38% vs. 7.99%), Gemma 3 27B vs. Gemma 3 4B (37.83% vs. 25.37%), and Qwen 2.5 VL 72B vs. Qwen 2.5 VL 32B (41.77% vs. 32.38%) (see Tab. 2)."
        },
        {
            "title": "4.7 Human-in-the-loop collaboration can partially",
            "content": "improve VLM performance We explore whether human collaboration can improve VLM performance on challenging Vietnamese multimodal questions, investigating practical approaches for educational applications where human expertise can augment AI capabilities. This addresses the potential for human-AI collaboration to overcome current limitations in cross-lingual multimodal understanding. Table 7: Human-in-the-loop collaboration, which allows humans to crop images in questions and then edit questions and image descriptions, can improve performance by up to +5.71. o3 Accuracy 77. o3 + Human (OCR) o3 + Human (OCR + Desc) 78.10 (+0.48) 83.33 (+5.71) Experiments We implement 2 human-in-the-loop approaches using o3 on the 210-question subset from Sec. 4.1: (1) humans manually crop image regions and edit OCR text output from the model, and (2) humans crop images and edit both OCR text and image descriptions generated by the model (see Fig. 5). In both conditions, human experts manually separate visual and textual content before model processing (Appendix E). Results OCR-only collaboration (o3 + Human OCR) yields modest gains (+0.48; Tab. 7), which is expected since VLMs already perform well on Vietnamese OCR tasks (Sec. 4.1), leaving little room for human improvement in text recognition alone. However, full collaboration allowing human editing of both text and image descriptions achieves substantial improvement (+5.71; Tab. 7). This demonstrates that human expertise in interpreting visual content and refining AI-generated descriptions can partially improve multimodal reasoning, though significant challenges remain even with human assistance."
        },
        {
            "title": "5 Discussion and Conclusion\nOur study shows that SOTA VLMs consistently underper-\nform on Vietnamese multimodal exam questions, falling be-\nlow average human performance across 7 domains. The think-\ning model o3 substantially outperforms non-thinking models\n(i.e., GPT 4.1 , Gemini 2.5 Flash , Sonnet 4.0\n), suggesting that explicit reasoning capabilities help with\ncomplex Vietnamese multimodal tasks. Our OCR analysis\nconfirms that VLM failures stem from multimodal reason-\ning challenges rather than basic Vietnamese text recognition\nlimitations.",
            "content": "The performance gap between English and Vietnamese questions for SOTA VLMs is relatively modest, with o3 achieving 82.9% on MMMU (Yue et al. 2024) versus 74.05% on ViExam, demonstrating reasonable cross-lingual multimodal capabilities. However, open-source models exhibit significant limitations when processing Vietnamese multimodal content. Gemma 3 27B achieves 64.9% on MMMU (Yue et al. 2024) but only 37.83% on ViExam, while Llama 4 Maverick achieves 73.4% on MMMU (Yue et al. 2024) but only 27.51% on ViExam. While SOTA models maintain relatively stable performance across languages (8.85 percentage point drop), open-source models suffer dramatic performance degradation when moving from English to Vietnamese contexts (27-45 percentage point drops). Future work We have not tested VLMs with tool-use capabilities, which might improve performance. It also might be interesting to compare whether Vietnamese and English exam questions of similar difficulty levels produce different performance patterns in VLMs. Acknowledgments This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2025-00573160). We also thank Patrick Vibild, Hoang Minh Son (KAIST), and Duc-Vu Ngo (Independent Researcher) for feedback and discussions of the earlier results. VTD was supported by Cohere Labs Research Grant, and AV was supported by the Hyundai Motor Chung Mong-Koo Global Scholarship."
        },
        {
            "title": "References",
            "content": "Introducing Claude 4. {https://www. Anthropic. 2025. anthropic.com/news/claude-4}. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; Zhong, H.; Zhu, Y.; Yang, M.; Li, Z.; Wan, J.; Wang, P.; Ding, W.; Fu, Z.; Xu, Y.; Ye, J.; Zhang, X.; Xie, T.; Cheng, Z.; Zhang, H.; Yang, Z.; Xu, H.; and Lin, J. 2025. Qwen2.5-VL Technical Report. CoRR, abs/2502.13923. Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Dao, X.; Le, N.; Vo, T.; Phan, X.; Ngo, B. B.; Nguyen, V.; Nguyen, T.; and Nguyen, H. P. 2023. VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models. CoRR, abs/2305.12199. Das, R. J.; Hristov, S. E.; Li, H.; Dimitrov, D.; Koychev, I.; and Nakov, P. 2024. EXAMS-V: Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models. In Ku, L.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 77687791. Association for Computational Linguistics. Do, T.; Tran, D. P.; Vo, A.; and Kim, D. 2025. ReferenceBased Post-OCR Processing with LLM for Precise Diacritic Text in Historical Document Recognition. In Walsh, T.; Shah, J.; and Kolter, Z., eds., AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, 27951 27959. AAAI Press. Hoa, T. T.; Duy, T. Q.; Tran, K. Q.; and Nguyen, K. V. 2025. ViFactCheck: New Benchmark Dataset and Methods for Multi-Domain News Fact-Checking In Vietnamese. In Walsh, T.; Shah, J.; and Kolter, Z., eds., AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, 308 316. AAAI Press. Liu, C.; Zhang, W.; Ying, J.; Aljunied, M.; Luu, A. T.; and Bing, L. 2025. SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia. In Chiruzzo, L.; Ritter, A.; and Wang, L., eds., Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, 61196136. Association for Computational Linguistics. Mesnard, T.; Hardin, C.; Dadashi, R.; Bhupatiraju, S.; Pathak, S.; Sifre, L.; Rivière, M.; Kale, M. S.; Love, J.; Tafti, P.; Hussenot, L.; Chowdhery, A.; Roberts, A.; Barua, A.; Botev, A.; Castro-Ros, A.; Slone, A.; Héliou, A.; Tacchetti, A.; Bulanova, A.; Paterson, A.; Tsai, B.; Shahriari, B.; Lan, C. L.; Choquette-Choo, C. A.; Crepy, C.; Cer, D.; Ippolito, D.; Reid, D.; Buchatskaya, E.; Ni, E.; Noland, E.; Yan, G.; Tucker, G.; Muraru, G.; Rozhdestvenskiy, G.; Michalewski, H.; Tenney, I.; Grishchenko, I.; Austin, J.; Keeling, J.; Labanowski, J.; Lespiau, J.; Stanway, J.; Brennan, J.; Chen, J.; Ferret, J.; Chiu, J.; and et al. 2024. Gemma: Open Models Based on Gemini Research and Technology. CoRR, abs/2403.08295. MetaAI. 2025. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation. {https://ai.meta. com/blog/llama-4-multimodal-intelligence/}. MistralAI. 2025a. Medium is the new large. {https://mistral. ai/news/mistral-medium-3}. MistralAI. 2025b. Mistral Small 3. {https://mistral.ai/news/ mistral-small-3}. Nguyen, M. T.; Tran, K.; Nguyen, N.; and Vu, X. 2023. ViGPTQA - State-of-the-Art LLMs for Vietnamese Question Answering: System Overview, Core Models Training, and Evaluations. In Wang, M.; and Zitouni, I., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP 2023 - Industry Track, Singapore, December 6-10, 2023, 754764. Association for Computational Linguistics. Nguyen, T.; Le, A.; and Nguyen, V. 2024. ViLLM-Eval: Comprehensive Evaluation Suite for Vietnamese Large Language Models. CoRR, abs/2404.11086. OpenAI. 2025a. Introducing GPT-4.1 in the API. {https: //openai.com/index/gpt-4-1/}. OpenAI. 2025b. Introducing OpenAI o3 and o4-mini. {https: //openai.com/index/introducing-o3-and-o4-mini/}. Pezeshkpour, P.; and Hruschka, E. 2024. Large Language Models Sensitivity to The Order of Options in MultipleChoice Questions. In Duh, K.; Gómez-Adorno, H.; and Bethard, S., eds., Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, 20062017. Association for Computational Linguistics. Singh, S.; Romanou, A.; Fourrier, C.; Adelani, D. I.; Ngui, J. G.; Vila-Suero, D.; Limkonchotiwat, P.; Marchisio, K.; Leong, W. Q.; Susanto, Y.; Ng, R.; Longpre, S.; Ruder, S.; Ko, W.; Bosselut, A.; Oh, A.; Martins, A. F. T.; Choshen, L.; Ippolito, D.; Ferrante, E.; Fadaee, M.; Ermis, B.; and Hooker, S. 2025. Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation. In Che, W.; Nabende, J.; Shutova, E.; and Pilehvar, M. T., eds., Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, 18761 18799. Association for Computational Linguistics. Smith, R. 2007. An Overview of the Tesseract OCR Engine. In 9th International Conference on Document Analysis and Recognition (ICDAR 2007), 23-26 September, Curitiba, Paraná, Brazil, 629633. IEEE Computer Society. Tran, K. V.; Phan, H. P.; Nguyen, K. V.; and Nguyen, N. L. 2024. ViCLEVR: visual reasoning dataset and hybrid multimodal fusion model for visual question answering in Vietnamese. Multim. Syst., 30(4): 199. Ustun, A.; Aryabumi, V.; Yong, Z. X.; Ko, W.; Dsouza, D.; Onilude, G.; Bhandari, N.; Singh, S.; Ooi, H.; Kayid, A.; Vargus, F.; Blunsom, P.; Longpre, S.; Muennighoff, N.; Fadaee, M.; Kreutzer, J.; and Hooker, S. 2024. Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model. In Ku, L.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 1589415939. Association for Computational Linguistics. Yue, X.; Ni, Y.; Zheng, T.; Zhang, K.; Liu, R.; Zhang, G.; Stevens, S.; Jiang, D.; Ren, W.; Sun, Y.; Wei, C.; Yu, B.; Yuan, R.; Sun, R.; Yin, M.; Zheng, B.; Yang, Z.; Liu, Y.; Huang, W.; Sun, H.; Su, Y.; and Chen, W. 2024. MMMU: Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, 95569567. IEEE. ZaloAI; and JAIST. 2025. Vietnamese Multitask Language Understanding Benchmark Suite for Large Language Models. {https://vmlu.ai}. Zhang, W.; Aljunied, M.; Gao, C.; Chia, Y. K.; and Bing, L. 2023. M3Exam: Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Appendix for: ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions? Models and access details We evaluate 4 state-of-the-art VLMs using the official APIs of each model with default settings, including one thinking model (o3 ) and three non-thinking models (Sonnet 4.0 , GPT 4.1 , Gemini 2.5 Flash ), along with 10 open-source VLMs to compare their performance and test their capabilities on the same dataset (see Tab. 8). Table 8: VLMs configuration and platform details. Model Model Name Temperature Platform/Provider Open Source Aya Vision 8B Aya Vision 32B Gemma 3 4B Gemma 3 27B Mistral Medium 3 Mistral Small 3.2 24B Qwen 2.5 VL 72B Qwen 2.5 VL 32B Llama 4 Maverick Llama 4 Scout Gemini 2.5 Flash Sonnet 4.0 GPT 4.1 o3 * Open-source VLMs 0.0 0.0 0.0 0.0 0.0 0. c4ai-aya-vision-8b c4ai-aya-vision-32b gemma-3-4b-it gemma-3-27b-it mistral-medium-3 mistral-small-3.224b-instruct qwen/qwen2.5-vl72b-instruct qwen/qwen2.5-vl32b-instruct llama-4-maverick llama-4-scout 0.0 0.0 0.0 0.0 Closed-source VLMs 0.0 0.0 gemini-2.5-flash claude-sonnet-420250514 gpt-4.1-2025-04-14 o3-2025-04- *reasoning_effort: medium (default thinking mode setting)"
        },
        {
            "title": "Cohere\nCohere\nGoogle AI Studio\nGoogle AI Studio\nOpenRouter\nOpenRouter",
            "content": "OpenRouter OpenRouter OpenRouter OpenRouter Google AI Studio Anthropic"
        },
        {
            "title": "Yes\nYes\nYes\nYes\nYes\nYes",
            "content": "Yes Yes Yes Yes No No No No 0.0 OpenAI OpenAI Question prompts The following are the prompts (see Figs. 6 to 9) used throughout the ViExam experiments (i.e., prompts for answering single multiple-choice questions, multiple questions in one image, and performing OCR on images in the dataset). All prompts are designed with bilingual English-Vietnamese versions and have specific instructions for output formatting to ensure consistency during the evaluation process."
        },
        {
            "title": "Question answering prompt",
            "content": "English prompt: Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z}. Vietnamese prompt: Trả lời câu hỏi trắc nghiệm trong hình. Ghi đáp án bạn chọn trong dấu ngoặc nhọn với chữ cái tương ứng, ví dụ: {Z}. Figure 6: The Vietnamese and English prompts used for multiple-choice question answering tasks. Question answering prompt (Multiple answers) English prompt: Answer the multiple-choice questions in the image. List your choices in curly brackets using option letters, e.g., {Y, Z}. Vietnamese prompt: Trả lời các câu hỏi trắc nghiệm trong hình. Liệt kê các đáp án bạn chọn trong dấu ngoặc nhọn, sử dụng chữ cái, ví dụ: {Y, Z}. Figure 7: The Vietnamese and English prompts used for question answering tasks on images containing multiple questions. OCR prompt English prompt: Perform OCR on the image and return the result in this JSON format: { Text: Extract only the textual content, excluding figures and tables. Use Unicode math symbols (e.g., , π) and / for fractions (e.g., 1/5)., Image description: If figures or tables are present, describe them in Vietnamese; otherwise, leave blank. } Vietnamese prompt: Thực hiện OCR trên hình ảnh và trả về kết quả theo định dạng JSON sau: { Text: Chỉ trích xuất phần văn bản, bỏ qua hình vẽ và bảng biểu. Sử dụng ký hiệu toán học Unicode (e.g., , π) và / cho phân số (ví dụ: 1/5)., Image description: Nếu có hình vẽ hoặc bảng biểu, mô tả chúng bằng tiếng Việt; nếu không có, để trống. } Figure 8: The Vietnamese and English prompts used for OCR tasks on individual question images in the dataset. OCR prompt (Multiple Answers) English prompt: Perform OCR on the image and return the result in this JSON format: { Text: Extract only the textual content, excluding figures and tables. Use Unicode math symbols (e.g., , π) and / for fractions (e.g., 1/5)., Image description: If figures or tables are present, describe them in Vietnamese; otherwise, leave blank. } Vietnamese prompt: Thực hiện OCR trên hình ảnh và trả về kết quả theo định dạng JSON sau: { Text: Chỉ trích xuất phần văn bản, bỏ qua hình vẽ và bảng biểu. Sử dụng ký hiệu toán học Unicode (e.g., , π) và / cho phân số (ví dụ: 1/5)., Image description: Nếu có hình vẽ hoặc bảng biểu, mô tả chúng bằng tiếng Việt; nếu không có, để trống. } Figure 9: The prompts used for OCR tasks on images containing two questions in the dataset. C.1 Data collection and preprocessing"
        },
        {
            "title": "C Dataset construction methodology",
            "content": "Objective: To automatically collect examination documents from heterogeneous sources, standardize them into unified format, and eliminate noisy components (e.g., answer keys and solution explanations) (see Tab. 9). Data crawling The data collection process begins with the deployment of Selenium-based bot capable of simulating user behaviors, including searching, page scrolling, button clicking, and file downloading from popular educational websites (see Fig. 10, Fig. 11). The output of this step is collection of examination documents in various formats (i.e.,.doc, .docx, and .pdf files). Table 9: ViExam benchmark data sources Subject Source Platform Mathematics Official exam tests, MOET sample tests, https://thuvienhoclieu.com, ttps://vndoc.vn"
        },
        {
            "title": "Chemistry",
            "content": "Biology Geography Driving Test IQ Test school mock exams Official exam tests, MOET sample tests, school mock exams Official exam tests, MOET sample tests, school mock exams Official exam tests, MOET sample tests Official exam tests, MOET sample tests, school mock exams Traffic police department, Ministry of Public Security, Ministry of Transport Manual collection https://thuvienhoclieu.com, https://vndoc.vn https://thuvienhoclieu.com, ttps://vndoc.vn https://thuvienhoclieu.com, ttps://vndoc.vn https://thuvienhoclieu.com, ttps://vndoc.vn https://daotaohoclaixeoto.com, https://tracuuphapluat.info https://vndoc.com Format standardization To establish consistency throughout the entire pipeline, all Word documents are converted to PDF format through processing workflow. This process generates the standardized_pdfs_full dataset, which comprises format-unified PDF files while preserving the complete original content, including answer keys and solution explanations. Raw content filtering via PDF segmentation Utilizing the pypdf library, the system implements preliminary content filtering algorithm based on termination keyword indicator strategies: Primary Keywords: These are high-confidence markers, such as hết or HẾT (Vietnamese for \"THE END\"), that definitively signal the conclusion of the core examination content. Their presence triggers an immediate truncation point. Fallback Keywords: These serve as secondary indicators, used when no primary keyword is found. They typically mark the beginning of supplementary sections, e.g., BẢNG ĐÁP ÁN (ANSWER KEY) or LỜI GIẢI CHI TIẾT . The processing workflow operates as follows: 1. Scan each page to identify the first occurrence of termination keywords 2. Generate new PDF file, keeping all pages from the start through the page containing the keyword 3. The resulting dataset, termed final_clean_pdfs, eliminates the majority of noisy content following the examination questions This methodology ensures systematic removal of extraneous content while preserving the integrity of the core examination material. C.2 Document to image conversion Objective: Convert PDF files into images to support computer vision processing steps and crop questions in the next steps. PDF to PNG image conversion Each PDF page in the final_clean_pdfs dataset is converted to PNG format images using the pdf2image library. Key configuration parameters include: Resolution: 300 DPI (preserving fine details such as subscripts and diagrams.) Page identification: Each image is accompanied by metadata storing positional and corresponding information in metadata.json. The output consists of directory structure organized by examination, where each directory contains page images and their corresponding metadata files. Figure 10: Interface showing test list from https://thuvienhoclieu.com C.3 Question extraction and classification Objective: Extract individual questions from page images, handle cases where questions span multiple pages, and classify according to geometric characteristics, yielding questions with images or questions without images. Content localization via OCR and marker detection The system utilizes Tesseract OCR to extract: Text content from each image Bounding box coordinates of each line From the OCR data, the system searches for two types of markers using regular expressions (Regex): QuestionMarker: Expressions (e.g., Câus?d+[:.]) to locate the starting point of each question. GroupDirective: Recognition of directives for question groups, e.g., Dùng dữ kiện cho câu 1-5 (Use data for questions 1-5), supporting extraction of related question groups. Crop question plan & fine-grained filtering Based on the list of located markers, detailed crop plan is constructed. Simultaneously, fine-grained filtering mechanism is activated as second defense layer. It performs rapid OCR scan on small region immediately below the starting point of question to search for termination keywords ( HẾT, ĐÁP ÁN). If detected, the crop boundary is adjusted to eliminate any remaining unwanted content on the same page. Execution of extraction, stitching and question classification This step implements the crop plan execution, comprising: 1. Slicing and stitching: Based on plan directives, image regions corresponding to individual questions are extracted. For questions spanning multiple pages, these image fragments are seamlessly concatenated vertically (cv2.vconcat) to form single complete question image. Figure 11: Automated button clicking for data download in https://thuvienhoclieu.com 2. Content classification via Heuristic-based Geometric analysis: Image preprocessing: Question images are converted to grayscale and subsequently subjected to adaptive thresholding to highlight all objects against the background. Object detection (Contour detection): The cv2.findContours function is employed to identify contours of all connected geometric blocks within the image. Heuristic analysis: The system iterates through each block and applies set of heuristic rules based on pre-configured parameters (cv_min_image_area, cv_text_height_range, etc.). These rules evaluate: Significance: Whether the objects area exceeds minimum threshold for consideration. Shape Profile Whether the width-to-height ratio conforms to characteristics of typical text blocks. Objects with irregular shapes (e.g., graphs, diagrams) will not satisfy this condition. Dimensionality: Whether the object is sufficiently large in both dimensions to be considered graphic image, rather than merely long but narrow line. Final decision: If there exists even single object that simultaneously satisfies the conditions of being sufficiently large, having non-text-like shape characteristics, and possessing significant dimensions, the system immediately classifies the question as question_with_images. If no anomalous objects are detected, the question is classified as question_text_only. C.4 Ground truth annotation & classification review Objective: Despite the high performance of the automated pipeline, human verification is an indispensable step to ensure the quality and reliability of the final dataset. This phase serves two primary purposes: Performance evaluation: Measuring the accuracy of the automated extraction and classification algorithms. Ground truth generation: Correcting classification errors while supplementing valuable information (correct answers) to create gold-standard dataset. To accomplish this task efficiently and consistently, the ViExam question review system has been developed (see Fig. 12). Figure 12: ViExam question review system Design ViExam question review system This is fully client-side web application built with React and TailwindCSS. This approach provides multiple advantages: no server installation requirements, ensuring portability and data security (since all files are processed locally on the annotators machine). Side-by-Side comparison interface: Left panel: Displays question images automatically extracted from Appendix C.3. Right panel: Displays the original document (.pdf or .docx) from which the question was extracted. The tool utilizes libraries (e.g., PDF.js and Mammoth.js) to render original document content directly in the browser. Visual Navigation: Users can easily switch between examinations (subdirectories) and browse through individual questions within an examination. Zoom functionality and page navigation (for PDF files) are integrated to support detailed inspection. Interactive labeling system: Classification verification: Three primary decision buttons: * Yes: Confirms the pipeline result is correct (this is question with images). * No: Corrects pipeline error (this is text-only question). * Modify: Marks this question as requiring manual editing due to cropping errors (e.g., incomplete cropping, excessive cropping). Answer annotation (Ground truth): set of buttons (A, B, C, D, ...) and free-text input field allowing annotators to provide correct answers for each question. Annotation and validation process The workflow of an annotator is standardized as follows: 1. Data loading: The annotator begins by uploading two directories: Directory containing the output results from Appendix C.4 (see Fig. 13). Directory containing original documents (.pdf, .docx). The system automatically maps each question image to its corresponding original document  (Fig. 14)  . Figure 13: ViExam question review system - Image question folder selection interface. Figure 14: ViExam question review system - Comparation and input grouth truth. 2. Review and decision making: The annotator selects an examination to begin. The tool displays the first question image on the left and the original document on the right. Task 1 Comparison and verification: The annotator compares the extracted image with the content in the original document to evaluate: Completeness: Is the question incompletely or excessively cropped? Classification accuracy: Does it actually contain images/diagrams or not? Task 2 Labeling: Based on the assessment, the annotator performs two actions: Press one of three buttons: Yes, No, or Modify  (Fig. 15)  . Select the correct answer for that question. This process repeats for all questions in the examination. The system includes progress bar to track completion status. Figure 15: ViExam question review system - Comparison and ground truth input interface. Restructuring and data export This is the final and most valuable step. After completing the labeling process for one or more examinations, users can export the results. Operation: When the Download ZIP button is pressed, the tool performs series of automatic tasks: Classification error correction: Based on the users Yes/No decisions, the system automatically corrects suffixes in filenames (e.g., changing .._text_only.png to .._image.png if the user selects Yes). Data enrichment: Correct answers provided by users are added to filenames following standard format (e.g., .._key_A.png). Directory reorganization: Image files are moved into new directory structure reflecting their verified nature: question_image/, question_full_text/, question_modify/. Packaging: All cleaned, enriched, and restructured data is compressed into single .zip file, along with the reviewed original documents. Output: complete ground truth dataset with high reliability, fully labeled and clearly structured, ready for training and evaluating machine learning models C.5 Cross-verification and ground truth finalization stage To ensure accuracy and consistency at the highest level, secondary, decisive verification stage is implemented. This stage serves as the final quality control gateway, particularly focusing on validating the answer keys assigned during the previous stage. Objectives of the final verification stage ground truth answer validation: The primary focus is to verify and reconfirm the accuracy of assigned answers (e.g., A, B, C, D) from the previous stage. Consistency assurance: Review and standardize handling procedures for complex or ambiguous questions, ensuring uniform standards across the entire dataset Dataset finalization: Officially finalize the grouth truth of the dataset after undergoing rigorous verification processes. Verification process and decision making This process applies rigorous cross-verification mechanism to achieve objectivity and minimize errors: Verification panel: panel consisting of three experienced expert reviewers is established Independent evaluation: Using the ViExam platform, each ground truth answer assigned from the previous stage is independently evaluated by all three panel members Unanimous agreement principle: The final and official answer label for each question is established only when there is unanimous agreement, meaning all three reviewers provide the same answer Conflict Resolution: In all other cases, i.e., scenarios where two reviewers agree and one disagrees (2-1), or all three have different opinions (1-1-1), the question is marked as having conflict. These questions are brought forward for discussion and consensus-building by the entire review team to reach final decision."
        },
        {
            "title": "D Subset image question and subset text question",
            "content": "D.1 Objective and rationale Primary objective: Construct text-only question dataset in parallel with the original image question (VQA) dataset. Rationale: The purpose of this dataset is to establish robust baseline. It allows us to evaluate the performance of language models in the absence of visual factors. Through this approach, we can precisely separate and quantify the contribution of visual information to the models problem-solving capabilities. D.2 Search and pairing methodology Our methodology is built upon hypothesis regarding similarity in difficulty and context (Proximity-Context-Difficulty Hypothesis): questions with adjacent sequential numbers within the same examination are highly likely to share the same knowledge context and have equivalent difficulty levels (see Fig. 16). To realize this, we have implemented search method with the following principles: Offset prioritization: The search process for suitable text question corresponding to an original image question is performed in ascending offset order (questions 1, then 2, 3, etc.). This principle ensures that the selected text question always has the closest position, maximizing contextual similarity. Page-Scope expansion: To handle cases where adjacent questions are located on different pages, if no suitable pair is found on the current page, the algorithm automatically expands the search scope to adjacent pages (pages 1). D.3 Data standardization To ensure integrity and connectivity of the parallel dataset, standardization process is implemented: Structured filename analysis: Leveraging the consistent filename structure (Subject_ID_test_Num_question_ Num_page_...), the system uses Regular Expressions to automatically extract important metadata (e.g., subject ID and examination code). One-to-One mapping creation: This is the crucial step for creating logical connections. After finding suitable text question, it is copied and renamed according to standardized format, inheriting the subject ID and examination code from the original image question while retaining its actual sequence number and page number. Example: Text question Biology_001_question_101_... found for image question Biology_0001_test_ 001_question_100_... will be renamed to Biology_0001_test_001_question_101_.... Result: This process creates new clearly structured subset where each text question is directly logically linked to an image question, facilitating subsequent comparative control experiments. Figure 16: Image questions and text-only questions are equally difficult. OCR ground truth and image description This is the final step to complete data preparation for all experiments. In this step, we create ground truth containing accurate textual content (OCR) and image descriptions for the subset image created in the previous step. E.1 Objectives Create reliable reference dataset for textual content and descriptions of each question. Evaluate whether human intervention in image descriptions can help VLMs better understand images and provide more accurate choices."
        },
        {
            "title": "Implementation process",
            "content": "E.2 Step 1: Initial data generation using LLM large language model is utilized via API to process each question image. The model is required to perform two tasks: Text recognition and extraction (OCR): Read the main question content and answer choices A, B, C, Image description generation: Create detailed, objective descriptions of image content within the question The result is JSON file containing LLM-generated data. We then shorten the json file and keep only the information needed to generate ground truth for the ocr task. Step 2: Manual verification and error correction LLM-generated data is verified and corrected by humans through simple web platform developed by the authors called ViExam - OCR ground truth( see Fig. 17). This tool features: Figure 17: ViExam OCR ground truth tool - Initial interface for setting up image folder and JSON file selection. Comparison interface: Displays question images alongside JSON content for easy comparison Direct editing: Users can immediately edit OCR content and image descriptions Formula support: Integrates MathJax for displaying and verifying LaTeX formulas After the data is loaded and the user has verified its completeness, clicking the Start button will immediately generate the ground truth for the OCR task (see Fig. 18). Reviewer tasks: Figure 18: ViExam OCR ground truth tool - Initial setup interface showing data loading workflow and editing preparation. Compare LLM results with original images in Fig. 19 Correct all errors: incorrect character recognition, formatting errors in questions and answers (OCR), making the most accurate corrections to match the exam. Additionally, edit image descriptions generated by the LLM and add necessary information if the model missed details. Classify correction levels: No Edit: LLM results are completely correct, requiring no modifications. Minor Edits: Small corrections (spelling, punctuation). Major Edits: Large corrections (complex formulas, rewriting descriptions). Final output: Upon completion, users export .zip file containing corrected JSON files, automatically categorized into directories corresponding to correction levels. This creates complete ground truth dataset and information about initial LLM performance. Figure 19: ViExam OCR ground truth creation interface - Comparing and generating OCR ground truth annotations. Table 10: OCR evaluation results across different VLMs, showing significant performance gap between open-source and closed-source models Model BLEU Precision Recall F1 Score CER (%) WER (%) Aya Vision 8B Aya Vision 32B Gemma 3 4B Gemma 3 27B Qwen 2.5 VL 72B Qwen 2.5 VL 32B Llama 4 Maverick Llama 4 Scout Mistral Medium 3 Mistral Small 3.2 24B Gemini 2.5 Flash Sonnet 4.0 GPT 4.1 o3 Mean Open-source VLMs 0.25 0.37 0.81 0.85 0.88 0.81 0.54 0.87 0.83 0.29 0.40 0.84 0.91 0.91 0.89 0.78 0.90 0.89 Closed-source VLMs 0.89 0.95 0.95 0.96 0. 0.92 0.95 0.96 0.99 0.95 0.03 0.10 0.65 0.79 0.82 0.71 0.39 0.81 0.72 0.83 0.89 0.90 0.94 0.89 0.23 0.37 0.82 0.88 0.89 0.84 0.61 0.88 0.85 0.90 0.95 0.95 0.97 0.94 229.30 150.73 24.79 16.18 10.73 48.77 217.90 9.08 19. 14.46 4.24 4.10 3.90 6.68 253.09 180.28 37.23 23.89 18.49 53.91 110.60 16.11 28.41 17.57 7.52 7.04 5.16 9.32 The results suggest there may be relationship between OCR capability and overall performance of VLMs. Closed-source models with better OCR capabilities (BLEU 0.89-0.94) achieve significantly higher performance compared to open-source models (see Tab. 10). Particularly within the open-source group, models with poor OCR capabilities such as Aya Vision (BLEU 0.03-0.10) tend to achieve lower performance (7.99-17.38%), while Qwen 2.5 VL 72B with good OCR capability (BLEU 0.82) achieves the highest performance (41.77%) (see Tab. 2)."
        },
        {
            "title": "F More quantitative results",
            "content": "Table 11: VLMs performance across subject areas using full english prompts."
        },
        {
            "title": "Math Physics Chemistry Biology Geography Driving Test IQ Test Task mean",
            "content": "Aya Vision 8B 6.80 4.16 Aya Vision 32B 16.67 17.73 Mistral Small 3.2 24B 18.42 9.97 Mistral Medium 3 23.96 8.31 Llama 4 Scout 28.95 13.30 Llama 4 Maverick 34.65 13.57 Gemma 3 4B 30.04 23.27 Gemma 3 27B 44.74 35.73 Qwen 2.5 VL 32B 22.81 8.31 Qwen 2.5 VL 72B 30.92 16."
        },
        {
            "title": "Mean",
            "content": "Gemini 2.5 Flash Sonnet 4.0 GPT 4.1 o3 Mean 25.79 15.12 53.29 32.41 52.63 44.32 43.42 32.41 85.96 67.59 58.83 44.18 10.93 23.51 24.83 23.51 33.77 37.42 23.18 46.36 15.56 36. 27.58 Open-source VLMs 15.59 23.49 32.85 24.12 63.83 61.12 26.20 51.56 50.31 56.34 12.61 29.03 26.69 25.51 37.24 42.52 19.65 33.72 33.14 34.02 29.41 40.54 Closed-source VLMs 58.94 52.65 44.04 83.57 59.80 60.70 37.54 45.75 71.62 53.90 82.95 50.31 68.61 89.60 72. 29.70 37.33 41.69 43.32 50.41 56.40 37.33 48.77 56.13 50.95 45.20 52.59 54.77 65.67 74.04 61.77 13.75 25.83 32.22 27.92 30.00 42.50 25.00 38.33 30.00 40.00 30. 42.50 43.33 39.17 56.67 45.42 13.36 24.80 26.67 25.23 36.79 41.17 26.38 42.75 30.89 37.98 30.60 54.77 47.94 48.44 75.58 56. Table 12: VLMs performance on text-only questions across subject areas Math Physics Chemistry Biology Geography Driving Test Task mean Gemini 2.5 Flash Sonnet 4.0 GPT 4.1 o3 Mean 45.71 34.29 54.29 97. 57.86 42.86 54.29 54.29 88.57 60.00 74.29 68.57 65.71 94.29 75.72 88.57 57.14 68.57 71. 71.43 68.57 51.43 68.57 77.14 66.43 100.00 85.71 85.71 97.14 92.14 70.00 58.57 66.19 87. 70.60 Qualitative results The performance analysis across different subjects reveals varying levels of difficulty for VLMs, with accuracy ranging from 44.6% to 72.81%. Mathematics: Achieves moderate performance (56.30% mean accuracy) (Figs. 20 and 21), suggesting that mathematical notation and formulas provide universal visual cues that transfer across languages. Question images often follow consistent visual patterns (e.g., graph of function, variation chart). Its minimally influenced by cultural context or Vietnam-specific knowledge, making it more accessible to general-purpose VLMs. Physics: Proves most challenging among academic subjects (44.6% mean accuracy), possibly due to complex diagram interpretation requiring domain-specific knowledge of physics concepts and relationships (see Figs. 22 and 23). Chemistry: Achieves moderate performance (61.59% mean accuracy) (see Figs. 24 to 26), suggesting that chemical formulas provide universal visual cues that transfer across languages, similar to mathematics. Biology: Falls in the middle range (54.26% mean accuracy), indicating moderate success with biological illustrations and diagrams (see Figs. 27 and 28). Geography: Emerges as the most accessible domain (72.81% mean accuracy), not only relying on general knowledge, but rather due to the nature of its visual content (i.e., charts and graphs that explicitly display numerical values and useful information). As result, answering these questions often requires only reading, understanding, and comparison (e.g., higher vs. lower), without the need for complex reasoning steps (see Figs. 29 and 30). Driving Test: Presents an interesting case (67.51% mean accuracy): While relatively straightforward for humans, VLMs achieve only 67.51%, far below the expected near-perfect performance (see Figs. 1, 31 and 32). This underperformance likely stems from the models tendency to rely on surface-level visual heuristics rather than grounded understanding of traffic rules. Such mistakes highlight the gap in VLMs understanding of culturally specific driving norms and visual distinctions unique to the Vietnamese context. IQ Test: Proves challenging for both humans and VLMs (47.08% mean accuracy), which is understandable given the abstract reasoning and pattern recognition required in these assessments (see Figs. 33 and 34). Math Question: : Given function = (x with the graph shown, determine how many real solutions the equation 3f (x) 5 = 0 has on the interval [0, 4]. A. 2 B. 0 C. 1 D. 3 Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {5} Sonnet 4.0 : {5} GPT 4.1 : {5} o3 : {D} Figure 20: VLMs fail on the task of graph reading and counting real solutions of an equation over specified interval."
        },
        {
            "title": "Math",
            "content": "Question: : The graph on the right is = (x) (the derivative of ). For g(x) = 2f (x) (x 1)2, what is the maximum possible number of extrema? A. 9 B. 13 C. 7 D. 11 Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {C} Sonnet 4.0 : {A} GPT 4.1 : {B} o3 : {C} Figure 21: VLMs fail on tasks of counting extreme points of complex function graphs."
        },
        {
            "title": "Physics",
            "content": "Question: : mechanical wave is travelling in the positive Ox direction as shown. What is the wavelength? A. 120cm B. 30cm C. 90cm D. 60cm Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {D} Sonnet 4.0 : {B} GPT 4.1 : {D} o3 : {D} Figure 22: VLMs fail on tasks of finding wavelengths in mechanical wave problems."
        },
        {
            "title": "Physics",
            "content": "Question: : vertical springmass oscillator (with = 10 m/s2) is shown. The graph on the right plots the elastic force Fdh and the restoring force Fhp acting on the mass versus time t. Knowing t2 t1 = s, what is the average speed of the mass from time t1 to t3? A. 1,52 m/s. B. 1,12 m/s. C. 1,43 m/s. D. 1,27 m/s. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : π 12 Gemini 2.5 Flash : {B} Sonnet 4.0 : {B} GPT 4.1 : {C} o3 : {B} Figure 23: VLMs fail on average velocity calculations from overlapping force/time graphs in oscillation problems."
        },
        {
            "title": "Chemistry",
            "content": "Question: : During electrolysis of solution containing Cu(NO3)2, CuSO4 and NaCl with inert electrodes (current = 5 A, 100 % efficiency), results are recorded in the table. Determine the value of t. A. 1158. B. 1544. C. 1737. D. 2316. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {2136} Sonnet 4.0 : {B} GPT 4.1 : {B} o3 : {D} Figure 24: VLMs fail on tasks of analyzing electrolysis data tables to determine unknown time parameters in chemistry problems."
        },
        {
            "title": "Chemistry",
            "content": "Question: : X, Y, Z, and are different substances chosen from the following four: acetylene, acetaldehyde, ethanol, and acetic acid. Their boiling points are recorded in the table below: Consider the following statements: (a) Substance is capable of participating in the silver mirror reaction. (b) Substance dissolves in Cu(OH)2 at room temperature. (c) Complete combustion of substance produces number of moles of CO2 greater than that of H2O. (d) The reaction between substance and substance (with concentrated H2SO4 as catalyst) is known as an esterification reaction. How many of the above statements are correct? A. 4. B. 3. C. 2. D. 1. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {H2SO4} Sonnet 4.0 : {A} GPT 4.1 : {B} o3 : {B} Figure 25: VLMs fail on tasks of counting correct statements based on boiling point data tables and chemical properties analysis."
        },
        {
            "title": "Chemistry",
            "content": "Question: : Four separate solutions X, Y, Z, are tested as described in the table. Based on the observations, choose the correct identification of the four solutions. A. Na2CO3, NaHSO4, Mg(HCO3)2, BaCl2. B. Ba(OH)2, Ca(HCO3)2, KHSO4, Na2CO3. C. K2CO3, KHSO4, Ca(HCO3)2, Ca(OH)2. D. NaHSO4, Ba(HCO3)2, Na2CO3, Mg(HCO3)2. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {B} Sonnet 4.0 : {B} GPT 4.1 : {B} o3 : {B} Figure 26: VLMs fail on tasks of identifying chemical substances based on observation tables and reaction phenomena analysis."
        },
        {
            "title": "Biology",
            "content": "Question: : The diagram below describes chromosome segment from the salivary gland cells of Drosophila, containing six bands (labeled 1 to 6), corresponding to six different gene loci whose order on the chromosome is unknown (denoted to F). Researchers have isolated five deletion mutations (segments to V), originating from Drosophila line homozygous dominant for all six loci (as shown below). Each mutant (from to V) was crossed with Drosophila line that is homozygous recessive at all six loci (denoted to f). The results of these crosses are shown in the table below. Based on the data above, theoretically, how many of the following statements are correct? I. Chromosomal deletions can help identify which segments contain specific gene loci. II. Segment contains loci A, C, and F. III. The order of gene loci is B. IV. cross between flies with deletions III and IV results in 25% of offspring that are non-viable. We can conclude with certainty that the recombinant offspring is still viable and can develop normally. A. 2. B. 3. C. 1. D. 4. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {C} Sonnet 4.0 : {B} GPT 4.1 : {D} o3 : {C} Figure 27: VLMs fail on analyzing genetic data tables for chromosome mapping and gene linkage analysis in Biology."
        },
        {
            "title": "Biology",
            "content": "Question: The pedigree below shows two independently inherited diseases, each controlled by single gene. Assume no mutations occur in any individual in the pedigree. Based on genetic theory, how many of the following statements are correct? I. The genotypes of 8 individuals can be determined. II. The probability that couple 1516 has firstborn son affected by both diseases is 1 36 . III. The probability that couple 1516 has firstborn son affected by only one disease is 5 18 . IV. The probability that couple 1516 has firstborn daughter who is unaffected is 25 72 . A. 4 B. 1 C. 2 D. 3 Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {B} Sonnet 4.0 : {A} GPT 4.1 : {D} o3 : {B} Figure 28: VLMs fail on analyzing pedigree charts for genetic probability calculations and inheritance pattern analysis."
        },
        {
            "title": "Geography",
            "content": "Question: : Using the table of fish-production volume and export value for Viet Nam (2010 2015), which type of chart is most appropriate to display all three data series? A. Stacked bar chart - line chart. B. Area chart. C. Bar chart - line chart. D. Grouped chart - line chart. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {D} Sonnet 4.0 : {D} GPT 4.1 : {D} o3 : {C} Figure 29: VLMs fail on tasks of selecting appropriate chart types for multi-series data visualization in geography problems."
        },
        {
            "title": "Geography",
            "content": "Question: : The pie charts display the modal structure of international tourist arrivals to Viet Nam in 2010 2015. Which statement best matches the trend shown? A. The proportion of waterway transport increases very rapidly. B. The proportion of air transport decreases. C. The proportion of road transport does not increase. D. The proportion of road transport is the highest. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {D} Sonnet 4.0 : {C} GPT 4.1 : {D} o3 : {D} Figure 30: VLMs fail on analyzing trends from multiple pie charts showing transportation modal changes over time."
        },
        {
            "title": "Driving Test",
            "content": "Question: : Which sign(s) allow mopeds to enter? 1Sign 1. 2Sign 2. 3Both signs. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets, e.g., {2} Ground truth: : 3 Gemini 2.5 Flash : {2} Sonnet 4.0 : {2} GPT 4.1 : {2} o3 : {2} Figure 31: VLMs fail to interpret traffic prohibition signs to determine vehicle access permissions."
        },
        {
            "title": "Driving Test",
            "content": "Question: : Following the arrows in the diagram, which vehicle(s) violate traffic regulations? 1Buses, trucks. 3Cars, trucks. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets, e.g., {2} Ground truth: : 1 4Buses, trucks, cars. 2Buses, cars. Gemini 2.5 Flash : {4} Sonnet 4.0 : {3} GPT 4.1 : {4} o3 : {4} Figure 32: VLMs fail on analyzing traffic intersection scenarios to identify vehicles violating traffic regulations."
        },
        {
            "title": "IQ Test",
            "content": "Question: : Select the figure that correctly completes the 3 3 pattern. Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {D} Sonnet 4.0 : {C} GPT 4.1 : {C} o3 : {D} Figure 33: VLMs fail on visual pattern recognition tasks requiring completion of 3x3 geometric pattern matrices."
        },
        {
            "title": "IQ Test",
            "content": "Question: : Which figure on the right is identical to the figure on the left? Prompt: : Answer the multiple-choice question in the image. Put your choice in curly brackets with the option letter, e.g., {Z} Ground truth: : Gemini 2.5 Flash : {D} Sonnet 4.0 : {C} GPT 4.1 : {C} o3 : {B} Figure 34: VLMs fail on visual similarity recognition tasks requiring the identification of identical geometric shapes. 1. General Paper Structure"
        },
        {
            "title": "Reproducibility Checklist",
            "content": "1.1. Includes conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) yes 1.2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) yes 1.3. Provides well-marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes/no) yes 2. Theoretical Contributions 2.1. Does this paper make theoretical contributions? (yes/no) no If yes, please address the following points: 2.2. All assumptions and restrictions are stated clearly and formally (yes/partial/no) NA 2.3. All novel claims are stated formally (e.g., in theorem statements) (yes/partial/no) NA 2.4. Proofs of all novel claims are included (yes/partial/no) NA 2.5. Proof sketches or intuitions are given for complex and/or novel results (yes/partial/no) NA 2.6. Appropriate citations to theoretical tools used are given (yes/partial/no) NA 2.7. All theoretical claims are demonstrated empirically to hold (yes/partial/no/NA) NA 2.8. All experimental code used to eliminate or disprove claims is included (yes/no/NA) NA 3. Dataset Usage 3.1. Does this paper rely on one or more datasets? (yes/no) yes If yes, please address the following points: 3.2. motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) yes 3.3. All novel datasets introduced in this paper are included in data appendix (yes/partial/no/NA) yes 3.4. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with license that allows free usage for research purposes (yes/partial/no/NA) yes 3.5. All datasets drawn from the existing literature (potentially including authors own previously published work) are accompanied by appropriate citations (yes/no/NA) yes 3.6. All datasets drawn from the existing literature (potentially including authors own previously published work) are publicly available (yes/partial/no/NA) yes 3.7. All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing (yes/partial/no/NA) NA 4. Computational Experiments 4.1. Does this paper include computational experiments? (yes/no) yes If yes, please address the following points: 4.2. This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting (yes/partial/no/NA) yes 4.3. Any code required for pre-processing data is included in the appendix (yes/partial/no) yes 4.4. All source code required for conducting and analyzing the experiments is included in code appendix (yes/partial/no) yes 4.5. All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with license that allows free usage for research purposes (yes/partial/no) yes 4.6. All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/partial/no) yes 4.7. If an algorithm depends on randomness, then the method used for setting seeds is described in way sufficient to allow replication of results (yes/partial/no/NA) yes 4.8. This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks (yes/partial/no) yes 4.9. This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics (yes/partial/no) yes 4.10. This paper states the number of algorithm runs used to compute each reported result (yes/no) yes 4.11. Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information (yes/no) no 4.12. The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank) (yes/partial/no) partial 4.13. This paper lists all final (hyper-)parameters used for each model/algorithm in the papers experiments (yes/partial/no/NA) yes"
        }
    ],
    "affiliations": [
        "KAIST"
    ]
}