{
    "paper_title": "A Comprehensive Survey on Long Context Language Modeling",
    "authors": [
        "Jiaheng Liu",
        "Dawei Zhu",
        "Zhiqi Bai",
        "Yancheng He",
        "Huanxuan Liao",
        "Haoran Que",
        "Zekun Wang",
        "Chenchen Zhang",
        "Ge Zhang",
        "Jiebin Zhang",
        "Yuanxing Zhang",
        "Zhuo Chen",
        "Hangyu Guo",
        "Shilong Li",
        "Ziqiang Liu",
        "Yong Shan",
        "Yifan Song",
        "Jiayi Tian",
        "Wenhao Wu",
        "Zhejian Zhou",
        "Ruijie Zhu",
        "Junlan Feng",
        "Yang Gao",
        "Shizhu He",
        "Zhoujun Li",
        "Tianyu Liu",
        "Fanyu Meng",
        "Wenbo Su",
        "Yingshui Tan",
        "Zili Wang",
        "Jian Yang",
        "Wei Ye",
        "Bo Zheng",
        "Wangchunshu Zhou",
        "Wenhao Huang",
        "Sujian Li",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient processing of long contexts has been a persistent pursuit in Natural Language Processing. With the growing number of long documents, dialogues, and other textual data, it is important to develop Long Context Language Models (LCLMs) that can process and analyze extensive inputs in an effective and efficient way. In this paper, we present a comprehensive survey on recent advances in long-context modeling for large language models. Our survey is structured around three key aspects: how to obtain effective and efficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate and analyze LCLMs comprehensively. For the first aspect, we discuss data strategies, architectural designs, and workflow approaches oriented with long context processing. For the second aspect, we provide a detailed examination of the infrastructure required for LCLM training and inference. For the third aspect, we present evaluation paradigms for long-context comprehension and long-form generation, as well as behavioral analysis and mechanism interpretability of LCLMs. Beyond these three key aspects, we thoroughly explore the diverse application scenarios where existing LCLMs have been deployed and outline promising future development directions. This survey provides an up-to-date review of the literature on long-context LLMs, which we wish to serve as a valuable resource for both researchers and engineers. An associated GitHub repository collecting the latest papers and repos is available at: \\href{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\\color[RGB]{175,36,67}{LCLM-Horizon}}."
        },
        {
            "title": "Start",
            "content": "Jiaheng Liu,, Dawei Zhu,, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu Guo, Shilong Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie Zhu, Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Yingshui Tan, Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li, Zhaoxiang Zhang NJU, PKU, CASIA, Alibaba, ByteDance, Tencent, Kuaishou, M-A-P"
        },
        {
            "title": "Abstract",
            "content": "Efficient processing of long contexts has been persistent pursuit in Natural Language Processing. With the growing number of long documents, dialogues, and other textual data, it is important to develop Long Context Language Models (LCLMs) that can process and analyze extensive inputs in an effective and efficient way. In this paper, we present comprehensive survey on recent advances in long-context modeling for large language models. Our survey is structured around three key aspects: how to obtain effective and efficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate and analyze LCLMs comprehensively. For the first aspect, we discuss data strategies, architectural designs, and workflow approaches oriented with long context processing. For the second aspect, we provide detailed examination of the infrastructure required for LCLM training and inference. For the third aspect, we present evaluation paradigms for long-context comprehension and long-form generation, as well as behavioral analysis and mechanism interpretability of LCLMs. Beyond these three key aspects, we thoroughly explore the diverse application scenarios where existing LCLMs have been deployed and outline promising future development directions. This survey provides an up-to-date review of the literature on long-context LLMs, which we wish to serve as valuable resource for both researchers and engineers. An associated GitHub repository collecting the latest papers and repos is available at: LCLM-Horizon. 5 2 0 2 0 2 ] . [ 1 7 0 4 7 1 . 3 0 5 2 : r Figure 1. Evolution of Long Context Processing over time. The advent of LCLMs efficiently processes millons of data in minutes and unlocks intriguing applications. Project Leaders (Equal Contribution). Core Contributors (Equal Contribution). Corresponding Authors."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Data"
        },
        {
            "title": "2.1 Pre-training .",
            "content": ". . . . ."
        },
        {
            "title": "2.2 Post-training .",
            "content": ". . . . ."
        },
        {
            "title": "2.3 Training Data .",
            "content": ". . . . . . . . . . . . . . . . . . . . 3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 7 8 8 8 9 9 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.1 Position Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Position Embedding Types . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.2 Extrapolation Methods of Position Embeddings . . . . . . . . . . . . . . . 3.2 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Transformer-Based Architecture . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Linear-Complexity Architecture . . . . . . . . . . . . . . . . . . . . . . . . 3.2.3 Hybrid Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Workflow Design 4.1 Prompt Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Hard Prompt Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Soft Prompt Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Memory-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 RAG-Based Methods . 4.4 Agent-Based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Infrastructure 5.1 Training of LCLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 I/O Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Optimizations on GPU Constraints and Memory Access . . . . . . . . . . 2 10 11 11 15 16 20 23 25 26 27 28 30 32 33"
        },
        {
            "title": "5.1.3 Optimizations on Communication-Computation Overlapping . . . . . . .",
            "content": "5.2 Inference of LCLMs . ."
        },
        {
            "title": "5.2.1 Quantization .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.2.4 GPU-CPU Parallel Inference",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.5 Speculative Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Evaluation"
        },
        {
            "title": "6.1.2 Evaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "6.1.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Evaluating Long-Form Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 Evaluation Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.2 Data Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.3 Evaluation Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Analysis 7.1 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.1 The False Promise of Support Context Length . . . . . . . . . . . . . . . . . 7.1.2 Relevance of Long Context Perplexity and Real-World Performance . . . 7.1.3 RAG Meets Long Context LLMs . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Model Structure Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.1 Positional Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.2 Attention and MLP Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.3 Layer Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Application 8.1 Application in Agent . 8.2 Application in RAG . . . . 8.3 Application in Chatbot . 8.4 Application in Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 37 38 38 39 40 40 40 40 47 47 48 51 51 53 53 54 54 55 55 57 58 58 58 59"
        },
        {
            "title": "8.7 Application in Specific Domains",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Future Directions"
        },
        {
            "title": "9.5 Towards Mechanistical Interpretability for Long Context Modeling . . . . . . . .",
            "content": "10 Conclusion 11 Contributions and Acknowledgments 60 61 62 63 63 65 66 66 68 4 1. Introduction Efficient data processing has long been humanitys aspiration, as our biological constraints limit us to localized, serial reading, making the manual processing of long-context of data painfully inefficient. As shown in Figure 1, consider these historical moments: In 300 BCE, librarians at the Library of Alexandria had to manually copy, proofread manuscripts, and compile catalogs to manage hundreds of thousands of ancient texts [553]. In 800s, astronomical officials of the Tang Dynasty needed to manually process vast astronomical measurement data to calculate seasonal divisions [554]. In 1400s, typesetters had to manually arrange enormous printing plates before newspapers could be printed [552]. Even into 1900s, scholars still needed to meticulously examine dozens or even hundreds of relevant documents before achieving comprehensive understanding of subject. revolutionary leap finally arrived with the emergence of language models [42, 108, 197, 312, 320, 503, 546, 566, 599, 600, 633, 635], which offer the promise of automatically processing text data in minutes. These language models operate within fixed context window, probabilistically modeling input sequences and enabling next-token prediction. Early language models were limited to processing only few or dozens of tokens [36, 61, 362, 445]. As context lengths expanded to several hundred or thousand tokens, represented by BERT [101] and GPT-3 [37] respectively, automatic processing of paragraphs, documents, and multi-turn conversations became achievable for the first time. Building on these advances, recent years have witnessed the advent of Long Context Language Models (LCLMs) with their context length growing exponentially from 4K to 128K [154], 1M [596] and even 10M [499] tokens, enabling single-pass ingestion of Tolstoyan-scale narratives (560K words) and effectively condensing 60 hours of human reading into minutes of computational processing. More importantly, these extensive context lengths provide sufficient space for test-time scaling [164, 386], where models can explore, reflect, backtrack, and summarize within single context, which fundamentally transforms our interaction with generative AI and unlocks series of intriguing abilities, including: o1-like long reasoning [164, 373, 386], complex agent workflows [506], superior in-context learning [366, 499], efficient information retrieval & comprehension [262, 531], and advanced multimodal intelligence [507, 550]. In this paper, we present comprehensive survey on long context language modeling. As shown in Figure 2, our survey is oriented with the following three pivotal dimensions: RQ1: How to obtain effective and efficient LCLMs? RQ2: How to train and deploy LCLMs efficiently? RQ3: How to evaluate and analyze LCLMs comprehensively? Beyond these three key aspects, we also thoroughly explore the diverse application scenarios of existing LCLMs. First, to obtain effective and efficient LCLMs (RQ1), we review data strategies ( 2), architecture designs ( 3), and workflow design ( 4). For data strategies, we provide detailed discussion on data engineering works for building LCLMs across both pre-training and posttraining phase, including data selection, data filtering, data synthesis, data mixture, etc. For architecture design, we first examine position embeddings commonly adopted by LCLMs, and their extrapolation strategies, then we systematically discuss three major types of architectural designs: transformer-based modifications, linear-complexity architectures, and hybrid approaches that integrate both paradigms. On top of this, we further introduce workflow designs that expand the scope of single LCLM, including prompt compression, memory-based workflow, RAG-based workflow, and agent-based workflow. Second, in order to efficiently train and deploy LCLMs (RQ2), we thoroughly summarize AI infrastructure optimization strategies ( 5). For training infrastructure, we examine 5 Pre-training Data Data Mixture Long-Context Data Engineering [127], GrowLength [228] Data Filtering Longwanjuan [326], LongAttn [561] Data (2) Architecture (3) Post-training Data Position Embeddings Data Synthesis ICP [456], SPLICE [471], Quest [130] Data Filtering GATEAU [462] Data Synthesis Ziya-Reader [175], MIMG [71], Chat QA2 [590] Position Embedding Types RoPE [476], Alibi [408], T5 [423], CoPE [153], XPOS [482] Extrapolation Methods of Position Embeddings ReRoPE [474] , ABF [583], NTK [401], YaRN [401], LongRoPE [102] Transformer-Based Architecture Transformer-XL [90], Longformer [26], MoA [126], NSA [626], GQA [79] How to obtain effective and efficient LCLMs? Workflow Design (4) Attention Linear-Complexity Architecture Mamba [155], lightning attention [414], RetNet [483], ReMamba [625] Hybrid Architecture Jamba [302], Hymba [105], Samba [427], YOCO [484], Minimax-01 [366] Prompt Compression Hard Prompt Compression AdaComp [646], PCRL [234], LLMLingua [216], CompAct [618] Soft Prompt Compression ICAE [140] , xRAG [72], UniICL [131], Gist [371], Beacon [645] Memory-Based Methods Continuous Memory LongMem [540], MemoryLLM [545] Language Memory MemoryBank [675], RecurrentGPT [680] RAG-Based Methods Agent-Based Methods Parametric Memory DSI [496], DSI++ [355], Generative Adapter [62] Chunking Retrieval Late Chunking [167], Contextual-Retrieval [14] BGE-M3 [55], ModernBERT [548], REAPER [231] Generation Fusion-in-Decoder [206], kNN-LM [242], Retro [31] Single-Agent Architectures ReadAgent [263], GraphReader [286], MemWalker [51], RecurrentGPT [680] Multi-Agent Systems CoA [656], LongAgent [663], Optimization on I/O Dataset-decomposition [406], SPLICE [470], Qwen2.5-1M [596], 3FS [96] Training of LCLMs Optimization on GPU Constraints DeepGEMM [661], FPTQ [283], FlashAttention [92], NSA [627], UP [208] i o a a e C L How to train and deploy LCLMs efficiently? Infrastructure (5) Optimization on Communication FLUX [45], DualPipe [97], TCCL [243], Multi-streams [468] Quantization FlexGen [454], SmoothQuant [578], KVQuant [187] Memory Management vLLM [257], SGLang [673] Inference of LCLMs Prefilling-Decoding Disaggregated Architecture Splitwise [396], Mooncake [411] GPU-CPU Parallel Inference PipeSwitch [22], FastDecode [173] Speculative Decoding Speculative Decoding [269], Medusa [39], Eagle [295] Evaluation (6) Evaluating Long Context Comprehension Evaluating Long-Form Comprehension Evaluation Paradigm LongPPL [121], SummHay [258], BABILong [254], SWE-Bench [224] Evaluation Benchmark NIAH [236, 370], RULER [189], LongBench [17, 20] Evaluation Benchmark ELI5 [118], LongWriter [21], FActScore [365], ProxyQA [486] Data Source LOT-Gen [160], MS-NLG [380], Self-Lengthen [417] Evaluation Paradigm ROUGE [303], BLEU [392], METEOR [23], LCFO [87], HelloBench [418] The False Promise of Supported Context Length Lost in the Middle [317], RULER [189] Performance Analysis Relevance of Long Context Perplexity and Real-World Performance Can PPL Reflect Long Text Understanding [193], LongPPL [121] Analysis (7) RAG Meets Long Context LLMs LongRAG [223], Long-context llms meet rag [226] Positional Embeddings RoPE [476], PI [60], Scaling Laws of RoPE-based Extrapolation [325] Model Structure Analysis Attention and MLP Analysis Retrieval Head [565], StreamingLLM [580], HeadKV [129], Voita et al. [516] Layer Interaction MiniMax-01 [366], Rope to Nope and Back Again [597] How to evaluate and analyze LCLMs comprehensively? Application in Agent Agents [681] , WebArena [678], OS-World [582], TravelAgent [47] Application in RAG Perplexity [259], Genspark [144], Microsoft Copilot [360], Deepsearch [435] Application in Chatbot ChatGPT [385], Character.ai [46], SpicyChat [469], Pi [204], Talkie [6] Application in Code RepoCoder [634], StarCoder2 [334], Qwen2.5-Coder [202], GitHub Copilot [146], Cursor [15] Application (8) Application in Traditional NLP Tasks Application in Multimodal Tasks Application in Specific Domains Document-level NMT [181], M3-Embedding [55], Text-Embedding-3-Large [384] Gemini [499] , Qwen2.5-VL [507], mPLUG-Owl3 [612], LongVILA [594], LongLLaVA [541] MedOdyssey [120], LongFin [354], MegaDNA [446] Figure 2. Taxonomy of Long Context Language Modeling. I/O optimization, optimizations on GPU constraints & memory access, and optimizations on communication-computation overlapping. For inference infrastructure, we review five types of efficient inference strategies, including quantization, memory management, prefilling-decoding disaggregated architecture, GPU-CPU parallel inference, and speculative decoding. Third, building on top of effective LCLMs and efficient infrastructure, we proceed to discuss their performance evaluation ( 6) and analysis ( 7) (RQ3). The evaluation section divides 6 Survey Data Architecture Workflow Design Infrastructure Evaluation Analysis Huang et al. [199] Zhao et al. [664] Li et al. [297] Dong et al. [106] Ours Table 1. Comparisons between our survey and other related long context modeling surveys. long context capabilities into two broad categories: long context comprehension and long-form generation. For each category, we discuss the evaluation paradigm, and present an overview of existing benchmarks. For the analysis section, we include both the external performance analysis (e.g., the effective context length, PPL metric, lost-in-the-middle) and the internal model structure analysis (e.g., position embedding, attention head, mlp layers). Finally, in Section 8, we summarize the main applications for long context large language models (LLMs), including the agent, RAG, Code, multimodal tasks and etc, and in Section 9, we propose five potential future directions for LCLMs including the long Chain-of-Thought reasoning, effective long context extension, efficient architecture and infrastructure, robust evaluation, and mechanistic interpretability. As shown in Table 1, we also compare with this survey paper with existing surveys [106, 327, 397] on long context modeling, and we observe that existing surveys usually focus on several specific topics in long context modeling. In contrast, this comprehensive survey provides an indepth exploration of the rapidly evolving landscape of LCLMs, and address the aforementioned RQs for long context modeling by covering wide range of topics. In summary, by providing comprehensive overview of the current state of long context language models, this survey aims to serve as valuable resource for researchers, practitioners, and enthusiasts in the field of NLP. We hope to shed light on the progress made thus far, highlight the remaining challenges, and inspire future innovations in this exciting research area. 2. Data In this section, we discuss the data-related topics for long context modeling. Specifically, as shown in Figure 3, the long context data is used both in the pre-training and post-training stages. To illustrate the data processing pipeline for LCLMs, we first provide the data strategies of long context pre-training data ( 2.1), including data filtering, data mixture and data synthesis. Then, we illustrate the data strategies of long context post-training data ( 2.2), including data filtering and data synthesis. Figure 3. Illustration of training pipeline of LCLMs. 2.1. Pre-training 2.1.1. Data Filtering Foundation models performance is significantly influenced by the quality of pre-training data as shown in many studies [110, 162, 421, 635], and various approaches have been employed to enhance data quality. Specifically, some works [423, 464, 635] have implemented many heuristic rules (e.g., removing short entries) and duplication methods (e.g., MinHashLSH deduplication [267]) to improve the data quality. Besides, SemDeDup [3] is proposed to utilize the embeddings from existing pre-trained models to remove semantic duplicates. In addition, several methods have been proposed to improve the diversity and complexity of the data. For example, Maharana et al. [350] viewed diversity and difficulty as complementary factors, employing dataset graph with bidirectional message passing for data selection, and Tirumala et al. [509] applied clustering techniques to broaden the data diversity. Nevertheless, the aforementioned approaches primarily address standard pre-training datasets, typically limited to 4,000 or 8,000 tokens. Recently, researchers have developed specialized criteria to evaluate the quality of extended context from various perspectives. For example, the work [326] introduced comprehensive framework for assessing the quality of long texts, which mainly includes three core linguistic aspects: coherence, cohesion, and complexity. To quantify these aspects, various metrics including statistical measures and evaluations based on pre-trained language models have been proposed. Meanwhile, apart from Longwanjuan, another representative work [58] is proposed to evaluate training samples based on their long-range dependency characteristics. This approach also utilizes three key metrics (i.e., dependency strength, dependency distance, dependency specificity) to assess and prioritize samples that are particularly beneficial for enhancing models long context comprehension. Recently, some works have begun to use the attention pattern to select high-quality long context data. For example, the LongAttn [561] uses the self-attention mechanism to quantify the long long-range dependencies for accurate and efficient data selection. 2.1.2. Data Mixture Language model training typically utilizes datasets drawn from diverse sources [37, 82, 109, 132, 135]. For instance, The Pile [132], widely accessible dataset, comprises various components including 24% web content, 9% Wikipedia, and 4% GitHub, etc. Besides, many works have observed that the composition of pre-training data significantly impacts an LMs performance [109, 184], and current approaches to determining domain weights (sampling probabilities for each source) often rely on intuition or specific downstream tasks. For example, The Pile employs heuristically chosen weights, which may not be optimal. PaLM [82] and GLaM [109] fine-tune these weights based on selected downstream tasks, which need to training numerous LMs with different weight configurations. To address this limitation, Data Mixing Laws [613] and RegMix [319] investigate the data-mixing scaling law to find the optimal domain mixture for improving the pre-training efficiency with minimal training costs. However, the above-mentioned methods usually focus on pre-training data within limited context windows (e.g., 4k/8k), and several works have begun to focus on the data mixture on the long context pre-training. For example, [127] explore five types of long context data composition including Cutting documents at 4K, Cutting documents at 128K, Global upsampling, Per-source upsampling and Upsampling Arxiv / Book / Github and have following observations: (1) Continual pre-training on small amount of long context data can significantly improve 7B models capability to accurately retrieve information over long context input. (2) When scaling up context length, it is crucial to oversample lengthy sequences while maintaining the original domain diversity of the pre-training datasets. ProLong [135] shows that incorporating code repositories and long books as long context sources, and mixing them with high-quality short-context sources is vital for improving performance on extended inputs while preserving the models proficiency with short contexts. The GrowLength approach [228] progressively expands the input size throughout the pre-training process, optimizing computational resources and boosting efficiency. 2.1.3. Data Synthesis As the long context data is usually rare in the real-world corpus (e.g., web documents), many methods have been proposed to synthesize or construct the long context pre-training data. For example, one approach clusters semantically related texts within single context window [166], while another work [271] demonstrates improved sentence representations by incorporating related, non-adjacent sentences in pre-training examples. Besides, to address document redundancy, traveling salesman algorithm has been employed in ICP [456]. Moreover, SPLICE [471] involves structured packing, where training samples are created by combining multiple similar retrieved documents. Additionally, query-centric synthesis method Quest [130] has been proposed to aggregate diverse yet semantically linked documents. This approach utilizes generative model to predict potential queries for each document, subsequently grouping documents with similar queries and keywords. 2.2. Post-training 2.2.1. Data Filtering Unlike pre-training, the data filtering strategy in post-training usually aims to select influential samples to empower the LLMs instruction-following capabilities. Chen et al. [54, 57], Liu et al. [323], Wu et al. [561] explore the use of feedback from proprietary language models to curate training samples. Meanwhile, Cao et al. [41], Ge et al. [141], Li et al. [281], Xia et al. [577] have developed sophisticated metrics based on open-source LLMs to evaluate the importance of samples. However, these methods only focus on selecting short-from SFT data, overlooking the specific challenges posed by long context alignment. Recently, the GATEAU [462] introduces two components (i.e., Homologous Models Guidance and Contextual Awareness Measurement) to identify the influential samples with long-range dependency relations. 2.2.2. Data Synthesis For the data synthesis of post-training, we need to construct the long context queries effectively, and many works have been proposed. For example, Ziya-Reader [175] constructs tailored Multi-doc QA task that requires concentration on different positions in contexts to address the lost in the middle problem [317]. Xiong et al. [584] proposes to select document from pre-training corpus and prompt the language model to write question-answer pairs based on information in the selected chunk. Recently, An et al. [13] presents information-intensive training to overcome lost-in-the-middle, which leverages synthesized long context questionanswer dataset including two types of questions (i.e., fine-grained information awareness on exactly one short segment, and the integration and reasoning of information from two or more segments). Xu et al. [590] introduce to assemble all the related paragraphs and randomly insert the ground truth summary to simulate real long document for obtaining long context instructions. Chen et al. [71] propose the Multi-agent Interactive Multihop Generation (MIMG) framework, incorporating Quality Verification Agent, Single-hop Question Generation Agent, 9 Training Data Characteristics Longwanjuan [326] Bilingual, filtered from SlimPajama [464] and Wanjuan [172] Long-Data-Collections [2] wide variety of data sources LongAttn [561] Long-range dependency selected using attention patterns Stage Pre-training Pre-training Post-training Post-training LongAlign [19] FILM [13] PAM QA [175] LongAlpaca [69] ChatQA2 [590] LongMIT [71] Diverse tasks and various sources, Self-Instruct Information-Intensive, context length balance, multi-hop reasoning Post-training Position-agnostic, multi-hop reasoning Self-collected, instruction following Synthesized from NarrativeQA [246] Multi-hop, diverse, automatic Post-training Post-training Post-training Post-training LongWriter-6k [21] Long-form generation, output lengths ranging from 2k to 32k word Post-training Long Reward [638] Bilingual, preference optimization LOGO [493] LongDPO [405] Preference optimization Long-form Generation, preference optimization, step-level Post-training Post-training Post-training Table 2. Overview of training datasets for long context modeling. Multiple Question Sampling Strategy, and Multi-hop Question Merger Agent, to improve the quality of long context instruction data. Recently, several works [493, 638] have begun to focus the preference optimization on long context models [387, 422] to align models with human preferences. representative work DPO [422] can eliminate the need for separate reward model, and teach the model to reject misaligned responses and accept preferred responses with differently assigned prediction scores. Meanwhile, many efforts have been made to enhance the effectiveness and efficiency of DPO, such as SimPO [358], ORPO [185], TPO [438], 2D-DPO [287], and Constitutional DPO [538]. However, these diverse approaches mainly focus on short-context scenarios, and the long context preference optimization is ignored. Recently, several works for long context preference optimization have been proposed. For example, LongReward [638] utilizes an off-the-shelf LLM to provide rewards for long context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, and then obtain the preference pairs based on the synthesized queries. LOGO [492] introduces the importance of scoring with an automatic evaluator to synthesize preference (aligned) and dispreference (misaligned) data in long context comprehension scenarios. More recently, long context generation has drawn great attentions, and Ping et al. [405] propose the LongDPO method, which uses the Monte Carlo Tree Search to gather stepwise preference pairs and adopt the step-level DPO to improve the capabilities of existing LLMs. 2.3. Training Data Summary on the long context datasets for pre-training and post-training are as shown in Table 2. 3. Architecture For long context language models, architectural design faces dual challenges: effectively processing extremely long texts while maintaining computational efficiency in both training and inference phases. This section presents comprehensive review of architectural designs tailored for long context modeling. First, we examine position embeddings commonly adopted by 10 Position Embeddings t i A Position Embedding types Extrapolation Methods Transformer-Based Architecture Attention Linear-Complexity Architecture Absolute Relative Sinusoidal[512], Learned[142], Complex[519] , NoPE[76, 241] RoPE[476], Alibi[408], Sandwich[75], XPOS[482], T5[423], HoPE[67] Content-Aware CoPE[153], DAPE[671] Position Reorganization DCA[11], ReRoPE[474] , String[12], SelfExtend[229] Position Interpolation ABF[583], NTK[401], YaRN[401], LongRoPE[102] Hierarchical Position Embedding Position Simulation Sparse Attention BiPE[178], HiRoPE[642] RandPos[433], PoSE[686], Cream[563], SkipAlign[564] , LongRecipe[194] Longformer[26], MoA[126], NSA[626], Weighted GQA[79], Duo Attention[579], CORM[89], LongHeADS[339], SnapKV[294] Hierarchical Attention HAN[609], Hi-Transformer[557], ERNIE-SPARSE[328] Recurrent Transformer Transformer-XL [90], Memformer [562], Block-Recurrent Transformer[203], RMT[38], Infinite Attention [374] SSM Mamba[155], ReMamba[625], TAIPAN[379], DeciMamba[27] Linear Attention Linear Transformer[240], lightning attention[414], RetNet[483] Layer-wise Jamba[302], RecurrentGemma[33], Minimax-01[366], Command R[86] Hybrid Architecture Prefill-Decode RWKV[398], YOCO[484], GoldFinch[152] Head-wise Hymba[105], Samba[427] Figure 4. Taxonomy of Long Context Model Architectures. LCLMs, and their extrapolation strategies ( 3.1). Then, we systematically discuss three major types of architectural designs: transformer-based modifications ( 3.2.1), linear-complexity architectures ( 3.2.2), and hybrid approaches that integrate both paradigms ( 3.2.3). 3.1. Position Embeddings Most long context LLMs are based on the popular Transformer architecture. Since the Transformer computes the representations of all tokens in parallel, it is necessary to integrate positional information within the sequence with the aid of positional encoding. In this subsection, we will first introduce different types of positional embeddings used in the Transformer, and then demonstrate the extrapolation methods for adapting the positional encoding to larger sequence lengths. 3.1.1. Position Embedding Types As shown in Table 3, positional embeddings in LLMs can be classified into absolute positional embeddings, relative positional embeddings, and content-aware positional embeddings according to the form of their positional information. Absolute Position Embedding Absolute Position Embedding provides each token with information about its absolute position in the sequence. This approach can be categorized into three types: (1) directly specifying the positional embedding (PE) for each position through 11 Category Position Embedding Parametric Representation Injection Method Absolute Relative Sinusoidal [512] Learned [142] Complex [519] NoPE [76, 241] Relative Position [451] T5 [423] Alibi [408] Kerple [74] Sandwich [75] FIRE [285] RoPE [476] XPOS [482] HoPE [67] Content-Aware CoPE [153] DAPE [670, 671] Embedding Embedding Embedding NA Embedding Bias Bias Bias Embedding Bias Embedding Embedding Embedding Embedding Bias Add Add Multiply NA Add Add Add Add Add Add Multiply Multiply Multiply Add Add Table 3. Overview of position embeddings for the Transformer architecture. functional approach; (2) setting the PE for each position as trainable vectors that are updated during training; and (3) omitting positional embeddings entirely and relying on unidirectional attention in decoder-only architectures to learn positions implicitly. Below are the details: Functional Positional Embedding [512]. The most representative example of functional position embedding is Sinusoidal Positional Embedding, which was proposed by the vanilla Transformer [512] and was widely used in variant models based on Transformers. It encodes positional information through periodic sine and cosine functions. In Transformer, it is usually added to word embeddings to inject absolute positional information into the network. Learned Positional Embedding [142]. It was firstly proposed by Meta to introduce positional information into the convolutional sequence-to-sequence learning. The embedding representations of different positions are treated as learnable parameters and trained along with the network. This is widely used in models such as BERT [101], GPT [37], and OPT [647]. There are also some variants [519] that represents the position embedding in the complex plane and combines it with the continuous word embedding which is also represented in complex vectors. No Position Embedding (NoPE) [76, 241]. NoPE proposes to incorporate no explicit position encoding into Transformer. It founds that the model still learns the permutation order of tokens. NoPE is also effective in language modeling and several probing tasks. [76] prove that NoPE in causal language models can incorporate implicit absolute positional information into the models. Note that NoPE also conveys relative positional information [241]. Relative Position Embedding Instead of focusing on absolute positional information, Relative Position Embedding [451] captures the relative distances between tokens, as it assumes that relative positional information is more crucial for language understanding. For LCLMs, commonly used Relative Position Embeddings can be categorized into following three types: T5-Style [423]. T5 initially maps the relative distance (ùëñ ùëó) between tokens at positions ùëñ 12 and ùëó to scalar bias value ùëè = ùëì (ùëñ ùëó), where ùëì represents lookup table. Subsequently, the relative bias ùëè (learned during the training process) is incorporated into the dot product of the query and key within the self-attention mechanism. The lookup table assigns the same parameter to distances exceeding certain threshold, thereby facilitating generalization to unobserved distances. Base on T5s position embedding, FIRE [285] proposes to map the positional representations as learnable function. To solve the length generalization issue when the test length varies, FIRE proposes progressive interpolation by normalizing the distance by query position index. In this way, FIRE can map any test length into the training range. ALiBi [408] and its Variants. ALiBi, which is adopted in BLOOM [439] and Baichuan [595], is similar to T5s position encoding. However, it differs in that it subtracts scalar bias from the attention score. This bias increases linearly in proportion to the distance between the query and key tokens. As result, this actually generates preference for tokens that are closer in position, which is known as recency bias. Alibi can be formulated as the following: ùëá qùëñk ùëó = (xùëñWùëû)(x ùëóWùëò)ùëá + ùëö( ùëó ùëñ) (1) where ùëö is head-specific scalar hyper-parameter. ùëñ and ùëó mean the position indices of and k. Wùëû and Wùëò represent the projection layer in the attention. Building on top of ALiBi, Kerple [74] introduces two trainable parameters for better length extrapolation, while Sandwich [75] simplifies the form of sinusoidal position embedding by only considering the cross term of position embeddings to alleviate the overfitting issue of sinusoidal position embedding. Rotary Position Embedding (RoPE) [476] and its variants. RoPE rotates the query and key representations by an angle that is proportional to their absolute positions prior to dot product attention. Owing to this rotation, the attention dot product will rely solely on the relative distance between tokens, thereby effectively transforming it into relative positional encoding. To elucidate, given hidden vector = [‚Ñé0, ‚Ñé1, ..., ‚Ñéùëë1], where ùëë is the hidden dimension, and position index ùëö, RoPE operates as follows: ùëì (h, ùëö) = ‚Ñé0 ‚Ñé1 ‚Ñé2 ‚Ñé3 ... ‚Ñéùëë2 ‚Ñéùëë1 (cid:169) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:173) (cid:171) (cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:172) cos ùëöùúÉ0 cos ùëöùúÉ0 (cid:169) (cid:173) (cid:173) cos ùëöùúÉ1 (cid:173) (cid:173) cos ùëöùúÉ1 (cid:173) (cid:173) ... (cid:173) (cid:173) (cid:173) cos ùëöùúÉùëë/21 (cid:173) cos ùëöùúÉùëë/21 (cid:171) (cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:172) + ‚Ñé1 ‚Ñé0 (cid:169) (cid:173) (cid:173) ‚Ñé3 (cid:173) (cid:173) ‚Ñé2 (cid:173) (cid:173) ... (cid:173) (cid:173) (cid:173) ‚Ñéùëë1 (cid:173) ‚Ñéùëë2 (cid:171) (cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:172) sin ùëöùúÉ0 sin ùëöùúÉ0 (cid:169) (cid:173) (cid:173) sin ùëöùúÉ1 (cid:173) (cid:173) sin ùëöùúÉ1 (cid:173) (cid:173) ... (cid:173) (cid:173) (cid:173) sin ùëöùúÉùëë/21 (cid:173) sin ùëöùúÉùëë/21 (cid:171) (cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:174) (cid:172) (2) where ùúÉ ùëó = 100002 ùëó/ùëë, ùëó {0, 1, ..., ùëë/2 1}. Given query at position ùëö and key at position ùëõ, attention score ùëé(q, k) is defined as: ùëé(q, k) =< ùëì (q, ùëö), ùëì (k, ùëõ) > ùëë/21 = ùëó=0 [(ùëû2 ùëóùëò2 ùëó + ùëû2 ùëó+1ùëò2 ùëó+1) cos (ùëö ùëõ)ùúÉ ùëó + (ùëû2 ùëóùëò2 ùëó+1 ùëû2 ùëó+1ùëò2 ùëó) sin (ùëö ùëõ)ùúÉ ùëó] := ùëî(q, k, (ùëö ùëõ)ùúΩ) (3) where g() is an abstract mapping function exclusively dependent on q, and (ùëö ùëõ)ùúΩ. Benefiting from this good nature, RoPE becomes the most pervasive position embedding strategy in the era of LLMs, including LLaMA [510], Qwen [596], etc. Sun et al. [482] Intuition Position Reorganization Position Interpolation Hierarchical Position Position Simulation Method SelfExtend [229] DCA [11] ReRoPE [474] String [12] PI [60] NTK [401] ABF [583] YaRN [401] Truncated Basis [389] CLEX [49] Resonance RoPE [537] LongRoPE [102] MsPoE [659] BiPE [178] HiRoPE [642] RandomPE [433] PoSE [686] SkipAlign [564] Cream [563] LongRecipe [194] Training-free Table 4. RoPE variants of length generalization. ascribes that the weak extrapolation ability of RoPE is related to the significant oscillation in their attention expectations. To address this problem, they propose XPOS that incorporates balancing term to penalize the oscillation of unstable dimensions while maintaining the distribution of stable dimensions. Also building upon RoPEs foundation, HoPE [67] enhances length extrapolation by replacing specific components with position-independent ones while retaining only high-frequency signals. Content-Aware Position Embedding More recently, there has been growing interest in Content-Aware Position Embedding [153, 671], which argues that position measurement should take into account more semantically meaningful units such as words or sentences, as illustrated below: CoPE [153]. CoPE considers the joint modeling of content and position information by leveraging the content of the tokens and their positions in the sequence. Specifically, CoPE first calculates context-dependent gate values, and then employs these values to determine token positions through cumulative summation process. In this way, positions embeddings are equipped with contextualized information. DAPE [670, 671]. It proposes to model the positional information dynamically with the attention. Specifically, DAPE determines the position bias by not only the position indices but also the semantics information. In this way, DAPE overcomes the inflexibility and achieves relatively optimal performance for each individual instance by dynamically adjusting on each specific input data. 14 3.1.2. Extrapolation Methods of Position Embeddings For language model with an original context window size of ùêøùëú, when processing target sequence of length ùêøùë° (with scaling factor ùõº = ùêøùë°/ùêøùëú) that exceeds this range, the first challenge is the position encoding OOD problem, as ùêøùëú position encodings cannot cover larger range. As mentioned in LM-Infinite [168], position encoding OOD is significant factor hindering length extrapolation. From the perspective of avoiding out-of-distribution positions, position encoding-based length extrapolation strategies can be mainly divided into two categories: (1) Mapping the target sequence to the position range supported by the model, and (2) Enabling the model to support position ranges larger than the context window size. Notably, most methods in the first category can perform well without additional tuning, while designs in the second category often rely on training to be effective. Mapping target sequences to the models supported position range can be further divided into two lines: Position Reorganization: This approach reorganizes, particularly reuses, position indices that appeared during training to handle inputs exceeding training length. This idea of reorganizing position indices was evident in the T5 model [423], which supports 512 tokens input but only contains 32 types of relative positions. For mainstream large language models using RoPE encoding, similar approaches can be applied. In SelfExtend [679], for each token, normal relative positions are maintained for the nearest ùë§ tokens, while distant tokens are grouped. DCA [11] follows similar approach. ReRoPE [474] makes relative positions beyond window ùë§ increase at smaller intervals. Furthermore, String [12] discovered that even within the models supported position range, it performs better at shorter relative positions, thus utilizing well-trained positions more extensively. Position Interpolation: Unlike reusing existing positions, position interpolation chooses to monotonically reduce all input token position indices to not exceed the maximum training value. The first proposed interpolation strategy was linear interpolation, which directly scales down each tokens position index ùëö to ùëö/ùõº [60]. Under RoPE position encoding, this is equivalent to uniformly reducing the angle ùúÉ. According to neural tangent kernel theory [209], this approach might prevent the model from learning high-frequency features. To address this, NTK interpolation [401], also known as ABF [583], reduces the scaling ratio for high-frequency parts while increasing it for low-frequency parts. In practice, NTK interpolation directly adjusts the original ùúÉ ùëó = 100002 ùëó/ùëë to ùúÉ ùëó = (10000ùúÜ) 2 ùëó/ùëë, where ùúÜ is typically chosen to be slightly larger than ùë†. YaRN [401] discovered that using ramp function to perform NTK interpolation at different ratios across dimensions could achieve better results. Building on YaRN, Resonance RoPE [537] further optimized RoPEs features using integer wavelengths. LongRoPE [102] directly uses evolution search to find optimal frequency scaling parameters for each dimension. Some work has explored assigning different scaling factors to different attention heads [659] or layers [66] to improve model performance on long context tasks. In addition to the extrapolation method based on RoPE, some researchers have also applied linear and NTK interpolation techniques to ALiBi [1, 8]. On the other hand, enabling models to support position ranges larger than the context window size is mainly achieved through two approaches: Hierarchical Position Embedding: Similar to number system, this approach greatly increases the representable range by introducing hierarchy in position encoding. BiPE [178] 15 introduces two-layer position encoding system responsible for modeling positions within and between segments. HiRoPE [642] focuses on code scenarios, utilizing codes natural hierarchy by using RoPEs lower ùëë/2 dimensions and higher ùëë/2 dimensions to handle token-level and function-level distances respectively. Position Simulation: Interestingly, line of research explores the use of short training data to simulate long training data, effectively decoupling training length from inference length. RandPos [433] randomly samples set of positions from longer position sequence, sorts them in ascending order, and uses them as position indices for shorter input data. Encoders trained using this approach demonstrated superior length extrapolation capabilities. For LLMs, Zhu et al. [686] proposed PoSE, which divides the original context window into several blocks, ensuring continuous position indices within blocks while allowing jumps between blocks, thereby covering longer relative positions with shorter context window. CREAM [563], LongRecipe [194], and SkipAlign [564] have made further improvements upon PoSE. Specifically, CREAM employs strategies such as Gaussian Sampling to optimize text block partitioning, enhancing both continuity and relativity of position indices. LongRecipe similarly optimizes PoSEs text block partitioning and introduces \"Impactful Token Analysis\" to select text content for padding within each block. SkipAlign determines block sizes and position index skip rates based on specific instruction-tuning requirements, achieving performance comparable to GPT-3.5-Turbo-16k on LongBench. 3.2. Attention Figure 5. Illustration of different model architectures. 3.2.1. Transformer-Based Architecture The predominant Large Language Models are based on the Transformer architecture [512]. Specifically, this architecture consists of multiple stacked Transformer layers, where each layer is composed of Multi-Head Self-Attention (MHSA), Feed-Forward Networks (FFNs), and Layer 16 Normalization (LN). Each layer takes the output from the preceding layer as its input and passes its own output to the subsequent layer. Classic configurations encompass encoder-only architectures such as BERT [100] and GLM [112] for encoding models, decoder-only structures like GPT [37] and LLaMA [510] for generative models, and encoder-decoder frameworks like T5 in sequence-to-sequence modeling. The core Self-attention [512] mechanism is as follows: ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëÑ, ùêæ, ùëâ) = ùë†ùëú ùëì ùë°ùëöùëéùë• ( ùëÑùêæT ùëëùëò )ùëâ, (4) where ùëÑ Rùëõùëëùëò, ùêæ Rùëõùëëùëò, ùëâ Rùëõùëëùë£. the time complexity of ùëÑùêæùëá is ùëÇ(ùëõ2). Due to the inherent quadratic computational complexity associated with the MHSA component within the Transformer architecture, most Transformer-based models encounter limitations concerning length and efficiency, posing challenges for effective training and inference on long texts. Consequently, various approaches have emerged to enhance the Transformer structure, primarily focusing on improvements within the Attention mechanism and modifications to the overall architecture. The following sections of this chapter will delineate these methods including sparse attention, hierarchical attention, recurrent transformer, and efficiency-driven modifications. Sparse Attention To address the quadratic complexity of attention in Transformer-based models and the challenge posed by the extremely large KV cache in long context scenarios, numerous prior works have explored sparse attention mechanisms, which reduce both computational and memory overhead, enabling faster processing and improved scalability for real-world applications. Sparse attention can be broadly categorized into training-based and training-free approaches, depending on whether it is applied during training or inference. For training-based methods, the sparsity is mainly reflected from two perspectives, head dimension (e.g. group query attention) and context window dimension (e.g. sliding window attention). The most common function in head dimension sparse attention is group query attention(GQA) [7]. Grouped-query attention organizes query heads into ùê∫ distinct groups, where each group utilizes shared key head and value head. When ùê∫ = 1, GQA has only one group, one key and one value head, which is comparable to Multi-Query Attention (MQA). When ùê∫ = ùêª (the number of groups ùê∫ is equal to the number of heads ùêª), the MQA is analogous to standard Multi-Head Attention (MHA). Weighted Grouped Query Attention [79] adds novel learnable parameters for every key and value head in the attention blocks to achieve aggregation among these key and value heads. Mixture of Attention (MoA) [126] automatically tailors distinct sparse attention configurations for different heads and layers, where the MoA constructs and navigates search space of various attention patterns and their scaling rules relative to input sequence lengths. As the context length grows, sparsity in the context window dimension can effectively reduce the extra computation caused by increasing the length and capturing valid information [270, 340, 536]. Beltagy et al. [25] introduce the Longformer with an attention mechanism that scales linearly with sequence length to process documents of thousands of tokens. Zebra [466] groups local-global attention layers into blocks during the training and inference phases. Xiao et al. [581] have found that keeping the KV of initial tokens will largely recover the performance of window attention, where these initial tokens lack meaningful semantics, but attract substantial attention scores. Gao et al. [136] adopts fully learning-based approach to adaptively identify attention sparsity in LLMs and leverage the learned sparsity for efficient inference. Recently, 17 following the principles of Mixture of Experts (MoE), the MoBA [335] has been proposed, where the trainable block sparse attention and parameter-less gating mechanism are introduced. However, most previous methods require training model from scratch, limiting their usability as direct plugins for ready-to-use large language models. Some training-free sparse attention approaches are as follows: Static Strategy. variety of methods have been proposed to enhance efficiency while maintaining performance during inference. Window attention [25, 78, 214] maintains fixedsize sliding window on the KV states of most recent tokens, ensuring efficiency but suffering from performance degradation once the initial tokens are evicted. StreamingLLM [581] mitigates this issue by identifying attention sinks\" where the initial tokens often receive disproportionately high attention scores, allowing LLMs trained on finite attention windows to handle infinite-length sequences without fine-tuning. LM-Infinite [168] designs Œõ-shaped attention mask and ceiling on attention distances and enables LLMs to generalize to extreme sequence lengths without any parameter updates. Dynamic Strategy. Unlike previous methods that rely on static, fixed-window strategies, later methods employ dynamic strategy to select and retain the most important tokens. H2O [658] retains only the Heavy Hitters\" tokens in the KV cache using accumulated attention scores. Similarly, Scissorhands [331] prioritizes important tokens based on the Persistence of Importance Hypothesis\". CORM [89] is KV cache eviction policy to dynamically retains important key-value pairs for inference without model fine-tuning. SnapKV [294] improves efficiency by utilizing attention scores to identify and cluster significant tokens. FastGen [139] recognizes several fundamental attention patterns and introduces an adaptive KV cache eviction policy. MInference [217] accelerates the prefilling stage using dynamic sparse attention with spatial aggregation patterns, allowing seamless integration into existing LLMs. Quest [491] is query-aware KV cache selection algorithm which tracks the minimum and maximum keys and values in KV cache pages. Layer-Level Optimization. Most of the aforementioned methods adopt the same strategy across layers to reduce the KV cache, ignoring the varying importance of information processing at different layers. PyramidKV [40] and PyramidInfer [598] have discovered that LLMs aggregate information through pyramid-shaped information funnel and dynamically adjust the KV cache size across different layers, allocating more KV cache to lower layers and less to higher layers. LazyLLM [125] apply layer-wise token pruning in each generation step. DynamicKV [684] dynamically optimizes token retention by adjusting the number of tokens retained at each layer to better suit specific tasks. LightTransfer [654] minimizes inter-layer KV cache redundancies by selectively removing cache from identified lazy layers. TidalDecode [603] accelerates LLM decoding through position persistent sparse attention. Head-Level Optimization. Previous works have not focused on the differences in attention patterns across different heads. RazorAttention [490] and DuoAttention [579] significantly improves the long context efficiency by dividing the attention heads into Retrieval Heads (which require full KV cache) and Non-retrieval Heads (which only need fixed-size KV cache), which substantially reduces memory consumption and maintains accuracy while increasing both decoding and prefilling speeds. AdaKV [124] adaptively allocates the KV cache budget across attention heads to optimize cache eviction strategies. HeadKV [128] incorporates Retrieval Heads identification to introduce new adaptive budget allocation strategy, which globally allocates budgets individually for each attention head. LONGHEADS [339] splits the sequence context into chunks and each attention head focus on parts of chunks. When generating particular token, LONGHEADS selects the 18 ùëò blocks that are most relevant to the current token based on its query vector and chunk representation. Other Methods. Despite these advancements, many approaches suffer from permanent information loss due to aggressive token pruning. Loki [463] is PCA-based sparse attention mechanism by leveraging the low-dimensionality of key vectors in the attention block, yielding inference speedup and maintaining efficacy. Sparse attention methods have shown promising results in reducing the computational costs of Transformer-based models by limiting the scope of attention to subset of tokens, thus enabling faster processing and better scalability. However, these techniques often involve trade-offs, particularly when it comes to maintaining important contextual information. While efficiency is crucial, preserving key elements of the input sequence is vital for performance, especially for tasks involving long context reasoning. To address this, future research should focus on developing adaptive mechanisms that can dynamically balance the need for efficiency with the retention of critical information, ensuring that models continue to perform robustly across diverse applications without sacrificing the accuracy of their understanding. This balance will be key in further improving the effectiveness and applicability of sparse attention methods in real-world scenarios. Hierarchical Attention Various hierarchical mechanisms have been proposed to introduce structured hierarchy into self-attention, which leverages high-level global information and low-level local attention for multi-scaled contextual receptive fields. HAN [609] pioneers the use of two-level attention mechanism. It first applies self-attention to word features to obtain sentence representation, then employs self-attention on sentence-level features to generate document-level features. Hi-Transformer [557] models documents in hierarchical way, which learns both the sentence representation and the document representation. ERNIE-SPARSE[328] leverages Hierarchical Sparse Transformer to sequentially unify local and global information. Recurrent Transformer Apart from the relatively high computational complexity, the selfattention mechanism integrates global and local information in the encoding of each token, rendering it less suitable for tasks such as long-text understanding. Based on this insight, some works strive to combine Transformer architecture with recurrent structures, enhancing the capture of long-term dependencies and improving the ability to understand long contexts [447, 569, 620]. In the earlier attempts, Transformer-XL [90] extends the context window beyond fixed lengths by employing segment-level recurrence and relative positional encoding. Memformer [562] utilizes an external dynamic memory to encode and retrieve past information, attaining linear time complexity and constant memory space complexity when processing long sequences. Compressive Transformer [420] maps past hidden activations (memories) to smaller set of compressed representations (compressed memories) for long-range sequence learning. BlockRecurrent Transformer [203] applies transformer layer in recurrent manner along sequence. During training, the recurrent cell operates on blocks of tokens rather than individual tokens and exploits parallel computation within block to efficiently utilize accelerator hardware, exhibiting linear complexity with respect to the sequence length. Unlike existing recurrent-based models that use special global tokens to store representations and place them at the beginning of the input sequence, RMT[38] incorporates memory mechanism through special memory tokens added to the input sequence, enabling the model to store and process both local and global information efficiently. SRformer[333] divides the input sequence into segments to compute 19 segmented attention and adds recurrent attention to aggregate global information over segments. Infinite Attention[374] incorporates compressive memory into the attention mechanism and incorporates both masked local attention and long-term linear attention mechanisms within single Transformer block. ARMT[430] is based on transformer self-attention for local context and segment-level recurrence for storing task-specific information distributed over long context. Efficiency-Driven Modifications By caching the previously computed key/value vectors, the decoders can reuse them for the current generation step. The key-value (KV) cache avoids encoding the history again for each token. Although this can significantly improve the inference efficiency, it imposes expensive memory overhead as the sequence length grows. To alleviate the KV cache memory consumption, some efficiency-driven modifications have been proposed, including reducing KV-heads, such as Grouped-Query Attention (GQA), Multi-Query Attention (MQA) and Multi-head Latent Attention (MLA) mechanism [308]. Note that the GQA and MQA has been discussed in Sparse Attention. For MLA, instead of directly reducing KV-heads, which is primarily considered trade-off between performance and memory consumption, the MLA compresses the key and value into latent vector to reduce the caching consumption and decompresses the key and value head for each query head when generating. 3.2.2. Linear-Complexity Architecture Linear complexity methods fall into two broad categories, including Mamba methods based on SSM architecture, and improvement methods based on Linear Attention. First, we focus on the State Space Model (SSM) [235] and its variations. With the context length increasing rapidly, higher training and inference costs become bottleneck. To speed up reasoning and capture long-term dependencies, the SSM models are gradually coming into peoples view. SSM derived from modern control system theory, is mathematical model that describes the behavior of dynamic system using set of first-order differential equations (continuous-time systems) or difference equations (discrete-time systems) to represent the evolution of the internal state of the system, while using another set of equations to describe the relationship between the state and the output of the system [515]. SSM SSM is proposed based on classical Kalman filtering Gu et al. [157], Kim and Bang [244]. It describes the state representation of sequence at each time and predicts its next state based on the input. The SSM consists of two main parts - state and observation. The state equation expresses the trend of the future state, while the observation equation predicts the current output based on the current state and input. ùë•(ùë°) = Aùë• (ùë°) + Bùë¢(ùë°) ùë¶(ùë°) = Cùë• (ùë°) + Dùë¢(ùë°) (5) ùë• (ùë°) Rùëõ represents the state vector, ùë¢(ùë°) Rùëëùëñ represents the input signal and ùë¶(ùë°) Rùëëùëú represents the output signal. Rùëõùëõ, Rùëõùëëùëñ, Rùëëùëú ùëõ and Rùëëùëú ùëëùëñ is the state matrix, input matrix, output matrix, and feed-forward matrix, which are parameters learned by gradient descent. because the term Dùë¢(ùë°) can be viewed as skip connection and is easy to compute. We can hypothesis is zero matrix. The rewritten formula is as follows: 20 ùë•(ùë°) = Aùë• (ùë°) + Bùë¢(ùë°) ùë¶(ùë°) = Cùë• (ùë°) (6) The classical SSM model is used to process continuous signals, while discrete inputs are common in the field of NLP [158]. Linear State Space Layer (LSSL) introduces continuous SSM to obtain two discretized representations including the Recurrent Representation and the Convolutional Representation. Mamba [155] leveraged the zero-order hold (ZOH) to discretize SSM. The Recurrent Representation of discretization is as follows: ùë•ùë° = Aùë•ùë°1 + Bùë¢ùë° ùë¶ùë° = Cùë•ùë° (7) = exp(ŒîA), = (ŒîA) 1(exp(ŒîA) I) ŒîB, Œî is step size that represents the resolution of the input. if we use ùë• instead of ùë¢ and ‚Ñé instead of ùë•, We can see that it has similar formula as recurrent neural networks (RNNs), ‚Ñéùë° = A‚Ñéùë°1 + Bùë•ùë° ùë¶ùë° = C‚Ñéùë° (8) Similar to RNN, the recurrent representation can be performed for efficient inference, but it cannot be used for parallel training. Therefore, LSSL proposes the convolutional representation for training. Recently, the Structured State Space Sequence model (S4) [159] is proposed based on SSM to modeling the long sequences, which can be computed more efficiently than previous methods while retaining their theoretical advantages. Mamba and its Variants Prior work found that the SSM shows poor performance as it is Linear Time-Invariant (LTI) system that is deeply connected to recurrence and convolutions. In other words, (Œî, A, B, C, A, B) in Equation 7 are fixed for all time-steps. However, LTI has fundamental limitations in modeling certain types of data. It loses the ability to efficiently select data in an input-dependent manner. Gu and Dao [156] propose Mamba to design simple selection mechanism by parameterizing the SSM parameters. The details are as follows: Rùëõùëë Rùêøùëõ = ùë†ùêµ (ùë•) Rùëõùëë Rùêøùëõ = ùë†ùêµ (ùë•) ùê∂ Rùëõùëë Rùêøùëõ = ùë†ùê∂ (ùë•) Œî Rùëë Œî Rùêøùëë = ùúèŒî (ùëÉùëéùëüùëéùëöùëíùë°ùëíùëü + ùë†Œî (ùë•)) (9) where ùë†ùêµ, ùë†ùê∂ and ùë†Œî are the Linear layer and ùúèŒî = ùë†ùëú ùëì ùë° ùëùùëôùë¢ùë†. Meanwhile, Mamba introduces the hardware-aware algorithm that computes the model recurrently with parallel scan instead of convolution. Notably, it avoids IO access between different levels of the GPU memory hierarchy without modeling the expanded state. Finally, Mamba has 5x higher throughput than the Transformer and has linear scaling over sequence length, which is friendly with long context capabilities. Many methods have been proposed to optimize Mamba [65, 625]. For example, ReMamba [625] employs selective compression and adaptation methods to compress and retain essential information, minimizing data degradation and reducing state space updates to alleviate information 21 loss. Similar to ReMamba, TAIPAN [379] combines Mamba-2 with selective attention layers for enhancing performance and ensuring computational efficiency, where the selective attention layer is used to filter out irrelevant information. DeciMamba [27] introduces context-extension technique tailored for Mamba, which leverages concealed filtering mechanism, and allows the trained model to effectively extrapolate without further training. Linear Attention Transformer has significant advantage in long context tasks, and the current model has been extended to 128k or higher. But the time complexity and space complexity of the Transformer are both ùëÇ(ùëõ2). As result, when ùëõ is significantly large, the computational burden of the Transformer model becomes challenging. Recently, considerable efforts have been made to lessen the computational demands of self-attention in Transformer [16, 169, 413, 453, 585]. In this section, we will discuss mainstream works on linear attention as follows: First, Katharopoulos et al. [240] propose the linear transformer model by using kernelbased formulation [419] of self-attention and the associative property of matrix products to calculate the self-attention weights, which greatly reduces memory usage and scales linearly to the context length. Choromanski et al. [81] introduce the Performers, which utilize the fast attention via positive orthogonal random features and implement the kernelizable attention mechanisms beyond softmax. RetNet [483] includes three computation paradigms, i.e., parallel, recurrent, and chunk-wise recurrent, and the chunk-wise recurrent paradigm achieves linear complexity for sequence modeling. which achieves training parallelism, good performance, and low inference cost simultaneously. Munkhdalai et al. [375] propose the Infini-attention and include compressive memory into the vanilla attention mechanism, which uses masked local attention and longterm linear attention modules in the Transformer block. There are similar methods, such as Lightning Attention-2 [414] employs tiling strategy to distinctly manage intra-block and inter-block elements during linear attention computation, where the MiniMax-01 [366] model uses this attention module. It adopts standard attention mechanisms for intra-block processing while implementing linear attention kernel techniques for inter-block operations. TransformerVQ [306] is transformer decoder that performs dense self-attention computations in linear time relative to sequence length. This efficiency is achieved by integrating vector-quantized keys, localized positional biases, and compressive cache designed for efficient attention processing. RWKV Family Unlike previous attempts with Recurrent Transformers, RWKV [398] introduces an enhanced linear attention mechanism, merging the computational efficiency of RNNs with the parallelism and expressive power of Transformers. This enables highly parallelized training and efficient inference, with its linear time complexity making it particularly suitable for longsequence tasks. The acronym \"RWKV\" represents four key elements: (Receptance): gating vector that integrates historical information. (Weight): trainable decay factor applied across positions. (Key): Functions similarly to the key vector in standard attention mechanisms. (Value): Operates like the value vector in traditional attention systems. RWKV-4 is the first publicly released version, following experimental iterations (RWKV-1, RWKV-2, and RWKV-3). The RWKV-4 model consists of multiple residual blocks, each featuring time-mixing and channel-mixing components. Building on RWKV-4, RWKV-5 (Eagle) and RWKV-6 (Finch) [399] introduce further innovations. Eagle enhances expressive power by 22 replacing vector-valued states with multi-head matrix-valued states and refining the learning decay strategy. It also reconfigures receptive states and adds gating mechanisms for improved performance. RWKV-6 (Finch) further boosts expressiveness and adaptability by integrating data-driven functions, such as parameterized linear interpolation, into the time-mixing and token-shifting modules. It also introduces low-rank adaptation functions, making weight matrices trainable and enabling context-sensitive optimization of decay vectors. These advancements maintain RNN-like inference efficiency while significantly enhancing the models capabilities. 3.2.3. Hybrid Architecture Waleffe et al. [517] and Park et al. [395] demonstrate that although the Mamba and Mamba-2 models perform well in language modeling, they fall short compared to Transformer models in long context tasks, such as in-context learning and long context retrieval. [64] and [517] find that adding few standard Transformer layers back into the architecture enables the model to overcome these issues. The essence of hybrid architecture involves combining linear complexity attention with standard attention modules. This integration has developed into three main approaches: First, layer-wise hybrid architecture where full attention and linear attention are mixed at the layer level. Its worth noting that models incorporating Sliding Window Attention (SWA) also fall into this category, even though early researchers might not have classified them as hybrid architectures. Second, prefilling-decoding hybrid architecture where different architectures are used for prefilling and decoding stages. This approach exclusively uses linear complexity attention layers during the prefilling phase, while the decoding stage implements hybrid architecture that incorporates full attention mechanisms. The pioneering work in this direction is YOCO [484]. Third, head-wise hybrid architecture that leverages the multi-head principle of attention mechanisms. This method assigns specific attention heads to perform full attention while designating other heads to execute linear attention operations. Hymba [105] stands as the seminal work in this approach. The following sections will explore these three approaches in detail. Layer-Wise Hybrid Architecture The evolution of hybrid architecture in large language models represents significant advancement in balancing performance and efficiency, particularly for handling long contexts. The layer-wise approach, combining linear complexity attention with standard attention modules, has emerged as the predominant strategy. Jamba [302] pioneered this hybrid architecture at scale, effectively combining Transformer and Mamba layers with MoE module, discovering that 7:1 ratio (Mamba to Transformer layers) provided optimal balance. Jamba 1.5 [501] scaled this approach beyond 100B parameters, maintaining the same 7:1 ratio while requiring only 9GB for its KV cache with 256K context lengthcompared to 80-88GB for similarly sized transformer models. Googles RecurrentGemma [33] employed the Griffin architecture combining linear recurrences with local attention, using mixture of mechanisms within each layer rather than interleaving different layer types. Meanwhile, Microsofts Samba [428] combined Mamba with SWA rather than full attention, maintaining linear complexity while providing the benefits of attention for local contexts. Comprehensive experimentation demonstrated that this hybrid approach outperformed both pure Mamba and pure attention models of similar size across standard benchmarks. Zyphras Zamba [149] innovated 23 by incorporating shared attention mechanismutilizing global shared attention block that appeared every few Mamba blocks but shared parameters across all instances, reducing memory requirements while maintaining strong performance. Zamba2 [148] expanded on these innovations in multiple size levels, switching from Mamba to Mamba-2 and using two alternating shared attention blocks with non-shared low-rank adapters. MiniMax-01 [366] became the first hybrid model to achieve true state-of-the-art performance across comprehensive benchmarks, using configuration where one transformer block with softmax attention followed every seven Transformer blocks with lightning attention [415]. It matched the performance of leading commercial models while supporting context lengths up to 4 million tokens. The sliding window approach represents another variant of hybrid attention. Googles Gemma [500] employed more balanced 1:1 ratio of full attention to SWA layers, emphasizing balanced performance across tasks rather than maximizing context length. Coheres Command R-7B [86] implemented hybrid approach with SWA, enabling efficient processing of longer contexts while maintaining strong benchmark performance. Research from Character.ai [46] found that approximately 6:1 (linear complexity to full attention) yielded optimal resultsaligning with findings from other hybrid models. This convergence on ratios of approximately 6:1 or 7:1 across multiple independent research efforts indicates sweet spot in the design space, suggesting that hybrid architectures represent not just compromise but potentially the optimal approach for next-generation language models efficiently handling diverse contexts. Prefilling-Decoding Hybrid Architecture The prefilling-decoding hybrid approach optimizes language model operation by applying different attention mechanisms based on the operational phaseusing linear complexity attention during prefilling and implementing hybrid architectures during decoding. YOCO [484] pioneered this paradigm with decoder-decoder structure that dramatically reduces memory requirements. The self-decoder processes input context using linear-complexity attention and produces single global key-value cache, which is then reused by all layers of the cross-decoder. This design reduces KV cache memory requirements by approximately factor equal to the number of model layers, which enables early exit during prefilling and maintains performance comparable to traditional Transformers. GoldFinch [152] represents another innovation in this category, stacking GOLD transformer on top of an enhanced Finch (RWKV-6) architecture. Its key contribution is the TokenCat mechanism, which generates highly compressed key cache in linear time and space. This approach yields remarkable efficiency gains, with cache size savings 756-2550 times smaller than traditional transformer caches. Both approaches demonstrate that prefilling-decoding hybridization offers powerful alternative to pure architectures, enabling dramatic efficiency improvements for long context inference without sacrificing model quality. Head-Wise Hybrid Architecture The head-wise hybridization approach combines linear and attention-based mechanisms in parallel within the same layer, allowing different attention heads to process identical inputs using different mechanisms simultaneously. Hymba [105] pioneered this approach with hybrid-head architecture that integrates transformer attention mechanisms with state space models within each layer. Attention heads provide high-resolution recall capabilities while SSM heads enable efficient context summarization. This design can be interpreted as mimicking human cognition: attention heads function like snapshot memories storing detailed recollections, while SSM heads act as memories that retain core information while forgetting details. Samba [428] combined the Mamba block and sliding window attention block together with an MLP layer. It combines local and global attention mechanisms. Local attention maintains sensitivity to near-neighbor information, while global attention allows the Figure 6. Design of LCLMs based on Workflow strategy. model to capture dependencies over long distances. 4. Workflow Design Recent advancements in long context modeling often focus on modifying model parameters or altering the architecture to process long contexts, typically involving fine-tuning large language models or implementing complex internal mechanisms. In contrast, the methods presented in this section aim to enhance the long context processing capabilities of LLMs without altering their parameters, instead leveraging external components to augment the models ability to handle long contexts. As shown in Figure 6, the strategies discussed in this chapter are as follows: (1)Prompt Compression (Sec. 4.1) reduces the size of the input context while retaining essential information. (2)Memory-Based Methods (Sec. 4.2) utilize an external memory module to store long contexts for efficient retrieval. (3)RAG-Based Methods (Sec. 4.3) extract or recall specific information from long contexts to reduce the context. (4)Agent-Based Methods (Sec. 4.4) leverage the LLM agents memory, planning, and reflection capabilities to handle long contexts effectively. 25 4.1. Prompt Compression As real-world tasks such as Chain-of-Thought (CoT), In-Context Learning (ICL) and RetrievalAugmented Generation (RAG) become more complex, prompts for LLMs often grow longer to include detailed requirements, contextual information and examples [298]. Nevertheless, lengthy prompts reduce inference speed, increase memory costs or API bills and degrade user experience. Prompt compression seeks to enhance LLM efficiency by reducing input complexity of LLMs. These techniques typically require an additional module to compress prompts while make minimal or no changes to the original LLMs parameters, allowing for plug-and-play integration. Based on the form of the compressed prompt, prompt compression methods can be divided into two types: 1) Hard Prompt Compression maintains the use of natural language words or sub-words (Sec. 4.1.1); 2) Soft Prompt Compression converts natural language into embedding representations (Sec. 4.1.2). 4.1.1. Hard Prompt Compression Hard prompts are natural language prompts composed of tokens from the LLMs vocabulary, representing specific words or subwords. Hard prompt compression aims to streamline natural language prompts by reducing their length or complexity, while still maintaining their effectiveness in eliciting the desired response from the model. This process typically includes two approaches: (1) selecting relevant tokens of the prompt or (2) rewriting it for brevity and clarity. Selecting SelectiveContext [291] enhances LLM inference efficiency by calculating token selfinformation with small language model, grouping tokens into lexical units, and removing redundant content. AdaComp [646] adaptively selects relevant documents by considering both query complexity and retrieval quality. CPC [307] introduces sentence-level compression technique leveraging context-aware sentence encoder to rank sentences by their the embedding similarity to the query and removing less relevant sentences. TCRA-LLM [316] proposes token compression scheme with two complementary methods: summarization compression which uses T5-based model to reduce token size, and semantic compression which eliminates words with lower semantic impact. Besides, reinforcement learning (RL) has been applied to improve the efficiency and effectiveness of prompt compression methods. DynaICL [679] enhances in-context learning efficiency by using reinforcement learning to fine-tune meta-controller that dynamically adjusts the number of few-shot examples based on input query difficulty, balancing efficiency and performance. PCRL [234] applies discrete prompt compression method with reinforcement learning. TACORL [444] leverages task-specific reward signals to fine-tune encoder-based compression model using on-policy RL, enabling effective compression while maintaining performance. In addition, the LLMLingua series aims to build specialized language for LLMs through prompt compression, which accelerate model inference, reduce costs and improve downstream performance. LLMLingua [216] leverages smaller language model to calculate perplexity (PPL) and remove redundant tokens, featuring budget controller, iterative prompt algorithm, and alignment techniques to achieve up to 20x prompt compression while preserving semantic integrity. LongLLMLingua [219] proposes question-aware coarse-to-fine compression method, document reordering mechanism, dynamic compression ratios and subsequence recovery strategy to improve LLMs ability to perceive and prioritize key information. LLMLingua2 [390] introduces task-agnostic prompt compression method, trained via data distillation from GPT-4 with BERT-level encoder, achieving 3x-6x speed improvements over LLMLingua 26 while preserving crucial information. Rewriting An alternative approach involves rewriting prompts instead of selecting them. Nano-Capsulator [83] encapsulate lengthy prompts into shorter ones while adhering to specific generation length constraints, maintaining performance through an explicit semantic-preserving objective with reward scoring. CompAct [618] captures pivotal information from extensive documents by dynamically retaining essential contexts and incorporating information. FAVICOMP [233] is decoding-time evidence compression approach that produces refined evidence set more familiar to the target model, enhancing RAG performance while seamlessly integrating parametric knowledge. 4.1.2. Soft Prompt Compression Soft prompts [268] present an innovative paradigm by eliminating the reliance on discrete tokens. Instead, they directly learn series of continuous embeddings via backpropagation, which can be fed into the transformer without requiring mapping to any actual language tokens. Such soft prompts can serve as an efficient alternative to conventional plain-text context, significantly reducing computational overhead during inference. Depending on whether the origin LLMs parameters are updated, soft prompt compression can be broadly categorized into two types: 1) LLM-fixed methods and 2) Gist token-based methods. LLM-fixed LLM-fixed methods focus on compressing prompts without updating the original LLMs parameters, offering an efficient solution for managing input context length. Instead, they involve training additional modules that transform discrete tokens into continuous embeddings, facilitating efficient prompt compression. Contrastive conditioning [555] focuses on learning compact soft prompts to simulate the original natural language prompt by minimizing the Kullback-Leibler (KL) divergence. However, contrastive conditioning incurs substantial computational overhead, as it requires retraining from scratch for each new incoming prompt, making it impractical for broad application. ICAE [140] generates compact and informative memory slots to represent the original context, enabling LLM to encode more information within the same context length. This improves the models capability to handle long contexts and reduces computation and memory overheads during inference. 500xCompressor [299] builds upon ICAE but utilizes KV values for compressed tokens instead of embeddings,achieving remarkable compression ratios. xRAG [72] uses frozen embedding model as the encoder and trainable adapter between the encoder and decoder LLM. Through modality fusion, it integrates document embeddings into the LLMs representation space, eliminating the need for textual representations and achieving high compression rates. UniICL [131] compresses demonstrations into compressed features, which are then converted into compressed virtual tokens via learnable projection layer. These virtual tokens replace original demonstrations to shorten input length and help select potential demonstrations. The queries and virtual tokens are then fed into the frozen LLM for response generation. In summary, LLM-fixed methods offer diverse strategies for prompt compression by optimizing input representations while preserving the frozen parameters of the original LLM. These methods significantly enhance computational and memory efficiency, enabling more effective utilization of LLMs for broad range of tasks. 27 Gist Token-based Gist token-based methods aim to compress prompts by condensing the context into small set of special tokens, referred to as gist tokens\" [371]. These tokens replace the original context, enabling efficient caching and reuse while reducing computational costs. However, such methods typically require modifying the parameters of the LLM. Gist [371] introduces method to train models for prompt compression without additional cost over standard instruction finetuning, which inserts gist tokens after the prompt and modifies Transformer attention masks to prevent tokens after the gist tokens from attending to those before them, allowing the model to learn both prompt compression and instruction following simultaneously. AutoCompressors [73] process long documents by recursively generating gist tokens which are passed as soft prompts to all subsequent segments and result in more compact soft prompts. Activation Beacon [645] serves as plug-in for Transformer-based LLMs that enables effective, efficient, and flexible compression of long contexts, featuring progressive compression workflow that distills the context into small set of activations. In summary, gist token-based methods provide powerful framework for reducing input length by converting context into compact, reusable tokens. These approaches offer significant efficiency gains, though they typically involve updating the LLMs parameters to enable effective compression and integration. 4.2. Memory-Based Methods Memory-based methods aim to utilize an external module (i.e., memory module) to store long contexts. This approach effectively reduces the computational burden of directly relying on long contexts and mitigates potential information reduction associated with prompt compression. Generally, memory modules not only store essential historical information but also dynamically update, enabling efficient management of long context environments. Memory augmentation has long been an actively explored topic. Even before the emergence of ChatGPT, numerous methods had been proposed and discussed. For example, MemPrompt [348] and [91] store user feedback on model responses in textual form, allowing for more personalized and accurate responses in subsequent interactions. Socratic Models [631] enables LLMs (Large Language Models), ALMs (Audio-Language Models), and VLMs (Vision-Language Models) to share unified memory unit that stores logs of numerous observations captured in egocentric videos. By extracting key information from these logs, LLMs can operate effectively within long context environments. Token Turing Machine [436] utilizes an external memory unit composed of continuous vectors. Through cross-attention mechanisms, the model can read information from the external environment for processing or write processed information back into the memory module. This enables the model to achieve long-horizon robotic control. Since the emergence of ChatGPT, LLMs have entered the era of agentic workflow. This paradigm involves equipping LLMs with abilities such as memorization, retrieval, planning, reflection, and tool-use, enabling them to adapt and respond contextually to complex, real-world tasks. In this section, we focus on how these LLMs or agents are augmented by memorization to understand long contexts or maintain consistency during long text generation. It is important to note that the discussions in 4.4 differ from this focus. Instead, the latter ones emphasize leveraging the agentic workflow specifically for enhancing LLMs long-text capabilities. Memory-based methods for long-text language models can be categorized into three paradigms based on the form of memories: (1) continuous memory (latent vector representations), (2) language memory (textual content), and (3) parametric memory (model weights). 28 Language Memory. Language Memory stores historical information as human-readable text fragments and typically reduces its length by retrieval and extraction. For example, Generative Agents [394] employs memory stream with triple scoring mechanism for item retrieval: (1) Recency, which prioritizes recent interactions using an exponential decay function; (2) Importance, where an LLM assigns 110 score to each memory item based on its semantic significance; and (3) Relevance, computed as the embedding similarity between the current query and memory item. This mechanism effectively manages complex contexts with extremely long interaction records in sandbox environment. Reflexion [458] stores textual feedback from self-reflection in memory bank. When similar errors occur in subsequent generations, the model retrieves historical reflection records for real-time correction. MemoryBank [675] incorporates the Ebbinghaus Forgetting Curve to gradually weaken infrequently accessed memories while reinforcing frequently used ones, simulating the formation of long-term human memory. AdaPlanner [478] and Voyager [522] address long-horizon task planning through skill library mechanism, where successful textual action plans from previous interactions are archived as reusable templates. These stored plans can be dynamically synthesized into complex strategies for novel tasks. This capability can be seen as simplifying past memories, which are in the form of long texts, into skill items, and retrieving and synthesizing them into manageable and effective contexts for subsequent text generation tasks based on on-the-fly needs. RecurrentGPT [680] enhances the coherence for long-text story generation by combining long-term memory retrieval of relevant paragraphs with short-term memory maintained through iterative plot summarization, mimicking human cognitive processes for sustained narrative consistency. Continuous Memory. Continuous Memory encodes long context information into latent vector representations, enabling efficient retrieval of historical context without explicitly storing raw text. For instance, LongMem [540] splits lengthy texts into fixed-length segments and caches their key-value pairs from intermediate Transformer layers in an external memory bank. During inference, it retrieves top-k relevant historical key-value pairs through query-key attention operation and integrates them with current hidden states via joint attention mechanism. newly designed cross-network residual connection between the LLM backbone and trainable Transformer-based SideNet conducts such memory retrieval and fusion operations. MemoryLLM [545] embeds trainable memory tokens within each Transformer layer as fixed-size memory pool, implementing self-update mechanism that selectively overwrites less frequently accessed memory information with new information by replacing only portion of memory units per update cycle. Parametric Memory. Parametric Memory internalizes long context information within the models weights. For example, DSI (Differentiable Search Index) [496] reformulates document retrieval as generation task, training the model to directly output document IDs for queries, which allowing for the memorization of document-query mappings in models parameters. This approach eliminates the need for external storage and retrieval. Further, DSI++ [355] addresses the catastrophic forgetting issues of DSI in continual learning settings by introducing sharpnessaware loss function: ùëöùëñùëõùúÉùëöùëéùë• ùúñ 2 ùúå) (ùúÉ + ùúñ), where ùúÉ is the model parameters, and ùúå is threshold. This loss encourages the model to converge to flatter minima, which are empirically shown to improve memory retention. Additionally, DSI++ employs generative memory technique for memory replay, where generator synthesizes pseudo-queries for previously indexed documents. These queries are mixed with new document data during continual training to mitigate forgetting and maintain updating over time. Generative Adapter [62] dynamically generates lightweight adapter modules (Œîùë°) for previous context chunks ([ùê∂1, ùê∂2, ..., ùê∂ùë°]) based 29 on their hidden states ([ùêª1, ùêª2, ..., ùêªùë°]) from the base LM, using an adapter generator (ùê∫) during test-time contextualization. Then, during inference, the generated adapter is integrated into the base LM for text generation. Pros & Cons. Memory-based methods provide critical theoretical advantages for long-text language models by addressing the inherent limitation of Transformer-based LMs imposed by their finite context windows, which either truncate long sequences or incur quadratic computational costs from attention mechanisms. By externalizing the storage of long context, these methods bypass the architectural bottleneck of vanilla Transformer while retaining critical historical information. The computational completeness of memory augmentation is further underscored by Schuurmans [441], who prove that vanilla Transformer-based LMs are computationally limited, which can be overcome through read-write memory augmentation. Their experiments demonstrate that 540B-parameter LLM (i.e., Flan-U-PaLM-540B) with associative read-write memory can simulate universal Turing machines. Similar conclusions are drawn from [98]. Nevertheless, memory-based methods face practical challenges including retrieval latency, inconsistencies between memorized and live contexts, and the complexity of memory updating, among others. 4.3. RAG-Based Methods In line with memory-based approaches for LLMs long-text capabilities, Retrieval Augmented Generation (RAG) aims to extract or recall specific information from the long texts to reduce the context. The retrieval sources can be training corpus, knowledge base, Internet, the provided long context, or the aforementioned external memories1. Specifically, leveraging RAG techniques for enhancing long-text capabilities of LLMs involves three-stage workflow: (1) chunking to partition long context into manageable units, (2) retrieval to extract the most relevant information, and (3) generation to synthesize retrieved knowledge with intrinsic model understanding into coherent and contextually rich outputs. Chunking. Vanilla chunking units involve fixed token length [272], sentences or paragraphs, delimiters such as or t, structural markers like Markdown headers or LaTeX sections, and semantically similar sentence neighbors2. However, these approaches have several key issues as follows: 1. Determining Optimal Chunk Size: Overly large chunks may exceed input limits of the embedding models, while excessively small chunks can fragment context, making retrieval more challenging and potentially degrading the quality of retrieved information. 2. Maintaining Contextual Integrity: Ensuring that splitting does not disrupt semantic meaning or omit critical information such as anaphoric references (e.g., it, they). 3. Balancing Efficiency and Effectiveness: Chunking must be computationally efficient while preserving sufficient detail for downstream tasks. 4. Adapting to Data Formats: Different data formats (e.g., text, code, or tables) may require specific chunking strategies. 1Typically, the retrieved memories are so-called long-term memories [680]. 2https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/Le velsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb 30 To mitigate these issues, for example, Late Chunking [167] first embeds the long document with an embedding model supporting long inputs and then conducts chunking on the embeddings. This approach ensures that each chunk embedding captures the whole contextual semantics. Sliding Window Chunking3 divides the text into overlapping chunks of fixed size, ensuring that contextual dependencies across chunk boundaries are preserved. Contextual Retrieval [14] leverages long context LLM to augment the target chunk before embedding by taking the entire document as its context. The LLM generates an enriched version of the target chunk by integrating relevant contextual information from the document. Retrieval. The retrieval stage can be categorized into two approaches according to the retriever types: (1) Sparse Retriever which refers to the retriever that primarily uses sparse representations of text data such as TF-IDF[48], BM25[440] or Jaccard index[634]; and (2) Dense Retriever which uses dense vectors with continuous values such as BERT[101] features. Among these two approaches, dense retriever is mostly used due to its ability to capture semantic similarities rather than relying on lexical matching. By encoding queries and documents (e.g., long context) into shared latent space, dense retriever achieves higher retrieval accuracy and extends its functionalities such as instruction-following[372], conversational search[368], complex reasoning[231], as well as scaling laws[122] when combined with particular techniques. For example, BGE-M3[55] is multilingual retrieval model that integrates sparse and dense retrieval, supporting an extended context window of up to 8192 tokens. ModernBERT[548] is BERT-based model trained on 2 trillion tokens with an 8192-token context length, showcasing exceptional performance in natural language understanding (NLU) and long context retrieval tasks. REAPER [231] enhances retrieval by incorporating complex reasoning, breaking down the retrieval query into series of planned and chained steps. Moreover, several techniques augment the queries to enhance the retrieval quality. For example, Query2Doc [529] generates pseudo-documents using few-shot prompting and expands the original query with these documents. HyDE [133] produces hypothetical document through an instruction-following language model, encodes it with contrastively learned encoder, and retrieves similar real documents from the corpus using the resulting embedding vector. Rewrite-Retrieve-Read [344] employs trainable small language model as rewriter, optimized via reinforcement learning with feedback from the LLM, to refine the query before retrieving contexts from web search engine and processing them with the LLM. Generation. After the long context being reduced by chunking and retrieval, the following step is to integrate the reduced information into the LLMs for generation. Typically, without modeling training, the reduced information is fed into the LLMs by simple context concatenation [166, 207, 457], or after further prompt compression [587]. For example, Fusion-inDecoder [206] initially encodes all retrieved passages along with the corresponding questions into sequence of soft features, which are then concatenated and input into the decoder-only LLM. The retrieved information can also be leveraged to adjust the decoding process. For example, kNN-LM [242] blends the models prediction distribution with that of the nearest neighbors from the retrieval knowledge to adjust the generation on the output side. For certain specialized model architectures, such as Retro [31], trained cross-attention modules are employed to integrate the information. 3https://safjan.com/from-fixed-size-to-nlp-chunking-a-deep-dive-into-text-chunking-t echniques/ 4.4. Agent-Based Methods LLM agent is an autonomous LLM entity that possesses cognitive architecture consisting of several key components: perception capabilities to receive and process inputs, memory systems (both short-term and long-term) for information storage, action generation abilities for text output and tool manipulation, reflection mechanisms for self-evaluation, planning capabilities for goal setting and task decomposition, and reasoning abilities for logical thinking and decision-making [527, 573]. In the context of long context LLMs, the LLM agent leverages its memory, planning, and reflection capabilities to process long texts effectively. For example, Generative Agent [394] demonstrates these capabilities through comprehensive architecture where agents can extract and retrieve relevant information from their memory stream, generate periodic reflections to synthesize high-level insights from low-level observations, and create detailed plans that can be dynamically adjusted based on new information or interactions. This cognitive architecture enables agents to maintain coherent understanding and generate appropriate responses even when dealing with extensive context. Agent-based approaches for enhancing LLMs long-text capabilities can be broadly categorized into two main types: single-agent and multi-agent approaches. These two paradigms differ in their architectural design and operational mechanisms: (1) single-agent architectures, and (2) multi-agent systems. Single-agent approaches focus on developing comprehensive cognitive architectures within single LLM agent entity. These methods emphasize building robust memory management systems, sophisticated planning mechanisms, and effective reflection processes. In contrast, multi-agent systems distribute the cognitive load across multiple specialized agents, each handling specific aspects of the long-text processing task. This paradigm emphasizes division of labor and collaborative problem-solving. Agents can engage in structured dialogues, debates, or collaborative analysis to understand or generate long texts more effectively. Single-Agent Architectures. For long-text understanding, ReadAgent [263] features threestage memory management approach powered by LLMs. The system first groups related content into coherent memory units, then distills these units into condensed summaries for efficient storage. When specific information is needed, it can intelligently navigate back to the source text to retrieve precise details for task completion. PEARL [480] employs threestage prompt framework to enhance reasoning about long documents: action mining, plan generation, and plan execution. When given question about long document, PEARL breaks it down into specific actions (such as summarizing, finding events, and identifying relationships), then executes these actions on the document to derive an answer. Self-Notes [260] generates multiple QA notes interleaved with the input context and the question to handle the long context reasoning problems. MemWalker [51] functions as an interactive reading system that breaks down long documents into hierarchical structure of summaries. When given query, it systematically explores this structured information, moving through different summary levels until it finds and collects the relevant details needed to provide an accurate response. GraphReader [286] introduces an innovative approach that transforms lengthy documents into navigable graph structures. The system deploys an AI agent that intelligently traverses this graph representation. When presented with query, the agent begins by conducting detailed analysis to create search strategy. Using specialized navigation tools, it systematically moves through the graph, examining both individual nodes and their connections in hierarchical manner. The agent maintains dynamic record of its discoveries and continuously evaluates its progress, adjusting its approach as needed until it collects enough data to construct comprehensive response. RoleAgent [311] employs hierarchical memory system to distill long-text observations into shorter events, key points, and insights, enabling coarse-to-fine 32 information seeking during user-LLM interactions. As for long-text generation, Re3 [602] follows an iterative five-stage process for long story generation. The process begins with the premise, where given premise is provided to initiate the story. Based on this premise, the plan stage generates the setting, characters, and an outline using an LLM. Following this, the draft stage involves writing story continuations by prompting the model with both the plan and previously generated content. The process then enters an iterative cycle between the rewrite and edit stages: in the rewrite stage, the generated story continuations are reranked for their coherence with the plot and relevance to the premise, while in the edit stage, selected continuations are refined to ensure long-range factual consistency. This cycle of drafting, rewriting, and editing continues until the final story is generated, where the text flows naturally and aligns with the original premise. RecurrentGPT [680] builds two streams of memory flow for long-text generation, where the LLM agent is equipped with both short-term and long-term memory streams. During the generation of long text, the agent maintains and updates short summary of the previously generated paragraphs as part of context, which is also termed as short-term memory. Moreover, to integrate more detailed history paragraph information, the LLM agent is also enhanced with retrieval mechanism to recall the most semantically relevant history paragraphs with the current generated paragraph as the query, which is also termed as long-term memory. Both memories are updated when new paragraph is generated. Multi-Agent Systems. Chain of Agent (CoA) [656] utilizes multi-agent framework to process long context tasks. It divides the context into smaller segments, each handled by worker agent. These worker agents sequentially communicate with central manager agent, which synthesizes the contributions into coherent output. This approach ensures efficient context processing by assigning each agent short context and interleaving reading and reasoning. Similarly, LongAgent [663] employs leader-agent model to process long texts. The leader understands the users intent and directs member agents to extract information from the document. An inter-agent communication mechanism resolves conflicts between agents responses, ensuring the leader gathers accurate data. This multi-agent approach enables efficient handling of long contexts while mitigating hallucination issues. 5. Infrastructure In this section, we investigate the AI infrastructures that support LCLM training and inference, highlighting the key differences from those techniques typically used for general LLM. The majority of the methods discussed are designed primarily for enhanced efficiency. The others represent the perspective of the combination of engineering and algorithmic approaches, initially proposed to improve the model performance brought by the long context. 5.1. Training of LCLMs With advancements in LLM algorithms, optimizations in training efficiency have also undergone significant innovation. Given the scale of parameters and computational complexity, distributed training is the standard solution for LLMs. Since the advent of ChatGPT, mainstream NVIDIA GPU compute resources have evolved through Volta, Turing, Ampere, Hopper, and Blackwell architectures 4, resulting in hundred-fold increase in single-GPU compute capability (e.g., 4https://www.nvidia.com/en-us/technologies/ 33 Efficient Strategies Computational Overhead I/O GPU HBM Communication Overhead Memory Overhead Fundamental I/O optimization Data packing File systems Mixed-precision training Low-precision training Optimized memory access Computation partition Communication-computation overlapping Quantization Virtual Memory management Scheduling Strategies Prefilling-Decoding Disaggregation GPU-CPU Parallel Inference Speculative Decoding Training Inference Table 5. Comparison of Optimization in AI Infrastructure. means optimization in this aspect, denotes negative impact on this aspect, while means no impact on this aspect or not involved. from 16.4 TFLOPS single precision for V100S to 2,250 TFLOPS for B200). Maximizing this compute power in distributed setting is the key challenge in LLM training. Optimizations in computing, communication, memory management, and parallelization strategies have become standard practice and yield substantial performance gains. These approaches are integrated into prominent training frameworks such as Megatron [459], DeepSpeed [424], and FSDP [667], facilitating model development and training. Numerous surveys [34, 113, 329] have documented these foundational techniques. Nevertheless, long context lengths introduce further challenges. Limited GPU memory, in particular, renders most common training optimizations ineffective. Reducing batch size may enable execution, but inefficient computation and memory access bottlenecks severely degrade overall training efficiency. High-performance long context training primarily explores parallel computation strategies and sophisticated communication-computation overlap to maximize hardware utilization and continuity. Specifically, these methods address I/O, GPU resource constraints, and communication bottlenecks, as illustrated in Table 5. 5.1.1. I/O Optimization Ultimate Foundational Optimization of I/O Training LCLMs inherently requires larger token batch sizes and significantly varying data lengths. Given constraints in memory, network bandwidth, and PCIe bandwidth, reading and transferring these large data volumes from memory to the GPU substantially slows down batch construction. Because I/O performance typically lags behind modern GPU compute performance, I/O becomes major bottleneck in the training process. Strategic approaches, such as increasing the number of I/O threads and utilizing pinned memory, are commonly employed to mitigate this issue. However, the optimal hyperparameters for these strategic I/O optimizations require case-by-case tuning based on model size, context window length, and hardware configuration; otherwise, CPU cores or memory may become new bottlenecks. 34 Sophisticated Data Packing Data packing concatenates samples into longer sequences. To maximize effective data utilization in batches, packing requires the appropriate combinatorial arrangement and essential truncation [337, 470]. Packing would alter the data reading order and the actual training distribution, potentially affecting the training performance. Attention masks can differentiate packed samples [253], ensuring isolation when necessary. However, constructing non-causal attention masks introduces fragmented operations, and non-causal attention negatively impacts training efficiency. Some methods [18, 249] employ data sampling to ensure that sample lengths follow predefined maximum. There exist other solutions which dynamically adjust context lengths. For example, Hydraulis [276] avoids the usage of fixed context windows, but uses dynamic programming to address data sampling imbalance and data packing imbalance. Data Decomposition [406] curates data to different windows using multiple bucket sizes. Building upon these established data organization strategies, many recent pre-trained models have adopted progressive length extension approach. This method has demonstrated significant performance improvements on commonly used long text benchmarks [58, 135, 596, 665]. Distributed File Systems and Pre-fetching In the LLM training process, data is read from disk/network to host cache, then transferred to GPU via PCIe. In distributed training, data retrieval and distribution by CPUs are limited by PCIe bandwidth. Pre-fetching with nearend data workers [662] or caching [103] can effectively overlap I/O and computation, even hiding and eliminating the I/O latency. Upon the observation that SSD throughput and RDMA bandwidth were underutilized, 3FS [96] introduces distributed, random-access approach, which enhances both performance and usability by leveraging the available resources more effectively. 5.1.2. Optimizations on GPU Constraints and Memory Access Mixed-precision Training GPU memory is significant bottleneck in long context LLM training, with parameters, gradients, optimizer states, and especially activations (proportional to sequence length) consuming substantial resources. Reducing numerical precision [161] through low-precision or mixed-precision computation is natural solution. Mixed-precision training [359] is common memory optimization technique. Typically, floating-point operations use FP32 (single-precision), allocating 8 exponent bits, and 23 mantissa bits. In the memoryconstrained scenarios, sacrificing some precision reduces storage requirements, e.g., FP16 requires half the storage. However, FP16s maximum integer value of 65,536 can lead to the NaN exception or Inf exception owing to the numerical overflow. BF16, another half-precision format (requiring Ampere architecture or later), increases exponent bits, mitigating overflow at the cost of some precision. FP16/BF16 are now standard for LLM training, with FP32 reserved [523] for precision-sensitive operations (RoPE, LayerNorm, Softmax) and communication reductions (where BF16 should be avoided). Quantization and Low-Precision Training Recent quantized training methods incorporate quantized fine-tuning, using 8-bit floats (FP8) or integers (INT8), and preserve accuracy in the inference phase. FP8 are currently usable in TransformerEngine and other derivative acceleration libraries [661] on Hopper GPUs, primarily for less precision-sensitive matrix multiplications in Transformer layers. It applies E4M3 (4-bit exponent, 3-bit mantissa) for forward and E5M2 for backward passes, potentially offering 2x performance boost over BF16. INT8 integers are more widely applicable, forming the basis of most current quantization methods. Weight 35 quantization is the most intuitive approach, mapping values to 256 discrete levels within statistically or pre-defined range for each tensor. Quantization significantly reduces memory footprint, and with appropriate hardware and software support, dequantization overhead can be fused and minimized, significantly boosting performance. However, reduced precision can irreversibly impact model performance. Some studies observe that the error can be properly compensated at critical locations in the training process [198, 402, 623]. Optimizer states are more sensitive to precision, requiring careful training with dynamic range scaling to align their distribution with the FP8 representation, reducing both memory footprint and quantization error [99, 571]. Apart from the weights and optimizer state quantization, activation quantization is also attracting significant research. Intuitively, quantizing activations leads to substantial information loss. Fortunately, latest research discovers that this loss primarily attributes to the activation outliers, where suppressing these outliers during training can enable reliable activation quantization [283, 304, 578]. Optimized Memory Access and Blockwise Computation The quadratic time and memory complexity of the Transformers multi-head attention mechanism presents significant computational and memory challenges, particularly with long input sequences. FlashAttention [93] addresses this by leveraging the high bandwidth, but limited capacity, of GPU shared memory (e.g., 19TB/s bandwidth with 20MB limit) for block-wise processing. It employs log-sumexp trick for optimized softmax computation and caches key values and cumulative sums to minimize memory access. FlashAttention demonstrates substantial performance gains in both standard and long context training scenarios. Alongside these optimizations for standard attention, variants like sparse FlashAttention [388] have gained traction. FlashAttention-v2 [92] refines the computation order of its predecessor, reducing non-matrix multiplications and optimizing warp scheduling within thread blocks to minimize shared memory access and maximize GPU parallelism. Furthermore, leveraging the Transformer Engine introduced with the Hopper GPU architecture, FlashAttention-v3 [442] incorporates FP8 acceleration and three key techniques: asynchronous data loading and computation via producer-consumer model, overlapping softmax and GEMM computations, and block-wise quantization. This block-wise optimization paradigm facilitates further algorithmic and engineering co-optimization, as exemplified by NSAs [627] hybrid approach combining token compression, selection, and sliding window, and MoBAs [336] block-wise attention mechanism, achieving favorable balance between full and sparse attention using MoE-like strategy. Similar solutions have already been verified on MLA [278]. Partition on Computation Distributed computing with sharding effectively mitigates the memory pressure imposed by long context windows. Standard self-attention has computational complexity of O(n2), where is the sequence length. Ring attention [309] reduces this to O(n) by restricting each tokens attention to fixed number of surrounding tokens, significantly decreasing computational and memory costs, and enabling the processing of longer sequences. Hybrid approaches combining local and global attention mechanisms further improve performance by maintaining long-range dependency modeling while reducing computational overhead [35, 310]. More direct methods employ various parallelization strategies: Sequence Parallelism [248] distributes model layers across devices, reducing individual device load but requiring substantial inter-device communication, and currently only shards Dropout and LayerNorm activations, leaving room for optimization. Context Parallelism 5 divides the context window into segments processed in parallel, subsequently aggregating their representations. This effectively reduces 5https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html 36 memory requirements and increases training speed. Ulysses Parallelism [208], integrated into the DeepSpeed framework, combines the advantages of both by sharding both model layers and the context window with an interleaved strategy, minimizing communication overhead while maximizing parallel efficiency, demonstrating significant advantages for extremely long context windows. 5.1.3. Optimizations on Communication-Computation Overlapping Distributed LLM training requires inter-node communication for gradient aggregation and intermediate calculation results. Performance discrepancies between computation and communication lead to hardware underutilization, exacerbated in long context training. Since CPUs are heavily utilized for I/O in such scenarios, offloading computation to CPUs may not yield expected gains, and thus communication optimization is crucial. Overlapping techniques enable parallel execution of forward computation with parameter gathering or backward gradient aggregation, which improves the overall efficiency. Advanced bidirectional pipeline parallelism techniques effectively mitigate pipeline bubbles [97]. Gradient Accumulation (GA) is crucial method for improving the overlap between computation and communication. In LCLM training, high per-sample memory usage restricts local batch sizes on individual nodes. GA addresses this by implementing mini-batches: performing forward passes on smaller batches and accumulating gradients across multiple mini-batches before single, consolidated parameter update. This allows for full parallelization of mini-batch forward passes with the preceding backward pass communication, effectively simulating large-batch training under memory constraints. However, integrating GA with DeepSpeed-ZeRO requires careful consideration. While ZeRO-1 necessitates only single communication step during backward propagation due to its non-partitioned gradients, ZeRO-2, which partitions gradients across nodes, requires communication after each mini-batch forward pass to gather all gradient partitions, potentially hindering complete overlap. Training engines often incorporate GA as component, leveraging different CUDA streams to enable customized optimization [468]. In LLM training, strategically utilizing distinct CUDA streams within kernels allows for optimal overlapping strategies tailored to diverse model architectures and hardware configurations [45, 243], effectively addressing discrepancies between communication and computation speeds. 5.2. Inference of LCLMs Inference can be divided into two distinct phases: i) The prefill phase, where the input prompt is processed to generate the KV cache for large language models. ii) The decoding phase, where the model utilizes the KV cache to generate subsequent tokens. Typically, the prefill phase is compute-bound, meaning that reducing computational overhead is key to accelerating this step. In contrast, the decoding phase is bandwidth-bound, requiring optimization of memory transfer during computation to improve efficiency. In long context inference, the infrastructure faces four primary challenges: First, during the prefill stage, the computation of attention incurs quadratic time complexity with respect to the sequence length. Besides, during generation, the use of an extremely large KV cache imposes significant memory and I/O pressure. When performing multi-device or cross-device computations, there is also communication overhead. The challenges addressed by various approaches are delineated in the table 5. Computational Overhead. During the prefill stage, the computation of KV cache incurs quadratic time complexity with respect to the sequence length. Research efforts aim to accelerate the prefilling process [218, 292], primarily mitigate the computational overhead. 37 In contrast, the decoding phase is predominantly bandwidth-limited. Optimizations for decoding, such as Quantization and Speculative Decoding, often strategically trade increased computational workload for reduced memory transfer requirements. I/O Overhead. In Autoregressive Decoding, I/O operations become the primary bottleneck for speed. Each I/O operation requires transmitting both the model parameters and the KV cache into the computation unit to generate the next token. For long inputs, compressing the KV cache memory can significantly improve the speed. For long outputs, Spectulative Decoding and Model Parameters Quantization accelerate the process by reducing the memory of transmissions of model parameters. GPU HBM Memory. Long-text tasks also impose significant HBM pressure on GPUs. As result, some approaches offload the KV cache to the CPU and use retrieval techniques to alleviate the memory usage on HBM. Quantization applied to the model parameters or the KV cache, can also help reduce the pressure on HBM memory. Communication Overhead. In large-scale deployments, it is common to cache the precomputed KV cache of prompts, and when receiving the same prompt requests, the KV cache is transmitted between different machines via communication. Memory Management and Prefilling-Decoding Disaggregation methods can optimize this aspect. In the following subsections, we will introduce several common techniques for enhancing inference performance through AI infrastructure improvements. These techniques include: quantization, memory management, PD Disaggregation, GPU-CPU Parallel Inference, and Speculative Decoding. 5.2.1. Quantization In long context LLMs, processing extended input sequences leads to significant increase in Key-Value (KV) cache size. Furthermore, generating lengthy output sequences necessitates repeated transfers of both the KV cache and model parameters to computational units, exacerbating bandwidth demands. To mitigate these challenges and accelerate the decoding phase, quantization emerges as crucial strategy by reducing the volume of data transferred. Quantization for long context LLMs can be applied to either the KV cache alone or both the model parameters and KV cache. One strategy focuses on quantizing solely the KV cache [104, 187, 237, 332, 649]. However, due to the prevalent lack of native hardware support for mixed-precision operations (e.g., FP16 INT4) on mainstream architectures, efficient fusion computations often necessitate the development of specialized kernels. Within KV cache quantization, common optimization techniques include: quantizing the Key and Value with different methods [187, 332], filtering outliers [187, 668], recording or adjusting the size of the channel [114, 578, 668]. In contrast, quantizing both model weights and activations to uniform low precision [318, 454, 578, 611, 629, 668] allows for the direct utilization of existing hardware-supported low-precision operations. 5.2.2. Memory Management Virtual Memory Management In long context inference, the KV cache can grow extremely large, leading to memory fragmentation and inefficient usage. Virtual memory management techniques address these challenges by optimizing how KV cache memory is allocated and accessed, reducing waste and improving performance for long sequences. PagedAttention [257] utilizes virtual memory to place the KV cache of tokens at the same position across different layers and heads within the same memory page. vTensor [589] introduces virtual memory 38 abstraction that decouples computation from defragmentation. KV-Compress [426] extends PageAttention to support token-based KV compression. Scheduling Strategies When dealing with long context, especially in batched inference or scenarios with shared prefixes (like in conversational AI), efficient scheduling of KV cache and computations becomes critical. Scheduling strategies tackle this by organizing data structures to facilitate deduplication and sharing, maximizing memory utilization and computational efficiency for long inputs. ChunkAttention [614] and MemServe [190] organize data structures to enable efficient cache deduplication and sharing of common prefixes, thereby improving both memory utilization and computational efficiency. SGLang [673] use RadixAttention for sharing common prefixes between batches. 5.2.3. Prefilling-Decoding Disaggregated Architecture Demanding both increased computation and memory for KV caches, long context LLMs inherently suffer from higher latency, resource consumption, and potential bottlenecks. To address these challenges, Prefilling-Decoding disaggregation decouples the computationally intensive prefill stage from the bandwidth-sensitive decode stage, assigning each to dedicated server pools optimized for their distinct resource demands [396, 411]. By strategically allocating hardware, PD disaggregation boosts inference efficiency, demonstrably improving both Time to First Token (TTFT) for the prefill phase and Time Per Output Token (TPOT) for decoding phase [676]. Distserve [676] customizes resource allocation, parallelism strategies, deployment algorithms, and runtime scheduling optimizations for each stage. Splitwise [396] investigates how to allocate machines within cluster to handle the prefill phase and decoding phase effectively. Mooncake [411] particularly excelling in long context scenarios and under heavy user loads, developed prediction-based early rejection policy. CacheGen [330] reduces the transmission time of precomputed KV caches across machines by adopting an optimized storage format. These studies collectively highlight the potential of PD disaggregation to accelerate LLM inference, paving the way for more scalable and cost-effective deployment of these powerful models. 5.2.4. GPU-CPU Parallel Inference GPU memory, designed for high-bandwidth access by computational units, is inherently limited and frequently inadequate to accommodate the ever-growing size of the KV cache, especially in long context scenarios. cost-effective strategy to mitigate this constraint is offloading the KV cache to CPU memory, with the potential for further offloading to hard disk or network storage [215, 593]. While offloading reduces GPU memory pressure, it introduces new bottleneck: the slow PCIe bus becomes limiting factor when transferring the KV cache from CPU to the GPU for computation [173, 619]. To mitigate the issue of slow PCIe bandwidth, GPU-CPU Parallel Inference methods leverage concurrent CPU computation during PCIe transfers to either reduce the amount of data that needs to be transferred or optimize subsequent GPU computations, thereby improving overall efficiency. FlexGen [454] and PipeSwitch [22] are techniques that attempt to overlap GPU-based computation of the current layer with the concurrent loading of the KV cache for the subsequent layer. FastDecode [173] proposes computing attention scores directly on the CPU, leveraging its faster memory access to the KV cache relative to the GPU. Another methods employ CPU-GPU heterogeneous execution strategies to mitigate data transfer overhead by strategically performing computations on the CPU [393, 593, 619]. 39 5.2.5. Speculative Decoding In long output scenarios, the decoding phase becomes dominant factor in overall inference time. Greedy Decoding requires transmitting the model parameters for every new token computation. Speculative Decoding, on the other hand, accelerates this process by generating multiple potential tokens in single pass and processing them together, thereby reducing the frequency of transferring model parameters from HBM to the computation units. Specifically, Speculative Decoding [269] consists of smaller draft model and larger target model, the core idea is: (1) The draft model generates ùõæ candidate tokens. (2) The target model verifies the ùõæ tokens simultaneously. (3) The target model then either generates the first rejected token or adds new token if all previous tokens are accepted. Self-Speculative Decoding [115, 186, 641] leverage layer-skipping techniques to use the target model itself as the draft model, thereby reducing the overhead associated with maintaining separate draft model. This approach sometimes allows the draft model and the final model to share the KV cache [115], further optimizing resource usage. MagicDec [52] and TRIFORCE [477] use draft model with fixed KV budget using sparse attention. Medusa [39] and Eagle [295] introduce additional heads trained after the target model to predict the next few tokens directly. More details about Speculative Decoding can be seen in this survey [575]. 6. Evaluation This section introduces the evaluation of long context models. We generally divide the capacity of long context modeling into two aspects: processing long inputs and generating long outputs, namely Long Context Comprehension and Long-Form Generation. For long context comprehension, we will first introduce its evaluation paradigm (Sec. 6.1.1), then summarize recent benchmarks (Sec. 6.1.2), and discuss how to design better benchmarks for long context comprehension (Sec. 6.1.3). For long-form generation, we will first review its definition and introduce representative benchmarks (Sec. 6.2.1). Then, we summarize the data sources (Sec. 6.2.2) and present the commonly used evaluation methods (Sec. 6.2.3). After that, we discuss the challenges and trends in long-form generation (Sec. 6.2.4). 6.1. Evaluating Long Context Comprehension 6.1.1. Evaluation Paradigm This section presents the evaluation paradigm for long context comprehension. As shown in Figure 7, we categorize the capabilities of LCLMs in processing long inputs into five hierarchical levels: language modeling, retrieval, aggregation, reasoning, and real-world adaptation. At the foundation lies language modeling, which is the most basic capability for understanding the input text. Building upon this foundation, retrieval, aggregation, and reasoning constitute the core capabilities for long-text modeling. For each of these three capabilities, we provide detailed taxonomy along with corresponding synthetic tasks specifically designed to evaluate each aspect. At the highest level, real-world adaptation examines how well LCLMs can leverage their long context capabilities in practical scenarios. Specifically, we provide detailed introduction to the most representative long context tasks, including question answering, summarization, document retrieval & reranking, retrieval-augmented generation, many-shot in-context learning, coding, etc. Performance on these tasks are among the most important indicators for assessing LCLMs long-text understanding abilities. Figure 7. Evaluation paradigm for long context comprehension. Language Modeling Language modeling is the most fundamental capability for long context comprehension. Typically, low perplexity is an indicator that the model have good understanding of the given input document. In the realm of long context modeling, it is not only required that LCLMs have an overall low perplexity, but also that this perplexity decreases as the context window grows, indicating that the models can make more accurate predictions of target tokens by fully utilizing the rich context information. In practice, there are two common ways to examine the efficacy of LCLMs through the lens of perplexity. The first way is to inspect cumulative average negative log-likelihood (NLL) as token position in long documents increases, as exemplified by Gemini-1.5 [499]. decreasing trend in this cumulative NLL indicates that model can make better predictions of future tokens as the context window grows. An alternative way is to inspect PPLs calculated by the sliding window approach [60, 408, 686] under various window sizes. Specifically, the sliding window approach have pre-defined window size ùë§. The model first predicts the probability of the beginning ùë§ tokens, then gradually slides the window forward to process the remaining tokens and calculate overall PPL. In this case, decreasing PPL as the window size increases indicates that the model can effectively utilize long context information. Retrieval Retrieval requires LCLMs to identify and extract target information from one or multiple locations within long contexts. This capability underlies most long context tasks, as they typically require implicit retrieval of relevant content before subsequent processing. Categories The retrieval capability comprises two distinct levels based on their complexity: Explicit Retrieval and Semantic Retrieval. Explicit retrieval refers to string matching based on given queries, where models must locate and extract matching content from the source text. Semantic retrieval, which is typically more challenging, requires models to extract semantically relevant content based on the semantic meaning of queries. Synthetic Tasks Needle Retrieval (or Needle-in-a-Haystack, NIAH) serves as prototypical synthetic task to evaluate retrieval capabilities. This task inserts one or more needles into long sequence and queries the model to retrieve the corresponding needles [17, 189, 236, 274, 282, 313, 370, 429, 586, 688]. The specific instantiation of needles and documents takes on various forms. The needles can be n-digit numbers, specific sentences, UUIDs, dictionaries, functions, passages, etc. while documents can be repeated noisy sentences, meaningful content extracted from essays, code repositories, or other sources. In line with capability categorization, the needle retrieval task includes explicit and semantic variants. In Explicit (or Literal) Needle Retrieval, models simply need to perform exact matching of needles, such as locating the completion of partial sentence within long text [189, 236, 274, 282, 370]. Semantic Needle Retrieval, however, requires models to identify content based on semantic correspondence [369], such as retrieving paragraphs based on their abstracts [17, 688] or functions based on their behavioral descriptions [313]. Notably, while these needle retrieval tasks primarily assess retrieval capabilities, they can be highly challenging and thus serve as crucial evaluation tasks for long context modeling abilities. Aggregation Aggregation refers to the models ability to integrate information from multiple locations or even globally across long contexts. Building upon retrieval capabilities, this competency encompasses two key aspects: first, it requires the model to process the text piece-by-piece across multiple locations or even the entire context; second, it demands the model to recognize meaningful connections between these pieces and synthesize them into coherent higher-level representations. Categories Based on the nature of aggregation targets, Aggregation can be further divided into two types: Statistical Aggregation and Semantic Aggregation. Statistical Aggregation requires models to perform quantitative analysis over long contexts, while Semantic Aggregation focuses on combining and synthesizing semantic information from different parts of the context. Synthetic Tasks variety of synthetic statistical tasks have been proposed to evaluate Statistical Aggregation capabilities [189, 467, 653]. These tasks include tracing variable states, extracting frequent patterns, computing descriptive statistics (e.g., maximum, median, mode), and performing other numerical operations across extended sequences. For Semantic Aggregation, SummHay [258] requires LCLMs to process the synthesized Haystack of documents and generate summary. Additionally, some synthetic tasks specifically focus on solely evaluating the models piece-by-piece processing capability across long context, such as stacked news labeling [416] and stacked typo detection [416]. Reasoning Long context reasoning refers to LCLMs ability to perform logical inference over information distributed across long contexts. While both reasoning and aggregation involve identifying and processing multiple pieces of information, reasoning further emphasizes the logical deduction and inference process rather than mere information collection and summarization. Categories In the realm of long context comprehension, Reasoning can be further categorized into Parallel Reasoning and Iterative Reasoning. Parallel Reasoning involves gathering all relevant information first before conducting the reasoning process, while Iterative Reasoning requires step-by-step approach, where each reasoning step informs the next information gathering target, forming continuous cycle until reaching the final conclusion. Synthetic Tasks In this aspect, multi-needle reasoning serves as prototypical synthetic task that emphasizes both parallel reasoning and iterative reasoning capabilities. It requires models to reason across multiple needles distributed throughout long document [254, 282]. These needles are typically logically related facts extracted from existing reasoning benchmarks. For example, the BABILong benchmark [254] utilizes samples from the bAbI dataset [551] as needles, while NeedleBench [282] opts for the R4C dataset [205]. 42 Real-World Adaptation Real-world adaptation represents the highest level in the capability hierarchy for long context comprehension, where models must effectively integrate and apply their fundamental capabilities (language modeling) and core capabilities (retrieval, aggregation, and reasoning) to address practical challenges. Unlike synthetic tasks that evaluate specific capabilities in controlled settings, real-world tasks present complex scenarios that often require multiple capabilities working in concert. These tasks not only test models individual capabilities but also their ability to appropriately combine and deploy these capabilities based on task requirements. Introductions to the most representative real-world tasks involving long context comprehension are provided below. For each task, we use stars to denote which of the three core capabilities - retrieval , aggregation , and reasoning - the task tends to rely more heavily on. Question Answering [ / / ] The Question Answering task requires models to provide accurate answers based on real-world long contexts and queries. These questions and contexts span diverse domains, including literature (NarrativeQA [245], QuALITY [391], LStQA [416], NoCha [239]), academic papers (Qasper [94]), encyclopedias (WikiQA [606], HotpotQA [608], 2WikiMultihopQA [183], DuReader [176], AltQA [389]), conversation history (LCvMem [416]), financial reports (DocFinQA [425]), structured data (Table QA [651]), hybrid scenarios (MultiFieldQA [17]), etc. Notably, due to the diversity of questions and domains, different QA tasks may emphasize different aspects of long context capabilities - some requiring primarily information retrieval, others focusing on information aggregation across multiple parts, and yet others demanding complex reasoning over the aggregated information. Summarization [] Summarizing key information from long sequences of text has long been crucial research area in NLP. Throughout the development of this field, researchers have created diverse summarization datasets spanning nearly all sorts of relevant domains, including novels ([653], LStSum [416], SQuALITY [518]), government reports (GovReport [195]), meeting scripts (QMSum [674], VCSUM [559]), news articles (MultiNews [117]), patents (BigPatent [450]), screenplays (SummScreen [59]), legal documents (MultiLexSum [452]), and more. Notably, long context summarization also stands as one of the primary application scenarios for long context models. Document Retrieval and Reranking [ / ] Document Retrieval and Reranking are crucial components of modern Information Retrieval (IR) systems, responsible for retrieving relevant documents from pool of candidates and reordering them based on their relevance to the query, respectively. The advent of LCLMs has enabled new paradigm where models directly process all candidate documents and produce their rankings in generative manner [346, 481, 531]. Conversely, retrieval and reranking performance can serve as indicators of these models long context capabilities, with the former emphasizing the ability to retrieve relevant information and the later focusing on reasoning across different portions of long context [262, 616]. For example, the HELMET benchmark includes Reranking test where candidate documents are sampled from the MSMARCO [380] retrieval dataset, and LCLMs are challenged with generating top-10 document IDs ranked by relevance. Beyond relevance-based reranking, the task encompasses broader forms of sequence organization, such as timeline ordering [277, 534, 643] and paragraph order reconstruction [107]. Retrieval-Augmented Generation [] Retrieval-augmented generation enhances the accuracy and reliability of generative models by providing them with corpus of facts retrieved from external sources. In long context scenarios, where the input corpus can extend up to 1M tokens [262], relevant information becomes more dispersed and sparse, challenging LCLMs ability to aggregate information across extended contexts. Existing 43 evaluation approaches [262, 616] primarily employ open-domain QA to assess LCLMs performance in retrieval-augmented generation. These evaluations typically process opendomain QA datasets such as Natural Questions [256], TriviaQA [232], and HotpotQA [608] by concatenating their passages into large corpus. The model input consists of this corpus along with queries targeting specific passages, requiring LCLMs to locate relevant information within the extended corpus to generate appropriate answers. In-Context Learning [] With extended context window size, In-Context Learning (ICL) expands the scale of demonstration examples from dozens, as seen in traditional ICL frameworks, to hundreds or even thousands. This scenario challenges LCLMs to generate precise predictions based on this substantially larger set of examples[28, 262, 288, 591]. Recent empirical studies have revealed that existing LCLMs exhibit several limitations in this setting: they demonstrate significant performance degradation beyond certain context lengths, show susceptibility to example ordering effects, display recency bias by favoring predictions aligned with labels presented near the sequence terminus, etc. [288, 591] These findings indicate that there remains substantial journey ahead for LCLMs to fully unleash the potential of ICL. Code Relevant Real-World Tasks [] Processing repository-level code represents particularly compelling application scenario that necessitates LCLMs. Among various specific tasks, code completion has garnered significant attention due to its comprehensive reflection of models capability to process entire repositories [30, 224, 321, 634]. Additionally, several tasks address other potential challenges in repository-level code processing, including CI builds repair, commit message generation, bug localization, module summarization, execution simulation, code translation, etc. [30, 544, 653]. Despite significant advances, the intrinsic complexity of repository-level code still poses substantial challenges for current LCLMs in addressing real-world code processing requirements, as demonstrated by their performance limitations on SWE-Bench [224]. 6.1.2. Evaluation Benchmarks Following the evaluation paradigm established in Section 6.1.1, we present comprehensive overview of recent long context comprehension benchmarks. These benchmarks can be categorized into two groups: synthetic benchmarks  (Table 6)  that are entirely constructed using heuristic rules, and real-world benchmarks  (Table 7)  that primarily consist of real-world tasks with human annotations. For each benchmark, we present its supported length (maximum token count), characteristics, evaluation setup, and evaluation metrics. The evaluation setup encompasses four main formats: multiple-choice question answering with provided options, free-form generation, classification, and language modeling. The evaluation metrics include automatic metrics, LLM-as-a-Judge approaches, and hybrid methods combining both. Additionally, for synthetic benchmarks, we annotate their primary focus on specific aspects of long context comprehension capabilities, as these artificial tasks are typically designed to test particular comprehension skills. For real-world benchmarks, we document their covered scenarios, including Question Answering, Summarization, Document Retrieval & Reranking, RetrievalAugmented Generation, In-Context Learning, Code-Relevant Tasks, and various synthetic tasks. Based on Table 6, we make several observations about existing synthetic benchmarks. First, significant portion of synthetic benchmarks consists of various NIAH task variants, featuring more complex forms or different domains, essentially evaluating the robustness of models long context retrieval capabilities. Second, most synthetic benchmarks rely solely on automatic metrics such as accuracy and F1 scores, with only few requiring LLM or hybrid evaluation 44 Benchmark Support Length Characteristics Target Eval Aspects Setup Metrics Ada-LEval [521] BABILong [254] DENIAHL [88] HoloBench [349] LIFBENCH [567] LongIns [137] General Domain Synthetic Benchmarks 128k Adaptable length 10m BAbI in long context 4k Different NIAHs 64k Database aggregation&reasoning 128k 16k Instruction-following&stability, new metric Instruction-centric LongPiBench [508] 256k Focusing on position bias LongRangeArena [495] 16k Early attempts, multiple modalities LongReason [305] 128k Long Context reasoning mLongRR [5] M4LE [255] 64k Multi-lingual retrieval&reasoning 8k Bilingual, semi-realistic Michelangelo [514] 128k Latent structure queries MLNeedle [180] 32k Multilingual NIAH NeedleThreading [429] 900k Different NIAHs NoLiMA [369] RULER [189] S3Eval [266] 32k Semantic NIAHs w/ minimal lexical overlap 128k Different NIAHs + other synthetic tasks 80k SQL-centric SummHay [258] 100k Synthetic tasks for summarization Specific Domain Synthetic Benchmarks LongHealth [4] MathHay [528] RepoQA [313] 8k Medical, fictional patient cases 128k Mathematical retrieval&reasoning 16k Code-style NIAH G G C G G G Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto Auto LLM Auto Hybrid Auto Table 6. Overview of synthetic benchmarks for long context comprehension. Support Length is the maximum token count of samples in the benchmark (k=210, m=220). Characteristics describe the unique features of the benchmark. Target Aspects indicate the core long context capabilities the benchmark aims to evaluate, including Retrieval , Aggregation , and Reasoning . Eval Setup denotes the evaluation format the benchmark adopts: for multi-choice question answering, for free-form generation, for classification. Metrics lists the evaluation metrics used in the benchmark, including automatic metrics (Auto), LLM-as-a-Judge approaches (LLM), and hybrid styles combining both (Hybrid). Benchmark Support Length Characteristics Scenarios General Domain Real-World Benchmarks BAMBOO [107] 16k Multi-task CLongEval [416] 100k Chinese, human-curated (partly) 1 6 1 2 DetectiveQA [592] 250k Bilingual, human-curated, detective novels 1 ETHIC [264] 100k Requiring high information coverage Eval Setup Metrics Auto G 1 2 1 2 6 1 2 3 4 5 1 2 1 1 G InfinityBench [653] HELMET [616] L-CiteEval [494] L-Eval [10] LIBRA [84] LOFT [262] Bilingual, long average length 100k 128k Application-centric, robust eval 48k Answer with citations (faithfulness) 200k Diverse data, improved metrics 128k Long benchmark for Russian analysis 1m Real tasks of extreme lengths 3 4 5 Long2RAG [410] 32k Long retrieval&output + new metric 4 LongBench [17] LongBench-v2 [20] LongBench-Cite [637] Bilingual, application-centric 16k 2m Challenging tasks, expert curation 70k QA with citations (trustworthiness) LongICLBench [288] 50k Extreme-label classification LongMemEval [558] 1.5m Long-term chat memory Loong [534] LooGLE [277] LV-Eval [628] Bilingual, semi-realistic, human-curated 250k 24k Human curation, long dependency 256k Adaptable length, less knowledge leakage ManyICLBench [693] Marathon [643] NoCha [239] TCELongBench [660] Extensive ICL test, new metric 128k 80k Human curation, multi-choice QA 336k Global reasoning over entire novels 12k Temporal complex events ZeroSCROLLS [443] 8k Early attempts Specific Domain Real-World Benchmarks DocFinQA [425] FinTextQA [53] 200k Finance, FinQA in long context 30k Finance LongCodeArena [30] 2m+ Code-centric MedOdyssey [119] 200k Medical, diverse tasks, new metrics NEPAQuAD1.0 [404] 600k Environmental statements&acts 1 2 5 6 1 5 6 1 2 5 1 1 3 1 2 1 1 3 1 1 3 1 2 1 1 6 1 1 M G G M G G Auto Auto Hybrid Auto Auto Auto Hybrid Auto Auto Auto Auto Auto Auto Auto Hybrid LLM Hybrid Auto Auto Auto Auto Auto Auto Auto Hybrid Auto Auto Hybrid Table 7. Overview of real-world benchmarks for long context comprehension. Support Length is the maximum token count of samples in the benchmark. Characteristics describe the unique features of the benchmark. Scenarios indicate the real-world task types covered by each benchmark: Question Answering 1 , Summarization 2 , Document Retrieval & Reranking 3 , Retrieval-Augmented Generation 4 , In-Context Learning 5 , Code relevant tasks 6 , and synthetic tasks . Eval Setup denotes the evaluation format the benchmark adopts: for multi-choice question answering, for free-form generation, for language modeling. Metrics lists the evaluation metrics used in the benchmark, including automatic metrics (Auto), LLM-as-a-Judge approaches (LLM), and hybrid styles combining both (Hybrid). 46 approaches when assessing longer generated outputs like summaries or mathematical reasoning processes. Third, the controlled construction process of synthetic benchmarks enables systematic comparisons of specific factors such as language, position, and patterns under controlled variables. Analysis of Table 7 reveals several patterns in real-world benchmarks. First, Question Answering emerges as the most prevalent scenario, which aligns with the inherent diversity of QA tasks, followed by summarization tasks. Second, some real-world benchmarks adopt multiplechoice QA formats to guide model generation, possibly due to the diverse nature of real-world task outputs where unconstrained generation might complicate evaluation. Third, interestingly, not all real-world benchmarks rely entirely on human annotations; many incorporate synthetic tasks, likely to ensure comprehensive evaluation coverage. Furthermore, examining both synthetic and real-world benchmarks collectively reveals that medical, financial, and code-related domains particularly demand strong long context processing capabilities. 6.1.3. Discussion What Makes Good Benchmarks for Long Context Comprehension From high-level perspective, an effective long context comprehension benchmark should satisfy three key requirements: coverage of sample lengths matching models context windows, evaluation of fundamental long context modeling capabilities, and assessment of downstream task performance. Given that current LCLMs typically feature context windows exceeding 128k tokens, benchmarks primarily focusing on documents under 32k tokens may inadequately assess the capabilities of mainstream LCLMs. For fundamental capabilities, including retrieval, aggregation and reasoning as discussed in Section 6.1.1, the main challenges lie in the dispersion of key information and the difficulty of extraction [151], with more dispersed and semantically oriented (rather than literal) information posing greater challenges [369]. Although synthetic tasks are particularly well suited for evaluating these aspects, research indicates that excellence in synthetic tasks alone does not guarantee downstream competence. Therefore, comprehensive benchmarks must incorporate specific downstream long context tasks, particularly question answering, summarization, RAG, and in-context learning, which serve as relatively complete proxy for real-world applications. final notable trend in evaluation methodology is the increasing formulation of tasks such as multiple choice question answering, allowing direct performance assessment through accuracy measurements. 6.2. Evaluating Long-Form Generation Beyond long context comprehension, where the inputs are lengthy, another common paradigm for long context modeling is long-form generation, where the outputs are lengthy. Long-form generation is classic task in the field of NLP [32, 80, 556]. In the realm of LLMs, the increasing practical demand for document-level text generation [341, 513, 624] and repository-level code completion [315, 520] has brought growing attention to long-form generation, leading to the emergence of several evaluation benchmarks and improvement approaches [21, 418]. In this subsection, we first review the definition of the long-form generation and introduce representative benchmarks. Then, we summarize the data sources and present the commonly used evaluation methods. After that, we discuss the challenges and trends in long-form generation. The overview of long-form generation is illustrated in Figure 8. Figure 8. Overview of evaluating long-form generation. 6.2.1. Evaluation Benchmark long-form generation is generally defined as generating long, coherent, and contextually relevant text in response to given input or instruction. To better understand the scope of the long-form generation, we provide the following clarification: First, the instruction must either explicitly or implicitly indicate the necessity for long response. Explicit requirements refer to instructions that clearly specify the expectation for long response, either through specified word count or direct statement. Implicit requirements, in contrast, do not specify desired length but instead arise from tasks that inherently demand detailed responses. For instance, open-ended questions often require comprehensive examples or in-depth discussions to effectively convey perspective, naturally leading to longer responses compared to close-ended questions. Second, the definition of what qualifies as long response varies based on the nature of the task. In the context of writing task, response is typically considered long if it exceeds 1,000 words, whereas in question-answering task, response may be regarded as long if it surpasses 500 words. Accordingly, long-form generation can be defined as tasks that require responses substantially exceeding the average length associated with the specific task type. Table 8 lists representative benchmarks for long-form generation. We summarize the task types of long-form generation into four categories: Question Answering (QA), Summarization (Summ), Instruction Following (IF), and Mixed. Question Answering (QA) In long-form generation, QA mainly refers to long-form QA. ELI5 [118] is the first large-scale benchmark for long-form QA, where the task is to generate multi-sentence explanations in response to open-ended questions sourced from Reddit. Similarly, MS-NLG is subset of MS MARCO [380] that focuses on natural language generation. In addition to general long-form QA, ASQA [472] and QASA [265], which focus on ambiguous questions and scientific questions, respectively. In the realm of LLMs, ExpertQA [351] and ProxyQA [486] are high-quality long-form QA benchmarks with questions crafted by experts across various fields, along with expert-verified golden answers. Retrieval Augmented Generation (RAG) is an important application of LLMs, CLAPNQ [431] and Long2RAG [410] are benchmarks designed to evaluate LLM-based RAG systems. Besides, generating long-form and well-supported responses is particularly essential in high-stakes domains such as medicine [188, 212]. Although LLMs have demonstrated strong performance in many tasks, they often contain factual errors when generating long responses [549]. Min et al. [365] propose novel framework for factual evaluation, comprising three steps: decomposing the response into atomic facts, annotating each fact as supported, unsupported, or irrelevant, and computing the proportion of supported 48 Benchmark Size Task Target Aspect Source Eval Method ELI5 [118] MS-NLG [380] ExpertQA [351] ProxyQA [486] LongGenBench-HUST [324] ASQA [472] QASA [265] CLAPNQ [431] Long2RAG [410] LFMedQA [188] MedLFQA [212] FActScore [365] LongFact [549] LTF-TEST [213] AQUAMUSE [251] Multi-News [117] LCFO [87] LongForm-C [247] Suri [403] LongBench-Write [21] LonGen [417] LOT-OutGen [160] LongLaMP [252] DoLoMiTes [352] LongGenBench-NUS [568] LongProc [615] HelloBench [418] FACTS Grounding [210] 272K 183K 2K 100 16K 6K 2K 5K 280 1K 5K 1K 12K 6K 56K 252 28K 20K 120 240 2K 63K 2K 400 2K 647 2K QA QA QA QA QA QA QA QA QA QA QA QA QA QA Summ Summ Summ IF IF IF IF IF IF IF IF IF Mixed Mixed General General General General General Ambiguous Scientific Web User CrowdSrc CrowdSrc PADs PADs Auto, Human Auto Auto LLM Auto Auto, Human CrowdSrc Auto RAG RAG Medical Medical Factual Factual Fairness General General General General General Writing Writing Writing Personalized Structured Structured Structured General Factual PADs PADs User PADs Web Synthesis Synthesis Auto, Human Auto LLM Auto LLM LLM LLM Web, PADs Auto, Human Web PADs PADs PADs User User Web PADs Web Synthesis PADs Auto, Human Auto, LLM, Human Auto Auto, Human Auto, LLM Auto, LLM Auto, Human Auto LLM Auto Auto Web, PADs LLM, Human CrowdSrc LLM Table 8. Representative benchmarks for long-form generation. Web refers to web-sourced data, meaning the data comes from the web. User means the data comes from real users. CrowdSrc is the abbreviation for crowdsourcing, indicating that the data comes from annotation teams. PADs stands for Publicly Available Datasets, referring to the data sourced from publicly available datasets. Synthesis means the data is synthetic. Auto refers to automatic evaluation metrics. Human represents human evaluation. LLM refers to the evaluation based on LLMs, also known as LLM-as-a-Judge. 49 facts. In addition to concerns related to factuality, long-form responses are also susceptible to biases. To evaluate fairness in long-form generation, Jeung et al. [213] propose an evaluation framework that covers 14 topics and 10 demographic axes. Summarization (Summ) Summarization is classic task in long context modeling. As input documents become increasingly numerous and lengthy, the length of summaries has also grown significantly. Compared to early summarization benchmarks where summaries were around 50 words long [377], recent summarization benchmarks feature much longer summaries. As result, summarization not only evaluates models ability to comprehend and condense long input documents but also tests its capability for generating long-form summaries. MultiNews [117] is the first large-scale multi-document news summarization benchmark, collected from newser.com. AQUAMUSE [251] proposes scalable method to automatically mine summarizations from Natural Questions [256] and Common Crawl. LCFO [87] is human-annotated benchmark for evaluating long context summarization and summary expansion, consisting of 252 long documents from diverse domains. Instruction Following (IF) Apart from using questions as inputs for LLMs, instructions and other forms of prompts can also serve as inputs. Such tasks prioritize the evaluation of models ability to follow instructions, encompassing both straightforward instruction adherence and more complex, constrained instruction-following. To evaluate the ability of LLMs in structured problem-solving, DoLoMiTes [352] consists of 519 domain-specific long-form methodical tasks with 1,857 examples collected from experts across 25 fields. Similarly, LongGenBench-NUS6 [568] and LongProc [615] are designed to evaluate the capability of LLMs to generate high-quality long-form text while adhering to complex instructions. Besides, LongLaMP [252] is benchmark for personalized generation, covering four tasks: personalized email completion, abstract generation, review writing, and topic writing. Recently, increasing attention has been given to aligning LLMs with long-form output instructions, such as creative writing and story generation. To enhance instruction tuning for long-form generation, LongForm-C [247] gathers instructionfollowing examples via reverse instructions on C4 and English Wikipedia. Bai et al. [21] propose long instruction-following dataset LongWriter-6K aimed at extending the output length of existing models to over 10,000 words. Similarly, Suri [403] is benchmark consisting of 20,000 long-form human-written texts, each paired with backtranslated instructions with multiple constraints. Quan et al. [417] propose Self-Lengthen, an iterative training framework that utilizes LLMs intrinsic knowledge without relying on auxiliary data. To evaluate its effectiveness, they introduce LongGen, benchmark for evaluating LLMs long-form generation capabilities in Chinese and English across various tasks, with length-constrained user instructions and corresponding responses. Mixed Some benchmarks include multiple task types to enable more comprehensive evaluation. Facts Grounding [210] evaluates LLMs ability to generate factually accurate long-form text based on given context and user requests, encompassing tasks QA, summarization, and document rewriting. HelloBench [418] is comprehensive benchmark for evaluating LLMs long-form generation capabilities, consisting of 647 samples across 38 subcategories and 5 tasks (QA, summarization, chat, completion, generation), sourced from real-world scenarios and publicly available datasets. 6In fact, there are two benchmarks named LongGenBench [324, 568]. To distinguish them, we add the suffix of the respective primary institution. 50 6.2.2. Data Source The data source of benchmark determines its data quality. reasonable, high-quality, and easily accessible data source can significantly enhance the overall quality of the benchmark. Data sources for long-form generation benchmarks can be categorized into five types: Web-Sourced Data: Web contains abundant and vast amounts of text content. The advantage of web-sourced data is its richness and diversity. However, not all web content meets high-quality standards and typically necessitates processes such as data cleaning and deduplication. Among long-form generation benchmarks, ELI5 [118] collects QA pairs from Reddit, FActScore [365] gathers people biographies from Wikipedia, and LOTOutGen [160] collects human-written stories by crawling web pages. User-Sourced Data: The main characteristic of user-sourced data is its practicality, as it reflects real-world scenarios faced by users. For example, MS-NLG [380] collects data from users search logs on Bing. LongBench-Write[21], LonGen [417], and LFMedQA [188] collect user data from their respective platforms: GLM, Qwen, and Lavita Medical AI Assist. Synthetic Data: Synthetic data is also commonly used method for constructing benchmarks. This method refers to predefining template and then filling it with different content to achieve both structure and diversity. The content can either be predefined or AI-generated. Benchmarks like LongFact [549], LTF-TEST [213], and LongGenBenchNUS [568] are constructed using this method. Crowdsourcing (CrowdSrc): Data collection processes are usually non-automated, requiring human involvement to ensure data quality. ExpertQA [351], ProxyQA [486], and QASA [265] collect questions from experts. Specifically, ExpertQA recruits experts through Prolific to write questions within their areas of expertise. ProxyQA manually creates meta-questions with the help of five experienced researchers. QASA involves AI/ML researchers to label data. Meanwhile, FACTS Grounding [210] instructs third-party human raters to design prompts that require both the processing of long-form input and the generation of long-form output. Publicly Available Datasets (PADs): Standing on the shoulders of giants is also an effective strategy for data collection. On one hand, the data quality of previous datasets is generally reliable, on the other hand, previous datasets often exhibit consistency, making it easier to process and adapt them. Many long-form generation benchmarks source their data from publicly available datasets. For example, LongGenBench-HUST [324] collects its data from MMLU [179], GSM8K [85], and CommonSenseQA [485]. ASQA [472] sources its data from AMBIGQA [364]. Long2RAG [410] sources its data from ELI5. 6.2.3. Evaluation Paradigm Evaluation methods are critical for benchmarks, as an appropriate evaluation method can provide unified standard for evaluating the performance of different models, highlight their strengths and weaknesses, and guide model iteration. Evaluating the performance of long-form generation presents significant challenges. Here, we summarize the evaluation methods used in existing long-form generation benchmarks and categorize them into three types, offering insights for the development of future evaluation methods. Automatic Metrics Automatic metrics are evaluation methods that evaluate model-generated responses without the need for additional human involvement or the use of LLMs. Most 51 traditional metrics are classified as automatic metrics. Existing automatic metrics in the field of long-form generation can be categorized into four types: Semantic Aspect: This type of metrics evaluates the semantic aspect of the response. Metrics like ROUGE [303] and BLEU [392] are the most commonly used and were initially developed for summarization tasks and machine translation tasks. ROUGE and BLEU measure the semantic similarity between the response and the reference answer, providing an indication of the generation quality of LLMs. They are adopted as evaluation metrics in 13 out of 28 benchmarks. Similarly, METEOR [23] is also used to measure the semantic similarity and is applied in LongForm-C [247] and LongLamp [252]. Besides, BERTScore [648] is metric that evaluates the semantic similarity of two texts using finetuned BERT model [212]. Repetitive Aspect: These metrics evaluate the repetitiveness or fluency of the responses. For instance, Perplexity (PPL) [301] calculates the overall probability of the response. Lower perplexity indicates higher fluency and suggests the sentence is more likely to be generated by an LLM. Metrics like Repetition [448] and Distinct [279] are used to measure repetitiveness. Repetition-n calculates the percentage of n-grams that appear at least twice in the text, while Distinct-n quantifies text diversity by computing the number of unique n-grams in the output. These metrics are used in the LOT-OutGen [160] and LCFO [87]. Accuracy-Based: Accuracy is more commonly used in short-form generation scenarios like multiple-choice QA. In long-form generation, LongGenBench-HUST [324] combines multiple questions into single input, requiring the model to generate responses for all of them. Each response is paired with corresponding standard answer (either specific option or number), making it well-suited for accuracy-based evaluation. Wu et al. [568] propose the CR and STIC metrics, both designed to measure the accuracy of responses in specific aspects. In addition, QASA [265] uses the standard accuracy as the evaluation metric. Task-Specific: Due to the varying focus of different benchmarks, task-specific metrics are often used. For example, ExpertQA [351] uses QAFactEval [116] to evaluate the factual consistency of the responses, while ASQA [472] uses Disambiguation Metrics to evaluate whether the response resolves ambiguities. Retrieval-oriented metrics, such as nDCG [431] and KPR [410], are used in CLAPNQ and Long2RAG to evaluate the effectiveness of the LLM-based RAG systems. Human Evaluation Although automatic metrics are convenient, recent studies have shown that the correlation between automatic metrics and human evaluation is quite low [250, 588]. To achieve more accurate evaluations, the best approach is to establish standardized evaluation criteria and have human evaluators evaluate the responses of LLMs [432]. ELI5 [118], ASQA [472], Long2RAG [410], AQUAMUSE [251], Multi-News [117], LongForm-C [247], LongBench-Write [21], LOT-OutGen [160], and HelloBench [418] have all conducted experiments involving human evaluation or provided human evaluation criteria. LLM-as-a-Judge The main drawback of human evaluation lies in its time-consuming, laborintensive, and inefficient nature. Recently, there has been growing trend of using strongperforming LLMs to replace humans for fine-grained evaluations [672]. This strategy is also known as LLM-as-a-Judge. In ProxyQA [486], LLMs are used to evaluate whether the responses to proxy questions are correct. In LFMedQA [212], GPT-4o and Claude-3.5 are used for pairwise comparisons. In HelloBench [418], predefined checklists are set for each sub-task, and the LLM 52 determines whether each checklist is satisfied. The overall score is calculated using humancorrelated weights. 6.2.4. Discussion What Kind of Data Sources Are Better? The data source lays the foundation for the quality of benchmark. In Section 6.2.2, we categorize the data sources for long-form generation into five types: web-sourced data, user-sourced data, synthetic data, PADs, and CrowdSrc. Web-sourced data is advantageous in terms of scale and accessibility, but it often suffers from low quality. Similarly, while user-sourced data is directly obtained from users, its quality can also vary significantly. Synthetic data has the advantage of making evaluation easier, as it allows for the use of automatic metrics with high evaluation accuracy. However, such data often lacks alignment with real-world scenarios. Previous available datasets pose risks such as data leakage. While CrowdSrc provides high-quality data, it may still deviate from practical use cases. It is clear that each data source has its own limitations. So, what type of data can be considered good for long-form generation? We argue that user-sourced data combined with detailed postprocessing is promising direction for future research. Specifically, the optimization goal of LLMs is to serve users, which makes understanding the real issues users encounter critical. As such, obtaining first-hand interaction data directly from users is ideal. However, user-sourced data often contains low-quality samples, making data filtering top priority. We believe that human involvement in the data post-processing phase is essential to achieve better outcomes. As long as data quality is ensured, having slightly fewer evaluation samples is unlikely to introduce significant evaluation bias. 7. Analysis Figure 9. Illustration of analysis of LCLMs. As shown in Figure 9, we provide the performance analysis and model structure analysis to understand neural network models externally and internally. This section provides detailed review of these works. 7.1. Performance Analysis First, LCLMs are analyzed as black boxes to examine their behavioral characteristics. In this subsection, we will first dive into the false promise of support context length, related to the well-known lost-in-the-middle phenomenon. Then, we explore the relationship between long context perplexity and real-world performance. We also discuss the ongoing debate between Retrieval-Augmented Generation (RAG) and long context modeling. 53 7.1.1. The False Promise of Support Context Length Recent years have witnessed remarkable growth in model context length, expanding from 4K tokens [510] to 128K tokens [154], and even 10M tokens [499]. Despite this impressive scaling of context extension, there exists significant gap between the context length these models claim to support and the actual context length they can process effectively [189, 317]. particularly noteworthy phenomenon in this regard is the lost-in-the-middle effect. Liu et al. [317] demonstrate that many LCLMs exhibit distinctive U-shaped performance curve: performance remains robust when information is located at the beginning or end of the input, but deteriorates substantially when critical information is positioned in the middle. An et al. [13] and He et al. [174] also observed that models progressively lose track of target information as the relative distance increases. To systematically assess the prevalence of this gap between supported and effective context lengths, RULER [189] conducted comprehensive evaluations of more than ten models, including both open source and proprietary ones, as illustrated in Table 9. The findings reveal consistent pattern: For most models across both categories, the effective context length rarely exceeds half of the claimed length. This discrepancy underscores that, alongside pursuing ever-larger context window sizes, improving model performance within already supported context lengths is equally important. Models OpenSource Claimed Length Effective Length Effective Ratio Llama2 (7B) Gemini-1.5-Pro GPT-4 Llama3.1 (70B) Qwen2 (72B) Command-R-plus (104B) GLM4 (9B) Llama3.1 (8B) GradientAI/Llama3 (70B) Mixtral-8x22B (39B/141B) Yi (34B) Phi3-medium (14B) Mistral-v0.2 (7B) LWM (7B) DBRX (36B/132B) Together (7B) LongChat (7B) LongAlpaca (13B) 4K 1M 128K 128K 128K 128K 1M 128K 1M 64K 200K 128K 32K 1M 32K 32K 32K 32K - > 128ùêæ 64K 64K 32K 32K 64K 32K 16K 32K 32K 32K 16K < 4ùêæ 8K 4K < 4ùêæ < 4ùêæ - > 12.8% 50% 50% 25% 25% 6.4% 25% 1.6% 50% 16% 25% 50% < 4% 25% 12.5% < 12.5% < 12.5% Table 9. Comparison of Claimed Length and Effective Length of Various Models [189]. 7.1.2. Relevance of Long Context Perplexity and Real-World Performance Beyond measuring language modeling capabilities, empirical evidence has demonstrated that language models perplexity on short texts exhibits strong correlation with their performance on short-text downstream tasks [200]. Specifically, lower perplexity scores on held-out set of short texts consistently predict superior performance on downstream short-text tasks. However, this correlation becomes substantially weaker in long context scenarios. Hu et al. [193], An et al. [10], and Sun et al. [479] all observed that different models perplexity scores on long contexts fail to correlate with their long context comprehension capabilities. Recent work has reestablished the role of perplexity in evaluating models long context modeling capabilities. Through comprehensive experiments, Lu et al. [338] demonstrate that when fine-tuning single base model (LLaMA2-7B) with various context-extension methodsincluding PI [60], YaRN [401], NTK [400], LongLora [68], Landmark Attention [370], and CLEX [50]the resulting models perplexity scores on the GovReport [195] dataset exhibit significant correlation with their performance on long context downstream tasks (including Needle-in-a-Haystack, LongBench, and RULER). Concurrently, Fang et al. [121] introduced LongPPL, refined metric that eliminates interference from context-irrelevant tokens by computing perplexity exclusively on context-sensitive token distributions. Under this improved metric, models long context perplexity scores show robust correlation with their long context downstream performance. Together, these investigations reestablish perplexity as an effective indicator of long context modeling capabilities, providing valuable reference for future reliable evaluation of LCLMs. 7.1.3. RAG Meets Long Context LLMs Since the emergence of LCLMs, they have frequently been compared with Retrieval-Augmented Generation (RAG) in terms of performance. Given query, the RAG pipeline first retrieves key information from corpora, then uses an LLM to generate answers based on this information, while LCLMs process entire corpora as input, implicitly identifying relevant information before producing answers. Lee et al. [262], Li et al. [296] revealed that when ample computational resources are available, LCLMs deliver superior average performance compared to RAG, despite not being specifically trained for these tasks. However, compared to RAG, directly employing LCLMs to generate responses based on entire corpora presents significant efficiency limitations. Therefore, instead of comparing RAG against LCLMs, current research has shifted toward combining these approaches to get the best of both worlds. Li et al. [296] proposed the Self-Route method, which dynamically directs queries to either RAG or LCLMs based on the models self-assessment, thereby optimizing the balance between performance and computational cost. Jiang et al. [223] leveraged LCLMs to obtain larger, semantically more coherent retrieval units for RAG. Jin et al. [225] discovered that \"hard-negative\" examples significantly impact LCLM-based RAG, and therefore designed both training-free and training-based approaches to mitigate this challenge. 7.2. Model Structure Analysis Section 7.1 examines LCLMs as black boxes, discussing the false promise of supported context length, the relevance between long context perplexity and real-world performance, and the ongoing debate between LCLMs and RAG. Taking step further, this section approaches LCLMs from glass-box perspective, providing deeper insights at the granularity of individual model components (e.g., positional embeddings, attention and MLP modules, transformer layers), thereby illuminating the internal workings that drive long context capabilities. 7.2.1. Positional Embedding In long context modeling, the larger the models context length is, the more possibilities there are in downstream applications. However, models are typically trained on small and fixed context length during pretraining. Length extrapolation studies the problem of enabling models to digest longer texts than the training distribution. In this section, we review how position embeddings (specifically RoPE [476]) could be extended for length extrapolation. 55 RoPE Essentials RoPE [476] explicitly encodes the relative position between tokens into attention. Recall that for model with even hidden size ùëë, the RoPE embedding [473] for absolute position ùëõ would be: (cid:32) ùëõ (cid:34) ùõΩ 0 (cid:32) ùëõ ùõΩ ùëë2 (cid:32) ùëõ ùõΩ ùëë (cid:32) ùëõ ùõΩ 0 (cid:32) ùëõ ùõΩ 2 (cid:32) ùëõ ùõΩ 2 , . . . , cos , cos , sin , sin , sin cos (10) (cid:33)(cid:35) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) ùëë ùëë ùëë ùëë ùëë ùëë The ùõΩ in Equation 10 is the rotation base. The trigonometric functions could be associated 1 (cid:111) ùëë with hypothetical wave lengths ùúÜ2ùëñ = , which are indicative of the models context ùëñ=0 capacity. For given ùõΩ, the wave lengths range from 2ùúã to 2ùúãùõΩ [401]. typical choice for ùõΩ is 10000 [476], yielding max wave length 63ùêæ. The hypothetical frequency associated with wave length is ùëì2ùëñ = 2ùúã ùúÜ2ùëñ . Therefore, the larger the base, the larger the wavelength and the lower the frequency is. 2ùúãùõΩ 2ùëñ (cid:110) 2 ùëë Position Interpolation (PI) Position interpolation [60] scales positions linearly to enlarge context length. Suppose that the model is pretrained with context length ùêø and we wish to extend it to ùêø > ùêø. Let ùë† = ùêø ùêø denote the scaling factor. PI maps the original position ùëõ to hypothetical position ùëõ = ùëõ ùë† . The largest position ùêø = ùë†ùêø would be mapped to ùêø. Therefore, all mapped positions still fall within the training range. From the perspective of wavelength, new wavelengths would be ùúÜ ùëÉùêº fine-tuning steps ( 1000). . PI enables length extrapolation with minimal 2ùúãùë†ùõΩ 2ùëñ (cid:111) ùëë ùëñ=0 2ùëñ = 1 (cid:110) ùëë Figure 10. Associated wave lengths of RoPE, PI, NTK, and YaRN. Figure taken from Peng et al. [401]. The y-axis uses log scale. For the sake of illustration, ùõº and ùõΩ parameters for YaRN are set 2 and 64 to 1024 2 . 2 . In practice, they are set to 2 2 and 2048 Frequency-Specific Scaling Cited works in this paragraph build on an important insight: injecting high frequency embeddings improves performance and convergence [488]. On the contrary, PI scales up wavelengths, which transforms high-frequency embeddings into lowfrequency ones. An initial solution named NTK-aware scaling [29] starts from unscaled high-frequency embeddings, then gradually transits to scaled low-frequency embeddings. Concretely, ùúÜ ùëÅùëá ùêæ 2ùëñ (cid:26) 2ùúã = (cid:16) ùë† ùëë ùëë2 ùõΩ ùëë (cid:27) (cid:17) 2ùëñ 1 . YaRN preserves unscaled high-frequency embeddings ùëë 2 ùëñ= more conservatively than NTK, and transits to scaled low-frequency embeddings faster. Figure 10 illustrates the wavelengths of RoPE, PI, NTK, and YaRN. 56 Critical Rotation Base Liu et al. [325] investigates the impact of both scaling up and scaling down the rotation base. Their setting is similar to PI, and they calculate critical scaling factors. Suppose that the model is pretrained with context length ùêø. Denote ùêø as ùúÜtrain to connect it with trigonometric functions. The critical rotation base has the largest wave length that is smaller than ùúÜtrain. Let ùêºùëê denote the critical dimension index, then ùêºùëê = 2 ùêºùëê is the critical rotation base ùõΩùëê and the critical wave length ùúÜ ùëê. . Associated with (cid:106) ùëë 2 logùõΩ train 2ùúã (cid:107) ùúÜ When scaling down the rotation base from ùõΩ to ùõΩ down, phase change happens when the largest wave length 2ùúãùõΩ down is equal to ùúÜ ùëê. The gains of scaling down the rotation base is prominent until ùõΩùëê. When scaling up the rotation base from ùõΩ to ùõΩup, ùúÜ ùëê would correspond to {ùúÜ ùëê}up, which is indicative of the phase change point: models perplexity would explode on texts longer than {ùúÜ ùëê}up. Men et al. [356] further scrutinizes scaling rotation bases. They point out that scaling down rotation bases can only achieve superficial long context capabilities. The authors take one more step to derive the lower bound of the rotation base for an expected model context length. The derivation is based on the following theoretical insight: the models ability to attend to similar token decays as the distance increases, but the decay rate is slower for larger bases. Empirically, there is polynomial relationship between effective context lengths and rotation bases. 7.2.2. Attention and MLP Analysis Attention Head Attention heads typically serve specialized purposes. For example, previous research finds induction heads, name mover heads and arithmetic heads in LLMs [383, 526, 650]. Strides have been made in identifying crucial components for long context modeling. Wu et al. [565] identifies retrieval heads, which are responsible for extracting information from long contexts. Ablating such heads leads to significant degradation across model families and model sizes. Razor Attention [489] refines retrieval heads into two subsets: echo heads and induction heads. Echo heads attend to identical tokens from the current location, while induction heads attend to the token after the identical token. Fu et al. [129] further points out Retrieval-Reasoning (R2) Heads, which combines retrieval capabilities with reasoning abilities. The Softmax Function and Attention Patterns The softmax function poses severe limitations on long context modeling. As sequences grow longer, attention scores become increasingly uniform, thus the model cannot focus effectively [168]. To mitigate this during length extrapolation, scaling coefficients are commonly adopted [401]. Another prominent phenomenon is the attention sink effect [580], where attention heads put heavy scores on the first token. The removal of these tokens from the KV cache can drastically impair model performance. Two solutions have been proposed to address this issue: 1. Introducing dedicated trainable Sink Token to hold excessive attention scores. 2. Implementing alternative attention mechanisms to softmax, such as SoftMax-One [363]. MLP Layers Voita et al. [516] finds that the activation of certain MLP neurons is highly correlated with the current tokens position. It also reveals that neurons could act as n-gram detectors, capturing local patterns. While the authors inspect only absolute positional embeddings, one could expect that there also exists neurons that activate strongly on certain relative positions. 57 Figure 11. Overview of applications of long context language models and techniques across various tasks. 7.2.3. Layer Interaction Modern LLMs are composed of stack of transformer layers each incorporating attention and MLP modules. Interestingly, researchers have discovered that alternating between full attention and linear-complexity mechanisms (such as window attention [597] or lightning attention [366]) across layers yield superior extrapolation performance. Regarding position embedding, Yang et al. [597] further discover that alternating between NoPE [241] and RoPE can get the best of both worlds, as NoPE offers advantages in information retrieval, while RoPE better models local information due to its built-in recency bias. 8. Application The strong multitasking capabilities enable LLMs to be adapted to wide range of applications. In real-world scenarios, tasks frequently require the processing of extensive contextual information. Consequently, long context LLMs significantly enhance the practical utility of LLMs in various domains, including GUI copilots and RAG-based systems, as illustrated in Figure 11. In this section, we briefly discuss the applications of long context LLMs and the associated techniques. 8.1. Application in Agent LLM-based agents complete tasks via iterative interactions with environments, predicting next steps based on previous interaction histories [572, 610, 682]. Since both the environment observations and the interaction trajectories can be very lengthy, long context capability is crucial for developing effective agents. Long context comprehension capabilities enable LLM agents to process complex agent tasks with extended observations. One example is GUI agent task [192], where the agents are required to comprehend the rich layout and text information [314, 582, 678]. Another example is software engineering agent [224]. The agents are asked to interact with repository to solve real-world coding tasks [543, 574, 601, 655]. Some agent tasks inherently require long-horizon reasoning and planning capabilities. For example, gaming agents must track the game state and plan actions over an extended horizon to progress or win [487]. Tasks like planning multi-day travel itineraries [47] or solving complex machine learning problems [43, 196] also necessitate longhorizon planning. Automated agent optimization [683, 691] also requires good long context understanding and generation capabilities so that the LLM-based agent optimizer can effectively reason with long-horizon agentic workflows and intermediate results and generate optimized prompts, tools, and agentic workflows. The long-context capabilities of large language models have facilitated the development of various agent applications. Open-source LLM agent development platforms, such as Dify [498] and Coze [497], enable users to easily orchestrate LLM apps ranging from simple agents to complex AI workflows. General agents, like the OpenAI Computer-Using Agent [505] and Manus [504], function as comprehensive AI assistants that handle real-world tasks by performing actions such as web browsing and coding within virtual PC environment. 8.2. Application in RAG Integrating long context technology into Retrieval-Augmented Generation (RAG) systems significantly enhances performance by enabling the processing of larger text chunks, retrieving more relevant information, and supporting the development of complex systems to address complex queries [225, 622]. For instance, LongRAG [222], which combines long retriever and long reader demonstrates notable improvements in document question answering. Similarly, studies [225, 455] show that models capable of retrieving longer text fragments significantly enhance answer relevance and completeness. Furthermore, Li et al. introduces Reasoning with Attributions which leverages long context models to improve multi-hop reasoning benchmarks by providing larger context window for complex prompts. These advancements highlight how long context models reduce traditional RAG systems reliance on external tools, offering robust end-to-end modeling approach that enhances system performance and robustness. Due to the aforementioned benefits, the rapid development of long context technology has spurred the growth of RAG systems, leading to popular applications like Perplexity [259] and Genspark [144], which focus on retrieval. Other systems, such as ChatGPT [481] and Deepseek [95], have also integrated retrieval capabilities. Notably, Deepsearch [435], developed by xAI, enhances information navigation through advanced context understanding and intent recognition. These innovations underscore the critical role of long context technology in modern retrieval systems. 8.3. Application in Chatbot Recent advancements in long context processing have significantly enhanced dialogue systems by enabling extended memory retention and contextual coherence. Long context windows allow chatbots to process extensive conversational histories, thereby improving interaction fluency and supporting long-term memory capabilities. This technological leap is foundational for applications requiring sustained user engagement, as evidenced by frameworks exploring prompt-based memorization [261], memory-augmented architectures [675], and context 59 extension techniques [540, 542]. Due to the aforementioned benefits, long context processing technology has given rise to diverse array of AI dialogue systems, demonstrating its broad value in practical applications. Major platforms like ChatGPT [385] and Pi [204] utilize persistent memory to maintain user preferences and conversation history, enabling personalized interactions. Specialized systems such as Character AI [46] and Talkie [6] further exemplify how long context windows support style-consistent dialogues and contextual continuity. These capabilities prove particularly valuable in longitudinal applications including educational tutoring, healthcare monitoring, and therapeutic counseling, where preserving the context over multiple conversations is critical [230]. 8.4. Application in Code The advancement of long context technology has significantly enhanced repository-level code tasks and software development agent tasks [220]. Such tasks often require processing complex and extensive contextual information, which previously constrained models ability to integrate comprehensive context. For instance, RepoCoder [634] employs similarity-based retrieval mechanism to enrich context and improve code completion quality. Similarly, the RLPG framework [461] integrates repository structures with relevant file contexts to generate repository-level prompts. These intricate system designs were necessitated by the limitations of earlier context windows. However, breakthroughs in long context technology have effectively alleviated these constraints, enabling the development of more sophisticated systems that enhance task execution precision and efficiency [460]. Recent models incorporating long context technologies, such as StarCoder2 [334], Qwen2.5Coder [202], and Granite Code Models [367], have demonstrated superior performance in long context code tasks. Unlike earlier approaches that relied on complex architectures to manage extended contexts, these models leverage robust long context comprehension capabilities, providing scalable and practical solutions for software development. This progress has streamlined tasks such as code completion, contextual understanding, and repository-level code generation. Due to these advancements, long context technologies have spurred the development of advanced AI-powered code applications, such as GitHub Copilot [146] and Anysphere Cursor [15]. These tools utilize long context models to enhance code completion, deliver contextual documentation, and enable predictive debugging. By maintaining coherent understanding of large codebases, they improve developer productivity, reduce manual effort, and support more intuitive workflows. This integration highlights the transformative impact of long context technology on modern software development. 8.5. Application in Traditional NLP Tasks The introduction of long context technology has led to transformative breakthroughs in numerous traditional NLP tasks by addressing the limitations of conventional methods, which were constrained by restricted context windows. This technological advancement has significantly enhanced the performance and practical applicability of various NLP applications. In this section, we briefly discuss three representative tasks that have particularly benefited from long context technology. Document Summarization Traditional document summarization methods often encounter challenges such as inconsistencies and limited contextual understanding [639, 652]. The development of long context models enables the processing of entire documents at once, facilitating more comprehensive understanding of the original content and improved identification of redundant information [150, 539, 630]. Models such as Longformer [25] and LongT5 [165] have demonstrated exceptional performance in document summarization tasks, thereby advancing the practical implementation of applications such as summarizing news articles and academic papers [227]. Information Retrieval Traditionally, most vector models were limited by window size, which required segmenting the data into blocks when performing long context semantic modeling, often resulting in the loss of coherent semantic information [530, 690]. However, with advancements in long context technology, semantic vector models can now accommodate longer text inputs, such as text-embedding-3-large [384], jina-embeddings-v2 [163], and BGE-M3 [55]. This enables the processing of longer texts such as chapters and documents, thereby significantly improving the practical usability of semantic vector models in real-world applications [437, 687]. Machine Translation Document translation is key research area in machine translation. Early approaches focused on modifying the transformer architecture to encode more contextual information [24, 361, 532, 636]. Long context models, however, can directly translate lengthy, complex documents by processing extended context windows, which improves the translation of polysemous words [181, 533]. This advancement significantly enhances the translation quality of long documents and makes it increasingly feasible for large language models to translate entire novels and books [342]. 8.6. Application in Multimodal Tasks Understanding extensive videos, large collections of images, audio streams, and lengthy text inputs is often essential in real-world scenarios. Therefore, multimodal long context modeling has become key area of research in long-text applications. This section provides an overview of relevant data and training strategies, models, and frameworks, as well as benchmarks for long context multimodal large language models (MLLMs). Data and Training Strategies To extend the context windows of MLLMs, many studies design the data curation pipeline to filter and synthetic long context multimodal training data. Typically, these datasets, unlike textual long context training data, are constructed by increasing the length of the multimodal part in the context, such as multi-image [617], long video [111, 289]. Harnessing this, some methods [171, 273, 541, 594, 644, 666] stimulate the capability of MLLMs to handle long texts by gradually increasing the length of these long context multimodal training data. After that, some work [284] utilizes the variants of preference optimization to improve the long context modeling of MLLMs. Alternatively, some work utilizes long context LLMs as the foundational model of MLLMs for training [211] to inherent long context modeling ability of LLMs. Model Structure and Framework To enhance the ability of MLLMs to understand multimodal long context input and reduce the computational costs in model training and inference, some work focuses on modifying position embeddings and introducing emergent modules in MLLMs for better inference efficiency and performance. Next, we will elaborate on the details of these techniques. Unlike text-only long context language modeling, to effectively model the 61 positional information of visual tokens in multimodal long context inputs, some studies [138, 475, 535] propose extending the dimensionality of the ROPE adopted by numerous MLLMs. The dimensional extension enables ROPE can improve the sequential modeling in multimodal tokens, the spatial modeling in images, and the temporal modeling between video frames, thereby improving the differentiating the positional information of visual tokens in multimodal long context inputs. In addition to enhancing the ability of multimodal models to capture positional information, some studies introduce specialized modules to further reduce the context window length occupied by visual tokens with MLP [535], pixel shuffle [70], Q-former-like module [9, 280, 612, 689], LoRA [343], which integrate effective important visual information into LLMs eliminating redundant information in visual representations. However, the aforementioned methods to reduce the length of visual tokens necessitate training MLLMs from scratch for adaptation. To compress the number of visual tokens to reduce inference costs for well-trained MLLMs, some approaches [170, 322, 511, 604] in training-free manner preserve important visual tokens by assessing their relevance with input text. Generally, for multimodal long context inputs, videos occupy the largest portion of the context window due to containing numerous frames. Therefore, some work utilizes tree structure [547], reward models [607], similar scores based on CLIP [300], trained video frame selector [171, 621], and aggregating methods [123] to select keyframes and decrease the length of video in context. Benchmarks After introducing the basic and advanced development of long context MLLMs in the community, in this part, we will review existing evaluation benchmarks for assessing the performance of multimodal long context modeling capabilities of MLLMs. Multimodal long context benchmarks can primarily be divided into two types: those that use multiple images and those that use long videos as the context. These benchmarks then require the model to answer questions based on the contextual information to evaluate the MLLMs capability of multimodal long context modeling. For benchmarks using multi-image context, some works [77, 347, 407] use large number of multimodal cases as context to evaluate the models many-shot incontext learning capabilities. Other works employ needle in haystack [524], temporal, and semantic multi-image tasks [465] as the primary evaluation methods. Additionally, some studies [221, 434, 449] not only assess the understanding of long multimodal contexts but also evaluate the multiple document comprehension capabilities. Compared to multi-image long context data, understanding long videos is more challenging due to their larger context length and the inclusion of more redundant and noisy information. Some work [44, 143, 677, 692] utilizes diverse long video and multiple challenging tasks to evaluate multimodal long context modeling of MLLMs. Other works [111, 376, 560] collect videos with rich events through both automated and manual methods and pose questions on the events. 8.7. Application in Specific Domains Long context technology has demonstrated significant potential across various domains by enabling more efficient processing of complex and lengthy information [150]. In news, it improves the generation of coherent summaries by aggregating data from multiple sources [134]. In the legal field, it simplifies the interpretation of large volumes of documents, helping professionals quickly extract key insights [238]. In healthcare, long context models enhance the synthesis of patient records and medical literature, thereby supporting informed decision-making [119]. Financial applications utilize these models to analyze extensive reports and derive insights into market trends [353, 381, 425]. In the field of biology, long context technology aids in understanding molecular structures and genomic sequences, fostering advancements in drug discovery and personalized medicine [182, 446]. Overall, long context models have significantly enhanced 62 the effectiveness and accuracy of information processing in these areas, making data synthesis and information extraction more efficient. 9. Future Directions 9.1. Long Context Modeling for o1-like Long Reasoning Most recently, o1-like long reasoning models have attracted significant attention due to their exceptional performance on complex reasoning tasks [56, 164, 201, 373, 412, 502, 657, 669]. This test-time scaling paradigm that first generates extended CoT reasoning before producing answers essentially equips models with the ability to perform trial-and-error, backtracking, correction, and iteration auto-regressively within the context window, unlocking significant performance improvements [275, 293, 525]. Despite its promise, current practices leveraging LongCoT remain far from satisfactory, primarily due to two critical drawbacks: the rough quality of generated CoTs severely impacts reasoning efficiency, containing substantial redundancy and irrelevant information [63, 191, 345, 576, 640]; and the difficulty in scaling CoT length significantly constrains the potential for further performance gains, as numerous studies have observed performance degradation in particularly long reasoning chains [570, 605, 632]. Addressing these challenges requires advances in long context language models, which are essential for two key aspects: reliable evaluation of long reasoning processes (currently significant challenge for existing Process Reward Models [177]), and developing robust long-form generation capabilities for producing longer and more reliable reasoning chains. Furthermore, tailoring efficiency-oriented techniques developed for general long context scenarios, such as KV cache compression and prompt compression, to the specific needs of long reasoning models presents promising direction for future research. 9.2. Further Extending Context Window and Improving Modeling Capabilities Context length stands as one of the most fundamental attributes of language models. Its evolution closely tracks the development of increasingly sophisticated modeling capabilities - from basic n-gram models constrained to mere tokens of context [36], to BERT-style architectures processing hundreds of tokens for paragraph-level understanding [101], to early iterations of ChatGPT managing 2k-4k tokens for general-purpose dialogue, and now to unprecedented scales of 100k to 1M tokens which has enabled sophisticated capabilities including long chainof-thought reasoning [164], long in-context learning [499], long video processing [507], and supporting complex agent systems with extensive interaction histories [506]. This progression demonstrates how expanding context length has consistently unlocked new frontiers in language modeling, enabling increasingly sophisticated applications and use cases. Therefore, extending context windows to even greater scales holds the promise of unleashing more advanced capabilities and transformative applications. On the other hand, existing research has demonstrated significant gap between models claimed supporting context length and their effective context length [12, 189]. Consequently, enhancing models long context modeling capabilities within their supported context lengths remains crucial. Regarding the expansion of context windows and improvement of long context modeling capabilities within supported context windows, we have identified several promising directions: Long Context Reinforcement Learning Despite its immense potential, reinforcement learning remains underexplored in long context scenarios. While combining LongCoT with RL has 63 demonstrated significant improvements for tasks with definitive answers, many long context applications still face fundamental challenges as they require referring to lengthy inputs. One example is alignment scenarios, where long context RLHF practiceswhether employing long context models as reward models (e.g., GLM-Long [147]) or utilizing long context preference data (e.g., LLaMA-3.1 [154])have yet to yield substantial benefits. The core challenge of long context RL lies in developing reward models capable of effectively evaluating extensive inputs, such as lengthy reasoning chains, narrations, and dialogues. Oriented toward this goal, collecting long context preference data, either through human annotation or carefully designed synthesis protocols, merits further investigation. Recipe for Collecting, Filtering and Synthesizing High-Quality Training Data The advancement in long context modeling capabilities is fundamentally data-driven. The data recipe for training long context models presents several promising directions for future exploration: (1) developing fine-grained filtering strategies beyond heuristics to identify training data with long-range dependencies; (2) synthesizing challenging queries that require integrating key information dispersed throughout the text, along with their correct answers; (3) exploring task types particularly suited for long context RL training that best generalize to other long context tasks; and (4) optimizing the distribution of domains and sequence lengths to achieve Pareto efficiency between computational cost and model performance. Addressing these challenges will be essential for developing more efficient and effective data recipes for future LCLMs. Long Context Distillation Model distillation is common strategy to enhance the capabilities of smaller models by leveraging larger, more powerful models. The de facto approach involves generating responses from strong model for set of queries, then fine-tuning the smaller model on these question-answer pairs. This method has proven effective in various aspects, including improving human preference ratings for model responses and enhancing models chain-of-thought reasoning abilities. Within the long context domain, an interesting research question emerges: how can we utilize large models with strong long context modeling capabilities to improve models with weaker long context abilities? Beyond generating high-quality responses [19], an intriguing observation is that powerful LCLMs can effectively identify and filter training data that exhibits strong long-range dependencies, with more capable models demonstrating superior filtering efficacy [561]. Given that LCLM training is complex process spanning both pre-training and post-training stages, we believe that models with strong long context modeling capabilities can play more significant role in the LCLM training process, improving both training efficiency and resulting model performance. Performance-Oriented Architecture Design Architectural design remains an ongoing research topic in long context modeling. Beyond improving training and deployment efficiency, architectural modifications such as enhanced positional encodings and hybrid attention mechanisms can lead to better extrapolation capabilities and improved modeling within supported context lengths, as discussed in Section 3. We anticipate continued advances in this direction to further enhance LCLMs modeling capabilities. Optimizing Long-Form Generation Current research on LCLMs predominantly focuses on processing long inputs, such as information retrieval from extensive documents or concise summary generation. By contrast, long-form generation remains relatively unexplored, despite its promising applications in areas such as long-form narrative creation, repository-scale code 64 generation, and long CoT. Advancing long-form generation capabilities presents two significant challenges: First, beyond context comprehension, the automated generation of high-quality long-form content demands sophisticated output planning, requiring higher level of language modeling capabilities. Second, the evaluation of generated long-form content poses substantial difficulties - automated metrics like ROUGE demonstrate limited reliability for lengthy texts, while manual assessment proves prohibitively time-consuming. Consequently, future advances in long-form generation may need to develop in tandem with improvements in automated evaluation methodologies for long-form content. Domain-Specific Enhancement As mentioned above, the expansion of context windows has unlocked diverse application domains for language models, ranging from text-intensive fields such as legal and medical domains to multimodal scenarios where vision-language models process extensive visual content, and to intelligent systems maintaining rich interaction histories. While preserving robust general-purpose long context modeling capabilities remains essential, investigating targeted optimizations for these domain-specific challenges presents promising research direction. 9.3. Efficient Architecture Design, Training, and Deployment of LCLMs Model Architecture The inherent limitations of Transformer models become pronounced in long context scenarios, primarily due to the substantial KV-cache memory overhead. The memory significantly exceeds the High Bandwidth Memory (HBM) capacity of contemporary GPUs. Consequently, the exploration of memory-efficient KV-cache architectures, such as the investigation of linear attention mechanisms, emerges as critical directions for the architectural advancement of Long Context Language Models. Training and Inference Frameworks To mitigate training complexities, the implementation of refined partitioning strategies [409] and local recomputation techniques [248] assumes paramount importance. These methodologies which balance bandwidth overhead with computation time are crucial to effectively address the growing divergence between increasingly complex model architectures and the continuously evolving capabilities of GPU. For inference frameworks, the strategic deployment of operator fusion and the rearrangement of computational sequences to curtail the generation of extensive intermediate matrices remains salient avenue for investigation [92, 93, 442]. Despite prevailing precision norms anchored at 16-bit, state-of-the-art hardware architectures already accommodate high-throughput computation at reduced precisions, down to 8-bit [382]. The application of 8-bit quantization to selectively chosen modules within models, applicable to both training and inference paradigms, has demonstrated considerable promise [308]. Prospective research endeavors focused on exploring even lower precision quantization levels, and the expanded application of quantization across broader spectrum of model modules, are anticipated to yield substantial performance enhancements. Recently, the advent of long context Reinforcement Learning (RL) training [164] introduces novel challenges to existing frameworks, necessitating targeted optimizations to bolster training efficiency and stability. Customized Hardware Design The rapid improvement in hardware computational speed has surpassed the corresponding gains in bandwidth, thereby exacerbating bandwidth bottlenecks, particularly during the decoding phase [145]. While existing Prefill-Decoding disaggregated architectures facilitate the independent optimization of prefill and decoding stages, there remains 65 notable absence of specialized hardware designed for decoding. Decoding processes are inherently characterized by substantial demands for GPU with more HBM capacity and higher bandwidth, while exhibiting comparatively lower computational intensity. Therefore, the development of dedicated hardware specifically engineered for decoding, characterized by enhanced HBM capacity and bandwidth, constitutes crucial direction for future hardware evolution in this domain. 9.4. More Reliable Evaluation Framework for Long Context Language Modeling As discussed in Section 6, the capabilities of long context language modeling can be categorized into two main aspects: long context comprehension and long-form generation. Consequently, future research directions naturally align with these two fundamental tracks. Towards Real-World and Scenario-Specific Long Context Comprehension Regarding long context comprehension, given that numerous synthetic and real-world benchmarks have emerged to evaluate various aspects of this capability, future research can be directed towards more specific real-world applications. This includes mining user logs to understand authentic long context usage patterns, assessing LCLM performance across specialized domains (such as legal, medical, and financial sectors), and evaluating its effectiveness in different academic disciplines (from social sciences to natural sciences), etc. Notably, the extensive input lengths pose significant challenges for obtaining high-quality human annotations. Therefore, developing efficient annotation frameworks specifically designed for long context scenarios presents another valuable research direction. Coarse-to-Fine Evaluation of Long-Form Generation In the field of long-form generation, human evaluation is frequently used, mainly because automatic metrics struggle to effectively evaluate long responses and are less suited to open-ended generation tasks. Human evaluation and automatic metrics are two extremes: automatic metrics are highly efficient but lack precision, while human evaluation offers high accuracy but is significantly resource-intensive. How can we combine the advantages of both? By observing the trends of evaluation methods for long-form generation, we can notice transition from combination of automatic metrics and human evaluation toward the adoption of LLM-as-a-Judge. Nonetheless, fully end-to-end evaluation of long responses remains challenge for current LLMs. To address this, an evaluation workflow can be designed, decomposing the entire evaluation into granular aspects, following systematic coarse-to-fine adaptation. For simpler evaluation aspects, LLMs combined with prompt engineering can serve as alternatives to human evaluators. For components where LLMs struggle to evaluate, further decomposition of the evaluation workflow or adopting proxy-based methodologies can be explored. This systematic breakdown allows us to preserve high evaluation accuracy while enhancing efficiency, ultimately demystifying the challenges tied to long-form generation evaluation. Currently, ProxyQA [486], HelloBench [418], and LongFact [549] reflect this trend, but we believe there is still potential for optimizing these systems further to achieve even higher accuracy. 9.5. Towards Mechanistical Interpretability for Long Context Modeling Mechanistic Interpretability (MI) [378] aims to reverse engineer models at module level. In MI literature [357, 526, 650], researchers typically pinpoint sparse set of attention heads and MLPs that are responsible for some downstream task. One exemplar of MI for long context modeling 66 is the identification of retrieval heads [565] that are tied to long-range dependencies. MI for long context modeling is an exciting intersection for both worlds. We point out several promising research questions to answer. Mechanistic Understanding of Position Embeddings How do LLMs make use of positional embeddings? Voita et al. [516] showcases that the activation weights of certain neurons are highly correlated with the current tokens position, yet they analyze only absolute positional embeddings. Future works could shed light on whether the same holds for relative positional embeddings, and on the interaction between MLPs and attention modules when processing positional information. Identifying Causes of Long Context Issues We advocate problem-driven approach to MI for long context modeling. For example, Liu et al. [317] points out that models have positional bias for processing information, favoring tokens at the start or at the end. We take one step further and ask: What modules of the model lead to such bias? Based on possible discoveries, can we devise method to mitigate it mechanistically? Interpretability-Driven Enhancements Closely related is the problem of length extrapolation. Zhou et al. [685] demonstrates that LLMs fail to align digits when performing addition on numbers longer than those in the training distribution. Similarly, we ask: What modules of the model are responsible for this failure? Can we build on the findings to improve length extrapolation? At high level, MI provides useful toolchains to understand the inner workings of LLMs and to pinpoint problematic components. On the other hand, long context modeling is challenging field with its unique interests and huge application space. MI for long context has huge potential for efficiently identifying and addressing issues in long context modeling. 10. Conclusion In this paper, we provide comprehensive survey on the recent advances on long context modeling for large language models. Specifically, we first discuss the long context data strategy including data filtering, data mixture and data construction strategies for both pre-training and post-training stages. Then, we provide comprehensive discussion of the recent advancement on model including Transformer-based LLM architectures and position embeddings, which aim at enhancing the long context capabilities of LLMs. After that, we provide the workflowbased LCLM methods, such as prompt compression and agent-based methods, to enhance the long context abilities. Besides, we discuss the long context evaluation benchmarks including understanding and generation abilities. Moreover, we provide the AI infrastructure method including training and inference strategies optimized for long context modeling. In addition, we also provide detailed analysis for better interpretability of LCLMs. Furthermore, we summarize existing applications (e.g., agent, RAG, Code) on long context modeling, and discuss the remaining issues or challenges for future directions. Finally, we hope our survey could guide the developers to better understand the related techniques for LCLMs and facilitate the growth of foundation models. 67 11. Contributions and Acknowledgments Project Leaders Jiaheng Liu, Nanjing University, Institute of Automation, Chinese Academy of Sciences, M-A-P, Organize the whole project Dawei Zhu, Peking University, Organize the whole project Core Contributors (Alphabet Order) Zhiqi Bai, Alibaba Group, Architecture Yancheng He, Alibaba Group, Application Huanxuan Liao, Institute of Automation, Chinese Academy of Sciences, Data Haoran Que, M-A-P, Evaluation Zekun Wang, Kuaishou Technology, Workflow Design Chenchen Zhang, Tencent, Analysis Ge Zhang, ByteDance, M-A-P, Architecture & Evaluation Jiebin Zhang, Peking University, Infrastructure Yuanxing Zhang, Kuaishou Technology, Infrastructure Contributors (Alphabet Order) Zhuo Chen, Alibaba Group, Architecture Hangyu Guo, M-A-P, Application Shilong Li, Alibaba Group, Application Ziqiang Liu, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Workflow Design Yong Shan, ByteDance, Architecture Yifan Song, Peking University, Application Jiayi Tian, Alibaba Group, Data Wenhao Wu, Peking University, Future Directions Zhejian Zhou, University of Southern California, Analysis Ruijie Zhu, UC Santa Cruz, Architecture Sponsor Committee (Alphabet Order) Junlan Feng, China Mobile Research Institute Yang Gao, Nanjing University Shizhu He, Institute of Automation, Chinese Academy of Sciences Zhoujun Li, AiStrong Tianyu Liu, M-A-P Fanyu Meng, China Mobile Research Institute Wenbo Su, Alibaba Group Yingshui Tan, Alibaba Group Zili Wang, Independent Researcher Jian Yang, Alibaba Group Wei Ye, Peking University Bo Zheng, Alibaba Group Wangchunshu Zhou, OPPO, M-A-P 68 Corresponding Authors Jiaheng Liu, Nanjing University, Institute of Automation, Chinese Academy of Sciences, M-A-P Dawei Zhu, Peking University Wenhao Huang, ByteDance, M-A-P Sujian Li, Peking University Zhaoxiang Zhang, Institute of Automation, Chinese Academy of Sciences"
        },
        {
            "title": "References",
            "content": "1 Ntk-alibi: Long text extrapolation of alibi position encoding through interpolation, August 2023. URL https://github.com/keezen/ntk_alibi. 2 Long-data-collections. https://huggingface.co/datasets/togethercomputer/L ong-Data-Collections, 2023. 3 Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. CoRR, abs/2303.09540, 2023. doi: 10.48550/ARXIV.2303.09540. URL https://doi.org/10.48550/arXiv.230 3.09540. 4 Lisa Adams, Felix Busch, Tianyu Han, Jean-Baptiste Excoffier, Matthieu Ortala, Alexander L√∂ser, Hugo JWL Aerts, Jakob Nikolas Kather, Daniel Truhn, and Keno Bressem. Longhealth: question answering benchmark with long clinical documents. arXiv preprint arXiv:2401.14490, 2024. 5 Ameeta Agrawal, Andy Dang, Sina Bagheri Nezhad, Rhitabrat Pokharel, and Russell Scheinberg. Evaluating multilingual long-context models for retrieval and reasoning. In Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024), pages 216231, 2024. 6 Talkie Ai. Talkie ai-native character community, 2024. URL https://www.talkie-ai. com/. 7 Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr√≥n, and Sumit Sanghai. GQA: training generalized multi-query transformer models from multihead checkpoints. In EMNLP, pages 48954901. Association for Computational Linguistics, 2023. 8 Faisal Al-Khateeb, Nolan Dey, Daria Soboleva, and Joel Hestness. Position interpolation improves alibi extrapolation. arXiv preprint arXiv:2310.13017, 2023. 9 Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar√©n Simonyan. Flamingo: visual language model for few-shot learning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL 69 http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177 ccccbb411a7d800-Abstract-Conference.html. 10 Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023. 11 Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong. Training-free long-context scaling of large language models. arXiv preprint arXiv:2402.17463, 2024. 12 Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng Kong. Why does the effective context length of llms fall short? arXiv preprint arXiv:2410.18745, 2024. 13 Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Make your LLM fully utilize the context. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=YGTV EmBXtV. 14 Anthropic. Introducing contextual retrieval. https://www.anthropic.com/news/con textual-retrieval. 15 Anysphere. Cursor - the ai code editor. https://www.cursor.com/en, 2025. URL https://www.cursor.com. 16 Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher R√©. Simple linear attention language models balance the recall-throughput tradeoff, 2024. URL https://arxiv.org/abs/24 02.18668. 17 Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. 18 Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. Longalign: recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058, 2024. 19 Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign: recipe for long context alignment of large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 13761395, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.74. URL https://aclanthology.org/2024.findings-emnlp.74. 20 Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. 21 Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. 22 Zhihao Bai, Zhen Zhang, Yibo Zhu, and Xin Jin. {PipeSwitch}: Fast pipelined context switching for deep learning applications. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 499514, 2020. 23 Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. 24 Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, and Weihua Luo. G-transformer for document-level machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 34423455. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.ACL-LONG.267. URL https://doi.org/10.18653/v1/2021.acl-long.267. 25 Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150. 26 Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. URL https://arxiv.org/abs/2004.05150. 27 Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba, 2024. URL https://arxiv.org/abs/2406.14528. 28 Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024. 29 bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., 2023. URL https: //www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_ allows_llama_models_to_have/. 30 Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, et al. Long code arena: set of benchmarks for long-context code models. arXiv preprint arXiv:2406.11612, 2024. 31 Sebastian Borgeaud, A. Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, G. Irving, O. Vinyals, Simon Osindero, K. Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. International Conference on Machine Learning, 2021. 32 Antoine Bosselut, Asli Celikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, and Yejin Choi. Discourse-aware neural rewards for coherent text generation. arXiv preprint arXiv:1805.03766, 2018. 71 33 Aleksandar Botev, Soham De, Samuel L. Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, L√©onard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz GUStavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur Zucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Cl√©ment Farabet, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee Whye Teh, and Nando de Frietas. Recurrentgemma: Moving past transformers for efficient open language models. CoRR, abs/2404.07839, 2024. 34 Felix Brakel, Uraz Odyurt, and Ana-Lucia Varbanescu. Model parallelism on distributed infrastructure: literature review from theory to llm case-studies. arXiv preprint arXiv:2403.03699, 2024. 35 William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, and Jonathan Ragan-Kelley. Striped attention: Faster ring attention for causal transformers. arXiv preprint arXiv:2311.09431, 2023. 36 Thorsten Brants, Ashok Popat, Peng Xu, Franz Josef Och, and Jeffrey Dean. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858867, 2007. 37 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. 38 Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to 1m tokens and beyond with RMT. CoRR, abs/2304.11062, 2023. 39 Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. In Proceedings of the 41st International Conference on Machine Learning, pages 52095235, 2024. 40 Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, and Wen Xiao. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling, 2024. URL https://arxiv.org/abs/2406 .02069. 41 Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. Instruction mining: Instruction data selection for tuning large language models. In First Conference on Language Modeling, 2024. 42 Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, et al. Mceval: Massively multilingual code evaluation. arXiv preprint arXiv:2406.07436, 2024. 43 Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander Madry. Mle-bench: Evaluating machine learning agents on machine learning engineering. CoRR, abs/2410.07095, 2024. doi: 10.48550/ARXIV.2410.07095. URL https://doi.org/ 10.48550/arXiv.2410.07095. 44 Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Crist√≥bal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: 1-hour video-language understanding. CoRR, abs/2411.04998, 2024. doi: 10.48550/ARXIV .2411.04998. URL https://doi.org/10.48550/arXiv.2411.04998. 45 Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, et al. Flux: fast software-based communication overlap on gpus through kernel fusion. arXiv preprint arXiv:2406.06858, 2024. 46 Character AI. Character ai. Retrieved September 14, 2023 from https://character.ai/, 2023. URL https://character.ai/. 47 Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, and Jiangjie Chen. Travelagent: An AI assistant for personalized travel planning. CoRR, abs/2409.08069, 2024. doi: 10.48550/ARX IV.2409.08069. URL https://doi.org/10.48550/arXiv.2409.08069. 48 Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain questions. In Association for Computational Linguistics (ACL), 2017. 49 Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. Clex: Continuous length extrapolation for large language models. arXiv preprint arXiv:2310.16450, 2023. 50 Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. Clex: Continuous length extrapolation for large language models. In The Twelfth International Conference on Learning Representations, 2024. 51 Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv: 2310.05029, 2023. 52 Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, and Beidi Chen. Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding. arXiv preprint arXiv:2408.11049, 2024. 53 Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, and Junwei Liang. Fintextqa: dataset for long-form financial question answering. arXiv preprint arXiv:2405.09980, 2024. 54 Jianghao Chen, Junhong Wu, Yangyifan Xu, and Jiajun Zhang. Ladm: Long-context training data selection with attention-based dependency measurement for llms. 2025. URL https://api.semanticscholar.org/CorpusID:276768135. 73 55 Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. BGE m3embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. CoRR, abs/2402.03216, 2024. doi: 10.48550/ARXIV.2402.03216. URL https://doi.org/10.48550/arXiv.2402.03216. 56 Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Dee p-Agent/R1-V, 2025. Accessed: 2025-02-02. 57 Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024. 58 Longze Chen, Ziqiang Liu, Wanwei He, Yinhe Zheng, Hao Sun, Yunshui Li, Run Luo, and Min Yang. Long context is not long at all: prospector of long-dependency data for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 82228234, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.447. URL https://aclanthology.org/2 024.acl-long.447. 59 Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86028615, 2022. 60 Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. 61 Stanley Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling. Computer Speech & Language, 13(4):359394, 1999. 62 Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin Van Durme, Luke Zettlemoyer, Jianfeng Gao, and Hao Cheng. Generative adapter: Contextualizing language models in parameters with single forward pass. arXiv preprint arXiv: 2411.05877, 2024. 63 Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. 64 Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, and Maosong Sun. Stuffed mamba: State collapse and state capacity of rnn-based long-context modeling. arXiv preprint arXiv:2410.07145, 2024. 65 Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, and Maosong Sun. Stuffed mamba: State collapse and state capacity of rnn-based long-context modeling, 2024. URL https://arxiv.org/abs/2410.07145. 66 Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, and Rui Yan. Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use. arXiv preprint arXiv:2312.04455, 2023. 67 Yuhan Chen, Ang Lv, Jian Luan, Bin Wang, and Wei Liu. Hope: novel positional encoding without long-term decay for enhanced context awareness and extrapolation. arXiv preprint arXiv:2410.21216, 2024. 68 Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. 69 Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The International Conference on Learning Representations (ICLR), 2024. 70 Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. CoRR, abs/2404.16821, 2024. doi: 10.48550/ARXIV.2404.16821. URL https://doi.org/10.48550/arXiv.2404.16821. 71 Zhi Chen, Qiguang Chen, Libo Qin, Qipeng Guo, Haijun Lv, Yicheng Zou, Wanxiang Che, Hang Yan, Kai Chen, and Dahua Lin. What are the essential factors in crafting effective long context multi-hop instruction datasets? insights and best practices. ArXiv, abs/2409.01893, 2024. 72 Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. xRAG: Extreme context compression for retrieval-augmented generation with one token. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=6pTlXqrO0p. 73 Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 38293846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.232. URL https://aclanthology.org/2023.emnlp-m ain.232/. 74 Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:83868399, 2022. 75 Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. arXiv preprint arXiv:2212.10356, 2022. 76 Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, and Peter Ramadge. Latent positional information is in the self-attention variance of transformer language models without positional embeddings. arXiv preprint arXiv:2305.13571, 2023. 77 Yew Ken Chia, Liying Cheng, Hou Pong Chan, Chaoqun Liu, Maojia Song, Sharifah Mahani Aljunied, Soujanya Poria, and Lidong Bing. M-longdoc: benchmark for multimodal super-long document understanding and retrieval-aware tuning framework. CoRR, abs/2411.06176, 2024. doi: 10.48550/ARXIV.2411.06176. URL https://doi.org/10.485 50/arXiv.2411.06176. 78 Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. URL https://arxiv.org/abs/1904.10509. 79 Sai Sena Chinnakonduru and Astarag Mohapatra. Weighted grouped query attention in transformers, 2024. URL https://arxiv.org/abs/2407.10855. 80 Woon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun Li, Michel Galley, Chris Brockett, Mengdi Wang, and Jianfeng Gao. Towards coherent and cohesive long-form text generation. arXiv preprint arXiv:1811.00511, 2018. 81 Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam√°s Sarl√≥s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In ICLR. OpenReview.net, 2021. 82 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sashank Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: scaling language modeling with pathways. J. Mach. Learn. Res., 24 (1), March 2024. ISSN 1532-4435. 83 Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Hu. Learning to compress prompt in natural language formats. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 77567767, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.429. URL https://aclanthology.org/2 024.naacl-long.429. 84 Igor Churin, Murat Apishev, Maria Tikhonova, Denis Shevelev, Aydar Bulatov, Yuri Kuratov, Sergej Averkiev, and Alena Fenogenova. Long input benchmark for russian analysis. arXiv preprint arXiv:2408.02439, 2024. 85 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 86 Cohere and Cohere For AI. C4ai command r7b: 7 billion parameter multilingual model, December 2024. URL https://huggingface.co/CohereForAI/c4ai-command-r7b -12-2024. Access requires agreement to the License Agreement and adherence to C4AIs Acceptable Use Policy. 76 87 Marta Costa-juss√†, Pierre Andrews, Mariano Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo S√°nchez, Holger Schwenk, et al. Lcfo: Long context and long form output dataset and benchmarking. arXiv preprint arXiv:2412.08268, 2024. 88 Hui Dai, Dan Pechi, Xinyi Yang, Garvit Banga, and Raghav Mantri. Deniahl: In-context features influence llm needle-in-a-haystack abilities. arXiv preprint arXiv:2411.19360, 2024. 89 Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, and Shuming Shi. Corm: Cache optimization with recent message for large language model inference, 2024. URL https://arxiv.org/abs/2404.15949. 90 Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. In ACL (1), pages 29782988. Association for Computational Linguistics, 2019. 91 Bhavana Dalvi Mishra, Oyvind Tafjord, and Peter Clark. Towards teachable reasoning systems: Using dynamic memory of user feedback for continual system improvement. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 94659480, Abu Dhabi, United Arab Emirates, dec 2022. Association for Computational Linguistics. doi: 10.18653/v1/20 22.emnlp-main.644. URL https://aclanthology.org/2022.emnlp-main.644. 92 Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. 93 Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. 94 Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 45994610, 2021. 95 DeepSeek. Deepseek official website, 2023. URL https://www.deepseek.com/. 96 DeepSeek. 3fs: Fire-flyer file system. https://github.com/deepseek-ai/3FS, 2025. 97 DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412 .19437. 98 Mostafa Dehghani, Stephan Gouws, O. Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. International Conference on Learning Representations, 2018. 99 Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. 100 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. 101 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy 77 Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclantholo gy.org/N19-1423/. 102 Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. 103 Jianbo Dong, Zheng Cao, Tao Zhang, Jianxi Ye, Shaochuang Wang, Fei Feng, Li Zhao, Xiaoyong Liu, Liuyihan Song, Liwei Peng, et al. Eflops: Algorithm and system co-design for high performance distributed training platform. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 610622. IEEE, 2020. 104 Shichen Dong, Wen Cheng, Jiayu Qin, and Wei Wang. Qaq: Quality adaptive quantization for llm kv cache, 2024. URL https://arxiv.org/abs/2403.04643. 105 Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. Hymba: hybrid-head architecture for small language models. arXiv preprint arXiv:2411.13676, 2024. 106 Zican Dong, Tianyi Tang, Junyi Li, and Wayne Xin Zhao. survey on long text modeling with transformers. CoRR, abs/2302.14502, 2023. doi: 10.48550/ARXIV.2302.14502. URL https://doi.org/10.48550/arXiv.2302.14502. 107 Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. Bamboo: comprehensive benchmark for evaluating long text modeling capacities of large language models. arXiv preprint arXiv:2309.13345, 2023. 108 Longxu Dou, Qian Liu, Fan Zhou, Changyu Chen, Zili Wang, Ziqi Jin, Zichen Liu, Tongyao Zhu, Cunxiao Du, Penghui Yang, Haonan Wang, Jiaheng Liu, Yongchi Zhao, Xiachong Feng, Xin Mao, Man Tsung Yeung, Kunat Pipatanakul, Fajri Koto, Min Si Thu, Hynek Kydl√≠Àácek, Zeyi Liu, Qunshu Lin, Sittipong Sripaisarnmongkol, Kridtaphad Sae-Khow, Nirattisai Thongchim, Taechawat Konkaew, Narong Borijindargoon, Anh Dao, Matichon Maneegard, Phakphum Artkaew, Zheng-Xin Yong, Quan Nguyen, Wannaphong Phatthiyaphaibun, Hoang H. Tran, Mike Zhang, Shiqi Chen, Tianyu Pang, Chao Du, Xinyi Wan, Wei Lu, and Min Lin. Sailor2: Sailing in south-east asia with inclusive multilingual llm. arXiv preprint arXiv:2502.12982, 2025. 109 Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Z. Chen, and Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, 2021. 110 Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling of language 78 models with mixture-of-experts. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv√°ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 55475569. PMLR, 2022. URL https://proceedings.mlr.pr ess/v162/du22c.html. 111 Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, and Ji-Rong Wen. Towards event-oriented long video understanding. CoRR, abs/2406.14129, 2024. doi: 10.48550/ARXIV.2406.14129. URL https://doi.org/ 10.48550/arXiv.2406.14129. 112 Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320335, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/20 22.acl-long.26. URL https://aclanthology.org/2022.acl-long.26/. 113 Jiangfei Duan, Shuo Zhang, Zerui Wang, Lijuan Jiang, Wenwen Qu, Qinghao Hu, Guoteng Wang, Qizhen Weng, Hang Yan, Xingcheng Zhang, et al. Efficient training of large language models on distributed infrastructures: survey. arXiv preprint arXiv:2407.20018, 2024. 114 Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng ZHANG, and Dahua Lin. Skvq: Sliding-window key and value cache quantization for large language models. In First Conference on Language Modeling. 115 Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed Aly, Beidi Chen, and Carole-Jean Wu. LayerSkip: Enabling early exit inference and selfspeculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), August 2024. 116 Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. Qafacteval: Improved qa-based factual consistency evaluation for summarization. arXiv preprint arXiv:2112.08542, 2021. 117 Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 10741084, 2019. 118 Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form question answering. arXiv preprint arXiv:1907.09190, 2019. 119 Yongqi Fan, Hongli Sun, Kui Xue, Xiaofan Zhang, Shaoting Zhang, and Tong Ruan. Medodyssey: medical domain benchmark for long context evaluation up to 200k tokens. CoRR, abs/2406.15019, 2024. doi: 10.48550/ARXIV.2406.15019. URL https://doi.org/10.48550/arXiv.2406.15019. 120 Yongqi Fan, Hongli Sun, Kui Xue, Xiaofan Zhang, Shaoting Zhang, and Tong Ruan. Medodyssey: medical domain benchmark for long context evaluation up to 200k tokens. arXiv preprint arXiv:2406.15019, 2024. 79 121 Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. What is wrong with perplexity for long-context language modeling? arXiv preprint arXiv:2410.23771, 2024. 122 Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, and Yiqun Liu. Scaling laws for dense retrieval. arXiv preprint arXiv: 2403.18684, 2024. 123 Gueter Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, Hung-Ting Su, Shang-Hong Lai, and Winston H. Hsu. Hermes: temporal-coherent long-form understanding with episodes and semantics. CoRR, abs/2408.17443, 2024. doi: 10.48550/ARXIV.2408.17443. URL https://doi.org/10.48550/arXiv.2408.17443. 124 Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and S. Kevin Zhou. Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference, 2025. URL https://arxiv.org/abs/2407.11550. 125 Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, and Mahyar Najibi. Lazyllm: Dynamic token pruning for efficient long context llm inference, 2024. URL https://arxiv.org/abs/2407.14057. 126 Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Moa: Mixture of sparse attention for automatic large language model compression, 2024. URL https://arxiv.org/abs/2406.14909. 127 Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hanna Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. ArXiv, abs/2402.10171, 2024. 128 Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning, 2024. URL https://arxiv.org/abs/2410.19258. 129 Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads matter: head-level KV cache compression method with integrated retrieval and reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=FJFVmeXusW. 130 Chaochen Gao, Xing Wu, Qingfang Fu, and Songlin Hu. Quest: Query-centric data synthesis approach for long-context scaling of large language model. ArXiv, abs/2405.19846, 2024. 131 Jun Gao, Ziqiang Cao, and Wenjie Li. Unifying demonstration selection and compression for in-context learning, 2024. URL https://arxiv.org/abs/2405.17062. 132 Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027, 2020. 133 Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv: 2212.10496, 2022. 134 Shen Gao, Xiuying Chen, Piji Li, Zhaochun Ren, Lidong Bing, Dongyan Zhao, and Rui Yan. Abstractive text summarization by incorporating reader comments. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications 80 of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 63996406. AAAI Press, 2019. doi: 10.1609/AAAI.V33I01.33016399. URL https://doi.org/10.1609/aaai.v33i01.33016399. 135 Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024. 136 Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. Seerattention: Learning intrinsic sparse attention in your llms, 2024. URL https://arxiv.org/abs/2410.13276. 137 Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, and Ge Zhang. Longins: challenging long-context instruction-based exam for llms. arXiv preprint arXiv:2406.17588, 2024. 138 Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu. V2pe: Improving multimodal long-context capability of vision-language models with variable visual position encoding. CoRR, abs/2412.09616, 2024. doi: 10.48550/ARXIV.2412.09616. URL https://doi.org/10.48550/arXiv.2412.09616. 139 Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model In The Twelfth tells you what to discard: Adaptive KV cache compression for LLMs. International Conference on Learning Representations, 2024. URL https://openreview.net /forum?id=uNrFpDPMyo. 140 Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=uREj4ZuG JE. 141 Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Mahong Xia, Zhang Li, Boxing Chen, Hao Yang, Bei Li, Tong Xiao, and JingBo Zhu. Clustering and ranking: Diversity-preserved instruction selection through expert-aligned quality estimation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 464478, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v 1/2024.emnlp-main.28. URL https://aclanthology.org/2024.emnlp-main.28. 142 Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 12431252. PMLR, 2017. 143 Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, and Feng Zheng. Longvale: Vision-audio-language-event benchmark towards time-aware omni-modal perception of long videos. CoRR, abs/2411.19772, 2024. doi: 10.48550/ARXIV.2411.19772. URL https://doi.org/10.48550/arXiv.2411.19772. 144 genspark. genspark.ai. URL https://www.genspark.ai/. 145 Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. IEEE Micro, 2024. 146 GitHub. Github copilot, 2022. URL https://github.com/copilot. 81 147 Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. 148 Paolo Glorioso, Quentin Anthony, Yury Tokpanov, Anna Golubeva, Vasudev Shyam, James Whittington, Jonathan Pilault, and Beren Millidge. The zamba2 suite: Technical report. arXiv preprint arXiv:2411.15242, 2024. 149 Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b ssm hybrid model. arXiv preprint arXiv:2405.16712, 2024. 150 Aditi S. Godbole, Jabin Geevarghese George, and Smita Shandilya. Leveraging long-context large language models for multi-document understanding and summarization in enterprise applications. CoRR, abs/2409.18454, 2024. doi: 10.48550/ARXIV.2409.18454. URL https://doi.org/10.48550/arXiv.2409.18454. 151 Omer Goldman, Alon Jacovi, Aviv Slobodkin, Aviya Maimon, Ido Dagan, and Reut Tsarfaty. Is it really long context if all you need is retrieval? towards genuinely difficult long context nlp. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1657616586, 2024. 152 Daniel Goldstein, Fares Obeid, Eric Alcaide, Guangyu Song, and Eugene Cheah. Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression. arXiv preprint arXiv:2407.12077, 2024. 153 Olga Golovneva, Tianlu Wang, Jason Weston, and Sainbayar Sukhbaatar. Contextual position encoding: Learning to count whats important. arXiv preprint arXiv:2405.18719, 2024. 154 Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 155 Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. 156 Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum ?id=tEYskw1VY2. 157 Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R√©. Hippo: Recurrent memory with optimal polynomial projections. In NeurIPS, 2020. 158 Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R√©. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In NeurIPS, pages 572585, 2021. 82 159 Albert Gu, Karan Goel, and Christopher R√©. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. 160 Jian Guan, Zhuoer Feng, Yamei Chen, Ruilin He, Xiaoxi Mao, Changjie Fan, and Minlie Huang. Lot: story-centric benchmark for evaluating chinese long text understanding and generation. Transactions of the Association for Computational Linguistics, 10:434451, 2022. 161 Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, and Hao Yu. Aptq: Attention-aware post-training mixed-precision quantization for large language models. In Proceedings of the 61st ACM/IEEE Design Automation Conference, pages 16, 2024. 162 Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. CoRR, abs/2306.11644, 2023. doi: 10.48550/ARXIV.2306.11644. URL https://doi.org/10.485 50/arXiv.2306.11644. 163 Michael G√ºnther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. Jina embeddings 2: 8192-token generalpurpose text embeddings for long documents. CoRR, abs/2310.19923, 2023. doi: 10.48550/ARXIV.2310.19923. URL https://doi.org/10.48550/arXiv.2310.19923. 164 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 165 Mandy Guo, Joshua Ainslie, David C. Uthus, Santiago Onta√±√≥n, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer for long sequences. In Marine Carpuat, Marie-Catherine de Marneffe, and Iv√°n Vladimir Meza Ru√≠z, editors, Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 724736. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-NAACL.55. URL https://doi.org/10.18653/v1/20 22.findings-naacl.55. 166 Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: retrieval-augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML20. JMLR.org, 2020. 167 Michael G√ºnther, Isabelle Mohr, Daniel James Williams, Bo Wang, and Han Xiao. Late chunking: Contextual chunk embeddings using long-context embedding models. arXiv preprint arXiv: 2409.04701, 2024. 168 Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LM-infinite: Zero-shot extreme length generalization for large language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 39914008, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.222. URL https://aclanthology.org/2024.naacl-long.222/. 83 169 Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten transformer: Vision transformer using focused linear attention. In ICCV, pages 59385948. IEEE, 2023. 170 Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang. Rethinking token reduction in mllms: Towards unified paradigm for training-free acceleration. CoRR, abs/2411.17686, 2024. doi: 10.48550/ARXIV.2411.17686. URL https://doi.org/10.48550/arXiv.2411.17686. 171 Tanveer Hannan, Md Mohaiminul Islam, Jindong Gu, Thomas Seidl, and Gedas Bertasius. Revisionllm: Recursive vision-language model for temporal grounding in hour-long videos. CoRR, abs/2411.14901, 2024. doi: 10.48550/ARXIV.2411.14901. URL https://doi.org/ 10.48550/arXiv.2411.14901. 172 Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models. ArXiv, abs/2308.10755, 2023. URL https://api.semanticsc holar.org/CorpusID:261049100. 173 Jiaao He and Jidong Zhai. Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines. arXiv preprint arXiv:2403.11421, 2024. 174 Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Qianguo Sun, Yuxin Liang, Hao Wang, Enming Zhang, and Jiaxing Zhang. Never lost in the middle: Mastering long-context question answering with position-agnostic decompositional training. arXiv preprint arXiv:2311.09198, 2023. 175 Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, LiuYiBo LiuYiBo, Qianguosun Qianguosun, Yuxin Liang, Hao Wang, Enming Zhang, and Jiaxing Zhang. Never lost in the middle: Mastering long-context question answering with position-agnostic decompositional training. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1362813642, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.736. URL https://aclanthology.org/2024.acl-lon g.736. 176 Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, et al. Dureader: chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 3746, 2018. 177 Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Wenbo Su, and Bo Zheng. Can large language models detect errors in long chain-of-thought reasoning? arXiv preprint arXiv:2502.19361, 2025. 178 Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang, Liwei Wang, Jingjing Xu, Zhi Zhang, Hongxia Yang, and Di He. Two stones hit one bird: Bilevel positional encoding for better length extrapolation. arXiv preprint arXiv:2401.16421, 2024. 179 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 180 Amey Hengle, Prasoon Bajpai, Soham Dan, and Tanmoy Chakraborty. Multilingual needle in haystack: Investigating long-context behavior of multilingual large language models, 2024. URL https://arxiv.org/abs/2408.10151. 181 Christian Herold and Hermann Ney. Improving long context document-level machine translation. CoRR, abs/2306.05183, 2023. doi: 10.48550/ARXIV.2306.05183. URL https: //doi.org/10.48550/arXiv.2306.05183. 182 Lukas Hilgert, Danni Liu, and Jan Niehues. Evaluating and training long-context large language models for question answering on scientific papers. In Sachin Kumar, Vidhisha Balachandran, Chan Young Park, Weijia Shi, Shirley Anugrah Hayati, Yulia Tsvetkov, Noah Smith, Hannaneh Hajishirzi, Dongyeop Kang, and David Jurgens, editors, Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for Domain, Application, Group, or Individual (CustomNLP4U), pages 220236, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.c ustomnlp4u-1.17. URL https://aclanthology.org/2024.customnlp4u-1.17/. 183 Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, 2020. 184 Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 185 Jiwoo Hong, Noah Lee, and James Thorne. ORPO: Monolithic preference optimizaIn Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung tion without reference model. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1117011189, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.626. URL https://aclanthology.org/2024.emnlp-main.626. 186 Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Sophia Shao. Speed: Speculative pipelined execution for efficient decoding. arXiv preprint arXiv:2310.12072, 2023. 187 Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems, 37:12701303, 2025. 188 Pedram Hosseini, Jessica Sin, Bing Ren, Bryceton Thomas, Elnaz Nouri, Ali Farahanchi, and Saeed Hassanpour. benchmark for long-form medical question answering. arXiv preprint arXiv:2411.09834, 2024. 189 Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? In First Conference on Language Modeling, 2024. 85 190 Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, et al. Memserve: Context caching for disaggregated llm serving with elastic memory pool. arXiv preprint arXiv:2406.17565, 2024. 191 Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, Xusheng Chen, Tao Xie, and Yizhou Shan. Efficient long-decoding inference with reasoningaware attention sparsity. arXiv preprint arXiv:2502.11147, 2025. 192 Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for general computing devices use, 2024. 193 Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, and Yansong Feng. Can perplexity reflect large language models ability in long text understanding? In The Second Tiny Papers Track at ICLR 2024. 194 Zhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh Tuan Luu, See-Kiong Ng, Zhiwei Jiang, et al. Longrecipe: Recipe for efficient long context generalization in large language models. arXiv preprint arXiv:2409.00509, 2024. 195 Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14191436, 2021. 196 Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Benchmarking large language models as AI research agents. CoRR, abs/2310.03302, 2023. doi: 10.48550/ARXIV.2310.03302. URL https://doi.org/10.48550/arXiv.2310.03302. 197 Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code large language models. 2024. URL https://arxiv.org/pdf/ 2411.04905. 198 Wei Huang, Haotong Qin, Yangdong Liu, Yawei Li, Xianglong Liu, Luca Benini, Michele Magno, and Xiaojuan Qi. Slim-llm: Salience-driven mixed-precision quantization for large language models. arXiv preprint arXiv:2405.14917, 2024. 199 Yunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Taolue Chen, Zenan Li, Yuan Yao, Xiaoxing Ma, Lijuan Yang, Hao Chen, Shupeng Li, and Penghao Zhao. Advancing transformer architecture in long-context large language models: comprehensive survey, 2024. 200 Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He. Compression represents intelligence linearly. In First Conference on Language Modeling. 201 Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. 202 Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 86 2024. doi: 10.48550/ARXIV.2409.12186. URL https://doi.org/10.48550/arXiv.240 9.12186. 203 DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. In NeurIPS, 2022. 204 Inflection. Im pi, your personal ai. https://inflection.ai/, 2023. URL https: //inflection.ai/. 205 Naoya Inoue, Pontus Stenetorp, and Kentaro Inui. R4c: benchmark for evaluating rc systems to get the right answer for the right reason. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 67406750, 2020. 206 Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874880, Online, apr 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. URL https://aclanthology.org/2021.eacl-main.74/. 207 Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24: 251:1251:43, 2023. URL https://jmlr.org/papers/v24/23-0037.html. 208 Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. 209 Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. 210 Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, et al. The facts grounding leaderboard: Benchmarking llms ability to ground responses to long-form input. arXiv preprint arXiv:2501.03200, 2025. 211 Young Kyun Jang, Junmo Kang, Yong Jae Lee, and Donghyun Kim. MATE: meet at the embedding - connecting images with long texts. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 16591672. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.findings-emnlp.90. 212 Minbyul Jeong, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, and Jaewoo Kang. Olaph: Improving factuality in biomedical long-form question answering. arXiv preprint arXiv:2405.12701, 2024. 213 Wonje Jeung, Dongjae Jeon, Ashkan Yousefpour, and Jonghyun Choi. Large language models still exhibit bias in long text. arXiv preprint arXiv:2410.17519, 2024. 214 Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. 215 Chaoyi Jiang, Lei Gao, Hossein Entezari Zarch, and Murali Annavaram. Efficient llm inference with i/o-aware partial kv cache recomputation. arXiv preprint arXiv:2411.17089, 2024. 216 Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: In Houda Compressing prompts for accelerated inference of large language models. Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1335813376, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.825. URL https://aclanthology.org/2023.emnlp-main.825/. 217 Huiqiang Jiang, Yucheng LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 5248152515. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_fil es/paper/2024/file/5dfbe6f5671e82c76841ba687a8a9ecb-Paper-Conference. pdf. 218 Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. 219 Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16581677, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.91. URL https://aclanthology.org/2024.acl-long.91/. 220 Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. CoRR, abs/2406.00515, 2024. doi: 10.48550/ARXIV.2 406.00515. URL https://doi.org/10.48550/arXiv.2406.00515. 221 Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan H. Chen, and Andrew Y. Ng. Many-shot in-context learning in multimodal foundation models. CoRR, abs/2405.09798, 2024. doi: 10.48550/ARXIV.2405.09798. URL https://doi.org/10.485 50/arXiv.2405.09798. 222 Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented generation with long-context llms. CoRR, abs/2406.15319, 2024. doi: 10.48550/ARXIV.2406. 15319. URL https://doi.org/10.48550/arXiv.2406.15319. 88 223 Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented generation with long-context llms. arXiv preprint arXiv:2406.15319, 2024. 224 Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=VT F8yNQM66. 225 Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan √ñ. Arik. Long-context llms meet RAG: overcoming challenges for long inputs in RAG. CoRR, abs/2410.05983, 2024. doi: 10.48550 /ARXIV.2410.05983. URL https://doi.org/10.48550/arXiv.2410.05983. 226 Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Arik. Long-context llms meet rag: Overcoming challenges for long inputs in rag. arXiv preprint arXiv:2410.05983, 2024. 227 Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, and Jinghua Tan. comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods. CoRR, abs/2403.02901, 2024. doi: 10.48550/ARXIV.2403.02901. URL https://doi.org/ 10.48550/arXiv.2403.02901. 228 Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Chia yuan Chang, and Xia Hu. Growlength: Accelerating llms pretraining by progressively growing training length. ArXiv, abs/2310.00576, 2023. 229 Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325, 2024. 230 Eunkyung Jo, Yuin Jeong, SoHyun Park, Daniel A. Epstein, and Young-Ho Kim. Understanding the impact of long-term memory on self-disclosure with large language model-driven chatbots for public health intervention. In Florian Floyd Mueller, Penny Kyburz, Julie R. Williamson, Corina Sas, Max L. Wilson, Phoebe O. Toups Dugas, and Irina Shklovski, editors, Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, pages 440:1440:21. ACM, 2024. doi: 10.1145/3613904.3642420. URL https://doi.org/10.1145/3613904.3642420. 231 Ashutosh Joshi, Sheikh Muhammad Sarwar, Samarth Varshney, Sreyashi Nag, Shrivats Agrawal, and Juhi Naik. Reaper: Reasoning based retrieval planning for complex rag systems. arXiv preprint arXiv: 2407.18553, 2024. 232 Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, 2017. 233 Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, and Muhao Chen. Familiarityaware evidence compression for retrieval-augmented generation, 2024. URL https: //arxiv.org/abs/2409.12468. 234 Hoyoun Jung and Kyung-Joong Kim. Discrete prompt compression with reinforcement learning. IEEE Access, 12:7257872587, 2024. ISSN 2169-3536. doi: 10.1109/access.2024.3403 426. URL http://dx.doi.org/10.1109/ACCESS.2024.3403426. 235 Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. 236 Greg Kamradt. Needle in haystack - pressure testing llms. https://github.com/gka mradt/LLMTest_NeedleInAHaystack, 2023. 237 Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. Gear: An efficient kv cache compression recipe for near-lossless generative inference of llm, 2024. URL https://arxiv.org/abs/2403.05527. 238 Sayash Kapoor, Peter Henderson, and Arvind Narayanan. Promises and pitfalls of artificial intelligence for legal applications. CoRR, abs/2402.01656, 2024. doi: 10.48550/ARXIV.2402. 01656. URL https://doi.org/10.48550/arXiv.2402.01656. 239 Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. One thousand and one pairs: \"novel\" challenge for long-context language models, 2024. 240 Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 51565165. PMLR, 2020. 241 Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024. 242 Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. International Conference on Learning Representations, 2019. 243 Heehoon Kim, Junyeol Ryu, and Jaejin Lee. Tccl: Discovering better communication paths for pcie gpu clusters. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pages 9991015, 2024. 244 Youngjoo Kim and Hyochoong Bang. Introduction to kalman filter and its applications. In Felix Govaers, editor, Introduction and Implementations of the Kalman Filter, chapter 2. IntechOpen, Rijeka, 2018. 245 Tom√°≈° KoÀácisk `y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G√°bor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. 246 Tom√°≈° KoÀácisk√Ω, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G√°bor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. doi: 10.1162/tacl _a_00023. URL https://aclanthology.org/Q18-1023/. 247 Abdullatif K√∂ksal, Timo Schick, Anna Korhonen, and Hinrich Sch√ºtze. Longform: Effective instruction tuning with reverse instructions. arXiv preprint arXiv:2304.08460, 2023. 248 Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5:341353, 2023. 249 Mario Michael Krell, Matej Kosec, Sergio Perez, and Andrew Fitzgibbon. Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. arXiv preprint arXiv:2107.02027, 2021. 90 250 Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering. arXiv preprint arXiv:2103.06332, 2021. 251 Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. Aquamuse: Automatically generating datasets for query-based multi-document summarization. arXiv preprint arXiv:2010.12694, 2020. 252 Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, et al. arXiv preprint Longlamp: benchmark for personalized long-form text generation. arXiv:2407.11016, 2024. 253 Achintya Kundu, Rhui Dih Lee, Laura Wynter, Raghu Kiran Ganti, and Mayank Mishra. Enhancing training efficiency using packing with flash attention. arXiv preprint arXiv:2407.09105, 2024. 254 Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. Babilong: Testing the limits of llms with long context reasoning-in-ahaystack. arXiv preprint arXiv:2406.10149, 2024. 255 Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Lifeng Shang, Qun Liu, and Kam-Fai Wong. M4le: multi-ability multi-range multi-task multi-domain longcontext evaluation benchmark for large language models. arXiv preprint arXiv:2310.19240, 2023. 256 Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. 257 Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large In Proceedings of the 29th Symposium on language model serving with pagedattention. Operating Systems Principles, pages 611626, 2023. 258 Philippe Laban, Alexander Fabbri, Caiming Xiong, and Chien-Sheng Wu. Summary of haystack: challenge to long-context llms and rag systems. arXiv preprint arXiv:https://arxiv.org/pdf/2407.01370, 2024. 259 Perplexity Labs. perplexity. URL https://www.perplexity.ai. Accessed: 2024-05-30. 260 Jack Lanchantin, Shubham Toshniwal, Jason Weston, arthur szlam, and Sainbayar Sukhbaatar. Learning to reason and memorize with self-notes. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1189111911. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/274d01461 44643ee2459a602123c60ff-Paper-Conference.pdf. 261 Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. Prompted llms as chatbot modules for long open-domain conversation. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 45364554. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-ACL .277. URL https://doi.org/10.18653/v1/2023.findings-acl.277. 91 262 Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, S√©bastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. Can arXiv preprint long-context language models subsume retrieval, rag, sql, and more? arXiv:2406.13121, 2024. 263 Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John F. Canny, and Ian Fischer. humaninspired reading agent with gist memory of very long contexts. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=OTmcsyEO5G. 264 Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, and Jaewoo Kang. Ethic: Evaluating large language models on long-context tasks with high information coverage. arXiv preprint arXiv:2410.16848, 2024. 265 Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. Qasa: advanced question answering on scientific articles. In International Conference on Machine Learning, pages 1903619052. PMLR, 2023. 266 Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, and Kang Liu. S3eval: synthetic, scalable, systematic evaluation suite for large language model. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 12591286, 2024. 267 Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. Mining of massive data sets. Cambridge university press, 2020. 268 Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 30453059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243. 269 Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. 270 Yaniv Leviathan, Matan Kalman, and Yossi Matias. Selective attention improves transformer, 2024. URL https://arxiv.org/abs/2410.02703. 271 Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The inductive bias of in-context learning: Rethinking pretraining example design. In International Conference on Learning Representations, 2022. 272 Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv: 2005.11401, 2020. 273 Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. doi: 10.48550/ARXIV.2408.03326. URL https://doi.org/10.485 50/arXiv.2408.03326. 92 274 Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. 275 Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir Patil, Matei Zaharia, Joseph Gonzalez, and Ion Stoica. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025. 276 Haoyang Li, Fangcheng Fu, Sheng Lin, Hao Ge, Xuanyu Wang, Jiawen Niu, Jie Jiang, and Bin Cui. Demystifying workload imbalances in large transformer model training over variable-length sequences. arXiv preprint arXiv:2412.07894, 2024. 277 Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939, 2023. 278 Jiashi Li. Flashmla: Efficient mla decoding kernels. https://github.com/deepseek-a i/FlashMLA, 2025. 279 Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015. 280 Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR, 2023. URL https://proceedings.mlr.press/v202/li2 3q.html. 281 Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. ArXiv, abs/2308.12032, 2023. 282 Mo Li, Songyang Zhang, Yunxin Liu, and Kai Chen. Needlebench: Can llms do retrieval and reasoning in 1 million context window? arXiv preprint arXiv:2407.11963, 2024. 283 Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, and Yuchen Xie. Fptq: Fine-grained post-training quantization for large language models. arXiv preprint arXiv:2308.15987, 2023. 284 Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. Temporal preference optimization for long-form video understanding. CoRR, abs/2501.13919, 2025. doi: 10.48550/ARXIV.2501.13919. URL https://doi.org/10.48550/arXiv.2501.13 919. 285 Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023. 286 Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, and Bo Zheng. Graphreader: Building graph-based agent to enhance long-context abilities of large language models. Conference on Empirical Methods in Natural Language Processing, 2024. doi: 10.48550/arXiv.2406.14550. 93 287 Shilong Li, Yancheng He, Hui Huang, Xingyuan Bu, Jiaheng Liu, Hangyu Guo, Weixun Wang, Jihao Gu, Wenbo Su, and Bo Zheng. 2d-dpo: Scaling direct preference optimization with 2-dimensional supervision. ArXiv, abs/2410.19720, 2024. 288 Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060, 2024. 289 Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. Videochatflash: Hierarchical compression for long-context video modeling. CoRR, abs/2501.00574, 2025. doi: 10.48550/ARXIV.2501.00574. URL https://doi.org/10.48550/arXiv.250 1.00574. 290 Yanyang Li, Shuo Liang, Michael R. Lyu, and Liwei Wang. Making long-context language models better multi-hop reasoners. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 24622475. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.135. URL https://doi.org/10.18653/v1/2024.acl-long.135. 291 Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 63426353, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.391. URL https://aclanthology.org /2023.emnlp-main.391. 292 Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, et al. Scbench: kv cache-centric analysis of long-context methods. arXiv preprint arXiv:2412.10319, 2024. 293 Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143, 2025. 294 Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: LLM knows what you are looking for before generation. In NeurIPS, 2024. 295 Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: speculative sampling requires rethinking feature uncertainty. In Proceedings of the 41st International Conference on Machine Learning, pages 2893528948, 2024. 296 Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrieval augmented generation or long-context llms? comprehensive study and hybrid approach. arXiv preprint arXiv:2407.16833, 2024. 297 Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language models: survey. ArXiv, abs/2410.12388, 2024. URL https://api.semantic scholar.org/CorpusID:273375474. 298 Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language models: survey, 2024. URL https://arxiv.org/abs/2410.12388. 94 299 Zongqian Li, Yixuan Su, and Nigel Collier. 500xcompressor: Generalized prompt compression for large language models, 2024. URL https://arxiv.org/abs/2408.03094. 300 Hao Liang, Jiapeng Li, Tianyi Bai, Xijie Huang, Linzhuang Sun, Zhengren Wang, Conghui He, Bin Cui, Chong Chen, and Wentao Zhang. Keyvideollm: Towards large-scale video keyframe selection. CoRR, abs/2407.03104, 2024. doi: 10.48550/ARXIV.2407.03104. URL https://doi.org/10.48550/arXiv.2407.03104. 301 Xiaobo Liang, Zecheng Tang, Juntao Li, and Min Zhang. Open-ended long text generation via masked language modeling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 223241, 2023. 302 Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. 303 Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. 304 Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. 305 Zhan Ling, Kang Liu, Kai Yan, Yifan Yang, Weijian Lin, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, and Jiecao Chen. Longreason: synthetic long-context reasoning benchmark via context expansion. arXiv preprint arXiv:2501.15089, 2025. 306 Lucas D. Lingle. Transformer-vq: Linear-time transformers via vector quantization. In ICLR. OpenReview.net, 2024. 307 Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. Prompt compression with context-aware sentence encoding for fast and improved llm inference, 2024. URL https://arxiv.org/abs/2409.01227. 308 Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. CoRR, 2024. 309 Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. 310 Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv e-prints, pages arXiv2402, 2024. 311 Jiaheng Liu, Zehao Ni, Haoran Que, Tao Sun, Noah Wang, Jian Yang, JiakaiWang, Hongcheng Guo, Z.Y. Peng, Ge Zhang, Jiayi Tian, Xingyuan Bu, Ke Xu, Wenge Rong, Junran Peng, and Zhaoxiang Zhang. Roleagent: Building, interacting, and benchmarking high-quality role-playing agents from scripts. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https: //openreview.net/forum?id=hORTHzt2cE. 312 Jiaheng Liu, ZhiqiBai ZhiqiBai, Yuanxing Zhang, Chenchen Zhang, YuangZh YuangZh, Ge Zhang, JiakaiWang JiakaiWang, Haoran Que, Yukang Chen, Wenbo Su, Tiezheng Ge, Jie 95 Fu, Wenhu Chen, and Bo Zheng. E2-LLM: Efficient and extreme length extension of large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 42434253, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-a cl.252. URL https://aclanthology.org/2024.findings-acl.252/. 313 Jiawei Liu, Jia Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan Katherine Wang, Jun Yang, and Lingming Zhang. Repoqa: Evaluating long context code understanding. arXiv preprint arXiv:2406.06025, 2024. 314 Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? CoRR, abs/2404.05955, 2024. doi: 10.48550/ARXIV.2404.05955. URL https://doi.org/10.48550/arXiv.2404.05955. 315 Junwei Liu, Yixuan Chen, Mingwei Liu, Xin Peng, and Yiling Lou. Stall+: Boosting llmbased repository-level code completion with static analysis. arXiv preprint arXiv:2406.10018, 2024. 316 Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. TCRA-LLM: Token compression retrieval augmented large language model for inference cost reduction. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 97969810, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.655. URL https://aclanthology.org/2023.findings-emnlp.655. 317 Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. 318 Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Yipeng Ma, Tao Wang, and Ji-Rong Wen. Unlocking data-free low-bit quantization with matrix decomposition for kv cache compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 24302440, 2024. 319 Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024. 320 Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, et al. Mdeval: Massively multilingual code debugging. arXiv preprint arXiv:2411.02310, 2024. 321 Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repositorylevel code auto-completion systems. In The Twelfth International Conference on Learning Representations, 2024. 322 Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. Multistage vision token dropping: Towards efficient multimodal large language model. CoRR, abs/2411.10803, 2024. doi: 10.48550/ARXIV.2411.10803. URL https://doi.org/10.485 50/arXiv.2411.10803. 96 323 Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. 324 Xiang Liu, Peijie Dong, Xuming Hu, and Xiaowen Chu. Longgenbench: Long-context generation benchmark. arXiv preprint arXiv:2410.04199, 2024. 325 Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023. 326 Xiaoran Liu, Kai Lv, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, and Dahua Lin. LongWanjuan: Towards systematic measurement for long text quality. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 57095725, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.327. URL https://aclanthology.org/2024.findings-emnlp.327. 327 Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xuanjing Huang, and Xipeng Qiu. Thus spake long-context large language model. 2025. URL https://api.semanticscholar. org/CorpusID:276575712. 328 Yang Liu, Jiaxiang Liu, Li Chen, Yuxiang Lu, Shikun Feng, Zhida Feng, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-SPARSE: learning hierarchical efficient transformer through regularized self-attention. CoRR, abs/2203.12276, 2022. 329 Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, et al. Understanding llms: comprehensive overview from training to inference. arXiv preprint arXiv:2401.02038, 2024. 330 Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, et al. Cachegen: Kv cache compression and streaming for fast large language model serving. In Proceedings of the ACM SIGCOMM 2024 Conference, pages 3856, 2024. 331 Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 5234252364. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/a452a7c6c 463e4ae8fbdc614c6e983e6-Paper-Conference.pdf. 332 Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. KIVI: tuning-free asymmetric 2bit quantization for KV cache. In Proceedings of the 41st International Conference on Machine Learning, pages 3233232344, 2024. 333 Yinghan Long, Sayeed Shafayet Chowdhury, and Kaushik Roy. Segmented recurrent transformer: An efficient sequence-to-sequence model. In EMNLP (Findings), pages 8325 8337. Association for Computational Linguistics, 2023. 334 Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max 97 Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau√ü, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian J. McAuley, Han Hu, Torsten Scholak, S√©bastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, and et al. Starcoder 2 and the stack v2: The next generation. CoRR, abs/2402.19173, 2024. doi: 10.48550/ARXIV.2402.19173. URL https://doi.org/10.48550/arXiv.2402.19173. 335 Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, and Jiezhong Qiu. Moba: Mixture of block attention for long-context llms, 2025. URL https://arxiv.org/abs/2502.13189. 336 Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for longcontext llms. arXiv preprint arXiv:2502.13189, 2025. 337 Keer Lu, Xiaonan Nie, Zheng Liang, Da Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, et al. Datasculpt: Crafting data landscapes for long-context llms through multi-objective partitioning. arXiv preprint arXiv:2409.00997, 2024. 338 Yi Lu, Jing Nathan Yan, Songlin Yang, Justin Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, and Alexander Rush. controlled study on long context extension and generalization in llms. arXiv preprint arXiv:2409.12181, 2024. 339 Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, and Xuanjing Huang. Longheads: Multi-head attention is secretly long context processor. In EMNLP (Findings), pages 71367148. Association for Computational Linguistics, 2024. 340 Evan Lucas, Dylan Kangas, and Timothy Havens. Extra global attention designation using keyword detection in sparse transformer architectures, 2024. URL https://arxiv. org/abs/2410.08971. 341 Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. Repoagent: An llm-powered open-source framework for repository-level code documentation generation. arXiv preprint arXiv:2402.16667, 2024. 342 Chenyang Lyu, Zefeng Du, Jitao Xu, Yitao Duan, Minghao Wu, Teresa Lynn, Alham Fikri Aji, Derek F. Wong, and Longyue Wang. paradigm shift: The future of machine translation lies with large language models. In Nicoletta Calzolari, Min-Yen Kan, V√©ronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pages 13391352. ELRA and ICCL, 2024. URL https://aclanthology.org/2024.lrec-main.120. 343 Feipeng Ma, Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, and Xiaoyan Sun. Visual perception by large 98 language models weights. CoRR, abs/2405.20339, 2024. doi: 10.48550/ARXIV.2405.20339. URL https://doi.org/10.48550/arXiv.2405.20339. 344 Xinbei Ma, Yeyun Gong, Pengcheng He, hai zhao, and Nan Duan. Query rewriting in retrieval-augmented large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=gXq1cw kUZc. 345 Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Length-compressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025. 346 Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 24212425, 2024. 347 Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, and Aixin Sun. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. CoRR, abs/2407.01523, 2024. doi: 10.48550/ARXIV.240 7.01523. URL https://doi.org/10.48550/arXiv.2407.01523. 348 Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve gpt-3 after deployment. arXiv preprint arXiv: 2201.06009, 2022. 349 Seiji Maekawa*, Hayate Iso*, and Nikita Bhutani. Holistic reasoning with long-context lms: benchmark for database operations on massive textual data, 2024. URL https: //arxiv.org/abs/2410.11996. *These authors contributed equally to this work. 350 Adyasha Maharana, Prateek Yadav, and Mohit Bansal. D2 pruning: Message passing for balancing diversity and difficulty in data pruning. CoRR, abs/2310.07931, 2023. doi: 10.48550/ARXIV.2310.07931. URL https://doi.org/10.48550/arXiv.2310.07931. 351 Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. Expertqa: Expert-curated questions and attributed answers. arXiv preprint arXiv:2309.07852, 2023. 352 Chaitanya Malaviya, Priyanka Agrawal, Kuzman Ganchev, Pranesh Srinivasan, Fantine Huot, Jonathan Berant, Mark Yatskar, Dipanjan Das, Mirella Lapata, and Chris Alberti. Dolomites: Domain-specific long-form methodical tasks. arXiv preprint arXiv:2405.05938, 2024. 353 Ahmed Masry and Amir Hajian. Longfin: multimodal document understanding model for long financial domain documents. CoRR, abs/2401.15050, 2024. doi: 10.48550/ARXIV.2 401.15050. URL https://doi.org/10.48550/arXiv.2401.15050. 354 Ahmed Masry and Amir Hajian. Longfin: multimodal document understanding model for long financial domain documents. arXiv preprint arXiv:2401.15050, 2024. 355 Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, J. Rao, Marc Najork, Emma Strubell, and Donald Metzler. Dsi++: Updating transformer memory with new documents. Conference on Empirical Methods in Natural Language Processing, 2022. doi: 10.48550/arXiv.2212.09744. 99 356 Xin Men, Mingyu Xu, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng Chen. Base of rope bounds context length, 2024. URL https://arxiv.org/ab s/2405.14591. 357 Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 1735917372. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/p aper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-C onference.pdf. 358 Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. ArXiv, abs/2405.14734, 2024. 359 Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. 360 Microsoft. Microsoft copilot, 2023. URL https://copilot.microsoft.com/. 361 Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. Document-level neural machine translation with hierarchical attention networks. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 29472954. Association for Computational Linguistics, 2018. doi: 10.18653/V1/ D18-1325. URL https://doi.org/10.18653/v1/d18-1325. 362 Tomas Mikolov, Martin Karafi√°t, Lukas Burget, Jan Cernock `y, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pages 10451048. Makuhari, 2010. 363 Evan Miller. Attention is off by one, 7 2023. URL https://www.evanmiller.org/att ention-is-off-by-one.html. 364 Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering ambiguous open-domain questions. arXiv preprint arXiv:2004.10645, 2020. 365 Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. 366 MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji 100 Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, and Zijia Wu. Minimax-01: Scaling foundation models with lightning attention, 2025. URL https: //arxiv.org/abs/2501.08313. 367 Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, Manish Sethi, Xuan-Hong Dang, Pengyuan Li, Kun-Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju Pavuluri, Yan Koyfman, Boris Lublinsky, Maximilien de Bayser, Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Yi Zhou, Chris Johnson, Aanchal Goyal, Hima Patel, S. Yousaf Shah, Petros Zerfos, Heiko Ludwig, Asim Munawar, Maxwell Crouse, Pavan Kapanipathi, Shweta Salaria, Bob Calio, Sophia Wen, Seetharami Seelam, Brian Belgodere, Carlos A. Fonseca, Amith Singhee, Nirmit Desai, David D. Cox, Ruchir Puri, and Rameswar Panda. Granite code models: family of open foundation models for code intelligence. CoRR, abs/2405.04324, 2024. doi: 10.48550/ARXIV.2405.04324. URL https://doi.org/10.48550/arXiv.2405.04324. 368 Fengran Mo, Kelong Mao, Ziliang Zhao, Hongjin Qian, Haonan Chen, Yiruo Cheng, Xiaoxi Li, Yutao Zhu, Zhicheng Dou, and Jian-Yun Nie. survey of conversational search. arXiv preprint arXiv: 2410.15576, 2024. 369 Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan Rossi, Seunghyun Yoon, and Hinrich Sch√ºtze. Nolima: Long-context evaluation beyond literal matching. arXiv preprint arXiv:2502.05167, 2025. 370 Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. 371 Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1932719352. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/fi le/3d77c6dcc7f143aa2154e7f4d5e22d68-Paper-Conference.pdf. 372 Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. arXiv preprint arXiv: 2402.09906, 2024. 373 Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand√®s, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. 374 Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. CoRR, abs/2404.07143, 2024. 375 Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention, 2024. URL https://arxiv. org/abs/2404.07143. 376 Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, and Tobias Weyand. Neptune: The long orbit to benchmarking long video understanding. CoRR, abs/2412.09582, 2024. doi: 10.48550/ARXIV.2412.09582. URL https://doi.org/10.48550/arXiv.2412.09582. 101 377 Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016. 378 Neel Nanda. Mechanistic interpretability quickstart guide, 2023. URL https://www.ne elnanda.io/mechanistic-interpretability/quickstart. Accessed: 2025-01-30. 379 Chien Van Nguyen, Huy Huu Nguyen, Thang M. Pham, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Ryan A. Rossi, Trung Bui, Viet Dac Lai, Franck Dernoncourt, and Thien Huu Nguyen. Taipan: Efficient and expressive state space language models with selective attention, 2024. URL https://arxiv.org/abs/2410.18572. 380 Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: human-generated machine reading comprehension dataset. 2016. 381 Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Vincent Poor, Qingsong Wen, and Stefan Zohren. survey of large language models for financial applications: Progress, prospects and challenges. CoRR, abs/2406.11903, 2024. doi: 10.48550/ARXIV.2406.11903. URL https://doi.org/10.48550/arXiv.2406.11903. 382 NVIDIA Corporation. Nvidia h200, 2023. URL https://www.nvidia.com/en-us/data -center/h200/. 383 Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learningand-induction-heads/index.html. 384 OpenAI. New embedding models and api updates, 2024. URL https://openai.com/i ndex/new-embedding-models-and-api-updates/. Accessed: 2024-01-25. 385 OpenAI. Memory and new controls for chatgpt, 2024. URL https://openai.com/ind ex/memory-and-new-controls-for-chatgpt. Accessed: 2024-02-13. 386 OpenAI. Learning to reason with large language models. https://openai.com/index /learning-to-reason-with-llms/, 2024. 387 Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscho lar.org/CorpusID:246426909. 388 Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Fran√ßois Fleuret. Fast attention over long sequences with dynamic sparse flash attention. Advances in Neural Information Processing Systems, 36:5980859831, 2023. 389 Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint arXiv:2308.10882, 2023. 102 390 Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor R√ºhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 963981, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.57. URL https://aclanthology.org/2024.findings-acl.57/. 391 Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, et al. Quality: Question In Proceedings of the 2022 Conference of the North answering with long input texts, yes! American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 53365358, 2022. 392 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 393 Daon Park and Bernhard Egger. Improving throughput-oriented llm inference with cpu computations. In Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques, pages 233245, 2024. 394 J. Park, Joseph C. OBrien, Carrie J. Cai, M. Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. ACM Symposium on User Interface Software and Technology, 2023. doi: 10.1145/3586183.3606763. 395 Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? comparative study on in-context learning tasks. In ICML. OpenReview.net, 2024. 396 Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, √ç√±igo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 118132. IEEE, 2024. 397 Saurav Pawar, S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Aman Chadha, and Amitava Das. The what, why, and how of context length extension techniques in large language models - detailed survey. CoRR, abs/2401.07872, 2024. doi: 10.48550/A RXIV.2401.07872. URL https://doi.org/10.48550/arXiv.2401.07872. 398 Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanislaw Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: reinventing rnns for the transIn EMNLP (Findings), pages 1404814077. Association for Computational former era. Linguistics, 2023. 399 Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, Przemys≈Çaw Kazienko, Kranthi Kiran GV, Jan Koco n, Bart≈Çomiej Koptyra, Satyapriya Krishna, Ronald McClelland Jr., Jiaju Lin, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, 103 Cahya Wirawan, Stanis≈Çaw Wozniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence, 2024. URL https://arxiv.org/abs/2404.05892. 400 Bowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled _rope_allows_llama_models_to_have, 2023. 401 Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. 402 Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. 403 Chau Minh Pham, Simeng Sun, and Mohit Iyyer. Suri: Multi-constraint instruction following for long-form text generation. arXiv preprint arXiv:2406.19371, 2024. 404 Hung Phan, Anurag Acharya, Rounak Meyur, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, et al. Examining long-context large language models for environmental review document comprehension. arXiv preprint arXiv:2407.07321, 2024. 405 Bowen Ping, Jiali Zeng, Fandong Meng, Shuo Wang, Jie Zhou, and Shanghang Zhang. Longdpo: Unlock better long-form generation abilities for llms via critique-augmented stepwise information, 2025. URL https://arxiv.org/abs/2502.02095. 406 Hadi Pouransari, Chun-Liang Li, Jen-Hao Rick Chang, Pavan Kumar Anasosalu Vasu, Cem Koc, Vaishaal Shankar, and Oncel Tuzel. Dataset decomposition: Faster llm training with variable sequence length curriculum. arXiv preprint arXiv:2405.13226, 2024. 407 Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. SPIQA: dataset for multimodal question answering on scientific papers. CoRR, abs/2407.09413, 2024. doi: 10.48550/ARXIV.2407.09413. URL https://doi.org/10.48550/arXiv.2407.09413. 408 Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. 409 Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin. Zero bubble pipeline parallelism. arXiv preprint arXiv:2401.10241, 2023. 410 Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, and Wei Xu. Long2rag: Evaluating long-context & long-form retrieval-augmented generation with key point recall. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 48524872, 2024. 411 Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: kvcache-centric disaggregated architecture for llm serving. arXiv preprint arXiv:2407.00079, 2024. 412 Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. 104 413 Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In ICLR. OpenReview.net, 2022. 414 Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models, 2024. URL https://arxiv.org/abs/2401.04658. 415 Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Various lengths, constant speed: Efficient language modeling with lightning attention. In ICML. OpenReview.net, 2024. 416 Zexuan Qiu, Jingjing Li, Shijue Huang, Xiaoqi Jiao, Wanjun Zhong, and Irwin King. Clongeval: chinese benchmark for evaluating long-context large language models. arXiv preprint arXiv:2403.03514, 2024. 417 Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, and Junyang Lin. Language models can self-lengthen to generate long texts. arXiv preprint arXiv:2410.23933, 2024. 418 Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. 419 Alec Radford. Improving language understanding by generative pre-training. 2018. 420 Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In ICLR. OpenReview.net, 2020. 421 Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson dAutume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. CoRR, abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446. 422 Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. ArXiv, abs/2305.18290, 2023. URL https://api.semanticscholar.org/Corp usID:258959321. 105 423 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 424 Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506, 2020. 425 Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. Docfinqa: long-context financial reasoning dataset. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024 - Short Papers, Bangkok, Thailand, August 11-16, 2024, pages 445458. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.acl-short.42. 426 Isaac Rehg. Kv-compress: Paged kv-cache compression with variable compression rates per attention head. arXiv preprint arXiv:2410.00161, 2024. 427 Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. 428 Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling, 2024. URL https://arxiv.org/abs/2406.07522. 429 Jonathan Roberts, Kai Han, and Samuel Albanie. Needle threading: Can llms follow threads through near-million-scale haystacks? arXiv preprint arXiv:2411.05000, 2024. 430 Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and Mikhail Burtsev. Associative recurrent memory transformer. CoRR, abs/2407.04841, 2024. 431 Sara Rosenthal, Avirup Sil, Radu Florian, and Salim Roukos. Clapnq: Cohesive long-form answers from passages in natural questions for rag systems. arXiv preprint arXiv:2404.02103, 2024. 432 Jie Ruan, Wenqing Wang, and Xiaojun Wan. Defining and detecting vulnerability in human evaluation guidelines: preliminary study towards reliable nlg evaluation. arXiv preprint arXiv:2406.07935, 2024. 433 Anian Ruoss, Gr√©goire Del√©tang, Tim Genewein, Jordi Grau-Moya, R√≥bert Csord√°s, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023. 434 Anian Ruoss, Fabio Pardo, Harris Chan, Bonnie Li, Volodymyr Mnih, and Tim Genewein. Lmact: benchmark for in-context imitation learning with long multimodal demonstrations. CoRR, abs/2412.01441, 2024. doi: 10.48550/ARXIV.2412.01441. URL https://doi.org/10.48550/arXiv.2412.01441. 435 Shiwangi Shah Rupesh Bansal. Deepsearch: Semantic search on multimedia sources like audio, video and images. https://github.com/deepsearch-ai/deepsearch, 2023. URL https://github.com/deepsearch-ai/deepsearch. 106 436 Michael S. Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Julian Ibarz, and Anurag Arnab. Token turing machines. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1907019081, June 2023. 437 Jon Saad-Falcon, Daniel Y. Fu, Simran Arora, Neel Guha, and Christopher R√©. Benchmarking and building long-context retrieval models with loco and M2-BERT. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=HkCRgoGtt6. 438 Amir Saeidi, Shivanshu Verma, Aswin RRV, and Chitta Baral. Triple preference optimization: Achieving better alignment with less data in single step optimization, 2024. 439 Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Fran√ßois Yvon, Matthias Gall√©, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno√Æt Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, Eduardo Gonzalez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Natan, Francesco De Toni, G√©rard Dupont, Germ√°n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, Hieu Trung Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu√±oz, Maraim Masoud, Mar√≠a Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Moham mad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Lopez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma Sharma, S. Longpre, Somaieh Nikpoor, S. Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, Saiful Bari, Maged S. Al-Shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault F√©vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois Lavallee, R√©mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St√©phane Requena, Suraj Patil, Tim Dettmers, 107 Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurelie Neveol, Charles Lovering, Daniel Garrette, Deepak R. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, S. Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, ZdenÀáek Kasner, ZdenÀáek Kasner, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ayoade Ajibade, Bharat Kumar Saxena, Carlos Mu√±oz Ferrandis, Danish Contractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatim Tahirah Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, L√≠via Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran An, R. P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl√©mentine Fourrier, Daniel Leon Perinan, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John Giorgi, Jonas Golde, Jos√© D. Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P√†mies, Mar√≠a Andrea Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, Patrick Haller, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Pratap Bharati, Tanmay Laud, Th√©o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yu Xu, Zhee Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100, 2022. 440 Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, M. Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Neural Information Processing Systems, 2023. doi: 10.48550/arXiv.2302.04761. 441 Dale Schuurmans. Memory augmented large language models are computationally universal. arXiv preprint arXiv: 2301.04589, 2023. URL https://arxiv.org/abs/2301.04589 v1. 442 Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. 108 Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2025. 443 Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023. 444 Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, and Victor R√ºhle. Taco-rl: Task aware prompt compression optimization with reinforcement learning, 2024. URL https://arxiv.org/abs/2409.13035. 445 Claude Elwood Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. 446 Bin Shao and Jiawei Yan. long-context language model for deciphering and generating bacteriophage genomes. Nature Communications, 15(1):9392, 2024. 447 Ninglu Shao, Shitao Xiao, Zheng Liu, and Peitian Zhang. Extensible embedding: flexible multipler for llms context length. CoRR, abs/2402.11577, 2024. 448 Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. Long and diverse text generation with planning-based hierarchical variational model. arXiv preprint arXiv:1908.06605, 2019. 449 Aditya Sharma, Michael Saxon, and William Yang Wang. Losing visual needles in image haystacks: Vision language models are easily distracted in short and long contexts. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 54295451. Association for Computational Linguistics, 2024. URL https://aclantholo gy.org/2024.findings-emnlp.312. 450 Eva Sharma, Chen Li, and Lu Wang. Bigpatent: large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 22042213, 2019. 451 Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018. 452 Zejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey. Multi-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities. Advances in Neural Information Processing Systems, 35:1315813173, 2022. 453 Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 35313539, 2021. 454 Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher R√©, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with single gpu. In International Conference on Machine Learning, pages 3109431116. PMLR, 2023. 455 Kaize Shi, Xueyao Sun, Qing Li, and Guandong Xu. Compressing long context for enhancing RAG with amr-based concept distillation. CoRR, abs/2405.03085, 2024. doi: 10.48550/ARX IV.2405.03085. URL https://doi.org/10.48550/arXiv.2405.03085. 109 456 Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A. Smith, Luke S. Zettlemoyer, Scott Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document boundaries. ArXiv, abs/2310.10638, 2023. 457 Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: Retrieval-augmented black-box language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83718384, Mexico City, Mexico, jun 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.463. URL https://aclanthology.org/2024.naacl-long.463/. 458 Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv: 2303.11366, 2023. 459 Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. 460 Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion: Training code models to understand your repository. CoRR, abs/2306.10998, 2023. doi: 10.48550/ARXIV.2306.10998. URL https://doi.org/10.48550/arXiv.230 6.10998. 461 Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large language models of code. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 3169331715. PMLR, 2023. URL https://proceedings.mlr.press/v202/shrivastava23a.html. 462 Shuzheng Si, Haozhe Zhao, Gang Chen, Yunshui Li, Kangyang Luo, Chuancheng Lv, Kaikai An, Fanchao Qi, Baobao Chang, and Maosong Sun. Gateau: Selecting influential sample for long context alignment. 2024. 463 Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele. Loki: Low-rank keys for efficient sparse attention. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 1669216723. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/1e027da6b ec9ceb2ec37951ceeccae93-Paper-Conference.pdf. 464 Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, June 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. 465 Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. CoRR, abs/2404.18532, 2024. doi: 10.48550/ARXIV.2404.18532. URL https://doi.org/10.48550/arXiv.2404.18532. 466 Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, and Dong Yu. Zebra: Extending context window with layerwise grouped local-global attention, 2023. URL https://arxiv.org/abs/2312.08618. 110 467 Mingyang Song, Mao Zheng, and Xuan Luo. Counting-stars: multi-evidence, positionaware, and scalable benchmark for evaluating long-context large language models. Preprint, 2024. 468 Mohammed Sourouri, Tor Gillberg, Scott Baden, and Xing Cai. Effective multi-gpu communication using multiple cuda streams and threads. In 2014 20th IEEE International Conference on Parallel and Distributed Systems (ICPADS), pages 981986. IEEE, 2014. 469 spicychat.ai. Spicychat, 2024. URL https://spicychat.ai/. 470 Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Yu Zhao, Henryk Michalewski, ≈Åukasz Kuci nski, and Piotr Mi≈Ços. Structured packing in llm training improves long context utilization. arXiv preprint arXiv:2312.17296, 2023. 471 Konrad Staniszewski, Szymon Tworkowski, Yu Zhao, Sebastian Jaszczur, Henryk Michalewski, Lukasz Kucinski, and Piotr Milos. Structured packing in llm training improves long context utilization. ArXiv, abs/2312.17296, 2023. 472 Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form answers. arXiv preprint arXiv:2204.06092, 2022. 473 Jianlin Su. Transformer 10, Aug 2023. URL https://spaces.ac.cn/archives/9675. 474 Jianlin Su. Transformer 12, Aug 2023. URL https://kexue.fm/archives/9708. 475 Jianlin Su. Transformer 17. Online Resource, Mar 2024. URL https://spaces.ac.cn/a rchives/10040. 476 Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 477 Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. In First Conference on Language Modeling. 478 Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. arXiv preprint arXiv: 2305.16653, 2023. 479 Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 807822, 2021. 480 Simeng Sun, Y. Liu, Shuo Wang, Chenguang Zhu, and Mohit Iyyer. Pearl: Prompting large language models to plan and execute actions over long documents. Conference of the European Chapter of the Association for Computational Linguistics, 2023. doi: 10.48550/arXiv.2305.14564. 481 Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. Is chatgpt good at search? investigating large language models as re-ranking agents. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1491814937. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.923. URL https://doi.org/10.18653/v1/20 23.emnlp-main.923. 111 482 Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022. 483 Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models, 2023. URL https://arxiv.org/abs/2307.08621. 484 Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. Advances in Neural Information Processing Systems, 37:73397361, 2025. 485 Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: arXiv preprint question answering challenge targeting commonsense knowledge. arXiv:1811.00937, 2018. 486 Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, et al. Proxyqa: An alternative framework for evaluating long-form text generation with large language models. arXiv preprint arXiv:2401.15042, 2024. 487 Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun Wang, B√∂rje F. Karlsson, Bo An, and Zongqing Lu. Towards general computer control: multimodal agent for red dead redemption II as case study. CoRR, abs/2403.03186, 2024. doi: 10.48550 /ARXIV.2403.03186. URL https://doi.org/10.48550/arXiv.2403.03186. 488 Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 75377547. Curran Associates, Inc., 2020. URL https: //proceedings.neurips.cc/paper_files/paper/2020/file/550536832689576 97aa39fba6f231c68-Paper.pdf. 489 Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Shikuan Hong, Yiwu Yao, and Gongyi Wang. Razorattention: Efficient kv cache compression through retrieval heads. arXiv preprint arXiv:2407.15891, 2024. 490 Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Danning Ke, Shikuan Hong, Yiwu Yao, and Gongyi Wang. Razorattention: Efficient KV cache compression through retrieval heads. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=tkiZQlL04w. 491 Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. QUEST: Query-aware sparsity for efficient long-context LLM inference. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 4790147911. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/tang24l.html. 492 Zecheng Tang, Zechen Sun, Juntao Li, Qiaoming Zhu, and Min Zhang. Logo - long context alignment via efficient preference optimization. ArXiv, abs/2410.18533, 2024. 112 493 Zecheng Tang, Zechen Sun, Juntao Li, Qiaoming Zhu, and Min Zhang. Logolong context alignment via efficient preference optimization. arXiv preprint arXiv:2410.18533, 2024. 494 Zecheng Tang, Keyan Zhou, Juntao Li, Baibei Ji, Jianye Hou, and Min Zhang. L-citeeval: Do long-context models truly leverage context for responding?, 2024. 495 Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: benchmark for efficient transformers, 2020. URL https://arxiv.org/abs/2011.04006. 496 Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. Transformer memory as differentiable search index. arXiv preprint arXiv: 2202.06991, 2022. 497 Coze Team. Coze. https://www.coze.cn/, 2025. 498 Dify Team. The innovation engine for genai applications. https://dify.ai/, 2025. 499 Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 500 Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L√©onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram√©, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 501 Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. Jamba-1.5: Hybrid transformermamba models at scale. arXiv preprint arXiv:2408.12570, 2024. 502 Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 503 M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhen-Nan Wei, Chujie Zheng, Kaixin Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Si-Xuan Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Ze Wang, Junting Zhou, Yu Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun-Jing Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yi-Hui Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ru-Qing Yuan, Yuan hao Yue, Tianyang Zhan, Chun Zhang, Jing-Yun Zhang, Xiyue Zhang, Xing Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. 2025. 504 Manus Team. Leave it to manus. https://manus.im/, 2025. 505 OpenAI Team. Computer-using agent. https://openai.com/index/computer-usi ng-agent/, 2025. 506 OpenAI Team. Introducing deep research. https://openai.com/index/introducing -deep-research/, 2025. 507 Qwen Team. Qwen2.5-vl, January 2025. URL https://qwenlm.github.io/blog/qwen 2.5-vl/. 508 Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, et al. Distance between relevant information pieces causes bias in long-context llms. arXiv preprint arXiv:2410.14641, 2024. 509 Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S. Morcos. D4: improving LLM pretraining via document de-duplication and diversification. CoRR, abs/2308.12284, 2023. doi: 10.48550/ARXIV.2308.12284. URL https://doi.org/10.48550/arXiv.230 8.12284. 510 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 511 Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, and Oncel Tuzel. Fastvlm: Efficient vision encoding for vision language models. CoRR, abs/2412.13303, 2024. doi: 10.48550/A RXIV.2412.13303. URL https://doi.org/10.48550/arXiv.2412.13303. 512 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 59986008, 2017. 513 Saranya Venkatraman, Nafis Irtiza Tripto, and Dongwon Lee. Collabstory: Multi-llm collaborative story generation and authorship analysis. arXiv preprint arXiv:2406.12665, 2024. 514 Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et al. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. arXiv preprint arXiv:2409.12640, 2024. 515 Aaron R. Voelker and Chris Eliasmith. Improving spiking dynamical networks: Accurate delays, higher-order synapses, and time cells. Neural Comput., 30(3), 2018. 516 Elena Voita, Javier Ferrando, and Christoforos Nalmpantis. Neurons in large language models: Dead, n-gram, positional. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1288 1301, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.75. URL https://aclanthology.org/2024.findings -acl.75/. 517 Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024. 518 Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel Bowman. Squality: Building long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11391156, 2022. 519 Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. Encoding word order in complex embeddings. arXiv preprint arXiv:1912.12333, 2019. 520 Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, and Xin Peng. Teaching code llms to use autocompletion tools in repository-level code generation. arXiv preprint arXiv:2401.06391, 2024. 521 Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, and Kai Chen. Ada-leval: Evaluating long-context llms with length-adaptable benchmarks, 2024. 522 Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv: 2305.16291, 2023. 523 Haonan Wang, Qian Liu, Chao Du, Tongyao Zhu, Cunxiao Du, Kenji Kawaguchi, and Tianyu Pang. When precision meets position: Bfloat16 breaks down rope in long-context training. arXiv preprint arXiv:2411.13476, 2024. 524 Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. Multimodal needle in haystack: Benchmarking longcontext capability of multimodal large language models. CoRR, abs/2406.11230, 2024. doi: 10.48550/ARXIV.2406.11230. URL https://doi.org/10.48550/arXiv.2406.11230. 525 Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou. Drt-o1: Optimized deep reasoning translation via long chain-of-thought. arXiv preprint arXiv:2412.17498, 2024. 526 Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the Wild: Circuit for Indirect Object Identification in GPT-2 Small. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=NpsVSN6o4ul. 527 Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao ran Yang, Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji rong Wen. survey on large language model based autonomous agents. Frontiers Comput. Sci., 2023. doi: 10.1007/s11704-024-40231-1. 528 Lei Wang, Shan Dong, Yuhui Xu, Hanze Dong, Yalu Wang, Amrita Saha, Ee-Peng Lim, Caiming Xiong, and Doyen Sahoo. Mathhay: An automated benchmark for long-context mathematical reasoning in llms. arXiv preprint arXiv:2410.04698, 2024. 529 Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models. Conference on Empirical Methods in Natural Language Processing, 2023. doi: 10.48550 /arXiv.2303.07678. 530 Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 115 2024, pages 1189711916. Association for Computational Linguistics, 2024. doi: 10.18653/V 1/2024.ACL-LONG.642. URL https://doi.org/10.18653/v1/2024.acl-long.642. 531 Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Large search model: Redefining search stack in the era of llms. In ACM SIGIR Forum, volume 57, pages 116. ACM New York, NY, USA, 2024. 532 Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. Document-level machine translation with large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1664616661. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.E MNLP-MAIN.1036. URL https://doi.org/10.18653/v1/2023.emnlp-main.1036. 533 Longyue Wang, Zefeng Du, Wenxiang Jiao, Chenyang Lyu, Jianhui Pang, Leyang Cui, Kaiqiang Song, Derek F. Wong, Shuming Shi, and Zhaopeng Tu. Benchmarking and improving long-text translation with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 71757187. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.428. URL https://doi.org/10.18653/v1/2024.findings-acl.428. 534 Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et al. Leave no document behind: Benchmarking In Proceedings of the 2024 Conference on long-context llms with extended multi-doc qa. Empirical Methods in Natural Language Processing, pages 56275646, 2024. 535 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 536 Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Selfattention with linear complexity, 2020. URL https://arxiv.org/abs/2006.04768. 537 Suyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi Rezagholizadeh, and Bang Liu. Resonance rope: Improving context length generalization of large language models. arXiv preprint arXiv:2403.00071, 2024. 538 Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, 2024. URL https://arxiv.org/abs/2401.1 7268. 539 Ting Wang, Chuan Yang, Maoyang Zou, Jiaying Liang, Dong Xiang, Wenjie Yang, Hongyang Wang, and Jia Li. study of extractive summarization of long documents incorporating local topic and hierarchical information. Scientific Reports, 14(1):10140, 2024. 116 540 Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ebd82705f44793b6f 9ade5a669d0f0bf-Abstract-Conference.html. 541 Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. CoRR, abs/2409.02889, 2024. doi: 10.48550/ARXIV.2409.02889. URL https://doi.org/ 10.48550/arXiv.2409.02889. 542 Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. Beyond the limits: survey of techniques to extend the context length in large language models. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024, pages 82998307. ijcai.org, 2024. URL https://www.ijcai.org/proceedings/2024/917. 543 Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2024. 544 Yanli Wang, Yanlin Wang, Suiquan Wang, Daya Guo, Jiachi Chen, John Grundy, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, et al. Repotransbench: real-world benchmark for repository-level code translation. arXiv preprint arXiv:2412.17744, 2024. 545 Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, and Julian J. McAuley. MEMORYLLM: towards self-updatable large language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=p0lKWzdikQ. 546 Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, and Junran Peng. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv: 2310.00746, 2023. 547 Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive tree-based video representation for LLM reasoning on long videos. CoRR, abs/2405.19209, 2024. doi: 10.48550/ARXIV.2405.19209. URL https://doi.org/10.48550/arXiv.2405.19209. 548 Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, Orion Weller, Oskar Hallstr√∂m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv preprint arXiv: 2412.13663, 2024. URL https://arxiv.org/abs/2412.13663. 549 Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802, 2024. 117 550 Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2024. 551 Jason Weston, Antoine Bordes, Sumit Chopra, Alexander Rush, Bart Van Merri√´nboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015. 552 wiki. Printing press, 2023. URL https://en.wikipedia.org/wiki/Printing_press. 553 wiki. Library of Alexandria, 2024. URL https://en.wikipedia.org/wiki/Library_ of_Alexandria. 554 wiki. Chinese astronomy, 2024. URL https://en.wikipedia.org/wiki/Chinese_as tronomy. 555 David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 56215634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.finding s-emnlp.412. URL https://aclanthology.org/2022.findings-emnlp.412/. 556 Sam Wiseman, Stuart Shieber, and Alexander Rush. Challenges in data-to-document generation. arXiv preprint arXiv:1707.08052, 2017. 557 Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. Hi-transformer: Hierarchical interactive transformer for efficient and effective long document modeling. In ACL/IJCNLP (2), pages 848853. Association for Computational Linguistics, 2021. 558 Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813, 2024. URL https://arxiv.org/abs/2410.10813. 559 Han Wu, Mingjie Zhan, Haochen Tan, Zhaohui Hou, Ding Liang, and Linqi Song. Vcsum: versatile chinese meeting summarization dataset. In Findings of the Association for Computational Linguistics: ACL 2023, pages 60656079, 2023. 560 Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. CoRR, abs/2407.15754, 2024. doi: 10.48550/ARXIV.2407.15754. URL https://doi.org/10.48550/arXiv.2407.15754. 561 Longyun Wu, Dawei Zhu, Guangxiang Zhao, Zhuocheng Yu, Junfeng Ran, Xiangyu Wong, Lin Sun, and Sujian Li. Longattn: Selecting long-context training data via token-level attention. arXiv preprint arXiv:2502.16860, 2025. 562 Qingyang Wu, Zhenzhong Lan, Jing Gu, and Zhou Yu. Memformer: The memoryaugmented transformer. CoRR, abs/2010.06891, 2020. URL https://arxiv.org/abs/20 10.06891. 563 Tong Wu, Yanpeng Zhao, and Zilong Zheng. An efficient recipe for long context extension via middle-focused positional encoding. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. 118 564 Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li. Long context alignment with short instructions and synthesized positions. arXiv preprint arXiv:2405.03939, 2024. 565 Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=EytBpUGB 1Z. 566 Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, et al. Tablebench: comprehensive and complex benchmark for table question answering. arXiv preprint arXiv:2408.09174, 2024. 567 Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, and Wei Zhang. Lifbench: Evaluating the instruction following performance and stability of large language models in long-context scenarios. arXiv preprint arXiv:2411.07037, 2024. 568 Yuhao Wu, Ming Shan Hee, Zhiqing Hu, and Roy Ka-Wei Lee. Longgenbench: Benchmarking long-form generation in long context llms. arXiv preprint arXiv:2409.02076, 2024. 569 Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In ICLR. OpenReview.net, 2022. 570 Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025. 571 Haocheng Xi, Han Cai, Ligeng Zhu, Yao Lu, Kurt Keutzer, Jianfei Chen, and Song Han. Coat: Compressing optimizer states and activation for memory-efficient fp8 training. arXiv preprint arXiv:2410.19313, 2024. 572 Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: survey. CoRR, abs/2309.07864, 2023. doi: 10.48550/ARXIV.2309.07864. URL https://doi.org/10.48550/arXiv.2309.07864. 573 Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: survey. arXiv preprint arXiv: 2309.07864, 2023. 574 Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. CoRR, abs/2407.01489, 2024. doi: 10.48550/ARXIV.2407.01489. URL https://doi.org/10.48550/arXiv.2407.01489. 575 Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. In ACL (Findings), 2024. 119 576 Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. 577 Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: Selecting influential data for targeted instruction tuning. In International Conference on Machine Learning (ICML), 2024. 578 Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023. 579 Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads, 2024. URL https://arxiv.org/abs/2410.10819. 580 Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NG7sS51z VF. 581 Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In ICLR. OpenReview.net, 2024. 582 Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/202 4/hash/5d413e48f84dc61244b6be550f1cd8f5-Abstract-Datasets_and_Benchm arks_Track.html. 583 Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. 584 Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 46434663, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024 .naacl-long.260. URL https://aclanthology.org/2024.naacl-long.260. 585 Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr√∂mformer: nystr√∂m-based algorithm for approximating self-attention. In AAAI, pages 1413814148. AAAI Press, 2021. 120 586 Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, and Dimitris Papailiopoulos. From artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data. arXiv preprint arXiv:2406.19292, 2024. 587 Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. arXiv preprint arXiv: 2310.04408, 2023. 588 Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. critical evaluation of evaluations for long-form question answering. arXiv preprint arXiv:2305.18201, 2023. 589 Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu Feng, Shixuan Sun, Changxu Shao, Yuhong Guo, et al. vtensor: Flexible virtual tensor management for efficient llm serving. arXiv preprint arXiv:2407.15309, 2024. 590 Peng Xu, Wei Ping, Xianchao Wu, Zihan Liu, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa 2: Bridging the gap to proprietary llms in long context and rag capabilities. ArXiv, abs/2407.14482, 2024. URL https://api.semanticscholar.org/CorpusID:271310 244. 591 Xiaoyue Xu, Qinyuan Ye, and Xiang Ren. Stress-testing long-context language models with lifelong icl and task haystack. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 592 Zhe Xu, Jiasheng Ye, Xiangyang Liu, Tianxiang Sun, Xiaoran Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, and Xipeng Qiu. Detectiveqa: Evaluating long-context reasoning on detective novels. arXiv preprint arXiv:2409.02465, 2024. 593 ZHAO XUANLEI, Bin Jia, Haotian Zhou, Ziming Liu, Shenggan Cheng, and Yang You. Hetegen: Efficient heterogeneous parallel inference for large language models on resourceconstrained devices. Proceedings of Machine Learning and Systems, 6:162172, 2024. 594 Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos. CoRR, abs/2408.10188, 2024. doi: 10.48550/ARXIV.2408.10188. URL https://doi.org/10.48550/arXiv.2408.10188. 595 Ai Ming Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Hai Zhao, Hang Xu, Hao-Lun Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kuncheng Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, Mingan Lin, Nuolan Nie, Pei Guo, Ruiyang Sun, Zhang Tao, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yan-Bin Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. ArXiv, abs/2309.10305, 2023. 596 An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et al. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025. 597 Bowen Yang, Bharat Venkitesh, Dwarak Talupuru, Hangyu Lin, David Cairuz, Phil Blunsom, and Acyr Locatelli. Rope to nope and back again: new hybrid attention strategy, 2025. URL https://arxiv.org/abs/2501.18795. 598 Dongjie Yang, Xiaodong Han, Yan Gao, Yao Hu, Shilin Zhang, and Hai Zhao. PyramidInfer: Pyramid KV cache compression for high-throughput LLM inference. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 32583270, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.195. URL https: //aclanthology.org/2024.findings-acl.195/. 599 Jian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, and Junyang Lin. Evaluating and aligning codellms on human preference. arXiv preprint arXiv:2412.05210, 2024. 600 Jian Yang, Jiajun Zhang, Jiaxi Yang, Ke Jin, Lei Zhang, Qiyao Peng, Ken Deng, Yibo Miao, Tianyu Liu, Zeyu Cui, et al. Execrepobench: Multi-level executable code completion evaluation. arXiv preprint arXiv:2412.11990, 2024. 601 John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/5a7c947568c1b1328 ccc5230172e1e7c-Abstract-Conference.html. 602 Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 4393 4479. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-M AIN.296. URL https://doi.org/10.18653/v1/2022.emnlp-main.296. 603 Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, and Zhihao Jia. Tidaldecode: Fast and accurate llm decoding with position persistent sparse attention, 2024. URL https: //arxiv.org/abs/2410.05076. 604 Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. CoRR, abs/2412.04467, 2024. doi: 10.48550/ARXIV.2412.04467. URL https://doi.org/10.485 50/arXiv.2412.04467. 605 Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080, 2025. 606 Yi Yang, Wen-tau Yih, and Christopher Meek. Wikiqa: challenge dataset for open-domain question answering. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 20132018, 2015. 607 Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, and Chuang Gan. Vca: Video curious agent for long video understanding. CoRR, abs/2412.10471, 2024. doi: 10.48550/ARXIV.2 412.10471. URL https://doi.org/10.48550/arXiv.2412.10471. 608 Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop 122 question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. 609 Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J. Smola, and Eduard H. Hovy. Hierarchical attention networks for document classification. In HLT-NAACL, pages 14801489. The Association for Computational Linguistics, 2016. 610 Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. 611 Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:2716827183, 2022. 612 Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. CoRR, abs/2408.04840, 2024. doi: 10.48550/ARXIV.2408.04840. URL https://doi.org/10.48550/arXiv.2408.04840. 613 Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024. 614 Lu Ye, Ze Tao, Yong Huang, and Yang Li. Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1160811620, 2024. 615 Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, and Danqi Chen. Longproc: Benchmarking long-context language models on long procedural generation. arXiv preprint arXiv:2501.05414, 2025. 616 Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly. arXiv preprint arXiv:2410.02694, 2024. 617 Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Tong Xu, Xing Sun, Ran He, Caifeng Shan, and Enhong Chen. T2vid: Translating long text into multi-image is the catalyst for video-llms. CoRR, abs/2411.19951, 2024. doi: 10.48550/ARXIV.2411.19951. URL https://doi.org/10.48550/arXiv.2411.19951. 618 Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. CompAct: Compressing retrieved documents actively for question answering. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2142421439, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp -main.1194. URL https://aclanthology.org/2024.emnlp-main.1194/. 619 Chengye Yu, Tianyu Wang, Zili Shao, Linjie Zhu, Xu Zhou, and Song Jiang. Twinpilots: new computing paradigm for gpu-cpu parallel llm inference. In Proceedings of the 17th ACM International Systems and Storage Conference, pages 91103, 2024. 123 620 Haofei Yu, Cunxiang Wang, Yue Zhang, and Wei Bi. TRAMS: training-free memory selection for long-range language modeling. In EMNLP (Findings), pages 49664972. Association for Computational Linguistics, 2023. 621 Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, and Qianru Sun. Framevoyager: Learning to query frames for video large language models. CoRR, abs/2410.03226, 2024. doi: 10.48550/ARXIV.2410.03226. URL https://doi.org/10.48550/arXiv.241 0.03226. 622 Tan Yu, Anbang Xu, and Rama Akkiraju. In defense of RAG in the era of long-context language models. CoRR, abs/2409.01666, 2024. doi: 10.48550/ARXIV.2409.01666. URL https://doi.org/10.48550/arXiv.2409.01666. 623 Tao Yu, Gaurav Gupta, Karthick Gopalswamy, Amith Mamidala, Hao Zhou, Jeffrey Huynh, Youngsuk Park, Ron Diamant, Anoop Deoras, and Luke Huan. Collage: Light-weight low-precision strategy for llm training. arXiv preprint arXiv:2405.03637, 2024. 624 Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. Wordcraft: story writing with large language models. In Proceedings of the 27th International Conference on Intelligent User Interfaces, pages 841852, 2022. 625 Danlong Yuan, Jiahao Liu, Bei Li, Huishuai Zhang, Jingang Wang, Xunliang Cai, and Dongyan Zhao. Remamba: Equip mamba with effective long-sequence modeling, 2025. URL https://arxiv.org/abs/2408.15496. 626 Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention, 2025. URL https://arxiv.org/abs/2502.11089. 627 Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. 628 Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. Lv-eval: balanced long-context benchmark with 5 length levels up to 256k, 2024. 629 Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, and Liqiang Nie. Wkvquant: Quantizing weight and key/value cache for large language models gains more. arXiv preprint arXiv:2402.12065, 2024. 630 Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta√±√≥n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips. cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html. 631 Andy Zeng, Adrian S. Wong, Stefan Welker, K. Choromanski, F. Tombari, Aveek Purohit, M. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Peter R. Florence. Socratic 124 models: Composing zero-shot multimodal reasoning with language. International Conference on Learning Representations, 2022. doi: 10.48550/arXiv.2204.00598. 632 Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities? arXiv preprint arXiv:2502.12215, 2025. 633 Alexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, Yuanxing Zhang, Zhexu Wang, Weixun Wang, Yancheng He, Ken Deng, Wangchunshu Zhou, Wenhao Huang, and Zhaoxiang Zhang. Codecriticbench: holistic code critique benchmark for large language models. 2025. 634 Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, JianGuang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 24712484. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.151. URL https://doi.org/ 10.18653/v1/2023.emnlp-main.151. 635 Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024. 636 Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu. Improving the transformer translation model with document-level context. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 533542. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1049. URL https://doi.org/10.18653/v1/d18-1049. 637 Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, et al. Longcite: Enabling llms to generate fine-grained citations in long-context qa. arXiv preprint arXiv:2409.02897, 2024. 638 Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin Niu, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li. Longreward: Improving long-context large language models with ai feedback. arXiv preprint arXiv:2410.21252, 2024. 639 Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. PEGASUS: pre-training In Proceedings of the 37th with extracted gap-sentences for abstractive summarization. International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1132811339. PMLR, 2020. URL http://proceedings.mlr.press/v119/zhang20ae.html. 640 Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. Lightthinker: Thinking step-by-step compression. arXiv preprint arXiv:2502.15589, 2025. 641 Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft& verify: Lossless large language model acceleration via self-speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), August 2024. 125 642 Kechi Zhang, Ge Li, Huangzhao Zhang, and Zhi Jin. Hirope: Length extrapolation for code models using hierarchical position. arXiv preprint arXiv:2403.19115, 2024. 643 Lei Zhang, Yunshui Li, Ziqiang Liu, Junhao Liu, Longze Chen, Run Luo, Min Yang, et al. Marathon: race through the realm of long context with large language models. arXiv preprint arXiv:2312.09542, 2023. 644 Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlmxcomposer-2.5: versatile large vision language model supporting long-contextual input and output. CoRR, abs/2407.03320, 2024. doi: 10.48550/ARXIV.2407.03320. URL https://doi.org/10.48550/arXiv.2407.03320. 645 Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. Long context compression with activation beacon. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=1eQT9Ozf NQ. 646 Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, and Zhiming Zheng. Adacomp: Extractive context compression with adaptive predictor for retrieval-augmented large language models, 2024. URL https://arxiv.org/abs/2409.01579. 647 Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/2205.01068. 648 Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. 649 Tianyi Zhang, Jonah Yi, Zhaozhuo Xu, and Anshumali Shrivastava. Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization. Advances in Neural Information Processing Systems, 37:33043331, 2024. 650 Wei Zhang, Chaoqun Wan, Yonggang Zhang, Yiu-Ming Cheung, Xinmei Tian, Xu Shen, and Jieping Ye. Interpreting and improving large language models in arithmetic calculation. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5993259950. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/zhang24bk.h tml. 651 Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, et al. Tablellm: Enabling tabular data manipulation by llms in real office usage scenarios. arXiv preprint arXiv:2403.19318, 2024. 652 Xingxing Zhang, Furu Wei, and Ming Zhou. HIBERT: document level pre-training of hierarchical bidirectional transformers for document summarization. In Anna Korhonen, David R. Traum, and Llu√≠s M√†rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 126 1: Long Papers, pages 50595069. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1499. URL https://doi.org/10.18653/v1/p19-1499. 653 Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, et al. Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1526215277, 2024. 654 Xuan Zhang, Fengzhuo Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, and Min Lin. Lighttransfer: Your long-context LLM is secretly hybrid model with effortless adaptation. In Workshop on Reasoning and Planning for Large Language Models, 2025. URL https://openreview.net/forum?id=DfgfGTfObm. 655 Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Maria Christakis and Michael Pradel, editors, Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2024, Vienna, Austria, September 16-20, 2024, pages 15921604. ACM, 2024. doi: 10.1145/3650212.3680384. URL https://doi.org/10.1145/3650212.3680384. 656 Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan √ñ. Arik. Chain of agents: Large language models collaborating on long-context tasks. arXiv preprint arXiv: 2406.02818, 2024. 657 Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. 658 Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, Zhangyang \"Atlas\" Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3466134710. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/fi le/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf. 659 Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, and Zhangyang Wang. Found in the middle: How language models use long contexts better via plug-and-play positional encoding. arXiv preprint arXiv:2403.04797, 2024. 660 Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, and Tat-Seng Chua. Analyzing temporal complex events with large language models? benchmark towards temporal, long context understanding. arXiv preprint arXiv:2406.02472, 2024. 661 Chenggang Zhao, Liang Zhao, Jiashi Li, and Zhean Xu. Deepgemm: clean and efficient fp8 gemm kernels with fine-grained scaling. https://github.com/deepseek-ai/DeepGE MM, 2025. 662 Hanyu Zhao, Zhi Yang, Yu Cheng, Chao Tian, Shiru Ren, Wencong Xiao, Man Yuan, Langshi Chen, Kaibo Liu, Yang Zhang, et al. Goldminer: Elastic scaling of training data pre-processing pipelines for deep learning. Proceedings of the ACM on Management of Data, 1 (2):125, 2023. 663 Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, and Xuanjing Huang. Longagent: Scaling language models to 128k context through multi-agent collaboration. arXiv preprint arXiv: 2402.11550, 2024. 127 664 Liang Zhao, Xiachong Feng, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin, and Ting Liu. Length extrapolation of transformers: survey from the perspective of positional encoding. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 99599977, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.582. URL https://aclanthology.o rg/2024.findings-emnlp.582/. 665 Liang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo Zhu, et al. Longskywork: training recipe for efficiently extending context length in large language models. arXiv preprint arXiv:2406.00605, 2024. 666 Tiancheng Zhao, Qianqian Zhang, Kyusong Lee, Peng Liu, Lu Zhang, Chunxin Fang, Jiajia Liao, Kelei Jiang, Yibo Ma, and Ruochen Xu. Omchat: recipe to train multimodal language models with strong long context and video understanding. CoRR, abs/2407.04923, 2024. doi: 10.48550/ARXIV.2407.04923. URL https://doi.org/10.48550/arXiv.2407.04923. 667 Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. 668 Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196209, 2024. 669 Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024. 670 Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, et al. Dape v2: Process attention score as feature map for length extrapolation. arXiv preprint arXiv:2410.04798, 2024. 671 Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, et al. Dape: Data-adaptive positional encoding for length extrapolation. Advances in Neural Information Processing Systems, 37:2665926700, 2025. 672 Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mtbench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 673 Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody_Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Efficiently programming large language models using sglang. 2023. 674 Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 59055921, 2021. 128 675 Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1972419731. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29946. URL https://doi.org/10.1609/aaai.v38i17.29946. 676 Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193210, 2024. 677 Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. MLVU: comprehensive benchmark for multi-task long video understanding. CoRR, abs/2406.04264, 2024. doi: 10.48550/ARXIV.2406.04264. URL https://doi.org/10.48550/arXiv.2406.04264. 678 Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=oKn9c6ytLx. 679 Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan. Efficient prompting via dynamic in-context learning, 2023. URL https://arxiv.org/abs/2305 .11170. 680 Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long text. arXiv preprint arXiv: 2305.13304, 2023. 681 Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous language agents, 2023. URL https: //arxiv.org/abs/2309.07870. 682 Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An opensource framework for autonomous language agents. CoRR, abs/2309.07870, 2023. doi: 10.48550/ARXIV.2309.07870. URL https://doi.org/10.48550/arXiv.2309.07870. 683 Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic learning enables self-evolving agents. CoRR, abs/2406.18532, 2024. doi: 10.48550/ARXIV.2406.18532. URL https://doi.org/10.48550/arXiv.2406.18532. 684 Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, and Liang Ding. Dynamickv: Task-aware adaptive kv cache compression for long context llms, 2025. URL https://arxiv.org/abs/2412.14838. 685 Zhejian Zhou, JIayu Wang, Dahua Lin, and Kai Chen. Scaling behavior for large language models regarding numeral systems: An example using pythia. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 38063820, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.218. URL https: //aclanthology.org/2024.findings-emnlp.218/. 686 Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. In The Twelfth International Conference on Learning Representations, 2023. 687 Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Longembed: Extending embedding models for long context retrieval. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 802816. Association for Computational Linguistics, 2024. URL https://aclantho logy.org/2024.emnlp-main.47. 688 Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. LongEmbed: Extending embedding models for long context retrieval. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 802816, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.47. URL https://aclanthology.org/2024.emnlp-main.47. 689 Yuke Zhu, Chi Xie, Shuang Liang, Bo Zheng, and Sheng Guo. Focusllava: coarse-to-fine approach for efficient and effective visual token compression. CoRR, abs/2411.14228, 2024. doi: 10.48550/ARXIV.2411.14228. URL https://doi.org/10.48550/arXiv.2411.14 228. 690 Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: survey. CoRR, abs/2308.07107, 2023. doi: 10.48550/ARXIV.2308.07107. URL https: //doi.org/10.48550/arXiv.2308.07107. 691 Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and J√ºrgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=uTC9AFXIhg. 692 Heqing Zou, Tianze Luo, Guiyang Xie, Fengmao Lv, Guangcong Wang, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang, et al. Hlv-1k: large-scale hour-long video benchmark for time-specific long video understanding. CoRR, abs/2501.01645, 2025. doi: 10.48550/ARXIV.2501.01645. URL https://doi.org/10.48550/arXiv.2501.01 645. 693 Kaijian Zou, Muhammad Khalifa, and Lu Wang. Retrieval or global context understanding? on many-shot in-context learning for long-context evaluation. arXiv preprint arXiv:2411.07130, 2024."
        }
    ],
    "affiliations": [
        "Alibaba",
        "ByteDance",
        "CASIA",
        "Kuaishou",
        "M-A-P",
        "NJU",
        "PKU",
        "Tencent"
    ]
}