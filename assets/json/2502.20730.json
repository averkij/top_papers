{
    "paper_title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
    "authors": [
        "Zhuoqun Li",
        "Haiyang Yu",
        "Xuanang Chen",
        "Hongyu Lin",
        "Yaojie Lu",
        "Fei Huang",
        "Xianpei Han",
        "Yongbin Li",
        "Le Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications."
        },
        {
            "title": "Start",
            "content": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking Zhuoqun Li1,2, Haiyang Yu3, Xuanang Chen1, Hongyu Lin1, Yaojie Lu1, Fei Huang3, Xianpei Han1, Yongbin Li3, Le Sun1 1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3Tongyi Lab {lizhuoqun2021,chenxuanang,hongyu,luyaojie}@iscas.ac.cn {xianpei,sunle}@iscas.ac.cn {yifei.yhy,f.huang,shuide.lyb}@alibaba-inc.com Abstract 5 2 0 2 8 2 ] . [ 1 0 3 7 0 2 . 2 0 5 2 : r Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce new benchmark, SolutionBench, to evaluate systems ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications. https: //github.com/Li-Z-Q/DeepSolution."
        },
        {
            "title": "Introduction",
            "content": "Designing solutions for complex engineering requirements is crucial work in human production activities (Ogot and Kremer, 2004; ElMaraghy et al., 2012). These requirements typically include multiple real-world constraints and expect complete and feasible solutions (e.g., Design safe and efficient hospital construction plan in an area with annual rainfall of 3000 millimeters, expansive soil conditions, and frequent seismic activity). Human experts complete such work by consulting extensive professional knowledge, carefully designing, and strictly deliberating, which require significant time and human resources (Kalogerakis et al., 2010; De Weck et al., 2011). With the continuous development of retrieval-augmented generation (RAG) techniques, the engineering fields anticipate credible RAG system that can automatically generate 1 Figure 1: This paper proposes the complex engineering solution design task and new system that can generate reliable solutions via the bi-point thinking tree. reliable solutions for these complex engineering requirements (Yu et al., 2024; Zhou et al., 2024). Unfortunately, prior works in RAG field do not sufficiently research the complex engineering solution design task. Existing relevant papers mainly focus on Long-form QA or Multi-hop QA (Zhu et al., 2024; Tan et al., 2024), where the questions are integrated or composed of multiple sub-questions and the expected answers are typically assembled knowledge paragraphs or entity fragments. Unlike these tasks, requirements of the complex engineering solution design task involve multiple real-world constraints and demand complete and feasible solutions (Fortus et al., 2005; Jonassen et al., 2006), as shown in Figure 1. Therefore, researching complex engineering solution design based on RAG technology is valuable gap that needs to be filled. To fill this gap, we first introduce new benchmark, SolutionBench, to evaluate whether system can generate complete and feasible solutions for complex engineering requirements with multiple constraints. Firstly, to ensure the data sources authority, authenticity, and diversity, we collect thousands of engineering reports about solution design from authoritative journals in various engineering domains. Then, to build data that is convenient for testing and evaluation, we refer to the generative information extraction technologies (Lu et al., 2022; Zhang et al., 2025) and employ LLMs to extract useful content from these reports based on manually formatted template, capturing realworld complex requirements, expert-authored solutions, analytical knowledge used to interpret the requirements, technical knowledge applied in addressing the requirements, and explanations for the experts solution design process. Finally, we manually verify and revise the extracted content, merge all knowledge within the same domain into unified knowledge base, and then harvest complete benchmark for complex engineering solution design that covers eight engineering domains. To further advance complex engineering solution design, we propose SolutionRAG, which can generate reliable solutions through tree-based exploration and bi-point thinking. Firstly, the improvement process from suboptimal solutions to reliable solutions is flexible, rather than with fixed reasoning pattern. Therefore, SolutionRAG conducts the tree-based exploration, where each branch represents different improvement direction. Secondly, due to the presence of multiple realworld constraints within the requirements, systemgenerated solutions cannot guarantee the satisfaction of all constraints. Therefore, SolutionRAG employs the bi-point thinking, which alternates between solution designing and reviewing during the tree growth, gradually improving reliability of generated solutions. Finally, to balance inference efficiency and performance, SolutionRAG implements pruning based on node evaluation, which can keep the inference process along the most promising solutions and the most helpful reviewed comments. In experiments, we evaluate various types of methods on SolutionBench to assess their ability in complex engineering solution design, including deep thinking models without RAG, standard RAG approaches, multi-round iterative RAG methods, and our SolutionRAG. Experimental results show that LLMs relying solely on internal knowledge cannot effectively solve such tasks. Previous RAG methods also fail to generate satisfactory solutions. In contrast, our proposed SolutionRAG proves to be more advanced approach. The main contributions of this paper can be summarized as follows: We construct SolutionBench, which can evaluate systems ability for complex engineering solution design from real-world scenarios. We propose SolutionRAG, which can boost complex engineering solution design through tree-based exploration and bi-point thinking. We conduct extensive experiments, and results show existing methods perform poorly and SolutionRAG is an advanced improvement."
        },
        {
            "title": "2 SolutionBench",
            "content": "As mentioned above, research on complex engineering solution design tasks has significant value in enhancing the productivity of human society, but previous works in RAG field do not explore this in depth. Therefore, this paper introduces new benchmark, SolutionBench, which can evaluate systems ability to design solutions for complex engineering requirements. Specifically, as illustrated in Figure 2, we first collect engineering technical reports about complex solution design from authoritative journals across various engineering fields. Then, based on manually formatted extraction templates, we use powerful LLMs to implement useful content extraction. Finally, after manually checking and removing redundancy, the extracted content is integrated into complete benchmark. Here is detailed process of constructing SolutionBench:"
        },
        {
            "title": "2.1 Authoritative Data Source",
            "content": "To ensure the credibility of benchmark, we primarily consider two key factors when determining data sources: the authority and authenticity of data, as well as the diversity of engineering domains. Authority and Authenticity. In order to ensure the benchmarks evaluation results can accurately reflect the systems capabilities under real engineering requirements, it is essential to ensure the data sources come from authoritative experts and real-world scenarios. To this end, we select authoritative journals in engineering fields as data sources, choosing engineering reports that involve complex engineering solution design. The requirements in these reports are derived from real industrial scenarios and provided by industry experts under strict peer review, thus ensuring the authenticity and authority of data sources. The detailed list of used engineering journals is shown in Appendix A. Domain Diversity. Since the need for complex engineering solution design is urgent in multiple 2 Figure 2: Illustration of the SolutionBench construction method, which includes collecting technology reports from engineering journals to ensure authority and authenticity, extracting useful content based on manually formatted template and powerful LLMs, and finally harvesting the benchmark after manual verification and merging. engineering domains, the data sources used to construct benchmark must cover broad range of domains to ensure comprehensive evaluation. To this end, we select authoritative journals from eight major categories based on the discipline classification mechanism of the search websites: Environment, Mining, Transportation, Aerospace, Telecom, Architecture, Water Resource, and Farming. The coverage of these fields ensures that the data sources include diverse engineering scenarios, providing broad reference for system evaluation."
        },
        {
            "title": "2.2 Template-based Extraction via LLM",
            "content": "To transform original engineering technical reports into data for evaluation and scoring, we format template manually and extract following content from each report via LLMs: requirement, solution, analytical knowledge, technical knowledge, and explanation, based on the generative information extraction (Lu et al., 2022; Zhang et al., 2025). Template. In order to facilitate the testing and scoring, we format an extraction template including following keys: (1) Requirement, which refers to the complex needs from real engineering scenarios addressed in reports, (2) Solution, which is the complete and reliable solution designed by top industry experts, (3) Analytical Knowledge, which is the professional knowledge used by experts when analyzing the complex requirements during solution design process (e.g., Impact of earthquakes is mainly reflected in horizontal vibration), (4) Technical Knowledge, which is the professional knowledge used by experts to address the complex requirements and develop the complete solutions (e.g., Nano bearings can reduce the horizontal seismic vibration by special structure), (5) Explanation, which outlines how the experts use analytical knowledge and technical knowledge to analyze the complex requirements and gradually design complete solutions. This explanation can serve as an auxiliary reference during the evaluation process. The complete template used to implement the extraction process is shown in the Appendix B. Extraction Process. Since the original engineering reports are in PDF format and cannot be directly processed for content extraction, we first use the marker tool1 to convert the PDF files into plain text. And then we input the plain text along with the manually formatted template into GPT-4o (OpenAI, 2024a), extracting content as described in the template. Finally we transform extracted content into JSON format and save it for further process."
        },
        {
            "title": "2.3 Manual Data Verification",
            "content": "To further ensure the credibility of the benchmark, we manually check correctness and remove the redundancy for the extracted content. Correctness Checking. Since the LLM is probabilistic model and cannot guarantee that every extracted piece of content aligns with our specifications, we manually check each extracted content. On one hand, we examine whether the content matches the information in original engineering reports, on the other hand, we assess whether the content adheres to definitions in the template. For incorrect content, we directly correct it manually. Redundancy Removing. Since we select many technical reports as data sources for each engineering domain, the analytical knowledge and technical knowledge used to address complex requirements from the same domain may be similar or even identical, resulting in redundancy when constructing large knowledge base. Therefore, we manually check duplicates for the knowledge in each domain. If duplicates are found, we manually merge the redundant knowledge to one knowledge. 1https://github.com/VikParuchuri/marker"
        },
        {
            "title": "Engineering Domain",
            "content": "# Datapoint # Knowledge Environment (Env.) Mining (Min.) Transportation (Tra.) Aerospace (Aer.) Telecom (Tel.) Architecture (Arc.) Water Resource (Wat.) Farming (Far.) 119 117 124 115 116 118 119 122 554 543 870 802 840 858 802 868 Table 1: Statistics of the SolutionBench, which include data and knowledge across eight engineering domains. The number of datapoints in dataset and the number of knowledge in knowledge base are shown above."
        },
        {
            "title": "2.4 Datapoint and Knowledge Base",
            "content": "After above manual verification, we do content integrate and get 8 high-quality datasets for the 8 domains, correspondingly with 8 knowledge base. The detailed statistics of benchmark is in Table 1. Datapoint Format. The content of datapoints of every domain is as following formula: = {qi, si, {k(a) }Ai j=1, {k(t) }Ti j=1, ei}N i=1 (1) where is the dataset for one domain, is data number, qi is one requirement, si is the goldn solution, k(a) is an analytical knowledge used for qi and Ai is the total number, k(t) is technical knowledge used for qi and Ti is the total number. Knowledge Base. In order to get the referential knowledge base for each engineering domain, we and k(t) collect all the k(a) within the same domain into large corpus, as shown in following: K = [{k(a) }Ai j=1, {k(t) }Ti j=1] = {ki}M i=1 (2) where is the knowledge base for one domain, and is the number of knowledge in K. Evaluation Formulating. There are two ways to using SolutionBench for evaluation. The first one is that given requirement and expect an reliable solution ˆs, as shown in following formula: ˆs = F(q) (3) And the second one is RAG setting, which extra provides the relevant knowledge base for retrieval and augmentation, as shown in following formula: Since the completion of above tasks requires various engineering expertise, which is prone to hallucination issues in regular-sized LLMs (Jiang et al., 2023), we mainly focus on the RAG setting in this paper. At the same time, we also test some powerful deep reasoning LLMs in experiments without using RAG, the details are in Section 4."
        },
        {
            "title": "3 SolutionRAG",
            "content": "To further advance research in complex engineering solution design, we propose SolutionRAG, system that can generate reliable solutions through tree-based exploration and bi-point thinking. Specially, as illustrated in Figure 3, since the improvement process from suboptimal solution to reliable one is flexible and lacks fixed reasoning pattern, SolutionRAG performs tree-base exploration to find the most effective improvement process for each input requirement. Moreover, due to the multiple real-world constraints within the requirements, the system cannot directly guarantee the generated solutions satisfy all constraints. Therefore, SolutionRAG employs bi-point thinking approach, alternating between solution design and review, gradually enhancing the solutions completeness and reliability. Finally, to balance inference performance and efficiency, SolutionRAG employs node evaluation to prune the tree, ensuring that the inference process follows the most promising solutions and the most helpful reviewed comments."
        },
        {
            "title": "3.1 Bi-point Thinking Tree",
            "content": "To explore the optimal process for solution improvement during inference and ensure the output solutions meet all constraints in the requirements, SolutionRAG performs inference based on bipoint thinking tree, which consists of alternating connected solution nodes and comment nodes. Solution Node. The content within solution nodes is the solution designed for the given requirement, which is expected to be complete and feasible solution meeting all constraints specified in the requirement. The solution nodes at the shallower levels of the tree typically have lower degree of reliability for the given requirement, while those at deeper levels have higher degree of reliability. For convenience, we use s(i) represents the j-th solution node at the i-th layer of the tree. ˆs = F(q, K) Comment Node. The content within comment node is the comment obtained from reviewing (4) Figure 3: Illustration of SolutionRAG, we set the child number of each node as 2 for easy presentation above. SolutionRAG uses tree-based exploration to find optimal solution improvement process, bi-point thinking to guarantee generated solutions satisfy all constraints, and pruning mechanism to balance efficiency and performance. particular solution, which indicates the aspects in which the solution still has deficiencies with respect to the given requirement. For convenience of description, we use c(i+1) represents the j-th comment node at the (i + 1)-th layer of the tree. Tree Structure. The above-mentioned solution nodes and comment nodes are alternately connected to form bi-point thinking tree, where the child nodes of solution node are comment nodes, and the child nodes of comment node are solution nodes, as shown in the following formula: {c(i+1) s(i) }H h=1 c(i+1) {s(i+2) }H h=1 (5) (6) where is the number of child node in tree. The content of the root node of the tree is the requirement q, so is at least one in above formula."
        },
        {
            "title": "3.2 Solution Improvement via Tree Growth",
            "content": "In this section, we introduce how SolutionRAG specifically achieves continuous improvement of solutions through the growth process of aforementioned bi-point thinking tree, including the node expansion and node evaluation process. Node Expansion. During the growth process of the bi-point thinking tree, there are two types of node expansion actions, one is based on the requirement or comment node to design new solution nodes, and the other is based on reviewing the solution node to create new comment nodes. (1) Design. Given the requirement and speas input (if is at least one), cific comment c(i+1) the design process generate proposals {ph}H h=1 through random sampling using LLM, representing different directions for designing: {ph}H h=1 = LLM(q, c(i+1) ) (7) Then, small-scale relevant knowledge Kh is retrieved from the knowledge base for each ph: Kh = Retrieval(ph, K) = {kr}R r=1 (8) Finally, q, c(i+1) , Kh, and the history solution s(i) are concatenated as input, allowing the LLM to output more refined new solution: s(i+2) = LLM(q, s(i) , c(i+1) , Kh) (9) as empty text. and c(i+1) Thus, we obtain new solutions {s(i+2) }H h=1 refined based on the comment c(i+1) . Note that during the generation of solution nodes in the first layer, there are no previous solutions or comments, so we initialize s(i) (2) Review. Similar to the previous process, the review process also consists of three steps: First, proposals {ph}H h=1 are generated based on and s(i) , representing distinct review directions. Next, knowledge Kh is retrieved for each ph. Fih=1 are generated for s(i) nally, comments {c(i+1) }H based on q, s(i) , and Kh. The maximum depth of the bi-point thinking tree, denoted as L, is hyperparameter. Prompts for this part are Appendix C. Node Evaluation. As described in above node expansion part, the number of nodes becomes enormous as the tree grows, leading to significant time 5 consumption during inference. To this end, during tree growth, we do prune by each node score from its child nodes, meaning whether s(i) is an reliable h=1 and whether c(i+1) solution based on {c(i+1) }H is helpful comment for solution improvement based on {s(i+2) }H h=1. Specifically, for judging , c(i+1) , we put s(i) s(i) , and suffix us together as the LLM input, and get the reliability score Jh(s(i) ) by calculating LLM-predicted average logits of us: h Jh(s(i) ) = Logits(uss(i) , c(i+1) ) (10) by average all {Jh(s(i) where us is According to the comment, above solution is reliable. And then get final score (s(i) ) for s(i) h=1. Similarly, for judging c(i+1) , and uc as input, and get the helpfulness score Jh(c(i+1) ) by calculating LLM-predicted average logits: )}H , c(i+1) , we use s(i) , s(i+2) Jh(c(i+1) ) = Logits(ucs(i) , c(i+1) , s(i+2) ) (11) where uc is Comparing the new solution and old solution, the comment is helpful, and get (c(i+1) ) after same averaging process. During the tree growth, for each layer we only keep the highest-scoring nodes, aiming to keep the inference process always focus on the most promising solutions and the most helpful reviewed comments, thus achieving balance between efficiency and performance. The nodes that are used in final inference process are called retained nodes, while those not-used are pruned nodes."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Evaluation Metrics. Since expected system output in SolutionBench are solutions that may have various textual expressions, rule-based metrics are difficult to provide score that aligns with human habits (Xu et al., 2023; Mayfield et al., 2024). To this end, we follow metrics of previous Long-form QA evaluation methods (Tan et al., 2024; Wang et al., 2024a), using GPT-4o2 as score evaluator to compute two scores, (1) Analytical Score: We integrate the expert-designed solution, analytical knowledge used by experts, and the explanation as reference, and then let evaluator judge whether the system-designed solution, like the expert-designed one, uses the correct analytical knowledge to adequately analysis the complex constraints in requirements. (2) Technical Score: Similarly, we integrate the expert-designed solution, technical knowledge used by experts, and the explanation as reference, and then let evaluator judge whether the system-designed solution, like the expert-designed one, uses the correct technical knowledge to tackle the complex constraints in requirements. Both analytical score and technical score are range from 0 to 100. Prompts for this part are in Appendix D. Selected Baselines. In order to comprehensively evaluate the abilities of various types of systems in solving complex engineering solution design tasks, we extensively select multiple types of methods as baselines in the experiments. Specifically, (1) Deep Reasoning Models: This type includes models like o1-2024-12-17 (OpenAI, 2024b), GLM-Zero-Preview (Zhipu, 2024), and QwQ-32B-Preview (Qwen, 2024), which possess strong long-chain reasoning capabilities, but do not utilize external knowledge like RAG. (2) Singleround RAG Methods: These methods perform only one round of retrieval and generation, where Naive-RAG (Lewis et al., 2020) does not process the retrieval results, while Rerank-RAG (Li et al., 2023) uses an additional model to re-rank the retrieval results. (3) Multi-round RAG Methods: These methods conduct multiple rounds of RAG, iteratively performing tasks such as question rewriting, retrieval, filtering, and generating intermediate answer. We choose 3 accredited methods, which are Self-RAG (Asai et al., 2024), GenGround (Shi et al., 2024), and RQ-RAG (Chan et al., 2024). Implementation Details. For deep reasoning models in baselines, we directly use official API for experiments. For the single-round and multiround RAG methods, we follow their official process. For SolutionRAG, we set maximum tree depth as 5, number of child per node as 2, and number of retained node in pruning as 1. To ensure fair comparison, we adopt the following same implementation setting for SolutionRAG and all RAG-based methods in baselines: base model is Qwen2.5-7B-Instruct (Team, 2024), retrieval model is NV-Embed-v2 (Lee et al., 2025), and the number of retrieval results is setting as 10. For convenience, in all RAG-based experiments, we deploy the base model as API by vLLM3. 2https://openai.com/index/hello-gpt-4o/ 3https://pypi.org/project/vllm/"
        },
        {
            "title": "Method",
            "content": "Env. Min. Tra. Aer. Tel. Arc. Wat. Far. AS TS AS TS AS TS AS TS AS TS AS TS AS TS AS TS Deep Reasoning Models o1-2024-12-17 (OpenAI, 2024b) GLM-Zero-Preview (Zhipu, 2024) QwQ-32B-Preview (Qwen, 2024) 60.5 47.0 54.3 48.3 30.6 38.7 51.9 43.2 48.0 37.5 22.2 27. 57.3 45.2 47.2 44.7 27.0 29.3 57.8 42.3 47.4 47.6 25.7 31.9 63.5 45.1 52.2 52.3 31.7 35. 61.2 47.7 51.3 52.0 32.4 35.6 59.9 47.3 49.2 50.4 30.8 33.0 62.9 51.4 53.4 52.2 36.6 37. Naïve-RAG (Lewis et al., 2020) Rerank-RAG (Li et al., 2023) 64.8 62.7 62.2 60.7 57.2 53.4 40.1 38.4 62.7 60. 54.9 49.7 67.7 65.6 65.4 65.2 67.4 66.1 66.8 63.4 66.2 66. 63.3 62.8 66.0 64.1 57.5 55.4 65.7 64.0 63.0 59.7 Single-round RAG Methods Multi-round RAG Methods Self-RAG (Asai et al., 2024) GenGround (Shi et al., 2024) RQ-RAG (Chan et al., 2024) 64.2 54.8 53.5 63.6 46.1 44.4 56.1 53.0 48.9 41.6 33.3 28. 62.9 54.7 53.8 56.5 37.2 38.8 68.8 55.7 55.0 69.9 46.0 46.1 67.6 58.3 57.9 66.9 50.7 44. 66.7 60.1 56.3 65.9 50.7 46.9 64.8 60.4 54.3 58.6 48.9 39.8 65.1 59.8 57.2 61.1 52.7 45. SolutionRAG (Ours) 66.4 67.9 59.7 50.5 64. 58.5 69.9 72.7 68.8 69.0 67. 68.0 66.0 60.7 66.9 65.2 Tree-based Exploration and Bi-point Thinking Table 2: Main experimental results on SolutionBench with eight engineering domains, the AS is the analytical score and TS is the technical score. The table shows that previous methods perform poorly for complex engineering solution design. In contrast, our SolutionRAG is able to output more complete and reliable solutions. Method Env. Min. Tra. Aer. Tel. Arc. Wat. Far. Overall AS TS AS TS AS TS AS TS AS TS AS TS AS TS AS TS AS TS SolutionRAG w/o tree structure w/o bi-point thinking 66.4 63.5 62.8 67.9 66.5 64.7 59.7 57.3 55.6 50.5 46.2 47.3 64.1 63.1 61. 58.5 57.4 55.7 69.9 60.8 63.2 72.7 68.4 68.3 68.8 60.9 62.6 69.0 63.7 64.8 67.9 66.2 67. 68.0 67.2 67.3 66.0 65.6 64.4 60.7 59.9 59.1 66.9 64.2 65.2 65.2 63.9 64.7 66.2 62.7 62. 64.1 61.7 61.5 Table 3: Ablation results for tree-based exploration and bi-point thinking. The table shows that both mechanisms have obviously positive effects for SolutionRAG and exhibit similar level of importance in the overall."
        },
        {
            "title": "4.2 Overall Results",
            "content": "Results compared with baselines are shown in the Table 2, there are two main conclusions: Previous methods fail to effectively address the complex engineering solution design. The table shows that, on one hand, deep reasoning models without RAG perform poorly across all eight domains in SolutionBench. For example, GLMZero-Preview achieves an analytical score of only 42.3 in the aerospace domain. On the other hand, RAG-based methods achieve some better performance but still remain at relatively low levels. For instance, Naive-RAG obtains technical score of only 40.1 in the mining engineering domain, and Self-RAG achieves technical score of just 63.6 in the environmental engineering domain. In contrast, SolutionRAG is an effective system for complex engineering solution design tasks. The table shows that SolutionRAG achieves SOTA performance across all of eight domains in the benchmark, demonstrating significant improvement over baseline methods. For example, in the mining domain, SolutionRAG improves the technical score by 10.4 compared to Naive-RAG and by 8.9 compared to Self-RAG. These experimental results confirm that SolutionRAG can effectively handle complex solution design tasks in various real-world engineering scenarios."
        },
        {
            "title": "4.3 Ablation Results",
            "content": "Since tree-based exploration and bi-point thinking are two key mechanisms in SolutionRAG, we conduct two ablation experiments, results are shown in Table 3, where w/o tree structure is that each node generates only one child, resulting in singlechain inference pattern, and w/o bi-point thinking is that the tree does not include reviewing and all nodes are solutions, leading to uni-point thinking inference pattern. There are two main conclusions: Both of the tree-based exploration and bipoint thinking have positive effects. The table shows that removing either mechanism leads to significant decline in performance, indicating that these two mechanisms are indeed central to solving complex engineering solution design tasks. Tree-based exploration and bi-point thinking exhibit similar level of importance. The table shows that after removing these two mechanisms, overall performance decline is quite similar, indicating these two mechanisms hold comparable level of importance in SolutionRAG. 7 Figure 4: Performance changes during the tree growth. The figure shows that scores become higher as the tree grows, proving SolutionRAG can indeed improve the solution scores as inference being deep. Figure 5: Effectiveness of node evaluation mechanism. The figure shows that scores in retained nodes are higher than in pruned nodes, thus the node evaluation is an effective method for judging and pruning in SolutionRAG."
        },
        {
            "title": "4.4 Detailed Analysis",
            "content": "In order to further validate the SolutionRAG, we do some detailed analysis, including performance changeing during the tree growth process and effectiveness of the node evaluation in SolutionRAG. Performance during Tree Growth. To examine whether the solutions actually improve as the tree depth increases in SolutionRAG inference, we score the solutions from the layer-1, 3, and 5 of the tree. The experimental results are shown in Figure 4, performance gradually improves from the shallow layer to the deep layer, which proves that SolutionRAG can indeed improve the solution as inference process being deep. Effectiveness of Node Evaluation. To examine whether node evaluation mechanism for pruning the tree is effective, we compare the scores of solutions from the retained nodes with those from the pruned nodes. The results are shown in Figure 5, where the scores of solutions from the retained nodes are significantly higher than pruned nodes, which proves that node evaluation is an effective mechanism for judging and pruning. Stelmakh et al., 2022; Tan et al., 2024; Qi et al., 2024). Compared to above two tasks, questions of complex engineering solution design are with multiple real-world constraints. And the expected answer is solution needing flexible improvement process, rather than an entity fragment or simply integrated paragraph. Therefore, complex engineering solution design is novel and challenging task. Advanced RAG. Prior advanced RAG systems use multi-round approach to iteratively perform rewriting, retrieval, reranking, and generating intermediate answers (Asai et al., 2024; Shi et al., 2024; Chan et al., 2024; Wang et al., 2024b; Tran et al., 2024; Yu et al., 2024). Compared to these systems, SolutionRAG is with bi-point thinking tree, which can respond to challenges of complex engineering solution design. Recently some papers construct RAG systems based on MCTS, achieving better performance through deep thinking (Jiang et al., 2024; Li et al., 2025a; Wu et al., 2025). However, these methods lack mechanism to ensure that all engineering requirements are met, thus failing to guarantee the reliability of solutions."
        },
        {
            "title": "6 Conclusion",
            "content": "Complex QA Tasks. Recent works in the RAG field mainly focused on knowledge-based question answering tasks that require some level of reasoning. (1) Multi-hop QA. The question is combination of multiple sub-questions, and the expected answer is an entity fragment from relevant knowledge documents (Yang et al., 2018; Ho et al., 2020; Zhu et al., 2024; Wu et al., 2024). (2) Long-form QA. The question is an open-ended and comprehensive question, and the expected answer is text paragraph formed by integrating knowledge fragments from relevant documents (Fan et al., 2019; In this paper, we first construct SolutionBench based on engineering reports across various domains, which can examine the ability of systems on complex engineering solution design. Further, we propose SolutionRAG, which explore the optimal solution-improvement process and gradually generates reliable solutions by bi-point thinking tree. In experiments, previous methods perform poorly in complex engineering solution design task, while SolutionRAG represents good improvement over existing approaches. This paper offers promising direction and can inspire the further research."
        },
        {
            "title": "7 Limitations",
            "content": "Complex engineering solution design is task requiring deep research based on professional knowledge, which demands the model has strong capabilities in problem analysis, solution reasoning, and critical thinking. In this paper, due to limited GPU computational resources, we construct the system by the existing capabilities of LLMs, without considering special training. Therefore, possible direction for future work is to use reinforcement learning to train LLMs, in order to develop more powerful complex engineering solution design systems. Additionally, due to the same limitation in GPU computational resources, we do not extensively explore hyperparameters such as the width and depth of the tree in our experiments. This could be valuable research topic for future work."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations. Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to refine queries for retrieval augmented generation. In First Conference on Language Modeling. Olivier De Weck, Daniel Roos, and Christopher Magee. 2011. Engineering systems: Meeting human needs in complex technological world. Mit Press. Waguih ElMaraghy, Hoda ElMaraghy, Tetsuo Tomiyama, and Laszlo Monostori. 2012. Complexity in engineering design and manufacturing. CIRP annals, 61(2):793814. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 35583567, Florence, Italy. Association for Computational Linguistics. David Fortus, Joseph Krajcik, Ralph Charles Dershimer, Ronald Marx, and Rachel Mamlok-Naaman. 2005. Design-based science and real-world problemsolving. International Journal of Science Education, 27(7):855879. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online). International Committee on Computational Linguistics. Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and Tao Zhang. 2024. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement. Preprint, arXiv:2412.12881. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 79697992, Singapore. Association for Computational Linguistics. David Jonassen, Johannes Strobel, and Chwee Beng Lee. 2006. Everyday problem solving in engineering: Lessons for engineering educators. Journal of engineering education, 95(2):139151. Katharina Kalogerakis, Christian Lüthje, and Cornelius Herstatt. 2010. Developing innovations based on analogies: experience from design and engineering consultants. Journal of Product Innovation Management, 27(3):418436. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2025. Nv-embed: Improved techniques for training llms as generalist embedding models. Preprint, arXiv:2405.17428. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeIn Advances in Neural Inforintensive nlp tasks. mation Processing Systems, volume 33, pages 9459 9474. Curran Associates, Inc. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a. Search-o1: Agentic search-enhanced large reasoning models. Preprint, arXiv:2501.05366. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. Preprint, arXiv:2308.03281. Zhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei Han, Le Sun, and Yongbin Li. 2025b. StructRAG: Boosting knowledge intensive reasoning of LLMs via inference-time hybrid information structurization. In The Thirteenth International Conference on Learning Representations. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. 2022. Unified structure generation for universal information extraction. Preprint, arXiv:2203.12277. 9 James Mayfield, Eugene Yang, Dawn Lawrie, Sean MacAvaney, Paul McNamee, Douglas W. Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Kayi, Kate Sanders, Marc Mason, and Noah Hibbler. 2024. On the evaluation of machine-generated reports. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, page 19041915, New York, NY, USA. Association for Computing Machinery. Madara Ogot and Gul Kremer. 2004. Engineering design: practical guide. Trafford Publishing. OpenAI. 2024a. Gpt-4o system card. Preprint, arXiv:2410.21276. OpenAI. 2024b. Openai o1 system card. Preprint, arXiv:2412.16720. Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, and Wei Xu. 2024. long2rag: Evaluating long-context & long-form retrieval-augmented generation with key point recall. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 48524872, Miami, Florida, USA. Association for Computational Linguistics. Team Qwen. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. 2024. Generate-then-ground in retrieval-augmented generation for multi-hop question answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73397353, Bangkok, Thailand. Association for Computational Linguistics. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and MingWei Chang. 2022. ASQA: Factoid questions meet long-form answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 82738288, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, and Linqi Song. 2024. ProxyQA: An alternative framework for evaluating longform text generation with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68066827, Bangkok, Thailand. Association for Computational Linguistics. Qwen Team. 2024. Qwen2.5: party of foundation models. Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, and Hong Yu. 2024. Rare: Retrievalaugmented reasoning enhancement for large language models. Preprint, arXiv:2412.02830. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. 2024a. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. Preprint, arXiv:2406.17419. Ruobing Wang, Daren Zha, Shi Yu, Qingfei Zhao, Yuxuan Chen, Yixuan Wang, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, and Maosong Sun. 2024b. Retriever-and-memory: Towards adaptive note-enhanced retrieval-augmented generation. Preprint, arXiv:2410.08821. Feijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding, and Jing Gao. 2025. Talk to right specialists: Routing and planning in multi-agent system for question answering. Preprint, arXiv:2501.07813. Jian Wu, Linyi Yang, Zhen Wang, Manabu Okumura, and Yue Zhang. 2024. Cofca: step-wise counterfactual multi-hop qa benchmark. Preprint, arXiv:2402.11924. Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32253245, Toronto, Canada. Association for Computational Linguistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. Preprint, arXiv:1809.09600. Tian Yu, Shaolei Zhang, and Yang Feng. 2024. Auto-rag: Autonomous retrieval-augmented generation for large language models. arXiv preprint arXiv:2411.19443. Zikang Zhang, Wangjie You, Tianci Wu, Xinrui Wang, Juntao Li, and Min Zhang. 2025. survey of genIn Proceedings of erative information extraction. the 31st International Conference on Computational Linguistics, pages 48404870, Abu Dhabi, UAE. Association for Computational Linguistics. Zhipu. 2024. Glm-zero. Yujia Zhou, Zheng Liu, and Zhicheng Dou. 2024. Boosting the potential of large language models with an intelligent information assistant. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. Andrew Zhu, Alyssa Hwang, Liam Dugan, and Chris Callison-Burch. 2024. Fanoutqa: multi-hop, multidocument question answering benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1837."
        },
        {
            "title": "A List of Engineering Journals",
            "content": "In order to ensure that the data sources used to construct the benchmark are authentic, authoritative, and diverse, we select engineering reports on solution design from authoritative journals in multiple engineering fields as our data sources. If the report is in Chinese, we extract the useful content, then use GPT-4o to translate the content into English and manually verify its accuracy. We list the used engineering journals, including the journal name and ISSN meaning the international standard serial number. The detailed list of used engineering journals is shown in Table 4, 5 and 6."
        },
        {
            "title": "B Template for Extraction",
            "content": "In order to obtain the necessary content for evaluating and judging systems from engineering reports, we manually format template. When extracting, we combine the report with this template, input it into GPT-4o, and then organize the output into JSON format and save it. The extracted content includes: real-world complex requirements, expertauthored solutions, analytical knowledge used to interpret the requirements, technical knowledge applied in addressing the requirements, and explanations for the experts solution design process. The complete template is shown in Figure 6."
        },
        {
            "title": "C Prompt for Node Expansion",
            "content": "In the growth of the tree, there are two expansion processes: design and review. The review process is divided into two stages: generating proposals based on parent node information and generating comments based on retrieved documents. The design process is also divided into two stages: generating proposals based on parent node information and generating solutions based on retrieved documents. Moreover, the design process based on the root node and the design process based on the comment node use different prompts due to the differences in input information. All the prompts mentioned in this section are shown in Figure 7."
        },
        {
            "title": "D Prompt for Scores Calculation",
            "content": "In order to evaluate the solutions provided by the system, we follow the methods from previous Longform QA evaluation (Tan et al., 2024; Wang et al., 2024a; Li et al., 2025b), and use LLM-based scoring method. Specifically, for given solution generated by the system, we calculate two Journal Name Environment Journal of Environmental Engineering Technology Environmental Sanitation Engineering The Administration and Technique of Environmental Monitoring Environment and Development Environmental Protection and Technology Green Environmental Protection Building Materials Journal of Henan University of Urban Construction Urban Management and Science & Technology Science and Technology Square Construction Materials & Decoration Intelligent City Instrument Standardization & Metrology Northwest Hydropower Technology & Economics in Petrochemicals Water Purification Technology Construction Science and Technology Urban Geology Engineering and Construction Engineering and Technological Research Scientific and Technological Innovation Engineering & Test Inner Mongolia Water Resources China Cement Guangdong Chemical Industry Jiangxi Building Materials Tianjin Science & Technology Journal of Zhejiang University of Water Resources and Electric Power China Municipal Engineering China Storage & Transport Mining Journal Name Coal Engineering Mining Engineering Mechanical Management and Development Coal and Chemical Industry Colliery Mechanical & Electrical Technology Modern Mining China Mine Engineering Shandong Coal Science and Technology Jiangxi Coal Science & Technology Metal Mine Modern Chemical Research Petroleum Geology and Engineering Coal Mine Modernization Shaanxi Coal Drilling Engineering Mineral Resources and Geology Mine Surveying Coal Mining Equipment Inner Mongolia Coal Economy Inner Mongolia Petrochemical Industry Energy and Energy Conservation China Plant Engineering Engineering and Construction Scientific and Technological Innovation Engineering & Test Energy Technology and Management Coal Technology ISSN 1674-991X 1005-8206 1006-2009 2095-672X 1674-0254 1673-6680 1674-7046 1008-2271 1671-4792 1673-0038 2096-1936 1672-5611 1006-2610 1674-1099 1009-0177 1671-3915 2097-3764 1673-5781 2096-2789 2096-4390 1674-3407 1009-0088 1671-8321 1007-1865 1006-2890 1006-8945 2095-7092 1004-4655 1005-0434 ISSN 1671-0959 1671-8550 1003-773X 2095-5979 1001-0874 1674-6082 1672-609X 1005-2801 1006-2572 1001-1250 1672-8114 1673-8217 1009-0797 1671-749X 2096-9686 1001-5663 1001-358X 1005-2798 2095-1418 1008-0155 1006-7981 2095-0802 1671-0711 1673-5781 2096-4390 1674-3407 1672-9943 1008-8725 Table 4: List of the engineering journals used for construction the benchmark. The information for environment domain and mining domain is shown above, and information for other domains is in Table 5 and 6. scores: (1) Analytical score, which uses the golden solution, explanation, and corresponding analytical knowledge as references, allowing GPT-4o to assess whether the systems solution sufficiently consider the challenges posed by the complex constraints in the requirements. (2) Technical score, which uses the golden solution, explanation, and corresponding technical knowledge as references, allowing GPT-4o to evaluate whether the systems solution correctly apply the appropriate technologies to address the complex constraints in the requirements. Both analytical score and technical score are range from 0 to 100. The used prompts for score calculation are shown in Figure 8. 11 Figure 6: Template used to extract useful content from original engineering reports, aiming to capture real-world complex requirements, expert-authored solutions, analytical knowledge used to interpret the requirements, technical knowledge applied in addressing the requirements, and explanations for the experts solution design process. 12 Figure 7: Prompts used in node expansion of tree growth, including generating solution proposals and solutions based on the root node, generating comment proposals and comments based on solution node, and generating solution proposals and solutions based on comment node. Figure 8: Prompts for calculating analytical score and technical score, which uses the golden solution, explanation, and corresponding analytical and technical knowledge as references, allowing GPT-4o to assess whether the systems solution sufficiently consider the challenges posed by the complex constraints and apply the appropriate technologies to address the complex constraints in the requirements. 14 Transportation Journal Name Railway Construction Technology Northern Communications China Municipal Engineering Highway Urban Roads Bridges & Flood Control Technology Innovation and Application Marine Equipment/Materials & Marketing Engineering and Construction Port Operation Structural Engineers China Highway Engineering and Technological Research Construction Machinery Technology & Management TranspoWorld Railway Investigation and Surveying Transport Construction & Management Guangdong Water Resources and Hydropower Western China Communications Science & Technology Jiangsu Science and Technology Information Value Engineering Hoisting and Conveying Machinery Jiangxi Building Materials Scientific and Technological Innovation Transport Business China Sichuan Cement Aerospace Journal Name Spacecraft Engineering Aeronautical Manufacturing Technology Aviation Maintenance & Engineering Journal of Ordnance Equipment Engineering Aeroengine Space International Avionics Technology System Simulation Technology Journal of Civil Aviation Safety & EMC Internal Combustion Engine & Parts Aeronautical Computing Technique Meteorological Science and Technology Journal of Astronautics Communications Technology Laser & Optoelectronics Progress Engineering & Test Chinese Space Science and Technology Ship Electronic Engineering China Science and Technology Information Journal of Deep Space Exploration China Educational Technology & Equipment Micromotors Spacecraft Recovery & Remote Sensing Journal of Chengdu Aeronautic Polytechnic Telecom Journal Name Systems Engineering and Electronics Electronic Technology & Software Engineering Video Engineering Telecom Engineering Technics and Standardization Radio & Television Network Study on Optical Communications Electronics Quality Radio & Television Information Changjiang Information & Communications Automation in Petro-Chemical Industry Telecommunications Science Computer Knowledge and Technology Journal of Electronics & Information Technology Laser & Optoelectronics Progress China Digital Cable TV Radio Engineering Journal of Beijing Electronic Science and Technology Institute Laser Journal Designing Techniques of Posts and Telecommunications Wireless Internet Science and Technology Journal of University of South China(Science and Technology) Audio Engineering Automation Application Chinese Journal of Lasers Journal of Smart Agriculture ISSN 1009-4539 1673-6052 1004-4655 0451-0712 1009-7716 2095-2945 1006-6969 1673-5781 1000-8969 1005-0159 1006-3897 2096-2789 1004-0005 1006-8872 1672-7479 1673-8098 1008-0112 1673-4874 1004-7530 1006-4311 1001-0785 1006-2890 2096-4390 1673-3681 0451-0712 ISSN 1673-8748 1671-833X 1672-0989 2096-2304 2096-2304 2096-2304 1006-141X 1673-1964 2096-4994 1005-9776 1674-957X 1671-654X 1671-6345 1000-1328 1002-0802 1006-4125 1674-3407 1000-758X 1672-9730 1672-9730 2096-9287 1671-489X 1671-489X 1009-8518 1671-4024 ISSN 1001-506X 2095-5650 1002-8692 1008-5599 2096-806X 1005-8788 1003-0107 1007-1997 2096-9759 1007-7324 1000-0801 1009-3044 1009-5896 1006-4125 1007-7022 1003-3106 1672-464X 0253-2743 1007-3043 1672-6944 1673-0062 1002-8684 1674-778X 0258-7025 2096-9902 Architecture Journal Name Building Technology Development Building Structure Construction & Design for Engineering Modern Paint & Finishing Architecture Technology Theoretical Research in Urban Construction Urban Architecture Space Art and Design Architecture & Culture Journal of Yangzhou Polytechnic College Heating Ventilating & Air Conditioning Construction Machinery & Maintenance China Science and Technology Information Construction Machinery and Equipment Journal of Municipal Technology Jiangxi Building Materials Urban Roads Bridges & Flood Control Fujian Construction Science & Technology Sichuan Cement Engineering and Technological Research Journal of North China Institute of Science and Technology Tianjin Construction Science and Technology World Forestry Research Jiangsu Building Materials Shanghai Construction Science & Technology Water Resource Journal Name Design of Water Resources & Hydroelectric Engineering Hydro Science and Cold Zone Engineering Journal of Water Resources and Architectural Engineering Mechanical & Electrical Technique of Hydropower Station Yangtze River Port & Waterway Engineering Technical Supervision in Water Resources Small Hydro Power Pearl River Water Conservancy Construction and Management Water Conservancy Science and Technology and Economy Water Resources Planning and Design Construction Quality Henan Water Resources and South-to-North Water Diversion Engineering and Construction Technology and Market Beijing Water Port Engineering Technology Water Resources & Hydropower of Northeast China Mechanical and Electrical Information Maritime Safety Gansu Water Resources and Hydropower Technology Water Power Shanxi Water Resources Haihe Water Resources Journal Name Farming Modern Agricultural Science and Technology Farm Machinery Cereal & Feed Industry Journal of Agricultural Mechanization Research Forestry Machinery & Woodworking Equipment Transactions of the Chinese Society of Agricultural Engineering Forest Research Times Agricultural Machinery Protection Forest Science and Technology Journal of Beijing University of Agriculture Contemporary Horticulture China Southern Agricultural Machinery Forest Inventory and Planning Agricultural Machinery Using & Maintenance Journal of Green Science and Technology China Forest Products Industry Forestry Machinery & Woodworking Equipment The Food Industry Journal of Hebei Forestry Science and Technology Electrical Automation Journal of Library and Information Science Forest Science and Technology Chinese Journal of Ecology Popular Standardization Management & Technology of SME ISSN 1001-523X 1002-848X 1007-9467 1007-9548 1000-4726 2095-2104 2097-1141 1008-2832 1672-4909 1008-3693 1002-8501 1006-2114 1001-8972 1000-1212 1009-7767 1006-2890 1009-7716 1006-3943 1007-6344 2096-2789 1672-7169 1008-3197 1001-4241 1004-5538 1005-6637 ISSN 1007-6980 2096-5419 1672-1144 1672-5387 1001-4179 1002-4972 1008-1305 1007-7642 1001-9235 2097-0528 1006-7175 1672-2469 1671-3702 1673-8853 1673-5781 1006-8554 1673-4637 2097-3519 1002-0624 1671-0797 2097-1745 2095-0144 0559-9342 1004-7042 1004ISSN 1007-5739 1000-9868 1003-6202 1003-188X 2095-2953 1002-6819 1001-1498 2095-980X 1005-5215 1002-3186 1006-4958 1672-3872 1671-3168 2097-4515 1674-9944 1001-5299 2095-2953 1004-471X 1002-3356 1000-3886 2096-1162 2097-0285 1000-4890 1007-1350 1673-1069 Table 5: List of the engineering journals used for construction the benchmark. Table 6: List of the engineering journals used for construction the benchmark."
        }
    ],
    "affiliations": [
        "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
        "Tongyi Lab",
        "University of Chinese Academy of Sciences"
    ]
}