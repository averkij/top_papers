{
    "paper_title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
    "authors": [
        "Yiwen Tang",
        "Zoey Guo",
        "Zhuhao Wang",
        "Ray Zhang",
        "Qizhi Chen",
        "Junli Liu",
        "Delin Qu",
        "Zhigang Wang",
        "Dong Wang",
        "Xuelong Li",
        "Bin Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL"
        },
        {
            "title": "Start",
            "content": "Exploring the Potential of Encoder-free Architectures in 3D LMMs Yiwen Tang * 1 2 Zoey Guo * 3 Zhuhao Wang * 4 Ray Zhang * 3 Qizhi Chen 2 Junli Liu 2 Delin Qu 2 Zhigang Wang 2 Dong Wang 2 Xuelong Li 2 Bin Zhao 1 2 5 2 0 2 3 1 ] . [ 1 0 2 6 9 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoderbased 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract highlevel semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL, whose 7B model rivals the current state-of-the-art ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate the encoderfree architecture to be highly promising in the field of 3D LMMs. Code is released at https: //github.com/Ivan-Tang-3D/ENEL. 1. Introduction Large Language Models (LLMs) (Touvron et al., 2023; Bai et al., 2023; Jiang et al., 2023; Cai et al., 2024) have gained *Equal contribution Project Lead 1Northwestern Polytechnical University 2Shanghai AI Laboratory 3The Chinese University of Hong Kong 4Tsinghua University. 1 unprecedented attention for their proficiency in understanding and generating complex language scenarios. Building upon these advances, many recent efforts have been made to develop Large Multimodal Models (LMMs), empowering LLMs with the capability to interpret multimodal information, such as 2D images (Liu et al., 2024; Li et al., 2024a; Zhang et al., 2024a;c;b; Li et al., 2024b) and 3D point clouds (Guo et al., 2023b; Xu et al., 2025; Guo et al., 2023a; Guo* et al., 2024; Zhang et al., 2022b; Jia et al., 2024). Mainstream LMMs are typically encoder-based, relying on heavyweight yet powerful multimodal encoders (e.g., CLIP (Radford et al., 2021) for 2D (Liu et al., 2021; Oquab et al., 2023) and I2P-MAE (Zhang et al., 2023a) for 3D). While these pre-trained encoders offer robust multimodal embeddings enriched with pre-existing knowledge, they also introduce challenges that could limit the future advancement of multimodal understanding. Specifically for 3D LMMs, the encoder-based architecture has the following potential drawbacks: (1) Point Cloud Resolution Limitation. 3D encoders are often pre-trained on point cloud data at fixed resolution, such as 1,024 points in PointLLM (Xu et al., 2025). However, during inference, the resolution of point clouds may vary (e.g., 8,192 or 512 points). This difference between training and inference resolutions can result in the loss of spatial information when extracting 3D embeddings, leading to difficulties for LLMs to comprehend, as showcased in Figure 1 (a). (2) Embedding Semantic Discrepancy. 3D encoders are typically pre-trained using self-supervised methods like MAE (Pang et al., 2022; Tang et al., 2024a;b) and contrastive learning (Xie et al., 2020; Qi et al., 2023), but these training objectives may not align with the specific semantic needs of LLMs. In other words, they may not capture the most relevant semantics for LLMs to understand 3D objects, as visualized in Figure 1 (b). Even when projection layer is used to connect 3D encoders with LLMs, simple MLPs are often insufficient for complete semantic transformation. Given these issues, we ask: Is it possible to explore an encoder-free architecture for 3D LMMs, eliminating the 3D encoder and instead integrating its functionality directly within the LLM itself? Exploring the Potential of Encoder-free Architectures in 3D LMMs Figure 1. Issues of encoder-based 3D LMMs. (a) Point Cloud Resolution Limitation. During training, the point cloud size (P.T. size) and point token size (P.T. size) are fixed at 8192 and 512, respectively. And we adjust these two sizes during inference, point cloud size from 2K to 16K and the corresponding point token size from 128 to 2048. We evaluate them on the captioning task of the Objaverse benchmark using GPT-4 scores as the evaluation metric. (b) Embedding Semantic Discrepancy. We visualize the attention scores of the average text token to the point tokens, where red indicates higher values. The point tokens in the encoder-free architecture exhibit stronger textual semantic relevance needed for the LLM. In this paper, we present the first systematic investigation into the potential of an encoder-free architecture for 3D LMMs. To minimize external influences and ensure clarity, we use the pioneering and sufficiently concise PointLLM (Xu et al., 2025) as our encoder-based baseline, which consists of two progressive training stages: pretraining and instruction tuning. We evaluate the performance on 3D classification (Deitke et al., 2023) and 3D captioning (Deitke et al., 2023) tasks. Specifically, to remove the encoder while mitigating any performance degradation, we explore solutions to the following two key questions: (1) How can we compensate for the high-level 3D semantics originally extracted by the 3D encoder? In 3D LMMs, the raw point cloud input is first passed through token embedding module for low-level tokenization, before being processed by the main 3D encoder, usually Transformer (Vaswani, 2017), to generate high-level embeddings. Skipping the encoder entirely poses challenge in capturing the complex spatial structures of 3D point clouds. To address this, we propose strategy called LLM-embedded Semantic Encoding in the pre-training stage. First, we adopt simple yet effective token embedding module that captures as much informative semantic content as possible. These 3D tokens are then directly fed into the LLM. Next, we aim to shift the responsibility of capturing high-level 3D semantics to the LLM itself. To facilitate this, we make the early layers of the LLM learnable, allowing them to specialize in 3D encoding. To guide this process, we explore various 3D self-supervised loss functions, such as reconstruction loss, masked modeling loss, and distillation loss, and ultimately propose the Hybrid Semantic Loss as the most effective choice. Observation: Our adopted token embedding module, learnable LLM layers, and Hybrid Semantic Loss achieve comparable effectiveness to that of pre-trained 3D encoder, effectively substituting it for high-level 3D semantics. (2) How can we integrate inductive bias into LLMs for better perception of 3D geometric structures? Traditional 3D encoders typically embed explicit inductive bias into their architectures to progressively capture multi-level 3D geometries. For instance, models like Point-M2AE (Zhang et al., 2022a) use local-to-global hierarchy, which is concept also common in convolutional layers for 2D image processing (He et al., 2016). In contrast, LLMs employ standard Transformer architectures, where each layer processes the same number of tokens, representing the same semantic level across the network. In the absence of the encoder, we introduce the approach of Hierarchical Geometry Aggregation during the second fine-tuning stage. In the early layers of the LLM, we aggregate 3D tokens based on their geometric distribution using Farthest Point Sampling (FPS) and k-Nearest Neighbor (k-NN) sampling. This ap2 Exploring the Potential of Encoder-free Architectures in 3D LMMs Figure 2. Overall Pipeline of ENEL. The training is divided into two stages: the pre-training stage and the instruction tuning stage. In the first stage, we set the first layers to be learnable and apply the proposed Hybrid Semantic Loss to embed high-level semantics into the LLM. In the second stage, we adopt the Hierarchical Geometric Aggregation strategy to capture local structures of point clouds. proach enables the LLM to gradually integrate detailed 3D semantics and develop more holistic understanding of the 3D object. In the later layers, we reverse this aggregation, propagating the tokens back to their original distribution to maintain the fine-grained representation necessary for effective semantic communication. Observation: We find that this hierarchical design can facilitate the acquisition of multi-level knowledge and better comprehend the 3D geometries of complex point clouds. Through series of experimental investigations, we have uncovered the strong potential of applying encoder-free architecture to the 3D LMM domain. Building on our insights, we introduce ENEL, an ENcoder-freE 3D LMM evolved from Vicuna-7B (Chiang et al., 2023) using the same training dataset from PointLLM. Notably, without any 3D encoders, ENEL achieves comparable performance to the current state-of-the-art ShapeLLM-13B (Qi et al., 2024), attaining scores of 55.0% and 50.92% on the classification and captioning tasks, respectively. We hope ENEL may provide the community with scalable and effective path for adapting the encoder-free architecture to 3D scenarios. Our main contributions are summarized as follows: We present the first comprehensive empirical study of applying encoder-free architectures to the 3D LMM domain, offering valuable insights for the field. We aim to transfer the original roles of 3D encoders to the LLM itself, and propose the LLM-embedded Semantic Encoding and Hierarchical Geometry Aggregation strategy, both of which have been validated as effective. We further introduce ENEL, concise and well-performed encoder-free 3D LMM, which achieves 50.92%, 55.0% and 42.70% on 3D captioning, classification, and 3D VQA tasks, respectively, on par with existing encoder-based models. 2. Investigation of Encoder-free 3D LMM Encoder-free modeling has been explored in the 2D vision domain to address issues related to image resolution and deployment overload. In this study, we conduct comprehensive investigation to analyze the feasibility of adopting encoder-free architectures for 3D understanding tasks. 2.1. Overall Architecture Task Formulation. We present the first attempt to extend the encoder-free architecture to 3D LMMs, such as PointLLM (Xu et al., 2023) and ShapeLLM (Qi et al., 2024), in order to efficiently handle the complex tasks like embodied agent (Guo et al., 2023a) and vision-language navigation. We select PointLLM as the baseline model for the exploration and evaluate the performance of different strategies on the Objaverse dataset (Deitke et al., 2023), using GPT-4 scores combined with traditional metrics as our evaluation met3 Exploring the Potential of Encoder-free Architectures in 3D LMMs Table 1. Token Embedding. We evaluate the performance on the Objaverse benchmark and adopt PointLLM-7B as the baseline model. Cls and Cap represent classification and captioning tasks, respectively. S-BERT refers to the Sentence-BERT. T.E. stands for our designed token embedding module. Method Cls Cap GPT-4 GPT-4 S-BERT PointLLM-7B - Encoder + 2-layer T.E. + 3-layer T.E. + 4-layer T.E. 53.00 35.50 42.50 47.31 45.00 44.85 33.37 41.35 43.86 42. 47.47 41.19 44.25 45.89 44.51 Table 2. Further 3D Encoding. We set the LLM early layers to be learnable. LR represents the learning rate during the pre-training stage, with the original learning rate set to 2e-3. Method PointLLM-7B + 2 learnable layers + 4 learnable layers + 8 learnable layers LR 2e-3 2e-3 4e2e-3 4e-4 2e-3 4e-4 Cls Cap GPT-4 GPT-4 S-BERT 53.00 41.06 45.5 44.85 49.11 43.76 48.00 44.85 42.23 44. 41.53 45.39 39.71 44.49 47.47 45.92 47.35 46.77 47.71 42.38 47. rics. The benchmark is highly challenging, as it requires the model to achieve high-quality alignment between 3D semantics and textual space while also capturing the intricate geometric structures of 3D objects. In the overall architecture, the encoder-free 3D LMM directly utilizes token embedding module to convert point cloud data into discrete point tokens, which are then concatenated with text tokens to serve as input to the LLM. As shown in Figure 2, to assume the role of the encoder, the LLM is guided to extract high-level semantic features of the point clouds and acquire multi-level knowledge from both global and local perspectives. In the subsequent sections, we primarily explore two strategies within the encoder-free architecture: LLM-embedded Semantic Encoding (Section 2.2) and Hierarchical Geometry Aggregation (Section 2.3). Token Embedding. We first remove the encoder of PointLLM and adopt the original token embedding (Yu et al., 2022). However, the coarse structural design results in significant performance degradation, as observed in Table 1, where the GPT-4 scores for the classification and captioning tasks decrease by 17.5% and 10.48%, respectively. To mitigate excessive information loss and provide refined local features to the LLM, we adopt small network with limited number of parameters, which is lightweight variant of Point-PN (Zhang et al., 2023b). Specifically, for the input {Pi}N i=1, we apply Farthest Point Sampling (FPS) for downsampling the number of points, k-Nearest Neighbors (k-NN) with group size for local aggregation, and learnable linear layers for feature encoding. After series of repetitive operations and the projection layer, we transform the point i=1 RM D1. In cloud into high-dimensional vectors {Fi}M Table 1, we experiment with token embedding at different depths and find that three layers yield the best performance, while two layers fail to capture complex point features and four layers introduce noise. Further 3D Encoding. We discover that the absence of the encoder results in lack of context modeling in point cloud feature processing. Therefore, we attempt to have the early layers of the LLM take on the encoders role in capturing global interactions of features, further encoding the point cloud features. In the pre-training stage, we set the first layers of the frozen LLM to be learnable, utilizing the self-attention mechanism to capture global geometric structures. Meanwhile, we experiment with both the original learning rate and smaller learning rate. As shown in Table 2, the smaller learning rate generally leads to better results. This is because smaller learning rate can make the optimization process of the early layers more stable. Based on the designed token embedding module, setting the first four layers to be learnable yields the best results, as it effectively encodes low-level features into high-level representations with considerable computational efficiency. 2.2. LLM-embedded Semantic Encoding The lack of the 3D encoder results in insufficient encoding of point cloud semantic information, which greatly hinders the LLM to understand the structural details of point clouds. Most existing 3D encoders use self-supervised losses to embed the high-level semantics of point clouds into the transformer, primarily categorized into four types: Masked Modeling Loss (Pang et al., 2022), Reconstruction Loss (Qi et al., 2023), Contrastive Loss (Khosla et al., 2020), and Knowledge Distillation Loss (Zhang et al., 2023a). Based on the proposed token embedding module and LLM learnable early layers, we implement and evaluate the effects of these losses on the encoder-free 3D LMM in the pre-training stage, as described in Figure 3. Finally, we propose the Hybrid Semantic Loss, which assists the LLM to learn the relationship between local spatial information in the point clouds and grasp the high-level 3D semantics. Masked Modeling Loss. In the pre-training stage, we apply the Masked Modeling Loss to the point tokens processed by the LLM, as shown in Figure 3 (a). Through the Far4 Exploring the Potential of Encoder-free Architectures in 3D LMMs Figure 3. Point Cloud Self-Supervised Learning Losses. In the pre-training stage, we explore common self-supervised learning losses for the encoder-free 3D LMM: (a) Masked Modeling Loss, (b) Reconstruction Loss, (c) Contrastive Loss, and (d) Knowledge Distillation Loss. The (e) represents our proposed Hybrid Semantic Loss, specifically designed for the encoder-free architecture. i=1 are divided into point patches {Gi}M thest Point Sampling (FPS) and k-Nearest Neighbors (kNN) algorithms in the token embedding, the point clouds i=1 RM k3 {Pi}N and the corresponding point tokens {Fi}M i=1. We randomly mask the point tokens with masking ratio r, and replace them with learnable tokens. The masked feature tokens }M can be denoted as {Fgti i=1 , which serve as the ground truth for the loss computation. After the learnable tokens are concatenated with visible tokens and processed by the LLM, linear layer is applied to extract the point tokens i=1 RM rD1, and the Mean Squared Error }M {Fprei (MSE) is computed between the predicted Fpre and the ground truth Fgt. The optimization can be written as Lmask = 1 r (cid:88) i=1 (cid:0)Fprei Fgt (cid:1) . 2 2 (1) The specific process of applying Masked Modeling to point patches is detailed in Appendix C.1. Reconstruction Loss. After the point feature tokens {Fi}M i=1 are encoded by the LLM, the tokens are transi=1 RM k3 through }M formed to the point patches {Gprei linear layer. We utilize the l2 chamfer distance to align the predicted Gpre with the ground truth G, reconstructing the original spatial information, as illustrated in Figure 3 (b). This approach encourages the LLM to learn the high-level semantics of the point cloud while preserving the critical structure and key features of the point cloud input. The optimization target Lrecon can be written as 1 (cid:88) i=1 (cid:18) min ai bj 2 + min bi aj2 2 (cid:19) , (2) where = Gpre and = G. The detailed procedure for reconstructing feature tokens can be found in Appendix C.1. Contrastive Loss. We conduct contrastive learning (Khosla et al., 2020) at the point cloud level, where we contrast two transformed versions of the point cloud in the Figure 3 5 Table 3. LLM-embedded Semantic Encoding. In the pre-training stage, we explore the effects of various self-supervised learning losses targeting point tokens. Ψ represents mask ratio of 60%, while Φ represents mask ratio of 30%. The subscript patch and feat represent the loss target. For Hybrid Semantic Loss, the subscript patch and feat represent the masked modeling target, while the reconstruction target is the corresponding feat and patch. Method PointLLM-7B Masked Modeling Losspatch Masked Modeling Losspatch Ψ Masked Modeling Lossfeat Masked Modeling Lossfeat Φ Ψ Φ Reconstruction Losspatch Reconstruction Lossfeat Contrastive Loss Knowledge Distillation Loss Hybrid Semantic Losspatch Hybrid Semantic Lossfeat + Position Embedding Cls Cap GPT-4 GPT-4 S-BERT 53. 48.50 50.00 50.00 49.50 49.50 48.50 43.50 49.50 50.50 52.00 53.00 44.85 45.34 46.80 45.80 47. 46.96 45.95 42.91 45.43 46.84 48.51 48.85 47.47 46.36 47.29 46.29 47.93 47.33 47. 44.77 47.09 47.59 48.06 48.00 (c). Given sampled point cloud {Pi}N i=1, we apply two random geometric transformations T1 and T2, including rotation and translation, to obtain PT 1 and PT 2. The two augmented point clouds are separately paired with the original text query and processed through the LLM to obtain their respective feature tokens FT 1 RM D1 and FT 2 RM D1. Within the mini-batch, the two feature tokens derived from the same point cloud serve as positive pairs, while they are considered negative pairs with other point clouds. Based on the NCESoftmaxLoss, we aim to maximize the similarity of positive pairs and minimize the similarity of negative pairs, forcing the LLM to learn the geometric equivariance of point clouds. The specific formula Lcontrast is as follows: (cid:32) log 1 B (cid:88) i=1 exp(FT 1i FT 2i/τ ) j=1 exp(FT 1i FT 2j /τ ) (cid:80)B (cid:33) , (3) Exploring the Potential of Encoder-free Architectures in 3D LMMs Table 4. Hierarchical Geometry Aggregation. In the instruction tuning stage, we conduct the experiments of Hierarchical Geometry Aggregation strategy. represents the number of aggregation and propagation operations. refers to the LLM layers between aggregation and propagation operations. + Self-Attn. represents the incorporation of the gated self-attention in the aggregation. Method Cls Cap GPT-4 GPT-4 S-BERT PointLLM-7B 53.00 l=1 l=2 l=3 H=2 H=4 H= + Self-Attn. 52.50 50.00 48.00 53.50 52.50 51.00 55.00 44.85 48.86 46.76 45. 49.13 48.39 48.95 50.92 47.47 48.14 47.95 46.85 48.33 47.75 47.97 48. where stands for the training batch size. Knowledge Distillation Loss. We select the powerful Uni3D-L (Zhou et al., 2023) as the teacher encoder, input the point cloud into the 3D encoder, and obtain the output feature Fteacher RM D2. The Mean Squared Error (MSE) between the LLM output tokens Fstudent and Fteacher is computed to align Fstudent as closely as possible to Fteacher, thereby transferring the knowledge embedded in the 3D encoder to the LLM. By obtaining additional supervision from the Uni3D, the LLM better captures the complex structures in the point cloud data, as displayed in Figure 3 (d). The objective function can be defined as LKD = 1 (cid:88) i= (cid:0)Fstudenti Fteacheri2 2 (cid:1) . (4) Experiments and Insights. As shown in Table 3, we compare the effects of common self-supervised learning losses in the pre-training stage, where they are summed with the LLM cross-entropy loss (Touvron et al., 2023), each with coefficient of 1. The observations are summarized as below: The point cloud self-supervised learning losses generally benefit the encoder-free 3D LMM. Compared to previous experimental results, where the GPT scores for the classification and captioning tasks are 49.11% and 45.39%, the self-supervised losses bring about the significant improvements. This is because the self-supervised learning loss forces transformations on the complex point clouds through certain task design. This encourages the LLM to not simply memorize specific point cloud data but to learn the underlying geometric relationships and high-level semantic information. Among the self-supervised learning losses, the Masked 6 Figure 4. Hierarchical Geometry Aggregation Strategy. In the instruction tuning stage, we apply aggregation and propagation operations to the point tokens to capture the local structural details. Modeling Loss demonstrates the strongest performance improvement. It achieves GPT-4 scores of 49.5% and 47.35% for classification and captioning tasks, respectively. The application of the masked modeling to the point features facilitates the embedding of high-level semantics from point clouds into the LLM. However, the mask ratio is directly proportional to the training optimization difficulty, and increasing it from 30% to 60% results in performance degradation. In addition, explicitly reconstructing point patches is not as effective as masked modeling in capturing the critical features of the input, but it does help the LLM learn the complex patterns within point clouds. Knowledge Distillation Loss falls short compared to the first two losses. Finally, Contrastive Loss, which fails to extract the detailed semantics, achieves the lowest performance. Hybrid Semantic Loss. Based on the experimental results above, we propose the self-supervised learning loss specifically designed for the encoder-free 3D LMMHybrid Semantic Loss, as showcased in Figure 3 (e). We apply masking ratio to randomly mask point tokens from the token embedding. The masked tokens and the corresponding patches are referred to as {Fmaski}M i=1 , respectively. The remaining tokens are denoted as{Fvisi}M (1r) and {Gvisi}M (1r) . For the masked portion, we adopt masked modeling, and for the visible portion, we use the reconstruction strategy. The inverse modeling process is described in Appendix C.1. Considering the autoregressive nature of the LLM and the unordered attribute of point clouds, we directly concatenate learnable tokens {Flearni}M i=1 to i=1 and {Gmaski }M i=1 i=1 Exploring the Potential of Encoder-free Architectures in 3D LMMs Table 5. Comparison of different models on various 3D understanding tasks. primary focus is placed on GPT-4 evaluation, along with data-driven metrics (Sentence-BERT and SimCSE). Model GPTSentence-BERT SimCSE BLEU-1 ROUGE-L METEOR GPT-4 GPT-4 Cap Cls QA InstructBLIP-7B (Dai et al., 2023) InstructBLIP-13B (Dai et al., 2023) LLaVA-7B (Liu et al., 2024) LLaVA-13B (Liu et al., 2024) 3D-LLM (Hong et al., 2023) PointLLM-7B (Xu et al., 2023) PointLLM-13B (Xu et al., 2023) ShapeLLM-7B (Qi et al., 2024) ShapeLLM-13B (Qi et al., 2024) ENEL-7B 45.34 44.97 46.71 38.28 33.42 44.85 48.15 46.92 48.94 50.92 47.41 45.90 45.61 46.37 44.48 47.47 47.91 48.20 48.52 48.61 48.48 48.86 47.10 45.90 43.68 48.55 49.12 49.23 49.98 49. 4.27 4.65 3.64 4.02 16.91 3.87 3.83 3.88 8.28 8.85 7.70 8.15 19.48 7.30 7.23 7.20 12.99 13.23 12.14 12.58 19.73 11.92 12.26 12. 43.50 34.25 50.00 51.75 45.25 53.00 54.00 54.50 54.00 55.00 47.90 41.20 46.60 47.40 53.10 42.70 the end of Fvis, replacing the masked tokens. After passing point tokens through the LLM, we compute the MSE between Flearn and Fmask. The visible features Fvis are transformed into Gpred, and the L2 Chamfer distance is computed between Gpred and Gvis. These two are added to the original cross-entropy loss with coefficients all equal to 1. This approach not only embeds high-level semantics into the LLM but also ensures geometric consistency throughout the point cloud learning process. With mask ratio of 30%, it achieves 52.00% and 48.51% for the classification and captioning tasks. Adding positional encodings to the learnable and visible tokens at each layer of the LLM further yields improvements of +1% and +0.34%, respectively, enhancing the perception of spatial positional information. 2.3. Hierarchical Geometry Aggregation 3D encoders are designed with specific structures tailored for point clouds, such as local-to-global hierarchy (Zhang et al., 2022a) for exploring the geometric structure of the point cloud. However, in encoder-free architectures, the LLM itself does not have an explicit local modeling module. The self-attention mechanism is intended for modeling global interactions. Therefore, building upon the proposed Hybrid Semantic Loss, we explore in the instruction tuning stage how to enable the LLM to actively perceive 3D local details and complement the learned global semantics. To this end, we propose the Hierarchical Geometry Aggregation strategy in the LLM early layers. Implementation Details. As depicted in Figure 4, from }M the LLM second layer, the input point tokens {Finputi i=1, }M based on their corresponding coordinates {Pinputi i=1, are downsampled using the Farthest Point Sampling (FPS), reducing the token number from to M/2, denoted as input, which serve as the local centers. Then, using the k-Nearest Neighbor (k-NN) algorithm, we obtain the neighinput RM/2kD1 for the center points. boring points For input, we employ the gated self-attention mechanism for intra-group interactions, grasping the local geometric structure. We multiply the self-attention output by learnable parameter initialized from zero to adaptively adjust the required knowledge. We formulate it as = tanh(α) Self-Attn.(F input) + input. (5) input On top of this, we apply pooling to fuse the features input within each neighbor and add them to the original center }M/2 tokens input, yielding aggregated tokens {F 1 i=1 , formulated as aggi agg = Pooling(F input ) + input. (6) aggi Then we perform 1 iterations of geometry aggregation, }M/2l resulting in {F i=1 . To ensure that the LLM fully extracts the local information, we choose to perform further semantic modeling using LLM layers after aggregation operations. This allows the model to learn the interactions between local information while preventing the loss of finegrained geometric details. Subsequently, we perform iterations of geometry propagation. Following the approach of PointNet++ (Qi et al., 2017), we propagate the aggregated features agg from the local center points to their surrounding neighboring points, generating {F 1 . After iterations, we obtain point tokens of length , which are then processed by the remaining 2l (H + 1) layers. }M/2(l1) proi i=1 Experiments and Insights. We conduct step-by-step experiments on the Hierarchical Geometry Aggregation strategy, sequentially evaluating the impacts of the number of aggregation and propagation operations (l), the number of LLM layers between aggregation and propagation (H), and the incorporation of the gated self-attention mechanism. The best performance is achieved when is set to 1 and the performance decreases as increases. As observed Exploring the Potential of Encoder-free Architectures in 3D LMMs Figure 5. Difference in Semantic Encoding. By visualizing the attention scores of the average text token to the point tokens on the Objaverse dataset, we compare the semantic encoding potential of encoder-based and encoder-free architectures, where red indicates higher values. And (a) represents chairs, (b) represents airplanes, and (c) represents lamps. in Table 4, performing single aggregation and propagation operation achieves 48.86% and 52.5% performance on the captioning and classification tasks, respectively, while additional operations lead to performance drop of 3%- 4%. This is because as increases, repeated aggregation operations progressively simplify spatial relationships, causing the loss of fine-grained geometric structures in the point clouds. same training dataset as PointLLM, these results validate the effectiveness of our proposed LLM-embedded Semantic Encoding and Hierarchical Geometry Aggregation strategies for the encoder-free architecture. Additionally, on the 3D-VQA task of the 3D MM-Vet dataset (Qi et al., 2024), despite the lack of spatial and embodied interaction-related data in the training set, ENEL achieves the GPT score of 42.7%, surpassing PointLLM-7B by 1.5%. Compared to setting to 4 or 8, the highest performance is achieved when is set to 2. It reaches 53.5% and 49.13% on the classification and captioning tasks, respectively. The excessive number of LLM layers between aggregation and propagation can lead to the oversmoothing of the aggregated local information, resulting in the loss of local structural details. The introduction of the gated self-attention mechanism effectively enhances performance, achieving the GPT score of 55% in the classification task and 50.92% in the captioning task. The adaptive control of attention output ensures that global contextual information is utilized only when necessary, preventing it from disrupting local geometric structures. Additionally, it allows the model to adjust to different tasks. Visualization. In the Figure 5, we visualize the attention scores between the average text token and the point tokens in the last layer of both PointLLM and ENEL. Three object categories, including the chair, the airplane, and the desk lamp, are selected from the Objaverse dataset (Deitke et al., 2023). In the Figure 5, red indicates higher values. We observe that in encoder-based 3D LMMs, the semantic relevance between the text tokens and the processed point tokens is relatively low. In contrast, ENEL, with its encoderfree architecture, achieves high correlation between the features of the two different modalities, with the average text token focusing on key geometric structures of the objects, such as the backrest of the chair, the wings of the airplane, and the lampshade of the desk lamp. 4. Conclusion 3. Results and Visualization Results. In Table 5, on the Objaverse benchmark (Deitke et al., 2023), ENEL-7B achieves the GPT score of 50.92% on the 3D object captioning task, setting new state-of-theart (SOTA) performance. In traditional metrics, SentenceBERT and SimCSE reaches 48.61% and 49.31%, respectively, comparable to the ShapeLLM-13B. For the 3D object classification task, ENEL-7B outperformes prior encoderbased 3D LMMs, achieving GPT score of 55%. Given the In this study, we investigate the potential of the encoder-free architecture in 3D understanding. Through systematic analysis, we demonstrate that transferring the functionality of the 3D encoder to the LLM itself can effectively compensate for the performance degradation caused by the removal of the 3D encoder. To achieve this, we introduce the LLMembedded Semantic Encoding strategy and the Hierarchical Geometry Aggregation strategy in the pre-training and instruction tuning stages. These strategies enable the encoding of high-level point cloud semantics while capturing critical Exploring the Potential of Encoder-free Architectures in 3D LMMs local information. Our experiments highlight the promising prospects of the encoder-free architecture."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., and Tasırlar, S. Introducing our multimodal models, 2023. URL https://www.adept. ai/blog/fuyu-8b. Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. ChameleonTeam. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Chen, Y., Wang, X., Peng, H., and Ji, H. single transformer for scalable vision-language modeling. arXiv preprint arXiv:2407.06438, 2024a. Chen, Y., Yang, S., Huang, H., Wang, T., Lyu, R., Xu, R., Lin, D., and Pang, J. Grounded 3d-llm with referent tokens. arXiv preprint arXiv:2405.10370, 2024b. Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint arXiv:2305.06500, 2, 2023. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., and Farhadi, A. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13142 13153, 2023. Diao, H., Cui, Y., Li, X., Wang, Y., Lu, H., and Wang, X. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024a. Diao, H., Cui, Y., Li, X., Wang, Y., Lu, H., and Wang, X. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024b. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Fu, R., Liu, J., Chen, X., Nie, Y., and Xiong, W. Scene-llm: Extending language model for 3d visual understanding and reasoning. arXiv preprint arXiv:2403.11401, 2024. Guo, Z., Tang, Y., Zhang, R., Wang, D., Wang, Z., Zhao, B., and Li, X. Viewrefer: Grasp the multi-view knowledge for 3d visual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1537215383, 2023a. Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., et al. Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615, 2023b. Guo*, Z., Zhang*#, R., Zhu, X., Tong, C., Gao, P., Li, C., and Heng, P.-A. Sam2point: Segment any 3d as videos in zero-shot and promptable manners. arXiv preprint arXiv:2408.16768, 2024. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., and Gan, C. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., and Gan, C. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36, 2024. Jia, Y., Liu, J., Chen, S., Gu, C., Wang, Z., Luo, L., Lee, L., Wang, P., Wang, Z., Zhang, R., et al. Lift3d foundation policy: Lifting 2d large-scale pretrained models for robust 3d robotic manipulation. arXiv preprint arXiv:2411.18623, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., and Krishnan, D. Supervised Exploring the Potential of Encoder-free Architectures in 3D LMMs contrastive learning. Advances in neural information processing systems, 33:1866118673, 2020. visual models from natural language supervision. ICML, volume 139, pp. 87488763, 2021. In Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., and Li, C. Llava-next-interleave: Tackling multiimage, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024a. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., and Li, C. Llava-next-interleave: Tackling multiimage, video, and 3d in large multimodal models. ICLR 2025 Spotlight, 2024b. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Pang, Y., Wang, W., Tay, F. E., Liu, W., Tian, Y., and Yuan, L. Masked autoencoders for point cloud self-supervised In European conference on computer vision, learning. pp. 604621. Springer, 2022. Qi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. Qi, Z., Dong, R., Fan, G., Ge, Z., Zhang, X., Ma, K., and Yi, L. Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. In International Conference on Machine Learning, pp. 2822328243. PMLR, 2023. Qi, Z., Dong, R., Zhang, S., Geng, H., Han, C., Ge, Z., Yi, L., and Ma, K. Shapellm: Universal 3d object understanding for embodied interaction. arXiv preprint arXiv:2402.17766, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable Tang, Y., Zhang, R., Guo, Z., Ma, X., Zhao, B., Wang, Z., Wang, D., and Li, X. Point-peft: Parameter-efficient finetuning for 3d pre-trained models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 51715179, 2024a. Tang, Y., Zhang, R., Liu, J., Guo, Z., Zhao, B., Wang, Z., Gao, P., Li, H., Wang, D., and Li, X. Any2point: Empowering any-modality large models for efficient 3d understanding. In European Conference on Computer Vision, pp. 456473. Springer, 2024b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Wang, Z., Huang, H., Zhao, Y., Zhang, Z., and Zhao, Z. Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769, 2023. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Xie, S., Gu, J., Guo, D., Qi, C. R., Guibas, L., and Litany, O. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pp. 574591. Springer, 2020. Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., and Lin, D. Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911, 2023. Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., and Lin, D. Pointllm: Empowering large language models to understand point clouds. In European Conference on Computer Vision, pp. 131147. Springer, 2025. Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., and Lu, J. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1931319322, 2022. Zhang, R., Guo, Z., Gao, P., Fang, R., Zhao, B., Wang, D., Qiao, Y., and Li, H. Point-m2ae: multi-scale masked 10 Exploring the Potential of Encoder-free Architectures in 3D LMMs autoencoders for hierarchical point cloud pre-training. Advances in neural information processing systems, 35: 2706127074, 2022a. Zhang, R., Guo, Z., Zhang, W., Li, K., Miao, X., Cui, B., Qiao, Y., Gao, P., and Li, H. Pointclip: Point cloud understanding by CLIP. 2022b. Zhang, R., Wang, L., Qiao, Y., Gao, P., and Li, H. Learning 3d representations from 2d pre-trained models via imageIn Proceedings of the to-point masked autoencoders. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2176921780, 2023a. Zhang, R., Wang, L., Wang, Y., Gao, P., Li, H., and Shi, J. Starting from non-parametric networks for 3d point cloud analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5344 5353, 2023b. Zhang, R., Han, J., Liu, C., Zhou, A., Lu, P., Qiao, Y., Li, H., and Gao, P. Llama-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In ICLR 2024, 2024a. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Gao, P., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ECCV 2024, 2024b. Zhang, R., Wei, X., Jiang, D., Guo, Z., Li, S., Zhang, Y., Tong, C., Liu, J., Zhou, A., Wei, B., et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024c. Zhou, J., Wang, J., Ma, B., Liu, Y.-S., Huang, T., and Wang, X. Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773, 2023. 11 Exploring the Potential of Encoder-free Architectures in 3D LMMs A. Related Work 3D LMM. Recent advancements in integrating large language models (LLMs) with 3D data have led to significant progress in both object-level and scene-level understanding. At the object level, early approaches like (Hong et al., 2024) utilize 2D rendering to leverage 2D LLMs, but this sacrifices geometric details. More recent models, including Point-Bind LLM (Guo et al., 2023b), PointLLM (Xu et al., 2023) and ShapeLLM (Qi et al., 2024), directly encode point clouds and align them with LLMs, by combining the 3D encoder with powerful language model, effectively fusing geometric, appearance, and linguistic information. At the scene level, models like Chat-3D (Wang et al., 2023) and Scene-LLM (Fu et al., 2024) focus on understanding complex spatial relationships through dialogue and tasks like captioning. Scene-LLM (Fu et al., 2024) enhances embodied agents abilities in interactive 3D indoor environments by integrating both scene-level and egocentric 3D information. Grounded 3D-LLM (Chen et al., 2024b) utilizes referent tokens to reference specific objects within 3D scenes, enabling tasks such as object detection and language grounding. Encoder-free Vision-Language Models. Traditional vision-language models (VLMs) often rely on vision encoders to extract visual features before processing them with language models, integrating image encoders like CLIP (Radford et al., 2021) and DINO V2 (Oquab et al., 2023). However, recent efforts have explored encoder-free VLMs for their simplicity. Approaches like (ChameleonTeam, 2024; Xie et al., 2024) use VQ tokenizers (Esser et al., 2021) or linear projection layers (Diao et al., 2024a; Chen et al., 2024a) to represent images. Fuyu-8B (Bavishi et al., 2023), pure decoder-only model, directly processes image patches through linear projections, handling high-resolution images but showing only average performance. The EVE (Diao et al., 2024b) eliminates the need for separate vision encoder by bridging vision-language representation within unified decoder and enhancing visual recognition capabilities through additional supervision. Figure 6. Variants of Point Cloud Self-Supervised Learning Losses. (a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss. B. Experimental Settings Implementation Details. We use the LLaMA model (Touvron et al., 2023) as our LLM backbone, with the 7B Vicunav1.1 (Chiang et al., 2023) checkpoint as the default setting. In the token embedding layer, the point cloud is first processed by linear layer to expand its dimension from 6 to 288. The input point cloud initially consists of 8192 points, followed by three iterations of farthest point sampling (FPS), reducing the size to 512, 256, and 128, respectively. After each FPS operation, k-Nearest Neighbors (k-NN) is applied with cluster size of 81. And geometric features are extracted using triangular encoding, followed by linear layers that progressively increase the dimension to 576, 1152, and 2304. Finally, the projection layer maps the features to the LLM dimension of 4096. In the pre-training stage, we unfreeze the first four LLM layers. Within the LLM-embedded Semantic Encoding strategy, Hybrid Semantic Loss applies masked modeling to 30% of the tokens and reconstructs the patches for the remaining 70% visible tokens. In the instruction tuning stage, we apply geometric aggregation in the second LLM layer, reducing the number of point tokens from 128 to 64. MaxMean pooling is used to retain more information. After passing through two LLM layers, the geometric aggregation is applied in the fifth layer to restore the point size count to 128. 12 Exploring the Potential of Encoder-free Architectures in 3D LMMs Table 6. Ablation Experiments. We begin the ablation experiments by changing the single configuration of the module from ENEL. Ψ represents mask ratio of 60%, while Φ represents mask ratio of 30%. For Hybrid Semantic Loss, the subscript patch and eat represent the masked modeling target, while the reconstruction target is the corresponding eat and patch. represents the number of aggregation and propagation operations. refers to the LLM layers between aggregation and propagation operations. refers to the LLM layer between two individual aggregation or propagation operations. Model ENEL-7B Hybrid Semantic Loss Hybrid Semantic Losspatch Hybrid Semantic Losspatch Ψ Hybrid Semantic Lossf eat Φ Ψ gate mechanism l=2,H=2,O=0 l=2,H=4,O=0 l=2,H=2,O=2 l=2,H=4,O=2 GPT-4 Sentence-BERT SimCSE BLEU-1 ROUGE-L METEOR GPT-4 Cap Cls 50.92 47.19 49.05 48.96 49.63 49.26 48.81 49.02 48.96 49. 48.61 48.07 48.82 48.38 48.00 48.41 48.10 48.47 47.96 48.70 49. 48.31 49.20 49.00 48.62 48.93 48.57 48.61 48.89 48.84 3.88 3. 4.01 3.66 3.78 3.71 3.70 3.65 3.80 3.84 7.20 7.41 7.25 6.97 6. 7.12 6.99 7.10 7.05 7.56 12.50 11.84 12.38 11.98 12.33 12. 12.01 12.31 12.55 12.76 55.00 50.61 52.20 52.00 51.50 53.50 51.50 52.00 52.00 53. Training and Evaluation Details. During the two-stage training, each stage utilizes the same dataset and preprocessing method as PointLLM. All training are conducted on 4 80G A100 GPUs in BF16 precision, utilizing FlashAttention, the AdamW optimizer, and cosine learning rate schedule. During the pre-training stage, the model is trained for three epochs with batch size of 128 and learning rate of 4e-4. In the instruction tuning stage, it is conducted for three epochs with batch size of 32 and learning rate of 2e-5. The GPT-4 model (Achiam et al., 2023) used for classification and captioning tasks evaluation refers to gpt-4-0613 version consistent with PointLLM (Xu et al., 2023). In contrast, the GPT-4 model employed for QA performance evaluation corresponds to gpt-4-0125 version aligning with ShapeLLM (Qi et al., 2024). Additionally, the GPT evaluation prompts for classification and captioning are identical to those used in PointLLM, while the prompts for QA follow those in ShapeLLM. C. More Experiments C.1. Variants of Point Cloud Self-Supervised Learning Losses. In the Figure 6, we exhibit the other variants of Masked Modeling Loss, Reconstruction Loss and Hybrid Semantic Loss. As seen in Figure 6 (a), in the Masked Modeling Loss, after the learnable tokens are processed by the LLM, the tokens i=1 RM rk3 through linear layer. We utilize the l2 chamfer distance }M are transformed to the point patches {Gprei to align the predicted Gpre with the point patches Gmask corresponding to the masked tokens, reconstructing the spatial information. The optimization can be written as 1 M (cid:88) (cid:18) i=1 min ai bj2 2 + min bi aj2 2 (cid:19) , (7) where = Gpre and = Gmask. As shown in Figure 6 (b), after the point feature tokens {Fi}M computed between the predicted Fpre and the ground truth . The optimization can be written as i=1 are encoded by the LLM, the Mean Squared Error (MSE) is Lmask = 1 (cid:88) i=1 (cid:0)Fprei i2 2 (cid:1) . (8) Finally, in the Figure 6 (c) Hybrid Semantic Loss, the masked tokens and the corresponding patches are referred to as and {Gvisi }M (1r) {Fmaski}M . i=1 , respectively. The remaining tokens are denoted as{Fvisi}M (1r) i=1 and {Gmaski}M i=1 i=1 13 Exploring the Potential of Encoder-free Architectures in 3D LMMs After passing point tokens through the LLM, we compute the MSE between Fpre and Fvis. The learnable tokens Flearn are transformed into Gpred, and the L2 Chamfer distance is computed between Gpred and Gmask. These two are added to the original cross-entropy loss with coefficients all equal to 1. C.2. More Ablation Experiments We begin the ablation experiments starting from the ENEL, which is the reverse order compared to the experiments in the main text, as showcased in Tabel 6 The Effects of LLM-embedded Semantic Encoding Strategy. In the Table 6, on the basis of ENEL, removing the Hybrid Semantic Loss during the pre-training stage significantly degrades performance. The GPT-4 score for the captioning task drops from 50.92% to 47.19%, and the GPT-4 score for the classification task decreases to 50.61%. This is because the proposed self-supervised learning function for point clouds effectively captures the detailed structures and high-level semantics of the point clouds. Based on ENEL, we find that setting the mask ratio in the Hybrid Semantic Loss to 30% consistently yields better results than 60%. Additionally, the configuration where the masked token part predicts features while the visible token part reconstructs patches outperforms the reverse settingwhere the masked token part predicts patches and the visible token part reconstructs features. This phenomenon can be explained as follows: mask ratio of 30% retains critical information while facilitating the model to effectively utilize the visible tokens to derive the masked parts. When the mask ratio is set too high, the model fails to learn the global context knowledge adequately. Moreover, when the masked token part is tasked with predicting features, the model focuses on learning the high-level context semantics, while the patch reconstruction aids in accurately capturing low-level details. In contrast, when the masked token part predicts patches, the model becomes excessively dependent on local features during the process of semantic reconstruction. The Effects of Hierarchical Geometry Aggregation Strategy. After removing the gating mechanism in the self-attention of the aggregation operation, the performance drops to 49.26% and 53.50% on the captioning and classification tasks, respectively. The gating mechanism helps the model to adaptively filter information, allowing it to focus on more discriminative features. Without the dynamic adjustment to focus on different parts of the input, the generated text from the LLM lacks accuracy and coherence, leading to decrease in performance. The performance generally degrades with an increasing number of aggregation and propagation operations. This degradation can be attributed to the progressive loss of local geometric details through repeated aggregation operations, while multiple propagation operations typically rely on interpolation which tends to amplify high-frequency noise. We observe that increasing the number of LLM layers between the final aggregation operation and the first propagation operation leads to improved performance. This suggests that cascaded aggregation operations necessitate deeper architectural capacity for highlevel feature abstraction, as insufficient network depth may lead to degradation of hierarchical representations. Furthermore, the presence of LLM layers between each aggregation or propagation operation enhances performance by allowing the model to process and transform compressed information. Through self-attention mechanisms, these intermediate layers can recapture and restore details lost during the aggregation process. D. Model Output In Figure 7, we showcase more model output, where our ENEL provides precise and diverse responses with multi-modal 3D instruction input. 14 Exploring the Potential of Encoder-free Architectures in 3D LMMs Figure 7. ENEL Output Examples. We demonstrate that ENEL provides precise and diverse responses when addressing different problems."
        }
    ],
    "affiliations": [
        "Northwestern Polytechnical University",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}