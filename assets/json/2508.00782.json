{
    "paper_title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation",
    "authors": [
        "Kien T. Pham",
        "Yingqing He",
        "Yazhou Xing",
        "Qifeng Chen",
        "Long Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 2 8 7 0 0 . 8 0 5 2 : r SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation Kien T. Pham Hong Kong University of Science and Technology Clear Water Bay, Hong Kong tkpham@connect.ust.hk Yingqing He Hong Kong University of Science and Technology Clear Water Bay, Hong Kong yhebm@connect.ust.hk Yazhou Xing Hong Kong University of Science and Technology Clear Water Bay, Hong Kong yxingag@connect.ust.hk Qifeng Chen Hong Kong University of Science and Technology Clear Water Bay, Hong Kong cqf@ust.hk Long Chen Hong Kong University of Science and Technology Clear Water Bay, Hong Kong longchen@ust.hk Figure 1: Audio-driven Spatially-aware Video Generation targets to synthesize realistic videos that are semantically and spatially aligned with input audio recordings. Our proposed SpA2V framework accomplishes this task by decomposing generation process into two stages: Audio-guided Video Planning and Layout-grounded Video Generation, achieving audio-video correspondence via leveraging VSLs as intermediate representation to capture auditory cues and guide the generation process respectively. Here ground-truth videos are for visual comparisons with generated videos only and are not inputted into our framework. Abstract Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify Long Chen is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. MM 25, Dublin, Ireland 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2035-2/2025/10 https://doi.org/10.1145/3746027.3755705 the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt state-of-theart MLLM for novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in training-free manner. Extensive experiments demonstrate that SpA2V excels in MM 25, October 2731, 2025, Dublin, Ireland Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, and Long Chen generating realistic videos with semantic and spatial alignment to the input audios. CCS Concepts Computing methodologies Computer vision tasks; Image and video acquisition; Animation; Spatial and physical reasoning. Keywords Video Generation, Audio-driven, Spatially-aware, MLLM, Diffusion Models, Training-free ACM Reference Format: Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, and Long Chen. 2025. SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation. In Proceedings of the 33rd ACM International Conference on Multimedia (MM 25), October 2731, 2025, Dublin, Ireland. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3746027."
        },
        {
            "title": "1 Introduction\nContent creation has witnessed a significant transformation in\nrecent years, leading to a proliferation of novel creative tasks\nthat were previously unimaginable. This evolution is driven by\nthe emergence of various powerful generative models capable of\ngenerating and manipulating content in different modalities, in-\ncluding text [11, 39, 53, 54, 63], image [12, 27, 33, 43, 44, 56], au-\ndio [8, 35, 36, 65], and video [1, 15, 19, 20, 29, 34, 61]. Particularly\nin the context of video generation, the advancement is becoming\nmore elusive with many current works making progress in synthe-\nsizing video content based on text description [20, 61] and initial\nimage [1, 61]. Despite showing impressive results, they often fall\nshort in capturing the richness and temporal coherence of real-\nworld events, because of the inherent ambiguity and static nature\nof their respective conditions.",
            "content": "Audio, in contrast, naturally grounds video in reality and encodes abundant temporal and contextual information on soundemitting objects, their interactions, and the spatial arrangement of the soundscape. These intrinsic values provide unique advantages for generating more nuanced, immersive, and temporally consistent video content, leading to more realistic and engaging experiences. In addition, similar to the human ability to use auditory information to depict corresponding visual scenes and events, audio-to-video generation can be applied to diverse applications that span across industry verticals. Some of these include automated scene visualization in filmmaking, dynamic product creation in multimedia, engaging advertisements in marketing, and accessible learning materials in education. In light of these significant advantages and useful applications, it is imperative to explore the field of audio-driven video generation. The prevailing audio-to-video generation methods typically rely on global semantic features extracted from audio tracks for synthesis. Although this approach can produce semantically aligned videos, it only works for specific simple soundscapes and often results in poor content quality and misaligned spatial composition with input audio in general scenarios. For example, existing works including [14, 38, 50, 51] can generate talking head videos conditioned on speech, yet are not applicable to other domains. Other Figure 2: Different frameworks for audio-driven video generation. From top to bottom are the typical Audio Video direct approach, two-stage Audio Text Video method, and our proposed novel Audio Video Scene Layout Video pipeline respectively. methods such as [6, 47, 58, 62] can synthesize videos of different contexts that are globally aligned with the specific semantic categories (e.g., dancing, drumming, landscape, etc.) of the input audio recording. However, their results lack spatial coherence between visual and auditory elements, affecting realism and ultimately diminishing the immersive experience. Some current approaches [3, 64] alleviate such an issue by directly providing an initial frame or video segment which already establishes spatial correspondence with audio as an additional visual input, but inevitably limits the diversity of the generated content. Surprisingly, all the aforementioned works have largely overlooked the fact that sound inherently encompasses rich spatial information such as location and movement of sounding sources present in the scenes. Such information can be harnessed to generate according visual components with not only semantic but also spatial coherence to input audios. To this end, the first critical question that arises is: Q1: Can we directly decode the spatial information embedded within audio to drive video generation? We draw inspiration from the fact that humans spontaneously perform similar tasks to perceive and navigate the environment in our daily hearing. We intuitively utilize our multisensory and commonsense knowledge to exploit specific auditory cues from environmental sounds, then reason on them to derive necessary information. For instance, considering the top-left example in Fig. 1, we can instinctively imagine an approaching car when hearing its engine sound getting louder. This is because we know what car generally sounds and looks like (semantic clue) and deduce that increasing in volume (spatial clue) implies approaching motion. By targeting these auditory cues, we contemplate that strong foundational model with human-like multimodal understanding and reasoning capabilities like MLLM has the potential to adapt and replicate this human instinct, driving us to explore it extensively to address this challenge. Once Q1 is properly resolved, the important subsequent question that emerges is Q2: How should these information be represented to bridge the gap between audio and video modalities and guide the generation process? At first thought, text description seems like viable option. However, it suffers from inherent ambiguity, leading to inconsistent results and lack of precise spatial control over scene SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation MM 25, October 2731, 2025, Dublin, Ireland composition in generation process. Video Scene Layout (VSL), on the other hand, offers structured and unambiguous representation, enabling fine-grained manipulation of object placement and scene structure. Considering our concentration on spatial relationships between auditory and visual elements, VSL is intuitively advantageous compared to the textual counterpart. Therefore, we adopt it as our intermediate representation to capture the semantic and spatial attributes of the sounding sources extracted from input audio and then control the video generation process as shown in Fig. 2. We propose novel framework dubbed SpA2V which is the first attempt to explicitly exploit spatial auditory information for video generation conditioning solely on audio. SpA2V decomposes the generation process into two respective stages, namely Audio-guided Video Planning and Layout-grounded Video Generation. The first stage is responsible for identifying sounding objects occurring in an input audio and inferring their semantic and spatial attributes to construct VSL as guidance for generation in the subsequent stage. We employ state-of-the-art Multimodal Large Language Model (MLLM), such as Gemini 2.0 [52] or GPT4o [40], with demonstrated powerful understanding and reasoning capabilities across different modalities as the Video Planner for our SpA2V. We adapt them for our new task of audio-driven VSL generation via meticulously designed prompting mechanism that leverages In-context Learning [2], allowing it to effectively and efficiently harness semantic and spatial cues presented in input audio. Following VSL generation, we synthesize the final video by conditioning on the VSL in the second stage. Our approach incorporates pre-trained diffusion models in an efficient and effective way, inspired by MIGC [67] and AnimateDiff [19]. These methods augment the pre-trained Stable Diffusion model with spatial grounding and motion modules for layout-to-image and text-to-video tasks. We exploit the fact that they train only these new modules while keeping the backbone intact. By directly integrating their learned modules into the same frozen backbone, we create layout-to-video diffusion model capable of spatial grounding and motion modeling simultaneously without further training. We hereby employ it as our VSL-grounded video generator to complete this stage. To assess the capability of our SpA2V framework, we introduce new benchmark named AVLBench curated from real-world stereo audio-video recordings [13, 16, 69, 70] and repurposed for our specific use cases. It includes diverse test scenarios featuring different numbers of sounding objects with various spatial attributes. Results from our experiments on this benchmark demonstrate that SpA2V achieves high degree of semantic and spatial correspondence between the generated VSLs, videos, and the input audios, marking the first successful attempt of spatially-aware audio-to-video generation. Overall, our contributions are listed as follows: We propose novel task of audio-driven spatially-aware video generation which aims at synthesizing videos with spatial correspondence to audio conditions. We present SpA2V, the first framework attempting to fulfill the task by decomposing the generation process into two stages Audio-guided Video Planning and Layout-to-Video Generation and leveraging powerful pre-trained MLLMs and diffusion models to accomplish each stage, respectively. We introduce AVLBench, new benchmark for evaluating alignment between input audios and generated VSLs and videos. Extensive experiments on the benchmark highlight the capability of SpA2V in generating realistic VSLs and videos where visual elements correspond both semantically and spatially to the sound sources in input audio. The implementation will be released on GitHub1."
        },
        {
            "title": "2 Related Work\nAudio-Visual Learning. Recent years have witnessed growing\nresearch efforts in audio-visual learning. Early studies primarily\nfocused on cross-modal Audio-Visual Synchronisation [4, 24, 25],\nwhich employed self-supervised learning to align temporal relation-\nships between audio and video, establishing foundational represen-\ntations for downstream tasks. Despite resolving temporal alignment,\nthese methods largely overlooked semantic and spatial correlations\nbetween audio and visual modalities. To provide more detailed\naudio-visual spatial alignment and support the audio as a guiding\nsignal, [7, 68] explore the Audio-Visual Segmentation (AVS) task\nthat pioneered the prediction of sounded object segmentation maps\nin video frames conditioned on audio input. However, they focused\non perception rather than generation, limiting their applicability\nto generative tasks. Critically, they also neglected to model the\nspatial attributes of audio (e.g., sound source localization or motion\ntrajectories), which are vital for grounding visual scenes in physical\nreality. Recent advances have started to explore spatial audio for\naudio-visual tasks. For example, BAT [66] leverages large language\nmodels (LLMs) for spatial sound reasoning, while ELSA [9] learns\nspatially-aware language-audio representations for fine-grained\nlocalization. Building on these insights, our work is the first to ex-\nplicitly exploit spatial audio cues for audio-guided video generation,\nenabling the synthesis of videos where visual elements are both\nsemantically and spatially coherent with sound sources.\nAudio-to-Video (A2V) Generation. A2V generation focuses on\nproducing visual content that aligns with given audio inputs. Sev-\neral studies have explored this domain by leveraging audio to pro-\nvide semantic cues and temporal dynamics for video generation.\nSound2Sight [3] and CCVS [30] utilize audio alongside preceding\nvideo frames to forecast subsequent frames, capturing visual dynam-\nics driven by the input audio. [31] employs StyleGAN, projecting\naudio into its latent space to navigate trajectories within this space,\neffectively aligning audio with visual content. Seeing and Hear-\ning [58] introduces a diffusion latent aligner to synchronize audio\nwith visual elements, enhancing the coherence between them. Tem-\npoTokens [62] adapts a pre-trained text-to-video diffusion model for\nA2V generation, aligning audio and visual components to improve\nsynchronization. Although these approaches focus on semantic and\ntemporal alignment, they often overlook the spatial aspect when\nprocessing input audio. Spatial information such as the location\nand distance of sounded objects can bring significant enhancement\nto the generated results. In this work, we pioneer the exploration of\nharnessing important spatial cues from input audio to guide video\ngeneration and fulfill this gap. We term the task as audio-driven\nspatially-aware video generation.",
            "content": "1https://github.com/tkpham3105/SpA2V MM 25, October 2731, 2025, Dublin, Ireland Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, and Long Chen Figure 3: Illustration for the overall framework of SpA2V which is decomposed into two stages: Audio-guided Video Planning and Layout-grounded Video Generation. In the first stage (Section 3.1), given an input audio A, we retrieve 𝑘 example conversations {E1, E2, . . . E𝑘 } from candidate database via Retrieval Module and feed them together with System Instruction and the audio itself into the MLLM Video Planner to perform reasoning and generate desired VSL sequence containing 𝑁 consecutive keyframe layouts {L1, L2, . . . L𝑁 } with respective global video caption and local frame captions. In the second stage (Section 3.2), the obtained VSL and its captions are incorporated to guide video diffusion model consisting of pretrained Base, Motion, and Spatial Grounding Modules to generate the final video that is semantically and spatially coherent with the input A."
        },
        {
            "title": "3.1 Stage 1: Audio-guided Video Planning\nOverview. In this stage, we introduce a novel task: generating\nvideo scene layouts (VSLs) depicting spatial arrangements of sound-\ning objects presented in corresponding audio recordings. This task\nnecessitates a model to first identify the categories of sounding\nsources (semantic component) and their respective locations and\nmovements (spatial components) from the input audio. Then, the\nmodel must use this information to organize the objects into a co-\nherent VSL, accurately reflecting their spatial correspondence with\nthe audio and maintaining content consistency across the video\nsequence. Given these requirements, Multimodal Large Language\nModels (MLLMs) are particularly well-suited due to their strong\nmultimodal understanding, reasoning abilities, and broad founda-\ntional knowledge. Consequently, we empirically investigate the\npotential of MLLMs to effectively address this challenging task.\nInstruction Setup. To generate an audio-conditioned VSL using\nan MLLM, we query it with a prompt consisting of three compo-\nnents: a system instruction, a set of example conversations, and\na user-specified audio recording. The system instruction includes",
            "content": "task definition and guidance regarding the desired behavior and response for each request that the MLLM must follow. Specifically, we instruct the MLLM to act as video director to plan VSLs that capture the content of the input audio recordings. We then outline the task requirements for the MLLM to fulfill, such as the expected layout format, coordinate system, canvas size, and number of frames. In complement, the example conversations provide the MLLM with reference query-response pairs, allowing it to efficiently learn and adapt to the given task. Finally, after supplying the above contextual information, we query the MLLM to perform completion on the users input audio recording to generate the desired VSL. VSL Structure. We ask the MLLM to generate VSLs according to predefined template which is connected sequence of 𝑁 consecutive keyframe layouts {L1, L2, . . . , L𝑁 }. Every layout L𝑖 contains set of 𝑁𝑖 bounding boxes {B1, B2, . . . , B𝑁𝑖 }, each denotes sounding object that occurs in the input audio. Each bounding box is represented by its location and size in numerical coordinates along with labeling phrase that specifies the enclosed object. In addition, each box is assigned unique numerical identifier, establishing and maintaining object correspondence across frames without the need for dedicated box tracker. Finally, each VSL also entails shared global video caption and local frame caption for each keyframe describing the global content and local dynamic transition of the intended video creation. Note that in these captions, the MLLM has the freedom to bring about special information that cannot be inferred from input audio but is beneficial for video generation later such as visual appearance of sounding objects. SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation MM 25, October 2731, 2025, Dublin, Ireland Spatial Reasoning. Spatial information can be inferred by reasoning on the fundamental spatial auditory cues, such as Interaural Time Difference (ITD), Interaural Level Difference (ILD), pitch and volume, and directional shift. ITD and ILD are typically used to infer the location of sounding objects, while pitch and volume often indicate their distance, and directional shift can imply their movement. To accurately deduce the corresponding spatial attributes and minimize spurious hallucinations, we explicitly instruct the MLLM in the system instruction to focus on analyzing these key indicators. Consequently, we ask the MLLM to output brief statement summarizing its reasoning and the extracted spatial cues before generating the VSL to enhance the interpretability of the final response. In-context Learning. Solely relying on the system instruction to provide task descriptions and reasoning guidance may fall short in allowing the MLLM to comprehend our need for precise realworld understanding and reasoning on the aforementioned physical sound properties, causing it to hallucinate incorrect spatial information with fuzzy or non-sensical reasoning, and eventually generate VSLs misaligned with the input audio recordings as shown in Fig. 4. Inspired by [2, 10, 37, 60] which show that In-context Learning can enhance the LLMs task adaptability and compliance in various contexts, we employ it to further guide the behavior of the MLLM and mitigate mentioned problem. For each query, we provide the MLLM with example conversations, each including reference prompt and high-quality VSL with corresponding reasoning statement. Akin to [37], we hypothesize that the more semantically similar the audio recordings of the examples to that of the query, the more informative it can be for the MLLM. Therefore, we conduct Top-k Nearest Neighbor (𝑘NN) search on CLAP [57] embedding space in our Retrieval Module to select 𝑘 examples for each query."
        },
        {
            "title": "3.2 Stage 2: Layout-grounded Video Generation\nOverview. Leveraging the ability of MLLMs to generate seman-\ntically and spatially aligned VSLs and descriptive captions from\nauditory cues, we subsequently introduce an approach for video\nsynthesis controlled by these VSLs in this stage. Our VSL-grounded\nVideo Generator connects off-the-shelf layout-to-image and text-\nto-video diffusion models into a single pipeline. By combining their\nrespective grounding and temporal modeling capabilities, our gen-\nerator produces videos that adhere to the conditioned VSLs and\nentailed captions, thereby maintaining consistency with the input\naudio. Our method operates in a training-free manner that effi-\nciently reduces computational cost and time, eliminates the need\nfor extensive data annotation, and avoids potential catastrophic\nforgetting incurred by training.\nBase Diffusion Model. We build our VSL-grounded Video Gener-\nator based on the pre-trained text-to-image LDM [45], a.k.a Stable\nDiffusion, of which the diffusion procedure follows the standard\nformulation in [22, 48, 49] that comprises a forward diffusion and\na backward denoising process. Given a data sample X ∼ P (X), an\nautoencoder consisting of an encoder ℰ and a decoder 𝒟 will first\nproject its latent correspondence Z0 = ℰ(X). Subsequently, the\ndiffusion and denoising processes are conducted in latent space.\nIn one hand, the forward diffusion is essentially a fixed Markov\nprocess of 𝑇 timesteps that gradually perturbs Z0 to yield Z𝑡 via:",
            "content": "Z𝑡 = 𝛼𝑡 Z0 + 1 𝛼𝑡 𝜖, 𝜖 (0, I), (1) Figure 4: In-context Learning helps guide the MLLM to derive the correct spatial cues from the right physical sound properties and hence generate highly-aligned VSL. for 𝑡 = 1, 2, . . . ,𝑇 . Here 𝛼𝑡 is pre-defined parameter which determines the noise strength at each timestep 𝑡. Eventually, Z0 turns into Z𝑇 that is indistinguishable from Gaussian noise. On the other hand, the backward process leverages denoising network 𝜖𝜃 with training objective of minimizing: , E𝑡,C,Z𝑡 ,𝜖N (0,I) 𝜖 𝜖𝜃 (Z𝑡 , 𝑡, 𝜏𝜃 (C))2 (2) 2 where is the condition and 𝜏𝜃 represents its encoder, to iteratively denoise Z𝑡 . Once the denoising is finished and final clean latent ˆZ0 is obtained, the generated sample can be decoded via ˆX = 𝒟( ˆZ0). In Stable Diffusion, 𝜖𝜃 adopts UNet [46] architecture comprising of down/middle/up blocks each consisting of ResNet [21], spatial selfattention layers, and cross-attention layers that incorporate text conditions. For conciseness, we call these blocks Base Modules in our VSL-grounded Video Generator, responsible for preserving prelearned knowledge to generate diverse and high-fidelity samples guided by text prompts in image domain. Integrating Grounding and Temporal Modeling. With Stable Diffusion as the base model, we respectively integrate pretrained Temporal Modules and Grounding Modules from AnimateDiff [19] and MIGC [67] into our VSL-grounded Video Generator, enabling spatial grounding and motion modeling capabilities to synthesize high-quality videos aligned with input VSLs. Specifically, AnimateDiff proposes learning meaningful motion priors by injecting temporal transformer blocks, namely Motion Modules, to inflate Stable Diffusion, allowing it to generate motion dynamics of visual content over time while alleviating quality degradation. Meanwhile, MIGC incorporates set of articulated instance enhancement attention layers, which we call Spatial Grounding Modules, into Stable Diffusion to enable precise generations of multiple instances in the resulting image following layout input. Since only these external modules are trained to learn their designated objectives while the same base modules are kept frozen, we hypothesize then empirically verify that directly combining them into single end-to-end pipeline, i.e. our VSL-grounded Video Generator, can achieve both spatial grounding and motion modeling abilities. Video Generation with VSL Guidance. Every VSL comprises sequence of 𝑁 consecutive keyframe layouts, each containing set of object bounding boxes, shared global video caption, and local MM 25, October 2731, 2025, Dublin, Ireland Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, and Long Chen Figure 5: Qualitative comparisons of our SpA2V with prior SOTA works in audio-to-video generation. Here GT denotes groundtruth videos and VSLs for illustration of visual elements present in input audios. Zoom-in for details. frame caption. To control our VSL-grounded Video Generator to synthesize video of 𝑛 frames, we first perform temporal-wise linear interpolation on the coordinates of the bounding boxes for each object to obtain denser VSL with expanded length of 𝑛 layouts. Each layout will then serve as grounding signal for corresponding frame. Since the base diffusion model uses text prompt as global condition for generation, we also input the global video caption of the VSL to preserve its pre-trained generative capability and maintain global consistency across generated frames. In addition, for the 𝑁 keyframes, we use their local frame caption as alternative to global caption that empirically helps produce better frame transitions with more natural local dynamics."
        },
        {
            "title": "4 Experiments\n4.1 Setup\nBenchmark. As our proposed two-stage Audio → VSL → Video\npipeline is novel, there is no existing benchmark suitable for evalu-\nating our SpA2V framework. Therefore, we created AVLBench, a\nnew benchmark specifically designed for our use case, curated from\nreal-world stereo audio-video recording datasets [13, 16, 69, 70]\nspanning a variety of sound sources, including instruments and\nmoving vehicles in indoor and outdoor environments. We begin by\nmanually selecting recordings for which the audio contains strong",
            "content": "semantic and spatial signals clearly indicating the sounding sources and their attributes within the video. After filtering, we apply flip and reverse augmentations with quality control to increase the data diversity while maintaining strong correspondence between auditory and visual elements. Subsequently, we use Track Anything [59] to generate ground-truth VSLs by tracking the sounding objects in the videos. Since we need to provide SpA2Vs Video Planner with example conversations for In-context Learning, we adopt LLaVA-OneVision [32] to generate global video caption and local frame captions for each video. We also include an accurate manually written reasoning statement for every sample. Eventually, AVLBench contains 7274 testing samples, of which 4702 samples are used to assess scenarios of single or multiple instruments playing while having Stationary motion in indoor settings, whereas the rest 2572 samples target cases of single or multiple vehicles with Translational movement in outdoor settings. Implementation Details. The overall structure of our SpA2V framework is illustrated in Fig. 3. In Stage 1, we select Gemini 2.0 Flash [53] as our MLLM Video Planner to balance cost-effectiveness and performance. For each input audio, we provide it with 𝑘 = 3 example conversations retrieved from the candidate database via Retrieval Module that performs 𝑘NN search based on the similarity between CLAP embeddings of the input and the candidates. SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation MM 25, October 2731, 2025, Dublin, Ireland Method Combo IL Setup Eg. Sel. (M)LLM 𝜏 AC + LVD [34] Full w/o SR w/o IL Vanilla 3-shot Default GPT4 0. 3-shot 𝑘NN N/A N/A G2.0F 0. Full 3-shot 2-shot 1-shot Full 3-shot 𝑘NN G2.0F 0.5 𝑘NN Random G2.0F 0.5 SpA2V (Ours) Full 3-shot 𝑘NN G2.0F G1.5F G4oM Full 3-shot 𝑘NN G2.0F 0.5 0.5 1.0 1.5 MaxIoU 0.96 18.55 8.75 1.71 1.77 18.55 8.03 5.30 18.55 2.28 18.55 3.13 11.78 18.55 16.87 14.31 0.94 19.45 12.03 3.00 3.42 19.45 11.72 8.02 19.45 3.58 19.45 5.43 14.54 19.45 16.42 14.22 0.92 20.16 14.57 3.93 4.63 20.16 14.46 10.18 20.16 4.47 20.16 7.04 17.15 20.16 16.54 14. Stationary LTSim 46.32 74.43 69.24 56.01 56.37 74.43 70.59 66.32 74.43 56.33 74.43 65.05 68.11 74.43 73.92 73.04 40.48 76.73 74.90 62.64 66.56 76.73 74.58 71.17 76.73 62.07 76.73 70.19 72.16 76.73 75.73 74.76 42.97 75.73 72.41 59.84 62.23 75.73 72.86 69.12 75.73 59.57 75.73 68.11 70.21 75.73 74.83 74.02 DocSim 4.91 15.06 13.10 4.18 4.61 15.06 14.30 11.03 15.06 7.34 15.06 11.44 11.92 15.06 14.47 13.63 4.61 15.47 13.90 4.40 5.10 15.47 14.72 10.65 15.47 7.13 15.47 11.91 13.09 15.47 14.49 13.68 4.40 15.69 14.39 4.55 5.47 15.69 15.01 10.39 15.69 6.86 15.69 12.24 13.96 15.69 14.59 13. MaxIoU 1.51 20.13 15.43 2.22 2.58 20.13 11.28 7.08 20.13 4.46 20.13 4.41 8.69 20.13 16.51 16.74 1.77 22.24 16.87 4.96 6.32 22.24 11.26 7.80 22.24 7.71 22.24 6.26 12.11 22.24 18.24 15.84 1.79 22.62 17.10 5.19 6.58 22.62 11.27 7.86 22.62 8.01 22.62 6.40 12.40 22.62 18.45 15.87 Translational LTSim 45.17 73.90 72.27 56.36 60.52 73.90 71.35 66.61 73.90 67.03 73.90 67.38 63.45 73.90 72.57 72.19 47.51 77.55 75.09 62.72 67.37 77.55 73.35 70.40 77.55 71.43 77.55 71.55 66.23 77.55 76.11 75.39 47.35 77.21 74.88 62.26 66.91 77.21 73.24 70.14 77.21 71.10 77.21 71.25 66.04 77.21 75.85 75. DocSim 3.98 13.66 12.99 4.71 4.05 13.66 12.51 9.65 13.66 8.18 13.66 8.74 8.13 13.66 12.64 12.33 3.71 16.50 16.74 5.98 5.93 16.50 14.54 10.39 16.50 9.03 16.50 8.95 9.75 16.50 14.70 13.91 3.69 16.77 17.03 6.07 6.05 16.77 14.70 10.44 16.77 9.11 16.77 8.96 9.91 16.77 14.89 14.05 Table 1: Quantitative results and ablation analysis conducted for Audio-driven Video Planning in Stage 1. Here AC, SR, IL, and Eg. Sel. are shortened for Audio Captioning, Spatial Reasoning, In-context Learning, and Example Selection, respectively. 𝜏 denotes the temperature value of (M)LLM and indicates higher values are better. and represent subsets of data samples having single or multiple sounding sources, while represents combinations of all scenarios. G2.0F, G1.5F, and G4oM stands for Gemini 2.0 Flash, Gemini 1.5 Flash, and GPT4o Mini accordingly. Subsequently, we prompt the Video Planner to generate VSL consisting of 𝑁 = 5 keyframe layouts of resolution 454 256 with temperature of 𝜏 = 0.5 to control the randomness of its response. In Stage 2, Stable Diffusion 1.5 [44] is adopted as the base diffusion model of our VSL-grounded Video Generator. We then follow default settings in MIGC [67] and AnimateDiff [19] to accordingly deploy Spatial Grounding Modules and Motion Modules onto the base model. With this complete architecture, our Video Generator performs inference to synthesize video of 𝑛 = 16 frames with resolution 512 320 conditioned on the VSL obtained from Stage 1. Unless otherwise specified, these settings are kept by default. Metrics. In Stage 1, to measure the quality of the results in alignment with the input audio, we compute the similarity between the generated VSL and the ground-truth utilizing three metrics namely LTSim [41] , MaxIoU [28], DocSim [42]. These metrics are designed for image layouts that contain bounding boxes of close-set labels. To calculate the similarity between pair of layouts (L, L), they first match their enclosed set of bounding boxes ({B𝑖 }, {B 𝑗 }) then accumulate coordinate IoU scores of the matched boxes. Matching typically involves an indicator function 𝑓𝑎𝑏𝑠 (B𝑖, 𝑗 } that fully ignores box pairs with different categories. Nevertheless, our method generates VSL which is essentially sequence of image layouts consisting of bounding boxes with free labels. Therefore, we adjust this indicator function to soft version 𝑓𝑠𝑜 𝑓 𝑡 (B𝑖, 𝑗 ) = 𝑐𝑜𝑠𝑖𝑛𝑒 (𝑃 (𝑐𝑖 ), 𝑃 (𝑐 𝑗 )) that measures the similarity of the two categories (𝑐𝑖, 𝑐 𝑗 ) in the projector 𝑃s embedding space [5]. We then follow the rest of calculations for all metrics and average the score across frames for each VSL. 𝑗 ) = I{𝑐𝑖 =𝑐 For Stage 2, we adopt the standard FVD [55] and AV-Align [62] to accordingly assess the overall content quality of generated videos and their temporal alignment with input audios. Especially, to evaluate spatial correspondence, we first utilize OV-AVSS [18] to localize the input audios sounding objects within the synthesized videos to obtain respective VSLs. Subsequently, we compute the LTSim [41] scores between these and the ground-truth VSLs. Baselines. Since there is no previous work explicitly explores audio-driven video planning, we choose relevant baseline named LVD [34] for comparison in Stage 1. Note that this approach generates dynamic scene layout conditioning on text prompt, whereas our task requires audio as the sole input guidance. Therefore, we adopt an audio captioning (AC) model [17] to generate textual description for each input audio and feed them into LVD respectively. For Stage 2, we additionally compare our SpA2V framework with TempoTokens [62], Seeing and Hearing [58], and LTX [20], alongside LVD for system-level evaluations of audio-to-video generation capabilities. TempoTokens follows the typical Audio Video direct pipeline for generation. Meanwhile, since Seeing and Hearing and LTX use textual condition, we provide them with the same audio captions as LVD. Therefore, they can be categorized as two-stage Audio Text Video approaches, as shown in Fig. 2 respectively."
        },
        {
            "title": "4.2 Evaluation of Audio-guided Video Planning",
            "content": "Overall Results. As demonstrated in Tab. 1 and Fig. 5, our SpA2V framework can generate VSLs with high similarity to the groundtruth VSLs which indicate strong alignments to the input audios. SpA2V significantly outperforms the baseline of combining audio captioning with LVD [34] in all metrics and test scenarios. Component Ablation. We ablate each component of the MLLM Video Planner to evaluate their effectiveness accordingly. As indicated in Tab. 1, both In-context Learning and Spatial Reasoning are crucial for the planner to appropriately adapt to the instructed task and generate high-quality VSLs, omitting either one will lead to significant performance degradations. Interestingly, Spatial Reasoning needs to be accompanied by In-context Learning to synergistically help the planner achieve the best performance. Incorporating it alone may detrimentally confuse the planner and lead to subpar performance compared to not integrating both (Vanilla). In-context Learning Setup. We assess the performance of the MLLM Video Planner when providing it with different numbers MM 25, October 2731, 2025, Dublin, Ireland Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, and Long Chen Method Cap. Sel. VSL Sel. TempoTokens [62] Seeing and Hearing [58] AC + LTX [20] AC + LVD [34] Mix Global Local SpA2V (Ours) Mix Gen. Gen. Gen. GT FVD 759.22 708.58 525.14 793.48 527.31 529.57 536.77 527.31 515. 691.89 664.87 543.81 712.55 633.05 637.84 643.30 633.05 619.05 878.70 715.77 619.97 814.03 776.63 779.08 790.47 776.63 744.79 Stationary AV-Align 0.134 0.105 0.088 0.129 0.155 0.153 0.154 0.155 0.158 0.153 0.111 0.091 0.156 0.186 0.184 0.176 0.186 0.185 0.145 0.109 0.090 0.144 0.173 0.170 0.166 0.173 0.173 LTSim 34.47 41.81 39.55 33.65 50.62 49.90 50.02 50.62 52. 34.22 38.72 36.79 32.43 48.10 47.12 47.14 48.10 50.77 34.49 36.47 34.75 31.40 46.22 45.07 44.99 46.22 49.83 1549.51 1144.97 1094.19 1306.68 302.88 308.44 313.23 302.88 244.55 FVD 1355.67 979.19 1154.74 793.48 594.38 596.19 598.37 594.38 576.18 1462.94 1049.42 1022.49 1196.76 278.99 282.39 288.01 278.99 231.84 Translational AV-Align 0.171 0.127 0.130 0.126 0.187 0.178 0.184 0.187 0.180 0.179 0.149 0.154 0.156 0.171 0.171 0.161 0.171 0.171 0.179 0.151 0.156 0.158 0.170 0.170 0.159 0.170 0.170 LTSim 28.90 33.88 37.61 46.27 61.71 59.48 62.24 61.71 65.13 29.56 29.76 32.92 42.58 68.96 68.79 68.94 68.96 77.72 29.61 29.45 32.57 42.31 69.50 69.49 69.44 69.50 78. Table 2: Quantitative results and ablation analysis conducted for Layout-grounded Video Generation in Stage 2 and system-wise comparisons. Here Cap. Sel. and VSL Sel. denotes Caption Selection and Video Scene Layout Selection, and indicate higher or lower values are better, and Gen. and GT are shortened for Generated and Ground-truth VSL. Besides, and represent subsets of data samples having single or multiple sounding sources, while represents combinations of all scenarios. of example conversations. Compared to the zero-shot Vanilla, Incontext Learning consistently brings improvements to the planner by delivering more context information via selective examples. Example Selection. We aim to empirically verify our assumption in Section 3.1 that the more reference audio recordings in the example conversations are semantically similar to that of the query, the better information it can bring to the MLLM Video Planner to generate higher quality VSLs. We replace the 𝑘NN Searching strategy in the Retrieval Module with simple random selection while keeping other settings as default. As shown in Tab. 1, this adjustment severely harms the overall performance, highlighting the advantages of the 𝑘NN Searching strategy we adopt. Choices of MLLM. Since the design of our SpA2V is flexible, it allows better models selected as its components to attain better performance. Here we try to employ different state-of-the-art MLLMs as the Video Planner for SpA2V. Specifically, we conduct the same experiments but switch from the default Gemini 2.0 Flash to its predecessor Gemini 1.5 Flash and GPT4o Mini [40]. The results in Tab. 1 demonstrate that the default option significantly exceeds these alternatives, making it the best choice to accomplish this task. Temperature. We test the performance of our SpA2Vs MLLM Video Planner with different values for temperature which controls the randomness of its response with higher values being more creative while lower ones being more deterministic. Apparently, low value of 0.5 best suits our need for the task, as shown in Tab. 1."
        },
        {
            "title": "4.3 Evaluation of Layout-to-Video Generation",
            "content": "Overall Results. As shown in Tab. 3.2 and Fig. 5, our SpA2V framework can generate high-quality videos with compelling semantic and spatial correspondence to input audios across various scenarios. Meanwhile, the synthesized videos of prior works are prone to having limited semantic coherence and inconsistent spatial composition with input audios. Additionally, these methods tend to create videos with minimal dynamics and struggle with cases where sounding objects have large movements. These results highlight the superiority of our proposed SpA2V framework and its two-stage Audio VSL Video pipeline in harnessing informative auditory cues from input audios for video generation objectives. Besides, SpA2V also achieve competitive AV-Align scores which imply strong temporal alignment between generated videos and input audios. We attribute this to the MLLM Video Planner which has the innate potential to capture temporal features in complement of semantic and spatial cues from input audios. These information will then be harmoniously organized into according VSLs and propagated to the subsequent video generation. Caption Selection. As indicated in Tab. 2, simultaneously utilizing shared global and local keyframe captions as text conditions alongside VSL is empirically effective in enhancing SpA2Vs performance. While the former helps preserve the pre-trained generative capability of employed diffusion model and maintain global consistency, the latter encourages better frame transitions with more natural local dynamics across generated frame. Impact of VSL Quality. To evaluate the importance of VSL quality in generating high-fidelity videos aligned with input audios, we skip the video planning steps in Stage 1 and directly use groundtruth VSLs as alternative control signals to guide the video synthesis process in Stage 2. As demonstrated in Tab. 2, such an adjustment substantially enhances overall performance, indicating that the better the quality of VSLs, the better results we can achieve. Since our two-stage pipeline is implementation-agnostic, this observation further implies that our SpA2V framework can continue to improve generation quality and audio-video alignment by adopting more capable MLLMs and video diffusion models in flexible manner."
        },
        {
            "title": "5 Conclusion\nWe have presented SpA2V, the first framework capable of harness-\ning spatial auditory cues for audio-driven spatially-aware video syn-\nthesis. SpA2V decomposes the generation process into two stages:\nAudio-guided Video Planning and Layout-grounded Video Genera-\ntion. In Stage 1, we adopt a SOTA MLLM as the Video Planner and\ninstruct it to generate VSLs from input audios through a diligently\ndesigned prompting mechanism. In Stage 2, we propose an effec-\ntive Video Generator which efficiently incorporates off-the-shelf\ndiffusion models to synthesize videos grounded by the VSLs ob-\ntained from previous stage. The experimental results on our newly\nintroduced AVLBench benchmark highlight the superiority of our\nSpA2V in producing videos with high semantic and spatial con-\nsistency to the input audios, outperforming previous methods by\nlarge margins. We hope that our pipeline will encourage further\nexploration into related areas of study in the future.",
            "content": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation MM 25, October 2731, 2025, Dublin, Ireland Acknowledgement. This research was supported by the Innovation and Technology Fund of HKSAR (GHX/054/21GD), the Hong Kong SAR RGC Early Career Scheme (26208924), the National Natural Science Foundation of China Young Scholar Fund (62402408), and the HKUST Sports Science and Technology Research Grant (SSTRG24EG04). References [1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. 2023. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. arXiv:2311.15127 [cs.CV] https://arxiv.org/abs/2311.15127 [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 18771901. https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf [3] Moitreya Chatterjee and Anoop Cherian. 2020. Sound2Sight: Generating Visual Dynamics from Sound and Context. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII (Glasgow, United Kingdom). Springer-Verlag, Berlin, Heidelberg, 701719. doi:10.1007/978-3-030-58583-9_ [4] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. 2021. Audio-visual synchronisation in the wild. arXiv preprint arXiv:2112.04432 (2021). [5] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2023. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2309.07597 [cs.CL] [6] Lele Chen, Sudhanshu Srivastava, Zhiyao Duan, and Chenliang Xu. 2017. Deep Cross-Modal Audio-Visual Generation. In Proceedings of the on Thematic Workshops of ACM Multimedia 2017 (Mountain View, California, USA) (Thematic Workshops 17). Association for Computing Machinery, New York, NY, USA, 349357. doi:10.1145/3126686.3126723 [7] Yuanhong Chen, Yuyuan Liu, Hu Wang, Fengbei Liu, Chong Wang, Helen Frazer, and Gustavo Carneiro. 2024. Unraveling instance associations: closer look for audio-visual segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2649726507. [8] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. 2023. Simple and Controllable Music Generation. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 4770447720. https://proceedings.neurips.cc/paper_files/paper/2023/ file/94b472a1842cd7c56dcb125fb2765fbd-Paper-Conference.pdf [9] Bhavika Devnani, Skyler Seto, Zakaria Aldeneh, Alessandro Toso, Elena Menyaylenko, Barry-John Theobald, Jonathan Sheaffer, and Miguel Sarabia. 2024. Learning Spatially-Aware Language and Audio Embeddings. Advances in Neural Information Processing Systems 37 (2024), 3350533537. [10] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024. Survey on In-context Learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 11071128. doi:10.18653/v1/2024.emnlp-main. [11] Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, and Yejin Choi. 2022. Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: Framework for Scrutinizing Machine Text. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 72507274. doi:10.18653/v1/2022.acllong.501 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. 2024. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. arXiv:2403.03206 [cs.CV] https://arxiv.org/abs/2403.03206 [13] Magdalena Fuentes, Bea Steers, Pablo Zinemanas, Martín Rocamora, Luca Bondi, Julia Wilkins, Qianyi Shi, Yao Hou, Samarjit Das, Xavier Serra, and Juan Pablo Bello. 2022. Urban Sound & Sight: Dataset And Benchmark For Audio-Visual Urban Scene Understanding. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 141145. doi:10.1109/ICASSP43922.2022.9747644 [14] Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, and Yi Yang. 2023. Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2263422645. [15] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, and Long Chen. 2025. Ca2-vdm: Efficient autoregressive video diffusion model with causal generation and cache sharing. In Forty-Second International Conference on Machine Learning. [16] Ruohan Gao and Kristen Grauman. 2019. 2.5D Visual Sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). [17] Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, Sakshi, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha. 2024. GAMA: Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 62886313. doi:10.18653/v1/2024.emnlp-main.361 [18] Ruohao Guo, Liao Qu, Dantong Niu, Yanyu Qi, Wenzhen Yue, Ji Shi, Bowei Xing, and Xianghua Ying. 2024. Open-Vocabulary Audio-Visual Semantic Segmentation. In Proceedings of the 32nd ACM International Conference on Multimedia (Melbourne VIC, Australia) (MM 24). Association for Computing Machinery, New York, NY, USA, 75337541. doi:10.1145/3664647.3681586 [19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2024. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In The Twelfth International Conference on Learning Representations. https://openreview. net/forum?id=Fx2SbBgcte [20] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. 2024. LTX-Video: Realtime Video Latent Diffusion. arXiv preprint arXiv:2501.00103 (2024). [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 68406851. https://proceedings.neurips.cc/paper_files/paper/2020/ file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf [23] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. https: //openreview.net/forum?id=nZeVKeeFYf9 [24] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. 2022. Sparse in space and time: Audio-visual synchronisation with trainable selectors. arXiv preprint arXiv:2210.07055 (2022). [25] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. 2024. Synchformer: Efficient synchronization from sparse cues. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 53255329. [26] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. 2024. Synchformer: Efficient Synchronization From Sparse Cues. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 53255329. doi:10.1109/ICASSP48485.2024.10448489 [27] Ziqi Jiang, Zhen Wang, and Long Chen. 2025. Clipdrag: Combining text-based and drag-based instructions for image editing. [28] Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. 2021. Constrained Graphic Layout Generation via Latent Optimization. In Proceedings of the 29th ACM International Conference on Multimedia (Virtual Event, China) (MM 21). Association for Computing Machinery, New York, NY, USA, 8896. doi:10.1145/3474085. [29] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David Ross, Bryan Seybold, and Lu Jiang. 2024. VideoPoet: Large Language Model for ZeroShot Video Generation. In Proceedings of the 41st International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR, 2510525124. https: //proceedings.mlr.press/v235/kondratyuk24a.html [30] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. 2021. Ccvs: Contextaware controllable video synthesis. Advances in Neural Information Processing Systems 34 (2021), 1404214055. MM 25, October 2731, 2025, Dublin, Ireland Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, and Long Chen [31] Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Hyunjun Cho, Jihyun Bae, Jinkyu Kim, and Sangpil Kim. 2022. Sound-guided semantic video generation. In European Conference on Computer Vision. Springer, 3450. [32] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2025. LLaVAOneVision: Easy Visual Task Transfer. Transactions on Machine Learning Research (2025). https://openreview.net/forum?id=zKv8qULV6n [33] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2023. GLIGEN: Open-Set Grounded Text-toImage Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2251122521. [34] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and Boyi Li. 2024. LLMgrounded Video Diffusion Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=exKHibougU [35] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D. Plumbley. 2023. AudioLDM: text-to-audio generation with latent diffusion models. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML23). JMLR.org, Article 886, 25 pages. [36] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. 2024. AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing 32 (2024), 28712883. doi:10.1109/TASLP.2024.3399607 [37] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?. In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, Eneko Agirre, Marianna Apidianaki, and Ivan Vulić (Eds.). Association for Computational Linguistics, Dublin, Ireland and Online, 100114. doi:10.18653/v1/2022.deelio-1.10 [38] Yunfei Liu, Lijian Lin, Fei Yu, Changyin Zhou, and Yu Li. 2023. MODA: MappingOnce Audio-driven Portrait Animation with Dual Attentions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2302023029. [39] OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv. org/abs/2303.08774 [40] OpenAI. 2024. GPT-4o System Card. arXiv:2410.21276 [cs.CL] https://arxiv.org/ abs/2410.21276 [41] Mayu Otani, Naoto Inoue, Kotaro Kikuchi, and Riku Togashi. 2024. LTSim: Layout Transportation-based Similarity Measure for Evaluating Layout Generation. arXiv:2407.12356 [cs.CV] https://arxiv.org/abs/2407.12356 [42] Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, and Hadar Averbuch-Elor. 2020. READ: Recursive Autoencoders for Document Layout Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. [43] Kien T. Pham, Jingye Chen, and Qifeng Chen. 2024. TALE: Training-free Crossdomain Image Composition via Adaptive Latent Manipulation and Energy-guided Optimization. In Proceedings of the 32nd ACM International Conference on Multimedia (Melbourne VIC, Australia) (MM 24). Association for Computing Machinery, New York, NY, USA, 31603169. doi:10.1145/3664647.3681079 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1068410695. [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. ArXiv abs/1505.04597 (2015). https://api.semanticscholar.org/CorpusID:3719281 [47] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. 2023. MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1021910228. doi:10.1109/CVPR52729.2023.00985 [48] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37 (Lille, France) (ICML15). JMLR.org, 22562265. [49] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations. https://openreview.net/forum?id=PxTIG12RRHS [50] Shuai Tan, Bin Ji, Yu Ding, and Ye Pan. 2024. Say Anything with Any Style. Proceedings of the AAAI Conference on Artificial Intelligence 38, 5 (Mar. 2024), 50885096. doi:10.1609/aaai.v38i5.28314 [51] Shuai Tan, Bin Ji, and Ye Pan. 2024. Style2Talker: high-resolution talking head generation with emotion style and art style. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence (AAAI24/IAAI24/EAAI24). AAAI Press, Article 565, 9 pages. doi:10.1609/aaai.v38i5. [52] Gemini Team. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530 [cs.CL] https://arxiv.org/abs/ 2403.05530 [53] Gemini Team. 2024. Gemini: Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL] https://arxiv.org/abs/2312.11805 [54] Llama 3 Team. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [55] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2019. Towards Accurate Generative Models of Video: New Metric & Challenges. arXiv:1812.01717 [cs.CV] https://arxiv. org/abs/1812. [56] Zhen Wang, Yilei Jiang, Dong Zheng, Jun Xiao, and Long Chen. 2025. Eventcustomized image generation. In Forty-Second International Conference on Machine Learning. [57] Yusong Wu*, Ke Chen*, Tianyu Zhang*, Yuchen Hui*, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP. [58] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. 2024. Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 71517161. [59] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. 2023. Track Anything: Segment Anything Meets Videos. arXiv:2304.11968 [cs.CV] [60] Tong Yang, Yu Huang, Yingbin Liang, and Yuejie Chi. 2024. In-Context Learning with Representations: Contextual Generalization of Trained Transformers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=ik37kKxKBm [61] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. 2025. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=LQzN6TRFg9 [62] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. 2024. Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation. Proceedings of the AAAI Conference on Artificial Intelligence 38, 7 (Mar. 2024), 66396647. doi:10.1609/aaai.v38i7. [63] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. 2024. MM-LLMs: Recent Advances in MultiModal Large Language Models. In Findings of the Association for Computational Linguistics: ACL 2024, LunWei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1240112430. doi:10.18653/v1/2024.findingsacl.738 [64] Lin Zhang, Shentong Mo, Yijing Zhang, and Pedro Morgado. 2024. AudioSynchronized Visual Animation. In Proceedings of the European Conference on Computer Vision (ECCV). [65] Minglu Zhao, Wenmin Wang, Rui Zhang, Haomei Jia, and Qi Chen. 2025. TIA2V: Video generation conditioned on triple modalities of text-image-audio. Expert Syst. Appl. 268 (2025), 126278. https://doi.org/10.1016/j.eswa.2024.126278 [66] Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Xie Chen, Eunsol Choi, and David Harwath. 2024. Bat: Learning to reason about spatial sounds with large language models. arXiv preprint arXiv:2402.01591 (2024). [67] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. 2024. MIGC: MultiInstance Generation Controller for Text-to-Image Synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 68186828. [68] Jinxing Zhou, Xuyang Shen, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, et al. 2024. Audiovisual segmentation with semantics. International Journal of Computer Vision (2024), 121. [69] Jannik Zürn and Wolfram Burgard. 2022. Self-Supervised Moving Vehicle Detection From Audio-Visual Cues. IEEE Robotics and Automation Letters 7 (2022), 74157422. https://api.semanticscholar.org/CorpusID: [70] Ivana Čavor and Slobodan Djukanović. 2023. Vehicle Speed Estimation From Audio Signals Using 1D Convolutional Neural Networks. In 2023 27th International Conference on Information Technology (IT). 14. doi:10.1109/IT57431.2023. 10078724 SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation MM 25, October 2731, 2025, Dublin, Ireland Additional Implementation Details System Instruction. We present in Fig. 6 the complete system instruction that we used to disclose task definition and guidelines to the MLLM Video Planner to control its behavior and response as we desired. This system instruction is inputted during the initialization of the MLLM. In-context Example Conversation. In Fig. 7, we present the full template for each in-context example conversation that provides explicit context information for the MLLM to enhance its adaptability and adherence to the task. Each example contains reference pair of user query and MLLM response comprised of reasoning statement and the according visual scene layout (VSL). Every VSL in the examples follow the same structure as illustrated in Fig. 7 and described in Section 3.1 in the main paper. Motion and Spatial Grounding Modules. Since our focus is to demonstrate the potential of our proposed Audio Layout Video direction for audio-driven video generation, we adopt the best configuration for the Motion and Spatial Grounding Modules from AnimateDiff and MIGC respectively for simplicity. Specifically, Motion Modules are inserted into every upand down-sample block in Stable Diffusions UNet, while Spatial Grounding Modules are deployed only on the middle block and the lowest-resolution upsample block. Benchmark Construction Here we aim to provide more detailed information about the construction of our AVLBench benchmark specifically designed to assess Audio VSL Video generation abilities. Concretely, we build the benchmark following below four steps: (1) Sourcing. We begin by curating data samples from existing datasets namely FAIR-Play [16], VS13 [70], Urbansas [13], and Freiburg Audio-Visual Vehicles [69]. While the first contains various sample pairs of stereo audios and respective video recordings about instruments such as piano, trumpet, drums... being played in real-world indoor settings, the latter three target driving domains and their data samples capture moving vehicles in outdoor environments. We leverage these dataset for our use cases considering their high spatial alignment between auditory and visual elements of their sample pairs. (2) Filtering. We then manually select recordings and crop them into segments where there exists strong semantic and spatial signals in the audio that clearly indicates the sounding sources and their spatial attributes within the video, and remove noisy samples in which those signals are vague or unidentifiable. (3) Augmenting. After careful filtering, we adopt flip and reverse augmentations with quality control to enrich the data diversity while preserving the strong correspondence between auditory and visual elements in the original samples. For flip, we apply it horizontally on the video frames while swapping the two channels of the paired audio for each sample. For reverse, we apply it on the temporal order of both video frames and audio. We observe that for audios containing sounds with high-frequencies such as instruments, applying reverse augmentation produce unnatural sounds Figure 6: Our system instruction for the MLLM Video Planner to generate VSLs based on input audios. with noisy artifacts, whereas it is not the case for low-level sound such as vehicle engines. Therefore, we only apply flip augmentation for data samples originated from FAIR-Play, while we apply both augmentations for ones about moving vehicles. (4) Annotating. Finally, we proceed to annotate each obtained sample to get their video scene layouts. Given the sounding sources in the audio, we use the Track Anything [59] tool to track their locations and movements in the video. Besides, as required by the example conversations for In-context Learning of the MLLM Video Planner, we then utilize LLaVAOneVision [32] to generate global video caption and local frame captions on the video, and include manually-written reasoning statement for every sample. Eventually, AVLBench comprises 7274 testing samples, with 4702 samples designed to assess scenarios involving single or multiple instruments played in stationary indoor settings. The remaining 2572 samples focus on cases of single or multiple vehicles exhibiting translational movement in outdoor environments. The breakdown MM 25, October 2731, 2025, Dublin, Ireland Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, and Long Chen Figure 8: Breakdown statistics of AVLBench. Stationary Translational Single Multiple 2004 Subtotal 4702 Single Multiple 2392 180 Subtotal 2572 Total 7274 Table 3: Statistics on scene distribution. Stationary Translational Left Center Right Subtotal Left Right Crossing Crossing Approaching Receding Subtotal Total 3083 5616 3083 11782 582 800 800 2764 14546 Table 4: Statistics on spatial attribute. use fixed set of example conversations to provide in-context information for every query. The performance drops shown in Tab. 6 indicate that the retrieval effectiveness is indeed sensitive against the database size and quality. This is reasonable because reducing the size and quality of retrieval corpora decreases the likelihood of retrieving relevant examples and creates more challenging outof-domain scenarios. In future work, our aims are to expand the current dataset to cover more domains and scenarios as well as train an MLLM specialist for audio-driven video planning, mitigating this issue and enhancing the frameworks feasibility in practice. User study. To subjectively assess the performance of our SpA2V compared to other methods, we invite 25 users to participate in user study. We ask each user to complete set of 20 ranking questions, each composed of query sample randomly selected from our benchmark and 5 videos generated by SpA2V and other 4 baselines. Users are required to rank them based on two criteria: (1) visual quality, and (2) audio-video alignment, with 1 indicating the best and 5 denoting the worst. The average ranking scores in Tab. 7 highlight the preference of users for the videos generated by our SpA2V over the others in both criteria. Figure 7: The template of our in-context example conversations to provide context for the MLLM Video Planner. statistics of AVLBench on scene distribution and spatial attribute are detailed in Tab. 3, 4 and Fig. 8. Note that due to the noisy nature of outdoor environments, this domain mainly contains samples with single sounding vehicle after filtering. Besides, the spatial attribute statistics are accumulated per sounding object. Additional Experiments Retrieval with more neighbors. We conduct additional analysis on In-context Learning with more example conversations retrieved and provided to the MLLM Video Planner in Stage 1. The results shown in Tab 5 indicate that 𝑘 = 3 is the optimal setting, and further increasing the number of neighbors saturate the performance. Therefore, we use 3 in-context examples by default in the paper. Impact of retrieval database size and quality. We conduct additional experiments under two adverse settings that reduce the size and diminish the quality of the retrieval database to demonstrate the influence of these factors on the planning stage. In the first setting, we halve the size of the retrieval database for each query randomly. In the second one, to exacerbate the challenge, we only SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation MM 25, October 2731, 2025, Dublin, Ireland IL Setup 0-shot 1-shot 2-shot 3-shot 5-shot 7-shot MaxIoU 3.00 8.02 11.72 19.45 16.77 16.49 Stationary Translational LTSim DocSim MaxIoU 4.40 10.65 14.72 15.47 15.14 15.24 59.84 69.12 72.86 75.73 74.18 74. 4.96 7.80 11.26 22.24 20.21 19.63 LTSim DocSim 62.26 70.14 73.24 77.21 76.41 76.16 5.98 10.39 14.54 16.50 17.01 16.93 Table 5: Planning results with different retrieval settings. Retrieval Setup Full Size Half Size Fixed Set MaxIoU 19.45 12.76 3.68 Stationary Translational LTSim DocSim MaxIoU 15.47 13.69 7.90 75.73 71.77 59.44 22.24 17.12 7. LTSim DocSim 77.21 75.17 71.59 16.50 15.00 9.14 Table 6: Impact of retrieval database size and quality. Method Visual quality Audio-video alignment SpA2V Seeing and Hearing AC + LTX AC + LVD TempoTokens 2.79 2. 4.24 3.91 3.20 3.34 2.79 2.88 1.97 1.95 Table 7: User preference of SpA2V over prior works. DeSync Stationary Translational SpA2V Seeing and Hearing AC + LTX AC + LVD TempoTokens 1.726 1.758 1.658 1.136 1.782 1.782 1.849 1.620 1.823 1.584 Table 8: Additional quantitative results. Extra quantitative evaluation. Since AV-Align [62] is known to not work well in complex scenes, we additionally use DeSync metric which leverages Syncformer [26] to measure audio-video temporal misalignment and show the results in Tab. 8. As consistently observed, our SpA2V achieves competitive performance that indicates strong temporal alignment between the generated videos and input audios. Ablation on Motion and Spatial Grounding Modules. Since removing the Motion Modules will degrade our Layout-to-Video generator into Layout-to-Image generator that deviates from our video synthesis objective, we omit ablating these modules. We only conduct additional analysis to ablate Spatial Grounding Modules which will degrade our generator into Text-to-Video model. The results shown in Tab. 9 highlight the importance of these modules in achieving better video generation quality and especially semantic and spatial alignment with input audios. Limitations and Future Work Although SpA2V introduces novel two-stage Audio VSL Video pipeline for semantically and spatially aligned audio-driven video generation and achieve promising results that outperforms prior methods, there is still much room for further improvements. Firstly, as SpA2V involves two stages, failures in either stage will be detrimental to the whole generation process. For example, an incorrect VSL generated by the Video Planner in Stage 1 will inevitably lead to synthesized video with misalignment in Stage 2 as shown in Fig. 9 (a). Secondly, since our SpA2V framework adopts pre-trained MLLMs and diffusion models as its Video Planner and Video Generator, it also inherits their existing limitations and its performance is hence heavily reliant on them. If they struggle to respond properly to specific conditional guidance and fall short Figure 9: Illustration for limitations of our SpA2V. Method Full No Spatial Grounding Stationary Translational FVD AV-Align 633.05 730.26 0.173 0.063 LTSim DeSync 48.10 42.02 1.758 1.773 FVD AV-Align 278.99 760. 0.171 0.121 LTSim DeSync 68.96 63.89 1.136 1.486 Table 9: Ablation on Spatial Grounding Modules. to generate accurate contents, such issue is likely to be propagated to SpA2V as shown in Fig. 9 (b). We anticipate that these two challenges can be appropriately mitigated by adopting or introducing more powerful models as the components of SpA2V. Finally, since we directly incorporate Spatial Grounding and Motion Modules from MIGC [67] and AnimateDiff [19] although they are trained on datasets, such domain gap can lead to the imbalance between grounding and motion modeling capabilities of the Video Generator, causing it to produce videos with inconsistency problems like having objects changing appearance over time as shown in Fig. 9 (c). We contemplate that further finetuning step for the whole framework using techniques such as LoRA [23] can help alleviate this issue and leave this exploration for future research. Societal Impacts SpA2V empowers individuals, regardless of their video-photography ability, to generate videos that are both semantically and spatially aligned with audio inputs. However, employing our framework carries potential risks. It could be misused for malicious purposes, such as inappropriate content creation or the dissemination of misinformation. Furthermore, given our reliance on pre-trained MLLMs and diffusion models, our framework may inherit biases present in their training data, potentially perpetuating harmful stereotypes. While generated content may currently be readily distinguishable from original works, future technological advancements may blur this distinction, making infringement more difficult to detect. Therefore, we strongly urge users to exercise caution and utilize this method only for legitimate purposes."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology Clear Water Bay, Hong Kong"
    ]
}