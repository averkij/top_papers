{
    "paper_title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
    "authors": [
        "Hansheng Chen",
        "Kai Zhang",
        "Hao Tan",
        "Leonidas Guibas",
        "Gordon Wetzstein",
        "Sai Bi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 4 7 9 4 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "PI-FLOW: POLICY-BASED FEW-STEP GENERATION VIA IMITATION DISTILLATION Hansheng Chen1 Kai Zhang2 Hao Tan2 Leonidas Guibas1 Gordon Wetzstein1 Sai Bi2 1Stanford University 2Adobe Research https://github.com/Lakonik/piFlow Figure 1: High quality 4-NFE text-to-image generations by π-Flow, distilled from FLUX.1-12B (top-right three images) and Qwen-Image-20B (all remaining images). π-Flow preserves the teachers coherent structures, fine details (e.g., skin and hair), and accurate text rendering, while avoiding diversity collapse (see Fig. 4 for sample diversity)."
        },
        {
            "title": "ABSTRACT",
            "content": "Few-step diffusion or flow-based generative models typically distill velocitypredicting teacher into student that predicts shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from qualitydiversity trade-off. To address this, we propose policy-based flow models (π-Flow). π-Flow modifies the output layer of student flow model to predict network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policys ODE trajectory to the teachers, we introduce novel imitation distillation approach, which matches the policys velocity to the teachers along the policys trajectory using standard ℓ2 flow matching loss. By simply mimicking the teachers behavior, π-Flow enables stable and scalable training and avoids the qualitydiversity trade-off. On ImageNet 2562, it attains 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, π-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion and flow matching models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song & Ermon, 2019; Lipman et al., 2023; Albergo & Vanden-Eijnden, 2023) have become the dominant method for visual generation, delivering compelling image quality and diversity. However, these models rely on costly denoising process for inference, which integrates probability flow ODE (Song et al., 2021) over multiple timesteps, each step requiring neural network evaluation. Commonly, the inference cost of diffusion models is quantified by the number of function (network) evaluations (NFEs). To reduce the inference cost, diffusion distillation methods compress pre-trained multi-step model (the teacher) into student that requires only one or few network evaluation steps. Existing distillation approaches avoid ODE integration by taking one or few shortcut steps that map noise to data, where each shortcut path is predicted by the student network, referred to as shortcut-predicting model. Learning these shortcuts is significant challenge because they cannot be directly inferred from the teacher model. This necessitates the use of complex training methods, such as progressive distillation (Salimans & Ho, 2022; Liu et al., 2023; 2024), consistency distillation (Song et al., 2023), and distribution matching (Sauer et al., 2024a; Yin et al., 2024b;a; Salimans et al., 2024). In turn, the sophisticated training often lead to degraded image quality from error accumulation or compromised diversity due to mode collapse. To sidestep the difficulties in shortcut-predicting distillation, we propose novel policy-based flow model (π-Flow or pi-Flow) paradigm: given noisy data at one timestep, the student network predicts network-free policy, which maps new noisy states to their corresponding flow velocities with negligible overhead, allowing fast and accurate ODE integration using multiple substeps of policy velocities instead of network evaluations. To train the student network, we introduce policy-based imitation distillation (π-ID), DAggerstyle (Ross et al., 2011) on-policy imitation learning (IL) method. π-ID trains the policy on its own trajectory: at visited states, we query the teacher velocity and match the policys output to it, using the teachers corrective signal to teach the policy to recover from its own mistakes and reduce error accumulation. Specifically, the matching employs standard ℓ2 loss aligned with the teachers flow matching objective, thus naturally preserving its quality and diversity. We validate our paradigm with two types of policies: simple dynamicˆx(t) (DX) policy and an 0 advanced GMFlow policy based on Chen et al. (2025). Experiments show that GMFlow policy outperforms DX policy and delivers the best ImageNet 2562 FID for 1-NFE generation using the standard DiT architecture (Peebles & Xie, 2023). To demonstrate its scalability, we distill FLUX.112B (Black Forest Labs, 2024b) and Qwen-Image-20B (Wu et al., 2025) text-to-image models into 4-NFE π-Flow students, which achieve state-of-the-art diversity, while maintaining teacher-level quality. We summarize the contributions of this work as follows: We propose π-Flow, new paradigm that decouples ODE integration substeps from network evaluation steps, enabling both fast generation and straightforward distillation. We introduce π-ID, novel on-policy IL method for few-step π-Flow distillation, which reduces the training objective to simple ℓ2 flow matching loss. We demonstrate strong performance and scalability of π-Flow, particularly, its superior diversity and teacher alignment compared to other state-of-the-art 4-NFE text-to-image models."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In this section, we briefly introduce flow matching models (Lipman et al., 2023; Liu et al., 2023) and the notations used in this paper. Let p(x0) denote the (latent) data probability density, where x0 RD is data point. standard flow model defines an interpolation between data sample and random Gaussian noise ϵ (0, I), yielding the diffused noisy data xt = αtx0 + σtϵ, where (0, 1] denotes the diffusion time, and αt = 1 t, σt = are the linear flow noise schedule. The optimal transport map across all marginal densities p(xt) = (cid:82) I)p(x0) dx0 can be described by RD (xt; αtx0, σ"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Comparison between (a) standard flow model (teacher), (b) shortcut-predicting model, and (c) our policy-based model. The shortcut-predicting model skips all intermediate states, whereas the our-based model retains all intermediate substeps with minimal overhead. the following probability flow ODE (Song et al., 2021; Liu, 2022): dxt dt = xt = xt Ex0p(x0xt)[x0] = xt (cid:82) RD x0p(x0xt) dx0 , (1) with the denoising posterior p(x0xt) := samples by first initializing the noise x1 ϵ and then solving the ODE to obtain limt0 xt. In practice, flow matching models approximate the ODE velocity dxt Gθ(xt, t) with learnable parameters θ, trained using the ℓ2 flow matching loss: . At test time, the model can generate dt using neural network (xt;αtx0,σ2 p(xt) I)p(x0) Lθ = Et,x0,xt (cid:20) 1 Gθ(xt, t)2 (cid:21) , with sample velocity := xt x0 . (2) Since each velocity query requires evaluating the network (Fig. 2 (a)), flow matching models couple sampling efficiency with solver precision. Despite the progress in advanced solvers (Karras et al., 2022; Zhang & Chen, 2023; Lu et al., 2022; 2023; Zhao et al., 2023), high-quality sampling typically requires over 10 steps due to inherent ODE truncation error, making it computationally expensive. 3 π-FLOW: POLICY-BASED FEW-STEP GENERATION In π-Flow, we define the policy as network-free function π : RD RD that maps state (xt, t) to flow velocity. policy can be network-free if it only needs to describe single ODE trajectory, which is fully determined by its initial state (xtsrc , tsrc) with tsrc t. In this case, the policy for each trajectory must be dynamically predicted by neural network conditioned on that initial state (xtsrc, tsrc). We therefore adapt flow model to output not single velocity, but an entire dynamic policy that governs the full trajectory. Formally, define the policy function space := (cid:8) π : RD RD (cid:9). Then, our goal is to distill policy generator network Gϕ : RD with learnable parameters ϕ, such that π(xt, t) = Gϕ(xtsrc, tsrc)(xt, t). As shown in Fig. 2 (c), π-Flow performs ODE-based denoising from tsrc to tdst via two stages: policy generation step, which feeds the initial state (xtsrc, tsrc) to the student network Gϕ to produce the policy π, i.e., π Gϕ(xtsrc , tsrc). Multiple policy integration substeps, which integrates the ODE by querying the policy velocity over multiple substeps, obtaining less noisy state by xtdst xtsrc + (cid:82) tdst π(xt, t) dt. tsrc Unlike previous few-step distillation methods, π-Flow decouples network evaluation steps from ODE integration substeps. This allows it to combine the key advantages of two paradigms: it performs only few network evaluations for efficient generation, similar to shortcut-predicting model, while also executing dense integration substeps, just like standard flow matching teacher. Thanks to its teacher-like ODE integration process, π-Flow student offers unprecedented advantage in training, as we can now follow well-established imitation learning (IL) approaches to directly match the policy velocity π(xt, t) to the teacher velocity Gθ(xt, t), as discussed later in 4. To identify the appropriate student policy families for fast image generation, we need to satisfy the following requirements: Efficiency. The policy should provide closed-form velocities with minimal overhead, so that rolling out dense (e.g., 100+) substeps incurs negligible cost compared to network evaluation."
        },
        {
            "title": "Preprint",
            "content": "Compatibility. The policy should have compact set of parameters that can be easily predicted by the student Gϕ with standard backbones (e.g., DiT (Peebles & Xie, 2023)). Expressiveness. The policy should be able to approximate complicated ODE trajectory starting from certain initial state xtsrc. Robustness. The policy should be able to handle trajectory variations that arise from perturbations to the initial state xtsrc. For instance, suboptimal student network will produce an erroneous mapping from xtsrc to π. This introduces the randomness that the policy needs to accommodate throughout the rollout. Consequently, the policy function should adapt its velocity output to variations in its state input xt, which is challenging requirement for network-free functions."
        },
        {
            "title": "3.1 DYNAMIC- ˆx(t)",
            "content": "0 POLICY 0 , where ˆx(t) 0 We introduce simple baseline policy called dynamicˆx(t) 0 policy (DX policy). DX policy defines π(xt, t) := xt ˆx(t) approximates the posterior moment Ex0p(x0xt)[x0] in Eq. (1). Along fixed trajectory starting from an initial state (xtsrc , tsrc), the posterior moment is only dependent on t. Therefore, we first predict grid of ˆx(ti) at evenly spaced times t1, ..., tN [tdst, tsrc] 0 by single evaluation of the student network Gϕ(xtsrc, tsrc). This is achieved by expanding the output channels of the student network and performing u-to-x0 reparameterization. Then, for arbitrary [tdst, tsrc], we obtain the approximated moment ˆx(t) Apparently, DX policy is fast, compatible, and expressive enough so that any -step teacher trajectory can be matched with grid points. However, its robustness is limited because ˆx(t) is not 0 adaptive to perturbations in xt. 0 by linear interpolation over the grid. 3.2 GMFLOW POLICY i=1 (cid:80)K For stronger robustness, we incorporate an advanced GMFlow policy based on the closedform GM velocity field in Chen et al. (2025). GMFlow policy expands the network output channels to predict factorized Gaussian mixture (GM) velocity distribution q(uxtsrc) = k=1 AikN (cid:0)ui; µik, s2I(cid:1), where Aik R+, µik RC, R+ are GM parameters pre- (cid:81)L dicted by the network, factorizes the data dimension into sequence length and channel size C, and is hyperparameter specifying the number of mixture components. This representation enables closed-form velocity expression at any 0 < < tsrc (see for details). Its speed and compatibility has already been discussed in Chen et al. (2025), thus we focus on analyzing its expresiveness and robustness. Expressiveness. With the factorization, each individual C-dimensional GM needs to be expressive enough to approximate C-dimensional chunk of the teacher trajectory. In E, we rigorously prove the following theorem, demonstrating GMFlows expressiveness. Theorem 1 (A GMFlow policy with = can accurately approximate any -step trajectory). xtn RC for = 1, . . . , , Given pairwise distinct times t1, . . . , tN (0, 1] and vectors xtn , there exists GM parameterization of p(x0) with components, such that xtn can be approximated arbitrarily well using Eq (1) at = tn for every = 1, . . . , . In practice, we can use (e.g., = 8) since the teacher trajectory is mostly smooth. More analysis of GMFlow hyperparameters are presented in C.1. Robustness. GMFlow is highly robust against trajectory perturbation due to its probabilistic origin. Unlike DX policy, GMFlow models fully dynamic denoising posterior (Eq. (23)) dependent on both xt and t. Leveraging its robustness, the policy can be flexibly altered via GM dropout in training ( 4) and GM temperature in inference ( B.1), both improving generalization performance. 4 π-ID: POLICY-BASED IMITATION DISTILLATION With the policy rollout sharing the same format as the teachers ODE integration, it is straightforward to adopt imitation learning to learn the policy by directly matching the policys velocity to the"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1: On-policy π-ID. Input: NFE , teacher Gθ, student Gϕ, condition 2 NFE , NFE , , 1 (cid:9) 1 Sample tsrc from (cid:8) 1 2 Initialize xtsrc (data-free or data-dependent) 3 π Gϕ(xtsrc , tsrc, c) 4 πD stopgrad(π) 5 Lϕ 0 6 for finite samples (cid:0)tsrc 1 πD(xt, t) dt 7 NFE , tsrc (cid:1) do xt xtsrc + (cid:82) Lϕ Lϕ + 1 9 ϕ Adam(ϕ, ϕLϕ) 8 tsrc 2 Gθ(xt, t, c) π(xt, t)2 // optimizer step Figure 3: On-policy flow imitation distillation. Intermediate states are sampled along the detached policy rollout, where the loss matches the policy to the teacher. teachers velocity. In this section, we introduce simple policy-based imitation distillation (π-ID) algorithm based on DAgger-style (Ross et al., 2011) on-policy imitation. On-policy imitation learning is robust to error accumulation since it trains the policy on its own trajectory, allowing the teachers corrective signal to steer deviating trajectory back on track. As shown in Fig. 3 and Algorithm 1, for time interval from tsrc to tdst (i.e., 1-NFE segment), we first feed the initial state (xtsrc , tsrc) to the student network Gϕ to obtain the policy π. We then sample an intermediate time (tdst, tsrc] and roll out detached policy πD from tsrc to using highaccuracy ODE integration (with small step size of 1/128), yielding an intermediate state xt on the policy trajectory. This state is fed to both the learner policy π and the frozen teacher Gθ, which produce their respective velocities. Finally, we compute standard ℓ2 flow matching loss between the two velocities, and backpropagate its gradients through the policy π to the student network Gϕ. Because the student forward/backward pass dominates compute while policy and teacher queries are relatively cheap, we may repeat the rollout-and-matching step multiple times for additional teacher supervisions. In practice, we sample two intermediate states per student forward pass. Data-dependent and data-free π-ID. The initial state xtsrc can be obtained via forward diffusion from real data x0 (data-dependent Algorithm 2), or via π-Flows reverse denoising from random noise x1 (data-free Algorithm 3). Both methods have roughly the same computational cost, and comparable performance, as demonstrated in the experiments ( 5)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "To demonstrate the versatility of π-Flow, we evaluate it with three distinct image generation models of different scales and architectures: DiT(SiT)-XL/2 (675M) (Peebles & Xie, 2023; Ma et al., 2024; Vaswani et al., 2017) for ImageNet 2562 (Deng et al., 2009) class-conditioned generation, FLUX.1-12B (Black Forest Labs, 2024b) and Qwen-Image-20B (Wu et al., 2025) for text-to-image generation. 5.1 IMPLEMENTATION DETAILS In this subsection, we discuss key implementation details essential to model performance. More training details and hyperparameter choices are presented in C. GM dropout. Dropout is widely adopted technique in supervised/imitation learning and reinforcement learning to improve generalization (Srivastava et al., 2014; Cobbe et al., 2019). For the GMFlow policy, we introduce GM dropout in training to stochastically perturb and diversify π-ID rollouts to make the policy more robust to potential trajectory variations. Given the GM mixture weights Aik of the detached policy πD, we sample binary mask for each component = 1, , and multiply it into Aik synchronously across all = 1, , L. The masked weights are then renormalized and used for the detached rollout. By exploring alternative GM modes, this simple technique improves the policys robustness, yielding better FID on ImageNet 2562 ( 5.2). Handling guidance-distilled teachers. On-policy imitation learning assumes the teacher is robust to out-of-distribution (OOD) intermediate states and can steer trajectories back on track. This generally holds for standard flow models with classifier-free guidance (CFG) (Ho & Salimans, 2021),"
        },
        {
            "title": "Preprint",
            "content": "Table 1: 1-NFE generation results of π-Flow with DX and GMFlow policies on ImageNet. Tested after 40k training iterations. FM stands for standard flow matching. Policy DX (N = 10) DX (N = 20) DX (N = 40) GM (K = 8) GM (K = 32) GM (K = 32) FM GM (K = 32) w/o dropout FM Teacher FID IS Precision Recall REPA REPA REPA REPA REPA 4.73 4.44 4.90 3.07 3.08 3.65 4.14 327.6 329.8 321.8 336.9 341.7 282.0 279.6 0.781 0.786 0.778 0.789 0. 0.797 0.799 0.514 0.531 0.537 0.572 0.562 0.533 0.525 Table 2: Comparison with previous few-step DiTs on ImageNet. Model NFE FID iCT iMM MeanFlow FACM (REPA) π-Flow (GM-REPA) iCT Shortcut MeanFlow π-Flow (GM-FM) π-Flow (GM-REPA) 2 12 2 2 2 1 1 1 1 1 20.30 7.77 2.20 1.52 1.97 34.24 10.60 3.43 3.34 2. which exhibit error-correction behavior (Chidambaram et al., 2024). However, FLUX.1 dev (Black Forest Labs, 2024b) is guidance-distilled model without true CFG and is less robust to OOD inputs. To mitigate OOD exposure, we adopt scheduled trajectory mixing strategy, which rolls out the trajectory using mixture of teacher and student with linearly decaying teacher ratio (see B.2 for details). 5.2 IMAGENET DIT Our study utilizes two pretrained teachers with the same DiT architecture: standard flow matching (FM) DiT (the baseline in Chen et al. (2025)), and the REPA DiT (Yu et al., 2025). Interval CFG (Kynkaanniemi et al., 2024) is applied to both teachers to maximize their performance. Each π-Flow student is initialized with the teacher weights and then fully finetuned using the π-ID loss. Evaluation metrics. We adopt the standard evaluation protocol in ADM (Dhariwal & Nichol, 2021) with the following metrics: Frechet Inception Distance (FID) (Heusel et al., 2017), Inception Score (IS), and PrecisionRecall (Kynkaanniemi et al., 2019). Comparison of DX and GMFlow policies. As shown in Table 1, both policies yield strong 1NFE FIDs after 40k training iterations, with the GMFlow policy consistently outperforming the DX policy by clear margin. Notably, the DX policy exhibits sensitivity to the hyperparameter (number of grid points), whereas the GMFlow policy produces consistent results across different values of (number of Gaussians). Comparison with prior few-step DiTs. In Table 2, we compare π-Flow (GM policy with = 32) to prior few-step DiTs on ImageNet 2562: iCT (Song & Dhariwal, 2024), Shortcut models (Frans et al., 2025), iMM (Zhou et al., 2025), MeanFlow (Geng et al., 2025), and FACM (Peng et al., 2025). The concurrent work FACM improves MeanFlow with an auxiliary loss to achieve leading 2-NFE FID, though its 1-NFE performance on the standard DiT architecture is not reported. Our π-Flow adopts simpler imitation learning objective without the expensive JVP operation in MeanFlow, yet still outperforms the original MeanFlow DiT across both 1-NFE and 2-NFE generation. Ablation study on GM dropout. From the two bottom rows in Table 1 we conclude that our standard implementation with 0.05 GM dropout rate yields better FID and Recall compared to the setting without dropout, confirming the effectiveness of our GM dropout technique. 5.3 FLUX.1-12B AND QWEN-IMAGE-20B For text-to-image generation, we distill the 12B FLUX.1 dev (Black Forest Labs, 2024b) and 20B Qwen-Image (Wu et al., 2025) models into π-Flow students. During student training, we freeze the base parameters inherited from the teacher and finetune only the expanded output layer along with 256-rank LoRA adapters (Hu et al., 2022) on the feed-forward layers. For data-dependent distillation, we prepare 2.3M one-megapixel (1MP) images captioned with Qwen2.5-VL (Bai et al., 2025). In the data-free setting, we use only the generated captions as conditioning inputs while keeping the same 1MP resolution when initializing the noise. Evaluation protocol. We conduct comprehensive evaluation on 10242 high-resolution image generation from three distinct prompt sets: (a) 10K captions from the COCO 2014 validation set (Lin"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Quantitative comparisons on COCO-10k dataset and HPSv2 prompt set. COCO-10k prompts HPSv2 prompts Model Distill method NFE Data align. Pref. align. FID pFID CLIP VQA HPSv2.1 FID pFID CLIP VQA HPSv2.1 Pref. align. Teacher align. Prompt align. Prompt align. FLUX.1 dev FLUX Turbo Hyper-FLUX π-Flow (GM-FLUX) - GAN CD+Re π-ID SenseFlow (FLUX) π-Flow (GM-FLUX) π-Flow (GM-FLUX) VSD+CD+GAN π-ID π-ID (data-free) 50 8 8 4 4 4 27.8 26.7 29.8 29.0 34.1 29.8 29.7 Qwen-Image - 502 34.1 Qwen-Image Lightning π-Flow (GM-Qwen) π-Flow (GM-Qwen) VSD π-ID π-ID (data-free) 4 4 4 37.5 36.0 36.0 34. 32.0 33.3 35.4 44.2 36.1 36.2 45.6 51.6 46.1 45.7 0.268 0. 0.267 0.268 0.268 0.266 0.269 0.269 0.900 0.894 0.901 0.879 0.903 0.905 0.282 0. 0.280 0.281 0.282 0.935 0.934 0.936 0.309 0.308 0.309 0.311 0.308 0.308 0.310 0. 0.322 0.314 0.315 - 13.8 15.6 12.6 23.3 14.3 14.4 - 15.6 12.8 12. - 0.284 0.805 18.5 22.2 15.9 28.2 19.2 19.7 0.286 0.285 0. 0.283 0.288 0.287 0.814 0.807 0.810 0.806 0.816 0.813 - 0.302 0. 19.7 16.6 16.8 0.299 0.300 0.301 0.867 0.860 0.862 0.314 0.313 0.315 0.316 0.318 0.313 0. 0.309 0.328 0.310 0.312 Table 4: Quantitative comparisons on OneIG-Bench (Chang et al., 2025). Distill Method NFE Alignment Text Diversity Style Reasoning Model FLUX.1 dev FLUX Turbo Hyper-FLUX π-Flow (GM-FLUX) SenseFlow (FLUX) π-Flow (GM-FLUX) π-Flow (GM-FLUX) - GAN CD+Re π-ID VSD+CD+GAN π-ID π-ID (data-free) 50 8 8 8 4 4 4 Qwen-Image - 502 Qwen-Image Lightning π-Flow (GM-Qwen) π-Flow (GM-Qwen) VSD π-ID π-ID (data-free) 4 4 4 0.790 0.791 0.790 0. 0.776 0.799 0.799 0.880 0.885 0.875 0.881 0.556 0.334 0.530 0.517 0.384 0.437 0. 0.888 0.923 0.892 0.890 0.238 0.234 0.198 0.234 0.151 0.229 0.224 0. 0.116 0.180 0.176 0.370 0.370 0.369 0.369 0.343 0.360 0.363 0.427 0.417 0.434 0. 0.257 0.239 0.254 0.256 0.238 0.251 0.249 0.306 0.311 0.298 0.300 et al., 2014), (b) 3200 prompts from the HPSv2 benchmark (Wu et al., 2023), and (c) 1120 prompts from OneIG-Bench (Chang et al., 2025). For the COCO and HPSv2 sets, we report common metrics including FID (Heusel et al., 2017), patch FID (pFID) (Lin et al., 2024a), CLIP similarity (Radford et al., 2021), VQAScore (Lin et al., 2024b), and HPSv2.1 (Wu et al., 2023). On COCO prompts, FIDs are computed against real images, reflecting data alignment. On HPSv2, FIDs are computed against the 50-step teacher generations, reflecting teacher alignment. CLIP and VQAScore measure prompt alignment, while HPSv2 captures human preference alignment. For OneIG-Bench, we adopt its official evaluation protocol and metrics. All quantitative results are presented in Table 3 and 4. Competitor models. We compare π-Flow against other few-step student models distilled from the same teacher. For FLUX, we compare against: 4-NFE SenseFlow (Ge et al., 2025), primarily leveraging variational score distillation (VSD) (Wang et al., 2023), also known as distribution matching distillation (DMD) (Yin et al., 2024b); 8-NFE Hyper-FLUX (Ren et al., 2024), trained with consistency distillation (CD) (Song et al., 2023) and reward models (Re) (Xu et al., 2023); 8-NFE FLUX Turbo, based on GAN-like adversarial distillation (Goodfellow et al., 2014; Sauer et al., 2024b). For Qwen-Image, we compare with the 4-NFE Qwen-Image Lighting based on VSD (ModelTC, 2025). Note that the 4-NFE FLUX.1 schnell is distilled from the closed-source FLUX.1 pro instead of the publicly available FLUX.1 dev (Black Forest Labs, 2024a), so we do not compare with it directly, but include further discussion in D. Strong all-around performance. As shown in Table 3 and Table 4, π-Flow demonstrates strong allaround performance, outperforming other few-step students on roughly 70% of all metrics, without exhibiting obvious weaknesses in any specific area. Superior diversity and teacher alignment. π-Flow consistently achieves the highest diversity scores and the best teacher-referenced FIDs by clear margins, especially in the 4-NFE setting. These results strongly suggest that π-Flow effectively avoids both diversity collapse and style drift. As result, most of its scores closely match those of the teacher, with some even slightly surpassing the teacher scores (e.g., prompt alignment and several Qwen-Image OneIG scores). Its strong"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Images generated from the same batch of initial noise by π-Flows, teachers, and VSD students (SenseFlow, Qwen-Image Lightning). π-Flow models produce diverse structures that closely mirror the teachers. In contrast, VSD students tend to repeat the same structure. Notably, SenseFlow mostly generates symmetric images. teacher alignment is also evident in Fig. 4, where π-Flow generates structurally similar images to the teachers from the same initial noise. Comparison with VSD (DMD) students. VSD models are notable for high visual quality, sometimes surpassing teachers in quality and preference metrics. However, they are widely known to suffer from mode collapse, as reflected in our experiments: both SenseFlow and Qwen-Image Lightning show significant drops in diversity and FIDs. Visual examples in Fig. 4 further highlight the collapse, where different initial noises produce visually similar images with only minor variations. In contrast, π-Flow maintains high quality and diversity without sacrificing either aspect. Comparison with other students. FLUX Turbo achieves better data alignment FIDs than the teacher due to GAN training, yet its text rendering performance is significantly weaker, as shown in Fig. 5. Meanwhile, Hyper-FLUX often produces undesirable texture artifacts and fuzzy details, whereas π-Flow achieves superior detail rendering, as shown in Fig. 6."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Images generated from the same initial noise by π-Flow and FLUX Turbo. π-Flow renders coherent texts, whereas FLUX Turbo underperforms in text rendering. Figure 6: Images generated from the same initial noise by π-Flow and Hyper-FLUX. π-Flow produces notably finer details, as highlighted in the zoomed-in patches. Data-dependent vs. data-free. As shown in Table 3 and Table 4, data-dependent and data-free π-Flow models achieve nearly identical results. This demonstrates the practicality of π-Flow in scenarios where high-quality data is unavailable. GMFlow vs. DX policy. Consistent with prior ImageNet findings, the DX policy slightly underperforms comapred to the GMFlow policy  (Table 5)  , highlighting the latters superior robustness. Convergence. Figure 7 illustrates the convergence of π-Flow (GM-Qwen) over training iterations. Both FID and Patch FID scores initially improve rapidly, outperforming Qwen-Image Lightning within the first 400 iterations, and continue to improve steadily thereafter. This contrasts with previous GAN or VSD-based methods that often require frequent checkpointing and cherry-picking (Ge et al., 2025), demonstrating the scalability and robustness of our approach."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Prior work on diffusion model distillation primarily focuses on predicting shortcuts towards less noisy states, with training objectives ranging from direct regression to distribution matching. Early work (Luhman & Luhman, 2021) directly regresses the teachers ODE integral in single step, but suffers from degraded quality, since regressing x0 with an ℓ2 loss tends to produce blurry results. Progressive distillation methods (Salimans & Ho, 2022; Liu et al., 2023; 2024; Frans et al., 2025) make further improvements via multi-stage process that progressively increases the students step size and reduces its NFE by regressing the previous stages multi-step outputs with fewer steps, yet this introduces error accumulation. Consistency-based models (Song et al., 2023; Kim et al., 2024; Song & Dhariwal, 2024; Geng et al., 2025) implicitly impose velocity-based regression loss, which improves quality compared to x-based regression. However, the velocity of shortcut-predicting student must be constructed implicitly using either inaccurate finite differences or expensive Jacobianvector products (JVPs). Moreover, their quality is still limited due to accumulation of velocity errors into the integrated"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Comparisons between DX and GMFlow policies on text-to-image generation. Policy Teacher HPSv2 prompts OneIG-Bench FID pFID HPSv2.1 Text Diversity DX (N = 10) FLUX GM (K = 8) FLUX 14.9 14.3 DX (N = 10) Qwen-Image 12.7 GM (K = 8) Qwen-Image 12.8 20.9 19.2 17.0 16.6 0.313 0. 0.306 0.310 0.397 0.437 0.869 0.892 0.225 0.229 0.185 0.180 Teacher-referenced Figure 7: FID and Patch FID of GM-Qwen evaluated on HPSv2 prompts. state. Therefore, in practice, consistency distillation is often augmented with additional objectives to improve quality (Ren et al., 2024; Zheng et al., 2025), further complicating training. Conversely, distribution matching approaches (Yin et al., 2024b;a; Sauer et al., 2024b; Zhou et al., 2024; Luo et al., 2024; Salimans et al., 2024; Zhou et al., 2025) adopt score matching and adversarial training to align the students output distribution with the teachers, achieving superior quality but risking diversity loss due to mode collapse. Their common reliance on auxiliary networks also introduces additional tuning complexity and can lead to stability issues at scale (Ge et al., 2025)."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced policy-based flow models (π-Flow), novel framework for few-step generation in which the network outputs fast policy that enables accurate ODE integration via dense substeps reach the denoised state. To distill π-Flow models, we proposed simple on-policy imitation learning approach that reduces the training objective to single ℓ2 loss, mitigating error accumulation and qualitydiversity trade-offs. Extensive experiments distilling ImageNet DiT, FLUX.1-12B, and Qwen-Image-20B models show that few-step π-Flows consistently attain teacher-level image quality while significantly outperforming competitors in diversity and teacher alignment. π-Flow offers scalable, principled paradigm for efficient, high-quality generation and opens new directions for future research, such as exploring more robust policy families, improved distillation objectives, and extensions to other applications (e.g., video generation). Reproducibility statement. To facilitate reproduction, we describe the detailed training procedures in Algorithms 2 and 3, and list all important hyperparameters in C. Acknowledgements. This project was partially done while Hansheng Chen was supported by the Qualcomm Innovation Fellowship and partially done while Hansheng Chen was an intern at Adobe Research. We would like to thank Jianming Zhang and Hailin Jin for their great support throughout the internship, and Xingtong Ge for the help in evaluating SenseFlow."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence In NeurIPS, NIPS15, pp. 11711179, Cambridge, prediction with recurrent neural networks. MA, USA, 2015. MIT Press. Black Forest Labs. Flux.1 [schnell]. https://huggingface.co/spaces/ black-forest-labs/FLUX.1-schnell, 2024a."
        },
        {
            "title": "Preprint",
            "content": "Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024b. Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. In NeurIPS, 2025. Hansheng Chen, Kai Zhang, Hao Tan, Zexiang Xu, Fujun Luan, Leonidas Guibas, Gordon Wetzstein, and Sai Bi. Gaussian mixture flow matching models. In ICML, 2025. Muthu Chidambaram, Khashayar Gatmiry, Sitan Chen, Holden Lee, and Jianfeng Lu. What does In NeurIPS, 2024. URL https: guidance do? fine-grained analysis in simple setting. //openreview.net/forum?id=AdS3H8SaPi. Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. In ICML, 2019. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pp. 248255, 2009. doi: 10.1109/CVPR.2009.5206848. Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. In ICLR, 2022. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), NeurIPS, 2021. URL https://openreview.net/forum?id=AAWuCvzaVt. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. In ICLR, 2025. URL https://openreview.net/forum?id=OlzB6LnXcS. Xingtong Ge, Xin Zhang, Tongda Xu, Yi Zhang, Xinjie Zhang, Yan Wang, and Jun Zhang. Senseflow: Scaling distribution matching for flow-based text-to-image distillation, 2025. URL https://arxiv.org/abs/2506.00523. Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. In NeurIPS, 2025. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), NeurIPS, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/ paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In NeurIPS, 2022. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In CVPR, 2024."
        },
        {
            "title": "Preprint",
            "content": "Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In ICLR, 2024. URL https://openreview.net/ forum?id=ymjI8feDTD. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2014. Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila."
        },
        {
            "title": "Improved",
            "content": "precision and recall metric for assessing generative models. In NeurIPS, 2019. Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In NeurIPS, 2024. Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation, 2024a. URL https://arxiv.org/abs/2402.13929. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), ECCV, pp. 740755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In ECCV, 2024b. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow In ICLR, 2023. URL https://openreview.net/ matching for generative modeling. forum?id=PqvMRDCJT9t. Qiang Liu. Rectified flow: marginal preserving approach to optimal transport, 2022. URL https://arxiv.org/abs/2209.14577. Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=XVjTT1nw5z. Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and qiang liu. Instaflow: One step is enough In ICLR, 2024. URL https:// for high-quality diffusion-based text-to-image generation. openreview.net/forum?id=1k4yZbbDqX. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), NeurIPS, 2022. URL https: //openreview.net/forum?id=2uAaGwlP_V. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023. URL https://arxiv. org/abs/2211.01095. Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed, 2021. URL https://arxiv.org/abs/2101.02388. Weijian Luo, Zemin Huang, Zhengyang Geng, J. Zico Kolter, and Guo jun Qi. One-step diffusion distillation through score implicit matching. In NeurIPS, 2024. Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. ModelTC. Qwen-image-lightning. Qwen-Image-Lightning, 2025. https://github.com/ModelTC/ William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023."
        },
        {
            "title": "Preprint",
            "content": "Yansong Peng, Kai Zhu, Yu Liu, Pingyu Wu, Hebei Li, Xiaoyan Sun, and Feng Wu. Flow-anchored consistency models, 2025. URL https://arxiv.org/abs/2507.03738. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763, 2021. Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, XING WANG, and Xuefeng Xiao. Hyper-SD: Trajectory segmented consistency model for efficient image synthesis. In NeurIPS, 2024. URL https://openreview.net/forum?id=O5XbOoi0x3. Hans Richter. Parameterfreie abschatzung und realisierung von erwartungswerten. Blatter der doi: 10.1007/BF02808864. URL ISSN 1864-0303. DGVFM, 3(2):147162, Apr 1957. https://doi.org/10.1007/BF02808864. Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Geoffrey Gordon, David Dunson, and Miroslav Dudık (eds.), AISTATS, volume 15 of Proceedings of Machine Learning Research, pp. 627635, Fort Lauderdale, FL, USA, 1113 Apr 2011. PMLR. URL https://proceedings.mlr. press/v15/ross11a.html. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. NeurIPS, 37:3604636070, 2024. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia, SA 24, New York, NY, USA, 2024a. Association for Computing Machinery. ISBN 9798400711312. doi: 10.1145/3680528.3687625. URL https://doi.org/10. 1145/3680528.3687625. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In ECCV, pp. 87103, 2024b. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pp. 22562265, 2015. Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In ICLR, 2024. URL https://openreview.net/forum?id=WNzy9bRDvG. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In ICML, 2023. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: simple way to prevent neural networks from overfitting. JMLR, 15(1):19291958, January 2014. ISSN 1532-4435. Vladimir Tchakaloff. Formules de cubatures mecaniques `a coefficients non negatifs. Bulletin des Sciences Mathematiques, 81(2):123134, 1957. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), NeurIPS, volume 30. Curran Associates, Inc., 2017. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In NeurIPS, 2023."
        },
        {
            "title": "Preprint",
            "content": "Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/ 2508.02324. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis, 2023. URL https://arxiv.org/abs/2306.09341. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In NeurIPS, pp. 1590315935, 2023. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and In William T. Freeman. NeurIPS, 2024a. URL https://openreview.net/forum?id=tQukGCDaNT. Improved distribution matching distillation for fast image synthesis. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024b. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In ICLR, 2023. URL https://openreview.net/forum?id=Loek7hfb46P. Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. In NeurIPS, 2023. Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Large scale diffusion distillation via scoreregularized continuous-time consistency, 2025. URL https://arxiv.org/abs/2510. 08431. Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. In ICML, 2025. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: exponentially fast distillation of pretrained diffusion models for one-step generation. In ICML, 2024. 14 7 8 9 10 11 12 14 15 6 7 8 9 10 11 13 14"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 2: Data-dependent on-policy π-ID training loop with time shifting. Input: NFE , teacher Gθ, data-condition distribution p(dx0, dc), shift Output: Student Gϕ 2 1 Initialize student params ϕ NFE , , 1 (cid:9) 2 (cid:8) 1 NFE , 3 for finite samples x0, p(dx0, dc), ϵ (0, I), τ (0, 1) do 4 5 6 τsrc min{ τsrc τsrc and τsrc τ } τdst max{ τdst τdst { 0 } and τdst < τsrc } tsrc mτsrc 1+(m1)τsrc xtsrc αtsrc x0 + σtsrc ϵ π Gϕ(xtsrc , tsrc, c) πD stopgrad(π) or πD dropout(stopgrad(π)) Lϕ 0 for finite samples τ (τdst, τsrc) do 1+(m1)τ mτ xt xtsrc + (cid:82) Lϕ Lϕ + 1 ϕ Adam(ϕ, ϕLϕ) πD(xt, t) dt tsrc 2 Gθ(xt, t, c) π(xt, t)2 // can be adjusted to reduce final step size // time shifting (Esser et al., 2024) // can be replaced with Eq. (6) // optimizer step Algorithm 3: Data-free on-policy π-ID training loop with time shifting. Input: NFE , teacher Gθ, condition distribution p(dc), shift Output: Student Gϕ 1 Initialize student params ϕ 2 for finite samples p(dc), x1 (0, I) do 3 4 5 τsrc 1, Lϕ 0 while τsrc > 0 do tsrc 1 1+(m1)τdst τdst τsrc 1 NFE tdst mτdst π Gϕ(xtsrc , tsrc, c) πD stopgrad(π) or πD dropout(stopgrad(π)) for finite samples τ (τdst, τsrc) do 1+(m1)τ mτ xt xtsrc + (cid:82) Lϕ Lϕ + τsrcτdst tsrc xtdst xtsrc + (cid:82) tdst τsrc τdst, tsrc tsrc tdst πD(xt, t) dt 2 πD(xt, t) dt Gθ(xt, t, c) π(xt, t)2 // can be adjusted to reduce final step size // time shifting (Esser et al., 2024) // can be replaced with Eq. (6) ϕ Adam(ϕ, ϕLϕ) // optimizer step"
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "In preparing this manuscript, we used large language models (LLMs) as general-purpose writing assistants for grammar corrections, rephrasing, and clarity/concision edits. All LLM-suggested edits were reviewed and verified by the authors, who take full responsibility for the final manuscript."
        },
        {
            "title": "B ADDITIONAL TECHNICAL DETAILS",
            "content": "B.1 GM TEMPERATURE Inspired by the temperature parameter in language models, we introduce similar temperature parameter for the GMFlow policy during inference. Let > 0 be the temperature parameter. Given k=1 AkN (cid:0)u; µk, s2I(cid:1), the new GM probC-dimensional GM velocity distribution q(uxtsrc) = (cid:80)K"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Three stages of scheduled trajectory mixing. (a) Off-policy behavior cloning with teacher ratio of 1. (b) Mixed teacher and detached-policy segments with decaying teacher ratio. (c) Onpolicy imitation learning with teacher ratio of 0  (Fig. 3)  . ability with temperature is defined as: 1 (uxtsrc) (cid:82) RC 1 (uxtsrc) du Although qT (uxtsrc) does not have general closed-form expression, it can be approximated by the following expression, which works very well as practical implementation: qT (uxtsrc) := (3) . qT (uxtsrc) (cid:88) k=1 (cid:80)K 1 z=1 1 N (cid:0)u; µk, s2T I(cid:1). (4) For the distilled FLUX and Qwen-Image models, we set = 0.3 for 4-NFE generation and = 0.7 for 8-NFE generation. An exception is that we do not apply temperature scaling to the final step, as we found this can impair texture details. As shown in Table 6, ablating GM temperature from the 4-NFE GM-FLUX leads to degraded teacher alignment. B.2 SCHEDULED TRAJECTORY MIXING FOR GUIDANCE-DISTILLED TEACHERS To reduce out-of-distribution exposure in imitation learning, scheduled sampling (Bengio et al., 2015) stochastically alternates between expert (teacher) and learner policy during trajectory integration, decaying the expert probability from 1 to 0. However, naively applying it to π-ID is impractical because the teacher flow model Gθ is much slower than the network-free policy πD. To maintain constant compute throughout training, we introduce scheduled trajectory mixing strategy. Since the teacher is slow, we fix the total number of teacher queries, allow each query to cover coarse, longer step initially, and gradually shrink the teacher step size while filling the gaps with the fast policy πD. As shown in Fig. 8 (a), training initially adopts fully off-policy teacher trajectory (behavior cloning). At the beginning time ta of each teacher step, we roll in the learner policy π, integrate it over the same interval from ta to tb, and match its average velocity to the teacher velocity with the ℓ2 loss: (cid:34) Lϕ = (cid:13) (cid:13) Gθ(xta , ta) (cid:13) (cid:13) 1 2 1 tb ta (cid:90) tb ta π(xt, t) dt (cid:13) (cid:13) (cid:13) (cid:13) 2(cid:35) . (5) As training progresses (Fig. 8 (b)), we then mix teacher and detached-policy segments while using the same loss, and linearly decay the teacher ratiothe sum of teacher step lengths divided by the total interval length tsrc tdst. Finally, when the teacher ratio reaches 0, training reduces to onpolicy π-ID. All teacher step boundaries (starts and ends) are randomly sampled within the interval [tdst, tsrc] under the teacher ratio constraint, so that step sizes and locations vary while the total teacher-covered length follows the current ratio schedule. We apply scheduled trajectory mixing exclusively when distilling the FLUX.1 dev model, as it lacks real CFG. Since omitting CFG doubles the teachers speed, we increase the number of intermediate samples (teacher steps) to 4 accordingly. B.3 MICRO-WINDOW VELOCITY MATCHING For on-policy π-ID, in practice we found that replacing the instantaneous velocity matching loss in Algorithm 1 with modified average velocity loss over micro time window generally benefits"
        },
        {
            "title": "Preprint",
            "content": "Table 6: Ablation study on 4-NFE π-Flow (GM-FLUX), evaluated on the HPSv2 prompt set using teacher-referenced FID metrics (reflecting teacher alignment). Method π-Flow (GM-FLUX) 14.3 w/o GM temperature 14.9 14.6 w/o micro window FID pFID 19.2 20.1 20.3 Figure 9: The 128-NFE FLUX.1 dev often generates blurry images, whereas the 43-NFE FLUX.1 dev reduces the blur and produces sharper edges. training. The modified loss is defined as: Lϕ = 1 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Gθ(xt, t) 1 (cid:90) tt π(xt, t) dt 2 , (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (6) where is the window size. We set = 3/128 (three policy integration steps) for all FLUX.1 and Qwen-Image experiments. The benefits of micro-window velocity matching are threefold: It generally smooths the training signal, reducing sensitivity to sharp local variations in the teacher trajectory. It stabilizes the less robust DX policy. In the ImageNet experiments, we observe that training with the DX policy diverges without this modification. With = 3/128, the policy effectively mimics teacher sampling with 128 3 43 steps instead of 128 steps. For the guidance-distilled FLUX.1 dev model, we observe that the teacher often generates blurry images using 128-step sampling, while 43-step sampling yields sharper results (see Fig. 9). This behavior is inherited by the student, so micro-window velocity matching helps reduce blur. As shown in Table 6, ablating the micro window trick from the 4-NFE GM-FLUX leads to degraded teacher alignment."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Hyperparameters used in the ImageNet experiments. GM-FM (K = 32) GM-REPA (K = 8) GM-REPA (K = 32) DX-REPA (N = 10) DX-REPA (N = 20) DX-REPA (N = 40) GM-REPA (K = 32) 1-NFE 2-NFE GM dropout 0. 0.05 0.05 # of intermediate states Window size (raw) τ Shift Teacher CFG 2 - 1.0 2.7 2 - 1.0 3.2 2 - 1.0 3. - 2 - 2 - 10/128 5/128 3/128 1.0 3.2 1. 3.2 1.0 3.2 0.05 2 - 1.0 2.8 Teacher CFG interval [0, 0.6] [0, 0.7] [0, 0.7] [0, 0.7] [0, 0.7] [0, 0.7] [0, 0.7] Learning rate Batch size # of training iterations in Table 2 EMA param γ in Karras et al. (2024) 5e-5 4096 140K 7.0 5e4096 - 7.0 5e-5 4096 140K 7.0 5e-5 4096 - 7.0 5e4096 - 7.0 5e-5 4096 - 7.0 5e-5 4096 24K 7.0 Table 8: Hyperparameters used in FLUX and Qwen-Image experiments. 4-NFE 8-NFE GM-FLUX (K = 8) GM-Qwen (K = 8) DX-FLUX (N = 10) DX-Qwen (N = 10) GM-FLUX (K = 8) GM dropout GM temperature # of intermediate states 0.1 0. 4 0.1 0.3 2 - - 4 - - 2 0.1 0. 4 Window size (raw) τ 3/128 3/128 3/128 3/ 3/128 Shift Final step size scale Teacher CFG Learning rate Batch size # of training iterations # of decay iterations ( B.2) EMA param γ in Karras et al. (2024) 3.2 0.5 3.5 1e256 3K 2K 7.0 3.2 0. 4.0 1e-4 256 9K - 7. 3.2 0.5 3.5 1e-4 256 3K 2K 7.0 3.2 0.5 4.0 1e256 9K - 7.0 3.2 0. 3.5 1e-4 256 3K 2K 7. B.4 TIME SAMPLING For high resolution image generation, Esser et al. (2024) proposed time shifting mechanism to rescale the noise strength. Let τ be the pre-shift raw time and be the shift hyperparameter, the shifted time is defined as := mτ 1+(m1)τ . Following this idea, π-ID samples times uniformly in raw-time space and then applies the shift to remap those samples. Detailed time sampling routines are given in Algorithms 2 and 3. For FLUX.1 and Qwen-Image, we use fixed shift = 3.2, which is rounded approximation of FLUX.1s official dynamic shift at 1MP resolution. In addition, several diffusion/flow models reduce the noise strength at the final step to improve detail (Karras et al., 2022; Wu et al., 2025). Accordingly, for FLUX.1 and Qwen-Image we halve the final step size (relative to previous steps) in raw-time space."
        },
        {
            "title": "C ADDITIONAL IMPLEMENTATION DETAILS AND HYPERPARAMETERS",
            "content": "All models are trained with BF16 mixed precision, using the 8-bit Adam optimizer (Kingma & Ba, 2014; Dettmers et al., 2022) without weight decay. For inference, we use EMA weights with"
        },
        {
            "title": "Preprint",
            "content": "Table 9: FLUX.1 schnell evaluation results on COCO-10k dataset and HPSv2 prompt set. COCO-10k prompts HPSv2 prompts Model Distill method NFE Data align. Pref. align. FID pFID CLIP VQA HPSv2.1 CLIP VQA HPSv2.1 Prompt align. Prompt align. Pref. align. FLUX.1 schnell GAN 4 21.8 29.1 0.274 0.913 0. 0.297 0.843 0.301 Figure 10: Typical failure cases of FLUX.1 schnell. For reference, we also show the corresponding FLUX.1 dev and π-Flow results from the same initial noise. dynamic moment schedule (Karras et al., 2024). Detailed hyperparameter choices are listed in Table 7 and 8. C.1 DISCUSSION ON GMFLOW POLICY HYPERPARAMETERS For the GMFlow policy, we observed that the hyperparameters suggested by Chen et al. (2025) (K = 8, = VAE latent channel size) generally work well. These parameters play important roles"
        },
        {
            "title": "Preprint",
            "content": "in balancing compatibility, expresiveness, and robustness. larger improves expresiveness but impairs compatibility as it may complicate network training. larger improves robustness (since GMFlow models correlations within each C-dimensional chunk) but impairs expresiveness (raises the theoretical = bound). In addition, improving expresiveness may generally compromise robustness, due to the increased chance of encountering outlier trajectories during inference. DISCUSSION ON FLUX.1 SCHNELL The official 4-NFE FLUX.1 schnell model (Black Forest Labs, 2024a) (based on adversarial distillation (Sauer et al., 2024a)) is distilled from the closed-source FLUX.1 pro instead of the publicly available FLUX.1 dev. This makes direct comparison to the student models in Table 3 inequitable. For reference, nevertheless, we include the COCO-10k and HPSv2 metrics for FLUX.1 schnell in Table 9. These metrics reveal trade-off: while FLUX.1 schnell achieves significantly better data and prompt alignment than FLUX.1 dev, its preference alignment is substantially weaker than FLUX.1 dev and all of its students. To validate this observation, we conducted human preference study. Our 4-NFE π-Flow (GMFLUX) was compared against FLUX.1 schnell on 200 images generated from HPSv2 prompts. π-Flow was preferred by users 59.5% of the time, aligning with the HPSv2.1 preference metric. Furthermore, qualitative comparisons in Fig. 10 reveals that FLUX.1 schnell is prone to frequent structural errors (e.g., missing/extra/distorted limbs), whereas π-Flow maintains coherent structures. PROOF OF THEOREM We will prove that GM with components suffices for approximating any -step trajectory in RC by first establishing Theorem 2, and then applying the RichterTchakaloff theorem to show that mixture of Dirac deltas satisfy all ODE moment equations, which finally leads to Gaussian components. xtn RC for Theorem 2. Given pairwise distinct times t1, . . . , tN (0, 1] and vectors xtn, = 1, . . . , , there exists probability measure p(dx0) on RC, such that Eq (1) holds at = tn for every = 1, . . . , . E.1 MOMENT EQUATION For every (0, 1], the ODE moment equation has the following equivalent forms: xt = (cid:90) RC xt x0 (cid:90) p(dx0xt) = (cid:90) xt p(dx0xt) xt x0 p(dx0xt) RC (cid:90) RC x0 xt + xt (x0 xt + xt)N (cid:0)xt; αtx0, σ2 RC p(dx0xt) = I(cid:1)p(dx0) = 0 tp(xt) (cid:90) RC (cid:90) RC (x0 xt + xt)N (cid:0)xt; αtx0, σ2 I(cid:1)p(dx0) = 0. (7) Let g(t, x0) := (x0 xt + xt)N (cid:0)xt; αtx0, σ2 written as multivariate homogeneous Fredholm integral equation of the first kind: I(cid:1) be kernel function. The above equation can be (cid:90) RC g(t, x0)p(dx0) = 0. (8) To prove Theorem 2, we need to show that there exists probability measure p(dx0) on RC that solves the Fredholm equation at = tn for every = 1, , ."
        },
        {
            "title": "Preprint",
            "content": "E.2 UNIVARIATE MOMENT EQUATION To prove the existence of solution to the multivariate Fredholm equation, we can simplify the proof into univariate case by showing that an element-wise probability factorization p(dx0) = (cid:81)C i=1 p(dxi0) exists that solves the Fredholm equation. In this case, Eq. (7) can be written as: = 1, 2, , C, (cid:90) (xi0 xit + xit)N (cid:0)xit; αtxi0, σ2 R (cid:1)p(dxi0) (cid:90) (cid:89) j=i (cid:0)xj t; αtxj 0, σ2 (cid:1)p(dxj 0) = 0 = 1, 2, , C, (cid:90) (xi0 xit + xit)N (cid:0)xit; αtxi0, σ2 (cid:1)p(dxi0) = 0. (9) To see this, we need to prove that there exists probability measure p(x0) on that solves the following univariate Fredholm equation at = tn for every = 1, , : (cid:90) g(t, x0)p(dx0) = 0, (10) (11) (12) (13) (14) (15) where g(t, x0) := (x0 xt + xt)N (cid:0)xt; αtx0, σ2 (cid:1) is the univariate kernel function. E.3 CONVEX COMBINATION Lemma 1. Define the vector function: γ : RN , γ(x0) = (g(t1, x0), g(t2, x0), , g(tN , x0)). Then, the zero vector lies in the convex hull in RN , i.e.: 0 conv{ γ(x0) x0 } RN . Proof. Define := conv{ γ(x0) x0 }. Assume for the sake of contradiction that 0 / S. By the supporting and separating hyperplane theorem, there exists = 0 RN , such that: In particular, this implies that: χ S, w, χ 0. x0 R, w, γ(x0) 0. Define h(x0) := w, γ(x0) = (cid:80)N n=1 wng(tn, x0). Recall the definition of g(t, x0) that: g(t, x0) = (x0 xt + xt)N (cid:0)xt; αtx0, σ2 (cid:1) = x0 xt + xt 2πt2 (cid:18) exp (xt αtx0)2 2σ2 (cid:19) . Let be an index with wn = 0 for which the exponential term above decays the slowest, i.e.: (cid:26) α2 tn 2σ2 tn α2 tn 2σ2 wn = 0 = min (cid:12) (cid:12) (cid:12) (cid:12) (cid:27) . tn (16) Note that since α2 2σ2 as x0 , h(x0) is dominated by the n-th component, i.e.: is monotonic, for every = with wn = 0, we have h(x0) = wn x0 xtn + tn xtn (cid:112)2πt2 (cid:18) exp (xtn αtn x0)2 2σ2 tn . Therefore, α2 2σ2 tn tn α2 tn 2σ2 tn > (cid:19) (1 + O(1)). (17) Because the term x0 xtn +tn xtn changes sign between and +, h(x0) takes both positive and negative values. This contradicts the hyperplane implication that h(x0) 0. Therefore, we conclude that 0 S. By Lemma 1 and Caratheodorys theorem, the zero vector can be expressed as convex combination of at most +1 points on γ(x0). Therefore, there exists finite-support probability measure p(dx0) consisting of + 1 Dirac delta components that solves the univariate Fredholm equation at = tn for every = 1, , , completing the proof of Theorem 2."
        },
        {
            "title": "Preprint",
            "content": "E.4 COMPONENTS SUFFICE Richters extension to Tchakaloffs theorem states as follows. Theorem 3 (Richter (1957); Tchakaloff (1957)). Let be finite-dimensional space of measurable functions on RC. For some probability measure p(dx0) on RC, define the moment functional: Λ : R, Λ[g] := (cid:90) RC g(x0)p(dx0). (18) Then there exists K-atomic measure p(dx0) = (cid:80)K such that: k=1 Akδµk (dx0) with Ak > 0 and dim V, Λ[g] = (cid:90) RC g(x0)p(dx0) = (cid:88) k=1 Akg(µk). (19) By Theorem 2, we know that for = span{ gi(tn, x0) = 1, , C, = 1, , } with the scalar function gi(tn, x0) := (xi0 xit +t xit)N (cid:0)xt; αtx0, σ2 I(cid:1), there exists probability measure p(dx0) such that (cid:82) RD gi(tn, x0)p(dx0) = 0 for every i, n. Then, by the RichterTchakaloff theorem, there also exists K-atomic measure with dim that satisfies all the moment equations. By taking the upper bound, this implies the existence of an C-atomic probability measure p(dx0) = (cid:80)N k=1 Ak = 1 that solves the Fredholm equation (Eq. (8)) at = tn for every = 1, , . I(cid:1) is continuous, the Dirac deltas in p(dx0) can be replaced Finally, since (cid:0)xt; αtx0, σ2 by mixture of narrow Gaussians, such that xn is approximated arbitrarily well for every n, i.e.: k=1 Akδµk (dx0) with Ak > 0, (cid:80)N n = 1, , N, xtn (cid:82) lim s0 tn RD x0p(dx0xtn) (cid:12) (cid:12) (cid:12) (cid:12)p(dx0)=(cid:80)N k=1 AkN (dx0;µk,s2I) xtn (cid:82) RD x0p(dx0xtn ) tn = = xtn This completes the proof of Theorem 1. (cid:12) (cid:12) (cid:12) (cid:12)p(dx0)=(cid:80)N k=1 Akδµk (dx0) (20) DERIVATION OF CLOSED-FORM GMFLOW VELOCITY In this section, we provide details regarding the derivation of closed-form GMFlow velocity, which was originally presented by Chen et al. (2025) but not covered in detail. Given the u-based GM prediction q(uxtsrc) = (cid:80)K R+, we first convert it into the x0-based parameterization by substituting = xtsrc x0 density function, which yields: k=1 AkN (cid:0)u; µk, s2I(cid:1) with Ak R+, µk RC, into the σtsrc q(x0xtsrc) = (cid:88) k= AkN (cid:0)x0; µxk, s2 xI(cid:1), (21) with the new parameters µxk = xtsrc σtsrc µk and sx = σtsrc s. Then, for any < tsrc and any xt RC, the denoising posterior at (xt, t) is given by: q(x0xt) = p(xtx0) p(xtsrcx0) q(x0xtsrc ), (22)"
        },
        {
            "title": "Preprint",
            "content": "where is normalization factor dependent on xtsrc, xt, tsrc, t. Using the definition of forward I(cid:1), we have: diffusion p(xtsrc x0) = (cid:0)xtsrc; αtsrc x0, σ2 tsrc I(cid:1) (cid:0)xt; αtx0, σ2 (cid:0)xtsrc; αtsrcx0, σ2 xt, σ2 x0; 1 α2 αt I(cid:1) q(x0xtsrc ) tsrc (cid:17) q(x0xt) = (cid:16) (cid:17) q(x0xtsrc) tsrc σ2 α2 tsrc (cid:19) = (cid:16) x0; 1 αtsrc xtsrc, (cid:18) = x0;"
        },
        {
            "title": "1\nZ ′′ N\nwhere ν = σ2\ntsrc\nζ = σ2\ntsrc",
            "content": ", σ2 σ2 ν tsrc ζ ζ αtxt σ2 α2 σ2 α2 tsrc . q(x0xtsrc), αtsrc xtsrc, The result can be further simplified into new GM: q(x0xt) = (cid:88) k=1 (cid:16) kN x0; µ k, s2I (cid:17) , with the following parameters: s2 = µ = = σ2 σ2 σ2 µxk σ2 xσ2 s2 tsrc xζ + σ2 s2 tsrc xν + σ2 s2 tsrc xζ + σ2 s2 tsrc exp k=1 exp (cid:80)K , where the new logit is given by: = log Ak 1 2 ν ζµxk2 σ2 ζσ2 + ζ 2s2 tsrc . Finally, the closed-form GMFlow velocity at (xt, t) is given by function π: π : RC RC, π(xt, t) = = xt Ex0q(x0xt)[x0] xt (cid:80)K k=1 kµ . (23) (24) (25) (26) (27) (28) Extension to discrete support. The closed-form GMFlow velocity can also be generalized to discrete support by taking limsx0 π(xt, t), which yields the simplified parameters: µ = µxk = log Ak 1 2 ν ζµxk2 ζσ2 tsrc σ2 . (29) (30) ADDITIONAL QUALITATIVE RESULTS. We show additional uncurated results of FLUX-based models in Fig. 11 and 12."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: An uncurated random batch from the OneIG-Bench prompt set, part A."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: An uncurated random batch from the OneIG-Bench prompt set, part B."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Stanford University"
    ]
}