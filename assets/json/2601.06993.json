{
    "paper_title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
    "authors": [
        "Jie Zhu",
        "Yiyang Su",
        "Xiaoming Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \\alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \\alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at \\href{https://github.com/jiezhu23/ReFine-RFT}{Project Link}."
        },
        {
            "title": "Start",
            "content": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification? Jie Zhu, Yiyang Su, Xiaoming Liu Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824 {zhujie4, suyiyan1, liuxm}@msu.edu 6 2 0 2 1 1 ] . [ 1 3 9 9 6 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on FineGrained Visual Classification (FGVC), core perception task that requires subtle visual discrimination and is crucial for many real-world applications. widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the Cost of Thinking. Building on this finding, we make two key contributions: (1) MRN, simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, framework that combines ensemble rewards with MRN to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-ofthe-art performance across FGVC benchmarks. Code and models are available at Project Link. 1. Introduction Multi-modal large language models (MLLMs) have demonstrated remarkable capabilities in general vision-language understanding, enabling seamless interaction across images and text and driving progress toward versatile, general-purpose AI systems [1, 2, 50]. As these models are increasingly deFigure 1. Performance degradation with CoT and reasoning collapse in RFT. In zero-shot evaluation (top), MLLMs predict the correct label directly, but adding CoT reasoning leads to wrong answer. During RFT (bottom), reasoning length steadily shrinks while accuracy improves, indicating reasoning collapse. ployed as unified interfaces for perception and reasoning, their ability to handle fine-grained visual understanding becomes particularly critical [10]. Fine-grained Visual Classification (FGVC) requires discriminating among subordinatelevel categories that exhibit only subtle visual differences (e.g., car models or plant varieties) and serves as foundation for more advanced perception-centric tasks such as objectcentric visual question answering [8, 10, 46]. For example, model unable to reliably differentiate between similarlooking pet breeds (e.g., golden retriever vs. labrador) is also likely to fail in answering follow-up questions about their behavioral traits or health conditions. Unlike general-domain recognition, FGVC demands precise visual perception and sensitivity to subtle cues such as fur texture, fine-grained shape differences, or minor pattern variations. Studying FGVC in the context of MLLMs directly probes their visual grounding and fine-grained feature extraction abilities. This lets us assess whether MLLMs can act as trustworthy assistants in visually demanding domains (e.g., ecology, medical imaging, industrial inspection) [46]. Despite their sophisticated architectures, current MLLMs exhibit clear limitations on FGVC, often failing to capture the subtle distinctions that define fine-grained categories [8, 10, 46], especially in few-shot and open-ended scenarios where training data is scarce and models must adapt to specialized domains from limited supervision without overfitting. This naturally raises the question of whether the textual description can help compensate for these perceptual weaknesses. widely held belief in the community is that eliciting Chain-of-Thought (CoT) reasoning improves performance on complex tasks such as math and coding [13, 28, 32, 40, 45, 47], and recent visual-oriented frameworks such as Visual-RFT [21] introduce CoT to steer MLLMs toward enhanced visual perception capabilities, achieving state-of-the-art performance on FGVC. However, several prior works have shown that explicit textual reasoning can paradoxically reduce accuracy compared to direct predictions [15, 18, 20, 34]. These studies, however, though informative, generally examine only limited settings: either focusing solely on zero-shot evaluation or comparing CoT and answer-only predictions at coarse level. This motivates us to systematically re-examine the role of textual reasoning from broader and in-depth evaluation and training perspectives. Specifically, we formulate key research question: Is textual reasoning detrimental to fine-grained visual perception, or do current methods simply employ it in suboptimal way? To answer this question, we conduct comprehensive investigation through two aspects: i) zero-shot scenario, and ii) using Reinforcement Fine-Tuning (RFT) framework to diagnose and manage this trade-off between reasoning and visual accuracy. Our diagnostic experiments reveal several key observations: First, CoT harms zero-shot FGVC performance, shown in Fig. 1 (top) and Tab. 1; Second, Reasoning Collapse in RFT, shown in Fig. 1 (bottom) and Fig. 2, where MLLMs gradually learn to suppress verbose reasoning while optimizing for accuracy during RFT. Our in-depth analysis not only corroborates previous observations but further extends them by revealing central insight: the length of textual reasoning itself is critical factor for fine-grained visual perception. We observe consistent negative correlation between reasoning length and accuracy: the longer the reasoning content, the worse the visual perception performance. We term these the Cost of Thinking, which reveals that fine-grained visual perception tasks might benefit from concise rather than elaborate reasoning for MLLMs. Based on our findings, we introduce ReFine-RFT, novel RFT framework designed to constrain reasoning and improve accuracy. Our framework features two key technical innovations to solve the core challenges of this task. First, to overcome the sparse and semantically naive signal of binary accuracy rewards, we introduce an ensemble, semantically-aware reward, which provides dense and continuous learning signal while explicitly restricting the reasoning length. Second, to optimize multi-objective reward space, we propose Multi-reward Normalization (MRN), plug-and-play module that stabilizes training by reducing and smoothing the variance of the reward signals. Our contributions are fourfold: We empirically characterize the Cost of Thinking in FGVC, showing that verbose CoT systematically degrades MLLM performance on fine-grained perception tasks. We propose MRN, plug-and-play normalization that independently normalizes heterogeneous reward signals in multi-objective settings. We introduce ReFine-RFT, an RFT framework that integrates MRN with ensemble rewards to primarily optimize accuracy while controlling reasoning length. ReFine-RFT achieves state-of-the-art across multiple FGVC benchmarks, validating the effectiveness of our findings and the proposed method. 2. Related Works Reasoning Ability of MLLMs. Prior works have demonstrated that reasoning could improve performance on complex tasks such as math and coding [24, 40, 48, 49]. However, empirical evidence [15, 20, 31, 43] shows that CoT reasoning can introduce spurious explanations and degrade visual perception accuracy. For example, No-ThinkingRFT [18] presents that visual tasks do not need thinking. However, it primarily compares performance between training with CoT and answer-only prompts. We systematically re-evaluate textual reasoning for fine-grained perception across zero-shot and different training regimes, revealing that thinking length is the key to Cost of Thinking. Fine-grained Visual Classification in MLLMs. FGVC [16, 22, 23, 25, 37, 41] focuses on subcategory-level recognition (e.g., bird species, car models) that requires sensitivity to subtle visual cues. With the advent of MLLMs, recent works [4, 7, 10, 19, 27, 36] explore prompting and adaptation strategies to improve fine-grained visual discrimination of MLLMs. Visual-RFT [21] further applies RFT with CoT-style reasoning and achieves additional gains, while No-Thinking-RFT [18] optimizes without CoT. Building on our findings, we propose ReFine-RFT, which combines ensemble rewards with MRN to explicitly constrain textual reasoning while further enhancing the fine-grained visual perception capability of MLLMs. Reinforcement Fine-tuning. Recent research demonstrates that Reinforcement Learning can significantly enhance the reasoning and problem-solving capabilities of LLMs and MLLMs compared with Supervised Fine-tuning 2 Model Qwen2-VL-2B Qwen2-VL-7B Qwen2.5-VL-7B InternVL2.5-8B InternVL3-8B Aircrafts-102 FlowersCars-196 Pets-37 Average Answer-only CoT Answer-only CoT Answer-only CoT Answer-only CoT Answer-only CoT 47.5 53.5 54.0 13.8 14.2 55.7 55.8 51.1 20.1 23. Open-source Non-reasoning Models 54.8 51.1 36.3 12.9 10.1 Open-source Reasoning Models 59.2 82.6 83.9 73.0 33.5 42.2 - - 56.8 76.5 66.9 31.9 36.5 49. 45.9 42.3 41.7 11.9 14.5 42.0 56.4 51.4 52.5 48.1 37.6 - 66.4 61.1 62.4 50.4 42.8 68. 60.5 61.2 57.7 28.9 29.3 - 55.9 57.8 51.8 26.8 25.9 54.8 R1-OneVision-7B-RL - Table 1. Performance Degradation of CoT on FGVC benchmarks. We evaluate several open-source MLLMs on four FGVC datasets under two prompting settings: Answer-only and Chain-of-Thought (CoT). Results show notable performance degradation when CoT reasoning is applied. For R1-OneVision-7B-RL, Answer-only are omitted, as it generates CoT-style outputs even under Answer-only. (SFT) [3, 9, 13, 28, 33]. DeepSeek-R1 [9] introduces Group Relative Policy Optimization (GRPO), substantially improving reasoning and generalization. Follow-up works further apply GRPO to other tasks such as visual grounding [5, 12, 21, 26, 29, 35, 42, 44], typically using simple rulebased signals such as accuracy. However, existing methods ignore heterogeneity across reward functions. We propose MRN to balance multi-reward signals, and use an ensemble of rewards to provide denser reward feedback. 3. Cost of Thinking in FGVC 3.1. Experiment Setup Datasets. We select widely adopted FGVC benchmarks: FGVC-Aircraft [22], Stanford-Cars [16], Flowers-102 [23], and Oxford-Pets [25]. Considering real-world scenario where fine-grained labeled data might be scarce, we use 4-shot dataset provided by [21]. We perform FGVC as an open-ended QA task to mimic the real-world application. Models and Prompts. We evaluate Answer-only and CoT prompts across several open-source MLLMs: Qwen2/2.5VL series [2, 39], InternVL series [6, 50], and the reasoning model R1-OneVision [42]. For RFT training, we use Qwen2VL-2B [39] as the base model. We use the CoT prompt from [21], and the following Answer-only prompt as an example for Flowers-102: This is an image containing plant. Please identify the species of the plant based on the image. Only provide the final answer directly, without any explanation or special formatting. Reward Functions for RFT. We follow Visual-RFT [21] and use format reward Rf (oi) and accuracy reward Rcls(ai, y) to improve the instruction-following capability and answer accuracy. The format reward is binary signal that enforces strict adherence to the required structured output template, assigning 1 if the models response oi correctly follows the sequential <think>...</think> and <answer>...</answer> tags, and 0 otherwise. The classification reward Rcls(ai, y) measures prediction accuracy based on the class encoded within the <answer>...</answer> tags, yielding 1 when the the ground-truth label is in the predicted label ai extracted from the answer tags of oi and 0 otherwise; To investigate the effects of thinking length, we introduce thinking length reward Rlen(oi) that assigns binary score based on whether the thinking length lies within specified range. Let ti denote the reasoning content extracted from the model output oi, and let Li = ti be its character length. Given predefined bounds (Lmin, Lmax), the reward is computed as: Rlen(oi) = 1, if Lmin Li Lmax, 0, otherwise. (1) This formulation encourages the model to produce reasoning traces whose lengths fall within the desired interval, enabling explicit control over the degree of internal deliberation. When the <think> tags are missing or improperly formatted, the reward is set to 0. 3.2. Results & Findings Performance Degradation in CoT under Zero-Shot. As shown in Tab. 1, incorporating Chain-of-Thought (CoT) prompting consistently leads to accuracy degradation across all FGVC datasets. Non-reasoning models exhibit an average drop of 36% when switching from Answer-only to CoT prompts, while even reasoning-oriented model, R1OneVision, achieves only moderate accuracy under CoT and still generates CoT response for Answer-only. This indicates that visual reasoning chains often introduce useless or hallucinatory explanations rather than improving decision [15, 20, 34] also reveal similar phenomena that quality. reasoning might be harmful to visual recognition. However, this observation raises fundamental question: Is reasoning 3 Figure 2. Dynamics of reasoning length during RFT across FGVC datasets. The dark green lines denote the running average of completion lengths throughout RFT FGVC tasks. Across all datasets, the reasoning content length rapidly decreases and stabilizes at shorter range, suggesting that RFT discourages excessive reasoning generation and promotes concise, decision-focused responses. [Zero-shot: average content length of base model on the evaluation set; Step: the cumulative number of gradient update steps.] intrinsically harmful to visual perception tasks, or is the degradation simply byproduct of zero-shot misalignment between CoT prompting and model training? We further explore how reasoning evolves under RFT, as models adapt their generation strategy through reward-driven learning. Reasoning Collapse in RFT. To probe the dynamics of reasoning adaptation, we track the change of completion length throughout RFT. We set up the experiment following Visual-RFT [21] with format reward and classification reward described in Sec. 3.1. As shown in Fig. 2, the average reasoning length exhibits consistent downward trend across all FGVC datasets. At the early stages of RFT, model outputs are verbose and exploratory, but as training progresses, the content length rapidly declines and stabilizes at compact range. Notably, the final completion lengths after RFT are shorter than those in the zero-shot setting. We refer to this phenomenon as reasoning collapse: an emergent behavior where RFT implicitly discourages long reasoning chains for visual perception tasks, optimizing instead for concise, confident answer prediction. This collapse suggests that the model learns to suppress reasoning steps that do not contribute to reward maximization, thereby aligning its behavior more closely with discriminative objectives. In other words, RFT appears to regularize the reasoning process itself, favoring precision and efficiency over verbosity and exploration, tendency that aligns with findings from [18]. However, this behavior may also result from reward hacking, since no explicit constraint is imposed on the reasoning process, leading the MLLM to generate only minimal reasoning content. Building upon this observation, we design the subsequent experiments to further verify and quantify the effect for reasoning. Effects of Thinking Length in RFT. To further examine whether the reasoning length is beneficial or detrimental, we explicitly manipulate the reasoning process by involving the thinking length reward during RFT. We gradually limit the reasoning content length from [0, 20] to [60, 80]. As shown in Fig. 3, enforcing longer reasoning sequences 4 Figure 3. Impact of reasoning length on FGVC performance. We analyze the relationship between average reasoning (thinking) length and classification accuracy across FGVC datasets. As the average thinking length increases, performance consistently declines, indicating that excessive reasoning generation introduces noise or distracting the model from key discriminative visual cues. leads to clear decline in classification accuracy across all FGVC datasets. This inverse correlation demonstrates that extended reasoning is not only unhelpful but can actively degrade performance by introducing textural reasoning into the responses. In contrast, shorter reasoning traces yield higher accuracy, indicating that optimal visual performance is achieved with minimal reasoning and precise visual perception and localization. However, the degradation may also stem from low-quality CoT, as reasoning quality is unsupervised during RFT. This motivates analyzing whether higher-quality CoT can close this gap. Answer-only Surpasses CoT in SFT. To analyze the effects of CoT quality, we use GPT-4o [14] to generate the highquality CoT data for SFT-CoT training. As shown in Tab. 2, SFT-AO consistently outperforms SFT-CoT, indicating that the degradation is not simply due to the quality of CoT. This finding complements our Cost of Thinking analysis from the SFT perspective, showing that excessive reasoning can harm fine-grained visual perception in both training and inference. Taken together with the observations under zeroshot and RFT, these results reveal key finding: Finding 1: For fine-grained visual tasks, thinking length is the key factor: excessive reasoning hurts performance, and MLLMs benefit more from concise responses than from elaborate reasoning. Figure 4. Overview of ReFine-RFT. Given question, the model generates multiple candidate responses, each evaluated using an ensemble reward that combines rule-based rewards and model-based rewards like MLLM-based accuracy reward and embedding similarity reward. The proposed MRN then normalizes the rewards for each function to compute the final advantages used to update the MLLM. 4. Methods Inspired by our findings, we propose ReFine-RFT, which improves RFT by combining the ensemble reward with Multi-Reward Normalization scheme. The ensemble reward jointly constrains reasoning length and provide dense accuracy feedback, while Multi-Reward Normalization stabilizes optimization across heterogeneous reward signals. An overview of ReFine-RFT is shown in Fig. 4. 4.1. Ensemble Reward The ensemble reward is composed of the format, accuracy, and thinking-length rewards defined in Sec. 3.1, together with two complementary rewards: an MLLM-based accuracy reward and an embedding similarity reward, which jointly provide richer and accuracy-centric feedback. MLLM-based Accuracy Reward. The classification reward provides only binary supervision through exact string matching between the predicted and ground-truth answers, which fails to capture semantic similarity. For example, predictions such as Datura stramonium vs. thorn apple denote the same subcategory but would be penalized under hard matching, and Dodge Dakota vs. 2007 Dodge Dakota Club Cab, which is missing some details. To provide smoother and more semantically-aware signal, we introduce an MLLM-based Accuracy Reward Rmllm that employs an MLLM as teacher to grade each prediction. Given predicted answer and the reference label, the MLLM is prompted to output score from 0 to 10 based on its semantic alignment, which is then normalized to [0, 1]. This continuous reward function provides fine-grained feedback by assigning high scores to fully correct answers, intermediate scores to semantically similar ones, and low scores to irrelevant predictions. To mitigate potential scoring biases of the reward model [17, 30], we include few-shot grading examples in the prompt and use this reward together with the embedding similarity reward. Embedding Similarity Reward. To further provide smooth and continuous supervision signal, we introduce an embedding similarity reward Remb that measures the semantic closeness between the predicted and ground-truth answers in an embedding space. Given the extracted predicted answer ai from the <answer> tags of the model output and the reference label, both are encoded into text embeddings using pretrained text embedding model. The cosine similarity between the predicted embedding ei and the ground-truth embedding egt is used as the reward: Remb = cos(ei, egt) [0, 1]. This continuous reward provides differentiable measure of semantic alignment, encouraging the model to produce answers that are semantically close to the reference even when lexical forms differ. 4.2. Multi-reward Normalization (MRN) As shown in Fig. 4, for given question and image x, GRPO requires the model to sample diverse responses o1, o2, . . . , oG from the current model πθ and obtains final rewards r1, r2, . . . , rG for o1, o2, . . . , oG, respectively. GRPO assesses the relative quality by normalizing ri using the mean and standard deviation of the group reward: Ai = ri mean(r1, . . . , rG) std(r1, . . . , rG) , (2) where Ai denotes the advantage of the i-th response. With the group normalization, GRPO encourages the model to 5 Algorithm 1 Advantage Normalization with MRN Input: Number of generations G, number of reward functions , reward matrix RGM , ϵ. Output: Aggregated advantage RG. // 1. Compute mean and std for each reward function µ Mean(R, axis=0) σ Std(R, axis=0) RM RM // 2. Normalize rewards per function (element-wise) Anorm (R µ)/(σ + ϵ) RGM // 3. Aggregate scores into final advantage Sum(Anorm, axis=1) RG // Original GRPO Method (for comparison) // 1. Aggregates rewards // Ragg Sum(R, axis=1) // 2. Normalizes the aggregated rewards // µagg Mean(Ragg); σagg Std(Ragg) // AGRPO (Ragg µagg)/(σagg + ϵ) return 5.1. Results of ReFine-RFT Tab. 2 summarizes the performance across four FGVC datasets, from which several consistent patterns emerge: Lora Outperforms Fully-FT. We find that LoRA finetuning [11] consistently surpasses fully fine-tuning (FullyFT) under both SFT and RFT settings. This confirms that LoRA not only reduces computational cost but also enables the model to better leverage limited fine-grained visual data. RFT Provides Stronger Gains. Transitioning from SFT to RFT yields substantial accuracy improvements across all FGVC benchmarks. RFT enables the model to directly optimize accuracy-centric objectives, correcting undesirable generation behaviors and yielding more stable, discriminative predictions. This is especially desirable in few-shot FGVC, where precise category boundaries must be learned from limited supervision. Superiority of ReFine-RFT. Building upon our findings, our proposed ReFine-RFT further improves performance across all benchmarks. By integrating: (i) an ensemble, semantically-aware reward that provides dense and accuracyaligned feedback, and (ii) the Multi-Reward Normalization module (MRN) that stabilizes heterogeneous rewards, ReFine-RFT achieves consistent gains over Visual-RFT and other baselines. Importantly, ReFine-RFT with only 2B backbone and 4-shot training significantly surpasses Finedefics-8B trained on the full FGVC datasets, underscoring the efficiency and scalability of our approach. Figure 5. Differences among rewards during training. Each reward exhibits distinct convergence speed, value range, and saturation point, reflecting the heterogeneity of different rewards. sample preferred answers with higher reward. In practical training scenarios, multiple reward signals (e.g., format and classification) are often combined to guide optimization. In the original GRPO, these heterogeneous rewards are first aggregated into single scalar value before performing group normalization. However, in reality, different rewards exhibit varying levels of difficulty and convergence rates. As shown in Fig. 5, the format reward may quickly saturate in early training, thereby dominating the total reward and diluting the influence of other, more informative rewards such as accuracy. To address this issue, we propose Multi-reward Normalization (MRN), which performs group normalization independently for each reward component before aggregation. Specifically, given reward types ri (K) for the i-th response oi, we compute the normalized advantage for each reward as: (k) mean(r1 ri std(r1 (k), . . . , rG (k)) (k), . . . , rG (2), . . . , ri (1), ri (k) = (k)) Ai (3) , and then aggregate them to obtain the final advantage: Ai = (cid:88) k=1 Ai (k). (4) This normalization places all rewards on comparable scale, leading to more stable and balanced optimization. The pseudocode is provided in Alg. 1. 5. Experiments Implementation Details. Qwen2-VL-2B-Instruct [39] is used as the base model. We train with 4 NVIDIA H100 GPUs with 81G of memory. We use Qwen2-VL-7BInstruct [39] as the reward model for MLLM-based accuracy reward, and E5 [38] as the embedding model for answer embedding similarity reward. To constrain the reasoning length, we set Lmin=0, Lmax=10 for ReFine-RFT. We use γ=64 and α=128 for LoRA, and learning rate of 2e 5 with 64 as the accumulated batch size. We set the number of generations G=8 and β=0.04 for GRPO. Each experiment trains with 200 step maximum, and all seeds are fixed across the training and evaluation procedures to ensure reproducibility and fairness. More details are in the supplementary. 6 Methods FT Methods FT Types Aircrafts-102 Flowers-102 CarsPets-37 Average Qwen2-VL-2B [39] Finedefics-8B [10] Zero-shot SFT SFT-AO SFT-AO SFT-CoT Visual-RFT [21] Visual-RFT [21] No-Thinking-RFT [18] ReFine-RFT-AO (Ours) ReFine-RFT-CoT (Ours) SFT RFT - Fully-FT Fully-FT Lora Lora Fully-FT Lora Fully-FT Lora Lora 45.9 63. 67.9 78.3 73.9 54.8 89.9 58.5 74.8 74.4 56.8 84.7 40.5 80.0 52.3 66.4 92. 55.5 87.6 87.5 56.0 82.7 55.6 80.2 72.0 74.8 75.6 - 78.7 95.3 95.7 - 93.1 79.3 (+3.7%) 81.0 (+6.9%) 97.1 (+1.4%) 88.6 (+2.6%) 86.5 (+3.6%) 86.1 86.0 86.1 87. 81.9 82.9 - 85.2 71.4 74.1 71.2 81.4 Table 2. Performance comparison on FGVC datasets. Compared to SFT (with/without CoT data) and Visual-RFT baselines, our ReFine-RFT achieves the best results with consistent gains across datasets (values in parentheses denote relative improvements over the Visual-RFT (Lora) [21] baseline). [AO: Answer-only prompt; SFT-CoT: SFT with CoT data; Best and second best are highlighted.] Reasoning Control Outweighs Prompt Style. We observe that controlling the reasoning length has larger impact on performance than the choice of prompt style. First, NoThinking-RFT uses an Answer-only-style prompt, whereas Visual-RFT uses CoT-style prompt, yet they yield similar performance. This suggests that prompt style is not the primary performance determinant. Second, within our ReFine-RFT, ReFine-RFT-CoT is slightly better than the ReFine-RFT-AO. Together, these observations suggest that performance depends more on reasoning-length control than on whether the prompt elicits CoT. In our view, suppressing reasoning encourages the model to focus on accuracy as the main optimization target, while still allowing it to generate reasoning when reasoning is genuinely beneficial. This further supports our Cost of Thinking analysis and Finding 1 in Sec. 3, confirming that reasoning length is the key factor influencing fine-grained visual perception. We then derive the following conclusion: Conclusion 1: For fine-grained visual perception, jointly using multi-perspective, accuracy-centric rewards and explicit reasoning length control leads to stronger visual perception capabilities. 5.2. Ablation Studies Effects of MRN. We employ format reward and classification reward to investigate the impact of MRN. As shown in Tab. 3, integrating MRN consistently improves performance across all three FGVC datasets, yielding gains of +1.1%/+2.1%/+0.4% under full fine-tuning and +0.7%/+1.5%/+0.6% under LoRA. The improvements are more pronounced with larger training capacity, suggesting that MRN can better leverage additional parameters. Overall, these results confirm that MRN effectively boosts model performance while preserving strong efficacy in parameterMethods Aircrafts-102 Flowers-102 Cars-196 Fully fine-tuing GRPO [9] + MRN (Ours) 74.0 75. 68.6 70.7 Lora (r=64, α=128) GRPO [9] + MRN (Ours) 75.6 76.3 74.1 75.6 94.7 95. 95.7 96.3 Table 3. Comparison of MRN under two training regimes. MRN serves as plug-and-play module and consistently enhances accuracy across datasets under fully fine-tuning and LoRA. efficient training regimes. Effects of Ensemble Reward. We ablate the effects of the ensemble reward design in Tab. 5. The results demonstrate that incorporating multiple reward components leads to consistent performance gains. Each reward contributes complementary information, guiding the model toward robust learning objectives. Notably, when all reward functions are jointly combined as the ensemble reward, the model achieves the best overall performance, suggesting that aggregating complementary reward signals provides richer and more stable guidance for optimization than any individual reward. Fig. 6 shows the reward curves during training, validating the effectiveness of ReFine-RFT. Effects of Trainable Parameters. We analyze the impact of trainable parameters using the format and classification rewards on ReFine-RFT. As shown in Tab. 4, model performance consistently improves with increasing LoRA capacity. In particular, the configuration with r=64, α=128 achieves the highest accuracy, surpassing the Fully-FT baseline across all datasets. In contrast, the smaller setting (r=16, α=32) leads to notable performance drop. These results indicate that appropriately chosen LoRA rank and scaling factors can outperform fully fine-tuning in few-shot scenarios, providing an efficient and effective approach for model adaptation. Methods Fully-FT = 16, α = 32 = 32, α = 64 = 64, α = 128 Aircrafts-102 Flowers-102 Cars-196 75.1 Lora 71.3 75.4 76.3 70.7 64.6 70.4 75.6 95.1 94.0 95.0 96.3 Table 4. Comparison of trainable parameters. LoRA with larger ranks (r) and scaling factors (α) progressively improves accuracy, eventually surpassing full fine-tuning across all datasets. Rf Rcls Rlen Rmllm Remb Aircrafts-102 Pets-37 86.8 87.5 85.7 86.3 88.6 76.3 78.5 77.5 79.0 79.3 Figure 7. Training reward and its standard deviation comparison on Aircrafts-102. MRN + GRPO achieves consistently higher reward values and lower variance throughout training, indicating improved stability and optimization efficiency. Table 5. Effects of ensemble reward. Combining multiple reward functions consistently improves performance, and using all rewards yields the best overall results. Figure 8. Comparison of responses. SFT-CoT and Visual-RFT produce long reasoning with incorrect answers, while ReFine-RFT achieves concise reasoning and higher accuracy. More results and analyses are in the supplementary. Qualitative Results. As shown in Fig. 8, ReFine-RFT demonstrates clear advantage in both reasoning efficiency and accuracy, encouraging minimal reasoning steps. 6. Conclusion We reveal the Cost of Thinking in FGVC for MLLMs, showing that excessive textual reasoning degrades finegrained visual perception performance from both the inference and training perspectives. Our systematic study across zero-shot and multiple fine-tuning regimes indicates that perception-centric tasks benefit more from concise reasoning. Motivated by this, we propose ReFine-RFT, reasoningconstrained RFT framework that enhances visual perception by integrating ensemble, semantically-aware rewards with Multi-Reward Normalization (MRN) for optimization. Extensive results demonstrate that ReFine-RFT achieves stateof-the-art performance across FGVC benchmarks, highlighting that effective visual perception emerges from constraint thinking and accuracy-centric reward shaping. Future work will probe the mechanisms behind the Cost of Thinking and extend ReFine-RFT to broader multimodal tasks. Figure 6. Reward curves of ReFine-RFT on Flowers-102. Rewards consistently increase over training, demonstrating the effectiveness of our reward design. Reward Distribution Comparison. As shown in Fig. 7, our proposed MRN consistently achieves higher reward values and maintains significantly lower reward standard deviation compared to the baseline GRPO. Throughout training, MRN exhibits steady improvement in reward, indicating more stable and efficient policy optimization. In contrast, GRPO shows larger standard deviation, reflecting less stable learning behavior. The notably lower reward variance of MRN suggests that it effectively mitigates gradient noise and reduces policy fluctuation, leading to smoother and more reliable reward progression. These observations demonstrate that MRN not only enhances training stability but also enables more consistent reward maximization, thereby improving optimization robustness and efficiency."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 3 [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 3 [4] Junwen Chen, Jie Zhu, and Yu Kong. Atm: Action temporality modeling for video question answering. In ACM MM, 2023. 2 [5] Xiaxu Chen, Wei Li, Chunxu Liu, Chi Xie, Xiaoyan Hu, Chengqian Ma, Feng Zhu, and Rui Zhao. On the suitability of reinforcement fine-tuning to visual tasks. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2025. 3 [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 3 [7] Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, and Tat-Seng Chua. Fine-grained verifiers: Preference modeling as next-token prediction in visionlanguage alignment. arXiv preprint arXiv:2410.14148, 2024. 2 [8] Gregor Geigle, Radu Timofte, and Goran Glavaˇs. African or european swallow? benchmarking large vision-language models for fine-grained object classification. arXiv preprint arXiv:2406.14496, 2024. 1, [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3, 7 [10] Hulingxiao He, Geng Li, Zijun Geng, Jinglin Xu, and Yuxin Peng. Analyzing and boosting the power of fine-grained visual recognition for multi-modal large language models. arXiv preprint arXiv:2501.15140, 2025. 1, 2, 7 [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2022. 6 [12] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 3 [13] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. 2, [14] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 4 [15] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. 2, 3 [16] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops, 2013. 2, 3 [17] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Lester James Validad Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. In NAACL, 2025. 5 [18] Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Haoquan Zhang, Wang Bill Zhu, and Kaipeng Zhang. Think or not think: study of explicit thinking in rule-based visual reinforcement fine-tuning. NeurIPS, 2025. 2, 4, 7 [19] Mingxuan Liu, Subhankar Roy, Wenjing Li, Zhun Zhong, Nicu Sebe, and Elisa Ricci. Democratizing fine-grained visual recognition with large language models. arXiv preprint arXiv:2401.13837, 2024. 2 [20] Ryan Liu, Jiayi Geng, Addison Wu, Ilia Sucholutsky, Tania Lombrozo, and Thomas Griffiths. Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse. arXiv preprint arXiv:2410.21333, 2024. 2, [21] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. ICCV, 2025. 2, 3, 4, 7 [22] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 2, 3 [23] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing. IEEE, 2008. 2, 3 [24] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. 2 [25] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012. 2, 3 [26] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. 3 9 [40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. NeurIPS, 35:2482424837, 2022. [41] Xiu-Shen Wei, Yi-Zhe Song, Oisin Mac Aodha, Jianxin Wu, Yuxin Peng, Jinhui Tang, Jian Yang, and Serge Belongie. Fine-grained image analysis with deep learning: survey. TPAMI, 2021. 2 [42] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. 3 [43] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. 2 [44] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. 3 [45] Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. Chain of preference optimization: Improving chain-of-thought reasoning in llms. NeurIPS, 2024. 2 [46] Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, and Serena Yeung-Levy. Why are visually-grounded language models bad at image classification? NeurIPS, 2024. 1, 2 [47] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. [48] Zhuosheng Zhang, Aston Zhang, Mu Li, George Karypis, Alex Smola, et al. Multimodal chain-of-thought reasoning in language models. TMLR, 2023. 2 [49] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In ICLR, 2023. 2 [50] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 1, 3 [27] Zhiyuan Ren, Yiyang Su, and Xiaoming Liu. Chatgptpowered hierarchical comparisons for image classification. NeurIPS, 36:6970669718, 2023. 2 [28] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 3 [29] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [30] Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuan-Jing Huang. Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback. In EMNLP, 2023. 5 [31] Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. 2 [32] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. 2 [33] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, YuXiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 3 [34] Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? study on the impact of format restrictions on performance of large language models. arXiv preprint arXiv:2408.02442, 2024. 2, 3 [35] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reasonrft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. 3 [36] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, pages 95689578, 2024. [37] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 Dataset. 2011. 2 [38] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. 6 [39] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 6,"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, Michigan State University"
    ]
}