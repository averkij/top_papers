{
    "paper_title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation",
    "authors": [
        "Yuhui Zhang",
        "Yuchang Su",
        "Yiming Liu",
        "Xiaohan Wang",
        "James Burgess",
        "Elaine Sui",
        "Chenyu Wang",
        "Josiah Aklilu",
        "Alejandro Lozano",
        "Anjiang Wei",
        "Ludwig Schmidt",
        "Serena Yeung-Levy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 5 2 2 3 0 . 1 0 5 2 : r Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation Yuhui Zhang1* Yuchang Su2 Yiming Liu2 Xiaohan Wang1 Elaine Sui1 Chenyu Wang3 James Burgess1 Josiah Aklilu1 Alejandro Lozano1 Anjiang Wei1 Ludwig Schmidt1 Serena Yeung-Levy1 1Stanford University 2Tsinghua University 3MIT"
        },
        {
            "title": "Abstract",
            "content": "The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, benchmark created by transforming 20 existing VQA datasets into unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting new standard for scalable, consistent, and reproducible VLM evaluation. 1. Introduction With the rapid advancements in vision language models (VLMs) [2, 27, 40, 51], rigorous and reliable evaluation methods are critical to gauge model performance and guide further research. Visual question answering (VQA) has emerged as standard evaluation protocol, typically structured with open-ended or multiple-choice questions. Openended questions [13, 35, 58] allow models to generate freeform, natural language responses, while multiple-choice questions [31, 47, 59] ask models to choose the answer *Equal contribution. Equal advising. Correspondence to: {yuhuiz,suyc,ludwigsc,syyeung}@stanford.edu. Project page: AutoConverter-Website/. https : / / yuhui - zh15 . github . io / from predefined options. Most existing VQA benchmarks are in open-ended format [10, 23] because of the complexity and extensive resources required to design high-quality multiple-choice questions. We revisit current evaluation strategies for open-ended questions, focusing on the challenge of measuring semantic similarity between model-generated and ground-truth answers (3). Two main evaluation methods exist: rulebased, which evaluates word or phrase level overlap, and model-based, which uses large language models (e.g., GPT4o) for semantic matching. While rule-based evaluation is computationally efficient, our experiments on the VQAv2 dataset [13] demonstrate that it fails to capture semantic nuances and formatting differences, yielding poor correlation (0.09) with true VLM performance. In contrast, model-based metrics, though more semantically accurate, are costly and unstable updates in model versions (e.g., GPT-4o from 0513 to 0806) constantly increase scores by 6% on the MMVet dataset [58], making results incomparable and raising future reproducibility issues. To mitigate these issues, we propose an alternative for VLM evaluation by converting open-ended VQA questions into multiple-choice format. This shift allows for more objective and reproducible assessment by providing defined options and simplifying answer validation. Nonetheless, creating multiple-choice questions is inherently complex, particularly in generating distractor options that are plausible yet challenging given the correct answer process traditionally requiring extensive human expertise and effort to simulate different errors students could make. We introduce AutoConverter (4), novel multi-agent system powered by GPT-4o, designed to automatically convert open-ended questions into challenging yet correct multiple-choice questions. To enhance difficulty, specialized distractor proposer agents collaborate with reviewer agent in an iterative manner to generate large pool of challenging distractors focused on common error types, including conceptual misunderstandings, visual misinterpretations, and reasoning errors. selector agent then chooses 1 Figure 1. Overview. (Left) We analyze existing open-ended VQA evaluation metrics, underscoring their limitations in providing accurate and reproducible assessments. (Middle) We introduce AutoConverter, multi-agent system that automatically converts open-ended questions into multiple-choice format, enabling objective assessment while reducing the costly question creation process. (Right) Using AutoConverter, we convert and refine 20 existing VQA datasets into unified multiple-choice benchmark to support future VLM research. the most challenging distractors. To ensure correctness, question correctness evaluator agent assesses the similarity between distractors and the correct answer, providing feedback that allows refiner agent to adjust distractors to improve question correctness. This agentic pipeline ensures that the generated multiple-choice questions are both correct and challenging, delivering strong discriminative power for VLM evaluation. Our experiments validate AutoConverters ability to produce correct and challenging multiple-choice questions. When applied to established multiple-choice VQA datasets such as MMMU [59], MathVista [32], and AI2D [19], AutoConverter generates distractors that match or even surpass the difficulty of human-crafted alternatives, as evidenced by comparable or lower accuracy scores across various VLMs, with only 3% of questions with the highest correctness score marked incorrect by human annotators. detailed ablation study further demonstrates that each component of AutoConverter significantly enhances the correctness and difficulty of questions compared to naive conversion methods. These results indicate AutoConverters broad applicability beyond converting open-ended questions to multiple-choice format, such as refining existing multiple-choice datasets to increase difficulty or generating challenging questions for students in an educational context. Building on AutoConverter, we introduce VMCBench, benchmark comprising 9,018 multiple-choice VQA questions compiled from 20 existing datasets (5). These original datasets, which contain either open-ended or multiplechoice questions, have been converted or refined using AutoConverter. Any questions flagged as uncertain by the correctness evaluator were manually verified to ensure quality and accuracy. VMCBench provides standardized benchmark to evaluate diverse model capabilities across various question types. We evaluated 33 state-of-the-art VLMs on VMCBench, establishing new standard for scalable, consistent, and reproducible vision language model evaluation. Our contributions are summarized in Figure 1. 2. Related Works Vision language model evaluation. With the rapid development of vision language models (VLMs) [2, 27, 40, 51], numerous benchmarks have emerged to assess various capabilities of VLMs, including general understanding [6, 13, 14, 24, 35, 47, 58], reasoning [17, 31, 32, 53, 56, 59], OCR [39, 49], and document and chart comprehension [19, 20, 3638]. These datasets typically use either open-ended or multiple-choice formats. Open-ended questions are easier to create but challenging to evaluate accurately, while multiple-choice questions simplify evaluation but demand greater effort in their creation. Previous studies have found limitations of open-ended evaluation for VLMs [5, 21, 34], often focusing on case studies and comparisons with human performance. Their proposed solutions are modelbased evaluation methods [11, 64]. In this work, we systematically analyze various VLMs on two widely-used openended VQA datasets in zero-shot evaluation settings, quantitatively revealing significant challenges in VLM evaluation for open-ended questions. We demonstrate how rulebased evaluation poorly correlates with actual model rankings in zero-shot scenarios. Additionally, we highlight the substantial limitations of model-based evaluation: modelbased evaluations are inconsistent across versions, which leads to serious reproducibility issues. These issues seem 2 Figure 2. Challenges in evaluating open-ended questions. (Left) Rule-based metrics significantly underestimate model performance and penalize models that do not strictly follow the expected format. (Right) Model-based evaluations using two different versions of GPT yield substantially different scores, making comparisons inconsistent and raising reproducibility issues. intrinsic and difficult to resolve. Consequently, we propose to convert open-ended questions to multiple-choice format in order to streamline reliable VLM evaluation. Converting open-ended to multiple-choice. Converting open-ended questions to multiple-choice requires generating distractors that are both challenging and accuratea task that traditionally demands extensive human expertise and has limited the construction of multiple-choice benchmarks. Some approaches attempt to use knowledge graphs [57], retrieval [33], reinforcement learning [30], and supervised learning [9] to automatically produce distractors that resemble correct answers. However, these methods require external resources and often result in relatively simple distractors. Recent advances in large language models (LLMs) have inspired attempts to automate distractor generation to support human creators, yet these approaches still necessitate significant human input to ensure question quality [55, 60]. In our work, we systematically define two essential criteriacorrectness and difficultyfor effective multiple-choice questions, and develop an agentic pipeline [28, 43, 48] to automate their creation. Our approach generates multiple-choice questions with distractors that are both challenging and correct, achieving quality comparable to or even surpassing human-crafted questions while significantly reducing the need for human effort. This tool has broad utility: it can convert open-ended questions to multiple-choice for VLM evaluation, refine poorly constructed questions, and generate questions beyond VLM evaluation, such as educational assessments. 3. Open-Ended Question Evaluation Challenge In this section, we discuss the challenges of evaluating vision language models (VLMs) using open-ended questions. The primary issue lies in accurately and robustly measuring the semantic similarity between model-generated answers and ground-truth answers, long-standing challenge in natural language processing [12, 62, 64]. 3.1. Rule-based Evaluation Background. When ground-truth answers are short, rulebased evaluation metrics such as exact match or quasi-exact match (e.g., BLEU [42], ROUGE [25]) are typically used to measure word-level overlap between the ground truth and model predictions. However, these methods are limited in accounting for semantic variations, such as synonyms, paraphrasing, or formatting differences. This limitation is even more pronounced when evaluating current VLMs in zeroshot settings, where models are instruction-tuned to generate slightly more detailed and lengthier responses for clarity. Experiment. To examine the limitations of these evaluation metrics, we tested 12 state-of-the-art VLMs on the In zero-shot setting, widely-used VQAv2 dataset [13]. we prompted the models with, Please try to answer the question with short words or phrases if possible. and used VQAv2s official rule-based evaluation code. We then conducted detailed analysis of the cases where GPT-4o did not receive full marks, using human judgment to correct scores where the rule-based evaluation failed. Based on these human annotations, we developed model-based evaluation method using GPT-4o with well-crafted prompt to assess the semantic match between model outputs and groundtruth answers. Our model-based metric achieved 0.95 agreement with human evaluations on VQAv2, making it reliable proxy for human judgment. Findings. Analysis of GPT-4os errors revealed that approximately 66% of the misclassifications were due to evaluation failures rather than the models inability to answer the questions. With human corrections, GPT-4o achieved an accuracy of 88% on VQAv2, significantly higher than the 32% accuracy reported by rule-based metrics. comparison between rule-based and model-based evaluations (i.e., human-proxy evaluation) showed Spearman correlation of only 0.09, as seen in Figure 2 (left), demonstrating that rulebased metrics provide nearly random and unreliable scores, failing to distinguish between state-of-the-art VLMs. Method Human Naive w/o Concept w/o Reason w/o Vision w/o Data w/o Bias w/o Reviewer w/o Refiner AutoConverter () M1 M2 M3 Avg () 4.59 4.59 4.66 4.69 4.68 4.66 4.65 4.64 4.28 4.69 33. 35.2 46.0 52.4 34.2 40.6 50. 59.0 33.6 30.0 29.0 29.4 30.6 30.2 25.0 37.8 40.2 39.2 36.6 37.6 38.2 34.2 46.4 47.8 45.8 47.0 48.6 45.4 42.0 57.4 56.2 57.2 57.8 60.2 57.2 50.0 27. 37.2 44.2 53.6 41.8 46.0 43.8 43.5 42.8 42.7 44.2 42.7 37. 40.7 (b) Ablation of AutoConverter on MMMU. is correctness (higher is better), Avg is average model performance (lower is better). M1 is PaliGemma-3B, M2 is LLaVA-1.5-7B, M3 is Phi-3.5-Vision, M4 is Qwen2-VL-7B. (a) AutoConverter framework. Figure 3. AutoConverter framework and results. increasing difficulty and ensuring the correctness of the converted question. (Right) We perform an ablation study on AutoConverter and find that each component is crucial for enhancing question correctness and achieving the desired level of difficulty. (Left) AutoConverter is multi-agent framework with two key steps: 3.2. Model-based Evaluation cially when older versions are deprecated. Background. When ground-truth answers are longer, reflecting more realistic use cases for VLMs, rule-based metrics become even less effective at capturing semantic equivalence between predictions and answers. Consequently, the community increasingly relies on model-based evaluation [11, 64], which leverages advances in language models like GPT-4o with carefully designed prompts to generate similarity scores between predictions and answers. Although these similarity scores correlate well with human evaluations, changes in model versions can lead to significant shifts in evaluation results. Experiment. To evaluate the stability and robustness of model-based metrics, we tested 12 state-of-the-art VLMs on the MMVet dataset [58], where answer length is 63 words on average. Using two versions of GPT-4o (GPT-4o-0806 and GPT-4o-0513) as evaluators, we kept all other variables constant, such as model responses and evaluation prompts. Findings. While model-based evaluation is reliable in ranking models comparatively, it is costly and introduces notable shifts in absolute scores. As shown in Figure 2 (right), we observed perfect 1.0 correlation in VLM rankings between the two GPT-4o versions, underscoring the model-based evaluations strong alignment with human judgment. However, the absolute scores from GPT-4o-0806 were consistently 6% higher than those from GPT-4o-0513. Further analysis indicated that GPT-4o-0806 tended to assign score of 1.0 instead of 0.9 for nearly-correct responses, while GPT-4o-0513 showed the opposite tendency. These differences in absolute values hinder the comparability of results across different model versions, significantly impacting the reproducibility of research findings, espe4. AutoConverter: An Agentic Pipeline Generating Challenging Multi-Choice Questions Given the challenge of evaluating open-ended questions for vision language models (VLMs) detailed in 3, how can we mitigate these issues? We propose to convert openended questions into multiple-choice format, capitalizing on the simplicity and objectivity of evaluating multiplechoice questions. However, traditionally, creating multiplechoice questions, especially reasonable yet challenging distractor options, requires substantial human expertise and effort. In this section, we introduce AutoConverter, an agentic pipeline that automatically generates high-quality multiplechoice questions from open-ended ones. 4.1. Problem Formulation and Key Desiderata We define the multiple-choice question conversion process as follows: given an image, question, and the correct answer (v, q, a) Y, where and represent sets of images and texts respectively, we aim to generate set of distractor choices Cd = {ci [N ]}, where is the number of distractors. We define the candidate choice set as = Cd {a}, forming the final multiple-choice question as (v, q, C). We choose = 3 in our work because 4-choice is the most common configuration for multiplechoice questions and minimizes the risk of option selection bias for language models [63]. Two key desiderata are crucial for the multiple-choice conversion process. The first is correctness, ensuring that the multiple-choice question is valid with only one correct answer. The second is difficulty, ensuring that the ques4 Figure 4. AutoConverter generates challenging multiple-choice questions. Using AutoConverter, we generated distractors for questions and answers from three existing multiple-choice datasets: MMMU, MathVista, and AI2D, and compared them with original human-created distractors. We evaluated various VLMs on both the AutoConverter-generated and the original questions, finding that VLMs consistently achieved similar or even lower accuracy on the AutoConverter-generated questions compared to the original ones. tion cannot be answered trivially and possesses sufficient discriminative power for test takers. We leverage recent advances in language model agent research, particularly in role-playing [43] and self-reflection techniques [48], to accomplish these goals. 4.2. Ensuring Correctness Correctness is essential when converting open-ended questions to multiple-choice format, defined as having exactly one correct answer. Since our conversion process retains the original open-ended question stem and its answer, we only need to ensure that the other distractors are incorrect given the question and its correct answer. To achieve this, we employ multi-agent framework to rigorously evaluate distractor options, further enhancing the correctness of the generated multiple-choice questions through iterative refinement. Evaluating correctness. We use GPT-4o as correctness checker, which evaluates each distractors similarity to the correct answer and assigns 5-point Likert score for the overall correctness of the question. score of 5 indicates strong confidence in having single correct answer, while score of 1 suggests ambiguity or the presence of multiple correct answers. The evaluator was developed based on small-scale study of correct and incorrect questions. To validate the effectiveness of this evaluator, we conducted large-scale human annotations on 2,400 questions to assess their correctness (details in 5). Our results indicate that 51%, 51%, 63%, 84%, and 95% of questions are deemed correct when assigned correctness scores of 1, 2, 3, 4, and 5, respectively, by our evaluator. Thus, we can use this correctness evaluator as judge to assess question correctness and refine questions with low correctness scores. Refining questions to enhance correctness. Since the correctness evaluator can accurately flag problematic questions, we refine questions with low correctness scores in an iterative manner to enhance their accuracy. Specifically, when the correctness score of generated question falls below threshold of 4, we use GPT-4o as refiner to adjust distractors based on feedback from the correctness evaluator. This refinement continues until the evaluators correctness score meets the required threshold or until maximum of three refinement rounds is reached. The entire process is shown in Figure 3a (right). Our ablation studies demonstrate that this process plays significant role in enhancing the correctness of the questions. 4.3. Increase Difficulty Difficulty is another critical factor in designing effective multiple-choice questions. Our goal is to prevent questions from being trivially answered due to obviously incorrect options or shortcuts, ensuring that each question retains the discriminative power needed to rigorously test VLM capabilities. Inspired by human strategies for creating challenging distractors, we introduce multi-agent framework powered by GPT-4o. This framework iteratively generates and refines large set of difficult distractors from multiple perspectives and ultimately selects those that pose the highest level of difficulty. Generating diverse set of distractors. We first create large pool of candidate distractors based on common error types to capture range of potential challenges. We categorize these error types as follows: concept misunderstanding, visual misinterpretation, reasoning error, data processing error, and question bias. Detailed definitions for each error type are provided in the Appendix. For each error type, GPT-4o proposes set of distractors based on the image, question, and correct answer, along with accompanying rationales. This pool of candidates forms comprehensive 5 Figure 5. Qualitative comparison of the original questions, naive baseline-generated questions, and AutoConverter-generated questions. AutoConverter simulates errors from different perspectives and produces correct and challenging multiple-choice questions. foundation for our selection process. Iterative refinement of distractors. After generating initial distractors, we employ another GPT-4o agent as reviewer to provide targeted feedback for each distractor. This reviewer assesses the plausibility and challenge level of each distractor given the context of the question and answer, offering suggestions for improvement. The specialized proposer agent then refines the distractors based on this feedback, ensuring that each distractor is as challenging as possible and removing those considered too close to the correct answer to maintain correctness. Selecting high-difficulty distractors. From the refined pool of distractors with their creation rationales, we finally use GPT-4o as selector to evaluate each candidates difficulty and correctness, selecting the most challenging distractors along with justifications for their selection. This process results in final set of high-quality distractors that rigorously test VLM performance. This process is illustrated in Figure 3a (left). Our detailed ablation studies demonstrate that each of these agents contributes to improving the difficulty of the questions. 4.4. AutoConverter Results To evaluate the effectiveness of AutoConverter in generating high-quality multiple-choice questions, we leverage three existing multiple-choice VQA datasets: MMMU [59], MathVista [32], and AI2D [19]. We retain all questions with four choices and regenerate the distractors based on the image, question, and correct answer. We then compare AutoConverter-generated distractors with the original humancrafted ones by evaluating various VLMs on both the original and converted datasets."
        },
        {
            "title": "AutoConverter generates",
            "content": "correct multiple-choice questions. Using correctness evaluator and refiner pipeline, we ensure the accuracy of the generated questions. Our analysis shows that the generated questions maintain correctness level comparable to that of the original human-crafted questions. After filtering to keep only questions with correctness score of 5, large-scale human annotation indicates that only 3% of questions are marked as incorrect for MMMU, MathVista, and AI2D. Among these, 52% of errors are due to incorrect original answers, while only 48% are introduced by the AutoConverter process. These results demonstrate that most AutoConverter-generated questions are accurate and suitable for reliable VLM evaluation. AutoConverter generates challenging multiple-choice questions. As shown in Figure 4, AutoConverter produces highly challenging questions, with broad range of VLMs achieving similar or even lower accuracy on the generated distractors compared to the original human-crafted ones across MMMU, MathVista, and AI2D. Notably, MMMU is regarded as one of the most challenging VQA datasets, as it is sourced from exams and textbooks. These results highlight AutoConverters capability not only in converting open-ended questions to multiple-choice but also in refining existing multiple-choice questions to enhance their difficulty. Furthermore, AutoConverter could have applications beyond VLM evaluation, such as generating challenging questions for educational purposes. Each agent in AutoConverter contributes to jointly improving correctness and difficulty. To analyze each agents role in AutoConverter, we conduct an ablation study by removing each component, as shown in Table 3b. First, removing specialized error proposers results in decrease in difficulty, with an average increase of 1.6% in relative accuracy. Next, omitting the reviewer for iterative distractor refinement leads to 4.9% relative increase in average VLM performance, indicating lower difficulty. Finally, removing the question evaluator and refiner, which serve as correctness safeguards, causes an 8.7% relative decrease in correctness score. We also compare AutoConverter with naive baseline that uses the prompt Please generate 3 distractors given the question, answer, and image. As shown 6 Figure 6. VMCBench overview. (Left) VMCBench is constructed by converting 12 open-ended (OE) and refining 8 multiple-choice (MC) VQA datasets into unified multiple-choice format, with human validation ensuring correctness. The number of questions per dataset is listed. (Right) Example questions from VMCBench, showcasing diverse question types across multiple domains. in Table 3b, AutoConverter significantly outperforms this baseline in both correctness (2.2% relative increase) and difficulty (11.5% relative decrease in average VLM performance), demonstrating the effectiveness of our approach. We provide qualitative comparisons of original and AutoConverter-generated questions, as well as the naive baseline, in Figure 5, which demonstrate the effectiveness of AutoConverter in creating hard yet correct questions. 5. VMCBench: Unified Multiple-Choice Visual Question Answering Benchmark Using our AutoConverter method, detailed in 4, we introduce VMCBench benchmark that unifies 20 existing visual question answering (VQA) datasets into consistent multiple-choice format. VMCBench spans diverse array of visual and linguistic contexts, rigorously testing various model capabilities. By transforming open-ended questions into multiple-choice format, VMCBench enhances vision language model evaluation by mitigating ambiguities while preserving the complexity of the tasks. This benchmark provides reliable and reusable resource for the evaluation of future vision language models (VLMs). 5.1. Benchmark Overview VMCBench transforms 20 widely-used VQA datasets into unified multiple-choice benchmark. These datasets can be broadly categorized to assess general capabilities of VLMs (VQAv2 [13], OKVQA [35], MMVet [58], VizWiz [14], A-OKVQA [47], MMStar [6], SEEDBench [24]), reasoning capabilities (MathVision [53], GQA [17], MMMU [59], RealWorldQA [56], MathVista [32], ScienceQA [31]), OCR tasks (OCRVQA [39], TextVQA [49]), and document and chart understanding (DocVQA [37], InfoVQA [38], ChartQA [36], TableVQABench [20], AI2D [19]). Among these, 12 datasets consist of open-ended questions, which we convert into the multiple-choice format to facilitate evaluation. The remaining 8 datasets are already in multiple-choice format; for these, we apply AutoConverter to refine distractors, increasing their difficulty. These refined datasets can further test model robustness and help identify dataset contamination [15, 41, 46, 61]. For each dataset, we randomly sampled up to 500 questions, as recent studies suggest this sample size may suffice for evaluating model performance [44, 45], resulting in total of 9,450 questions across the 20 datasets. We apply AutoConverter to these 9,450 questions. To ensure the correctness of the converted questions, AutoConverter includes an internal correctness evaluator that assigns correctness score to each converted question, ensuring only one correct answer exists. Out of the 9,450 converted questions, 103, 162, 399, 635, and 8,151 questions received correctness scores of 1, 2, 3, 4, and 5, respectively. Six PhD student annotators reviewed all questions with correctness scores below 5 and randomly sampled 1,101 questions with score of 5, resulting in total of 2,400 annotated questions. The correctness rates for questions with scores of 1, 2, 3, 4, and 5 were 51%, 51%, 63%, 84%, and 95%, respectively. This high accuracy of the correctness evaluation supports the reliability of questions with confidence score of 5 for future dataset construction. Additionally, we annotated the source of errors, distinguishing between errors from the ground-truth answers (original dataset errors) and those introduced by AutoConverter. Notably, 59% of errors were due to issues in the ground-truth answers, indi7 Model General Reasoning OCR Doc&Chart Avg. 5.2. Evaluation Results 88.5 Qwen2-VL-72B 85.2 GPT-4o 82.9 Molmo-72B 84.5 Qwen2-VL-7B 81.3 Claude-3.5-Sonnet 83.7 Cambrian-34B 79.6 Gemini-1.5-Pro 82.5 VILA1.5-40B 80.9 GPT-4o-Mini 77.9 Qwen2-VL-2B 78.1 CogVLM2-19B 74.1 Phi-3-Vision 79.3 Cambrian-13B 77.9 Cambrian-8B 73.2 Molmo-7B-D 77.8 Idefics2-8B 72.6 Molmo-7B-O 71.4 Phi-3.5-Vision 74.6 VILA1.5-13B 73.2 DeepSeek-VL-7B 69.4 Molmo-1B 72.3 CogVLM-17B 72.4 VILA1.5-8B 59.7 Gemini-1.5-Flash 71.7 PaliGemma-3B VILA1.5-3B 70.3 DeepSeek-VL-1.3B 68.9 66.4 LLaVA1.5-13B 63.6 LLaVA1.5-7B 53.1 Chameleon-30B 55.1 InstructBLIP-7B 54.8 InstructBLIP-13B 41.0 Chameleon-7B 72.6 66.9 66.6 62.7 62.8 65.9 64.7 65.3 58.8 55.8 55.6 56.4 54.6 56.4 55.5 55.8 54.3 55.3 54.1 52.6 50.1 48.8 51.8 53.8 51.3 48.0 43.6 46.2 44.7 41.2 35.2 34.7 34.7 96.8 96.4 94.7 96.4 93.4 95.7 92.6 93.2 93.8 93.1 92.3 90.6 92.3 91.0 91.7 92.7 88.5 87.2 85.3 85.8 87.4 77.8 81.8 79.9 53.1 78.9 79.5 75.8 74.0 48.0 47.7 48.7 42.3 90.1 83.1 81.1 80.1 84.6 73.3 72.6 67.4 74.8 72.5 72.6 73.8 66.6 65.4 72.1 61.8 68.9 68.6 50.2 52.9 60.2 54.6 46.5 56.3 53.0 42.3 43.8 37.0 35.0 33.9 29.9 26.5 29. 85.0 80.3 78.7 78.1 77.8 77.0 74.7 74.7 74.0 71.5 71.4 70.3 70.0 69.6 69.5 68.7 67.8 67.4 63.4 63.2 63.1 61.3 60.7 59.1 59.0 57.5 56.1 53.9 51.8 44.2 42.1 41.1 36.4 Table 1. Performance of 33 VLMs on VMCBench test set. cating that if all original open-ended questions were correct, AutoConverter would introduce only 2% errors in total for questions with correctness score of 5. We removed all questions that were identified as incorrect, resulting in final total of 9,018 questions. VMCBench spans diverse range of visual and linguistic contexts and includes broad array of question types, offering comprehensive framework to evaluate VLMs across multiple vision language tasks. It provides unified interface for efficient and accurate assessment of VLM capabilities. We split the VMCBench into 1,000 validation questions with provided answers and 8,018 test questions without answers. An online test server is available to support future VLM development. The question distribution and examples of converted questions are shown in Figure 6. We evaluated 33 state-of-the-art vision language models (VLMs) on the VMCBench test set to assess their performance, including GPT-4 [40], Gemini-1.5 [51], Claude3.5 [2], Qwen2-VL [54], Molmo [8], Cambrian [52], VILA [26], CogVLM [16], Phi Idefics2 [22], DeepSeek-VL [29], PaliGemma [3], LLaVA1.5 [27], Chameleon [50], and InstructBLIP [7]. The evaluation is conducted in the zero-shot setting with prompt Question: {question} Options: A. {A} B. {B} C. {C} D. {D} Answer with the options letter from the given choices directly. The results are summarized in Table 1. Detailed results are in Appendix. [1], Our findings reveal several key insights: Private models remain the top performers, but the gap is narrowing. As shown in Table 1, the bestperforming model on VMCBench is GPT-4o, achieving 80.6% overall accuracy. However, the gap between GPT-4o and the second-best model, Qwen2-VL-7B, is only 2.0%. This open-source model demonstrates that the performance difference between private and public models is decreasing. Rapid advances in VLM development. Another notable trend is the significant improvement from InstructBLIP to Qwen2-VL-7B, nearly doubling in performance (40.9% vs. These models were released in 2023 and 2024, respectively, illustrating the rapid pace of progress in VLMs. 78.6%). Scaling up models generally improves performance. As expected, scaling up model sizes typically leads to performance gains. For instance, across different model families (such as VILA, Cambrian, Chameleon, DeepSeek, and LLaVA), we observe clear improvement with larger models, similar to scaling trends in language models [4, 18]. Evaluating across diverse datasets is essential to uncovering model limitations. comparison between Phi-3 and Phi-3.5 reveals that while Phi-3.5 achieves better performance on some reasoning datasets such as MMMU and MathVista, it performs significantly worse on OCR and document/chart understanding tasks, which reduces its overall performance. This underscores the importance of holistic evaluation of models across broad range of datasets. Previously, this was challenging due to variations in dataset formats and evaluation protocols. VMCBench addresses this by unifying the task into multiple-choice format, thus reducing evaluation costs and standardizing comparisons. Continuous feature inputs outperform discrete tokens. Chameleon is the only model in our experiments that utilizes VQGAN discrete tokens, yet it performs significantly worse than smaller models trained with continuous features using less computational resources. This observation raises an intriguing question: why might VQGAN tokens be less effective for image understanding tasks?"
        },
        {
            "title": "We believe that VMCBench is a valuable benchmark that",
            "content": "8 will play significant role in advancing VLM development by facilitating simple, fast, and accurate model evaluation. 6. Conclusion We address limitations in open-ended visual question answering benchmarks for evaluating vision language models (VLMs) by introducing AutoConverter, multi-agent system that transforms questions into multiple-choice format with challenging distractors. AutoConverter effectively creates questions that match or exceed the difficulty of humanwritten distractors, with VLMs often achieving similar or lower accuracy. Building on this, VMCBench provides benchmark of 9,018 multiple-choice questions, setting new standard for consistent and rigorous VLM evaluation."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Ross Girshick for discussing the idea behind this project. We also thank OpenAIs Researcher Access Program (ID 0000005368) for granting us API access. Serena Yeung-Levy is Chan Zuckerberg Biohub San Francisco Investigator."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 8 [2] Anthropic. Introducing the next generation of claude, 2024. 1, 2, 8 [3] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 8 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 8 [5] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In ICML, 2024. 2 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In NeurIPS, 2024. 2, 7 [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. In NeurIPS, 2023. 8 [8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [9] Wenjian Ding, Yao Zhang, Jun Wang, Adam Jatowt, and Zhenglu Yang. Can we learn question, answer, and distractors all from an image? new task for multiple-choice visual question answering. In COLING, 2024. 3 [10] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In MM, 2024. 1 [11] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023. 2, 4 [12] Alexander Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. SumTACL, meval: Re-evaluating summarization evaluation. 2021. 3 [13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating 9 the role of image understanding in visual question answering. In CVPR, 2017. 1, 2, 3, [14] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018. 2, 7 [15] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: critical analysis of out-of-distribution generalization. In ICCV, 2021. 7 [16] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 8 [17] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 2, 7 [18] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 8 [19] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. 2, 6, [20] Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqabench: visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024. 2, 7 [21] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Building and better understanding visionarXiv Tronchon. language models: preprint arXiv:2408.12637, 2024. 2 insights and future directions. [22] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. 8 [23] Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, et al. Vhelm: arXiv holistic evaluation of vision language models. preprint arXiv:2410.07112, 2024. 1 [24] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In CVPR, 2024. 2, [25] CY LIN. Rouge: package for automatic evaluation of summaries. In ACL Workshop, 2004. 3 [26] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. 8 [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 2, 8 [28] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. In ICLR, 2024. 10 [29] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. 8 [30] Jiaying Lu, Xin Ye, Yi Ren, and Yezhou Yang. Good, better, best: Textual distractors generation for multiple-choice visual question answering via reinforcement learning. In CVPR, 2022. 3 [31] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS, 2022. 1, 2, 7 [32] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. 2, 6, 7 [33] Haohao Luo, Yang Deng, Ying Shen, See-Kiong Ng, and Tat-Seng Chua. Chain-of-exemplar: enhancing distractor generation for multimodal educational question generation. In ACL, 2024. [34] Oscar Manas, Benno Krojer, and Aishwarya Agrawal. Improving automatic vqa evaluation using large language models. In AAAI, 2024. 2 [35] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In CVPR, 2019. 1, 2, 7 [36] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL Findings, 2022. 2, 7 [37] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. 7 [38] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In WACV, 2022. 2, 7 [39] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 2, [40] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2, 8 [41] Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. Proving test set contamination in black-box language models. In ICLR, 2024. 7 [42] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, 2002. [43] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In UIST, 2023. 3, 5 [44] Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, [59] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 1, 2, 6, 7 [60] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multiarXiv discipline multimodal understanding benchmark. preprint arXiv:2409.02813, 2024. 3 [61] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, et al. careful examination of large language model performance on grade school arithmetic. arXiv preprint arXiv:2405.00332, 2024. 7 [62] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori Hashimoto. Benchmarking large language models for news summarization. TACL, 2024. [63] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In ICML, 2021. 4 [64] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS, 2023. 2, 3, 4 and Leshem Choshen. Efficient benchmarking (of language models). In NAACL, 2024. 7 [45] Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples. In ICML, 2024. 7 [46] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019. 7 [47] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In ECCV, 2022. 1, 2, 7 [48] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In NeurIPS, 2024. 3, 5 [49] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. 2, [50] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 8 [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 2, 8 [52] Shengbang Tong, Ellis Brown II, Penghao Wu, Sanghyun Woo, ADITHYA JAIRAM IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 8 [53] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. 2, 7 [54] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 8 [55] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. [56] xAI. Realworldqa dataset, 2024. 2, 7 [57] Han-Cheng Yu, Yu-An Shih, Kin-Man Law, Kai-Yu Hsieh, Yu-Chen Cheng, Hsin-Chih Ho, Zih-An Lin, Wen-Chuan Hsu, and Yao-Chung Fan. Enhancing distractor generation for multiple-choice questions with retrieval augmented pretraining and knowledge graph integration. arXiv preprint arXiv:2406.13578, 2024. 3 [58] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In ICML, 2024. 1, 2, 4,"
        },
        {
            "title": "Broader Impacts and Ethics Statement",
            "content": "AutoConverter and VMCBench simplify and standardize vision language model (VLM) evaluation, providing valuable tool for advancing VLM development through scalable, consistent, and reproducible evaluation. Beyond VLM evaluation, AutoConverter can be applied to other domains, such as education, to generate challenging and high-quality questions. However, human verification is essential to ensure the generated questions align with their intended purpose, maintain proper value, and achieve the appropriate level of difficulty without introducing biases."
        },
        {
            "title": "Reproducibility Statement",
            "content": "We provide an open-source implementation of our work at https : / / github . com / yuhui - zh15 / AutoConverter. This will enable researchers to reproduce all the experiments in paper and conduct their own analyses."
        },
        {
            "title": "Limitations",
            "content": "AutoConverter achieves high question correctness, but 5% of the questions with the highest correctness scores still contain errors, although half of which are due to flaws in the original datasets. Moreover, while VMCBench includes diverse range of models and datasets, its coverage remains incomplete. Moving forward, we aim to leverage AutoConverter to expand dataset and model coverage, addressing evolving needs in VLM evaluation."
        },
        {
            "title": "Summary of Appendix",
            "content": "In A, we present further analysis of the evaluation failures for open-ended questions discussed in 3. In B, we provide supplementary details on the AutoConverter framework described in 4. In C, we elaborate more details of VMCBench as outlined in 5. A. Supplementary Section 3 A.1. Examples of Rule-based Evaluation Failures In the main paper, we demonstrated that rule-based evaluation of open-ended questions leads to poor evaluation outcomes. Table 2 provides six examples of rule-based evaluation failures, clearly illustrating that these methods fail to account for semantic similarity and penalize formatting errors, resulting in highly inaccurate evaluation results. A.2. Examples of Model-based Evaluation Failures In the main paper, we demonstrated that model-based evaluation is sensitive to model versions. To investigate this further, Table 3 presents six examples of inconsistencies caused by different model-based evaluations. We observe that GPT-4o-0806 often assigns perfect score of 1.0 for similar predictions and answers, whereas GPT-4o-0513 tends to assign score of 0.9. This behavior variation introduces significant differences in evaluation results and raises concerns about reproducibility in future research. B. Supplementary Section 4 B.1. Prompts for AutoConverter In the main paper, we introduced the agentic system of AutoConverter, comprising the proposer, reviewer, selector, evaluator, and refiner agents. This system creates large pool of high-quality distractor options to increase question difficulty while ensuring correctness in the converted multiple-choice questions. Detailed prompts for the proposers, designed to create distractors addressing vision, reasoning, data, concept, and bias errors, are shown in Figures 11, 12, 13, 14, and 15, respectively. These prompts are carefully crafted to align with common types of human errors, as defined in the corresponding sections. Figure 16 presents prompts for the reviewer, whose feedback iteratively refines the distractors to improve their quality. The selector prompts in Figure 17 guide the selection of the three most challenging distractors to enhance question difficulty. Figures 18 and 19 show prompts for the evaluator and refiner, respectively. This iterative process ensures the correctness of the generated questions, guaranteeing that there is only one correct answer. B.2. AutoConverter Results for Additional Datasets In the main paper, we demonstrated AutoConverters ability to generate challenging multiple-choice questions for three datasets: MMMU, MathVista, and AI2D. Figure 7 extends this analysis to five additional datasets with human-created distractors: A-OKVQA, RealWorldQA, ScienceQA, SEEDBench, and MMStar. Across these datasets, VLMs consistently achieved similar or lower accuracy on AutoConverter-generated questions compared to the original ones, demonstrating the systems capability to produce highly challenging multiple-choice questions. B.3. AutoConverter Results for Different Models In the main paper, we constructed AutoConverter using GPT-4o. potential concern is whether this choice introduces bias into the generated questions. To examine this, we used three state-of-the-art proprietary VLMsGPT-4o, Claude-3.5-Sonnet, and Gemini1.5-Proto generate questions. We evaluated various VLMs on these questions and computed the correlations of If no bias exists, we expect their performance rankings. C.2. Full Evaluation Results on VMCBench In the main paper, we reported model performances grouped by benchmarked capabilities. Table 4 presents the full evaluation results for 33 VLMs across 20 individual benchmarks, further validating our conclusions. C.3. Scaling Trends on VMCBench Scaling laws are cornerstone of model development, demonstrating that larger models typically achieve better performance. To explore this, we plotted the scaling trends of VLM families in log scale based on known model sizes, as shown in Figure 10. Surprisingly, we observe clear log-linear scaling trend across most VLM families, indicating that VMCBench offers smooth evaluation gradient for varying capabilities. Certain model families outperform others, leaving further exploration of these scaling trends to future work. C.4. Additional Examples from VMCBench To provide deeper understanding of VMCBench, we include 60 examples (three from each of the 20 sources) in Tables 5 through 11. These examples demonstrate the high quality of our dataset and offer valuable insights for readers. high correlations across all question sets, as they should reflect the true discriminative power of the models. Figure 8 supports our hypothesis. The rankings derived from GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5Pro questions exhibit near-perfect correlations (0.90 Spearman correlation averaged over three datasets), indicating that the choice of generator does not introduce significant bias. B.4. Correlation between Open-ended and Multiple-choice Questions In the main paper, we highlighted the challenges of accurately evaluating open-ended questions. Therefore, we propose an alternative solution to convert open-ended questions into multiple-choice format. key question here is whether converting open-ended questions into multiplechoice format preserves their discriminative power. To address this, we treat model-based evaluation of open-ended questions as proxy for ground-truth evaluation. Specifically, if model-based evaluator determines that model outperforms model B, we consider this ranking correct. This assumption is valid because model-based evaluations have high correlation with human judgments, albeit with instability across versions. By using the same GPT version as the evaluator, we eliminate such instability. We compare the correlation between model-based evaluation of open-ended questions and rule-based evaluation of multiple-choice questions against the correlation between model-based and rule-based evaluation of open-ended questions. Our findings, shown in Figure 9, reveal that the correlation for multiple-choice questions is significantly higher0.85, 0.71, and 0.97 for VQAv2, OKVQA, and VizWiz, respectivelycompared to rule-based open-ended evaluations, which achieve correlations of 0.09, 0.19, and 0.00. This demonstrates that converting to multiple-choice questions improves evaluation accuracy and retains discriminative power. C. Supplementary Section 5 C.1. Human Evaluation Results on VMCBench VMCBench provides scalable, consistent, and reproducible benchmark for evaluating and advancing VLMs. The current best-performing model, GPT-4o, achieves an accuracy of 80.6%. To assess the remaining room for improvement, we conducted human evaluation of VMCBench. Human experts achieved an accuracy of 91.7%, highlighting significant opportunities for further model improvement. Among the 8.3% human errors, approximately three-quarters of the questions require extensive knowledge, while quarter are ambiguous and unanswerable. 13 VQAScore ModelScore Image Question Where is the mans left hand holding? Answer chin Prediction On his chin. How many sinks are there? 2 Two sinks. What is the light source? street lights Streetlights What is the horse jumping over? log Log fence Where is the train going? svanemollen Svanemøllen 0 0 0 0 What colors are on this train? blue and yellow and silver Blue, yellow, silver. and 0 1 1 1 1 1 Table 2. Examples of rule-based evaluation failures of open-ended questions. Rule-based methods fail to account for semantic similarity and penalize formatting errors, resulting in highly inaccurate evaluation results. 14 Image Question Answer Prediction Does the giraffe appear dangerous? no Is the man happy? yes Is that wine in the background? no all What the are scene text in the image? S3302CD The giraffe in the image does not appear to be dangerous. It seems to be calm and at ease with the persons presence, indicating level of trust and familiarity between them. The man in the image appears to be happy. His facial expression, characterized by broad smile and eyes that seem to be crinkling at the corners, typically indicates joy or amusement... No, there is no wine in the background. The image shows pizza and can of Coca-Cola on desk, with laptop and some other items in the background. The scene text in the image reads 3302 CD. GPT-4o0513 Score GPT-4o0806 Score 0.9 1.0 0. 1.0 0.9 1.0 0.9 1.0 is this banWhat knote? ten pound or 10 pound or British pound This is ten-pound banknote from the United Kingdom, featuring the portrait of Queen Elizabeth II. 0.9 1.0 How many birds are there? 6 There are six birds perched on the branch, each displaying unique coloration and size, which suggests they may belong to different species. 0.9 1.0 Table 3. Examples of model-based evaluation failures of open-ended questions. GPT-4o-0806 often assigns perfect score of for similar predictions and answers, whereas GPT-4o-0513 tends to assign score of 0.9. This behavior variation introduces significant differences in evaluation results and raises concerns about reproducibility in future research. Model SEEDBench MMStar A-OKVQA VizWiz MMVet VQAv OKVQA MMMU MathVista ScienceQA Qwen2-VL-72B GPT-4o Molmo-72B Qwen2-VL-7B Claude-3.5-Sonnet Cambrian-34B Gemini-1.5-Pro VILA1.5-40B GPT-4o-Mini Qwen2-VL-2B CogVLM2-19B Phi-3-Vision Cambrian-13B Cambrian-8B Molmo-7B-D Idefics2-8B Molmo-7B-O Phi-3.5-Vision VILA1.5-13B DeepSeek-VL-7B Molmo-1B CogVLM-17B VILA1.5-8B Gemini-1.5-Flash PaliGemma-3B VILA1.5-3B DeepSeek-VL-1.3B LLaVA1.5-13B LLaVA1.5-7B Chameleon-30B InstructBLIP-7B InstructBLIP-13B Chameleon-7B 84.2 84.2 82.0 82.7 78.3 83.5 77.3 81.2 79.3 77.8 77.3 78.3 79.3 78.3 74.1 77.8 75.1 74.8 77.5 75.6 70.9 70.9 74.3 56.3 74.6 74.3 70.9 66.2 62.2 53.6 52.8 48.4 44.9 71.5 62.2 62.5 60.3 53.7 59.4 52.0 58.0 47.7 44.7 48.2 46.8 48.5 53.0 46.8 49.4 45.8 45.8 44.9 39.9 43.0 42.8 40.9 38.0 39.9 38.5 37.5 37.3 34.2 33.0 34.7 29.0 31.6 93.4 92.0 88.5 90.1 86.4 91.1 88.0 90.8 86.1 86.6 87.8 81.6 86.4 84.5 82.4 85.4 80.5 76.5 81.9 80.5 77.9 80.7 77.9 67.0 87.3 76.9 74.4 76.5 72.5 57.2 61.9 63.3 46. 94.9 94.4 88.5 92.4 87.2 90.7 89.0 90.2 92.2 88.2 87.3 79.9 87.3 88.0 81.9 84.8 78.9 75.7 83.3 82.1 80.9 85.0 79.4 68.5 77.0 80.9 82.1 76.0 73.8 52.2 65.4 64.2 40.4 88.5 80.6 79.1 82.0 87.8 81.3 79.9 77.0 80.6 70.5 73.4 67.6 76.3 68.3 63.3 69.8 66.2 64.0 63.3 63.3 54.7 59.7 64.7 53.2 50.4 57.6 52.5 59.7 54.0 48.9 39.6 43.2 29.5 92.4 88.7 85.6 91.4 84.7 88.4 80.8 87.3 88.2 88.7 85.2 79.2 87.0 85.6 80.3 86.8 78.2 79.4 82.4 83.6 75.7 80.1 80.8 64.1 87.7 78.5 80.3 64.1 66.7 58.3 59.7 64.1 41.4 94.8 94.1 94.1 92.6 91.1 91.9 90.4 93.3 92.1 88.6 87.4 85.2 90.6 87.9 83.5 90.4 83.2 83.5 88.6 87.7 83.0 86.7 88.6 70.6 85.2 85.4 84.7 85.2 82.2 68.6 71.9 71.6 53.1 70.2 70.0 59.4 54.1 59.6 55.0 59.6 58.2 56.5 46.2 39.7 44.7 41.3 43.3 43.0 38.5 44.0 45.2 42.8 41.1 36.3 37.5 38.0 48.1 29.1 34.4 31.0 37.5 35.6 34.4 31.0 25.7 32.7 70.3 51.0 60.9 48.0 56.9 60.9 56.9 65.3 43.1 39.1 35.6 38.6 39.6 45.5 37.6 41.6 35.6 39.1 48.5 33.2 27.7 36.1 49.0 57.9 30.7 39.6 22.3 31.7 31.2 32.7 22.8 19.8 22. 87.1 86.9 89.4 86.9 79.9 83.5 83.0 83.3 78.7 76.0 90.5 92.1 77.4 77.4 91.9 91.9 90.0 87.3 72.6 86.7 89.1 66.7 71.5 66.3 94.3 64.9 63.8 66.3 68.6 57.9 46.8 50.0 53.6 Model RealWorldQA GQA MathVision TextVQA OCRVQA AI2D ChartQA DocVQA InfoVQA TableVQABench Avg. Qwen2-VL-72B GPT-4o Molmo-72B Qwen2-VL-7B Claude-3.5-Sonnet Cambrian-34B Gemini-1.5-Pro VILA1.5-40B GPT-4o-Mini Qwen2-VL-2B CogVLM2-19B Phi-3-Vision Cambrian-13B Cambrian-8B Molmo-7B-D Idefics2-8B Molmo-7B-O Phi-3.5-Vision VILA1.5-13B DeepSeek-VL-7B Molmo-1B CogVLM-17B VILA1.5-8B Gemini-1.5-Flash PaliGemma-3B VILA1.5-3B DeepSeek-VL-1.3B LLaVA1.5-13B LLaVA1.5-7B Chameleon-30B InstructBLIP-7B InstructBLIP-13B Chameleon-7B 78.7 75.0 70.0 67.9 63.2 65.6 60.6 63.1 65.8 57.6 56.9 54.6 58.3 61.5 58.7 55.0 55.0 47.9 48.2 48.4 45.2 44.0 45.0 48.0 46.6 43.1 42.4 34.6 35.1 35.3 30.7 26.1 29. 91.4 84.4 83.1 89.0 81.9 87.5 79.0 88.8 80.2 83.9 81.4 79.0 86.8 85.3 74.1 80.0 75.3 81.2 83.6 82.6 76.0 78.2 79.2 62.1 86.1 79.5 76.5 81.2 72.6 59.2 59.4 60.6 44.5 38.0 34.4 36.9 30.6 35.1 42.7 49.2 33.3 28.3 32.4 29.4 29.2 24.5 25.2 27.9 27.6 26.1 30.8 29.0 23.6 26.3 29.9 27.9 40.2 20.9 26.5 25.6 25.8 25.4 27.6 20.2 25.8 26.1 98.4 97.5 95.3 97.3 93.6 94.8 91.5 91.2 95.1 94.8 96.2 92.1 91.0 92.4 93.0 91.7 90.8 82.9 81.6 83.6 87.0 65.4 75.5 79.6 34.6 76.0 78.9 67.4 64.3 39.3 50.8 48.1 35.7 95.1 95.3 94.0 95.6 93.2 96.6 93.8 95.1 92.5 91.5 88.3 89.1 93.5 89.6 90.4 93.8 86.3 91.5 89.1 88.1 87.8 90.2 88.1 80.2 71.5 81.9 80.1 84.2 83.7 56.7 44.6 49.2 49.0 85.2 81.8 79.7 75.2 72.9 74.9 72.2 71.3 72.7 68.8 68.3 69.5 71.8 71.3 66.3 66.3 67.4 73.1 65.8 59.5 60.1 59.5 61.3 53.1 64.7 53.8 47.6 56.0 47.4 47.2 38.3 33.0 36.0 90.1 80.0 81.2 78.9 87.9 78.9 72.9 73.9 71.6 70.9 75.7 78.0 71.6 70.9 74.8 67.0 74.3 73.6 55.3 60.8 62.4 61.5 46.3 55.2 51.8 44.5 47.7 29.8 28.0 28.4 24.5 20.0 24. 99.1 98.2 94.7 98.0 97.7 89.3 83.1 77.1 95.3 95.5 92.7 93.3 82.6 83.1 88.9 81.5 84.4 80.0 55.9 61.9 79.1 65.7 49.7 69.9 62.6 51.2 53.9 38.5 37.2 32.1 31.0 28.3 26.7 92.6 79.0 74.9 78.6 79.1 64.5 64.3 57.1 70.5 65.9 62.4 61.8 51.8 45.4 65.7 47.0 58.3 53.7 35.7 38.9 51.4 40.6 34.3 51.3 42.9 30.4 33.6 30.4 34.3 32.0 32.0 24.4 32.0 83.3 76.4 75.2 69.8 85.6 59.0 70.7 57.9 64.0 61.5 63.7 66.4 55.4 56.5 64.9 47.3 59.9 62.6 38.5 43.5 48.2 45.7 40.8 51.8 42.8 31.5 36.3 30.0 27.9 29.7 23.9 26.6 29.1 85.0 80.3 78.7 78.1 77.8 77.0 74.7 74.7 74.0 71.5 71.4 70.3 70.0 69.6 69.5 68.7 67.8 67.4 63.4 63.2 63.1 61.3 60.7 59.1 59.0 57.5 56.1 53.9 51.8 44.2 42.1 41.1 36.4 Table 4. Performance of 33 vision language models on 20 subsets of VMCBench test set. Figure 7. AutoConverter generates challenging multiple-choice questions. Using AutoConverter, we generated distractors for questions and answers from five multiple-choice datasets: A-OKVQA, RealWorldQA, ScienceQA, SEEDBench, and MMStar, and compared them with original human-created distractors. We evaluated various VLMs on both the AutoConverter-generated and the original questions, finding that VLMs consistently achieved similar or even lower accuracy on the AutoConverter-generated questions compared to the original ones. 17 Figure 8. AutoConverter results for different models. To examine whether GPT-4o used in AutoConverter introduces model bias, we used three state-of-the-art proprietary VLMsGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Proto generate questions. We evaluated 18 VLMs on these questions and computed the correlations of their performance rankings. We observe high correlations across all question sets, indicating no model bias exists, and these questions reflect the true discriminative power of the models. 18 Figure 9. Converting to multiple-choice questions improves evaluation accuracy and retains discriminative power. We treat modelbased evaluation of open-ended questions as proxy for ground-truth evaluation. We compare the correlation between model-based evaluation of open-ended questions and rule-based evaluation of multiple-choice questions against the correlation between model-based and rule-based evaluation of open-ended questions. We find that the correlation for multiple-choice questions is significantly higher compared to rule-based open-ended evaluations, demonstrating that converting open-ended questions into multiple-choice format preserves their discriminative power and simplifies the evaluation. Figure 10. Scaling trends on VMCBench. We observe clear log-linear scaling trend across most VLM families, indicating that VMCBench offers smooth evaluation gradient for varying capabilities. 20 You are an expert in creating challenging and educational multiple-choice questions, specializing in visual (cid:44) interpretation errors. Your task is to generate plausible but incorrect options (distractors) for given image-based question(s), focusing on (cid:44) misinterpretations of visual information. Given: 1. One or more images 2. An open-ended question about the image(s) 3. The correct answer to the question Your task: 1. Carefully analyze and understand the provided image(s). Briefly describe the image content(s) (for your (cid:44) understanding only, do not output this). 2. Generate {num_choice} unique and plausible distractor options based on visual interpretation errors. Each (cid:44) distractor should: - Be directly related to misinterpretation of the image(s) - Seem potentially correct at first glance - Be very misleading for students due to visual misunderstanding - Contain subtle error in interpreting visual information that makes it incorrect - Vary in difficulty and the type of visual misinterpretation it represents 3. Ensure you understand how the correct answer relates to specific visual elements in the image(s). 4. Focus on common visual interpretation errors, including: - Misreading Graphs or Charts: Create options that misinterpret trends, scales, or relationships in visual data - Spatial Misinterpretation: Develop options that misunderstand spatial relationships or perspectives in the (cid:44) image(s) - Color Confusion: Include options that misinterpret color-coded information or subtle color differences - Pattern Misrecognition: Generate options that incorrectly identify or extend patterns in the image(s) - Detail Oversight: Create options that miss crucial details or focus on irrelevant visual elements - Scale Misjudgment: Include options that misinterpret the scale or proportions of elements in the image(s) - Cross-Image Miscomparison: When multiple images are provided, create options that incorrectly compare or (cid:44) contrast elements across images 5. Aim for diverse set of distractors that test different aspects of visual interpretation and analysis. 6. Each distractor should be based on plausible misreading of the visual information but ultimately be incorrect. 7. Consider the specific type(s) of image(s) (e.g., photograph, diagram, graph) and generate errors typical for those (cid:44) visual formats. 8. Adapt the complexity of your distractors to match the simplicity or complexity of the given question and correct (cid:44) answer. 9. If multiple images are provided, ensure some distractors address relationships or comparisons between the images. 10. For each distractor, provide maximum of three sentences explaining why it was generated. The explanation should (cid:44) describe why this distractor is plausible, the subtle flaw it contains, and how it challenges advanced (cid:44) understanding. Output format: - For each generated distractor, format your response as: Option: option: [Option text] reason: [A concise explanation (maximum 3 sentences) of why the distractor was created] - Do not add any additional commentary. Remember: - Your goal is to create challenging yet ultimately incorrect options that specifically target misinterpretations of (cid:44) visual information. - All distractors should be plausible enough to be considered by student who hasnt fully developed their visual (cid:44) literacy skills, but clear enough to be definitively incorrect upon careful visual analysis. - Focus exclusively on visual interpretation errors rather than other types of mistakes (e.g., conceptual (cid:44) misunderstandings, reasoning errors). - The distractors should directly relate to misunderstandings of the image(s) itself, not just the general topic of (cid:44) the question. - Distractors must be incorrect and should not be overly wordy or complex compared to the correct answer. - Ensure consistency in capitalization across all options, including the correct answer. For example, if the correct (cid:44) answer starts with uppercase letter, adjust all distractors to match. - When dealing with multiple images, consider how visual interpretation errors might arise from comparing or (cid:44) contrasting information across the images. - Pay attention to any visual relationships, patterns, or differences that span multiple images, and create (cid:44) distractors that plausibly misinterpret these inter-image connections. Figure 11. Detailed prompt for the proposer designed to create distractors addressing vision errors. You are an expert in creating challenging and educational multiple-choice questions, specializing in reasoning errors. Your task is to generate plausible but incorrect options (distractors) for given image-based question(s), focusing on (cid:44) flaws in logical reasoning and inference. Given: 1. One or more images 2. An open-ended question about the image(s) 3. The correct answer to the question Your task: 1. Carefully analyze and understand the provided image(s). Briefly describe the image content(s) (for your (cid:44) understanding only, do not output this). 2. Generate {num_choice} unique and plausible distractor options based on reasoning errors. Each distractor should: - Be related to the image(s) and question - Seem potentially correct at first glance - Be very misleading for students due to faulty reasoning - Contain subtle logical flaw that makes it incorrect - Vary in difficulty and the type of reasoning error it represents 3. Ensure you understand the logical steps required to correctly answer the question based on the image(s). 4. Focus on common reasoning errors, including: - Complex Reasoning Flaws: Create options that require multi-step reasoning but contain logical gaps or invalid (cid:44) assumptions - Causal Inversion: Develop options that reverse cause and effect relationships - Context Neglect: Include options that ignore important contextual information provided in the question or (cid:44) image(s) - False Analogies: Generate options that draw incorrect parallels or comparisons - Hasty Generalizations: Create options that jump to conclusions based on insufficient evidence - Cross-Image Fallacies: When multiple images are provided, create options that make invalid logical connections (cid:44) or comparisons between images 5. Aim for diverse set of distractors that test different aspects of logical reasoning and critical thinking. 6. Each distractor should follow seemingly logical path but ultimately lead to an incorrect conclusion due to (cid:44) flawed reasoning. 7. If the question involves specific subject area, consider common logical pitfalls or fallacies unique to that (cid:44) field. 8. If the question does not involve explicit reasoning, focus on creating plausible reasoning statements that could (cid:44) be mistakenly associated with the correct answer. 9. Adapt the complexity of your distractors to match the simplicity or complexity of the given question and correct (cid:44) answer. 10. If multiple images are provided, ensure some distractors address relationships or comparisons between the images, (cid:44) focusing on logical errors in interpreting these relationships. 11. For each distractor, provide maximum of three sentences explaining why it was generated. The explanation should (cid:44) describe why this distractor is plausible, the subtle flaw it contains, and how it challenges advanced (cid:44) understanding. Output format: - For each generated distractor, format your response as: Option: option: [Option text] reason: [A concise explanation (maximum 3 sentences) of why the distractor was created] - Do not add any additional commentary. Remember: - Your goal is to create challenging yet ultimately incorrect options that specifically target flaws in logical (cid:44) reasoning and inference. - All distractors should be plausible enough to be considered by student who hasnt fully developed their critical (cid:44) thinking skills, but clear enough to be definitively incorrect upon careful logical analysis. - Focus exclusively on reasoning errors rather than other types of mistakes (e.g., conceptual misunderstandings, (cid:44) visual misinterpretations). - Distractors must be incorrect and should not be overly wordy or complex compared to the correct answer. - Ensure consistency in capitalization across all options, including the correct answer. For example, if the correct (cid:44) answer starts with uppercase letter, adjust all distractors to match. - When dealing with multiple images, consider how reasoning errors might arise from comparing or contrasting (cid:44) information across the images. - Pay attention to any logical relationships, patterns, or differences that span multiple images, and create (cid:44) distractors that plausibly misinterpret these inter-image connections using faulty reasoning. Figure 12. Detailed prompt for the proposer designed to create distractors addressing reasoning errors. 22 You are an expert in creating challenging and educational multiple-choice questions, specializing in data processing errors. Your task is to generate plausible but incorrect options (distractors) for given image-based question(s), focusing on mistakes in handling quantitative information (cid:44) and data analysis. Given: 1. One or more images 2. An open-ended question about the image(s) 3. The correct answer to the question Your task: 1. Carefully analyze and understand the provided image(s), paying special attention to any numerical data, charts, graphs, or quantitative information presented. (cid:44) Briefly describe the image content(s) (for your understanding only, do not output this). 2. Generate {num_choice} unique and plausible distractor options based on data processing errors. Each distractor should: - Be directly related to mishandling of numerical or quantitative information in the image(s) - Seem potentially correct at first glance - Be very misleading for students due to data processing mistakes - Contain subtle error in calculation, interpretation, or application of quantitative information - Vary in difficulty and the type of data processing error it represents 3. Ensure you understand how the correct answer relates to the quantitative elements in the image(s). 4. Focus on common data processing errors, including: - Numerical Errors: Create options with incorrect calculations or use of wrong numerical values - Unit Conversion Mistakes: Develop options that misapply or neglect unit conversions - Statistical Misinterpretation: Include options that misunderstand statistical concepts or misapply statistical tests - Data Range Errors: Generate options that incorrectly interpret data ranges or outliers - Temporal/Sequential Errors: Create options with mistakes in the order or timing of data points or processes - Correlation/Causation Confusion: Include options that mistake correlation for causation in data relationships - Sampling Errors: Develop options that misinterpret sample sizes or sampling methods - Rounding Errors: Create options with incorrect rounding or significant figure usage 5. Aim for diverse set of distractors that test different aspects of quantitative reasoning and data analysis. 6. Each distractor should be based on plausible mishandling of the quantitative information but ultimately be incorrect. 7. Consider the specific type of data presented (e.g., discrete vs. continuous, time series, categorical) and generate errors typical for that data type. 8. If the question does not involve explicit numerical data, focus on creating plausible quantitative statements that could be mistakenly associated with the (cid:44) correct answer. 9. Adapt the complexity of your distractors to match the simplicity or complexity of the given question and correct answer. 10. If multiple images are provided, ensure that your distractors consider the relationships and comparisons between the images when relevant. 11. When generating numerical distractors: - Carefully analyze the structure and precision of the correct answer - Create distractors that closely mimic the format, precision, and magnitude of the correct answer - Use mix of common calculation errors, transposition mistakes, and misinterpretations to generate deceptive options - For answers with specific formats (e.g., currency with cents, percentages, or large numbers with commas), maintain this format in the distractors - Include options that could result from typical mental math errors or misreading of data - If the correct answer has trailing zeros (e.g., 123,000), some distractors should also have trailing zeros to maintain consistency - For precise answers (e.g., $493.02), create distractors with same precision (e.g., $439.20, $493.20, $492.03) to increase difficulty while maintaining (cid:44) consistency in decimal places 12. Ensure high deceptiveness in your distractors: - Create options that could result from common misinterpretations of the data or question - Include distractors that swap digits, misplace decimal points, or make sign errors (e.g., positive instead of negative) - Generate options that could result from using the wrong operation (e.g., addition instead of subtraction) - For multi-step calculations, include results that would occur if step was omitted or performed incorrectly - Consider psychological factors that might lead to specific errors, such as anchoring bias or confirmation bias 13. For each distractor, provide maximum of three sentences explaining why it was generated. The explanation should describe why this distractor is plausible, the (cid:44) subtle flaw it contains, and how it challenges advanced understanding. Output format: - For each generated distractor, format your response as: Option: option: [Option text] reason: [A concise explanation (maximum 3 sentences) of why the distractor was created] - Do not add any additional commentary. Remember: - Your goal is to create challenging yet ultimately incorrect options that specifically target errors in handling and interpreting quantitative information. - All distractors should be plausible enough to be considered by student who hasnt fully developed their quantitative reasoning skills, but clear enough to be (cid:44) definitively incorrect upon careful analysis. - Focus exclusively on data processing errors rather than other types of mistakes (e.g., conceptual misunderstandings, visual misinterpretations). - The distractors should directly relate to mishandling of the quantitative information in the image(s), not just the general topic of the question. - Ensure that the errors are subtle enough to be challenging but still clearly incorrect when carefully examined. - Distractors must be incorrect and should not be overly wordy or complex compared to the correct answer. - Ensure consistency in capitalization across all options, including the correct answer. For example, if the correct answer starts with uppercase letter, adjust (cid:44) all distractors to match. - When dealing with multiple images, consider how data processing errors might arise from comparing or contrasting information across the images. - Pay attention to any relationships, trends, or patterns that span multiple images, and create distractors that plausibly misinterpret these inter-image (cid:44) connections. Figure 13. Detailed prompt for the proposer designed to create distractors addressing data processing errors. 23 You are an expert in creating challenging and educational multiple-choice questions, specializing in conceptual (cid:44) errors. Your task is to generate plausible but incorrect options (distractors) for given image-based question(s), focusing on (cid:44) conceptual misunderstandings and misconceptions. Given: 1. One or more images 2. An open-ended question about the image(s) 3. The correct answer to the question Your task: 1. Carefully analyze and understand the provided image(s). Briefly describe the image content(s) (for your (cid:44) understanding only, do not output this). 2. Generate {num_choice} unique and plausible distractor options based on conceptual errors. Each distractor should: - Be related to the image(s) and question - Seem potentially correct at first glance - Be very misleading for students due to conceptual misunderstandings - Contain subtle flaw or misconception that makes it incorrect - Vary in difficulty and the type of conceptual error it represents 3. Ensure you understand the connection between the image(s), question, and the underlying concepts. 4. Focus on common conceptual misconceptions in the subject area, including: - Concept Confusion: Create options that are similar to the correct concept but with subtle differences - Partial Correctness: Include options that contain partially correct information but are incomplete or misleading - Overgeneralization: Develop options that incorrectly apply specific cases to general situations - Cross-Image Misconceptions: When multiple images are provided, create options that misapply concepts across (cid:44) different images 5. Aim for diverse set of distractors that test different aspects of conceptual understanding. 6. Each distractor should have some relation to the correct answer, but ensure they are distinctly different and (cid:44) incorrect due to conceptual misunderstandings. 7. If the question involves specific subject area, consider common conceptual difficulties unique to that field. 8. Adapt the complexity of your distractors to match the simplicity or complexity of the given question and correct (cid:44) answer. 9. If multiple images are provided, ensure some distractors address relationships or comparisons between the images, (cid:44) focusing on conceptual errors in interpreting these relationships. 10. For each distractor, provide maximum of three sentences explaining why it was generated. The explanation should (cid:44) describe why this distractor is plausible, the subtle flaw it contains, and how it challenges advanced (cid:44) understanding. Output format: - For each generated distractor, format your response as: Option: option: [Option text] reason: [A concise explanation (maximum 3 sentences) of why the distractor was created] - Do not add any additional commentary. Remember: - Your goal is to create challenging yet ultimately incorrect options that specifically target conceptual (cid:44) misunderstandings. - All distractors should be plausible enough to be considered by student who doesnt fully grasp the concept, but (cid:44) clear enough to be definitively incorrect upon careful consideration. - Focus exclusively on conceptual errors rather than other types of mistakes (e.g., calculation errors, visual (cid:44) misinterpretations). - Distractors must be incorrect and should not be overly wordy or complex compared to the correct answer. - Ensure consistency in capitalization across all options, including the correct answer. For example, if the correct (cid:44) answer starts with uppercase letter, adjust all distractors to match. - When dealing with multiple images, consider how conceptual errors might arise from comparing or contrasting (cid:44) information across the images. - Pay attention to any conceptual relationships, patterns, or differences that span multiple images, and create (cid:44) distractors that plausibly misinterpret these inter-image connections due to conceptual misunderstandings. Figure 14. Detailed prompt for the proposer designed to create distractors addressing concept errors. 24 You are an expert in creating extremely challenging multiple-choice questions, specializing in highly sophisticated (cid:44) question-focused distractors. Your task is to generate plausible but incorrect options (distractors) for given questions, focusing on creating the (cid:44) most difficult and deceptive answers based on the question text. Given: 1. An open-ended question 2. The correct answer to the question Your task: 1. Generate {num_choice} unique and highly challenging distractor options. Each distractor should: - Be closely related to the question text - Seem very plausible and potentially correct even upon careful consideration - Be extremely misleading, requiring deep understanding to recognize as incorrect - Contain subtle, sophisticated flaws that make them incorrect - Represent the highest level of difficulty and complexity 2. Focus on creating distractors that: - Leverage advanced knowledge or nuanced interpretations of the subject matter - Provide logically sound but ultimately incorrect answers based on the question - Exploit common high-level misconceptions or advanced misinterpretations - Offer highly plausible alternatives that might be true in many situations but are incorrect in this specific context 3. Aim for diverse set of sophisticated distractors that challenge different aspects of advanced understanding and (cid:44) critical thinking. 4. Each distractor should be intricately related to the question topic and the correct answer, but with crucial (cid:44) differences that make them incorrect. 5. If the question involves specific subject area, incorporate advanced concepts and potential misunderstandings at (cid:44) an expert level. 6. For each distractor, provide maximum of three sentences explaining why it was generated. The explanation should (cid:44) describe why this distractor is plausible, the subtle flaw it contains, and how it challenges advanced (cid:44) understanding. Output format: - For each generated distractor, format your response as: Option: option: [Option text] reason: [A concise explanation (maximum 3 sentences) of why the distractor was created] - Do not add any additional commentary. Remember: - Create only the most challenging and deceptive options possible. - All distractors should be sophisticated enough to give even knowledgeable individuals pause. - Focus on creating answers that require deep analysis and expert knowledge to discern as incorrect. - Ensure distractors are incorrect but highly plausible and closely related to the correct answer. - Maintain consistency in style, complexity, and structure across all options, matching the correct answers (cid:44) sophistication. - Distractors must be incorrect and should not be overly wordy or complex compared to the correct answer. Figure 15. Detailed prompt for the proposer designed to create distractors addressing question bias errors. 25 Task: Analyze and enhance the provided distractors, which were generated based on {type} error type, to maximize (cid:44) their difficulty and deceptiveness while ensuring they remain incorrect. Given: 1. One or more images 2. question about the image(s) 3. The correct answer 4. set of distractor options for specific error type (e.g., reasoning error, question bias, etc.) 5. The reasoning provided for why each distractor was created For each distractor, your task is to: 1. Evaluate the distractors effectiveness in challenging students understanding while remaining incorrect. 2. Assess how well the distractor aligns with the {type} error and the given image(s) context. 3. Determine if the distractor could be interpreted as the correct answer. If so, add suggestions towards this. 4. If the distractor is effective and challenging, state that it should be retained. 5. If improvements are needed, provide specific suggestions to increase the distractors difficulty and deceptiveness (cid:44) without: a. Increasing the options length or adding unnecessary modifiers b. Making the distractor correct 6. Ensure your evaluation and suggestions are concise, not exceeding four sentences. Guidelines: - Prioritize the distractors conceptual difficulty over linguistic complexity. - If distractor is correct or could be interpreted as correct, clearly state this and suggest how to modify it to (cid:44) make it unambiguously incorrect. - Focus on enhancing the distractors plausibility within the context of the {type} error and the image(s). - Suggest refinements that make the distractor more tempting without compromising its fundamental incorrectness. - Ensure all suggestions maintain clear distinction between the distractor and the correct answer. For each option, format your response as: Option: option: [Option text] comment: [Your evaluation and specific suggestions, if needed, or confirmation of effectiveness] Figure 16. Detailed prompt for the reviewer, whose feedback iteratively refines the distractors to improve their quality. 26 You are an expert Selection Agent tasked with curating the most challenging and high-quality distractor options for (cid:44) multiple-choice questions based on one or more provided images. Your goal is to select the best {fusion_selected_choice_num} unique distractors from pool of multiple distractors, (cid:44) ensuring diverse, non-repetitive, and challenging set of options that are relevant to the given image(s). Given: - One or more images related to the question - dictionary containing multiple distractor options, organized into five categories: 1. Concept Error ({num_choice} options) 2. Reasoning Error ({num_choice} options) 3. Visual Interpretation Error ({num_choice} options) 4. Data Processing Error ({num_choice} options) 5. Question Bias ({num_choice} options) - Each distractor is accompanied by reason explaining why it was generated. Your task: 1. Carefully review all distractor options in the context of the provided image(s). 2. Select the top {fusion_selected_choice_num} distractors based on the following criteria: - Image relevance: Prioritize distractors that are closely related to the content, context, or details present in the (cid:44) given image(s). - Difficulty: Prioritize options that are more challenging and require deeper understanding to discern their (cid:44) incorrectness. - Quality: Choose options that are well-crafted, plausible, and closely related to the correct answer. - Diversity: Ensure balanced representation of different error types and subtypes. - Subtlety: Prefer distractors with subtle errors that require careful analysis to detect. - Educational value: Select options that, when revealed as incorrect, provide valuable insights into the topic. - Uniqueness: Ensure that each selected distractor is distinct from others in meaning and approach, avoiding (cid:44) repetition or highly similar concepts. - Reason-based selection: Carefully consider the provided reason for each distractors creation. Prioritize (cid:44) distractors whose reasoning aligns well with the image context, question intent, or presents strong (cid:44) challenge for test-takers. Use the quality of these reasons to guide your selection process. 3. Ensure diverse representation across the different error types, with the following guidelines: - You may select more distractors from categories that are particularly relevant to the image(s) and question. - The total number of selected distractors should be {fusion_selected_choice_num}. 4. You should never change selected distractors and never include the correct answer among your selected distractors. Output format: - Provide list of {fusion_selected_choice_num} distractor options based on your careful selection. - For each selected distractor, format your response as: Option: option: [Option text] reason: [A concise explanation (maximum 3 sentences) of why the distractor was selected] - Do not add any additional commentary. Remember: - Your primary goal is to create challenging yet educational set of distractors that will effectively test (cid:44) students understanding of the subject matter in relation to the provided image(s). - If the given correct answer is list, ensure that none of the selected distractors are included in the correct (cid:44) answer. - Ensure that the selected distractors work well together as set, offering range of challenges and testing (cid:44) different aspects of the topic. - Consider how each distractor might interact with the others and with the correct answer to create cohesive and (cid:44) challenging question. - Distractors must be incorrect and should not be overly wordy or complex compared to the correct answer. - Ensure consistency in capitalization across all options, including the correct answer. If the correct answer begins (cid:44) with uppercase letter, adjust all distractors to match. - Pay special attention to visual elements, objects, or text present in the image(s) when selecting distractors. (cid:44) Incorporate these image-based elements into your selections when relevant. - If multiple images are provided, ensure that the selected distractors are relevant across all images or (cid:44) specifically address the relationships between the images. - Avoid selecting distractors that are too similar to each other or convey the same idea in different words. Figure 17. Detailed prompt for the selector, which guides the selection of the three most challenging distractors to enhance question difficulty. 27 Your task is to evaluate multiple-choice question (with accompanying image) to determine if any incorrect choices (cid:44) (distractors) could also be considered correct answers. CRITICAL: The marked correct answer MUST always be treated as valid and correct, regardless of your own assessment. (cid:44) Never question or evaluate the correct answer - your task is to accept it as an absolute truth and evaluate (cid:44) only whether other choices could also be correct. Score the questions correctness using this scale: 5 - Perfect: All other choices are clearly incorrect 4 - Good: Other choices are mostly wrong but have minor elements of correctness 3 - Fair: At least one other choice could be partially correct 2 - Poor: At least one other choice could be equally correct 1 - Invalid: Multiple choices are equally valid as the correct answer Provide: 1. Score (1-5) 2. Brief explanation focusing specifically on any problematic distractor choices 3. Suggested improvements for the problematic distractors (if applicable) Remember: Never analyze whether the marked correct answer is right or wrong - it is ALWAYS correct by definition. (cid:44) Focus exclusively on whether other choices could also be valid answers. Figure 18. Detailed prompt for the evaluator, which evaluates the correctness of the generated questions, defined as there is only one correct answer. You are an expert in educational assessment design specializing in multiple-choice question improvement. Your task is (cid:44) to enhance question effectiveness by revising problematic distractors (incorrect answer choices) while (cid:44) maintaining the existing correct answer. Input Required: 1. The complete question 2. The current correct answer 3. Any associated images/materials 4. Specific feedback about problematic distractors 5. Suggested improvements (if provided) Analysis Steps: 1. Review the question content and learning objective 2. Analyze the designated correct answer 3. Examine the feedback regarding problematic distractors 4. Evaluate any provided suggestions for improvement: - Assess if suggestions fully address the identified issues - Determine if suggestions align with best practices - Identify any gaps or weaknesses in the suggestions 5. Develop exactly 3 improved distractors that: - Are plausible but clearly incorrect - Address the identified issues - Align with common student misconceptions - Maintain consistent format and length with other options - Go beyond provided suggestions when necessary for better quality Guidelines: 1. Treat the marked correct answer as fixed and unchangeable 2. Only modify distractors specifically identified as problematic 3. Preserve any well-functioning distractors 4. Maintain the original difficulty level of the question 5. Use your expertise to improve upon or deviate from provided suggestions if they: - Are too vague or incomplete - Dont fully address the identified issues - Could be enhanced for better assessment quality - Miss important misconceptions or learning opportunities Output: 1. Brief analysis of the distractor issues and improvement approach 2. Three improved distractors Figure 19. Detailed prompt for the refiner, which ensures the correctness of the generated questions, guaranteeing that there is only one correct answer. 28 Source Image Question Choices A-OKVQA What season of the year is shown here? A. late summer with green leaves B. early spring with blooming flowers C. fall D. early winter with snow A-OKVQA A-OKVQA AI2D AI2D AI2D ChartQA ChartQA ChartQA What occasion are the bears probably sitting at the table enjoying? What kind of beverage is the red sign advertising? Which shows the first stage? What part of plants the diagram depicts? A. Thanksgiving B. Easter C. New Years Eve D. Christmas A. Dr Pepper B. Coca Cola C. Red Bull D. Pepsi A. B. C. D. A. Leaf B. Stem C. Root D. Flower petal Which is the exterior portion of the earth? A. B. C. D. What percentage of respondents own lots of vinyl records? A. 35 B. 24 C. 30 D. 28 In which year is the ACSI score is lowest? A. 2015 B. 2017 C. 2018 D. What is the total ratio of 2014 through 2017? A. 5.36 B. 5.11 C. 4.57 D. 5.25 Table 5. Examples of VMCBench (1/7). Each example has dataset source, image, question, and four choices (correct choice highlighted in orange). 29 Source Image Question DocVQA What is the table number? DocVQA In the plot, what is the value of r? DocVQA Which category items advertisement is this? GQA GQA GQA InfoVQA InfoVQA InfoVQA What food is to the left of the table? Who is looking at the cell phone? What does the woman hold? Which states/UT has been included under Certain risk of community transmission? What percentage of Canadian still go to brick and mortar stores to buy items? How many women out of every 4 women are domestic violence survivors? Choices A. 7 B. 8 C. 10 D. 9 A. 0.994 B. 0.949 C. 1.004 D. 0.980 A. health supplements B. foods C. home appliances D. clothing A. can of soup B. bag of flour C. cereal box D. dog food A. man B. tourist C. child D. nobody A. cell phone B. digital camera C. remote control D. compact mirror A. manipur, tamil nadu B. maharashtra, gujarat C. rajasthan, karnataka D. telangana, delhi A. 30% B. 70% C. 80% D. 60% A. 4 B. 3 C. 1.5 D. 2 Table 6. Examples of VMCBench (2/7). Each example has dataset source, image, question, and four choices (correct choice highlighted in orange). 30 Source MMMU MMMU MMMU MMStar Image Question The average wave velocity value of the bored pile body at certain site is 3555.6m/s, and the low strain reflected wave dynamic test curve of certain column is shown in Figure 10-3, corresponding to the values of time t1, t2, and t3 in the figure, which of the following options is the closest value to the length of the pile? How many molecules of the sweetener saccharin can be prepared from 30 atoms, 25 atoms, 12 atoms, 8 atoms, and 14 atoms? The accompanying sketch shows the schematic arrangement for measuring the thermal conductivity by the guarded hot plate method. Two similar 1 cm thick specimens receive heat from 6.5 cm by 6.5 cm guard heater. When the power dissipation by the wattmeter was 15 W, the thermocouples inserted at the hot and cold surfaces indicated temperatures as 325 and 300 K. What is the thermal conductivity of the test specimen material? What color is the ribbon that the man on the right is holding? Choices A. 22.0m B. 24.0m C. 26.0m D. 23.0m A. 6 B. 4 C. 2 D. 5 A. 0.86 W/m B. 0.5 W/m C. 0.68 W/m D. 0.71 W/m A. Red B. Blue C. Yellow D. Green MMStar What is the main feature of the building in the image? MMStar who is this person? A. The colorful facade B. The large stained glass windows C. The marble columns D. The stone wall A. Awkwafina B. Sandra Oh C. Ali Wong D. Lucy Liu MMVet What is the original price for pork belly before discount? A. 10 B. 12 C. 14 D. 15 MMVet What is the answer to the second equation on the right? MMVet What occasions would someone use this meme? A. 11 B. 5 C. 7 D. 9 A. Sharing relatable humor about feeling sleepy or having conflicting desires, especially during the day. B. Expressing excitement about new bedtime routine C. Celebrating an all-nighter successfully pulled off D. Promoting productivity in the workplace Table 7. Examples of VMCBench (3/7). Each example has dataset source, image, question, and four choices (correct choice highlighted in orange). 31 Source Image Question Choices MathVision MathVision MathVision MathVista MathVista MathVista In the grid, how many grey squares have to be coloured white, so that in each row and each column there is exactly one grey square? Tom, John and Lily each shot six arrows at target. Arrows hitting anywhere within the same ring scored the same number of points. Tom scored 46 points and John scored 34 points, as shown. How many points did Lily score? point is chosen in the interior of ABC so that when lines are drawn through parallel to the sides of ABC, the resulting smaller triangles, t1, t2, and t3 in the figure, have areas 4, 9, and 49, respectively. Find the area of ABC. In the figure, 9 = 75. Find the measure of 6. (Original in Chinese) As shown in the figure, in ABC, AD is the angle bisector, and AE is the altitude. If = 40 and = 70, then the measure of EAD is (). What is the green curve? OCRVQA Who wrote this book? OCRVQA What is the title of this book? OCRVQA What is the title of this book? A. 5 B. 8 C. 6 D. 7 A. 42 B. 40 C. 44 D. 38 A. 81 B. 128 C. 144 D. 72 A. 120 B. 135 C. 150 D. A. 25 B. 20 C. 30 D. 15 A. cubic function B. trigonometric function C. an exponential function D. logarithmic function A. John D. Smith B. Ruth E. McCall BS MT(ASCP) C. Michael A. Johnson D. Emily J. Brown A. Climate Change and Urban Adaptation Strategies B. Building Sustainable Future: Climate Change Adaptation C. Adapting Urban Spaces: Sustainability in the 21st Century D. Adapting Buildings and Cities for Climate Change A. Cracking the PSAT/NMSQT, 2013 Edition (College Test Preparation) B. Cracking the SAT, 2013 Edition (College Test Preparation) C. Crushing the PSAT/NMSQT, 2013 Edition D. Cracking the PSAT, 2014 Edition (College Test Preparation) Table 8. Examples of VMCBench (4/7). Each example has dataset source, image, question, and four choices (correct choice highlighted in orange). Source Image Question OKVQA What food is being sold? Choices A. hamburger B. sandwich C. hot dog D. kebab OKVQA What is the proper response when traveling in vehicle and seeing red traffic light? A. proceed with caution B. speed up to clear the intersection C. stop D. yield only if necessary OKVQA How long does this animal usually live? RealWorldQA How many oncoming vehicles are there? A. 25 years B. 10 years C. 20 years D. 8 years A. 4 B. 3 C. 1 D. 2 RealWorldQA Which way does this door open? RealWorldQA Which object is bigger than the other? SEEDBench What can be found in the image? SEEDBench Which object is emitting smoke in the image? SEEDBench What is the boy doing in the image? A. The door opens outward, swinging to the right. B. The door is sliding door, moving to the right. C. The door opens inward, swinging to the right. D. The door opens outward, swinging to the left. A. Both objects are the same size. B. The right object is bigger. C. The left object has more volume. D. The left object is bigger. A. group of people sitting down and young boy with basketball player. B. basketball player signing autographs for line of fans. C. group of musicians playing instruments D. Several students in classroom setting A. Factory smokestack in the background B. House chimney C. Chimney of nearby house D. Train A. Jumping B. Smiling C. Reading book D. Waving Table 9. Examples of VMCBench (5/7). Each example has dataset source, image, question, and four choices (correct choice highlighted in orange). 33 Source Image Question ScienceQA What is the name of the colony shown? ScienceQA Which continent is highlighted? Choices A. Rhode Island B. Connecticut C. New Hampshire D. Massachusetts A. Australia B. Arctic C. South America D. Antarctica What can Turner and Mona trade to each get what they want? A. Turner can trade his tomatoes for Monas broccoli. B. Turner can trade his oranges for Monas water. C. Turner can trade his water for Monas almonds. D. Turner can trade his sandwich for Monas hot dog. ScienceQA TableVQABench TableVQABench TableVQABench What is the total amount of Purchase Obligations due after 2021? what other name did asian cougar have? how many metals did netherlands and the us win together? TextVQA what number is shown? TextVQA TextVQA what male name is written on the white book? which company is giving the presentation? A. $6.5 B. $80.7 C. $0.0 D. $214.9 A. Gamma B. Kooga C. Kuuga D. Black Buffalo A. 18 B. 20 C. 22 D. A. 21 B. 25 C. 22 D. 20 A. mike B. sean C. dave D. steve A. ibm B. sap C. google D. oracle Table 10. Examples of VMCBench (6/7). Each example has dataset source, image, question, and four choices (correct choice highlighted in orange). 34 Source VQAv2 Image Question Choices How many buttons on the mans shirt? VQAv How many men are in the picture? VQAv2 What type of shoes is the man wearing? VizWiz What is this picture of? VizWiz What color is this Starburst? VizWiz What is this? Its from Schwans. A. 5 B. 2 C. 3 D. 1 A. 2 B. 1 C. 0 D. 3 A. tennis shoes B. dress shoes C. hiking boots D. sandals A. squirrel B. wolf C. hamster D. dog A. yellow B. blue C. green D. red A. hawaiian style pizza B. cheese pizza C. meat lovers pizza D. vegetable pizza Table 11. Examples of VMCBench (7/7). Each example has dataset source, image, question, and four choices (correct choice highlighted in orange)."
        }
    ],
    "affiliations": [
        "MIT",
        "Stanford University",
        "Tsinghua University"
    ]
}