{
    "paper_title": "Subject-driven Video Generation via Disentangled Identity and Motion",
    "authors": [
        "Daneul Kim",
        "Jingxu Zhang",
        "Wonjoon Jin",
        "Sunghyun Cho",
        "Qi Dai",
        "Jaesik Park",
        "Chong Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, which are computationally expensive and require extensive annotation. In contrast to the previous approach, we introduce the use of an image customization dataset directly on training video customization models, factorizing the video customization into two folds: (1) identity injection through image customization dataset and (2) temporal modeling preservation with a small set of unannotated videos through the image-to-video training method. Additionally, we employ random image token dropping with randomized image initialization during image-to-video fine-tuning to mitigate the copy-and-paste issue. To further enhance learning, we introduce stochastic switching during joint optimization of subject-specific and temporal features, mitigating catastrophic forgetting. Our method achieves strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings, demonstrating the effectiveness of our framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 6 1 8 7 1 . 4 0 5 2 : r Subject-driven Video Generation via Disentangled Identity and Motion Daneul Kim1,2, Jingxu Zhang2 Wonjoon Jin3 Sunghyun Cho3 Qi Dai2 Jaesik Park1 Chong Luo2 1Seoul National University 2Microsoft Research Asia 3POSTECH carpedkm@snu.ac.kr v-jingxuz@microsoft.com {jinwj1996, sodomau}@postech.ac.kr qid@microsoft.com jaesik.park@snu.ac.kr cluo@microsoft.com Figure 1. Result on video customization with our proposed decomposed subject-driven generation, demonstrating high-quality results across various scenarios."
        },
        {
            "title": "Abstract",
            "content": "We propose to train subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. traditional method for video customization that is tuning-free often relies on large, annotated video datasets, which are computationally expensive and require extensive annotation. In contrast to the previous approach, we introduce the use of an image customization dataset directly on training video customization models, factorizing the video customization into two folds: (1) identity injection through image customization dataset and (2) temporal modeling preservation with small set of unannotated videos Work done while at Microsoft Research Asia. through the image-to-video training method. Additionally, we employ random image token dropping with randomized image initialization during image-to-video fine-tuning to mitigate the copy-and-paste issue. To further enhance learning, we introduce stochastic switching during joint optimization of subject-specific and temporal features, mitigating catastrophic forgetting. Our method achieves strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings, demonstrating the effectiveness of our framework. 1. Introduction Recent advancements in video diffusion models [6, 17, 44] have significantly improved controllability by incorporating various conditioning mechanisms, ranging from text-to-video (T2V) synthesis to video customization us1 ing key points, edges, or reference images [3, 19, 30, 43, 46]. Among them, subject-driven video customization, i.e. subject-to-video (S2V) generation [20, 22, 40, 41], aims to generate videos that maintain consistent subject identity across different scenes, motions, and contexts. S2V generation has gained significant attention for its wide range of applications, including personalized content creation, marketing, and entertainment. However, early approaches [9, 33, 40, 43] typically require per-subject optimization, which restricts their applicability due to the additional optimization time. To eliminate the need for per-subject optimization, recent studies [11, 20, 22, 41] have developed zero-shot S2V methods based on training with subject-driven video customization datasets (S2V datasets). These datasets typically contain tuples of subject image, corresponding text prompt, and video clip. However, acquiring sufficiently large S2V dataset for training remains major challenge. Early studies [22] rely on relatively small-scale S2V datasets, leading to lack of generalization capabilities and identity preservation. For better generalization capability, recent studies have attempted to gather large-scale S2V datasets [11, 20, 46]. However, obtaining such large-scale dataset requires tremendous human labor and resources. Furthermore, no large-scale public S2V datasets are currently available, which restricts further progress in the field of S2V generation."
        },
        {
            "title": "This paper proposes an approach that",
            "content": "leverages subject-driven image customization dataset (S2I dataset) for zero-shot S2V training. Our approach provides several key advantages. First, by using an S2I dataset [37], we remove the need for expensive and hard-to-obtain S2V datasets. Second, because S2I datasets are readily available at large scales, we utilize such datasets to achieve better generalization and improved subject identity preservation. In addition, our approach reduces computational resources by primarily utilizing images instead of videos for training. To train our model with an S2I dataset, our key idea is to decompose S2V training into two components: identity injection and temporal awareness preservation. Specifically, the identity injection is achieved by fine-tuning pretrained video synthesis model on an S2I dataset [37], allowing the model to learn subject-specific features without requiring video customization datasets. However, solely relying on an S2I dataset during fine-tuning might not preserve the pre-trained temporal awareness of video synthesis model. Therefore, to preserve the temporal awareness, we introduce an Image-to-Video (I2V) fine-tuning strategy that utilizes small-scale non-annotated video dataset [1]. This strategy guides the model to learn video synthesis from single frame of video instead of subject image. While straightforward approach might be sequential training for identity injection and temporal awareness preservation, it often suffers from catastrophic forgetting, where the model forgets previously learned subject identity during temporal awareness training. To overcome this issue, we introduce stochastically-switched fine-tuning that balances identity injection and temporal awareness preservation. Specifically, our method alternates between identityinjection and temporal-awareness-preservation objectives within single optimization framework. This approach effectively prevents catastrophic forgetting and reduces the risk of overfitting to either modality. To further promote temporal awareness for S2V generation, we adopt two simple yet effective techniques during the I2V fine-tuning. We first employ random-frame selection, which utilizes randomly selected frame as reference image during I2V fine-tuning, rather than the first frame of the video. Second, we introduce random image-token dropping. Inspired by dropout regularization [35] and classifierfree guidance [16], this method randomly removes image tokens during training, guiding the model to rely more on temporal dependency during video generation rather than the reference image. These techniques effectively reduce severe overfitting to the first frame, which causes the model to replicate the first frame instead of preserving the subjects identity, i.e. the copy-and-paste phenomenon. The key contributions of our work are as follows: We propose training approach for subject-driven video generation using an image customization dataset, eliminating the need for hard-to-obtain video customization datasets. To this end, our approach decomposes the training of S2V generation into identity injection and temporal awareness preservation. We introduce stochastically-switched fine-tuning, which alternately performs identity injection and temporal awareness preservation to mitigate catastrophic forgetting and effectively preserve temporal awareness. Extensive evaluations demonstrate the effectiveness of our method for zero-shot subject-driven video generation, despite not using video customization datasets. 2. Related Work Subject-Driven Image Generation. Recent advances in diffusion models [10, 13, 25] have greatly expanded the capabilities of text-to-image synthesis, image-to-image translation, and image editing [29, 48], enabling flexible, highfidelity content generation. prominent challenge in this progress is injecting novel subjects into generative models while maintaining accurate subject identity across diverse prompts. Early works, such as ControlNet [48] and T2I-Adapter [14], employed spatially aligned conditioning signals fed into the denoising process. While effective for well-aligned references, these methods are challenged when the subjects pose or perspective deviates from those in the reference image. To address this, approaches like IPAdapter [45] and SSR-Encoder [49] utilize cross-attention mechanisms to more robustly integrate features from reference images, ensuring improved subject fidelity even in less predictable spatial scenarios. Recent advances like DreamBooth [33] and Textual Inversion [15] focus on specialized embeddings or lightweight updates for subject-specific attributes. Further works include reinforcement learning-based [31], tuningfree [12, 37, 47], multi-subject [24, 28], and subjectagnostic [7] approaches. However, disentangling subject identity from background and stylistic elements remains challenging. Disentangled fine-tuning and adapter-based approaches [9] mitigate overfitting and reduce computational costs, crucial for scaling to video generation. These image-level methods lay the groundwork for video-based approaches, where temporal coherence adds complexity to maintaining subject consistency across frames. Subject-Driven Video Generation. Building on the successes of subject-driven image customization, researchers have extended personalized generation techniques to video, while striving to preserve temporal coherence. Traditional video generation methods typically involve training or finetuning models by annotating on large video datasets [1, 2, 5]. This approach often leads to high labor costs with large expanses, as the model was required to learn both appearance and motion representations simultaneously. More recent efforts, however, have explored image-driven strategies for video generation, such as Still-Moving [8] and EMU-Video-Edit [34], which utilize pre-trained text-toimage customization techniques to guide video generation while preserving subject fidelity across frames. Recent video generation techniques can be diinvolves vided into two major approaches. The first lightweight test-time optimization for each sample, such as DreamVideo [40] and MotionBooth [42], decomposing the video generation process into separate modules for appearance and motion. This separation enables the preservation of realistic motion dynamics while allowing for minimal subject-specific updates to inject identity. The second approach targets zero-shot or tuning-free solutions, including Consis-ID [46], which integrates identity features through frequency-based into video diffusion model decomposition without and optimization, Concept-Master [20], which learns separate embeddings to handle multi-subject scenarios and to avoid identity mixing. Methods like VideoBooth [22] and MagicMirror [50] further explore reference-image-based inference to produce personalized videos without fine-tuning. additional Our work aligns with the zero-shot tuning-free solutions. However, unlike traditional methods that require extensive video-specific training [11, 20, 22], our approach mitigates the need for large-scale annotated video dataset 3 for subject-driven video (S2V) generation. Similar to StillMoving [8] and Emu-video-edit [34], we aim to solve video-specific tasks without video-annotated data, enhancing scalability and efficiency, and making personalized video generation more accessible and computationally feasible. One easy way is to adapt the method of StillMoving [8], but as Still-Moving requires plugging in of T2I weights, it does not apply to models like multi-modal diffusion transformers, where spatial and temporal modeling are not disentangled and require multi-stage training, making the process complicated. Therefore, we propose to factorize the S2V synthesis into ID injection and temporal awareness preservation by utilizing stochastically-switched training on factorized components. We maintain identity preservation and temporal coherence, achieving high-fidelity, scalable, and computationally efficient video customization. 3. Method 3.1. Preliminary Our framework builds upon the Multi-Modal Diffusion Transformer (MM-DiT) [32], employed in architectures such as FLUX.1 [25], Stable Diffusion 3 [13] or CogVideo [17, 44] and Wan 2.1 [39] in Videos. DiT adopts Transformer-based denoising network that iteratively refines noisy image tokens through multi-modal attention. At each denoising step, DiT processes noisy visual tokens (i.e., image or video tokens) RN and text tokens CT RM d, sharing the same embedding dimension d. These tokens maintain consistent shapes throughout Transformer blocks, simplifying multi-modal integration. Each DiT block consists of Layer Normalization (LN) followed by Multi-Modal Attention (MMA), which fuses visual and textual modalities. Spatial positions are encoded using Rotary Position Embedding (RoPE) [36] as: Xi,j Xi,j R(i, j), (1) where R(i, j) is rotation matrix based on spatial coordinates (i, j). MMA computes attention between tokens as: MMA(cid:0)[X; CT ](cid:1) = softmax (cid:19) (cid:18) QK V, (2) where [X; CT ] denotes concatenated image and text tokens. 3.2. Factorization We build upon pretrained MM-DiT video synthesis model (CogVideoX [44]), originally designed for T2V synthesis. To adapt this T2V model to subject-driven video customization, we factorize the video customization training into two key objectives: identity injection and temporal awareness preservation. For identity injection, we utilize an S2I dataset [37] comprising image pairs depicting the same subject in various poses and contexts. This enables the model Figure 2. Overview of our framework. We factorize the subject-driven video customization (S2V) into temporal-awareness preservation and ID injection (Left). To optimize the two objectives, we utilize stochastically-switched finetuning, randomly switching between two training objectives (Right). to robustly capture subject-specific features without relying on costly annotated video data. To preserve temporal awareness, we leverage an unpaired T2V dataset to preserve the backbones inherent capability to model temporal dynamics. By training with general text-video pairs, we ensure that the model maintains video synthesis capabilities to synthesize temporally coherent videos with dynamic motions. Through this factorization illustrated in Figure 2, we aim to achieve subject-driven video customization without customized video data, i.e., subject and video pairs, utilizing only the unpaired videos to keep the temporal awareness intact. 3.3. ID Injection To inject subject identity, we adopt an image customization (S2I) approach [37] for subject-driven customization. At each training step, we have source image Iinput RHW 3 and target frame Ioutput RHW 3, along with text prompt . Following the notation in Section 3.1, we encode these inputs as visual tokens Xin, Xout RN and text tokens CT RM d: Xin = VAE(Iinput), Xout = VAE(Ioutput), CT = T5(P ), (3) where is the token embedding dimension, is the number of spatial tokens, and is the number of text tokens."
        },
        {
            "title": "These",
            "content": "tokens, Xin, Xout, pass through MM-DiT, which applies Low-Rank Adaptation (LoRA) [18]. We train the LoRA parameters by and CT , then injecting subject-specific information without altering the models handling of Xout or CT . Specifically, Layer Normalizations (LN) linear layer is updated only when Xin is passed through, frozen for other inputs. (i.e., Xout, CT ) This ensures that identity-driven updates do not interfere with temporal or textual representations. Afterwards, the concatenated tokens [Xin; CT ; Xout] are passed into the 3D Full Attention, injecting the subject identity across the Xout following the context of CT. Crucially, our method requires only single source frame Iinput and Ioutput to capture identity, which reduces the number of tokens and speeds up training in 3D Full Attention. Maintaining Identity with Reference Token. We introduce special <CLS> token that helps map the text tokens CT and input tokens Xin to the output tokens Xout more explicitly. During fine-tuning, we prepend <CLS> to the text prompt; for instance, An armchair is in the living room becomes An <CLS> arm chair is in the living room. 3.4. Temporal Awareness Preservation While fine-tuning an S2I dataset injects strong identity information, it may cause the model to lose its pre-trained temporal awareness. Consequently, the model may generate frames with limited motion or temporal coherence. We introduce an I2V fine-tuning strategy to address this issue on small unpaired video dataset. Specifically, I2V fine-tuning employs single reference image to guide the generation of subsequent video frames, bridging the gap between identity 4 injection and temporal motion modeling. Unlike fully annotated S2V dataset, which is difficult to acquire at scale, this unpaired video corpus merely provides text captions, avoiding the need for large-scale subject-specific video annotations. I2V Fine-Tuning vs. T2V Fine-Tuning. In principle, one could preserve temporal information by additional finetuning on T2V data. However, in our settingwhere we want to inject identity from single imageI2V alignment more closely matches the input and output modalities (image input, video output). As result, I2V fine-tuning better retains the subject identity while also reviving motion dynamics. In practice, we find T2V fine-tuning less effective because of the mismatch between textual inputs and the newly introduced subject from images. Mitigating First-Frame Overreliance. Despite its advantages for temporal consistency, naive I2V fine-tuning can cause copy-and-paste artifact, where the model reuses the initial frame for subsequent frames, producing repetitive or unnatural motion. We tackle this with two lightweight yet effective techniques: Random-Frame Selection: Instead of always using the first frame as the reference, we randomly pick any frame from the video to serve as the input. This weakens the models tendency to overfit to particular frame layout. Image-Token Dropping: Inspired by dropout regularization [35] and classifier-free guidance [16], we randomly drop image tokens, compelling the network to learn motion patterns rather than relying on static reference frame. This encourages diverse motion synthesis and reduces temporal artifacts. By combining I2V fine-tuning along with these two regularization strategies, we preserve robust subject identity and revitalize the pre-trained temporal modeling capabilities, resulting in more natural and dynamic videos under zero-shot S2V settings. 3.5. Stochastic Switching To jointly optimize identity injection and temporal awareness without suffering catastrophic forgetting, we employ stochastic switching scheme. Rather than training on the S2I dataset and unpaired video dataset in separate stages, we alternate between them within each training epoch. probability parameter controls which dataset to sample at each iteration: 1. Sample random value from uniform distribution U(0, 1). 2. If < p, draw mini-batch from the unpaired video dataset (for I2V fine-tuning) and perform the update on the model. 3. Otherwise, draw mini-batch from the image customization dataset (S2I) and update the model. By adjusting p, we can emphasize either deeper temporal learning (larger p) or stronger identity fidelity (smaller p). This switching mechanism balances both objectives and helps prevent either from dominating the training. Stochastically-switched vs. Two-Stage Fine-Tuning. naive two-stage approach first adapts the model to S2I data, then fine-tunes on video data, but this often leads to catastrophic forgetting, with the model losing previously learned identity features. In contrast, our stochastically-switched finetuning updates the model on both data sources in an interleaved fashion, maintaining focus on identity and motion simultaneously and promoting better generalization. Objectives. Let (I (1), (2)) represent an S2I image pair of the same subject in different views or contexts. We optimize an identity reconstruction loss, Limg(I (1), (2)), following the original v-prediction pipeline [44]. For each unpaired video sample, which includes text caption and video , we minimize video reconstruction loss Lvid(T, ) also following the original v-prediction pipeline [44]. When switching to the video dataset, we effectively restore or maintain temporal awareness. At every iteration: (cid:40) Ltotal = Lvid(T, ), Limg(I (1), (2)), with probability 1 p. with probability p, (4) By stochastically alternating between these objectives, the model learns identity injection (from S2I pairs) and temporal coherence (from unpaired video), retaining strengths in both domains without catastrophic forgetting. 4. Experiments 4.1. Setup Implementation Details. Our method is built upon the CogVideoX-5B backbone, which employs 3D Multimodal Diffusion Transformer (MM-DiT) architecture utilizing v-prediction objective [44]. Following the standard CogVideoX training pipeline, we adopt the AdamW optimizer with learning rate of 5105 and cosine annealing learning rate scheduler. Training was performed for 4,000 steps, requiring approximately 288 GPU hours on NVIDIA A100 GPUs. We utilize batch size of 256 for image data and 32 for video data during joint fine-tuning. To facilitate efficient training, we use mixed-precision BF16 training. Our method employs LoRA (Low-Rank Adaptation) with rank 128 and dropout of 0.1 for efficient fine-tuning. We set the probability parameter = 0.2 for stochastic switching, ensuring balanced optimization between identity injection and temporal modeling. We utilize OmniControls Subject200K dataset for image customization, comprising 200K subject-specific image pairs covering various poses, styles, and contexts. Additionally, for video fine-tuning, we 5 Training Method Used Data Motion Dynamic Smoothness Degree CLIP-T CLIP-I DINO-I VideoBooth Custom T2V OmniControl+I2V Custom T2I BLIP+I2V Custom T2I IP-Adapter+I2V Custom T2I Ours Custom T2I 96.95 98.21 97.53 97.21 98.72 51.67 51.67 49.17 55.83 60. 29.59 66.06 34.54 31.89 28.19 26.97 72.58 79.29 73.86 54.16 56.58 45. 32.24 73.70 59.29 Table 1. Quantitative comparison with other methods on VBench. leverage 1% of the Pexels 400K dataset by randomly selecting approximately 4,000 unpaired videos. We provide additional experiments with varying sizes of video datasets in the supplement. Baseline. For baseline comparison, we utilize video S2V model VideoBooth [22], and state-of-the-art image S2V models [26, 37, 45] along with image-to-video (I2V) model. We utilize OmniControl [37], and BLIP-Diffusion [26] and IP-Adapter [45]. Each baseline initially performs image customization independently using its original setup, after which we apply the CogVideoX-5B I2V model to get video from the images. These methods represent state-of-the-art in subject-driven video customization, allowing us to comprehensively evaluate the effectiveness of our proposed approach under zero-shot conditions. Evaluation. We gather 30 reference images from state-ofthe-art image customization papers [4, 8] along with the traditional DreamBooth dataset [33]. We utilize GPT to generate four prompts for each image and evaluate using VBench [21]. Additionally, we assess temporal modeling performance using 300 videos sampled from the Pexels dataset, ensuring no overlap with videos used during training. Following FloVD [23], we classify these videos into three groups based on optical flow magnitude (small: 25, medium: 25 50, large: 50) for detailed analysis. We additionally demonstrate the preprocessing details to evaluate temporal modeling performance in the supplement. 4.2. Quantitative Result Table 1 compares our method with baselines on zero-shot video customization. Our approach achieves superior scores in motion smoothness (98.72), dynamic degree (60.19), text alignment (CLIP-T, 32.24), and identity consistency (DINO-I, 59.29). We see notable gains in identity preservation (DINO-I: +24.75) and prompt alignment (CLIPT: +2.65) relative to VideoBooth. While OmniControl + I2V yields strong motion smoothness and visual similarity (CLIP-I: 72.58), our method further boosts dynamic degree (+8.52) and temporal coherence (+5.13 in DINO-I). BLIPDiffusion obtains slightly higher CLIP-I (79.29), indicating closer frame-to-frame visual alignment; however, our model provides more balanced performance across metrics such as diverse motion and subject fidelity. These reFigure 3. Qualitative comparison on Dreambooth dataset. sults underscore the effectiveness and generalization of our method in zero-shot video customization. 4.3. Qualitative Result We first compare our approach against OmniControl + I2V, Vidu 2.0, and VideoBooth to assess subject identity preservation and motion quality in challenging scenarios, along with our quantitative evaluation. As illustrated in Figure 3, our method demonstrates superior detail retention in ID consistency while generating natural temporal transitions. Our model faithfully reproduces the backpacks intricate details in the first column of examples, whereas Vidu 2.0 preserves ID features but exhibits slightly erratic movement. OmniControl + I2V also captures the subject reasonably well but inherits the inherent limitations of combining two separate models, leading to occasional artifacts in both motion and identity. VideoBooth, which is trained on relatively small video dataset, yields the weakest ID fidelity, sometimes losing the subjects key characteristics. On the second column with dog sequences, OmniControl + I2V again maintains decent likeness overall. However, Vidu 2.0 struggles to keep the dog appearance consistent across frames, producing different-looking dogs. VideoBooth retains the general dog shape, but its frame-to-frame identity consistency deteriorates significantly, resulting in disjointed visuals. Overall, our approach outperforms the baselines in preserving subject identity and sustaining coherent motion across video frames for all the examples, aligning well with the quantitative improvements discussed in Section 4.2. Additionally, in Figure 4, we compare our method with Still-Moving [8] using videos from their homepage. Our results exhibit more consistent color and detailed whiskers on the first row of pink cats, indicating better identity preservation. Similarly, our model preserves eye shape and coloration more faithfully for the pig samples, demonstrating higher subject fidelity compared to Still-Moving. 6 Figure 4. Several examples of qualitative comparison with Still-Moving [8] (left) and ours (right). Training Method Motion Dynamic Smoothness Degree CLIP-T CLIP-I DINO-I Image-only Two-stage Ours 99.60 96. 98.72 0.84 81.51 60.19 32.67 28.96 71.15 84.73 43.19 76. 32.24 73.70 59.29 Table 2. Ablation result on training strategy of alternating optimization with image-only and two-stage training approaches. (0.84). Qualitative examples (Figure 5) also demonstrate that image-only method generates video with barely moving subject and thus cheats the motion smoothness metric. Further corroborating this, Table 3 shows that image-only incurs large FVD on all motion scales, underscoring loss of temporal coherence despite superficially smooth frames. By contrast, two-stage training produces higher dynamic degree (81.51) and better ID similarity (CLIP-I: 84.73, DINO-I: 76.13), yet suffers from severe artifacts and catastrophic forgetting of the models S2V capability. These artifacts inflate the frame-to-frame similarity metrics and degrade the video quality, as reflected in its high FVD in Table 3. In other words, while two-stage training can preserve Figure 5. Qualitative result on ablation study of our component in temporal awareness preservance. 4.4. Ablation Study Training Strategy. We conduct an ablation on three training strategies: image-only, two-stage, and our alternating (stochastically switched) approach. As shown in Table 2, the image-only method achieves the highest motion smoothness (99.60) but has almost zero dynamic degree 7 Method Small Medium Large CogVideoX 597.54 594.26 573.86 Image-only Two-stage 641.92 801.97 636.42 872. 680.34 824.03 Ours 512.30 511.66 550.14 Table 3. Temporal Evaluation following FloVD [23], assessing whether motion dynamics improve compared to image-only or two-stage training. Small - Medium - Large - with each number representing FVD. denotes Pexels [1]-finetuned version of CogVideoX [44]. Method Omini+I2V VideoBooth Vidu 2.0 Ours ID Prompt Motion Overall Consistency Alignment Quality Quality 3.80 3.25 3. 4.08 3.78 3.20 3.24 3.82 3.62 3.08 3.22 3.88 3.44 2.91 3. 3.71 Table 4. Result on human preference study in Likert scale of 1-5. 4.5. Human Preference Study While benchmark metrics offer quantitative insights, they can sometimes be misled by cheating behaviors such as static outputs with artificially high scores. To complement our objective measurements, we conducted human preference study using 20 randomly chosen samples from each baseline and our approach, without cherry-picking. total of 30 participants were asked to rate the generated videos on five-point Likert scale across dimensions of ID consistency, Prompt alignment, Motion quality, and Overall visual appeal. Our method consistently outperformed the baselines, suggesting that our balanced approach to identity preservation and temporal awareness best aligns with human judgments of video realism and quality when viewed holistically. 5. Conclusion We presented zero-shot subject-driven video generation approach that sidesteps the need for large-scale, fully annotated S2V datasets. Our framework achieves robust subject fidelity and natural motion without per-subject optimization by leveraging an image customization (S2I) dataset for identity injection and small, unpaired video corpus for temporal awareness preservation. We further introduced stochastically switched fine-tuning to prevent catastrophic forgetting, alongside random-frame selection and imagetoken dropping to mitigate first-frame over-reliance. Quantitative results, qualitative comparisons, and human studies all confirm the effectiveness of our strategy in preserving subject identity while delivering coherent and realistic motion. We hope that this factorized, data-efficient perspective on subject-driven video generation contributes to research into more scalable and flexible customization pipelines. Figure 6. Effect of random initial frame and image token dropping. certain image details, it often fails to produce coherent, high-quality motion once it switches from the image-based objective. Our proposed alternating strategy (Ours) balances these trade-offs by interleaving S2I and unpaired video data. It achieves strong overall metrics (i.e., 98.72 motion smoothness and 60.19 dynamic degree) while avoiding the static collapse of image-only training and the catastrophic forgetting seen in two-stage training. The corresponding lower FVD scores confirm better motion realism and temporal consistency. Additionally, even compared to original CogVideoX [44], ours shows compatible FVD across all motion dynamics, demonstrating that alternating strategy is effective in preserving motion dynamics. We describe details in the supplement. Random Initial Frame Selection & Dropping. We also examined the effect of random frame selection and imagetoken dropping on I2V fine-tuning. Without either technique, the model often shows superficially strong motion smoothness but drastically reduced dynamic degree, indicating near-static outcome where the subject barely moves. Qualitative observations (Figure 6) reveal that the first reference image dominates subsequent frames, inflating identity metrics like CLIP-I and DINO-I while eliminating meaningful motion. By enabling only random frame selection, we recover some degree of temporal variability yet introduce artifacts in scenes such as shirts or oranges, thereby lowering image alignment scores. By contrast, the combination of frame selection and token dropping strikes an effective balance. We can observe that artifacts are greatly reduced with the reference being more naturally blended. This confirms the importance of mitigating first-frame over-reliance and excessive conditioning on single reference image for smoother, more natural video generation. Ethics Statement. This work is purely research project. Currently, we have no plans to incorporate developed method into product or expand access to the public. Our research paper accounts for the ethical concerns associated with video generation research. To mitigate issues associated with training data, we have implemented rigorous filtering process to purge our training data of inappropriate content, such as explicit imagery and offensive language, to minimize the likelihood of generating inappropriate content."
        },
        {
            "title": "References",
            "content": "[1] Pexels. https://huggingface .co/datasets/ jovianzm/Pexels400k. Accessed: 2025-03-07. 2, 3, 8, 12, 13 [2] PANDA70M dataset. https : / / snap - research . github.io/Panda70M/, 2022. Accessed: 2025-0307. 3 [3] Yuval Atzmon, Rinon Gal, Yoad Tewel, Yoni Kasten, and Gal Chechik. Multi-shot character consistency for text-tovideo generation. arXiv:2412.07750, 2024. 2 [4] Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. The chosen one: Consistent characters in textto-image diffusion models. arXiv:2311.10093, 2023. 6 [5] Max Bain et al. WebVid-10M: large-scale video-text dataset for multi-modal learning. https://github. com/m-bain/webvid, 2021. Accessed: 2025-03-07. 3 [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv:2311.15127, 2023. 1 [7] Kelvin C.K. Chan, Yang Zhao, Xuhui Jia, Ming-Hsuan Yang, and Huisheng Wang. Improving subject-driven image synthesis with subject-agnostic guidance. In CVPR, 2024. 3 [8] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video data. arXiv:2407.08674, 2024. 3, 6, [9] Hong Chen, Yipeng Zhang, Simin Wu, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Identitypreserving disentangled tuning for subject-driven text-toimage generation. In CVPR, 2024. 2, 3 [10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv:2310.00426, 2023. 2 [11] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. arXiv:2501.06187, 2025. 2, 3 [12] Ganggui Ding et al. Freecustom: Tuning-free customized image generation for multi-concept composition. arXiv:2405.13870, 2024. 3 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, [14] Xun Fu, Xiang Li, Guanjie Lai, Yu Xie, and Ying Liu. T2iadapter: Learning the condition for text-to-image generation. In CVPR, 2023. 2 [15] Ravid Gal, Malachi Shachaf, et al. Textual inversion: Imagespecific textual tokens for image editing. arXiv:2208.01618, 2022. 3 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv:2207.12598, 2022. 2, 5 [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022. 1, 3 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. [19] Zhihao Hu and Dong Xu. Videocontrolnet: motionguided video-to-video translation framework by using diffusion model with controlnet. arXiv:2307.14073, 2023. 2 [20] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Concept-master: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv:2501.04698, 2025. 2, 3 [21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 6 [22] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In CVPR, 2024. 2, 3, 6 [23] Wonjoon Jin, Qi Dai, Chong Luo, Seung-Hwan Baek, and Sunghyun Cho. Flovd: Optical flow meets video diffusion model for enhanced camera-controlled video synthesis. arXiv:2502.08244, 2025. 6, 8, 12, 13 [24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. In CVPR, 2023. [25] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3 [26] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. In NeurIPS, 2023. 6 [27] Jiahe Liu, Youran Qu, Qi Yan, Xiaohui Zeng, Lele Wang, and Renjie Liao. Frechet video motion distance: metric for evaluating motion consistency in videos. arXiv:2407.16124, 2024. 13 9 [41] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subjectdriven video customization with precise motion control. arXiv:2410.13830, 2024. [42] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. arXiv:2406.17758, 2024. 3 [43] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv:2408.13239, 2024. 2 [44] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv:2408.06072, 2024. 1, 3, 5, 8, 11, 13 [45] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv:2308.06721, 2023. 3, 6 [46] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, YuIdentityjun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. preserving text-to-video generation by frequency decomposition. arXiv:2411.17440, 2024. 2, 3 [47] Yu Zeng, Vishal M. Patel, Haochen Wang, Xun Huang, TingChun Wang, Ming-Yu Liu, and Yogesh Balaji. Jedi: Jointimage diffusion models for finetuning-free personalized textto-image generation. In CVPR, 2024. 3 [48] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [49] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, 2024. 3 [50] Yuechen Zhang, Yaoyang Liu, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, and Jiaya Jia. Magicmirror: Id-preserved video generation in video diffusion transformers. In AAAI, 2025. 3 [28] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Customizable image synthesis with multiple subjects. In Advances in neural information processing systems, 2023. 3 [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv:2108.01073, 2022. 2 [30] Xiang Meng, Kai Wang, et al. Animatediff: Text-to-video generation using diffusion models. In NeurIPS, 2023. 2 [31] Yanting Miao et al. generation via preference-based reinforcement arXiv:2407.12164, 2024. 3 Subject-driven text-to-image learning. [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3 [33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2, 3, [34] Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, and Yaniv Taigman. Video editing via factorized diffusion distillation (emu-video-edit). arXiv:2403.09334, 2024. 3 [35] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: simple JMLR, way to prevent neural networks from overfitting. 2014. 2, 5 [36] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv:2104.09864, 2023. 3 [37] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv:2411.15098, 2024. 2, 3, 4, 6 [38] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. arXiv:2003.12039, 2020. 13 [39] WanTeam, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv:2503.20314, 2025. 3 [40] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos In CVPR, 2024. 2, with customized subject and motion. 3 Subject-driven Video Generation via Disentangled Identity and Motion"
        },
        {
            "title": "Supplementary Material",
            "content": "Algorithm 1: Proposed S2V Framework Input : Pretrained MM-DiT model fθ; Image dataset DS2I (pairs (I (1), (2)) for identity); (T, )); Unpaired video dataset Dvid (text-video pairs Probability [0, 1] for stochastic switching; RandomFrameSelect() and RandomImageDrop() utilities. Output: Trained model fθ for zero-shot subject-driven video generation Factorization (Sec. 3.2). We decompose the problem into: Identity Injection: Fine-tune on DS2I to inject subject-specific features. Temporal Awareness Preservation: Fine-tune on Dvid to maintain or recover motion dynamics. Stochastic Switching (Sec. 3.5). foreach training iteration 1, . . . , Tmax do Sample (0, 1) if < then /* Temporal phase: use unpaired video data (I2V fine-tuning) */ Fetch mini-batch {(Ti, Vi)} from Dvid Vref RandomFrameSelect(Vi) Vref RandomImageDrop(Vref) Compute Lvid(Ti, Vi) via fθ // v-pred loss θ θ ηθLvid else /* Identity phase: use S2I image pairs */ Fetch mini-batch {(I (1) , (2) Compute Limg(I (1) loss , (2) ) via fθ )} from DS2I // v-pred θ θ ηθLimg return fθ // Return the final model A. Supplementary Material In this section, we provide additional details on our zeroshot subject-driven video (S2V) framework, including full pseudocode for the proposed method and extended ablation results that further validate our design choices. A.1. Method Details subject-driven video (S2V) framework, as outOur lined in Algorithm 1, begins by initializing pretrained multi-modal diffusion transformer fθ (for exam11 ple, CogVideoX [44]), which already possesses general textvisual alignment and motion priors. Two datasets are made available: the first is an image-based S2I dataset DS2I containing image pairs (I (1), (2)) depicting the same subject under varying poses or contexts, and the second is an unlabeled video dataset Dvid consisting of textvideo pairs (T, ). Our key insight is to factorize video customization into two tasksidentity injection and temporal modelingand then interleave them stochastically during training. At each iteration of training, uniform random variable is drawn from U(0, 1). If < p, we sample mini-batch of unlabeled video (Ti, Vi) and perform temporal-phase optimization (lines 914 in Algorithm 1), referred to as I2V fine-tuning. This phase leverages the random frame selection and image-token dropping steps (RandomFrameSelect and RandomImageDrop) to discourage the model from fixating on single reference frame. By computing the video reconstruction loss Lvid(Ti, Vi) in v-prediction manner, we update fθ to maintain or recover realistic motion characteristics. Alternatively, if p, we focus on identity-phase optimization (lines 1619), sampling mini-batch of image pairs (I (1) , (2) ) from DS2I and minimizing the identity injection loss Limg(I (1) ). Crucially, we only tune the LoRAbased parameters dealing with the subject-specific tokens Xin in this phase, preserving the models capacity to handle text tokens CT and output frames Xout. , (2) By stochastically switching between these two objectives, the model balances subject fidelity and temporal consistency throughout training, avoiding the pitfalls of purely sequential or single-focus approaches. After Tmax iterations, fθ emerges as zero-shot S2V model capable of generating videos that simultaneously preserve the subjects identity and exhibit coherent motioneven though no large-scale annotated S2V dataset was required. A.2. Ablation Studies A.2.1. Effect of the Reference Token Tab. 5 demonstrates how adding dedicated <CLS> token to the prompt affects our models performance. Without this reference token, the model attains slightly higher motion smoothness (98.84) and marginally better CLIP-T (32.87), but it underperforms in dynamic degree (54.55) and identity-focused metrics (CLIP-I: 73.36, DINO-I: 57.51). Introducing <CLS> evidently improves subject fidelity (CLIP-I increases to 73.70 and DINO-I to 59.29) and fosters more diverse motion (dynamic degree rises to 60.19). Training Method Motion Smoothness Dynamic Degree CLIP-T CLIP-I DINO-I Video Count Motion Smoothness Dynamic Degree CLIP-T CLIP-I DINO-I w/o Ref. token w/ Ref. token 98.84 98. 54.55 60.19 32.87 32.24 73.36 73. 57.51 59.29 Table 5. Ablation on the reference token. Adding <CLS> yields improved subject identity scores (CLIP-I, DINO-I) and higher dynamic degree. 1K 2K 3K 4K (Ours) 99.03 98.96 98.79 98.72 59.66 52.25 55.46 60. 32.16 32.49 32.04 32.24 72.69 72.13 72.79 73.70 56.98 54.42 55.57 59.29 Table 7. Abalation study on using different number of videos. Motion Smoothness Dynamic Degree CLIP-T CLIP-I DINO-I Video Ratio Motion Smoothness Dynamic Degree CLIP-T CLIP-I DINO-I Training Method T2I + T2V (joint) Ours 98.85 98.72 44.14 60. 33.41 32.24 72.71 73.70 48.68 59. Table 6. Comparison with T2V-only stochastically-switched finetuning, with image drop probaiblity of 1. Switching to textonly input (T2V) moderately boosts CLIP-T but hurts subject fidelity and dynamic degree. We attribute these gains to the reference token guiding the alignment of subject tokens (Xin) with the textual prompt more explicitly, resulting in both stronger identity preservation and more coherent variations in motion. A.2.2. T2V vs. I2V Training We also ablate replacing our image-to-video (I2V) training with text-to-video (T2V) setup. In Tab. 7, the T2I+T2V (joint) attains slightly better CLIP-T but lower dynamic degree and subject alignment (CLIP-I, DINO-I). This suggests T2V training struggles when introducing novel subject identity purely through text, yielding weaker overall identity preservation. By contrast, our I2V approach strikes better balance, preserving subject details (CLIP-I: 73.70, DINO-I: 59.29) and maintaining sufficient motion (dynamic degree: 60.19). A.3. Effect of Varying the Video Dataset Size Tab. 7 reports how our methods performance changes when using different amounts of unlabeled video data for I2V fine-tuning (1K, 2K, 3K, and 4K videos). Notably, with only 1K videos, we already obtain relatively strong results, suggesting that even small unlabeled corpus can restore temporal consistency to some extent. However, increasing the video count to 4K (our default setting) steadily improves dynamic degree from 59.66 to 60.19 and also boosts identity fidelity (DINO-I) from 56.98 up to 59.29, indicating more consistent subject representation across frames. We also observe modest variation in CLIP-T and CLIPI scores when moving from 1K to 4K videos, implying that larger video dataset helps balance subject detail preservation and temporal motion, without overfitting to particular frames or motion patterns. In short, while our method is fairly robust to smaller unlabeled datasets, using around 4K (i.e., 1% of Pexels [1] dataset) videos offers the best trade12 0.0 0.2 (Ours) 0.4 0.6 99.60 98.72 98.31 98.33 0.84 60.19 59.12 60.87 32.67 32.24 31.94 31. 71.15 73.70 73.53 76.71 43.19 59.29 56.60 62.84 Table 8. Abalation study on using different to choose I2V finetuning. off between data efficiency and stable motion/appearance results. A.4. Effect of Varying the Switching Probability In Tab. 8, we examine how different values of pthe probability of sampling unlabeled video data (I2V finetuning)affect overall performance. When = 0.0, the model relies solely on image-based training (S2I) and achieves relatively high dynamic degree (63.03) but moderate identity scores (CLIP-I: 72.86, DINO-I: 57.86). Increasing to 0.2 or 0.4 yields balanced improvements across most metrics, reflecting better coordination between identity and motion. At = 0.6, the model dedicates greater share of updates to I2V training, strengthening identity alignment (CLIP-I: 76.71, DINO-I: 62.84) while keeping dynamic degree stable (60.87). Although different values trade off between motion smoothness and identity fidelity to varying degrees, our chosen = 0.2 demonstrates strong overall balance, as highlighted in the main paper. A.5. Temporal Modeling Evaluation We assess our models capability to capture realistic object motion using protocol adapted from FloVD [23], while ensuring minimal or no camera movement in the test data. Specifically, we collect 100K videos from Pexels [1] that are not used during our stochastically-switched fine-tuning, then apply the following steps to create three benchmark subsets (small, medium, large) based on foreground motion magnitude: 1) ForegroundBackground Segmentation. For each video, we use an off-the-shelf segmentation model (e.g., Grounded-SAM2) on the first frame to separate foreground and background regions. This allows us to measure object (foreground) motion independently from any camerainduced background shifts. 2) Optical Flow Computation. We estimate optical flow between the first frame and each subsequent frame using standard flow estimator (e.g., RAFT [38]). Let uf (x) and ub(x) denote the per-pixel flow vectors for the foreground and background pixels, respectively, at position x. We record: A.6. Additional Qualitative Result Please refer to next page. A.7. Qualitative Result with Videos Please refer to supplement video attached. FlowMagf = FlowMagb ="
        },
        {
            "title": "1\nNb",
            "content": "(cid:88) xfg (cid:88) xbg uf (x), ub(x). where Nf and Nb are the respective pixel counts in the foreground and background masks. 3) Dataset Filtering. To ensure negligible camera motion, we discard any video whose average magnitude of background flow FlowMagb exceeds 10 pixels. This filtering step excludes scenes with significant global shifts, retaining only those with primarily object-centric motion. 4) Category Assignment. Based on the average magnitude of foreground flow FlowMagf (averaged over all frames), we categorize videos into: Small: 0 FlowMagf 25 Medium: 25 < FlowMagf 50 Large: FlowMagf > 50 Each category contains 300 videos, ensuring balanced evaluation of low-, moderate-, and high-motion scenarios. 5) Evaluation Protocol. Within each subset, we use only the first frame (including any textual or reference cues, if required) to generate video of the same length. We then compute FVD [27] between the generated outputs and the ground-truth videos. By comparing FVD across small, medium, and large motion classes, we obtain clearer picture of how each model (ours vs. baselines) adapts to varying object-motion intensities. Discussion. This motion-focused split highlights each methods strengths and weaknesses. For example, model might produce near-static outputs for low-motion datacheating on metrics like smoothnessyet fail to track fast-moving objects in high-motion videos. As observed in FloVD [23], categorizing by foreground flow magnitude reveals these nuances more effectively than aggregated scores alone. Additional Details of Original CogVideoX. To ensure fair comparison with our method, we additionally finetune the original CogVideoX [44] using subset of the Pexels [1] dataset equivalent to the one used in our training. Since our model is trained for 4K steps with sampling ratio = 0.2, we match this by finetuning CogVideoX for 800 steps (0.2 4000). As result, we achieve comparable performance to the original CogVideoX in terms of motion dynamics evaluation. on Evaluation 13 Figure 7. Additional qualitative result. Comparison with other baselines. 14 Figure 8. Additional qualitative result. Comparison with other baselines."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "POSTECH",
        "Seoul National University"
    ]
}