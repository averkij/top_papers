{
    "paper_title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
    "authors": [
        "Yuchuan Tian",
        "Yuchen Liang",
        "Jiacheng Sun",
        "Shuo Zhang",
        "Guangwen Yang",
        "Yingte Shu",
        "Sibo Fang",
        "Tianyu Guo",
        "Kai Han",
        "Chao Xu",
        "Hanting Chen",
        "Xinghao Chen",
        "Yunhe Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 6 7 7 6 0 . 2 1 5 2 : r From Next-Token to Next-Block: Principled Adaptation Path for Diffusion LLMs Peking University & Huawei Technologies"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating throughput bottleneck. Diffusion Language Models (DLMs)especially block-wise variantsenable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior adaptation attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into block-diffusion recipe, leaving fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as intra-paradigm path from AR to BlockDiffusion by viewing AR as Block-Diffusion with blocksize = 1. Concretely, we design the pathway of adaptation as follows: we use context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDIFF-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are rapidly permeating real-world applications because of their strong generative capability. However, the dominance of AutoRegressive (AR) LLMs is built on fundamental trade-off: powerful left-to-right causal generation at the cost of strictly sequential, token-by-token decoding. This trade-off creates an inference bottleneck that limits the decoding speed of AR LLMs. In contrast, Diffusion Language Models (DLMs), particularly block-wise paradigms, offer promising alternative by enabling parallel generation and flexible bidirectional reasoning. However, training large-scale DLMs from scratch is computationally prohibitive and discards the vast knowledge already encoded in mature, open-source AR checkpoints. Among diffusion paradigms, masked diffusion that trains model to denoise masked tokens is uniquely suited for adaptation, as it shares the standard transformer architecture and objectives with AR LLMs. More practically, Block-Diffusion [2; 7] has emerged as state-of-the-art approach, generating text block-by-block. This semi-autoregressive method retains the left-to-right structure of AR models at the block level, while permitting bidirectional reasoning within each block, offering compelling balance of quality, speed, and parallelism. This naturally raises critical question: how can we efficiently adapt pre-trained AR model into high-performing Block-Diffusion model? Existing adaptation methods are lacking. Early attempts Preprint. Under review. used simplistic logit shifts or random attention mask growth to full-sequence diffusion, struggling to scale. More recent block-wise adaptations simply transplant the AR model into Block-Diffusion training setup as is. They do not investigate the core mismatch between AR and Block-Diffusion. This leaves clear gap: how to adapt an AR model to Block-Diffusion while maximally preserving its powerful, pre-trained inductive bias. Our approach is grounded in key insight: AR and Block-Diffusion are fundamentally similar; AR generation is special case of BlockDiffusion with blocksize of 1. This insight reframes adaptation not as crude switch, but as smooth transition across spectrum. Under this unified view, we look for transition path from AR to Block-Diffusion. Our design consists of context-causal attention mask that preserves AR inductive bias in committed context, parallel training with an auxiliary AR objective that stabilizes the early phase of adaptation, and gradual growth of block size. The design provides path-consistent architecture and training strategy that progressively unlocks bidirectional refinement within the generating block while maintaining strict traininference alignment. Our contributions are as follows: Figure 1: Comparison of our model with baselines. After adaptation from an open-sourced AR LLM, our model has good long-sequence and reasoning capabilities and shows outstanding performance in various benchmarks. 1. We propose curriculum that smoothly transitions the model from causal (blocksize = 1) to blockwise bidirectional token generation. 2. We design Context-Causal Attention Mask tailored for this adaptation, which preserves AR knowledge in the context while enabling efficient bidirectional intra-block generation. We develop an efficient parallel training strategy that aligns with inference and incorporates an auxiliary AR loss, markedly improving convergence speed and knowledge retention. We develop gradual block growth approach that alleviates the gap between AR and Block-Diffusion models, improving adaptation. 3. We demonstrate the effectiveness of our approach with NBDIFF-7B, which, after efficient adaptation from its strong AR counterparts, could model long contexts (up to 32K sequence lengths) and perform reasoning, achieving state-of-the-art performance. both NBDIFF7B-BASE and NBDIFF-7B-INSTRUCT outperform strong baselines like LLaDA [18], Dream [30], and SDAR [7] on general, math, and code benchmarks."
        },
        {
            "title": "2 Related Work",
            "content": "Discrete diffusion for language. Foundational studies extended diffusion to categorical spaces and showed that discrete corruption, i.e. denoising, can be learned effectively for text, shaping design choices such as uniform vs. structured transitions and whether to use an absorbing state [3]. This laid the groundwork for language diffusion beyond continuous relaxations and clarified objective links to classical LM training. Currently, two practical framings dominate. Masked diffusion iteratively reveals hidden tokens and has been shown to support controllable generation and strong likelihoods without left-to-right decoding [15]. Recent \"masked diffusion language models\" further streamline training and close much of the perplexity gap to AR LMs with simple recipes [20]. In parallel, absorbing-state diffusion pushes tokens toward sink symbol during noising; recent analyses connect its objective to conditional distributions and offer insights into calibration and sampling behavior [19]. Large-scale systems trained from scratch (e.g., LLaDA [18]) argue that maskeddiffusion-style pretraining can be competitive with strong AR baselines, and can be extended to multimodal instruction tuning [18; 31]. These results crystallize the viability of masked diffusion LMs at billion-parameter scale. Full-Sequence vs. Block-Diffusion (semi-autoregressive). Full-sequence diffusion enables fully bidirectional context but can be compute-heavy on long texts and misaligned with left-to-right 2 Figure 2: The diffusion paradigm of our NBDIFF-7B-INSTRUCT model. We compare popular language generation paradigms. Diffusion LLMs adapted from AR adopt logit shift and attention mask growth; Block-Diffusion uses block-wise autoregressive and maintains an intra-block bidirectional mask; Our model adopts Block-Diffusion where bidirectional attention is used intra-block, but features causal context. inductive biases. Some recent works looks at reusing intermediate computes as \"cache\" [16; 17; 27], but they are not resolving the inefficiency fundamentally. Block-Diffusion addresses this by committing previous context while denoising the current block bidirectionally, enabling parallel token updates and arbitrary-length generation [10; 2]. This framing provides practical dialsblock size and refinement stepsto trade off quality and speed. Adapting AR models to diffusion. Beyond training from scratch, several efforts convert strong AR checkpoints into diffusion-style decoders with lightweight adaptation, often at the block level. Reports from industry and open research describe objective connections and practical recipes for conversion, as well as synergistic AR-diffusion paradigms that retain AR quality while unlocking parallel generation [9; 7; 27; 28]. These trends motivate our focus on principled AR-Block-Diffusion adaptation via context-causal masking, parallel training aligned with inference, auxiliary AR supervision, and block-size growth curriculum. Recent Advances in Diffusion Reasoning. Early diffusion-based reasoning systems explored inference-time or temporal scaling to strengthen multi-step computation (e.g., deeper noising/denoising schedules, re-masking, and in-place prompting) but generally operated under modest 3 context lengths, which constrained the achievable reasoning depth and fidelity [29; 23; 12]. More recent efforts target harder domains such as math and code, often coupling diffusion LLMs with reinforcement learning to optimize long-horizon decision signals [1; 32; 22; 24]. Despite progress, these approaches still under-utilize the rich priors of strong AR LLMs and remain sensitive to limited context, leading to bottlenecks on math/coding benchmarks. In contrast, we adapt AR models into block-diffusion generators, preserving context left-to-right causality while gradually introducing intra-block bidirectionality; this leverages pretrained AR knowledge with long-context capabilities and provides smoother optimization path than jumping directly to fully bidirectional diffusion."
        },
        {
            "title": "3 Rethinking DLM Adaptation from AR: to Where, and How?",
            "content": "3.1 Revisiting Previous Adaptation Prior adaptation work [9] mainly focuses on adapting full-sequence diffusion model from an AR model. The authors observe the difference in attention mechanism, and proposes random \"annealing\", or random growth, of the attention mask from lower-triangular causal attention mask to full, bidirectional attention mask. While the work [9] is trying to bridge the AR and Diffusion generation paradigms, we hold different opinions on random growth of attention masks: its transition is not \"natural.\" In practice, training sees unknown future corpora; sporadically granting early tokens access to random subset of future tokens yields incomplete and potentially misleading context, thus limiting adaptation potentials. Hence, we try to answer two major questions regarding this transition, i.e. where and how. Firstly, what should be the destination of this transition? Secondly, is there smoother way to transition from an AR model to DLM model? 3.2 Introducing Block-Diffusion Unlike previous adaptation methods [9] that focuses merely on Full-Sequence Diffusion models, recent diffusion LLMs [7; 28] increasingly adopt Block-Diffusion [2], which is both more efficient and performant than full-sequence diffusion and conceptually sits between diffusion and AR: generation proceeds left-to-right across blocks while remaining bidirectional within block. Let token sequence x1:L be partitioned into contiguous blocks of size b. Denote the k-th block by Bk = {sk, . . . , ek} with ek sk + 1 = (the last block may be shorter). In Block-Diffusion, generation for block Bk is conditioned on the entire prefix x<sk and proceeds via iterative refinement steps: pθ(xBk x<sk ) = (cid:89) t=1 (cid:16) pθ x(t1) Bk (cid:12) (cid:12) x(t) (cid:12) Bk , x<sk (cid:17) , (1) denotes the partially revealed (noised) state of the block at diffusion step t, and x(0) Bk where x(t) xBk Bk is the final clean block. At inference time, the language sequence is generated left-to-right, following block-wise causal manner: pθ(x1:L) = (cid:89) k=1 pθ(xBk x<sk ) . (2) In the most basic recipe for training, we sample span from the corpus, choose block size b, and treat the last tokens as the active block BK. We then apply diffusion corruption within this last block (the preceding context x<sK remains unmasked) and train the model to denoise the masked positions inside BK: (cid:104) Lblock(θ) = (cid:88) iBK : m(i)=0 log pθ (cid:0)xi x(t) BK , x<sK (cid:1)(cid:105) , (3) where m() is the step-dependent visibility mask inside BK. This captures the spirit of diffusion LLMs: bidirectional refinement inside the active block, while the prefix acts as fixed conditioning. The performance advantage of using block-diffusion is two-fold: 4 Block-Diffusion enables the use of KV-Cache. Different from full-sequence diffusion where the whole sequence has to pass through the model together for each inference step, Block-Diffusion keeps previous tokens fixed while only performs decoding in the last block of the generated tokens. Thus it could re-use the KV-Cache from previous block generations and only the last block to be generated needs to be passed into the model, reducing significant inference costs. Block-Diffusion is not limiting fast inference. In practice, though Block-Diffusion has designated the causal (left-to-right) block generation sequence, the use of bidirectional attention within the last generating block enables parallel token-decoding. In practice, we use the blocksize of 32 tokens (larger than previous same-scale DLMs [7]) to tap the speed potential of the proposed model to the full. 3.3 When Block-Diffusion Meets Adaptation Apart from the performance advantages, Block-Diffusion helps easy adaptation from AR. Instead of forcing global jump from causal to full-sequence bidirectional attention, we treat Block-Diffusion as the destination. By preserving left-to-right semantics at the block level and relaxing bidirectionality inside the active block, we try to partially align with the AR inductive bias for better adaptation, which greatly reduces the difficulty for alignment. In addition, the blockwise semi-AR manner could also enable parallel training, improving data utility and model convergence. In the next section, we will stick to the paradigm of Block-Diffusion and seek way for fast transition from AR model."
        },
        {
            "title": "4 Designing Transition Paradigms",
            "content": "4.1 How Should the Unmasked Context Attend? Comparing the attention mechanisms of Block-Diffusion and AR (Block-Diffusion of blocksize = 1), the key difference that requires our adaptation efforts lies in the bidirectional attention within the last active block; namely, we have to grow the attention mask at the end of the generating sequence from blocksize = 1 to target block size. However, apart from the attention within the active block region, different transition solutions arises from the decoded context: how tokens in the unmasked context (x<sK ) should attend to each other? Here we analyze two possible attention pathways of Block-Diffusion as follows: Block-Causal (widely used in Block-Diffusion [2] / D2F [25] / SDAR [7]). Tokens have bidirectional attention within every block (both past/committed blocks and the active block), and causal flow across blocks (each token can see all tokens in earlier blocks). This maximizes intra-block interaction everywhere, not only in the active block. Context-Causal (our preferred setting). The context (prompt + already generated/committed blocks) remains strictly causal: each token only attends to itself and predecessors. Only the last (active) block is given bidirectional attention to support diffusion-style refinement; future blocks are hidden. Table 1: Comparison of Block-Causal and Context-Causal attention schemes. Context-Causal gains clear advantage in adaptation from AR. Scheme GSM8K MATH500 HumanEval MBPP Avg Block-Causal Context-Causal (Ours) 60.1 68.8 1.6 36.8 24.4 41. 39.4 47.4 31.4 48.6 To examine the two schemes, we adapt two Block-Diffusion models from an AR model (based on Block-Causal and Context-Causal, respectively) by training 2000 iterations, and examine their performance on popular math and coding benchmarks. The results are shown in Table 1. 5 In these preliminary adaptation experiments, Context-Causal Empirical takeaway and intuition. consistently outperforms Block-Causal by large margins: the accuracy is significantly higher when the context keeps strict causality and only the active block is bidirectional. We attribute this to: (i) Inductive-bias alignment with AR pretraining, which reduces the gap between AR and Block-Diffusion by preserving causal self-attention in the context; and (ii) Generationparadigm consistency: although the active block is refined bidirectionally (no fixed order inside the block), the overall decoding remains left-to-right across blocks. Keeping the context causal does not reduce the visibility required for the block being generated and avoids introducing spurious, partially bidirectional signals into earlier (already finalized) content. 4.2 Training Parallelism The naive block-diffusion recipe is data-inefficient: random cropping wastes the remaining tokens of each sequence, and only small subset of masked tokens inside the last block contributes to the loss. Unlike AR pretrainingwhere every token can supervise next-token predictionswitching to next-block prediction sharply reduces token utilization. We therefore restructure training so that all blocks provide learning signal in single forward pass. We seek to model all blockwise conditionals in parallel using single transformer call. Instead of invoking the denoiser times, we concatenate noised view xt (partitioned into blocks) with the clean sequence x: xall = xt (length 2L). structured attention mask Mall {0, 1}2L2L updates all token representations in one shot: Mall = (cid:20)MBD MOBC 0 MCC (cid:21) . (4) Within the noised view xt, attention is restricted to be block-wise (block-diagonal): [MBD]ij = (cid:26)1, 0, and are in the same block, otherwise. From noised tokens to the clean context, we allow only earlier clean blocks as conditioning (offset block-causal): [MOBC]ij = (cid:26)1, clean position lies in block strictly before the block of i, 0, otherwise. Inside the clean context, we keep strict left-to-right causality (context-causal): [MCC]ij = (cid:26)1, 0, i, > i. The lower-left tile is zero so the clean context never reads from the noised view, matching inferencetime semantics. Let index all blocks and Mt be the step-dependent visibility inside the noised view. Under Eq. (4), one forward pass supplies gradients for all masked tokens across all blocks: Lparallel(θ) = (cid:88) (cid:88) log pθ (cid:0)xi xt, x; Mall (cid:1) BB iB: Mt(i)=0 . (5) Processing xt and jointly amortizes KV-cache construction, maximizes per-step token utilization, and empirically stabilizes training compared with randomly growing global masks. An example for L=16 and block size b=4 is illustrated in Fig. 3; but in reality, we use b=32 4.3 AR Loss Guidance Even with the one-pass parallel recipe, only logits on the noised (active) blocks directly incur diffusion loss, while logits on the clean-context branch of xall = xt primarily act as conditioning; since Figure 3: Our Parallel Training Diagram. The diagram shows the parallel training form of our Context-Causal setting (we use blocksize = 4 as an example; the actual blocksize is 32). We concatenate clean, unmasked token sequence to the noised sequence. The attention mask Mall is designed (shown in the right) such that strictly-causal attention is applied in the unmasked input; for the masked input, each token has bidirectional attention intra-block, but causal attention to past inter-block tokens that are unmasked. AR loss LAR is introduced in addition to the canonical masked loss LMDM for faster adaptation. this branch follows strictly causal self-attention, we convert those otherwise unused predictions into additional next-token supervision. Let index tokens on the clean context, xi be the ground-truth token at position i, and MCC denote the context-causal mask; we attach standard LM head and define an autoregressive objective over the context as (cid:34) LAR(θ) = (cid:88) iC log pθ (cid:0)xi+1 xi; MCC (cid:1) (cid:35) , (6) which turns every context logit into supervised next-token prediction without altering diffusion-side conditioning. Let LMDM(θ) be the masked/block-diffusion denoising loss computed on the noised view under Mall (as in Eq. (5)); we then train with an affine combination controlled by λ 0: Ltotal(θ) = LMDM(θ) + λ LAR(θ). (7) Typically, λ is set as 0.5 to keep the number of tokens involved in MDM or AR loss at the same level. We hold that this coupled objective is more beneficial than masked modeling alone. The benefits are: LAR increases per-step supervision density, reusing context logits that would otherwise be ignored. LAR mitigates AR knowledge forgetting, keeping the long-sequence and reasoning capabilities of the model. 4.4 Gradual Block Growth We pursue smoother adaptation by rethinking token generation via the perspective of BlockDiffusion. As noted, AR could be viewed as Block-Diffusion of blocksize = 1; hence, the transition could be viewed under the Block-Diffusion paradigm, where we start from block size of 1 and end at the target block size. Naturally, we gradually increase the generation block size from ARs singletoken steps toward larger blocks, so that the model transitions continuously from next-token prediction to next-block refinement. This monotonic growth retains left-to-right causality while progressively introducing intra-block bidirectionality, narrowing the procedural gap and easing optimization. Starting from block size b=1 (AR), we interpolate to larger bidirectional blocks by growing in integer powers of user-chosen base {2, 4, . . . } (on normal basis the power of 2 is adopted) at fixed training intervals. Let be the global training step, the interval (in steps) between growth events, s0 an optional warmup before the first growth, b0=1 the starting size, and bmax the inference target. The schedule is as follows. b(s) = min (cid:110) bmax, b0 (cid:4) max(0, ss0) (cid:5)(cid:111) , (8) 7 Figure 4: Our Parallel Training Diagram. The diagram shows the parallel training form of our Context-Causal setting (we use blocksize = 4 as an example; the actual blocksize is 32). We concatenate clean, unmasked token sequence to the noised sequence. An attention mask is designed such that strictly-causal attention is applied in the unmasked input; for the masked input, each token has bidirectional attention intra-block, but causal attention to past inter-block tokens that are unmasked. AR loss LAR is introduced in addition to the canonical masked loss LMDM for faster adaptation. which holds constant on plateaus [s0 +k, s0 +(k+1)) and multiplies it by whenever crosses multiple of (e.g., 1 r2 . . . ) until capped by bmax. This integer-power curriculum reduces the AR diffusion adaptation gap by aligning early optimization with the AR inductive bias (small b) and gradually unlocking intra-block bidirectionality and parallel supervision as increases. In practice, we keep train-inference semantics matched at each plateau via the same context-causal mask family, optionally co-schedule compute by shrinking the refinement steps per block (s) 1/b(s) to maintain roughly stable token-update budget, and anneal the AR-loss weight λ as blocks grow to reallocate gradient capacity toward the diffusion objective."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Setup We start from the open-source Pangu-Embedded-7B [5] autoregressive checkpoint and adapt it into diffusion language model (DLM) using the training corpus that accompanies the release. The adaptation follows the methodology introduced earlier: (i) parallel context-causal training with the concatenated noised/clean views and the structured mask Mall, (ii) the AR loss on the cleancontext branch to densify supervision, and (iii) gradual block-growth curriculum that interpolates from AR (block size b=1) to larger bidirectional blocks via the integer-power schedule b(s) = min{bmax, r(ss0)/}. Training. The pretraining adaptation stage uses two-phase learning-rate schedule: we keep the learning rate constant for the first 24,000 iterations and then apply learning-rate cooldown over the final 60,000 iterations, for total of 84,000 iterations. We train with sequence length ℓ=8k and global batch size B=1024. The effective tokens processed per iteration are 8M tokens, so across 84,000 iterations the total token volume is approximately 700B tokens. Then, to equip the model with long-sequence generative capabilities, we extend the pretraining sequence length ℓ=32k and train for 23,800 iterations (approximately 100B tokens), equipping the model with long-sequence modeling capabilities. Finally, we use 10B-token SFT data of sequence length ℓ=32k to finetune the model for 10 epochs (approximately 17,000 iterations) and equip it with reasoning capabilities. We use uniform masking strategy over the diffusion step Uniform[0, 1] (sampled and mapped to the discrete step index), and keep the inference/training mask families matched at each curriculum 8 plateau. All other optimizer and system-level settings follow the default configuration of the PanguEmbedded-7B [5] release. Inference. For sampling during inference, we build on Fast-DLLM-v2 [27], i.e., Block-Diffusion [2] instantiation of Fast-DLLM [27]: at the macro level the sequence is generated left-to-right by blocks (causal across blocks), while inside each block we permit bidirectional attention to refine tokens jointly. For speed, the inner refinement can follow the v2 small-block schedule or be collapsed into single full-block bidirectional pass when latency matters. Compared with the vanilla recipe, we replace confidence-based scheduling with an entropy-based parallel decoding rule, which is more sensitive to distributional uncertainty and reduces premature commitment under ambiguous contexts. To balance train-inference consistency and throughput, we set the macro block size to 32, which aligns masking between training and decoding and yields substantial parallelism, and follow Fast-DLLM-v2 [28] for the small block size of 4 during intra-block refinement. 5.2 Evaluation We primarily evaluate NBDIFF-7B-INSTRUCT in both Base and Instruct settings across three capability areascode, math, and general knowledgeand compare its performance against several baseline models to understand relative strengths and trade-offs. General. MMLU [11] covers 57 subjects across STEM, humanities, and social sciences (mostly multiple-choice), probing broad knowledge plus light reasoning; MMLU-Pro [26] is tougher, more rigorous variant stressing deeper domain understanding and application. CEVAL is Chinese comprehensive exam-style suite assessing Chinese knowledge and test-taking ability; CMMLU [14] broadens Chinese multitask coverage to gauge breadth and fine-grained domain knowledge. BBH (BIG-Bench Hard) [21] is curated set of particularly difficult tasks targeting abstraction, compositionality, and complex reasoning. For SFT version, we test our model on IFEval [33], which is an instruction-following benchmark that checks whether models outputs faithfully satisfy explicit constraints and formatting requirements across diverse prompts. Math. GSM8K [8] contains grade-school to middle-school math word problems in English, testing multi-step arithmetic reasoning and translating language into calculations. MATH500 [13] is significantly harder, with competition-style problems (algebra, geometry, number theory, etc.) that demand rigorous, step-by-step reasoning. Code. MBPP [4] evaluates beginner-to-intermediate Python programming: given naturallanguage spec, the model must write function that passes provided tests. HumanEval [6] focuses on code synthesis from textual specs, measuring whether the generated Python function passes hidden unit testsemphasizing semantic understanding and executable correctness. Here we summarize the comparative results for our NBDIFF-7B-BASE against strong 7B baselines  (Table 2)  . Overall, NBDIFF-7B-BASE attains the highest macro average, surpassing Dream-v0Base-7B and both LLaDA bases. On general knowledge, it leads on MMLU-Pro (52.7), CMMLU (76.9), CEval (75.9), and BBH (69.4), and remains competitive on MMLU (69.1, second only to Dreams 69.5). In math, NBDIFF-7B-BASE ranks first on both GSM8K (79.6) and MATH500 (46.0), indicating strong multi-step and competition-style reasoning. In coding, it is consistently runnerup, slightly behind Dream-v0 [30] but ahead of the LLaDA [18] baselines. Taken together, these results show that diffusion-style LLM can match or outperform autoregressive bases across diverse evaluations, with particularly clear gains on harder general-reasoning and Chinese benchmarks. Finally, we present the SFT (Instruct) results in Table 3. NBDIFF-7B-INSTRUCT delivers the highest macro average (78.8) among SFT baselines, substantially outperforming SDAR-8B [7] and LLaDA [18] variants. On general knowledge, it sets the pace on MMLU (81.7), MMLU-Pro (71.3), and CMMLU (76.4), and ranks third on CEval (70.8) while remaining competitive on IFEval (60.8) despite ties among baselines. For math, NBDIFF-7B-INSTRUCT achieves state-of-the-art GSM8K (91.9) and MATH500 (84.3), indicating strong multi-step and competition-style reasoning under instruction following. In coding, it tops both MBPP (84.1) and HumanEval (87.8), narrowing and in most cases reversing the AR-favoring gap seen in some base models. Taken together, these results show that instruction tuning on diffusion LLM not only preserves the Base models breadth, but 9 Table 2: Comparison between the Base version of our model and latest base version diffusion language models. Our base model shows strong performance on general, math, and coding benchmarks. LLaDA-MoE-7B LLaDA-8B Dream-v0 Ours-7B"
        },
        {
            "title": "Benchmark",
            "content": "A1B-Base Base Base-7B Base General MMLU MMLU-Pro CMMLU CEval BBH Math GSM8K MATH"
        },
        {
            "title": "Coding\nMBPP\nHumanEval",
            "content": "Avg 64.6 39.2 65.7 65.6 52.7 66.4 36.1 52.4 45.7 54.3 65.9 41.8 69.9 70.5 49. 70.7 27.3 38.2 33.5 52.0 69.5 48.2 60.9 59.2 57.9 77.8 39.6 56.2 57. 60.0 69.1 52.7 76.9 75.9 69.4 79.6 46.0 55.8 54.3 64.3 Table 3: Comparison between the Instruct version of our model and the latest SFT (Instruct) version diffusion language models. Our model demonstrates strong performance on general, math, and coding tasks, and outcompetes latest diffusion baselines by large margins. * indicates non-official replications. Benchmark General MMLU MMLU-Pro CMMLU CEval IFEval Math GSM8K MATH500 Coding MBPP HumanEval Avg LLaDA-MoE 7B-A1B LLaDA2.0-mini Dream-v0 preview 16BA1B Instruct-7B SDAR Ours-7B Instruct 8B 67.2 44.6 64.3 63.9 59.3 82.4 58.7 70.0 61. 61.1 72.0 49.2 67.5 66.5 62.5 89.0 73.5 77.8 80.5 71.0 67.0 43.3 58.8 87.8 62. 81.0 39.2 58.8 55.5 58.2 78.6 56.9 75.7 72.7* 61.4 91.3 78.6 72.0 78. 74.0 81.7 71.3 76.4 70.8 60.8 91.9 84.3 84.1 87.8 78.8 amplifies performance across general, math, and coding by large margins, establishing NBDIFF-7BINSTRUCT as strong, balanced SFT model in the 7B class. 5.3 Ablation Study In this section, we analyze the effectiveness of the proposed adaptation methodologies. Comparison with existing baselines. On the basis of logit shift introduced in DiffuLLaMA [9], we also adopt two methods as baselines. \"Annealed Attention Mask\" [9] proposes random growth from the auto-regressive causal attention mask to the targeted Block-Diffusion attention mask; \"Baseline\" directly uses the targeted attention mask for training. From Table 4, we observe that \"Annealed Attention Mask\" is not performant in the adaptation from AR to Block-Diffusion. Our method could outcompete both baselines by considerable margins. 10 Table 4: Ablations of the proposed adaptation methods from AR to Block-Diffusion. Compared with plain finetuning (Baseline), our adaptation method demonstrates faster and better adaptation to Block-Diffusion paradigms after 4K pretraining iterations. Previous method is not reaching its expected performance in our Block-Diffusion paradigm."
        },
        {
            "title": "Method",
            "content": "GSM8K MATH500 HumanEval MBPP"
        },
        {
            "title": "Avg",
            "content": "Annealed Attention Mask [9] Baseline +AR loss +Graudal Growth (Ours) 70.05 72.63 75.13 76.57 35.46 36.14 43.30 44. 26.83 39.63 40.24 44.51 45.00 47.40 53.20 54.60 44.34 48.95 52.97 54. Contribution of Adaptation. In Table 4, we also ablate our two adaptation components: AR loss and gradual block-size growthstarting from plain fine-tuning baseline. Adding AR loss lifts the overall Avg from 48.95 to 52.97 (+4%), with the largest gains on math and MBPP and modest improvements elsewhere. Stacking gradual block-size growth further raises Avg to 54.94, indicating that smoother progression from next-token to next-block generation improves stability and yields consistent, additional benefitsespecially for coding and multi-step reasoning."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose principled adaptation framework that bridges the gap between Autoregressive (AR) and Block-Diffusion models. By reframing adaptation as continuous interpolation viewing AR as Block-Diffusion model with block size of onewe introduce the context-causal attention mechanism and an efficient parallel training recipe with auxiliary AR supervision, which maximally preserves the pre-trained knowledge of the source model. We also propose block-size growth curriculum that smoothly transitions the model from sequential to parallel generation. Our resulting model, NBDIFF-7B, has achieves state-of-the-art performance among 7B-parameter diffusion models, outperforming strong baselines such as LLaDA [18], Dream [30], and SDAR [7] on math, code, and general reasoning benchmarks. These results demonstrate that expensive pre-training from scratch is not necessary to build high-quality diffusion LLMs. Instead, our method offers compute-efficient pathway to unlock parallel generation capabilities in existing open-source AR checkpoints, potentially accelerating the deployment of faster and more flexible generative models. Future work will explore scaling this paradigm to larger parameter counts and multimodal settings."
        },
        {
            "title": "References",
            "content": "[1] Anonymous. Diffucoder: Understanding and improving masked diffusion models for code generation. In Submitted to The Fourteenth International Conference on Learning Representations, 2025. under review. [2] Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In ICLR, 2025. [3] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In NeurIPS, 2021. [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Hanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei Mi, Mingjian Zhu, Bin Wang, Kaikai Song, Yifei Fu, Xu He, Yu Luo, Chong Zhu, Quan He, Xueyu Wu, Wei He, Hailin Hu, Yehui Tang, Dacheng Tao, Xinghao Chen, and Yunhe Wang. Pangu embedded: An efficient dual-system llm reasoner with metacognition, 2025. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, 11 Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. [7] Shuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, and Bowen Zhou. Sdar: synergistic diffusionautoregression paradigm for scalable sequence generation. arXiv preprint arXiv:2510.06303, 2025. [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [9] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. Scaling diffusion language models via adaptation from autoregressive models, 2025. [10] Xu Han, Xiaoya Li, Shuhe Wang, Tianwei Zhang, Boshi Wang, Yuxian Meng, Jiwei Li, and Fei Wu. Ssd-lm: Semi-autoregressive simplex-based diffusion language modeling. In ACL, 2023. [11] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [12] Xiangqi Jin, Yuxuan Wang, Yifeng Gao, Zichen Wen, Biqing Qi, Dongrui Liu, and Linfeng Zhang. Thinking inside the mask: In-place prompting in diffusion llms, 2025. [13] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. [14] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023. [15] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto. Diffusion-lm improves controllable text generation. In NeurIPS, 2022. [16] Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. [17] Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. [18] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [19] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. [20] Subhendra Sekhar Sahoo, Quentin Anthony, Mengzhou Xia, Wojciech Stokowiec, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. In NeurIPS, 2024. [21] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [22] Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. wd1: Weighted policy optimization for reasoning in diffusion language models, 2025. [23] Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking discrete diffusion models with inference-time scaling, 2025. [24] Guanghan Wang, Yair Schiff, Gilad Turok, and Volodymyr Kuleshov. d2: Improved techniques for training reasoning diffusion language models, 2025. 12 [25] Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing, 2025. [26] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. [27] Chengyue Wu et al. Fast-dllm: Training-free acceleration of diffusion llm by block-wise kv caching and dependency repair. arXiv preprint arXiv:2505.22618, 2025. [28] Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, and Enze Xie. Fast-dllm v2: Efficient block-diffusion large language model. arXiv preprint arXiv:2509.26328, 2025. [29] Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, and Lingpeng Kong. Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models, 2024. [30] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models, 2025. [31] Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. [32] Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning, 2025. [33] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        },
        {
            "title": "Contributors",
            "content": "Yuchuan Tian1, Yuchen Liang2, Jiacheng Sun2, Shuo Zhang2, Guangwen Yang2, Yingte Shu1, Sibo Fang2, Tianyu Guo2, Kai Han2, Chao Xu1, Hanting Chen2, Xinghao Chen2#, Yunhe Wang2# 1 State Key Lab of General AI, School of Intelligence Science and Technology, Peking University. 2 Huawei Technologies. Equal Contribution; Decided by Flipping Coins. Both Orders are Valid. Project Lead. #Corresponding Author."
        },
        {
            "title": "Acknowledgement",
            "content": "We are very grateful to Yulong Li, Xuechun Wang, Renjie Jiang, Chen Chen and Hang Zhou for their generous help."
        }
    ],
    "affiliations": [
        "Huawei Technologies",
        "Peking University"
    ]
}