{
    "paper_title": "Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning",
    "authors": [
        "Chenghao Yang",
        "Lin Gui",
        "Chenxiao Yang",
        "Victor Veitch",
        "Lizhu Zhang",
        "Zhuokai Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 1 5 2 5 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "LET IT CALM: EXPLORATORY ANNEALED DECODING FOR VERIFIABLE REINFORCEMENT LEARNING Lin Gui2*, Lizhu Zhang5, Chenxiao Yang3*, Chenghao Yang1*, Victor Veitch2,4, Zhuokai Zhao5 1Department of Computer Science, University of Chicago 2Department of Statistics, University of Chicago 3Toyota Technological Insitute at Chicago 4Data Science Institute, University of Chicago 5Meta AI {chenghao, glin6}@uchicago.edu, chenxiao@ttic.edu, zhuokai@meta.com *Equal Contribution Codebase Website"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning with verifiable rewards (RLVR) is powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define sequences semantic direction. EAD implements an intuitive explore-at-the-beginning, exploitat-the-end strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixedtemperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers robust path to improving LLM reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning with verifiable rewards (RLVR) is powerful approach to enhance the capabilities of Large Language Models (LLMs) in domains such as mathematical reasoning and code generation (OpenAI, 2024; Guo et al., 2025; Team et al., 2025; Yang et al., 2025). In this framework, an LLM learns by iteratively generating potential solutions (i.e., rollouts), and receiving feedback on its attempts. central challenge lies in guiding language models to explore diverse yet high-quality solutions in their vast output space (Cheng et al., 2025); this reflects the long-standing hard trade-off between exploration and exploitation in RL (Thrun, 1992; Sutton et al., 1998). To achieve effective exploration, the sampling process can be modified to increase the variance of its underlying distribution. However, any such modification involves two fundamental challenges. First, it must preserve sample quality. Increasing diversity at the cost of generating low-quality, nonsensical outputs is counterproductive. Second, it must ensure training stability. Modifying the sampler creates discrepancy between the behavior policy (used for sampling) and the target policy (being optimized), which necessitates an importance sampling (IS) correction in the gradient update (Degris et al., 2012). If the probability ratio in the IS weight is too large, the gradients can have high variance, destabilizing the entire training process (Schulman et al., 2017). An ideal exploration"
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "technique must therefore increase diversity while keeping the sampling distribution close enough to the target policy to allow for stable learning (Haarnoja et al., 2018; Ziegler et al., 2019). widely adopted and principled way to this trade-off is to adjust the sampling temperature (Ackley et al., 1985; Hou et al., 2025). Beyond its implementation simplicity, this method is variationally optimal, that is, maximizing entropy (thereby increasing diversity) while bounding the KL divergence from the target policy (Jaynes, 1957). However, relying on single fixed temperature creates tension: high temperature promotes diversity but produces nonsensical text (Renze, 2024; Wang et al., 2025b), whereas low temperature improves quality but limits exploration, leading to generic and repetitive outputs (Holtzman et al., 2020; Guo et al., 2025). In this work, we propose Exploratory Annealed Decoding (EAD), strategy that improves the balance of this trade-off by leveraging key insight into sequential generation: exploration is not equally valuable at every step. The initial tokens shape sequences semantic direction and structure, making early exploration crucial for discovering diverse valid solutions. Later tokens, however, fill in details within the established context, where excessive exploration can harm coherence. This insight motivates our core strategy: explore at the beginning, exploit at the end. This simple principle elegantly addresses the twin challenges of quality and stability. Injecting randomness early promotes diverse, high-level exploration, while reducing it later ensures completions are both coherent and close to the target policyan essential property for stable off-policy learning. In summary, our contributions are as follows: ① We propose EAD, simple and effective exploration strategy for RLVR that dynamically anneals temperature to encourage meaningful diversity while maintaining high sample quality. ② We show EAD is plug-and-play enhancement that improves sample efficiency over temperature sampling, delivering robust gains across various RLVR algorithms including GRPO (Shao et al., 2024), DAPO (Yu et al., 2025), and EntropyMech (Cui et al., 2025) on both small and larger models. ③ We show that EAD can be adapted for test-time inference, where tuned temperature schedule further enhances generation quality."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "Notations. Let be prompt from dataset D, and let = (y1, . . . , yy) be generated response sequence, where y<t denotes the prefix (y1, . . . , yt1). We define an LLM as policy πθ parameterized by θ, and denote the reference policy, i.e., the starting point for RL, as πref . The probability of generating given is defined autoregressively as πθ(y x) = (cid:81)y t=1 πθ(yt [x, y<t]). reward model R(x, y) evaluates the quality of prompt-response pair; for our RLVR experiments, we use the rule-based Math-Verify reward model.1 Finally, we use to denote the length of sequence or the cardinality of set, and use the shorthand 1 : for the set {1, . . . , n}. Reinforcement Learning with Verifiable Rewards (RLVR). The standard objective for Reinforcement fine-tuning of LLMs is to maximize the expected reward over prompt dataset D: max θ J(θ) := ExD,yπθ(x) [R(x, y)] . (1) However, on complex reasoning tasks, learned reward models R(x, y) are prone to reward hacking, where they assign high scores to plausible but incorrect solutions (Gao et al., 2023; Perez et al., 2023; Weng, 2024; Wang et al., 2025a). RLVR addresses this by replacing the learned model with verifiable, rule-based reward signalsuch as verifier that provides binary feedback on solutions correctness (Guo et al., 2025). This ensures that the policy is optimized using reliable signal. RLVR is commonly implemented using policy gradient algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017). However, standard PPO often requires complex token-level advantage estimation and separate value model. To better suit RLVRs trajectory-level binary rewards, subsequent methods simplify the advantage calculation (Shao et al., 2024; Yu et al., 2025). prominent example is Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) (Yu 1https://github.com/huggingface/Math-Verify"
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "et al., 2025), which optimizes: JDAPO(θ) = xD,y(1:G) iid πθold (x) (cid:80)G 1 i=1 y(i) (cid:88) y(i) (cid:88) i=1 t= s.t. 0 < (cid:12) (cid:12){y(i) : y(i) is correct} (cid:12) (cid:12) (cid:12) (cid:12) < G, min (cid:110) r(i) (θ)Ai, clip (cid:16) r(i) (θ), 1 εlow, 1 + εhigh (cid:17) Ai (cid:111) (θ) = πθ(y(i) where πθold refers to previous policy and r(i) . The asymmetric bound εhigh > πθold (y(i) εlow is proposed to relax the restriction on probability increase and encourage more exploration in the training. The advantage Ai is computed by normalizing the binary rewards across batch of responses, thus avoiding the need for value model: [x,y(i) <t]) [x,y(i) <t]) Ai = Ri meank1:G(Rk) stdk1:G(Rk) , where Ri = R(x, y(i)). Temperature. Temperature sampling (Ackley et al., 1985) is widely used method to control the stochasticity of the policy πθ. At each generation step t, the LLM computes vector of logits, h, over the vocabulary based on the prompt and the preceding tokens y<t. Temperature sampling rescales these logits with parameter τ > 0 before applying the softmax function to form the next-token probability distribution: πθ(yt = [x, y<t]; τ ) = exp(hv/τ ) vV exp(hv/τ ) (cid:80) , where is token in the vocabulary and hv is its corresponding logit. The temperature τ directly modulates the sharpness of the output distribution. higher temperature (τ > 1) flattens the distribution, increasing output diversity by making less likely tokens more probable. Conversely, lower temperature (τ < 1) sharpens it, leading to more deterministic, greedy outputs. Pass@k. Pass@k measures the probability that at least one of independent outputs from language model is correct. Let px denote the underlying accuracy of the language model π given one prompt x. The pass@k accuracy is defined as ExD (cid:2)1 (1 px)k(cid:3) . The inner term 1 (1 px)k quickly approaches one unless px is near zero. For example, when = 64, any px 0.0695 already yields probability of at least 0.99. large gap between pass@1 and pass@k suggests that for some prompts, px is essentially zero. High diversity may benefit this metric because model with diverse outputs is more likely to maintain non-negligible px values across prompts, leading to stronger pass@k performance. Entropy. Given prompt and prefix y<t, the token-level entropy at step is defined over the policys conditional distribution: H(Yt [x, y<t]; θ) := (cid:88) vV πθ(yt = [x, y<t]) log πθ(yt = [x, y<t]), where the sum is over all tokens in the vocabulary . The average entropy of the policy, H(πθ), is the expected token-level entropy over all prompts and generation steps. We can estimate this value empirically using Monte Carlo sampling. For each prompt D, we generate i.i.d. responses y(1), . . . , y(G). The average entropy is then approximated by averaging the token-level entropies across all generated tokens: H(πθ) 1 (cid:88) xD (cid:32) (cid:80)G i=1 (cid:80)y(i) t=1 H(Yt [x, y(i) (cid:80)G i=1 y(i) <t]; θ) (cid:33) ."
        },
        {
            "title": "3 SEQUENTIAL EXPLORATION: EXPLORE EARLY, EXPLOIT LATE",
            "content": "Exploration is cornerstone of reinforcement learning, enabling agents to discover high-quality policies rather than settling on suboptimal solutions (Sutton et al., 1998; Ladosz et al., 2022). This principle becomes particularly vital in deep RL, where vast action spaces render exhaustive search infeasible. In the context of RL for language models (e.g., RLVR), insufficient exploration often manifests as entropy collapse, i.e., premature narrowing of the generation distribution during training (Yu et al., 2025; Wang et al., 2025b; Cui et al., 2025). common simple tool to encourage exploration is temperature sampling. However, fixed temperature imposes difficult trade-off. high temperature promotes diversity (as indicated by increased entropy2), but it risks degrading output quality with nonsensical tokens and hallucinations (Renze, 2024; Wang et al., 2025b). In contrast, low temperature limits the discovery of novel solutions, leading to generic and repetitive outputs (Holtzman et al., 2020; Guo et al., 2025). The key to resolving this dilemma lies not in finding single best temperature, but in recognizing that exploration requirements vary throughout the generation process. This key insight stems directly from the autoregressive nature of language models. At the beginning of sequence, the context is minimal and uncertainty is high, allowing wide range of valid continuations. As more tokens are produced, the context becomes increasingly specific, constraining subsequent choices. the data This intuition is supported by information theory: processing inequality (Shannon, 1948) states that expected conditional entropy tends to decrease with each step3: Figure 1: Average entropy shrinks with output positions for Llama-38B-Instruct on MMLU dataset. H(Yt[x, Y<t]; θ) = Ey<t (cid:2)H(Yt[x, y<t]; θ)(cid:3) (cid:125) (cid:123)(cid:122) Expected entropy at step (average over all prefixes y<t) (cid:124) Ey<t+1 (cid:124) (cid:2)H(Yt+1[x, y<t+1]; θ)(cid:3) (cid:125) (cid:123)(cid:122) Expected entropy at step t+1 (average over all prefixes y<t+1) = H(Yt+1[x, Y<t+1]; θ) We further validate this empirically by examining position-wise entropy trend on the MMLU dataset (Hendrycks et al., 2021)4 with Llama-3-8B-Instruct (Grattafiori et al., 2024) (see Fig. 1). This entropy decay has direct performance implication: effective exploration, producing diverse, highquality responses, is most beneficial in an uncertain, high-entropy phase at the start of generation. To verify this, we replicate controlled forking experiment (Yang & Holtzman, 2025) on DeepSeek-Distilled Llama3-8B (Guo et al., 2025), representative RLVR-fine-tuned model from the same Llama-3 family. As shown in Fig. 2, trajectories branched from early generation steps consistently outperform those branched later. This finding is also consistent with observations from inference-time analysis, where forced, late-stage exploration tends to degrade output quality (Liao et al., 2025; Yang & Holtzman, 2025; Fu et al., 2025). Figure 2: forking experiment on DeepSeek-Llama-3 shows early branching (high-entropy region) yields higher Maj@k on MMLU than late branching (low-entropy region). Aligning our strategy with the natural dynamics of generation, we arrive at simple yet powerful design principle: explore early and exploit late. 2See for the proof. 3While specific rollouts may have late high-entropy positions, the probability of this is exponentially small with position (Yang & Holtzman, 2025), making the overall trend reliable heuristic. 4We use MMLU as held-out dataset with Chain-of-Thought prompting (Wei et al., 2022) to incentivize longer reasoning outputs, aligning with typical RLVR scenario."
        },
        {
            "title": "4 A METHOD FOR SEQUENTIAL EXPLORATION",
            "content": "To put the principle of explore early, exploit late into practice, we introduce Exploratory Annealed Decoding (EAD), which uses an annealed temperature schedule starting from higher-than-standard initial temperature (i.e., τ > 1). To adapt this strategy to RLVR, we further incorporate global-stepaware decay rate, ensuring that the temperature schedule remains effective as the typical response length increases during training. Exploratory Annealed Decoding. Instead of fixed temperature, our method dynamically adjusts the temperature τt for each token in rollout. The schedule starts at high temperature τmax > 1 and decreases progressively throughout the generation process. Specifically, we sample the t-th token for one rollout with the token-level temperature τt = max{1 + τmax et/d, τmin}, where we apply the annealed schedule with decay rate controlling the annealing speed. As illustrated in Fig. 3, the decay rate controls how long the policy remains in high-exploration state. larger front-loads exploration across more initial tokens, while smaller transitions to exploitation more quickly. In practice, we let τt = 1.0 for < c, where is predetermined initial position for the sake of model-specific or prompt-specific template tokens injected in the training process. During RLVR, language models tend to generate some template tokens such as lets verify step by steps or repeat the question. We fix the temperature at τ = 1.0 in this part to avoid interfering with the generation process. Figure 3: The annealing schedule with different decay rates d. larger slows the cooling, front-loading exploration over more tokens. We set = 10, τmax = 1.2, τmin = 0.1 for illustration. Global-Step-Aware Decay Rate. As training progresses and response lengths increase,5 the decay rate should be adjusted in accordance with the training step. Otherwise, an excessive number of tokens may be generated under extremely low temperatures, which degrades response quality and leads to undesirable behaviors such as repetition (Guo et al., 2025), off-topic drift (Spataru et al., 2024), and unnecessary verbosity (Holtzman et al., 2020). In particular, we adopt the following global-step-aware decay rate: ds = min(d0 + 5s, 40000). Ensuring Stability with Truncated Importance Sampling. With aggressive annealing schedules (e.g., very small τmin and d), sampling low-probability, long-tail tokens can cause the annealed policy to deviate significantly from the one being optimized. This creates an off-policy discrepancy that risks training instability. To mitigate this, we employ truncated importance sampling (TIS) (Heckman et al., 1998; Hilton et al., 2022; Yao et al., 2025) to correct the objective, ensuring stable optimization even under highly exploratory schedules (see for details). Overall, this annealed decoding strategy offers compelling combination of effectiveness and efficiency. As plug-and-play modification to standard temperature sampling, it incurs negligible computational overhead and is fully compatible with existing RLVR pipelines and diverse policy optimization algorithms like DAPO, GRPO, etc. 5We illustrate increased length for EAD in D."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Models, Data, and Training Frameworks. To ensure rigorous and controlled comparison, we follow the Minimal-RL recipe (Xiong et al., 2025),6 training all models on the Numina-Math dataset (Beeching et al., 2024), which contains 860k math prompts. To assess the generality of our method, we experiment with both Qwen-2.5-Math-1.5B (Yang et al., 2024) and Llama-3.2-1BInstruct (Dubey et al., 2024).7 We also include the larger Qwen-2.5-Math-7B model to evaluate how our approach scales. While our primary experiments are conducted within the DAPO framework (Yu et al., 2025), we demonstrate broader applicability by additionally integrating EAD with GRPO (Shao et al., 2024) and EntropyMech (Cui et al., 2025). Baselines and Controlled Comparison. We evaluate EAD against fixed-temperature sampling, standard and strong baseline, using temperatures τ {0.6, 1.0, 1.2} as recommended by prior work (Renze, 2024; Guo et al., 2025; Hou et al., 2025). For fair comparison focused specifically on the sampling strategy, we disable two orthogonal techniques for all methods: (1) dynamic data sampling (Yu et al., 2025), to maintain consistent training set for all runs, and (2) rollout length penalties, to avoid confounding the reward signal with length-based biases. Hyperparameters. Unless otherwise stated, we use default configuration of τmax = 1.2 and d0 = 25 for EAD. For τmin, we observed optimal values varied by model capability. For the 1B and 1.5B models, we set τmin = 0.1. For the more capable 7B model, we found that an overly low temperature could lead it to generate plausible but incorrect solutions; we therefore used higher value of τmin = 0.8 to mitigate this effect. All hyperparameters are tuned based on prior study over held-out datasets. 5.2 EAD IMPROVES RLVR TRAINING EAD Improves RL Exploration and Training Efficiency. As shown in Fig. 4, EAD significantly improves training efficiency. For Pass@16 accuracy, EAD (w/o TIS) consistently outperforms the baselines on the Llama and Qwen models (EAD (w/ TIS) also outperforms on the Llama model), demonstrating more effective exploration. Under the stricter Worst@16 metric, the inclusion of TIS becomes essential for maintaining stable performance gains, highlighting its importance for correcting the offpolicy training dynamic introduced by EAD. Through bootstrapping evaluation as in Hochlehnert et al. (2025), the standard deviation of both Pass@16 performance and worst@16 are way below 0.01 and thus all comparisons here are significant. Figure 5: Pass@16 performance on Qwen2.5-Math-7B. EAD enables better exploration than fixed-temperature sampling, yielding sustained gains in Pass@16 throughout training. To verify that our method generalizes, we evaluated it on the larger Qwen-2.5-Math-7B model. The results, presented in Fig. 5, confirm that the performance gains from EAD remain significant. This demonstrates that our approach is effective not only on smaller models but also scales successfully. EAD Mitigates Entropy Collapse. One major problem in RLVR training is entropy collapse (Cui et al., 2025), which causes the exploration space to shrink and constrains improvement during the plateau stage (Deng et al., 2025). We plot the entropy dynamic in Fig. 6, where we can see that the entropy dynamic for EAD-empowered methods is not monotonically decreasing from the beginning. Instead, it tries to gradually transition out from local opti6More training details and hyperparameter setups are illustrated in A. 7We also experimented with the Llama-3.2-1B base model. However, consistent with Wang et al. (2025d), we found that applying RL to base models without intermediate domain-specific fine-tuning yields limited gains across all methods. We defer deeper investigation to future work."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "Figure 4: Pass@16 and Worst@16 performance evaluation in RL training. While EAD improves exploration of high-quality samples (even the worst outperform temperature sampling), the gain diminishes over time; importance sampling can supplement to correct bias and sustain training. Figure 6: Entropy Dynamics in RL Training. Under commonly-used temperature sampling, trained with RL algorithm would make entropy decrease, sharply shrinking the exploration space for RL from beginning. While EAD could help RL algorithm to escape local minimum and do exploration when needed in the middle of RL training. mum (Kirkpatrick et al., 1983; Bertsimas & Tsitsiklis, 1993) in natural, continuous way without any external intervention, such as introducing tree search in rollout sampling (Li et al., 2025). Sample efficiency of EAD. Increasing the number of rollouts is common but computationally expensive strategy to enhance exploration (Hou et al., 2025). We test the sample efficiency of EAD by varying the number of rollouts, adjusting the learning rate accordingly as suggested by prior work (Chen et al., 2025). As shown in Fig. 7, while more rollouts can further improve performance, EAD achieves strong results with just 4 or 8 rollouts. For instance, the optimal configuraFigure 7: EAD would bring further performance improvement via increased numbers of rollouts, but the commonly used 4 or 8 is already good enough."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "Figure 10: EAD is compatible with various RL algorithms and can significantly improve the model performance over time. tions are = 8 with learning rate of 106 for Llama-3.2-1B-Instruct and 2 106 for Qwen2.5-Math-1.5B. This highlights the sample efficiency of our approach, offering way to reduce the computational cost of the rollout phase. To assess whether EADs advantage persists with extensive exploration, we compare it against baselines using larger set of 16 rollouts. Fig. 8 shows that although the relative performance gain diminishes, EAD still outperforms fixed-temperature baselines by clear margin. 5.3 EAD IMPROVES INFERENCE-TIME SCALING Figure 8: When scaling out the rollout number to 16, the relative advantages of our methods diminished; however, it still outperforms traditional same-temperature sampling. To understand whether the success of EAD in RL training is driven by its ability to generate high-quality samples, we conduct an evaluation at inference time. This experiment is designed to isolate the sampling strategys effectiveness from the dynamics of RL optimization (Berseth, 2025). Using off-the-shelf Qwen-2.5 models without any fine-tuning, we compare EAD against fixed-temperature sampling. We use majority voting (Majority@N ) to measure how performance scales with the number of samples (Wang et al., 2023; Snell et al., 2024). As shown in Fig. 9, EAD consistently improves over the baseline for most values of . This result confirms that EADs advantage stems from its inherent capacity to discover higher-quality solutions, even without any training. Figure 9: Inference-Time Scaling Evaluation for Different Decoding Methods using off-the-shelf Qwen2.5 models. We could see that EAD improves traditional temperature sampling. We set τmax = 1.2, τmin = 0.1, = 25 for EAD. 5.4 EAD IS COMPATIBLE WITH VARIOUS RL ALGORITHMS To demonstrate that EAD is general, plug-and-play exploration strategy, we evaluate its performance when integrated into two other prominent RL algorithms: GRPO (Shao et al., 2024) and EntropyMech (Cui et al., 2025). These algorithms provide diverse testbeds. GRPO is more conservative, constraining policy updates with KL divergence penalty and stricter clipping mechanism that can limit exploration (Yu et al., 2025), while EntropyMech uses specialized token-clipping mechanism to mitigate entropy collapse. As shown in Fig. 10, EAD consistently outperforms fixed-temperature sampling in both frameworks. These results confirm the broad applicability of our method as an improved exploration strategy across different RL algorithms."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Reinforcement Learning with Verifiable Rewards. Recent large-scale reasoning models such as OpenAI o1 (OpenAI, 2024), DeepSeek-R1 (Guo et al., 2025) have demonstrated that reinforcementlearning-based post-training can substantially enhance LLM reasoning. Motivated by this reinforcement learning with verifiable rewards (RLVR) (e.g. Shao et al. (2024); Guo et al. (2025); Lambert et al. (2024); Yang et al. (2025); Hu et al. (2025); Yu et al. (2025); Guan et al. (2025); Zeng et al. (2025) among many other works) has become major approach for post-training LLMs to improve reasoning. broad literature studies how to make RLVR training effective and efficient at scale, including novel reinforcement learning algorithms and objectives (Yu et al., 2025; Liu et al., 2025b; Yue et al., 2025b; Zheng et al., 2025a), verifier architecture and reward deigns (Zuo et al., 2025; Zhao et al., 2025b; Agarwal et al., 2025; Prabhudesai et al., 2025), and mechanisms that manage exploration diversity and entropy (Cheng et al., 2025; Chen et al., 2025; Cui et al., 2025; Wang et al., 2025b), or takes critical view on the current evaluation (Yue et al., 2025a; Zhao et al., 2025a; Hochlehnert et al., 2025). Despite steady progress, fundamental challenge is to balance exploration and exploitation along long reasoning trajectories without brittle heuristics. We adopt simple yet effective annealed sampling schedule that front-loads exploration and cools later steps during rollout to encourage exploration while keeping training stable. Exploration Control in RLVR. line of works that is close to our work studies how to control exploration and sampling while doing RLVR (Hou et al., 2025; Cheng et al., 2025; Chen et al., 2025; Cui et al., 2025; Wang et al., 2025b; Xiong et al., 2025; Deng et al., 2025; Xu et al., 2025; Zheng et al., 2025b; Dou et al., 2025; Li et al., 2025). In particular, Cheng et al. (2025) propose per-token entropy to focus exploration at branching tokens in sampling; Chen et al. (2025) transform per-prompt rewards to optimize pass@k, guiding exploration across samples; Cui et al. (2025) control high-covariance tokens to prevent entropy collapse and sustain exploration; Wang et al. (2025b) update only high-entropy tokens, concentrating exploration where decisions split. Different from prior work, this paper presents the first systematic analysis of sampling temperature and introduce purely sampling-level annealed schedule that encourages exploration and then progressively stabilize answers, thereby enabling discovery of new solutions while yielding more stable training. Simulated Annealing. Simulated Annealing (SA) is probabilistic optimization technique inspired by annealing in metallurgy, designed to find the global optimum in large search space (Kirkpatrick et al., 1983; Bertsimas & Tsitsiklis, 1993). The core principle involves temperature parameter that controls the probability of accepting suboptimal states. Initially, high temperature allows the search to escape local minima by exploring broadly (exploration). As the temperature gradually decreases, the algorithm increasingly favors better states, converging towards high-quality solution (exploitation). This coarse-to-fine search dynamic, where high temperatures establish solutions general structure and low temperatures refine its details, strongly parallels the generative process of LLMs (Yang & Holtzman, 2025). SA has been adapted in various machine learning contexts to manage the exploration-exploitation trade-off, including recent applications in graph optimization (Liu et al., 2021), text editing (Zhang et al., 2024), non-autoregressive generation (Israel et al., 2025), and efficient Best-of-N sampling (Manvi et al., 2024). However, these prior applications invariably apply single, uniform temperature across all positions in generated sequence. This approach fails to account for the heterogeneous roles of tokens at different positions (Wang et al., 2025b). Our work departs from this convention. To the best of our knowledge, we are the first to introduce an intra-sequence annealed temperature schedule, where the temperature varies dynamically within the generation of single sequence. This novel approach allows for more nuanced control over exploration and leads to significant performance gains in RLVR."
        },
        {
            "title": "7 DISCUSSION",
            "content": "Our work addresses central challenge in RLVR: achieve an effective balance between exploration and exploitation. We introduce Exploratory Annealed Decoding (EAD), simple yet powerful sampling strategy that avoids heavy computation and intricate heuristics. Specifically, EAD employs temperature-annealing schedule that begins with high sampling temperature and gradually cools, enabling LLMs to explore broadly at the beginning of generation and converge toward precise, high-quality completions throughout the decoding process. As RLVR often relies on multiple rollouts"
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "to estimate rewards, this annealing schedule effectively improves sampling diversity while controlling variance, making it well suited for RL training. At the same time, EAD can also be applied directly at test time to enhance inference efficiency and scaling, improving the quality of singleor multi-sample decoding without additional computation cost. Despite the encouraging results, our study has several limitations that suggest directions for future work. First, the scaling behavior of our method is not fully explored because of limited computational resources. We adopt the current settings with reference to (Xiong et al., 2025; Shao et al., 2025; Wang et al., 2025c), and argue that the efficacy of the method is still convincing, as we evaluate it across diverse model structures (LLaMA and Qwen) and multiple model sizes. systematic scaling study remains an important next step. Second, while EAD is designed as complementary component, comprehensive study combining it with other advanced exploration-promoting RLVR algorithms (see 6) remains promising direction for future work. Third, our current experiments adopt uniform temperature schedule for all prompts. Although an adaptive schedule tailored to individual prompts could potentially enhance performance, developing such mechanism is nontrivial. In RLVR, training is iterative, so any prior information about prompt distributions may shift during optimization, and collecting extra statistics (e.g., token-wise entropy quantile (Wang et al., 2025b), or probability-advantage covariance (Cui et al., 2025)) to track these changes for every prompt would add computational overhead and system complexity (Li et al., 2025; Liu et al., 2025a). For these reasons, we focus on the vanilla schedule to test the core efficacy of our method, leaving adaptive scheduling for future investigation. In summary, EAD provides simple yet general way to couple exploration with the inherent progression of language generation. By reducing algorithmic overhead while improving trajectory quality, it opens new avenues for both efficient inference and effective reinforcement fine-tuning."
        },
        {
            "title": "REFERENCES",
            "content": "David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. learning algorithm for boltzmann machines. Cognitive Science, 9(1):147169, 1985. ISSN 0364-0213. doi: https://doi.org/ 10.1016/S0364-0213(85)80012-4. URL https://www.sciencedirect.com/science/ article/pii/S0364021385800124. Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Edward Beeching, Shengyi Costa Huang, Albert Jiang, Jia Li, Benjamin Lipkin, Zihan Qina, Kashif Rasul, Ziju Shen, Roman Soletskyi, and Lewis Tunstall. Numinamath 7b cot. https:// huggingface.co/AI-MO/NuminaMath-7B-CoT, 2024. Glen Berseth. Is exploration or optimization the problem for deep reinforcement learning? arXiv preprint arXiv:2508.01329, 2025. Dimitris Bertsimas and John Tsitsiklis. Simulated annealing. Statistical science, 8(1):1015, 1993. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Thomas Degris, Martha White, and Richard Sutton. Off-policy actor-critic. arXiv preprint arXiv:1205.4839, 2012. Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng Gao, Wayne Xin Zhao, and Ji-Rong Wen. From trial-and-error to improvement: systematic analysis of llm exploration mechanisms in rlvr. arXiv preprint arXiv:2508.07534, 2025. Shihan Dou, Muling Wu, Jingwen Xu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Improving rl exploration for llm reasoning through retrospective replay. arXiv preprint arXiv:2504.14363, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018. James Heckman, Hidehiko Ichimura, and Petra Todd. Matching as an econometric evaluation estimator. The review of economic studies, 65(2):261294, 1998. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob In International ConferSteinhardt. Measuring massive multitask language understanding. ence on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Jacob Hilton, Karl Cobbe, and John Schulman. Batch size-invariance for policy optimizaIn S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Adtion. vances in Neural Information Processing Systems, volume 35, pp. 1708617098. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/6ceb6c2150bbf46fd75528a6cd6be793-Paper-Conference.pdf. Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rygGQyrFvH. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. T1: Advancing language model reasoning through reinforcement learning and inference scaling. In Forty-second International Conference on Machine Learning, 2025. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Daniel Israel, Aditya Grover, and Guy Van den Broeck. Enabling autoregressive models to fill in masked tokens. arXiv preprint arXiv:2502.06901, 2025. Edwin Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957. Scott Kirkpatrick, Daniel Gelatt Jr, and Mario Vecchi. Optimization by simulated annealing. science, 220(4598):671680, 1983. Pawel Ladosz, Lilian Weng, Minwoo Kim, and Hyondong Oh. Exploration in deep reinforcement learning: survey. Information Fusion, 85:122, 2022. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, et al. Treepo: Bridging the gap of policy optimization and efficacy and inference efficiency with heuristic tree-based modeling. arXiv preprint arXiv:2508.17445, 2025. Baohao Liao, Xinyi Chen, Sara Rajaee, Yuhui Xu, Christian Herold, Anders Søgaard, Maarten de Rijke, and Christof Monz. Lost at the beginning of reasoning. arXiv preprint arXiv:2506.22058, 2025. Xianggen Liu, Pengyong Li, Fandong Meng, Hao Zhou, Huasong Zhong, Jie Zhou, Lili Mou, and Sen Song. Simulated annealing for optimization of graphs and sequences. Neurocomputing, 465: 310324, 2021."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "Zheng Liu, Mengjie Liu, Siwei Wen, Mengzhang Cai, Bin Cui, Conghui He, and Wentao Zhang. From uniform to heterogeneous: Tailoring policy optimization to every tokens nature. arXiv preprint arXiv:2509.16591, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Rohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024. OpenAI. reason with learning-to-reason-with-llms/, 2024. Accessed: 2025-05-01."
        },
        {
            "title": "Learning",
            "content": "llms. to https://openai.com/index/ Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. In Findings of the association for computational linguistics: ACL 2023, pp. 1338713434, 2023. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. Matthew Renze. The effect of sampling temperature on problem solving in large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 73467356, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.432. URL https://aclanthology.org/2024.findings-emnlp.432/. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Claude Shannon. mathematical theory of communication. The Bell system technical journal, 27 (3):379423, 1948. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Vaishnavi Shrivastava, Ahmed Awadallah, Vidhisha Balachandran, Shivam Garg, Harkirat Behl, and Dimitris Papailiopoulos. Sample more to think less: Group filtered policy optimization for concise reasoning. arXiv preprint arXiv:2508.09726, 2025. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Ava Spataru, Eric Hambro, Elena Voita, and Nicola Cancedda. Know when to stop: study of semantic drift in text generation. arXiv preprint arXiv:2404.05411, 2024. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Sebastian Thrun. Efficient exploration in reinforcement learning. Carnegie Mellon University, 1992. Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, et al. Beyond reward hacking: Causal rewards for large language model alignment. arXiv preprint arXiv:2501.09620, 2025a."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025c. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025d. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024. URL https://lilianweng.github.io/posts/2024-11-28-reward-hacking/. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. Yixuan Even Xu, Yash Savani, Fei Fang, and Zico Kolter. Not all rollouts are useful: Down-sampling rollouts in llm reinforcement learning. arXiv preprint arXiv:2504.13818, 2025. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Chenghao Yang and Ari Holtzman. How alignment shrinks the generative horizon. arXiv preprint arXiv:2506.17871, 2025. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https: //fengyao.notion.site/off-policy-rl. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025a. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025b. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Shimao Zhang, Yu Bao, and Shujian Huang. Edt: Improving large language models generation by entropy-based dynamic temperature sampling. arXiv preprint arXiv:2403.14541, 2024."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025a. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025b. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025a. Haizhong Zheng, Yang Zhou, Brian Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts. arXiv preprint arXiv:2506.02177, 2025b. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "A MINIMAL-RL TRAINING DETAILS We mainly follow the Minimal-RL recipe (Xiong et al., 2025) in our experiments to ensure fair comparison among different rollout sampling strategies. Specifically, we set series of hyperparameters as in Table 1:"
        },
        {
            "title": "Hyperparameter",
            "content": "Value(s)"
        },
        {
            "title": "Training Batch Size\nMax Prompt Length\nMax Response Length\nMini Batch Size\nMicro Batch Size Per GPU\nLearning Rate",
            "content": "1024 1024 3072 256 4 106 Table 1: Hyperparameter Setup for Running Minimal-RL recipe. OFF-POLICY ISSUE AND TRUNCATED IMPORTANCE SAMPLING"
        },
        {
            "title": "CORRECTION",
            "content": "B.1 SAMPLING TECHNIQUES CAN INTRODUCE OFF-POLICY ISSUE One subtle yet troublesome drawback of reinforcement learning with sampling techniques is that it simultaneously introduces the off-policy problem: there is gap between the behavior policy (used for sampling) and the target policy (being optimized and parametrized by θ). This might introduce instability to the training and cause it to fail (See example training of RLVR with EAD Fig. 11). (a) Clip Fraction Surge (b) Gradient Norm Surge. (c) Drastic Best@16 Drop Figure 11: Off-policy samples bring training instability. The base model is Qwen2.5-Math-1.5B. Noted that this off-policy phenomenon widely exists for any efficient sampling framework (Yao et al., 2025) and sampling strategy (for instance, when applying best-of-n sampling (Xiong et al., 2025) or filtering out responses (Shrivastava et al., 2025), the underlying distribution of responses is implicitly altered). To be more precise, we take the policy gradient loss as an example: xD,yπsampling θold (cid:20) πθ(y x) πθold(y x) (cid:21) A(y; x) (x) = ExD,yπθold (x) (cid:34) (y x) πsampling θold πθold(y x) πθ(y x) πθold(y x) (cid:35) A(y; x) , where πsampling implicitly added to each response in addition to its advantage A(y; x). ( x) represents the underlying sampling distribution. In such case, an extra weight is θold This extra weight can significantly inflate the variance of the policy gradient, posing stability challenge that our proposed EAD needs to mitigate. To quantify this effect in our proposed EAD, we now analyze such variance under standard, fixed temperature sampling. We use τ = 1 to define policy π and consider the effect of τ on the variance of the gradient estimator. We reduce the problem to analyzing the variance inflation factor Eyπ(x;τ ) (cid:20) π(y x; 1)2 π(y x; τ ) (cid:21) . 16 (2)"
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "We begin with one-token case. Let oi denote the ith token in the vocabulary and hi is its logit. Then equation 2 can be rewritten as (cid:88) i=1 hi/((cid:80)V /((cid:80)V j=1 hj) j=1 h1/τ h1/τ ) hi (cid:80)V j=1 hj = (cid:80)V i=1 h21/τ (cid:16)(cid:80)V (cid:80)V i=1 h1/τ (cid:17)2 i=1 hi Proposition B.1. Suppose hi [0, 1] for all . (cid:80)V τ 1 and increasing when τ 1, which implies it has global minimum at τ = 1. i=1 h21/τ i=1 h1/τ (cid:80)V is decreasing when Proof. Let = 1/τ . We define (x) = log h2x + log (cid:88) i=1 (cid:88) hx . i="
        },
        {
            "title": "Its derivative is",
            "content": "f (x) = (cid:80)V i=1 h2x (cid:80)V i=1 h2x log hi + (cid:80)V i=1 hx (cid:80)V log hi . i=1 hx (cid:80)V log hi i=1 hx (cid:80)V i=1 hx To analyze the sign of (x), we define helper function g(x) = . Then, (x) = g(x) g(2 x) and its sign depends on whether g(x) is greater than, less than, or equal to g(2 x). We take look at derivative of g: g(x) = (cid:16)(cid:80)V i=1 hx (log hi)2(cid:17) (cid:16)(cid:80)V (cid:16)(cid:80)V i=1 hx i=1 hx (cid:17) (cid:17)2 (cid:16)(cid:80)V i=1 hx log hi (cid:17)2 0. Hence, is an increasing function and (x) = g(x) g(2 x) 0, when 1 (x) = g(x) g(2 x) = 0, when = 1 (x) = g(x) g(2 x) 0, when 1. Accordingly, is increasing when 1 and is decreasing when 1. Then the proposition easily follows. The same conclusion can be proved for multiple-token sequence by induction. Therefore, we get that when the temperature is far from 1, the off-policy issue could be severe and lead to large variance of the gradient estimator. B.2 TRUNCATED IMPORTANCE SAMPLING RATIO CORRECTION To cancel such bias, an importance sampling ratio can be introduced (Hilton et al., 2022; Yao et al., 2025): ExD,yπθold (x) (cid:20) πθ(y x) πθold(y x) (cid:21) A(y; x) = xD,yπsampling θold (cid:34) (x) πθold (y x) πsampling θold (y x) πθ(y x) πθold(y x) (cid:35) A(y; x) . To further prevent negative effects by the extreme likelihood ratios and boost training stability, we truncate the likelihood ratio with an upper bound. That is, truncated importance sampling technique (Heckman et al., 1998). Taking the vanilla policy gradient loss as an example, the modified loss for EAD is as follows: (cid:34) (cid:32) xD,yπEAD θold (x) min πθold(y x) πEAD (y x) θold (cid:33) , ε πθ(y x) πθold (y x) (cid:35) A(y; x) ."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "C PROOF OF TEMPERATURE-ENTROPY RELATIONSHIP Proposition C.1. The entropy of the softmax distribution is non-decreasing function of the temperature τ > 0. Proof. The strategy is to show that the entropy is non-increasing function of the inverse temperature β = 1/τ > 0. The probability of sampling token with temperature τ is given by the policy πθ. For simplicity in the derivation, we denote this probability as pv(τ ): pv(τ ) πθ(yt = [x, y<t]; τ ) Let hv be the logit for token in the vocabulary . The probability of token as function of β is given by: pv(β) = exp(βhv) vV exp(βhv) (cid:80) exp(βhv) Z(β) , where Z(β) is the partition function. The entropy, as function of β, is: H(β) = (cid:88) vV pv(β) log pv(β). We can rewrite the entropy by substituting log pv(β) = βhv log Z(β): H(β) = (cid:88) vV pv(β)(βhv log Z(β)) = log Z(β) (cid:32) (cid:88) (cid:33) pv(β) β vV = log Z(β) β Evp(β)[hv]. (cid:88) vV hvpv(β) Now, we differentiate H(β) with respect to β. Let h(β) = E[hv]. dH dβ (log Z(β)) dβ dβ = (βh(β)). First, we find the derivative of the log-partition function: dβ (log Z(β)) = (β) Z(β) = (cid:80) hv exp(βhv) Z(β) (cid:88) = hvpv(β) = h(β). Next, we use the product rule for the second term: Combining these gives: dβ (βh(β)) = h(β) + β dh dβ . dH dβ = h(β) (cid:18) h(β) + β (cid:19) dh dβ = β dh dβ . The derivative dh dβ is the variance of the logits. We can show this by differentiating h(β): (cid:19) (cid:18) (cid:80) dh dβ dβ ((cid:80) = = = hv exp(βhv) Z(β) v exp(βhv))Z(β) ((cid:80) Z(β)2 hv exp(βhv))Z (β) (cid:88) h2 vpv(β) (cid:32) (cid:88) hvpv(β) (cid:33) (cid:18) (β) Z(β) (cid:19) = E[h2] (E[h])2 = Varvp(β)(hv)."
        },
        {
            "title": "Exploratory Annealed Decoding",
            "content": "Substituting this back, we arrive at the final expression for the derivative of entropy: dH dβ = β Varvp(β)(hv). By definition, the temperature τ > 0, so the inverse temperature β > 0. The variance of any random variable is non-negative. This can be formally shown using Jensens inequality: for the convex function ϕ(x) = x2, we have E[ϕ(h)] ϕ(E[h]), which means E[h2] (E[h])2, and thus Var(h) 0. Therefore, the derivative of entropy with respect to β is non-positive: dH dβ = β (cid:124)(cid:123)(cid:122)(cid:125) 0 Var(hv) (cid:124) (cid:123)(cid:122) (cid:125) 0 0. Since H(β) is non-increasing function of β, and β is inversely proportional to , it follows that H(τ ) must be non-decreasing function of the temperature τ ."
        },
        {
            "title": "D INCREASING LENGTH IN RL TRAINING",
            "content": "During RL training, our algorithm (EAD) incentivizes the model to generate longer, more effective reasoning chains for difficult problems, especially for 7B models  (Fig. 12)  . While both EAD and temperature sampling initially learn to shorten their responses by shifting from complex code-based solutions to direct mathematical reasoning, their behavior later diverges. The response length from temperature sampling stabilizes, whereas EAD learns to selectively increase reasoning length for harder problems, which boosts final performance. For these experiments, EAD is applied in the DAPO algorithm for sampling rollouts. We use the same training setup as detailed in 5. Figure 12: Compared with normal temperature sampling, EAD can naturally incentivize the model to generate longer reasoning chains."
        }
    ],
    "affiliations": [
        "Data Science Institute, University of Chicago",
        "Department of Computer Science, University of Chicago",
        "Department of Statistics, University of Chicago",
        "Meta AI",
        "Toyota Technological Institute at Chicago"
    ]
}