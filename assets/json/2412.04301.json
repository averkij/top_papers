{
    "paper_title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
    "authors": [
        "Trong-Tung Nguyen",
        "Quang Nguyen",
        "Khoi Nguyen",
        "Anh Tran",
        "Cuong Pham"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/"
        },
        {
            "title": "Start",
            "content": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion Trong-Tung Nguyen1 Quang Nguyen1 Khoi Nguyen1 Anh Tran1 Cuong Pham1,2 1VinAI Research 2Posts & Telecom. Inst. of Tech., Vietnam 4 2 0 2 5 ] . [ 1 1 0 3 4 0 . 2 1 4 2 : r Figure 1. SwiftEdit empowers instant, localized image editing using only text prompts, freeing users from the need to define masks. In just 0.23 seconds on single A100 GPU, it unlocks world of creative possibilities demonstrated across diverse editing scenarios."
        },
        {
            "title": "Abstract",
            "content": "io/. Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based textto-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: one-step inversion framework that enables one-step image reconstruction via inversion and mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant textguided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain competitive performance in editing results. Our project page is at https://swift-edit.github. 1. Introduction Recent text-to-image diffusion models [2325, 27] have achieved remarkable results in generating high-quality images semantically aligned with given text prompts. To generate realistic images, most of them rely on multi-step sampling techniques, which reverse the diffusion process starting from random noise to realistic image. To overcome this time-consuming sampling process, some works focus on reducing the number of sampling steps to few (4-8 steps) [27] or even one step [5, 19, 37, 38] via distillation techniques while not compromising results. These approaches not only accelerate image generation but also enable faster inference for downstream tasks, such as image editing. For text-guided image editing, recent approaches [11, 18, 29] use an inversion process to determine the initial noise for source image, allowing for (1) source image reconstruction and (2) content modification aligned with guided text while preserving other details. Starting from this inverted noise, additional techniques, such as attention matrained inversion network and guidance prompts. The mask is then used in our novel attention-rescaling technique to blend and control the edit strength while preserving background elements, enabling high-quality editing results. To the best of our knowledge, our work is the first to explore diffusion-based one-step inversion using one-step text-to-image generation model to instantly perform textguided image editing (in 0.23 seconds). While being significantly fast compared to other multi-step and few-step editing methods, our approach achieves competitive editing result as shown in Fig. 2. In summary, our main contribution includes: We propose novel one-step inversion framework trained with two-stage strategy. Once trained, our framework can invert any input images into an editable latent in single step without further retraining or finetuning. We show that our well-trained inversion framework can produce an editing mask guided by source and target text prompts within single batchified forward pass. We propose novel attention-rescaling technique for mask-based editing, offering flexible control over editing strength while preserving key background information. 2. Related Work 2.1. Text-to-image Diffusion Models Diffusion-based text-to-image models [2325] typically rely on computationally expensive iterative denoising to generate realistic images from Gaussian noise. Recent advances [15, 17, 26, 30] alleviate this by distilling the knowledge from multi-step teacher models into few-step student network. Notable works [5, 14, 15, 19, 30, 37, 38] show that this knowledge can be distilled even into onestep student model. Specifically, Instaflow [14] uses rectified flow to train one-step network, while DMD [38] applies distribution-matching objectives for knowledge transfer. DMDv2 [37] removes costly regression losses, enabling efficient few-step sampling. SwiftBrush [19] utilizes an image-free distillation method with text-to-3D generation objectives, and SwiftBrushv2 [5] integrates posttraining model merging and clamped CLIP loss, surpassing its teacher model to achieve state-of-the-art one-step textto-image performance. These one-step models provide rich prior information about text-image alignment and are extremely fast, making them ideal for our one-step text-based image editing approach. 2.2. Text-based Image Editing Several approaches leverage the strong prior of imagetext relationships in text-to-image models to perform textguided multi-step image editing via an inverse-to-edit approach. First, they invert the source image into informative noise. Methods like DDIM Inversion [29] use Figure 2. Comparing our one-step SwiftEdit with few-step and multi-step diffusion editing methods in terms of background preservation (PSNR), editing semantics (CLIP score), and runtime. Our method delivers lightning-fast text-guided editing while achieving competitive results. nipulation and hijacking [3, 20, 33], are applied at each denoising step to inject edits gradually while preserving key background elements. This typical approach, however, is resource-intensive, requiring two lengthy multi-step processes: inversion and editing. To address this, recent works [6, 8, 31] use few-step diffusion models, like SD-Turbo [28], to reduce the sampling steps required for inversion and editing, incorporating additional guidance for disentangled editing via text prompts. However, these methods still struggle to achieve sufficiently fast text-guided image editing for on-device applications while maintaining performance competitive with multistep approaches. In this work, we take different approach by building on one-step text-to-image model for image editing. We introduce SwiftEdit the first one-step text-guided image editing tool which achieves at least 50 faster execution than previous multi-step methods while maintaining competitive editing quality. Notably, both the inversion and editing in SwiftEdit are accomplished in single step. Inverting one-step diffusion models is challenging, as existing techniques like DDIM Inversion [29] and Null-text Inversion [18] are unsuitable for our one-step real-time editing goal. To achieve this, we design novel one-step inversion framework inspired by encoder-based GAN Inversion methods [34, 35, 39]. Unlike GAN inversion, which requires domain-specific networks and retraining, our inversion framework generalizes to any input images. For this, we leverage SwiftBrushv2 [5], recent one-step textto-image model known for speed, diversity, and quality, using it as both the one-step image generator and backbone for our one-step inversion network. We then train it with weights initialized from SwiftBrushv2 to handle any source inputs through two-stage training strategy, combining supervision from both synthetic and real data. Following the one-step inversion, we introduce an efficient mask-based editing technique. Our method can either accept an input editing mask or infer it directly from the 2 linear approximations of noise prediction, while Nulltext Inversion [18] enhances reconstruction quality through costly per-step optimization. Direct Inversion [11] bypasses these issues by disentangling source and target generation branches. Second, editing methods such as [3, 10, 20, 21, 33] manipulate attention maps to embed edits while preserving background content. However, their multi-step diffusion process remains too slow for practical applications. To address this issue, several works [6, 8, 31] enable few-step image editing using fast generation models [27]. ICD [31] achieves accurate inversion in 3-4 steps with consistency distillation framework, followed by text-guided editing. ReNoise [8] refines the sampling process with an iterative renoising technique at each step. TurboEdit [6] uses shifted noise schedule to align inverted noise with the expected schedule in fast models like SDXL Turbo [28]. Though these methods reduce inference time, they fall short of instant text-based image editing needed for fast applications. Our one-step inversion and one-step localized editing approach dramatically boosts time efficiency while surpassing few-step methods in editing performance. 2.3. GAN Inversion GAN inversion [2, 4, 13, 16, 22, 34, 39] maps source image into the latent space of pre-trained GAN, allowing the generator to recreate the image, which is valuable for tasks like image editing. Effective editing requires latent space that can both reconstruct the image and support realistic edits through variations in the latent code. Approaches fall into three groups: encoder-based [22, 39, 40], optimizationbased [4, 13, 16], and hybrid [1, 2, 39]. Encoder-based methods learn mapping from the image to the latent code for fast reconstruction. Optimization-based methods refine code by iteratively optimizing it, while hybrid methods combine both, using an encoders output as initialization for further optimization. Inspired by encoder-based speed, we develop one-step inversion network, but instead of GAN, we leverage one-step text-to-image diffusion model. This allows us to achieve text-based image editing across diverse domains rather than being restricted to specific domain as in GAN-based methods. 3. Preliminaries Multi-step diffusion model. Text-to-image diffusion model ϵϕ attempts to generate image ˆx given the target prompt embedding cy (extracted from the CLIP text encoder of given text prompt y) through iterative denoising steps, starting from Gaussian noise, zT = ϵ (0, I): zt1 = zt σtϵϕ(zt, t, cy) αt + δtϵt, ϵt (0, I), (1) produce the image ˆx = D(z). One-step diffusion model. The traditional diffusion models sampling process requires multiple steps, making it time-consuming. To address this, one-step text-toimage diffusion models like InstaFlow [14], DMD [38], DMD2 [37], SwiftBrush [19], and SwiftBrushv2 [5] have been developed, reducing the sampling steps to single step. Specifically, one-step text-to-image diffusion model aims to transform noise input ϵ (0, 1), given text prompt embedding cy, directly into an image latent ˆz, without iterative denoising steps, or ˆz = G(ϵ, cy). SwiftBrushv2 (SBv2) stands out in one-step image generation by quickly producing high-quality, diverse outputs, forming the basis of our approach. Building on its predecessor, SBv2 integrates key improvements: it uses SD-Turbo initialization for enhanced output quality, clamped CLIP loss to strengthen visual-text alignment, and model fusion with post-enhancement techniques, all contributing to superior performance and visual fidelity. Score Distillation Sampling (SDS) is popular objective function that utilizes the strong prior learned by 2D diffusion models to optimize target data point by calculating its gradient as follows: (cid:20) w(t) (ϵϕ(zt, t, cy) ϵ) θLSDS Et,ϵ (2) (cid:21) , θ where = g(θ) is rendered by differentiable image generator parameterized by θ, zt denotes perturbed version of with random amount of noise ϵ, and w(t) is scaling function corresponding to the timestep t. The objective of SDS loss is to provide an updated direction that would move to high-density region of the data manifold using the score function of the diffusion model ϵϕ(zt, t, cy). Notably, this gradient omits the Jacobian term of the diffusion backbone, removing the expensive computation when backpropagating through the entire diffusion model U-Net. Image-Prompt via Decoupled Cross-Attention. IPAdapter [36] introduces an image-prompt condition that can be seamlessly integrated into pre-trained text-toimage generation model. It achieves this through decoupled cross-attention mechanism, which separates the conditioning effects of text and image features. This is done by adding an extra cross-attention layer to each cross-attention layer in the original U-Net. Given image features cx (extracted from by CLIP image encoder), text features cy (from text prompt using CLIP text encoder), and query features Zl from the previous U-Net layer 1, the output hl of the decoupled cross-attention is computed as: hl = Attn(Ql, Ky, Vy) + sx Attn(Ql, Kx, Vx), (3) where is the timestep, and σt, αt, δt are three coefficients. The final latent = z0 is then input to VAE decoder to where Attn(.) denotes the attention operation. Scaling factors sx is used to control the influence of cx on the gener3 Figure 3. Proposed two-stage training for our one-step inversion framework. In stage 1, we warms up our inversion network on synthetic data generated by SwiftBrushv2. At stage 2, we shift our focus to real images, enabling our inversion framework to instantly invert any input images without additional fine-tuning or retraining. ated output. Ql = QZl is the query matrix projected by the weight matrix Q. The key and value matrices for text features cy are Ky = cy, respectively, while the projected key and value matrices for image features cx are Kx = cx. Notably, only the two weight matrices are trainable, while the remaining weights remain frozen to preserve the original behavior of the pretrained diffusion model. cx and Vx = and y cy and Vy = 4. Proposed Method Our goal is to enable instant image editing with the onestep text-to-image model, SBv2. In Sec. 4.1, we develop one-step inversion network that predicts inverted noise to reconstruct source image when passed through SBv2. We introduce two-stage training strategy for this inversion network, enabling single-step reconstruction of any input images without further retraining. An overview is shown in Fig. 3. During inference, as described in Sec. 4.2, we use self-guided editing mask to locate edited regions. Our attention-rescaling technique then utilizes the mask to achieve disentangled editing and control the editing strength while preserving the background. 4.1. Inversion Network and Two-stage Training Given an input image that may be synthetic (generated by model like SBv2) or real, our first objective is to inverse and reconstruct it using SBv2 model. To achieve this, we develop one-step inversion network Fθ to transform the image latent into an inverted noise ˆϵ = Fθ(z, cy), and then feed back to SBv2 to compute the reconstructed latent ˆz = G(ˆϵ, cy) = G(Fθ(z, cy), cy). For synthetic images, training Fθ is straightforward, with pairs (ϵ, z), where ϵ is the noise used to generate z, allowing direct regression of ˆϵ to ϵ, and aligning the inverted noise with SBv2s input noise distribution. However, for real images, the domain gap poses challenge, as the original noise ϵ is unavailable, preventing us from computing regression objective and potentially causing ˆϵ to deviate from the desired distribution. In the following section, we discuss our inversion network and two-stage training strategy designed to overcome these challenges effectively. Our Inversion Network Fθ follows the architecture of the one-step diffusion model and is initialized with Gs weights. However, we found this approach suboptimal: the inverted noise ˆϵ predicted by Fθ attempts to perfectly reconstruct the input image, leading to overfitting on specific patterns from the input. This tailoring makes the noise overly dependent on input features, which limits editing flexibility. To overcome this, we introduce an auxiliary, imageconditioned branch similar to IP-Adapter [36] within the one-step generator G, named GIP. This branch integrates image features encoded from the input image along with text prompt y, aiding in reconstruction and reducing the need for Fθ to embed extensive visual details from the input image. This approach effectively alleviates the burden on ˆϵ, enhancing both reconstruction and editing capabilities. We compute the inverted noise ˆϵ along with the reconstructed image latent ˆz as follows: ˆϵ = Fθ(z, cy), ˆz = GIP(ˆϵ, cy, cx). (4) Stage 1: Training with synthetic images. As mentioned above, this stage aims to pretrain the inversion network Fθ with synthetic training data sampled from text-to-image diffusion network G, i.e., SBv2. In Fig. 3, we visualize the flow of stage 1 training in orange color. Pairs of training samples (ϵ, z) are created as follows: ϵ (0, 1), = G(ϵ, cy). (5) We combine the reconstruction loss Lstage1 rec and regression encourages ˆϵ to capture source image patterns, aiding reconstruction but constraining future editing flexibility (see Fig. 4, column 2). To address this, we introduce new regularization term Lstage2 regu , inspired by Score Distillation Sampling (SDS) as defined in Eq. (2). The SDS gradient steers the optimized latent toward dense regions of the data manifold. Given that the real image latent = E(x) already lies in high-density region, we shift the optimization focus to the noise term ϵ, treating our inverted noise as an added noise to z. We then compute the loss gradient as follows: ˆϵ = Fθ(z, cy), zt = αtz + σtˆϵ, θLstage2 regu t,ˆϵ (cid:20) w(t) (ˆϵ ϵϕ(zt, t, cy)) (cid:21) . ˆϵ θ (8) Our regularization gradient has the opposite sign of Eq. (2) since it optimizes ˆϵ instead of (derivation details in Appendix). After initializing from stage 1, ˆϵ resembles Gaussian noise (0, 1), making the noisy latent zt compatible with the multi-step teachers training data. This allows the teacher to accurately predict ϵϕ(zt, t, cy), and achieve ϵϕ(zt, t, cy) ˆϵ 0. Thus, ˆϵ stays the same. Over time, the reconstruction loss nudges Fθ to generate an inverted noise, ˆϵ, tailored for reconstruction, diverging from (0, 1) and creating an unfamiliar zt. The resulting gradient prevents excessive drift from the original distribution, reinforcing stability from stage 1, as shown in third column of Fig. 4. Similar to stage 1, we combine both perceptual losses Lstage2 regu where we set λstage2 = 1. During training , we train only the inversion network, keeping the IP-Adapter branch and decoupled cross-attention layers frozen to retain the image prior features learned in stage 1. Flow of training stage 2 are visualized as teal color in Fig. 3. perceptual and regularization loss Lstage2 4.2. Attention Rescaling for Mask-aware Editing (ARaM) During inference, given source image xsource, source prompt ysource, and an editing prompt yedit, our target is to produce an edited image xedit following the editing prompt without modifying irrelevant background elements. After two-stage training, we obtain well-trained inversion network Fθ to transform source image latent zsource = E(xsource) to inverted noise ˆϵ. Intuitively, we can use the one-step image generator, GIP(.), to regenerate the image but with an edit prompt embedding cedit as guided prompt instead. The edited image latent is computed via zedit = GIP(ˆϵ, cedit , cx). As discussed in Sec. 4.1, the source image condition cx is crucial for reconstruction, with its influence modulated by sx as shown in Eq. (3). To illustrate this, we vary sx while generating the edited image xedit = D(zedit) in orange block of Fig. 5b. As shown, higher values of sx enforce fidelity to the source image, limiting editing flexiy Figure 4. Comparison of inverted noise predicted by our inversion network when trained without and with stage 2 regularization loss. regr loss Lstage1 to train the inversion network Fθ and part of the IP-Adapter branch (including the linear mapping and crossattention layers for image conditions). The regression loss Lstage1 encourages Fθ(.) to produce an inverted noise ˆϵ that regr closely follows SBv2s input noise distribution by regressing ˆϵ to ϵ. This ensures that the inverted noise remains close to the multivariate normal distribution, which is crucial for effective editability as shown in prior work [18]. On the other hand, the reconstruction loss Lstage1 enforces alignment between the reconstructed latent ˆz and the original source latent z, preserving input image details. In summary, the training objectives are as follows: rec Lstage1 rec = ˆz2 2, Lstage regr = ϵ ˆϵ2 2, Lstage1 = Lstage1 rec + λstage1.Lstage1 regr , (6) (7) where we set λstage1 = 1 during training. After this stage, our inversion framework could reconstruct source input images generated by the SBv2 model. However, it fails to work with real images due to the domain gap which motivates us to continue training with stage 2. Stage 2: Training with real images. We replace the reconstruction loss from stage 1 with perceptual loss using the Deep Image Structure and Texture Similarity (DISTS) metric [7]. This perceptual loss, Lstage2 perceptual = DISTS(x, ˆx), compares ˆx = D(ˆz) (where ˆz = GIP(ˆϵ, cy, cx)) with the real input image x. DISTS is trained on real images, capturing perceptual details in structure and texture, making it more robust visual similarity measure than the pixel-wise reconstruction loss used in stage 1. Since the original noise ϵ, used to reconstruct in SBv2, is unavailable at this stage, we cannot directly apply the regression objective from stage 1. Training stage 2 solely with Lstage2 perceptual can cause the inverted noise ˆϵ to drift from the ideal noise distribution (0, I), as the perceptual loss 5 (a) Self-guided editing mask extraction. Given source and editing prompts, our inversion network predicts two different noise maps, highlighting the editing regions . in an edited image which both follow prompt edit semantics and achieve good background preservation compared to using the same sx. On the other hand, we introduce the additional sy to lessen/strengthen the edit prompt-alignment effect within the editing region which could be used to control the editing strength as shown in Fig. 5c. The editing mask discussed above can either be provided by the user or generated automatically from our inversion network Fθ. To extract self-guided editing mask, we observe that well-trained Fθ can discern spatial semantic differences in the inverted noise maps when conditioned on varying text prompts. As shown in Fig. 5a, we input the source image latent zsource to Fθ with two different text prompts: the source csource . The difference noise map, ˆϵsource ˆϵedit, is then computed and normalized, yielding the editing mask , which effectively highlights the editing areas. and the edit cedit (b) Effect of global scale and our edit-aware scale. Comparison of edited results between varying global image condition scale sx with our ARaM. 5. Experiments 5.1. Experimental Setup Dataset and evaluation metrics. We evaluate our editing performance on PieBench [11], popular benchmark containing 700 samples across 10 diverse editing types. Each sample includes source prompt, edit prompt, instruction prompt, source image, and manually annotated editing mask. Using PieBenchs metrics, we assess both background preservation and editing semantics, aiming for balance between them for high-quality edits. Background preservation is evaluated with PSNR and MSE scores on unedited regions of the source and edited images. Editing alignment is assessed using CLIP-Whole and CLIP-Edited scores, measuring prompt alignment with the full image and edited region, respectively. Implementation details. Our inversion network is based on the architecture of SBv2, initialized with SBv2 weights for stage 1 training. In stage 2, we continue training from stage 1s pretrained weights. For image encoding, we adopt the IP-Adapter [36] design, using pretrained CLIP image encoder followed by small projection network that maps the image embeddings to sequence of features with length = 4, matching the text feature dimensions of the diffusion model. Both stages use the Adam optimizer [12] with weight decay of 1e-4, learning rate of 1e-5, and an exponential moving average (EMA) in every iteration. In stage 1, we train with batch size of 4 for 100k iterations on synthetic samples generated by SBv2, paired with 40k captions from the JourneyDB dataset [32]. For stage 2, we train with batch size of 1 and train over 180k iterations using 5k real images and their prompt descriptions from the CommonCanvas dataset [9]. All experiments are conducted on single NVIDIA A100 40GB GPU. (c) Effect of editing strength scale. Visualization of edited results when varying mask-based text-alignment scale sy. Figure 5. Illustration of Attention Rescaling for Mask-aware Editing (ARaM). We apply attention rescaling with our selfguided editing mask to achieve local image editing and enable editing strength control. bility due to tight control by cx. Conversely, lower sx allows more flexible edits but reduces reconstruction quality. Based on this observation, we introduce Attention Rescaling for Mask-aware editing (ARaM) in GIP, guided by the editing mask . The key idea is to amplify the influence of cx in non-edited regions for better preservation while reducing its effect within edited regions, providing greater editing flexibility. To implement this, we reformulate the computation in Eq. (3) within GIP by removing the global scale sx and introducing region-specific scales as follows: hl = sy.M. Attn(Ql, Ky, Vy) + sedit.M. Attn(Ql, Kx, Vx) + snon-edit.(1 ). Attn(Ql, Kx, Vx). (9) This disentangled cross-attention differs slightly from Eq. (3) in three scaling factors: sy, sedit, and snon-edit, apply on different image regions. Two scaling factors sedit, and snon-edit are used to separately control the influence of the image condition cx on the editing and non-editing regions. As shown in violet block of Fig. 5b, this effectively results 6 Type Method Background Preservation CLIP Semantics PSNR MSE104 Whole Edited DDIM + P2P NT-Inv + P2P DDIM + MasaCtrl Direct Inversion + MasaCtrl DDIM + P2P-Zero Direct Inversion + P2P-Zero DDIM + PnP Direct Inversion + PnP ReNoise (SDXL Turbo) TurboEdit ICD (SD 1.5) SwiftEdit (Ours) SwiftEdit (Ours with GT masks) 17.87 27.03 22.17 22.64 20.44 21.53 22.28 22.46 20.28 22.43 26.93 23.33 23. Multi-step (50 steps) Few-steps (4 steps) One-step 219.88 35.86 86.97 81.09 144.12 127. 83.64 80.45 54.08 9.48 3.32 6.60 6.18 25.01 24.75 23.96 24.38 22.80 23. 25.41 25.41 24.29 25.49 22.42 25.16 25.56 22.44 21.86 21.16 21.35 20.54 21. 22.55 22.62 21.07 21.82 19.07 21.25 21.91 Runtime (seconds) 25.98 134.06 23.21 29. 35.57 35.34 12.62 12.79 5.11 1.32 1.62 0.23 0.23 Table 1. Quantitative comparison of SwiftEdit against other editing methods with metrics employed from PieBench [11]. . Figure 6. Comparative edited results. The first column shows the source image, while source and edit prompts are noted under each row. Comparison Methods. We perform an extensive comparison of SwiftEdit with representative multi-step and recently introduced few-step image editing methods. For multi-step methods, we choose Prompt-to-Prompt (P2P) [10], MasaCtrl [3], Pix2Pix-Zero (P2P-Zero) [21], and Plug-and-Play [33], combined with corresponding inversion methods such as DDIM [29], Null-text Inversion (NT-Inv) [18], and Direct Inversion [11]. For few-step methods, we select Renoise [8], TurboEdit [6], and ICD [31]. 5.2. Comparison with Prior Methods Quantitative Results. In Tab. 1, we present the quantitative results comparing SwiftEdit to various multi-step 7 Figure 7. User Study. Method PSNR LPIPS103 MSE104 SSIM102 w/o stage 1 w/o stage 2 w/o IP-Adapter 22.26 17.95 18.57 111.57 305.23 165.78 Full Setting (Ours) 24. 89.69 7.03 17.46 16.11 4.59 72.39 55.97 63.87 76.34 Table 2. Impact of inversion framework design on real image reconstruction. Setting Lstage1 regr Lstage2 regu Setting 1 Setting 2 Setting 3 Setting 4 (Full) CLIP Semantics Whole () Edited() 22.91 22.98 24.19 25.16 19.07 19.01 20.55 21.25 Table 3. Effect of loss on editing semantics score. and few-step image editing methods. Overall, SwiftEdit demonstrates superior time efficiency due to our one-step inversion and editing process, while maintaining competitive editing performance. Compared to multi-step methods, SwiftEdit shows strong results in background preservation scores, surpassing most approaches. Although it achieves slightly lower PSNR score than NT-Inv + P2P, it has better MSE score and is approximately 500 times faster. In terms of CLIP Semantics, we also achieve competitive results in CLIP-Whole (second best) and CLIP-Edited. Compared with few-step methods, SwiftEdit performs as the secondbest in background preservation (with ICD being the best) and second-best in CLIP Semantics (with TurboEdit leading), while maintaining speed advantage, being at least 5 times faster than these methods. Since SwiftEdit allows for user-defined editing masks, we also report results using the ground-truth editing masks from PieBench [11]. As shown in the last row of Tab. 1, results with the ground-truth masks show slight improvements, indicating that our self-guided editing masks are nearly as accurate as the ground truth. Qualitative Results. In Fig. 6, we present visual comparisons of editing results generated by SwiftEdit and other methods. As illustrated, SwiftEdit successfully adheres to the given edit prompt while preserving essential background details. This balance demonstrates SwiftEdits strength over other multi-step methods, as it produces highquality edits while being significantly faster. When compared to few-step methods, SwiftEdit demonstrates clear advantage in edit quality. Although ICD [31] scores high on background preservation (as shown in Tab. 1), it often fails to produce edits that align with the prompt. TurboEdit 8 [6], while achieving higher CLIP score than SwiftEdit, generates lower-quality results that compromise key background elements, as seen in the first, second, and fifth rows of Fig. 6. This further emphasizes SwiftEdits ability to produce high-quality edits with prompt alignment, and background preservation. User Study. We conducted user study with 140 participants to evaluate preferences for different editing results. Using 20 random edit prompts from PieBench [11], participants compared images edited by three methods: Null-text Inversion [18], TurboEdit [6], and our SwiftEdit. Participants selected the most appropriate edits based on background preservation and editing semantics. As shown in Fig. 7, SwiftEdit was the preferred choice, with 47.8% favoring it for editing semantics and 40% for background preservation, while also surpassing other methods in speed. 6. Ablation Study Analysis of Inversion Framework Design. We conduct ablation studies to evaluate the impact of our inversion framework and two-stage training on image reconstruction. Our two-stage strategy is essential for the one-step inversion frameworks effectiveness. In Tab. 2, we show that omitting any stages degrades reconstruction quality. The IP-Adapter with decoupled cross-attention is critical; removing it leads to poor reconstruction, as seen in row 3. Effect of loss on Editing Quality. As noted by [18], an editable noise should follow normal distribution to ensure flexibility. We conduct ablation studies to assess the impact of our loss functions on noise editability. As shown in Tab. 3, omitting any loss component reduces editability, measured by CLIP Semantics, while using both yields the highest scores. This emphasizes the importance of each loss in maintaining noise distributions that enhance editability. 7. Conclusion and Discussion Conclusion. In this work, we introduce SwiftEdit, lightning-fast text-guided image editing tool capable of instant edits in 0.23 seconds. Extensive experiments demonstrate SwiftEdits ability to deliver high-quality results while significantly surpassing previous methods in speed, enabled by its one-step inversion and editing process. We hope SwiftEdit will facilitate interactive image editing. Discussion. While SwiftEdit achieves instant-level image editing, challenges remain. Its performance still relies on the quality of the SBv2 generator, thus, biases in the training data can transfer to our inversion network. For future work, we want to improve the method by transitioning from instant-level to real-time editing capabilities. This enhancement would address current limitations and have significant impact across various fields. SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we first provide detailed derivation of the regularization loss used in Stage 2, as outlined in Sec. 8. Next, we present an additional ablation study on various one-step diffusion models, along with sensitivity analysis of different scales for sedit, snon-edit, and sy in Sec. 9. Finally, we include more qualitative results in Sec. 10, and discuss societal impacts in Sec. 11. 8. Derivation of the Regularization Loss in Stage 2 We provide detailed derivation of the gradient of our proposed regularization loss, as defined in Eq. (8) of the main paper. The regularization loss is formulated as follows: regu = Lstage2 t,ˆϵ (cid:2)w(t)ϵϕ(zt, t, cy) ˆϵ2 2 (cid:3) , (10) where ϵϕ(.) is teacher denoising UNet, here, we use SD 2.1 in our implementation. The gradient of the loss w.r.t our inversion networks parameters θ is computed as: θLstage2 regu t,ˆϵ [w(t)(ϵϕ(zt, t, cy) ˆϵ) ( ϵϕ(zt, t, cy) θ ˆϵ θ )], (11) where we absorb all constants into w(t). Expanding the term ϵϕ(zt,t,cy) , we have: θ ϵϕ(zt, t, cy) θ = ϵϕ(zt, t, cy) zt zt z θ . (12) Since (extracted from real images) and θ are independent, θ = 0, thus, we can turn Eq. (11) into: θLstage2 regu t,ˆϵ (cid:20) w(t)(ϵϕ(zt, t, cy) ˆϵ)( (cid:20) w(t)(ˆϵ ϵϕ(zt, t, cy)) = t,ˆϵ ˆϵ θ (cid:21) ) ˆϵ θ (cid:21) , (13) (14) which has the opposite sign of the SDS gradient w.r.t loss as discussed in the main paper. 9. Additional Ablation Studies Combined with other one-step text-to-image models. As discussed in the main paper, our inversion framework is not limited to SBv2 and can be seamlessly integrated with other one-step text-to-image generators. To demonstrate this, we conducted experiments replacing SBv2 with alternative models, including DMD2 [37], InstaFlow [14], and 1 Model PSNR CLIP-Whole CLIP-Edited Ours + InstaFlow 24.88 Ours + DMD2 26.08 Ours + SBv1 25.09 Ours + SBv2 (SwiftEdit) 23.33 24.03 23.35 23.64 25.16 20.47 19.84 19.96 21.25 Table 4. Ablation studies on combining our technique with other means that these one-step text-to-image generation models. models are based on SD 1.5 while means that these models are based on SD 2.1. Figure 8. Qualitative results when combining our inversion framework with other one-step text-to-image generation models. SBv1 [19]. For these experiments, the architecture and pretrained weights of each generator were used to initialize our inversion network in Stage 1. Specifically, DMD2 was implemented using the SD 1.5 backbone, while InstaFlow uses SD 1.5. All training experiments for both stages were conducted on the same dataset, similar to the experiments presented in Tab. 1 of the main paper. Figure 8 presents edited results obtained by integrating our inversion framework with different one-step image generators. As shown, these one-step models integrate well with our framework, enabling effective edits. Additionally, quantitative results are provided in Tab. 4. The results indicate that our inversion framework combined with SBv2 (SwiftEdit) achieves the best editing performance in terms of CLIP-Whole and CLIP-Edited scores, while DMD2 demonstrates superior background preservation. Varying scales. To better understand the effect of varying scales used in Eq. (9) in the main paper, we present two comprehensive plots evaluating the performance of (a) Varying sedit scale at different levels of snon-edit with default sy = 2. (b) Varying sy scale at different levels of snon-edit with default sedit = 0. Figure 9. Effects on background preservation and editing semantics while varying sedit and sy at different levels of snon-edit. Figure 10. Visualization of our extracted mask along with edited results using guided text described under each image row. SwiftEdit on 100 random test samples from the PieBench benchmark. Particularly, the plots depict results for varying sedit {0, 0.2, 0.4, 0.6, 0.8, 1} (see Fig. 9a) or sy {0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4} (see Fig. 9b) at different levels of snon-edit {0.2, 0.4, 0.6, 0.8, 1}. As shown in Fig. 9a, it is evident at different levels of snon-edit that lower sedit generally improves editing semantics (CLIP-Edited scores) but slightly compromises background preservation (PSNR). Conversely, higher sy can enhance prompt-image alignment (CLIP-Edited scores, Fig. 9b), but excessive values (sy > 2) In all of our expermay harm prompt-alignment result. iments, we use default choice of scale parameters setting where we set sedit = 0, snon-edit = 1, and sy = 2. Figure 11. Edit images with flexible prompting. SwiftEdit achieves satisfactory reconstructed and edited results with flexible source and edit prompt input (denoted under each image). 10. More Qualitative Results Self-guided Editing Mask. In Fig. 10, we show more editing examples along with self-guided editing masks extracted directly from our inversion network. Flexible Prompting. As shown in Fig. 11, SwiftEdit con2 Figure 12. Face identity and expression editing via simple prompts. Given portrait input image, SwiftEdit can perform variety of facial identities along with expression editing scenarios guided by simple text within just 0.23 seconds. sistently reconstructs images with high fidelity, even with minimal source prompt input. It operates effectively with just single keyword (last three rows) or no prompt at all (first two rows). Notably, SwiftEdit performs complex edits with ease, as demonstrated in the last row of Fig. 11, by simply combining keywords in the edit prompt. These results highlight its capabilities as lightning-fast and userfriendly editing tool. Facial Identity and Expression Editing. In Fig. 12, given simple source prompt man and portrait image, SwiftEdit can achieve face identity and facial expression editing via simple edit prompt by just combining expression word (denoted on each row) and identity word (denoted on each column). Additional Results on PieBench. In Figs. 13 to 15, we provide extensive editing results compared with other methods on the PieBench benchmark. 11. Societal Impacts As an AI-powered visual generation tool, SwiftEdit delivers lightning-fast, high-quality, and customizable editing capabilities through simple prompt inputs, significantly enhancing the efficiency of various visual creation tasks. However, societal challenges may arise as such tools could be exploited for unethical purposes, including generating sensitive or harmful content to spread disinformation. Addressing these concerns are essential and several ongoing works have been conducted to detect and localize AI-manipulated images to mitigate potential misuse."
        },
        {
            "title": "References",
            "content": "[1] David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and Antonio Torralba. Inverting layers of large generator. In ICLR workshop, page 4, 2019. 3 [2] David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and Antonio Torralba. Seeing what gan cannot generate. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4502 4511, 2019. 3 [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2256022570, 2023. 2, 3, 7 [4] Antonia Creswell and Anil Anthony Bharath. Inverting the generator of generative adversarial network. IEEE transactions on neural networks and learning systems, 30(7):1967 1974, 2018. 3 [5] Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher, 2024. 1, 2, 3 Figure 13. Comparative results on the PieBench benchmark 4 Figure 14. Comparative results on the PieBench benchmark 5 Figure 15. Comparative results on the PieBench benchmark [6] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models, 2024. 2, 3, 7, 8 [21] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation, 2023. 3, 7 [7] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli. Image quality assessment: Unifying structure and texture similarity. CoRR, abs/2004.07728, 2020. 5 [22] Guim Perarnau, Joost Van De Weijer, Bogdan Raducanu, and Jose Alvarez. Invertible conditional gans for image editing. arXiv preprint arXiv:1611.06355, 2016. 3 [8] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising, 2024. 2, 3, 7 [9] Aaron Gokaslan, Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: An open diffusion model trained with creativecommons images. arXiv preprint arXiv:2310.16825, 2023. [10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022. 3, 7 [11] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. International Conference on Learning Representations (ICLR), 2024. 1, 3, 6, 7, 8 [12] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [13] Zachary Lipton and Subarna Tripathi. Precise recovery of latent vectors from generative adversarial networks. arXiv preprint arXiv:1702.04782, 2017. 3 [14] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for high-quality In International diffusion-based text-to-image generation. Conference on Learning Representations, 2024. 2, 3, 1 [15] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [16] Fangchang Ma, Ulas Ayaz, and Sertac Karaman. Invertibility of convolutional generative networks from partial measurements. Advances in Neural Information Processing Systems, 31, 2018. 3 [17] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 2 [18] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 60386047, 2023. 1, 2, 3, 5, 7, 8 [23] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 1, 2 [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. [25] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. 1, [26] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 2 [27] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation, 2024. 1, 3 [28] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2025. 2, 3 [29] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. CoRR, abs/2010.02502, 2020. 1, 2, 7 [30] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. 2 [31] Nikita Starodubcev, Mikhail Khoroshikh, Artem Babenko, and Dmitry Baranchuk. Invertible consistency distillation for text-guided image editing in around 7 steps. arXiv preprint arXiv:2406.14539, 2024. 2, 3, 7, 8 [32] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in Neural Information Processing Systems, 36, 2024. 6 [33] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19211930, 2023. 2, 3, 7 [19] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation, 2024. 1, 2, 3 [34] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. High-fidelity gan inversion for image attribute editing, 2024. 2, [20] Trong-Tung Nguyen, Duc-Anh Nguyen, Anh Tran, and Cuong Pham. Flexedit: Flexible and controllable diffusionbased object-centric image editing, 2024. 2, 3 [35] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: survey, 2022. 2 7 [36] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. 2023. 3, 4, 6 [37] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William T. Freeman. Improved distribution matching distillation for fast image synthesis, 2024. 1, 2, 3 [38] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation, 2024. 1, 2, [39] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. Indomain gan inversion for real image editing, 2020. 2, 3 [40] Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei Efros. Generative visual manipulation on the natural image manifold. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part 14, pages 597613. Springer, 2016."
        }
    ],
    "affiliations": [
        "Posts & Telecom. Inst. of Tech., Vietnam",
        "VinAI Research"
    ]
}